natural language processing

jacob eisenstein

november 13, 2018

contents

contents

preface

background .
.
how to use this book .

.

.

.

.

.
.

.
.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

introduction
1.1 natural language processing and its neighbors . . . . . . . . . . . . . . . . .
1.2 three themes in natural language processing . . . . . . . . . . . . . . . . . .
1.2.1 learning and knowledge . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.2
search and learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1.2.3 relational, compositional, and distributional perspectives . . . . . .

i learning

2 linear text classi   cation
2.1 the bag of words .
2.2 na    ve bayes .
.

.

.

.
.

.
.

.
.

.
.
.

.
.
.

.
.
.

.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.1 types and tokens .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
2.2.2 prediction .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.3 estimation .
.
smoothing .
2.2.4
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
setting hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2.5
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 id88 .
2.3.2 averaged id88 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 id168s and large-margin classi   cation . . . . . . . . . . . . . . . . .
2.4.1 online large margin classi   cation . . . . . . . . . . . . . . . . . . . .
2.4.2
*derivation of the online support vector machine . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.3 discriminative learning .
.

2.5 id28 .

.

.

.

.

.

.

.

1

1

i
i
ii

1
1
6
6
7
9

11

13
13
17
19
20
21
22
23
24
25
27
27
30
32
35

2

contents

.

.
.

2.6 optimization .

. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.5.1 id173 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5.2 gradients
.
.
2.6.1 batch optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6.2 online optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
*additional topics in classi   cation . . . . . . . . . . . . . . . . . . . . . . .
.
2.7.1
feature selection by id173 . . . . . . . . . . . . . . . . . . . .
2.7.2 other views of id28 . . . . . . . . . . . . . . . . . . . . .
.

2.8 summary of learning algorithms . . . . . . . . . . . . . . . . . . . . . . . .

2.7

3 nonlinear classi   cation

. . . . . . . . . . . . . . . . . . . . . . . . . . .

3.1 feedforward neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2 designing neural networks
.
3.2.1 id180 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.2 network structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.2.3 outputs and id168s . . . . . . . . . . . . . . . . . . . . . . . .
3.2.4
. . . . . . . . . . . . . . . . . . . . . . . . .
3.3 learning neural networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.1 id26 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.2 id173 and dropout . . . . . . . . . . . . . . . . . . . . . . . .
3.3.3
*learning theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.3.4 tricks .
3.4 convolutional neural networks . . . . . . . . . . . . . . . . . . . . . . . . . .

inputs and lookup layers

.

.

.

.

4 linguistic applications of classi   cation

4.1 sentiment and opinion analysis . . . . . . . . . . . . . . . . . . . . . . . . .

4.3 design decisions for text classi   cation . . . . . . . . . . . . . . . . . . . . .

.
4.1.1 related problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.2 alternative approaches to id31 . . . . . . . . . . . . . .
4.2 id51 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.2.1 how many word senses?
. . . . . . . . . . . . . . . . . . . . . . . . .
4.2.2 id51 as classi   cation . . . . . . . . . . . . . .
.
4.3.1 what is a word? . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.2 how many words? . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3.3 count or binary? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.4 evaluating classi   ers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
4.4.1 precision, recall, and f -measure . . . . . . . . . . . . . . . . . . .
4.4.2 threshold-free metrics . . . . . . . . . . . . . . . . . . . . . . . . . .
.
4.4.3 classi   er comparison and statistical signi   cance . . . . . . . . . . . .
4.4.4
*multiple comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4.5 building datasets .

36
37
37
38
39
41
41
41
43

47
48
50
50
51
52
53
53
55
57
58
59
62

69
69
70
72
73
74
75
76
76
79
80
80
81
83
84
87
88

jacob eisenstein. draft of november 13, 2018.

contents

3

4.5.1 metadata as labels
4.5.2 labeling data .
.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

88
88

.

5 learning without supervision
5.1 unsupervised learning .

95
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
5.1.1 id116 id91 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
5.1.2 expectation-maximization (em)
. . . . . . . . . . . . . . . . . . . . .
98
5.1.3 em as an optimization algorithm . . . . . . . . . . . . . . . . . . . . . 102
5.1.4 how many clusters? . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.2 applications of expectation-maximization . . . . . . . . . . . . . . . . . . . . 104
5.2.1 word sense induction . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
5.2.2
semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . 105
5.2.3 multi-component modeling . . . . . . . . . . . . . . . . . . . . . . . . 106
semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.3.1 multi-view learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.3.2 graph-based algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 109
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.4.1
supervised id20 . . . . . . . . . . . . . . . . . . . . . . 111
5.4.2 unsupervised id20 . . . . . . . . . . . . . . . . . . . . 112
*other approaches to learning with latent variables
. . . . . . . . . . . . . . 114
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
5.5.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.5.2

.
sampling .
spectral learning .

5.4 id20 .

5.3

5.5

.

.

.

.

.

.

.

.

ii sequences and trees

123

6 language models

.

.

.

.

.

.

smoothing .

125
6.1 id165 language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
6.2
smoothing and discounting . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
6.2.1
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
6.2.2 discounting and backoff . . . . . . . . . . . . . . . . . . . . . . . . . . 130
6.2.3
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
*interpolation .
*kneser-ney smoothing . . . . . . . . . . . . . . . . . . . . . . . . . . 133
6.2.4
6.3 recurrent neural network language models . . . . . . . . . . . . . . . . . . . 133
6.3.1 id26 through time . . . . . . . . . . . . . . . . . . . . . . 136
6.3.2 hyperparameters .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
6.3.3 gated recurrent neural networks . . . . . . . . . . . . . . . . . . . . . 137
6.4 evaluating language models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
6.4.1 held-out likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
6.4.2 perplexity .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141

.
6.5 out-of-vocabulary words

.

.

.

under contract with mit press, shared under cc-by-nc-nd license.

4

contents

7 sequence labeling

.

.

id136 .

7.4.1 estimation .
7.4.2
.

7.3.1 example .
7.3.2 higher-order features

145
7.1 sequence labeling as classi   cation . . . . . . . . . . . . . . . . . . . . . . . . 145
7.2 sequence labeling as structure prediction . . . . . . . . . . . . . . . . . . . . 147
7.3 the viterbi algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
. . . . . . . . . . . . . . . . . . . . . . . . . . . 153
7.4 id48 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
7.5 discriminative sequence labeling with features . . . . . . . . . . . . . . . . . 157
structured id88 . . . . . . . . . . . . . . . . . . . . . . . . . . . 160
7.5.1
7.5.2
structured support vector machines . . . . . . . . . . . . . . . . . . . 160
7.5.3 conditional random    elds . . . . . . . . . . . . . . . . . . . . . . . . . 162
7.6 neural sequence labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
7.6.1 recurrent neural networks
. . . . . . . . . . . . . . . . . . . . . . . . 167
7.6.2 character-level models . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
7.6.3 convolutional neural networks for sequence labeling . . . . . . . . 170
*unsupervised sequence labeling . . . . . . . . . . . . . . . . . . . . . . . . . 170
7.7.1 linear dynamical systems . . . . . . . . . . . . . . . . . . . . . . . . . 172
7.7.2 alternative unsupervised learning methods
. . . . . . . . . . . . . . 172
semiring notation and the generalized viterbi algorithm . . . . . . . 172
7.7.3

7.7

8 applications of sequence labeling

175
8.1 part-of-speech tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
8.1.1 parts-of-speech . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
8.1.2 accurate part-of-speech tagging . . . . . . . . . . . . . . . . . . . . . 180
8.2 morphosyntactic attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
8.3 id39 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
8.4 id121 .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
8.5 code switching .
8.6 dialogue acts
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187

.
.
.

.

.

9 formal language theory
9.1 regular languages

191
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
9.1.1
finite state acceptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
9.1.2 morphology as a regular language . . . . . . . . . . . . . . . . . . . . 194
9.1.3 weighted    nite state acceptors . . . . . . . . . . . . . . . . . . . . . . 196
finite state transducers
9.1.4
. . . . . . . . . . . . . . . . . . . . . . . . . . 201
9.1.5
*learning weighted    nite state automata . . . . . . . . . . . . . . . . 206
9.2 context-free languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
. . . . . . . . . . . . . . . . . . . . . . . . . . 208

9.2.1 context-free grammars

jacob eisenstein. draft of november 13, 2018.

contents

5

9.2.2 natural language syntax as a context-free language . . . . . . . . . . 211
9.2.3 a phrase-structure grammar for english . . . . . . . . . . . . . . . . 213
9.2.4 grammatical ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . 218
*mildly context-sensitive languages
. . . . . . . . . . . . . . . . . . . . . . . 218
9.3.1 context-sensitive phenomena in natural language . . . . . . . . . . . 219
9.3.2 id35
. . . . . . . . . . . . . . . . . . . . 220

9.3

10 context-free parsing

.

.

.

.

.
.

.
.

10.2 ambiguity .

.
.
10.2.1 parser evaluation .
.
10.2.2 local solutions .

225
10.1 deterministic bottom-up parsing . . . . . . . . . . . . . . . . . . . . . . . . . 226
10.1.1 recovering the parse tree . . . . . . . . . . . . . . . . . . . . . . . . . 227
10.1.2 non-binary productions . . . . . . . . . . . . . . . . . . . . . . . . . . 227
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
10.1.3 complexity .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
10.3 weighted context-free grammars . . . . . . . . . . . . . . . . . . . . . . . . 232
10.3.1 parsing with weighted context-free grammars . . . . . . . . . . . . . 234
10.3.2 id140 . . . . . . . . . . . . . . . . . . . 235
10.3.3 *semiring weighted context-free grammars . . . . . . . . . . . . . . . 237
10.4 learning weighted context-free grammars . . . . . . . . . . . . . . . . . . . . 238
10.4.1 id140 . . . . . . . . . . . . . . . . . . . 238
10.4.2 feature-based parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
10.4.3 *conditional random    eld parsing . . . . . . . . . . . . . . . . . . . . 240
10.4.4 neural context-free grammars
. . . . . . . . . . . . . . . . . . . . . . 242
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
10.5.1 parent annotations and other tree transformations . . . . . . . . . . . 243
10.5.2 lexicalized context-free grammars . . . . . . . . . . . . . . . . . . . . 244
10.5.3 *re   nement grammars
. . . . . . . . . . . . . . . . . . . . . . . . . . 248
10.6 beyond context-free parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
10.6.1 reranking .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
10.6.2 transition-based parsing . . . . . . . . . . . . . . . . . . . . . . . . . . 251

10.5 grammar re   nement .

.

.

.

.

.

.

11 id33

.

11.1 dependency grammar .

257
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
11.1.1 heads and dependents . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
11.1.2 labeled dependencies . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
. . . . . . . . . . . . . . . . . 260
11.1.3 dependency subtrees and constituents
11.2 graph-based id33 . . . . . . . . . . . . . . . . . . . . . . . . 262
11.2.1 graph-based parsing algorithms . . . . . . . . . . . . . . . . . . . . . 264
11.2.2 computing scores for dependency arcs . . . . . . . . . . . . . . . . . 265
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
11.2.3 learning .

.

.

.

.

.

under contract with mit press, shared under cc-by-nc-nd license.

6

contents

11.3 transition-based id33 . . . . . . . . . . . . . . . . . . . . . . 268
11.3.1 transition systems for id33 . . . . . . . . . . . . . . . 269
11.3.2 scoring functions for transition-based parsers
. . . . . . . . . . . . . 273
. 274
11.3.3 learning to parse . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277

11.4 applications .

.

.

.

iii meaning

283

12 logical semantics

285
12.1 meaning and denotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286
. 287
12.2 logical representations of meaning . . . . . . . . . . . . . . . . . . . . . . .
. 287
12.2.1 id118 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12.2.2 id85 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. 288
. 291
12.3 id29 and the id198 . . . . . . . . . . . . . . . . . . .
. 292
12.3.1 the id198 . . . . . . . . . . . . . . . . . . . . . . . . . . .
12.3.2 quanti   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. 293
12.4 learning semantic parsers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
. 297
. 299
. 301

12.4.1 learning from derivations . . . . . . . . . . . . . . . . . . . . . . . .
12.4.2 learning from logical forms . . . . . . . . . . . . . . . . . . . . . . .
12.4.3 learning from denotations
. . . . . . . . . . . . . . . . . . . . . . .

13 predicate-argument semantics

.
.

.
.

.
13.1 semantic roles .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13.1.1 verbnet
13.1.2 proto-roles and propbank . . . . . . . . . . . . . . . . . . . . . . . .
13.1.3 framenet
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

305
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
. 308
. 309
. 310
13.2 id14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312
13.2.1 id14 as classi   cation . . . . . . . . . . . . . . . . .
. 312
13.2.2 id14 as constrained optimization . . . . . . . . . . 315
. 317
13.2.3 neural id14 . . . . . . . . . . . . . . . . . . . . . .
13.3 id15 . . . . . . . . . . . . . . . . . . . . . . . .
. 318
. 321
13.3.1 amr parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

.

14 distributional and distributed semantics

325
14.1 the distributional hypothesis . . . . . . . . . . . . . . . . . . . . . . . . . . . 325
. 327
14.2 design decisions for word representations . . . . . . . . . . . . . . . . . . .
. 327
14.2.1 representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. 328
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14.2.2 context .
.
14.2.3 estimation .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. 329
14.3 latent semantic analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329

.

jacob eisenstein. draft of november 13, 2018.

contents

7

.

.

.

.

.

.

.

.

.

.

14.4 brown clusters .
.
14.5 neural id27s .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
14.5.1 continuous bag-of-words (cbow) . . . . . . . . . . . . . . . . . . . . 334
14.5.2 skipgrams .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
14.5.3 computational complexity . . . . . . . . . . . . . . . . . . . . . . . . 335
14.5.4 id27s as id105 . . . . . . . . . . . . . . . . 337
14.6 evaluating id27s . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
14.6.1 intrinsic evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
14.6.2 extrinsic evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
14.6.3 fairness and bias .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
14.7 distributed representations beyond distributional statistics . . . . . . . . . . 341
14.7.1 word-internal structure . . . . . . . . . . . . . . . . . . . . . . . . . . 341
14.7.2 lexical semantic resources . . . . . . . . . . . . . . . . . . . . . . . . . 343
14.8 distributed representations of multiword units . . . . . . . . . . . . . . . . . 344
14.8.1 purely distributional methods
. . . . . . . . . . . . . . . . . . . . . . 344
14.8.2 distributional-compositional hybrids . . . . . . . . . . . . . . . . . . 345
14.8.3 supervised compositional methods
. . . . . . . . . . . . . . . . . . . 346
14.8.4 hybrid distributed-symbolic representations . . . . . . . . . . . . . . 346

15 reference resolution

.

.

.

.

.
.
.

.
.
.

15.1 forms of referring expressions

15.1.1 pronouns
.
15.1.2 proper nouns .
15.1.3 nominals
.

351
. . . . . . . . . . . . . . . . . . . . . . . . . . 352
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357
15.2 algorithms for coreference resolution . . . . . . . . . . . . . . . . . . . . . . 358
15.2.1 mention-pair models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
15.2.2 mention-ranking models
. . . . . . . . . . . . . . . . . . . . . . . . . 360
15.2.3 transitive closure in mention-based models . . . . . . . . . . . . . . . 361
15.2.4 entity-based models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
15.3 representations for coreference resolution . . . . . . . . . . . . . . . . . . . . 367
15.3.1 features .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
15.3.2 distributed representations of mentions and entities . . . . . . . . . . 370
15.4 evaluating coreference resolution . . . . . . . . . . . . . . . . . . . . . . . . . 373

.

.

.

.

.

16 discourse

16.1 segments .

.

.

.

.

.

.

.

.

.

379
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379
16.1.1 topic segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
16.1.2 functional segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . 381
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 382
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383

.
16.2.1 centering theory .
.
16.2.2 the entity grid .

.

16.2 entities and reference .

under contract with mit press, shared under cc-by-nc-nd license.

8

contents

16.3 relations .

.

.

.

.

.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16.2.3 *formal semantics beyond the sentence level . . . . . . . . . . . . . . 384
. 385
16.3.1 shallow discourse relations . . . . . . . . . . . . . . . . . . . . . . . . 385
16.3.2 hierarchical discourse relations . . . . . . . . . . . . . . . . . . . . . . 389
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
16.3.3 argumentation . .
16.3.4 applications of discourse relations . . . . . . . . . . . . . . . . . . .
. 393

iv applications

401

.

.

.

.

.

.

.

.

.

.

.

17 information extraction
.

17.1 entities .

17.2 relations .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

403
. 405
17.1.1 entity linking by learning to rank . . . . . . . . . . . . . . . . . . . . 406
17.1.2 collective entity linking . . . . . . . . . . . . . . . . . . . . . . . . . . 408
17.1.3 *pairwise ranking id168s . . . . . . . . . . . . . . . . . . . . . 409
. 411
17.2.1 pattern-based id36 . . . . . . . . . . . . . . . . . . . . . 412
17.2.2 id36 as a classi   cation task . . . . . . . . . . . . . . .
. 413
17.2.3 knowledge base population . . . . . . . . . . . . . . . . . . . . . . . . 416
17.2.4 id10 . . . . . . . . . . . . . . . . . . . . . . . 419
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
17.3 events
17.4 hedges, denials, and hypotheticals . . . . . . . . . . . . . . . . . . . . . . .
. 422
17.5 id53 and machine reading . . . . . . . . . . . . . . . . . . . . 424
17.5.1 formal semantics .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
17.5.2 machine reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 425

.

.

.

.

.

.

.

18 machine translation

.

.

.

. . . .

18.2 id151 . . . . . . . . . . . . . . . . . . . . . . . . . .

18.1 machine translation as a task . . . . . . . . . . . . . . . . . . . . . . . . . .

18.1.1 evaluating translations
18.1.2 data .

431
. 431
. . . . . . . . . . . . . . . . . . . . . . . . . . 433
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
. 436
18.2.1 statistical translation modeling . . . . . . . . . . . . . . . . . . . . . . 437
18.2.2 estimation .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
18.2.3 phrase-based translation . . . . . . . . . . . . . . . . . . . . . . . . . . 439
18.2.4 *syntax-based translation . . . . . . . . . . . . . . . . . . . . . . . . . 441
. 442
18.3.1 neural attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
18.3.2 *id4 without recurrence . . . . . . . . . . . . 446
18.3.3 out-of-vocabulary words . . . . . . . . . . . . . . . . . . . . . . . . . 448
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
. 451

. . . . . . . . . . . . . . . . . . . .

18.4 decoding .
18.5 training towards the evaluation metric

18.3 id4 . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . .

.

.

.

.

.

jacob eisenstein. draft of november 13, 2018.

contents

9

19.2 text-to-text generation .

19 text generation

19.1 data-to-text generation .

.

457
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
19.1.1 latent data-to-text alignment . . . . . . . . . . . . . . . . . . . . . . . 459
19.1.2 neural data-to-text generation . . . . . . . . . . . . . . . . . . . . . . 460
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 464
19.2.1 neural abstractive summarization . . . . . . . . . . . . . . . . . . . . 464
19.2.2 sentence fusion for id57 . . . . . . . . . . 465
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
19.3.1 finite-state and agenda-based dialogue systems . . . . . . . . . . . . 467
19.3.2 id100
. . . . . . . . . . . . . . . . . . . . . . . . 468
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
19.3.3 neural chatbots .

.

.

.

.

.

.

.

.

.

.

.

.

19.3 dialogue .

a id203

475
a.1 probabilities of event combinations . . . . . . . . . . . . . . . . . . . . . . . . 475
a.1.1 probabilities of disjoint events
. . . . . . . . . . . . . . . . . . . . . . 476
a.1.2 law of total id203 . . . . . . . . . . . . . . . . . . . . . . . . . . 477
a.2 id155 and bayes    rule . . . . . . . . . . . . . . . . . . . . . 477
.
.
a.3 independence .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 479
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
.
a.4 random variables .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
a.5 expectations .
.
.
a.6 modeling and estimation .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482

.
.
.

.
.
.

.
.
.

.

.

.

b numerical optimization
.

485
b.1 id119
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
b.2 constrained optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486
b.3 example: passive-aggressive online learning . . . . . . . . . . . . . . . . . . 487

.

.

.

.

.

bibliography

489

under contract with mit press, shared under cc-by-nc-nd license.

preface

the goal of this text is focus on a core subset of the natural language processing, uni   ed
by the concepts of learning and search. a remarkable number of problems in natural
language processing can be solved by a compact set of methods:

search. viterbi, cky, minimum spanning tree, shift-reduce, integer id135,

id125.

learning. maximum-likelihood estimation, id28, id88, expectation-

maximization, id105, id26.

this text explains how these methods work, and how they can be applied to a wide range
of tasks: document classi   cation, id51, part-of-speech tagging,
id39, parsing, coreference resolution, id36, discourse
analysis, id38, and machine translation.

background

because natural language processing draws on many different intellectual traditions, al-
most everyone who approaches it feels underprepared in one way or another. here is a
summary of what is expected, and where you can learn more:

mathematics and machine learning. the text assumes a background in multivariate cal-
culus and id202: vectors, matrices, derivatives, and partial derivatives. you
should also be familiar with id203 and statistics. a review of basic proba-
bility is found in appendix a, and a minimal review of numerical optimization is
found in appendix b. for id202, the online course and textbook from strang
(2016) provide an excellent review. deisenroth et al. (2018) are currently preparing
a textbook on mathematics for machine learning, a draft can be found online.1 for
an introduction to probabilistic modeling and estimation, see james et al. (2013); for

1https://mml-book.github.io/

i

ii

preface

a more advanced and comprehensive discussion of the same material, the classic
reference is hastie et al. (2009).

linguistics. this book assumes no formal training in linguistics, aside from elementary
concepts likes nouns and verbs, which you have probably encountered in the study
of english grammar. ideas from linguistics are introduced throughout the text as
needed, including discussions of morphology and syntax (chapter 9), semantics
(chapters 12 and 13), and discourse (chapter 16). linguistic issues also arise in the
application-focused chapters 4, 8, and 18. a short guide to linguistics for students
of natural language processing is offered by bender (2013); you are encouraged to
start there, and then pick up a more comprehensive introductory textbook (e.g., ak-
majian et al., 2010; fromkin et al., 2013).

computer science. the book is targeted at computer scientists, who are assumed to have
taken introductory courses on the analysis of algorithms and complexity theory. in
particular, you should be familiar with asymptotic analysis of the time and memory
costs of algorithms, and with the basics of id145. the classic text
on algorithms is offered by cormen et al. (2009); for an introduction to the theory of
computation, see arora and barak (2009) and sipser (2012).

how to use this book
after the introduction, the textbook is organized into four main units:

learning. this section builds up a set of machine learning tools that will be used through-
out the other sections. because the focus is on machine learning, the text represen-
tations and linguistic phenomena are mostly simple:    bag-of-words    text classi   ca-
tion is treated as a model example. chapter 4 describes some of the more linguisti-
cally interesting applications of word-based text analysis.

sequences and trees. this section introduces the treatment of language as a structured
phenomena. it describes sequence and tree representations and the algorithms that
they facilitate, as well as the limitations that these representations impose. chap-
ter 9 introduces    nite state automata and brie   y overviews a context-free account of
english syntax.

meaning. this section takes a broad view of efforts to represent and compute meaning
from text, ranging from formal logic to neural id27s. it also includes
two topics that are closely related to semantics: resolution of ambiguous references,
and analysis of multi-sentence discourse structure.

applications. the    nal section offers chapter-length treatments on three of the most promi-

nent applications of natural language processing: information extraction, machine

jacob eisenstein. draft of november 13, 2018.

iii

translation, and text generation. each of these applications merits a textbook length
treatment of its own (koehn, 2009; grishman, 2012; reiter and dale, 2000); the chap-
ters here explain some of the most well known systems using the formalisms and
methods built up earlier in the book, while introducing methods such as neural at-
tention.

each chapter contains some advanced material, which is marked with an asterisk.
this material can be safely omitted without causing misunderstandings later on. but
even without these advanced sections, the text is too long for a single semester course, so
instructors will have to pick and choose among the chapters.

chapters 1-3 provide building blocks that will be used throughout the book, and chap-
ter 4 describes some critical aspects of the practice of language technology. language
models (chapter 6), sequence labeling (chapter 7), and parsing (chapter 10 and 11) are
canonical topics in natural language processing, and distributed id27s (chap-
ter 14) have become ubiquitous. of the applications, machine translation (chapter 18) is
the best choice: it is more cohesive than information extraction, and more mature than text
generation. many students will bene   t from the review of id203 in appendix a.

    a course focusing on machine learning should add the chapter on unsupervised
learning (chapter 5). the chapters on predicate-argument semantics (chapter 13),
reference resolution (chapter 15), and text generation (chapter 19) are particularly
in   uenced by recent progress in machine learning, including deep neural networks
and learning to search.

    a course with a more linguistic orientation should add the chapters on applica-
tions of sequence labeling (chapter 8), formal language theory (chapter 9), semantics
(chapter 12 and 13), and discourse (chapter 16).

    for a course with a more applied focus, i recommend the chapters on applications
of sequence labeling (chapter 8), predicate-argument semantics (chapter 13), infor-
mation extraction (chapter 17), and text generation (chapter 19).

acknowledgments
several colleagues, students, and friends read early drafts of chapters in their areas of
expertise, including yoav artzi, kevin duh, heng ji, jessy li, brendan o   connor, yuval
pinter, shawn ling ramirez, nathan schneider, pamela shapiro, noah a. smith, sandeep
soni, and luke zettlemoyer. i also thank the anonymous reviewers, particularly reviewer
4, who provided detailed line-by-line edits and suggestions. the text bene   ted from high-
level discussions with my editor marie lufkin lee, as well as kevin murphy, shawn ling
ramirez, and bonnie webber. in addition, there are many students, colleagues, friends,
and family who found mistakes in early drafts, or who recommended key references.

under contract with mit press, shared under cc-by-nc-nd license.

iv

preface

these include: parminder bhatia, kimberly caras, jiahao cai, justin chen, rodolfo del-
monte, murtaza dhuliawala, yantao du, barbara eisenstein, luiz c. f. ribeiro, chris gu,
joshua killingsworth, jonathan may, taha merghani, gus monod, raghavendra murali,
nidish nair, brendan o   connor, dan oneata, brandon peck, yuval pinter, nathan schnei-
der, jianhao shen, zhewei sun, rubin tsui, ashwin cunnapakkam vinjimur, denny
vrande  ci  c, william yang wang, clay washington, ishan waykul, aobo yang, xavier yao,
yuyu zhang, and several anonymous commenters. clay washington tested some of the
programming exercises, and varun gupta tested some of the written exercises. thanks to
kelvin xu for sharing a high-resolution version of figure 19.3.

most of the book was written while i was at georgia tech   s school of interactive com-
puting. i thank the school for its support of this project, and i thank my colleagues there
for their help and support at the beginning of my faculty career. i also thank (and apol-
ogize to) the many students in georgia tech   s cs 4650 and 7650 who suffered through
early versions of the text. the book is dedicated to my parents.

jacob eisenstein. draft of november 13, 2018.

notation

as a general rule, words, word counts, and other types of observations are indicated with
roman letters (a, b, c); parameters are indicated with greek letters (  ,   ,   ). vectors are
indicated with bold script for both random variables x and parameters   . other useful
notations are indicated in the table below.

basics

exp x
log x
{xn}n
xj
i
x(j)
i

n=1

the base-2 exponent, 2x
the base-2 logarithm, log2 x
the set {x1, x2, . . . , xn}
xi raised to the power j
indexing by both i and j

id202
x(i)
xj:k
[x; y]
[x, y]
en
  (cid:62)
      x(i)
x
xi,j

diag(x)
x   1

a column vector of feature counts for instance i, often word counts
elements j through k (inclusive) of a vector x
vertical concatenation of two column vectors
horizontal concatenation of two column vectors
a    one-hot    vector with a value of 1 at position n, and zero everywhere
else
the transpose of a column vector   

j

j=1   j    x(i)
a matrix
row i, column j of matrix x

the dot product(cid:80)n
a matrix with x on the diagonal, e.g.,      

the inverse of matrix x

x1
0
0

0
x2
0

0
0
x3

      

v

preface

vi

text datasets

wm
n
m
v
y(i)
  y
y
k
(cid:3)
(cid:4)
y(i)
y(w)
   
(cid:7)

word token at position m
number of training instances
length of a sequence (of words or tags)
number of words in vocabulary
the true label for instance i
a predicted label
the set of all possible labels
number of possible labels k = |y|
the start token
the stop token
a structured label for instance i, such as a tag sequence
the set of possible labelings for the word sequence w
the start tag
the stop tag

probabilities

pr(a)
pr(a | b)
pb(b)
pb|a(b | a)
a     p

a | b     p

id203 of event a
id203 of event a, conditioned on event b
the marginal id203 of random variable b taking value b; written
p(b) when the choice of random variable is clear from context
the id203 of random variable b taking value b, conditioned on a
taking value a; written p(b | a) when clear from context
the random variable a is distributed according to distribution p. for
example, x     n (0, 1) states that the random variable x is drawn from
a normal distribution with zero mean and unit variance.
conditioned on the random variable b, a is distributed according to p.2

machine learning
  (x(i), y)
f (x(i), y)
  
(cid:96)(i)
l
l
  

the score for assigning label y to instance i
the feature vector for instance i with label y
a (column) vector of weights
loss on an individual instance i
objective function for an entire dataset
log-likelihood of a dataset
the amount of id173

jacob eisenstein. draft of november 13, 2018.

chapter 1

introduction

natural language processing is the set of methods for making human language accessi-
ble to computers. in the past decade, natural language processing has become embedded
in our daily lives: automatic machine translation is ubiquitous on the web and in so-
cial media; text classi   cation keeps our email inboxes from collapsing under a deluge of
spam; search engines have moved beyond string matching and network analysis to a high
degree of linguistic sophistication; id71 provide an increasingly common and
effective way to get and share information.

these diverse applications are based on a common set of ideas, drawing on algo-
rithms, linguistics, logic, statistics, and more. the goal of this text is to provide a survey
of these foundations. the technical fun starts in the next chapter; the rest of this current
chapter situates natural language processing with respect to other intellectual disciplines,
identi   es some high-level themes in contemporary natural language processing, and ad-
vises the reader on how best to approach the subject.

1.1 natural language processing and its neighbors

natural language processing draws on many other intellectual traditions, from formal
linguistics to statistical physics. this section brie   y situates natural language processing
with respect to some of its closest neighbors.

computational linguistics most of the meetings and journals that host natural lan-
guage processing research bear the name    computational linguistics   , and the terms may
be thought of as essentially synonymous. but while there is substantial overlap, there is
an important difference in focus. in linguistics, language is the object of study. computa-
tional methods may be brought to bear, just as in scienti   c disciplines like computational
biology and computational astronomy, but they play only a supporting role. in contrast,

1

2

chapter 1. introduction

natural language processing is focused on the design and analysis of computational al-
gorithms and representations for processing natural human language. the goal of natu-
ral language processing is to provide new computational capabilities around human lan-
guage: for example, extracting information from texts, translating between languages, an-
swering questions, holding a conversation, taking instructions, and so on. fundamental
linguistic insights may be crucial for accomplishing these tasks, but success is ultimately
measured by whether and how well the job gets done.

machine learning contemporary approaches to natural language processing rely heav-
ily on machine learning, which makes it possible to build complex computer programs
from examples. machine learning provides an array of general techniques for tasks like
converting a sequence of discrete tokens in one vocabulary to a sequence of discrete to-
kens in another vocabulary     a generalization of what one might informally call    transla-
tion.    much of today   s natural language processing research can be thought of as applied
machine learning. however, natural language processing has characteristics that distin-
guish it from many of machine learning   s other application domains.

    unlike images or audio, text data is fundamentally discrete, with meaning created
by combinatorial arrangements of symbolic units. this is particularly consequential
for applications in which text is the output, such as translation and summarization,
because it is not possible to gradually approach an optimal solution.

    although the set of words is discrete, new words are always being created. further-
more, the distribution over words (and other linguistic elements) resembles that of a
power law1 (zipf, 1949): there will be a few words that are very frequent, and a long
tail of words that are rare. a consequence is that natural language processing algo-
rithms must be especially robust to observations that do not occur in the training
data.

    language is compositional: units such as words can combine to create phrases,
which can combine by the very same principles to create larger phrases. for ex-
ample, a noun phrase can be created by combining a smaller noun phrase with a
prepositional phrase, as in the whiteness of the whale. the prepositional phrase is
created by combining a preposition (in this case, of ) with another noun phrase (the
whale). in this way, it is possible to create arbitrarily long phrases, such as,

(1.1)

. . . huge globular pieces of the whale of the bigness of a human head.2

the meaning of such a phrase must be analyzed in accord with the underlying hier-
archical structure. in this case, huge globular pieces of the whale acts as a single noun

1throughout the text, boldface will be used to indicate keywords that appear in the index.
2throughout the text, this notation will be used to introduce linguistic examples.

jacob eisenstein. draft of november 13, 2018.

1.1. natural language processing and its neighbors

3

phrase, which is conjoined with the prepositional phrase of the bigness of a human
head. the interpretation would be different if instead, huge globular pieces were con-
joined with the prepositional phrase of the whale of the bigness of a human head    
implying a disappointingly small whale. even though text appears as a sequence,
machine learning methods must account for its implicit recursive structure.

arti   cial intelligence the goal of arti   cial intelligence is to build software and robots
with the same range of abilities as humans (russell and norvig, 2009). natural language
processing is relevant to this goal in several ways. on the most basic level, the capacity for
language is one of the central features of human intelligence, and is therefore a prerequi-
site for arti   cial intelligence.3 second, much of arti   cial intelligence research is dedicated
to the development of systems that can reason from premises to a conclusion, but such
algorithms are only as good as what they know (dreyfus, 1992). natural language pro-
cessing is a potential solution to the    knowledge bottleneck   , by acquiring knowledge
from texts, and perhaps also from conversations. this idea goes all the way back to tur-
ing   s 1949 paper computing machinery and intelligence, which proposed the turing test for
determining whether arti   cial intelligence had been achieved (turing, 2009).

conversely, reasoning is sometimes essential for basic tasks of language processing,
such as resolving a pronoun. winograd schemas are examples in which a single word
changes the likely referent of a pronoun, in a way that seems to require knowledge and
reasoning to decode (levesque et al., 2011). for example,

(1.2) the trophy doesn   t    t into the brown suitcase because it is too [small/large].

when the    nal word is small, then the pronoun it refers to the suitcase; when the    nal
word is large, then it refers to the trophy. solving this example requires spatial reasoning;
other schemas require reasoning about actions and their effects, emotions and intentions,
and social conventions.

such examples demonstrate that natural language understanding cannot be achieved
in isolation from knowledge and reasoning. yet the history of arti   cial intelligence has
been one of increasing specialization: with the growing volume of research in subdisci-
plines such as natural language processing, machine learning, and id161, it is

3this view is shared by some, but not all, prominent researchers in arti   cial intelligence. michael
jordan, a specialist in machine learning, has said that if he had a billion dollars to spend on any large
research project, he would spend it on natural language processing (https://www.reddit.com/r/
machinelearning/comments/2fxi6v/ama_michael_i_jordan/). on the other hand, in a public dis-
cussion about the future of arti   cial intelligence in february 2018, id161 researcher yann lecun
argued that despite its many practical applications, language is perhaps    number 300    in the priority list
for arti   cial intelligence research, and that it would be a great achievement if ai could attain the capa-
bilities of an orangutan, which do not include language (http://www.abigailsee.com/2018/02/21/
deep-learning-structure-and-innate-priors.html).

under contract with mit press, shared under cc-by-nc-nd license.

4

chapter 1. introduction

dif   cult for anyone to maintain expertise across the entire    eld. still, recent work has
demonstrated interesting connections between natural language processing and other ar-
eas of ai, including id161 (e.g., antol et al., 2015) and game playing (e.g.,
branavan et al., 2009). the dominance of machine learning throughout arti   cial intel-
ligence has led to a broad consensus on representations such as id114 and
computation graphs, and on algorithms such as id26 and combinatorial opti-
mization. many of the algorithms and representations covered in this text are part of this
consensus.

computer science the discrete and recursive nature of natural language invites the ap-
plication of theoretical ideas from computer science. linguists such as chomsky and
montague have shown how formal language theory can help to explain the syntax and
semantics of natural language. theoretical models such as    nite-state and pushdown au-
tomata are the basis for many practical natural language processing systems. algorithms
for searching the combinatorial space of analyses of natural language utterances can be
analyzed in terms of their computational complexity, and theoretically motivated approx-
imations can sometimes be applied.

the study of computer systems is also relevant to natural language processing. large
datasets of unlabeled text can be processed more quickly by parallelization techniques
like mapreduce (dean and ghemawat, 2008; lin and dyer, 2010); high-volume data
sources such as social media can be summarized ef   ciently by approximate streaming
and sketching techniques (goyal et al., 2009). when deep neural networks are imple-
mented in production systems, it is possible to eke out speed gains using techniques such
as reduced-precision arithmetic (wu et al., 2016). many classical natural language process-
ing algorithms are not naturally suited to graphics processing unit (gpu) parallelization,
suggesting directions for further research at the intersection of natural language process-
ing and computing hardware (yi et al., 2011).

speech processing natural language is often communicated in spoken form, and speech
recognition is the task of converting an audio signal to text. from one perspective, this is
a signal processing problem, which might be viewed as a preprocessing step before nat-
ural language processing can be applied. however, context plays a critical role in speech
recognition by human listeners: knowledge of the surrounding words in   uences percep-
tion and helps to correct for noise (miller et al., 1951). for this reason, id103
is often integrated with text analysis, particularly with statistical language models, which
quantify the id203 of a sequence of text (see chapter 6). beyond id103,
the broader    eld of speech processing includes the study of speech-based dialogue sys-
tems, which are brie   y discussed in chapter 19. historically, speech processing has often
been pursued in electrical engineering departments, while natural language processing

jacob eisenstein. draft of november 13, 2018.

1.1. natural language processing and its neighbors

5

has been the purview of computer scientists. for this reason, the extent of interaction
between these two disciplines is less than it might otherwise be.

ethics as machine learning and arti   cial intelligence become increasingly ubiquitous, it
is crucial to understand how their bene   ts, costs, and risks are distributed across differ-
ent kinds of people. natural language processing raises some particularly salient issues
around ethics, fairness, and accountability:

access. who is natural language processing designed to serve? for example, whose lan-

guage is translated from, and whose language is translated to?

bias. does language technology learn to replicate social biases from text corpora, and

does it reinforce these biases as seemingly objective computational conclusions?

labor. whose text and speech comprise the datasets that power natural language pro-
cessing, and who performs the annotations? are the bene   ts of this technology
shared with all the people whose work makes it possible?

privacy and internet freedom. what is the impact of large-scale text processing on the
right to free and private communication? what is the potential role of natural lan-
guage processing in regimes of censorship or surveillance?

this text lightly touches on issues related to fairness and bias in    14.6.3 and    18.1.1,
but these issues are worthy of a book of their own. for more from within the    eld of
computational linguistics, see the papers from the annual workshop on ethics in natural
language processing (hovy et al., 2017; alfano et al., 2018). for an outside perspective on
ethical issues relating to data science at large, see boyd and crawford (2012).

others natural language processing plays a signi   cant role in emerging interdisciplinary
   elds like computational social science and the digital humanities. text classi   cation
(chapter 4), id91 (chapter 5), and information extraction (chapter 17) are particularly
useful tools; another is probabilistic topic models (blei, 2012), which are not covered in
this text. information retrieval (manning et al., 2008) makes use of similar tools, and
conversely, techniques such as latent semantic analysis (   14.3) have roots in information
retrieval. id111 is sometimes used to refer to the application of data mining tech-
niques, especially classi   cation and id91, to text. while there is no clear distinction
between id111 and natural language processing (nor between data mining and ma-
chine learning), id111 is typically less concerned with linguistic structure, and more
interested in fast, scalable algorithms.

under contract with mit press, shared under cc-by-nc-nd license.

6

chapter 1. introduction

1.2 three themes in natural language processing

natural language processing covers a diverse range of tasks, methods, and linguistic phe-
nomena. but despite the apparent incommensurability between, say, the summarization
of scienti   c articles (   16.3.4) and the identi   cation of suf   x patterns in spanish verbs
(   9.1.4), some general themes emerge. the remainder of the introduction focuses on these
themes, which will recur in various forms through the text. each theme can be expressed
as an opposition between two extreme viewpoints on how to process natural language.
the methods discussed in the text can usually be placed somewhere on the continuum
between these two extremes.

1.2.1 learning and knowledge

a recurring topic of debate is the relative importance of machine learning and linguistic
knowledge. on one extreme, advocates of    natural language processing from scratch    (col-
lobert et al., 2011) propose to use machine learning to train end-to-end systems that trans-
mute raw text into any desired output structure: e.g., a summary, database, or transla-
tion. on the other extreme, the core work of natural language processing is sometimes
taken to be transforming text into a stack of general-purpose linguistic structures: from
subword units called morphemes, to word-level parts-of-speech, to tree-structured repre-
sentations of grammar, and beyond, to logic-based representations of meaning. in theory,
these general-purpose structures should then be able to support any desired application.
the end-to-end approach has been buoyed by recent results in id161 and
id103, in which advances in machine learning have swept away expert-
engineered representations based on the fundamentals of optics and phonology (krizhevsky
et al., 2012; graves and jaitly, 2014). but while machine learning is an element of nearly
every contemporary approach to natural language processing, linguistic representations
such as syntax trees have not yet gone the way of the visual edge detector or the auditory
triphone. linguists have argued for the existence of a    language faculty    in all human be-
ings, which encodes a set of abstractions specially designed to facilitate the understanding
and production of language. the argument for the existence of such a language faculty
is based on the observation that children learn language faster and from fewer examples
than would be possible if language was learned from experience alone.4 from a practi-
cal standpoint, linguistic structure seems to be particularly important in scenarios where
training data is limited.

there are a number of ways in which knowledge and learning can be combined in
natural language processing. many supervised learning systems make use of carefully
engineered features, which transform the data into a representation that can facilitate

4the language instinct (pinker, 2003) articulates these arguments in an engaging and popular style. for

arguments against the innateness of language, see elman et al. (1998).

jacob eisenstein. draft of november 13, 2018.

1.2. three themes in natural language processing

7

learning. for example, in a task like search, it may be useful to identify each word   s stem,
so that a system can more easily generalize across related terms such as whale, whales,
whalers, and whaling. (this issue is relatively benign in english, as compared to the many
other languages which include much more elaborate systems of pre   xed and suf   xes.)
such features could be obtained from a hand-crafted resource, like a dictionary that maps
each word to a single root form. alternatively, features can be obtained from the output of
a general-purpose language processing system, such as a parser or part-of-speech tagger,
which may itself be built on supervised machine learning.

another synthesis of learning and knowledge is in model structure: building machine
learning models whose architectures are inspired by linguistic theories. for example, the
organization of sentences is often described as compositional, with meaning of larger
units gradually constructed from the meaning of their smaller constituents. this idea
can be built into the architecture of a deep neural network, which is then trained using
contemporary deep learning techniques (dyer et al., 2016).

the debate about the relative importance of machine learning and linguistic knowl-
edge sometimes becomes heated. no machine learning specialist likes to be told that their
engineering methodology is unscienti   c alchemy;5 nor does a linguist want to hear that
the search for general linguistic principles and structures has been made irrelevant by big
data. yet there is clearly room for both types of research: we need to know how far we
can go with end-to-end learning alone, while at the same time, we continue the search for
linguistic representations that generalize across applications, scenarios, and languages.
for more on the history of this debate, see church (2011); for an optimistic view of the
potential symbiosis between computational linguistics and deep learning, see manning
(2015).

1.2.2 search and learning

many natural language processing problems can be written mathematically in the form
of optimization,6

  y = argmax
y   y(x)

  (x, y;   ),

[1.1]

where,

    x is the input, which is an element of a set x ;
    y is the output, which is an element of a set y(x);
5ali rahimi argued that much of deep learning research was similar to    alchemy    in a presentation at
the 2017 conference on neural information processing systems. he was advocating for more learning theory,
not more linguistics.

6throughout this text, equations will be numbered by square brackets, and linguistic examples will be

numbered by parentheses.

under contract with mit press, shared under cc-by-nc-nd license.

8

chapter 1. introduction

the real numbers;

       is a scoring function (also called the model), which maps from the set x    y to
       is a vector of parameters for   ;
      y is the predicted output, which is chosen to maximize the scoring function.
this basic structure can be applied to a huge range of problems. for example, the input
x might be a social media post, and the output y might be a labeling of the emotional
sentiment expressed by the author (chapter 4); or x could be a sentence in french, and the
output y could be a sentence in tamil (chapter 18); or x might be a sentence in english,
and y might be a representation of the syntactic structure of the sentence (chapter 10); or
x might be a news article and y might be a structured record of the events that the article
describes (chapter 17).

this formulation re   ects an implicit decision that language processing algorithms will

have two distinct modules:

search. the search module is responsible for computing the argmax of the function   . in
other words, it    nds the output   y that gets the best score with respect to the input x.
this is easy when the search space y(x) is small enough to enumerate, or when the
scoring function    has a convenient decomposition into parts. in many cases, we
will want to work with scoring functions that do not have these properties, moti-
vating the use of more sophisticated search algorithms, such as bottom-up dynamic
programming (   10.1) and id125 (   11.3.1). because the outputs are usually
discrete in language processing problems, search often relies on the machinery of
combinatorial optimization.

learning. the learning module is responsible for    nding the parameters   . this is typ-
ically (but not always) done by processing a large dataset of labeled examples,
i=1. like search, learning is also approached through the framework
{(x(i), y(i))}n
of optimization, as we will see in chapter 2. because the parameters are usually
continuous, learning algorithms generally rely on numerical optimization to iden-
tify vectors of real-valued parameters that optimize some function of the model and
the labeled data. some basic principles of numerical optimization are reviewed in
appendix b.

the division of natural language processing into separate modules for search and
learning makes it possible to reuse generic algorithms across many tasks and models.
much of the work of natural language processing can be focused on the design of the
model        identifying and formalizing the linguistic phenomena that are relevant to the
task at hand     while reaping the bene   ts of decades of progress in search, optimization,
and learning. this textbook will describe several classes of scoring functions, and the
corresponding algorithms for search and learning.

jacob eisenstein. draft of november 13, 2018.

1.2. three themes in natural language processing

9

when a model is capable of making subtle linguistic distinctions, it is said to be ex-
pressive. expressiveness is often traded off against ef   ciency of search and learning. for
example, a word-to-word translation model makes search and learning easy, but it is not
expressive enough to distinguish good translations from bad ones. many of the most im-
portant problems in natural language processing seem to require expressive models, in
which the complexity of search grows exponentially with the size of the input. in these
models, exact search is usually impossible. intractability threatens the neat modular de-
composition between search and learning: if search requires a set of heuristic approxima-
tions, then it may be advantageous to learn a model that performs well under these spe-
ci   c heuristics. this has motivated some researchers to take a more integrated approach
to search and learning, as brie   y mentioned in chapters 11 and 15.

1.2.3 relational, compositional, and distributional perspectives
any element of language     a word, a phrase, a sentence, or even a sound     can be
described from at least three perspectives. consider the word journalist. a journalist is
a subcategory of a profession, and an anchorwoman is a subcategory of journalist; further-
more, a journalist performs journalism, which is often, but not always, a subcategory of
writing. this relational perspective on meaning is the basis for semantic ontologies such
as id138 (fellbaum, 2010), which enumerate the relations that hold between words
and other elementary semantic units. the power of the relational perspective is illustrated
by the following example:

(1.3) umashanthi interviewed ana. she works for the college newspaper.

who works for the college newspaper? the word journalist, while not stated in the ex-
ample, implicitly links the interview to the newspaper, making umashanthi the most likely
referent for the pronoun. (a general discussion of how to resolve pronouns is found in
chapter 15.)

yet despite the inferential power of the relational perspective, it is not easy to formalize
computationally. exactly which elements are to be related? are journalists and reporters
distinct, or should we group them into a single unit? is the kind of interview performed by
a journalist the same as the kind that one undergoes when applying for a job? ontology
designers face many such thorny questions, and the project of ontology design hearkens
back to borges    (1993) celestial emporium of benevolent knowledge, which divides animals
into:

(a) belonging to the emperor; (b) embalmed; (c) tame; (d) suckling pigs; (e)
sirens; (f) fabulous; (g) stray dogs; (h) included in the present classi   cation;
(i) frenzied; (j) innumerable; (k) drawn with a very    ne camelhair brush; (l) et
cetera; (m) having just broken the water pitcher; (n) that from a long way off
resemble    ies.

under contract with mit press, shared under cc-by-nc-nd license.

10

chapter 1. introduction

dif   culties in ontology construction have led some linguists to argue that there is no task-
independent way to partition up word meanings (kilgarriff, 1997).

some problems are easier. each member in a group of journalists is a journalist: the -s
suf   x distinguishes the plural meaning from the singular in most of the nouns in english.
similarly, a journalist can be thought of, perhaps colloquially, as someone who produces or
works on a journal. (taking this approach even further, the word journal derives from the
french jour+nal, or day+ly = daily.) in this way, the meaning of a word is constructed from
the constituent parts     the principle of compositionality. this principle can be applied
to larger units: phrases, sentences, and beyond. indeed, one of the great strengths of the
compositional view of meaning is that it provides a roadmap for understanding entire
texts and dialogues through a single analytic lens, grounding out in the smallest parts of
individual words.

but alongside journalists and anti-parliamentarians, there are many words that seem
to be linguistic atoms: think, for example, of whale, blubber, and nantucket.
idiomatic
phrases like kick the bucket and shoot the breeze have meanings that are quite different from
the sum of their parts (sag et al., 2002). composition is of little help for such words and
expressions, but their meanings can be ascertained     or at least approximated     from the
contexts in which they appear. take, for example, blubber, which appears in such contexts
as:

(1.4)

a. the blubber served them as fuel.
b.
c. amongst oily substances, blubber has been employed as a manure.

. . . extracting it from the blubber of the large    sh . . .

these contexts form the distributional properties of the word blubber, and they link it to
words which can appear in similar constructions: fat, pelts, and barnacles. this distribu-
tional perspective makes it possible to learn about meaning from unlabeled data alone;
unlike relational and id152, no manual annotation or expert knowl-
edge is required. id65 is thus capable of covering a huge range of
linguistic phenomena. however, it lacks precision: blubber is similar to fat in one sense, to
pelts in another sense, and to barnacles in still another. the question of why all these words
tend to appear in the same contexts is left unanswered.

the relational, compositional, and distributional perspectives all contribute to our un-
derstanding of linguistic meaning, and all three appear to be critical to natural language
processing. yet they are uneasy collaborators, requiring seemingly incompatible represen-
tations and algorithmic approaches. this text presents some of the best known and most
successful methods for working with each of these representations, but future research
may reveal new ways to combine them.

jacob eisenstein. draft of november 13, 2018.

part i

learning

11

chapter 2

linear text classi   cation

we begin with the problem of text classi   cation: given a text document, assign it a dis-
crete label y     y, where y is the set of possible labels. text classi   cation has many ap-
plications, from spam    ltering to the analysis of electronic health records. this chapter
describes some of the most well known and effective algorithms for text classi   cation,
from a mathematical perspective that should help you understand what they do and why
they work. text classi   cation is also a building block in more elaborate natural language
processing tasks. for readers without a background in machine learning or statistics, the
material in this chapter will take more time to digest than most of the subsequent chap-
ters. but this investment will pay off as the mathematical principles behind these basic
classi   cation algorithms reappear in other contexts throughout the book.

2.1 the bag of words

to perform text classi   cation, the    rst question is how to represent each document, or
instance. a common approach is to use a column vector of word counts, e.g., x =
[0, 1, 1, 0, 0, 2, 0, 1, 13, 0 . . .](cid:62), where xj is the count of word j. the length of x is v (cid:44) |v|,
where v is the set of possible words in the vocabulary. in linear classi   cation, the classi-
   cation decision is based on a weighted sum of individual feature counts, such as word
counts.

the object x is a vector, but it is often called a bag of words, because it includes only
information about the count of each word, and not the order in which the words appear.
with the bag of words representation, we are ignoring grammar, sentence boundaries,
paragraphs     everything but the words. yet the id159 is surprisingly
effective for text classi   cation. if you see the word whale in a document, is it    ction or non-
   ction? what if you see the word molybdenum? for many labeling problems, individual
words can be strong predictors.

13

14

chapter 2. linear text classification

to predict a label from a bag-of-words, we can assign a score to each word in the vo-
cabulary, measuring the compatibility with the label. for example, for the label fiction,
we might assign a positive score to the word whale, and a negative score to the word
molybdenum. these scores are called weights, and they are arranged in a column vector   .
suppose that you want a multiclass classi   er, where k (cid:44) |y| > 2. for example, you
might want to classify news stories about sports, celebrities, music, and business. the goal
is to predict a label   y, given the bag of words x, using the weights   . for each label y     y,
we compute a score   (x, y), which is a scalar measure of the compatibility between the
bag-of-words x and the label y. in a linear bag-of-words classi   er, this score is the vector
inner product between the weights    and the output of a feature function f (x, y),

  (x, y) =       f (x, y) =(cid:88)j

  jfj(x, y).

[2.1]

as the notation suggests, f is a function of two arguments, the word counts x and the
label y, and it returns a vector output. for example, given arguments x and y, element j
of this feature vector might be,

fj(x, y) =(cid:40)xwhale,

0,

if y = fiction
otherwise

[2.2]

this function returns the count of the word whale if the label is fiction, and it returns zero
otherwise. the index j depends on the position of whale in the vocabulary, and of fiction
in the set of possible labels. the corresponding weight   j then scores the compatibility of
the word whale with the label fiction.1 a positive score means that this word makes the
label more likely.

the output of the feature function can be formalized as a vector:

f (x, y = 1) = [x; 0; 0; . . . ; 0
]
(k   1)  v
f (x, y = 2) = [0; 0; . . . ; 0

(cid:123)(cid:122)

(cid:124)

(cid:125)

f (x, y = k) = [0; 0; . . . ; 0
(k   1)  v

; x],

; x; 0; 0; . . . ; 0
]
(k   2)  v

(cid:123)(cid:122)

(cid:125)

(cid:124)

(cid:124)
(cid:124)

v

(cid:123)(cid:122)
(cid:123)(cid:122)

(cid:125)
(cid:125)

[2.3]

[2.4]

[2.5]

(cid:124)

(cid:123)(cid:122)

where [0; 0; . . . ; 0
(k   1)  v

] is a column vector of (k     1)    v zeros, and the semicolon indicates
(cid:125)
vertical concatenation. for each of the k possible labels, the feature function returns a
1in practice, both f and    may be implemented as a dictionary rather than vectors, so that it is not
necessary to explicitly identify j. in such an implementation, the tuple (whale, fiction) acts as a key in both
dictionaries; the values in f are feature counts, and the values in    are weights.

jacob eisenstein. draft of november 13, 2018.

2.1. the bag of words

15

vector that is mostly zeros, with a column vector of word counts x inserted in a location
that depends on the speci   c label y. this arrangement is shown in figure 2.1. the notation
may seem awkward at    rst, but it generalizes to an impressive range of learning settings,
particularly structure prediction, which is the focus of chapters 7-11.

given a vector of weights,        rv k, we can now compute the score   (x, y) by equa-
tion 2.1. this inner product gives a scalar measure of the compatibility of the observation
x with label y.2 for any document x, we predict the label   y,

  y = argmax

y   y   (x, y)

  (x, y) =      f (x, y).

[2.6]

[2.7]

this inner product notation gives a clean separation between the data (x and y) and the
parameters (  ).

while vector notation is used for presentation and analysis, in code the weights and
feature vector can be implemented as dictionaries. the inner product can then be com-
puted as a loop. in python:

def compute_score(x,y,weights):

total = 0
for feature,count in feature_function(x,y).items():

total += weights[feature] * count

return total

this representation is advantageous because it avoids storing and iterating over the many
features whose counts are zero.

it is common to add an offset feature at the end of the vector of word counts x, which
is always 1. we then have to also add an extra zero to each of the zero vectors, to make the
vector lengths match. this gives the entire feature vector f (x, y) a length of (v + 1)    k.
the weight associated with this offset feature can be thought of as a bias for or against
each label. for example, if we expect most emails to be spam, then the weight for the
offset feature for y = spam should be larger than the weight for the offset feature for
y = not-spam.

returning to the weights   , where do they come from? one possibility is to set them
by hand.
if we wanted to distinguish, say, english from spanish, we can use english
and spanish dictionaries, and set the weight to one for each word that appears in the
2only v    (k     1) features and weights are necessary. by stipulating that   (x, y = k) = 0 regardless of
x, it is possible to implement any classi   cation rule that can be achieved with v    k features and weights.
this is the approach taken in binary classi   cation rules like y = sign(    x+a), where    is a vector of weights,
a is an offset, and the label set is y = {   1, 1}. however, for multiclass classi   cation, it is more concise to
write       f (x, y) for all y     y.

under contract with mit press, shared under cc-by-nc-nd license.

16

chapter 2. linear text classification

figure 2.1: the bag-of-words and feature vector representations, for a hypothetical text
classi   cation task.

associated dictionary. for example,3

  (e,bicycle) =1
  (e,bicicleta) =0
  (e,con) =1
  (e,ordinateur) =0

  (s,bicycle) =0
  (s,bicicleta) =1
  (s,con) =1
  (s,ordinateur) =0.

similarly, if we want to distinguish positive and negative sentiment, we could use posi-
tive and negative sentiment lexicons (see    4.1.2), which are de   ned by social psycholo-
gists (tausczik and pennebaker, 2010).

but it is usually not easy to set classi   cation weights by hand, due to the large number
of words and the dif   culty of selecting exact numerical weights. instead, we will learn the
weights from data. email users manually label messages as spam; newspapers label their
own articles as business or style. using such instance labels, we can automatically
acquire weights using supervised machine learning. this chapter will discuss several
machine learning approaches for classi   cation. the    rst is based on id203. for a
review of id203, consult appendix a.

3in this notation, each tuple (language, word) indexes an element in   , which remains a vector.

jacob eisenstein. draft of november 13, 2018.

it was the best of times, it was the worst of times...xitwasthebestworsttimes12of222210...00...0...0...0...0...0...0...01x000f(x ,y=news)y=fictiony=newsy=gossipy=sportsbag of wordsfeature vectororiginal text<offset>aardvarkzyxt2.2. na  ive bayes

2.2 na    ve bayes

17

the joint id203 of a bag of words x and its true label y is written p(x, y). suppose
i=1, which we assume are indepen-
we have a dataset of n labeled instances, {(x(i), y(i))}n
dent and identically distributed (iid) (see    a.3). then the joint id203 of the entire
dataset, written p(x(1:n ), y(1:n )), is equal to(cid:81)n

what does this have to do with classi   cation? one approach to classi   cation is to set
the weights    so as to maximize the joint id203 of a training set of labeled docu-
ments. this is known as id113:

i=1 px,y (x(i), y(i)).4

     = argmax

  

p(x(1:n ), y(1:n );   )

= argmax

  

= argmax

  

n(cid:89)i=1
n(cid:88)i=1

p(x(i), y(i);   )

log p(x(i), y(i);   ).

[2.8]

[2.9]

[2.10]

the notation p(x(i), y(i);   ) indicates that    is a parameter of the id203 function. the
product of probabilities can be replaced by a sum of log-probabilities because the log func-
tion is monotonically increasing over positive arguments, and so the same    will maxi-
mize both the id203 and its logarithm. working with logarithms is desirable because
of numerical stability: on a large dataset, multiplying many probabilities can under   ow
to zero.5

the id203 p(x(i), y(i);   ) is de   ned through a generative model     an idealized
random process that has generated the observed data.6 algorithm 1 describes the gener-
ative model underlying the na    ve bayes classi   er, with parameters    = {  ,   }.

    the    rst line of this generative model encodes the assumption that the instances are
mutually independent: neither the label nor the text of document i affects the label
or text of document j.7 furthermore, the instances are identically distributed: the

4the notation px,y (x(i), y(i)) indicates the joint id203 that random variables x and y take the
speci   c values x(i) and y(i) respectively. the subscript will often be omitted when it is clear from context.
for a review of random variables, see appendix a.

5throughout this text, you may assume all logarithms and exponents are base 2, unless otherwise indi-
cated. any reasonable base will yield an identical classi   er, and base 2 is most convenient for working out
examples by hand.

6generative models will be used throughout this text. they explicitly de   ne the assumptions underlying
the form of a id203 distribution over observed and latent variables. for a readable introduction to
generative models in statistics, see blei (2014).

7can you think of any cases in which this assumption is too strong?

under contract with mit press, shared under cc-by-nc-nd license.

18

chapter 2. linear text classification

algorithm 1 generative process for the na    ve bayes classi   cation model

for instance i     {1, 2, . . . , n} do:

draw the label y(i)     categorical(  );
draw the word counts x(i) | y(i)     multinomial(  y(i)).

distributions over the label y(i) and the text x(i) (conditioned on y(i)) are the same
for all instances i. in other words, we make the assumption that every document
has the same distribution over labels, and that each document   s distribution over
words depends only on the label, and not on anything else about the document. we
also assume that the documents don   t affect each other: if the word whale appears
in document i = 7, that does not make it any more or less likely that it will appear
again in document i = 8.

    the second line of the generative model states that the random variable y(i) is drawn
from a categorical distribution with parameter   . categorical distributions are like
weighted dice: the column vector    = [  1;   2; . . . ;   k] gives the probabilities of
each label, so that the id203 of drawing label y is equal to   y. for example, if
y = {positive, negative, neutral}, we might have    = [0.1; 0.7; 0.2]. we require
(cid:80)y   y   y = 1 and   y     0,    y     y: each label   s id203 is non-negative, and the
sum of these probabilities is equal to one. 8
    the third line describes how the bag-of-words counts x(i) are generated. by writing
x(i) | y(i), this line indicates that the word counts are conditioned on the label, so
that the joint id203 is factored using the chain rule,

px,y (x(i), y(i)) = px|y (x(i) | y(i))    py (y(i)).

[2.11]

the speci   c distribution px|y is the multinomial, which is a id203 distribu-
tion over vectors of non-negative counts. the id203 mass function for this
distribution is:

pmult(x;   ) =b(x)

  xj
j

v(cid:89)j=1
b(x) =(cid:16)(cid:80)v
j=1 xj(cid:17)!
(cid:81)v

j=1(xj!)

[2.12]

[2.13]

.

8formally, we require           k   1, where    k   1 is the k     1 id203 simplex, the set of all vectors of
k nonnegative numbers that sum to one. because of the sum-to-one constraint, there are k     1 degrees of
freedom for a vector of size k.

jacob eisenstein. draft of november 13, 2018.

2.2. na  ive bayes

19

as in the categorical distribution, the parameter   j can be interpreted as a probabil-
ity: speci   cally, the id203 that any given token in the document is the word j.
the multinomial distribution involves a product over words, with each term in the
product equal to the id203   j, exponentiated by the count xj. words that have
zero count play no role in this product, because   0
j = 1. the term b(x) is called the
multinomial coef   cient. it doesn   t depend on   , and can usually be ignored. can
you see why we need this term at all?9
the notation p(x | y;   ) indicates the id155 of word counts x
given label y, with parameter   , which is equal to pmult(x;   y). by specifying the
multinomial distribution, we describe the multinomial na    ve bayes classi   er. why
   na    ve   ? because the multinomial distribution treats each word token indepen-
dently, conditioned on the class: the id203 mass function factorizes across the
counts.10

2.2.1 types and tokens

a slight modi   cation to the generative model of na    ve bayes is shown in algorithm 2.
instead of generating a vector of counts of types, x, this model generates a sequence of
tokens, w = (w1, w2, . . . , wm ). the distinction between types and tokens is critical: xj    
{0, 1, 2, . . . , m} is the count of word type j in the vocabulary, e.g., the number of times
the word cannibal appears; wm     v is the identity of token m in the document, e.g. wm =
cannibal.

the id203 of the sequence w is a product of categorical probabilities. algorithm 2
makes a conditional independence assumption: each token w(i)
m is independent of all other
tokens w(i)
n(cid:54)=m, conditioned on the label y(i). this is identical to the    na    ve    independence
assumption implied by the multinomial distribution, and as a result, the optimal parame-
ters for this model are identical to those in multinomial na    ve bayes. for any instance, the
id203 assigned by this model is proportional to the id203 under multinomial
na    ve bayes. the constant of proportionality is the multinomial coef   cient b(x). because
b(x)     1, the id203 for a vector of counts x is at least as large as the id203
for a list of words w that induces the same counts: there can be many word sequences
that correspond to a single vector of counts. for example, man bites dog and dog bites man
correspond to an identical count vector, {bites : 1, dog : 1, man : 1}, and b(x) is equal to
the total number of possible word orderings for count vector x.

9technically, a multinomial distribution requires a second parameter, the total number of word counts
in x. in the bag-of-words representation is equal to the number of words in the document. however, this
parameter is irrelevant for classi   cation.

10you can plug in any id203 distribution to the generative story and it will still be na    ve bayes, as
long as you are making the    na    ve    assumption that the features are conditionally independent, given the
label. for example, a multivariate gaussian with diagonal covariance is na    ve in exactly the same sense.

under contract with mit press, shared under cc-by-nc-nd license.

20

chapter 2. linear text classification

algorithm 2 alternative generative process for the na    ve bayes classi   cation model

for instance i     {1, 2, . . . , n} do:

draw the label y(i)     categorical(  );
for token m     {1, 2, . . . , mi} do:

draw the token w(i)

m | y(i)     categorical(  y(i)).

sometimes it is useful to think of instances as counts of types, x; other times, it is
better to think of them as sequences of tokens, w.
if the tokens are generated from a
model that assumes conditional independence, then these two views lead to id203
models that are identical, except for a scaling factor that does not depend on the label or
the parameters.

2.2.2 prediction
the na    ve bayes prediction rule is to choose the label y which maximizes log p(x, y;   ,   ):
[2.14]

log p(x, y;   ,   )

  y = argmax

y

= argmax

y

log p(x | y;   ) + log p(y;   )

now we can plug in the id203 distributions from the generative story.

log p(x | y;   ) + log p(y;   ) = log      b(x)

  xj

y,j       + log   y

v(cid:89)j=1
v(cid:88)j=1

= log b(x) +

xj log   y,j + log   y

= log b(x) +       f (x, y),

where

   = [  (1);   (2); . . . ;   (k)]

  (y) = [log   y,1; log   y,2; . . . ; log   y,v ; log   y]

[2.15]

[2.16]

[2.17]

[2.18]

[2.19]
[2.20]

the feature function f (x, y) is a vector of v word counts and an offset, padded by
zeros for the labels not equal to y (see equations 2.3-2.5, and figure 2.1). this construction
ensures that the inner product       f (x, y) only activates the features whose weights are
in   (y). these features and weights are all we need to compute the joint log-id203
log p(x, y) for each y. this is a key point: through this notation, we have converted the
problem of computing the log-likelihood for a document-label pair (x, y) into the compu-
tation of a vector inner product.

jacob eisenstein. draft of november 13, 2018.

2.2. na  ive bayes

21

2.2.3 estimation
the parameters of the categorical and multinomial distributions have a simple interpre-
tation: they are vectors of expected frequencies for each possible event. based on this
interpretation, it is tempting to set the parameters empirically,

  y,j =

count(y, j)
(cid:80)v
j(cid:48)=1 count(y, j(cid:48))

= (cid:80)i:y(i)=y x(i)
(cid:80)v
j(cid:48)=1(cid:80)i:y(i)=y x(i)

j(cid:48)

j

,

[2.21]

where count(y, j) refers to the count of word j in documents with label y.

equation 2.21 de   nes the relative frequency estimate for   . it can be justi   ed as a

maximum likelihood estimate: the estimate that maximizes the id203 p(x(1:n ), y(1:n );   ).
based on the generative model in algorithm 1, the log-likelihood is,

l(  ,   ) =

n(cid:88)i=1

log pmult(x(i);   y(i)) + log pcat(y(i);   ),

[2.22]

which is now written as a function l of the parameters    and   . let   s continue to focus
on the parameters   . since p(y) is constant with respect to   , we can drop it:

l(  ) =

n(cid:88)i=1

log pmult(x(i);   y(i)) =

log b(x(i)) +

n(cid:88)i=1

v(cid:88)j=1

x(i)
j

log   y(i),j,

[2.23]

where b(x(i)) is constant with respect to   .

maximum-likelihood estimation chooses    to maximize the log-likelihood l. how-

ever, the solution must obey the following constraints:

v(cid:88)j=1

  y,j = 1    y

[2.24]

these constraints can be incorporated by adding a set of lagrange multipliers to the objec-
tive (see appendix b for more details). to solve for each   y, we maximize the lagrangian,

(cid:96)(  y) = (cid:88)i:y(i)=y

v(cid:88)j=1

x(i)
j

log   y,j       (

v(cid:88)j=1

  y,j     1).

differentiating with respect to the parameter   y,j yields,

   (cid:96)(  y)
     y,j

= (cid:88)i:y(i)=y

x(i)
j /  y,j       .

[2.25]

[2.26]

under contract with mit press, shared under cc-by-nc-nd license.

22

chapter 2. linear text classification

the solution is obtained by setting each element in this vector of derivatives equal to zero,

    y,j = (cid:88)i:y(i)=y
  y,j     (cid:88)i:y(i)=y

x(i)
j

x(i)
j =

n(cid:88)i=1

  (cid:16)y(i) = y(cid:17) x(i)

j = count(y, j),

[2.27]

[2.28]

where   (cid:0)y(i) = y(cid:1) is a delta function, also sometimes called an indicator function, which
returns one if y(i) = y. the symbol     indicates that   y,j is proportional to the right-hand
side of the equation.

equation 2.28 shows three different notations for the same thing: a sum over the word
counts for all documents i such that the label y(i) = y. this gives a solution for each
j=1   y,j = 1, which
arises because   y represents a vector of probabilities for each word in the vocabulary.
this constraint leads to an exact solution, which does not depend on   :

  y up to a constant of proportionality. now recall the constraint(cid:80)v

this is equal to the relative frequency estimator from equation 2.21. a similar derivation

  y,j =

count(y, j)
(cid:80)v
j(cid:48)=1 count(y, j(cid:48))

.

[2.29]

gives   y    (cid:80)n

i=1   (cid:0)y(i) = y(cid:1).

2.2.4 smoothing
with text data, there are likely to be pairs of labels and words that never appear in the
training set, leaving   y,j = 0. for example, the word molybdenum may have never yet
appeared in a work of    ction. but choosing a value of   fiction,molybdenum = 0 would allow
this single feature to completely veto a label, since p(fiction | x) = 0 if xmolybdenum > 0.
this is undesirable, because it imposes high variance: depending on what data hap-
pens to be in the training set, we could get vastly different classi   cation rules. one so-
lution is to smooth the probabilities, by adding a    pseudocount    of    to each count, and
then normalizing.

  y,j =

   + count(y, j)

j(cid:48)=1 count(y, j(cid:48))

v    +(cid:80)v

[2.30]

this is called laplace smoothing.11 the pseudocount    is a hyperparameter, because it
controls the form of the log-likelihood function, which in turn drives the estimation of   .
11laplace smoothing has a bayesian justi   cation, in which the generative model is extended to include   
as a random variable. the resulting distribution over    depends on both the data (x and y) and the prior
id203 p(  ;   ). the corresponding estimate of    is called maximum a posteriori, or map. this is in
contrast with maximum likelihood, which depends only on the data.

jacob eisenstein. draft of november 13, 2018.

2.2. na  ive bayes

23

smoothing reduces variance, but moves us away from the maximum likelihood esti-
mate: it imposes a bias. in this case, the bias points towards uniform probabilities. ma-
chine learning theory shows that errors on heldout data can be attributed to the sum of
bias and variance (mohri et al., 2012). in general, techniques for reducing variance often
increase the bias, leading to a id160.

    unbiased classi   ers may over   t the training data, yielding poor performance on

unseen data.

    but if the smoothing is too large, the resulting classi   er can under   t instead. in the
limit of           , there is zero variance: you get the same classi   er, regardless of the
data. however, the bias is likely to be large.

similar issues arise throughout machine learning. later in this chapter we will encounter
id173, which controls the id160 for id28 and large-
margin classi   ers (   2.5.1);    3.3.2 describes techniques for controlling variance in deep
learning; chapter 6 describes more elaborate methods for smoothing empirical probabili-
ties.

2.2.5 setting hyperparameters
returning to na    ve bayes, how should we choose the best value of hyperparameters like
  ? maximum likelihood will not work: the maximum likelihood estimate of    on the
training set will always be    = 0. in many cases, what we really want is accuracy: the
number of correct predictions, divided by the total number of predictions. (other mea-
sures of classi   cation performance are discussed in    4.4.) as we will see, it is hard to opti-
mize for accuracy directly. but for scalar hyperparameters like   , tuning can be performed
by a simple heuristic called grid search: try a set of values (e.g.,        {0.001, 0.01, 0.1, 1, 10}),
compute the accuracy for each value, and choose the setting that maximizes the accuracy.
the goal is to tune    so that the classi   er performs well on unseen data. for this reason,
the data used for hyperparameter tuning should not overlap the training set, where very
small values of    will be preferred. instead, we hold out a development set (also called
a tuning set) for hyperparameter selection. this development set may consist of a small
fraction of the labeled data, such as 10%.

we also want to predict the performance of our classi   er on unseen data. to do this,
we must hold out a separate subset of data, called the test set. it is critical that the test set
not overlap with either the training or development sets, or else we will overestimate the
performance that the classi   er will achieve on unlabeled data in the future. the test set
should also not be used when making modeling decisions, such as the form of the feature
function, the size of the vocabulary, and so on (these decisions are reviewed in chapter 4.)
the ideal practice is to use the test set only once     otherwise, the test set is used to guide

under contract with mit press, shared under cc-by-nc-nd license.

24

chapter 2. linear text classification

the classi   er design, and test set accuracy will diverge from accuracy on truly unseen
data. because annotated data is expensive, this ideal can be hard to follow in practice,
and many test sets have been used for decades. but in some high-impact applications like
machine translation and information extraction, new test sets are released every year.

when only a small amount of labeled data is available, the test set accuracy can be
unreliable. k-fold cross-validation is one way to cope with this scenario: the labeled
data is divided into k folds, and each fold acts as the test set, while training on the other
folds. the test set accuracies are then aggregated. in the extreme, each fold is a single data
point; this is called leave-one-out cross-validation. to perform hyperparameter tuning
in the context of cross-validation, another fold can be used for grid search. it is important
not to repeatedly evaluate the cross-validated accuracy while making design decisions
about the classi   er, or you will overstate the accuracy on truly unseen data.

2.3 discriminative learning

na    ve bayes is easy to work with: the weights can be estimated in closed form, and the
probabilistic interpretation makes it relatively easy to extend. however, the assumption
that features are independent can seriously limit its accuracy. thus far, we have de   ned
the feature function f (x, y) so that it corresponds to bag-of-words features: one feature
per word in the vocabulary. in natural language, bag-of-words features violate the as-
sumption of conditional independence     for example, the id203 that a document
will contain the word na    ve is surely higher given that it also contains the word bayes    
but this violation is relatively mild.

however, good performance on text classi   cation often requires features that are richer

than the bag-of-words:

    to better handle out-of-vocabulary terms, we want features that apply to multiple

words, such as pre   xes and suf   xes (e.g., anti-, un-, -ing) and capitalization.

    we also want id165 features that apply to multi-word units: bigrams (e.g., not
good, not bad), trigrams (e.g., not so bad, lacking any decency, never before imagined), and
beyond.

these features    agrantly violate the na    ve bayes independence assumption. consider
what happens if we add a pre   x feature. under the na    ve bayes assumption, the joint
id203 of a word and its pre   x are computed with the following approximation:12

pr(word = un   t, pre   x = un- | y)     pr(pre   x = un- | y)    pr(word = un   t | y).
12the notation pr(  ) refers to the id203 of an event, and p(  ) refers to the id203 density or mass

for a random variable (see appendix a).

jacob eisenstein. draft of november 13, 2018.

2.3. discriminative learning

25

to test the quality of the approximation, we can manipulate the left-hand side by applying
the chain rule,

   pr(word = un   t | y)

pr(word = un   t, pre   x = un- | y) = pr(pre   x = un- | word = un   t, y)

[2.31]
[2.32]
but pr(pre   x = un- | word = un   t, y) = 1, since un- is guaranteed to be the pre   x for the
word un   t. therefore,
pr(word = un   t, pre   x = un- | y) =1

   pr(word = un   t | y) [2.33]
(cid:29) pr(pre   x = un- | y)    pr(word = un   t | y), [2.34]
because the id203 of any given word starting with the pre   x un- is much less than
one. na    ve bayes will systematically underestimate the true probabilities of conjunctions
of positively correlated features. to use such features, we need learning algorithms that
do not rely on an independence assumption.

the origin of the na    ve bayes independence assumption is the learning objective,
p(x(1:n ), y(1:n )), which requires modeling the id203 of the observed text. in clas-
si   cation problems, we are always given x, and are only interested in predicting the label
y. in this setting, modeling the id203 of the text x seems like a dif   cult and unnec-
essary task. discriminative learning algorithms avoid this task, and focus directly on the
problem of predicting y.

2.3.1 id88
in na    ve bayes, the weights can be interpreted as parameters of a probabilistic model. but
this model requires an independence assumption that usually does not hold, and limits
our choice of features. why not forget about id203 and learn the weights in an error-
driven way? the id88 algorithm, shown in algorithm 3, is one way to do this.

the algorithm is simple: if you make a mistake, increase the weights for features that
are active with the correct label y(i), and decrease the weights for features that are active
with the guessed label   y. id88 is an online learning algorithm, since the classi   er
weights change after every example. this is different from na    ve bayes, which is a batch
learning algorithm: it computes statistics over the entire dataset, and then sets the weights
in a single operation. algorithm 3 is vague about when this online learning procedure
terminates. we will return to this issue shortly.

the id88 algorithm may seem like an unprincipled heuristic: na    ve bayes has a
solid foundation in id203, but the id88 is just adding and subtracting constants
from the weights every time there is a mistake. will this really work? in fact, there is some
nice theory for the id88, based on the concept of linear separability. informally,
a dataset with binary labels (y     {0, 1}) is linearly separable if it is possible to draw a
under contract with mit press, shared under cc-by-nc-nd license.

26

chapter 2. linear text classification

t     0
  (0)     0
repeat

algorithm 3 id88 learning algorithm
1: procedure id88(x(1:n ), y(1:n ))
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

t     t + 1
select an instance i
  y     argmaxy   (t   1)    f (x(i), y)
if   y (cid:54)= y(i) then
else

  (t)       (t   1) + f (x(i), y(i))     f (x(i),   y)
  (t)       (t   1)

until tired
return   (t)

hyperplane (a line in many dimensions), such that on each side of the hyperplane, all
instances have the same label. this de   nition can be formalized and extended to multiple
labels:
i=1 is linearly separable iff
de   nition 1 (linear separability). the dataset d = {(x(i), y(i))}n
(if and only if) there exists some weight vector    and some margin    such that for every instance
(x(i), y(i)), the inner product of    and the feature function for the true label,       f (x(i), y(i)), is
at least    greater than inner product of    and the feature function for every other possible label,
      f (x(i), y(cid:48)).

     ,    > 0 :    (x(i), y(i))     d,

      f (x(i), y(i))        + max
y(cid:48)(cid:54)=y(i)

      f (x(i), y

(cid:48)

).

[2.35]

linear separability is important because of the following guarantee: if your data is
linearly separable, then the id88 algorithm will    nd a separator (novikoff, 1962).13
so while the id88 may seem heuristic, it is guaranteed to succeed, if the learning
problem is easy enough.

how useful is this proof? minsky and papert (1969) famously proved that the simple
logical function of exclusive-or is not separable, and that a id88 is therefore inca-
pable of learning this function. but this is not just an issue for the id88: any linear
classi   cation algorithm, including na    ve bayes, will fail on this task. text classi   cation
problems usually involve high dimensional feature spaces, with thousands or millions of

13it is also possible to prove an upper bound on the number of training iterations required to    nd the

separator. proofs like this are part of the    eld of machine learning theory (mohri et al., 2012).

jacob eisenstein. draft of november 13, 2018.

2.4. id168s and large-margin classification

27

features. for these problems, it is very likely that the training data is indeed separable.
and even if the dataset is not separable, it is still possible to place an upper bound on the
number of errors that the id88 algorithm will make (freund and schapire, 1999).

2.3.2 averaged id88
the id88 iterates over the data repeatedly     until    tired   , as described in algo-
rithm 3. if the data is linearly separable, the id88 will eventually    nd a separator,
and we can stop once all training instances are classi   ed correctly. but if the data is not
linearly separable, the id88 can thrash between two or more weight settings, never
converging. in this case, how do we know that we can stop training, and how should
we choose the    nal weights? an effective practical solution is to average the id88
weights across all iterations.

this procedure is shown in algorithm 4. the learning algorithm is nearly identical,
but we also maintain a vector of the sum of the weights, m. at the end of the learning
procedure, we divide this sum by the total number of updates t, to compute the average
weights,   . these average weights are then used for prediction. in the algorithm sketch,
the average is computed from a running sum, m     m +   . however, this is inef   cient,
because it requires |  | operations to update the running sum. when f (x, y) is sparse,
|  | (cid:29) |f (x, y)| for any individual (x, y). this means that computing the running sum will
be much more expensive than computing of the update to    itself, which requires only
2    |f (x, y)| operations. one of the exercises is to sketch a more ef   cient algorithm for
computing the averaged weights.

even if the dataset is not separable, the averaged weights will eventually converge.
one possible stopping criterion is to check the difference between the average weight
vectors after each pass through the data: if the norm of the difference falls below some
prede   ned threshold, we can stop training. another stopping criterion is to hold out some
data, and to measure the predictive accuracy on this heldout data. when the accuracy
on the heldout data starts to decrease, the learning algorithm has begun to over   t the
training set. at this point, it is probably best to stop; this stopping criterion is known as
early stopping.

generalization is the ability to make good predictions on instances that are not in
the training data. averaging can be proven to improve generalization, by computing an
upper bound on the generalization error (freund and schapire, 1999; collins, 2002).

2.4 id168s and large-margin classi   cation

na    ve bayes chooses the weights    by maximizing the joint log-likelihood log p(x(1:n ), y(1:n )).
by convention, optimization problems are generally formulated as minimization of a loss
function. the input to a id168 is the vector of weights   , and the output is a

under contract with mit press, shared under cc-by-nc-nd license.

28

chapter 2. linear text classification

t     0
  (0)     0
repeat

algorithm 4 averaged id88 learning algorithm
1: procedure avg-id88(x(1:n ), y(1:n ))
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:

  (t)       (t   1) + f (x(i), y(i))     f (x(i),   y)
  (t)       (t   1)
m     m +   (t)
until tired
       1
t m
return   

t     t + 1
select an instance i
  y     argmaxy   (t   1)    f (x(i), y)
if   y (cid:54)= y(i) then
else

non-negative number, measuring the performance of the classi   er on a training instance.
formally, the loss (cid:96)(  ; x(i), y(i)) is then a measure of the performance of the weights    on
the instance (x(i), y(i)). the goal of learning is to minimize the sum of the losses across all
instances in the training set.

we can trivially reformulate maximum likelihood as a id168, by de   ning the

id168 to be the negative log-likelihood:

log p(x(1:n ), y(1:n );   ) =

log p(x(i), y(i);   )

(cid:96)nb(  ; x(i), y(i)) =     log p(x(i), y(i);   )

n(cid:88)i=1

     = argmin

  

(cid:96)nb(  ; x(i), y(i))

= argmax

  

log p(x(i), y(i);   ).

n(cid:88)i=1
n(cid:88)i=1

[2.36]

[2.37]

[2.38]

[2.39]

the problem of minimizing (cid:96)nb is thus identical to maximum-likelihood estimation.

id168s provide a general framework for comparing learning objectives. for

example, an alternative id168 is the zero-one loss,

(cid:96)0-1(  ; x(i), y(i)) =(cid:40)0,

y(i) = argmaxy       f (x(i), y)

1, otherwise

[2.40]

jacob eisenstein. draft of november 13, 2018.

2.4. id168s and large-margin classification

29

the zero-one loss is zero if the instance is correctly classi   ed, and one otherwise. the
sum of zero-one losses is proportional to the error rate of the classi   er on the training
data. since a low error rate is often the ultimate goal of classi   cation, this may seem
ideal. but the zero-one loss has several problems. one is that it is non-convex,14 which
means that there is no guarantee that gradient-based optimization will be effective. a
more serious problem is that the derivatives are useless: the partial derivative with respect
to any parameter is zero everywhere, except at the points where     f (x(i), y) =     f (x(i),   y)
for some   y. at those points, the loss is discontinuous, and the derivative is unde   ned.

the id88 optimizes a id168 that has better properties for learning:

(cid:96)id88(  ; x(i), y(i)) = max

y   y       f (x(i), y)           f (x(i), y(i)),

[2.41]

when   y = y(i), the loss is zero; otherwise, it increases linearly with the gap between the
score for the predicted label   y and the score for the true label y(i). plotting this loss against
the input maxy   y       f (x(i), y)           f (x(i), y(i)) gives a hinge shape, motivating the name
hinge loss.

to see why this is the id168 optimized by the id88, take the derivative

with respect to   ,

   
     

(cid:96)id88(  ; x(i), y(i)) = f (x(i),   y)     f (x(i), y(i)).

[2.42]

at each instance, the id88 algorithm takes a step of magnitude one in the opposite
direction of this gradient,      (cid:96)id88 =    
      (cid:96)id88(  ; x(i), y(i)). as we will see in
   2.6, this is an example of the optimization algorithm stochastic id119, applied
to the objective in equation 2.41.

15 careful readers will notice the tacit assump-
*breaking ties with subid119
tion that there is a unique   y that maximizes       f (x(i), y). what if there are two or more
labels that maximize this function? consider binary classi   cation: if the maximizer is y(i),
then the gradient is zero, and so is the id88 update; if the maximizer is   y (cid:54)= y(i),
then the update is the difference f (x(i), y(i))     f (x(i),   y). the underlying issue is that the
id88 loss is not smooth, because the    rst derivative has a discontinuity at the hinge
point, where the score for the true label y(i) is equal to the score for some other label   y. at
this point, there is no unique gradient; rather, there is a set of subgradients. a vector v is
14a function f is convex iff   f (xi)+(1     )f (xj)     f (  xi +(1     )xj), for all        [0, 1] and for all xi and xj
on the domain of the function. in words, any weighted average of the output of f applied to any two points is
larger than the output of f when applied to the weighted average of the same two points. convexity implies
that any local minimum is also a global minimum, and there are many effective techniques for optimizing
convex functions (boyd and vandenberghe, 2004). see appendix b for a brief review.

15throughout this text, advanced topics will be marked with an asterisk.

under contract with mit press, shared under cc-by-nc-nd license.

30

chapter 2. linear text classification

a subgradient of the function g at u0 iff g(u)     g(u0)     v    (u     u0) for all u. graphically,
this de   nes the set of hyperplanes that include g(u0) and do not intersect g at any other
point. as we approach the hinge point from the left, the gradient is f (x,   y)   f (x, y); as we
approach from the right, the gradient is 0. at the hinge point, the subgradients include all
vectors that are bounded by these two extremes. in subid119, any subgradient
can be used (bertsekas, 2012). since both 0 and f (x,   y)     f (x, y) are subgradients at the
hinge point, either one can be used in the id88 update. this means that if multiple
labels maximize       f (x(i), y), any of them can be used in the id88 update.

id88 versus na    ve bayes the id88 id168 has some pros and cons
with respect to the negative log-likelihood loss implied by na    ve bayes.

    both (cid:96)nb and (cid:96)id88 are convex, making them relatively easy to optimize. how-
ever, (cid:96)nb can be optimized in closed form, while (cid:96)id88 requires iterating over
the dataset multiple times.

    (cid:96)nb can suffer in   nite loss on a single example, since the logarithm of zero id203
is negative in   nity. na    ve bayes will therefore overemphasize some examples, and
underemphasize others.

    the na    ve bayes classi   er assumes that the observed features are conditionally in-
dependent, given the label, and the performance of the classi   er depends on the
extent to which this assumption holds. the id88 requires no such assump-
tion.

    (cid:96)id88 treats all correct answers equally. even if    only gives the correct answer

by a tiny margin, the loss is still zero.

2.4.1 online large margin classi   cation

this last comment suggests a potential problem with the id88. suppose a test ex-
ample is very close to a training example, but not identical. if the classi   er only gets the
correct answer on the training example by a small amount, then it may give a different
answer on the nearby test instance. to formalize this intuition, de   ne the margin as,

  (  ; x(i), y(i)) =       f (x(i), y(i))     max
y(cid:54)=y(i)

      f (x(i), y).

[2.43]

the margin represents the difference between the score for the correct label y(i), and
the score for the highest-scoring incorrect label. the intuition behind large margin clas-
si   cation is that it is not enough to label the training data correctly     the correct label
should be separated from other labels by a comfortable margin. this idea can be encoded

jacob eisenstein. draft of november 13, 2018.

2.4. id168s and large-margin classification

31

figure 2.2: margin, zero-one, and logistic id168s.

into a id168,

(cid:96)margin(  ; x(i), y(i)) =(cid:40)0,

  (  ; x(i), y(i))     1,

1       (  ; x(i), y(i)), otherwise
=(cid:16)1       (  ; x(i), y(i))(cid:17)+

,

[2.44]

[2.45]

where (x)+ = max(0, x). the loss is zero if there is a margin of at least 1 between the
score for the true label and the best-scoring alternative   y. this is almost identical to the
id88 loss, but the hinge point is shifted to the right, as shown in figure 2.2. the
margin loss is a convex upper bound on the zero-one loss.

the margin loss can be minimized using an online learning rule that is similar to per-
ceptron. we will call this learning rule the online support vector machine, for reasons
that will be discussed in the derivation. let us    rst generalize the notion of a classi   ca-
tion error with a cost function c(y(i), y). we will focus on the simple cost function,

c(y(i), y) =(cid:40)1,

y(i) (cid:54)=   y

0, otherwise,

[2.46]

but it is possible to design specialized cost functions that assign heavier penalties to espe-
cially undesirable errors (tsochantaridis et al., 2004). this idea is revisited in chapter 7.

using the cost function, we can now de   ne the online support vector machine as the

under contract with mit press, shared under cc-by-nc-nd license.

   2   1012    f(x(i),y(i))       f(x(i),  y)0123loss0/1lossmarginlosslogisticloss32

chapter 2. linear text classification

following classi   cation rule:

  y = argmax

y   y       f (x(i), y) + c(y(i), y)

  (t)    (1       )  (t   1) + f (x(i), y(i))     f (x(i),   y)

[2.47]

[2.48]

this update is similar in form to the id88, with two key differences.

    rather than selecting the label   y that maximizes the score of the current classi   -
cation model, the argmax searches for labels that are both strong, as measured by
      f (x(i), y), and wrong, as measured by c(y(i), y). this maximization is known as
cost-augmented decoding, because it augments the maximization objective to favor
high-cost labels. if the highest-scoring label is y = y(i), then the margin loss for
this instance is zero, and no update is needed. if not, then an update is required to
reduce the margin loss     even if the current model classi   es the instance correctly.
cost augmentation is only done while learning; it is not applied when making pre-
dictions on unseen data.
    the previous weights   (t   1) are scaled by (1       ), with        (0, 1). the effect of this
term is to cause the weights to    decay    back towards zero. in the support vector
machine, this term arises from the minimization of a speci   c form of the margin, as
described below. however, it can also be viewed as a form of id173, which
can help to prevent over   tting (see    2.5.1). in this sense, it plays a role that is similar
to smoothing in na    ve bayes (see    2.2.4).

2.4.2

*derivation of the online support vector machine

the derivation of the online support vector machine is somewhat involved, but gives
further intuition about why the method works. begin by returning the idea of linear sep-
arability (de   nition 1): if a dataset is linearly separable, then there is some hyperplane   
that correctly classi   es all training instances with margin   . this margin can be increased
to any desired value by multiplying the weights by a constant.

||  ||2

(cid:113)(cid:80)j   2

plane is given by   (  ;x(i),y(i))

now, for any datapoint (x(i), y(i)), the geometric distance to the separating hyper-
, where the denominator is the norm of the weights, ||  ||2 =
j . the geometric distance is sometimes called the geometric margin, in contrast to
the functional margin   (  ; x(i), y(i)). both are shown in figure 2.3. the geometric margin
is a good measure of the robustness of the separator: if the functional margin is large, but
the norm ||  ||2 is also large, then a small change in x(i) could cause it to be misclassi   ed.
we therefore seek to maximize the minimum geometric margin across the dataset, subject

jacob eisenstein. draft of november 13, 2018.

2.4. id168s and large-margin classification

33

figure 2.3: functional and geometric margins for a binary classi   cation problem. all
separators that satisfy the margin constraint are shown. the separator with the largest
geometric margin is shown in bold.

to the constraint that the margin loss is always zero:

max

  
s.t.

  (  ; x(i), y(i))

min

i=1,2,...n

  (  ; x(i), y(i))     1,

||  ||2
   i.

[2.49]

this is a constrained optimization problem, where the second line describes constraints
on the space of possible solutions   .
in this case, the constraint is that the functional
margin always be at least one, and the objective is that the minimum geometric margin
be as large as possible.

constrained optimization is reviewed in appendix b. in this case, further manipula-
tion yields an unconstrained optimization problem. first, note that the norm ||  ||2 scales
linearly: ||a  ||2 = a||  ||2. furthermore, the functional margin    is a linear function of   ,
so that   (a  , x(i), y(i)) = a  (  , x(i), y(i)). as a result, any scaling factor on    will cancel in
the numerator and denominator of the geometric margin. if the data is linearly separable
at any    > 0, it is always possible to rescale the functional margin to 1 by multiplying   
by a scalar constant. we therefore need only minimize the denominator ||  ||2, subject to
the constraint on the functional margin. the minimizer of ||  ||2 is also the minimizer of
j , which is easier to work with. this yields a simpler optimization prob-
2||  ||2

2 = 1

1

under contract with mit press, shared under cc-by-nc-nd license.

2(cid:80)   2

functionalmargingeometricmargin34

lem:

chapter 2. linear text classification

2

1
2||  ||2
  (  ; x(i), y(i))     1,

   i.

[2.50]

min

.

  
s.t.

this problem is a quadratic program: the objective is a quadratic function of the pa-
rameters, and the constraints are all linear inequalities. one solution to this problem is
to incorporate the constraints through lagrange multipliers   i     0, i = 1, 2, . . . , n. the
instances for which   i > 0 are called support vectors; other instances are irrelevant to the
classi   cation boundary. this motivates the name support vector machine.

thus far we have assumed linear separability, but many datasets of interest are not
linearly separable. in this case, there is no    that satis   es the margin constraint. to add
more    exibility, we can introduce a set of slack variables   i     0. instead of requiring that
the functional margin be greater than or equal to one, we require that it be greater than or
equal to 1       i. ideally there would not be any slack, so the slack variables are penalized
in the objective function:

min
  ,  

s.t.

  i

n(cid:88)i=1

2 + c

1
2||  ||2
  (  ; x(i), y(i)) +   i     1,
  i     0,

   i.

   i

[2.51]

the hyperparameter c controls the tradeoff between violations of the margin con-
straint and the preference for a low norm of   . as c        , slack is in   nitely expensive,
and there is only a solution if the data is separable. as c     0, slack becomes free, and
there is a trivial solution at    = 0. thus, c plays a similar role to the smoothing parame-
ter in na    ve bayes (   2.2.4), trading off between a close    t to the training data and better
generalization. like the smoothing parameter of na    ve bayes, c must be set by the user,
typically by maximizing performance on a heldout development set.

to solve the constrained optimization problem de   ned in equation 2.51, we can    rst

solve for the slack variables,

  i     (1       (  ; x(i), y(i)))+.

[2.52]
the inequality is tight: the optimal solution is to make the slack variables as small as
possible, while still satisfying the constraints (ratliff et al., 2007; smith, 2011). by plugging
in the minimum slack variables back into equation 2.51, the problem can be transformed
into the unconstrained optimization,

min

  

  
2||  ||2

2 +

n(cid:88)i=1

(1       (  ; x(i), y(i)))+,

[2.53]

jacob eisenstein. draft of november 13, 2018.

2.5. id28

35

where each   i has been substituted by the right-hand side of equation 2.52, and the factor
of c on the slack variables has been replaced by an equivalent factor of    = 1
c on the
norm of the weights.

equation 2.53 can be rewritten by expanding the margin,

min

  

  
2||  ||2

2 +

n(cid:88)i=1(cid:18)max

y   y (cid:16)      f (x(i), y) + c(y(i), y)(cid:17)           f (x(i), y(i))(cid:19)+

,

[2.54]

where c(y, y(i)) is the cost function de   ned in equation 2.46. we can now differentiate
with respect to the weights,

     lid166 =     +

n(cid:88)i=1

f (x(i),   y)     f (x(i), y(i)),

[2.55]

where lid166 refers to minimization objective in equation 2.54 and   y = argmaxy   y      
f (x(i), y) + c(y(i), y). the online support vector machine update arises from the appli-
cation of stochastic id119 (described in    2.6.2) to this gradient.

2.5 id28

thus far, we have seen two broad classes of learning algorithms. na    ve bayes is a prob-
abilistic method, where learning is equivalent to estimating a joint id203 distribu-
tion. the id88 and support vector machine are discriminative, error-driven algo-
rithms: the learning objective is closely related to the number of errors on the training
data. probabilistic and error-driven approaches each have advantages: id203 makes
it possible to quantify uncertainty about the predicted labels, but the id203 model of
na    ve bayes makes unrealistic independence assumptions that limit the features that can
be used.

id28 combines advantages of discriminative and probabilistic classi-
   ers. unlike na    ve bayes, which starts from the joint id203 px,y , id28
de   nes the desired id155 py |x directly. think of       f (x, y) as a scoring
function for the compatibility of the base features x and the label y. to convert this score
into a id203, we    rst exponentiate, obtaining exp (      f (x, y)), which is guaranteed
to be non-negative. next, we normalize, dividing over all possible labels y(cid:48)
    y. the
resulting id155 is de   ned as,

p(y | x;   ) =

exp (      f (x, y))

(cid:80)y(cid:48)   y exp (      f (x, y(cid:48)))

.

[2.56]

under contract with mit press, shared under cc-by-nc-nd license.

36

chapter 2. linear text classification

tional likelihood,

log p(y(1:n ) | x(1:n );   ) =

given a dataset d = {(x(i), y(i))}n
n(cid:88)i=1
n(cid:88)i=1

=

i=1, the weights    are estimated by maximum condi-

log p(y(i) | x(i);   )
      f (x(i), y(i))     log(cid:88)y(cid:48)   y

[2.57]

[2.58]

exp(cid:16)      f (x(i), y

(cid:48)

)(cid:17) .

the    nal line is obtained by plugging in equation 2.56 and taking the logarithm.16 inside
the sum, we have the (additive inverse of the) logistic loss,

(cid:96)logreg(  ; x(i), y(i)) =          f (x(i), y(i)) + log(cid:88)y(cid:48)   y

exp(      f (x(i), y

(cid:48)

))

[2.59]

the logistic loss is shown in figure 2.2 on page 31. a key difference from the zero-one
and hinge losses is that logistic loss is never zero. this means that the objective function
can always be improved by assigning higher con   dence to the correct label.

2.5.1 id173
as with the support vector machine, better generalization can be obtained by penalizing
2 to the
the norm of   . this is done by adding a multiple of the squared norm   
2 is the squared l2
minimization objective. this is called l2 id173, because ||  ||2
norm of the vector   . id173 forces the estimator to trade off performance on the
training data against the norm of the weights, and this can help to prevent over   tting.
consider what would happen to the unregularized weight for a base feature j that is
active in only one instance x(i): the conditional log-likelihood could always be improved
by increasing the weight for this feature, so that   (j,y(i))         and   (j,  y(cid:54)=y(i))           , where
(j, y) is the index of feature associated with x(i)

j and label y in f (x(i), y).

2||  ||2

in    2.2.4 (footnote 11), we saw that smoothing the probabilities of a na    ve bayes clas-
si   er can be justi   ed as a form of maximum a posteriori estimation, in which the param-
eters of the classi   er are themselves random variables, drawn from a prior distribution.
the same justi   cation applies to l2 id173. in this case, the prior is a zero-mean
gaussian on each term of   . the log-likelihood under a zero-mean gaussian is,

log n (  j; 0,   2)        

1
2  2   2
j ,

[2.60]

  2 .
so that the id173 weight    is equal to the inverse variance of the prior,    = 1

16the log-sum-exp term is a common pattern in machine learning. it is numerically unstable, because it
will under   ow if the inner product is small, and over   ow if the inner product is large. scienti   c computing
libraries usually contain special functions for computing logsumexp, but with some thought, you should be
able to see how to create an implementation that is numerically stable.

jacob eisenstein. draft of november 13, 2018.

2.6. optimization

37

2.5.2 gradients
logistic loss is minimized by optimization along the gradient. speci   c algorithms are de-
scribed in the next section, but    rst let   s compute the gradient with respect to the logistic
loss of a single example:

1

   (cid:96)
     

=     f (x(i), y(i)) +

(cid:96)logreg =           f (x(i), y(i)) + log (cid:88)y(cid:48)   y
(cid:80)y(cid:48)(cid:48)   y
=     f (x(i), y(i)) + (cid:88)y(cid:48)   y
=     f (x(i), y(i)) + (cid:88)y(cid:48)   y
=     f (x(i), y(i)) + ey |x [f (x(i), y)].

exp(cid:16)      f (x(i), y(cid:48))(cid:17)
exp(cid:0)      f (x(i), y(cid:48)(cid:48))(cid:1)    (cid:88)y(cid:48)   y
exp(cid:0)      f (x(i), y(cid:48))(cid:1)
exp(cid:0)      f (x(i), y(cid:48)(cid:48))(cid:1)    f (x(i), y(cid:48))
(cid:80)y(cid:48)(cid:48)   y
p(y(cid:48) | x(i);   )    f (x(i), y(cid:48))

exp(cid:16)      f (x(i), y(cid:48))(cid:17)    f (x(i), y(cid:48))

[2.61]

[2.62]

[2.63]

[2.64]

[2.65]

the    nal step employs the de   nition of a conditional expectation (   a.5). the gradient of
the logistic loss is equal to the difference between the expected counts under the current
model, ey |x [f (x(i), y)], and the observed feature counts f (x(i), y(i)). when these two
vectors are equal for a single instance, there is nothing more to learn from it; when they
are equal in sum over the entire dataset, there is nothing more to learn from the dataset as
a whole. the gradient of the hinge loss is nearly identical, but it involves the features of
the predicted label under the current model, f (x(i),   y), rather than the expected features
ey |x [f (x(i), y)] under the conditional distribution p(y | x;   ).

the regularizer contributes      to the overall gradient:

llogreg =

  
2||  ||2

     llogreg =        

2    

            f (x(i), y(i))     log(cid:88)y(cid:48)   y
n(cid:88)i=1
n(cid:88)i=1(cid:16)f (x(i), y(i))     ey|x[f (x(i), y)](cid:17) .

exp       f (x(i), y

(cid:48)

)      

[2.66]

[2.67]

2.6 optimization

each of the classi   cation algorithms in this chapter can be viewed as an optimization
problem:

    in na    ve bayes, the objective is the joint likelihood log p(x(1:n ), y(1:n )). maximum

likelihood estimation yields a closed-form solution for   .

under contract with mit press, shared under cc-by-nc-nd license.

38

chapter 2. linear text classification

    in the support vector machine, the objective is the regularized margin loss,

lid166 =

  
2||  ||2

2 +

n(cid:88)i=1

y   y (      f (x(i), y) + c(y(i), y))           f (x(i), y(i)))+,
(max

[2.68]

there is no closed-form solution, but the objective is convex. the id88 algo-
rithm minimizes a similar objective.

    in id28, the objective is the regularized negative log-likelihood,

llogreg =

  
2||  ||2

2    

n(cid:88)i=1

            f (x(i), y(i))     log(cid:88)y   y

exp(cid:16)      f (x(i), y)(cid:17)       [2.69]

again, there is no closed-form solution, but the objective is convex.

these learning algorithms are distinguished by what is being optimized, rather than
how the optimal weights are found. this decomposition is an essential feature of con-
temporary machine learning. the domain expert   s job is to design an objective function
    or more generally, a model of the problem. if the model has certain characteristics,
then generic optimization algorithms can be used to    nd the solution. in particular, if an
objective function is differentiable, then gradient-based optimization can be employed;
if it is also convex, then gradient-based optimization is guaranteed to    nd the globally
optimal solution. the support vector machine and id28 have both of these
properties, and so are amenable to generic id76 techniques (boyd and
vandenberghe, 2004).

2.6.1 batch optimization
in batch optimization, each update to the weights is based on a computation involving
the entire dataset. one such algorithm is id119, which iteratively updates the
weights,

  (t+1)       (t)       (t)     l,

[2.70]

where      l is the gradient computed over the entire training set, and   (t) is the learning
rate at iteration t.
if the objective l is a convex function of   , then this procedure is
guaranteed to terminate at the global optimum, for appropriate schedule of learning rates,
  (t).17

(cid:80)   t=1   (t) =     and (cid:80)   t=1(  (t))2 <     (bottou et al., 2016). these properties are satis   ed by any learning

17convergence proofs typically require the learning rate to satisfy the following conditions:

rate schedule   (t) =   (0)t      for        [1, 2].

jacob eisenstein. draft of november 13, 2018.

2.6. optimization

39

in practice, id119 can be slow to converge, as the gradient can become
in   nitesimally small. faster convergence can be obtained by second-order newton opti-
mization, which incorporates the inverse of the hessian matrix,

hi,j =

   2l
     i     j

[2.71]

the size of the hessian matrix is quadratic in the number of features. in the bag-of-words
representation, this is usually too big to store, let alone invert. quasi-network optimiza-
tion techniques maintain a low-rank approximation to the inverse of the hessian matrix.
such techniques usually converge more quickly than id119, while remaining
computationally tractable even for large feature sets. a popular quasi-newton algorithm
is l-bfgs (liu and nocedal, 1989), which is implemented in many scienti   c computing
environments, such as scipy and matlab.

for any gradient-based technique, the user must set the learning rates   (t). while con-
vergence proofs usually employ a decreasing learning rate, in practice, it is common to    x
  (t) to a small constant, like 10   3. the speci   c constant can be chosen by experimentation,
although there is research on determining the learning rate automatically (schaul et al.,
2013; wu et al., 2018).

2.6.2 online optimization
batch optimization computes the objective on the entire training set before making an up-
date. this may be inef   cient, because at early stages of training, a small number of train-
ing examples could point the learner in the correct direction. online learning algorithms
make updates to the weights while iterating through the training data. the theoretical
basis for this approach is a stochastic approximation to the true objective function,

n(cid:88)i=1

(cid:96)(  ; x(i), y(i))     n    (cid:96)(  ; x(j), y(j)),

(x(j), y(j))     {(x(i), y(i))}n

i=1,

[2.72]

where the instance (x(j), y(j)) is sampled at random from the full dataset.

in stochastic id119, the approximate gradient is computed by randomly
sampling a single instance, and an update is made immediately. this is similar to the
id88 algorithm, which also updates the weights one instance at a time. in mini-
batch stochastic id119, the gradient is computed over a small set of instances.
a typical approach is to set the minibatch size so that the entire batch    ts in memory on a
graphics processing unit (gpu; neubig et al., 2017). it is then possible to speed up learn-
ing by parallelizing the computation of the gradient over each instance in the minibatch.
algorithm 5 offers a generalized view of id119. in standard gradient de-
scent, the batcher returns a single batch with all the instances. in stochastic gradient de-

under contract with mit press, shared under cc-by-nc-nd license.

40

chapter 2. linear text classification

       0
t     0
repeat

algorithm 5 generalized id119. the function batcher partitions the train-
ing set into b batches such that each instance appears in exactly one batch. in gradient
descent, b = 1; in stochastic id119, b = n; in minibatch stochastic gradient
descent, 1 < b < n.
1: procedure gradient-descent(x(1:n ), y(1:n ), l,   (1...   ), batcher, tmax)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:

t     t + 1
  (t)       (t   1)       (t)     l(  (t   1); x(b(n)
if converged(  (1,2,...,t)) then

(b(1), b(2), . . . , b(b))     batcher(n )
for n     {1, 2, . . . , b} do

until t     tmax
return   (t)

2 ,...), y(b(n)

return   (t)

2 ,...))

1 ,b(n)

1 ,b(n)

scent, it returns n batches with one instance each. in mini-batch settings, the batcher
returns b minibatches, 1 < b < n.

there are many other techniques for online learning, and research in this area is on-
going (bottou et al., 2016). some algorithms use an adaptive learning rate, which can be
different for every feature (duchi et al., 2011). features that occur frequently are likely
to be updated frequently, so it is best to use a small learning rate; rare features will be
updated infrequently, so it is better to take larger steps. the adagrad (adaptive gradient)
algorithm achieves this behavior by storing the sum of the squares of the gradients for
each feature, and rescaling the learning rate by its inverse:

gt =     l(  (t); x(i), y(i))

  (t+1)
j      (t)

j    

where j iterates over features in f (x, y).

(cid:113)(cid:80)t

  (t)
t(cid:48)=1 g2

t,j

gt,j,

[2.73]

[2.74]

in most cases, the number of active features for any instance is much smaller than the
number of weights. if so, the computation cost of online optimization will be dominated
by the update from the id173 term,     . the solution is to be    lazy   , updating
each   j only as it is used. to implement lazy updating, store an additional parameter   j,
which is the iteration at which   j was last updated. if   j is needed at time t, the t       
id173 updates can be performed all at once. this strategy is described in detail
by kummerfeld et al. (2015).

jacob eisenstein. draft of november 13, 2018.

2.7. *additional topics in classification

41

2.7 *additional topics in classi   cation

this section presents some additional topics in classi   cation that are particularly relevant
for natural language processing, especially for understanding the research literature.

2.7.1 feature selection by id173

in id28 and large-margin classi   cation, generalization can be improved by
regularizing the weights towards 0, using the l2 norm. but rather than encouraging
weights to be small, it might be better for the model to be sparse: it should assign weights
of exactly zero to most features, and only assign non-zero weights to features that are

clearly necessary. this idea can be formalized by the l0 norm, l0 = ||  ||0 =(cid:80)j    (  j (cid:54)= 0),

which applies a constant penalty for each non-zero weight. this norm can be thought
of as a form of feature selection: optimizing the l0-regularized conditional likelihood is
equivalent to trading off the log-likelihood against the number of active features. reduc-
ing the number of active features is desirable because the resulting model will be fast,
low-memory, and should generalize well, since irrelevant features will be pruned away.
unfortunately, the l0 norm is non-convex and non-differentiable. optimization under l0
id173 is np-hard, meaning that it can be solved ef   ciently only if p=np (ge et al.,
2011).

a useful alternative is the l1 norm, which is equal to the sum of the absolute values

of the weights, ||  ||1 =(cid:80)j |  j|. the l1 norm is convex, and can be used as an approxima-

tion to l0 (tibshirani, 1996). conveniently, the l1 norm also performs feature selection,
by driving many of the coef   cients to zero; it is therefore known as a sparsity inducing
regularizer. the l1 norm does not have a gradient at   j = 0, so we must instead optimize
the l1-regularized objective using subgradient methods. the associated stochastic sub-
id119 algorithms are only somewhat more complex than conventional sgd;
sra et al. (2012) survey approaches for estimation under l1 and other regularizers.

gao et al. (2007) compare l1 and l2 id173 on a suite of nlp problems,    nding
that l1 id173 generally gives similar accuracy to l2 id173, but that l1
id173 produces models that are between ten and    fty times smaller, because more
than 90% of the feature weights are set to zero.

2.7.2 other views of id28

in binary classi   cation, we can dispense with the feature function, and choose y based on
the inner product of       x. the id155 py |x is obtained by passing this
under contract with mit press, shared under cc-by-nc-nd license.

42

chapter 2. linear text classification

inner product through a logistic function,

  (a) (cid:44) exp(a)

1 + exp(a)

= (1 + exp(   a))

   1

[2.75]

p(y | x;   ) =  (      x).

[2.76]
this is the origin of the name    id28.    id28 can be viewed as
part of a larger family of generalized linear models (glms), in which various other link
functions convert between the inner product       x and the parameter of a conditional
id203 distribution.

id28 and related models are sometimes referred to as log-linear, be-
cause the log-id203 is a linear function of the features. but in the early nlp liter-
ature, id28 was often called maximum id178 classi   cation (berger et al.,
1996). this name refers to an alternative formulation, in which the goal is to    nd the max-
imum id178 id203 function that satis   es moment-matching constraints. these
constraints specify that the empirical counts of each feature should match the expected
counts under the induced id203 distribution py |x;  ,

n(cid:88)i=1

n(cid:88)i=1(cid:88)y   y

fj(x(i), y(i)) =

p(y | x(i);   )fj(x(i), y),

   j

[2.77]

the moment-matching constraint is satis   ed exactly when the derivative of the condi-
tional log-likelihood function (equation 2.65) is equal to zero. however, the constraint
can be met by many values of   , so which should we choose?

the id178 of the id155 distribution py |x is,

h(py |x ) =    (cid:88)x   x

px (x)(cid:88)y   y

py |x (y | x) log py |x (y | x),

[2.78]

where x is the set of all possible feature vectors, and px (x) is the id203 of observing
the base features x. the distribution px is unknown, but it can be estimated by summing
over all the instances in the training set,

  h(py |x ) =    

1
n

n(cid:88)i=1(cid:88)y   y

py |x (y | x(i)) log py |x (y | x(i)).

[2.79]

if the id178 is large, the likelihood function is smooth across possible values of y;
if it is small, the likelihood function is sharply peaked at some preferred value; in the
limiting case, the id178 is zero if p(y | x) = 1 for some y. the maximum-id178 cri-
terion chooses to make the weakest commitments possible, while satisfying the moment-
matching constraints from equation 2.77. the solution to this constrained optimization
problem is identical to the maximum conditional likelihood (logistic-loss) formulation
that was presented in    2.5.

jacob eisenstein. draft of november 13, 2018.

2.8. summary of learning algorithms

43

2.8 summary of learning algorithms

it is natural to ask which learning algorithm is best, but the answer depends on what
characteristics are important to the problem you are trying to solve.

na    ve bayes pros: easy to implement; estimation is fast, requiring only a single pass over
the data; assigns probabilities to predicted labels; controls over   tting with smooth-
ing parameter. cons: often has poor accuracy, especially with correlated features.

id88 pros: easy to implement; online; error-driven learning means that accuracy
is typically high, especially after averaging. cons: not probabilistic; hard to know
when to stop learning; lack of margin can lead to over   tting.

support vector machine pros: optimizes an error-based metric, usually resulting in high
accuracy; over   tting is controlled by a id173 parameter. cons: not proba-
bilistic.

id28 pros: error-driven and probabilistic; over   tting is controlled by a reg-
ularization parameter. cons: batch learning requires black-box optimization; logistic
loss can    overtrain    on correctly labeled examples.

one of the main distinctions is whether the learning algorithm offers a id203
over labels. this is useful in modular architectures, where the output of one classi   er
is the input for some other system. in cases where id203 is not necessary, the sup-
port vector machine is usually the right choice, since it is no more dif   cult to implement
than the id88, and is often more accurate. when id203 is necessary, logistic
regression is usually more accurate than na    ve bayes.

additional resources

a machine learning textbook will offer more classi   ers and more details (e.g., murphy,
2012), although the notation will differ slightly from what is typical in natural language
processing. probabilistic methods are surveyed by hastie et al. (2009), and mohri et al.
(2012) emphasize theoretical considerations. bottou et al. (2016) surveys the rapidly mov-
ing    eld of online learning, and kummerfeld et al. (2015) empirically review several opti-
mization algorithms for large-margin learning. the python toolkit scikit-learn includes
implementations of all of the algorithms described in this chapter (pedregosa et al., 2011).
appendix b describes an alternative large-margin classi   er, called passive-aggressive.
passive-aggressive is an online learner that seeks to make the smallest update that satis   es
the margin constraint at the current instance. it is closely related to mira, which was used
widely in nlp in the 2000s (crammer and singer, 2003).

under contract with mit press, shared under cc-by-nc-nd license.

44

exercises

chapter 2. linear text classification

there will be exercises at the end of each chapter. in this chapter, the exercises are mostly
mathematical, matching the subject material. in other chapters, the exercises will empha-
size linguistics or programming.

1. let x be a bag-of-words vector such that(cid:80)v

j=1 xj = 1. verify that the multinomial
id203 pmult(x;   ), as de   ned in equation 2.12, is identical to the id203 of
the same document under a categorical distribution, pcat(w;   ).

2. suppose you have a single feature x, with the following conditional distribution:

p(x | y) =

  ,
x = 0, y = 0
1       , x = 1, y = 0
1       , x = 0, y = 1
x = 1, y = 1.
  ,

                                 

[2.80]

further suppose that the prior is uniform, pr(y = 0) = pr(y = 1) = 1
both    > 1
what is the id203 of making an error?

2, and that
2. given a na    ve bayes classi   er with accurate parameters,

2 and    > 1

3. derive the maximum-likelihood estimate for the parameter    in na    ve bayes.

4. the classi   cation models in the text have a vector of weights for each possible label.
while this is notationally convenient, it is overdetermined: for any linear classi   er
that can be obtained with k    v weights, an equivalent classi   er can be constructed
using (k     1)    v weights.
a) describe how to construct this classi   er. speci   cally, if given a set of weights
   and a feature function f (x, y), explain how to construct alternative weights
and feature function   (cid:48) and f(cid:48)(x, y), such that,

(cid:48)

   y, y

    y,       f (x, y)           f (x, y

(cid:48)

) =   

(cid:48)

(cid:48)

   f

(x, y)       

(cid:48)

).

(x, y

(cid:48)

(cid:48)

   f

[2.81]

b) explain how your construction justi   es the well-known alternative form for
   x), where   

binary id28, pr(y = 1 | x;   ) =
is the sigmoid function.

1+exp(     (cid:48)  x) =   (  (cid:48)

1

5. suppose you have two labeled datasets d1 and d2, with the same features and la-

bels.

    let   (1) be the unregularized id28 (lr) coef   cients from training

on dataset d1.

jacob eisenstein. draft of november 13, 2018.

2.8. summary of learning algorithms

45

    let   (2) be the unregularized lr coef   cients (same model) from training on
dataset d2.
    let       be the unregularized lr coef   cients from training on the combined
dataset d1     d2.

under these conditions, prove that for any feature j,

  

  

   
j     min(  (1)
   
j     max(  (1)

j

j

,   (2)
j )
,   (2)

j ).

the solution to the same problem, with l2 id173. prove that ||     

6. let      be the solution to an unregularized id28 problem, and let       be
2     ||     ||2
2.
||2
7. as noted in the discussion of averaged id88 in    2.3.2, the computation of the
running sum m     m +    is unnecessarily expensive, requiring k    v operations.
give an alternative way to compute the averaged weights   , with complexity that is
independent of v and linear in the sum of feature sizes(cid:80)n
distinct labels y(1) (cid:54)= y(2). assume all features are binary, xj     {0, 1} for all j.
now suppose that the averaged id88 always trains on the instance (xi(t), yi(t)),
where i(t) = 2     (t mod 2), which is 1 when the training iteration t is odd, and 2
when t is even. further suppose that learning terminates under the following con-
dition:

8. consider a dataset that is comprised of two identical instances x(1) = x(2) with

i=1 |f (x(i), y(i))|.

      max

j

1

t(cid:88)t

  (t)
j    

  (t   1)

j

1

t     1(cid:88)t

.

[2.82]

in words, the algorithm stops when the largest change in the averaged weights is
less than or equal to  . compute the number of iterations before the averaged per-
ceptron terminates.

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

9. prove that the margin loss is convex in   . use this de   nition of the margin loss:

l(  ) =          f (x, y

   

) + max

y

      f (x, y) + c(y

   

, y),

where y    is the gold label. as a reminder, a function f is convex iff,

f (  x1 + (1       )x2)       f (x1) + (1       )f (x2),

[2.83]

[2.84]

for any x1, x2 and        [0, 1].

under contract with mit press, shared under cc-by-nc-nd license.

46

chapter 2. linear text classification

10. if a function f is m-strongly convex, then for some m > 0, the following inequality

holds for all x and x(cid:48) on the domain of the function:

(cid:48)

f (x

)     f (x) + (   xf )    (x

(cid:48)

    x) +

(cid:48)

m
2 ||x

    x||2
2.

[2.85]

let f (x) = l(  (t)), representing the loss of the classi   er at iteration t of gradient
descent; let f (x(cid:48)) = l(  (t+1)). assuming the id168 is m-convex, prove that
l(  (t+1))     l(  (t)) for an appropriate constant learning rate   , which will depend
on m. explain why this implies that id119 converges when applied to an
m-strongly convex id168 with a unique minimum.

jacob eisenstein. draft of november 13, 2018.

chapter 3

nonlinear classi   cation

linear classi   cation may seem like all we need for natural language processing. the bag-
of-words representation is inherently high dimensional, and the number of features is
often larger than the number of labeled training instances. this means that it is usually
possible to    nd a linear classi   er that perfectly    ts the training data, or even to    t any ar-
bitrary labeling of the training instances! moving to nonlinear classi   cation may therefore
only increase the risk of over   tting. furthermore, for many tasks, lexical features (words)
are meaningful in isolation, and can offer independent evidence about the instance label
    unlike id161, where individual pixels are rarely informative, and must be
evaluated holistically to make sense of an image. for these reasons, natural language
processing has historically focused on linear classi   cation.

but in recent years, nonlinear classi   ers have swept through natural language pro-
cessing, and are now the default approach for many tasks (manning, 2015). there are at
least three reasons for this change.

    there have been rapid advances in deep learning, a family of nonlinear meth-
ods that learn complex functions of the input through multiple layers of compu-
tation (goodfellow et al., 2016).

    deep learning facilitates the incorporation of id27s, which are dense
vector representations of words. id27s can be learned from large amounts
of unlabeled data, and enable generalization to words that do not appear in the an-
notated training data (id27s are discussed in detail in chapter 14).

    while cpu speeds have plateaued, there have been rapid advances in specialized
hardware called graphics processing units (gpus), which have become faster, cheaper,
and easier to program. many deep learning models can be implemented ef   ciently
on gpus, offering substantial performance improvements over cpu-based comput-
ing.

47

48

chapter 3. nonlinear classification

this chapter focuses on neural networks, which are the dominant approach for non-
linear classi   cation in natural language processing today.1 historically, a few other non-
linear learning methods have been applied to language data.

    kernel methods are generalizations of the nearest-neighbor classi   cation rule, which
classi   es each instance by the label of the most similar example in the training set.
the application of the kernel support vector machine to information extraction is
described in chapter 17.

    id90 classify instances by checking a set of conditions. scaling decision
trees to bag-of-words inputs is dif   cult, but id90 have been successful in
problems such as coreference resolution (chapter 15), where more compact feature
sets can be constructed (soon et al., 2001).

    boosting and related ensemble methods work by combining the predictions of sev-
eral    weak    classi   ers, each of which may consider only a small subset of features.
boosting has been successfully applied to text classi   cation (schapire and singer,
2000) and syntactic analysis (abney et al., 1999), and remains one of the most suc-
cessful methods on machine learning competition sites such as kaggle (chen and
guestrin, 2016).

hastie et al. (2009) provide an excellent overview of these techniques.

3.1 feedforward neural networks

consider the problem of building a classi   er for movie reviews. the goal is to predict a
label y     {good, bad, okay} from a representation of the text of each document, x. but
what makes a good movie? the story, acting, cinematography, editing, soundtrack, and
so on. now suppose the training set contains labels for each of these additional features,
z = [z1, z2, . . . , zkz ](cid:62). with a training set of such information, we could build a two-step
classi   er:

1. use the text x to predict the features z. speci   cally, train a id28 clas-

si   er to compute p(zk | x), for each k     {1, 2, . . . , kz}.

2. use the features z to predict the label y. again, train a id28 classi   er
to compute p(y | z). on test data, z is unknown, so we will use the probabilities
p(z | x) from the    rst layer as the features.

this setup is shown in figure 3.1, which describes the proposed classi   er in a computa-
tion graph: the text features x are connected to the middle layer z, which is connected to
the label y.

1i will use    deep learning    and    neural networks    interchangeably.

jacob eisenstein. draft of november 13, 2018.

3.1. feedforward neural networks

49

figure 3.1: a feedforward neural network. shaded circles indicate observed features,
usually words; squares indicate nodes in the computation graph, which are computed
from the information carried over the incoming arrows.

if we assume that each zk is binary, zk     {0, 1}, then the id203 p(zk | x) can be

modeled using binary id28:

pr(zk = 1 | x;   (x   z)) =   (  (x   z)

[3.1]
where    is the sigmoid function (shown in figure 3.2), and the matrix   (x   z)     rkz  v is
constructed by stacking the weight vectors for each zk,

   x) = (1 + exp(     (x   z)

   x))

   1,

k

k

  (x   z) = [  (x   z)

1

,   (x   z)

2

, . . . ,   (x   z)

kz

(cid:62)

]

.

[3.2]

we will assume that x contains a term with a constant value of 1, so that a corresponding
offset parameter is included in each   (x   z)

.

k

the output layer is computed by the multi-class id28 id203,

pr(y = j | z;   (z   y), b) =

exp(  (z   y)

j

(cid:80)j(cid:48)   y exp(  (z   y)

j(cid:48)

   z + bj)

   z + bj(cid:48))

,

[3.3]

where bj is an offset for label j, and the output weight matrix   (z   y)     rky  kz is again
constructed by concatenation,

  (z   y) = [  (z   y)

1

,   (z   y)

2

, . . . ,   (z   y)

ky

(cid:62)
]

.

the vector of probabilities over each possible value of y is denoted,

p(y | z;   (z   y), b) = softmax(  (z   y)z + b),

[3.4]

[3.5]

where element j in the output of the softmax function is computed as in equation 3.3.

this set of equations de   nes a multilayer classi   er, which can be summarized as,

p(z | x;   (x   z)) =  (  (x   z)x)
p(y | z;   (z   y), b) = softmax(  (z   y)z + b),

[3.6]
[3.7]

under contract with mit press, shared under cc-by-nc-nd license.

......xzy50

chapter 3. nonlinear classification

figure 3.2: the sigmoid, tanh, and relu id180

where the function    is now applied elementwise to the vector of inner products,

  (  (x   z)x) = [  (  (x   z)

1

   x),   (  (x   z)

2

   x), . . . ,   (  (x   z)

kz

(cid:62)
   x)]

.

[3.8]

now suppose that the hidden features z are never observed, even in the training data.
we can still construct the architecture in figure 3.1. instead of predicting y from a discrete
vector of predicted values z, we use the probabilities   (  k    x). the resulting classi   er is
barely changed:

[3.9]
p(y | x;   (z   y), b) = softmax(  (z   y)z + b).
[3.10]
this de   nes a classi   cation model that predicts the label y     y from the base features x,
through a   hidden layer    z. this is a feedforward neural network.2

z =  (  (x   z)x)

3.2 designing neural networks

there several ways to generalize the feedforward neural network.

3.2.1 id180
if the hidden layer is viewed as a set of latent features, then the sigmoid function in equa-
tion 3.9 represents the extent to which each of these features is    activated    by a given
input. however, the hidden layer can be regarded more generally as a nonlinear trans-
formation of the input. this opens the door to many other id180, some of
which are shown in figure 3.2. at the moment, the choice of id180 is more
art than science, but a few points can be made about the most popular varieties:

2the architecture is sometimes called a multilayer id88, but this is misleading, because each layer

is not a id88 as de   ned in the previous chapter.

jacob eisenstein. draft of november 13, 2018.

321012310123valuessigmoidtanhrelu32101230.00.20.40.60.81.0derivatives3.2. designing neural networks

51

    the range of the sigmoid function is (0, 1). the bounded range ensures that a cas-
cade of sigmoid functions will not    blow up    to a huge output, and this is impor-
tant for deep networks with several hidden layers. the derivative of the sigmoid is
   a   (a) =   (a)(1       (a)). this derivative becomes small at the extremes, which can
   
make learning slow; this is called the vanishing gradient problem.
    the range of the tanh activation function is (   1, 1): like the sigmoid, the range
is bounded, but unlike the sigmoid, it includes negative values. the derivative is
   a tanh(a) = 1     tanh(a)2, which is steeper than the logistic function near the ori-
   
gin (lecun et al., 1998). the tanh function can also suffer from vanishing gradients
at extreme values.

    the recti   ed linear unit (relu) is zero for negative inputs, and linear for positive

inputs (glorot et al., 2011),

relu(a) =(cid:40)a, a     0

0, otherwise.

[3.11]

the derivative is a step function, which is 1 if the input is positive, and zero other-
wise. once the activation is zero, the gradient is also zero. this can lead to the prob-
lem of    dead neurons   , where some relu nodes are zero for all inputs, throughout
learning. a solution is the leaky relu, which has a small positive slope for negative
inputs (maas et al., 2013),

leaky-relu(a) =(cid:40)a,

a     0

.0001a, otherwise.

[3.12]

sigmoid and tanh are sometimes described as squashing functions, because they squash
an unbounded input into a bounded range. glorot and bengio (2010) recommend against
the use of the sigmoid activation in deep networks, because its mean value of 1
2 can cause
the next layer of the network to be saturated, leading to small gradients on its own pa-
rameters. several other id180 are reviewed in the textbook by goodfellow
et al. (2016), who recommend relu as the    default option.   

3.2.2 network structure
deep networks stack up several hidden layers, with each z(d) acting as the input to the
next layer, z(d+1). as the total number of nodes in the network increases, so does its
capacity to learn complex functions of the input. given a    xed number of nodes, one
must decide whether to emphasize width (large kz at each layer) or depth (many layers).
at present, this tradeoff is not well understood.3

3with even a single hidden layer, a neural network can approximate any continuous function on a closed
and bounded subset of rn to an arbitrarily small non-zero error; see section 6.4.1 of goodfellow et al. (2016)

under contract with mit press, shared under cc-by-nc-nd license.

52

chapter 3. nonlinear classification

it is also possible to    short circuit    a hidden layer, by propagating information directly
from the input to the next higher level of the network. this is the idea behind residual net-
works, which propagate information directly from the input to the subsequent layer (he
et al., 2016),

z = f (  (x   z)x) + x,

[3.13]

where f is any nonlinearity, such as sigmoid or relu. a more complex architecture is
the highway network (srivastava et al., 2015; kim et al., 2016), in which an addition gate
controls an interpolation between f (  (x   z)x) and x,

t =  (  (t)x + b(t))

z =t (cid:12) f (  (x   z)x) + (1     t) (cid:12) x,

[3.14]
[3.15]

where (cid:12) refers to an elementwise vector product, and 1 is a column vector of ones. as
before, the sigmoid function is applied elementwise to its input; recall that the output of
this function is restricted to the range (0, 1). gating is also used in the long short-term
memory (lstm), which is discussed in chapter 6. residual and highway connections
address a problem with deep architectures: repeated application of a nonlinear activation
function can make it dif   cult to learn the parameters of the lower levels of the network,
which are too distant from the supervision signal.

3.2.3 outputs and id168s

in the multi-class classi   cation example, a softmax output produces probabilities over
each possible label. this aligns with a negative conditional log-likelihood,

log p(y(i) | x(i);   ).
where    = {  (x   z),   (z   y), b} is the entire set of parameters.

   l =    

this loss can be written alternatively as follows:

n(cid:88)i=1

  yj (cid:44) pr(y = j | x(i);   )
   l =    
ey(i)    log   y

n(cid:88)i=1

[3.16]

[3.17]

[3.18]

for a survey of these theoretical results. however, depending on the function to be approximated, the width
of the hidden layer may need to be arbitrarily large. furthermore, the fact that a network has the capacity to
approximate any given function does not imply that it is possible to learn the function using gradient-based
optimization.

jacob eisenstein. draft of november 13, 2018.

3.3. learning neural networks

53

where ey(i) is a one-hot vector of zeros with a value of 1 at position y(i). the inner product
between ey(i) and log   y is also called the multinomial cross-id178, and this terminology
is preferred in many neural networks papers and software packages.

it is also possible to train neural networks from other objectives, such as a margin loss.
in this case, it is not necessary to use softmax at the output layer: an af   ne transformation
of the hidden layer is enough:

[3.19]

  (y; x(i),   ) =  (z   y)
(cid:96)margin(  ; x(i), y(i)) = max

y

   z + by

.

[3.20]

y(cid:54)=y(i)(cid:16)1 +   (y; x(i),   )       (y(i); x(i),   )(cid:17)+
in regression problems, the output is a scalar or vector (see    4.1.2). for these problems, a
typical id168 is the squared error (y       y)2 or squared norm ||y       y||2
2.
3.2.4
in text classi   cation, the input layer x can refer to a bag-of-words vector, where xj is
the count of word j. the input to the hidden unit zk is then(cid:80)v
xj, and word j is
represented by the vector   (x   z)
. this vector is sometimes described as the embedding of
word j, and can be learned from unlabeled data, using techniques discussed in chapter 14.
the columns of   (x   z) are each kz-dimensional id27s.

inputs and lookup layers

j=1   (x   z)

j,k

j

chapter 2 presented an alternative view of text documents, as a sequence of word
tokens, w1, w2, . . . , wm . in a neural network, each word token wm is represented with a
one-hot vector, ewm, with dimension v . the matrix-vector product   (x   z)ewm returns
the embedding of word wm. the complete document can represented by horizontally
concatenating these one-hot vectors, w = [ew1, ew2, . . . , ewm ], and the bag-of-words rep-
resentation can be recovered from the matrix-vector product w[1, 1, . . . , 1](cid:62), which sums
each row over the tokens m = {1, 2, . . . , m}. the matrix product   (x   z)w contains the
horizontally concatenated embeddings of each word in the document, which will be use-
ful as the starting point for convolutional neural networks (see    3.4). this is sometimes
called a lookup layer, because the    rst step is to lookup the embeddings for each word in
the input text.

3.3 learning neural networks

the feedforward network in figure 3.1 can now be written as,

z    f (  (x   z)x(i))
  y     softmax(cid:16)  (z   y)z + b(cid:17)
(cid:96)(i)         ey(i)    log   y,

[3.21]

[3.22]

[3.23]

under contract with mit press, shared under cc-by-nc-nd license.

54

chapter 3. nonlinear classification

where f is an elementwise activation function, such as    or relu, and (cid:96)(i) is the loss at
instance i. the parameters   (x   z),   (z   y), and b can be estimated using online gradient-
based optimization. the simplest such algorithm is stochastic id119, which
was discussed in    2.6. each parameter is updated by the gradient of the loss,

k

  (z   y)
  (x   z)

n

b    b       (t)   b(cid:96)(i)
     (z   y)
     (x   z)

      (t)     (z   y)
      (t)     (x   z)

n

k

n

k

(cid:96)(i)

(cid:96)(i),

[3.24]
[3.25]

[3.26]

where   (t) is the learning rate on iteration t, (cid:96)(i) is the loss on instance (or minibatch) i,
and   (x   z)

is column n of the matrix   (x   z), and   (z   y)

is column k of   (z   y).

n

k

the gradients of the negative log-likelihood on b and   (z   y)

ents in id28. for   (z   y), the gradient is,

k

are similar to the gradi-

     (z   y)

k

   (cid:96)(i)
     (z   y)

k,j

(cid:62)

,

k,2

k,1

, . . . ,

      

     (z   y)

   (cid:96)(i)
     (z   y)

(cid:96)(i) =          (cid:96)(i)
   z      
=(cid:16)pr(y = j | z;   (z   y), b)       (cid:16)j = y(i)(cid:17)(cid:17) zk,

   (cid:96)(i)
     (z   y)
   z     log(cid:88)y   y

        (z   y)

exp   (z   y)

     (z   y)

=    

k,ky

y(i)

k,j

   

y

where   (cid:0)j = y(i)(cid:1) is a function that returns one when j = y(i), and zero otherwise. the
gradient    b(cid:96)(i) is similar to equation 3.29.

the gradients on the input layer weights   (x   z) are obtained by the chain rule of

differentiation:

   (cid:96)(i)
     (x   z)

n,k

=

   (cid:96)(i)
   zk

=

   (cid:96)(i)
   zk

n,k

   zk
     (x   z)
   f (  (x   z)
     (x   z)
(  (x   z)
(cid:48)

n,k

k

k

   x)

=

   (cid:96)(i)
   zk    f

   x)    xn,

[3.27]

[3.28]

[3.29]

[3.30]

[3.31]

[3.32]

where f(cid:48)(  (x   z)

k

   x) is the derivative of the activation function f, applied at the input
jacob eisenstein. draft of november 13, 2018.

3.3. learning neural networks

  (x   z)

k

   x. for example, if f is the sigmoid function, then the derivative is,

   (cid:96)(i)
     (x   z)

n,k

=

   (cid:96)(i)

   zk      (  (x   z)

k

   x)    (1       (  (x   z)

k

   x))    xn

=

   (cid:96)(i)
   zk    zk    (1     zk)    xn.

for intuition, consider each of the terms in the product.

55

[3.33]

[3.34]

    if the negative log-likelihood (cid:96)(i) does not depend much on zk, then    (cid:96)(i)
case it doesn   t matter how zk is computed, and so    (cid:96)(i)
n,k     0.
     (x   z)

   zk     0. in this

    if zk is near 1 or 0, then the curve of the sigmoid function is nearly    at (figure 3.2),
and changing the inputs will make little local difference. the term zk    (1     zk) is
maximized at zk = 1
    if xn = 0, then it does not matter how we set the weights   (x   z)

2, where the slope of the sigmoid function is steepest.
, so    (cid:96)(i)
     (x   z)

= 0.

n,k

n,k

is reused in computing the derivatives with respect to each   (x   z)

3.3.1 id26
the equations above rely on the chain rule to compute derivatives of the loss with respect
to each parameter of the model. furthermore, local derivatives are frequently reused: for
example,    (cid:96)(i)
. these
   zk
terms should therefore be computed once, and then cached. furthermore, we should only
compute any derivative once we have already computed all of the necessary    inputs   
demanded by the chain rule of differentiation. this combination of sequencing, caching,
and differentiation is known as id26. it can be generalized to any directed
acyclic computation graph.

n,k

a computation graph is a declarative representation of a computational process. at
each node t, compute a value vt by applying a function ft to a (possibly empty) list of
parent nodes,   t. figure 3.3 shows the computation graph for a feedforward network
with one hidden layer. there are nodes for the input x(i), the hidden layer z, the predicted
output   y, and the parameters   . during training, there is also a node for the ground truth
label y(i) and the loss (cid:96)(i). the predicted output   y is one of the parents of the loss (the
other is the label y(i)); its parents include    and z, and so on.

computation graphs include three types of nodes:

variables. in the feedforward network of figure 3.3, the variables include the inputs x,
the hidden nodes z, the outputs y, and the id168. inputs are variables that
do not have parents. id26 computes the gradients with respect to all

under contract with mit press, shared under cc-by-nc-nd license.

56

chapter 3. nonlinear classification

algorithm 6 general id26 algorithm.
in the computation graph g, every
node contains a function ft and a set of parent nodes   t; the inputs to the graph are x(i).
1: procedure backprop(g = {ft,   t}t
vt(n)     x(i)
2:
for t     topologicalsort(g) do
3:
4:
5:
vt     ft(v  t,1, v  t,2, . . . , v  t,nt

n for all n and associated computation nodes t(n).

(cid:46) forward pass: compute value at each node

if |  t| > 0 then

t=1}, x(i))

)

6:
7:
8:

9:

gobjective = 1
for t     reverse(topologicalsort(g)) do
gt    (cid:80)t(cid:48):t     t(cid:48)
the gradient gt(cid:48), scaled by the local gradient    vtvt(cid:48)
return {g1, g2, . . . , gt}

gt(cid:48)       vtvt(cid:48)

(cid:46) backward pass: compute gradients at each node
(cid:46) sum over all t(cid:48) that are children of t, propagating

variables except the inputs, and propagates these gradients backwards to the pa-
rameters.

parameters. in a feedforward network, the parameters include the weights and offsets.
in figure 3.3, the parameters are summarized in the node   , but we could have
separate nodes for   (x   z),   (z   y), and any offset parameters. parameter nodes do
not have parents; they are not computed from other nodes, but rather, are learned
by id119.

loss. the loss (cid:96)(i) is the quantity that is to be minimized during training. the node rep-
resenting the loss in the computation graph is not the parent of any other node; its
parents are typically the predicted label   y and the true label y(i). id26
begins by computing the gradient of the loss, and then propagating this gradient
backwards to its immediate parents.

if the computation graph is a directed acyclic graph, then it is possible to order the
nodes with a topological sort, so that if node t is a parent of node t(cid:48), then t < t(cid:48). this
means that the values {vt}t
t=1 can be computed in a single forward pass. the topolog-
ical sort is reversed when computing gradients: each gradient gt is computed from the
gradients of the children of t, implementing the chain rule of differentiation. the general
id26 algorithm for computation graphs is shown in algorithm 6.

while the gradients with respect to each parameter may be complex, they are com-
posed of products of simple parts. for many networks, all gradients can be computed
through automatic differentiation. this means that you need only specify the feedfor-
ward computation, and the gradients necessary for learning can be obtained automati-
cally. there are many software libraries that perform automatic differentiation on compu-

jacob eisenstein. draft of november 13, 2018.

3.3. learning neural networks

57

figure 3.3: a computation graph for the feedforward neural network shown in figure 3.1.

tation graphs, such as torch (collobert et al., 2011), tensorflow (abadi et al., 2016),
and dynet (neubig et al., 2017). one important distinction between these libraries is
whether they support dynamic computation graphs, in which the structure of the compu-
tation graph varies across instances. static computation graphs are compiled in advance,
and can be applied to    xed-dimensional data, such as bag-of-words vectors. in many nat-
ural language processing problems, each input has a distinct structure, requiring a unique
computation graph. a simple case occurs in recurrent neural network language models
(see chapter 6), in which there is one node for each word in a sentence. more complex
cases include id56s (see chapter 14), in which the network is a tree
structure matching the syntactic organization of the input.

3.3.2 id173 and dropout

in linear classi   cation, over   tting was addressed by augmenting the objective with a reg-
2. this same approach can be applied to feedforward neural net-
ularization term,   ||  ||2
works, penalizing each matrix of weights:

l =

(cid:96)(i) +   z   y||  (z   y)||2

f +   x   z||  (x   z)||2

f ,

[3.35]

n(cid:88)i=1

f =(cid:80)i,j   2

i,j is the squared frobenius norm, which generalizes the l2 norm
where ||  ||2
to matrices. the bias parameters b are not regularized, as they do not contribute to the
sensitivity of the classi   er to the inputs.
in gradient-based optimization, the practical
effect of frobenius norm id173 is that the weights    decay    towards zero at each
update, motivating the alternative name weight decay.

another approach to controlling model complexity is dropout, which involves ran-
domly setting some computation nodes to zero during training (srivastava et al., 2014).
for example, in the feedforward network, on each training instance, with id203    we

under contract with mit press, shared under cc-by-nc-nd license.

x(i)z  y   (i)y(i)  vxvzv  yv  g  yg   gzgzvyv  g   g  y58

chapter 3. nonlinear classification

set each input xn and each hidden layer node zk to zero. srivastava et al. (2014) recom-
mend    = 0.5 for hidden units, and    = 0.2 for input units. dropout is also incorporated
in the gradient computation, so if node zk is dropped, then none of the weights   (x   z)
will
be updated for this instance. dropout prevents the network from learning to depend too
much on any one feature or hidden node, and prevents feature co-adaptation, in which a
hidden unit is only useful in combination with one or more other hidden units. dropout is
a special case of feature noising, which can also involve adding gaussian noise to inputs
or hidden units (holmstrom and koistinen, 1992). wager et al. (2013) show that dropout is
approximately equivalent to    adaptive    l2 id173, with a separate id173
penalty for each feature.

k

3.3.3

*learning theory

chapter 2 emphasized the importance of convexity for learning: for convex objectives,
the global optimum can be found ef   ciently. the negative log-likelihood and hinge loss
are convex functions of the parameters of the output layer. however, the output of a feed-
forward network is generally not a convex function of the parameters of the input layer,
  (x   z). feedforward networks can be viewed as function composition, where each layer
is a function that is applied to the output of the previous layer. convexity is generally not
preserved in the composition of two convex functions     and furthermore,    squashing   
id180 like tanh and sigmoid are not convex.

the non-convexity of hidden layer neural networks can also be seen by permuting the
elements of the hidden layer, from z = [z1, z2, . . . , zkz ] to   z = [z  (1), z  (2), . . . , z  (kz)]. this
corresponds to applying    to the rows of   (x   z) and the columns of   (z   y), resulting in
permuted parameter matrices   (x   z)
. as long as this permutation is applied
consistently, the loss will be identical, l(  ) = l(    ): it is invariant to this permutation.
however, the loss of the linear combination l(     + (1       )    ) will generally not be
identical to the loss under    or its permutations. if l(  ) is better than the loss at any
points in the immediate vicinity, and if l(  ) = l(    ), then the id168 does not
satisfy the de   nition of convexity (see    2.4). one of the exercises asks you to prove this
more rigorously.

and   (z   y)

  

  

in practice, the existence of multiple optima is not necessary problematic, if all such
optima are permutations of the sort described in the previous paragraph.
in contrast,
   bad    local optima are better than their neighbors, but much worse than the global op-
timum. fortunately, in large feedforward neural networks, most local optima are nearly
as good as the global optimum (choromanska et al., 2015). more generally, a critical
point is one at which the gradient is zero. critical points may be local optima, but they
may also be saddle points, which are local minima in some directions, but local maxima
in other directions. for example, the equation x2
2 has a saddle point at x = (0, 0).
in large networks, the overwhelming majority of critical points are saddle points, rather

1     x2

jacob eisenstein. draft of november 13, 2018.

3.3. learning neural networks

59

than local minima or maxima (dauphin et al., 2014). saddle points can pose problems
for gradient-based optimization, since learning will slow to a crawl as the gradient goes
to zero. however, the noise introduced by stochastic id119, and by feature
noising techniques such as dropout, can help online optimization to escape saddle points
and    nd high-quality optima (ge et al., 2015). other techniques address saddle points
directly, using local reconstructions of the hessian matrix (dauphin et al., 2014) or higher-
order derivatives (anandkumar and ge, 2016).

another theoretical puzzle about neural networks is how they are able to generalize
to unseen data. given enough parameters, a two-layer feedforward network can    mem-
orize    its training data, attaining perfect accuracy on any training set. a particularly
salient demonstration was provided by zhang et al. (2017), who showed that neural net-
works can learn to perfectly classify a training set of images, even when the labels are
replaced with random values! of course, this network attains only chance accuracy when
applied to heldout data. the concern is that when such a powerful learner is applied to
real training data, it may learn a pathological classi   cation function, which exploits irrel-
evant details of the training data and fails to generalize. yet this extreme over   tting is
rarely encountered in practice, and can usually be prevented by id173, dropout,
and early stopping (see    3.3.4). recent papers have derived generalization guarantees for
speci   c classes of neural networks (e.g., kawaguchi et al., 2017; brutzkus et al., 2018), but
theoretical work in this area is ongoing.

3.3.4 tricks

getting neural networks to work sometimes requires heuristic    tricks    (bottou, 2012;
goodfellow et al., 2016; goldberg, 2017b). this section presents some tricks that are espe-
cially important.

initialization initialization is not especially important for linear classi   ers, since con-
vexity ensures that the global optimum can usually be found quickly. but for multilayer
neural networks, it is helpful to have a good starting point. one reason is that if the mag-
nitude of the initial weights is too large, a sigmoid or tanh nonlinearity will be saturated,
leading to a small gradient, and slow learning. large gradients can cause training to di-
verge, with the parameters taking increasingly extreme values until reaching the limits of
the    oating point representation.

initialization can help avoid these problems by ensuring that the variance over the
initial gradients is constant and bounded throughout the network. for networks with
tanh id180, this can be achieved by sampling the initial weights from the

under contract with mit press, shared under cc-by-nc-nd license.

60

chapter 3. nonlinear classification

following uniform distribution (glorot and bengio, 2010),

  i,j    u(cid:34)   

   6

(cid:112)din(n) + dout(n)

,

   6

(cid:112)din(n) + dout(n)(cid:35) ,

[3.36]

[3.37]

for the weights leading to a relu activation function, he et al. (2015) use similar argu-
mentation to justify sampling from a zero-mean gaussian distribution,

  i,j     n (0,(cid:112)2/din(n))

[3.38]

rather than initializing the weights independently, it can be bene   cial to initialize each
layer jointly as an orthonormal matrix, ensuring that   (cid:62)   = i (saxe et al., 2014). or-
thonormal matrices preserve the norm of the input, so that ||  x|| = ||x||, which prevents
the gradients from exploding or vanishing. orthogonality ensures that the hidden units
are uncorrelated, so that they correspond to different features of the input. orthonormal
initialization can be performed by applying singular value decomposition to a matrix of
values sampled from a standard normal distribution:

ai,j    n (0, 1)
a ={ai,j}din(j),dout(j)
(cid:62)

i=1,j=1

=svd(a)

u, s, v

  (j)    u.

[3.39]
[3.40]
[3.41]
[3.42]

the matrix u contains the singular vectors of a, and is guaranteed to be orthonormal.
for more on singular value decomposition, see chapter 14.

even with careful initialization, there can still be signi   cant variance in the    nal re-
sults. it can be useful to make multiple training runs, and select the one with the best
performance on a heldout development set.

clipping and id172 learning can be sensitive to the magnitude of the gradient:
too large, and learning can diverge, with successive updates thrashing between increas-
ingly extreme values; too small, and learning can grind to a halt. several heuristics have
been proposed to address this issue.

    in gradient clipping (pascanu et al., 2013), an upper limit is placed on the norm of

the gradient, and the gradient is rescaled when this limit is exceeded,

clip(  g) =(cid:40)g

||  g|| <   

  ||g|| g otherwise.

[3.43]

jacob eisenstein. draft of november 13, 2018.

3.3. learning neural networks

61

    in batch id172 (ioffe and szegedy, 2015), the inputs to each computation
node are recentered by their mean and variance across all of the instances in the
minibatch b (see    2.6.2). for example, in a feedforward network with one hidden
layer, batch id172 would tranform the inputs to the hidden layer as follows:

  (b) =

x(i)

1

|b|(cid:88)i   b
|b|(cid:88)i   b

1

s(b) =

(x(i)       (b))2
x(i) =(x(i)       (b))/(cid:112)s(b).

[3.44]

[3.45]

[3.46]

empirically, this speeds convergence of deep architectures. one explanation is that
it helps to correct for changes in the distribution of activations during training.

    in layer id172 (ba et al., 2016), the inputs to each nonlinear activation func-

tion are recentered across the layer:

a =  (x   z)x

   =

1
kz

s =

1
kz

kz(cid:88)k=1
kz(cid:88)k=1

z =(a       )/   s.

ak

(ak       )2

[3.47]

[3.48]

[3.49]

[3.50]

layer id172 has similar motivations to batch id172, but it can be
applied across a wider range of architectures and training conditions.

online optimization there is a cottage industry of online optimization algorithms that
attempt to improve on stochastic id119. adagrad was reviewed in    2.6.2; its
main innovation is to set adaptive learning rates for each parameter by storing the sum
of squared gradients. rather than using the sum over the entire training history, we can
keep a running estimate,

j =  v(t   1)
v(t)

j

+ (1       )g2
t,j,

[3.51]
where gt,j is the gradient with respect to parameter j at time t, and        [0, 1]. this term
places more emphasis on recent gradients, and is employed in the adadelta (zeiler, 2012)
and adam (kingma and ba, 2014) optimizers. online optimization and its theoretical
background are reviewed by bottou et al. (2016). early stopping, mentioned in    2.3.2,
can help to avoid over   tting by terminating training after reaching a plateau in the per-
formance on a heldout validation set.

under contract with mit press, shared under cc-by-nc-nd license.

62

chapter 3. nonlinear classification

practical advice the bag of tricks for training neural networks continues to grow, and
it is likely that there will be several new ones by the time you read this. today, it is
standard practice to use gradient clipping, early stopping, and a sensible initialization of
parameters to small random values. more bells and whistles can be added as solutions to
speci   c problems     for example, if it is dif   cult to    nd a good learning rate for stochastic
id119, then it may help to try a fancier optimizer with an adaptive learning
rate. alternatively, if a method such as layer id172 is used by related models
in the research literature, you should probably consider it, especially if you are having
trouble matching published results. as with linear classi   ers, it is important to evaluate
these decisions on a held-out development set, and not on the test set that will be used to
provide the    nal measure of the model   s performance (see    2.2.5).
3.4 convolutional neural networks

a basic weakness of the bag-of-words model is its inability to account for the ways in
which words combine to create meaning, including even simple reversals such as not
pleasant, hardly a generous offer, and i wouldn   t mind missing the    ight. id161
faces the related challenge of identifying the semantics of images from pixel features
that are uninformative in isolation. an earlier generation of id161 research
focused on designing    lters to aggregate local pixel-level features into more meaningful
representations, such as edges and corners (e.g., canny, 1987). similarly, earlier nlp re-
search attempted to capture multiword linguistic phenomena by hand-designed lexical
patterns (hobbs et al., 1997). in both cases, the output of the    lters and patterns could
then act as base features in a linear classi   er. but rather than designing these feature ex-
tractors by hand, a better approach is to learn them, using the magic of id26.
this is the idea behind convolutional neural networks.

following    3.2.4, de   ne the base layer of a neural network as,

x(0) =   (x   z)[ew1, ew2, . . . , ewm ],

[3.52]

where ewm is a column vector of zeros, with a 1 at position wm. the base layer has dimen-
sion x(0)     rke  m , where ke is the size of the id27s. to merge information
across adjacent words, we convolve x(0) with a set of    lter matrices c(k)     rke  h. convo-
lution is indicated by the symbol    , and is de   ned,
k,m = f(cid:32)bk +

x(1) =f (b + c     x(0)) =    x(1)

k(cid:48),m+n   1(cid:33) ,

k(cid:48),n    x(0)
c(k)

[3.53]

ke(cid:88)k(cid:48)=1

h(cid:88)n=1

where f is an activation function such as tanh or relu, and b is a vector of offsets. the
convolution operation slides the matrix c(k) across the columns of x(0). at each position
m, we compute the elementwise product c(k) (cid:12) x(0)

m:m+h   1, and take the sum.

jacob eisenstein. draft of november 13, 2018.

3.4. convolutional neural networks

63

figure 3.4: a convolutional neural network for text classi   cation

a simple    lter might compute a weighted average over nearby words,

c(k) =            

0.5
0.5
. . .
0.5

1
1
. . .
1

0.5
0.5
. . .
0.5

             ,

[3.54]

thereby representing trigram units like not so unpleasant. in one-dimensional convolu-
tion, each    lter matrix c(k) is constrained to have non-zero values only at row k (kalch-
brenner et al., 2014). this means that each dimension of the id27 is processed
by a separate    lter, and it implies that kf = ke.

to deal with the beginning and end of the input, the base matrix x(0) may be padded
with h column vectors of zeros at the beginning and end; this is known as wide convolu-
tion. if padding is not applied, then the output from each layer will be h    1 units smaller
than the input; this is known as narrow convolution. the    lter matrices need not have
identical    lter widths, so more generally we could write hk to indicate to width of    lter
c(k). as suggested by the notation x(0), multiple layers of convolution may be applied,
so that x(d) is the input to x(d+1).

after d convolutional layers, we obtain a matrix representation of the document x(d)    
rkz  m . if the instances have variable lengths, it is necessary to aggregate over all m word
positions to obtain a    xed-length representation. this can be done by a pooling operation,

under contract with mit press, shared under cc-by-nc-nd license.

  x(0)cc*x(1)zconvolutionpoolingpredictionmkekfkf64

chapter 3. nonlinear classification

figure 3.5: a dilated convolutional neural network captures progressively larger context
through recursive application of the convolutional operator

such as max-pooling (collobert et al., 2011) or average-pooling,

z = maxpool(x(d)) =    zk = max(cid:16)x(d)
m(cid:88)m=1
z = avgpool(x(d)) =    zk =

1
m

x(d)
k,m.

k,1 , x(d)

k,2 , . . . x(d)

k,m(cid:17)

[3.55]

[3.56]

the vector z can now act as a layer in a feedforward network, culminating in a prediction
  y and a loss (cid:96)(i). the setup is shown in figure 3.4.

just as in feedforward networks, the parameters (c(k), b,   ) can be learned by back-
propagating from the classi   cation loss. this requires backpropagating through the max-
pooling operation, which is a discontinuous function of the input. but because we need
only a local gradient, id26    ows only through the argmax m:

=(cid:40)1, x(d)

k,m = max(cid:16)x(d)

0, otherwise.

   zk
   x(d)
k,m

k,1 , x(d)

k,2 , . . . x(d)

k,m(cid:17)

[3.57]

the id161 literature has produced a huge variety of convolutional archi-
tectures, and many of these innovations can be applied to text data. one avenue for
improvement is more complex pooling operations, such as k-max pooling (kalchbrenner
et al., 2014), which returns a matrix of the k largest values for each    lter. another innova-
tion is the use of dilated convolution to build multiscale representations (yu and koltun,
2016). at each layer, the convolutional operator applied in strides, skipping ahead by s
steps after each feature. as we move up the hierarchy, each layer is s times smaller than
the layer below it, effectively summarizing the input (kalchbrenner et al., 2016; strubell
et al., 2017). this idea is shown in figure 3.5. multi-layer convolutional networks can also
be augmented with    shortcut    connections, as in the residual network from    3.2.2 (john-
son and zhang, 2017).

jacob eisenstein. draft of november 13, 2018.

3.4. convolutional neural networks

65

additional resources

the deep learning textbook by goodfellow et al. (2016) covers many of the topics in this
chapter in more detail. for a comprehensive review of neural networks in natural lan-
guage processing, see goldberg (2017b). a seminal work on deep learning in natural
language processing is the aggressively titled    natural language processing (almost)
from scratch   , which uses convolutional neural networks to perform a range of language
processing tasks (collobert et al., 2011), although there is earlier work (e.g., henderson,
2004). this chapter focuses on feedforward and convolutional neural networks, but recur-
rent neural networks are one of the most important deep learning architectures for natural
language processing. they are covered extensively in chapters 6 and 7.

the role of deep learning in natural language processing research has caused angst
in some parts of the natural language processing research community (e.g., goldberg,
2017a), especially as some of the more zealous deep learning advocates have argued that
end-to-end learning from    raw    text can eliminate the need for linguistic constructs such
as sentences, phrases, and even words (zhang et al., 2015, originally titled    text under-
standing from scratch   ). these developments were surveyed by manning (2015). while
reports of the demise of linguistics in natural language processing remain controversial
at best, deep learning and id26 have become ubiquitous in both research and
applications.

exercises

1. figure 3.3 shows the computation graph for a feedforward neural network with one

layer.

a) update the computation graph to include a residual connection between x and

z.

b) update the computation graph to include a highway connection between x

and z.

2. prove that the softmax and sigmoid functions are equivalent when the number of
possible labels is two. speci   cally, for any   (z   y) (omitting the offset b for simplic-
ity), show how to construct a vector of weights    such that,

softmax(  (z   y)z)[0] =   (      z).

[3.58]

3. convolutional neural networks often aggregate across words by using max-pooling
(equation 3.55 in    3.4). a potential concern is that there is zero gradient with re-
spect to the parts of the input that are not included in the maximum. the following

under contract with mit press, shared under cc-by-nc-nd license.

66

chapter 3. nonlinear classification

questions consider the gradient with respect to an element of the input, x(0)
they assume that all parameters are independently distributed.

m,k, and

a) first consider a minimal network, with z = maxpool(x(0)). what is the prob-

ability that the gradient

   (cid:96)
   x(0)
m,k

is non-zero?

b) now consider a two-level network, with x(1) = f (b + c     x(0)). express the
is non-zero, in terms of the input length m,

id203 that the gradient
the    lter size n, and the number of    lters kf .

   (cid:96)
   x(0)
m,k

c) using a calculator, work out the id203 for the case m = 128, n = 4, kf =

32.

d) now consider a three-level network, x(2) = f (b + c     x(1)). give the general
is non-zero, and compute the numerical
equation for the id203 that
id203 for the scenario in the previous part, assuming kf = 32 and n = 4
at both levels.

   (cid:96)
   x(0)
m,k

4. design a feedforward network to compute the xor function:

f (x1, x2) =

.

[3.59]

   1, x1 = 1, x2 = 1
x1 = 1, x2 = 0
1,
1,
x1 = 0, x2 = 1
   1, x1 = 0, x2 = 0

                                 

your network should have a single output node which uses the sign activation func-

. use a single hidden layer, with relu activation func-

tion, f (x) = (cid:40)1,

x > 0
   1, x     0.

tions. describe all weights and offsets.

5. consider the same network as above (with relu activations for the hidden layer),
with an arbitrary differentiable id168 (cid:96)(y(i),   y), where   y is the activation of
the output node. suppose all weights and offsets are initialized to zero. show that
id119 will not learn the desired function from this initialization.

6. the simplest solution to the previous problem relies on the use of the relu activa-
tion function at the hidden layer. now consider a network with arbitrary activations
on the hidden layer. show that if the initial weights are any uniform constant, then
id119 will not learn the desired function from this initialization.

7. consider a network in which: the base features are all binary, x     {0, 1}m ; the
hidden layer activation function is sigmoid, zk =   (  k    x); and the initial weights
are sampled independently from a standard normal distribution,   j,k     n (0, 1).

jacob eisenstein. draft of november 13, 2018.

3.4. convolutional neural networks

    show how the id203 of a small initial gradient on any weight,    zk

depends on the size of the input m. hint: use the lower bound,

     j,k

pr(  (  k    x)    (1       (  k    x)) <   )     2 pr(  (  k    x) <   ),

and relate this id203 to the variance v [  k    x].

    design an alternative initialization that removes this dependence.

67

<   ,

[3.60]

8. the relu activation function can lead to    dead neurons   , which can never be acti-
vated on any input. consider the following two-layer feedforward network with a
scalar output y:

zi =relu(  (x   z)
y =  (z   y)    z.

i

   x + bi)

[3.61]
[3.62]

suppose that the input is a binary vector of observations, x     {0, 1}d.
a) under what condition is node zi    dead   ? your answer should be expressed in

terms of the parameters   (x   z)

i

and bi.

b) suppose that the gradient of the loss on a given instance is    (cid:96)

   y = 1. derive the

gradients    (cid:96)
   bi

and

   (cid:96)

     (x   z)

j,i

for such an instance.

c) using your answers to the previous two parts, explain why a dead neuron can

never be brought back to life during gradient-based learning.

9. suppose that the parameters    = {  (x   z),   (z     y), b} are a local optimum of a

feedforward network in the following sense: there exists some   > 0 such that,

(cid:16)||     (x   z)       (x   z)||2
   (cid:16)l(     ) > l(  )(cid:17)

f + ||     (z   y)       (z   y)||2

f + ||  b     b||2

2 <  (cid:17)

[3.63]

de   ne the function    as a permutation on the hidden units, as described in    3.3.3,
so that for any   , l(  ) = l(    ). prove that if a feedforward network has a local
optimum in the sense of equation 3.63, then its loss is not a convex function of the
parameters   , using the de   nition of convexity from    2.4

10. consider a network with a single hidden layer, and a single output,

y =   (z   y)    g(  (x   z)x).

[3.64]
assume that g is the relu function. show that for any matrix of weights   (x   z), it
is permissible to rescale each row to have a norm of one, because an identical output
can be obtained by    nding a corresponding rescaling of   (z   y).

under contract with mit press, shared under cc-by-nc-nd license.

chapter 4

linguistic applications of
classi   cation

having covered several techniques for classi   cation, this chapter shifts the focus from
mathematics to linguistic applications. later in the chapter, we will consider the design
decisions involved in text classi   cation, as well as best practices for evaluation.

4.1 sentiment and opinion analysis

a popular application of text classi   cation is to automatically determine the sentiment
or opinion polarity of documents such as product reviews and social media posts. for
example, marketers are interested to know how people respond to advertisements, ser-
vices, and products (hu and liu, 2004); social scientists are interested in how emotions
are affected by phenomena such as the weather (hannak et al., 2012), and how both opin-
ions and emotions spread over social networks (coviello et al., 2014; miller et al., 2011).
in the    eld of digital humanities, literary scholars track plot structures through the    ow
of sentiment across a novel (jockers, 2015).1

id31 can be framed as a direct application of document classi   cation,
assuming reliable labels can be obtained.
in the simplest case, id31 is a
two or three-class problem, with sentiments of positive, negative, and possibly neu-
tral. such annotations could be annotated by hand, or obtained automatically through
a variety of means:

    tweets containing happy emoticons can be marked as positive, sad emoticons as

negative (read, 2005; pak and paroubek, 2010).

1comprehensive surveys on id31 and related problems are offered by pang and lee (2008)

and liu (2015).

69

70

chapter 4. linguistic applications of classification

negative (pang et al., 2002).

    reviews with four or more stars can be marked as positive, three or fewer stars as
    statements from politicians who are voting for a given bill are marked as positive
(towards that bill); statements from politicians voting against the bill are marked as
negative (thomas et al., 2006).

the bag-of-words model is a good    t for id31 at the document level: if
the document is long enough, we would expect the words associated with its true senti-
ment to overwhelm the others. indeed, lexicon-based id31 avoids machine
learning altogether, and classi   es documents by counting words against positive and neg-
ative sentiment word lists (taboada et al., 2011).

lexicon-based classi   cation is less effective for short documents, such as single-sentence

reviews or social media posts. in these documents, linguistic issues like negation and ir-
realis (polanyi and zaenen, 2006)     events that are hypothetical or otherwise non-factual
    can make bag-of-words classi   cation ineffective. consider the following examples:

(4.1)

a. that   s not bad for the    rst day.
b. this is not the worst thing that can happen.
c.
d. there is no reason at all to believe that the polluters are suddenly going to

it would be nice if you acted like you understood.

become reasonable. (wilson et al., 2005)

e. this    lm should be brilliant. the actors are    rst grade. stallone plays a
happy, wonderful man. his sweet wife is beautiful and adores him. he has
a fascinating gift for living life fully. it sounds like a great plot, however, the
   lm is a failure. (pang et al., 2002)

a minimal solution is to move from a bag-of-words model to a bag-of-bigrams model,

where each base feature is a pair of adjacent words, e.g.,

(that   s, not), (not, bad), (bad, for), . . .

[4.1]
bigrams can handle relatively straightforward cases, such as when an adjective is immedi-
ately negated; trigrams would be required to extend to larger contexts (e.g., not the worst).
but this approach will not scale to more complex examples like (4.1d) and (4.1e). more
sophisticated solutions try to account for the syntactic structure of the sentence (wilson
et al., 2005; socher et al., 2013), or apply more complex classi   ers such as convolutional
neural networks (kim, 2014), which are described in chapter 3.

4.1.1 related problems
subjectivity closely related to id31 is subjectivity detection, which re-
quires identifying the parts of a text that express subjective opinions, as well as other non-

jacob eisenstein. draft of november 13, 2018.

4.1. sentiment and opinion analysis

71

factual content such as speculation and hypotheticals (riloff and wiebe, 2003). this can be
done by treating each sentence as a separate document, and then applying a bag-of-words
classi   er: indeed, pang and lee (2004) do exactly this, using a training set consisting of
(mostly) subjective sentences gathered from movie reviews, and (mostly) objective sen-
tences gathered from plot descriptions. they augment this bag-of-words model with a
graph-based algorithm that encourages nearby sentences to have the same subjectivity
label.

stance classi   cation in debates, each participant takes a side: for example, advocating
for or against proposals like adopting a vegetarian lifestyle or mandating free college ed-
ucation. the problem of stance classi   cation is to identify the author   s position from the
text of the argument. in some cases, there is training data available for each position,
so that standard document classi   cation techniques can be employed. in other cases, it
suf   ces to classify each document as whether it is in support or opposition of the argu-
ment advanced by a previous document (anand et al., 2011). in the most challenging
case, there is no labeled data for any of the stances, so the only possibility is group docu-
ments that advocate the same position (somasundaran and wiebe, 2009). this is a form
of unsupervised learning, discussed in chapter 5.

targeted id31 the expression of sentiment is often more nuanced than a
simple binary label. consider the following examples:

(4.2)

a. the vodka was good, but the meat was rotten.
b. go to heaven for the climate, hell for the company.    mark twain

these statements display a mixed overall sentiment: positive towards some entities (e.g.,
the vodka), negative towards others (e.g., the meat). targeted id31 seeks to
identify the writer   s sentiment towards speci   c entities (jiang et al., 2011). this requires
identifying the entities in the text and linking them to speci   c sentiment words     much
more than we can do with the classi   cation-based approaches discussed thus far. for
example, kim and hovy (2006) analyze sentence-internal structure to determine the topic
of each sentiment expression.

aspect-based opinion mining seeks to identify the sentiment of the author of a review
towards prede   ned aspects such as price and service, or, in the case of (4.2b), climate
and company (hu and liu, 2004). if the aspects are not de   ned in advance, it may again
be necessary to employ unsupervised learning methods to identify them (e.g., branavan
et al., 2009).

emotion classi   cation while id31 is framed in terms of positive and neg-
ative categories, psychologists generally regard emotion as more multifaceted. for ex-
ample, ekman (1992) argues that there are six basic emotions     happiness, surprise, fear,

under contract with mit press, shared under cc-by-nc-nd license.

72

chapter 4. linguistic applications of classification

sadness, anger, and contempt     and that they are universal across human cultures. alm
et al. (2005) build a linear classi   er for recognizing the emotions expressed in children   s
stories. the ultimate goal of this work was to improve text-to-id133, so that
stories could be read with intonation that re   ected the emotional content. they used bag-
of-words features, as well as features capturing the story type (e.g., jokes, folktales), and
structural features that re   ect the position of each sentence in the story. the task is dif   -
cult: even human annotators frequently disagreed with each other, and the best classi   ers
achieved accuracy between 60-70%.

4.1.2 alternative approaches to id31

regression a more challenging version of id31 is to determine not just
the class of a document, but its rating on a numerical scale (pang and lee, 2005). if the
scale is continuous, it is most natural to apply regression, identifying a set of weights   
that minimize the squared error of a predictor   y =       x + b, where b is an offset. this
approach is called id75, and sometimes least squares, because the regression
coef   cients    are determined by minimizing the squared error, (y       y)2. if the weights are
2, then it is ridge regression. unlike id28,
regularized using a penalty   ||  ||2
both id75 and ridge regression can be solved in closed form as a system of
linear equations.

ordinal ranking in many problems, the labels are ordered but discrete: for example,
product reviews are often integers on a scale of 1     5, and grades are on a scale of a     f .
such problems can be solved by discretizing the score       x into    ranks   ,

  y = argmax
r:     x   br

r,

[4.2]

where b = [b1 =       , b2, b3, . . . , bk] is a vector of boundaries. it is possible to learn the
weights and boundaries simultaneously, using a id88-like algorithm (crammer and
singer, 2001).

lexicon-based classi   cation id31 is one of the only nlp tasks where
hand-crafted feature weights are still widely employed. in lexicon-based classi   cation (taboada
et al., 2011), the user creates a list of words for each label, and then classi   es each docu-
ment based on how many of the words from each list are present. in our linear classi   ca-
tion framework, this is equivalent to choosing the following weights:

  y,j =(cid:40)1,

j     ly

0, otherwise,

[4.3]

jacob eisenstein. draft of november 13, 2018.

4.2. id51

73

where ly is the lexicon for label y. compared to the machine learning classi   ers discussed
in the previous chapters, lexicon-based classi   cation may seem primitive. however, su-
pervised machine learning relies on large annotated datasets, which are time-consuming
and expensive to produce. if the goal is to distinguish two or more categories in a new
domain, it may be simpler to start by writing down a list of words for each category.

an early lexicon was the general inquirer (stone, 1966). today, popular sentiment lexi-
cons include sentiid138 (esuli and sebastiani, 2006) and an evolving set of lexicons
from liu (2015). for emotions and more    ne-grained analysis, linguistic inquiry and word
count (liwc) provides a set of lexicons (tausczik and pennebaker, 2010). the mpqa lex-
icon indicates the polarity (positive or negative) of 8221 terms, as well as whether they are
strongly or weakly subjective (wiebe et al., 2005). a comprehensive comparison of senti-
ment lexicons is offered by ribeiro et al. (2016). given an initial seed lexicon, it is possible
to automatically expand the lexicon by looking for words that frequently co-occur with
words in the seed set (hatzivassiloglou and mckeown, 1997; qiu et al., 2011).

4.2 id51

consider the the following headlines:

(4.3)

iraqi head seeks arms

a.
b. prostitutes appeal to pope
c. drunk gets nine years in violin case2

these headlines are ambiguous because they contain words that have multiple mean-
ings, or senses. id51 is the problem of identifying the intended
sense of each word token in a document. id51 is part of a larger
   eld of research called lexical semantics, which is concerned with meanings of the words.
at a basic level, the problem of id51 is to identify the correct
sense for each word token in a document. part-of-speech ambiguity (e.g., noun versus
verb) is usually considered to be a different problem, to be solved at an earlier stage.
from a linguistic perspective, senses are not properties of words, but of lemmas, which
are canonical forms that stand in for a set of in   ected words. for example, arm/n is a
lemma that includes the in   ected form arms/n     the /n indicates that it we are refer-
ring to the noun, and not its homonym arm/v, which is another lemma that includes
the in   ected verbs (arm/v, arms/v, armed/v, arming/v). therefore, word sense disam-
biguation requires    rst identifying the correct part-of-speech and lemma for each token,

2these examples, and many more, can be found at http://www.ling.upenn.edu/  beatrice/

humor/headlines.html

under contract with mit press, shared under cc-by-nc-nd license.

74

chapter 4. linguistic applications of classification

and then choosing the correct sense from the inventory associated with the corresponding
lemma.3 (part-of-speech tagging is discussed in    8.1.)
4.2.1 how many word senses?
words sometimes have many more than two senses, as exempli   ed by the word serve:

    [function]: the tree stump served as a table
    [contribute to]: his evasive replies only served to heighten suspicion
    [provide]: we serve only the rawest    sh
    [enlist]: she served in an elite combat unit
    [jail]: he served six years for a crime he didn   t commit
    [legal]: they were served with subpoenas4
these sense distinctions are annotated in id138 (http://id138.princeton.

edu), a lexical semantic database for english. id138 consists of roughly 100,000
synsets, which are groups of lemmas (or phrases) that are synonymous. an example
synset is {chump1, fool2, sucker1, mark9}, where the superscripts index the sense of each
lemma that is included in the synset: for example, there are at least eight other senses of
mark that have different meanings, and are not part of this synset. a lemma is polysemous
if it participates in multiple synsets.

id138 de   nes the scope of the id51 problem, and, more
generally, formalizes lexical semantic knowledge of english. (id138s have been cre-
ated for a few dozen other languages, at varying levels of detail.) some have argued
that id138   s sense granularity is too    ne (ide and wilks, 2006); more fundamentally,
the premise that word senses can be differentiated in a task-neutral way has been criti-
cized as linguistically na    ve (kilgarriff, 1997). one way of testing this question is to ask
whether people tend to agree on the appropriate sense for example sentences: accord-
ing to mihalcea et al. (2004), people agree on roughly 70% of examples using id138
senses; far better than chance, but less than agreement on other tasks, such as sentiment
annotation (wilson et al., 2005).

*other lexical semantic relations besides synonymy, id138 also describes many
other lexical semantic relationships, including:

    antonymy: x means the opposite of y, e.g. friend-enemy;
3navigli (2009) provides a survey of approaches for word-sense disambiguation.
4several of the examples are adapted from id138 (fellbaum, 2010).

jacob eisenstein. draft of november 13, 2018.

4.2. id51

75

hypernymy;

    hyponymy: x is a special case of y, e.g. red-color; the inverse relationship is
    meronymy: x is a part of y, e.g., wheel-bicycle; the inverse relationship is holonymy.
classi   cation of these relations can be performed by searching for characteristic pat-
terns between pairs of words, e.g., x, such as y, which signals hyponymy (hearst, 1992),
or x but y, which signals antonymy (hatzivassiloglou and mckeown, 1997). another ap-
proach is to analyze each term   s distributional statistics (the frequency of its neighboring
words). such approaches are described in detail in chapter 14.

4.2.2 id51 as classi   cation
how can we tell living plants from manufacturing plants? the context is often critical:

(4.4)

a. town of   cials are hoping to attract new manufacturing plants through weak-

ened environmental regulations.

b. the endangered plants play an important role in the local ecosystem.

it is possible to build a feature vector using the bag-of-words representation, by treat-

ing each context as a pseudo-document. the feature function is then,

f ((plant, the endangered plants play an . . . ), y) =
{(the, y) : 1, (endangered, y) : 1, (play, y) : 1, (an, y) : 1, . . .}

as in document classi   cation, many of these features are irrelevant, but a few are very
in this example, the context word endangered is a strong signal that
strong predictors.
the intended sense is biology rather than manufacturing. we would therefore expect a
learning algorithm to assign high weight to (endangered, biology), and low weight to
(endangered, manufacturing).5

it may also be helpful to go beyond the bag-of-words: for example, one might encode

the position of each context word with respect to the target, e.g.,

f ((bank, i went to the bank to deposit my paycheck), y) =
{(i     3, went, y) : 1, (i + 2, deposit, y) : 1, (i + 4, paycheck, y) : 1}

these are called collocation features, and they give more information about the speci   c
role played by each context word. this idea can be taken further by incorporating addi-
tional syntactic information about the grammatical role played by each context feature,
such as the dependency path (see chapter 11).

5the context bag-of-words can be also used be used to perform word-sense disambiguation without
machine learning: the lesk (1986) algorithm selects the word sense whose dictionary de   nition best overlaps
the local context.

under contract with mit press, shared under cc-by-nc-nd license.

76

chapter 4. linguistic applications of classification

using such features, a classi   er can be trained from labeled data. a semantic concor-
dance is a corpus in which each open-class word (nouns, verbs, adjectives, and adverbs)
is tagged with its word sense from the target dictionary or thesaurus. semcor is a seman-
tic concordance built from 234k tokens of the brown corpus (francis and kucera, 1982),
annotated as part of the id138 project (fellbaum, 2010). semcor annotations look
like this:

(4.5) as of sunday1

n night1

n there was4

v no word2

n . . . ,

with the superscripts indicating the annotated sense of each polysemous word, and the
subscripts indicating the part-of-speech.

as always, supervised classi   cation is only possible if enough labeled examples can
be accumulated. this is dif   cult in id51, because each polysemous
lemma requires its own training set: having a good classi   er for the senses of serve is no
help towards disambiguating plant. for this reason, unsupervised and semi-supervised
methods are particularly important for id51 (e.g., yarowsky, 1995).
these methods will be discussed in chapter 5. unsupervised methods typically lean on
the heuristic of    one sense per discourse   , which means that a lemma will usually have
a single, consistent sense throughout any given document (gale et al., 1992). based on
this heuristic, we can propagate information from high-con   dence instances to lower-
con   dence instances in the same document (yarowsky, 1995). semi-supervised methods
combine labeled and unlabeled data, and are discussed in more detail in chapter 5.

4.3 design decisions for text classi   cation

text classi   cation involves a number of design decisions. in some cases, the design deci-
sion is clear from the mathematics: if you are using id173, then a id173
weight    must be chosen. other decisions are more subtle, arising only in the low level
   plumbing    code that ingests and processes the raw data. such decision can be surpris-
ingly consequential for classi   cation accuracy.

4.3.1 what is a word?

the bag-of-words representation presupposes that extracting a vector of word counts
from text is unambiguous. but text documents are generally represented as a sequences of
characters (in an encoding such as ascii or unicode), and the conversion to bag-of-words
presupposes a de   nition of the    words    that are to be counted.

jacob eisenstein. draft of november 13, 2018.

4.3. design decisions for text classification

77

whitespace
treebank
tweet
toktok (dehdari, 2014)

n   t

isn   t ahab, ahab?
ahab
is
isn   t ahab
,
t
isn

   

;)
,
ahab ?
ahab ,

;

ahab ?
;)
ahab ?

)

;

)

figure 4.1: the output of four nltk tokenizers, applied to the string isn   t ahab, ahab? ;)

id121

the    rst subtask for constructing a bag-of-words vector is id121: converting the
text from a sequence of characters to a sequence of word!tokens. a simple approach is
to de   ne a subset of characters as whitespace, and then split the text on these tokens.
however, whitespace-based id121 is not ideal: we may want to split conjunctions
like isn   t and hyphenated phrases like prize-winning and half-asleep, and we likely want
to separate words from commas and periods that immediately follow them. at the same
time, it would be better not to split abbreviations like u.s. and ph.d. in languages with
roman scripts, id121 is typically performed using id157, with mod-
ules designed to handle each of these cases. for example, the nltk package includes a
number of tokenizers (loper and bird, 2002); the outputs of four of the better-known tok-
enizers are shown in figure 4.1. social media researchers have found that emoticons and
other forms of orthographic variation pose new challenges for id121, leading to the
development of special purpose tokenizers to handle these phenomena (o   connor et al.,
2010).

id121 is a language-speci   c problem, and each language poses unique chal-
lenges. for example, chinese does not include spaces between words, nor any other
consistent orthographic markers of word boundaries. a    greedy    approach is to scan the
input for character substrings that are in a prede   ned lexicon. however, xue et al. (2003)
notes that this can be ambiguous, since many character sequences could be segmented in
multiple ways. instead, he trains a classi   er to determine whether each chinese character,
or hanzi, is a word boundary. more advanced sequence labeling methods for word seg-
mentation are discussed in    8.4. similar problems can occur in languages with alphabetic
scripts, such as german, which does not include whitespace in compound nouns, yield-
ing examples such as freundschaftsbezeigungen (demonstration of friendship) and dilet-
tantenaufdringlichkeiten (the importunities of dilettantes). as twain (1997) argues,    these
things are not words, they are alphabetic processions.    social media raises similar problems
for english and other languages, with hashtags such as #trueloveinfourwords requiring
decomposition for analysis (brun and roux, 2014).

under contract with mit press, shared under cc-by-nc-nd license.

78

chapter 4. linguistic applications of classification

original
porter stemmer
lancaster stemmer
id138 lemmatizer the williams

the williams
sisters
the william sister
the william sist

sister

are
are
ar
are

leaving this
leav
thi
leav
thi
leaving this

tennis
tenni
ten
tennis

centre
centr
cent
centre

figure 4.2: sample outputs of the porter (1980) and lancaster (paice, 1990) stemmers, and
the id138 lemmatizer

text id172

after splitting the text into tokens, the next question is which tokens are really distinct.
is it necessary to distinguish great, great, and great? sentence-initial capitalization may
be irrelevant to the classi   cation task. going further, the complete elimination of case
distinctions will result in a smaller vocabulary, and thus smaller feature vectors. however,
case distinctions might be relevant in some situations: for example, apple is a delicious
pie    lling, while apple is a company that specializes in proprietary dongles and power
adapters.

for roman script, case conversion can be performed using unicode string libraries.
many scripts do not have case distinctions (e.g., the devanagari script used for south
asian languages, the thai alphabet, and japanese kana), and case conversion for all scripts
may not be available in every programming environment. (unicode support is an im-
portant distinction between python   s versions 2 and 3, and is a good reason for mi-
grating to python 3 if you have not already done so. compare the output of the code
"\`a l\   h  otel".upper() in the two language versions.)

case conversion is a type of text id172, which refers to string transforma-
tions that remove distinctions that are irrelevant to downstream applications (sproat et al.,
2001). other forms of id172 include the standardization of numbers (e.g., 1,000 to
1000) and dates (e.g., august 11, 2015 to 2015/11/08). depending on the application, it may
even be worthwhile to convert all numbers and dates to special tokens, !num and !date.
in social media, there are additional orthographic phenomena that may be normalized,
such as expressive lengthening, e.g., cooooool (aw et al., 2006; yang and eisenstein, 2013).
similarly, historical texts feature spelling variations that may need to be normalized to a
contemporary standard form (baron and rayson, 2008).

a more extreme form of id172 is to eliminate in   ectional af   xes, such as the
-ed and -s suf   xes in english. on this view, whale, whales, and whaling all refer to the
same underlying concept, so they should be grouped into a single feature. a stemmer is
a program for eliminating af   xes, usually by applying a series of regular expression sub-
stitutions. character-based id30 algorithms are necessarily approximate, as shown
in figure 4.2: the lancaster stemmer incorrectly identi   es -ers as an in   ectional suf   x of

jacob eisenstein. draft of november 13, 2018.

4.3. design decisions for text classification

79

(a) movie review data in english

(b) news articles in brazilian portuguese

figure 4.3: tradeoff between token coverage (y-axis) and vocabulary size, on the nltk
movie review dataset, after sorting the vocabulary by decreasing frequency. the red
dashed lines indicate 80%, 90%, and 95% coverage.

sisters (by analogy to    x/   xers), and both stemmers incorrectly identify -s as a suf   x of this
and williams. fortunately, even inaccurate id30 can improve bag-of-words classi   -
cation models, by merging related strings and thereby reducing the vocabulary size.

accurately handling irregular orthography requires word-speci   c rules. lemmatizers
are systems that identify the underlying lemma of a given wordform. they must avoid the
over-generalization errors of the stemmers in figure 4.2, and also handle more complex
transformations, such as geese   goose. the output of the id138 lemmatizer is shown in
the    nal line of figure 4.2. both id30 and lemmatization are language-speci   c: an
english stemmer or lemmatizer is of little use on a text written in another language. the
discipline of morphology relates to the study of word-internal structure, and is described
in more detail in    9.1.2.

the value of id172 depends on the data and the task. id172 re-
duces the size of the feature space, which can help in generalization. however, there
is always the risk of merging away linguistically meaningful distinctions. in supervised
machine learning, id173 and smoothing can play a similar role to id172
    preventing the learner from over   tting to rare features     while avoiding the language-
speci   c engineering required for accurate id172. in unsupervised scenarios, such
as content-based information retrieval (manning et al., 2008) and id96 (blei
et al., 2003), id172 is more critical.

4.3.2 how many words?
limiting the size of the feature vector reduces the memory footprint of the resulting mod-
els, and increases the speed of prediction. id172 can help to play this role, but
a more direct approach is simply to limit the vocabulary to the n most frequent words
in the dataset. for example, in the movie-reviews dataset provided with nltk (origi-
nally from pang et al., 2002), there are 39,768 word types, and 1.58m tokens. as shown

under contract with mit press, shared under cc-by-nc-nd license.

010000200003000040000vocabulary size0.51.0token coveragepang and lee movie reviews (english)010000200003000040000500006000070000vocabulary size0.51.0token coveragemac-morpho corpus (brazilian portuguese)80

chapter 4. linguistic applications of classification

in figure 4.3a, the most frequent 4000 word types cover 90% of all tokens, offering an
order-of-magnitude reduction in the model size. such ratios are language-speci   c: in for
example, in the brazilian portuguese mac-morpho corpus (alu    sio et al., 2003), attain-
ing 90% coverage requires more than 10000 word types (figure 4.3b). this re   ects the
morphological complexity of portuguese, which includes many more in   ectional suf   xes
than english.

eliminating rare words is not always advantageous for classi   cation performance: for
example, names, which are typically rare, play a large role in distinguishing topics of news
articles. another way to reduce the size of the feature space is to eliminate stopwords such
as the, to, and and, which may seem to play little role in expressing the topic, sentiment,
or stance. this is typically done by creating a stoplist (e.g., nltk.corpus.stopwords),
and then ignoring all terms that match the list. however, corpus linguists and social psy-
chologists have shown that seemingly inconsequential words can offer surprising insights
about the author or nature of the text (biber, 1991; chung and pennebaker, 2007). further-
more, high-frequency words are unlikely to cause over   tting in discriminative classi   ers.
as with id172, stopword    ltering is more important for unsupervised problems,
such as term-based document retrieval.

another alternative for controlling model size is feature hashing (weinberger et al.,
2009). each feature is assigned an index using a hash function. if a hash function that
permits collisions is chosen (typically by taking the hash output modulo some integer),
then the model can be made arbitrarily small, as multiple features share a single weight.
because most features are rare, accuracy is surprisingly robust to such collisions (ganchev
and dredze, 2008).

4.3.3 count or binary?
finally, we may consider whether we want our feature vector to include the count of each
word, or its presence. this gets at a subtle limitation of linear classi   cation: it   s worse to
have two failures than one, but is it really twice as bad? motivated by this intuition, pang
et al. (2002) use binary indicators of presence or absence in the feature vector: fj(x, y)    
{0, 1}. they    nd that classi   ers trained on these binary vectors tend to outperform feature
vectors based on word counts. one explanation is that words tend to appear in clumps:
if a word has appeared once in a document, it is likely to appear again (church, 2000).
these subsequent appearances can be attributed to this tendency towards repetition, and
thus provide little additional information about the class label of the document.

4.4 evaluating classi   ers

in any supervised machine learning application, it is critical to reserve a held-out test set.
this data should be used for only one purpose: to evaluate the overall accuracy of a single

jacob eisenstein. draft of november 13, 2018.

4.4. evaluating classifiers

81

classi   er. using this data more than once would cause the estimated accuracy to be overly
optimistic, because the classi   er would be customized to this data, and would not perform
as well as on unseen data in the future. it is usually necessary to set hyperparameters or
perform feature selection, so you may need to construct a tuning or development set for
this purpose, as discussed in    2.2.5.

there are a number of ways to evaluate classi   er performance. the simplest is accu-

racy: the number of correct predictions, divided by the total number of instances,

acc(y,   y) =

1
n

n(cid:88)i

  (y(i) =   y).

[4.4]

exams are usually graded by accuracy. why are other metrics necessary? the main
reason is class imbalance. suppose you are building a classi   er to detect whether an
electronic health record (ehr) describes symptoms of a rare disease, which appears in
only 1% of all documents in the dataset. a classi   er that reports   y = negative for
all documents would achieve 99% accuracy, but would be practically useless. we need
metrics that are capable of detecting the classi   er   s ability to discriminate between classes,
even when the distribution is skewed.

one solution is to build a balanced test set, in which each possible label is equally rep-
resented. but in the ehr example, this would mean throwing away 98% of the original
dataset! furthermore, the detection threshold itself might be a design consideration: in
health-related applications, we might prefer a very sensitive classi   er, which returned a
positive prediction if there is even a small chance that y(i) = positive. in other applica-
tions, a positive result might trigger a costly action, so we would prefer a classi   er that
only makes positive predictions when absolutely certain. we need additional metrics to
capture these characteristics.

4.4.1 precision, recall, and f -measure
for any label (e.g., positive for presence of symptoms of a disease), there are two possible
errors:

    false positive: the system incorrectly predicts the label.
    false negative: the system incorrectly fails to predict the label.

similarly, for any label, there are two ways to be correct:

    true positive: the system correctly predicts the label.
    true negative: the system correctly predicts that the label does not apply to this

instance.

under contract with mit press, shared under cc-by-nc-nd license.

82

chapter 4. linguistic applications of classification

classi   ers that make a lot of false positives have low precision: they predict the label
even when it isn   t there. classi   ers that make a lot of false negatives have low recall: they
fail to predict the label, even when it is there. these metrics distinguish these two sources
of error, and are de   ned formally as:

recall(y,   y, k) =

precision(y,   y, k) =

tp

tp + fn
tp + fp .

tp

[4.5]

[4.6]

recall and precision are both conditional likelihoods of a correct prediction, which is why
their numerators are the same. recall is conditioned on k being the correct label, y(i) = k,
so the denominator sums over true positive and false negatives. precision is conditioned
on k being the prediction, so the denominator sums over true positives and false positives.
note that true negatives are not considered in either statistic. the classi   er that labels
every document as    negative    would achieve zero recall; precision would be 0
0.

recall and precision are complementary. a high-recall classi   er is preferred when
false positives are cheaper than false negatives: for example, in a preliminary screening
for symptoms of a disease, the cost of a false positive might be an additional test, while a
false negative would result in the disease going untreated. conversely, a high-precision
classi   er is preferred when false positives are more expensive: for example, in spam de-
tection, a false negative is a relatively minor inconvenience, while a false positive might
mean that an important message goes unread.

the f -measure combines recall and precision into a single metric, using the har-

monic mean:

f -measure(y,   y, k) =

2rp
r + p

,

[4.7]

where r is recall and p is precision.6

evaluating multi-class classi   cation recall, precision, and f -measure are de   ned with
respect to a speci   c label k. when there are multiple labels of interest (e.g., in word sense
disambiguation or emotion classi   cation), it is necessary to combine the f -measure
across each class. macro f -measure is the average f -measure across several classes,

macro-f (y,   y) =

1

|k|(cid:88)k   k

f -measure(y,   y, k)

[4.8]

6f -measure is sometimes called f1, and generalizes to f   = (1+  2)rp

  2p+r . the    parameter can be tuned to

emphasize recall or precision.

jacob eisenstein. draft of november 13, 2018.

4.4. evaluating classifiers

83

figure 4.4: roc curves for three classi   ers of varying discriminative power, measured by
auc (area under the curve)

in multi-class problems with unbalanced class distributions, the macro f -measure is a
balanced measure of how well the classi   er recognizes each class. in micro f -measure,
we compute true positives, false positives, and false negatives for each class, and then add
them up to compute a single recall, precision, and f -measure. this metric is balanced
across instances rather than classes, so it weights each class in proportion to its frequency
    unlike macro f -measure, which weights each class equally.

4.4.2 threshold-free metrics

in binary classi   cation problems, it is possible to trade off between recall and precision by
adding a constant    threshold    to the output of the scoring function. this makes it possible
to trace out a curve, where each point indicates the performance at a single threshold. in
the receiver operating characteristic (roc) curve,7 the x-axis indicates the false positive
rate,
fp+tn, and the y-axis indicates the recall, or true positive rate. a perfect classi   er
attains perfect recall without any false positives, tracing a    curve    from the origin (0,0) to
the upper left corner (0,1), and then to (1,1). in expectation, a non-discriminative classi   er
traces a diagonal line from the origin (0,0) to the upper right corner (1,1). real classi   ers
tend to fall between these two extremes. examples are shown in figure 4.4.

fp

the roc curve can be summarized in a single number by taking its integral, the area
under the curve (auc). the auc can be interpreted as the id203 that a randomly-
selected positive example will be assigned a higher score by the classi   er than a randomly-

7the name    receiver operator characteristic    comes from the metric   s origin in signal processing applica-
tions (peterson et al., 1954). other threshold-free metrics include precision-recall curves, precision-at-k, and
balanced f -measure; see manning et al. (2008) for more details.

under contract with mit press, shared under cc-by-nc-nd license.

0.00.20.40.60.81.0false positive rate0.00.20.40.60.81.0true positive rateauc=0.89auc=0.73auc=0.584

chapter 4. linguistic applications of classification

selected negative example. a perfect classi   er has auc = 1 (all positive examples score
higher than all negative examples); a non-discriminative classi   er has auc = 0.5 (given
a randomly selected positive and negative example, either could score higher with equal
id203); a perfectly wrong classi   er would have auc = 0 (all negative examples score
higher than all positive examples). one advantage of auc in comparison to f -measure
is that the baseline rate of 0.5 does not depend on the label distribution.

4.4.3 classi   er comparison and statistical signi   cance
natural language processing research and engineering often involves comparing different
classi   cation techniques. in some cases, the comparison is between algorithms, such as
id28 versus averaged id88, or l2 id173 versus l1. in other
cases, the comparison is between feature sets, such as the bag-of-words versus positional
bag-of-words (see    4.2.2). ablation testing involves systematically removing (ablating)
various aspects of the classi   er, such as feature groups, and testing the null hypothesis
that the ablated classi   er is as good as the full model.

a full treatment of hypothesis testing is beyond the scope of this text, but this section
contains a brief summary of the techniques necessary to compare classi   ers. the main
aim of hypothesis testing is to determine whether the difference between two statistics
    for example, the accuracies of two classi   ers     is likely to arise by chance. we will
be concerned with chance    uctuations that arise due to the    nite size of the test set.8 an
improvement of 10% on a test set with ten instances may re   ect a random    uctuation that
makes the test set more favorable to classi   er c1 than c2; on another test set with a different
ten instances, we might    nd that c2 does better than c1. but if we observe the same 10%
improvement on a test set with 1000 instances, this is highly unlikely to be explained
by chance. such a    nding is said to be statistically signi   cant at a level p, which is the
id203 of observing an effect of equal or greater magnitude when the null hypothesis
is true. the notation p < .05 indicates that the likelihood of an equal or greater effect is
less than 5%, assuming the null hypothesis is true.9

the binomial test

the statistical signi   cance of a difference in accuracy can be evaluated using classical tests,
such as the binomial test.10 suppose that classi   ers c1 and c2 disagree on n instances in a

8other sources of variance include the initialization of non-convex classi   ers such as neural networks,

and the ordering of instances in online learning such as stochastic id119 and id88.

9statistical hypothesis testing is useful only to the extent that the existing test set is representative of
the instances that will be encountered in the future. if, for example, the test set is constructed from news
documents, no hypothesis test can predict which classi   er will perform best on documents from another
domain, such as electronic health records.

10a well-known alternative to the binomial test is mcnemar   s test, which computes a test statistic based
on the number of examples that are correctly classi   ed by one system and incorrectly classi   ed by the other.

jacob eisenstein. draft of november 13, 2018.

4.4. evaluating classifiers

85

figure 4.5: id203 mass function for the binomial distribution. the pink highlighted
areas represent the cumulative id203 for a signi   cance test on an observation of
k = 10 and n = 30.

test set with binary labels, and that c1 is correct on k of those instances. under the null hy-
pothesis that the classi   ers are equally accurate, we would expect k/n to be roughly equal
to 1/2, and as n increases, k/n should be increasingly close to this expected value. these
properties are captured by the binomial distribution, which is a id203 over counts
of binary random variables. we write k     binom(  , n ) to indicate that k is drawn from
a binomial distribution, with parameter n indicating the number of random    draws   ,
and    indicating the id203 of    success    on each draw. each draw is an example on
which the two classi   ers disagree, and a    success    is a case in which c1 is right and c2 is
wrong. (the label space is assumed to be binary, so if the classi   ers disagree, exactly one
of them is correct. the test can be generalized to multi-class classi   cation by focusing on
the examples in which exactly one classi   er is correct.)

the id203 mass function (pmf) of the binomial distribution is,

pbinom(k; n,   ) =(cid:18)n

k(cid:19)  k(1       )n   k,
[4.9]
with   k representing the id203 of the k successes, (1       )n   k representing the prob-
k(cid:1) =
ability of the n     k unsuccessful draws. the expression(cid:0)n
k!(n   k)! is a binomial
coef   cient, representing the number of possible orderings of events; this ensures that the
distribution sums to one over all k     {0, 1, 2, . . . , n}.

likely to be right, so    = 1
k < n

under the null hypothesis, when the classi   ers disagree, each classi   er is equally
2. now suppose that among n disagreements, c1 is correct
2 times. the id203 of c1 being correct k or fewer times is the one-tailed p-value,
the null hypothesis distribution for this test statistic is known to be drawn from a chi-squared distribution
with a single degree of freedom, so a p-value can be computed from the cumulative density function of this
distribution (dietterich, 1998). both tests give similar results in most circumstances, but the binomial test is
easier to understand from    rst principles.

n !

under contract with mit press, shared under cc-by-nc-nd license.

051015202530instances where c1 is right and c2 is wrong0.000.050.100.15p(kn=30,=0.5)86

chapter 4. linguistic applications of classification

because it is computed from the area under the binomial id203 mass function from
0 to k, as shown in the left tail of figure 4.5. this cumulative id203 is computed as
a sum over all values i     k,

binom(cid:18)count(  y(i)

pr

2 = y(i) (cid:54)=   y(i)

1 )     k; n,    =

1

2(cid:19) =

k(cid:88)i=0

pbinom(cid:18)i; n,    =

1

2(cid:19) .

[4.10]

the one-tailed p-value applies only to the asymmetric null hypothesis that c1 is at least
as accurate as c2. to test the two-tailed null hypothesis that c1 and c2 are equally accu-
rate, we would take the sum of one-tailed p-values, where the second term is computed
from the right tail of figure 4.5. the binomial distribution is symmetric, so this can be
computed by simply doubling the one-tailed p-value.

two-tailed tests are more stringent, but they are necessary in cases in which there is
no prior intuition about whether c1 or c2 is better. for example, in comparing logistic
regression versus averaged id88, a two-tailed test is appropriate. in an ablation
test, c2 may contain a superset of the features available to c1. if the additional features are
thought to be likely to improve performance, then a one-tailed test would be appropriate,
if chosen in advance. however, such a test can only prove that c2 is more accurate than
c1, and not the reverse.

*randomized testing

the binomial test is appropriate for accuracy, but not for more complex metrics such as
f -measure. to compute statistical signi   cance for arbitrary metrics, we can apply ran-
domization. speci   cally, draw a set of m bootstrap samples (efron and tibshirani, 1993),
by resampling instances from the original test set with replacement. each bootstrap sam-
ple is itself a test set of size n. some instances from the original test set will not appear
in any given bootstrap sample, while others will appear multiple times; but overall, the
sample will be drawn from the same distribution as the original test set. we can then com-
pute any desired evaluation on each bootstrap sample, which gives a distribution over the
value of the metric. algorithm 7 shows how to perform this computation.

to compare the f -measure of two classi   ers c1 and c2, we set the function   (  ) to
compute the difference in f -measure on the bootstrap sample. if the difference is less
than or equal to zero in at least 5% of the samples, then we cannot reject the one-tailed
null hypothesis that c2 is at least as good as c1 (berg-kirkpatrick et al., 2012). we may
also be interested in the 95% con   dence interval around a metric of interest, such as
the f -measure of a single classi   er. this can be computed by sorting the output of
algorithm 7, and then setting the top and bottom of the 95% con   dence interval to the
values at the 2.5% and 97.5% percentiles of the sorted outputs. alternatively, you can    t
a normal distribution to the set of differences across bootstrap samples, and compute a
gaussian con   dence interval from the mean and variance.

jacob eisenstein. draft of november 13, 2018.

4.4. evaluating classifiers

87

algorithm 7 bootstrap sampling for classi   er evaluation.
{x(1:n ), y(1:n )}, the metric is   (  ), and the number of samples is m.

procedure bootstrap-sample(x(1:n ), y(1:n ),   (  ), m)

the original test set is

for t     {1, 2, . . . , m} do

for i     {1, 2, . . . , n} do

j     uniforminteger(1, n )
  x(i)     x(j)
  y(i)     y(j)

d(t)       (   x(1:n ),   y(1:n ))

return {d(t)}m

t=1

as the number of bootstrap samples goes to in   nity, m        , the bootstrap estimate
is increasingly accurate. a typical choice for m is 104 or 105; larger numbers of samples
are necessary for smaller p-values. one way to validate your choice of m is to run the test
multiple times, and ensure that the p-values are similar; if not, increase m by an order of
magnitude. this is a heuristic measure of the variance of the test, which can decreases
with the square root    m (robert and casella, 2013).

4.4.4

*multiple comparisons

sometimes it is necessary to perform multiple hypothesis tests, such as when compar-
ing the performance of several classi   ers on multiple datasets. suppose you have    ve
datasets, and you compare four versions of your classi   er against a baseline system, for a
total of 20 comparisons. even if none of your classi   ers is better than the baseline, there
will be some chance variation in the results, and in expectation you will get one statis-
tically signi   cant improvement at p = 0.05 = 1
20. it is therefore necessary to adjust the
p-values when reporting the results of multiple comparisons.

one approach is to require a threshold of   

m to report a p value of p <    when per-
forming m tests. this is known as the bonferroni correction, and it limits the overall
id203 of incorrectly rejecting the null hypothesis at   . another approach is to bound
the false discovery rate (fdr), which is the fraction of null hypothesis rejections that are
incorrect. benjamini and hochberg (1995) propose a p-value correction that bounds the
fraction of false discoveries at   : sort the p-values of each individual test in ascending
m   . if k > 1, the
order, and set the signi   cance threshold equal to largest k such that pk     k
fdr adjustment is more permissive than the bonferroni correction.

under contract with mit press, shared under cc-by-nc-nd license.

88

chapter 4. linguistic applications of classification

4.5 building datasets

sometimes, if you want to build a classi   er, you must    rst build a dataset of your own.
this includes selecting a set of documents or instances to annotate, and then performing
the annotations. the scope of the dataset may be determined by the application: if you
want to build a system to classify electronic health records, then you must work with a
corpus of records of the type that your classi   er will encounter when deployed. in other
cases, the goal is to build a system that will work across a broad range of documents. in
this case, it is best to have a balanced corpus, with contributions from many styles and
genres. for example, the brown corpus draws from texts ranging from government doc-
uments to romance novels (francis, 1964), and the google web treebank includes an-
notations for    ve    domains    of web documents: question answers, emails, newsgroups,
reviews, and blogs (petrov and mcdonald, 2012).

4.5.1 metadata as labels

annotation is dif   cult and time-consuming, and most people would rather avoid it. it
is sometimes possible to exploit existing metadata to obtain labels for training a classi-
   er. for example, reviews are often accompanied by a numerical rating, which can be
converted into a classi   cation label (see    4.1). similarly, the nationalities of social media
users can be estimated from their pro   les (dredze et al., 2013) or even the time zones of
their posts (gouws et al., 2011). more ambitiously, we may try to classify the political af-
   liations of social media pro   les based on their social network connections to politicians
and major political parties (rao et al., 2010).

the convenience of quickly constructing large labeled datasets without manual an-
notation is appealing. however this approach relies on the assumption that unlabeled
instances     for which metadata is unavailable     will be similar to labeled instances.
consider the example of labeling the political af   liation of social media users based on
their network ties to politicians. if a classi   er attains high accuracy on such a test set,
is it safe to assume that it accurately predicts the political af   liation of all social media
users? probably not. social media users who establish social network ties to politicians
may be more likely to mention politics in the text of their messages, as compared to the
average user, for whom no political metadata is available. if so, the accuracy on a test set
constructed from social network metadata would give an overly optimistic picture of the
method   s true performance on unlabeled data.

4.5.2 labeling data

in many cases, there is no way to get ground truth labels other than manual annotation.
an annotation protocol should satisfy several criteria: the annotations should be expressive
enough to capture the phenomenon of interest; they should be replicable, meaning that

jacob eisenstein. draft of november 13, 2018.

4.5. building datasets

89

another annotator or team of annotators would produce very similar annotations if given
the same data; and they should be scalable, so that they can be produced relatively quickly.
hovy and lavid (2010) propose a structured procedure for obtaining annotations that
meet these criteria, which is summarized below.

1. determine what to annotate. this is usually based on some theory of the under-
lying phenomenon: for example, if the goal is to produce annotations about the
emotional state of a document   s author, one should start with a theoretical account
of the types or dimensions of emotion (e.g., mohammad and turney, 2013). at this
stage, the tradeoff between expressiveness and scalability should be considered: a
full instantiation of the underlying theory might be too costly to annotate at scale,
so reasonable approximations should be considered.

2. optionally, one may design or select a software tool to support the annotation
effort. existing general-purpose annotation tools include brat (stenetorp et al.,
2012) and mmax2 (m   uller and strube, 2006).

3. formalize the instructions for the annotation task. to the extent that the instruc-
tions are not explicit, the resulting annotations will depend on the intuitions of the
annotators. these intuitions may not be shared by other annotators, or by the users
of the annotated data. therefore explicit instructions are critical to ensuring the an-
notations are replicable and usable by other researchers.

4. perform a pilot annotation of a small subset of data, with multiple annotators for
each instance. this will give a preliminary assessment of both the replicability and
scalability of the current annotation instructions. metrics for computing the rate of
agreement are described below. manual analysis of speci   c disagreements should
help to clarify the instructions, and may lead to modi   cations of the annotation task
itself. for example, if two labels are commonly con   ated by annotators, it may be
best to merge them.

5. annotate the data. after    nalizing the annotation protocol and instructions, the
main annotation effort can begin. some, if not all, of the instances should receive
multiple annotations, so that inter-annotator agreement can be computed. in some
annotation projects, instances receive many annotations, which are then aggregated
into a    consensus    label (e.g., danescu-niculescu-mizil et al., 2013). however, if the
annotations are time-consuming or require signi   cant expertise, it may be preferable
to maximize scalability by obtaining multiple annotations for only a small subset of
examples.

6. compute and report inter-annotator agreement, and release the data.

in some
cases, the raw text data cannot be released, due to concerns related to copyright or

under contract with mit press, shared under cc-by-nc-nd license.

90

chapter 4. linguistic applications of classification

privacy. in these cases, one solution is to publicly release stand-off annotations,
which contain links to document identi   ers. the documents themselves can be re-
leased under the terms of a licensing agreement, which can impose conditions on
how the data is used. it is important to think through the potential consequences of
releasing data: people may make personal data publicly available without realizing
that it could be redistributed in a dataset and publicized far beyond their expecta-
tions (boyd and crawford, 2012).

measuring inter-annotator agreement

to measure the replicability of annotations, a standard practice is to compute the extent to
which annotators agree with each other. if the annotators frequently disagree, this casts
doubt on either their reliability or on the annotation system itself. for classi   cation, one
can compute the frequency with which the annotators agree; for rating scales, one can
compute the average distance between ratings. these raw agreement statistics must then
be compared with the rate of agreement by chance     the expected level of agreement that
would be obtained between two annotators who ignored the data.

cohen   s kappa is widely used for quantifying the agreement on discrete labeling

tasks (cohen, 1960; carletta, 1996),11

   =

agreement     e[agreement]

1     e[agreement]

.

[4.11]

the numerator is the difference between the observed agreement and the chance agree-
ment, and the denominator is the difference between perfect agreement and chance agree-
ment. thus,    = 1 when the annotators agree in every case, and    = 0 when the annota-
tors agree only as often as would happen by chance. various heuristic scales have been
proposed for determining when    indicates    moderate   ,    good   , or    substantial    agree-
ment; for reference, lee and narayanan (2005) report        0.45     0.47 for annotations
of emotions in spoken dialogues, which they describe as    moderate agreement   ; stolcke
et al. (2000) report    = 0.8 for annotations of dialogue acts, which are labels for the pur-
pose of each turn in a conversation.

when there are two annotators, the expected chance agreement is computed as,

e[agreement] =(cid:88)k

  pr(y = k)2,

[4.12]

where k is a sum over labels, and   pr(y = k) is the empirical id203 of label k across
all annotations. the formula is derived from the expected number of agreements if the
annotations were randomly shuf   ed. thus, in a binary labeling task, if one label is applied
to 90% of instances, chance agreement is .92 + .12 = .82.

11 for other types of annotations, krippendorf   s alpha is a popular choice (hayes and krippendorff, 2007;

artstein and poesio, 2008).

jacob eisenstein. draft of november 13, 2018.

4.5. building datasets

id104

91

id104 is often used to rapidly obtain annotations for classi   cation problems.
for example, amazon mechanical turk makes it possible to de   ne    human intelligence
tasks (hits)   , such as labeling data. the researcher sets a price for each set of annotations
and a list of minimal quali   cations for annotators, such as their native language and their
satisfaction rate on previous tasks. the use of relatively untrained    crowdworkers    con-
trasts with earlier annotation efforts, which relied on professional linguists (marcus et al.,
1993). however, id104 has been found to produce reliable annotations for many
language-related tasks (snow et al., 2008). id104 is part of the broader    eld
of human computation (law and ahn, 2011).for a critical examination of ethical issues
related to id104, see fort et al. (2011).

additional resources

many of the preprocessing issues discussed in this chapter also arise in information re-
trieval. see manning et al. (2008) for discussion of id121 and related algorithms.
for more on hypothesis testing in particular and replicability in general, see (dror et al.,
2017, 2018).

exercises

1. as noted in    4.3.3, words tend to appear in clumps, with subsequent occurrences
of a word being more probable. more concretely, if word j has id203   y,j
of appearing in a document with label y, then the id203 of two appearances
(x(i)
suppose you are applying na    ve bayes to a binary classi   cation. focus on a word j
which is more probable under label y = 1, so that,

j = 2) is greater than   2

y,j.

pr(w = j | y = 1) > pr(w = j | y = 0).

[4.13]

j > 1. all else equal, will the classi   er overestimate or under-

now suppose that x(i)
estimate the posterior pr(y = 1 | x)?

2. prove that f-measure is never greater than the arithmetic mean of recall and preci-

sion, r+p

2 . your solution should also show that f-measure is equal to r+p

2

iff r = p.

3. given a binary classi   cation problem in which the id203 of the    positive    label
is equal to   , what is the expected f -measure of a random classi   er which ignores
2? (assume that p(  y)   p(y).) what is
the data, and selects   y = +1 with id203 1
the expected f -measure of a classi   er that selects   y = +1 with id203    (also
independent of y(i))? depending on   , which random classi   er will score better?

under contract with mit press, shared under cc-by-nc-nd license.

92

chapter 4. linguistic applications of classification

4. suppose that binary classi   ers c1 and c2 disagree on n = 30 cases, and that c1 is

correct in k = 10 of those cases.

    write a program that uses primitive functions such as exp and factorial to com-
pute the two-tailed p-value     you may use an implementation of the    choose   
function if one is avaiable. verify your code against the output of a library for
computing the binomial test or the binomial cdf, such as scipy.stats.binom
in python.

    then use a randomized test to try to obtain the same p-value. in each sample,
draw from a binomial distribution with n = 30 and    = 1
2. count the fraction
of samples in which k     10. this is the one-tailed p-value; double this to
compute the two-tailed p-value.

    try this with varying numbers of bootstrap samples: m     {100, 1000, 5000, 10000}.

for m = 100 and m = 1000, run the test 10 times, and plot the resulting p-
values.

    finally, perform the same tests for n = 70 and k = 25.

5. semcor 3.0 is a labeled dataset for id51. you can download

it,12 or access it in nltk.corpora.semcor.
choose a word that appears at least ten times in semcor (   nd), and annotate its
id138 senses across ten randomly-selected examples, without looking at the ground
truth. use online id138 to understand the de   nition of each of the senses.13 have
a partner do the same annotations, and compute the raw rate of agreement, expected
chance rate of agreement, and cohen   s kappa.

6. download the pang and lee movie review data, currently available from http:
//www.cs.cornell.edu/people/pabo/movie-review-data/. hold out a
randomly-selected 400 reviews as a test set.
download a sentiment lexicon, such as the one currently available from bing liu,
https://www.cs.uic.edu/  liub/fbs/sentiment-analysis.html. tokenize
the data, and classify each document as positive iff it has more positive sentiment
words than negative sentiment words. compute the accuracy and f -measure on
detecting positive reviews on the test set, using this lexicon-based classi   er.
then train a discriminative classi   er (averaged id88 or id28) on
the training set, and compute its accuracy and f -measure on the test set.
determine whether the differences are statistically signi   cant, using two-tailed hy-
pothesis tests: binomial for the difference in accuracy, and bootstrap for the differ-
ence in macro-f -measure.

12e.g.,

https://github.com/google-research-datasets/word_sense_disambigation_

corpora or http://globalid138.org/id138-annotated-corpora/

13http://id138web.princeton.edu/perl/webwn

jacob eisenstein. draft of november 13, 2018.

4.5. building datasets

93

the remaining problems will require you to build a classi   er and test its properties. pick
a multi-class text classi   cation dataset that is not already tokenized. one example is a
dataset of new york times headlines and topics (boydstun, 2013).14 divide your data
into training (60%), development (20%), and test sets (20%), if no such division already
exists. if your dataset is very large, you may want to focus on a few thousand instances at
   rst.

7. compare various vocabulary sizes of 102, 103, 104, 105, using the most frequent words
in each case (you may use any reasonable tokenizer). train id28 clas-
si   ers for each vocabulary size, and apply them to the development set. plot the
accuracy and macro-f -measure with the increasing vocabulary size. for each vo-
cabulary size, tune the regularizer to maximize accuracy on a subset of data that is
held out from the training set.

8. compare the following id121 algorithms:

    whitespace, using a regular expression;
    the id32 tokenizer from nltk;
    splitting the input into non-overlapping    ve-character units, regardless of whites-

pace or punctuation.

compute the token/type ratio for each tokenizer on the training data, and explain
what you    nd. train your classi   er on each tokenized dataset, tuning the regularizer
on a subset of data that is held out from the training data. tokenize the development
set, and report accuracy and macro-f -measure.

9. apply the porter and lancaster stemmers to the training set, using any reasonable
tokenizer, and compute the token/type ratios. train your classi   er on the stemmed
data, and compute the accuracy and macro-f -measure on stemmed development
data, again using a held-out portion of the training data to tune the regularizer.

10. identify the best combination of vocabulary    ltering, id121, and id30
from the previous three problems. apply this preprocessing to the test set, and
compute the test set accuracy and macro-f -measure. compare against a baseline
system that applies no vocabulary    ltering, whitespace id121, and no stem-
ming.
use the binomial test to determine whether your best-performing system is signi   -
cantly more accurate than the baseline.

14available

as

a

csv

   le

at

supplementary-information-for-making-the-news.html.
this problem.

http://www.amber-boydstun.com/
use the    eld topic 2digit for

under contract with mit press, shared under cc-by-nc-nd license.

94

chapter 4. linguistic applications of classification

use the bootstrap test with m = 104 to determine whether your best-performing
system achieves signi   cantly higher macro-f -measure.

jacob eisenstein. draft of november 13, 2018.

chapter 5

learning without supervision

so far, we have assumed the following setup:

    a training set where you get observations x and labels y;
    a test set where you only get observations x.

without labeled data, is it possible to learn anything? this scenario is known as unsu-
pervised learning, and we will see that indeed it is possible to learn about the underlying
structure of unlabeled observations. this chapter will also explore some related scenarios:
semi-supervised learning, in which only some instances are labeled, and domain adap-
tation, in which the training data differs from the data on which the trained system will
be deployed.

5.1 unsupervised learning

to motivate unsupervised learning, consider the problem of id51
(   4.2). the goal is to classify each instance of a word, such as bank into a sense,

    bank#1: a    nancial institution
    bank#2: the land bordering a river

it is dif   cult to obtain suf   cient training data for id51, because
even a large corpus will contain only a few instances of all but the most common words.
is it possible to learn anything about these different senses without labeled data?

id51 is usually performed using feature vectors constructed
from the local context of the word to be disambiguated. for example, for the word

95

96

chapter 5. learning without supervision

figure 5.1: counts of words from two different context groups

bank, the immediate context might typically include words from one of the following two
groups:

1.    nancial, deposits, credit, lending, capital, markets, regulated, reserve, liquid, assets

2. land, water, geography, stream, river,    ow, deposits, discharge, channel, ecology

now consider a scatterplot, in which each point is a document containing the word bank.
the location of the document on the x-axis is the count of words in group 1, and the
location on the y-axis is the count for group 2. in such a plot, shown in figure 5.1, two
   blobs    might emerge, and these blobs correspond to the different senses of bank.

here   s a related scenario, from a different problem. suppose you download thousands
of news articles, and make a scatterplot, where each point corresponds to a document:
the x-axis is the frequency of the group of words (hurricane, winds, storm); the y-axis is the
frequency of the group (election, voters, vote). this time, three blobs might emerge: one
for documents that are largely about a hurricane, another for documents largely about a
election, and a third for documents about neither topic.

these clumps represent the underlying structure of the data. but the two-dimensional
scatter plots are based on groupings of context words, and in real scenarios these word
lists are unknown. unsupervised learning applies the same basic idea, but in a high-
dimensional space with one dimension for every context word. this space can   t be di-
rectly visualized, but the goal is the same: try to identify the underlying structure of the
observed data, such that there are a few clusters of points, each of which is internally
coherent. id91 algorithms are capable of    nding such structure automatically.

5.1.1 id116 id91
id91 algorithms assign each data point to a discrete cluster, zi     1, 2, . . . k. one of
the best known id91 algorithms is id116, an iterative algorithm that maintains

jacob eisenstein. draft of november 13, 2018.

010203040densityofwordgroup102040densityofwordgroup25.1. unsupervised learning

97

for i     1 . . . n do
repeat

algorithm 8 id116 id91 algorithm
1: procedure id116(x1:n , k)
2:
3:
4:
5:
6:

z(i)     randomint(1, k)
for k     1 . . . k do
  (z(i)=k)(cid:80)n

  k     1

for i     1 . . . n do

z(i)     argmink ||x(i)       k||2

i=1   (z(i) = k)x(i)

7:
8:
9:
10:

until converged
return {z(i)}

(cid:46) initialize cluster memberships

(cid:46) recompute cluster centers

(cid:46) reassign instances to nearest clusters

(cid:46) return cluster assignments

a cluster assignment for each instance, and a central (   mean   ) location for each cluster.
id116 iterates between updates to the assignments and the centers:

1. each instance is placed in the cluster with the closest center;

2. each center is recomputed as the average over points in the cluster.

this procedure is formalized in algorithm 8. the term ||x(i)       ||2 refers to the squared
euclidean norm,(cid:80)v
j       j)2. an important property of id116 is that the con-
verged solution depends on the initialization, and a better id91 can sometimes be
found simply by re-running the algorithm from a different random starting point.

j=1(x(i)

soft id116 is a particularly relevant variant.

instead of directly assigning each
point to a speci   c cluster, soft id116 assigns to each point a distribution over clusters
k=1 q(i)(k) = 1, and    k, q(i)(k)     0. the soft weight q(i)(k) is computed from
the distance of x(i) to the cluster center   k. in turn, the center of each cluster is computed
from a weighted average of the points in the cluster,

q(i), so that(cid:80)k

1

i=1 q(i)(k)

q(i)(k)x(i).

n(cid:88)i=1

[5.1]

  k =

(cid:80)n

we will now explore a probablistic version of soft id116 id91, based on expectation-
maximization (em). because em id91 can be derived as an approximation to maximum-
likelihood estimation, it can be extended in a number of useful ways.

under contract with mit press, shared under cc-by-nc-nd license.

98

chapter 5. learning without supervision

5.1.2 expectation-maximization (em)

expectation-maximization combines the idea of soft id116 with na    ve bayes classi   -
cation. to review, na    ve bayes de   nes a id203 distribution over the data,

log p(x, y;   ,   ) =

n(cid:88)i=1

log(cid:16)p(x(i) | y(i);   )    p(y(i);   )(cid:17)

[5.2]

now suppose that you never observe the labels. to indicate this, we   ll refer to the label
of each instance as z(i), rather than y(i), which is usually reserved for observed variables.
by marginalizing over the latent variables z, we obtain the marginal id203 of the
observed instances x:

log p(x;   ,   ) =

=

=

n(cid:88)i=1
n(cid:88)i=1
n(cid:88)i=1

log p(x(i);   ,   )

log

log

k(cid:88)z=1
k(cid:88)z=1

p(x(i), z;   ,   )

p(x(i) | z;   )    p(z;   ).

[5.3]

[5.4]

[5.5]

the parameters    and    can be obtained by maximizing the marginal likelihood in
equation 5.5. why is this the right thing to maximize? without labels, discriminative
learning is impossible     there   s nothing to discriminate. so maximum likelihood is all
we have.

when the labels are observed, we can estimate the parameters of the na    ve bayes
id203 model separately for each label. but marginalizing over the labels couples
these parameters, making direct optimization of log p(x) intractable. we will approxi-
mate the log-likelihood by introducing an auxiliary variable q(i), which is a distribution
over the label set z = {1, 2, . . . , k}. the optimization procedure will alternate between
updates to q and updates to the parameters (  ,   ). thus, q(i) plays here as in soft k-
means.

to derive the updates for this optimization, multiply the right side of equation 5.5 by

jacob eisenstein. draft of november 13, 2018.

5.1. unsupervised learning

99

the ratio q(i)(z)
q(i)(z)

= 1,

log p(x;   ,   ) =

=

log

log

[5.6]

q(i)(z)
q(i)(z)

p(x(i) | z;   )    p(z;   )   

q(i)(z)    p(x(i) | z;   )    p(z;   )   

k(cid:88)z=1
k(cid:88)z=1
log eq(i)(cid:34)p(x(i) | z;   )p(z;   )

n(cid:88)i=1
n(cid:88)i=1
n(cid:88)i=1
z=1 q(i)(z)    f (z) refers to the expectation of the function f under

(cid:35) ,

q(i)(z)

q(i)(z)

[5.7]

[5.8]

=

1

where eq(i) [f (z)] =(cid:80)k
the distribution z     q(i).

jensen   s inequality says that because log is a concave function, we can push it inside

the expectation, and obtain a lower bound.

n(cid:88)i=1
log p(x;   ,   )    
j (cid:44) n(cid:88)i=1
n(cid:88)i=1

=

(cid:35)

q(i)(z)

p(x(i) | z;   )p(z;   )

eq(i)(cid:34)log
eq(i)(cid:104)log p(x(i) | z;   ) + log p(z;   )     log q(i)(z)(cid:105)
eq(i)(cid:104)log p(x(i), z;   ,   )(cid:105) + h(q(i))

[5.9]

[5.10]

[5.11]

we will focus on equation 5.10, which is the lower bound on the marginal log-likelihood
of the observed data, log p(x). equation 5.11 shows the connection to the information
z=1 q(i)(z) log q(i)(z), which measures the av-
erage amount of information produced by a draw from the distribution q(i). the lower
bound j is a function of two groups of arguments:

theoretic concept of id178, h(q(i)) =    (cid:80)k
    the distributions q(i) for each instance;
    the parameters    and   .

the id83 maximizes the bound with respect to each
of these arguments in turn, while holding the other    xed.

the e-step

the step in which we update q(i) is known as the e-step, because it updates the distribu-
tion under which the expectation is computed. to derive this update,    rst write out the

under contract with mit press, shared under cc-by-nc-nd license.

100

chapter 5. learning without supervision

expectation in the lower bound as a sum,

j =

n(cid:88)i=1

k(cid:88)z=1

q(i)(z)(cid:104)log p(x(i) | z;   ) + log p(z;   )     log q(i)(z)(cid:105) .

[5.12]

when optimizing this bound, we must also respect a set of    sum-to-one    constraints,
z=1 q(i)(z) = 1 for all i. just as in na    ve bayes, this constraint can be incorporated into a

(cid:80)k

lagrangian:

jq =

n(cid:88)i=1

k(cid:88)z=1

q(i)(z)(cid:16)log p(x(i) | z;   ) + log p(z;   )     log q(i)(z)(cid:17) +   (i)(1    

k(cid:88)z=1

q(i)(z)),

[5.13]

where   (i) is the lagrange multiplier for instance i.

the lagrangian is maximized by taking the derivative and solving for q(i):

   jq

= log p(x(i) | z;   ) + log p(z;   )     log q(i)(z)     1       (i)

   q(i)(z)
log q(i)(z) = log p(x(i) | z;   ) + log p(z;   )     1       (i)

q(i)(z)    p(x(i) | z;   )    p(z;   ).

applying the sum-to-one constraint gives an exact solution,

q(i)(z) =

p(x(i) | z;   )    p(z;   )
(cid:80)k
z(cid:48)=1 p(x(i) | z(cid:48);   )    p(z(cid:48);   )
=p(z | x(i);   ,   ).

[5.14]

[5.15]
[5.16]

[5.17]

[5.18]

after normalizing, each q(i)     which is the soft distribution over clusters for data x(i)    
is set to the posterior id203 p(z | x(i);   ,   ) under the current parameters. although
the lagrange multipliers   (i) were introduced as additional parameters, they drop out
during id172.

the m-step

next, we hold    xed the soft assignments q(i), and maximize with respect to the pa-
rameters,    and   . let   s focus on the parameter   , which parametrizes the likelihood
p(x | z;   ), and leave    for an exercise. the parameter    is a distribution over words for
each cluster, so it is optimized under the constraint that(cid:80)v
j=1   z,j = 1. to incorporate this

jacob eisenstein. draft of november 13, 2018.

5.1. unsupervised learning

101

constraint, we introduce a set of lagrange multiplers {  z}k

z=1, and from the lagrangian,

j   =

n(cid:88)i=1

k(cid:88)z=1

q(i)(z)(cid:16)log p(x(i) | z;   ) + log p(z;   )     log q(i)(z)(cid:17) +

k(cid:88)z=1

  z(1    

v(cid:88)j=1

  z,j).

[5.19]

the term log p(x(i) | z;   ) is the conditional log-likelihood for the multinomial, which

expands to,

log p(x(i) | z,   ) = c +

xj log   z,j,

[5.20]

v(cid:88)j=1

where c is a constant with respect to        see equation 2.12 in    2.2 for more discussion
of this id203 function.

setting the derivative of j   equal to zero,

   j  
     z,j

=

  z,j    

n(cid:88)i=1
n(cid:88)i=1

q(i)(z)   

x(i)
j
  z,j       z

q(i)(z)    x(i)
j .

[5.21]

[5.22]

because   z is constrained to be a id203 distribution, the exact solution is computed
as,

  z,j = (cid:80)n
i=1 q(i)(z)    x(i)
(cid:80)v
j(cid:48)=1(cid:80)n

i=1 q(i)(z)    x(i)

j(cid:48)

j

=

eq [count(z, j)]
(cid:80)v
j(cid:48)=1 eq [count(z, j(cid:48))]

,

[5.23]

this update sets   z equal to the relative frequency estimate of the expected counts under
the distribution q. as in supervised na    ve bayes, we can smooth these counts by adding
i=1 q(i)(z) = eq [count(z)], which is the
expected frequency of cluster z. these probabilities can also be smoothed. in sum, the
m-step is just like na    ve bayes, but with expected counts rather than observed counts.

where the counter j     {1, 2, . . . , v } indexes over base features, such as words.
a constant   . the update for    is similar:   z    (cid:80)n
the multinomial likelihood p(x | z) can be replaced with other id203 distribu-
tions: for example, for continuous observations, a gaussian distribution can be used. in
some cases, there is no closed-form update to the parameters of the likelihood. one ap-
proach is to run gradient-based optimization at each m-step; another is to simply take a
single step along the gradient step and then return to the e-step (berg-kirkpatrick et al.,
2010).

under contract with mit press, shared under cc-by-nc-nd license.

102

chapter 5. learning without supervision

figure 5.2: sensitivity of expectation-maximization to initialization. each line shows the
progress of optimization from a different random initialization.

5.1.3 em as an optimization algorithm
algorithms that update a global objective by alternating between updates to subsets of the
parameters are called coordinate ascent algorithms. the objective j (the lower bound on
the marginal likelihood of the data) is separately convex in q and (  ,   ), but it is not jointly
convex in all terms; this condition is known as biconvexity. each step of the expectation-
maximization algorithm is guaranteed not to decrease the lower bound j, which means
that em will converge towards a solution at which no nearby points yield further im-
provements. this solution is a local optimum     it is as good or better than any of its
immediate neighbors, but is not guaranteed to be optimal among all possible con   gura-
tions of (q,   ,   ).

the fact that there is no guarantee of global optimality means that initialization is
important: where you start can determine where you    nish. to illustrate this point,
figure 5.2 shows the objective function for em with ten different random initializations:
while the objective function improves monotonically in each run, it converges to several
different values.1 for the convex objectives that we encountered in chapter 2, it was not
necessary to worry about initialization, because gradient-based optimization guaranteed
to reach the global minimum. but in expectation-maximization     as in the deep neural
networks from chapter 3     initialization matters.

in hard em, each q(i) distribution assigns id203 of 1 to a single label   z(i), and zero
id203 to all others (neal and hinton, 1998). this is similar in spirit to id116 clus-
tering, and can outperform standard em in some cases (spitkovsky et al., 2010). another
variant of expectation-maximization incorporates stochastic id119 (sgd): after
performing a local e-step at each instance x(i), we immediately make a gradient update
to the parameters (  ,   ). this algorithm has been called incremental expectation maxi-
mization (neal and hinton, 1998) and online expectation maximization (sato and ishii,

1the    gure shows the upper bound on the negative log-likelihood, because optimization is typically

framed as minimization rather than maximization.

jacob eisenstein. draft of november 13, 2018.

02468iteration430000440000450000negativelog-likelihoodbound5.1. unsupervised learning

103

2000; capp  e and moulines, 2009), and is especially useful when there is no closed-form
optimum for the likelihood p(x | z), and in online settings where new data is constantly
streamed in (see liang and klein, 2009, for a comparison for online em variants).

5.1.4 how many clusters?

so far, we have assumed that the number of clusters k is given. in some cases, this as-
sumption is valid. for example, a lexical semantic resource like id138 might de   ne
the number of senses for a word. in other cases, the number of clusters could be a parame-
ter for the user to tune: some readers want a coarse-grained id91 of news stories into
three or four clusters, while others want a    ne-grained id91 into twenty or more. but
many times there is little extrinsic guidance for how to choose k.

one solution is to choose the number of clusters to maximize a metric of id91
quality. the other parameters    and    are chosen to maximize the log-likelihood bound
j, so this might seem a potential candidate for tuning k. however, j will never decrease
with k: if it is possible to obtain a bound of jk with k clusters, then it is always possible
to do at least as well with k + 1 clusters, by simply ignoring the additional cluster and
setting its id203 to zero in q and   . it is therefore necessary to introduce a penalty
for model complexity, so that fewer clusters are preferred. for example, the akaike infor-
mation crition (aic; akaike, 1974) is the linear combination of the number of parameters
and the log-likelihood,

aic = 2m     2j,

[5.24]

where m is the number of parameters. in an expectation-maximization id91 algo-
rithm, m = k    v + k. since the number of parameters increases with the number of
clusters k, the aic may prefer more parsimonious models, even if they do not    t the data
quite as well.

another choice is to maximize the predictive likelihood on heldout data. this data
is not used to estimate the model parameters    and   , and so it is not the case that the
likelihood on this data is guaranteed to increase with k. figure 5.3 shows the negative
log-likelihood on training and heldout data, as well as the aic.

*bayesian nonparametrics an alternative approach is to treat the number of clusters
as another latent variable. this requires statistical id136 over a set of models with a
variable number of clusters. this is not possible within the framework of expectation-
maximization, but there are several alternative id136 procedures which can be ap-
plied, including id115 (mcmc), which is brie   y discussed in
   5.5 (for more details, see chapter 25 of murphy, 2012). bayesian nonparametrics have
been applied to the problem of unsupervised word sense induction, learning not only the
word senses but also the number of senses per word (reisinger and mooney, 2010).

under contract with mit press, shared under cc-by-nc-nd license.

104

chapter 5. learning without supervision

figure 5.3: the negative log-likelihood and aic for several runs of expectation-
maximization, on synthetic data. although the data was generated from a model with
k = 10, the optimal number of clusters is   k = 15, according to aic and the heldout
log-likelihood. the training set log-likelihood continues to improve as k increases.

5.2 applications of expectation-maximization

em is not really an    algorithm    like, say, quicksort. rather, it is a framework for learning
with missing data. the recipe for using em on a problem of interest is:

should also be easy to estimate the associated parameters, given knowledge of z.

    introduce latent variables z, such that it is easy to write the id203 p (x, z). it
    derive the e-step updates for q(z), which is typically factored as q(z) =(cid:81)n
    the m-step updates typically correspond to the soft version of a probabilistic super-

where i is an index over instances.

vised learning algorithm, like na    ve bayes.

i=1 qz(i)(z(i)),

this section discusses a few of the many applications of this general framework.

5.2.1 word sense induction
the chapter began by considering the problem of id51 when the
senses are not known in advance. expectation-maximization can be applied to this prob-
lem by treating each cluster as a word sense. each instance represents the use of an
ambiguous word, and x(i) is a vector of counts for the other words that appear nearby:
sch   utze (1998) uses all words within a 50-word window. the id203 p(x(i) | z) can be
set to the multinomial distribution, as in na    ve bayes. the em algorithm can be applied
directly to this data, yielding clusters that (hopefully) correspond to the word senses.

better performance can be obtained by    rst applying singular value decomposition
(svd) to the matrix of context-counts cij = count(i, j), where count(i, j) is the count of
word j in the context of instance i. truncated singular value decomposition approximates

jacob eisenstein. draft of november 13, 2018.

1020304050numberofclusters220000240000260000negativelog-likelihoodboundaic1020304050numberofclusters750008000085000out-of-samplenegativeloglikelihood5.2. applications of expectation-maximization

105

the matrix c as a product of three matrices, u, s, v, under the constraint that u and v
are orthonormal, and s is diagonal:

[5.25]

(cid:62)

u,s,v||c     usv
||f
min
s.t.u     rv   k, uu
    rnp  k, vv

s = diag(s1, s2, . . . , sk)
= i,
v

= i

(cid:62)

(cid:62)

(cid:62)

where ||    ||f is the frobenius norm, ||x||f = (cid:113)(cid:80)i,j x 2

i,j. the matrix u contains the
left singular vectors of c, and the rows of this matrix can be used as low-dimensional
representations of the count vectors ci. em id91 can be made more robust by setting
the instance descriptions x(i) equal to these rows, rather than using raw counts (sch   utze,
1998). however, because the instances are now dense vectors of continuous numbers, the
id203 p(x(i) | z) must be de   ned as a multivariate gaussian distribution.

in truncated singular value decomposition, the hyperparameter k is the truncation
limit: when k is equal to the rank of c, the norm of the difference between the original
matrix c and its reconstruction usv(cid:62) will be zero. lower values of k increase the recon-
struction error, but yield vector representations that are smaller and easier to learn from.
singular value decomposition is discussed in more detail in chapter 14.

5.2.2 semi-supervised learning
expectation-maximization can also be applied to the problem of semi-supervised learn-
ing: learning from both labeled and unlabeled data in a single model. semi-supervised
learning makes use of annotated examples, ensuring that each label y corresponds to the
desired concept. by adding unlabeled examples, it is possible to cover a greater fraction of
the features than would appear in labeled data alone. other methods for semi-supervised
learning are discussed in    5.3, but for now, let   s approach the problem within the frame-
work of expectation-maximization (nigam et al., 2000).

suppose we have labeled data {(x(i), y(i))}n(cid:96)

i=n(cid:96)+1, where
n(cid:96) is the number of labeled instances and nu is the number of unlabeled instances. we can
learn from the combined data by maximizing a lower bound on the joint log-likelihood,

i=1, and unlabeled data {x(i)}n(cid:96)+nu

l =

=

log p(x(i), y(i);   ,   ) +

n(cid:96)(cid:88)i=1
n(cid:96)(cid:88)i=1(cid:16)log p(x(i) | y(i);   ) + log p(y(i);   )(cid:17) +

n(cid:96)+nu(cid:88)j=n(cid:96)+1

log p(x(j);   ,   )

n(cid:96)+nu(cid:88)j=n(cid:96)+1

log

k(cid:88)y=1

p(x(j), y;   ,   ).

[5.27]

[5.26]

under contract with mit press, shared under cc-by-nc-nd license.

106

chapter 5. learning without supervision

algorithm 9 generative process for the na    ve bayes classi   er with hidden components

for instance i     {1, 2, . . . , n} do:

draw the label y(i)     categorical(  );
draw the component z(i)     categorical(  y(i));
draw the word counts x(i) | y(i), z(i)     multinomial(  z(i)).

the left sum is identical to the objective in na    ve bayes; the right sum is the marginal log-
likelihood for expectation-maximization id91, from equation 5.5. we can construct a
lower bound on this log-likelihood by introducing distributions q(j) for all j     {n(cid:96) + 1, . . . , n(cid:96) + nu}.
the e-step updates these distributions; the m-step updates the parameters    and   , us-
ing the expected counts from the unlabeled data and the observed counts from the labeled
data.

a critical issue in semi-supervised learning is how to balance the impact of the labeled
and unlabeled data on the classi   er weights, especially when the unlabeled data is much
larger than the labeled dataset. the risk is that the unlabeled data will dominate, caus-
ing the parameters to drift towards a    natural id91    of the instances     which may
not correspond to a good classi   er for the labeled data. one solution is to heuristically
reweight the two components of equation 5.26, tuning the weight of the two components
on a heldout development set (nigam et al., 2000).

5.2.3 multi-component modeling

as a    nal application, let   s return to fully supervised classi   cation. a classic dataset for
text classi   cation is 20 newsgroups, which contains posts to a set of online forums, called
newsgroups. one of the newsgroups is comp.sys.mac.hardware, which discusses ap-
ple computing hardware. suppose that within this newsgroup there are two kinds of
posts: reviews of new hardware, and question-answer posts about hardware problems.
the language in these components of the mac.hardware class might have little in com-
mon; if so, it would be better to model these components separately, rather than treating
their union as a single class. however, the component responsible for each instance is not
directly observed.

recall that na    ve bayes is based on a generative process, which provides a stochastic
explanation for the observed data. in na    ve bayes, each label is drawn from a categorical
distribution with parameter   , and each vector of word counts is drawn from a multi-
nomial distribution with parameter   y. for multi-component modeling, we envision a
slightly different generative process, incorporating both the observed label y(i) and the
latent component z(i). this generative process is shown in algorithm 9. a new parameter
  y(i) de   nes the distribution of components, conditioned on the label y(i). the component,
and not the class label, then parametrizes the distribution over words.

jacob eisenstein. draft of november 13, 2018.

5.3. semi-supervised learning

107

(5.1) (cid:44)
(5.2) (cid:47)

villeneuve a bel et bien r  eussi son pari de changer de perspectives tout en assurant

une coh  erence `a la franchise.2

il est   egalement trop long et bancal dans sa narration, ti`ede dans ses intentions, et
tiraill  e entre deux personnages et directions qui ne parviennent pas `a coexister en har-
monie.3

(5.3) denis villeneuve a r  eussi une suite parfaitement maitris  ee4
(5.4) long, bavard, hyper design, `a peine agit  e (le comble de l   action : une bagarre dans la

   otte), m  etaphysique et, surtout, ennuyeux jusqu   `a la catalepsie.5

(5.5) une suite d   une   ecrasante puissance, m  elant parfaitement le contemplatif au narratif.6
(5.6) le    lm impitoyablement bavard    nit quand m  eme par se taire quand se l`eve l   esp`ece
de bouquet    nal o `u semble se d  echa    ner, comme en libre parcours de poulets d  ecapit  es,
l   arm  ee des graphistes num  eriques griffant nerveusement la palette graphique entre ag-
onie et orgasme.7

table 5.1: labeled and unlabeled reviews of the    lms blade runner 2049 and transformers:
the last knight.

the labeled data includes (x(i), y(i)), but not z(i), so this is another case of missing
data. again, we sum over the missing data, applying jensen   s inequality to as to obtain a
lower bound on the log-likelihood,

kz(cid:88)z=1

log p(x(i), y(i)) = log

p(x(i), y(i), z;   ,   ,   )

[5.28]

    log p(y(i);   ) + e

q(i)
z|y

[log p(x(i) | z;   ) + log p(z | y(i);   )     log q(i)(z)].
[5.29]

we are now ready to apply expectation-maximization. as usual, the e-step updates

the distribution over the missing data, q(i)

  y,z =

  z,j =

z|y . the m-step updates the parameters,
eq [count(y, z)]
(cid:80)kz
z(cid:48)=1 eq [count(y, z(cid:48))]
eq [count(z, j)]
(cid:80)v
j(cid:48)=1 eq [count(z, j(cid:48))]

.

[5.30]

[5.31]

5.3 semi-supervised learning

in semi-supervised learning, the learner makes use of both labeled and unlabeled data.
to see how this could help, suppose you want to do id31 in french. in ta-

under contract with mit press, shared under cc-by-nc-nd license.

108

chapter 5. learning without supervision

ble 5.1, there are two labeled examples, one positive and one negative. from this data, a
learner could conclude that r  eussi is positive and long is negative. this isn   t much! how-
ever, we can propagate this information to the unlabeled data, and potentially learn more.

tive.

    if we are con   dent that r  eussi is positive, then we might guess that (5.3) is also posi-
    that suggests that parfaitement is also positive.
    we can then propagate this information to (5.5), and learn from the words in this
    similarly, we can propagate from the labeled data to (5.4), which we guess to be
negative because it shares the word long. this suggests that bavard is also negative,
which we propagate to (5.6).

example.

instances (5.3) and (5.4) were    similar    to the labeled examples for positivity and negativ-
ity, respectively. by using these instances to expand the models for each class, it became
possible to correctly label instances (5.5) and (5.6), which didn   t share any important fea-
tures with the original labeled data. this requires a key assumption: that similar instances
will have similar labels.

in    5.2.2, we discussed how expectation-maximization can be applied to semi-supervised

learning. using the labeled data, the initial parameters    would assign a high weight for
r  eussi in the positive class, and a high weight for long in the negative class. these weights
helped to shape the distributions q for instances (5.3) and (5.4) in the e-step. in the next
iteration of the m-step, the parameters    are updated with counts from these instances,
making it possible to correctly label the instances (5.5) and (5.6).

however, expectation-maximization has an important disadvantage: it requires using
a generative classi   cation model, which restricts the features that can be used for clas-
si   cation. in this section, we explore non-probabilistic approaches, which impose fewer
restrictions on the classi   cation model.

5.3.1 multi-view learning
em semi-supervised learning can be viewed as self-training: the labeled data guides the
initial estimates of the classi   cation parameters; these parameters are used to compute
a label distribution over the unlabeled instances, q(i); the label distributions are used to
update the parameters. the risk is that self-training drifts away from the original labeled
data. this problem can be ameliorated by multi-view learning. here we take the as-
sumption that the features can be decomposed into multiple    views   , each of which is
conditionally independent, given the label. for example, consider the problem of classi-
fying a name as a person or location: one view is the name itself; another is the context in
which it appears. this situation is illustrated in table 5.2.

jacob eisenstein. draft of november 13, 2018.

5.3. semi-supervised learning

109

x(1)

1. peachtree street
2. dr. walker
3. zanzibar
4. zanzibar
5. dr. robert
6. oprah

x(2)
y
loc
located on
per
said
?     loc
located in
?     loc
   ew to
recommended ?     per
recommended ?     per

table 5.2: example of multiview learning for named entity classi   cation

co-training is an iterative multi-view learning algorithm, in which there are separate
classi   ers for each view (blum and mitchell, 1998). at each iteration of the algorithm, each
classi   er predicts labels for a subset of the unlabeled instances, using only the features
available in its view. these predictions are then used as ground truth to train the classi   ers
associated with the other views. in the example shown in table 5.2, the classi   er on x(1)
might correctly label instance #5 as a person, because of the feature dr; this instance would
then serve as training data for the classi   er on x(2), which would then be able to correctly
label instance #6, thanks to the feature recommended. if the views are truly independent,
this procedure is robust to drift. furthermore, it imposes no restrictions on the classi   ers
that can be used for each view.

word-sense disambiguation is particularly suited to multi-view learning, thanks to the
heuristic of    one sense per discourse   : if a polysemous word is used more than once in
a given text or conversation, all usages refer to the same sense (gale et al., 1992). this
motivates a multi-view learning approach, in which one view corresponds to the local
context (the surrounding words), and another view corresponds to the global context at
the document level (yarowsky, 1995). the local context view is    rst trained on a small
seed dataset. we then identify its most con   dent predictions on unlabeled instances. the
global context view is then used to extend these con   dent predictions to other instances
within the same documents. these new instances are added to the training data to the
local context classi   er, which is retrained and then applied to the remaining unlabeled
data.

5.3.2 graph-based algorithms
another family of approaches to semi-supervised learning begins by constructing a graph,
in which pairs of instances are linked with symmetric weights   i,j, e.g.,

  i,j = exp(         ||x(i)     x(j)||2).

[5.32]

the goal is to use this weighted graph to propagate labels from a small set of labeled
instances to larger set of unlabeled instances.

under contract with mit press, shared under cc-by-nc-nd license.

110

chapter 5. learning without supervision

in label propagation, this is done through a series of matrix operations (zhu et al.,
2003). let q be a matrix of size n    k, in which each row q(i) describes the labeling
of instance i. when ground truth labels are available, then q(i) is an indicator vector,
with q(i)
y(cid:48)(cid:54)=y(i) = 0. let us refer to the submatrix of rows containing labeled
instances as ql, and the remaining rows as qu . the rows of qu are initialized to assign
equal probabilities to all labels, qi,k = 1
k .

y(i) = 1 and q(i)

now, let ti,j represent the    transition    id203 of moving from node j to node i,

ti,j (cid:44) pr(j     i) =

  i,j
k=1   k,j

(cid:80)n

.

[5.33]

we compute values of ti,j for all instances j and all unlabeled instances i, forming a matrix
of size nu    n. if the dataset is large, this matrix may be expensive to store and manip-
ulate; a solution is to sparsify it, by keeping only the    largest values in each row, and
setting all other values to zero. we can then    propagate    the label distributions to the
unlabeled instances,

  qu    tq
s       qu 1
qu    diag(s)

   1   qu .

[5.34]
[5.35]
[5.36]

the expression   qu 1 indicates multiplication of   qu by a column vector of ones, which is
equivalent to computing the sum of each row of   qu . the matrix diag(s) is a diagonal
matrix with the elements of s on the diagonals. the product diag(s)   1   qu has the effect
of normalizing the rows of   qu , so that each row of qu is a id203 distribution over
labels.

5.4 id20

in many practical scenarios, the labeled data differs in some key respect from the data
to which the trained model is to be applied. a classic example is in consumer reviews:
we may have labeled reviews of movies (the source domain), but we want to predict the
reviews of appliances (the target domain). a similar issue arises with genre differences:
most linguistically-annotated data is news text, but application domains range from social
media to electronic health records.
in general, there may be several source and target
domains, each with their own properties; however, for simplicity, this discussion will
focus mainly on the case of a single source and target domain.

the simplest approach is    direct transfer   : train a classi   er on the source domain, and
apply it directly to the target domain. the accuracy of this approach depends on the extent
to which features are shared across domains. in review text, words like outstanding and

jacob eisenstein. draft of november 13, 2018.

5.4. id20

111

disappointing will apply across both movies and appliances; but others, like terrifying, may
have meanings that are domain-speci   c. as a result, direct transfer performs poorly: for
example, an out-of-domain classi   er (trained on book reviews) suffers twice the error rate
of an in-domain classi   er on reviews of kitchen appliances (blitzer et al., 2007). domain
adaptation algorithms attempt to do better than direct transfer by learning from data in
both domains. there are two main families of id20 algorithms, depending
on whether any labeled data is available in the target domain.

5.4.1 supervised id20

in supervised id20, there is a small amount of labeled data in the target
domain, and a large amount of data in the source domain. the simplest approach would
be to ignore domain differences, and simply merge the training data from the source and
target domains. there are several other baseline approaches to dealing with this sce-
nario (daum  e iii, 2007):

interpolation. train a classi   er for each domain, and combine their predictions, e.g.,

  y = argmax

y

  s  s(x, y) + (1       s)  t(x, y),

[5.37]

where   s and   t are the scoring functions from the source and target domain clas-
si   ers respectively, and   s is the interpolation weight.

prediction. train a classi   er on the source domain data, use its prediction as an additional

feature in a classi   er trained on the target domain data,

  ys = argmax

y

  s(x, y)

  yt = argmax

y

  t([x;   ys], y).

[5.38]

[5.39]

priors. train a classi   er on the source domain data, and use its weights as a prior distri-
bution on the weights of the classi   er for the target domain data. this is equivalent
to regularizing the target domain weights towards the weights of the source domain
classi   er (chelba and acero, 2006),

(cid:96)(  t) =

n(cid:88)i=1

(cid:96)(i)(x(i), y(i);   t) +   ||  t       s||2
2,

[5.40]

where (cid:96)(i) is the prediction loss on instance i, and    is the id173 weight.

under contract with mit press, shared under cc-by-nc-nd license.

112

chapter 5. learning without supervision

an effective and    frustratingly simple    alternative is easyadapt (daum  e iii, 2007),
which creates copies of each feature: one for each domain and one for the cross-domain
setting. for example, a negative review of the    lm wonder woman begins, as boring and
   avorless as a three-day-old grilled cheese sandwich. . . .8 the resulting bag-of-words feature
vector would be,

f (x, y, d) = {(boring,(cid:47), movie) : 1, (boring,(cid:47),   ) : 1,

(   avorless,(cid:47), movie) : 1, (   avorless,(cid:47),   ) : 1,
(three-day-old,(cid:47), movie) : 1, (three-day-old,(cid:47),   ) : 1,
. . .},

with (boring,(cid:47), movie) indicating the word boring appearing in a negative labeled doc-
ument in the movie domain, and (boring,(cid:47),   ) indicating the same word in a negative

labeled document in any domain. it is up to the learner to allocate weight between the
domain-speci   c and cross-domain features: for words that facilitate prediction in both
domains, the learner will use the cross-domain features; for words that are relevant only
to a single domain, the domain-speci   c features will be used. any discriminative classi-
   er can be used with these augmented features.9

5.4.2 unsupervised id20

in unsupervised id20, there is no labeled data in the target domain. un-
supervised id20 algorithms cope with this problem by trying to make the
data from the source and target domains as similar as possible. this is typically done by
learning a projection function, which puts the source and target data in a shared space,
in which a learner can generalize across domains. this projection is learned from data in
both domains, and is applied to the base features     for example, the bag-of-words in text
classi   cation. the projected features can then be used both for training and for prediction.

linear projection

in linear projection, the cross-domain representation is constructed by a matrix-vector
product,

g(x(i)) = ux(i).

[5.41]

the projected vectors g(x(i)) can then be used as base features during both training (from
the source domain) and prediction (on the target domain).

8http://www.colesmithey.com/capsules/2017/06/wonder-woman.html, accessed october 9.

2017.

9easyadapt can be explained as a hierarchical bayesian model, in which the weights for each domain

are drawn from a shared prior (finkel and manning, 2009).

jacob eisenstein. draft of november 13, 2018.

5.4. id20

113

the projection matrix u can be learned in a number of different ways, but many ap-
proaches focus on compressing and reconstructing the base features (ando and zhang,
2005). for example, we can de   ne a set of pivot features, which are typically chosen be-
cause they appear in both domains: in the case of review documents, pivot features might
include evaluative adjectives like outstanding and disappointing (blitzer et al., 2007). for
each pivot feature j, we de   ne an auxiliary problem of predicting whether the feature is
present in each example, using the remaining base features. let   j denote the weights of
this classi   er, and us horizontally concatenate the weights for each of the np pivot features
into a matrix    = [  1,   2, . . . ,   np ].
we then perform truncated singular value decomposition on   , as described in    5.2.1,
obtaining        usv(cid:62). the rows of the matrix u summarize information about each base
feature: indeed, the truncated singular value decomposition identi   es a low-dimension
basis for the weight matrix   , which in turn links base features to pivot features. sup-
pose that a base feature reliable occurs only in the target domain of appliance reviews.
nonetheless, it will have a positive weight towards some pivot features (e.g., outstanding,
recommended), and a negative weight towards others (e.g., worthless, unpleasant). a base
feature such as watchable might have the same associations with the pivot features, and
therefore, ureliable     uwatchable. the matrix u can thus project the base features into a
space in which this information is shared.

non-linear projection

non-linear transformations of the base features can be accomplished by implementing
the transformation function as a deep neural network, which is trained from an auxiliary
objective.

denoising objectives one possibility is to train a projection function to reconstruct a
corrupted version of the original input. the original input can be corrupted in various
ways: by the addition of random noise (glorot et al., 2011; chen et al., 2012), or by the
deletion of features (chen et al., 2012; yang and eisenstein, 2015). denoising objectives
share many properties of the linear projection method described above: they enable the
projection function to be trained on large amounts of unlabeled data from the target do-
main, and allow information to be shared across the feature space, thereby reducing sen-
sitivity to rare and domain-speci   c features.

adversarial objectives the ultimate goal is for the transformed representations g(x(i))
to be domain-general. this can be made an explicit optimization criterion by comput-
ing the similarity of transformed instances both within and between domains (tzeng
et al., 2015), or by formulating an auxiliary classi   cation task, in which the domain it-
self is treated as a label (ganin et al., 2016). this setting is adversarial, because we want

under contract with mit press, shared under cc-by-nc-nd license.

114

chapter 5. learning without supervision

figure 5.4: a schematic view of adversarial id20. the loss (cid:96)y is computed
only for instances from the source domain, where labels y(i) are available.

to learn a representation that makes this classi   er perform poorly. at the same time, we
want g(x(i)) to enable accurate predictions of the labels y(i).

to formalize this idea, let d(i) represent the domain of instance i, and let (cid:96)d(g(x(i)), d(i);   d)

represent the loss of a classi   er (typically a deep neural network) trained to predict d(i)
from the transformed representation g(x(i)), using parameters   d. analogously, let (cid:96)y(g(x(i)), y(i);   y)
represent the loss of a classi   er trained to predict the label y(i) from g(x(i)), using param-
eters   y. the transformation g can then be trained from two criteria: it should yield accu-
rate predictions of the labels y(i), while making inaccurate predictions of the domains d(i).
this can be formulated as a joint optimization problem,

min
  g  y,  d

n(cid:96)+nu(cid:88)i=1

(cid:96)d(g(x(i);   g), d(i);   d)    

n(cid:96)(cid:88)i=1

(cid:96)y(g(x(i);   g), y(i);   y),

[5.42]

where n(cid:96) is the number of labeled instances and nu is the number of unlabeled instances,
with the labeled instances appearing    rst in the dataset. this setup is shown in figure 5.4.
the loss can be optimized by stochastic id119, jointly training the parameters
of the non-linear transformation   g, and the parameters of the prediction models   d and
  y.

5.5

*other approaches to learning with latent variables

expectation-maximization provides a general approach to learning with latent variables,
but it has limitations. one is the sensitivity to initialization; in practical applications,
considerable attention may need to be devoted to    nding a good initialization. a second
issue is that em tends to be easiest to apply in cases where the latent variables have a clear
decomposition (in the cases we have considered, they decompose across the instances).
for these reasons, it is worth brie   y considering some alternatives to em.

jacob eisenstein. draft of november 13, 2018.

   d   yd(i)y(i)xg(x)5.5. *other approaches to learning with latent variables

115

5.5.1 sampling
in em id91, there is a distribution q(i) for the missing data related to each instance.
the m-step consists of updating the parameters of this distribution. an alternative is to
draw samples of the latent variables. if the sampling distribution is designed correctly,
this procedure will eventually converge to drawing samples from the true posterior over
the missing data, p(z(1:nz) | x(1:nx)). for example, in the case of id91, the missing
data z(1:nz) is the set of cluster memberships, y(1:n ), so we draw samples from the pos-
terior distribution over id91s of the data. if a single id91 is required, we can
select the one with the highest conditional likelihood,   z = argmaxz p(z(1:nz) | x(1:nx)).

this general family of algorithms is called id115 (mcmc):
   monte carlo    because it is based on a series of random draws;    markov chain    because
the sampling procedure must be designed such that each sample depends only on the
previous sample, and not on the entire sampling history. id150 is an mcmc
algorithm in which each latent variable is sampled from its posterior distribution,

z(n) | x, z(   n)     p(z(n) | x, z(   n)),

[5.43]
where z(   n) indicates {z\z(n)}, the set of all latent variables except for z(n). repeatedly
drawing samples over all latent variables constructs a markov chain that is guaranteed
to converge to a sequence of samples from p(z(1:nz) | x(1:nx)). in probabilistic id91,
the sampling distribution has the following form,

p(z(i) | x, z(   i)) =

p(x(i) | z(i);   )    p(z(i);   )
(cid:80)k
z=1 p(x(i) | z;   )    p(z;   )
   multinomial(x(i);   z(i))      z(i).

[5.44]

[5.45]

in this case, the sampling distribution does not depend on the other instances: the poste-
rior distribution over each z(i) can be computed from x(i) and the parameters given the
parameters    and   .

in sampling algorithms, there are several choices for how to deal with the parameters.
one possibility is to sample them too. to do this, we must add them to the generative
story, by introducing a prior distribution. for the multinomial and categorical parameters
in the em id91 model, the dirichlet distribution is a typical choice, since it de   nes
a id203 on exactly the set of vectors that can be parameters: vectors that sum to one
and include only non-negative numbers.10

to incorporate this prior, the generative model must be augmented to indicate that
each   z     dirichlet(    ), and        dirichlet(    ). the hyperparameters    are typically set
10if(cid:80)k
to a constant vector    = [  ,   , . . . ,   ]. when    is large, the dirichlet distribution tends to
i   i = 1 and   i     0 for all i, then    is said to be on the k     1 simplex. a dirichlet distribution with

under contract with mit press, shared under cc-by-nc-nd license.

116

chapter 5. learning without supervision

generate vectors that are nearly uniform; when    is small, it tends to generate vectors that
assign most of their id203 mass to a few entries. given prior distributions over   
and   , we can now include them in id150, drawing values for these parameters
from posterior distributions that are conditioned on the other variables in the model.

unfortunately, sampling    and    usually leads to slow    mixing   , meaning that adja-
cent samples tend to be similar, so that a large number of samples is required to explore
the space of random variables. the reason is that the sampling distributions for the pa-
rameters are tightly constrained by the cluster memberships y(i), which in turn are tightly
constrained by the parameters. there are two solutions that are frequently employed:

    empirical bayesian methods maintain    and    as parameters rather than latent
variables. they still employ sampling in the e-step of the em algorithm, but they
update the parameters using expected counts that are computed from the samples
rather than from parametric distributions. this em-mcmc hybrid is also known
as monte carlo expectation maximization (mcem; wei and tanner, 1990), and is
well-suited for cases in which it is dif   cult to compute q(i) directly.

    in collapsed id150, we analytically integrate    and    out of the model.
the cluster memberships y(i) are the only remaining latent variable; we sample them
from the compound distribution,

p(y(i) | x(1:n ), y(   i);     ,     ) =(cid:90)  ,  

p(  ,    | y(   i), x(1:n );     ,     )p(y(i) | x(1:n ), y(   i),   ,   )d  d  .

[5.48]
for multinomial and dirichlet distributions, this integral can be computed in closed
form.

mcmc algorithms are guaranteed to converge to the true posterior distribution over
the latent variables, but there is no way to know how long this will take. in practice, the
rate of convergence depends on initialization, just as expectation-maximization depends
on initialization to avoid local optima. thus, while id150 and other mcmc
algorithms provide a powerful and    exible array of techniques for statistical id136 in
latent variable models, they are not a panacea for the problems experienced by em.
parameter        rk

+ has support over the k     1 simplex,
k(cid:89)
(cid:81)k
  ((cid:80)k

pdirichlet(   |   ) =

b(  ) =

b(  )

i=1

1

i=1   (  i)
i=1   i)

,

    i   1
i

[5.46]

[5.47]

with   (  ) indicating the gamma function, a generalization of the factorial function to non-negative reals.

jacob eisenstein. draft of november 13, 2018.

5.5. *other approaches to learning with latent variables

117

5.5.2 spectral learning
another approach to learning with latent variables is based on the method of moments,
which makes it possible to avoid the problem of non-convex log-likelihood. write x(i) for
j . then

the normalized vector of word counts in document i, so that x(i) = x(i)/(cid:80)v

we can form a matrix of word-word co-occurrence probabilities,

j=1 x(i)

x(i)(x(i))

(cid:62)

.

c =

n(cid:88)i=1

the expected value of this matrix under p(x |   ,   ), as

pr(z(i) = k;   )  k  

(cid:62)
k

e[c] =

=

k(cid:88)k=1

n(cid:88)i=1
k(cid:88)k

n   k  k  

=  diag(n   )  

(cid:62)
k

(cid:62)

,

[5.49]

[5.50]

[5.51]

[5.52]

where    is formed by horizontally concatenating   1 . . .   k, and diag(n   ) indicates a
diagonal matrix with values n   k at position (k, k). setting c equal to its expectation
gives,

(cid:62)
c =  diag(n   )  

[5.53]
which is similar to the eigendecomposition c = q  q(cid:62). this suggests that simply by
   nding the eigenvectors and eigenvalues of c, we could obtain the parameters    and   ,
and this is what motivates the name spectral learning.

,

while moment-matching and eigendecomposition are similar in form, they impose
different constraints on the solutions: eigendecomposition requires orthonormality, so
that qq(cid:62) = i; in estimating the parameters of a text id91 model, we require that   
and the columns of    are id203 vectors. spectral learning algorithms must therefore
include a procedure for converting the solution into vectors that are non-negative and
sum to one. one approach is to replace eigendecomposition (or the related singular value
decomposition) with non-negative id105 (xu et al., 2003), which guarantees
that the solutions are non-negative (arora et al., 2013).

after obtaining the parameters    and   , the distribution over clusters can be com-

puted from bayes    rule:

p(z(i) | x(i);   ,   )     p(x(i) | z(i);   )    p(z(i);   ).
under contract with mit press, shared under cc-by-nc-nd license.

[5.54]

118

chapter 5. learning without supervision

spectral learning yields provably good solutions without regard to initialization, and can
be quite fast in practice. however, it is more dif   cult to apply to a broad family of genera-
tive models than em and id150. for more on applying spectral learning across
a range of latent variable models, see anandkumar et al. (2014).

additional resources

there are a number of other learning paradigms that deviate from supervised learning.

    active learning: the learner selects unlabeled instances and requests annotations (set-

tles, 2012).

    multiple instance learning: labels are applied to bags of instances, with a positive
label applied if at least one instance in the bag meets the criterion (dietterich et al.,
1997; maron and lozano-p  erez, 1998).

    constraint-driven learning: supervision is provided in the form of explicit con-

straints on the learner (chang et al., 2007; ganchev et al., 2010).

    distant supervision: noisy labels are generated from an external resource (mintz

et al., 2009, also see    17.2.3).

    multitask learning: the learner induces a representation that can be used to solve

multiple classi   cation tasks (collobert et al., 2011).

    id21: the learner must solve a classi   cation task that differs from the

labeled data (pan and yang, 2010).

expectation-maximization was introduced by dempster et al. (1977), and is discussed
in more detail by murphy (2012). like most machine learning treatments, murphy focuses
on continuous observations and gaussian likelihoods, rather than the discrete observa-
tions typically encountered in natural language processing. murphy (2012) also includes
an excellent chapter on mcmc; for a textbook-length treatment, see robert and casella
(2013). for still more on bayesian latent variable models, see barber (2012), and for ap-
plications of bayesian models to natural language processing, see cohen (2016). surveys
are available for semi-supervised learning (zhu and goldberg, 2009) and domain adapta-
tion (s  gaard, 2013), although both pre-date the current wave of interest in deep learning.

exercises

1. derive the expectation maximization update for the parameter    in the em cluster-

ing model.

jacob eisenstein. draft of november 13, 2018.

5.5. *other approaches to learning with latent variables

119

2. derive the e-step and m-step updates for the following generative model. you may

assume that the labels y(i) are observed, but z(i)

m is not.

    for each instance i,

    draw label y(i)     categorical(  )
    for each token m     {1, 2, . . . , m (i)}
m     categorical(  )

    draw z(i)
    if z(i)
m = 0, draw the current token from a label-speci   c distribution,
w(i)
m       y(i)
    if z(i)
m = 1, draw the current token from a document-speci   c distribu-
tion, w(i)

m       (i)

3. using the iterative updates in equations 5.34-5.36, compute the outcome of the label

propagation algorithm for the following examples.

?

0

1

?

?

1

0

?

0

?

1

?

the value inside the node indicates the label, y(i)     {0, 1}, with y(i) =? for unlabeled
nodes. the presence of an edge between two nodes indicates wi,j = 1, and the
absence of an edge indicates wi,j = 0. for the third example, you need only compute
the    rst three iterations, and then you can guess at the solution in the limit.

4. use expectation-maximization id91 to train a word-sense induction system,

applied to the word say.

from nltk.corpus.

    import nltk, run nltk.download() and select semcor.
    the command semcor.tagged sentences(tag=   sense   ) returns an itera-
tor over sense-tagged sentences in the corpus. each sentence can be viewed
as an iterator over tree objects. for tree objects that are sense-annotated
words, you can access the annotation as tree.label(), and the word itself with
tree.leaves(). so semcor.tagged sentences(tag=   sense   )[0][2].label()
would return the sense annotation of the third word in the    rst sentence.

import semcor

    extract all sentences containing the senses say.v.01 and say.v.02.
    build bag-of-words vectors x(i), containing the counts of other words in those
sentences, including all words that occur in at least two sentences.
    implement and run expectation-maximization id91 on the merged data.
under contract with mit press, shared under cc-by-nc-nd license.

120

chapter 5. learning without supervision

    compute the frequency with which each cluster includes instances of say.v.01

and say.v.02.

in the remaining exercises, you will try out some approaches for semisupervised learn-
ing and id20. you will need datasets in multiple domains. you can obtain
product reviews in multiple domains here: https://www.cs.jhu.edu/  mdredze/
datasets/sentiment/processed_acl.tar.gz. choose a source and target domain,
e.g. dvds and books, and divide the data for the target domain into training and test sets
of equal size.

5. first, quantify the cost of cross-domain transfer.

uate it on the target domain test set.

    train a id28 classi   er on the source domain training set, and eval-
    train a id28 classi   er on the target domain training set, and eval-

uate it on the target domain test set. this it the    direct transfer    baseline.

compute the difference in accuracy, which is a measure of the transfer loss across
domains.

6. next, apply the label propagation algorithm from    5.3.2.

as a baseline, using only 5% of the target domain training set, train a classi   er, and
compute its accuracy on the target domain test set.
next, apply label propagation:

    compute the label matrix ql for the labeled data (5% of the target domain
training set), with each row equal to an indicator vector for the label (positive
or negative).

    iterate through the target domain instances, including both test and training
data. at each instance i, compute all wij, using equation 5.32, with    = 0.01.
use these values to    ll in column i of the transition matrix t, setting all but the
ten largest values to zero for each column i. be sure to normalize the column
so that the remaining values sum to one. you may need to use a sparse matrix
for this to    t into memory.

    apply the iterative updates from equations 5.34-5.36 to compute the outcome

of the label propagation algorithm for the unlabeled examples.

select the test set instances from qu , and compute the accuracy of this method.
compare with the supervised classi   er trained only on the 5% sample of the target
domain training set.

jacob eisenstein. draft of november 13, 2018.

5.5. *other approaches to learning with latent variables

121

7. using only 5% of the target domain training data (and all of the source domain train-
ing data), implement one of the supervised id20 baselines in    5.4.1.
see if this improves on the    direct transfer    baseline from the previous problem

8. implement easyadapt (   5.4.1), again using 5% of the target domain training data

and all of the source domain data.

9. now try unsupervised id20, using the    linear projection    method

described in    5.4.2. speci   cally:

    identify 500 pivot features as the words with the highest frequency in the (com-
plete) training data for the source and target domains. speci   cally, let xd
i be the
count of the word i in domain d: choose the 500 words with the largest values
of min(xsource

).

target
, x
i

i

document.

    train a classi   er to predict each pivot feature from the remaining words in the
    arrange the features of these classi   ers into a matrix   , and perform truncated
singular value decomposition, with k = 20
    train a classi   er from the source domain data, using the combined features
x(i)     u(cid:62)x(i)     these include the original bag-of-words features, plus the pro-
jected features.
    apply this classi   er to the target domain test set, and compute the accuracy.

under contract with mit press, shared under cc-by-nc-nd license.

part ii

sequences and trees

123

chapter 6

language models

in probabilistic classi   cation, the problem is to compute the id203 of a label, condi-
tioned on the text. let   s now consider the inverse problem: computing the id203 of
text itself. speci   cally, we will consider models that assign id203 to a sequence of
word tokens, p(w1, w2, . . . , wm ), with wm     v. the set v is a discrete vocabulary,

v = {aardvark, abacus, . . . , zither}.

[6.1]

why would you want to compute the id203 of a word sequence? in many appli-

cations, the goal is to produce word sequences as output:

text in a target language.

    in machine translation (chapter 18), we convert from text in a source language to
    in id103, we convert from audio signal to text.
    in summarization (   16.3.4;    19.2), we convert from long texts into short texts.
    in dialogue systems (   19.3), we convert from the user   s input (and perhaps an

external knowledge base) into a text response.

in many of the systems for performing these tasks, there is a subcomponent that com-
putes the id203 of the output text. the purpose of this component is to generate
texts that are more    uent. for example, suppose we want to translate a sentence from
spanish to english.

(6.1) el cafe negro me gusta mucho.

here is a literal word-for-word translation (a gloss):

(6.2) the coffee black me pleases much.

125

126

chapter 6. language models

a good language model of english will tell us that the id203 of this translation is

low, in comparison with more grammatical alternatives,

p(the coffee black me pleases much) < p(i love dark coffee).

[6.2]

how can we use this fact? warren weaver, one of the early leaders in machine trans-

lation, viewed it as a problem of breaking a secret code (weaver, 1955):

when i look at an article in russian, i say:    this is really written in english,
but it has been coded in some strange symbols. i will now proceed to decode.   

this observation motivates a generative model (like na    ve bayes):

    the english sentence w(e) is generated from a language model, pe(w(e)).
    the spanish sentence w(s) is then generated from a translation model, ps|e(w(s) | w(e)).

given these two distributions, translation can be performed by bayes    rule:

pe|s(w(e) | w(s))    pe,s(w(e), w(s))

=ps|e(w(s) | w(e))    pe(w(e)).

[6.3]
[6.4]

this is sometimes called the id87, because it envisions english text
turning into spanish by passing through a noisy channel, ps|e. what is the advantage of
modeling translation this way, as opposed to modeling pe|s directly? the crucial point is
that the two distributions ps|e (the translation model) and pe (the language model) can be
estimated from separate data. the translation model requires examples of correct trans-
lations, but the language model requires only text in english. such monolingual data is
much more widely available. furthermore, once estimated, the language model pe can
be reused in any application that involves generating english text, including translation
from other languages.

6.1 id165 language models

a simple approach to computing the id203 of a sequence of tokens is to use a relative
frequency estimate. consider the quote, attributed to picasso,    computers are useless, they
can only give you answers.    one way to estimate the id203 of this sentence is,

p(computers are useless, they can only give you answers)

=

count(computers are useless, they can only give you answers)

count(all sentences ever spoken)

[6.5]

jacob eisenstein. draft of november 13, 2018.

[6.6]
[6.7]

6.1. id165 language models

127

this estimator is unbiased: in the theoretical limit of in   nite data, the estimate will
be correct. but in practice, we are asking for accurate counts over an in   nite number of
events, since sequences of words can be arbitrarily long. even with an aggressive upper
bound of, say, m = 20 tokens in the sequence, the number of possible sequences is v 20,
where v = |v|. a small vocabularly for english would have v = 105, so there are 10100
possible sequences. clearly, this estimator is very data-hungry, and suffers from high vari-
ance: even grammatical sentences will have id203 zero if they have not occurred in
the training data.1 we therefore need to introduce bias to have a chance of making reli-
able estimates from    nite training data. the language models that follow in this chapter
introduce bias in various ways.

we begin with id165 language models, which compute the id203 of a sequence
as the product of probabilities of subsequences. the id203 of a sequence p(w) =
p(w1, w2, . . . , wm ) can be refactored using the chain rule (see    a.2):

p(w) =p(w1, w2, . . . , wm )

=p(w1)    p(w2 | w1)    p(w3 | w2, w1)    . . .    p(wm | wm   1, . . . , w1)

each element in the product is the id203 of a word given all its predecessors. we
can think of this as a word prediction task: given the context computers are, we want to com-
pute a id203 over the next token. the relative frequency estimate of the id203
of the word useless in this context is,

p(useless | computers are) =

=

count(computers are useless)

(cid:80)x   v count(computers are x)

count(computers are useless)

count(computers are)

.

we haven   t made any approximations yet, and we could have just as well applied the

chain rule in reverse order,

p(w) = p(wm )    p(wm   1 | wm )    . . .    p(w1 | w2, . . . , wm ),

[6.8]
or in any other order. but this means that we also haven   t really made any progress:
to compute the id155 p(wm | wm   1, wm   2, . . . , w1), we would need to
model v m   1 contexts. such a distribution cannot be estimated from any realistic sample
of text.

to solve this problem, id165 models make a crucial simplifying approximation: they

condition on only the past n     1 words.

p(wm | wm   1 . . . w1)    p(wm | wm   1, . . . , wm   n+1)

[6.9]

1chomsky famously argued that this is evidence against the very concept of probabilistic language mod-
els: no such model could distinguish the grammatical sentence colorless green ideas sleep furiously from the
ungrammatical permutation furiously sleep ideas green colorless.

under contract with mit press, shared under cc-by-nc-nd license.

128

chapter 6. language models

this means that the id203 of a sentence w can be approximated as

p(w1, . . . , wm )    

m(cid:89)m=1

p(wm | wm   1, . . . , wm   n+1)

[6.10]

to compute the id203 of an entire sentence, it is convenient to pad the beginning
and end with special symbols (cid:3) and (cid:4). then the bigram (n = 2) approximation to the
id203 of i like black coffee is:
p(i like black coffee) = p(i | (cid:3))    p(like | i)    p(black | like)    p(coffee | black)    p((cid:4) | coffee).
[6.11]

this model requires estimating and storing the id203 of only v n events, which is
exponential in the order of the id165, and not v m , which is exponential in the length of
the sentence. the id165 probabilities can be computed by relative frequency estimation,

p(wm | wm   1, wm   2) =

count(wm   2, wm   1, wm)

(cid:80)w(cid:48) count(wm   2, wm   1, w(cid:48))

[6.12]

the hyperparameter n controls the size of the context used in each conditional proba-
bility. if this is misspeci   ed, the language model will perform poorly. let   s consider the
potential problems concretely.

when n is too small. consider the following sentences:

(6.3) gorillas always like to groom their friends.
(6.4) the computer that   s on the 3rd    oor of our of   ce building crashed.

in each example, the words written in bold depend on each other: the likelihood
of their depends on knowing that gorillas is plural, and the likelihood of crashed de-
pends on knowing that the subject is a computer. if the id165s are not big enough
to capture this context, then the resulting language model would offer probabili-
ties that are too low for these sentences, and too high for sentences that fail basic
linguistic tests like number agreement.

when n is too big. in this case, it is hard good estimates of the id165 parameters from
our dataset, because of data sparsity. to handle the gorilla example, it is necessary to
model 6-grams, which means accounting for v 6 events. under a very small vocab-
ulary of v = 104, this means estimating the id203 of 1024 distinct events.

jacob eisenstein. draft of november 13, 2018.

6.2. smoothing and discounting

129

these two problems point to another id160 (see    2.2.4). a small n-
gram size introduces high bias, and a large id165 size introduces high variance. we
can even have both problems at the same time! language is full of long-range dependen-
cies that we cannot capture because n is too small; at the same time, language datasets
are full of rare phenomena, whose probabilities we fail to estimate accurately because n
is too large. one solution is to try to keep n large, while still making low-variance esti-
mates of the underlying parameters. to do this, we will introduce a different sort of bias:
smoothing.

6.2 smoothing and discounting

limited data is a persistent problem in estimating language models. in    6.1, we pre-
sented id165s as a partial solution. bit sparse data can be a problem even for low-order
id165s; at the same time, many linguistic phenomena, like subject-verb agreement, can-
not be incorporated into language models without high-order id165s.
it is therefore
necessary to add additional inductive biases to id165 language models. this section
covers some of the most intuitive and common approaches, but there are many more (see
chen and goodman, 1999).

6.2.1 smoothing

a major concern in id38 is to avoid the situation p(w) = 0, which could
arise as a result of a single unseen id165. a similar problem arose in na    ve bayes, and
the solution was smoothing: adding imaginary    pseudo    counts. the same idea can be
applied to id165 language models, as shown here in the bigram case,

psmooth(wm | wm   1) =

count(wm   1, wm) +   

(cid:80)w(cid:48)   v count(wm   1, w(cid:48)) + v   

.

[6.13]

this basic framework is called lidstone smoothing, but special cases have other names:

    laplace smoothing corresponds to the case    = 1.
    jeffreys-perks law corresponds to the case    = 0.5, which works well in practice

and bene   ts from some theoretical justi   cation (manning and sch   utze, 1999).

to ensure that the probabilities are properly normalized, anything that we add to the
numerator (  ) must also appear in the denominator (v   ). this idea is re   ected in the
concept of effective counts:

   
c
i = (ci +   )

m

m + v   

,

[6.14]

under contract with mit press, shared under cc-by-nc-nd license.

130

chapter 6. language models

counts

impropriety
offense
damage
de   ciencies
outbreak
in   rmity
cephalopods

8
5
4
2
1
0
0

unsmoothed
id203
0.4
0.25
0.2
0.1
0.05
0
0

lidstone smoothing,    = 0.1 discounting, d = 0.1
effective
counts
7.826
4.928
3.961
2.029
1.063
0.097
0.097

smoothed
id203
0.391
0.246
0.198
0.101
0.053
0.005
0.005

effective
counts
7.9
4.9
3.9
1.9
0.9
0.25
0.25

smoothed
id203
0.395
0.245
0.195
0.095
0.045
0.013
0.013

table 6.1: example of lidstone smoothing and absolute discounting in a bigram language
model, for the context (alleged, ), for a toy corpus with a total of twenty counts over the
seven words shown. note that discounting decreases the id203 for all but the un-
seen words, while lidstone smoothing increases the effective counts and probabilities for
de   ciencies and outbreak.

where ci is the count of event i, c   

i is the effective count, and m =(cid:80)v
i=1 ci is the total num-
ber of tokens in the dataset (w1, w2, . . . , wm ). this term ensures that(cid:80)v
i=1 c   

the discount for each id165 is then computed as,

i =(cid:80)v

i=1 ci = m.

di =

c   
i
ci

=

(ci +   )

m

ci

(m + v   )

.

6.2.2 discounting and backoff

discounting    borrows    id203 mass from observed id165s and redistributes it. in
lidstone smoothing, the borrowing is done by increasing the denominator of the relative
frequency estimates. the borrowed id203 mass is then redistributed by increasing
the numerator for all id165s. another approach would be to borrow the same amount
of id203 mass from all observed id165s, and redistribute it among only the unob-
served id165s. this is called absolute discounting. for example, suppose we set an
absolute discount d = 0.1 in a bigram model, and then redistribute this id203 mass
equally over the unseen words. the resulting probabilities are shown in table 6.1.

discounting reserves some id203 mass from the observed data, and we need not
redistribute this id203 mass equally. instead, we can backoff to a lower-order lan-
guage model: if you have trigrams, use trigrams; if you don   t have trigrams, use bigrams;
if you don   t even have bigrams, use unigrams. this is called katz backoff. in the simple

jacob eisenstein. draft of november 13, 2018.

6.2. smoothing and discounting

case of backing off from bigrams to unigrams, the bigram probabilities are,

   
c

(i, j) =c(i, j)     d

pkatz(i | j) =         

c   (i,j)
c(j)

  (j)   

(cid:80)

punigram(i)

i(cid:48):c(i(cid:48),j)=0 punigram(i(cid:48))

if c(i, j) > 0
if c(i, j) = 0.

131

[6.15]

[6.16]

the term   (j) indicates the amount of id203 mass that has been discounted for
context j. this id203 mass is then divided across all the unseen events, {i(cid:48) : c(i(cid:48), j) =
0}, proportional to the unigram id203 of each word i(cid:48). the discount parameter d can
be optimized to maximize performance (typically held-out log-likelihood) on a develop-
ment set.

6.2.3

*interpolation

backoff is one way to combine different order id165 models. an alternative approach
is interpolation: setting the id203 of a word in context to a weighted sum of its
probabilities across progressively shorter contexts.

instead of choosing a single n for the size of the id165, we can take the weighted
average across several id165 probabilities. for example, for an interpolated trigram
model,

pinterpolation(wm | wm   1, wm   2) =   3p   
+   2p   
+   1p   

3(wm | wm   1, wm   2)
2(wm | wm   1)
1(wm).

(cid:80)nmax

in this equation, p   
n is the unsmoothed empirical id203 given by an id165 lan-
guage model, and   n is the weight assigned to this model. to ensure that the interpolated
p(w) is still a valid id203 distribution, the values of    must obey the constraint,

n=1   n = 1. but how to    nd the speci   c values?
an elegant solution is expectation-maximization. recall from chapter 5 that we can
think about em as learning with missing data: we just need to choose missing data such
that learning would be easy if it weren   t missing. what   s missing in this case? think of
each word wm as drawn from an id165 of unknown size, zm     {1 . . . nmax}. this zm is
the missing data that we are looking for. therefore, the application of em to this problem
involves the following generative model:
for each token wm, m = 1, 2, . . . , m do:

draw the id165 size zm     categorical(  );
draw wm     p   
(wm | wm   1, . . . , wm   zm).

zm

under contract with mit press, shared under cc-by-nc-nd license.

132

chapter 6. language models

if the missing data {zm} were known, then    could be estimated as the relative fre-

quency,

  z =

count(zm = z)

m

   

m(cid:88)m=1

  (zm = z).

[6.17]

[6.18]

but since we do not know the values of the latent variables zm, we impute a distribution
qm in the e-step, which represents the degree of belief that word token wm was generated
from a id165 of order zm,

qm(z) (cid:44) pr(zm = z | w1:m;   )

=

p(wm | w1:m   1, zm = z)    p(z)
(cid:80)z(cid:48) p(wm | w1:m   1, zm = z(cid:48))    p(z(cid:48))
   p   
z(wm | w1:m   1)      z.

in the m-step,    is computed by summing the expected counts under q,

  z    

m(cid:88)m=1

qm(z).

[6.19]

[6.20]

[6.21]

[6.22]

a solution is obtained by iterating between updates to q and   . the complete algorithm
is shown in algorithm 10.

algorithm 10 expectation-maximization for interpolated id38
1: procedure estimate interpolated id165 (w1:m ,{p   

n}n   1:nmax)

(cid:46) initialization

for z     {1, 2, . . . , nmax} do

  z     1

nmax

repeat

z(wm | w1:m   )      z

for m     {1, 2, . . . , m} do

qm(z)     p   

for z     {1, 2, . . . , nmax} do
qm     normalize(qm)
for z     {1, 2, . . . , nmax} do
  z     1
m=1 qm(z)

m(cid:80)m

until tired
return   

(cid:46) e-step

(cid:46) m-step

jacob eisenstein. draft of november 13, 2018.

2:
3:

4:
5:
6:
7:

8:
9:
10:

11:
12:

6.3. recurrent neural network language models

133

*kneser-ney smoothing

6.2.4
kneser-ney smoothing is based on absolute discounting, but it redistributes the result-
ing id203 mass in a different way from katz backoff. empirical evidence points
to kneser-ney smoothing as the state-of-art for id165 id38 (goodman,
i recently visited .
2001). to motivate kneser-ney smoothing, consider the example:
which of the following is more likely: francisco or duluth?

1(duluth). nonetheless we would still guess that p(visited duluth) > p(visited francisco),

now suppose that both bigrams visited duluth and visited francisco are unobserved in
1(francisco) is greater

the training data, and furthermore, that the unigram id203 p   
than p   
because duluth is a more    versatile    word: it can occur in many contexts, while francisco
usually occurs in a single context, following the word san. this notion of versatility is the
key to kneser-ney smoothing.

writing u for a context of unde   ned length, and count(w, u) as the count of word w in

context u, we de   ne the kneser-ney bigram id203 as

pkn (w | u) =(cid:40) max(count(w,u)   d,0)

count(u)

,

count(w, u) > 0

|u : count(w, u) > 0|

  (u)    pcontinuation(w), otherwise
(cid:80)w(cid:48)   v |u(cid:48) : count(w(cid:48), u(cid:48)) > 0|

.

pcontinuation(w) =

[6.23]

[6.24]

id203 mass using absolute discounting d, which is taken from all unobserved
id165s. the total amount of discounting in context u is d    |w : count(w, u) > 0|, and
we divide this id203 mass among the unseen id165s. to account for versatility,
we de   ne the continuation id203 pcontinuation(w) as proportional to the number of ob-
served contexts in which w appears. the numerator of the continuation id203 is the
number of contexts u in which w appears; the denominator normalizes the id203 by
summing the same quantity over all words w(cid:48). the coef   cient   (u) is set to ensure that
the id203 distribution pkn (w | u) sums to one over the vocabulary w.

the idea of modeling versatility by counting contexts may seem heuristic, but there is
an elegant theoretical justi   cation from bayesian nonparametrics (teh, 2006). kneser-ney
smoothing on id165s was the dominant id38 technique before the arrival
of neural language models.

6.3 recurrent neural network language models

id165 language models have been largely supplanted by neural networks. these mod-
els do not make the id165 assumption of restricted context; indeed, they can incorporate
arbitrarily distant contextual information, while remaining computationally and statisti-
cally tractable.

under contract with mit press, shared under cc-by-nc-nd license.

134

chapter 6. language models

figure 6.1: the recurrent neural network language model, viewed as an    unrolled    com-
putation graph. solid lines indicate direct computation, dotted blue lines indicate proba-
bilistic dependencies, circles indicate random variables, and squares indicate computation
nodes.

the    rst insight behind neural language models is to treat word prediction as a dis-
criminative learning task.2 the goal is to compute the id203 p(w | u), where w     v is
a word, and u is the context, which depends on the previous words. rather than directly
estimating the word probabilities from (smoothed) relative frequencies, we can treat treat
id38 as a machine learning problem, and estimate parameters that maxi-
mize the log id155 of a corpus.

the second insight is to reparametrize the id203 distribution p(w | u) as a func-

tion of two dense k-dimensional numerical vectors,   w     rk, and vu     rk,

p(w | u) =

exp(  w    vu)

(cid:80)w(cid:48)   v exp(  w(cid:48)    vu)

,

[6.25]

where   w    vu represents a dot product. as usual, the denominator ensures that the prob-
ability distribution is properly normalized. this vector of probabilities is equivalent to
applying the softmax transformation (see    3.1) to the vector of dot-products,

p(   | u) = softmax([  1    vu,   2    vu, . . . ,   v    vu]).

[6.26]

the word vectors   w are parameters of the model, and are estimated directly. the
context vectors vu can be computed in various ways, depending on the model. a simple
but effective neural language model can be built from a recurrent neural network (id56;
mikolov et al., 2010). the basic idea is to recurrently update the context vectors while
moving through the sequence. let hm represent the contextual information at position m

2this idea predates neural language models (e.g., rosenfeld, 1996; roark et al., 2007).

jacob eisenstein. draft of november 13, 2018.

h0h1h2h3      x1x2x3      w1w2w3      6.3. recurrent neural network language models

135

[6.27]
[6.28]

[6.29]

,

in the sequence. id56 language models are de   ned,
xm (cid:44)  wm
hm =id56(xm, hm   1)

p(wm+1 | w1, w2, . . . , wm) =

exp(  wm+1    hm)

(cid:80)w(cid:48)   v exp(  w(cid:48)    hm)

where    is a matrix of id27s, and xm denotes the embedding for word wm.
the conversion of wm to xm is sometimes known as a lookup layer, because we simply
lookup the embeddings for each word in a table; see    3.2.4.

the elman unit de   nes a simple recurrent operation (elman, 1990),

id56(xm, hm   1) (cid:44) g(  hm   1 + xm),

[6.30]
where        rk  k is the recurrence matrix and g is a non-linear transformation function,
often de   ned as the elementwise hyperbolic tangent tanh (see    3.1).3 the tanh acts as a
squashing function, ensuring that each element of hm is constrained to the range [   1, 1].
although each wm depends on only the context vector hm   1, this vector is in turn
in   uenced by all previous tokens, w1, w2, . . . wm   1, through the recurrence operation: w1
affects h1, which affects h2, and so on, until the information is propagated all the way to
hm   1, and then on to wm (see figure 6.1). this is an important distinction from id165
language models, where any information outside the n-word window is ignored. in prin-
ciple, the id56 language model can handle long-range dependencies, such as number
agreement over long spans of text     although it would be dif   cult to know where exactly
in the vector hm this information is represented. the main limitation is that informa-
tion is attenuated by repeated application of the squashing function g. long short-term
memories (lstms), described below, are a variant of id56s that address this issue, us-
ing memory cells to propagate information through the sequence without applying non-
linearities (hochreiter and schmidhuber, 1997).

the denominator in equation 6.29 is a computational bottleneck, because it involves
a sum over the entire vocabulary. one solution is to use a hierarchical softmax function,
which computes the sum more ef   ciently by organizing the vocabulary into a tree (mikolov
et al., 2011). another strategy is to optimize an alternative metric, such as noise-contrastive
estimation (gutmann and hyv  arinen, 2012), which learns by distinguishing observed in-
stances from arti   cial instances generated from a noise distribution (mnih and teh, 2012).
both of these strategies are described in    14.5.3.

3in the original elman network, the sigmoid function was used in place of tanh. for an illuminating
mathematical discussion of the advantages and disadvantages of various nonlinearities in recurrent neural
networks, see the lecture notes from cho (2015).

under contract with mit press, shared under cc-by-nc-nd license.

136

chapter 6. language models

6.3.1 id26 through time
the recurrent neural network language model has the following parameters:

      i     rk, the    input    word vectors (these are sometimes called id27s,

since each word is embedded in a k-dimensional space; see chapter 14);

      i     rk, the    output    word vectors;
           rk  k, the recurrence operator;
    h0, the initial state.

each of these parameters can be estimated by formulating an objective function over the
training corpus, l(w), and then applying id26 to obtain gradients on the
parameters from a minibatch of training examples (see    3.3.1). gradient-based updates
can be computed from an online learning algorithm such as stochastic id119
(see    2.6.2).

the application of id26 to recurrent neural networks is known as back-
propagation through time, because the gradients on units at time m depend in turn on the
gradients of units at earlier times n < m. let (cid:96)m+1 represent the negative log-likelihood
of word m + 1,

[6.31]
we require the gradient of this loss with respect to each parameter, such as   k,k(cid:48), an indi-
vidual element in the recurrence matrix   . since the loss depends on the parameters only
through hm, we can apply the chain rule of differentiation,

(cid:96)m+1 =     log p(wm+1 | w1, w2, . . . , wm).

   (cid:96)m+1
     k,k(cid:48)

=

   (cid:96)m+1
   hm

   hm
     k,k(cid:48)

.

[6.32]

the vector hm depends on    in several ways. first, hm is computed by multiplying    by
the previous state hm   1. but the previous state hm   1 also depends on   :

hm =g(xm, hm   1)

(cid:48)

=g

   hm,k
     k,k(cid:48)

(xm,k +   k    hm   1)(hm   1,k(cid:48) +   k   

   hm   1
     k,k(cid:48)

),

[6.33]

[6.34]

where g(cid:48) is the local derivative of the nonlinear function g. the key point in this equation
is that the derivative    hm
, and
     k,k(cid:48)
so on, until reaching the initial state h0.

, which will depend in turn on    hm   2
     k,k(cid:48)

depends on    hm   1
     k,k(cid:48)

each derivative    hm
     k,k(cid:48)

will be reused many times: it appears in id26 from
the loss (cid:96)m, but also in all subsequent losses (cid:96)n>m. neural network toolkits such as
torch (collobert et al., 2011) and dynet (neubig et al., 2017) compute the necessary

jacob eisenstein. draft of november 13, 2018.

6.3. recurrent neural network language models

137

derivatives automatically, and cache them for future use. an important distinction from
the feedforward neural networks considered in chapter 3 is that the size of the computa-
tion graph is not    xed, but varies with the length of the input. this poses dif   culties for
toolkits that are designed around static computation graphs, such as tensorflow (abadi
et al., 2016).4

6.3.2 hyperparameters

the id56 language model has several hyperparameters that must be tuned to ensure good
performance. the model capacity is controlled by the size of the word and context vectors
k, which play a role that is somewhat analogous to the size of the id165 context. for
datasets that are large with respect to the vocabulary (i.e., there is a large token-to-type
ratio), we can afford to estimate a model with a large k, which enables more subtle dis-
tinctions between words and contexts. when the dataset is relatively small, then k must
be smaller too, or else the model may    memorize    the training data, and fail to generalize.
unfortunately, this general advice has not yet been formalized into any concrete formula
for choosing k, and trial-and-error is still necessary. over   tting can also be prevented by
dropout, which involves randomly setting some elements of the computation to zero (sri-
vastava et al., 2014), forcing the learner not to rely too much on any particular dimension
of the word or context vectors. the dropout rate must also be tuned on development data.

6.3.3 gated recurrent neural networks

in principle, recurrent neural networks can propagate information across in   nitely long
sequences. but in practice, repeated applications of the nonlinear recurrence function
causes this information to be quickly attenuated. the same problem affects learning: back-
propagation can lead to vanishing gradients that decay to zero, or exploding gradients
that increase towards in   nity (bengio et al., 1994). the exploding gradient problem can
be addressed by clipping gradients at some maximum value (pascanu et al., 2013). the
other issues must be addressed by altering the model itself.

the long short-term memory (lstm; hochreiter and schmidhuber, 1997) is a popular
variant of id56s that is more robust to these problems. this model augments the hidden
state hm with a memory cell cm. the value of the memory cell at each time m is a gated
sum of two quantities: its previous value cm   1, and an    update      cm, which is computed
from the current input xm and the previous hidden state hm   1. the next state hm is then
computed from the memory cell. because the memory cell is not passed through a non-
linear squashing function during the update, it is possible for information to propagate
through the network over long distances.

4see https://www.tensorflow.org/tutorials/recurrent (retrieved feb 8, 2018).

under contract with mit press, shared under cc-by-nc-nd license.

138

chapter 6. language models

figure 6.2: the long short-term memory (lstm) architecture. gates are shown in boxes
with dotted edges. in an lstm language model, each hm would be used to predict the
next word wm+1.

the gates are functions of the input and previous hidden state. they are computed
from elementwise sigmoid activations,   (x) = (1 + exp(   x))   1, ensuring that their values
will be in the range [0, 1]. they can therefore be viewed as soft, differentiable logic gates.
the lstm architecture is shown in figure 6.2, and the complete update equations are:

fm+1 =  (  (h   f )hm +   (x   f )xm+1 + bf )
im+1 =  (  (h   i)hm +   (x   i)xm+1 + bi)
  cm+1 = tanh(  (h   c)hm +   (w   c)xm+1)
cm+1 =fm+1 (cid:12) cm + im+1 (cid:12)   cm+1
om+1 =  (  (h   o)hm +   (x   o)xm+1 + bo)
hm+1 =om+1 (cid:12) tanh(cm+1)

forget gate
input gate
update candidate
memory cell update
output gate
output.

[6.35]
[6.36]
[6.37]
[6.38]
[6.39]
[6.40]

the operator (cid:12) is an elementwise (hadamard) product. each gate is controlled by a vec-
tor of weights, which parametrize the previous hidden state (e.g.,   (h   f )) and the current
input (e.g.,   (x   f )), plus a vector offset (e.g., bf ). the overall operation can be infor-
mally summarized as (hm, cm) = lstm(xm, (hm   1, cm   1)), with (hm, cm) representing
the lstm state after reading token m.

the lstm outperforms standard recurrent neural networks across a wide range of
problems. it was    rst used for id38 by sundermeyer et al. (2012), but can
be applied more generally: the vector hm can be treated as a complete representation of

jacob eisenstein. draft of november 13, 2018.

hmhm+1omom+1cmfm+1cm+1imim+1  cm  cm+1xmxm+16.4. evaluating language models

139

the input sequence up to position m, and can be used for any labeling task on a sequence
of tokens, as we will see in the next chapter.

there are several lstm variants, of which the gated recurrent unit (cho et al., 2014)
is one of the more well known. many software packages implement a variety of id56
architectures, so choosing between them is simple from a user   s perspective. jozefowicz
et al. (2015) provide an empirical comparison of various modeling choices circa 2015.

6.4 evaluating language models

id38 is not usually an application in itself: language models are typically
components of larger systems, and they would ideally be evaluated extrinisically. this
means evaluating whether the language model improves performance on the application
task, such as machine translation or id103. but this is often hard to do, and
depends on details of the overall system which may be irrelevant to id38.
in contrast, intrinsic evaluation is task-neutral. better performance on intrinsic metrics
may be expected to improve extrinsic metrics across a variety of tasks, but there is always
the risk of over-optimizing the intrinsic metric. this section discusses some intrinsic met-
rics, but keep in mind the importance of performing extrinsic evaluations to ensure that
intrinsic performance gains carry over to real applications.

6.4.1 held-out likelihood

the goal of probabilistic language models is to accurately measure the id203 of se-
quences of word tokens. therefore, an intrinsic evaluation metric is the likelihood that the
language model assigns to held-out data, which is not used during training. speci   cally,
we compute,

(cid:96)(w) =

m(cid:88)m=1

log p(wm | wm   1, . . . , w1),

[6.41]

treating the entire held-out corpus as a single stream of tokens.

typically, unknown words are mapped to the (cid:104)unk(cid:105) token. this means that we have
to estimate some id203 for (cid:104)unk(cid:105) on the training data. one way to do this is to    x
the vocabulary v to the v     1 words with the highest counts in the training data, and then
convert all other tokens to (cid:104)unk(cid:105). other strategies for dealing with out-of-vocabulary
terms are discussed in    6.5.

under contract with mit press, shared under cc-by-nc-nd license.

140

6.4.2 perplexity

chapter 6. language models

held-out likelihood is usually presented as perplexity, which is a deterministic transfor-
mation of the log-likelihood into an information-theoretic quantity,

perplex(w) = 2

    (cid:96)(w)
m ,

[6.42]

where m is the total number of tokens in the held-out corpus.

lower perplexities correspond to higher likelihoods, so lower scores are better on this

metric     it is better to be less perplexed. here are some special cases:

    in the limit of a perfect language model, id203 1 is assigned to the held-out

corpus, with perplex(w) = 2

m log2 1 = 20 = 1.

    1

    in the opposite limit, id203 zero is assigned to the held-out corpus, which cor-

responds to an in   nite perplexity, perplex(w) = 2

    1
m log2 0 = 2    =    .

    assume a uniform, unigram model in which p(wi) = 1

v for all words in the vocab-

ulary. then,

log2(w) =

m(cid:88)m=1

log2

1
v

=    

m(cid:88)m=1

log2 v =    m log2 v

perplex(w) =2

1
m m log2 v

=2log2 v
=v.

this is the    worst reasonable case    scenario, since you could build such a language
model without even looking at the data.

in practice, language models tend to give perplexities in the range between 1 and v .
a small benchmark dataset is the id32, which contains roughly a million to-
kens; its vocabulary is limited to 10,000 words, with all other tokens mapped a special
(cid:104)unk(cid:105) symbol. on this dataset, a well-smoothed 5-gram model achieves a perplexity of
141 (mikolov and zweig, mikolov and zweig), and an lstm language model achieves
perplexity of roughly 80 (zaremba, sutskever, and vinyals, zaremba et al.). various en-
hancements to the lstm architecture can bring the perplexity below 60 (merity et al.,
2018). a larger-scale id38 dataset is the 1b word benchmark (chelba et al.,
2013), which contains text from wikipedia. on this dataset, perplexities of around 25 can
be obtained by averaging together multiple lstm language models (jozefowicz et al.,
2016).

jacob eisenstein. draft of november 13, 2018.

6.5. out-of-vocabulary words

141

6.5 out-of-vocabulary words
so far, we have assumed a closed-vocabulary setting     the vocabulary v is assumed to be
a    nite set. in realistic application scenarios, this assumption may not hold. consider, for
example, the problem of translating newspaper articles. the following sentence appeared
in a reuters article on january 6, 2017:5

the report said u.s. intelligence agencies believe russian military intelligence,
the gru, used intermediaries such as wikileaks, dcleaks.com and the guc-
cifer 2.0    persona    to release emails...

suppose that you trained a language model on the gigaword corpus,6 which was released
in 2003. the bolded terms either did not exist at this date, or were not widely known; they
are unlikely to be in the vocabulary. the same problem can occur for a variety of other
terms: new technologies, previously unknown individuals, new words (e.g., hashtag), and
numbers.

one solution is to simply mark all such terms with a special token, (cid:104)unk(cid:105). while
training the language model, we decide in advance on the vocabulary (often the k most
common terms), and mark all other terms in the training data as (cid:104)unk(cid:105). if we do not want
to determine the vocabulary size in advance, an alternative approach is to simply mark
the    rst occurrence of each word type as (cid:104)unk(cid:105).

but is often better to make distinctions about the likelihood of various unknown words.
this is particularly important in languages that have rich morphological systems, with
many in   ections for each word. for example, portuguese is only moderately complex
from a morphological perspective, yet each verb has dozens of in   ected forms (see fig-
ure 4.3b). in such languages, there will be many word types that we do not encounter in a
corpus, which are nonetheless predictable from the morphological rules of the language.
to use a somewhat contrived english example, if transfenestrate is in the vocabulary, our
language model should assign a non-zero id203 to the past tense transfenestrated,
even if it does not appear in the training data.

one way to accomplish this is to supplement word-level language models with character-

level language models. such models can use id165s or id56s, but with a    xed vocab-
ulary equal to the set of ascii or unicode characters. for example, ling et al. (2015)
propose an lstm model over characters, and kim (2014) employ a convolutional neural
network. a more linguistically motivated approach is to segment words into meaningful
subword units, known as morphemes (see chapter 9). for example, botha and blunsom

5bayoumy, y. and strobel, w.

(2017,

ber campaign to help trump.
us-usa-russia-cyber-iduskbn14q1t8 on january 7, 2017.

reuters.

6https://catalog.ldc.upenn.edu/ldc2003t05

january 6).
putin directed cy-
retrieved from http://www.reuters.com/article/

report:

intel

u.s.

under contract with mit press, shared under cc-by-nc-nd license.

142

chapter 6. language models

(2014) induce vector representations for morphemes, which they build into a log-bilinear
language model; bhatia et al. (2016) incorporate morpheme vectors into an lstm.

additional resources

a variety of neural network architectures have been applied to id38. no-
table earlier non-recurrent architectures include the neural probabilistic language model (ben-
gio et al., 2003) and the log-bilinear language model (mnih and hinton, 2007). much more
detail on these models can be found in the text by goodfellow et al. (2016).

exercises

1. prove that id165 language models give valid probabilities if the id165 probabil-

ities are valid. speci   cally, assume that,

v

(cid:88)wm

p(wm | wm   1, wm   2, . . . , wm   n+1) = 1

[6.43]

for all contexts (wm   1, wm   2, . . . , wm   n+1). prove that(cid:80)w pn(w) = 1 for all w     v
   ,
where pn is the id203 under an id165 language model. your proof should
proceed by induction. you should handle the start-of-string case p(w1 | (cid:3), . . . , (cid:3)
),
(cid:123)(cid:122)
(cid:125)
n   1
but you need not handle the end-of-string token.

(cid:124)

2. first, show that id56 language models are valid using a similar proof technique to

the one in the previous problem.
next, let pr(w) indicate the id203 of w under id56 r. an ensemble of id56
language models computes the id203,

p(w) =

1
r

r(cid:88)r=1

pr(w).

[6.44]

does an ensemble of id56 language models compute a valid id203?

3. consider a unigram language model over a vocabulary of size v . suppose that a
word appears m times in a corpus with m tokens in total. with lidstone smoothing
of   , for what values of m is the smoothed id203 greater than the unsmoothed
id203?

4. consider a simple language in which each token is drawn from the vocabulary v

with id203 1

v , independent of all other tokens.

jacob eisenstein. draft of november 13, 2018.

6.5. out-of-vocabulary words

143

given a corpus of size m, what is the expectation of the fraction of all possible
bigrams that have zero count? you may assume v is large enough that 1

v     1

v    1.

5. continuing the previous problem, determine the value of m such that the fraction
of bigrams with zero count is at most       (0, 1). as a hint, you may use the approxi-
mation ln(1 +   )        for        0.

6. in real languages, words probabilities are neither uniform nor independent. assume
that word probabilities are independent but not uniform, so that in general p(w) (cid:54)=
v . prove that the expected fraction of unseen bigrams will be higher than in the iid
1
case.

7. consider a recurrent neural network with a single hidden unit and a sigmoid acti-
goes to

vation, hm =   (  hm   1 + xm). prove that if |  | < 1, then the gradient
zero as k        .7
8. zipf   s law states that if the word types in a corpus are sorted by frequency, then the
frequency of the word at rank r is proportional to r   s, where s is a free parameter,
usually around 1. (another way to view zipf   s law is that a plot of log frequency
against log rank will be linear.) solve for s using the counts of the    rst and second
most frequent words, c1 and c2.

   hm
   hm   k

9. download the wikitext-2 dataset.8 read in the training data and compute word

counts. estimate the zipf   s law coef   cient by,

  s = exp(cid:18) (log r)    (log c)
2 (cid:19) ,

|| log r||2

[6.45]

where r = [1, 2, 3, . . .] is the vector of ranks of all words in the corpus, and c =
[c1, c2, c3, . . .] is the vector of counts of all words in the corpus, sorted in descending
order.
make a log-log plot of the observed counts, and the expected counts according to
r=1 rs =   (s) is the riemann zeta function, available in

python   s scipy library as scipy.special.zeta.

zipf   s law. the sum (cid:80)   

10. using the pytorch library, train an lstm language model from the wikitext train-
ing corpus. after each epoch of training, compute its perplexity on the wikitext
validation corpus. stop training when the perplexity stops improving.

7this proof generalizes to vector hidden units by considering the largest eigenvector of the matrix    (pas-

canu et al., 2013).

8available

https://github.com/pytorch/examples/tree/master/word_language_
model/data/wikitext-2 in september 2018. the dataset is already tokenized, and already replaces rare
words with (cid:104)unk(cid:105), so no preprocessing is necessary.

at

under contract with mit press, shared under cc-by-nc-nd license.

chapter 7

sequence labeling

the goal of sequence labeling is to assign tags to words, or more generally, to assign
discrete labels to discrete elements in a sequence. there are many applications of se-
quence labeling in natural language processing, and chapter 8 presents an overview. for
now, we   ll focus on the classic problem of part-of-speech tagging, which requires tagging
each word by its grammatical category. coarse-grained grammatical categories include
nouns, which describe things, properties, or ideas, and verbs, which describe actions
and events. consider a simple input:

(7.1) they can    sh.

a dictionary of coarse-grained part-of-speech tags might include noun as the only valid
tag for they, but both noun and verb as potential tags for can and    sh. a accurate se-
quence labeling algorithm should select the verb tag for both can and    sh in (7.1), but it
should select noun for the same two words in the phrase can of    sh.

7.1 sequence labeling as classi   cation

one way to solve a tagging problem is to turn it into a classi   cation problem. let f ((w, m), y)
indicate the feature function for tag y at position m in the sequence w = (w1, w2, . . . , wm ).
a simple tagging model would have a single base feature, the word itself:

f ((w = they can    sh, m = 1), n) =(they, n)
f ((w = they can    sh, m = 2), v) =(can, v)
f ((w = they can    sh, m = 3), v) =(   sh, v).

[7.1]
[7.2]
[7.3]

here the feature function takes three arguments as input: the sentence to be tagged (e.g.,
they can    sh), the proposed tag (e.g., n or v), and the index of the token to which this tag

145

146

chapter 7. sequence labeling

is applied. this simple feature function then returns a single feature: a tuple including
the word to be tagged and the tag that has been proposed. if the vocabulary size is v
and the number of tags is k, then there are v    k features. each of these features must
be assigned a weight. these weights can be learned from a labeled dataset using a clas-
si   cation algorithm such as id88, but this isn   t necessary in this case: it would be
equivalent to de   ne the classi   cation weights directly, with   w,y = 1 for the tag y most
frequently associated with word w, and   w,y = 0 for all other tags.

however, it is easy to see that this simple classi   cation approach cannot correctly tag
both they can    sh and can of    sh, because can and    sh are grammatically ambiguous. to han-
dle both of these cases, the tagger must rely on context, such as the surrounding words.
we can build context into the feature set by incorporating the surrounding words as ad-
ditional features:

f ((w = they can    sh, 1), n) = {(wm = they, ym = n),
(wm   1 = (cid:3), ym = n),
(wm+1 = can, ym = n)}

f ((w = they can    sh, 2), v) = {(wm = can, ym = v),

(wm   1 = they, ym = v),
(wm+1 =    sh, ym = v)}

f ((w = they can    sh, 3), v) = {(wm =    sh, ym = v),
(wm   1 = can, ym = v),
(wm+1 = (cid:4), ym = v)}.

[7.4]

[7.5]

[7.6]

these features contain enough information that a tagger should be able to choose the
right tag for the word    sh: words that come after can are likely to be verbs, so the feature
(wm   1 = can, ym = v) should have a large positive weight.

however, even with this enhanced feature set, it may be dif   cult to tag some se-
quences correctly. one reason is that there are often relationships between the tags them-
selves. for example, in english it is relatively rare for a verb to follow another verb    
particularly if we differentiate modal verbs like can and should from more typical verbs,
like give, transcend, and befuddle. we would like to incorporate preferences against tag se-
quences like verb-verb, and in favor of tag sequences like noun-verb. the need for
such preferences is best illustrated by a garden path sentence:

(7.2) the old man the boat.

grammatically, the word the is a determiner. when you read the sentence, what
part of speech did you    rst assign to old? typically, this word is an adjective     abbrevi-
ated as j     which is a class of words that modify nouns. similarly, man is usually a noun.
the resulting sequence of tags is d j n d n. but this is a mistaken    garden path    inter-
pretation, which ends up leading nowhere. it is unlikely that a determiner would directly

jacob eisenstein. draft of november 13, 2018.

7.2. sequence labeling as structure prediction

147

follow a noun,1 and it is particularly unlikely that the entire sentence would lack a verb.
the only possible verb in (7.2) is the word man, which can refer to the act of maintaining
and piloting something     often boats. but if man is tagged as a verb, then old is seated
between a determiner and a verb, and must be a noun. and indeed, adjectives often have
a second interpretation as nouns when used in this way (e.g., the young, the restless). this
reasoning, in which the labeling decisions are intertwined, cannot be applied in a setting
where each tag is produced by an independent classi   cation decision.

7.2 sequence labeling as structure prediction

as an alternative, think of the entire sequence of tags as a label itself. for a given sequence
of words w = (w1, w2, . . . , wm ), there is a set of possible taggings y(w) = y m , where
y = {n, v, d, . . .} refers to the set of individual tags, and y m refers to the set of tag
sequences of length m. we can then treat the sequence labeling problem as a classi   cation
problem in the label space y(w),

  y = argmax
y   y(w)

  (w, y),

[7.7]

where y = (y1, y2, . . . , ym ) is a sequence of m tags, and    is a scoring function on pairs
of sequences, v m    y m     r. such a function can include features that capture the rela-
tionships between tagging decisions, such as the preference that determiners not follow
nouns, or that all sentences have verbs.

given that the label space is exponentially large in the length of the sequence m, can
it ever be practical to perform tagging in this way? the problem of making a series of in-
terconnected labeling decisions is known as id136. because natural language is full of
interrelated grammatical structures, id136 is a crucial aspect of natural language pro-
cessing. in english, it is not unusual to have sentences of length m = 20; part-of-speech
tag sets vary in size from 10 to several hundred. taking the low end of this range, we have
|y(w1:m )|     1020, one hundred billion billion possible tag sequences. enumerating and
scoring each of these sequences would require an amount of work that is exponential in
the sequence length, so id136 is intractable.

however, the situation changes when we restrict the scoring function. suppose we

choose a function that decomposes into a sum of local parts,

  (w, y) =

  (w, ym, ym   1, m),

[7.8]

m +1(cid:88)m=1

where each   (  ) scores a local part of the tag sequence. note that the sum goes up to m +1,
so that we can include a score for a special end-of-sequence tag,   (w1:m , (cid:7), ym , m + 1).
we also de   ne a special tag to begin the sequence, y0 (cid:44)    .

1the main exception occurs with ditransitive verbs, such as they gave the winner a trophy.

under contract with mit press, shared under cc-by-nc-nd license.

148

chapter 7. sequence labeling

in a linear model, local scoring function can be de   ned as a dot product of weights

and features,

  (w1:m , ym, ym   1, m) =       f (w, ym, ym   1, m).

[7.9]

the feature vector f can consider the entire input w, and can look at pairs of adjacent
tags. this is a step up from per-token classi   cation: the weights can assign low scores
to infelicitous tag pairs, such as noun-determiner, and high scores for frequent tag pairs,
such as determiner-noun and noun-verb.

in the example they can    sh, a minimal feature function would include features for
word-tag pairs (sometimes called emission features) and tag-tag pairs (sometimes called
transition features):

f (w = they can    sh, y = n v v ) =

f (w, ym, ym   1, m)

[7.10]

m +1(cid:88)m=1

=f (w, n,    , 1)

+ f (w, v, n, 2)
+ f (w, v, v, 3)
+ f (w, (cid:7), v, 4)

=(wm = they, ym = n) + (ym = n, ym   1 =    )

[7.11]

+ (wm = can, ym = v) + (ym = v, ym   1 = n)
+ (wm =    sh, ym = v) + (ym = v, ym   1 = v)
+ (ym = (cid:7), ym   1 = v).

[7.12]

there are seven active features for this example: one for each word-tag pair, and one
for each tag-tag pair, including a    nal tag ym +1 = (cid:7). these features capture the two main
sources of information for part-of-speech tagging in english: which tags are appropriate
for each word, and which tags tend to follow each other in sequence. given appropriate
weights for these features, taggers can achieve high accuracy, even for dif   cult cases like
the old man the boat. we will now discuss how this restricted scoring function enables
ef   cient id136, through the viterbi algorithm (viterbi, 1967).

jacob eisenstein. draft of november 13, 2018.

7.3. the viterbi algorithm

7.3 the viterbi algorithm

149

by decomposing the scoring function into a sum of local parts, it is possible to rewrite the
tagging problem as follows:

  y = argmax
y   y(w)

  (w, y)

= argmax

y1:m

= argmax

y1:m

m +1(cid:88)m=1
m +1(cid:88)m=1

  (w, ym, ym   1, m)

sm(ym, ym   1),

where the    nal line simpli   es the notation with the shorthand,
sm(ym, ym   1) (cid:44)   (w1:m , ym, ym   1, m).

[7.13]

[7.14]

[7.15]

[7.16]

this id136 problem can be solved ef   ciently using id145, an al-
gorithmic technique for reusing work in recurrent computations. we begin by solving an
auxiliary problem: rather than    nding the best tag sequence, we compute the score of the
best tag sequence,

max
y1:m

  (w, y1:m ) = max
y1:m

m +1(cid:88)m=1

sm(ym, ym   1).

[7.17]

this score involves a maximization over all tag sequences of length m, written maxy1:m .
this maximization can be broken into two pieces,

max
y1:m

  (w, y1:m ) = max
ym

max
y1:m   1

sm(ym, ym   1).

[7.18]

m +1(cid:88)m=1

within the sum, only the    nal term sm +1((cid:7), ym ) depends on ym , so we can pull this term
out of the second maximization,

  (w, y1:m ) =(cid:18)max

ym

sm +1((cid:7), ym )(cid:19) +(cid:32) max

y1:m   1

max
y1:m

sm(ym, ym   1)(cid:33) .

m(cid:88)m=1

[7.19]

the second term in equation 7.19 has the same form as our original problem, with m
replaced by m   1. this indicates that the problem can be reformulated as a recurrence. we
do this by de   ning an auxiliary variable called the viterbi variable vm(k), representing

under contract with mit press, shared under cc-by-nc-nd license.

150

chapter 7. sequence labeling

algorithm 11 the viterbi algorithm. each sm(k, k(cid:48)) is a local score for tag ym = k and
ym   1 = k(cid:48).

for k     {0, . . . k} do
v1(k) = s1(k,    )
for m     {2, . . . , m} do

for k     {0, . . . , k} do

vm(k) = maxk(cid:48) sm(k, k(cid:48)) + vm   1(k(cid:48))
bm(k) = argmaxk(cid:48) sm(k, k(cid:48)) + vm   1(k(cid:48))

ym = argmaxk sm +1((cid:7), k) + vm (k)
for m     {m     1, . . . 1} do
return y1:m

ym = bm(ym+1)

the score of the best sequence terminating in the tag k:

vm(ym) (cid:44) max
y1:m   1

m(cid:88)n=1

sn(yn, yn   1)

sm(ym, ym   1) + max
y1:m   2

sn(yn, yn   1)

sm(ym, ym   1) + vm   1(ym   1).

m   1(cid:88)n=1

= max
ym   1
= max
ym   1

[7.20]

[7.21]

[7.22]

each set of viterbi variables is computed from the local score sm(ym, ym   1), and from the
previous set of viterbi variables. the initial condition of the recurrence is simply the score
for the    rst tag,

v1(y1) (cid:44)s1(y1,    ).

the maximum overall score for the sequence is then the    nal viterbi variable,

  (w1:m , y1:m ) =vm +1((cid:7)).

max
y1:m

[7.23]

[7.24]

thus, the score of the best labeling for the sequence can be computed in a single forward
sweep:    rst compute all variables v1(  ) from equation 7.23, and then compute all variables
v2(  ) from the recurrence in equation 7.22, continuing until the    nal variable vm +1((cid:7)).
the viterbi variables can be arranged in a structure known as a trellis, shown in fig-
ure 7.1. each column indexes a token m in the sequence, and each row indexes a tag in
y; every vm   1(k) is connected to every vm(k(cid:48)), indicating that vm(k(cid:48)) is computed from
vm   1(k). special nodes are set aside for the start and end states.

jacob eisenstein. draft of november 13, 2018.

7.3. the viterbi algorithm

151

figure 7.1: the trellis representation of the viterbi variables, for the example they can    sh,
using the weights shown in table 7.1.

the original goal was to    nd the best scoring sequence, not simply to compute its
score. but by solving the auxiliary problem, we are almost there. recall that each vm(k)
represents the score of the best tag sequence ending in that tag k in position m. to compute
this, we maximize over possible values of ym   1. by keeping track of the    argmax    tag that
maximizes this choice at each step, we can walk backwards from the    nal tag, and recover
the optimal tag sequence. this is indicated in figure 7.1 by the thick lines, which we trace
back from the    nal position. these backward pointers are written bm(k), indicating the
optimal tag ym   1 on the path to ym = k.
the complete viterbi algorithm is shown in algorithm 11. when computing the initial
viterbi variables v1(  ), the special tag     indicates the start of the sequence. when comput-
ing the    nal tag ym , another special tag,(cid:7) indicates the end of the sequence. these special
tags enable the use of transition features for the tags that begin and end the sequence: for
example, conjunctions are unlikely to end sentences in english, so we would like a low
score for sm +1((cid:7), cc); nouns are relatively likely to appear at the beginning of sentences,
so we would like a high score for s1(n,    ), assuming the noun tag is compatible with the
   rst word token w1.

complexity if there are k tags and m positions in the sequence, then there are m    k
viterbi variables to compute. computing each variable requires    nding a maximum over
k possible predecessor tags. the total time complexity of populating the trellis is there-
fore o(m k2), with an additional factor for the number of active features at each position.
after completing the trellis, we simply trace the backwards pointers to the beginning of
the sequence, which takes o(m ) operations.

under contract with mit press, shared under cc-by-nc-nd license.

0theycan   sh-10n-3-9-9v-12-5-11152

chapter 7. sequence labeling

they

can    sh
n    2    3    3
v    10    1    3

(a) weights for emission features.

n v

(cid:7)
       1    2       
n    3    1    1
v    1    3    1

(b) weights for transition features.
the
   from    tags are on the columns, and the    to   
tags are on the rows.

table 7.1: feature weights for the example trellis shown in figure 7.1. emission weights
from     and (cid:7) are implicitly set to       .
7.3.1 example
consider the minimal tagset {n, v}, corresponding to nouns and verbs. even in this
tagset, there is considerable ambiguity: for example, the words can and    sh can each take
both tags. of the 2    2    2 = 8 possible taggings for the sentence they can    sh, four are
possible given these possible tags, and two are grammatical.2

the values in the trellis in figure 7.1 are computed from the feature weights de   ned in
table 7.1. we begin with v1(n), which has only one possible predecessor, the start tag    .
this score is therefore equal to s1(n,    ) =    2     1 =    3, which is the sum of the scores for
the emission and transition features respectively; the backpointer is b1(n) =    . the score
for v1(v) is computed in the same way: s1(v,    ) =    10     2 =    12, and again b1(v) =    .
the backpointers are represented in the    gure by thick lines.
things get more interesting at m = 2. the score v2(n) is computed by maximizing

over the two possible predecessors,

v2(n) = max(v1(n) + s2(n, n), v1(v) + s2(n, v))
   12     3     1) =    9

= max(   3     3     3,

b2(n) =n.

this continues until reaching v4((cid:7)), which is computed as,

v4((cid:7)) = max(v3(n) + s4((cid:7), n), v3(v) + s4((cid:7), v))

= max(   9 + 0     1,
=     10,

   11 + 0     1)

so b4((cid:7)) = n. as there is no emission w4, the emission features have scores of zero.

[7.25]
[7.26]
[7.27]

[7.28]
[7.29]
[7.30]

2the tagging they/n can/v    sh/n corresponds to the scenario of putting    sh into cans, or perhaps of

   ring them.

jacob eisenstein. draft of november 13, 2018.

7.4. id48

153

to compute the optimal tag sequence, we walk backwards from here, next checking
b3(n) = v, and then b2(v) = n, and    nally b1(n) =    . this yields y = (n, v, n), which
corresponds to the linguistic interpretation of the    shes being put into cans.

7.3.2 higher-order features

the viterbi algorithm was made possible by a restriction of the scoring function to local
parts that consider only pairs of adjacent tags. we can think of this as a bigram language
model over tags. a natural question is how to generalize viterbi to tag trigrams, which
would involve the following decomposition:

  (w, y) =

where y   1 =     and ym +2 = (cid:7).

m +2(cid:88)m=1

f (w, ym, ym   1, ym   2, m),

[7.31]

one solution is to create a new tagset y (2) from the cartesian product of the original
tagset with itself, y (2) = y    y. the tags in this product space are ordered pairs, rep-
resenting adjacent tags at the token level: for example, the tag (n, v) would represent a
noun followed by a verb. transitions between such tags must be consistent: we can have a
transition from (n, v) to (v, n) (corresponding to the tag sequence n v n), but not from
(n, v) to (n, n), which would not correspond to any coherent tag sequence. this con-
straint can be enforced in feature weights, with   ((a,b),(c,d)) =        if b (cid:54)= c. the remaining
feature weights can encode preferences for and against various tag trigrams.

in the cartesian product tag space, there are k2 tags, suggesting that the time com-
plexity will increase to o(m k4). however, it is unnecessary to max over predecessor tag
bigrams that are incompatible with the current tag bigram. by exploiting this constraint,
it is possible to limit the time complexity to o(m k3). the space complexity grows to
o(m k2), since the trellis must store all possible predecessors of each tag. in general, the
time and space complexity of higher-order viterbi grows exponentially with the order of
the tag id165s that are considered in the feature decomposition.

7.4 id48
the viterbi sequence labeling algorithm is built on the scores sm(y, y(cid:48)). we will now
discuss how these scores can be estimated probabilistically. recall from    2.2 that the
probabilistic na    ve bayes classi   er selects the label y to maximize p(y | x)     p(y, x). in
probabilistic sequence labeling, our goal is similar: select the tag sequence that maximizes
p(y | w)     p(y, w). the locality restriction in equation 7.8 can be viewed as a conditional
independence assumption on the random variables y.

under contract with mit press, shared under cc-by-nc-nd license.

154

chapter 7. sequence labeling

algorithm 12 generative process for the hidden markov model
y0        , m     1
repeat

ym     categorical(  ym   1)
wm     categorical(  ym)

until ym = (cid:7)

(cid:46) sample the current tag
(cid:46) sample the current word
(cid:46) terminate when the stop symbol is generated

na    ve bayes was introduced as a generative model     a probabilistic story that ex-
plains the observed data as well as the hidden label. a similar story can be constructed
for probabilistic sequence labeling:    rst, the tags are drawn from a prior distribution; next,
the tokens are drawn from a conditional likelihood. however, for id136 to be tractable,
additional independence assumptions are required. first, the id203 of each token
depends only on its tag, and not on any other element in the sequence:

[7.32]

[7.33]

p(w | y) =

p(wm | ym).

m(cid:89)m=1

second, each tag ym depends only on its predecessor,

p(y) =

p(ym | ym   1),

m(cid:89)m=1

where y0 =     in all cases. due to this markov assumption, probabilistic sequence labeling
models are known as id48 (id48s).

the generative process for the hidden markov model is shown in algorithm 12. given
the parameters    and   , we can compute p(w, y) for any token sequence w and tag se-
quence y. the id48 is often represented as a graphical model (wainwright and jordan,
2008), as shown in figure 7.2. this representation makes the independence assumptions
explicit: if a variable v1 is probabilistically conditioned on another variable v2, then there
is an arrow v2     v1 in the diagram.
if there are no arrows between v1 and v2, they
are conditionally independent, given each variable   s markov blanket.
in the hidden
markov model, the markov blanket for each tag ym includes the    parent    ym   1, and the
   children    ym+1 and wm.3

it is important to re   ect on the implications of the id48 independence assumptions.
a non-adjacent pair of tags ym and yn are conditionally independent; if m < n and we
are given yn   1, then ym offers no additional information about yn. however, if we are
not given any information about the tags in a sequence, then all tags are probabilistically
coupled.

3in general id114, a variable   s markov blanket includes its parents, children, and its children   s

other parents (murphy, 2012).

jacob eisenstein. draft of november 13, 2018.

7.4. id48

155

figure 7.2: graphical representation of the hidden markov model. arrows indicate prob-
abilistic dependencies.

7.4.1 estimation
the hidden markov model has two groups of parameters:

emission probabilities. the id203 pe(wm | ym;   ) is the emission id203, since

the words are treated as probabilistically    emitted   , conditioned on the tags.

transition probabilities. the id203 pt(ym | ym   1;   ) is the transition id203,

since it assigns id203 to each possible tag-to-tag transition.

both of these groups of parameters are typically computed from smoothed relative
frequency estimation on a labeled corpus (see    6.2 for a review of smoothing). the un-
smoothed probabilities are,

  k,i (cid:44) pr(wm = i | ym = k) =
| ym   1 = k) =
  k,k(cid:48)

(cid:44) pr(ym = k

(cid:48)

count(wm = i, ym = k)

count(ym = k)
count(ym = k(cid:48), ym   1 = k)

count(ym   1 = k)

.

smoothing is more important for the emission id203 than the transition id203,
because the vocabulary is much larger than the number of tags.

id136

7.4.2
the goal of id136 in the hidden markov model is to    nd the highest id203 tag
sequence,

  y = argmax

y

p(y | w).

[7.34]

as in na    ve bayes, it is equivalent to    nd the tag sequence with the highest log-id203,
since the logarithm is a monotonically increasing function. it is furthermore equivalent
to maximize the joint id203 p(y, w) = p(y | w)    p(w)     p(y | w), which is pro-
portional to the id155. putting these observations together, the id136

under contract with mit press, shared under cc-by-nc-nd license.

y1y2      ymw1w2      wm156

chapter 7. sequence labeling

problem can be reformulated as,

  y = argmax

y

log p(y, w).

we can now apply the id48 independence assumptions:

log p(y, w) = log p(y) + log p(w | y)

=

=

=

m +1(cid:88)m=1
m +1(cid:88)m=1
m +1(cid:88)m=1

log py (ym | ym   1) + log pw|y (wm | ym)

log   ym,ym   1 + log   ym,wm

sm(ym, ym   1),

sm(ym, ym   1) (cid:44) log   ym,ym   1 + log   ym,wm,

  (cid:7),w =(cid:40)1, w = (cid:4)

0, otherwise,

where,

and,

which ensures that the stop tag (cid:7) can only be applied to the    nal token (cid:4).

this derivation shows that id48 id136 can be viewed as an application of the
viterbi decoding algorithm, given an appropriately de   ned scoring function. the local
score sm(ym, ym   1) can be interpreted probabilistically,

sm(ym, ym   1) = log py(ym | ym   1) + log pw|y(wm | ym)

= log p(ym, wm | ym   1).

now recall the de   nition of the viterbi variables,

vm(ym) = max
ym   1
= max
ym   1

sm(ym, ym   1) + vm   1(ym   1)
log p(ym, wm | ym   1) + vm   1(ym   1).

by setting vm   1(ym   1) = maxy1:m   2 log p(y1:m   1, w1:m   1), we obtain the recurrence,

vm(ym) = max
ym   1
= max
y1:m   1
= max
y1:m   1

log p(ym, wm | ym   1) + max
y1:m   2
log p(ym, wm | ym   1) + log p(y1:m   1, w1:m   1)
log p(y1:m, w1:m).

log p(y1:m   1, w1:m   1)

jacob eisenstein. draft of november 13, 2018.

[7.35]

[7.36]

[7.37]

[7.38]

[7.39]

[7.40]

[7.41]

[7.42]
[7.43]

[7.44]

[7.45]

[7.46]

[7.47]

[7.48]

7.5. discriminative sequence labeling with features

157

in words, the viterbi variable vm(ym) is the log id203 of the best tag sequence ending
in ym, joint with the word sequence w1:m. the log id203 of the best complete tag
sequence is therefore,

log p(y1:m +1, w1:m +1) = vm +1((cid:7))

max
y1:m

[7.49]

*viterbi as an example of the max-product algorithm the viterbi algorithm can also be
implemented using probabilities, rather than log-probabilities. in this case, each vm(ym)
is equal to,

p(y1:m   1, ym, w1:m)
p(ym, wm | ym   1)    max
y1:m   2
p(ym, wm | ym   1)    vm   1(ym   1)

vm(ym) = max
y1:m   1
= max
ym   1
= max
ym   1
=pw|y(wm | ym)    max
ym   1

py(ym | ym   1)    vm   1(ym   1).

p(y1:m   2, ym   1, w1:m   1)

[7.50]

[7.51]

[7.52]

[7.53]

each viterbi variable is computed by maximizing over a set of products. thus, the viterbi
algorithm is a special case of the max-product algorithm for id136 in graphical mod-
els (wainwright and jordan, 2008). however, the product of probabilities tends towards
zero over long sequences, so the log-id203 version of viterbi is recommended in
practical implementations.

7.5 discriminative sequence labeling with features

today, id48 are rarely used for supervised sequence labeling. this is
because id48s are limited to only two phenomena:

    word-tag compatibility, via the emission id203 pw|y (wm | ym);
    local context, via the transition id203 py (ym | ym   1).

the viterbi algorithm permits the inclusion of richer information in the local scoring func-
tion   (w1:m , ym, ym   1, m), which can be de   ned as a weighted sum of arbitrary local fea-
tures,

  (w, ym, ym   1, m) =       f (w, ym, ym   1, m),

[7.54]

where f is a locally-de   ned feature function, and    is a vector of weights.

under contract with mit press, shared under cc-by-nc-nd license.

158

chapter 7. sequence labeling

the local decomposition of the scoring function    is re   ected in a corresponding de-

composition of the feature function:

  (w, y) =

=

  (w, ym, ym   1, m)

m +1(cid:88)m=1
m +1(cid:88)m=1
      f (w, ym, ym   1, m)
m +1(cid:88)m=1

=     
=      f (global)(w, y1:m ),

f (w, ym, ym   1, m)

[7.55]

[7.56]

[7.57]

[7.58]

where f (global)(w, y) is a global feature vector, which is a sum of local feature vectors,

m +1(cid:88)m=1

f (global)(w, y) =

f (w1:m , ym, ym   1, m),

[7.59]

with ym +1 = (cid:7) and y0 =     by construction.

let   s now consider what additional information these features might encode.

word af   x features. consider the problem of part-of-speech tagging on the    rst four
lines of the poem jabberwocky (carroll, 1917):

(7.3)

   twas brillig, and the slithy toves
did gyre and gimble in the wabe:
all mimsy were the borogoves,
and the mome raths outgrabe.

many of these words were made up by the author of the poem, so a corpus would offer
no information about their probabilities of being associated with any particular part of
speech. yet it is not so hard to see what their grammatical roles might be in this passage.
context helps: for example, the word slithy follows the determiner the, so it is probably a
noun or adjective. which do you think is more likely? the suf   x -thy is found in a number
of adjectives, like frothy, healthy, pithy, worthy. it is also found in a handful of nouns     e.g.,
apathy, sympathy     but nearly all of these have the longer coda -pathy, unlike slithy. so the
suf   x gives some evidence that slithy is an adjective, and indeed it is: later in the text we
   nd that it is a combination of the adjectives lithe and slimy.4

4morphology is the study of how words are formed from smaller linguistic units. chapter 9 touches on
computational approaches to morphological analysis. see bender (2013) for an overview of the underlying
linguistic principles, and haspelmath and sims (2013) or lieber (2015) for a full treatment.

jacob eisenstein. draft of november 13, 2018.

7.5. discriminative sequence labeling with features

159

fine-grained context. the hidden markov model captures contextual information in the
form of part-of-speech tag bigrams. but sometimes, the necessary contextual information
is more speci   c. consider the noun phrases this    sh and these    sh. many part-of-speech
tagsets distinguish between singular and plural nouns, but do not distinguish between
singular and plural determiners; for example, the well known id32 tagset fol-
lows these conventions. a hidden markov model would be unable to correctly label    sh as
singular or plural in both of these cases, because it only has access to two features: the pre-
ceding tag (determiner in both cases) and the word (   sh in both cases). the classi   cation-
based tagger discussed in    7.1 had the ability to use preceding and succeeding words as
features, and it can also be incorporated into a viterbi-based sequence labeler as a local
feature.

example. consider the tagging d j n (determiner, adjective, noun) for the sequence the
slithy toves, so that

w =the slithy toves
y =d j n.

let   s create the feature vector for this example, assuming that we have word-tag features
(indicated by w ), tag-tag features (indicated by t ), and suf   x features (indicated by m).
you can assume that you have access to a method for extracting the suf   x -thy from slithy,
-es from toves, and     from the, indicating that this word has no suf   x.5 the resulting
feature vector is,

f (the slithy toves, d j n) =f (the slithy toves, d,    , 1)

+ f (the slithy toves, j, d, 2)
+ f (the slithy toves, n, j, 3)
+ f (the slithy toves, (cid:7), n, 4)
={(t :    , d), (w : the, d), (m :    , d),
(t : d, j), (w : slithy, j), (m : -thy, j),
(t : j, n), (w : toves, n), (m : -es, n)
(t : n, (cid:7))}.

these examples show that local features can incorporate information that lies beyond
the scope of a hidden markov model. because the features are local, it is possible to apply
the viterbi algorithm to identify the optimal sequence of tags. the remaining question

5such a system is called a morphological segmenter. the task of morphological segmentation is brie   y
described in    9.1.4; a well known segmenter is morfessor (creutz and lagus, 2007). in real applications, a
typical approach is to include features for all orthographic suf   xes up to some maximum number of charac-
ters: for slithy, we would have suf   x features for -y, -hy, and -thy.

under contract with mit press, shared under cc-by-nc-nd license.

160

chapter 7. sequence labeling

is how to estimate the weights on these features.    2.3 presented three main types of
discriminative classi   ers: id88, support vector machine, and id28.
each of these classi   ers has a structured equivalent, enabling it to be trained from labeled
sequences rather than individual tokens.

7.5.1 structured id88
the id88 classi   er is trained by increasing the weights for features that are asso-
ciated with the correct label, and decreasing the weights for features that are associated
with incorrectly predicted labels:

  y = argmax

y   y       f (x, y)

  (t+1)       (t) + f (x, y)     f (x,   y).

we can apply exactly the same update in the case of structure prediction,

  y = argmax
y   y(w)

      f (w, y)

  (t+1)       (t) + f (w, y)     f (w,   y).

[7.60]

[7.61]

[7.62]

[7.63]

this learning algorithm is called structured id88, because it learns to predict the
structured output y. the only difference is that instead of computing   y by enumerating
the entire set y, the viterbi algorithm is used to ef   ciently search the set of possible tag-
gings, y m . structured id88 can be applied to other structured outputs as long as
ef   cient id136 is possible. as in id88 classi   cation, weight averaging is crucial
to get good performance (see    2.3.2).
example for the example they can    sh, suppose that the reference tag sequence is y(i) =
n v v, but the tagger incorrectly returns the tag sequence   y = n v n. assuming a model
with features for emissions (wm, ym) and transitions (ym   1, ym), the corresponding struc-
tured id88 update is:

  (   sh,v)       (   sh,v) + 1,
  (v,v)       (v,v) + 1,
  (v,(cid:7))       (v,(cid:7)) + 1,

  (   sh,n)       (   sh,n)     1
  (v,n)       (v,n)     1
  (n,(cid:7))       (n,(cid:7))     1.

[7.64]
[7.65]
[7.66]

7.5.2 structured support vector machines
large-margin classi   ers such as the support vector machine improve on the id88 by
pushing the classi   cation boundary away from the training instances. the same idea can

jacob eisenstein. draft of november 13, 2018.

7.5. discriminative sequence labeling with features

161

be applied to sequence labeling. a support vector machine in which the output is a struc-
tured object, such as a sequence, is called a structured support vector machine (tsochan-
taridis et al., 2004).6

in classi   cation, we formalized the large-margin constraint as,
   y (cid:54)= y(i),       f (x, y(i))           f (x, y)     1,

[7.67]

requiring a margin of at least 1 between the scores for all labels y that are not equal to the
correct label y(i). the weights    are then learned by constrained optimization (see    2.4.2).
this idea can be applied to sequence labeling by formulating an equivalent set of con-
straints for all possible labelings y(w) for an input w. however, there are two problems.
first, in sequence labeling, some predictions are more wrong than others: we may miss
only one tag out of    fty, or we may get all    fty wrong. we would like our learning algo-
rithm to be sensitive to this difference. second, the number of constraints is equal to the
number of possible labelings, which is exponentially large in the length of the sequence.
the    rst problem can be addressed by adjusting the constraint to require larger mar-
gins for more serious errors. let c(y(i),   y)     0 represent the cost of predicting label   y when
the true label is y(i). we can then generalize the margin constraint,
   y,       f (w(i), y(i))           f (w(i), y)     c(y(i), y).

[7.68]

this cost-augmented margin constraint specializes to the constraint in equation 7.67 if we
choose the delta function c(y(i), y) =    (() y(i) (cid:54)= y). a more expressive cost function is
the hamming cost,

c(y(i), y) =

  (y(i)

m (cid:54)= ym),

[7.69]

m(cid:88)m=1

which computes the number of errors in y. by incorporating the cost function as the
margin constraint, we require that the true labeling be seperated from the alternatives by
a margin that is proportional to the number of incorrect tags in each alternative labeling.
the second problem is that the number of constraints is exponential in the length
of the sequence. this can be addressed by focusing on the prediction   y that maximally
violates the margin constraint. this prediction can be identi   ed by solving the following
cost-augmented decoding problem:

  y = argmax
y(cid:54)=y(i)
= argmax
y(cid:54)=y(i)

      f (w(i), y)           f (w(i), y(i)) + c(y(i), y)
      f (w(i), y) + c(y(i), y),

[7.70]

[7.71]

6this model is also known as a max-margin markov network (taskar et al., 2003), emphasizing that the

scoring function is constructed from a sum of components, which are markov independent.

under contract with mit press, shared under cc-by-nc-nd license.

162

chapter 7. sequence labeling

where in the second line we drop the term       f (w(i), y(i)), which is constant in y.

we can now reformulate the margin constraint for sequence labeling,

      f (w(i), y(i))     max

y   y(w)(cid:16)      f (w(i), y) + c(y(i), y)(cid:17)     0.

[7.72]

if the score for       f (w(i), y(i)) is greater than the cost-augmented score for all alternatives,
then the constraint will be met. the name    cost-augmented decoding    is due to the fact
that the objective includes the standard decoding problem, max   y   y(w)       f (w,   y), plus
an additional term for the cost. essentially, we want to train against predictions that are
strong and wrong: they should score highly according to the model, yet incur a large loss
with respect to the ground truth. training adjusts the weights to reduce the score of these
predictions.

for cost-augmented decoding to be tractable, the cost function must decompose into
local parts, just as the feature function f (  ) does. the hamming cost, de   ned above,
obeys this property. to perform cost-augmented decoding using the hamming cost, we
need only to add features fm(ym) =   (ym (cid:54)= y(i)
m ), and assign a constant weight of 1 to
these features. decoding can then be performed using the viterbi algorithm.7

as with large-margin classi   ers, it is possible to formulate the learning problem in an
unconstrained form, by combining a id173 term on the weights and a lagrangian
for the constraints:

min

  

1
2||  ||2

2     c(cid:32)(cid:88)i

      f (w(i), y(i))     max

y   y(w(i))(cid:104)      f (w(i), y) + c(y(i), y)(cid:105)(cid:33) ,

[7.73]

in this formulation, c is a parameter that controls the tradeoff between the regulariza-
tion term and the margin constraints. a number of optimization algorithms have been
proposed for structured support vector machines, some of which are discussed in    2.4.2.
an empirical comparison by kummerfeld et al. (2015) shows that stochastic subgradient
descent     which is essentially a cost-augmented version of the structured id88    
is highly competitive.

7.5.3 conditional random    elds
the conditional random    eld (crf; lafferty et al., 2001) is a conditional probabilistic
model for sequence labeling; just as structured id88 is built on the id88 clas-
si   er, conditional random    elds are built on the id28 classi   er.8 the basic
7are there cost functions that do not decompose into local parts? suppose we want to assign a constant
loss c to any prediction   y in which k or more predicted tags are incorrect, and zero loss otherwise. this loss
function is combinatorial over the predictions, and thus we cannot decompose it into parts.

8the name    conditional random    eld    is derived from markov random    elds, a general class of models
in which the id203 of a con   guration of variables is proportional to a product of scores across pairs (or

jacob eisenstein. draft of november 13, 2018.

7.5. discriminative sequence labeling with features

163

id203 model is,

p(y | w) =

exp(  (w, y))

(cid:80)y(cid:48)   y(w) exp(  (w, y(cid:48)))

.

[7.74]

this is almost identical to id28 (   2.5), but because the label space is now
sequences of tags, we require ef   cient algorithms for both decoding (searching for the
best tag sequence given a sequence of words w and a model   ) and for id172
(summing over all tag sequences). these algorithms will be based on the usual locality

assumption on the scoring function,   (w, y) =(cid:80)m +1
decoding in crfs
decoding        nding the tag sequence   y that maximizes p(y | w)     is a direct applica-
tion of the viterbi algorithm. the key observation is that the decoding problem does not
depend on the denominator of p(y | w),

m=1   (w, ym, ym   1, m).

  y = argmax

y

= argmax

y

= argmax

y

log p(y | w)
  (y, w)     log (cid:88)y(cid:48)   y(w)
m +1(cid:88)m=1

  (y, w) = argmax

y

exp   (y

(cid:48)

, w)

sm(ym, ym   1).

this is identical to the decoding problem for structured id88, so the same viterbi
recurrence as de   ned in equation 7.22 can be used.

learning in crfs

as with id28, the weights    are learned by minimizing the regularized neg-
ative log-id203,

(cid:96) =

  
2||  ||2    

=

  
2||  ||2    

n(cid:88)i=1
n(cid:88)i=1

log p(y(i) | w(i);   )
      f (w(i), y(i)) + log (cid:88)y(cid:48)   y(w(i))

exp(cid:16)      f (w(i), y

(cid:48)

)(cid:17) ,

[7.75]

[7.76]

more generally, cliques) of variables in a factor graph. in sequence labeling, the pairs of variables include
all adjacent tags (ym, ym   1). the id203 is conditioned on the words w, which are always observed,
motivating the term    conditional    in the name.

under contract with mit press, shared under cc-by-nc-nd license.

164

chapter 7. sequence labeling

where    controls the amount of id173. the    nal term in equation 7.76 is a sum
over all possible labelings. this term is the log of the denominator in equation 7.74, some-
times known as the partition function.9 there are |y|m possible labelings of an input of
size m, so we must again exploit the decomposition of the scoring function to compute
this sum ef   ciently.
the sum(cid:80)y   yw(i) exp   (y, w) can be computed ef   ciently using the forward recur-

rence, which is closely related to the viterbi recurrence. we    rst de   ne a set of forward
variables,   m(ym), which is equal to the sum of the scores of all paths leading to tag ym at
position m:

  m(ym) (cid:44) (cid:88)y1:m   1
= (cid:88)y1:m   1

exp

m(cid:89)n=1

sn(yn, yn   1)

m(cid:88)n=1

exp sn(yn, yn   1).

[7.77]

[7.78]

note the similarity to the de   nition of the viterbi variable, vm(ym) = maxy1:m   1(cid:80)m

in the hidden markov model, the viterbi recurrence had an alternative interpretation as
the max-product algorithm (see equation 7.53); analogously, the forward recurrence is
known as the sum-product algorithm, because of the form of [7.78]. the forward variable
can also be computed through a recurrence:

n=1 sn(yn, yn   1).

  m(ym) = (cid:88)y1:m   1
= (cid:88)ym   1
= (cid:88)ym   1

exp sn(yn, yn   1)

m(cid:89)n=1
(exp sm(ym, ym   1)) (cid:88)y1:m   2
(exp sm(ym, ym   1))      m   1(ym   1).

m   1(cid:89)n=1

exp sn(yn, yn   1)

[7.79]

[7.80]

[7.81]

using the forward recurrence, it is possible to compute the denominator of the condi-

tional id203,

(cid:88)y   y(w)

(exp sm +1((cid:7), ym ))

  (w, y) =(cid:88)y1:m

=  m +1((cid:7)).

exp sm(ym, ym   1)

m(cid:89)m=1

[7.82]

[7.83]

9the terminology of    potentials    and    partition functions    comes from statistical mechanics (bishop,

2006).

jacob eisenstein. draft of november 13, 2018.

7.5. discriminative sequence labeling with features

165

the conditional log-likelihood can be rewritten,

(cid:96) =

  
2||  ||2    

n(cid:88)i=1

      f (w(i), y(i)) + log   m +1((cid:7)).

[7.84]

probabilistic programming environments, such as torch (collobert et al., 2011) and
dynet (neubig et al., 2017), can compute the gradient of this objective using automatic
differentiation. the programmer need only implement the forward algorithm as a com-
putation graph.

as in id28, the gradient of the likelihood with respect to the parameters

is a difference between observed and expected feature counts:

d(cid:96)
d  j

=    j +

n(cid:88)i=1

e[fj(w(i), y)]     fj(w(i), y(i)),

[7.85]

where fj(w(i), y(i)) refers to the count of feature j for token sequence w(i) and tag se-
quence y(i). the expected feature counts are computed    under the hood    when automatic
differentiation is applied to equation 7.84 (eisner, 2016).

before the widespread use of automatic differentiation, it was common to compute
the feature expectations from marginal tag probabilities p(ym | w). these marginal prob-
abilities are sometimes useful on their own, and can be computed using the forward-
backward algorithm. this algorithm combines the forward recurrence with an equivalent
backward recurrence, which traverses the input from wm back to w1.

*forward-backward algorithm

marginal probabilities over tag bigrams can be written as,10

pr(ym   1 = k

(cid:48)

, ym = k | w) =(cid:80)y:ym=k,ym   1=k(cid:48)(cid:81)m

n=1 exp sn(y(cid:48)

n, y(cid:48)

n   1)

n=1 exp sn(yn, yn   1)

.

[7.86]

the numerator sums over all tag sequences that include the transition (ym   1 = k(cid:48))    
(ym = k). because we are only interested in sequences that include the tag bigram, this
sum can be decomposed into three parts: the pre   xes y1:m   1, terminating in ym   1 = k(cid:48); the

(cid:80)y(cid:48)(cid:81)m

10recall the notational convention of upper-case letters for random variables, e.g. ym, and lower case
letters for speci   c values, e.g., ym, so that ym = k is interpreted as the event of random variable ym taking
the value k.

under contract with mit press, shared under cc-by-nc-nd license.

166

chapter 7. sequence labeling

figure 7.3: a schematic illustration of the computation of the marginal id203
pr(ym   1 = k(cid:48), ym = k), using the forward score   m   1(k(cid:48)) and the backward score   m(k).

transition (ym   1 = k(cid:48))     (ym = k); and the suf   xes ym:m , beginning with the tag ym = k:

(cid:88)y:ym=k,ym   1=k(cid:48)

m(cid:89)n=1

exp sn(yn, yn   1) = (cid:88)y1:m   1:ym   1=k(cid:48)
   (cid:88)ym:m :ym=k

   exp sm(k, k

(cid:48)

m   1(cid:89)n=1
m +1(cid:89)n=m+1

)

exp sn(yn, yn   1)

exp sn(yn, yn   1).

[7.87]

the result is product of three terms: a score that sums over all the ways to get to the
position (ym   1 = k(cid:48)), a score for the transition from k(cid:48) to k, and a score that sums over
all the ways of    nishing the sequence from (ym = k). the    rst term of equation 7.87 is
equal to the forward variable,   m   1(k(cid:48)). the third term     the sum over ways to    nish the
sequence     can also be de   ned recursively, this time moving over the trellis from right to
left, which is known as the backward recurrence:

  m(k) (cid:44) (cid:88)ym:m :ym=k

m +1(cid:89)n=m

exp sn(yn, yn   1)

exp sm+1(k

exp sm+1(k

(cid:48)

(cid:48)

, k) (cid:88)ym+1:m :ym=k(cid:48)

, k)      m+1(k

(cid:48)

).

=(cid:88)k(cid:48)   y
=(cid:88)k(cid:48)   y

m +1(cid:89)n=m+1

exp sn(yn, yn   1)

[7.88]

[7.89]

[7.90]

to understand this computation, compare with the forward recurrence in equation 7.81.

jacob eisenstein. draft of november 13, 2018.

ym   1=k0ym=k  m   1(k0)expsm(k,k0)  m(k)7.6. neural sequence labeling

in practice, numerical stability demands that we work in the log domain,

log   m(k) = log(cid:88)k(cid:48)   y
log   m   1(k) = log(cid:88)k(cid:48)   y

(cid:48)

exp(cid:0)log sm(k, k
exp(cid:0)log sm(k

(cid:48)

) + log   m   1(k

, k) + log   m(k

(cid:48)

)(cid:1)
)(cid:1) .

(cid:48)

167

[7.91]

[7.92]

the application of the forward and backward probabilities is shown in figure 7.3.
both the forward and backward recurrences operate on the trellis, which implies a space
complexity o(m k). because both recurrences require computing a sum over k terms at
each node in the trellis, their time complexity is o(m k2).
7.6 neural sequence labeling

in neural network approaches to sequence labeling, we construct a vector representa-
tion for each tagging decision, based on the word and its context. neural networks can
perform tagging as a per-token classi   cation decision, or they can be combined with the
viterbi algorithm to tag the entire sequence globally.

7.6.1 recurrent neural networks
recurrent neural networks (id56s) were introduced in chapter 6 as a language model-
ing technique, in which the context at token m is summarized by a recurrently-updated
vector,

hm =g(xm, hm   1), m = 1, 2, . . . m,

where xm is the vector embedding of the token wm and the function g de   nes the recur-
rence. the starting condition h0 is an additional parameter of the model. the long short-
term memory (lstm) is a more complex recurrence, in which a memory cell is through a
series of gates, avoiding repeated application of the non-linearity. despite these bells and
whistles, both models share the basic architecture of recurrent updates across a sequence,
and both will be referred to as id56s here.

a straightforward application of id56s to sequence labeling is to score each tag ym as

a linear function of hm:

  m(y) =  y    hm
  ym = argmax

y

  m(y).

[7.93]
[7.94]

the score   m(y) can also be converted into a id203 distribution using the usual soft-
max operation,

p(y | w1:m) =

exp   m(y)

(cid:80)y(cid:48)   y exp   m(y(cid:48))

.

[7.95]

under contract with mit press, shared under cc-by-nc-nd license.

168

chapter 7. sequence labeling

using this transformation, it is possible to train the tagger from the negative log-likelihood
of the tags, as in a conditional random    eld. alternatively, a hinge loss or margin loss
objective can be constructed from the raw scores   m(y).

the hidden state hm accounts for information in the input leading up to position m,
but it ignores the subsequent tokens, which may also be relevant to the tag ym. this can
be addressed by adding a second id56, in which the input is reversed, running the recur-
rence from wm to w1. this is known as a bidirectional recurrent neural network (graves
and schmidhuber, 2005), and is speci   ed as:

      h m =g(xm,      h m+1), m = 1, 2, . . . , m.

[7.96]
the hidden states of the left-to-right id56 are denoted       h m. the left-to-right and right-to-
left vectors are concatenated, hm = [      h m;      h m]. the scoring function in equation 7.93 is
applied to this concatenated vector.

bidirectional id56 tagging has several attractive properties. ideally, the representa-
tion hm summarizes the useful information from the surrounding context, so that it is not
necessary to design explicit features to capture this information. if the vector hm is an ad-
equate summary of this context, then it may not even be necessary to perform the tagging
jointly: in general, the gains offered by joint tagging of the entire sequence are diminished
as the individual tagging model becomes more powerful. using id26, the
word vectors x can be trained    end-to-end   , so that they capture word properties that are
useful for the tagging task. alternatively, if limited labeled data is available, we can use
id27s that are    pre-trained    from unlabeled data, using a id38
objective (as in    6.3) or a related id27 technique (see chapter 14). it is even
possible to combine both    ne-tuned and pre-trained embeddings in a single model.

neural structure prediction the bidirectional recurrent neural network incorporates in-
formation from throughout the input, but each tagging decision is made independently.
in some sequence labeling applications, there are very strong dependencies between tags:
it may even be impossible for one tag to follow another. in such scenarios, the tagging
decision must be made jointly across the entire sequence.

neural sequence labeling can be combined with the viterbi algorithm by de   ning the

local scores as:

sm(ym, ym   1) =   ym    hm +   ym   1,ym,

[7.97]
where hm is the id56 hidden state,   ym is a vector associated with tag ym, and   ym   1,ym
is a scalar parameter for the tag transition (ym   1, ym). these local scores can then be
incorporated into the viterbi algorithm for id136, and into the forward algorithm for
training. this model is shown in figure 7.4. it can be trained from the conditional log-
likelihood objective de   ned in equation 7.76, backpropagating to the tagging parameters

jacob eisenstein. draft of november 13, 2018.

7.6. neural sequence labeling

169

figure 7.4: bidirectional lstm for sequence labeling. the solid lines indicate computa-
tion, the dashed lines indicate probabilistic dependency, and the dotted lines indicate the
optional additional probabilistic dependencies between labels in the bilstm-crf.

   and   , as well as the parameters of the id56. this model is called the lstm-crf, due
to its combination of aspects of the long short-term memory and conditional random    eld
models (huang et al., 2015).

the lstm-crf is especially effective on the task of id39 (lample
et al., 2016), a sequence labeling task that is described in detail in    8.3. this task has strong
dependencies between adjacent tags, so structure prediction is especially important.

7.6.2 character-level models

as in id38, rare and unseen words are a challenge: if we encounter a word
that was not in the training data, then there is no obvious choice for the word embed-
ding xm. one solution is to use a generic unseen id27 for all such words.
however, in many cases, properties of unseen words can be guessed from their spellings.
for example, whimsical does not appear in the universal dependencies (ud) english tree-
bank, yet the suf   x -al makes it likely to be adjective; by the same logic, un   inchingly is
likely to be an adverb, and barnacle is likely to be a noun.

in feature-based models, these morphological properties were handled by suf   x fea-
tures; in a neural network, they can be incorporated by constructing the embeddings of
unseen words from their spellings or morphology. one way to do this is to incorporate
an additional layer of bidirectional id56s, one for each word in the vocabulary (ling
et al., 2015). for each such character-id56, the inputs are the characters, and the output
is the concatenation of the    nal states of the left-facing and right-facing passes,   w =

under contract with mit press, shared under cc-by-nc-nd license.

ym   1ymym+1      hm   1      hm      hm+1      hm   1      hm      hm+1xm   1xmxm+1170

chapter 7. sequence labeling

0

nw

;      h (w)

], where       h (w)

[      h (w)
nw is the    nal state of the right-facing pass for word w, and nw
is the number of characters in the word. the character id56 model is trained by back-
propagation from the tagging objective. on the test data, the trained id56 is applied to
out-of-vocabulary words (or all words), yielding inputs to the word-level tagging id56.
other approaches to compositional id27s are described in    14.7.1.

7.6.3 convolutional neural networks for sequence labeling

one disadvantage of recurrent neural networks is that the architecture requires iterating
through the sequence of inputs and predictions: each hidden vector hm must be com-
puted from the previous hidden vector hm   1, before predicting the tag ym. these iterative
computations are dif   cult to parallelize, and fail to exploit the speedups offered by graph-
ics processing units (gpus) on operations such as id127. convolutional
neural networks achieve better computational performance by predicting each label ym
from a set of matrix operations on the neighboring id27s, xm   k:m+k (col-
lobert et al., 2011). because there is no hidden state to update, the predictions for each
ym can be computed in parallel. for more on convolutional neural networks, see    3.4.
character-based id27s can also be computed using convolutional neural net-
works (santos and zadrozny, 2014).

7.7

*unsupervised sequence labeling

in unsupervised sequence labeling, the goal is to induce a hidden markov model from a
corpus of unannotated text (w(1), w(2), . . . , w(n )), where each w(i) is a sequence of length
m (i). this is an example of the general problem of structure induction, which is the
unsupervised version of structure prediction. the tags that result from unsupervised se-
quence labeling might be useful for some downstream task, or they might help us to better
understand the language   s inherent structure. for part-of-speech tagging, it is common
to use a tag dictionary that lists the allowed tags for each word, simplifying the prob-
lem (christodoulopoulos et al., 2010).

unsupervised learning in id48 can be performed using the baum-

welch algorithm, which combines the forward-backward algorithm (   7.5.3) with expectation-
maximization (em;    5.1.2). in the m-step, the id48 parameters from expected counts:

pr(w = i | y = k) =   k,i =

pr(ym = k | ym   1 = k

(cid:48)

) =   k(cid:48),k =

e[count(w = i, y = k)]

e[count(y = k)]

e[count(ym = k, ym   1 = k(cid:48))]

e[count(ym   1 = k(cid:48))]

jacob eisenstein. draft of november 13, 2018.

7.7. *unsupervised sequence labeling

171

the expected counts are computed in the e-step, using the forward and backward

recurrences. the local scores follow the usual de   nition for id48,

(cid:48)

sm(k, k

) = log pe(wm | ym = k;   ) + log pt (ym = k | ym   1 = k

(cid:48)

;   ).

[7.98]

the expected transition counts for a single instance are,

e[count(ym = k, ym   1 = k

(cid:48)

) | w] =

(cid:48)

pr(ym   1 = k

m(cid:88)m=1
=(cid:80)y:ym=k,ym   1=k(cid:48)(cid:81)m

(cid:80)y(cid:48)(cid:81)m

, ym = k | w)
n=1 exp sn(yn, yn   1)

[7.99]

.

[7.100]

n=1 exp sn(y(cid:48)

n, y(cid:48)

n   1)

as described in    7.5.3, these marginal probabilities can be computed from the forward-
backward recurrence,

pr(ym   1 = k

(cid:48)

, ym = k | w) =

  m   1(k(cid:48))    exp sm(k, k(cid:48))      m(k)

  m +1((cid:7))

.

[7.101]

in a hidden markov model, each element of the forward-backward computation has a

special interpretation:

(cid:48)

  m   1(k
exp sm(k, k

(cid:48)
(cid:48)

) =p(ym   1 = k
) =p(ym = k, wm | ym   1 = k

, w1:m   1)

  m(k) =p(wm+1:m | ym = k).

(cid:48)

)

[7.102]
[7.103]
[7.104]

applying the conditional independence assumptions of the hidden markov model (de-
   ned in algorithm 12), the product is equal to the joint id203 of the tag bigram and
the entire input,

(cid:48)

  m   1(k

)    exp sm(k, k

(cid:48)

)      m(k) =p(ym   1 = k

(cid:48)

, w1:m   1)

   p(ym = k, wm | ym   1 = k
   p(wm+1:m | ym = k)

=p(ym   1 = k

, ym = k, w1:m ).

(cid:48)

(cid:48)

)

dividing by   m +1((cid:7)) = p(w1:m ) gives the desired id203,

  m   1(k(cid:48))    sm(k, k(cid:48))      m(k)

  m +1((cid:7))

=

p(ym   1 = k(cid:48), ym = k, w1:m )

p(w1:m )

(cid:48)

= pr(ym   1 = k

, ym = k | w1:m ).

[7.105]

[7.106]

[7.107]

the expected emission counts can be computed in a similar manner, using the product

  m(k)      m(k).

under contract with mit press, shared under cc-by-nc-nd license.

172

chapter 7. sequence labeling

7.7.1 linear dynamical systems
the forward-backward algorithm can be viewed as bayesian state estimation in a discrete
state space. in a continuous state space, ym     rk, the equivalent algorithm is the kalman
smoother. it also computes marginals p(ym | x1:m ), using a similar two-step algorithm
of forward and backward passes. instead of computing a trellis of values at each step, the
kalman smoother computes a id203 density function qym(ym;   m,   m), character-
ized by a mean   m and a covariance   m around the latent state. connections between the
kalman smoother and the forward-backward algorithm are elucidated by minka (1999)
and murphy (2012).

7.7.2 alternative unsupervised learning methods
as noted in    5.5, expectation-maximization is just one of many techniques for structure
induction. one alternative is to use id115 (mcmc) sampling al-
gorithms, which are brie   y described in    5.5.1. for the speci   c case of sequence labeling,
id150 can be applied by iteratively sampling each tag ym conditioned on all the
others (finkel et al., 2005):

p(ym | y   m, w1:m )     p(wm | ym)p(ym | y   m).

[7.108]
id150 has been applied to unsupervised part-of-speech tagging by goldwater
and grif   ths (2007). beam sampling is a more sophisticated sampling algorithm, which
randomly draws entire sequences y1:m , rather than individual tags ym; this algorithm
was applied to unsupervised part-of-speech tagging by van gael et al. (2009). spectral
learning (see    5.5.2) can also be applied to sequence labeling. by factoring matrices of
co-occurrence counts of word bigrams and trigrams (song et al., 2010; hsu et al., 2012), it
is possible to obtain globally optimal estimates of the transition and emission parameters,
under mild assumptions.

7.7.3 semiring notation and the generalized viterbi algorithm
the viterbi and forward recurrences can each be performed over probabilities or log
probabilities, yielding a total of four closely related recurrences. these four recurrence
scan in fact be expressed as a single recurrence in a more general notation, known as
semiring algebra. let the symbols     and     represent generalized addition and multipli-
cation respectively.11 given these operators, a generalized viterbi recurrence is denoted,

vm(k) = (cid:77)k(cid:48)   y

(cid:48)

sm(k, k

)     vm   1(k

(cid:48)

).

[7.109]

11in a semiring, the addition and multiplication operators must both obey associativity, and multiplication
must distribute across addition; the addition operator must be commutative; there must be additive and
multiplicative identities 0 and 1, such that a     0 = a and a     1 = a; and there must be a multiplicative
annihilator 0, such that a     0 = 0.

jacob eisenstein. draft of november 13, 2018.

7.7. *unsupervised sequence labeling

173

each recurrence that we have seen so far is a special case of this generalized viterbi

recurrence:

    in the max-product viterbi recurrence over probabilities, the     operation corre-

sponds to maximization, and the     operation corresponds to multiplication.

    in the forward recurrence over probabilities, the     operation corresponds to addi-

tion, and the     operation corresponds to multiplication.

    in the max-product viterbi recurrence over log-probabilities, the     operation corre-
sponds to maximization, and the     operation corresponds to addition.12
    in the forward recurrence over log-probabilities, the     operation corresponds to log-

addition, a     b = log(ea + eb). the     operation corresponds to addition.
the mathematical abstraction offered by semiring notation can be applied to the soft-
ware implementations of these algorithms, yielding concise and modular implementa-
tions. for example, in the openfst library, generic operations are parametrized by the
choice of semiring (allauzen et al., 2007).

exercises

1. extend the example in    7.3.1 to the sentence they can can    sh, meaning that    they can
put    sh into cans.    build the trellis for this example using the weights in table 7.1,
and identify the best-scoring tag sequence. if the scores for noun and verb are tied,
then you may assume that the backpointer always goes to noun.

2. using the tagset y = {n, v }, and the feature set f (w, ym, ym   1, m) = {(wm, ym), (ym, ym   1)},

show that there is no set of weights that give the correct tagging for both they can
   sh (n v v) and they can can    sh (n v v n).

3. work out what happens if you train a structured id88 on the two exam-
ples mentioned in the previous problem, using the transition and emission features
(ym, ym   1) and (ym, wm). initialize all weights at 0, and assume that the viterbi algo-
rithm always chooses n when the scores for the two tags are tied, so that the initial
prediction for they can    sh is n n n.

4. consider the garden path sentence, the old man the boat. given word-tag and tag-tag
features, what inequality in the weights must hold for the correct tag sequence to
outscore the garden path tag sequence for this example?

12this is sometimes called the tropical semiring, in honor of the brazilian mathematician imre simon.

under contract with mit press, shared under cc-by-nc-nd license.

174

chapter 7. sequence labeling

5. using the weights in table 7.1, explicitly compute the log-probabilities for all pos-
sible taggings of the input    sh can. verify that the forward algorithm recovers the
aggregate log id203.

6. sketch out an algorithm for a variant of viterbi that returns the top-n label se-

quences. what is the time and space complexity of this algorithm?

7. show how to compute the marginal id203 pr(ym   2 = k, ym = k(cid:48)

terms of the forward and backward variables, and the potentials sn(yn, yn   1).

| w1:m ), in

8. suppose you receive a stream of text, where some of tokens have been replaced at

random with noise. for example:

    source: i try all things, i achieve what i can
    message received: i try noise noise, i noise what i noise

assume you have access to a pre-trained bigram language model, which gives prob-
abilities p(wm | wm   1). these probabilities can be assumed to be non-zero for all
bigrams.
show how to use the viterbi algorithm to recover the source by maximizing the
bigram language model log-id203. speci   cally, set the scores sm(ym, ym   1) so
that the viterbi algorithm selects a sequence of words that maximizes the bigram
language model log-id203, while leaving the non-noise tokens intact. your
solution should not modify the logic of the viterbi algorithm, it should only set the
scores sm(ym, ym   1).

9. let   (  ) and   (  ) indicate the forward and backward variables as de   ned in    7.5.3.
prove that   m +1((cid:7)) =   0(   ) =(cid:80)y   m(y)  m(y),   m     {1, 2, . . . , m}.

10. consider an id56 tagging model with a tanh activation function on the hidden
layer, and a hinge loss on the output. (the problem also works for the margin loss
and negative log-likelihood.) suppose you initialize all parameters to zero: this in-
cludes the id27s that make up x, the transition matrix   , the output
weights   , and the initial hidden state h0.

a) prove that for any data and for any gradient-based learning algorithm, all pa-

rameters will be stuck at zero.

b) would a sigmoid activation function avoid this problem?

jacob eisenstein. draft of november 13, 2018.

chapter 8

applications of sequence labeling

sequence labeling has applications throughout natural language processing. this chap-
ter focuses on part-of-speech tagging, morpho-syntactic attribute tagging, named entity
recognition, and id121. it also touches brie   y on two applications to interactive
settings: dialogue act recognition and the detection of code-switching points between
languages.

8.1 part-of-speech tagging

the syntax of a language is the set of principles under which sequences of words are
judged to be grammatically acceptable by    uent speakers. one of the most basic syntactic
concepts is the part-of-speech (pos), which refers to the syntactic role of each word in a
sentence. this concept was used informally in the previous chapter, and you may have
some intuitions from your own study of english. for example, in the sentence we like
vegetarian sandwiches, you may already know that we and sandwiches are nouns, like is a
verb, and vegetarian is an adjective. these labels depend on the context in which the word
appears: in she eats like a vegetarian, the word like is a preposition, and the word vegetarian
is a noun.

parts-of-speech can help to disentangle or explain various linguistic problems. recall

chomsky   s proposed distinction in chapter 6:

(8.1)

a. colorless green ideas sleep furiously.
b.

* ideas colorless furiously green sleep.

one difference between these two examples is that the    rst contains part-of-speech tran-
sitions that are typical in english: adjective to adjective, adjective to noun, noun to verb,
and verb to adverb. the second example contains transitions that are unusual: noun to
adjective and adjective to verb. the ambiguity in a headline like,

175

176

chapter 8. applications of sequence labeling

(8.2) teacher strikes idle children

can also be explained in terms of parts of speech: in the interpretation that was likely
intended, strikes is a noun and idle is a verb; in the alternative explanation, strikes is a verb
and idle is an adjective.

part-of-speech tagging is often taken as a early step in a natural language processing
pipeline.
indeed, parts-of-speech provide features that can be useful for many of the
tasks that we will encounter later, such as parsing (chapter 10), coreference resolution
(chapter 15), and id36 (chapter 17).

8.1.1 parts-of-speech
the universal dependencies project (ud) is an effort to create syntactically-annotated
corpora across many languages, using a single annotation standard (nivre et al., 2016). as
part of this effort, they have designed a part-of-speech tagset, which is meant to capture
word classes across as many languages as possible.1 this section describes that inventory,
giving rough de   nitions for each of tags, along with supporting examples.

part-of-speech tags are morphosyntactic, rather than semantic, categories. this means
that they describe words in terms of how they pattern together and how they are inter-
nally constructed (e.g., what suf   xes and pre   xes they include). for example, you may
think of a noun as referring to objects or concepts, and verbs as referring to actions or
events. but events can also be nouns:

(8.3)

. . . the howling of the shrieking storm.

here howling and shrieking are events, but grammatically they act as a noun and adjective
respectively.

the universal dependency part-of-speech tagset

the ud tagset is broken up into three groups: open class tags, closed class tags, and
   others.   

open class tags nearly all languages contain nouns, verbs, adjectives, and adverbs.2
these are all open word classes, because new words can easily be added to them. the
ud tagset includes two other tags that are open classes: proper nouns and interjections.

    nouns (ud tag: noun) tend to describe entities and concepts, e.g.,
1the ud tagset builds on earlier work from petrov et al. (2012), in which a set of twelve universal tags

was identi   ed by creating mappings from tagsets for individual languages.

2one prominent exception is korean, which some linguists argue does not have adjectives kim (2002).

jacob eisenstein. draft of november 13, 2018.

8.1. part-of-speech tagging

177

(8.4) toes are scarce among veteran blubber men.

in english, nouns tend to follow determiners and adjectives, and can play the subject
role in the sentence. they can be marked for the plural number by an -s suf   x.

    proper nouns (propn) are tokens in names, which uniquely specify a given entity,

(8.5)    moby dick?    shouted ahab.

    verbs (verb), according to the ud guidelines,    typically signal events and ac-
tions.    but they are also de   ned grammatically: they    can constitute a minimal
predicate in a clause, and govern the number and types of other constituents which
may occur in a clause.   3

(8.6)    moby dick?    shouted ahab.
(8.7) shall we keep chasing this murderous    sh?

english verbs tend to come in between the subject and some number of direct ob-
jects, depending on the verb. they can be marked for tense and aspect using suf   xes
such as -ed and -ing. (these suf   xes are an example of in   ectional morphology,
which is discussed in more detail in    9.1.4.)

    adjectives (adj) describe properties of entities,

(8.8)

a. shall we keep chasing this murderous    sh?
b. toes are scarce among veteran blubber men.

in the second example, scarce is a predicative adjective, linked to the subject by the
copula verb are. in contrast, murderous and veteran are attributive adjectives, modi-
fying the noun phrase in which they are embedded.

    adverbs (adv) describe properties of events, and may also modify adjectives or

other adverbs:

(8.9)

it is not down on any map; true places never are.
. . . treacherously hidden beneath the loveliest tints of azure

a.
b.
c. not drowned entirely, though.

    interjections (intj) are used in exclamations, e.g.,

(8.10) aye aye! it was that accursed white whale that razed me.

3http://universaldependencies.org/u/pos/verb.html

under contract with mit press, shared under cc-by-nc-nd license.

178

chapter 8. applications of sequence labeling

closed class tags closed word classes rarely receive new members. they are sometimes
referred to as function words     as opposed to content words     as they have little lexical
meaning of their own, but rather, help to organize the components of the sentence.

    adpositions (adp) describe the relationship between a complement (usually a noun

phrase) and another unit in the sentence, typically a noun or verb phrase.

(8.11)

a. toes are scarce among veteran blubber men.
b.
c. give not thyself up then.

it is not down on any map.

as the examples show, english generally uses prepositions, which are adpositions
that appear before their complement. (an exception is ago, as in, we met three days
ago). postpositions are used in other languages, such as japanese and turkish.

    auxiliary verbs (aux) are a closed class of verbs that add information such as

tense, aspect, person, and number.

(8.12)

a. shall we keep chasing this murderous    sh?
b. what the white whale was to ahab, has been hinted.
c. ahab must use tools.
d. meditation and water are wedded forever.
e. toes are scarce among veteran blubber men.

the    nal example is a copula verb, which is also tagged as an auxiliary in the ud
corpus.

    coordinating conjunctions (cconj) express relationships between two words or

phrases, which play a parallel role:

(8.13) meditation and water are wedded forever.

    subordinating conjunctions (sconj) link two clauses, making one syntactically

subordinate to the other:

(8.14)

it is the easiest thing in the world for a man to look as if he had a great
secret in him.

note that

    pronouns (pron) are words that substitute for nouns or noun phrases.

(8.15)

a. be it what it will, i   ll go to it laughing.

jacob eisenstein. draft of november 13, 2018.

8.1. part-of-speech tagging

179

b. i try all things, i achieve what i can.

the example includes the personal pronouns i and it, as well as the relative pronoun
what. other pronouns include myself, somebody, and nothing.

    determiners (det) provide additional information about the nouns or noun phrases

that they modify:

(8.16)

a. what the white whale was to ahab, has been hinted.
b.
c.
d. shall we keep chasing this murderous    sh?

it is not down on any map.
i try all things . . .

determiners include articles (the), possessive determiners (their), demonstratives
(this murderous    sh), and quanti   ers (any map).

    numerals (num) are an in   nite but closed class, which includes integers, fractions,

and decimals, regardless of whether spelled out or written in numerical form.

(8.17)

a. how then can this one small heart beat.
b.

i am going to put him down for the three hundredth.

    particles (part) are a catch-all of function words that combine with other words or
phrases, but do not meet the conditions of the other tags. in english, this includes
the in   nitival to, the possessive marker, and negation.

(8.18)

a. better to sleep with a sober cannibal than a drunk christian.
b. so man   s insanity is heaven   s sense
c.

it is not down on any map

as the second example shows, the possessive marker is not considered part of the
same token as the word that it modi   es, so that man   s is split into two tokens. (tok-
enization is described in more detail in    8.4.) a non-english example of a particle
is the japanese question marker ka:4

(8.19) sensei

desu
is

ka
?

teacher
is she a teacher?

4in this notation, the    rst line is the transliterated japanese text, the second line is a token-to-token gloss,

and the third line is the translation.

under contract with mit press, shared under cc-by-nc-nd license.

180

chapter 8. applications of sequence labeling

other the remaining ud tags include punctuation (pun) and symbols (sym). punc-
tuation is purely structural     e.g., commas, periods, colons     while symbols can carry
content of their own. examples of symbols include dollar and percentage symbols, math-
ematical operators, emoticons, emojis, and internet addresses. a    nal catch-all tag is x,
which is used for words that cannot be assigned another part-of-speech category. the x
tag is also used in cases of code switching (between languages), described in    8.5.
other tagsets

prior to the universal dependency treebank, part-of-speech tagging was performed us-
ing language-speci   c tagsets. the dominant tagset for english was designed as part of
the id32 (ptb), and it includes 45 tags     more than three times as many as
the ud tagset. this granularity is re   ected in distinctions between singular and plural
nouns, verb tenses and aspects, possessive and non-possessive pronouns, comparative
and superlative adjectives and adverbs (e.g., faster, fastest), and so on. the brown corpus
includes a tagset that is even more detailed, with 87 tags (francis, 1964), including special
tags for individual auxiliary verbs such as be, do, and have.

different languages make different distinctions, and so the ptb and brown tagsets are
not appropriate for a language such as chinese, which does not mark the verb tense (xia,
2000); nor for spanish, which marks every combination of person and number in the
verb ending; nor for german, which marks the case of each noun phrase. each of these
languages requires more detail than english in some areas of the tagset, and less in other
areas. the strategy of the universal dependencies corpus is to design a coarse-grained
tagset to be used across all languages, and then to additionally annotate language-speci   c
morphosyntactic attributes, such as number, tense, and case. the attribute tagging task
is described in more detail in    8.2.

social media such as twitter have been shown to require tagsets of their own (gimpel
et al., 2011). such corpora contain some tokens that are not equivalent to anything en-
countered in a typical written corpus: e.g., emoticons, urls, and hashtags. social media
also includes dialectal words like gonna (   going to   , e.g. we gonna be    ne) and ima (   i   m
going to   , e.g., ima tell you one more time), which can be analyzed either as non-standard
orthography (making id121 impossible), or as lexical items in their own right. in
either case, it is clear that existing tags like noun and verb cannot handle cases like ima,
which combine aspects of the noun and verb. gimpel et al. (2011) therefore propose a new
set of tags to deal with these cases.

8.1.2 accurate part-of-speech tagging
part-of-speech tagging is the problem of selecting the correct tag for each word in a sen-
tence. success is typically measured by accuracy on an annotated test set, which is simply
the fraction of tokens that were tagged correctly.

jacob eisenstein. draft of november 13, 2018.

8.1. part-of-speech tagging

181

baselines

a simple baseline for part-of-speech tagging is to choose the most common tag for each
word. for example, in the universal dependencies treebank, the word talk appears 96
times, and 85 of those times it is labeled as a verb: therefore, this baseline will always
predict verb for this word. for words that do not appear in the training corpus, the base-
line simply guesses the most common tag overall, which is noun. in the id32,
this simple baseline obtains accuracy above 92%. a more rigorous evaluation is the accu-
racy on out-of-vocabulary words, which are not seen in the training data. tagging these
words correctly requires attention to the context and the word   s internal structure.

contemporary approaches

conditional random    elds and structured id88 perform at or near the state-of-the-
art for part-of-speech tagging in english. for example, (collins, 2002) achieved 97.1%
accuracy on the id32, using a structured id88 with the following base
features (originally introduced by ratnaparkhi (1996)):

    current word, wm
    previous words, wm   1, wm   2
    next words, wm+1, wm+2
    previous tag, ym   1
    previous two tags, (ym   1, ym   2)
    for rare words:

       rst k characters, up to k = 4
    last k characters, up to k = 4
    whether wm contains a number, uppercase character, or hyphen.

similar results for the ptb data have been achieved using conditional random    elds (crfs;
toutanova et al., 2003).

more recent work has demonstrated the power of neural sequence models, such as the
long short-term memory (lstm) (   7.6). plank et al. (2016) apply a crf and a bidirec-
tional lstm to twenty-two languages in the ud corpus, achieving an average accuracy
of 94.3% for the crf, and 96.5% with the bi-lstm. their neural model employs three
types of embeddings:    ne-tuned id27s, which are updated during training;
pre-trained id27s, which are never updated, but which help to tag out-of-
vocabulary words; and character-based embeddings. the character-based embeddings
are computed by running an lstm on the individual characters in each word, thereby
capturing common orthographic patterns such as pre   xes, suf   xes, and capitalization.
extensive evaluations show that these additional embeddings are crucial to their model   s
success.

under contract with mit press, shared under cc-by-nc-nd license.

182

chapter 8. applications of sequence labeling

ptb tag

ud tag

ud attributes

word

the

german

dt

jj

expressionist nn

movement

nn

was

vbd

destroyed

vbn

as

a

result

.

in

dt

nn

.

definite=def prontype=art

degree=pos

number=sing

number=sing

mood=ind number=sing person=3
tense=past verbform=fin
tense=past verbform=part
voice=pass

definite=ind prontype=art

number=sing

det

adj

noun

noun

aux

verb

adp

det

noun

punct

figure 8.1: ud and ptb part-of-speech tags, and ud morphosyntactic attributes. example
selected from the ud 1.4 english corpus.

8.2 morphosyntactic attributes

there is considerably more to say about a word than whether it is a noun or a verb: in en-
glish, verbs are distinguish by features such tense and aspect, nouns by number, adjectives
by degree, and so on. these features are language-speci   c: other languages distinguish
other features, such as case (the role of the noun with respect to the action of the sen-
tence, which is marked in languages such as latin and german5) and evidentiality (the
source of information for the speaker   s statement, which is marked in languages such as
turkish). in the ud corpora, these attributes are annotated as feature-value pairs for each
token.6

an example is shown in figure 8.1. the determiner the is marked with two attributes:
prontype=art, which indicates that it is an article (as opposed to another type of deter-

5case is marked in english for some personal pronouns, e.g., she saw her, they saw them.
6the annotation and tagging of morphosyntactic attributes can be traced back to earlier work on turk-
ish (o   azer and kuru  oz, 1994) and czech (haji  c and hladk  a, 1998). multext-east was an early multilin-
gual corpus to include morphosyntactic attributes (dimitrova et al., 1998).

jacob eisenstein. draft of november 13, 2018.

8.3. id39

183

miner or pronominal modi   er), and definite=def, which indicates that it is a de   nite
article (referring to a speci   c, known entity). the verbs are each marked with several
attributes. the auxiliary verb was is third-person, singular, past tense,    nite (conjugated),
and indicative (describing an event that has happened or is currently happenings); the
main verb destroyed is in participle form (so there is no additional person and number
information), past tense, and passive voice. some, but not all, of these distinctions are
re   ected in the ptb tags vbd (past-tense verb) and vbn (past participle).

while there are thousands of papers on part-of-speech tagging, there is comparatively
little work on automatically labeling morphosyntactic attributes. faruqui et al. (2016)
train a support vector machine classi   cation model, using a minimal feature set that in-
cludes the word itself, its pre   xes and suf   xes, and type-level information listing all pos-
sible morphosyntactic attributes for each word and its neighbors. mueller et al. (2013) use
a conditional random    eld (crf), in which the tag space consists of all observed com-
binations of morphosyntactic attributes (e.g., the tag would be def+art for the word
the in figure 8.1). this massive tag space is managed by decomposing the feature space
over individual attributes, and pruning paths through the trellis. more recent work has
employed bidirectional lstm sequence models. for example, pinter et al. (2017) train
a bidirectional lstm sequence model. the input layer and hidden vectors in the lstm
are shared across attributes, but each attribute has its own output layer, culminating in
a softmax over all attribute values, e.g. ynumber
    {sing, plural, . . .}. they    nd that
character-level information is crucial, especially when the amount of labeled data is lim-
ited.

t

evaluation is performed by    rst computing recall and precision for each attribute.
these scores can then be averaged at either the type or token level to obtain micro- or
macro-f -measure. pinter et al. (2017) evaluate on 23 languages in the ud treebank,
reporting a median micro-f -measure of 0.95. performance is strongly correlated with the
size of the labeled dataset for each language, with a few outliers: for example, chinese is
particularly dif   cult, because although the dataset is relatively large (105 tokens in the ud
1.4 corpus), only 6% of tokens have any attributes, offering few useful labeled instances.

8.3 id39

a classical problem in information extraction is to recognize and extract mentions of
named entities in text. in news documents, the core entity types are people, locations, and
organizations; more recently, the task has been extended to include amounts of money,
percentages, dates, and times. in item 8.20a (figure 8.2), the named entities include: the
u.s. army, an organization; atlanta, a location; and may 14, 1864, a date. named en-
tity recognition is also a key task in biomedical natural language processing, with entity
types including proteins, dna, rna, and cell lines (e.g., collier et al., 2000; ohta et al.,
2002). figure 8.2 shows an example from the genia corpus of biomedical research ab-

under contract with mit press, shared under cc-by-nc-nd license.

184

chapter 8. applications of sequence labeling

(8.20)

a. the

b-org
b. number

o

army
i-org

u.s.
i-org
of
o

captured
o
glucocorticoid
b-protein

receptors
i-protein

atlanta
b-loc
in
o

may
on
o
b-date
lymphocytes
b-celltype

14
i-date
and
o

,
i-date
. . .
. . .

1864
i-date

figure 8.2: bio notation for id39. example (8.20b) is drawn from the
genia corpus of biomedical documents (ohta et al., 2002).

stracts.

a standard approach to tagging named entity spans is to use discriminative sequence
labeling methods such as conditional random    elds. however, the named entity recogni-
tion (ner) task would seem to be fundamentally different from sequence labeling tasks
like part-of-speech tagging: rather than tagging each token, the goal in is to recover spans
of tokens, such as the united states army.

this is accomplished by the bio notation, shown in figure 8.2. each token at the
beginning of a name span is labeled with a b- pre   x; each token within a name span is la-
beled with an i- pre   x. these pre   xes are followed by a tag for the entity type, e.g. b-loc
for the beginning of a location, and i-protein for the inside of a protein name. tokens
that are not parts of name spans are labeled as o. from this representation, the entity
name spans can be recovered unambiguously. this tagging scheme is also advantageous
for learning: tokens at the beginning of name spans may have different properties than
tokens within the name, and the learner can exploit this. this insight can be taken even
further, with special labels for the last tokens of a name span, and for unique tokens in
name spans, such as atlanta in the example in figure 8.2. this is called bilou notation,
and it can yield improvements in supervised id39 (ratinov and roth,
2009).

feature-based sequence labeling id39 was one of the    rst applica-
tions of conditional random    elds (mccallum and li, 2003). the use of viterbi decoding

restricts the feature function f (w, y) to be a sum of local features,(cid:80)m f (w, ym, ym   1, m),

so that each feature can consider only local adjacent tags. typical features include tag tran-
sitions, word features for wm and its neighbors, character-level features for pre   xes and
suf   xes, and    word shape    features for capitalization and other orthographic properties.
as an example, base features for the word army in the example in (8.20a) include:
(curr-word:army, prev-word:u.s., next-word:captured, prefix-1:a-,
prefix-2:ar-, suffix-1:-y, suffix-2:-my, shape:xxxx)

features can also be obtained from a gazetteer, which is a list of known entity names. for
example, the u.s. social security administration provides a list of tens of thousands of

jacob eisenstein. draft of november 13, 2018.

8.4. id121

185

figure 8.3: an example of id121 ambiguity in chinese (sproat et al., 1996)

given names     more than could be observed in any annotated corpus. tokens or spans
that match an entry in a gazetteer can receive special features; this provides a way to
incorporate hand-crafted resources such as name lists in a learning-driven framework.

neural sequence labeling for ner current research has emphasized neural sequence
labeling, using similar lstm models to those employed in part-of-speech tagging (ham-
merton, 2003; huang et al., 2015; lample et al., 2016). the bidirectional lstm-crf (fig-
ure 7.4 in    7.6) does particularly well on this task, due to its ability to model tag-to-tag
dependencies. however, strubell et al. (2017) show that convolutional neural networks
can be equally accurate, with signi   cant improvement in speed due to the ef   ciency of
implementing convnets on graphics processing units (gpus). the key innovation in
this work was the use of dilated convolution, which is described in more detail in    3.4.

8.4 id121

a basic problem for text analysis,    rst discussed in    4.3.1, is to break the text into a se-
quence of discrete tokens. for alphabetic languages such as english, deterministic scripts
usually suf   ce to achieve accurate id121. however, in logographic writing systems
such as chinese script, words are typically composed of a small number of characters,
without intervening whitespace. the id121 must be determined by the reader, with
the potential for occasional ambiguity, as shown in figure 8.3. one approach is to match
character sequences against a known dictionary (e.g., sproat et al., 1996), using additional
statistical information about word frequency. however, no dictionary is completely com-
prehensive, and dictionary-based approaches can struggle with such out-of-vocabulary
words.

chinese id40 has therefore been approached as a supervised sequence
labeling problem. xue et al. (2003) train a id28 classi   er to make indepen-
dent segmentation decisions while moving a sliding window across the document. a set
of rules is then used to convert these individual classi   cation decisions into an overall to-
kenization of the input. however, these individual decisions may be globally suboptimal,
motivating a structure prediction approach. peng et al. (2004) train a conditional random

under contract with mit press, shared under cc-by-nc-nd license.

(1)      japanese      octopus      how   ?sayhowtosayoctopusinjapanese?(2)   japan      essay      sh      how   ?say186

chapter 8. applications of sequence labeling

   eld to predict labels of start or nonstart on each character. more recent work has
employed neural network architectures. for example, chen et al. (2015) use an lstm-
crf architecture, as described in    7.6: they construct a trellis, in which each tag is scored
according to the hidden state of an lstm, and tag-tag transitions are scored according
to learned transition weights. the best-scoring segmentation is then computed by the
viterbi algorithm.

8.5 code switching

multilingual speakers and writers do not restrict themselves to a single language. code
switching is the phenomenon of switching between languages in speech and text (auer,
2013; poplack, 1980). written code switching has become more common in online social
media, as in the following extract from the website of canadian president justin trudeau:7

(8.21) although everything written on this site est
is

disponible
available

en
in

anglais
english

and in french, my personal videos seront
will be

bilingues
bilingual

accurately analyzing such texts requires    rst determining which languages are being
used. furthermore, quantitative analysis of code switching can provide insights on the
languages themselves and their relative social positions.

code switching can be viewed as a sequence labeling problem, where the goal is to la-
bel each token as a candidate switch point. in the example above, the words est, and, and
seront would be labeled as switch points. solorio and liu (2008) detect english-spanish
switch points using a supervised classi   er, with features that include the word, its part-of-
speech in each language (according to a supervised part-of-speech tagger), and the prob-
abilities of the word and part-of-speech in each language. nguyen and dogru  oz (2013)
apply a conditional random    eld to the problem of detecting code switching between
turkish and dutch.

code switching is a special case of the more general problem of word level language
identi   cation, which barman et al. (2014) address in the context of trilingual code switch-
ing between bengali, english, and hindi. they further observe an even more challenging
phenomenon: intra-word code switching, such as the use of english suf   xes with bengali
roots. they therefore mark each token as either (1) belonging to one of the three languages;
(2) a mix of multiple languages; (3)    universal    (e.g., symbols, numbers, emoticons); or
(4) unde   ned.

7as

quoted

in

http://blogues.lapresse.ca/lagace/2008/09/08/

justin-trudeau-really-parfait-bilingue/, accessed august 21, 2017.

jacob eisenstein. draft of november 13, 2018.

8.6. dialogue acts

187

utterance
so do you go college right now?
are yo-
yeah,
it   s my last year [laughter].

yes-no-question
abandoned

speaker dialogue act
a
a
b
b
a
b
b
a
b

yes-answer
statement
appreciation
backchannel

yes-answer
statement

declarative-question you   re a, so you   re a senior now.

yeah,
i   m working on my projects trying to graduate [laughter]

oh, good for you.
yeah.

figure 8.4: an example of dialogue act labeling (stolcke et al., 2000)

8.6 dialogue acts

the sequence labeling problems that we have discussed so far have been over sequences
of word tokens or characters (in the case of id121). however, sequence labeling
can also be performed over higher-level units, such as utterances. dialogue acts are la-
bels over utterances in a dialogue, corresponding roughly to the speaker   s intention    
the utterance   s illocutionary force (austin, 1962). for example, an utterance may state a
proposition (it is not down on any map), pose a question (shall we keep chasing this murderous
   sh?), or provide a response (aye aye!). stolcke et al. (2000) describe how a set of 42 dia-
logue acts were annotated for the 1,155 conversations in the switchboard corpus (godfrey
et al., 1992).8

an example is shown in figure 8.4. the annotation is performed over utterances,
with the possibility of multiple utterances per conversational turn (in cases such as inter-
ruptions, an utterance may split over multiple turns). some utterances are clauses (e.g., so
do you go to college right now?), while others are single words (e.g., yeah). stolcke et al. (2000)
report that id48 (id48s) achieve 96% accuracy on supervised utter-
ance segmentation. the labels themselves re   ect the conversational goals of the speaker:
the utterance yeah functions as an answer in response to the question you   re a senior now,
but in the    nal line of the excerpt, it is a backchannel (demonstrating comprehension).

for task of dialogue act labeling, stolcke et al. (2000) apply a hidden markov model.
the id203 p(wm | ym) must generate the entire sequence of words in the utterance,
and it is modeled as a trigram language model (   6.1). stolcke et al. (2000) also account
for acoustic features, which capture the id144 of each utterance     for example, tonal
and rhythmic properties of speech, which can be used to distinguish dialogue acts such

8dialogue act modeling is not restricted to speech; it is relevant in any interactive conversation. for
example, jeong et al. (2009) annotate a more limited set of speech acts in a corpus of emails and online
forums.

under contract with mit press, shared under cc-by-nc-nd license.

188

chapter 8. applications of sequence labeling

as questions and answers. these features are handled with an additional emission distri-
bution, p(am | ym), which is modeled with a probabilistic decision tree (murphy, 2012).
while acoustic features yield small improvements overall, they play an important role in
distinguish questions from statements, and agreements from backchannels.

recurrent neural architectures for dialogue act labeling have been proposed by kalch-
brenner and blunsom (2013) and ji et al. (2016), with strong empirical results. both models
are recurrent at the utterance level, so that each complete utterance updates a hidden state.
the recurrent-convolutional network of kalchbrenner and blunsom (2013) uses convolu-
tion to obtain a representation of each individual utterance, while ji et al. (2016) use a
second level of recurrence, over individual words. this enables their method to also func-
tion as a language model, giving probabilities over sequences of words in a document.

exercises

1. using the universal dependencies part-of-speech tags, annotate the following sen-
tences. you may examine the ud tagging guidelines. id121 is shown with
whitespace. don   t forget about punctuation.

(8.22)

i try all things , i achieve what i can .
it was that accursed white whale that razed me .

a.
b.
c. better to sleep with a sober cannibal , than a drunk christian .
d. be it what it will , i    ll go to it laughing .

2. select three short sentences from a recent news article, and annotate them for ud
part-of-speech tags. ask a friend to annotate the same three sentences without look-
ing at your annotations. compute the rate of agreement, using the kappa metric
de   ned in    4.5.2. then work together to resolve any disagreements.

3. choose one of the following morphosyntactic attributes: mood, tense, voice. re-
search the de   nition of this attribute on the universal dependencies website, http:
//universaldependencies.org/u/feat/index.html. returning to the ex-
amples in the    rst exercise, annotate all verbs for your chosen attribute. it may be
helpful to consult examples from an english-language universal dependencies cor-
pus, available at https://github.com/universaldependencies/ud_english-ewt/
tree/master.

4. download a dataset annotated for universal dependencies, such as the english tree-
bank at https://github.com/universaldependencies/ud_english-ewt/
tree/master. this corpus is already segmented into training, development, and
test data.

jacob eisenstein. draft of november 13, 2018.

8.6. dialogue acts

189

a) first, train a id28 or id166 classi   er using character suf   xes: char-
acter id165s up to length 4. compute the recall, precision, and f -measure
on the development data.

b) next, augment your classi   er using the same character suf   xes of the preced-

ing and succeeding tokens. again, evaluate your classi   er on heldout data.

c) optionally, train a viterbi-based sequence labeling model, using a toolkit such
as crfsuite (http://www.chokkan.org/software/crfsuite/) or your
own viterbi implementation. this is more likely to be helpful for attributes
in which agreement is required between adjacent words. for example, many
romance languages require gender and number agreement for determiners,
nouns, and adjectives.

5. provide bio-style annotation of the named entities (person, place, organization,

date, or product) in the following expressions:

(8.23)

a. the third mate was flask, a native of tisbury, in martha   s vineyard.
b.

its of   cial nintendo announced today that they will release the nin-
tendo 3ds in north america march 27 (ritter et al., 2011).
jessica reif, a media analyst at merrill lynch & co., said,    if they can
get up and running with exclusive programming within six months, it
doesn   t set the venture back that far.   9

c.

6. run the examples above through the online version of a named entity recogni-
tion tagger, such as the allen nlp system here: http://demo.allennlp.org/named-
entity-recognition. do the predicted tags match your annotations?

7. build a whitespace tokenizer for english:

a) using the nltk library, download the complete text to the novel alice in won-

derland (carroll, 1865). hold out the    nal 1000 words as a test set.

b) label each alphanumeric character as a segmentation point, ym = 1 if m is
the    nal character of a token. label every other character as ym = 0. then
concatenate all the tokens in the training and test sets.make sure that the num-
m=1 in your
ber of labels {ym}m
concatenated datasets.

m=1 is identical to the number of characters {cm}m

c) train a id28 classi   er to predict ym, using the surrounding char-
acters cm   5:m+5 as features. after training the classi   er, run it on the test set,
using the predicted segmentation points to re-tokenize the text.

9from the message understanding conference (muc-7) dataset (chinchor and robinson, 1997).

under contract with mit press, shared under cc-by-nc-nd license.

190

chapter 8. applications of sequence labeling

d) compute the per-character segmentation accuracy on the test set. you should

be able to get at least 88% accuracy.

e) print out a sample of segmented text from the test set, e.g.

thereareno mice in the air , i     m afraid , but y oumight cat

chabat , and that     svery like a mouse , youknow . but
docatseat bats , i wonder ?   

8. perform the following extensions to your tokenizer in the previous problem.

a) train a conditional random    eld sequence labeler, by incorporating the tag
bigrams (ym   1, ym) as additional features. you may use a structured predic-
tion library such as crfsuite, or you may want to implement viterbi yourself.
compare the accuracy with your classi   cation-based approach.

b) compute the token-level performance: treating the original id121 as
ground truth, compute the number of true positives (tokens that are in both
the ground truth and predicted id121), false positives (tokens that are in
the predicted id121 but not the ground truth), and false negatives (to-
kens that are in the ground truth but not the predicted id121). compute
the f-measure.
hint: to match predicted and ground truth tokens, add    anchors    for the start
character of each token. the number of true positives is then the size of the
intersection of the sets of predicted and ground truth tokens.

c) apply the same methodology in a more practical setting: id121 of chi-
nese, which is written without whitespace. you can    nd annotated datasets at
http://alias-i.com/lingpipe/demos/tutorial/chinesetokens/read-me.
html.

jacob eisenstein. draft of november 13, 2018.

chapter 9

formal language theory

we have now seen methods for learning to label individual words, vectors of word counts,
and sequences of words; we will soon proceed to more complex structural transforma-
tions. most of these techniques could apply to counts or sequences from any discrete vo-
cabulary; there is nothing fundamentally linguistic about, say, a hidden markov model.
this raises a basic question that this text has not yet considered: what is a language?

this chapter will take the perspective of formal language theory, in which a language
is de   ned as a set of strings, each of which is a sequence of elements from a    nite alphabet.
for interesting languages, there are an in   nite number of strings that are in the language,
and an in   nite number of strings that are not. for example:

    the set of all even-length sequences from the alphabet {a, b}, e.g., {   , aa, ab, ba, bb, aaaa, aaab, . . .};
    the set of all sequences from the alphabet {a, b} that contain aaa as a substring, e.g.,

{aaa, aaaa, baaa, aaab, . . .};

    the set of all sequences of english words (drawn from a    nite dictionary) that con-

tain at least one verb (a    nite subset of the dictionary);

    the python programming language.
formal language theory de   nes classes of languages and their computational prop-
erties. of particular interest is the computational complexity of solving the membership
problem     determining whether a string is in a language. the chapter will focus on
three classes of formal languages: regular, context-free, and    mildly    context-sensitive
languages.

a key insight of 20th century linguistics is that formal language theory can be usefully
applied to natural languages such as english, by designing formal languages that cap-
ture as many properties of the natural language as possible. for many such formalisms, a
useful linguistic analysis comes as a byproduct of solving the membership problem. the

191

192

chapter 9. formal language theory

membership problem can be generalized to the problems of scoring strings for their ac-
ceptability (as in id38), and of transducing one string into another (as in
translation).

9.1 regular languages

if you have written a regular expression, then you have de   ned a regular language: a
regular language is any language that can be de   ned by a regular expression. formally, a
regular expression can include the following elements:

    a literal character drawn from some    nite alphabet   .
    the empty string  .
    the concatenation of two id157 rs, where r and s are both regular
expressions. the resulting expression accepts any string that can be decomposed
x = yz, where y is accepted by r and z is accepted by s.

    the alternation r | s, where r and s are both id157. the resulting
expression accepts a string x if it is accepted by r or it is accepted by s.
    the kleene star r   , which accepts any string x that can be decomposed into a se-
quence of strings which are all accepted by r.
    parenthesization (r), which is used to limit the scope of the concatenation, alterna-

tion, and kleene star operators.

here are some example id157:
    the set of all even length strings on the alphabet {a, b}: ((aa)|(ab)|(ba)|(bb))   
    the set of all sequences of the alphabet {a, b} that contain aaa as a substring: (a|b)   aaa(a|b)   
    the set of all sequences of english words that contain at least one verb: w    v w    ,
where w is an alternation between all words in the dictionary, and v is an alterna-
tion between all verbs (v     w ).

this list does not include a regular expression for the python programming language,
because this language is not regular     there is no regular expression that can capture its
syntax. we will discuss why towards the end of this section.

regular languages are closed under union, intersection, and concatenation. this means
that if two languages l1 and l2 are regular, then so are the languages l1     l2, l1     l2,
and the language of strings that can be decomposed as s = tu, with s     l1 and t     l2.
regular languages are also closed under negation: if l is regular, then so is the language
l = {s /    l}.

jacob eisenstein. draft of november 13, 2018.

9.1. regular languages

193

figure 9.1: state diagram for the    nite state acceptor m1.

9.1.1 finite state acceptors
a regular expression de   nes a regular language, but does not give an algorithm for de-
termining whether a string is in the language that it de   nes. finite state automata are
theoretical models of computation on regular languages, which involve transitions be-
tween a    nite number of states. the most basic type of    nite state automaton is the    nite
state acceptor (fsa), which describes the computation involved in testing if a string is
a member of a language. formally, a    nite state acceptor is a tuple m = (q,   , q0, f,   ),
consisting of:

    a    nite alphabet    of input symbols;
    a    nite set of states q = {q0, q1, . . . , qn};
    a start state q0     q;
    a set of    nal states f     q;
    a transition function    : q    (       { })     2q. the transition function maps from a

state and an input symbol (or empty string  ) to a set of possible resulting states.

a path in m is a sequence of transitions,    = t1, t2, . . . , tn , where each ti traverses an
arc in the transition function   . the    nite state acceptor m accepts a string    if there is
an accepting path, in which the initial transition t1 begins at the start state q0, the    nal
transition tn terminates in a    nal state in q, and the entire input    is consumed.

example

consider the following fsa, m1.

   ={a, b}
q ={q0, q1}
f ={q1}
   ={(q0, a)     q0, (q0, b)     q1, (q1, b)     q1}.

[9.1]
[9.2]
[9.3]
[9.4]

this fsa de   nes a language over an alphabet of two symbols, a and b. the transition
function    is written as a set of arcs: (q0, a)     q0 says that if the machine is in state
under contract with mit press, shared under cc-by-nc-nd license.

q0startq1abb194

chapter 9. formal language theory

q0 and reads symbol a, it stays in q0. figure 9.1 provides a graphical representation of
m1. because each pair of initial state and symbol has at most one resulting state, m1 is
deterministic: each string    induces at most one accepting path. note that there are no
transitions for the symbol a in state q1; if a is encountered in q1, then the acceptor is stuck,
and the input string is rejected.

what strings does m1 accept? the start state is q0, and we have to get to q1, since this
is the only    nal state. any number of a symbols can be consumed in q0, but a b symbol is
required to transition to q1. once there, any number of b symbols can be consumed, but
an a symbol cannot. so the regular expression corresponding to the language de   ned by
m1 is a   bb   .

computational properties of    nite state acceptors

the key computational question for    nite state acceptors is: how fast can we determine
whether a string is accepted? for determistic fsas, this computation can be performed
by dijkstra   s algorithm, with time complexity o(v log v + e), where v is the number of
vertices in the fsa, and e is the number of edges (cormen et al., 2009). non-deterministic
fsas (nfsas) can include multiple transitions from a given symbol and state. any nsfa
can be converted into a deterministic fsa, but the resulting automaton may have a num-
ber of states that is exponential in the number of size of the original nfsa (mohri et al.,
2002).

9.1.2 morphology as a regular language
many words have internal structure, such as pre   xes and suf   xes that shape their mean-
ing. the study of word-internal structure is the domain of morphology, of which there
are two main types:

    derivational morphology describes the use of af   xes to convert a word from one
grammatical category to another (e.g., from the noun grace to the adjective graceful),
or to change the meaning of the word (e.g., from grace to disgrace).

    in   ectional morphology describes the addition of details such as gender, number,

person, and tense (e.g., the -ed suf   x for past tense in english).

morphology is a rich topic in linguistics, deserving of a course in its own right.1 the
focus here will be on the use of    nite state automata for morphological analysis. the
1a good starting point would be a chapter from a linguistics textbook (e.g., akmajian et al., 2010; bender,
2013). a key simpli   cation in this chapter is the focus on af   xes at the sole method of derivation and in   ec-
tion. english makes use of af   xes, but also incorporates apophony, such as the in   ection of foot to feet. semitic
languages like arabic and hebrew feature a template-based system of morphology, in which roots are triples
of consonants (e.g., ktb), and words are created by adding vowels: kataba (arabic: he wrote), kutub (books),
maktab (desk). for more detail on morphology, see texts from haspelmath and sims (2013) and lieber (2015).

jacob eisenstein. draft of november 13, 2018.

9.1. regular languages

195

current section deals with derivational morphology; in   ectional morphology is discussed
in    9.1.4.

suppose that we want to write a program that accepts only those words that are con-

structed in accordance with the rules of english derivational morphology:

(9.1)

a. grace, graceful, gracefully, *gracelyful
b. disgrace, *ungrace, disgraceful, disgracefully
c. allure, *allureful, alluring, alluringly
d.

fairness, unfair, *disfair, fairly

(recall that the asterisk indicates that a linguistic example is judged unacceptable by    u-
ent speakers of a language.) these examples cover only a tiny corner of english deriva-
tional morphology, but a number of things stand out. the suf   x -ful converts the nouns
grace and disgrace into adjectives, and the suf   x -ly converts adjectives into adverbs. these
suf   xes must be applied in the correct order, as shown by the unacceptability of *grace-
lyful. the -ful suf   x works for only some words, as shown by the use of alluring as the
adjectival form of allure. other changes are made with pre   xes, such as the derivation
of disgrace from grace, which roughly corresponds to a negation; however, fair is negated
with the un- pre   x instead. finally, while the    rst three examples suggest that the direc-
tion of derivation is noun     adjective     adverb, the example of fair suggests that the
adjective can also be the base form, with the -ness suf   x performing the conversion to a
noun.

can we build a computer program that accepts only well-formed english words, and
rejects all others? this might at    rst seem trivial to solve with a brute-force attack: simply
make a dictionary of all valid english words. but such an approach fails to account for
morphological productivity     the applicability of existing morphological rules to new
words and names, such as trump to trumpy and trumpkin, and clinton to clintonian and
clintonite. we need an approach that represents morphological rules explicitly, and for
this we will try a    nite state acceptor.

the dictionary approach can be implemented as a    nite state acceptor, with the vo-
cabulary    equal to the vocabulary of english, and a transition from the start state to the
accepting state for each word. but this would of course fail to generalize beyond the origi-
nal vocabulary, and would not capture anything about the morphotactic rules that govern
derivations from new words. the    rst step towards a more general approach is shown in
figure 9.2, which is the state diagram for a    nite state acceptor in which the vocabulary
consists of morphemes, which include stems (e.g., grace, allure) and af   xes (e.g., dis-, -ing,
-ly). this    nite state acceptor consists of a set of paths leading away from the start state,
with derivational af   xes added along the path. except for qneg, the states on these paths
are all    nal, so the fsa will accept disgrace, disgraceful, and disgracefully, but not dis-.

under contract with mit press, shared under cc-by-nc-nd license.

196

chapter 9. formal language theory

figure 9.2: a    nite state acceptor for a fragment of english derivational morphology. each
path represents possible derivations from a single root form.

this fsa can be minimized to the form shown in figure 9.3, which makes the gen-
erality of the    nite state approach more apparent. for example, the transition from q0 to
qj2 can be made to accept not only fair but any single-morpheme (monomorphemic) ad-
jective that takes -ness and -ly as suf   xes. in this way, the    nite state acceptor can easily
be extended: as new word stems are added to the vocabulary, their derived forms will be
accepted automatically. of course, this fsa would still need to be extended considerably
to cover even this small fragment of english morphology. as shown by cases like music
    musical, athlete     athletic, english includes several classes of nouns, each with its own
rules for derivation.
the fsas shown in figure 9.2 and 9.3 accept allureing, not alluring. this re   ects a dis-
tinction between morphology     the question of which morphemes to use, and in what
order     and orthography     the question of how the morphemes are rendered in written
language. just as orthography requires dropping the e preceding the -ing suf   x, phonol-
ogy imposes a related set of constraints on how words are rendered in speech. as we will
see soon, these issues can be handled by    nite state!transducers, which are    nite state
automata that take inputs and produce outputs.

9.1.3 weighted    nite state acceptors

according to the fsa treatment of morphology, every word is either in or out of the lan-
guage, with no wiggle room. perhaps you agree that musicky and    shful are not valid
english words; but if forced to choose, you probably    nd a    shful stew or a musicky trib-
ute preferable to behaving disgracelyful. rather than asking whether a word is acceptable,
we might like to ask how acceptable it is. aronoff (1976, page 36) puts it another way:

jacob eisenstein. draft of november 13, 2018.

q0startqn1qj1qa1grace-ful-lyqnegqn2qj2qa2dis-grace-ful-lyqn3qj3qa3allure-ing-lyqj4qn4qa4fair-ness-ly9.1. regular languages

197

figure 9.3: minimization of the    nite state acceptor shown in figure 9.2.

   though many things are possible in morphology, some are more possible than others.   
but    nite state acceptors give no way to express preferences among technically valid
choices.

weighted    nite state acceptors (wfsas) are generalizations of fsas, in which each
accepting path is assigned a score, computed from the transitions, the initial state, and the
   nal state. formally, a weighted    nite state acceptor m = (q,   ,   ,   ,   ) consists of:

    a    nite set of states q = {q0, q1, . . . , qn};
    a    nite alphabet    of input symbols;
    an initial weight function,    : q     r;
    a    nal weight function    : q     r;
    a transition function    : q          q     r.
wfsas depart from the fsa formalism in three ways: every state can be an initial
state, with score   (q); every state can be an accepting state, with score   (q); transitions are
possible between any pair of states on any input, with a score   (qi,   , qj). nonetheless,
fsas can be viewed as a special case: for any fsa m we can build an equivalent wfsa
by setting   (q) =     for all q (cid:54)= q0,   (q) =     for all q /    f , and   (qi,   , qj) =     for all
transitions {(q1,   )     q2} that are not permitted by the transition function of m.

the total score for any path    = t1, t2, . . . , tn is equal to the sum of these scores,

n(cid:88)n

d(  ) =   (from-state(t1)) +

  (tn) +   (to-state(tn )).

[9.5]

a shortest-path algorithm is used to    nd the minimum-cost path through a wfsa for
string   , with time complexity o(e + v log v ), where e is the number of edges and v is
the number of vertices (cormen et al., 2009).2

2shortest-path algorithms    nd the path with the minimum cost. in many cases, the path weights are log

under contract with mit press, shared under cc-by-nc-nd license.

q0startqnegqn1qj1qa1dis-grace-ful-lygraceqn2allure-ingqj2qn3fair-ness-ly198

chapter 9. formal language theory

id165 language models as wfsas
in id165 language models (see    6.1), the id203 of a sequence of tokens w1, w2, . . . , wm
is modeled as,

p(w1, . . . , wm )    

m(cid:89)m=1

pn(wm | wm   1, . . . , wm   n+1).

[9.6]

the log id203 under an id165 language model can be modeled in a wfsa. first
consider a unigram language model. we need only a single state q0, with transition scores
  (q0,   , q0) = log p1(  ). the initial and    nal scores can be set to zero. then the path score
for w1, w2, . . . , wm is equal to,

0 +

m(cid:88)m

  (q0, wm, q0) + 0 =

log p1(wm).

m(cid:88)m

[9.7]

for an id165 language model with n > 1, we need probabilities that condition on
the past history. for example, in a bigram language model, the transition weights must
represent log p2(wm | wm   1). the transition scoring function must somehow    remember   
the previous word or words. this can be done by adding more states: to model the bigram
id203 p2(wm | wm   1), we need a state for every possible wm   1     a total of v states.
the construction indexes each state qi by a context event wm   1 = i. the weights are then
assigned as follows:

  (qi,   , qj) =(cid:40)log pr(wm = j | wm   1 = i),    = j

   (cid:54)= j

      ,

  (qi) = log pr(w1 = i | w0 = (cid:3))
  (qi) = log pr(wm +1 = (cid:4) | wm = i).

the transition function is designed to ensure that the context is recorded accurately:
we can move to state j on input    only if    = j; otherwise, transitioning to state j is
forbidden by the weight of       . the initial weight function   (qi) is the log id203 of
receiving i as the    rst token, and the    nal weight function   (qi) is the log id203 of
receiving an    end-of-string    token after observing wm = i.

*semiring weighted    nite state acceptors

the id165 language model wfsa is deterministic: each input has exactly one accepting
path, for which the wfsa computes a score. in non-deterministic wfsas, a given input

probabilities, so we want the path with the maximum score, which can be accomplished by making each local
score into a negative log-id203.

jacob eisenstein. draft of november 13, 2018.

9.1. regular languages

199

may have multiple accepting paths. in some applications, the score for the input is ag-
gregated across all such paths. such aggregate scores can be computed by generalizing
wfsas with semiring notation,    rst introduced in    7.7.3.

let d(  ) represent the total score for path    = t1, t2, . . . , tn , which is computed as,

d(  ) =   (from-state(t1))       (t1)       (t2)     . . .       (tn )       (to-state(tn )).

[9.8]

this is a generalization of equation 9.5 to semiring notation, using the semiring multipli-
cation operator     in place of addition.

now let s(  ) represent the total score for all paths   (  ) that consume input   ,

s(  ) = (cid:77)       (  )

d(  ).

[9.9]

here, semiring addition (   ) is used to combine the scores of multiple paths.

the generalization to semirings covers a number of useful special cases. in the log-
id203 semiring, multiplication is de   ned as log p(x)     log p(y) = log p(x) + log p(y),
and addition is de   ned as log p(x)     log p(y) = log(p(x) + p(y)). thus, s(  ) represents
the log-id203 of accepting input   , marginalizing over all paths          (  ). in the
boolean semiring, the     operator is logical conjunction, and the     operator is logical
disjunction. this reduces to the special case of unweighted    nite state acceptors, where
the score s(  ) is a boolean indicating whether there exists any accepting path for   . in
the tropical semiring, the     operator is a maximum, so the resulting score is the score of
the best-scoring path through the wfsa. the openfst toolkit uses semirings and poly-
morphism to implement general algorithms for weighted    nite state automata (allauzen
et al., 2007).

*interpolated id165 language models
recall from    6.2.3 that an interpolated id165 language model combines the probabili-
ties from multiple id165 models. for example, an interpolated bigram language model
computes the id203,

  p(wm | wm   1) =   1p1(wm) +   2p2(wm | wm   1),

[9.10]

with   p indicating the interpolated id203, p2 indicating the bigram id203, and
p1 indicating the unigram id203. setting   2 = (1       1) ensures that the probabilities
sum to one.

interpolated bigram language models can be implemented using a non-deterministic
wfsa (knight and may, 2009). the basic idea is shown in figure 9.4. in an interpolated
bigram language model, there is one state for each element in the vocabulary     in this

under contract with mit press, shared under cc-by-nc-nd license.

200

chapter 9. formal language theory

figure 9.4: wfsa implementing an interpolated bigram/unigram language model, on
the alphabet    = {a, b}. for simplicity, the wfsa is contrained to force the    rst token to
be generated from the unigram model, and does not model the emission of the end-of-
sequence token.

case, the states qa and qb     which are capture the contextual conditioning in the bigram
probabilities. to model unigram probabilities, there is an additional state qu , which    for-
gets    the context. transitions out of qu involve unigram probabilities, p1(a) and p2(b);
transitions into qu emit the empty symbol  , and have id203   1, re   ecting the inter-
polation weight for the unigram model. the interpolation weight for the bigram model is
included in the weight of the transition qa     qb.

the epsilon transitions into qu make this wfsa non-deterministic. consider the score
for the sequence (a, b, b). the initial state is qu , so the symbol a is generated with score
p1(a)3 next, we can generate b from the unigram model by taking the transition qa     qb,
with score   2p2(b | a). alternatively, we can take a transition back to qu with score   1,
and then emit b from the unigram model with score p1(b). to generate the    nal b token,
we face the same choice: emit it directly from the self-transition to qb, or transition to qu
   rst.

the total score for the sequence (a, b, b) is the semiring sum over all accepting paths,

s(a, b, b) =(cid:0)p1(a)       2p2(b | a)       2p(b | b)(cid:1)
   (cid:0)p1(a)       1     p1(b)       2p(b | b)(cid:1)
   (cid:0)p1(a)       2p2(b | a)     p1(b)     p1(b)(cid:1)
   (cid:0)p1(a)       1     p1(b)     p1(b)     p1(b)(cid:1) .

[9.11]

each line in equation 9.11 represents the id203 of a speci   c path through the wfsa.
in the id203 semiring,     is multiplication, so that each path is the product of each
3we could model the sequence-initial bigram id203 p2(a | (cid:3)), but for simplicity the wfsa does not
admit this possibility, which would require another state.

jacob eisenstein. draft of november 13, 2018.

qaqustartqba:p1(a)b:p1(b)a:  2p2(a|a)b:  2p2(b|a)b:  2p2(b|b)a:  2p2(a|b) :  1 2:  19.1. regular languages

201

transition weight, which are themselves probabilities. the     operator is addition, so that
the total score is the sum of the scores (probabilities) for each path. this corresponds to
the id203 under the interpolated bigram language model.

9.1.4 finite state transducers

finite state acceptors can determine whether a string is in a regular language, and weighted
   nite state acceptors can compute a score for every string over a given alphabet. finite
state transducers (fsts) extend the formalism further, by adding an output symbol to each
transition. formally, a    nite state transducer is a tuple t = (q,   ,    ,   ,   ,   ), with     repre-
senting an output vocabulary and the transition function    : q   (       )   (        )   q     r
mapping from states, input symbols, and output symbols to states. the remaining ele-
ments (q,   ,   ,   ) are identical to their de   nition in weighted    nite state acceptors (   9.1.3).
thus, each path through the fst t transduces the input string into an output.

string id153

the id153 between two strings s and t is a measure of how many operations are
required to transform one string into another. there are several ways to compute edit
distance, but one of the most popular is the levenshtein id153, which counts the
minimum number of insertions, deletions, and substitutions. this can be computed by
a one-state weighted    nite state transducer, in which the input and output alphabets are
identical. for simplicity, consider the alphabet    =     = {a, b}. the id153 can be
computed by a one-state transducer with the following transitions,

  (q, a, a, q) =   (q, b, b, q) = 0

  (q, a, b, q) =   (q, b, a, q) = 1

  (q, a,  , q) =   (q, b,  , q) = 1

  (q,  , a, q) =   (q,  , b, q) = 1.

[9.12]
[9.13]
[9.14]
[9.15]

the state diagram is shown in figure 9.5.

for a given string pair, there are multiple paths through the transducer: the best-
scoring path from dessert to desert involves a single deletion, for a total score of 1; the
worst-scoring path involves seven deletions and six additions, for a score of 13.

the porter stemmer

the porter (1980) id30 algorithm is a    lexicon-free    algorithm for stripping suf   xes
from english words, using a sequence of character-level rules. each rule can be described

under contract with mit press, shared under cc-by-nc-nd license.

202

chapter 9. formal language theory

figure 9.5: state diagram for the levenshtein id153    nite state transducer. the
label x/y : c indicates a cost of c for a transition with input x and output y.

by an unweighted    nite state transducer. the    rst rule is:

-sses     -ss
-ies     -i
-ss     -ss
-s      

e.g., dresses     dress
e.g., parties     parti
e.g., dress     dress
e.g., cats     cat

[9.16]
[9.17]
[9.18]
[9.19]

the    nal two lines appear to con   ict; they are meant to be interpreted as an instruction
to remove a terminal -s unless it is part of an -ss ending. a state diagram to handle just
these    nal two lines is shown in figure 9.6. make sure you understand how this    nite
state transducer handles cats, steps, bass, and basses.

in   ectional morphology

in in   ectional morphology, word lemmas are modi   ed to add grammatical information
such as tense, number, and case. for example, many english nouns are pluralized by the
suf   x -s, and many verbs are converted to past tense by the suf   x -ed. english   s in   ectional
morphology is considerably simpler than many of the world   s languages. for example,
romance languages (derived from latin) feature complex systems of verb suf   xes which
must agree with the person and number of the verb, as shown in table 9.1.

the task of morphological analysis is to read a form like canto, and output an analysis
like cantar+verb+presind+1p+sing, where +presind describes the tense as present
indicative, +1p indicates the    rst-person, and +sing indicates the singular number. the
task of morphological generation is the reverse, going from cantar+verb+presind+1p+sing
to canto. finite state transducers are an attractive solution, because they can solve both
problems with a single model (beesley and karttunen, 2003). as an example, figure 9.7
shows a fragment of a    nite state transducer for spanish in   ectional morphology. the

jacob eisenstein. draft of november 13, 2018.

qstarta/a,b/b:0a/b,b/a:1a/ ,b/ :1 /a, /b:19.1. regular languages

203

figure 9.6: state diagram for    nal two lines of step 1a of the porter id30 diagram.
states q3 and q4    remember    the observations a and b respectively; the ellipsis . . . repre-
sents additional states for each symbol in the input alphabet. the notation   s/  s is not
part of the fst formalism; it is a shorthand to indicate a set of self-transition arcs for every
input/output symbol except s.

in   nitive
yo (1st singular)
tu (2nd singular)
  el, ella, usted (3rd singular)
nosotros (1st plural)
vosotros (2nd plural, informal)
ellos, ellas (3rd plural);
ustedes (2nd plural)

cantar (to sing)
canto
cantas
canta
cantamos
cant  ais
cantan

comer (to eat) vivir (to live)
como
comes
come
comemos
com  eis
comen

vivo
vives
vive
vivimos
viv    s
viven

table 9.1: spanish verb in   ections for the present indicative tense. each row represents
a person and number, and each column is a regular example from a class of verbs, as
indicated by the ending of the in   nitive form.

input vocabulary    corresponds to the set of letters used in spanish spelling, and the out-
put vocabulary     corresponds to these same letters, plus the vocabulary of morphological
features (e.g., +sing, +verb). in figure 9.7, there are two paths that take canto as input,
corresponding to the verb and noun meanings; the choice between these paths could be
guided by a part-of-speech tagger. by inversion, the inputs and outputs for each tran-
sition are switched, resulting in a    nite state generator, capable of producing the correct
surface form for any morphological analysis.

finite state morphological analyzers and other unweighted transducers can be de-
signed by hand. the designer   s goal is to avoid overgeneration     accepting strings or
making transductions that are not valid in the language     as well as undergeneration

under contract with mit press, shared under cc-by-nc-nd license.

q1startq2  s/  ss/ q3q4...a/sb/s /a /b204

chapter 9. formal language theory

figure 9.7: fragment of a    nite state transducer for spanish morphology. there are two
accepting paths for the input canto: canto+noun+masc+sing (masculine singular noun,
meaning a song), and cantar+verb+presind+1p+sing (i sing). there is also an accept-
ing path for canta, with output cantar+verb+presind+3p+sing (he/she sings).

    failing to accept strings or transductions that are valid. for example, a pluralization
transducer that does not accept foot/feet would undergenerate. suppose we       x    the trans-
ducer to accept this example, but as a side effect, it now accepts boot/beet; the transducer
would then be said to overgenerate. if a transducer accepts foot/foots but not foot/feet, then
it simultaneously overgenerates and undergenerates.

finite state composition

designing    nite state transducers to capture the full range of morphological phenomena
in any real language is a huge task. modularization is a classic computer science approach
for this situation: decompose a large and unwieldly problem into a set of subproblems,
each of which will hopefully have a concise solution. finite state automata can be mod-
ularized through composition: feeding the output of one transducer t1 as the input to
another transducer t2, written t2    t1. formally, if there exists some y such that (x, y)     t1
(meaning that t1 produces output y on input x), and (y, z)     t2, then (x, z)     (t2     t1).
because    nite state transducers are closed under composition, there is guaranteed to be
a single    nite state transducer that t3 = t2     t1, which can be constructed as a machine
with one state for each pair of states in t1 and t2 (mohri et al., 2002).

example: morphology and orthography in english morphology, the suf   x -ed is added
to signal the past tense for many verbs: cook   cooked, want   wanted, etc. however, english
orthography dictates that this process cannot produce a spelling with consecutive e   s, so
that bake   baked, not bakeed. a modular solution is to build separate transducers for mor-
phology and orthography. the morphological transducer tm transduces from bake+past
to bake+ed, with the + symbol indicating a segment boundary. the input alphabet of tm
includes the lexicon of words and the set of morphological features; the output alphabet
includes the characters a-z and the + boundary marker. next, an orthographic transducer
to is responsible for the transductions cook+ed     cooked, and bake+ed     baked. the input
alphabet of to must be the same as the output alphabet for tm , and the output alphabet

jacob eisenstein. draft of november 13, 2018.

startc/ca/an/nt/to/o /+noun /+masc /+sing /a /r /+verbo/+presind /+1p /+singa/+presind /+3p /+sing9.1. regular languages

205

is simply the characters a-z. the composed transducer (to     tm ) then transduces from
bake+past to the spelling baked. the design of to is left as an exercise.

example: id48 id48 (chapter 7) can be viewed as
weighted    nite state transducers, and they can be constructed by transduction. recall that
a hidden markov model de   nes a joint id203 over words and tags, p(w, y), which
can be computed as a path through a trellis structure. this trellis is itself a weighted    nite
state acceptor, with edges between all adjacent nodes qm   1,i     qm,j on input ym = j. the
edge weights are log-probabilities,

  (qm   1,i, ym = j, qm,j) = log p(wm, ym = j | ym   i = j)

= log p(wm | ym = j) + log pr(ym = j | ym   1 = i).

[9.20]
[9.21]

because there is only one possible transition for each tag ym, this wfsa is deterministic.
the score for any tag sequence {ym}m
m=1 is the sum of these log-probabilities, correspond-
ing to the total log id203 log p(w, y). furthermore, the trellis can be constructed by
the composition of simpler fsts.

    first, construct a    transition    transducer to represent a bigram id203 model
over tag sequences, tt . this transducer is almost identical to the id165 language
model acceptor in    9.1.3: there is one state for each tag, and the edge weights equal
to the transition log-probabilities,   (qi, j, j, qj) = log pr(ym = j | ym   1 = i). note
that tt is a transducer, with identical input and output at each arc; this makes it
possible to compose tt with other transducers.

    next, construct an    emission    transducer to represent the id203 of words given
tags, te. this transducer has only a single state, with arcs for each word/tag pair,
  (q0, i, j, q0) = log pr(wm = j | ym = i). the input vocabulary is the set of all tags,
and the output vocabulary is the set of all words.
    the composition te     tt is a    nite state transducer with one state per tag, as shown
in figure 9.8. each state has v    k outgoing edges, representing transitions to each
of the k other states, with outputs for each of the v words in the vocabulary. the
weights for these edges are equal to,

  (qi, ym = j, wm, qj) = log p(wm, ym = j | ym   1 = i).

[9.22]

    the trellis is a structure with m   k nodes, for each of the m words to be tagged and
each of the k tags in the tagset. it can be built by composition of (te    tt ) against an
unweighted chain fsa ma(w) that is specially constructed to accept only a given
input w1, w2, . . . , wm , shown in figure 9.9. the trellis for input w is built from the
composition ma(w)     (te     tt ). composing with the unweighted ma(w) does not
affect the edge weights from (te     tt ), but it selects the subset of paths that generate
the word sequence w.

under contract with mit press, shared under cc-by-nc-nd license.

206

chapter 9. formal language theory

figure 9.8: finite state transducer for id48, with a small tagset of nouns
and verbs. for each pair of tags (including self-loops), there is an edge for every word in
the vocabulary. for simplicity, input and output are only shown for the edges from the
start state. weights are also omitted from the diagram; for each edge from qi to qj, the
weight is equal to log p(wm, ym = j | ym   1 = i), except for edges to the end state, which
are equal to log pr(ym = (cid:7) | ym   1 = i).

figure 9.9: chain    nite state acceptor for the input they can    sh.

9.1.5

*learning weighted    nite state automata

in generative models such as id165 language models and id48, the
edge weights correspond to log probabilities, which can be obtained from relative fre-
quency estimation. however, in other cases, we wish to learn the edge weights from in-
put/output pairs. this is dif   cult in non-deterministic    nite state automata, because we
do not observe the speci   c arcs that are traversed in accepting the input, or in transducing
from input to output. the path through the automaton is a latent variable.

chapter 5 presented one method for learning with latent variables: expectation max-
imization (em). this involves computing a distribution q(  ) over the latent variable, and
iterating between updates to this distribution and updates to the parameters     in this
case, the arc weights. the forward-backward algorithm (   7.5.3) describes a dynamic
program for computing a distribution over arcs in the trellis structure of a hidden markov

jacob eisenstein. draft of november 13, 2018.

startstartnvendn/aardvarkn/abacusn/...v/aardvarkv/abacusv/...starttheycan   sh9.2. context-free languages

207

model, but this is a special case of the more general problem for    nite state automata.
eisner (2002) describes an expectation semiring, which enables the expected number of
transitions across each arc to be computed through a semiring shortest-path algorithm.
alternative approaches for generative models include id115 (chi-
ang et al., 2010) and spectral learning (balle et al., 2011).

further a   eld, we can take a id88-style approach, with each arc corresponding
to a feature. the classic id88 update would update the weights by subtracting the
difference between the feature vector corresponding to the predicted path and the feature
vector corresponding to the correct path. since the path is not observed, we resort to a
latent variable id88. the model is described formally in    12.4, but the basic idea
is to compute an update from the difference between the features from the predicted path
and the features for the best-scoring path that generates the correct output.

9.2 context-free languages

beyond the class of regular languages lie the context-free languages. an example of a
language that is context-free but not    nite state is the set of arithmetic expressions with
balanced parentheses. intuitively, to accept only strings in this language, an fsa would
have to    count    the number of left parentheses, and make sure that they are balanced
against the number of right parentheses. an arithmetic expression can be arbitrarily long,
yet by de   nition an fsa has a    nite number of states. thus, for any fsa, there will be
a string with too many parentheses to count. more formally, the pumping lemma is a
proof technique for showing that languages are not regular. it is typically demonstrated
for the simpler case anbn, the language of strings containing a sequence of a   s, and then
an equal-length sequence of b   s.4

there are at least two arguments for the relevance of non-regular formal languages
to linguistics. first, there are natural language phenomena that are argued to be iso-
morphic to anbn. for english, the classic example is center embedding, shown in fig-
ure 9.10. the initial expression the dog speci   es a single dog. embedding this expression
into the cat
chased speci   es a particular cat     the one chased by the dog. this cat can
then be embedded again to specify a goat, in the less felicitous but arguably grammatical
expression, the goat the cat the dog chased kissed, which refers to the goat who was kissed
by the cat which was chased by the dog. chomsky (1957) argues that to be grammatical,
a center-embedded construction must be balanced: if it contains n noun phrases (e.g., the
cat), they must be followed by exactly n     1 verbs. an fsa that could recognize such ex-
pressions would also be capable of recognizing the language anbn. because we can prove
that no fsa exists for anbn, no fsa can exist for center embedded constructions either. en-

4details of the proof can be found in an introductory computer science theory textbook (e.g., sipser, 2012).

under contract with mit press, shared under cc-by-nc-nd license.

208

chapter 9. formal language theory

the goat

the cat
the cat

the dog
the dog chased
the dog chased kissed
. . .

figure 9.10: three levels of center embedding

glish includes center embedding, and so the argument goes, english grammar as a whole
cannot be regular.5

a more practical argument for moving beyond regular languages is modularity. many
linguistic phenomena     especially in syntax     involve constraints that apply at long
distance. consider the problem of determiner-noun number agreement in english: we
can say the coffee and these coffees, but not *these coffee. by itself, this is easy enough to model
in an fsa. however, fairly complex modifying expressions can be inserted between the
determiner and the noun:

(9.2)

a.
b.
c.
d.
e.

the burnt coffee
the badly-ground coffee
the burnt and badly-ground italian coffee
these burnt and badly-ground italian coffees
* these burnt and badly-ground italian coffee

again, an fsa can be designed to accept modifying expressions such as burnt and badly-
ground italian. let   s call this fsa fm . to reject the    nal example, a    nite state acceptor
must somehow    remember    that the determiner was plural when it reaches the noun cof-
fee at the end of the expression. the only way to do this is to make two identical copies
of fm : one for singular determiners, and one for plurals. while this is possible in the
   nite state framework, it is inconvenient     especially in languages where more than one
attribute of the noun is marked by the determiner. context-free languages facilitate mod-
ularity across such long-range dependencies.

9.2.1 context-free grammars

context-free languages are speci   ed by context-free grammars (id18s), which are tuples
(n,   , r, s) consisting of:

5the claim that arbitrarily deep center-embedded expressions are grammatical has drawn skepticism.
corpus evidence shows that embeddings of depth greater than two are exceedingly rare (karlsson, 2007),
and that embeddings of depth greater than three are completely unattested. if center-embedding is capped
at some    nite depth, then it is regular.

jacob eisenstein. draft of november 13, 2018.

9.2. context-free languages

209

s    s op s | num
op    + |     |    |   

num    num digit | digit
digit    0 | 1 | 2 | . . . | 9

figure 9.11: a context-free grammar for arithmetic expressions

    a    nite set of non-terminals n;
    a    nite alphabet    of terminal symbols;
    a set of production rules r, each of the form a       , where a     n and        (      n )   ;
    a designated start symbol s.
in the production rule a       , the left-hand side (lhs) a must be a non-terminal;
   , n    
the right-hand side (rhs) can be a sequence of terminals or non-terminals, {n,   }
n,          . a non-terminal can appear on the left-hand side of many production rules.
a non-terminal can appear on both the left-hand side and the right-hand side; this is a
recursive production, and is analogous to self-loops in    nite state automata. the name
   context-free    is based on the property that the production rule depends only on the lhs,
and not on its ancestors or neighbors; this is analogous to markov property of    nite state
automata, in which the behavior at each step depends only on the current state, and not
on the path by which that state was reached.
a derivation    is a sequence of steps from the start symbol s to a surface string w          ,
which is the yield of the derivation. a string w is in a context-free language if there is
some derivation from s yielding w. parsing is the problem of    nding a derivation for a
string in a grammar. algorithms for parsing are described in chapter 10.

like id157, context-free grammars de   ne the language but not the com-
putation necessary to recognize it. the context-free analogues to    nite state acceptors are
pushdown automata, a theoretical model of computation in which input symbols can be
pushed onto a stack with potentially in   nite depth. for more details, see sipser (2012).

example
figure 9.11 shows a context-free grammar for arithmetic expressions such as 1 + 2   3    4.
in this grammar, the terminal symbols include the digits {1, 2, ..., 9} and the op-
erators {+,   ,  ,  }. the rules include the | symbol, a notational convenience that makes
it possible to specify multiple right-hand sides on a single line: the statement a     x | y
under contract with mit press, shared under cc-by-nc-nd license.

210

chapter 9. formal language theory

figure 9.12: some example derivations from the arithmetic grammar in figure 9.11

de   nes two productions, a     x and a     y. this grammar is recursive: the non-termals s
and num can produce themselves.

derivations are typically shown as trees, with production rules applied from the top
to the bottom. the tree on the left in figure 9.12 describes the derivation of a single digit,
through the sequence of productions s     num     digit     4 (these are all unary pro-
ductions, because the right-hand side contains a single element). the other two trees in
figure 9.12 show alternative derivations of the string 1 + 2     3. the existence of multiple
derivations for a string indicates that the grammar is ambiguous.

context-free derivations can also be written out according to the pre-order tree traver-

sal.6 for the two derivations of 1 + 2 - 3 in figure 9.12, the notation is:

(s (s (s (num (digit 1))) (op +) (s (num (digit 2)))) (op - ) (s (num (digit 3))))
(s (s (num (digit 1))) (op +) (s (num (digit 2)) (op - ) (s (num (digit 3))))).

[9.23]
[9.24]

grammar equivalence and chomsky normal form

a single context-free language can be expressed by more than one context-free grammar.
for example, the following two grammars both de   ne the language anbn for n > 0.

s    asb | ab
s    asb | aabb | ab

two grammars are weakly equivalent if they generate the same strings. two grammars
are strongly equivalent if they generate the same strings via the same derivations. the
grammars above are only weakly equivalent.

6this is a depth-   rst left-to-right search that prints each node the    rst time it is encountered (cormen

et al., 2009, chapter 12).

jacob eisenstein. draft of november 13, 2018.

snumdigit4ssnumdigit3op   ssnumdigit2op+snumdigit1sssnumdigit3op   snumdigit2op+snumdigit19.2. context-free languages

211

in chomsky normal form (cnf), the right-hand side of every production includes

either two non-terminals, or a single terminal symbol:

a    bc
a    a

all id18s can be converted into a cnf grammar that is weakly equivalent. to convert a
grammar into cnf, we    rst address productions that have more than two non-terminals
on the rhs by creating new    dummy    non-terminals. for example, if we have the pro-
duction,

it is replaced with two productions,

w     x y z,

[9.25]

w    x w\x

[9.26]
[9.27]
in these productions, w\x is a new dummy non-terminal. this transformation binarizes
the grammar, which is critical for ef   cient bottom-up parsing, as we will see in chapter 10.
productions whose right-hand side contains a mix of terminal and non-terminal symbols
can be replaced in a similar fashion.

w\x    y z.

unary non-terminal productions a     b are replaced as follows: for each production
b        in the grammar, add a new production a       . for example, in the grammar
described in figure 9.11, we would replace num     digit with num     1 | 2 | . . . | 9.
however, we keep the production num     num digit, which is a valid binary produc-
tion.

9.2.2 natural language syntax as a context-free language
context-free grammars can be used to represent syntax, which is the set of rules that
determine whether an utterance is judged to be grammatical. if this representation were
perfectly faithful, then a natural language such as english could be transformed into a
formal language, consisting of exactly the (in   nite) set of strings that would be judged to
be grammatical by a    uent english speaker. we could then build parsing software that
would automatically determine if a given utterance were grammatical.7

contemporary theories generally do not consider natural languages to be context-free
(see    9.3), yet context-free grammars are widely used in natural language parsing. the
reason is that context-free representations strike a good balance: they cover a broad range
of syntactic phenomena, and they can be parsed ef   ciently. this section therefore de-
scribes how to handle a core fragment of english syntax in context-free form, following
7to move beyond this cursory treatment of syntax, consult the short introductory manuscript by bender

(2013), or the longer text by akmajian et al. (2010).

under contract with mit press, shared under cc-by-nc-nd license.

212

chapter 9. formal language theory

the conventions of the id32 (ptb; marcus et al., 1993), a large-scale annotation
of english language syntax. the generalization to    mildly    context-sensitive languages is
discussed in    9.3.

the id32 annotation is a phrase-structure grammar of english. this means
that sentences are broken down into constituents, which are contiguous sequences of
words that function as coherent units for the purpose of linguistic analysis. constituents
generally have a few key properties:

movement. constituents can often be moved around sentences as units.

(9.3)

a. abigail gave (her brother) (a    sh).
b. abigail gave (a    sh) to (her brother).

in contrast, gave her and brother a cannot easily be moved while preserving gram-
maticality.

substitution. constituents can be substituted by other phrases of the same type.

(9.4)

a. max thanked (his older sister).
b. max thanked (her).

in contrast, substitution is not possible for other contiguous units like max thanked
and thanked his.

coordination. coordinators like and and or can conjoin constituents.

(9.5)

(abigail) and (her younger brother) bought a    sh.

a.
b. abigail (bought a    sh) and (gave it to max).
c. abigail (bought) and (greedily ate) a    sh.

units like brother bought and bought a cannot easily be coordinated.

these examples argue for units such as her brother and bought a    sh to be treated as con-
stituents. other sequences of words in these examples, such as abigail gave and brother
a    sh, cannot be moved, substituted, and coordinated in these ways. in phrase-structure
grammar, constituents are nested, so that the senator from new jersey contains the con-
stituent from new jersey, which in turn contains new jersey. the sentence itself is the max-
imal constituent; each word is a minimal constituent, derived from a unary production
from a part-of-speech tag. between part-of-speech tags and sentences are phrases.
in
phrase-structure grammar, phrases have a type that is usually determined by their head
word: for example, a noun phrase corresponds to a noun and the group of words that

jacob eisenstein. draft of november 13, 2018.

9.2. context-free languages

213

modify it, such as her younger brother; a verb phrase includes the verb and its modi   ers,
such as bought a    sh and greedily ate it.

in context-free grammars, each phrase type is a non-terminal, and each constituent is
the substring that the non-terminal yields. grammar design involves choosing the right
set of non-terminals. fine-grained non-terminals make it possible to represent more    ne-
grained linguistic phenomena. for example, by distinguishing singular and plural noun
phrases, it is possible to have a grammar of english that generates only sentences that
obey subject-verb agreement. however, enforcing subject-verb agreement is considerably
more complicated in languages like spanish, where the verb must agree in both person
and number with subject. in general, grammar designers must trade off between over-
generation     a grammar that permits ungrammatical sentences     and undergeneration
    a grammar that fails to generate grammatical sentences. furthermore, if the grammar is
to support manual annotation of syntactic structure, it must be simple enough to annotate
ef   ciently.

9.2.3 a phrase-structure grammar for english

to better understand how phrase-structure grammar works, let   s consider the speci   c
case of the id32 grammar of english. the main phrase categories in the penn
treebank (ptb) are based on the main part-of-speech classes: noun phrase (np), verb
phrase (vp), prepositional phrase (pp), adjectival phrase (adjp), and adverbial phrase
(advp). the top-level category is s, which conveniently stands in for both    sentence   
and the    start    symbol. complement clauses (e.g., i take the good old fashioned ground that
the whale is a    sh) are represented by the non-terminal sbar. the terminal symbols in
the grammar are individual words, which are generated from unary productions from
part-of-speech tags (the ptb tagset is described in    8.1).

this section describes some of the most common productions from the major phrase-
level categories, explaining how to generate individual tag sequences. the production
rules are approached in a    theory-driven    manner:    rst the syntactic properties of each
phrase type are described, and then some of the necessary production rules are listed. but
it is important to keep in mind that the id32 was produced in a    data-driven   
manner. after the set of non-terminals was speci   ed, annotators were free to analyze each
sentence in whatever way seemed most linguistically accurate, subject to some high-level
guidelines. the grammar of the id32 is simply the set of productions that were
required to analyze the several million words of the corpus. by design, the grammar
overgenerates     it does not exclude ungrammatical sentences. furthermore, while the
productions shown here cover some of the most common cases, they are only a small
fraction of the several thousand different types of productions in the id32.

under contract with mit press, shared under cc-by-nc-nd license.

214

sentences

chapter 9. formal language theory

the most common production rule for sentences is,

s    np vp

[9.28]

which accounts for simple sentences like abigail ate the kimchi     as we will see, the direct
object the kimchi is part of the verb phrase. but there are more complex forms of sentences
as well:

s    advp np vp
s    s cc s
s    vp

unfortunately abigail ate the kimchi.
abigail ate the kimchi and max had a burger.
eat the kimchi.

[9.29]
[9.30]
[9.31]

where advp is an adverbial phrase (e.g., unfortunately, very unfortunately) and cc is a
coordinating conjunction (e.g., and, but).8

noun phrases

noun phrases refer to entities, real or imaginary, physical or abstract: asha, the steamed
dumpling, parts and labor, nobody, the whiteness of the whale, and the rise of revolutionary syn-
dicalism in the early twentieth century. noun phrase productions include    bare    nouns,
which may optionally follow determiners, as well as pronouns:

np    nn | nns | nnp | prp
np    det nn | det nns | det nnp

[9.32]
[9.33]

the tags nn, nns, and nnp refer to singular, plural, and proper nouns; prp refers to
personal pronouns, and det refers to determiners. the grammar also contains terminal
productions from each of these tags, e.g., prp     i | you | we | . . . .
noun phrases may be modi   ed by adjectival phrases (adjp; e.g., the small russian dog)
and numbers (cd; e.g., the    ve pastries), each of which may optionally follow a determiner:

np    adjp nn | adjp nns | det adjp nn | det adjp nns
np    cd nns | det cd nns | . . .

[9.34]
[9.35]

some noun phrases include multiple nouns, such as the liberation movement and an

antelope horn, necessitating additional productions:

[9.36]
8notice that the grammar does not include the recursive production s     advp s. it may be helpful to

np    nn nn | nn nns | det nn nn | . . .

think about why this production would cause the grammar to overgenerate.

jacob eisenstein. draft of november 13, 2018.

9.2. context-free languages

215

these multiple noun constructions can be combined with adjectival phrases and cardinal
numbers, leading to a large number of additional productions.

recursive noun phrase productions include coordination, prepositional phrase attach-

ment, subordinate clauses, and verb phrase adjuncts:

np    np cc np
np    np pp
np    np sbar
np    np vp

e.g., the red and the black
e.g., the president of the georgia institute of technology
e.g., a whale which he had wounded
e.g., a whale taken near shetland

[9.37]
[9.38]
[9.39]
[9.40]

these recursive productions are a major source of ambiguity, because the vp and pp non-
terminals can also generate np children. thus, the the president of the georgia institute of
technology can be derived in two ways, as can a whale taken near shetland in october.

but aside from these few recursive productions, the noun phrase fragment of the penn
treebank grammar is relatively    at, containing a large of number of productions that go
from np directly to a sequence of parts-of-speech. if noun phrases had more internal
structure, the grammar would need fewer rules, which, as we will see, would make pars-
ing faster and machine learning easier. vadas and curran (2011) propose to add additional
structure in the form of a new non-terminal called a nominal modi   er (nml), e.g.,

(9.6)

a.
b.

(np (nn crude) (nn oil) (nns prices))
(np (nml (nn crude) (nn oil)) (nns prices))

(ptb analysis)

(nml-style analysis).

another proposal is to treat the determiner as the head of a determiner phrase (dp;
abney, 1987). there are linguistic arguments for and against determiner phrases (e.g.,
van eynde, 2006). from the perspective of context-free grammar, dps enable more struc-
tured analyses of some constituents, e.g.,

(9.7)

a.
b.

(np (dt the) (jj white) (nn whale))
(dp (dt the) (np (jj white) (nn whale)))

(ptb analysis)

(dp-style analysis).

verb phrases

verb phrases describe actions, events, and states of being. the ptb tagset distinguishes
several classes of verb in   ections: base form (vb; she likes to snack), present-tense third-
person singular (vbz; she snacks), present tense but not third-person singular (vbp; they
snack), past tense (vbd; they snacked), present participle (vbg; they are snacking), and past
participle (vbn; they had snacked).9 each of these forms can constitute a verb phrase on its

9this tagset is speci   c to english: for example, vbp is a meaningful category only because english mor-

phology distinguishes third-person singular from all person-number combinations.

under contract with mit press, shared under cc-by-nc-nd license.

216

own:

chapter 9. formal language theory

vp     vb | vbz | vbd | vbn | vbg | vbp

[9.41]

more complex verb phrases can be formed by a number of recursive productions,
including the use of coordination, modal verbs (md; she should snack), and the in   nitival
to (to):

vp     md vp
vp     vbd vp
vp     vbz vp
vp     vbn vp
vp     to vp
vp     vp cc vp

she will snack
she had snacked
she has been snacking
she has been snacking
she wants to snack
she buys and eats many snacks

[9.42]
[9.43]
[9.44]
[9.45]
[9.46]
[9.47]

each of these productions uses recursion, with the vp non-terminal appearing in both the
lhs and rhs. this enables the creation of complex verb phrases, such as she will have
wanted to have been snacking.

transitive verbs take noun phrases as direct objects, and ditransitive verbs take two

direct objects:

vp     vbz np
vp     vbg np
vp     vbd np np

she teaches algebra
she has been teaching algebra
she taught her brother algebra

[9.48]
[9.49]
[9.50]

these productions are not recursive, so a unique production is required for each verb
part-of-speech. they also do not distinguish transitive from intransitive verbs, so the
resulting grammar overgenerates examples like *she sleeps sushi and *she learns boyang
algebra. sentences can also be direct objects:

vp     vbz s
vp     vbz sbar

hunter wants to eat the kimchi
hunter knows that tristan ate the kimchi

[9.51]
[9.52]

the    rst production overgenerates, licensing sentences like *hunter sees tristan eats the
kimchi. this problem could be addressed by designing a more speci   c set of sentence
non-terminals, indicating whether the main verb can be conjugated.

verbs can also be modi   ed by prepositional phrases and adverbial phrases:

vp     vbz pp
vp     vbz advp
vp     advp vbg

she studies at night
she studies intensively
she is not studying

[9.53]
[9.54]
[9.55]

jacob eisenstein. draft of november 13, 2018.

9.2. context-free languages

217

again, because these productions are not recursive, the grammar must include produc-
tions for every verb part-of-speech.

a special set of verbs, known as copula, can take predicative adjectives as direct ob-

jects:

vp     vbz adjp
vp     vbp adjp

she is hungry
success seems increasingly unlikely

[9.56]
[9.57]

the ptb does not have a special non-terminal for copular verbs, so this production gen-
erates noid165matical examples such as *she eats tall.

particles (prt as a phrase; rp as a part-of-speech) work to create phrasal verbs:

vp     vb prt
vp     vbd prt np

she told them to fuck off
they gave up their ill-gotten gains

[9.58]
[9.59]

as the second production shows, particle productions are required for all con   gurations
of verb parts-of-speech and direct objects.

other contituents

the remaining constituents require far fewer productions. prepositional phrases almost
always consist of a preposition and a noun phrase,

pp     in np
pp     to np

the whiteness of the whale
what the white whale was to ahab, has been hinted

[9.60]
[9.61]

similarly, complement clauses consist of a complementizer (usually a preposition, pos-

sibly null) and a sentence,

sbar     in s
sbar     s

she said that it was spicy
she said it was spicy

[9.62]
[9.63]

adverbial phrases are usually bare adverbs (advp     rb), with a few exceptions:

advp     rb rbr
advp     advp pp

they went considerably further
they went considerably further than before

[9.64]
[9.65]

the tag rbr is a comparative adverb.

under contract with mit press, shared under cc-by-nc-nd license.

218

chapter 9. formal language theory

adjectival phrases extend beyond bare adjectives (adjp     jj) in a number of ways:
[9.66]
[9.67]
[9.68]
[9.69]
[9.70]
[9.71]
[9.72]

very hungry
more hungry
best possible
even bigger
high and mighty
west german
previously reported

adjp     rb jj
adjp     rbr jj
adjp     jjs jj
adjp     rb jjr
adjp     jj cc jj
adjp     jj jj
adjp     rb vbn

the tags jjr and jjs refer to comparative and superlative adjectives respectively.

all of these phrase types can be coordinated:

pp    pp cc pp

advp    advp cc advp
adjp    adjp cc adjp
sbar    sbar cc sbar

on time and under budget
now and two years ago
quaint and rather deceptive
whether they want control
or whether they want exports

[9.73]
[9.74]
[9.75]
[9.76]

9.2.4 grammatical ambiguity
context-free parsing is useful not only because it determines whether a sentence is gram-
matical, but mainly because the constituents and their relations can be applied to tasks
such as information extraction (chapter 17) and sentence compression (jing, 2000; clarke
and lapata, 2008). however, the ambiguity of wide-coverage natural language grammars
poses a serious problem for such potential applications. as an example, figure 9.13 shows
two possible analyses for the simple sentence we eat sushi with chopsticks, depending on
whether the chopsticks modify eat or sushi. realistic grammars can license thousands or
even millions of parses for individual sentences. weighted context-free grammars solve
this problem by attaching weights to each production, and selecting the derivation with
the highest score. this is the focus of chapter 10.

9.3

*mildly context-sensitive languages

beyond context-free languages lie context-sensitive languages, in which the expansion
of a non-terminal depends on its neighbors.
in the general class of context-sensitive
languages, computation becomes much more challenging: the membership problem for
context-sensitive languages is pspace-complete. since pspace contains the complexity
class np (problems that can be solved in polynomial time on a non-deterministic turing

jacob eisenstein. draft of november 13, 2018.

9.3. *mildly context-sensitive languages

219

figure 9.13: two derivations of the same sentence

machine), pspace-complete problems cannot be solved ef   ciently if p (cid:54)= np. thus, de-
signing an ef   cient parsing algorithm for the full class of context-sensitive languages is
probably hopeless.10

however, joshi (1985) identi   es a set of properties that de   ne mildly context-sensitive
languages, which are a strict subset of context-sensitive languages. like context-free lan-
guages, mildly context-sensitive languages are parseable in polynomial time. however,
the mildly context-sensitive languages include non-context-free languages, such as the
   copy language    {ww | w          
} and the language ambncmdn. both are characterized by
cross-serial dependencies, linking symbols at long distance across the string.11 for exam-
ple, in the language anbmcndm, each a symbol is linked to exactly one c symbol, regardless
of the number of intervening b symbols.

9.3.1 context-sensitive phenomena in natural language

such phenomena are occasionally relevant to natural language. a classic example is found
in swiss-german (shieber, 1985), in which sentences such as we let the children help hans
paint the house are realized by listing all nouns before all verbs, i.e., we the children hans the
house let help paint. furthermore, each noun   s determiner is dictated by the noun   s case
marking (the role it plays with respect to the verb). using an argument that is analogous
to the earlier discussion of center-embedding (   9.2), shieber describes these case marking
constraints as a set of cross-serial dependencies, homomorphic to ambncmdn, and therefore
not context-free.

10if pspace (cid:54)= np, then it contains problems that cannot be solved in polynomial time on a non-
deterministic turing machine; equivalently, solutions to these problems cannot even be checked in poly-
nomial time (arora and barak, 2009).

11a further condition of the set of mildly-context-sensitive languages is constant growth: if the strings in
the language are arranged by length, the gap in length between any pair of adjacent strings is bounded by
some language speci   c constant. this condition excludes languages such as {a2n | n     0}.

under contract with mit press, shared under cc-by-nc-nd license.

svpnpppnpchopsticksinwithnpsushiveatnpwesvpppnpchopsticksinwithvpnpsushiveatnpwe220

chapter 9. formal language theory

figure 9.14: a syntactic analysis in id35 involving forward and backward function appli-
cation

as with the move from regular to context-free languages, mildly context-sensitive
languages can also be motivated by expedience. while    nite sequences of cross-serial
dependencies can in principle be handled in a context-free grammar, it is often more con-
venient to use a mildly context-sensitive formalism like tree-adjoining grammar (tag)
and id35 (id35). tag-inspired parsers have been shown to
be particularly effective in parsing the id32 (collins, 1997; carreras et al., 2008),
and id35 plays a leading role in current research on id29 (zettlemoyer and
collins, 2005). these two formalisms are weakly equivalent: any language that can be
speci   ed in tag can also be speci   ed in id35, and vice versa (joshi et al., 1991). the re-
mainder of the chapter gives a brief overview of id35, but you are encouraged to consult
joshi and schabes (1997) and steedman and baldridge (2011) for more detail on tag and
id35 respectively.

9.3.2 id35

in id35, structural analyses are built up through a small set
of generic combinatorial operations, which apply to immediately adjacent sub-structures.
these operations act on the categories of the sub-structures, producing a new structure
with a new category. the basic categories include s (sentence), np (noun phrase), vp
(verb phrase) and n (noun). the goal is to label the entire span of text as a sentence, s.

complex categories, or types, are constructed from the basic categories, parentheses,
and forward and backward slashes: for example, s/np is a complex type, indicating a
sentence that is lacking a noun phrase to its right; s\np is a sentence lacking a noun
phrase to its left. complex types act as functions, and the most basic combinatory oper-
ations are function application to either the right or left neighbor. for example, the type
of a verb phrase, such as eats, would be s\np. applying this function to a subject noun
phrase to its left results in an analysis of abigail eats as category s, indicating a successful
parse.

transitive verbs must    rst be applied to the direct object, which in english appears to
the right of the verb, before the subject, which appears on the left. they therefore have the
more complex type (s\np)/np. similarly, the application of a determiner to the noun at
jacob eisenstein. draft of november 13, 2018.

abigaileatsthekimchinp(s\np)/np(np/n)n>np>s\np<s9.3. *mildly context-sensitive languages

221

figure 9.15: a syntactic analysis in id35 involving function composition (example modi-
   ed from steedman and baldridge, 2011)

its right results in a noun phrase, so determiners have the type np/n. figure 9.14 pro-
vides an example involving a transitive verb and a determiner. a key point from this
example is that it can be trivially transformed into phrase-structure tree, by treating each
function application as a constituent phrase. indeed, when id35   s only combinatory op-
erators are forward and backward function application, it is equivalent to context-free
grammar. however, the location of the    effort    has changed. rather than designing good
productions, the grammar designer must focus on the lexicon     choosing the right cate-
gories for each word. this makes it possible to parse a wide range of sentences using only
a few generic combinatory operators.

things become more interesting with the introduction of two additional operators:
composition and type-raising. function composition enables the combination of com-
plex types: x/y     y /z    b x/z (forward composition) and y \z     x\y    b x\z (back-
ward composition).12 composition makes it possible to    look inside    complex types, and
combine two adjacent units if the    input    for one is the    output    for the other. figure 9.15
shows how function composition can be used to handle modal verbs. while this sen-
tence can be parsed using only function application, the composition-based analysis is
preferable because the unit might learn functions just like a transitive verb, as in the exam-
ple abigail studies swahili. this in turn makes it possible to analyze conjunctions such as
abigail studies and might learn swahili, attaching the direct object swahili to the entire con-
joined verb phrase studies and might learn. the id32 grammar fragment from
   9.2.3 would be unable to handle this case correctly: the direct object swahili could attach
only to the second verb learn.

type raising converts an element of type x to a more complex type: x    t t /(t\x)
(forward type-raising to type t ), and x    t t\(t /x) (backward type-raising to type
t ). type-raising makes it possible to reverse the relationship between a function and its
argument     by transforming the argument into a function over functions over arguments!
an example may help. figure 9.15 shows how to analyze an object relative clause, a story
that abigail tells. the problem is that tells is a transitive verb, expecting a direct object to
its right. as a result, abigail tells is not a valid constituent. the issue is resolved by raising

12the subscript b follows notation from curry and feys (1958).

under contract with mit press, shared under cc-by-nc-nd license.

abigailmightlearnswahilinp(s\np)/vpvp/npnp>b(s\np)/np>s\np<s222

chapter 9. formal language theory

figure 9.16: a syntactic analysis in id35 involving an object relative clause

abigail from np to the complex type (s/np)\np). this function can then be combined
with the transitive verb tells by forward composition, resulting in the type (s/np), which
is a sentence lacking a direct object to its right.13 from here, we need only design the
lexical entry for the complementizer that to expect a right neighbor of type (s/np), and
the remainder of the derivation can proceed by function application.

composition and type-raising give id35 considerable power and    exibility, but at a
price. the simple sentence abigail tells max can be parsed in two different ways: by func-
tion application (   rst forming the verb phrase tells max), and by type-raising and compo-
sition (   rst forming the non-constituent abigail tells). this derivational ambiguity does
not affect the resulting linguistic analysis, so it is sometimes known as spurious ambi-
guity. hockenmaier and steedman (2007) present a translation algorithm for converting
the id32 into id35 derivations, using composition and type-raising only when
necessary.

exercises

1. sketch out the state diagram for    nite-state acceptors for the following languages

on the alphabet {a, b}.
a) even-length strings. (be sure to include 0 as an even number.)
b) strings that contain aaa as a substring.
c) strings containing an even number of a and an odd number of b symbols.
d) strings in which the substring bbb must be terminal if it appears     the string

need not contain bbb, but if it does, nothing can come after it.

2. levenshtein id153 is the number of insertions, substitutions, or deletions

required to convert one string to another.

13the missing direct object would be analyzed as a trace in id18-like approaches to syntax, including the

id32.

jacob eisenstein. draft of november 13, 2018.

astorythatabigailtellsnp(np\np)/(s/np)np(s\np)/np>ts/(s\np)>bs/np>np\np<np9.3. *mildly context-sensitive languages

223

a) de   ne a    nite-state acceptor that accepts all strings with id153 1 from

the target string, target.

b) now think about how to generalize your design to accept all strings with edit
distance from the target string equal to d. if the target string has length (cid:96), what
is the minimal number of states required?

3. construct an fsa in the style of figure 9.3, which handles the following examples:

    nation/n, national/adj, nationalize/v, nationalizer/n
    america/n, american/adj, americanize/v, americanizer/n

be sure that your fsa does not accept any further derivations, such as *nationalizeral
and *americanizern.

4. show how to construct a trigram language model in a weighted    nite-state acceptor.

make sure that you handle the edge cases at the beginning and end of the input.

5. extend the fst in figure 9.6 to handle the other two parts of rule 1a of the porter

stemmer: -sses     ss, and -ies     -i.

6.    9.1.4 describes to, a transducer that captures english orthography by transduc-
ing cook + ed     cooked and bake + ed     baked. design an unweighted    nite-state
transducer that captures this property of english orthography.
next, augment the transducer to appropriately model the suf   x -s when applied to
words ending in s, e.g. kiss+s     kisses.

7. add parenthesization to the grammar in figure 9.11 so that it is no longer ambigu-

ous.

8. construct three examples     a noun phrase, a verb phrase, and a sentence     which
can be derived from the id32 grammar fragment in    9.2.3, yet are not
grammatical. avoid reusing examples from the text. optionally, propose corrections
to the grammar to avoid generating these cases.

9. produce parses for the following sentences, using the id32 grammar frag-

ment from    9.2.3.

(9.8) this aggression will not stand.
(9.9)
(9.10) sometimes you eat the bar and sometimes the bar eats you.

i can get you a toe.

then produce parses for three short sentences from a news article from this week.

under contract with mit press, shared under cc-by-nc-nd license.

224

chapter 9. formal language theory

10. * one advantage of id35 is its    exibility in handling coordination:

(9.11)

a. hunter and tristan speak hawaiian
b. hunter speaks and tristan understands hawaiian

de   ne the lexical entry for and as

and := (x/x)\x,

[9.77]

where x can refer to any type. using this lexical entry, show how to parse the two
examples above. in the second example, swahili should be combined with the coor-
dination abigail speaks and max understands, and not just with the verb understands.

jacob eisenstein. draft of november 13, 2018.

chapter 10

context-free parsing

parsing is the task of determining whether a string can be derived from a given context-
free grammar, and if so, how. a parser   s output is a tree, like the ones shown in fig-
ure 9.13. such trees can answer basic questions of who-did-what-to-whom, and have ap-
plications in downstream tasks like semantic analysis (chapter 12 and 13) and information
extraction (chapter 17).

for a given input and grammar, how many parse trees are there? consider a minimal

context-free grammar with only one non-terminal, x, and the following productions:

x    x x
x    aardvark | abacus | . . . | zyther

the second line indicates unary productions to every nonterminal in   . in this gram-
mar, the number of possible derivations for a string w is equal to the number of binary
bracketings, e.g.,

((((w1 w2) w3) w4) w5),

(((w1 (w2 w3)) w4) w5),

((w1 (w2(w3 w4))) w5),

. . . .

the number of such bracketings is a catalan number, which grows super-exponentially
in the length of the sentence, cn = (2n)!
(n+1)!n!. as with sequence labeling, it is only possible to
exhaustively search the space of parses by resorting to locality assumptions, which make it
possible to search ef   ciently by reusing shared substructures with id145.
this chapter focuses on a bottom-up id145 algorithm, which enables
exhaustive search of the space of possible parses, but imposes strict limitations on the
form of scoring function. these limitations can be relaxed by abandoning exhaustive
search. non-exact search methods will be brie   y discussed at the end of this chapter, and
one of them     transition-based parsing     will be the focus of chapter 11.

225

226

chapter 10. context-free parsing

    np vp
s
np     np pp | we | sushi | chopsticks
pp     in np
in     with
vp     v np | vp pp
v     eat

table 10.1: a toy example context-free grammar

10.1 deterministic bottom-up parsing

the cky algorithm1 is a bottom-up approach to parsing in a context-free grammar. it
ef   ciently tests whether a string is in a language, without enumerating all possible parses.
the algorithm    rst forms small constituents, and then tries to merge them into larger
constituents.

to understand the algorithm, consider the input, we eat sushi with chopsticks. accord-
ing to the toy grammar in table 10.1, each terminal symbol can be generated by exactly
one unary production, resulting in the sequence np v np in np. in real examples, there
may be many unary productions for each individual token. in any case, the next step
is to try to apply binary productions to merge adjacent symbols into larger constituents:
for example, v np can be merged into a verb phrase (vp), and in np can be merged
into a prepositional phrase (pp). bottom-up parsing searches for a series of mergers that
ultimately results in the start symbol s covering the entire input.

the cky algorithm systematizes this search by incrementally constructing a table t in
which each cell t[i, j] contains the set of nonterminals that can derive the span wi+1:j. the
algorithm    lls in the upper right triangle of the table; it begins with the diagonal, which
corresponds to substrings of length 1, and then computes derivations for progressively
larger substrings, until reaching the upper right corner t[0, m ], which corresponds to the
entire input, w1:m . if the start symbol s is in t[0, m ], then the string w is in the language
de   ned by the grammar. this process is detailed in algorithm 13, and the resulting data
structure is shown in figure 10.1. informally, here   s how it works:

    begin by    lling in the diagonal: the cells t[m     1, m] for all m     {1, 2, . . . , m}. these
cells are    lled with terminal productions that yield the individual tokens; for the
word w2 = sushi, we    ll in t[1, 2] = {np}, and so on.

    then    ll in the next diagonal, in which each cell corresponds to a subsequence of
length two: t[0, 2], t[1, 3], . . . , t[m     2, m ]. these cells are    lled in by looking for
it is a special case of chart

1the name is for cocke-kasami-younger, the inventors of the algorithm.
parsing, because its stores reusable computations in a chart-like data structure.

jacob eisenstein. draft of november 13, 2018.

10.1. deterministic bottom-up parsing

227

binary productions capable of producing at least one entry in each of the cells corre-
sponding to left and right children. for example, vp can be placed in the cell t[1, 3]
because the grammar includes the production vp     v np, and because the chart
contains v     t[1, 2] and np     t[2, 3].

    at the next diagonal, the entries correspond to spans of length three. at this level,
there is an additional decision at each cell: where to split the left and right children.
the cell t[i, j] corresponds to the subsequence wi+1:j, and we must choose some
split point i < k < j, so that the span wi+1:k is the left child, and the span wk+1:j
is the right child. we consider all possible k, looking for productions that generate
elements in t[i, k] and t[k, j]; the left-hand side of all such productions can be added
to t[i, j]. when it is time to compute t[i, j], the cells t[i, k] and t[k, j] are guaranteed
to be complete, since these cells correspond to shorter sub-strings of the input.

    the process continues until we reach t[0, m ].

figure 10.1 shows the chart that arises from parsing the sentence we eat sushi with chop-
sticks using the grammar de   ned above.

10.1.1 recovering the parse tree

as with the viterbi algorithm, it is possible to identify a successful parse by storing and
traversing an additional table of back-pointers. if we add an entry x to cell t[i, j] by using
the production x     y z and the split point k, then we store the back-pointer b[i, j, x] =
(y, z, k). once the table is complete, we can recover a parse by tracing this pointers,
starting at b[0, m, s], and stopping when they ground out at terminal productions.

for ambiguous sentences, there will be multiple paths to reach s     t[0, m ]. for exam-
ple, in figure 10.1, the goal state s     t[0, m ] is reached through the state vp     t[1, 5], and
there are two different ways to generate this constituent: one with (eat sushi) and (with
chopsticks) as children, and another with (eat) and (sushi with chopsticks) as children. the
presence of multiple paths indicates that the input can be generated by the grammar in
more than one way. in algorithm 13, one of these derivations is selected arbitrarily. as
discussed in    10.3, weighted context-free grammars compute a score for all permissible
derivations, and a minor modi   cation of cky allows it to identify the single derivation
with the maximum score.

10.1.2 non-binary productions

as presented above, the cky algorithm assumes that all productions with non-terminals
on the right-hand side (rhs) are binary. in real grammars, such as the one considered in
chapter 9, there are other types of productions: some have more than two elements on the
right-hand side, and others produce a single non-terminal.

under contract with mit press, shared under cc-by-nc-nd license.

228

chapter 10. context-free parsing

for m     {1 . . . m} do
for (cid:96)     {2, 3, . . . , m} do

t[m     1, m]     {x : (x     wm)     r}
for m     {0, 1, . . . m     (cid:96)} do

algorithm 13 the cky algorithm for parsing a sequence w           in a context-free
grammar g = (n,   , r, s), with non-terminals n, production rules r, and start sym-
bol s. the grammar is assumed to be in chomsky normal form (   9.2.1). the function
pickfrom(b[i, j, x]) selects an element of the set b[i, j, x] arbitrarily. all values of t and
b are initialized to    .
1: procedure cky(w, g = (n,   , r, s))
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: procedure traceback(x, i, j, b)
16:
17:
18:
19:
20:

for k     {m + 1, m + 2, . . . , m + (cid:96)     1} do
if y     t[m, k]     z     t[k, m + (cid:96)] then

(cid:46) iterate over constituent lengths
(cid:46) iterate over left endpoints
(cid:46) iterate over split points
(cid:46) iterate over rules

t[m, m + (cid:96)]     t[m, m + (cid:96)]     x
b[m, m + (cid:96), x]     b[m, m + (cid:96), x]     (y, z, k)

(y, z, k)     pickfrom(b[i, j, x])
return x     (traceback(y, i, k, b), traceback(z, k, j, b))

(cid:46) add non-terminal to table
(cid:46) add back-pointers

if s     t[0, m ] then
else

for (x     y z)     r do

return traceback(s, 0, m, b)

if j = i + 1 then

return    

return x

else

    productions with more than two elements on the right-hand side can be binarized
by creating additional non-terminals, as described in    9.2.1. for example, the pro-
duction vp     v np np (for ditransitive verbs) can be converted to vp     vpditrans/np np,
by adding the non-terminal vpditrans/np and the production vpditrans/np     v np.

    what about unary productions like vp     v? while such productions are not a
part of chomsky normal form     and can therefore be eliminated in preprocessing
the grammar     in practice, a more typical solution is to modify the cky algorithm.
the algorithm makes a second pass on each diagonal in the table, augmenting each
cell t[i, j] with all possible unary productions capable of generating each item al-
ready in the cell: formally, t[i, j] is extended to its unary closure. suppose the ex-
ample grammar in table 10.1 were extended to include the production vp     v,
enabling sentences with intransitive verb phrases, like we eat. then the cell t[1, 2]
    corresponding to the word eat     would    rst include the set {v}, and would be
augmented to the set {v, vp} during this second pass.

jacob eisenstein. draft of november 13, 2018.

10.2. ambiguity

229

figure 10.1: an example completed cky chart. the solid and dashed lines show the back
pointers resulting from the two different derivations of vp in position t[1, 5].

10.1.3 complexity

for an input of length m and a grammar with r productions and n non-terminals, the
space complexity of the cky algorithm is o(m 2n ): the number of cells in the chart is
o(m 2), and each cell must hold o(n ) elements. the time complexity is o(m 3r): each
cell is computed by searching over o(m ) split points, with r possible productions for
each split point. both the time and space complexity are considerably worse than the
viterbi algorithm, which is linear in the length of the input.

10.2 ambiguity

in natural language, there is rarely a single parse for a given sentence. the main culprit is
ambiguity, which is endemic to natural language syntax. here are a few broad categories:

    attachment ambiguity: e.g., we eat sushi with chopsticks, i shot an elephant in my
pajamas. in these examples, the prepositions (with, in) can attach to either the verb
or the direct object.

    modi   er scope: e.g., southern food store, plastic cup holder. in these examples, the    rst

word could be modifying the subsequent adjective, or the    nal noun.

    particle versus preposition: e.g., the puppy tore up the staircase. phrasal verbs like
tore up often include particles which could also act as prepositions. this has struc-
if up is a preposition, then up the staircase is a prepositional
tural implications:
phrase; if up is a particle, then the staircase is the direct object to the verb.

    complement structure: e.g., the students complained to the professor that they didn   t
understand. this is another form of attachment ambiguity, where the complement

under contract with mit press, shared under cc-by-nc-nd license.

weeatsushiwithchopstickswenp   s   seatvvp   vpsushinp   npwithpppchopsticksnp230

chapter 10. context-free parsing

that they didn   t understand could attach to the main verb (complained), or to the indi-
rect object (the professor).

    coordination scope: e.g.,    i see,    said the blind man, as he picked up the hammer and
saw. in this example, the lexical ambiguity for saw enables it to be coordinated either
with the noun hammer or the verb picked up.

these forms of ambiguity can combine, so that seemingly simple headlines like fed
raises interest rates have dozens of possible analyses even in a minimal grammar.
in a
broad coverage grammar, typical sentences can have millions of parses. while careful
grammar design can chip away at this ambiguity, a better strategy is combine broad cov-
erage parsers with data-driven strategies for identifying the correct analysis.

10.2.1 parser evaluation

before continuing to parsing algorithms that are able to handle ambiguity, let us stop
to consider how to measure parsing performance. suppose we have a set of reference
parses     the ground truth     and a set of system parses that we would like to score. a
simple solution would be per-sentence accuracy: the parser is scored by the proportion of
sentences on which the system and reference parses exactly match.2 but as any student
knows, it always nice to get partial credit, which we can assign to analyses that correctly
match parts of the reference parse. the parseval metrics (grishman et al., 1992) score
each system parse via:

precision: the fraction of constituents in the system parse that match a constituent in the

reference parse.

recall: the fraction of constituents in the reference parse that match a constituent in the

system parse.

in labeled precision and recall, the system must also match the phrase type for each
constituent; in unlabeled precision and recall, it is only required to match the constituent
structure. as described in chapter 4, the precision and recall can be combined into an
f -measure by their harmonic mean.

suppose that the left tree of figure 10.2 is the system parse, and that the right tree is

the reference parse. then:

    s     w1:5 is true positive, because it appears in both trees.
2most parsing papers do not report results on this metric, but suzuki et al. (2018)    nd that a strong parser
recovers the exact parse in roughly 50% of all sentences. performance on short sentences is generally much
higher.

jacob eisenstein. draft of november 13, 2018.

10.2. ambiguity

231

(a) system output

(b) reference

figure 10.2: two possible analyses from the grammar in table 10.1

    vp     w2:5 is true positive as well.
    np     w3:5 is false positive, because it appears only in the system output.
    pp     w4:5 is true positive, because it appears in both trees.
    vp     w2:3 is false negative, because it appears only in the reference.

4 = 0.75, for
the labeled and unlabeled precision of this parse is 3
an f-measure of 0.75. for an example in which precision and recall are not equal, suppose
the reference parse instead included the production vp     v np pp. in this parse, the
reference does not contain the constituent w2:3, so the recall would be 1.3

4 = 0.75, and the recall is 3

10.2.2 local solutions
some ambiguity can be resolved locally. consider the following examples,

(10.1)

a. we met the president on monday.
b. we met the president of mexico.

each case ends with a prepositional phrase, which can be attached to the verb met or the
noun phrase the president. if given a labeled corpus, we can compare the likelihood of the
observing the preposition alongside each candidate attachment point,

p(on | met)     p(on | president)
p(of | met)     p(of | president).

[10.1]
[10.2]

3while the grammar must be binarized before applying the cky algorithm, evaluation is performed on
the original parses. it is therefore necessary to    unbinarize    the output of a cky-based parser, converting it
back to the original grammar.

under contract with mit press, shared under cc-by-nc-nd license.

svpnpppnpchopsticksinwithnpsushiveatnpwesvpppnpchopsticksinwithvpnpsushiveatnpwe232

chapter 10. context-free parsing

a comparison of these probabilities would successfully resolve this case (hindle and
rooth, 1993). other cases, such as the example we eat sushi with chopsticks, require con-
sidering the object of the preposition: consider the alternative we eat sushi with soy sauce.
with suf   cient labeled data, some instances of attachment ambiguity can be solved by
supervised classi   cation (ratnaparkhi et al., 1994).

however, there are inherent limitations to local solutions. while toy examples may
have just a few ambiguities to resolve, realistic sentences have thousands or millions of
possible parses. furthermore, attachment decisions are interdependent, as shown in the
garden path example:

(10.2) cats scratch people with claws with knives.

we may want to attach with claws to scratch, as would be correct in the shorter sentence
in cats scratch people with claws. but this leaves nowhere to attach with knives. the cor-
rect interpretation can be identi   ed only be considering the attachment decisions jointly.
the huge number of potential parses may seem to make exhaustive search impossible.
but as with sequence labeling, locality assumptions make it possible to search this space
ef   ciently.

10.3 weighted context-free grammars

let us de   ne a derivation    as a set of anchored productions,

   = {x       , (i, j, k)},

[10.3]

with x corresponding to the left-hand side non-terminal and    corresponding to the right-
hand side. for grammars in chomsky normal formal,    is either a pair of non-terminals or
a terminal symbol. the indices i, j, k anchor the production in the input, with x deriving
the span wi+1:j. for binary productions, wi+1:k indicates the span of the left child, and
wk+1:j indicates the span of the right child; for unary productions, k is ignored. for an
input w, the optimal parse is,

     = argmax
     t (w)

  (   ),

where t (w) is the set of derivations that yield the input w.

de   ne a scoring function    that decomposes across anchored productions,

  (   ) = (cid:88)(x     ,(i,j,k))     

  (x       , (i, j, k)).

[10.4]

[10.5]

this is a locality assumption, akin to the assumption in viterbi sequence labeling. in this
case, the assumption states that the overall score is a sum over scores of productions,

jacob eisenstein. draft of november 13, 2018.

10.3. weighted context-free grammars

233

  (  )
    np vp
s
0
np     np pp
   1
    we
   2
    sushi
   3
    chopsticks    3
pp     in np
0
in     with
vp     v np
    vp pp
    md v

0
   1
   2
   2
0

v     eat

exp   (  )
1
1
2
1
4
1
8
1
8
1

1
1
2
1
4
1
4
1

table 10.2: an example weighted context-free grammar (wid18). the weights are chosen
so that exp   (  ) sums to one over right-hand sides for each non-terminal; this is required
by id140, but not by wid18s in general.

which are computed independently. in a weighted context-free grammar (wid18), the
score of each anchored production x     (  , (i, j, k)) is simply   (x       ), ignoring the
anchor (i, j, k). in other parsing models, the anchors can be used to access features of the
input, while still permitting ef   cient bottom-up parsing.

example consider the weighted grammar shown in table 10.2, and the analysis in fig-
ure 10.2b.

  (   ) =  (s     np vp) +   (vp     vp pp) +   (vp     v np) +   (pp     in np)

+   (np     we) +   (v     eat) +   (np     sushi) +   (in     with) +   (np     chopsticks)

[10.6]
[10.7]

=0     2     1 + 0     2 + 0     3 + 0     3 =    11.

in the alternative parse in figure 10.2a, the production vp     vp pp (with score    2) is
replaced with the production np     np pp (with score    1); all other productions are the
same. as a result, the score for this parse is    10. this example hints at a problem with
wid18 parsing on non-terminals such as np, vp, and pp: a wid18 will always prefer
either vp or np attachment, regardless of what is being attached! solutions to this issue
are discussed in    10.5.

under contract with mit press, shared under cc-by-nc-nd license.

234

chapter 10. context-free parsing

algorithm 14 cky algorithm for parsing a string w           in a weighted context-free
grammar (n,   , r, s), where n is the set of non-terminals and r is the set of weighted
productions. the grammar is assumed to be in chomsky normal form (   9.2.1). the
function traceback is de   ned in algorithm 13.

procedure wcky(w, g = (n,   , r, s))

(cid:46) initialization

for all i, j, x do
t[i, j, x]     0
b[i, j, x]        
for all x     n do

for m     {1, 2, . . . , m} do

for (cid:96)     {2, 3, . . . m} do

t[m, m + 1, x]       (x     wm, (m, m + 1, m))

for k     {m + 1, m + 2, . . . , m + (cid:96)     1} do

for m     {0, 1, . . . , m     (cid:96)} do
t[m, m + (cid:96), x]     max
b[m, m + (cid:96), x]     argmax

k,y,z

k,y,z

  (x     y z, (m, m + (cid:96), k)) + t[m, k, y ] + t[k, m + (cid:96), z]
  (x     y z, (m + (cid:96), k)) + t[m, k, y ] + t[k, m + (cid:96), z]

return traceback(s, 0, m, b)

10.3.1 parsing with weighted context-free grammars

the optimization problem in equation 10.4 can be solved by modifying the cky algo-
rithm. in the deterministic cky algorithm, each cell t[i, j] stored a set of non-terminals
capable of deriving the span wi+1:j. we now augment the table so that the cell t[i, j, x]
is the score of the best derivation of wi+1:j from non-terminal x. this score is computed
recursively: for the anchored binary production (x     y z, (i, j, k)), we compute:

    the score of the anchored production,   (x     y z, (i, j, k));
    the score of the best derivation of the left child, t[i, k, y ];
    the score of the best derivation of the right child, t[k, j, z].

these scores are combined by addition. as in the unweighted cky algorithm, the table
is constructed by considering spans of increasing length, so the scores for spans t[i, k, y ]
and t[k, j, z] are guaranteed to be available at the time we compute the score t[i, j, x]. the
value t[0, m, s] is the score of the best derivation of w from the grammar. algorithm 14
formalizes this procedure.

as in unweighted cky, the parse is recovered from the table of back pointers b, where
each b[i, j, x] stores the argmax split point k and production x     y z in the derivation of
wi+1:j from x. the top scoring parse can be obtained by tracing these pointers backwards
from b[0, m, s], all the way to the terminal symbols. this is analogous to the computation

jacob eisenstein. draft of november 13, 2018.

10.3. weighted context-free grammars

235

of the best sequence of labels in the viterbi algorithm by tracing pointers backwards from
the end of the trellis. note that we need only store back-pointers for the best path to
t[i, j, x]; this follows from the locality assumption that the global score for a parse is a
combination of the local scores of each production in the parse.

example let   s revisit the parsing table in figure 10.1.
in a weighted id18, each cell
would include a score for each non-terminal; non-terminals that cannot be generated are
assumed to have a score of       . the    rst diagonal contains the scores of unary produc-
tions: t[0, 1, np] =    2, t[1, 2, v] = 0, and so on. the next diagonal contains the scores for
spans of length 2: t[1, 3, vp] =    1 + 0     3 =    4, t[3, 5, pp] = 0 + 0     3 =    3, and so on.
things get interesting when we reach the cell t[1, 5, vp], which contains the score for the
derivation of the span w2:5 from the non-terminal vp. this score is computed as a max
over two alternatives,

t[1, 5, vp] = max(  (vp     vp pp, (1, 3, 5)) + t[1, 3, vp] + t[3, 5, pp],
  (vp     v np, (1, 2, 5)) + t[1, 2, v] + t[2, 5, np])

= max(     2     4     3,    1 + 0     7) =    8.

[10.8]
[10.9]

since the second case is the argmax, we set the back-pointer b[1, 5, vp] = (v, np, 2), en-
abling the optimal derivation to be recovered.

10.3.2 id140

id140 (pid18s) are a special case of weighted context-
free grammars that arises when the weights correspond to probabilities. speci   cally, the
weight   (x       , (i, j, k)) = log p(   | x), where the id203 of the right-hand side    is
conditioned on the non-terminal x, and the anchor (i, j, k) is ignored. these probabilities
must be normalized over all possible right-hand sides, so that(cid:80)   p(   | x) = 1, for all x.
for a given parse   , the product of the probabilities of the productions is equal to p(   ),
under the generative model        drawsubtree(s), where the function drawsubtree
is de   ned in algorithm 15.

the id155 of a parse given a string is,

p(   | w) =

p(   )

(cid:80)  (cid:48)   t (w) p(  (cid:48))

=

exp   (   )

(cid:80)  (cid:48)   t (w) exp   (  (cid:48))

,

[10.10]

where   (   ) = (cid:80)x     ,(i,j,k)        (x       , (i, j, k)). because the id203 is monotonic
in the score   (   ), the maximum likelihood parse can be identi   ed by the cky algorithm
without modi   cation. if a normalized id203 p(   | w) is required, the denominator
of equation 10.10 can be computed by the inside recurrence, described below.

under contract with mit press, shared under cc-by-nc-nd license.

236

chapter 10. context-free parsing

algorithm 15 generative model for derivations from id140
in chomsky normal form (cnf).

procedure drawsubtree(x)
sample (x       )     p(   | x)
if    = (y z) then

else

return drawsubtree(y )     drawsubtree(z)
return (x       )

(cid:46) in cnf, all unary productions yield terminal symbols

example the wid18 in table 10.2 is designed so that the weights are log-probabilities,

satisfying the constraint(cid:80)   exp   (x       ) = 1. as noted earlier, there are two parses in
t (we eat sushi with chopsticks), with scores   (  1) = log p(  1) =    10 and   (  2) = log p(  2) =
   11. therefore, the id155 p(  1 | w) is equal to,

p(  1 | w) =

p(  1) + p(  2)

p(  1)

=

exp   (  1)

exp   (  1) + exp   (  2)

=

2   10 + 2   11 =

2
3

.

[10.11]

2   10

the inside recurrence the denominator of equation 10.10 can be viewed as a language
model, summing over all valid derivations of the string w,

p(w) = (cid:88)  (cid:48):yield(  (cid:48))=w

(cid:48)

).

p(  

[10.12]

just as the cky algorithm makes it possible to maximize over all such analyses, with
a few modi   cations it can also compute their sum. each cell t[i, j, x] must store the log
id203 of deriving wi+1:j from non-terminal x. to compute this, we replace the max-
imization over split points k and productions x     y z with a    log-sum-exp    operation,
which exponentiates the log probabilities of the production and the children, sums them
in id203 space, and then converts back to the log domain:

exp (  (x     y z) + t[i, k, y ] + t[k, j, z])

[10.13]

exp (log p(y z | x) + log p(y     wi+1:k) + log p(z     wk+1:j))

p(y z | x)    p(y     wi+1:k)    p(z     wk+1:j)

p(y z, wi+1:k, wk+1:j | x)

= log p(x (cid:32) wi+1:j),

[10.14]
[10.15]

[10.16]

[10.17]

jacob eisenstein. draft of november 13, 2018.

t[i, j, x] = log (cid:88)k,y,z
= log (cid:88)k,y,z
= log (cid:88)k,y,z
= log (cid:88)k,y,z

10.3. weighted context-free grammars

237

with x (cid:32) wi+1:j indicating the event that non-terminal x yields the span wi+1, wi+2, . . . , wj.
the recursive computation of t[i, j, x] is called the inside recurrence, because it computes
the id203 of each subtree as a combination of the probabilities of the smaller subtrees
that are inside of it. the name implies a corresponding outside recurrence, which com-
putes the id203 of a non-terminal x spanning wi+1:j, joint with the outside context
(w1:i, wj+1:m ). this recurrence is described in    10.4.3. the inside and outside recurrences
are analogous to the forward and backward recurrences in probabilistic sequence label-
ing (see    7.5.3). they can be used to compute the marginal probabilities of individual
anchored productions, p(x       , (i, j, k) | w), summing over all possible derivations of
w.

10.3.3

*semiring weighted context-free grammars

the weighted and unweighted cky algorithms can be uni   ed with the inside recurrence
using the same semiring notation described in    7.7.3. the generalized recurrence is:

t[i, j, x] = (cid:77)k,y,z

  (x     y z, (i, j, k))     t[i, k, y ]     t[k, j, z].

[10.18]

this recurrence subsumes all of the algorithms that have been discussed in this chapter to
this point.

unweighted cky. when   (x       , (i, j, k)) is a boolean truth value {(cid:62),   },     is logical
conjunction, and(cid:76) is logical disjunction, then we derive cky recurrence for un-
weighted context-free grammars, discussed in    10.1 and algorithm 13.
weighted cky. when   (x       , (i, j, k)) is a scalar score,     is addition, and(cid:76) is maxi-
mization, then we derive the cky recurrence for weighted context-free grammars,
discussed in    10.3 and algorithm 14. when   (x       , (i, j, k)) = log p(   | x),
this same setting derives the cky recurrence for    nding the maximum likelihood
derivation in a probabilistic context-free grammar.

inside recurrence. when   (x       , (i, j, k)) is a log id203,     is addition, and(cid:76) =
log(cid:80) exp, then we derive the inside recurrence for probabilistic context-free gram-
mars, discussed in    10.3.2. it is also possible to set   (x       , (i, j, k)) directly equal
to the id203 p(   | x). in this case,     is multiplication, and(cid:76) is addition.
while this may seem more intuitive than working with log probabilities, there is the
risk of under   ow on long inputs.

regardless of how the scores are combined, the key point is the locality assumption:
the score for a derivation is the combination of the independent scores for each anchored

under contract with mit press, shared under cc-by-nc-nd license.

238

chapter 10. context-free parsing

production, and these scores do not depend on any other part of the derivation. for exam-
ple, if two non-terminals are siblings, the scores of productions from these non-terminals
are computed independently. this locality assumption is analogous to the    rst-order
markov assumption in sequence labeling, where the score for transitions between tags
depends only on the previous tag and current tag, and not on the history. as with se-
quence labeling, this assumption makes it possible to    nd the optimal parse ef   ciently; its
linguistic limitations are discussed in    10.5.

10.4 learning weighted context-free grammars

like sequence labeling, context-free parsing is a form of structure prediction. as a result,
wid18s can be learned using the same set of algorithms: generative probabilistic models,
structured id88, maximum conditional likelihood, and maximum margin learning.
in all cases, learning requires a treebank, which is a dataset of sentences labeled with
context-free parses. parsing research was catalyzed by the id32 (marcus et al.,
1993), the    rst large-scale dataset of this type (see    9.2.2). phrase structure treebanks exist
for roughly two dozen other languages, with coverage mainly restricted to european and
east asian languages, plus arabic and urdu.

10.4.1 id140

id140 are similar to id48, in that they are
generative models of text. in this case, the parameters of interest correspond to probabil-
ities of productions, conditional on the left-hand side. as with id48,
these parameters can be estimated by relative frequency:

  (x       ) = log p(x       )
count(x       )
  p(x       ) =
count(x)

.

[10.19]

[10.20]

for example, the id203 of the production np     det nn is the corpus count of
this production, divided by the count of the non-terminal np. this estimator applies
to terminal productions as well: the id203 of nn     whale is the count of how often
whale appears in the corpus as generated from an nn tag, divided by the total count of the
nn tag. even with the largest treebanks     currently on the order of one million tokens
    it is dif   cult to accurately compute probabilities of even moderately rare events, such
as nn     whale. therefore, smoothing is critical for making pid18s effective.

jacob eisenstein. draft of november 13, 2018.

10.4. learning weighted context-free grammars

239

10.4.2 feature-based parsing

the scores for each production can be computed as an inner product of weights and fea-
tures,

  (x       , (i, j, k)) =       f (x,   , (i, j, k), w),

[10.21]

where the feature vector f is a function of the left-hand side x, the right-hand side   , the
anchor indices (i, j, k), and the input w.

the basic feature f (x,   , (i, j, k)) = {(x,   )} encodes only the identity of the produc-
tion itself. this gives rise to a discriminatively-trained model with the same expressive-
ness as a pid18. features on anchored productions can include the words that border the
span wi, wj+1, the word at the split point wk+1, the presence of a verb or noun in the left
child span wi+1:k, and so on (durrett and klein, 2015). scores on anchored productions
can be incorporated into cky parsing without any modi   cation to the algorithm, because
it is still possible to compute each element of the table t[i, j, x] recursively from its imme-
diate children.

other features can be obtained by grouping elements on either the left-hand or right-
hand side: for example it can be particularly bene   cial to compute additional features
by id91 terminal symbols, with features corresponding to groups of words with
similar syntactic properties. the id91 can be obtained from unlabeled datasets that
are much larger than any treebank, improving coverage. such methods are described in
chapter 14.

feature-based parsing models can be estimated using the usual array of discrimina-
tive learning techniques. for example, a structure id88 update can be computed
as (carreras et al., 2008),

f (  , w(i)) = (cid:88)(x     ,(i,j,k))     

f (x,   , (i, j, k), w(i))

     = argmax
     t (w)

      f (  , w(i))

      f (   (i), w(i))     f (     , w(i)).

[10.22]

[10.23]

[10.24]

a margin-based objective can be optimized by selecting      through cost-augmented decod-
ing (   2.4.2), enforcing a margin of    (     ,    ) between the hypothesis and the reference parse,
where     is a non-negative cost function, such as the hamming loss (stern et al., 2017). it
is also possible to train feature-based parsing models by conditional log-likelihood, as
described in the next section.

under contract with mit press, shared under cc-by-nc-nd license.

240

chapter 10. context-free parsing

figure 10.3: the two cases faced by the outside recurrence in the computation of   (i, j, x)

*conditional random    eld parsing

10.4.3
the score of a derivation   (   ) can be converted into a id203 by normalizing over all
possible derivations,

p(   | w) =

exp   (   )

(cid:80)  (cid:48)   t (w) exp   (  (cid:48))

.

[10.25]

using this id203, a wid18 can be trained by maximizing the conditional log-likelihood
of a labeled corpus.

just as in id28 and the conditional random    eld over sequences, the
gradient of the conditional log-likelihood is the difference between the observed and ex-
pected counts of each feature. the expectation e  |w[f (  , w(i));   ] requires summing over
all possible parses, and computing the marginal probabilities of anchored productions,
p(x       , (i, j, k) | w). in crf sequence labeling, marginal probabilities over tag bigrams
are computed by the two-pass forward-backward algorithm (   7.5.3). the analogue for
context-free grammars is the inside-outside algorithm, in which marginal probabilities
are computed from terms generated by an upward and downward pass over the parsing
chart:

    the upward pass is performed by the inside recurrence, which is described in    10.3.2.
each inside variable   (i, j, x) is the score of deriving wi+1:j from the non-terminal
in a pid18, this corresponds to the log-id203 log p(wi+1:j | x). this is
x.
computed by the recurrence,
  (i, j, x) (cid:44) log (cid:88)(x   y z)
exp (  (x     y z, (i, j, k)) +   (i, k, y ) +   (k, j, z)) .
[10.26]
the initial condition of this recurrence is   (m     1, m, x) =   (x     wm). the de-
nominator(cid:80)     t (w) exp   (   ) is equal to exp   (0, m, s).
    the downward pass is performed by the outside recurrence, which recursively pop-
ulates the same table structure, starting at the root of the tree. each outside variable

j(cid:88)k=i+1

jacob eisenstein. draft of november 13, 2018.

yzwj+1...wkxwi+1...wjyxwi+1...wjzwk+1...wi10.4. learning weighted context-free grammars

241

  (i, j, x) is the score of having a phrase of type x covering the span (i + 1 : j), joint
with the exterior context w1:i and wj+1:m . in a pid18, this corresponds to the log
id203 log p((x, i + 1, j), w1:i, wj+1:m ). each outside variable is computed by
the recurrence,

m(cid:88)k=j+1
exp   (i, j, x) (cid:44) (cid:88)(y    x z)
i   1(cid:88)k=0
+ (cid:88)(y    z x)

exp [  (y     x z, (i, k, j)) +   (j, k, z) +   (i, k, y )]

[10.27]

exp [  (y     z x, (k, i, j)) +   (k, i, z) +   (k, j, y )] .
[10.28]

the    rst line of equation 10.28 is the score under the condition that x is a left child
of its parent, which spans wi+1:k, with k > j; the second line is the score under the
condition that x is a right child of its parent y , which spans wk+1:j, with k < i.
the two cases are shown in figure 10.3.
in each case, we sum over all possible
productions with x on the right-hand side. the parent y is bounded on one side
by either i or j, depending on whether x is a left or right child of y ; we must sum
over all possible values for the other boundary. the initial conditions for the outside
recurrence are   (0, m, s) = 0 and   (0, m, x (cid:54)= s) =       .

the marginal id203 of a non-terminal x over span wi+1:j is written p(x (cid:32) wi+1:j | w).

this id203 can be computed from the inside and outside scores,

p(x (cid:32) wi+1:j | w) =

p(x (cid:32) wi+1:j, w)

p(w)

=

=

p(wi+1:j | x)    p(x, w1:i, xj+1:m )

p(w)

exp (  (i, j, x) +   (i, j, x))

exp   (0, m, s)

.

[10.29]

[10.30]

[10.31]

marginal probabilities of individual productions can be computed similarly (see exercise
2). these marginal probabilities can be used for training a conditional random    eld parser,
and also for the task of unsupervised grammar induction, in which a pid18 is estimated
from a dataset of unlabeled text (lari and young, 1990; pereira and schabes, 1992).

under contract with mit press, shared under cc-by-nc-nd license.

242

chapter 10. context-free parsing

10.4.4 neural context-free grammars
neural networks and can be applied to parsing by representing each span with a dense
numerical vector (socher et al., 2013; durrett and klein, 2015; cross and huang, 2016).4
for example, the anchor (i, j, k) and sentence w can be associated with a    xed-length
column vector,

v(i,j,k) = [uwi   1; uwi; uwj   1; uwj ; uwk   1; uwk ],

[10.32]
where uwi is a id27 associated with the word wi. the vector vi,j,k can then be
passed through a feedforward neural network, and used to compute the score of the an-
chored production. for example, this score can be computed as a bilinear product (durrett
and klein, 2015),

  v(i,j,k) =feedforward(v(i,j,k))

  (x       , (i, j, k)) =  v

(cid:62)
(i,j,k)  f (x       ),

[10.33]
[10.34]

where f (x       ) is a vector of features of the production, and    is a parameter ma-
trix. the matrix    and the parameters of the feedforward network can be learned by
backpropagating from an objective such as the margin loss or the negative conditional
log-likelihood.

10.5 grammar re   nement

the locality assumptions underlying id18 parsing depend on the granularity of the non-
terminals. for the id32 non-terminals, there are several reasons to believe that
these assumptions are too strong (johnson, 1998):

    the context-free assumption is too strict: for example, the id203 of the produc-
tion np     np pp is much higher (in the ptb) if the parent of the noun phrase is a
verb phrase (indicating that the np is a direct object) than if the parent is a sentence
(indicating that the np is the subject of the sentence).

    the id32 non-terminals are too coarse: there are many kinds of noun
phrases and verb phrases, and accurate parsing sometimes requires knowing the
difference. as we have already seen, when faced with prepositional phrase at-
tachment ambiguity, a weighted id18 will either always choose np attachment (if
  (np     np pp) >   (vp     vp pp)), or it will always choose vp attachment. to
get more nuanced behavior, more    ne-grained non-terminals are needed.
    more generally, accurate parsing requires some amount of semantics     understand-
ing the meaning of the text to be parsed. consider the example cats scratch people
4earlier work on neural constituent parsing used transition-based parsing algorithms (   10.6.2) rather

than cky-style chart parsing (henderson, 2004; titov and henderson, 2007).

jacob eisenstein. draft of november 13, 2018.

10.5. grammar refinement

243

figure 10.4: the left parse is preferable because of the conjunction of phrases headed by
france and italy, but these parses cannot be distinguished by a wid18.

with claws: knowledge of about cats, claws, and scratching is necessary to correctly
resolve the attachment ambiguity.

an extreme example is shown in figure 10.4. the analysis on the left is preferred
because of the conjunction of similar entities france and italy. but given the non-terminals
shown in the analyses, there is no way to differentiate these two parses, since they include
exactly the same productions. what is needed seems to be more precise non-terminals.
one possibility would be to rethink the linguistics behind the id32, and ask
the annotators to try again. but the original annotation effort took    ve years, and there
is a little appetite for another annotation effort of this scope. researchers have therefore
turned to automated techniques.

10.5.1 parent annotations and other tree transformations

the key assumption underlying context-free parsing is that productions depend only on
the identity of the non-terminal on the left-hand side, and not on its ancestors or neigh-
bors. the validity of this assumption is an empirical question, and it depends on the
non-terminals themselves: ideally, every noun phrase (and verb phrase, etc) would be
distributionally identical, so the assumption would hold. but in the id32, the
observed id203 of productions often depends on the parent of the left-hand side.
for example, noun phrases are more likely to be modi   ed by prepositional phrases when
they are in the object position (e.g., they amused the students from georgia) than in the subject
position (e.g., the students from georgia amused them). this means that the np     np pp
production is more likely if the entire constituent is the child of a vp than if it is the child

under contract with mit press, shared under cc-by-nc-nd license.

svpnpppnpnpnnpitalyccandnpnnpfrancepfromnnwinevlikesnpprpshesvpnpnpnnpitalyccandnpppnpnnpfrancepfromnnwinevlikesnpprpshe244

chapter 10. context-free parsing

figure 10.5: parent annotation in a id18 derivation

of s. the observed statistics are (johnson, 1998):

pr(np     np pp) =11%
pr(np under s     np pp) =9%
pr(np under vp     np pp) =23%.

[10.35]
[10.36]
[10.37]
this phenomenon can be captured by parent annotation (johnson, 1998), in which each
non-terminal is augmented with the identity of its parent, as shown in figure 10.5). this is
sometimes called vertical markovization, since a markov dependency is introduced be-
tween each node and its parent (klein and manning, 2003). it is analogous to moving from
a bigram to a trigram context in a hidden markov model. in principle, parent annotation
squares the size of the set of non-terminals, which could make parsing considerably less
ef   cient. but in practice, the increase in the number of non-terminals that actually appear
in the data is relatively modest (johnson, 1998).

parent annotation weakens the wid18 locality assumptions. this improves accuracy
by enabling the parser to make more    ne-grained distinctions, which better capture real
linguistic phenomena. however, each production is more rare, and so careful smoothing
or id173 is required to control the variance over production scores.

10.5.2 lexicalized context-free grammars
the examples in    10.2.2 demonstrate the importance of individual words in resolving
parsing ambiguity: the preposition on is more likely to attach to met, while the preposition
of is more likely to attachment to president. but of all word pairs, which are relevant to
attachment decisions? consider the following variants on the original examples:

(10.3)

a. we met the president of mexico.
b. we met the    rst female president of mexico.
c. they had supposedly met the president on monday.

the underlined words are the head words of their respective phrases: met heads the verb
phrase, and president heads the direct object noun phrase. these heads provide useful

jacob eisenstein. draft of november 13, 2018.

svpnpnnbeardtthevheardnpshesvp-snp-vpnn-npbeardt-npthevp-vpheardnp-sshe10.5. grammar refinement

245

(a) lexicalization and attachment ambiguity

(b) lexicalization and coordination scope ambiguity

figure 10.6: examples of lexicalization

semantic information. but they break the context-free assumption, which states that the
score for a production depends only on the parent and its immediate children, and not
the substructure under each child.

the incorporation of head words into context-free parsing is known as lexicalization,

and is implemented in rules of the form,

np(president)    np(president) pp(of)
np(president)    np(president) pp(on).

[10.38]
[10.39]

lexicalization was a major step towards accurate pid18 parsing in the 1990s and early
2000s. it requires solving three problems: identifying the heads of all constituents in a
treebank; parsing ef   ciently while keeping track of the heads; and estimating the scores
for lexicalized productions.

under contract with mit press, shared under cc-by-nc-nd license.

vp(meet)pp(on)npnnmondayponnp(president)nnpresidentdtthevbmeetvp(meet)np(president)pp(of)npnnmexicopofnp(president)nnpresidentdtthevbmeetnp(italy)np(italy)nnsitalyccandnp(wine)pp(from)np(france)nnpfranceinfromnp(wine)nnwinenp(wine)pp(from)np(italy)np(italy)nnsitalyccandnp(france)nnpfranceinfromnp(wine)nnwine246

chapter 10. context-free parsing

non-terminal direction priority
s
vp
np
pp

right
left
right
left

vp sbar adjp ucp np
vbd vbn md vbz to vb vp vbg vbp adjp np
n* ex $ cd qp prp . . .
in to fw

table 10.3: a fragment of head percolation rules for english (magerman, 1995; collins,
1997)

identifying head words

the head of a constituent is the word that is the most useful for determining how that
constituent is integrated into the rest of the sentence.5 the head word of a constituent is
determined recursively: for any non-terminal production, the head of the left-hand side
must be the head of one of the children. the head is typically selected according to a set of
deterministic rules, sometimes called head percolation rules. in many cases, these rules
are straightforward: the head of a noun phrase in a np     det nn production is the head
of the noun; the head of a sentence in a s     np vp production is the head of the verb
phrase.

table 10.3 shows a fragment of the head percolation rules used in many english pars-
ing systems. the meaning of the    rst rule is that to    nd the head of an s constituent,    rst
look for the rightmost vp child; if you don   t    nd one, then look for the rightmost sbar
child, and so on down the list. verb phrases are headed by left verbs (the head of can plan
on walking is planned, since the modal verb can is tagged md); noun phrases are headed by
the rightmost noun-like non-terminal (so the head of the red cat is cat),6 and prepositional
phrases are headed by the preposition (the head of at georgia tech is at). some of these
rules are somewhat arbitrary     there   s no particular reason why the head of cats and dogs
should be dogs     but the point here is just to get some lexical information that can support
parsing, not to make deep claims about syntax. figure 10.6 shows the application of these
rules to two of the running examples.

parsing lexicalized context-free grammars

a na    ve application of lexicalization would simply increase the set of non-terminals by
taking the cross-product with the set of terminal symbols, so that the non-terminals now

5this is a pragmatic de   nition, be   tting our goal of using head words to improve parsing; for a more

formal de   nition, see (bender, 2013, chapter 7).

6the noun phrase non-terminal is sometimes treated as a special case. collins (1997) uses a heuristic that
looks for the rightmost child which is a noun-like part-of-speech (e.g., nn, nnp), a possessive marker, or a
superlative adjective (e.g., the greatest). if no such child is found, the heuristic then looks for the leftmost np.
if there is no child with tag np, the heuristic then applies another priority list, this time from right to left.

jacob eisenstein. draft of november 13, 2018.

10.5. grammar refinement

247

include symbols like np(president) and vp(meet). under this approach, the cky parsing
algorithm could be applied directly to the lexicalized production rules. however, the
complexity would be cubic in the size of the vocabulary of terminal symbols, which would
clearly be intractable.

another approach is to augment the cky table with an additional index, keeping track
of the head of each constituent. the cell t[i, j, h, x] stores the score of the best derivation in
which non-terminal x spans wi+1:j with head word h, where i < h     j. to compute such
a table recursively, we must consider the possibility that each phrase gets its head from
either its left or right child. the scores of the best derivations in which the head comes
from the left and right child are denoted t(cid:96) and tr respectively, leading to the following
recurrence:

t(cid:96)[i, j, h, x] = max

(x   y z)

max
k>h

max
k<h(cid:48)   j

t[i, k, h, y ] + t[k, j, h

(cid:48)

(cid:48)
, z] +   (x(h)     y (h)z(h

))

tr[i, j, h, x] = max

(x   y z)

max
k<h

max
i<h(cid:48)   k

(cid:48)
t[i, k, h

(cid:48)
, y ] + t[k, j, h, z] + (  (x(h)     y (h

t[i, j, h, x] = max (t(cid:96)[i, j, h, x], tr[i, j, h, x]) .

[10.40]

)z(h)))

[10.41]
[10.42]

to compute t(cid:96), we maximize over all split points k > h, since the head word must be in
the left child. we then maximize again over possible head words h(cid:48) for the right child. an
analogous computation is performed for tr. the size of the table is now o(m 3n ), where
m is the length of the input and n is the number of non-terminals. furthermore, each
cell is computed by performing o(m 2) operations, since we maximize over both the split
point k and the head h(cid:48). the time complexity of the algorithm is therefore o(rm 5n ),
where r is the number of rules in the grammar. fortunately, more ef   cient solutions are
possible. in general, the complexity of parsing can be reduced to o(m 4) in the length of
the input; for a broad class of lexicalized id18s, the complexity can be made cubic in the
length of the input, just as in unlexicalized id18s (eisner, 2000).

estimating lexicalized context-free grammars

the    nal problem for lexicalized parsing is how to estimate weights for lexicalized pro-
ductions x(i)     y (j) z(k). these productions are said to be bilexical, because they
involve scores over pairs of words: in the example meet the president of mexico, we hope
to choose the correct attachment point by modeling the bilexical af   nities of (meet, of) and
(president, of). the number of such word pairs is quadratic in the size of the vocabulary,
making it dif   cult to estimate the weights of lexicalized production rules directly from
data. this is especially true for id140, in which the weights
are obtained from smoothed relative frequency. in a treebank with a million tokens, a

under contract with mit press, shared under cc-by-nc-nd license.

248

chapter 10. context-free parsing

vanishingly small fraction of the possible lexicalized productions will be observed more
than once.7 the charniak (1997) and collins (1997) parsers therefore focus on approxi-
mating the probabilities of lexicalized productions, using various smoothing techniques
and independence assumptions.

in discriminatively-trained weighted context-free grammars, the scores for each pro-
duction can be computed from a set of features, which can be made progressively more
   ne-grained (finkel et al., 2008). for example, the score of the lexicalized production
np(president)     np(president) pp(of) can be computed from the following features:
f (np(president)     np(president) pp(of)) = {np(*)     np(*) pp(*),

np(president)     np(president) pp(*),
np(*)     np(*) pp(of),
np(president)     np(president) pp(of)}
the    rst feature scores the unlexicalized production np     np pp; the next two features
lexicalize only one element of the production, thereby scoring the appropriateness of np
attachment for the individual words president and of ; the    nal feature scores the speci   c
bilexical af   nity of president and of. for bilexical pairs that are encountered frequently in
the treebank, this bilexical feature can play an important role in parsing; for pairs that are
absent or rare, id173 will drive its weight to zero, forcing the parser to rely on the
more coarse-grained features.

in chapter 14, we will encounter techniques for id91 words based on their distri-
butional properties     the contexts in which they appear. such a id91 would group
rare and common words, such as whale, shark, beluga, leviathan. word clusters can be used
as features in discriminative lexicalized parsing, striking a middle ground between full
lexicalization and non-terminals (finkel et al., 2008). in this way, labeled examples con-
taining relatively common words like whale can help to improve parsing for rare words
like beluga, as long as those two words are clustered together.

10.5.3

*re   nement grammars

lexicalization improves on context-free parsing by adding detailed information in the
form of lexical heads. however, estimating the scores of lexicalized productions is dif-
   cult. klein and manning (2003) argue that the right level of linguistic detail is some-
where between treebank categories and individual words. some parts-of-speech and non-
terminals are truly substitutable: for example, cat/n and dog/n. but others are not: for
example, the preposition of exclusively attaches to nouns, while the preposition as is more

7the real situation is even more dif   cult, because non-binary context-free grammars can involve trilexical
or higher-order dependencies, between the head of the constituent and multiple of its children (carreras et al.,
2008).

jacob eisenstein. draft of november 13, 2018.

10.6. beyond context-free parsing

249

likely to modify verb phrases. klein and manning (2003) obtained a 2% improvement in
f -measure on a parent-annotated pid18 parser by making a single change: splitting the
preposition category into six subtypes. they propose a series of linguistically-motivated
re   nements to the id32 annotations, which in total yielded a 40% error reduc-
tion.

non-terminal re   nement process can be automated by treating the re   ned categories
as latent variables. for example, we might split the noun phrase non-terminal into
np1, np2, np3, . . . , without de   ning in advance what each re   ned non-terminal cor-
responds to. this can be treated as partially supervised learning, similar to the multi-
component document classi   cation model described in    5.2.3. a latent variable pid18
can be estimated by expectation maximization (matsuzaki et al., 2005):8

    in the e-step, estimate a marginal distribution q over the re   nement type of each
non-terminal in each derivation. these marginals are constrained by the original
annotation: an np can be reannotated as np4, but not as vp3. marginal probabil-
ities over re   ned productions can be computed from the inside-outside algorithm,
as described in    10.4.3, where the e-step enforces the constraints imposed by the
original annotations.
    in the m-step, recompute the parameters of the grammar, by summing over the

probabilities of anchored productions that were computed in the e-step:

e[count(x     y z)] =

m(cid:88)i=0

m(cid:88)j=i

j(cid:88)k=i

p(x     y z, (i, j, k) | w).

[10.43]

as usual, this process can be iterated to convergence. to determine the number of re-
   nement types for each tag, petrov et al. (2006) apply a split-merge heuristic; liang et al.
(2007) and finkel et al. (2007) apply bayesian nonparametrics (cohen, 2016).

some examples of re   ned non-terminals are shown in table 10.4. the proper nouns
differentiate months,    rst names, middle initials, last names,    rst names of places, and
second names of places; each of these will tend to appear in different parts of grammatical
productions. the personal pronouns differentiate grammatical role, with prp-0 appear-
ing in subject position at the beginning of the sentence (note the capitalization), prp-1
appearing in subject position but not at the beginning of the sentence, and prp-2 appear-
ing in object position.

10.6 beyond context-free parsing

in the context-free setting, the score for a parse is a combination of the scores of individual
productions. as we have seen, these models can be improved by using    ner-grained non-
8spectral learning, described in    5.5.2, has also been applied to re   nement grammars (cohen et al., 2014).

under contract with mit press, shared under cc-by-nc-nd license.

250

chapter 10. context-free parsing

proper nouns
nnp-14
nnp-12
nnp-2
nnp-1
nnp-15
nnp-3
personal pronouns
prp-0
prp-1
prp-2

e.

oct. nov.
john robert
j.
bush noriega
new san
york

sept.
james
l.
peters
wall
francisco street

it
it
it

he
he
them

i
they
him

table 10.4: examples of automatically re   ned non-terminals and some of the words that
they generate (petrov et al., 2006).

terminals, via parent-annotation, lexicalization, and automated re   nement. however, the
inherent limitations to the expressiveness of context-free parsing motivate the consider-
ation of other search strategies. these strategies abandon the optimality guaranteed by
bottom-up parsing, in exchange for the freedom to consider arbitrary properties of the
proposed parses.

10.6.1 reranking

a simple way to relax the restrictions of context-free parsing is to perform a two-stage pro-
cess, in which a context-free parser generates a k-best list of candidates, and a reranker
then selects the best parse from this list (charniak and johnson, 2005; collins and koo,
2005). the reranker can be trained from an objective that is similar to multi-class classi-
   cation: the goal is to learn weights that assign a high score to the reference parse, or to
the parse on the k-best list that has the lowest error. in either case, the reranker need only
evaluate the k best parses, and so no context-free assumptions are necessary. this opens
the door to more expressive scoring functions:

    it is possible to incorporate arbitrary non-local features, such as the structural par-
allelism and right-branching orientation of the parse (charniak and johnson, 2005).

    reranking enables the use of id56s, in which each constituent
span wi+1:j receives a vector ui,j which is computed from the vector representa-
tions of its children, using a composition function that is linked to the production

jacob eisenstein. draft of november 13, 2018.

10.6. beyond context-free parsing

rule (socher et al., 2013), e.g.,

ui,j = f(cid:18)  x   y z(cid:20) ui,k
uk,j (cid:21)(cid:19)

251

[10.44]

the overall score of the parse can then be computed from the    nal vector,   (   ) =
  u0,m .

reranking can yield substantial improvements in accuracy. the main limitation is that it
can only    nd the best parse among the k-best offered by the generator, so it is inherently
limited by the ability of the bottom-up parser to    nd high-quality candidates.

10.6.2 transition-based parsing
structure prediction can be viewed as a form of search. an alternative to bottom-up pars-
ing is to read the input from left-to-right, gradually building up a parse structure through
a series of transitions. transition-based parsing is described in more detail in the next
chapter, in the context of id33. however, it can also be applied to id18
parsing, as brie   y described here.

for any context-free grammar, there is an equivalent pushdown automaton, a model
of computation that accepts exactly those strings that can be derived from the grammar.
this computational model consumes the input from left to right, while pushing and pop-
ping elements on a stack. this architecture provides a natural transition-based parsing
framework for context-free grammars, known as id132.

id132 is a type of transition-based parsing, in which the parser can take

the following actions:

    shift the next terminal symbol onto the stack;
    unary-reduce the top item on the stack, using a unary production rule in the gram-

mar;

    binary-reduce the top two items onto the stack, using a binary production rule in the

grammar.

the set of available actions is constrained by the situation: the parser can only shift if
there are remaining terminal symbols in the input, and it can only reduce if an applicable
production rule exists in the grammar. if the parser arrives at a state where the input
has been completely consumed, and the stack contains only the element s, then the input
is accepted.
if the parser arrives at a non-accepting state where there are no possible
actions, the input is rejected. a parse error occurs if there is some action sequence that
would accept an input, but the parser does not    nd it.

under contract with mit press, shared under cc-by-nc-nd license.

252

chapter 10. context-free parsing

example consider the input we eat sushi and the grammar in table 10.1. the input can
be parsed through the following sequence of actions:

1. shift the    rst token we onto the stack.
2. reduce the top item on the stack to np, using the production np     we.
3. shift the next token eat onto the stack, and reduce it to v with the production v    
4. shift the    nal token sushi onto the stack, and reduce it to np. the input has been

eat.

completely consumed, and the stack contains [np, v, np].

5. reduce the top two items using the production vp     v np. the stack now con-

tains [vp, np].

6. reduce the top two items using the production s     np vp. the stack now contains

[s]. since the input is empty, this is an accepting state.

one thing to notice from this example is that the number of shift actions is equal to the
length of the input. the number of reduce actions is equal to the number of non-terminals
in the analysis, which grows linearly in the length of the input. thus, the overall time
complexity of id132 is linear in the length of the input (assuming the com-
plexity of each individual classi   cation decision is constant in the length of the input).
this is far better than the cubic time complexity required by cky parsing.

transition-based parsing as id136
in general, it is not possible to guarantee that
a transition-based parser will    nd the optimal parse, argmax     (   ; w), even under the
usual id18 independence assumptions. we could assign a score to each anchored parsing
action in each context, with   (a, c) indicating the score of performing action a in context c.
one might imagine that transition-based parsing could ef   ciently    nd the derivation that
maximizes the sum of such scores. but this too would require backtracking and searching
over an exponentially large number of possible action sequences:
if a bad decision is
made at the beginning of the derivation, then it may be impossible to recover the optimal
action sequence without backtracking to that early mistake. this is known as a search
error. transition-based parsers can incorporate arbitrary features, without the restrictive
independence assumptions required by chart parsing; search errors are the price that must
be paid for this    exibility.

learning transition-based parsing transition-based parsing can be combined with ma-
chine learning by training a classi   er to select the correct action in each situation. this
classi   er is free to choose any feature of the input, the state of the parser, and the parse
history. however, there is no optimality guarantee: the parser may choose a suboptimal
parse, due to a mistake at the beginning of the analysis. nonetheless, some of the strongest

jacob eisenstein. draft of november 13, 2018.

10.6. beyond context-free parsing

253

id18 parsers are based on the shift-reduce architecture, rather than cky. a recent gener-
ation of models links id132 with recurrent neural networks, updating a
hidden state vector while consuming the input (e.g., cross and huang, 2016; dyer et al.,
2016). learning algorithms for transition-based parsing are discussed in more detail in
   11.3.

exercises

1. design a grammar that handles english subject-verb agreement. speci   cally, your

grammar should handle the examples below correctly:

(10.4)

(10.5)

a. she sings.
b. we sing.

a. *she sing.
b. *we sings.

2. extend your grammar from the previous problem to include the auxiliary verb can,

so that the following cases are handled:

(10.6)

(10.7)

a. she can sing.
b. we can sing.

a. *she can sings.
b. *we can sings.

3. french requires subjects and verbs to agree in person and number, and it requires
determiners and nouns to agree in gender and number. verbs and their objects need
not agree. assuming that french has two genders (feminine and masculine), three
persons (   rst [me], second [you], third [her]), and two numbers (singular and plural),
how many productions are required to extend the following simple grammar to
handle agreement?

    np vp
s
vp     v | v np | v np np
np     det nn

4. consider the grammar:

under contract with mit press, shared under cc-by-nc-nd license.

254

chapter 10. context-free parsing

    np vp
s
vp     v np
np     jj np
np        sh (the animal)
v        sh (the action of    shing)
jj        sh (a modi   er, as in    sh sauce or    sh stew)
apply the cky algorithm and identify all possible parses for the sentence    sh    sh
   sh    sh.

5. choose one of the possible parses for the previous problem, and show how it can be

derived by a series of shift-reduce actions.

6. to handle vp coordination, a grammar includes the production vp     vp cc vp.
to handle adverbs, it also includes the production vp     vp adv. assume all verbs
are generated from a sequence of unary productions, e.g., vp     v     eat.
a) show how to binarize the production vp     vp cc vp.
b) use your binarized grammar to parse the sentence they eat and drink together,

treating together as an adverb.

c) prove that a weighted id18 cannot distinguish the two possible derivations of
this sentence. your explanation should focus on the productions in the original,
non-binary grammar.

d) explain what condition must hold for a parent-annotated wid18 to prefer the

derivation in which together modi   es the coordination eat and drink.

7. consider the following pid18:

p(x     x x) =
p(x     y ) =
p(y       ) =

1
2
1
2
1
|  |

,            

[10.45]

[10.46]

[10.47]

a) compute the id203 p(     ) of the maximum id203 parse for a string

w       m .

b) compute the id155 p(     | w).

8. context-free grammars can be used to parse the internal structure of words. us-
ing the weighted cky algorithm and the following weighted context-free grammar,
identify the best parse for the sequence of morphological segments in+   ame+able.

jacob eisenstein. draft of november 13, 2018.

10.6. beyond context-free parsing

255

s
    v
s
    n
s
    j
v
    vpref n
j
    n jsuff
j
    v jsuff
j
    negpref j
    in+
vpref
negpref     in+
       ame
n
    +able
jsuff

0
0
0
-1
1
0
1
2
1
0
0

9. use the inside and outside scores to compute the marginal id203 p(xi+1:j     yi+1:k zk+1:j | w),

indicating that y spans wi+1:k, z spans wk+1:j, and x is the parent of y and z, span-
ning wi+1:j.

for all x. verify that the semiring inside recurrence from equation 10.26 generates

10. suppose that the potentials   (x       ) are log-probabilities, so that(cid:80)   exp   (x       ) = 1
the log-id203 log p(w) = log(cid:80)   :yield(   )=w p(   ).

under contract with mit press, shared under cc-by-nc-nd license.

chapter 11

id33

the previous chapter discussed algorithms for analyzing sentences in terms of nested con-
stituents, such as noun phrases and verb phrases. however, many of the key sources of
ambiguity in phrase-structure analysis relate to questions of attachment: where to attach a
prepositional phrase or complement clause, how to scope a coordinating conjunction, and
so on. these attachment decisions can be represented with a more lightweight structure:
a directed graph over the words in the sentence, known as a dependency parse. syn-
tactic annotation has shifted its focus to such dependency structures: at the time of this
writing, the universal dependencies project offers more than 100 dependency treebanks
for more than 60 languages.1 this chapter will describe the linguistic ideas underlying
dependency grammar, and then discuss exact and transition-based parsing algorithms.
the chapter will also discuss recent research on learning to search in transition-based
structure prediction.

11.1 dependency grammar

while dependency grammar has a rich history of its own (tesni`ere, 1966; k   ubler et al.,
2009), it can be motivated by extension from the lexicalized context-free grammars that
we encountered in previous chapter (   10.5.2). recall that lexicalization augments each
non-terminal with a head word. the head of a constituent is identi   ed recursively, using
a set of head rules, as shown in table 10.3. an example of a lexicalized context-free parse
is shown in figure 11.1a. in this sentence, the head of the s constituent is the main verb,
scratch; this non-terminal then produces the noun phrase the cats, whose head word is
cats, and from which we    nally derive the word the. thus, the word scratch occupies the
central position for the sentence, with the word cats playing a supporting role. in turn, cats

1universaldependencies.org

257

258

chapter 11. id33

(a) lexicalized constituency parse

(b) unlabeled dependency tree

figure 11.1: dependency grammar is closely linked to lexicalized id18s:
each lexical head has a dependency path to every other word in the constituent. (this
example is based on the lexicalization rules from    10.5.2, which make the preposition
the head of a prepositional phrase. in the more contemporary universal dependencies
annotations, the head of with claws would be claws, so there would be an edge scratch    
claws.)

occupies the central position for the noun phrase, with the word the playing a supporting
role.

the relationships between words in a sentence can be formalized in a directed graph,
based on the lexicalized phrase-structure parse: create an edge (i, j) iff word i is the head
of a phrase whose child is a phrase headed by word j. thus, in our example, we would
have scratch     cats and cats     the. we would not have the edge scratch     the, because
although s(scratch) dominates det(the) in the phrase-structure parse tree, it is not its im-
mediate parent. these edges describe syntactic dependencies, a bilexical relationship
between a head and a dependent, which is at the heart of dependency grammar.

continuing to build out this dependency graph, we will eventually reach every word
in the sentence, as shown in figure 11.1b. in this graph     and in all graphs constructed
in this way     every word has exactly one incoming edge, except for the root word, which
is indicated by a special incoming arrow from above. furthermore, the graph is weakly
connected: if the directed edges were replaced with undirected edges, there would be a
path between all pairs of nodes. from these properties, it can be shown that there are no
cycles in the graph (or else at least one node would have to have more than one incoming
edge), and therefore, the graph is a tree. because the graph includes all vertices, it is a
spanning tree.

11.1.1 heads and dependents
a dependency edge implies an asymmetric syntactic relationship between the head and
dependent words, sometimes called modi   ers. for a pair like the cats or cats scratch, how

jacob eisenstein. draft of november 13, 2018.

s(scratch)vp(scratch)pp(with)np(claws)nnsclawsinwithnp(people)nnspeoplevbscratchnp(cats)nnscatsdtthethecatsscratchpeoplewithclaws11.1. dependency grammar

259

do we decide which is the head? here are some possible criteria:

    the head sets the syntactic category of the construction: for example, nouns are the

heads of noun phrases, and verbs are the heads of verb phrases.

    the modi   er may be optional while the head is mandatory: for example, in the
sentence cats scratch people with claws, the subtrees cats scratch and cats scratch people
are grammatical sentences, but with claws is not.

    the head determines the morphological form of the modi   er: for example, in lan-
guages that require gender agreement, the gender of the noun determines the gen-
der of the adjectives and determiners.

    edges should    rst connect content words, and then connect function words.

these guidelines are not universally accepted, and they sometimes con   ict. the uni-
versal dependencies (ud) project has attempted to identify a set of principles that can be
applied to dozens of different languages (nivre et al., 2016).2 these guidelines are based
on the universal part-of-speech tags from chapter 8. they differ somewhat from the head
rules described in    10.5.2: for example, on the principle that dependencies should relate
content words, the prepositional phrase with claws would be headed by claws, resulting in
an edge scratch     claws, and another edge claws     with.
one objection to dependency grammar is that not all syntactic relations are asymmet-
ric. one such relation is coordination (popel et al., 2013): in the sentence, abigail and max
like kimchi (figure 11.2), which word is the head of the coordinated noun phrase abigail
and max? choosing either abigail or max seems arbitrary; fairness argues for making and
the head, but this seems like the least important word in the noun phrase, and selecting
it would violate the principle of linking content words    rst. the universal dependencies
annotation system arbitrarily chooses the left-most item as the head     in this case, abigail
    and includes edges from this head to both max and the coordinating conjunction and.
these edges are distinguished by the labels conj (for the thing begin conjoined) and cc
(for the coordinating conjunction). the labeling system is discussed next.

11.1.2 labeled dependencies

edges may be labeled to indicate the nature of the syntactic relation that holds between
the two elements. for example, in figure 11.2, the label nsubj on the edge from like to
abigail indicates that the subtree headed by abigail is the noun subject of the verb like;
similarly, the label obj on the edge from like to kimchi indicates that the subtree headed by

2the latest and most

speci   c guidelines are available at universaldependencies.org/

guidelines.html

under contract with mit press, shared under cc-by-nc-nd license.

260

chapter 11. id33

figure 11.2: in the universal dependencies annotation system, the left-most item of a
coordination is the head.

figure 11.3: a labeled dependency parse from the english ud treebank (reviews-361348-
0006)

kimchi is the object.3 the negation not is treated as an adverbial modi   er (advmod) on
the noun jook.

a slightly more complex example is shown in figure 11.3. the multiword expression
new york pizza is treated as a       at    unit of text, with the elements linked by the com-
pound relation. the sentence includes two clauses that are conjoined in the same way
that noun phrases are conjoined in figure 11.2. the second clause contains a copula verb
(see    8.1.1). for such clauses, we treat the    object    of the verb as the root     in this case,
it     and label the verb as a dependent, with the cop relation. this example also shows
how punctuations are treated, with label punct.

11.1.3 dependency subtrees and constituents

dependency trees hide information that would be present in a id18 parse. often what
is hidden is in fact irrelevant: for example, figure 11.4 shows three different ways of

3earlier work distinguished direct and indirect objects (de marneffe and manning, 2008), but this has

been dropped in version 2.0 of the universal dependencies annotation system.

jacob eisenstein. draft of november 13, 2018.

abigailandmaxlikekimchibutnotjookrootnsubjobjccconjconjccadvmodiknownewyorkpizzaandthisisnotit!!nsubjcompoundcompoundobjccnsubjcopadvmodconjpunctroot11.1. dependency grammar

261

v

np

vp

pp

vp

vp

pp

pp

vp

pp

with a fork

v
ate

np

on the table

dinner

(b) chomsky adjunction

ate

dinner

on the table

with a fork

(a) flat

vp

pp

pp

vp

v

np

on the table

with a fork

ate

dinner

(c) two-level (ptb-style)

(d) dependency representation

figure 11.4: the three different id18 analyses of this verb phrase all correspond to a single
dependency structure.

representing prepositional phrase adjuncts to the verb ate. because there is apparently no
meaningful difference between these analyses, the id32 decides by convention
to use the two-level representation (see johnson, 1998, for a discussion). as shown in
figure 11.4d, these three cases all look the same in a dependency parse.

but dependency grammar imposes its own set of annotation decisions, such as the
identi   cation of the head of a coordination (   11.1.1); without lexicalization, context-free
grammar does not require either element in a coordination to be privileged in this way.
dependency parses can be disappointingly    at: for example, in the sentence yesterday,
abigail was reluctantly giving max kimchi, the root giving is the head of every dependency!
the constituent parse arguably offers a more useful structural analysis for such cases.

projectivity thus far, we have de   ned dependency trees as spanning trees over a graph
in which each word is a vertex. as we have seen, one way to construct such trees is by
connecting the heads in a lexicalized constituent parse. however, there are spanning trees
that cannot be constructed in this way. syntactic constituents are contiguous spans. in a
spanning tree constructed from a lexicalized constituent parse, the head h of any con-
stituent that spans the nodes from i to j must have a path to every node in this span. this
is property is known as projectivity, and projective dependency parses are a restricted
class of spanning trees. informally, projectivity means that    crossing edges    are prohib-
ited. the formal de   nition follows:

under contract with mit press, shared under cc-by-nc-nd license.

atedinneronthetablewithafork262

chapter 11. id33

% non-projective edges % non-projective sentences
1.86%
czech
english
0.39%
german 2.33%

22.42%
7.63%
28.19%

table 11.1: frequency of non-projective dependencies in three languages (kuhlmann and
nivre, 2010)

figure 11.5: an example of a non-projective dependency parse. the    crossing edge    arises
from the relative clause which was vegetarian and the oblique temporal modi   er yesterday.

de   nition 2 (projectivity). an edge from i to j is projective iff all k between i and j are descen-
dants of i. a dependency parse is projective iff all its edges are projective.

figure 11.5 gives an example of a non-projective dependency graph in english. this
dependency graph does not correspond to any constituent parse. as shown in table 11.1,
non-projectivity is more common in languages such as czech and german. even though
relatively few dependencies are non-projective in these languages, many sentences have
at least one such dependency. as we will soon see, projectivity has important algorithmic
consequences.

11.2 graph-based id33
let y = {(i r       j)} represent a dependency graph, in which each edge is a relation r from
head word i     {1, 2, . . . , m, root} to modi   er j     {1, 2, . . . , m}. the special node root
indicates the root of the graph, and m is the length of the input |w|. given a scoring
function   (y, w;   ), the optimal parse is,

  y = argmax
y   y(w)

  (y, w;   ),

[11.1]

where y(w) is the set of valid dependency parses on the input w. as usual, the number
of possible labels |y(w)| is exponential in the length of the input (wu and chao, 2004).
jacob eisenstein. draft of november 13, 2018.

luciaateapizzayesterdaywhichwasvegetarianrootnsubjobjdetacl:relclobl:tmodnsubjcop11.2. graph-based id33

263

figure 11.6: feature templates for higher-order id33

algorithms that search over this space of possible graphs are known as graph-based de-
pendency parsers.

in sequence labeling and constituent parsing, it was possible to search ef   ciently over
an exponential space by choosing a feature function that decomposes into a sum of local
feature vectors. a similar approach is possible for id33, by requiring the
scoring function to decompose across dependency arcs:

  (y, w;   ) = (cid:88)i
r      j   y

  (i r       j, w;   ).

[11.2]

dependency parsers that operate under this assumption are known as arc-factored, since
the score of a graph is the product of the scores of all arcs.

higher-order id33 the arc-factored decomposition can be relaxed to al-
low higher-order dependencies. in second-order id33, the scoring func-
tion may include grandparents and siblings, as shown by the templates in figure 11.6.
the scoring function is,

  (y, w;   ) = (cid:88)i
r      j   y

  parent(i r       j, w;   )
+ (cid:88)k
r(cid:48)      i   y
+ (cid:88)i
r(cid:48)      s   y
s(cid:54)=j

  grandparent(i r       j, k, r

(cid:48)

, w;   )

  sibling(i r       j, s, r

(cid:48)

, w;   ).

[11.3]

the top line scores computes a scoring function that includes the grandparent k; the
bottom line computes a scoring function for each sibling s. for projective dependency
graphs, there are ef   cient algorithms for second-order and third-order dependency pars-
ing (eisner, 1996; mcdonald and pereira, 2006; koo and collins, 2010); for non-projective
dependency graphs, second-order id33 is np-hard (mcdonald and pereira,
2006). the speci   c algorithms are discussed in the next section.

under contract with mit press, shared under cc-by-nc-nd license.

firstorderhmsecondorderhsmghmthirdorderghsmhtsm264

chapter 11. id33

11.2.1 graph-based parsing algorithms
the distinction between projective and non-projective dependency trees (   11.1.3) plays
a key role in the choice of algorithms. because projective dependency trees are closely
related to (and can be derived from) lexicalized constituent trees, lexicalized parsing al-
gorithms can be applied directly. for the more general problem of parsing to arbitrary
spanning trees, a different class of algorithms is required. in both cases, arc-factored de-
r       j, w;   ) for each potential
pendency parsing relies on precomputing the scores   (i
edge. there are o(m 2r) such scores, where m is the length of the input and r is the
number of dependency relation types, and this is a lower bound on the time and space
complexity of any exact algorithm for arc-factored id33.

projective id33

any lexicalized constituency tree can be converted into a projective dependency tree by
creating arcs between the heads of constituents and their parents, so any algorithm for
lexicalized constituent parsing can be converted into an algorithm for projective depen-
dency parsing, by converting arc scores into scores for lexicalized productions. as noted
in    10.5.2, there are cubic time algorithms for lexicalized constituent parsing, which are
extensions of the cky algorithm. therefore, arc-factored projective id33
can be performed in cubic time in the length of the input.

second-order projective id33 can also be performed in cubic time, with
minimal modi   cations to the lexicalized parsing algorithm (eisner, 1996). it is possible to
go even further, to third-order id33, in which the scoring function may
consider great-grandparents, grand-siblings, and    tri-siblings   , as shown in figure 11.6.
third-order id33 can be performed in o(m 4) time, which can be made
practical through the use of pruning to eliminate unlikely edges (koo and collins, 2010).

non-projective id33

in non-projective id33, the goal is to identify the highest-scoring span-
ning tree over the words in the sentence. the arc-factored assumption ensures that the
score for each spanning tree will be computed as a sum over scores for the edges, which
are precomputed. based on these scores, we build a weighted connected graph. arc-
factored non-projective id33 is then equivalent to    nding the spanning
tree that achieves the maximum total score,   (y, w) = (cid:80)i
  (i r       j, w). the chu-
liu-edmonds algorithm (chu and liu, 1965; edmonds, 1967) computes this maximum
directed spanning tree ef   ciently. it does this by    rst identifying the best incoming edge
r       j for each vertex j. if the resulting graph does not contain cycles, it is the maxi-
i
mum spanning tree. if there is a cycle, it is collapsed into a super-vertex, whose incoming
and outgoing edges are based on the edges to the vertices in the cycle. the algorithm is

r      j   y

jacob eisenstein. draft of november 13, 2018.

11.2. graph-based id33

265

then applied recursively to the resulting graph, and process repeats until a graph without
cycles is obtained.

the time complexity of identifying the best incoming edge for each vertex is o(m 2r),
where m is the length of the input and r is the number of relations; in the worst case, the
number of cycles is o(m ). therefore, the complexity of the chu-liu-edmonds algorithm
is o(m 3r). this complexity can be reduced to o(m 2n ) by storing the edge scores in a
fibonnaci heap (gabow et al., 1986). for more detail on graph-based parsing algorithms,
see eisner (1997) and k   ubler et al. (2009).

higher-order non-projective id33 given the tractability of higher-order
projective id33, you may be surprised to learn that non-projective second-
order id33 is np-hard. this can be proved by reduction from the vertex
cover problem (neuhaus and br  oker, 1997). a heuristic solution is to do projective pars-
ing    rst, and then post-process the projective dependency parse to add non-projective
edges (nivre and nilsson, 2005). more recent work has applied techniques for approxi-
mate id136 in id114, including belief propagation (smith and eisner, 2008),
integer id135 (martins et al., 2009), variational id136 (martins et al.,
2010), and id115 (zhang et al., 2014).

11.2.2 computing scores for dependency arcs
the arc-factored scoring function   (i r       j, w;   ) can be de   ned in several ways:

linear
neural
generative

  (i r       j, w;   ) =       f (i r       j, w)
  (i r       j, w;   ) = feedforward([uwi; uwj ];   )
  (i r       j, w;   ) = log p(wj, r | wi).

[11.4]
[11.5]
[11.6]

linear feature-based arc scores

linear models for id33 incorporate many of the same features used in
sequence labeling and discriminative constituent parsing. these include:

    the length and direction of the arc;
    the words wi and wj linked by the dependency relation;
    the pre   xes, suf   xes, and parts-of-speech of these words;
    the neighbors of the dependency arc, wi   1, wi+1, wj   1, wj+1;
    the pre   xes, suf   xes, and part-of-speech of these neighbor words.

under contract with mit press, shared under cc-by-nc-nd license.

266

chapter 11. id33

each of these features can be conjoined with the dependency edge label r. note that
features in an arc-factored parser can refer to words other than wi and wj. the restriction
is that the features consider only a single arc.

bilexical features (e.g., sushi     chopsticks) are powerful but rare, so it is useful to aug-
ment them with coarse-grained alternatives, by    backing off    to the part-of-speech or
af   x. for example, the following features are created by backing off to part-of-speech tags
in an unlabeled dependency parser:

f (3        5, we eat sushi with chopsticks) = (cid:104)sushi     chopsticks,

sushi     nns,
nn     chopsticks,
nns     nn(cid:105).

regularized discriminative learning algorithms can then trade off between features at
varying levels of detail. mcdonald et al. (2005) take this approach as far as tetralexical
features (e.g., (wi, wi+1, wj   1, wj)). such features help to avoid choosing arcs that are un-
likely due to the intervening words: for example, there is unlikely to be an edge between
two nouns if the intervening span contains a verb. a large list of    rst and second-order
features is provided by bohnet (2010), who uses a hashing function to store these features
ef   ciently.

neural arc scores

given vector representations xi for each word wi in the input, a set of arc scores can be
computed from a feedforward neural network:

  (i r       j, w;   ) =feedforward([xi; xj];   r),

[11.7]

where unique weights   r are available for each arc type (pei et al., 2015; kiperwasser and
goldberg, 2016). kiperwasser and goldberg (2016) use a feedforward network with a
single hidden layer,

z =g(  r[xi; xj] + b(z)
r )

  (i r       j) =  rz + b(y)

r

,

[11.8]
[11.9]

where   r is a matrix,   r is a vector, each br is a scalar, and the function g is an elementwise
tanh activation function.

the vector xi can be set equal to the id27, which may be pre-trained or
learned by id26 (pei et al., 2015). alternatively, contextual information can
be incorporated by applying a bidirectional recurrent neural network across the input, as

jacob eisenstein. draft of november 13, 2018.

11.2. graph-based id33

267

described in    7.6. the id56 hidden states at each word can be used as inputs to the arc
scoring function (kiperwasser and goldberg, 2016).

feature-based arc scores are computationally expensive, due to the costs of storing
and searching a huge table of weights. neural arc scores can be viewed as a compact
solution to this problem. rather than working in the space of tuples of lexical features,
the hidden layers of a feedforward network can be viewed as implicitly computing fea-
ture combinations, with each layer of the network evaluating progressively more words.
an early paper on neural id33 showed substantial speed improvements
at test time, while also providing higher accuracy than feature-based models (chen and
manning, 2014).

probabilistic arc scores
if each arc score is equal to the log id203 log p(wj, r | wi), then the sum of scores
gives the log id203 of the sentence and arc labels, by the chain rule. for example,
consider the unlabeled parse of we eat sushi with rice,

y ={(root, 2), (2, 1), (2, 3), (3, 5), (5, 4)}

log p(w | y) = (cid:88)(i   j)   y

log p(wj | wi)

= log p(eat | root) + log p(we | eat) + log p(sushi | eat)

+ log p(rice | sushi) + log p(with | rice).

[11.10]
[11.11]

[11.12]

probabilistic generative models are used in combination with expectation-maximization
(chapter 5) for unsupervised id33 (klein and manning, 2004).

11.2.3 learning
having formulated graph-based id33 as a structure prediction problem,
we can apply similar learning algorithms to those used in sequence labeling. given a loss
function (cid:96)(  ; w(i), y(i)), we can compute gradient-based updates to the parameters. for a
model with feature-based arc scores and a id88 loss, we obtain the usual structured
id88 update,

  y = argmax
y(cid:48)   y(w)

      f (w, y

(cid:48)

)

   =   + f (w, y)     f (w,   y)

[11.13]

[11.14]

in this case, the argmax requires a maximization over all dependency trees for the sen-
tence, which can be computed using the algorithms described in    11.2.1. we can apply
all the usual tricks from    2.3: weight averaging, a large margin objective, and regular-
ization. mcdonald et al. (2005) were the    rst to treat id33 as a structure

under contract with mit press, shared under cc-by-nc-nd license.

268

chapter 11. id33

prediction problem, using mira, an online margin-based learning algorithm. neural arc
scores can be learned in the same way, backpropagating from a margin loss to updates on
the feedforward network that computes the score for each edge.

a conditional random    eld for arc-factored id33 is built on the proba-

bility model,

p(y | w) =

exp(cid:80)i

r      j   y
(cid:80)y(cid:48)   y(w) exp(cid:80)i

  (i r       j, w;   )
r      j   y(cid:48)

  (i r       j, w;   )

[11.15]

such a model is trained to minimize the negative log conditional-likelihood. just as in
crf sequence models (   7.5.3) and the id28 classi   er (   2.5), the gradients
involve marginal probabilities p(i r       j | w;   ), which in this case are probabilities over
individual dependencies.
in arc-factored models, these probabilities can be computed
in polynomial time. for projective dependency trees, the marginal probabilities can be
computed in cubic time, using a variant of the inside-outside algorithm (lari and young,
1990). for non-projective id33, marginals can also be computed in cubic
time, using the matrix-tree theorem (koo et al., 2007; mcdonald et al., 2007; smith and
smith, 2007). details of these methods are described by k   ubler et al. (2009).

11.3 transition-based id33

graph-based id33 offers exact id136, meaning that it is possible to re-
cover the best-scoring parse for any given model. but this comes at a price: the scoring
function is required to decompose into local parts     in the case of non-projective parsing,
these parts are restricted to individual arcs. these limitations are felt more keenly in de-
pendency parsing than in sequence labeling, because second-order dependency features
are critical to correctly identify some types of attachments. for example, prepositional
phrase attachment depends on the attachment point, the object of the preposition, and
the preposition itself; arc-factored scores cannot account for all three of these features si-
multaneously. graph-based id33 may also be criticized on the basis of
intuitions about human language processing: people read and listen to sentences sequen-
tially, incrementally building mental models of the sentence structure and meaning before
getting to the end (jurafsky, 1996). this seems hard to reconcile with graph-based algo-
rithms, which perform bottom-up operations on the entire sentence, requiring the parser
to keep every word in memory. finally, from a practical perspective, graph-based depen-
dency parsing is relatively slow, running in cubic time in the length of the input.

transition-based algorithms address all three of these objections. they work by mov-
ing through the sentence sequentially, while performing actions that incrementally up-
date a stored representation of what has been read thus far. as with the shift-reduce

jacob eisenstein. draft of november 13, 2018.

11.3. transition-based id33

269

parser from    10.6.2, this representation consists of a stack, onto which parsing substruc-
tures can be pushed and popped. in shift-reduce, these substructures were constituents;
in the transition systems that follow, they will be projective dependency trees over partial
spans of the input.4 parsing is complete when the input is consumed and there is only
a single structure on the stack. the sequence of actions that led to the parse is known as
the derivation. one problem with transition-based systems is that there may be multiple
derivations for a single parse structure     a phenomenon known as spurious ambiguity.

11.3.1 transition systems for id33
a transition system consists of a representation for describing con   gurations of the parser,
and a set of transition actions, which manipulate the con   guration. there are two main
transition systems for id33: arc-standard, which is closely related to shift-
reduce, and arc-eager, which adds an additional action that can simplify derivations (ab-
ney and johnson, 1991). in both cases, transitions are between con   gurations that are
represented as triples, c = (  ,   , a), where    is the stack,    is the input buffer, and a is
the list of arcs that have been created (nivre, 2008). in the initial con   guration,

cinitial = ([root], w,    ),

[11.16]

indicating that the stack contains only the special node root, the entire input is on the
buffer, and the set of arcs is empty. an accepting con   guration is,

caccept = ([root],    , a),

[11.17]

where the stack contains only root, the buffer is empty, and the arcs a de   ne a spanning
tree over the input. the arc-standard and arc-eager systems de   ne a set of transitions
between con   gurations, which are capable of transforming an initial con   guration into
an accepting con   guration. in both of these systems, the number of actions required to
parse an input grows linearly in the length of the input, making transition-based parsing
considerably more ef   cient than graph-based methods.

arc-standard

the arc-standard transition system is closely related to shift-reduce, and to the lr algo-
rithm that is used to parse programming languages (aho et al., 2006).
it includes the
following classes of actions:

    shift: move the    rst item from the input buffer on to the top of the stack,

4transition systems also exist for non-projective id33 (e.g., nivre, 2008).

(  , i|  , a)     (  |i,   , a),

[11.18]

under contract with mit press, shared under cc-by-nc-nd license.

270

chapter 11. id33

where we write i|   to indicate that i is the leftmost item in the input buffer, and   |i
to indicate the result of pushing i on to stack   .

    arc-left: create a new left-facing arc of type r between the item on the top of the
stack and the    rst item in the input buffer. the head of this arc is j, which remains
at the front of the input buffer. the arc j r       i is added to a. formally,

(  |i, j|  , a)     (  , j|  , a     j r       i),

[11.19]
where r is the label of the dependency arc, and     concatenates the new arc j r       i to
the list a.
    arc-right: creates a new right-facing arc of type r between the item on the top of
the stack and the    rst item in the input buffer. the head of this arc is i, which is
   popped    from the stack and pushed to the front of the input buffer. the arc i r       j
is added to a. formally,

(  |i, j|  , a)     (  , i|  , a     i r       j),

[11.20]

where again r is the label of the dependency arc.

each action has preconditions. the shift action can be performed only when the buffer
has at least one element. the arc-left action cannot be performed when the root node
root is on top of the stack, since this node must be the root of the entire tree. the arc-
left and arc-right remove the modi   er words from the stack (in the case of arc-left)
and from the buffer (in the case of arc-right), so it is impossible for any word to have
more than one parent. furthermore, the end state can only be reached when every word is
removed from the buffer and stack, so the set of arcs is guaranteed to constitute a spanning
tree. an example arc-standard derivation is shown in table 11.2.

arc-eager id33

in the arc-standard transition system, a word is completely removed from the parse once
it has been made the modi   er in a dependency arc. at this time, any dependents of
this word must have already been identi   ed. right-branching structures are common in
english (and many other languages), with words often modi   ed by units such as prepo-
sitional phrases to their right. in the arc-standard system, this means that we must    rst
shift all the units of the input onto the stack, and then work backwards, creating a series of
arcs, as occurs in table 11.2. note that the decision to shift bagels onto the stack guarantees
that the prepositional phrase with lox will attach to the noun phrase, and that this decision
must be made before the prepositional phrase is itself parsed. this has been argued to be
cognitively implausible (abney and johnson, 1991); from a computational perspective, it
means that a parser may need to look several steps ahead to make the correct decision.

jacob eisenstein. draft of november 13, 2018.

11.3. transition-based id33

271

  
[root]
[root, they]
[root]
[root, like]
[root, like, bagels]
[root, like, bagels, with]
[root, like, bagels]
[root, like]
[root]
[root]

  
they like bagels with lox
like bagels with lox
like bagels with lox
bagels with lox
with lox
lox
lox
bagels
like
   

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.

action
shift
arc-left
shift
shift
shift
arc-left
arc-right
arc-right
arc-right
done

arc added to a

(they     like)

(with     lox)
(bagels     lox)
(like     bagels)
(root     like)

table 11.2: arc-standard derivation of the unlabeled dependency parse for the input they
like bagels with lox.

arc-eager id33 changes the arc-right action so that right depen-
dents can be attached before all of their dependents have been found. rather than re-
moving the modi   er from both the buffer and stack, the arc-right action pushes the
modi   er on to the stack, on top of the head. because the stack can now contain elements
that already have parents in the partial dependency graph, two additional changes are
necessary:

    a precondition is required to ensure that the arc-left action cannot be applied

when the top element on the stack already has a parent in a.

    a new reduce action is introduced, which can remove elements from the stack if

they already have a parent in a:

(  |i,   , a)     (  ,   , a).

[11.21]

as a result of these changes, it is now possible to create the arc like     bagels before parsing
the prepositional phrase with lox. furthermore, this action does not imply a decision about
whether the prepositional phrase will attach to the noun or verb. noun attachment is
chosen in the parse in table 11.3, but verb attachment could be achieved by applying the
reduce action at step 5 or 7.

projectivity

the arc-standard and arc-eager transition systems are guaranteed to produce projective
dependency trees, because all arcs are between the word at the top of the stack and the

under contract with mit press, shared under cc-by-nc-nd license.

272

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

chapter 11. id33

  
[root]
[root, they]
[root]
[root, like]
[root, like, bagels]
[root, like, bagels, with]
[root, like, bagels]
[root, like, bagels, lox]
[root, like, bagels]
[root, like]
[root]

  
they like bagels with lox
like bagels with lox
like bagels with lox
bagels with lox
with lox
lox
lox
   
   
   
   

action
shift
arc-left
arc-right
arc-right
shift
arc-left
arc-right
reduce
reduce
reduce
done

arc added to a

(they     like)
(root     like)
(like     bagels)
(with     lox)
(bagels     lox)

table 11.3: arc-eager derivation of the unlabeled dependency parse for the input they like
bagels with lox.

left-most edge of the buffer (nivre, 2008). non-projective transition systems can be con-
structed by adding actions that create arcs with words that are second or third in the
stack (attardi, 2006), or by adopting an alternative con   guration structure, which main-
tains a list of all words that do not yet have heads (covington, 2001). in pseudo-projective
id33, a projective dependency parse is generated    rst, and then a set of
graph transformation techniques are applied, producing non-projective edges (nivre and
nilsson, 2005).

id125

in    greedy    transition-based parsing, the parser tries to make the best decision at each
con   guration. this can lead to search errors, when an early decision locks the parser into
a poor derivation. for example, in table 11.2, if arc-right were chosen at step 4, then
the parser would later be forced to attach the prepositional phrase with lox to the verb
likes. note that the likes     bagels arc is indeed part of the correct dependency parse, but
the arc-standard transition system requires it to be created later in the derivation.

id125 is a general technique for ameliorating search errors in incremental de-
coding.5 while searching, the algorithm maintains a set of partially-complete hypotheses,
called a beam. at step t of the derivation, there is a set of k hypotheses, each of which

5id125 is used throughout natural language processing, and beyond. in this text, it appears again

in coreference resolution (   15.2.4) and machine translation (   18.4).

jacob eisenstein. draft of november 13, 2018.

11.3. transition-based id33

273

figure 11.7: id125 for unlabeled id33, with beam size k = 2. the
arc lists for each con   guration are not shown, but can be computed from the transitions.

includes a score s(k)

t

and a set of dependency arcs a(k)

t

:

h(k)
t = (s(k)

t

, a(k)

t

)

[11.22]

each hypothesis is then    expanded    by considering the set of all valid actions from the
current con   guration c(k)
). this yields a large set of new hypotheses. for
each action a     a(c(k)
t     a. the top k hypotheses
by this scoring metric are kept, and parsing proceeds to the next step (zhang and clark,
2008). note that id125 requires a scoring function for action sequences, rather than
individual actions. this issue will be revisited in the next section.

, written a(c(k)
t
), we score the new hypothesis a(k)

t

t

figure 11.7 shows the application of id125 to id33, with a beam
size of k = 2. for the    rst transition, the only valid action is shift, so there is only
one possible con   guration at t = 2. from this con   guration, there are three possible
actions. the two best scoring actions are arc-right and arc-left, and so the resulting
hypotheses from these actions are on the beam at t = 3. from these con   gurations, there
are three possible actions each, but the best two are expansions of the bottom hypothesis
at t = 3. parsing continues until t = 5, at which point both hypotheses reach an accepting
state. the best-scoring hypothesis is then selected as the parse.

11.3.2 scoring functions for transition-based parsers
transition-based parsing requires selecting a series of actions. in greedy transition-based
parsing, this can be done by training a classi   er,

  a = argmax
a   a(c)

  (a, c, w;   ),

[11.23]

where a(c) is the set of admissible actions in the current con   guration c, w is the input,
and    is a scoring function with parameters    (yamada and matsumoto, 2003).

a feature-based score can be computed,   (a, c, w) =       f (a, c, w), using features that
may consider any aspect of the current con   guration and input sequence. typical features
for transition-based id33 include: the word and part-of-speech of the top

under contract with mit press, shared under cc-by-nc-nd license.

t=1t=2t=3t=4t=5(cid:20)[root]theycan   sh(cid:21)(cid:20)[root,they]can   sh(cid:21)(cid:20)[root,they]   sh(cid:21)(cid:20)[root,can]   (cid:21)(cid:20)[root]   (cid:21)(cid:20)[root,can]   sh(cid:21)(cid:20)[root,   sh]   (cid:21)(cid:20)[root]   (cid:21)shiftarc-rightarc-leftarc-rightarc-leftarc-rightarc-right274

chapter 11. id33

element on the stack; the word and part-of-speech of the    rst, second, and third elements
on the input buffer; pairs and triples of words and parts-of-speech from the top of the
stack and the front of the buffer; the distance (in tokens) between the element on the top
of the stack and the element in the front of the input buffer; the number of modi   ers of
each of these elements; and higher-order dependency features as described above in the
section on graph-based id33 (see, e.g., zhang and nivre, 2011).

parse actions can also be scored by neural networks. for example, chen and manning
(2014) build a feedforward network in which the input layer consists of the concatenation
of embeddings of several words and tags:

    the top three words on the stack, and the    rst three words on the buffer;
    the    rst and second leftmost and rightmost children (dependents) of the top two

words on the stack;

    the leftmost and right most grandchildren of the top two words on the stack;
    embeddings of the part-of-speech tags of these words.

let us call this base layer x(c, w), de   ned as,

c =(  ,   , a)

x(c, w) =[vw  1

, vt  1

vw  2

, vt  2

, vw  3

, vt  3

, vw  1

, vt  1

, vw  2

, vt  2

, . . .],

where vw  1 is the embedding of the    rst word on the stack, vt  2
is the embedding of the
part-of-speech tag of the second word on the buffer, and so on. given this base encoding
of the parser state, the score for the set of possible actions is computed through a feedfor-
ward network,

z =g(  (x   z)x(c, w))

  (a, c, w;   ) =  (z   y)

z,

a

[11.24]
[11.25]

where the vector z plays the same role as the features f (a, c, w), but is a learned represen-
tation. chen and manning (2014) use a cubic elementwise activation function, g(x) = x3,
so that the hidden layer models products across all triples of input features. the learning
algorithm updates the embeddings as well as the parameters of the feedforward network.

11.3.3 learning to parse
transition-based id33 suffers from a mismatch between the supervision,
which comes in the form of dependency trees, and the classi   er   s prediction space, which
is a set of parsing actions. one solution is to create new training data by converting parse
trees into action sequences; another is to derive supervision directly from the parser   s
performance.

jacob eisenstein. draft of november 13, 2018.

11.3. transition-based id33

275

oracle-based training

a transition system can be viewed as a function from action sequences (derivations) to
parse trees. the inverse of this function is a mapping from parse trees to derivations,
which is called an oracle. for the arc-standard and arc-eager parsing system, an oracle can
be computed in linear time in the length of the derivation (k   ubler et al., 2009, page 32).
both the arc-standard and arc-eager transition systems suffer from spurious ambiguity:
there exist dependency parses for which multiple derivations are possible, such as 1    
2     3.the oracle must choose between these different derivations. for example, the
algorithm described by k   ubler et al. (2009) would    rst create the left arc (1     2), and then
create the right arc, (1     2)     3; another oracle might begin by shifting twice, resulting
in the derivation 1     (2     3).

given such an oracle, a dependency treebank can be converted into a set of oracle ac-
tion sequences {a(i)}n
i=1. the parser can be trained by stepping through the oracle action
sequences, and optimizing on an classi   cation-based objective that rewards selecting the
oracle action. for transition-based id33, maximum conditional likelihood
is a typical choice (chen and manning, 2014; dyer et al., 2015):

p(a | c, w) =

exp   (a, c, w;   )

(cid:80)a(cid:48)   a(c) exp   (a(cid:48), c, w;   )

|a(i)|

log p(a(i)
t

[11.26]

[11.27]

| c(i)

t

, w),

(cid:88)t=1
where |a(i)| is the length of the action sequence a(i).

     = argmax

n(cid:88)i=1

  

recall that id125 requires a scoring function for action sequences. such a score
can be obtained by adding the log-likelihoods (or hinge losses) across all actions in the
sequence (chen and manning, 2014).

global objectives

the objective in equation 11.27 is locally-normalized: it is the product of normalized
probabilities over individual actions. a similar characterization could be made of non-
probabilistic algorithms in which hinge-loss objectives are summed over individual ac-
tions. in either case, training on individual actions can be sub-optimal with respect to
global performance, due to the label bias problem (lafferty et al., 2001; andor et al.,
2016).

as a stylized example, suppose that a given con   guration appears 100 times in the
training data, with action a1 as the oracle action in 51 cases, and a2 as the oracle action in
the other 49 cases. however, in cases where a2 is correct, choosing a1 results in a cascade
of subsequent errors, while in cases where a1 is correct, choosing a2 results in only a single

under contract with mit press, shared under cc-by-nc-nd license.

276

chapter 11. id33

error. a classi   er that is trained on a local objective function will learn to always choose
a1, but choosing a2 would minimize the overall number of errors.

this observation motivates a global objective, such as the globally-normalized condi-

tional likelihood,

p(a(i) | w;   ) =

t=1   (a(i)

exp(cid:80)|a(i)|
(cid:80)a(cid:48)   a(w) exp(cid:80)|a(cid:48)|

, c(i)
t
t=1   (a(cid:48)

t

, w)
t, c(cid:48)

t, w)

,

[11.28]

where the denominator sums over the set of all possible action sequences, a(w).6 in the
conditional random    eld model for sequence labeling (   7.5.3), it was possible to compute
this sum explicitly, using id145. in transition-based parsing, this is not
possible. however, the sum can be approximated using id125,

(cid:88)a(cid:48)   a(w)

exp

|a(cid:48)|

(cid:88)t=1

(cid:48)
(cid:48)
t, w)    
t, c
  (a

exp

k(cid:88)k=1

|a(k)|

(cid:88)t=1

  (a(k)

t

, c(k)

t

, w),

[11.29]

where a(k) is an action sequence on a beam of size k. this gives rise to the following loss
function,

l(  ) =    

|a(i)|

(cid:88)t=1

  (a(i)
t

, c(i)
t

, w) + log

exp

k(cid:88)k=1

|a(k)|

(cid:88)t=1

  (a(k)

t

, c(k)

t

, w).

[11.30]

the derivatives of this loss involve expectations with respect to a id203 distribution
over action sequences on the beam.

*early update and the incremental id88

when learning in the context of id125, the goal is to learn a decision function so that
the gold dependency parse is always reachable from at least one of the partial derivations
on the beam. (the combination of a transition system (such as id125) and a scoring
function for actions is known as a policy.) to achieve this, we can make an early update
as soon as the oracle action sequence    falls off    the beam, even before a complete analysis
is available (collins and roark, 2004; daum  e iii and marcu, 2005). the loss can be based
on the best-scoring hypothesis on the beam, or the sum of all hypotheses (huang et al.,
2012).

head of dependency arcs to both of the other two words.

for example, consider the id125 in figure 11.7. in the correct parse,    sh is the
in the arc-standard system,
6andor et al. (2016) prove that the set of globally-normalized conditional distributions is a strict superset
of the set of locally-normalized conditional distributions, and that globally-normalized conditional models
are therefore strictly more expressive.

jacob eisenstein. draft of november 13, 2018.

11.4. applications

277

      l(a(i)

this can be achieved only by using shift for the    rst two actions. at t = 3, the oracle
action sequence has fallen off the beam. the parser should therefore stop, and update the
parameters by the gradient    
1:3 is the    rst three actions of the
oracle sequence, and {a(k)
this integration of incremental search and learning was    rst developed in the incre-
mental id88 (collins and roark, 2004). this method updates the parameters with
respect to a hinge loss, which compares the top-scoring hypothesis and the gold action
sequence, up to the current point t. several improvements to this basic protocol are pos-
sible:

1:3};   ), where a(i)

1:3} is the beam.

1:3,{a(k)

    as noted earlier, the gold dependency parse can be derived by multiple action se-
quences. rather than checking for the presence of a single oracle action sequence on
the beam, we can check if the gold dependency parse is reachable from the current
beam, using a dynamic oracle (goldberg and nivre, 2012).

    by maximizing the score of the gold action sequence, we are training a decision
function to    nd the correct action given the gold context. but in reality, the parser
will make errors, and the parser is not trained to    nd the best action given a context
that may not itself be optimal. this issue is addressed by various generalizations of
incremental id88, known as learning to search (daum  e iii et al., 2009). some
of these methods are discussed in chapter 15.

11.4 applications

id33 is used in many real-world applications: any time you want to know
about pairs of words which might not be adjacent, you can use dependency arcs instead
of regular expression search patterns. for example, you may want to match strings like
delicious pastries, delicious french pastries, and the pastries are delicious.

it is possible to search the google id165s corpus by dependency edges,    nding the
trend in how often a dependency edge appears over time. for example, we might be inter-
ested in knowing when people started talking about writing code, but we also want write
some code, write good code, write all the code, etc. the result of a search on the dependency
edge write     code is shown in figure 11.8. this capability has been applied to research
in digital humanities, such as the analysis of gender in shakespeare muralidharan and
hearst (2013).

a classic application of id33 is id36, which is described

under contract with mit press, shared under cc-by-nc-nd license.

278

chapter 11. id33

figure 11.8: google id165s results for the bigram write code and the dependency arc write
=> code (and their morphological variants)

in chapter 17. the goal of id36 is to identify entity pairs, such as

(melville, moby-dick)
(tolstoy, war and peace)
(marqu   ez, 100 years of solitude)
(shakespeare, a midsummer night   s dream),

which stand in some relation to each other (in this case, the relation is authorship). such
entity pairs are often referenced via consistent chains of dependency relations. therefore,
dependency paths are often a useful feature in supervised systems which learn to detect
new instances of a relation, based on labeled examples of other instances of the same
relation type (culotta and sorensen, 2004; fundel et al., 2007; mintz et al., 2009).

cui et al. (2005) show how id33 can improve automated question an-

swering. suppose you receive the following query:

(11.1) what percentage of the nation   s cheese does wisconsin produce?

the corpus contains this sentence:

(11.2)

in wisconsin, where farmers produce 28% of the nation   s cheese, . . .

the location of wisconsin in the surface form of this string makes it a poor match for the
query. however, in the dependency graph, there is an edge from produce to wisconsin in
both the question and the potential answer, raising the likelihood that this span of text is
relevant to the question.

a    nal example comes from id31. as discussed in chapter 4, the polarity

of a sentence can be reversed by negation, e.g.

jacob eisenstein. draft of november 13, 2018.

11.4. applications

279

(11.3) there is no reason at all to believe the polluters will suddenly become reasonable.

by tracking the sentiment polarity through the dependency parse, we can better iden-
tify the overall polarity of the sentence, determining when key sentiment words are re-
versed (wilson et al., 2005; nakagawa et al., 2010).

additional resources

more details on dependency grammar and parsing algorithms can be found in the manuscript
by k   ubler et al. (2009). for a comprehensive but whimsical overview of graph-based de-
pendency parsing algorithms, see eisner (1997). jurafsky and martin (2019) describe an
agenda-based version of id125, in which the beam contains hypotheses of varying
lengths. new hypotheses are added to the beam only if their score is better than the worst
item currently on the beam. another search algorithm for transition-based parsing is
easy-   rst, which abandons the left-to-right traversal order, and adds the highest-scoring
edges    rst, regardless of where they appear (goldberg and elhadad, 2010). goldberg et al.
(2013) note that although transition-based methods can be implemented in linear time in
the length of the input, na    ve implementations of id125 will require quadratic time,
due to the cost of copying each hypothesis when it is expanded on the beam. this issue
can be addressed by using a more ef   cient data structure for the stack.

exercises

1. the dependency structure 1     2     3, with 2 as the root, can be obtained from more
than one set of actions in arc-standard parsing. list both sets of actions that can
obtain this parse. don   t forget about the edge root     2.

2. this problem develops the relationship between id33 and lexical-
ized context-free parsing. suppose you have a set of unlabeled arc scores {  (i    
j=1.
i,j=1     {  (root     j)}m
j)}m
a) assuming each word type occurs no more than once in the input ((i (cid:54)= j)    
(wi (cid:54)= wj)), how would you construct a weighted lexicalized context-free gram-
mar so that the score of any projective dependency tree is equal to the score of
some equivalent derivation in the lexicalized context-free grammar?

b) verify that your method works for the example they    sh.
c) does your method require the restriction that each word type occur no more

than once in the input? if so, why?

d) *if your method required that each word type occur only once in the input,

show how to generalize it.

under contract with mit press, shared under cc-by-nc-nd license.

280

chapter 11. id33

3. in arc-factored id33 of an input of length m, the score of a parse
is the sum of m scores, one for each arc. in second order id33, the
total score is the sum over many more terms. how many terms are the score of the
parse for figure 11.2, using a second-order dependency parser with grandparent
and sibling features? assume that a child of root has no grandparent score, and
that a node with no siblings has no sibling scores.

4.

a) in the worst case, how many terms can be involved in the score of an input of
length m, assuming second-order id33? describe the structure
of the worst-case parse. as in the previous problem, assume that there is only
one child of root, and that it does not have any grandparent scores.

b) what about third-order id33?

5. provide the ud-style unlabeled dependency parse for the sentence xi-lan eats shoots
and leaves, assuming shoots is a noun and leaves is a verb. provide arc-standard and
arc-eager derivations for this dependency parse.

6. compute an upper bound on the number of successful derivations in arc-standard
id132 for unlabeled dependencies, as a function of the length of the
input, m. hint: a lower bound is the number of projective id90,

m +1(cid:0)3m   2
2017), where(cid:0)a
7. the label bias problem arises when a decision is locally correct, yet leads to a cas-
cade of errors in some situations (   11.3.3). design a scenario in which this occurs.
speci   cally:

b(cid:1) = a!

(a   b)!b!.

1

m   1(cid:1) (zhang,

only the words at the top of the stack and at the front of the input buffer.

    assume an arc-standard dependency parser, whose action classi   er considers
    design two examples, which both involve a decision with identical features.

    in one example, shift is the correct decision; in the other example, arc-left

or arc-right is the correct decision.

    in one of the two examples, a mistake should lead to at least two attach-

ment errors.

    in the other example, a mistake should lead only to a single attachment

error.

for the following exercises, run a dependency parser, such as stanford   s corenlp

parser, on a large corpus of text (at least 105 tokens), such as nltk.corpus.webtext.

8. the dependency relation nmod:poss indicates possession. compute the top ten
words most frequently possessed by each of the following pronouns: his, her, our,
my, your, and their (inspired by muralidharan and hearst, 2013).

jacob eisenstein. draft of november 13, 2018.

11.4. applications

281

9. count all pairs of words grouped by the conj relation. select all pairs of words (i, j)
for which i and j each participate in conj relations at least    ve times. compute and
sort by the pointwise mutual information, which is de   ned in    14.3 as,

pmi(i, j) = log

p(i, j)
p(i)p(j)

.

[11.31]

here, p(i) is the fraction of conj relations containing word i (in either position), and
p(i, j) is the fraction of such relations linking i and j (in any order).

10. in    4.2, we encountered lexical semantic relationships such as synonymy (same
meaning), antonymy (opposite meaning), and hypernymy (i is a special case of
j). another relevant relation is co-hypernymy, which means that i and j share a
hypernym. of the top 20 pairs identi   ed by pmi in the previous problem, how many
participate in synsets that are linked by one of these four relations? use id138
to check for these relations, and count a pair of words if any of their synsets are
linked.

under contract with mit press, shared under cc-by-nc-nd license.

part iii

meaning

283

chapter 12

logical semantics

the previous few chapters have focused on building systems that reconstruct the syntax
of natural language     its structural organization     through tagging and parsing. but
some of the most exciting and promising potential applications of language technology
involve going beyond syntax to semantics     the underlying meaning of the text:

    answering questions, such as where is the nearest coffeeshop? or what is the middle name

of the mother of the 44th president of the united states?.

    building a robot that can follow natural language instructions to execute tasks.
    translating a sentence from one language into another, while preserving the under-

lying meaning.

    fact-checking an article by searching the web for contradictory evidence.
    logic-checking an argument by identifying contradictions, ambiguity, and unsup-

ported assertions.

semantic analysis involves converting natural language into a meaning representa-

tion. to be useful, a meaning representation must meet several criteria:

    c1: it should be unambiguous: unlike natural language, there should be exactly one

meaning per statement;

    c2: it should provide a way to link language to external knowledge, observations,

and actions;

    c3: it should support computational id136, so that meanings can be combined

to derive additional knowledge;

    c4: it should be expressive enough to cover the full range of things that people talk

about in natural language.

285

286

chapter 12. logical semantics

much more than this can be said about the question of how best to represent knowledge
for computation (e.g., sowa, 2000), but this chapter will focus on these four criteria.

12.1 meaning and denotation

the    rst criterion for a meaning representation is that statements in the representation
should be unambiguous     they should have only one possible interpretation. natural
language does not have this property: as we saw in chapter 10, sentences like cats scratch
people with claws have multiple interpretations.

but what does it mean for a statement to be unambiguous? programming languages
provide a useful example: the output of a program is completely speci   ed by the rules of
the language and the properties of the environment in which the program is run. for ex-
ample, the python code 5 + 3 will have the output 8, as will the codes (4*4)-(3*3)+1
and ((8)). this output is known as the denotation of the program, and can be written
as,

(cid:74)5+3(cid:75) =(cid:74)(4*4)-(3*3)+1(cid:75) =(cid:74)((8))(cid:75) = 8.

[12.1]

the denotations of these arithmetic expressions are determined by the meaning of the
constants (e.g., 5, 3) and the relations (e.g., +, *, (, )). now let   s consider another snippet

of python code, double(4). the denotation of this code could be,(cid:74)double(4)(cid:75) = 8, or
it could be(cid:74)double(4)(cid:75) = 44     it depends on the meaning of double. this meaning
respect to model m as(cid:74)  (cid:75)m, e.g.,(cid:74)double(cid:75)m = {(0,0), (1,2), (2,4), . . .}. the world
is de   ned in a world model m as an in   nite set of pairs. we write the denotation with
model would also de   ne the (in   nite) list of constants, e.g., {0,1,2,...}. as long as the
denotation of string    in model m can be computed unambiguously, the language can be
said to be unambiguous.

this approach to meaning is known as model-theoretic semantics, and it addresses
not only criterion c1 (no ambiguity), but also c2 (connecting language to external knowl-
edge, observations, and actions). for example, we can connect a representation of the
meaning of a statement like the capital of georgia with a world model that includes knowl-
edge base of geographical facts, obtaining the denotation atlanta. we might populate a
world model by detecting and analyzing the objects in an image, and then use this world
model to evaluate propositions like a man is riding a moose. another desirable property of
model-theoretic semantics is that when the facts change, the denotations change too: the
meaning representation of president of the usa would have a different denotation in the
model m2014 as it would in m2022.

jacob eisenstein. draft of november 13, 2018.

12.2. logical representations of meaning

287

12.2 logical representations of meaning

criterion c3 requires that the meaning representation support id136     for example,
automatically deducing new facts from known premises. while many representations
have been proposed that meet these criteria, the most mature is the language of    rst-order
logic.1

12.2.1 id118

the bare bones of logical meaning representation are boolean operations on propositions:

propositional symbols. greek symbols like    and    will be used to represent proposi-
tions, which are statements that are either true or false. for example,    may corre-
spond to the proposition, bagels are delicious.

boolean operators. we can build up more complex propositional formulas from boolean

operators. these include:

    negation     , which is true if    is false.
    conjunction,          , which is true if both    and    are true.
    disjunction,          , which is true if at least one of    and    is true
    implication,          , which is true unless    is true and    is false. implication
    equivalence,          , which is true if    and    are both true or both false. equiv-

has identical truth conditions to            .
alence has identical truth conditions to (         )     (         ).

it is not strictly necessary to have all    ve boolean operators: readers familiar with
boolean logic will know that it is possible to construct all other operators from either the
nand (not-and) or nor (not-or) operators. nonetheless, it is clearest to use all    ve
operators. from the truth conditions for these operators, it is possible to de   ne a number
of    laws    for these boolean operators, such as,

    commutativity:           =          ,           =          
    associativity:        (         ) = (         )       ,        (         ) = (         )       
    complementation:             =    ,             = (cid:62), where (cid:62) indicates a true proposition

and     indicates a false proposition.

1alternatives include the    variable-free    representation used in id29 of geographical
queries (zelle and mooney, 1996) and robotic control (ge and mooney, 2005), and dependency-based com-
positional semantics (liang et al., 2013).

under contract with mit press, shared under cc-by-nc-nd license.

288

chapter 12. logical semantics

these laws can be combined to derive further equivalences, which can support logical
id136s. for example, suppose    = the music is loud and    = max can   t sleep. then if
we are given,

          if the music is loud, max can   t sleep.

   the music is loud.

we can derive    (max can   t sleep) by application of modus ponens, which is one of a
set of id136 rules that can be derived from more basic laws and used to manipulate
propositional formulas. automated theorem provers are capable of applying id136
rules to a set of premises to derive desired propositions (loveland, 2016).

12.2.2 id85
id118 is so named because it treats propositions as its base units. however,
the criterion c4 states that our meaning representation should be suf   ciently expressive.
now consider the sentence pair,

(12.1)

if anyone is making noise, then max can   t sleep.
abigail is making noise.

people are capable of making id136s from this sentence pair, but such id136s re-
quire formal tools that are beyond id118. to understand the relationship
between the statement anyone is making noise and the statement abigail is making noise, our
meaning representation requires the additional machinery of    rst-order logic (fol).

in fol, logical propositions can be constructed from relationships between entities.

speci   cally, fol extends id118 with the following classes of terms:

relations. relations can be thought of as sets of entities, or sets of tuples. for example,
the relation can-sleep is de   ned as the set of entities who can sleep, and has the

constants. these are elements that name individual entities in the model, such as max
model, e.g.,(cid:74)max(cid:75) = m and(cid:74)abigail(cid:75) = a.
and abigail. the denotation of each constant in a model m is an element in the
denotation (cid:74)can-sleep(cid:75) = {a, m, . . .}. to test the truth value of the proposition
can-sleep(max), we ask whether(cid:74)max(cid:75)    (cid:74)can-sleep(cid:75). logical relations that are
such relations is a set of tuples, (cid:74)brother(cid:75) = {(m,a), (x,y), . . .}. to test the
((cid:74)max(cid:75),(cid:74)abigail(cid:75)) is in the denotation(cid:74)brother(cid:75).

de   ned over sets of entities are sometimes called properties.
relations may also be ordered tuples of entities. for example brother(max,abigail)
expresses the proposition that max is the brother of abigail. the denotation of

truth value of the proposition brother(max,abigail), we ask whether the tuple

jacob eisenstein. draft of november 13, 2018.

12.2. logical representations of meaning

289

using constants and relations, it is possible to express statements like max can   t sleep

and max is abigail   s brother:

  can-sleep(max)
brother(max,abigail).

these statements can also be combined using boolean operators, such as,

(brother(max,abigail)     brother(max,steve))       can-sleep(max).

this fragment of    rst-order logic permits only statements about speci   c entities. to
support id136s about statements like if anyone is making noise, then max can   t sleep,
two more elements must be added to the meaning representation:

variables. variables are mechanisms for referring to entities that are not locally speci   ed.
we can then write can-sleep(x) or brother(x, abigail). in these cases, x is a free
variable, meaning that we have not committed to any particular assignment.

quanti   ers. variables are bound by quanti   ers. there are two quanti   ers in    rst-order

logic.2

    the existential quanti   er    , which indicates that there must be at least one en-

tity to which the variable can bind. for example, the statement    xmakes-noise(x)
indicates that there is at least one entity for which makes-noise is true.

    the universal quanti   er    , which indicates that the variable must be able to

bind to any entity in the model. for example, the statement,

makes-noise(abigail)     (   x  can-sleep(x))

[12.3]

asserts that if abigail makes noise, no one can sleep.

the expressions    x and    x make x into a bound variable. a formula that contains
no free variables is a sentence.

functions. functions map from entities to entities, e.g.,(cid:74)capital-of(georgia)(cid:75) =(cid:74)atlanta(cid:75).

with functions, it is convenient to add an equality operator, supporting statements
like,

[12.4]
2in    rst-order logic, it is possible to quantify only over entities. in second-order logic, it is possible to
quantify over properties. this makes it possible to represent statements like butch has every property that a
good boxer has (example from blackburn and bos, 2005),

   x   ymother-of(x) = daughter-of(y).

   p   x((good-boxer(x)     p (x))     p (butch)).

[12.2]

under contract with mit press, shared under cc-by-nc-nd license.

290

chapter 12. logical semantics

note that mother-of is a functional analogue of the relation mother, so that
mother-of(x) = y if mother(x, y). any logical formula that uses functions can be
rewritten using only relations and quanti   cation. for example,

makes-noise(mother-of(abigail))

[12.5]

can be rewritten as    xmakes-noise(x)     mother(x, abigail).
an important property of quanti   ers is that the order can matter. unfortunately, natu-
ral language is rarely clear about this! the issue is demonstrated by examples like everyone
speaks a language, which has the following interpretations:

   x   y speaks(x, y)
   y   x speaks(x, y).

[12.6]
[12.7]

in the    rst case, y may refer to several different languages, while in the second case, there
is a single y that is spoken by everyone.

truth-conditional semantics

one way to look at the meaning of an fol sentence    is as a set of truth conditions,
or models under which    is satis   ed. but how to determine whether a sentence is true
or false in a given model? we will approach this inductively, starting with a predicate
applied to a tuple of constants. the truth of such a sentence depends on whether the
tuple of denotations of the constants is in the denotation of the predicate. for example,
capital(georgia,atlanta) is true in model m iff,

((cid:74)georgia(cid:75)m,(cid:74)atlanta(cid:75)m)    (cid:74)capital(cid:75)m.

[12.8]

the boolean operators    ,   , . . . provide ways to construct more complicated sentences,
and the truth of such statements can be assessed based on the truth tables associated with
these operators. the statement    x   is true if there is some assignment of the variable x
to an entity in the model such that    is true; the statement    x   is true if    is true under
all possible assignments of x. more formally, we would say that    is satis   ed under m,
written as m |=   .

truth conditional semantics allows us to de   ne several other properties of sentences
and pairs of sentences. suppose that in every m under which    is satis   ed, another
formula    is also satis   ed; then    entails   , which is also written as    |=   . for example,
[12.9]
a statement that is satis   ed under any model, such as            , is valid, written |= (      
    ). a statement that is not satis   ed under any model, such as            , is unsatis   able,
jacob eisenstein. draft of november 13, 2018.

capital(georgia,atlanta) |=    xcapital(georgia, x).

12.3. id29 and the id198

291

or inconsistent. a model checker is a program that determines whether a sentence   
is satis   ed in m. a model builder is a program that constructs a model in which   
is satis   ed. the problems of checking for consistency and validity in    rst-order logic
are undecidable, meaning that there is no algorithm that can automatically determine
whether an fol formula is valid or inconsistent.

id136 in    rst-order logic

our original goal was to support id136s that combine general statements if anyone is
making noise, then max can   t sleep with speci   c statements like abigail is making noise. we
can now represent such statements in    rst-order logic, but how are we to perform the
id136 that max can   t sleep? one approach is to use    generalized    versions of propo-
sitional id136 rules like modus ponens, which can be applied to fol formulas. by
repeatedly applying such id136 rules to a knowledge base of facts, it is possible to
produce proofs of desired propositions. to    nd the right sequence of id136s to derive
a desired theorem, classical arti   cial intelligence search algorithms like backward chain-
ing can be applied. such algorithms are implemented in interpreters for the prolog logic
programming language (pereira and shieber, 2002).

12.3 id29 and the id198

the previous section laid out a lot of formal machinery; the remainder of this chapter
links these formalisms back to natural language. given an english sentence like alex likes
brit, how can we obtain the desired    rst-order logical representation, likes(alex,brit)?
this is the task of id29. just as a syntactic parser is a function from a natu-
ral language sentence to a syntactic structure such as a phrase structure tree, a semantic
parser is a function from natural language to logical formulas.

as in syntactic analysis, id29 is dif   cult because the space of inputs and
outputs is very large, and their interaction is complex. our best hope is that, like syntactic
parsing, id29 can somehow be decomposed into simpler sub-problems. this
idea, usually attributed to the german philosopher gottlob frege, is called the principle
of compositionality: the meaning of a complex expression is a function of the meanings of
that expression   s constituent parts. we will de   ne these    constituent parts    as syntactic
constituents: noun phrases and verb phrases. these constituents are combined using
function application: if the syntactic parse contains the production x     y z, then the
semantics of x, written x.sem, will be computed as a function of the semantics of the

under contract with mit press, shared under cc-by-nc-nd license.

292

chapter 12. logical semantics

figure 12.1: the principle of compositionality requires that we identify meanings for the
constituents likes and likes brit that will make it possible to compute the meaning for the
entire sentence.

constituents, y.sem and z.sem.3 4

12.3.1 the id198
let   s see how this works for a simple sentence like alex likes brit, whose syntactic structure
is shown in figure 12.1. our goal is the formula, likes(alex,brit), and it is clear that the
meaning of the constituents alex and brit should be alex and brit. that leaves two more
constituents: the verb likes, and the verb phrase likes brit. the meanings of these units
must be de   ned in a way that makes it possible to recover the desired meaning for the
entire sentence by function application. if the meanings of alex and brit are constants,
then the meanings of likes and likes brit must be functional expressions, which can be
applied to their siblings to produce the desired analyses.

modeling these partial analyses requires extending the    rst-order logic meaning rep-
resentation. we do this by adding lambda expressions, which are descriptions of anony-
mous functions,5 e.g.,

  x.likes(x, brit).

[12.10]

this functional expression is the meaning of the verb phrase likes brit; it takes a single
argument, and returns the result of substituting that argument for x in the expression
3   9.3.2 brie   y discusses id35 (id35) as an alternative to a phrase-structure
analysis of syntax. id35 is argued to be particularly well-suited to id29 (hockenmaier and
steedman, 2007), and is used in much of the contemporary work on machine learning for id29,
summarized in    12.4.

4the approach of algorithmically building up meaning representations from a series of operations on the
syntactic structure of a sentence is generally attributed to the philosopher richard montague, who published
a series of in   uential papers on the topic in the early 1970s (e.g., montague, 1973).

5formally, all    rst-order logic formulas are lambda expressions; in addition, if    is a lambda expression,
then   x.   is also a lambda expression. readers who are familiar with functional programming will recognize
lambda expressions from their use in programming languages such as lisp and python.

jacob eisenstein. draft of november 13, 2018.

s:likes(alex,brit)vp:?np:britbritv:?likesnp:alexalex12.3. id29 and the id198

293

likes(x, brit). we write this substitution as,

(  x.likes(x, brit))@alex = likes(alex,brit),

[12.11]

with the symbol    @    indicating function application. function application in the lambda
calculus is sometimes called   -reduction or   -conversion. the expression   @   indicates
a function application to be performed by   -reduction, and   (  ) indicates a function or
predicate in the    nal logical form.

equation 12.11 shows how to obtain the desired semantics for the sentence alex likes
brit: by applying the lambda expression   x.likes(x, brit) to the logical constant alex.
this rule of composition can be speci   ed in a syntactic-semantic grammar, in which
syntactic productions are paired with semantic operations. for the syntactic production
s     np vp, we have the semantic rule vp.sem@np.sem.

the meaning of the transitive verb phrase likes brit can also be obtained by function
application on its syntactic constituents. for the syntactic production vp     v np, we
apply the semantic rule,

vp.sem =(v.sem)@np.sem

=(  y.  x.likes(x, y))@(brit)

=  x.likes(x, brit).

[12.12]
[12.13]
[12.14]

thus, the meaning of the transitive verb likes is a lambda expression whose output is
another lambda expression: it takes y as an argument to    ll in one of the slots in the likes
relation, and returns a lambda expression that is ready to take an argument to    ll in the
other slot.6

table 12.1 shows a minimal syntactic-semantic grammar fragment, g1. the complete
derivation of alex likes brit in g1 is shown in figure 12.2. in addition to the transitive
verb likes, the grammar also includes the intransitive verb sleeps; it should be clear how
to derive the meaning of sentences like alex sleeps. for verbs that can be either transitive
or intransitive, such as eats, we would have two terminal productions, one for each sense
(terminal productions are also called the lexical entries). indeed, most of the grammar is
in the lexicon (the terminal productions), since these productions select the basic units of
the semantic interpretation.

12.3.2 quanti   cation
things get more complicated when we move from sentences about named entities to sen-
tences that involve more general noun phrases. let   s consider the example, a dog sleeps,
6this can be written in a few different ways. the notation   y, x.likes(x, y) is a somewhat informal way to
indicate a lambda expression that takes two arguments; this would be acceptable in functional programming.
logicians (e.g., carpenter, 1997) often prefer the more formal notation   y.  x.likes(x)(y), indicating that each
lambda expression takes exactly one argument.

under contract with mit press, shared under cc-by-nc-nd license.

294

chapter 12. logical semantics

figure 12.2: derivation of the semantic representation for alex likes brit in the grammar
g1.

s
vp     vt np
vp     vi
vt     likes
vi     sleeps
np     alex
np     brit

    np vp vp.sem@np.sem
vt.sem@np.sem
vi.sem

  y.  x.likes(x, y)
  x.sleeps(x)
alex
brit

table 12.1: g1, a minimal syntactic-semantic context-free grammar

which has the meaning    xdog(x)     sleeps(x). clearly, the dog relation will be intro-
duced by the word dog, and the sleep relation will be introduced by the word sleeps.
the existential quanti   er     must be introduced by the lexical entry for the determiner a.7
however, this seems problematic for the compositional approach taken in the grammar
g1: if the semantics of the noun phrase a dog is an existentially quanti   ed expression, how
can it be the argument to the semantics of the verb sleeps, which expects an entity? and
where does the logical conjunction come from?

there are a few different approaches to handling these issues.8 we will begin by re-
versing the semantic relationship between subject nps and vps, so that the production
s     np vp has the semantics np.sem@vp.sem: the meaning of the sentence is now the
semantics of the noun phrase applied to the verb phrase. the implications of this change
are best illustrated by exploring the derivation of the example, shown in figure 12.3. let   s

7conversely, the sentence every dog sleeps would involve a universal quanti   er,    xdog(x)     sleeps(x).
the de   nite article the requires more consideration, since the dog must refer to some dog which is uniquely
identi   able, perhaps from contextual information external to the sentence. carpenter (1997, pp. 96-100)
summarizes recent approaches to handling de   nite descriptions.

8carpenter (1997) offers an alternative treatment based on id35.

jacob eisenstein. draft of november 13, 2018.

s:likes(alex,brit)vp:  x.likes(x,brit)np:britbritvt:  y.  x.likes(x,y)likesnp:alexalex12.3. id29 and the id198

295

figure 12.3: derivation of the semantic representation for a dog sleeps, in grammar g2

start with the inde   nite article a, to which we assign the rather intimidating semantics,

  p.  q.   xp (x)     q(x).

[12.15]

this is a lambda expression that takes two relations as arguments, p and q. the relation
p is scoped to the outer lambda expression, so it will be provided by the immediately
adjacent noun, which in this case is dog. thus, the noun phrase a dog has the semantics,

np.sem =det.sem@nn.sem

=(  p.  q.   xp (x)     q(x))@(dog)
=  q.   xdog(x)     q(x).

[12.16]
[12.17]
[12.18]

this is a lambda expression that is expecting another relation, q, which will be provided
by the verb phrase, sleeps. this gives the desired analysis,    xdog(x)     sleeps(x).9
if noun phrases like a dog are interpreted as lambda expressions, then proper nouns
like alex must be treated in the same way. this is achieved by type-raising from con-
stants to lambda expressions, x       p.p (x). after type-raising, the semantics of alex is
  p.p (alex)     a lambda expression that expects a relation to tell us something about
alex.10 again, make sure you see how the analysis in figure 12.3 can be applied to the
sentence alex sleeps.

9when applying   -reduction to arguments that are themselves lambda expressions, be sure to use unique
variable names to avoid confusion. for example, it is important to distinguish the x in the semantics for a
from the x in the semantics for likes. variable names are abstractions, and can always be changed     this is
known as   -conversion. for example,   x.p (x) can be converted to   y.p (y), etc.

10compositional semantic analysis is often supported by type systems, which make it possible to check
whether a given function application is valid. the base types are entities e and truth values t. a property,
such as dog, is a function from entities to truth values, so its type is written (cid:104)e, t(cid:105). a transitive verb has type
(cid:104)e,(cid:104)e, t(cid:105)(cid:105): after receiving the    rst entity (the direct object), it returns a function from entities to truth values,
which will be applied to the subject of the sentence. the type-raising operation x       p.p (x) corresponds
to a change in type from e to (cid:104)(cid:104)e, t(cid:105), t(cid:105): it expects a function from entities to truth values, and returns a truth
value.

under contract with mit press, shared under cc-by-nc-nd license.

s:   xdog(x)   sleeps(x)vp:  x.sleeps(x)vi:  x.sleeps(x)sleepsnp:  p.   xp(x)   dog(x)nn:dogdogdt:  q.  p.   x.p(x)   q(x)a296

chapter 12. logical semantics

figure 12.4: derivation of the semantic representation for a dog likes alex.

direct objects are handled by applying the same type-raising operation to transitive

verbs: the meaning of verbs such as likes is raised to,

[12.19]
as a result, we can keep the verb phrase production vp.sem = v.sem@np.sem, knowing
that the direct object will provide the function p in equation 12.19. to see how this works,
let   s analyze the verb phrase likes a dog. after uniquely relabeling each lambda variable,

  p.  x.p (  y.likes(x, y))

vp.sem =v.sem@np.sem

=(  p.  x.p (  y.likes(x, y)))@(  q.   zdog(z)     q(z))
=  x.(  q.   zdog(z)     q(z))@(  y.likes(x, y))
=  x.   zdog(z)     (  y.likes(x, y))@z
=  x.   zdog(z)     likes(x, z).

these changes are summarized in the revised grammar g2, shown in table 12.2. fig-
ure 12.4 shows a derivation that involves a transitive verb, an inde   nite noun phrase, and
a proper noun.

12.4 learning semantic parsers

as with syntactic parsing, any syntactic-semantic grammar with suf   cient coverage risks
producing many possible analyses for any given sentence. machine learning is the dom-
inant approach to selecting a single analysis. we will focus on algorithms that learn to
score logical forms by attaching weights to features of their derivations (zettlemoyer
and collins, 2005). alternative approaches include transition-based parsing (zelle and
mooney, 1996; misra and artzi, 2016) and methods inspired by machine translation (wong
and mooney, 2006). methods also differ in the form of supervision used for learning,
which can range from complete derivations to much more limited training signals. we
will begin with the case of complete supervision, and then consider how learning is still
possible even when seemingly key information is missing.

jacob eisenstein. draft of november 13, 2018.

s:   xdog(x)   likes(x,alex)vp:  x.likes(x,alex)np:  p.p(alex)nnp:alexalexvt:  p.  x.p(  y.likes(x,y))likesnp:  q.   xdog(x)   q(x)nn:dogdogdt:  p.  q.   xp(x)   q(x)a12.4. learning semantic parsers

297

np.sem@vp.sem
vt.sem@np.sem
vi.sem

s
    np vp
vp     vt np
vp     vi
np     det nn det.sem@nn.sem
np     nnp
det     a
det     every
    likes
vt
    sleeps
vi
nn     dog
nnp     alex
nnp     brit

  p.p (nnp.sem)
  p.  q.   xp (x)     q(x)
  p.  q.   x(p (x)     q(x))
  p.  x.p (  y.likes(x, y))
  x.sleeps(x)
dog
alex
brit

table 12.2: g2, a syntactic-semantic context-free grammar fragment, which supports
quanti   ed noun phrases

datasets early work on id29 focused on natural language expressions of
geographical database queries, such as what states border texas. the geoquery dataset
of zelle and mooney (1996) was originally coded in prolog, but has subsequently been
expanded and converted into the sql database query language by popescu et al. (2003)
and into    rst-order logic with id198 by zettlemoyer and collins (2005), pro-
viding logical forms like   x.state(x)     borders(x, texas). another early dataset con-
sists of instructions for robocup robot soccer teams (kate et al., 2005). more recent work
has focused on broader domains, such as the freebase database (bollacker et al., 2008),
for which queries have been annotated by krishnamurthy and mitchell (2012) and cai
and yates (2013). other recent datasets include child-directed speech (kwiatkowski et al.,
2012) and elementary school science exams (krishnamurthy, 2016).

12.4.1 learning from derivations
let w(i) indicate a sequence of text, and let y(i) indicate the desired logical form. for
example:

w(i) =alex eats shoots and leaves
y(i) =eats(alex,shoots)     eats(alex,leaves)

in the standard supervised learning paradigm that was introduced in    2.3, we    rst de-
   ne a feature function, f (w, y), and then learn weights on these features, so that y(i) =
argmaxy       f (w, y). the weight vector    is learned by comparing the features of the true
label f (w(i), y(i)) against either the features of the predicted label f (w(i),   y) (id88,

under contract with mit press, shared under cc-by-nc-nd license.

298

chapter 12. logical semantics

figure 12.5: derivation for gold semantic analysis of alex eats shoots and leaves

support vector machine) or the expected feature vector ey|w[f (w(i), y)] (logistic regres-
sion).

while this basic framework seems similar to discriminative syntactic parsing, there is
a crucial difference. in (context-free) syntactic parsing, the annotation y(i) contains all of
the syntactic productions; indeed, the task of identifying the correct set of productions
is identical to the task of identifying the syntactic structure. in id29, this is
not the case: the logical form eats(alex,shoots)     eats(alex,leaves) does not reveal
the syntactic-semantic productions that were used to obtain it. indeed, there may be spu-
rious ambiguity, so that a single logical form can be reached by multiple derivations.
(we previously encountered spurious ambiguity in transition-based id33,
   11.3.2.)
these ideas can be formalized by introducing an additional variable z, representing
the derivation of the logical form y from the text w. assume that the feature function de-
composes across the productions in the derivation, f (w, z, y) =(cid:80)t
t=1 f (w, zt, y), where
zt indicates a single syntactic-semantic production. for example, we might have a feature
for the production s     np vp : np.sem@vp.sem, as well as for terminal productions
like nnp     alex : alex. under this decomposition, it is possible to compute scores
for each semantically-annotated subtree in the analysis of w, so that bottom-up parsing
algorithms like cky (   10.1) can be applied to    nd the best-scoring semantic analysis.

figure 12.5 shows a derivation of the correct semantic analysis of the sentence alex
eats shoots and leaves, in a simpli   ed grammar in which the plural noun phrases shoots
and leaves are interpreted as logical constants shoots and leavesn. figure 12.6 shows a
derivation of an incorrect analysis. assuming one feature per production, the id88
update is shown in table 12.3. from this update, the parser would learn to prefer the
noun interpretation of leaves over the verb interpretation. it would also learn to prefer
noun phrase coordination over verb phrase coordination.

while the update is explained in terms of the id88, it would be easy to replace
the id88 with a conditional random    eld. in this case, the online updates would be

jacob eisenstein. draft of november 13, 2018.

s:eats(alex,shoots)   eats(alex,leavesn)vp:  x.eats(x,shoots)   eats(x,leavesn)np:  p.p(shoots)   p(leavesn)np:  p.p(leavesn)leavescc:  p.  q.  x.p(x)   q(x)andnp:  p.p(shoots)shootsvt:  p.  x.p(  y.eats(x,y))eatsnp:  p.p(alex)alex12.4. learning semantic parsers

299

figure 12.6: derivation for incorrect semantic analysis of alex eats shoots and leaves

np1     np2 cc np3
vp1     vp2 cc vp3
np     leaves
vp     vi
vi     leaves

(cc.sem@(np2.sem))@(np3.sem) +1
-1
(cc.sem@(vp2.sem))@(vp3.sem)
+1
leavesn
-1
vi.sem
-1
  x.leavesv

table 12.3: id88 update for analysis in figure 12.5 (gold) and figure 12.6 (predicted)

based on feature expectations, which can be computed using the inside-outside algorithm
(   10.6).
12.4.2 learning from logical forms
complete derivations are expensive to annotate, and are rarely available.11 one solution
is to focus on learning from logical forms directly, while treating the derivations as la-
tent variables (zettlemoyer and collins, 2005). in a conditional probabilistic model over
logical forms y and derivations z, we have,

p(y, z | w) =

exp(      f (w, z, y))

(cid:80)y(cid:48),z(cid:48) exp(      f (w, z(cid:48), y(cid:48)))

,

[12.20]

which is the standard log-linear model, applied to the logical form y and the derivation
z.

since the derivation z unambiguously determines the logical form y, it may seem silly
to model the joint id203 over y and z. however, since z is unknown, it can be
marginalized out,

p(y | w) =(cid:88)z

p(y, z | w).

[12.21]

11an exception is the work of ge and mooney (2005), who annotate the meaning of each syntactic con-

stituents for several hundred sentences.

under contract with mit press, shared under cc-by-nc-nd license.

s:eats(alex,shoots)   leavesv(alex)vp:  x.eats(x,shoots)   leavesv(x)vp:  x.leavesv(x)vi:  x.leavesv(x)leavescc:  p.  q.  x.p(x)   q(x)andvp:  x.eats(x,shoots)np:  p.p(shoots)shootsvt:  p.  x.p(  y.eats(x,y))eatsnp:  p.p(alex)alex300

chapter 12. logical semantics

the semantic parser can then select the logical form with the maximum log marginal
id203,

log(cid:88)z

p(y, z | w) = log(cid:88)z
    log(cid:88)z

exp(      f (w, z, y))

(cid:80) y(cid:48), z(cid:48) exp(      f (w, z(cid:48), y(cid:48)))

exp(      f (w, z

, y

))

(cid:48)

(cid:48)

    max

z

      f (w, z, y).

[12.22]

[12.23]

[12.24]

it is impossible to push the log term inside the sum over z, so our usual linear scoring
function does not apply. we can recover this scoring function only in approximation, by
taking the max (rather than the sum) over derivations z, which provides a lower bound.

learning can be performed by maximizing the log marginal likelihood,

(cid:96)(  ) =

=

n(cid:88)i=1
n(cid:88)i=1

log p(y(i) | w(i);   )
log(cid:88)z

p(y(i), z(i) | w(i);   ).

[12.25]

[12.26]

this log-likelihood is not convex in   , unlike the log-likelihood of a fully-observed condi-
tional random    eld. this means that learning can give different results depending on the
initialization.

the derivative of equation 12.26 is,

   (cid:96)i
     

p(z | y, w;   )f (w, z, y)    (cid:88)y(cid:48),z(cid:48)
=(cid:88)z
=ez|y,wf (w, z, y)     ey,z|wf (w, z, y)

p(y

(cid:48)

(cid:48)

, z

| w;   )f (w, z

(cid:48)

(cid:48)

)

, y

[12.27]

[12.28]

both expectations can be computed via bottom-up algorithms like inside-outside. al-
ternatively, we can again maximize rather than marginalize over derivations for an ap-
proximate solution. in either case, the    rst term of the gradient requires us to identify
derivations z that are compatible with the logical form y. this can be done in a bottom-
up id145 algorithm, by having each cell in the table t[i, j, x] include the
set of all possible logical forms for x (cid:32) wi+1:j. the resulting table may therefore be much
larger than in syntactic parsing. this can be controlled by using pruning to eliminate in-
termediate analyses that are incompatible with the    nal logical form y (zettlemoyer and
collins, 2005), or by using id125 and restricting the size of each cell to some    xed
constant (liang et al., 2013).

if we replace each expectation in equation 12.28 with argmax and then apply stochastic
id119 to learn the weights, we obtain the latent variable id88, a simple

jacob eisenstein. draft of november 13, 2018.

12.4. learning semantic parsers

301

       0
repeat

algorithm 16 latent variable id88
1: procedure latentvariableid88(w(1:n ), y(1:n ))
2:
3:
4:
5:
6:
7:
8:
9:

select an instance i
z(i)     argmaxz       f (w(i), z, y(i))
  y,   z     argmaxy(cid:48),z(cid:48)       f (w(i), z(cid:48), y(cid:48))
          + f (w(i), z(i), y(i))     f (w(i),   z,   y)

until tired
return   

and general algorithm for learning with missing data. the algorithm is shown in its most
basic form in algorithm 16, but the usual tricks such as averaging and margin loss can
be applied (yu and joachims, 2009). aside from id29, the latent variable
id88 has been used in tasks such as machine translation (liang et al., 2006) and
id39 (sun et al., 2009). in latent conditional random    elds, we use
the full expectations rather than maximizing over the hidden variable. this model has
also been employed in a range of problems beyond id29, including parse
reranking (koo and collins, 2005) and gesture recognition (quattoni et al., 2007).

12.4.3 learning from denotations

logical forms are easier to obtain than complete derivations, but the annotation of logical
forms still requires considerable expertise. however, it is relatively easy to obtain deno-
tations for many natural language sentences. for example, in the geography domain, the
denotation of a question would be its answer (clarke et al., 2010; liang et al., 2013):

text :what states border georgia?

logical form :  x.state(x)     border(x, georgia)
denotation :{alabama, florida, north carolina,

south carolina, tennessee}

similarly, in a robotic control setting, the denotation of a command would be an action or
sequence of actions (artzi and zettlemoyer, 2013). in both cases, the idea is to reward the
semantic parser for choosing an analysis whose denotation is correct: the right answer to
the question, or the right action.

learning from logical forms was made possible by summing or maxing over deriva-
tions. this idea can be carried one step further, summing or maxing over all logical forms
with the correct denotation. let vi(y)     {0, 1} be a validation function, which assigns a
under contract with mit press, shared under cc-by-nc-nd license.

302

chapter 12. logical semantics

binary score indicating whether the denotation(cid:74)y(cid:75) for the text w(i) is correct. we can then

learn by maximizing a conditional-likelihood objective,

(cid:96)(i)(  ) = log(cid:88)y
= log(cid:88)y

vi(y)    p(y | w;   )
vi(y)   (cid:88)z

p(y, z | w;   ),

[12.29]

[12.30]

which sums over all derivations z of all valid logical forms, {y : vi(y) = 1}. this cor-
responds to the log-id203 that the semantic parser produces a logical form with a
valid denotation.

differentiating with respect to   , we obtain,

   (cid:96)(i)
     

= (cid:88)y,z:vi(y)=1

p(y, z | w)f (w, z, y)    (cid:88)y(cid:48),z(cid:48)

(cid:48)

(cid:48)

, z

p(y

| w)f (w, z

(cid:48)

(cid:48)

),

, y

[12.31]

which is the usual difference in feature expectations. the positive term computes the
expected feature expectations conditioned on the denotation being valid, while the second
term computes the expected feature expectations according to the current model, without
regard to the ground truth. large-margin learning formulations are also possible for this
problem. for example, artzi and zettlemoyer (2013) generate a set of valid and invalid
derivations, and then impose a constraint that all valid derivations should score higher
than all invalid derivations. this constraint drives a id88-like learning rule.

additional resources

a key issue not considered here is how to handle semantic underspeci   cation: cases in
which there are multiple semantic interpretations for a single syntactic structure. quanti-
   er scope ambiguity is a classic example. blackburn and bos (2005) enumerate a number
of approaches to this issue, and also provide links between natural language semantics
and computational id136 techniques. much of the contemporary research on semantic
parsing uses the framework of id35 (id35). carpenter (1997)
provides a comprehensive treatment of how id35 can support compositional semantic
analysis. another recent area of research is the semantics of multi-sentence texts. this can
be handled with models of dynamic semantics, such as dynamic predicate logic (groe-
nendijk and stokhof, 1991).

alternative readings on formal semantics include an    informal    reading from levy
and manning (2009), and a more involved introduction from briscoe (2011). to learn more
about ongoing research on data-driven id29, readers may consult the survey

jacob eisenstein. draft of november 13, 2018.

12.4. learning semantic parsers

303

article by liang and potts (2015), tutorial slides and videos by artzi and zettlemoyer
(2013),12 and the source code by yoav artzi13 and percy liang.14

exercises

1. the modus ponens id136 rule states that if we know           and   , then    must
be true. justify this rule, using the de   nition of the     operator and some of the laws
provided in    12.2.1, plus one additional identity:            =   .

2. convert the following examples into    rst-order logic, using the relations can-sleep,

makes-noise, and brother.

    if abigail makes noise, no one can sleep.
    if abigail makes noise, someone cannot sleep.
    none of abigail   s brothers can sleep.
    if one of abigail   s brothers makes noise, abigail cannot sleep.

3. extend the grammar fragment g1 to include the ditransitive verb teaches and the
proper noun swahili. show how to derive the interpretation for the sentence alex
teaches brit swahili, which should be teaches(alex,brit,swahili). the grammar
need not be in chomsky normal form. for the ditransitive verb, use np1 and np2
to indicate the two direct objects.

4. derive the semantic interpretation for the sentence alex likes every dog, using gram-

mar fragment g2.

5. extend the grammar fragment g2 to handle adjectives, so that the meaning of an
angry dog is   p.   xdog(x)     angry(x)     p (x). speci   cally, you should supply the
lexical entry for the adjective angry, and you should specify the syntactic-semantic
productions np     det nom, nom     jj nom, and nom     nn.

6. extend your answer to the previous question to cover copula constructions with

predicative adjectives, such as alex is angry. the interpretation should be angry(alex).
you should add a verb phrase production vp     vcop jj, and a terminal production
vcop     is. show why your grammar extensions result in the correct interpretation.
7. in figure 12.5 and figure 12.6, we treat the plurals shoots and leaves as entities. revise
g2 so that the interpretation of alex eats leaves is    x.(leaf(x)     eats(alex, x)), and
show the resulting id88 update.

12videos are currently available at http://yoavartzi.com/tutorial/
13http://yoavartzi.com/spf
14https://github.com/percyliang/sempre

under contract with mit press, shared under cc-by-nc-nd license.

304

chapter 12. logical semantics

8. statements like every student eats a pizza have two possible interpretations, depend-

ing on quanti   er scope:

   x   ypizza(y)     (student(x)     eats(x, y))
   y   xpizza(y)     (student(x)     eats(x, y))

[12.32]
[12.33]

a) explain why these interpretations really are different.
b) which is generated by grammar g2? note that you may have to manipulate

the logical form to exactly align with the grammar.

9. *modify g2 so that produces the second interpretation in the previous problem.
hint: one possible solution involves changing the semantics of the sentence pro-
duction and one other production.

10. in the geoquery domain, give a natural language query that has multiple plausible
semantic interpretations with the same denotation. list both interpretaions and the
denotation.

hint: there are many ways to do this, but one approach involves using toponyms
(place names) that could plausibly map to several different entities in the model.

jacob eisenstein. draft of november 13, 2018.

chapter 13

predicate-argument semantics

this chapter considers more    lightweight    semantic representations, which discard some
aspects of    rst-order logic, but focus on predicate-argument structures. let   s begin by
thinking about the semantics of events, with a simple example:

(13.1) asha gives boyang a book.

a    rst-order logical representation of this sentence is,

   x.book(x)     give(asha, boyang, x)

[13.1]

in this representation, we de   ne variable x for the book, and we link the strings asha and
boyang to entities asha and boyang. because the action of giving involves a giver, a
recipient, and a gift, the predicate give must take three arguments.
now suppose we have additional information about the event:

(13.2) yesterday, asha reluctantly gave boyang a book.

one possible to solution is to extend the predicate give to take additional arguments,

   x.book(x)     give(asha, boyang, x, yesterday, reluctantly)

[13.2]

but this is clearly unsatisfactory: yesterday and relunctantly are optional arguments,
and we would need a different version of the give predicate for every possible combi-
nation of arguments. event semantics solves this problem by reifying the event as an
existentially quanti   ed variable e,

   e, x.give-event(e)     giver(e, asha)     gift(e, x)     book(e, x)     recipient(e, boyang)

    time(e, yesterday)     manner(e, reluctantly)

305

306

chapter 13. predicate-argument semantics

in this way, each argument of the event     the giver, the recipient, the gift     can be rep-
resented with a relation of its own, linking the argument to the event e. the expression
giver(e, asha) says that asha plays the role of giver in the event. this reformulation
handles the problem of optional information such as the time or manner of the event,
which are called adjuncts. unlike arguments, adjuncts are not a mandatory part of the
relation, but under this representation, they can be expressed with additional logical rela-
tions that are conjoined to the semantic interpretation of the sentence. 1

the event semantic representation can be applied to nested clauses, e.g.,

(13.3) chris sees asha pay boyang.

this is done by using the event variable as an argument:

   e1   e2 see-event(e1)     seer(e1, chris)     sight(e1, e2)

    pay-event(e2)     payer(e2, asha)     payee(e2, boyang)

[13.3]

as with    rst-order logic, the goal of event semantics is to provide a representation that

generalizes over many surface forms. consider the following paraphrases of (13.1):

(13.4)

a. asha gives a book to boyang.
b. a book is given to boyang by asha.
c. a book is given by asha to boyang.
d. the gift of a book from asha to boyang . . .

all have the same event semantic meaning as equation 13.1, but the ways in which the
meaning can be expressed are diverse. the    nal example does not even include a verb:
events are often introduced by verbs, but as shown by (13.4d), the noun gift can introduce
the same predicate, with the same accompanying arguments.

id14 (srl) is a relaxed form of id29, in which each
semantic role is    lled by a set of tokens from the text itself. this is sometimes called
   shallow semantics    because, unlike model-theoretic id29, role    llers need
not be symbolic expressions with denotations in some world model. a semantic role
labeling system is required to identify all predicates, and then specify the spans of text
that    ll each role. to give a sense of the task, here is a more complicated example:

(13.5) boyang wants asha to give him a linguistics book.

1this representation is often called neo-davidsonian event semantics. the use of existentially-
quanti   ed event variables was proposed by davidson (1967) to handle the issue of optional adjuncts. in
neo-davidsonian semantics, this treatment of adjuncts is extended to mandatory arguments as well (e.g.,
parsons, 1990).

jacob eisenstein. draft of november 13, 2018.

13.1. semantic roles

307

in this example, there are two predicates, expressed by the verbs want and give. thus, a
semantic role labeler might return the following output:

    (predicate : wants, wanter : boyang, desire : asha to give him a linguistics book)
    (predicate : give, giver : asha, recipient : him, gift : a linguistics book)

boyang and him may refer to the same person, but the id14 is not re-
quired to resolve this reference. other predicate-argument representations, such as ab-
stract meaning representation (amr), do require reference resolution. we will return to
amr in    13.3, but    rst, let us further consider the de   nition of semantic roles.
13.1 semantic roles

in event semantics, it is necessary to specify a number of additional logical relations to
link arguments to events: giver, recipient, seer, sight, etc. indeed, every predicate re-
quires a set of logical relations to express its own arguments. in contrast, adjuncts such as
time and manner are shared across many types of events. a natural question is whether
it is possible to treat mandatory arguments more like adjuncts, by identifying a set of
generic argument types that are shared across many event predicates. this can be further
motivated by examples involving related verbs:

(13.6)

a. asha gave boyang a book.
b. asha loaned boyang a book.
c. asha taught boyang a lesson.
d. asha gave boyang a lesson.

the respective roles of asha, boyang, and the book are nearly identical across the    rst
two examples. the third example is slightly different, but the fourth example shows that
the roles of giver and teacher can be viewed as related.

one way to think about the relationship between roles such as giver and teacher is
by enumerating the set of properties that an entity typically possesses when it ful   lls these
roles: givers and teachers are usually animate (they are alive and sentient) and volitional
(they choose to enter into the action).2 in contrast, the thing that gets loaned or taught is
usually not animate or volitional; furthermore, it is unchanged by the event.

building on these ideas, thematic roles generalize across predicates by leveraging the
shared semantic properties of typical role    llers (fillmore, 1968). for example, in exam-
ples (13.6a-13.6d), asha plays a similar role in all four sentences, which we will call the
2there are always exceptions. for example, in the sentence the c programming language has taught me a
lot about perseverance, the    teacher    is the the c programming language, which is presumably not animate or
volitional.

under contract with mit press, shared under cc-by-nc-nd license.

308

chapter 13. predicate-argument semantics

asha
agent

verbnet
propbank arg0: giver
framenet donor

asha
agent

verbnet
propbank arg0: teacher
framenet teacher

gave

boyang
recipient
arg2: entity given to arg1: thing given
recipient

a book
theme

theme

taught boyang

recipient
arg2: student
student

algebra
topic
arg1: subject
subject

figure 13.1: example semantic annotations according to verbnet, propbank, and
framenet

agent. this re   ects several shared semantic properties: she is the one who is actively and
intentionally performing the action, while boyang is a more passive participant; the book
and the lesson would play a different role, as non-animate participants in the event.

example annotations from three well known systems are shown in figure 13.1. we

will now discuss these systems in more detail.

13.1.1 verbnet

verbnet (kipper-schuler, 2005) is a lexicon of verbs, and it includes thirty    core    thematic
roles played by arguments to these verbs. here are some example roles, accompanied by
their de   nitions from the verbnet guidelines.3

    agent:    actor in an event who initiates and carries out the event intentionally or

consciously, and who exists independently of the event.   

    patient:    undergoer in an event that experiences a change of state, location or
condition, that is causally involved or directly affected by other participants, and
exists independently of the event.   

    recipient:    destination that is animate   
    theme:    undergoer that is central to an event or state that does not have control
over the way the event occurs, is not structurally changed by the event, and/or is
characterized as being in a certain position or condition throughout the state.   

    topic:    theme characterized by information content transferred to another partic-

ipant.   

3http://verbs.colorado.edu/verb-index/verbnet_guidelines.pdf

jacob eisenstein. draft of november 13, 2018.

13.1. semantic roles

309

verbnet roles are organized in a hierarchy, so that a topic is a type of theme, which in
turn is a type of undergoer, which is a type of participant, the top-level category.

in addition, verbnet organizes verb senses into a class hierarchy, in which verb senses
that have similar meanings are grouped together. recall from    4.2 that multiple meanings
of the same word are called senses, and that id138 identi   es senses for many english
words. verbnet builds on id138, so that verb classes are identi   ed by the id138
senses of the verbs that they contain. for example, the verb class give-13.1 includes
the    rst id138 sense of loan and the second id138 sense of lend.

each verbnet class or subclass takes a set of thematic roles. for example, give-13.1
takes arguments with the thematic roles of agent, theme, and recipient;4 the pred-
icate teach takes arguments with the thematic roles agent, topic, recipient, and
source.5 so according to verbnet, asha and boyang play the roles of agent and recip-
ient in the sentences,

(13.7)

a. asha gave boyang a book.
b. asha taught boyang algebra.

the book and algebra are both themes, but algebra is a subcategory of theme     a topic
    because it consists of information content that is given to the receiver.

13.1.2 proto-roles and propbank

detailed thematic role inventories of the sort used in verbnet are not universally accepted.
for example, dowty (1991, pp. 547) notes that    linguists have often found it hard to agree
on, and to motivate, the location of the boundary between role types.    he argues that a
solid distinction can be identi   ed between just two proto-roles:

proto-agent. characterized by volitional involvement in the event or state; sentience
and/or perception; causing an event or change of state in another participant; move-
ment; exists independently of the event.

proto-patient. undergoes change of state; causally affected by another participant; sta-
tionary relative to the movement of another participant; does not exist indepen-
dently of the event.6

4https://verbs.colorado.edu/verb-index/vn/give-13.1.php
5https://verbs.colorado.edu/verb-index/vn/transfer_mesg-37.1.1.php
6reisinger et al. (2015) ask crowd workers to annotate these properties directly,    nding that annotators
tend to agree on the properties of each argument. they also    nd that in english, arguments having more
proto-agent properties tend to appear in subject position, while arguments with more proto-patient proper-
ties appear in object position.

under contract with mit press, shared under cc-by-nc-nd license.

310

chapter 13. predicate-argument semantics

in the examples in figure 13.1, asha has most of the proto-agent properties: in giving
the book to boyang, she is acting volitionally (as opposed to boyang got a book from asha, in
which it is not clear whether asha gave up the book willingly); she is sentient; she causes a
change of state in boyang; she exists independently of the event. boyang has some proto-
agent properties: he is sentient and exists independently of the event. but he also has
some proto-patient properties: he is the one who is causally affected and who undergoes
change of state. the book that asha gives boyang has even fewer of the proto-agent
properties: it is not volitional or sentient, and it has no causal role. but it also lacks many
of the proto-patient properties: it does not undergo change of state, exists independently
of the event, and is not stationary.

the proposition bank, or propbank (palmer et al., 2005), builds on this basic agent-
patient distinction, as a middle ground between generic thematic roles and roles that are
speci   c to each predicate. each verb is linked to a list of numbered arguments, with arg0
as the proto-agent and arg1 as the proto-patient. additional numbered arguments are
verb-speci   c. for example, for the predicate teach,7 the arguments are:

    arg0: the teacher
    arg1: the subject
    arg2: the student(s)

verbs may have any number of arguments: for example, want and get have    ve, while
eat has only arg0 and arg1. in addition to the semantic arguments found in the frame
   les, roughly a dozen general-purpose adjuncts may be used in combination with any
verb. these are shown in table 13.1.

propbank-style id14 is annotated over the entire id32. this

annotation includes the sense of each verbal predicate, as well as the argument spans.

13.1.3 framenet

semantic frames are descriptions of situations or events. frames may be evoked by one
of their lexical units (often a verb, but not always), and they include some number of
frame elements, which are like roles (fillmore, 1976). for example, the act of teaching
is a frame, and can be evoked by the verb taught; the associated frame elements include
the teacher, the student(s), and the subject being taught. frame semantics has played a
signi   cant role in the history of arti   cial intelligence, in the work of minsky (1974) and
schank and abelson (1977). in natural language processing, the theory of frame semantics
has been implemented in framenet (fillmore and baker, 2009), which consists of a lexicon

7http://verbs.colorado.edu/propbank/framesets-english-aliases/teach.html

jacob eisenstein. draft of november 13, 2018.

13.1. semantic roles

311

time
location

general purpose

tmp
loc
mod modal verb
adv
mnr manner
dis
prp
dir
neg
ext
cau

discourse connective
purpose
direction
negation
extent
cause

boyang ate a bagel [am-tmp yesterday].
asha studies in [am-loc stuttgart]
asha [am-mod will] study in stuttgart
[am-adv luckily], asha knew algebra.
asha ate [am-mnr aggressively].
[am-dis however], asha prefers algebra.
barry studied [am-prp to pass the bar].
workers dumped burlap sacks [am-dir into a bin].
asha does [am-neg not] speak albanian.
prices increased [am-ext 4%].
boyang returned the book [am-cau because it was overdue].

table 13.1: propbank adjuncts (palmer et al., 2005), sorted by frequency in the corpus

of roughly 1000 frames, and a corpus of more than 200,000    exemplar sentences,    in which
the frames and their elements are annotated.8

rather than seeking to link semantic roles such as teacher and giver into the-
matic roles such as agent, framenet aggressively groups verbs into frames, and links
semantically-related roles across frames. for example, the following two sentences would
be annotated identically in framenet:

(13.8)

a. asha taught boyang algebra.
b. boyang learned algebra from asha.

this is because teach and learn are both lexical units in the education teaching frame.
furthermore, roles can be shared even when the frames are distinct, as in the following
two examples:

(13.9)

a. asha gave boyang a book.
b. boyang got a book from asha.

the giving and getting frames both have recipient and theme elements, so boyang
and the book would play the same role. asha   s role is different: she is the donor in the
giving frame, and the source in the getting frame. framenet makes extensive use of
multiple inheritance to share information across frames and frame elements: for example,
the commerce sell and lending frames inherit from giving frame.

8current details and data can be found at https://framenet.icsi.berkeley.edu/

under contract with mit press, shared under cc-by-nc-nd license.

312

chapter 13. predicate-argument semantics

13.2 id14

the task of id14 is to identify the parts of the sentence comprising the
semantic roles. in english, this task is typically performed on the propbank corpus, with
the goal of producing outputs in the following form:

(13.10)

[arg0 asha] [give.01 gave] [arg2 boyang   s mom] [arg1 a book] [am-tmp yesterday].

note that a single sentence may have multiple verbs, and therefore a given word may be
part of multiple role-   llers:

(13.11)

[want.01 wanted]
wanted

[arg0 asha]
asha
[arg1 boyang to give her the book].
[arg0 boyang] [give.01 to give] [arg2 her] [arg1 the book].

13.2.1 id14 as classi   cation
propbank is annotated on the id32, and annotators used phrasal constituents
(   9.2.2) to    ll the roles. propbank id14 can be viewed as the task of as-
signing to each phrase a label from the set r = {   , pred, arg0, arg1, arg2, . . . , am-loc, am-tmp, . . . }
with respect to each predicate. if we treat id14 as a classi   cation prob-
lem, we obtain the following functional form:

  y(i,j) = argmax

y

  (w, y, i, j,   ,    ),

[13.4]

where,

    (i, j) indicates the span of a phrasal constituent (wi+1, wi+2, . . . , wj);9
    w represents the sentence as a sequence of tokens;
       is the index of the predicate verb in w;
       is the structure of the phrasal constituent parse of w.
early work on id14 focused on discriminative feature-based models,
where   (w, y, i, j,   ,    ) =       f (w, y, i, j,   ,    ). table 13.2 shows the features used in a sem-
inal paper on framenet id14 (gildea and jurafsky, 2002). by 2005 there

9propbank roles can also be    lled by split constituents, which are discontinuous spans of text. this
[arg1 by addressing these problems], mr. maxwell said,
situation most frequently in reported speech, e.g.
[arg1 the new funds have become extremely attractive.] (example adapted from palmer et al., 2005). this issue
is typically addressed by de   ning    continuation arguments   , e.g. c-arg1, which refers to the continuation
of arg1 after the split.

jacob eisenstein. draft of november 13, 2018.

13.2. id14

313

predicate lemma and
pos tag
voice

phrase type

headword and pos
tag
position

syntactic path

subcategorization

the lemma of the predicate verb and its part-of-speech tag

whether the predicate is in active or passive voice, as deter-
mined by a set of syntactic patterns for identifying passive
voice constructions
the constituent phrase type for the proposed argument in
the parse tree, e.g. np, pp
the head word of the proposed argument and its pos tag,
identi   ed using the collins (1997) rules
whether the proposed argument comes before or after the
predicate in the sentence
the set of steps on the parse tree from the proposed argu-
ment to the predicate (described in detail in the text)
the syntactic production from the    rst branching node
above the predicate.
in figure 13.2, the
subcategorization feature around taught would be vp    
vbd np pp.

for example,

table 13.2: features used in id14 by gildea and jurafsky (2002).

were several systems for propbank id14, and their approaches and fea-
ture sets are summarized by carreras and m`arquez (2005). typical features include: the
phrase type, head word, part-of-speech, boundaries, and neighbors of the proposed argu-
ment wi+1:j; the word, lemma, part-of-speech, and voice of the verb w   (active or passive),
as well as features relating to its frameset; the distance and path between the verb and
the proposed argument. in this way, id14 systems are high-level    con-
sumers    in the nlp stack, using features produced from lower-level components such as
part-of-speech taggers and parsers. more comprehensive feature sets are enumerated by
das et al. (2014) and t  ackstr  om et al. (2015).

a particularly powerful class of features relate to the syntactic path between the ar-
gument and the predicate. these features capture the sequence of moves required to get
from the argument to the verb by traversing the phrasal constituent parse of the sentence.
the idea of these features is to capture syntactic regularities in how various arguments
are realized. syntactic path features are best illustrated by example, using the parse tree
in figure 13.2:

    the path from asha to the verb taught is nnp   np   s   vp   vbd. the    rst part of
the path, nnp   np   s, means that we must travel up the parse tree from the nnp
tag (proper noun) to the s (sentence) constituent. the second part of the path,
s   vp   vbd, means that we reach the verb by producing a vp (verb phrase) from
under contract with mit press, shared under cc-by-nc-nd license.

314

chapter 13. predicate-argument semantics

figure 13.2: id14 on the phrase-structure parse tree for a sentence. the
dashed line indicates the syntactic path from asha to the predicate verb taught.

the s constituent, and then by producing a vbd (past tense verb). this feature is
consistent with asha being in subject position, since the path includes the sentence
root s.

    the path from the class to taught is np   vp   vbd. this is consistent with the class
being in object position, since the path passes through the vp node that dominates
the verb taught.

because there are many possible path features, it can also be helpful to look at smaller
parts: for example, the upward and downward parts can be treated as separate features;
another feature might consider whether s appears anywhere in the path.

rather than using the constituent parse, it is also possible to build features from the de-
pendency path (see    11.4) between the head word of each argument and the verb (prad-
han et al., 2005). using the universal dependency part-of-speech tagset and dependency
relations (nivre et al., 2016), the dependency path from asha to taught is propn    nsubj
verb,
with asha. similarly, the dependency
because taught is the head of a relation of type    nsubj
path from class to taught is noun    dobj
verb, because class heads the noun phrase that is a
direct object of taught. a more interesting example is asha wanted to teach the class, where
the path from asha to teach is propn    nsubj
verb. the right-facing arrow in sec-
ond relation indicates that wanted is the head of its xcomp relation with teach.

verb    xcomp

jacob eisenstein. draft of november 13, 2018.

svppp(arg1)nnalgebrainaboutnp(arg2)nnclassdetthevbdtaughtnpnnp(arg0)asha13.2. id14

315

13.2.2 id14 as constrained optimization
a potential problem with treating srl as a classi   cation problem is that there are a num-
ber of sentence-level constraints, which a classi   er might violate.

    for a given verb, there can be only one argument of each type (arg0, arg1, etc.)
    arguments cannot overlap. this problem arises when we are labeling the phrases
in a constituent parse tree, as shown in figure 13.2: if we label the pp about algebra
as an argument or adjunct, then its children about and algebra must be labeled as    .
the same constraint also applies to the syntactic ancestors of this phrase.

these constraints introduce dependencies across labeling decisions. in structure pre-
diction problems such as sequence labeling and parsing, such dependencies are usually
handled by de   ning a scoring over the entire structure, y. ef   cient id136 requires
that the global score decomposes into local parts: for example, in sequence labeling, the
scoring function decomposes into scores of pairs of adjacent tags, permitting the applica-
tion of the viterbi algorithm for id136. but the constraints that arise in semantic role
labeling are less amenable to local decomposition.10 we therefore consider constrained
optimization as an alternative solution.

let the set c(   ) refer to all labelings that obey the constraints introduced by the parse
  . the id14 problem can be reformulated as a constrained optimization
over y     c(   ),

  (w, yi,j, i, j,   ,    )

max

y (cid:88)(i,j)     
s.t. y     c(   ).

in this formulation, the objective (shown on the    rst line) is a separable function of each
individual labeling decision, but the constraints (shown on the second line) apply to the

overall labeling. the sum(cid:80)(i,j)      indicates that we are summing over all constituent
spans in the parse   . the expression s.t. in the second line means that we maximize the
objective subject to the constraint y     c(   ).

a number of practical algorithms exist for restricted forms of constrained optimiza-
tion. one such restricted form is integer id135, in which the objective and
constraints are linear functions of integer variables. to formulate srl as an integer linear
program, we begin by rewriting the labels as a set of binary variables z = {zi,j,r} (pun-
yakanok et al., 2008),

zi,j,r =(cid:40)1,

yi,j = r

0, otherwise,

10id145 solutions have been proposed by tromble and eisner (2006) and t  ackstr  om et al.

(2015), but they involves creating a trellis structure whose size is exponential in the number of labels.

under contract with mit press, shared under cc-by-nc-nd license.

[13.5]

[13.6]

316

chapter 13. predicate-argument semantics

where r     r is a label in the set {arg0, arg1, . . . , am-loc, . . . ,    }. thus, the variables
z are a binarized version of the id14 y.

the objective can then be formulated as a linear function of z.

(cid:88)(i,j)     

  (w, yi,j, i, j,   ,    ) =(cid:88)i,j,r

  (w, r, i, j,   ,    )    zi,j,r,

[13.7]

which is the sum of the scores of all relations, as indicated by zi,j,r.

constraints
integer id135 permits linear inequality constraints, of the
general form az     b, where the parameters a and b de   ne the constraints. to make
this more concrete, let   s start with the constraint that each non-null role type can occur
only once in a sentence. this constraint can be written,

   r (cid:54)=    , (cid:88)(i,j)     

zi,j,r     1.

[13.8]

recall that zi,j,r = 1 iff the span (i, j) has label r; this constraint says that for each possible
label r (cid:54)=    , there can be at most one (i, j) such that zi,j,r = 1. rewriting this constraint
can be written in the form az     b, as you will    nd if you complete the exercises at the
end of the chapter.

now consider the constraint that labels cannot overlap. let   s de   ne the convenience
function o((i, j), (i(cid:48), j(cid:48))) = 1 iff (i, j) overlaps (i(cid:48), j(cid:48)), and zero otherwise. thus, o will
indicate if a constituent (i(cid:48), j(cid:48)) is either an ancestor or descendant of (i, j). the constraint
is that if two constituents overlap, only one can have a non-null label:

   (i, j)       , (cid:88)(i(cid:48),j(cid:48))     (cid:88)r(cid:54)=   

(cid:48)
o((i, j), (i

, j

(cid:48)

))    zi(cid:48),j(cid:48),r     1,

[13.9]

where o((i, j), (i, j)) = 1.

in summary, the id14 problem can thus be rewritten as the following

integer linear program,

max

z   {0,1}|  | (cid:88)(i,j)     (cid:88)r   r

zi,j,r  i,j,r

s.t.    r (cid:54)=    , (cid:88)(i,j)     

zi,j,r     1.

   (i, j)       , (cid:88)(i(cid:48),j(cid:48))     (cid:88)r(cid:54)=   

(cid:48)
o((i, j), (i

, j

(cid:48)

))    zi(cid:48),j(cid:48),r     1.

[13.10]

[13.11]

[13.12]

jacob eisenstein. draft of november 13, 2018.

13.2. id14

317

learning with constraints learning can be performed in the context of constrained op-
timization using the usual id88 or large-margin classi   cation updates. because
constrained id136 is generally more time-consuming, a key question is whether it is
necessary to apply the constraints during learning. chang et al. (2008)    nd that better per-
formance can be obtained by learning without constraints, and then applying constraints
only when using the trained model to predict semantic roles for unseen data.

how important are the constraints? das et al. (2014)    nd that an unconstrained, classi   cation-
based method performs nearly as well as constrained optimization for framenet parsing:
while it commits many violations of the    no-overlap    constraint, the overall f1 score is
less than one point worse than the score at the constrained optimum. similar results
were obtained for propbank id14 by punyakanok et al. (2008). he et al.
(2017)    nd that constrained id136 makes a bigger impact if the constraints are based
on manually-labeled    gold    syntactic parses. this implies that errors from the syntac-
tic parser may limit the effectiveness of the constraints. punyakanok et al. (2008) hedge
against parser error by including constituents from several different parsers; any con-
stituent can be selected from any parse, and additional constraints ensure that overlap-
ping constituents are not selected.

implementation integer id135 solvers such as glpk,11 cplex,12 and gurobi13
allow inequality constraints to be expressed directly in the problem de   nition, rather than
in the matrix form az     b. the time complexity of integer id135 is theoret-
ically exponential in the number of variables |z|, but in practice these off-the-shelf solvers
obtain good solutions ef   ciently. using a standard desktop computer, das et al. (2014)
report that the cplex solver requires 43 seconds to perform id136 on the framenet
test set, which contains 4,458 predicates.

recent work has shown that many constrained optimization problems in natural lan-
guage processing can be solved in a highly parallelized fashion, using optimization tech-
niques such as id209, which are capable of exploiting the underlying prob-
lem structure (rush et al., 2010). das et al. (2014) apply this technique to framenet se-
mantic role labeling, obtaining an order-of-magnitude speedup over cplex.

13.2.3 neural id14

neural network approaches to srl have tended to treat it as a sequence labeling task,
using a labeling scheme such as the bio notation, which we previously saw in named
entity recognition (   8.3). in this notation, the    rst token in a span of type arg1 is labeled

11https://www.gnu.org/software/glpk/
12https://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/
13http://www.gurobi.com/

under contract with mit press, shared under cc-by-nc-nd license.

318

chapter 13. predicate-argument semantics

b-arg1; all remaining tokens in the span are inside, and are therefore labeled i-arg1.
tokens outside any argument are labeled o. for example:

(13.12) asha

b-arg0

taught
pred

boyang
b-arg2

   s
i-arg2

mom
i-arg2

about
b-arg1

algebra
i-arg1

recurrent neural networks (   7.6) are a natural approach to this tagging task. for
example, zhou and xu (2015) apply a deep bidirectional multilayer lstm (see    7.6) to
propbank id14. in this model, each bidirectional lstm serves as input
for another, higher-level bidirectional lstm, allowing complex non-linear transforma-
tions of the original input embeddings, x = [x1, x2, . . . , xm ]. the hidden state of the    nal
, . . . , z(k)
lstm is z(k) = [z(k)
, z(k)
m ]. the    emission    score for each tag ym = y is equal
to the inner product   y    z(k)
m , and there is also a transition score for each pair of adjacent
tags. the complete model can be written,

1

2

z(1) =bilstm(x)
z(i) =bilstm(z(i   1))

  y = argmax

y

m(cid:88)m   1

  (y)z(k)

m +   ym   1,ym.

[13.13]
[13.14]

[13.15]

note that the    nal step maximizes over the entire labeling y, and includes a score for
each tag transition   ym   1,ym. this combination of lstm and pairwise potentials on tags
is an example of an lstm-crf. the maximization over y is performed by the viterbi
algorithm.

this model strongly outperformed alternative approaches at the time, including con-
strained decoding and convolutional neural networks.14 more recent work has combined
recurrent neural network models with constrained decoding, using the a    search algo-
rithm to search over labelings that are feasible with respect to the constraints (he et al.,
2017). this yields small improvements over the method of zhou and xu (2015). he et al.
(2017) obtain larger improvements by creating an ensemble of srl systems, each trained
on an 80% subsample of the corpus. the average prediction across this ensemble is more
robust than any individual model.

13.3 id15

id14 transforms the task of id29 to a labeling task. consider
the sentence,

14the successful application of convolutional neural networks to id14 by collobert and
weston (2008) was an in   uential early result in the current wave of neural networks in natural language
processing.

jacob eisenstein. draft of november 13, 2018.

13.3.

id15

319

(w / want-01

:arg0 (h / whale)
:arg1 (p / pursue-02

:arg0 (c / captain)
:arg1 h))

figure 13.3: two views of the amr representation for the sentence the whale wants the
captain to pursue him.

(13.13) the whale wants the captain to pursue him.

the propbank id14 analysis is:

    (predicate : wants, arg0 : the whale, arg1 : the captain to pursue him)
    (predicate : pursue, arg0 : the captain, arg1 : him)

the id15 (amr) uni   es this analysis into a graph struc-
ture, in which each node is a variable, and each edge indicates a concept (banarescu
et al., 2013). this can be written in two ways, as shown in figure 13.3. on the left is the
penman notation (matthiessen and bateman, 1991), in which each set of parentheses in-
troduces a variable. each variable is an instance of a concept, which is indicated with the
slash notation: for example, w / want-01 indicates that the variable w is an instance of
the concept want-01, which in turn refers to the propbank frame for the    rst sense of the
verb want; pursue-02 refers to the second sense of pursue. relations are introduced with
colons: for example, :arg0 (c / captain) indicates a relation of type arg0 with the
newly-introduced variable c. variables can be reused, so that when the variable h ap-
pears again as an argument to p, it is understood to refer to the same whale in both cases.
this arrangement is indicated compactly in the graph structure on the right, with edges
indicating concepts.

one way in which amr differs from propbank-style id14 is that it
rei   es each entity as a variable: for example, the whale in (13.13) is rei   ed in the variable
h, which is reused as arg0 in its relationship with w / want-01, and as arg1 in its
relationship with p / pursue-02. reifying entities as variables also makes it possible
to represent the substructure of noun phrases more explicitly. for example, asha borrowed
the algebra book would be represented as:

(b / borrow-01

:arg0 (p / person

:name (n / name

:op1 "asha"))

under contract with mit press, shared under cc-by-nc-nd license.

w/wants-01h/whalep/pursue-02c/captainarg0arg1arg1arg0320

chapter 13. predicate-argument semantics

:arg1 (b2 / book

:topic (a / algebra)))

this indicates that the variable p is a person, whose name is the variable n; that name
has one token, the string asha. similarly, the variable b2 is a book, and the topic of b2
is a variable a whose type is algebra. the relations name and topic are examples of
   non-core roles   , which are similar to adjunct modi   ers in propbank. however, amr   s
inventory is more extensive, including more than 70 non-core roles, such as negation,
time, manner, frequency, and location. lists and sequences     such as the list of tokens in
a name     are described using the roles op1, op2, etc.

another feature of amr is that a semantic predicate can be introduced by any syntac-

tic element, as in the following examples from banarescu et al. (2013):

(13.14)

a. the boy destroyed the room.
b.
c.

the destruction of the room by the boy . . .
the boy   s destruction of the room . . .

all these examples have the same semantics in amr,

(d / destroy-01

:arg0 (b / boy)
:arg1 (r / room))

the noun destruction is linked to the verb destroy, which is captured by the propbank
frame destroy-01. this can happen with adjectives as well: in the phrase the attractive
spy, the adjective attractive is linked to the propbank frame attract-01:

(s / spy

:arg0-of (a / attract-01))

in this example, arg0-of is an inverse relation, indicating that s is the arg0 of the
predicate a. inverse relations make it possible for all amr parses to have a single root
concept.

while amr goes farther than id14, it does not link semantically-
related frames such as buy/sell (as framenet does). amr also does not handle quan-
ti   cation (as    rst-order predicate calculus does), and it makes no attempt to handle noun
number and verb tense (as propbank does).

jacob eisenstein. draft of november 13, 2018.

13.3. id15

321

13.3.1 amr parsing

id15 is not a labeling of the original text     unlike propbank
id14, and most of the other tagging and parsing tasks that we have
encountered thus far. the amr for a given sentence may include multiple concepts for
single words in the sentence: as we have seen, the sentence asha likes algebra contains both
person and name concepts for the word asha. conversely, words in the sentence may not
appear in the amr: in boyang made a tour of campus, the light verb make would not appear
in the amr, which would instead be rooted on the predicate tour. as a result, amr
is dif   cult to parse, and even evaluating amr parsing involves considerable algorithmic
complexity (cai and yates, 2013).

a further complexity is that amr labeled datasets do not explicitly show the align-
ment between the amr annotation and the words in the sentence. for example, the link
between the word wants and the concept want-01 is not annotated. to acquire train-
ing data for learning-based parsers, it is therefore necessary to    rst perform an alignment
between the training sentences and their amr parses. flanigan et al. (2014) introduce a
rule-based parser, which links text to concepts through a series of increasingly high-recall
steps.

as with id33, amr can be parsed by graph-based methods that ex-
plore the space of graph structures, or by incremental transition-based algorithms. one
approach to graph-based amr parsing is to    rst group adjacent tokens into local sub-
structures, and then to search the space of graphs over these substructures (flanigan et al.,
2014). the identi   cation of concept subgraphs can be formulated as a sequence labeling
problem, and the subsequent graph search can be solved using integer linear program-
ming (   13.2.2). various transition-based parsing algorithms have been proposed. wang
et al. (2015) construct an amr graph by incrementally modifying the syntactic depen-
dency graph. at each step, the parser performs an action: for example, adding an amr
relation label to the current dependency edge, swapping the direction of a syntactic de-
pendency edge, or cutting an edge and reattaching the orphaned subtree to a new parent.

additional resources

practical id14 was    rst made possible by the propbank annotations on
the id32 (palmer et al., 2005). abend and rappoport (2017) survey several
semantic representation schemes, including id14 and amr. other lin-
guistic features of amr are summarized in the original paper (banarescu et al., 2013) and
the tutorial slides by schneider et al. (2015). recent shared tasks have undertaken seman-
tic id33, in which the goal is to identify semantic relationships between
pairs of words (oepen et al., 2014); see ivanova et al. (2012) for an overview of connections
between syntactic and semantic dependencies.

under contract with mit press, shared under cc-by-nc-nd license.

322

exercises

chapter 13. predicate-argument semantics

1. write out an event semantic representation for the following sentences. you may

make up your own predicates.

(13.15) abigail shares with max.
(13.16) abigail reluctantly shares a toy with max.
(13.17) abigail hates to share with max.

2. find the propbank framesets for share and hate at http://verbs.colorado.edu/
propbank/framesets-english-aliases/, and rewrite your answers from the
previous question, using the thematic roles arg0, arg1, and arg2.

3. compute the syntactic path features for abigail and max in each of the example sen-
tences (13.15) and (13.17) in question 1, with respect to the verb share. if you   re not
sure about the parse, you can try an online parser such as http://nlp.stanford.
edu:8080/parser/.

4. compute the dependency path features for abigail and max in each of the example
sentences (13.15) and (13.17) in question 1, with respect to the verb share. again, if
you   re not sure about the parse, you can try an online parser such as http://nlp.
stanford.edu:8080/parser/. as a hint, the dependency relation between share
and max is obl according to the universal dependency treebank.

5. propbank id14 includes reference arguments, such as,

(13.18)

[am-loc the bed] on [r-am-loc which] i slept broke.15

the label r-am-loc indicates that the word which is a reference to the bed, which
expresses the location of the event. reference arguments must have referents: the
tag r-am-loc can appear only when am-loc also appears in the sentence. show
how to express this as a linear constraint, speci   cally for the tag r-am-loc. be sure
to correctly handle the case in which neither am-loc nor r-am-loc appear in the
sentence.

6. explain how to express the constraints on id14 in equation 13.8

and equation 13.9 in the general form az     b.

7. produce the amr annotations for the following examples:

(13.19)

a. the girl likes the boy.

15example from 2013 naacl tutorial slides by shumin wu

jacob eisenstein. draft of november 13, 2018.

13.3. id15

323

b. the girl was liked by the boy.
c. abigail likes maxwell aristotle.
d. the spy likes the attractive boy.
e. the girl doesn   t like the boy.
f. the girl likes her dog.

for (13.19c), recall that multi-token names are created using op1, op2, etc. you will
need to consult banarescu et al. (2013) for (13.19e), and schneider et al. (2015) for
(13.19f). you may assume that her refers to the girl in this example.

8. in this problem, you will build a framenet sense classi   er for the verb can, which
can evoke two frames: possibility (can you order a salad with french fries?) and
capability(can you eat a salad with chopsticks?).
to build the dataset, access the framenet corpus in nltk:
import nltk
nltk.download(   framenet_v17   )
from nltk.corpus import framenet as fn

next,    nd instances in which the lexical unit can.v (the verb form of can) evokes a
frame. do this by iterating over fn.docs(), and then over sentences, and then
for doc in fn.docs():

if    sentence    in doc:

for sent in doc[   sentence   ]:

for anno_set in sent[   annotationset   ]:

if    luname    in anno_set and anno_set[   luname   ] ==    can.v   :

pass # your code here

use the    eld framename as a label, and build a set of features from the    eld text.
train a classi   er to try to accurately predict the framename, disregarding cases
other than capability and possibility. treat the    rst hundred instances as a train-
ing set, and the remaining instances as the test set. can you do better than a classi   er
that simply selects the most common class?

9. *download the propbank sample data, using nltk (http://www.nltk.org/

howto/propbank.html).

a) use a deep learning toolkit such as pytorch to train a bilstm sequence label-
ing model (   7.6) to identify words or phrases that are predicates, e.g., we/o
took/b-pred a/i-pred walk/i-pred together/o. your model should compute
the tag score from the bilstm hidden state   (ym) =   y    hm.

b) optionally, implement viterbi to improve the predictions of the model in the

previous section.

under contract with mit press, shared under cc-by-nc-nd license.

324

chapter 13. predicate-argument semantics

c) try to identify arg0 and arg1 for each predicate. you should again use the
bilstm and bio notation, but you may want to include the bilstm hidden
state at the location of the predicate in your prediction model, e.g.,   (ym) =
  y  [hm; h  r], where   r is the predicted location of the (   rst word of the) predicate.
10. using an off-the-shelf propbank srl system,16 build a simpli   ed question answer-
ing system in the style of shen and lapata (2007). speci   cally, your system should
do the following:

and should store the output as a tuple.

    for each document in a collection, it should apply the semantic role labeler,
    for a question, your system should again apply the semantic role labeler. if
any of the roles are    lled by a wh-pronoun, you should mark that role as the
expected answer phrase (eap).

    to answer the question, search for a stored tuple which matches the question as
well as possible (same predicate, no incompatible semantic roles, and as many
matching roles as possible). align the eap against its role    ller in the stored
tuple, and return this as the answer.

to evaluate your system, download a set of three news articles on the same topic,
and write down    ve factoid questions that should be answerable from the arti-
cles. see if your system can answer these questions correctly. (if this problem is
assigned to an entire class, you can build a large-scale test set and compare various
approaches.)

16at the time of writing, the following systems are availabe: senna (http://ronan.collobert.
(https://cogcomp.cs.illinois.edu/page/

com/senna/),
software_view/srl), and mate-tools (https://code.google.com/archive/p/mate-tools/).

semantic role labeler

illinois

jacob eisenstein. draft of november 13, 2018.

chapter 14

distributional and distributed
semantics

a recurring theme in natural language processing is the complexity of the mapping from
words to meaning. in chapter 4, we saw that a single word form, like bank, can have mul-
tiple meanings; conversely, a single meaning may be created by multiple surface forms,
a lexical semantic relationship known as synonymy. despite this complex mapping be-
tween words and meaning, natural language processing systems usually rely on words
as the basic unit of analysis. this is especially true in semantics: the logical and frame
semantic methods from the previous two chapters rely on hand-crafted lexicons that map
from words to semantic predicates. but how can we analyze texts that contain words
that we haven   t seen before? this chapter describes methods that learn representations
of word meaning by analyzing unlabeled data, vastly improving the generalizability of
natural language processing systems. the theory that makes it possible to acquire mean-
ingful representations from unlabeled data is the distributional hypothesis.

14.1 the distributional hypothesis

here   s a word you may not know: tezg  uino (the example is from lin, 1998). if you do not
know the meaning of tezg  uino, then you are in the same situation as a natural language
processing system when it encounters a word that did not appear in its training data.
now suppose you see that tezg  uino is used in the following contexts:

(14.1) a bottle of
(14.2) everybody likes
(14.3) don   t have
(14.4) we make

is on the table.

.

before you drive.

out of corn.

325

326

chapter 14. distributional and distributed semantics

(14.1)

(14.2)

(14.3)

(14.4)

...

tezg  uino
loud
motor oil
tortillas
choices
wine

1
0
1
0
0
1

1
0
0
1
1
1

1
0
0
0
0
1

1
0
1
1
0
0

table 14.1: distributional statistics for tezg  uino and    ve related terms

figure 14.1: lexical semantic relationships have regular linear structures in two dimen-
sional projections of distributional statistics (pennington et al., 2014).

what other words    t into these contexts? how about: loud, motor oil, tortillas, choices,
wine? each row of table 14.1 is a vector that summarizes the contextual properties for
each word, with a value of one for contexts in which the word can appear, and a value of
zero for contexts in which it cannot. based on these vectors, we can conclude: wine is very
similar to tezg  uino; motor oil and tortillas are fairly similar to tezg  uino; loud is completely
different.

these vectors, which we will call word representations, describe the distributional
properties of each word. does vector similarity imply semantic similarity? this is the dis-
tributional hypothesis, stated by firth (1957) as:    you shall know a word by the company
it keeps.    the distributional hypothesis has stood the test of time: distributional statistics
are a core part of language technology today, because they make it possible to leverage
large amounts of unlabeled data to learn about rare words that do not appear in labeled
training data.

distributional statistics have a striking ability to capture lexical semantic relationships

jacob eisenstein. draft of november 13, 2018.

432101234432101234sisterbrotherniecenephewauntunclewomanmanheiressheirmadamsircountessearlduchessdukequeenkingempressemperor420246432101234slowslowerslowestshortshortershorteststrongstrongerstrongestloudlouderloudestclearclearerclearestsoftsoftersoftestdarkdarkerdarkest14.2. design decisions for word representations

327

such as analogies. figure 14.1 shows two examples, based on two-dimensional projections
of distributional id27s, discussed later in this chapter. in each case, word-
pair relationships correspond to regular linear patterns in this two dimensional space. no
labeled data about the nature of these relationships was required to identify this underly-
ing structure.

id65 are computed from context statistics. distributed seman-
tics are a related but distinct idea: that meaning can be represented by numerical vectors
rather than symbolic structures. distributed representations are often estimated from dis-
tributional statistics, as in latent semantic analysis and id97, described later in this
chapter. however, distributed representations can also be learned in a supervised fashion
from labeled data, as in the neural classi   cation models encountered in chapter 3.

14.2 design decisions for word representations

there are many approaches for computing word representations, but most can be distin-
guished on three main dimensions: the nature of the representation, the source of contex-
tual information, and the estimation procedure.

14.2.1 representation

today, the dominant word representations are k-dimensional vectors of real numbers,
known as id27s. (the name is due to the fact that each discrete word is em-
bedded in a continuous vector space.) this representation dates back at least to the late
1980s (deerwester et al., 1990), and is used in popular techniques such as id97 (mikolov
et al., 2013).

id27s are well suited for neural networks, where they can be plugged
in as inputs. they can also be applied in linear classi   ers and structure prediction mod-
els (turian et al., 2010), although it can be dif   cult to learn linear models that employ
real-valued features (kummerfeld et al., 2015). a popular alternative is bit-string rep-
resentations, such as brown clusters (   14.4), in which each word is represented by a
variable-length sequence of zeros and ones (brown et al., 1992).

another representational question is whether to estimate one embedding per surface
form (e.g., bank), or to estimate distinct embeddings for each word sense or synset. in-
tuitively, if word representations are to capture the meaning of individual words, then
words with multiple meanings should have multiple embeddings. this can be achieved
by integrating unsupervised id91 with id27 estimation (huang and
yates, 2012; li and jurafsky, 2015). however, arora et al. (2018) argue that it is unnec-
essary to model distinct word senses explicitly, because the embeddings for each surface
form are a linear combination of the embeddings of the underlying senses.

under contract with mit press, shared under cc-by-nc-nd license.

328

chapter 14. distributional and distributed semantics

the moment one learns english, complications set in (alfau, 1999)
brown clusters
id97, h = 2
structured id97, h = 2
dependency contexts,

{one}
{moment, one, english, complications}
{(moment,   2), (one,   1), (english, +1), (complications, +2)}
{(one, nsubj), (english, dobj), (moment, acl   1)}

table 14.2: contexts for the word learns, according to various word representations. for
dependency context, (one, nsubj) means that there is a relation of type nsubj (nominal
subject) to the word one, and (moment, acl   1) means that there is a relation of type acl
(adjectival clause) from the word moment.

14.2.2 context

the distributional hypothesis says that word meaning is related to the    contexts    in which
the word appears, but context can be de   ned in many ways. in the tezg  uino example, con-
texts are entire sentences, but in practice there are far too many sentences. at the oppo-
site extreme, the context could be de   ned as the immediately preceding word; this is the
context considered in brown clusters. id97 takes an intermediate approach, using
local neighborhoods of words (e.g., h = 5) as contexts (mikolov et al., 2013). contexts
can also be much larger: for example, in latent semantic analysis, each word   s context
vector includes an entry per document, with a value of one if the word appears in the
document (deerwester et al., 1990); in explicit semantic analysis, these documents are
wikipedia pages (gabrilovich and markovitch, 2007).

in structured id97, context words are labeled by their position with respect to
the target word wm (e.g., two words before, one word after), which makes the result-
ing word representations more sensitive to syntactic differences (ling et al., 2015). an-
other way to incorporate syntax is to perform parsing as a preprocessing step, and then
form context vectors from the dependency edges (levy and goldberg, 2014) or predicate-
argument relations (lin, 1998). the resulting context vectors for several of these methods
are shown in table 14.2.

the choice of context has a profound effect on the resulting representations, which
can be viewed in terms of word similarity. applying latent semantic analysis (   14.3) to
contexts of size h = 2 and h = 30 yields the following nearest-neighbors for the word
dog:1

    (h = 2): cat, horse, fox, pet, rabbit, pig, animal, mongrel, sheep, pigeon
1the example is from lecture slides by marco baroni, alessandro lenci, and stefan evert, who applied
latent semantic analysis to the british national corpus. you can    nd an online demo here: http://clic.
cimec.unitn.it/infomap-query/

jacob eisenstein. draft of november 13, 2018.

14.3. latent semantic analysis

329

    (h = 30): kennel, puppy, pet, bitch, terrier, rottweiler, canine, cat, to bark, alsatian

which word list is better? each word in the h = 2 list is an animal, re   ecting the fact that
locally, the word dog tends to appear in the same contexts as other animal types (e.g., pet
the dog, feed the dog). in the h = 30 list, nearly everything is dog-related, including speci   c
breeds such as rottweiler and alsatian. the list also includes words that are not animals
(kennel), and in one case (to bark), is not a noun at all. the 2-word context window is more
sensitive to syntax, while the 30-word window is more sensitive to topic.

14.2.3 estimation
id27s are estimated by optimizing some objective: the likelihood of a set of
unlabeled data (or a closely related quantity), or the reconstruction of a matrix of context
counts, similar to table 14.1.

id113 likelihood-based optimization is derived from the
objective log p(w; u), where u     rk    v is matrix of id27s, and w =
m=1 is a corpus, represented as a list of m tokens. recurrent neural network lan-
{wm}m
guage models (   6.3) optimize this objective directly, backpropagating to the input word
embeddings through the recurrent structure. however, state-of-the-art id27s
employ huge corpora with hundreds of billions of tokens, and recurrent architectures are
dif   cult to scale to such data. as a result, likelihood-based id27s are usually
based on simpli   ed likelihoods or heuristic approximations.

id105 the matrix c = {count(i, j)} stores the co-occurrence counts of
word i and context j. word representations can be obtained by approximately factoring
this matrix, so that count(i, j) is approximated by a function of a id27 ui and
a context embedding vj. these embeddings can be obtained by minimizing the norm of
the reconstruction error,

u,v ||c       c(u, v)||f ,
min
v, and ||x||f indicates the frobenius norm,(cid:80)i,j x2

where   c(u, v) is the approximate reconstruction resulting from the embeddings u and
i,j. rather than factoring the matrix of
word-context counts directly, it is often helpful to transform these counts using information-
theoretic metrics such as pointwise mutual information (pmi), described in the next sec-
tion.

[14.1]

14.3 latent semantic analysis

latent semantic analysis (lsa) is one of the oldest approaches to distributed seman-
tics (deerwester et al., 1990). it induces continuous vector representations of words by

under contract with mit press, shared under cc-by-nc-nd license.

330

chapter 14. distributional and distributed semantics

factoring a matrix of word and context counts, using truncated singular value decompo-
sition (svd),

min

u   rv   k ,s   rk  k ,v   r|c|  k ||c     usv
u = i
v = i

s.t. u

(cid:62)
(cid:62)

v

(cid:62)

||f

   i (cid:54)= j, si,j = 0,

[14.2]

[14.3]
[14.4]
[14.5]

where v is the size of the vocabulary, |c| is the number of contexts, and k is size of the
resulting embeddings, which are set equal to the rows of the matrix u. the matrix s is
constrained to be diagonal (these diagonal elements are called the singular values), and
the columns of the product sv(cid:62) provide descriptions of the contexts. each element ci,j is
then reconstructed as a bilinear product,

ci,j    

k(cid:88)k=1

ui,kskvj,k.

[14.6]

the objective is to minimize the sum of squared approximation errors. the orthonormal-
ity constraints u(cid:62)u = v(cid:62)v = i ensure that all pairs of dimensions in u and v are
uncorrelated, so that each dimension conveys unique information. ef   cient implemen-
tations of truncated singular value decomposition are available in numerical computing
packages such as scipy and matlab.2

latent semantic analysis is most effective when the count matrix is transformed before
the application of svd. one such transformation is pointwise mutual information (pmi;
church and hanks, 1990), which captures the degree of association between word i and
context j,

pmi(i, j) = log

p(i, j)
p(i)p(j)

= log

= log

p(i | j)
p(i)

p(i | j)p(j)
p(i)p(j)
v(cid:88)i(cid:48)=1

count(i

) + log

(cid:48)

, j)

v(cid:88)i(cid:48)=1(cid:88)j(cid:48)   c

= log count(i, j)     log
    log(cid:88)j(cid:48)   c

count(i, j

(cid:48)

count(i

(cid:48)

(cid:48)

).

, j

[14.7]

[14.8]

[14.9]

the pointwise mutual information can be viewed as the logarithm of the ratio of the con-
ditional id203 of word i in context j to the marginal id203 of word i in all

2an important implementation detail is to represent c as a sparse matrix, so that the storage cost is equal

to the number of non-zero entries, rather than the size v    |c|.

jacob eisenstein. draft of november 13, 2018.

14.4. brown clusters

331

figure 14.2: subtrees produced by bottom-up brown id91 on news text (miller et al.,
2004).

contexts. when word i is statistically associated with context j, the ratio will be greater
than one, so pmi(i, j) > 0. the pmi transformation focuses latent semantic analysis on re-
constructing strong word-context associations, rather than on reconstructing large counts.
the pmi is negative when a word and context occur together less often than if they
were independent, but such negative correlations are unreliable because counts of rare
events have high variance. furthermore, the pmi is unde   ned when count(i, j) = 0. one
solution to these problems is to use the positive pmi (ppmi),

ppmi(i, j) =(cid:40)pmi(i, j), p(i | j) > p(i)

otherwise.

0,

[14.10]

bullinaria and levy (2007) compare a range of matrix transformations for latent se-
mantic analysis, using a battery of tasks related to word meaning and word similarity
(for more on evaluation, see    14.6). they    nd that ppmi-based latent semantic analysis
yields strong performance on a battery of tasks related to word meaning: for example,
ppmi-based lsa vectors can be used to solve multiple-choice word similarity questions
from the test of english as a foreign language (toefl), obtaining 85% accuracy.

14.4 brown clusters

learning algorithms like id88 and conditional random    elds often perform better
with discrete feature vectors. a simple way to obtain discrete representations from distri-

under contract with mit press, shared under cc-by-nc-nd license.

evaluationassessmentanalysisunderstandingopinionconversationdiscussionrepsrepresentativesrepresentativerepdayyearweekmonthquarterhalfaccountspeoplecustomersindividualsemployeesstudents332

chapter 14. distributional and distributed semantics

bitstring
011110100111

01111010100

011110101010

011110101011

ten most frequent words
excited thankful grateful stoked pumped anxious hyped psyched
exited geeked
talking talkin complaining talkn bitching tlkn tlkin bragging rav-
ing +k
thinking thinkin dreaming worrying thinkn speakin reminiscing
dreamin daydreaming fantasizing
saying sayin suggesting stating sayn jokin talmbout implying
insisting 5   2

011110101100 wonder dunno wondered duno donno dno dono wonda wounder

011110101101

011110101110

dunnoe
wondering wonders debating deciding pondering unsure won-
derin debatin woundering wondern
sure suree suuure suure sure- surre sures shuree

table 14.3: fragment of a brown id91 of twitter data (owoputi et al., 2013). each
row is a leaf in the tree, showing the ten most frequent words. this part of the tree
emphasizes verbs of communicating and knowing, especially in the present partici-
ple. each leaf node includes orthographic variants (thinking, thinkin, thinkn), semanti-
cally related terms (excited, thankful, grateful), and some outliers (5   2, +k). see http:
//www.cs.cmu.edu/  ark/tweetnlp/cluster_viewer.html for more.

butional statistics is by id91 (   5.1.1), so that words in the same cluster have similar
distributional statistics. this can help in downstream tasks, by sharing features between
all words in the same cluster. however, there is an obvious tradeoff: if the number of clus-
ters is too small, the words in each cluster will not have much in common; if the number
of clusters is too large, then the learner will not see enough examples from each cluster to
generalize.

a solution to this problem is hierarchical id91: using the distributional statistics
to induce a tree-structured representation. fragments of brown cluster trees are shown in
figure 14.2 and table 14.3. each word   s representation consists of a binary string describ-
ing a path through the tree: 0 for taking the left branch, and 1 for taking the right branch.
in the subtree in the upper right of the    gure, the representation of the word conversation
is 10; the representation of the word assessment is 0001. bitstring pre   xes capture similar-
ity at varying levels of speci   city, and it is common to use the    rst eight, twelve, sixteen,
and twenty bits as features in tasks such as id39 (miller et al., 2004)
and id33 (koo et al., 2008).

hierarchical trees can be induced from a likelihood-based objective, using a discrete

jacob eisenstein. draft of november 13, 2018.

14.4. brown clusters

latent variable ki     {1, 2, . . . , k} to represent the cluster of word i:

m(cid:88)m=1
log p(w; k)    
(cid:44) m(cid:88)m=1

log p(wm | wm   1; k)

log p(wm | kwm) + log p(kwm | kwm   1).

this is similar to a hidden markov model, with the crucial difference that each word can
be emitted from only a single cluster:    k (cid:54)= kwm, p(wm | k) = 0.

using the objective in equation 14.12, the brown id91 tree can be constructed
from the bottom up: begin with each word in its own cluster, and incrementally merge
clusters until only a single cluster remains. at each step, we merge the pair of clusters
such that the objective in equation 14.12 is maximized. although the objective seems to
involve a sum over the entire corpus, the score for each merger can be computed from
the cluster-to-cluster co-occurrence counts. these counts can be updated incrementally as
the id91 proceeds. the optimal merge at each step can be shown to maximize the
average mutual information,

333

[14.11]

[14.12]

[14.13]

i(k) =

p(k1, k2) =

k(cid:88)k1=1
k(cid:88)k2=1
k1(cid:48) =1(cid:80)k
(cid:80)k

p(k1, k2)    pmi(k1, k2)
count(k1, k2)
k2(cid:48) =1 count(k1(cid:48), k2(cid:48))

,

where p(k1, k2) is the joint id203 of a bigram involving a word in cluster k1 followed
by a word in k2. this id203 and the pmi are both computed from the co-occurrence
counts between clusters. after each merger, the co-occurrence vectors for the merged
clusters are simply added up, so that the next optimal merger can be found ef   ciently.

this bottom-up procedure requires iterating over the entire vocabulary, and evaluat-
ing k2
t possible mergers at each step, where kt is the current number of clusters at step t
of the algorithm. furthermore, computing the score for each merger involves a sum over
t clusters. the maximum number of clusters is k0 = v , which occurs when every word
k2
is in its own cluster at the beginning of the algorithm. the time complexity is thus o(v 5).
to avoid this complexity, practical implementations use a heuristic approximation
called exchange id91. the k most common words are placed in clusters of their
own at the beginning of the process. we then consider the next most common word, and
merge it with one of the existing clusters. this continues until the entire vocabulary has
been incorporated, at which point the k clusters are merged down to a single cluster,
forming a tree. the algorithm never considers more than k + 1 clusters at any step, and
the complexity is o(v k + v log v ), with the second term representing the cost of sorting
under contract with mit press, shared under cc-by-nc-nd license.

334

chapter 14. distributional and distributed semantics

(a) continuous bag-of-words (cbow)

(b) skipgram

figure 14.3: the cbow and skipgram variants of id97. the parameter u is the
matrix of id27s, and each vm is the context embedding for word wm.

the words at the beginning of the algorithm. for more details on the algorithm, see liang
(2005).

14.5 neural id27s

neural id27s combine aspects of the previous two methods: like latent se-
mantic analysis, they are a continuous vector representation; like brown clusters, they are
trained from a likelihood-based objective. let the vector ui represent the k-dimensional
embedding for word i, and let vj represent the k-dimensional embedding for context
j. the inner product ui    vj represents the compatibility between word i and context j.
by incorporating this inner product into an approximation to the log-likelihood of a cor-
pus, it is possible to estimate both parameters by id26. id97 (mikolov
et al., 2013) includes two such approximations: continuous bag-of-words (cbow) and
skipgrams.

14.5.1 continuous bag-of-words (cbow)
in recurrent neural network language models, each word wm is conditioned on a recurrently-
updated state vector, which is based on word representations going all the way back to the
beginning of the text. the continuous bag-of-words (cbow) model is a simpli   cation:
the local context is computed as an average of embeddings for words in the immediate
neighborhood m     h, m     h + 1, . . . , m + h     1, m + h,

vm =

1
2h

h(cid:88)n=1

vwm+n + vwm   n.

[14.14]

thus, cbow is a bag-of-words model, because the order of the context words does not
matter; it is continuous, because rather than conditioning on the words themselves, we
condition on a continuous vector constructed from the id27s. the parameter
h determines the neighborhood size, which mikolov et al. (2013) set to h = 4.

jacob eisenstein. draft of november 13, 2018.

vwm   2vwm   1vwmvwm+1vwm+2wm   2wm   1wmwm+1wm+2uvwmwmwm   1wm   2wm+1wm+2u14.5. neural id27s

the cbow model optimizes an approximation to the corpus log-likelihood,

log p(w)    

=

=

m(cid:88)m=1
m(cid:88)m=1
m(cid:88)m=1

log p(wm | wm   h, wm   h+1, . . . , wm+h   1, wm+h)

exp (uwm    vm)
j=1 exp (uj    vm)

log

(cid:80)v

uwm    vm     log

exp (uj    vm) .

v(cid:88)j=1

335

[14.15]

[14.16]

[14.17]

14.5.2 skipgrams
in the cbow model, words are predicted from their context. in the skipgram model, the
context is predicted from the word, yielding the objective:

log p(w)    

=

=

m(cid:88)m=1
m(cid:88)m=1
m(cid:88)m=1

hm(cid:88)n=1
hm(cid:88)n=1
hm(cid:88)n=1

log p(wm   n | wm) + log p(wm+n | wm)

exp(uwm   n    vwm)
j=1 exp(uj    vwm)

+ log

exp(uwm+n    vwm)
j=1 exp(uj    vwm)

log

(cid:80)v

uwm   n    vwm + uwm+n    vwm     2 log

exp (uj    vwm) .

(cid:80)v

v(cid:88)j=1

[14.18]

[14.19]

[14.20]

in the skipgram approximation, each word is generated multiple times; each time it is con-
ditioned only on a single word. this makes it possible to avoid averaging the word vec-
tors, as in the cbow model. the local neighborhood size hm is randomly sampled from
a uniform categorical distribution over the range {1, 2, . . . , hmax}; mikolov et al. (2013) set
hmax = 10. because the neighborhood grows outward with h, this approach has the effect
of weighting near neighbors more than distant ones. skipgram performs better on most
evaluations than cbow (see    14.6 for details of how to evaluate word representations),
but cbow is faster to train (mikolov et al., 2013).

14.5.3 computational complexity
the id97 models can be viewed as an ef   cient alternative to recurrent neural net-
work language models, which involve a recurrent state update whose time complexity
is quadratic in the size of the recurrent state vector. cbow and skipgram avoid this
computation, and incur only a linear time complexity in the size of the word and con-
text representations. however, all three models compute a normalized id203 over
word tokens; a na    ve implementation of this id203 requires summing over the entire

under contract with mit press, shared under cc-by-nc-nd license.

336

chapter 14. distributional and distributed semantics

figure 14.4: a fragment of a hierarchical softmax tree. the id203 of each word is
computed as a product of probabilities of local branching decisions in the tree.

vocabulary. the time complexity of this sum is o(v    k), which dominates all other com-
putational costs. there are two solutions: hierarchical softmax, a tree-based computation
that reduces the cost to a logarithm of the size of the vocabulary; and negative sampling,
an approximation that eliminates the dependence on vocabulary size. both methods are
also applicable to id56 language models.

hierarchical softmax

in brown id91, the vocabulary is organized into a binary tree. mnih and hin-
ton (2008) show that the normalized id203 over words in the vocabulary can be
reparametrized as a id203 over paths through such a tree. this hierarchical softmax
id203 is computed as a product of binary decisions over whether to move left or
right through the tree, with each binary decision represented as a sigmoid function of the
inner product between the context embedding vc and an output embedding associated
with the node un,

pr(left at n | c) =  (un    vc)
pr(right at n | c) =1       (un    vc) =   (   un    vc),

[14.21]
[14.22]

where    refers to the sigmoid function,   (x) =
interval (0, 1), and 1       (x) =   (   x).

1

1+exp(   x). the range of the sigmoid is the

as shown in figure 14.4, the id203 of generating each word is rede   ned as the
product of the probabilities across its path. the sum of all such path probabilities is guar-
anteed to be one, for any context vector vc     rk. in a balanced binary tree, the depth is
logarithmic in the number of leaf nodes, and thus the number of multiplications is equal
to o(log v ). the number of non-leaf nodes is equal to o(2v     1), so the number of pa-
rameters to be estimated increases by only a small multiple. the tree can be constructed
using an incremental id91 procedure similar to hierarchical brown clusters (mnih

jacob eisenstein. draft of november 13, 2018.

01234ahab  (u0  vc)whale  (   u0  vc)    (u2  vc)blubber  (   u0  vc)    (   u2  vc)14.5. neural id27s

337

and hinton, 2008), or by using the huffman (1952) encoding algorithm for lossless com-
pression.

negative sampling

likelihood-based methods are computationally intensive because each id203 must
be normalized over the vocabulary. these probabilities are based on scores for each word
in each context, and it is possible to design an alternative objective that is based on these
scores more directly: we seek id27s that maximize the score for the word that
was really observed in each context, while minimizing the scores for a set of randomly
selected negative samples:

  (i, j) = log   (ui    vj) + (cid:88)i(cid:48)   wneg

log(1       (ui(cid:48)    vj)),

[14.23]

where   (i, j) is the score for word i in context j, and wneg is the set of negative samples.
the objective is to maximize the sum over the corpus, (cid:80)m
m=1   (wm, cm), where wm is
token m and cm is the associated context.
the set of negative samples wneg is obtained by sampling from a unigram language
model. mikolov et al. (2013) construct this unigram language model by exponentiating
the empirical word probabilities, setting   p(i)     (count(i))
4 . this has the effect of redis-
tributing id203 mass from common to rare words. the number of negative samples
increases the time complexity of training by a constant factor. mikolov et al. (2013) report
that 5-20 negative samples works for small training sets, and that two to    ve samples
suf   ce for larger corpora.

3

14.5.4 id27s as id105
the negative sampling objective in equation 14.23 can be justi   ed as an ef   cient approx-
imation to the log-likelihood, but it is also closely linked to the id105 ob-
jective employed in latent semantic analysis. for a matrix of word-context pairs in which
all counts are non-zero, negative sampling is equivalent to factorization of the matrix m,
where mij = pmi(i, j)     log k: each cell in the matrix is equal to the pointwise mutual
information of the word and context, shifted by log k, with k equal to the number of neg-
ative samples (levy and goldberg, 2014). for word-context pairs that are not observed in
the data, the pointwise mutual information is       , but this can be addressed by consid-
ering only pmi values that are greater than log k, resulting in a matrix of shifted positive
pointwise mutual information,

mij = max(0, pmi(i, j)     log k).

[14.24]

id27s are obtained by factoring this matrix with truncated singular value
decomposition.

under contract with mit press, shared under cc-by-nc-nd license.

338

chapter 14. distributional and distributed semantics

word 1
love
stock
money
development
lad

word 2
sex
jaguar
cash
issue
brother

similarity
6.77
0.92
9.15
3.97
4.46

table 14.4: subset of the ws-353 (finkelstein et al., 2002) dataset of word similarity ratings
(examples from faruqui et al. (2016)).

glove (   global vectors   ) are a closely related approach (pennington et al., 2014), in
which the matrix to be factored is constructed from log co-occurrence counts, mij =
log count(i, j). the id27s are estimated by minimizing the sum of squares,

min
u,v,b,  b

s.t.

v(cid:88)j=1(cid:88)j   c

(cid:86)
log mij

(cid:86)    log mij(cid:17)2

f (mij)(cid:16)log mij
= ui    vj + bi +   bj,

[14.25]

where bi and   bj are offsets for word i and context j, which are estimated jointly with the
embeddings u and v. the weighting function f (mij) is set to be zero at mij = 0, thus
avoiding the problem of taking the logarithm of zero counts; it saturates at mij = mmax,
thus avoiding the problem of overcounting common word-context pairs. this heuristic
turns out to be critical to the method   s performance.

the time complexity of sparse matrix reconstruction is determined by the number of
non-zero word-context counts. pennington et al. (2014) show that this number grows
sublinearly with the size of the dataset: roughly o(n 0.8) for typical english corpora. in
contrast, the time complexity of id97 is linear in the corpus size. computing the co-
occurrence counts also requires linear time in the size of the corpus, but this operation can
easily be parallelized using mapreduce-style algorithms (dean and ghemawat, 2008).

14.6 evaluating id27s

distributed word representations can be evaluated in two main ways. intrinsic evalu-
ations test whether the representations cohere with our intuitions about word meaning.
extrinsic evaluations test whether they are useful for downstream tasks, such as sequence
labeling.

jacob eisenstein. draft of november 13, 2018.

14.6. evaluating id27s

339

intrinsic evaluations

14.6.1
a basic question for id27s is whether the similarity of words i and j is re-
   ected in the similarity of the vectors ui and uj. cosine similarity is typically used to
compare two id27s,

cos(ui, uj) =

ui    uj

||ui||2    ||uj||2

.

[14.26]

for any embedding method, we can evaluate whether the cosine similarity of word em-
beddings is correlated with human judgments of word similarity. the ws-353 dataset (finkel-
stein et al., 2002) includes similarity scores for 353 word pairs (table 14.4). to test the
accuracy of embeddings for rare and morphologically complex words, luong et al. (2013)
introduce a dataset of    rare words.    outside of english, word similarity resources are lim-
ited, mainly consisting of translations of ws-353 and the related siid113x-999 dataset (hill
et al., 2015).

word analogies (e.g., king:queen :: man:woman) have also been used to evaluate word
embeddings (mikolov et al., 2013). in this evaluation, the system is provided with the    rst
three parts of the analogy (i1 : j1 :: i2 :?), and the    nal element is predicted by    nding the
id27 most similar to ui1     uj1 + ui2. another evaluation tests whether word
embeddings are related to broad lexical semantic categories called supersenses (ciaramita
and johnson, 2003): verbs of motion, nouns that describe animals, nouns that describe
body parts, and so on. these supersenses are annotated for english synsets in word-
net (fellbaum, 2010). this evaluation is implemented in the qvec metric, which tests
whether the matrix of supersenses can be reconstructed from the matrix of word embed-
dings (tsvetkov et al., 2015).

levy et al. (2015) compared several dense word representations for english     includ-
ing latent semantic analysis, id97, and glove     using six word similarity metrics
and two analogy tasks. none of the embeddings outperformed the others on every task,
but skipgrams were the most broadly competitive. hyperparameter tuning played a key
role: any method will perform badly if the wrong hyperparameters are used. relevant
hyperparameters include the embedding size, as well as algorithm-speci   c details such
as the neighborhood size and the number of negative samples.

14.6.2 extrinsic evaluations
word representations contribute to downstream tasks like sequence labeling and docu-
ment classi   cation by enabling generalization across words. the use of distributed repre-
sentations as features is a form of semi-supervised learning, in which performance on a
supervised learning problem is augmented by learning distributed representations from
unlabeled data (miller et al., 2004; koo et al., 2008; turian et al., 2010). these pre-trained
word representations can be used as features in a linear prediction model, or as the input

under contract with mit press, shared under cc-by-nc-nd license.

340

chapter 14. distributional and distributed semantics

layer in a neural network, such as a bi-lstm tagging model (   7.6). word representations
can be evaluated by the performance of the downstream systems that consume them:
for example, glove embeddings are convincingly better than latent semantic analysis
as features in the downstream task of id39 (pennington et al., 2014).
unfortunately, extrinsic and intrinsic evaluations do not always point in the same direc-
tion, and the best word representations for one downstream task may perform poorly on
another task (schnabel et al., 2015).

when word representations are updated from labeled data in the downstream task,
they are said to be    ne-tuned. when labeled data is plentiful, pre-training may be un-
necessary; when labeled data is scarce,    ne-tuning may lead to over   tting. various com-
binations of pre-training and    ne-tuning can be employed. pre-trained embeddings can
be used as initialization before    ne-tuning, and this can substantially improve perfor-
mance (lample et al., 2016). alternatively, both    ne-tuned and pre-trained embeddings
can be used as inputs in a single model (kim, 2014).

in semi-supervised scenarios, pretrained id27s can be replaced by    con-
textualized    word representations (peters et al., 2018). these contextualized represen-
tations are set to the hidden states of a deep bi-directional lstm, which is trained as a
bi-directional language model, motivating the name elmo (embeddings from language
models). by running the language model, we obtain contextualized word representa-
tions, which can then be used as the base layer in a supervised neural network for any
task. this approach yields signi   cant gains over pretrained id27s on several
tasks, presumably because the contextualized embeddings use unlabeled data to learn
how to integrate linguistic context into the base layer of the supervised neural network.

14.6.3 fairness and bias
figure 14.1 shows how id27s can capture analogies such as man:woman ::
king:queen. while king and queen are gender-speci   c by de   nition, other professions or
titles are associated with genders and other groups merely by statistical tendency. this
statistical tendency may be a fact about the world (e.g., professional baseball players are
usually men), or a fact about the text corpus (e.g., there are professional basketball leagues
for both women and men, but the men   s basketball is written about far more often).

there is now considerable evidence that id27s do indeed encode such bi-
ases. bolukbasi et al. (2016) show that the words most aligned with the vector difference
she     he are stereotypically female professions homemaker, nurse, receptionist; in the other
direction are maestro, skipper, protege. caliskan et al. (2017) systematize this observation by
showing that biases in id27s align with well-validated gender stereotypes.
garg et al. (2018) extend these results to ethnic stereotypes of asian americans, and pro-
vide a historical perspective on how stereotypes evolve over 100 years of text data.

because id27s are the input layer for many other natural language pro-

jacob eisenstein. draft of november 13, 2018.

14.7. distributed representations beyond distributional statistics341

cessing systems, these    ndings highlight the risk that natural language processing will
replicate and amplify biases in the world, as well as in text. if, for example, word em-
beddings encode the belief that women are as unlikely to be programmers as they are to
be nephews, then software is unlikely to successfully parse, translate, index, and generate
texts in which women do indeed program computers. for example, contemporary nlp
systems often fail to properly resolve pronoun references in texts that cut against gender
stereotypes (rudinger et al., 2018; zhao et al., 2018). (the task of pronoun resolution is
described in depth in chapter 15.) such biases can have profound consequences: for exam-
ple, search engines are more likely to yield personalized advertisements for public arrest
records when queried with names that are statistically associated with african ameri-
cans (sweeney, 2013). there is now an active research literature on    debiasing    machine
learning and natural language processing, as evidenced by the growth of annual meet-
ings such as fairness, accountability, and transparency in machine learning (fat/ml).
however, given that the ultimate source of these biases is the text itself, it may be too
much to hope for a purely algorithmic solution. there is no substitute for critical thought
about the inputs to natural language processing systems     and the uses of their outputs.

14.7 distributed representations beyond distributional statistics

distributional word representations can be estimated from huge unlabeled datasets, thereby
covering many words that do not appear in labeled data: for example, glove embeddings
are estimated from 800 billion tokens of web data,3 while the largest labeled datasets for
nlp tasks are on the order of millions of tokens. nonetheless, even a dataset of hundreds
of billions of tokens will not cover every word that may be encountered in the future.
furthermore, many words will appear only a few times, making their embeddings un-
reliable. many languages exceed english in morphological complexity, and thus have
lower token-to-type ratios. when this problem is coupled with small training corpora, it
becomes especially important to leverage other sources of information beyond distribu-
tional statistics.

14.7.1 word-internal structure

one solution is to incorporate word-internal structure into id27s. purely
distributional approaches consider words as atomic units, but in fact, many words have
internal structure, so that their meaning can be composed from the representations of
sub-word units. consider the following terms, all of which are missing from google   s
pre-trained id97 embeddings:4

3http://commoncrawl.org/
4https://code.google.com/archive/p/id97/, accessed september 20, 2017

under contract with mit press, shared under cc-by-nc-nd license.

342

chapter 14. distributional and distributed semantics

figure 14.5: two architectures for building id27s from subword units. on the
left, morpheme embeddings u(m) are combined by addition with the non-compositional
id27   u (botha and blunsom, 2014). on the right, morpheme embeddings are
combined in a id56 (luong et al., 2013).

millicuries this word has morphological structure (see    9.1.2 for more on morphology):
the pre   x milli- indicates an amount, and the suf   x -s indicates a plural. (a millicurie
is an unit of radioactivity.)

caesium this word is a single morpheme, but the characters -ium are often associated
(caesium is the british spelling of a chemical element,

with chemical elements.
spelled cesium in american english.)

iaea this term is an acronym, as suggested by the use of capitalization. the pre   x i- fre-
quently refers to international organizations, and the suf   x -a often refers to agen-
cies or associations. (iaea is the international atomic energy agency.)

zhezhgan this term is in title case, suggesting the name of a person or place, and the
character bigram zh indicates that it is likely a id68. (zhezhgan is a mining
facility in kazakhstan.)

how can word-internal structure be incorporated into word representations? one
approach is to construct word representations from embeddings of the characters or mor-
phemes. for example, if word i has morphological segments mi, then its embedding can
be constructed by addition (botha and blunsom, 2014),

ui =   ui + (cid:88)j   mi

u(m )

j

,

[14.27]

where u(m )
m is a morpheme embedding and   ui is a non-compositional embedding of the
whole word, which is an additional free parameter of the model (figure 14.5, left side).
all embeddings are estimated from a log-bilinear language model (mnih and hinton,
2007), which is similar to the cbow model (   14.5), but includes only contextual informa-
tion from preceding words. the morphological segments are obtained using an unsuper-
vised segmenter (creutz and lagus, 2007). for words that do not appear in the training

jacob eisenstein. draft of november 13, 2018.

umillicuries  umillicuriesu(m)milli+u(m)curieu(m)+sumillicuriesumillicurieu(m)milli+u(m)curieu(m)+s14.7. distributed representations beyond distributional statistics343

data, the embedding can be constructed directly from the morphemes, assuming that each
morpheme appears in some other word in the training data. the free parameter   u adds
   exibility: words with similar morphemes are encouraged to have similar embeddings,
but this parameter makes it possible for them to be different.

word-internal structure can be incorporated into word representations in various other

ways. here are some of the main parameters.

subword units. examples like iaea and zhezhgan are not based on morphological com-
position, and a morphological segmenter is unlikely to identify meaningful sub-
word units for these terms. rather than using morphemes for subid27s,
one can use characters (santos and zadrozny, 2014; ling et al., 2015; kim et al., 2016),
character id165s (wieting et al., 2016a; bojanowski et al., 2017), and byte-pair en-
codings, a compression technique which captures frequent substrings (gage, 1994;
sennrich et al., 2016).

composition. combining the subid27s by addition does not differentiate
between orderings, nor does it identify any particular morpheme as the root. a
range of more    exible compositional models have been considered, including re-
currence (ling et al., 2015), convolution (santos and zadrozny, 2014; kim et al.,
2016), and id56s (luong et al., 2013), in which representa-
tions of progressively larger units are constructed over a morphological parse, e.g.
((milli+curie)+s), ((in+   am)+able), (in+(vis+ible)). a recursive embedding model is
shown in the right panel of figure 14.5.

estimation. estimating subid27s from a full dataset is computationally ex-
pensive. an alternative approach is to train a subword model to match pre-trained
id27s (cotterell et al., 2016; pinter et al., 2017). to train such a model, it
is only necessary to iterate over the vocabulary, and not the corpus.

14.7.2 lexical semantic resources

resources such as id138 provide another source of information about word meaning:
if we know that caesium is a synonym of cesium, or that a millicurie is a type of measurement
unit, then this should help to provide embeddings for the unknown words, and to smooth
embeddings of rare words. one way to do this is to retro   t pre-trained id27s
across a network of lexical semantic relationships (faruqui et al., 2015) by minimizing the
following objective,

min
u

v(cid:88)j=1

||ui       ui||2 + (cid:88)(i,j)   l

  ij||ui     uj||2,

[14.28]

under contract with mit press, shared under cc-by-nc-nd license.

344

chapter 14. distributional and distributed semantics

where   ui is the pretrained embedding of word i, and l = {(i, j)} is a lexicon of word
relations. the hyperparameter   ij controls the importance of adjacent words having
similar embeddings; faruqui et al. (2015) set it to the inverse of the degree of word i,
   1. retro   tting improves performance on a range of intrinsic evalu-
  ij = |{j : (i, j)     l}|
ations, and gives small improvements on an extrinsic document classi   cation task.

14.8 distributed representations of multiword units

can distributed representations extend to phrases, sentences, paragraphs, and beyond?
before exploring this possibility, recall the distinction between distributed and distri-
butional representations. neural embeddings such as id97 are both distributed
(vector-based) and distributional (derived from counts of words in context). as we con-
sider larger units of text, the counts decrease: in the limit, a multi-paragraph span of text
would never appear twice, except by plagiarism. thus, the meaning of a large span of
text cannot be determined from distributional statistics alone; it must be computed com-
positionally from smaller spans. but these considerations are orthogonal to the question
of whether distributed representations     dense numerical vectors     are suf   ciently ex-
pressive to capture the meaning of phrases, sentences, and paragraphs.

14.8.1 purely distributional methods

some multiword phrases are non-compositional: the meaning of such phrases is not de-
rived from the meaning of the individual words using typical id152.
this includes proper nouns like san francisco as well as idiomatic expressions like kick
the bucket (baldwin and kim, 2010). for these cases, purely distributional approaches
can work. a simple approach is to identify multiword units that appear together fre-
quently, and then treat these units as words, learning embeddings using a technique such
as id97.

the problem of identifying multiword units is sometimes called collocation extrac-
tion. a good collocation has high pointwise mutual information (pmi; see    14.3). for
example, na    ve bayes is a good collocation because p(wt = bayes | wt   1 = na    ve) is much
larger than p(wt = bayes). collocations of more than two words can be identi   ed by a
greedy incremental search: for example, mutual information might    rst be extracted as a
collocation and grouped into a single word type mutual information; then pointwise mu-
tual information can be extracted later. after identifying such units, they can be treated as
words when estimating skipgram embeddings. mikolov et al. (2013) show that the result-
ing embeddings perform reasonably well on a task of solving phrasal analogies, e.g. new
york : new york times :: baltimore : baltimore sun.

jacob eisenstein. draft of november 13, 2018.

14.8. distributed representations of multiword units

345

this was the only way
it was the only way
it was her turn to blink
it was hard to tell
it was time to move on
he had to do it again
they all looked at each other
they all turned to look back
they both turned to face him
they both turned and walked away

figure 14.6: by interpolating between the distributed representations of two sentences (in
bold), it is possible to generate grammatical sentences that combine aspects of both (bow-
man et al., 2016)

14.8.2 distributional-compositional hybrids
to move beyond short multiword phrases, composition is necessary. a simple but sur-
prisingly powerful approach is to represent a sentence with the average of its word em-
beddings (mitchell and lapata, 2010). this can be considered a hybrid of the distribu-
tional and compositional approaches to semantics: the id27s are computed
distributionally, and then the sentence representation is computed by composition.

the id97 approach can be stretched considerably further, embedding entire
sentences using a model similar to skipgrams, in the    skip-thought    model of kiros et al.
(2015). each sentence is encoded into a vector using a recurrent neural network: the encod-
ing of sentence t is set to the id56 hidden state at its    nal token, h(t)
mt. this vector is then
a parameter in a decoder model that is used to generate the previous and subsequent sen-
tences: the decoder is another recurrent neural network, which takes the encoding of the
neighboring sentence as an additional parameter in its recurrent update. (this encoder-
decoder model is discussed at length in chapter 18.) the encoder and decoder are trained
simultaneously from a likelihood-based objective, and the trained encoder can be used to
compute a distributed representation of any sentence. skip-thought can also be viewed
as a hybrid of distributional and compositional approaches: the vector representation of
each sentence is computed compositionally from the representations of the individual
words, but the training objective is distributional, based on sentence co-occurrence across
a corpus.

autoencoders are a variant of encoder-decoder models in which the decoder is trained
to produce the same text that was originally encoded, using only the distributed encod-
ing vector (li et al., 2015). the encoding acts as a bottleneck, so that generalization is
necessary if the model is to successfully    t the training data. in denoising autoencoders,

under contract with mit press, shared under cc-by-nc-nd license.

346

chapter 14. distributional and distributed semantics

the input is a corrupted version of the original sentence, and the auto-encoder must re-
construct the uncorrupted original (vincent et al., 2010; hill et al., 2016). by interpolating
between distributed representations of two sentences,   ui +(1     )uj, it is possible to gen-
erate sentences that combine aspects of the two inputs, as shown in figure 14.6 (bowman
et al., 2016).

autoencoders can also be applied to longer texts, such as paragraphs and documents.
this enables applications such as id53, which can be performed by match-
ing the encoding of the question with encodings of candidate answers (miao et al., 2016).

14.8.3 supervised compositional methods

given a supervision signal, such as a label describing the sentiment or meaning of a sen-
tence, a wide range of compositional methods can be applied to compute a distributed
representation that then predicts the label. the simplest is to average the embeddings
of each word in the sentence, and pass this average through a feedforward neural net-
work (iyyer et al., 2015). convolutional and recurrent neural networks go further, with
the ability to effectively capturing multiword phenomena such as negation (kalchbrenner
et al., 2014; kim, 2014; li et al., 2015; tang et al., 2015). another approach is to incorpo-
rate the syntactic structure of the sentence into a id56, in which the
representation for each syntactic constituent is computed from the representations of its
children (socher et al., 2012). however, in many cases, recurrent neural networks perform
as well or better than recursive networks (li et al., 2015).

whether convolutional, recurrent, or recursive, a key question is whether supervised
sentence representations are task-speci   c, or whether a single supervised sentence repre-
sentation model can yield useful performance on other tasks. wieting et al. (2016b) train a
variety of sentence embedding models for the task of labeling pairs of sentences as para-
phrases. they show that the resulting sentence embeddings give good performance for
id31. the stanford natural language id136 corpus classi   es sentence
pairs as entailments (the truth of sentence i implies the truth of sentence j), contradictions
(the truth of sentence i implies the falsity of sentence j), and neutral (i neither entails nor
contradicts j). sentence embeddings trained on this dataset transfer to a wide range of
classi   cation tasks (conneau et al., 2017).

14.8.4 hybrid distributed-symbolic representations

the power of distributed representations is in their generality: the distributed represen-
tation of a unit of text can serve as a summary of its meaning, and therefore as the input
for downstream tasks such as classi   cation, matching, and retrieval. for example, dis-
tributed sentence representations can be used to recognize the paraphrase relationship
between closely related sentences like the following:

jacob eisenstein. draft of november 13, 2018.

14.8. distributed representations of multiword units

347

(14.5)

a. donald thanked vlad profusely.
b. donald conveyed to vlad his profound appreciation.
c. vlad was showered with gratitude by donald.

symbolic representations are relatively brittle to this sort of variation, but are better
suited to describe individual entities, the things that they do, and the things that are done
to them. in examples (14.5a)-(14.5c), we not only know that somebody thanked someone
else, but we can make a range of id136s about what has happened between the entities
named donald and vlad. because distributed representations do not treat entities symbol-
ically, they lack the ability to reason about the roles played by entities across a sentence or
larger discourse.5 a hybrid between distributed and symbolic representations might give
the best of both worlds: robustness to the many different ways of describing the same
event, plus the expressiveness to support id136s about entities and the roles that they
play.

a    top-down    hybrid approach is to begin with logical semantics (of the sort de-
scribed in the previous two chapters), and but replace the prede   ned lexicon with a set
of distributional word clusters (poon and domingos, 2009; lewis and steedman, 2013). a
   bottom-up    approach is to add minimal symbolic structure to existing distributed repre-
sentations, such as vector representations for each entity (ji and eisenstein, 2015; wiseman
et al., 2016). this has been shown to improve performance on two problems that we will
encounter in the following chapters: classi   cation of discourse relations between adja-
cent sentences (chapter 16; ji and eisenstein, 2015), and coreference resolution of entity
mentions (chapter 15; wiseman et al., 2016; ji et al., 2017). research on hybrid seman-
tic representations is still in an early stage, and future representations may deviate more
boldly from existing symbolic and distributional approaches.

additional resources

turney and pantel (2010) survey a number of facets of vector word representations, fo-
cusing on id105 methods. schnabel et al. (2015) highlight problems with
similarity-based evaluations of id27s, and present a novel evaluation that
controls for word frequency. baroni et al. (2014) address linguistic issues that arise in
attempts to combine distributed and compositional representations.

in bilingual and multilingual distributed representations, embeddings are estimated
for translation pairs or tuples, such as (dog, perro, chien). these embeddings can improve
machine translation (zou et al., 2013; klementiev et al., 2012), transfer natural language

5at a 2014 workshop on id29, this critique of distributed representations was expressed by
ray mooney     a leading researcher in computational semantics     in a now well-known quote,    you can   t
cram the meaning of a whole sentence into a single vector!   

under contract with mit press, shared under cc-by-nc-nd license.

348

chapter 14. distributional and distributed semantics

processing models across languages (t  ackstr  om et al., 2012), and make monolingual word
embeddings more accurate (faruqui and dyer, 2014). a typical approach is to learn a pro-
jection that maximizes the correlation of the distributed representations of each element
in a translation pair, which can be obtained from a bilingual dictionary. distributed rep-
resentations can also be linked to perceptual information, such as image features. bruni
et al. (2014) use textual descriptions of images to obtain visual contextual information for
various words, which supplements traditional distributional context. image features can
also be inserted as contextual information in log bilinear language models (kiros et al.,
2014), making it possible to automatically generate text descriptions of images.

exercises

1. prove that the sum of probabilities of paths through a hierarchical softmax tree is

equal to one.

2. in skipgram id27s, the negative sampling objective can be written as,

l =(cid:88)i   v(cid:88)j   c

count(i, j)  (i, j),

[14.29]

with   (i, j) is de   ned in equation 14.23.
suppose we draw the negative samples from the empirical unigram distribution
  p(i) = punigram(i). first, compute the expectation of l with respect the negative
samples, using this id203.
next, take the derivative of this expectation with respect to the score of a single word
context pair   (ui  vj), and solve for the pointwise mutual information pmi(i, j). you
should be able to show that at the optimum, the pmi is a simple function of   (ui  vj)
and the number of negative samples.
(this exercise is part of a proof that shows that skipgram with negative sampling is
closely related to pmi-weighted id105.)

3. * in brown id91, prove that the cluster merge that maximizes the average mu-
tual information (equation 14.13) also maximizes the log-likelihood objective (equa-
tion 14.12).

4. a simple way to compute a distributed phrase representation is to add up the dis-
tributed representations of the words in the phrase. consider a id31
m=1 xm), where xm is
the vector representation of word m. prove that in such a model, the following two

model in which the predicted sentiment is,   (w) =       ((cid:80)m

jacob eisenstein. draft of november 13, 2018.

14.8. distributed representations of multiword units

349

inequalities cannot both hold:

  (good) >  (not good)
  (bad) <  (not bad).

[14.30]
[14.31]

then construct a similar example pair for the case in which phrase representations
are the average of the word representations.

5. now let   s consider a slight modi   cation to the prediction model in the previous

problem:

  (w) =       relu(

xm)

[14.32]

m(cid:88)m=1

show that in this case, it is possible to achieve the inequalities above. your solution
should provide the weights    and the embeddings xgood, xbad, and xnot.

for the next two problems, download a set of pre-trained id27s, such as the
id97 or polyglot embeddings.

6. use cosine similarity to    nd the most similar words to: dog, whale, before, however,

fabricate.

7. use vector addition and subtraction to compute target vectors for the analogies be-
low. after computing each target vector,    nd the top three candidates by cosine
similarity.

    dog:puppy :: cat: ?
    speak:speaker :: sing:?
    france:french :: england:?
    france:wine :: england:?

the remaining problems will require you to build a classi   er and test its properties. pick a
text classi   cation dataset, such as the cornell movie review data.6 divide your data into
training (60%), development (20%), and test sets (20%), if no such division already exists.

8. train a convolutional neural network, with inputs set to pre-trained word embed-
dings from the previous two problems. use an additional,    ne-tuned embedding
for out-of-vocabulary words. train until performance on the development set does
not improve. you can also use the development set to tune the model architecture,
such as the convolution width and depth. report f -measure and accuracy, as well
as training time.

6http://www.cs.cornell.edu/people/pabo/movie-review-data/

under contract with mit press, shared under cc-by-nc-nd license.

350

chapter 14. distributional and distributed semantics

9. now modify your model from the previous problem to    ne-tune the word embed-

dings. report f -measure, accuracy, and training time.

10. try a simpler approach, in which id27s in the document are averaged,
and then this average is passed through a feed-forward neural network. again, use
the development data to tune the model architecture. how close is the accuracy to
the convolutional networks from the previous problems?

jacob eisenstein. draft of november 13, 2018.

chapter 15

reference resolution

references are one of the most noticeable forms of linguistic ambiguity, af   icting not just
automated natural language processing systems, but also    uent human readers. warn-
ings to avoid    ambiguous pronouns    are ubiquitous in manuals and tutorials on writing
style. but referential ambiguity is not limited to pronouns, as shown in the text in fig-
ure 15.1. each of the bracketed substrings refers to an entity that is introduced earlier
in the passage. these references include the pronouns he and his, but also the shortened
name cook, and nominals such as the    rm and the    rm   s biggest growth market.

reference resolution subsumes several subtasks. this chapter will focus on corefer-
ence resolution, which is the task of grouping spans of text that refer to a single underly-
ing entity, or, in some cases, a single event: for example, the spans tim cook, he, and cook
are all coreferent. these individual spans are called mentions, because they mention an
entity; the entity is sometimes called the referent. each mention has a set of antecedents,
which are preceding mentions that are coreferent; for the    rst mention of an entity, the an-
tecedent set is empty. the task of pronominal id2 requires identifying
only the antecedents of pronouns. in entity linking, references are resolved not to other
spans of text, but to entities in a knowledge base. this task is discussed in chapter 17.

coreference resolution is a challenging problem for several reasons. resolving differ-
ent types of referring expressions requires different types of reasoning: the features and
methods that are useful for resolving pronouns are different from those that are useful
to resolve names and nominals. coreference resolution involves not only linguistic rea-
soning, but also world knowledge and pragmatics: you may not have known that china
was apple   s biggest growth market, but it is likely that you effortlessly resolved this ref-
erence while reading the passage in figure 15.1.1 a further challenge is that coreference

1this interpretation is based in part on the assumption that a cooperative author would not use the
expression the    rm   s biggest growth market to refer to an entity not yet mentioned in the article (grice, 1975).
pragmatics is the discipline of linguistics concerned with the formalization of such assumptions (huang,

351

352

chapter 15. reference resolution

(15.1)

[[apple inc] chief executive tim cook] has jetted into [china] for talks with
government of   cials as [he] seeks to clear up a pile of problems in [[the    rm]
   s biggest growth market] ... [cook] is on [his]    rst trip to [the country] since
taking over...

figure 15.1: running example (yee and jones, 2012). coreferring entity mentions are in
brackets.

resolution decisions are often entangled: each mention adds information about the entity,
which affects other coreference decisions. this means that coreference resolution must
be addressed as a structure prediction problem. but as we will see, there is no dynamic
program that allows the space of coreference decisions to be searched ef   ciently.

15.1 forms of referring expressions

there are three main forms of referring expressions     pronouns, names, and nominals.

15.1.1 pronouns
pronouns are a closed class of words that are used for references. a natural way to think
about pronoun resolution is smash (kehler, 2007):

    search for candidate antecedents;
    match against hard agreement constraints;
    and select using heuristics, which are    soft    constraints such as recency, syntactic

prominence, and parallelism.

search
in the search step, candidate antecedents are identi   ed from the preceding text or speech.2
any noun phrase can be a candidate antecedent, and pronoun resolution usually requires

2015).

m  arquez (1970):

2pronouns whose referents come later are known as cataphora, as in the opening line from a novel by

(15.1) many years later, as [he] faced the    ring squad, [colonel aureliano buend    a] was to remember that

distant afternoon when [his] father took him to discover ice.

jacob eisenstein. draft of november 13, 2018.

15.1. forms of referring expressions

353

parsing the text to identify all such noun phrases.3 filtering heuristics can help to prune
the search space to noun phrases that are likely to be coreferent (lee et al., 2013; durrett
and klein, 2013). in nested noun phrases, mentions are generally considered to be the
largest unit with a given head word (see    10.5.2): thus, apple inc. chief executive tim cook
would be included as a mention, but tim cook would not, since they share the same head
word, cook.

matching constraints for pronouns

references and their antecedents must agree on semantic features such as number, person,
gender, and animacy. consider the pronoun he in this passage from the running example:

(15.2) tim cook has jetted in for talks with of   cials as [he] seeks to clear up a pile of

problems...

the pronoun and possible antecedents have the following features:

    he: singular, masculine, animate, third person
    of   cials: plural, animate, third person
    talks: plural, inanimate, third person
    tim cook: singular, masculine, animate, third person

the smash method searches backwards from he, discarding of   cials and talks because they
do not satisfy the agreements constraints.

another source of constraints comes from syntax     speci   cally, from the phrase struc-
ture trees discussed in chapter 10. consider a parse tree in which both x and y are phrasal
constituents. the constituent x c-commands the constituent y iff the    rst branching node
above x also dominates y. for example, in figure 15.2a, abigail c-commands her, because
the    rst branching node above abigail, s, also dominates her. now, if x c-commands y,
government and binding theory (chomsky, 1982) states that y can refer to x only if it is
a re   exive pronoun (e.g., herself ). furthermore, if y is a re   exive pronoun, then its an-
tecedent must c-command it. thus, in figure 15.2a, her cannot refer to abigail; conversely,
if we replace her with herself, then the re   exive pronoun must refer to abigail, since this is
the only candidate antecedent that c-commands it.

now consider the example shown in figure 15.2b. here, abigail does not c-command
her, but abigail   s mom does. thus, her can refer to abigail     and we cannot use re   exive
3in the ontonotes coreference annotations, verbs can also be antecedents, if they are later referenced by

nominals (pradhan et al., 2011):

(15.1) sales of passenger cars [grew] 22%. [the strong growth] followed year-to-year increases.

under contract with mit press, shared under cc-by-nc-nd license.

354

chapter 15. reference resolution

(a)

(b)

(c)

figure 15.2: in (a), abigail c-commands her; in (b), abigail does not c-command her, but
abigail   s mom does; in (c), the scope of abigail is limited by the s non-terminal, so that she
or her can bind to abigail, but not both.

herself in this context, unless we are talking about abigail   s mom. however, her does not
have to refer to abigail. finally, figure 15.2c shows the how these constraints are limited.
in this case, the pronoun she can refer to abigail, because the s non-terminal puts abigail
outside the domain of she. similarly, her can also refer to abigail. but she and her cannot be
coreferent, because she c-commands her.

heuristics

after applying constraints, heuristics are applied to select among the remaining candi-
dates. recency is a particularly strong heuristic. all things equal, readers will prefer
the more recent referent for a given pronoun, particularly when comparing referents that
occur in different sentences. jurafsky and martin (2009) offer the following example:

(15.3) the doctor found an old map in the captain   s chest. jim found an even older map

hidden on the shelf. [it] described an island.

readers are expected to prefer the older map as the referent for the pronoun it.

however, subjects are often preferred over objects, and this can contradict the prefer-

ence for recency when two candidate referents are in the same sentence. for example,

(15.4) abigail loaned lucia a book on spanish. [she] is always trying to help people.

here, we may prefer to link she to abigail rather than lucia, because of abigail   s position in
the subject role of the preceding sentence. (arguably, this preference would not be strong
enough to select abigail if the second sentence were she is visiting valencia next month.)

a third heuristic is parallelism:

(15.5) abigail loaned lucia a book on spanish.

  ozlem loaned [her] a book on por-

tuguese.

jacob eisenstein. draft of november 13, 2018.

svpppherwithspeaksnpabigailsvpppherwithspeaksnpmom   sabigailsvpsvpherwithspeaksnpshevhopesnpabigail15.1. forms of referring expressions

355

figure 15.3: left-to-right breadth-   rst tree traversal (hobbs, 1978), indicating that the
search for an antecedent for it (np1) would proceed in the following order: 536; the castle
in camelot; the residence of the king; camelot; the king. hobbs (1978) proposes semantic con-
straints to eliminate 536 and the castle in camelot as candidates, since they are unlikely to
be the direct object of the verb move.

here lucia is preferred as the referent for her, contradicting the preference for the subject
abigail in the preceding example.

the recency and subject role heuristics can be uni   ed by traversing the document in
a syntax-driven fashion (hobbs, 1978): each preceding sentence is traversed breadth-   rst,
left-to-right (figure 15.3). this heuristic successfully handles (15.4): abigail is preferred as
the referent for she because the subject np is visited    rst. it also handles (15.3): the older
map is preferred as the referent for it because the more recent sentence is visited    rst. (an
alternative uni   cation of recency and syntax is proposed by centering theory (grosz et al.,
1995), which is discussed in detail in chapter 16.)

in early work on reference resolution, the number of heuristics was small enough that
a set of numerical weights could be set by hand (lappin and leass, 1994). more recent
work uses machine learning to quantify the importance of each of these factors. however,
pronoun resolution cannot be completely solved by constraints and heuristics alone. this
is shown by the classic example pair (winograd, 1972):

(15.6) the [city council] denied [the protesters] a permit because [they] advocated/feared

violence.

without reasoning about the motivations of the city council and protesters, it is unlikely
that any system could correctly resolve both versions of this example.

under contract with mit press, shared under cc-by-nc-nd license.

svpppnpsbarsvpppnpnnplondontotonp1prpitvbdmovednpprphewhpwhencd536inuntilnpnpppnpnnkingdettheinofnnresidencedetthevbdremainednpppnpnnpcamelotininnncastledetthe356

chapter 15. reference resolution

non-referential pronouns

while pronouns are generally used for reference, they need not refer to entities. the fol-
lowing examples show how pronouns can refer to propositions, events, and speech acts.

(15.7)

a. they told me that i was too ugly for show business, but i didn   t believe [it].
b. elifsu saw berthold get angry, and i saw [it] too.
c. emmanuel said he worked in security. i suppose [that]   s one way to put it.

these forms of reference are generally not annotated in large-scale coreference resolution
datasets such as ontonotes (pradhan et al., 2011).

pronouns may also have generic referents:

(15.8)

a. a poor carpenter blames [her] tools.
b. on the moon, [you] have to carry [your] own oxygen.
c. every farmer who owns a donkey beats [it]. (geach, 1962)

in the ontonotes dataset, coreference is not annotated for generic referents, even in cases
like these examples, in which the same generic entity is mentioned multiple times.

some pronouns do not refer to anything at all:

(15.9)

a.

b.
c.

raining.
pleut.

[it]   s
[il]
[it]    s money that she   s really after.
[it] is too bad that we have to work so hard.

(fr)

how can we automatically distinguish these usages of it from referential pronouns?

consider the the difference between the following two examples (bergsma et al., 2008):

(15.10)

a. you can make [it] in advance.
b. you can make [it] in showbiz.

in the second example, the pronoun it is non-referential. one way to see this is by substi-
tuting another pronoun, like them, into these examples:

(15.11)

a. you can make [them] in advance.
b. ? you can make [them] in showbiz.

the questionable grammaticality of the second example suggests that it is not referential.
bergsma et al. (2008) operationalize this idea by comparing distributional statistics for the
id165s around the word it, testing how often other pronouns or nouns appear in the
same context. in cases where nouns and other pronouns are infrequent, the it is unlikely
to be referential.

jacob eisenstein. draft of november 13, 2018.

15.1. forms of referring expressions

357

15.1.2 proper nouns
if a proper noun is used as a referring expression, it often corefers with another proper
noun, so that the coreference problem is simply to determine whether the two names
match. subsequent proper noun references often use a shortened form, as in the running
example (figure 15.1):

(15.12) apple inc chief executive [tim cook] has jetted into china . . . [cook] is on his

   rst business trip to the country . . .

a typical solution for proper noun coreference is to match the syntactic head words
of the reference with the referent. in    10.5.2, we saw that the head word of a phrase can
be identi   ed by applying head percolation rules to the phrasal parse tree; alternatively,
the head can be identi   ed as the root of the dependency subtree covering the name. for
sequences of proper nouns, the head word will be the    nal token.

there are a number of caveats to the practice of matching head words of proper nouns.

    in the european tradition, family names tend to be more speci   c than given names,
and family names usually come last. however, other traditions have other practices:
for example, in chinese names, the family name typically comes    rst; in japanese,
honori   cs come after the name, as in nobu-san (mr. nobu).

    in organization names, the head word is often not the most informative, as in georgia
tech and virginia tech. similarly, lebanon does not refer to the same entity as south-
ern lebanon, necessitating special rules for the speci   c case of geographical modi-
   ers (lee et al., 2011).

    proper nouns can be nested, as in [the ceo of [microsoft]], resulting in head word

match without coreference.

despite these dif   culties, proper nouns are the easiest category of references to re-
solve (stoyanov et al., 2009). in machine learning systems, one solution is to include a
range of matching features, including exact match, head match, and string inclusion. in
addition to matching features, competitive systems (e.g., bengtson and roth, 2008) in-
clude large lists, or gazetteers, of acronyms (e.g, the national basketball association/nba),
demonyms (e.g., the israelis/israel), and other aliases (e.g., the georgia institute of technol-
ogy/georgia tech).

15.1.3 nominals
in coreference resolution, noun phrases that are neither pronouns nor proper nouns are
referred to as nominals. in the running example (figure 15.1), nominal references include:
the    rm (apple inc); the    rm   s biggest growth market (china); and the country (china).

under contract with mit press, shared under cc-by-nc-nd license.

358

chapter 15. reference resolution

nominals are especially dif   cult to resolve (denis and baldridge, 2007; durrett and
klein, 2013), and the examples above suggest why this may be the case: world knowledge
is required to identify apple inc as a    rm, and china as a growth market. other dif   cult
examples include the use of colloquial expressions, such as coreference between clinton
campaign of   cials and the clinton camp (soon et al., 2001).

15.2 algorithms for coreference resolution

the ground truth training data for coreference resolution is a set of mention sets, where all
mentions within each set refer to a single entity.4 in the running example from figure 15.1,
the ground truth coreference annotation is:

c1 ={apple inc1:2, the    rm27:28}
c2 ={apple inc chief executive tim cook1:6, he17, cook33, his36}
c3 ={china10, the    rm    s biggest growth market27:32, the country40:41}

[15.1]
[15.2]
[15.3]

each row speci   es the token spans that mention an entity. (   singleton    entities, which are
mentioned only once (e.g., talks, government of   cials), are excluded from the annotations.)
equivalently, if given a set of m mentions, {mi}m
i=1, each mention i can be assigned to a
cluster zi, where zi = zj if i and j are coreferent. the cluster assignments z are invariant
under permutation. the unique id91 associated with the assignment z is written
c(z).

coreference resolution can thus be viewed as a structure prediction problem, involv-
ing two subtasks: identifying which spans of text mention entities, and then id91
those spans.

mention identi   cation the task of identifying mention spans for coreference resolution
is often performed by applying a set of heuristics to the phrase structure parse of each
sentence. a typical approach is to start with all noun phrases and named entities, and
then apply    ltering rules to remove nested noun phrases with the same head (e.g., [apple
ceo [tim cook]]), numeric entities (e.g., [100 miles], [97%]), non-referential it, etc (lee
et al., 2013; durrett and klein, 2013). in general, these deterministic approaches err in
favor of recall, since the mention id91 component can choose to ignore false positive
mentions, but cannot recover from false negatives. an alternative is to consider all spans
(up to some    nite length) as candidate mentions, performing mention identi   cation and
id91 jointly (daum  e iii and marcu, 2005; lee et al., 2017).

4in many annotations, the term markable is used to refer to spans of text that can potentially mention an
entity. the set of markables includes non-referential pronouns, which does not mention any entity. part of the
job of the coreference system is to avoid incorrectly linking these non-referential markables to any mention
chains.

jacob eisenstein. draft of november 13, 2018.

15.2. algorithms for coreference resolution

359

mention id91 the subtask of mention id91 will be the focus of the remainder
of this chapter. there are two main classes of models. in mention-based models, the scoring
function for a coreference id91 decomposes over pairs of mentions. these pairwise
decisions are then aggregated, using a id91 heuristic. mention-based coreference
id91 can be treated as a fairly direct application of supervised classi   cation or rank-
ing. however, the mention-pair locality assumption can result in incoherent clusters, like
{hillary clinton     clinton     mr clinton}, in which the pairwise links score well, but the
overall result is unsatisfactory. entity-based models address this issue by scoring entities
holistically. this can make id136 more dif   cult, since the number of possible entity
groupings is exponential in the number of mentions.

15.2.1 mention-pair models

in the mention-pair model, a binary label yi,j     {0, 1} is assigned to each pair of mentions
(i, j), where i < j. if i and j corefer (zi = zj), then yi,j = 1; otherwise, yi,j = 0. the
mention he in figure 15.1 is preceded by    ve other mentions: (1) apple inc; (2) apple inc
chief executive tim cook; (3) china; (4) talks; (5) government of   cials. the correct mention
pair labeling is y2,6 = 1 and yi(cid:54)=2,6 = 0 for all other i. if a mention j introduces a new entity,
such as mention 3 in the example, then yi,j = 0 for all i. the same is true for    mentions   
that do not refer to any entity, such as non-referential pronouns. if mention j refers to an
entity that has been mentioned more than once, then yi,j = 1 for all i < j that mention the
referent.

by transforming coreference into a set of binary labeling problems, the mention-pair
model makes it possible to apply an off-the-shelf binary classi   er (soon et al., 2001). this
classi   er is applied to each mention j independently, searching backwards from j until
   nding an antecedent i which corefers with j with high con   dence. after identifying a
single antecedent, the remaining mention pair labels can be computed by transitivity: if
yi,j = 1 and yj,k = 1, then yi,k = 1.

since the ground truth annotations give entity chains c but not individual mention-
pair labels y, an additional heuristic must be employed to convert the labeled data into
training examples for classi   cation. a typical approach is to generate at most one pos-
itive labeled instance yaj ,j = 1 for mention j, where aj is the index of the most recent
antecedent, aj = max{i : i < j     zi = zj}. negative labeled instances are generated for
all for all i     {aj + 1, . . . , j}. in the running example, the most recent antecedent of the
pronoun he is a6 = 2, so the training data would be y2,6 = 1 and y3,6 = y4,6 = y5,6 = 0.
the variable y1,6 is not part of the training data, because the    rst mention appears before
the true antecedent a6 = 2.

under contract with mit press, shared under cc-by-nc-nd license.

360

chapter 15. reference resolution

15.2.2 mention-ranking models
in mention ranking (denis and baldridge, 2007), the classi   er learns to identify a single
antecedent ai     { , 1, 2, . . . , i     1} for each referring expression i,

  ai = argmax

a   { ,1,2,...,i   1}

  m (a, i),

[15.4]

where   m (a, i) is a score for the mention pair (a, i). if a =  , then mention i does not refer
to any previously-introduced entity     it is not anaphoric. mention-ranking is similar to
the mention-pair model, but all candidates are considered simultaneously, and at most
a single antecedent is selected. the mention-ranking model explicitly accounts for the
possibility that mention i is not anaphoric, through the score   m ( , i). the determination
of anaphoricity can be made by a special classi   er in a preprocessing step, so that non- 
antecedents are identi   ed only for spans that are determined to be anaphoric (denis and
baldridge, 2008).

as a learning problem, ranking can be trained using the same objectives as in dis-
criminative classi   cation. for each mention i, we can de   ne a gold antecedent a   
i , and an
associated loss, such as the hinge loss, (cid:96)i = (1       m (a   
i , i) +   m (  a, i))+ or the negative
log-likelihood, (cid:96)i =     log p(a   
i | i;   ). (for more on learning to rank, see    17.1.1.) but as
with the mention-pair model, there is a mismatch between the labeled data, which comes
in the form of mention sets, and the desired supervision, which would indicate the spe-
i=1 relate to the mention
ci   c antecedent of each mention. the antecedent variables {ai}m
sets in a many-to-one mapping: each set of antecedents induces a single id91, but a
id91 can correspond to many different settings of antecedent variables.

a heuristic solution is to set a   

i = max{j : j < i     zj = zi}, the most recent mention in
the same cluster as i. but the most recent mention may not be the most informative: in the
running example, the most recent antecedent of the mention cook is the pronoun he, but
a more useful antecedent is the earlier mention apple inc chief executive tim cook. rather
than selecting a speci   c antecedent to train on, the antecedent can be treated as a latent
variable, in the manner of the latent variable id88 from    12.4.2 (fernandes et al.,
2014):

  a = argmax

a

   
a

= argmax
a   a(c)

         +

m(cid:88)i=1

  m (ai, i)

  m (ai, i)

m(cid:88)i=1
m(cid:88)i=1

   l
     

  m (a

   
i , i)    

   l
     

m(cid:88)i=1

  m (  ai, i)

[15.5]

[15.6]

[15.7]

jacob eisenstein. draft of november 13, 2018.

15.2. algorithms for coreference resolution

361

where a(c) is the set of antecedent structures that is compatible with the ground truth
coreference id91 c. another alternative is to sum over all the conditional probabili-
ties of antecedent structures that are compatible with the ground truth id91 (durrett
and klein, 2013; lee et al., 2017). for the set of mention m, we compute the following
probabilities:

p(c | m) = (cid:88)a   a(c)
p(ai | i, m) =

p(a | m) = (cid:88)a   a(c)

m(cid:89)i=1
(cid:80)a(cid:48)   { ,1,2,...,i   1} exp (  m (a(cid:48), i))

exp (  m (ai, i))

.

p(ai | i, m)

[15.8]

[15.9]

this objective rewards models that assign high scores for all valid antecedent structures.
in the running example, this would correspond to summing the probabilities of the two
valid antecedents for cook, he and apple inc chief executive tim cook. in one of the exer-
cises, you will compute the number of valid antecedent structures for a given id91.

15.2.3 transitive closure in mention-based models

a problem for mention-based models is that individual mention-level decisions may be
incoherent. consider the following mentions:

m1 =hillary clinton
m2 =clinton
m3 =bill clinton

[15.10]
[15.11]
[15.12]

a mention-pair system might predict   y1,2 = 1,   y2,3 = 1,   y1,3 = 0. similarly, a mention-
ranking system might choose   a2 = 1 and   a3 = 2. logically, if mentions 1 and 3 are both
coreferent with mention 2, then all three mentions must refer to the same entity. this
constraint is known as transitive closure.

transitive closure can be applied post hoc, revising the independent mention-pair or
mention-ranking decisions. however, there are many possible ways to enforce transitive
closure: in the example above, we could set   y1,3 = 1, or   y1,2 = 0, or   y2,3 = 0. for docu-
ments with many mentions, there may be many violations of transitive closure, and many
possible    xes. transitive closure can be enforced by always adding edges, so that   y1,3 = 1
is preferred (e.g., soon et al., 2001), but this can result in overid91, with too many
mentions grouped into too few entities.

mention-pair coreference resolution can be viewed as a constrained optimization prob-

under contract with mit press, shared under cc-by-nc-nd license.

362

lem,

max

y   {0,1}m

s.t.

m(cid:88)j=1

j(cid:88)i=1

chapter 15. reference resolution

  m (i, j)    yi,j

yi,j + yj,k     1     yi,k,

   i < j < k,

with the constraint enforcing transitive closure. this constrained optimization problem
is equivalent to graph partitioning with positive and negative edge weights: construct a
graph where the nodes are mentions, and the edges are the pairwise scores   m (i, j); the
goal is to partition the graph so as to maximize the sum of the edge weights between all
nodes within the same partition (mccallum and wellner, 2004). this problem is np-hard,
motivating approximations such as correlation id91 (bansal et al., 2004) and integer
id135 (klenner, 2007; finkel and manning, 2008, also see    13.2.2).
15.2.4 entity-based models
a weakness of mention-based models is that they treat coreference resolution as a classi   -
cation or ranking problem, when it is really a id91 problem: the goal is to group the
mentions together into clusters that correspond to the underlying entities. entity-based
approaches attempt to identify these clusters directly. such methods require a scoring
function at the entity level, measuring whether each set of mentions is internally consis-
tent. coreference resolution can then be viewed as the following optimization,

max

z (cid:88)e=1

  e({i : zi = e}),

[15.13]

where zi indicates the entity referenced by mention i, and   e({i : zi = e}) is a scoring
function applied to all mentions i that are assigned to entity e.

entity-based coreference resolution is conceptually similar to the unsupervised clus-
tering problems encountered in chapter 5: the goal is to obtain clusters of mentions that
are internally coherent. the number of possible id91s of n items is the bell number,
which is de   ned by the following recurrence (bell, 1934; luo et al., 2004),

bn =

n   1(cid:88)k=0

bk(cid:18)n     1

k (cid:19)b0 =

b1 = 1.

[15.14]

this recurrence is illustrated by the bell tree, which is applied to a short coreference prob-
lem in figure 15.4. the bell number bn grows exponentially with n, making exhaustive
search of the space of id91s impossible. for this reason, entity-based coreference
resolution typically involves incremental search, in which id91 decisions are based
on local evidence, in the hope of approximately optimizing the full objective in equa-
tion 15.13. this approach is sometimes called cluster ranking, in contrast to mention
ranking.

jacob eisenstein. draft of november 13, 2018.

15.2. algorithms for coreference resolution

363

figure 15.4: the bell tree for the sentence abigail hopes she speaks with her. which paths
are excluded by the syntactic constraints mentioned in    15.1.1?

*generative models of coreference entity-based coreference can be approached through
probabilistic generative models, in which the mentions in the document are conditioned
on a set of latent entities (haghighi and klein, 2007, 2010). an advantage of these meth-
ods is that they can be learned from unlabeled data (poon and domingos, 2008, e.g.,); a
disadvantage is that probabilistic id136 is required not just for learning, but also for
prediction. furthermore, generative models require independence assumptions that are
dif   cult to apply in coreference resolution, where the diverse and heterogeneous features
do not admit an easy decomposition into mutually independent subsets.

incremental cluster ranking
the smash method (   15.1.1) can be extended to entity-based coreference resolution by
building up coreference clusters while moving through the document (cardie and wagstaff,
1999). at each mention, the algorithm iterates backwards through possible antecedent
clusters; but unlike smash, a cluster is selected only if all members of its cluster are com-
patible with the current mention. as mentions are added to a cluster, so are their features
(e.g., gender, number, animacy). in this way, incoherent chains like {hillary clinton, clinton, bill clinton}
can be avoided. however, an incorrect assignment early in the document     a search error
    might lead to a cascade of errors later on.

more sophisticated search strategies can help to ameliorate the risk of search errors.
one approach is id125 (   rst discussed in    11.3), in which a set of hypotheses is
maintained throughout search. each hypothesis represents a path through the bell tree
(figure 15.4). hypotheses are    expanded    either by adding the next mention to an ex-
isting cluster, or by starting a new cluster. each expansion receives a score, based on
equation 15.13, and the top k hypotheses are kept on the beam as the algorithm moves
to the next step.

under contract with mit press, shared under cc-by-nc-nd license.

{abigail}{abigail,she}{abigail},{she}{abigail,she,her}{abigail,she},{her}{abigail},{she,her}{abigail,her},{she}{abigail},{she},{her}364

chapter 15. reference resolution

incremental cluster ranking can be made more accurate by performing multiple passes
over the document, applying rules (or    sieves   ) with increasing recall and decreasing
precision at each pass (lee et al., 2013). in the early passes, coreference links are pro-
posed only between mentions that are highly likely to corefer (e.g., exact string match
for full names and nominals). information can then be shared among these mentions,
so that when more permissive matching rules are applied later, agreement is preserved
across the entire cluster. for example, in the case of {hillary clinton, clinton, she}, the
name-matching sieve would link clinton and hillary clinton, and the pronoun-matching
sieve would then link she to the combined cluster. a deterministic multi-pass system
won nearly every track of the 2011 conll shared task on coreference resolution (prad-
han et al., 2011). given the dominance of machine learning in virtually all other areas
of natural language processing     and more than    fteen years of prior work on machine
learning for coreference     this was a surprising result, even if learning-based methods
have subsequently regained the upper hand (e.g., lee et al., 2018, the state of the art at the
time of this writing).

incremental id88

incremental coreference resolution can be learned with the incremental id88, as
described in    11.3.2. at mention i, each hypothesis on the beam corresponds to a cluster-
ing of mentions 1 . . . i    1, or equivalently, a path through the bell tree up to position i    1.
as soon as none of the hypotheses on the beam are compatible with the gold coreference
id91, a id88 update is made (daum  e iii and marcu, 2005). for concreteness,
consider a linear cluster ranking model,

  e({i : zi = e}) = (cid:88)i:zi=e

      f (i,{j : j < i     zj = e}),

[15.15]

where the score for each cluster is computed as the sum of scores of all mentions that are
linked into the cluster, and f (i,    ) is a set of features for the non-anaphoric mention that
initiates the cluster.

using figure 15.4 as an example, suppose that the ground truth is,

but that with a beam of size one, the learner reaches the hypothesis,

   

c

= {abigail, her},{she},

this hypothesis is incompatible with c   , so an update is needed:

  c = {abigail, she}.

   

         + f (c

)     f (  c)

=   + (f (abigail,    ) + f (she,    ))     (f (abigail,    ) + f (she,{abigail}))
=   + f (she,    )     f (she,{abigail}).

[15.16]

[15.17]

[15.18]
[15.19]
[15.20]

jacob eisenstein. draft of november 13, 2018.

15.2. algorithms for coreference resolution

365

this style of incremental update can also be applied to a margin loss between the gold
id91 and the top id91 on the beam. by backpropagating from this loss, it is also
possible to train a more complicated scoring function, such as a neural network in which
the score for each entity is a function of embeddings for the entity mentions (wiseman
et al., 2015).

id23

id23 is a topic worthy of a textbook of its own (sutton and barto,
1998),5 so this section will provide only a very brief overview, in the context of coreference
resolution. a stochastic policy assigns a id203 to each possible action, conditional
on the context. the goal is to learn a policy that achieves a high expected reward, or
equivalently, a low expected cost.

in incremental cluster ranking, a complete id91 on m mentions can be produced
by a sequence of m actions, in which the action zi either merges mention i with an existing
cluster or begins a new cluster. we can therefore create a stochastic policy using the cluster
scores (clark and manning, 2016),

pr(zi = e;   ) =

,

[15.21]

exp   e(i     {j : zj = e};   )
(cid:48);   )
}

(cid:80)e(cid:48) exp   e(i     {j : zj = e(cid:48)

where   e(i     {j : zj = e};   ) is the score under parameters    for assigning mention i to
cluster e. this score can be an arbitrary function of the mention i, the cluster e and its
(possibly empty) set of mentions; it can also include the history of actions taken thus far.

if a policy assigns id203 p(c;   ) to id91 c, then its expected loss is,

l(  ) = (cid:88)c   c(m)

p  (c)    (cid:96)(c),

[15.22]

where c(m) is the set of possible id91s for mentions m. the loss (cid:96)(c) can be based on
any arbitrary scoring function, including the complex id74 used in corefer-
ence resolution (see    15.4). this is an advantage of id23, which can be
trained directly on the evaluation metric     unlike traditional supervised learning, which
requires a id168 that is differentiable and decomposable across individual deci-
sions.

rather than summing over the exponentially many possible id91s, we can ap-
proximate the expectation by sampling trajectories of actions, z = (z1, z2, . . . , zm ), from

5a draft of

the second edition can be found here:

http://incompleteideas.net/book/
the-book-2nd.html. id23 has been used in spoken dialogue systems (walker, 2000)
and text-based game playing (branavan et al., 2009), and was applied to coreference resolution by clark and
manning (2015).

under contract with mit press, shared under cc-by-nc-nd license.

366

chapter 15. reference resolution

the current policy. each action zi corresponds to a step in the bell tree: adding mention
mi to an existing cluster, or forming a new cluster. each trajectory z corresponds to a
single id91 c, and so we can write the loss of an action sequence as (cid:96)(c(z)). the
policy gradient algorithm computes the gradient of the expected loss as an expectation
over trajectories (sutton et al., 2000),

   
     

l(  ) =ez   z(m)(cid:96)(c(z))

1
k

   

k(cid:88)k=1

(cid:96)(c(z(k)))

   
     

   
     

m(cid:88)i=1
m(cid:88)i=1

log p(zi | z1:i   1, m)

log p(z(k)

i

| z(k)

1:i   1, m),

[15.23]

[15.24]

where each action sequence z(k) is sampled from the current policy. unlike the incremen-
tal id88, an update is not made until the complete action sequence is available.

learning to search

policy gradient can suffer from high variance: while the average loss over k samples is
asymptotically equal to the expected reward of a given policy, this estimate may not be
accurate unless k is very large. this can make it dif   cult to allocate credit and blame to
individual actions. in learning to search, this problem is addressed through the addition
of an oracle policy, which is known to receive zero or small loss. the oracle policy can be
used in two ways:

    the oracle can be used to generate partial hypotheses that are likely to score well,
by generating i actions from the initial state. these partial hypotheses are then used
as starting points for the learned policy. this is known as roll-in.

    the oracle can be used to compute the minimum possible loss from a given state, by
generating m     i actions from the current state until completion. this is known as
roll-out.

the oracle can be combined with the existing policy during both roll-in and roll-out, sam-
pling actions from each policy (daum  e iii et al., 2009). one approach is to gradually
decrease the number of actions drawn from the oracle over the course of learning (ross
et al., 2011).

in the context of entity-based coreference resolution, clark and manning (2016) use
the learned policy for roll-in and the oracle policy for roll-out. algorithm 17 shows how
the gradients on the policy weights are computed in this case. in this application, the
oracle is    noisy   , because it selects the action that minimizes only the local loss     the
accuracy of the coreference id91 up to mention i     rather than identifying the action
sequence that will lead to the best    nal coreference id91 on the entire document.

jacob eisenstein. draft of november 13, 2018.

15.3. representations for coreference resolution

367

algorithm 17 learning to search for entity-based coreference resolution
1: procedure compute-gradient(mentions m, id168 (cid:96), parameters   )
2:
3:
4:
5:
6:
7:
8:

l(  )     0
z     p(z | m;   )
for i     {1, 2, . . . m} do
h     z1:i   1     z
for j     {i + 1, i + 2, . . . , m} do
hj     argminh (cid:96)(h1:j   1     h)
l(  )     l(  ) + p(z | z1:i   1, m;   )    (cid:96)(h)
      l(  )

(cid:46) all possible actions after history z1:i   1
(cid:46) concatenate history z1:i   1 with action z
(cid:46) roll-out
(cid:46) oracle selects action with minimum loss
(cid:46) update expected loss

for action z     z(z1:i   1, m) do

(cid:46) sample a trajectory from the current policy

return    

9:

10:

when learning from noisy oracles, it can be helpful to mix in actions from the current
policy with the oracle during roll-out (chang et al., 2015).

15.3 representations for coreference resolution

historically, coreference resolution has employed an array of hand-engineered features
to capture the linguistic constraints and preferences described in    15.1 (soon et al., 2001).
later work has documented the utility of lexical and bilexical features on mention pairs (bj  orkelund
and nugues, 2011; durrett and klein, 2013). the most recent and successful methods re-
place many (but not all) of these features with distributed representations of mentions
and entities (wiseman et al., 2015; clark and manning, 2016; lee et al., 2017).

15.3.1 features

coreference features generally rely on a preprocessing pipeline to provide part-of-speech
tags and phrase structure parses. this pipeline makes it possible to design features that
capture many of the phenomena from    15.1, and is also necessary for typical approaches
to mention identi   cation. however, the pipeline may introduce errors that propagate
to the downstream coreference id91 system. furthermore, the existence of such
a pipeline presupposes resources such as treebanks, which do not exist for many lan-
guages.6

6the universal dependencies project has produced dependency treebanks for more than sixty languages.
however, coreference features and mention detection are generally based on phrase structure trees, which
exist for roughly two dozen languages. a list is available here: https://en.wikipedia.org/wiki/
treebank

under contract with mit press, shared under cc-by-nc-nd license.

368

chapter 15. reference resolution

mention features

features of individual mentions can help to predict anaphoricity. in systems where men-
tion detection is performed jointly with coreference resolution, these features can also
predict whether a span of text is likely to be a mention. for mention i, typical features
include:

mention type. each span can be identi   ed as a pronoun, name, or nominal, using the
part-of-speech of the head word of the mention: both the id32 and uni-
versal dependencies tagsets (   8.1.1) include tags for pronouns and proper nouns,
and all other heads can be marked as nominals (haghighi and klein, 2009).

mention width. the number of tokens in a mention is a rough predictor of its anaphoric-
ity, with longer mentions being less likely to refer back to previously-de   ned enti-
ties.

lexical features. the    rst, last, and head words can help to predict anaphoricity; they are
also useful in conjunction with features such as mention type and part-of-speech,
providing a rough measure of agreement (bj  orkelund and nugues, 2011). the num-
ber of lexical features can be very large, so it can be helpful to select only frequently-
occurring features (durrett and klein, 2013).

morphosyntactic features. these features include the part-of-speech, number, gender,

and dependency ancestors.

the features for mention i and candidate antecedent a can be conjoined, producing
joint features that can help to assess the compatibility of the two mentions. for example,
durrett and klein (2013) conjoin each feature with the mention types of the anaphora
and the antecedent. coreference resolution corpora such as ace and ontonotes contain
documents from various genres. by conjoining the genre with other features, it is possible
to learn genre-speci   c feature weights.

mention-pair features

for any pair of mentions i and j, typical features include:

distance. the number of intervening tokens, mentions, and sentences between i and j
can all be used as distance features. these distances can be computed on the surface
text, or on a transformed representation re   ecting the breadth-   rst tree traversal
(figure 15.3). rather than using the distances directly, they are typically binned,
creating binary features.

jacob eisenstein. draft of november 13, 2018.

15.3. representations for coreference resolution

369

string match. a variety of string match features can be employed: exact match, suf   x
match, head match, and more complex matching rules that disregard irrelevant
modi   ers (soon et al., 2001).

compatibility. building on the model, features can measure the anaphor and antecedent
agree with respect to morphosyntactic attributes such as gender, number, and ani-
macy.

nesting. if one mention is nested inside another (e.g., [the president of [france]]), they

generally cannot corefer.

same speaker. for documents with quotations, such as news articles, personal pronouns
can be resolved only by determining the speaker for each mention (lee et al., 2013).
coreference is also more likely between mentions from the same speaker.

gazetteers. these features indicate that the anaphor and candidate antecedent appear in
a gazetteer of acronyms (e.g., usa/united states, gatech/georgia tech), demonyms
(e.g., israel/israeli), or other aliases (e.g., knickerbockers/new york knicks).

lexical semantics. these features use a lexical resource such as id138 to determine
whether the head words of the mentions are related through synonymy, antonymy,
and hypernymy (   4.2).

dependency paths. the dependency path between the anaphor and candidate antecedent
can help to determine whether the pair can corefer, under the government and bind-
ing constraints described in    15.1.1.

comprehensive lists of mention-pair features are offered by bengtson and roth (2008) and
rahman and ng (2011). neural network approaches use far fewer mention-pair features:
for example, lee et al. (2017) include only speaker, genre, distance, and mention width
features.

semantics
in many cases, coreference seems to require knowledge and semantic infer-
ences, as in the running example, where we link china with a country and a growth mar-
ket. some of this information can be gleaned from id138, which de   nes a graph
over synsets (see    4.2). for example, one of the synsets of china is an instance of an
asian nation#1, which in turn is a hyponym of country#2, a synset that includes
country.7 such paths can be used to measure the similarity between concepts (pedersen
et al., 2004), and this similarity can be incorporated into coreference resolution as a fea-
ture (ponzetto and strube, 2006). similar ideas can be applied to id13s in-
duced from wikipedia (ponzetto and strube, 2007). but while such approaches improve

7teletype font is used to indicate id138 synsets, and italics is used to indicate strings.

under contract with mit press, shared under cc-by-nc-nd license.

370

chapter 15. reference resolution

relatively simple classi   cation-based systems, they have proven less useful when added
to the current generation of techniques.8 for example, durrett and klein (2013) employ
a range of semantics-based features     id138 synonymy and hypernymy relations on
head words, named entity types (e.g., person, organization), and unsupervised cluster-
ing over nominal heads     but    nd that these features give minimal improvement over a
baseline system using surface features.

entity features

many of the features for entity-mention coreference are generated by aggregating mention-
pair features over all mentions in the candidate entity (culotta et al., 2007; rahman and
ng, 2011). speci   cally, for each binary mention-pair feature f (i, j), we compute the fol-
lowing entity-mention features for mention i and entity e = {j : j < i     zj = e}.

    all-true: feature f (i, j) holds for all mentions j     e.
    most-true: feature f (i, j) holds for at least half and fewer than all mentions j     e.
    most-false: feature f (i, j) holds for at least one and fewer than half of all men-

tions j     e.

    none: feature f (i, j) does not hold for any mention j     e.

for scalar mention-pair features (e.g., distance features), aggregation can be performed by
computing the minimum, maximum, and median values across all mentions in the cluster.
additional entity-mention features include the number of mentions currently clustered in
the entity, and all-x and most-x features for each mention type.

15.3.2 distributed representations of mentions and entities

recent work has emphasized distributed representations of both mentions and entities.
one potential advantage is that pre-trained embeddings could help to capture the se-
mantic compatibility underlying nominal coreference, helping with dif   cult cases like
(apple, the    rm) and (china, the    rm   s biggest growth market). furthermore, a distributed
representation of entities can be trained to capture semantic features that are added by
each mention.

mention embeddings

entity mentions can be embedded into a vector space, providing the base layer for neural
networks that score coreference decisions (wiseman et al., 2015).

8this point was made by michael strube at a 2015 workshop, noting that as the quality of the machine

learning models in coreference has improved, the bene   t of including semantics has become negligible.

jacob eisenstein. draft of november 13, 2018.

15.3. representations for coreference resolution

371

figure 15.5: a bidirectional recurrent model of mention embeddings. the mention is
represented by its    rst word, its last word, and an estimate of its head word, which is
computed from a weighted average (lee et al., 2017).

constructing the mention embedding various approaches for embedding multiword
units can be applied (see    14.8). figure 15.5 shows a recurrent neural network approach,
which begins by running a bidirectional lstm over the entire text, obtaining hidden states
from the left-to-right and right-to-left passes, hm = [      h m;      h m]. each candidate mention
span (s, t) is then represented by the vertical concatenation of four vectors:

u(s,t) = [u(s,t)

   rst ; u(s,t)

last ; u(s,t)

head;   (s,t)],

[15.25]

   rst = hs+1 is the embedding of the    rst word in the span, u(s,t)

where u(s,t)
embedding of the last word, u(s,t)
vector of surface features, such as the length of the span (lee et al., 2017).

last = ht is the
head is the embedding of the    head    word, and   (s,t) is a

attention over head words rather than identifying the head word from the output of a
parser, it can be computed from a neural attention mechanism:

    m =        hm
a(s,t) = softmax ([    s+1,     s+2, . . . ,     t])

u(s,t)

head =

t(cid:88)m=s+1

a(s,t)
m hm.

[15.26]
[15.27]

[15.28]

each token m gets a scalar score     m =         hm, which is the dot product of the lstm
hidden state hm and a vector of weights     . the vector of scores for tokens in the span
m     {s + 1, s + 2, . . . , t} is then passed through a softmax layer, yielding a vector a(s,t)
that allocates one unit of attention across the span. this eliminates the need for syntactic
parsing to recover the head word; instead, the model learns to identify the most important
words in each span. attention mechanisms were introduced in neural machine transla-
tion (bahdanau et al., 2014), and are described in more detail in    18.3.1.

under contract with mit press, shared under cc-by-nc-nd license.

u   rstuheadulast            inthe   rm   sbiggestgrowthmarket.372

chapter 15. reference resolution

using mention embeddings given a set of mention embeddings, each mention i and
candidate antecedent a is scored as,

  (a, i) =  s(a) +   s(i) +   m (a, i)
  s(a) =feedforwards(u(a))
  s(i) =feedforwards(u(i))

  m (a, i) =feedforwardm ([u(a); u(i); u(a) (cid:12) u(i); f (a, i, w)]),

[15.29]
[15.30]
[15.31]
[15.32]

where u(a) and u(i) are the embeddings for spans a and i respectively, as de   ned in equa-
tion 15.25.

    the scores   s(a) quantify whether span a is likely to be a coreferring mention, inde-
pendent of what it corefers with. this allows the model to learn identify mentions
directly, rather than identifying mentions with a preprocessing step.

    the score   m (a, i) computes the compatibility of spans a and i. its base layer is a
vector that includes the embeddings of spans a and i, their elementwise product
u(a) (cid:12) u(i), and a vector of surface features f (a, i, w), including distance, speaker,
and genre information.

lee et al. (2017) provide an error analysis that shows how this method can correctly link
a blaze and a    re, while incorrectly linking pilots and    ght attendants.
in each case, the
coreference decision is based on similarities in the id27s.

rather than embedding individual mentions, clark and manning (2016) embed men-
tion pairs. at the base layer, their network takes embeddings of the words in and around
each mention, as well as one-hot vectors representing a few surface features, such as the
distance and string matching features. this base layer is then passed through a multilayer
feedforward network with relu nonlinearities, resulting in a representation of the men-
tion pair. the output of the mention pair encoder ui,j is used in the scoring function of
a mention-ranking model,   m (i, j) =       ui,j. a similar approach is used to score cluster
pairs, constructing a cluster-pair encoding by pooling over the mention-pair encodings
for all pairs of mentions within the two clusters.

entity embeddings

in entity-based coreference resolution, each entity should be represented by properties of
its mentions. in a distributed setting, we maintain a set of vector entity embeddings, ve.
each candidate mention receives an embedding ui; wiseman et al. (2016) compute this
embedding by a single-layer neural network, applied to a vector of surface features. the
decision of whether to merge mention i with entity e can then be driven by a feedforward

jacob eisenstein. draft of november 13, 2018.

15.4. evaluating coreference resolution

373

network,   e(i, e) = feedforward([ve; ui]). if i is added to entity e, then its representa-
tion is updated recurrently, ve     f (ve, ui), using a recurrent neural network such as a
long short-term memory (lstm; chapter 6). alternatively, we can apply a pooling oper-
ation, such as max-pooling or average-pooling (chapter 3), setting ve     pool(ve, ui). in
either case, the update to the representation of entity e can be thought of as adding new
information about the entity from mention i.

15.4 evaluating coreference resolution

the state of coreference evaluation is aggravatingly complex. early attempts at sim-
ple id74 were found to be susceptible to trivial baselines, such as placing
each mention in its own cluster, or grouping all mentions into a single cluster. follow-
ing denis and baldridge (2009), the conll 2011 shared task on coreference (pradhan
et al., 2011) formalized the practice of averaging across three different metrics: muc (vi-
lain et al., 1995), b-cubed (bagga and baldwin, 1998a), and ceaf (luo, 2005). refer-
ence implementations of these metrics are available from pradhan et al. (2014) at https:
//github.com/conll/reference-coreference-scorers.

additional resources

ng (2010) surveys coreference resolution through 2010. early work focused exclusively
on pronoun resolution, with rule-based (lappin and leass, 1994) and probabilistic meth-
ods (ge et al., 1998). the full coreference resolution problem was popularized in a shared
task associated with the sixth message understanding conference, which included coref-
erence annotations for training and test sets of thirty documents each (grishman and
sundheim, 1996). an in   uential early paper was the decision tree approach of soon et al.
(2001), who introduced mention ranking. a comprehensive list of surface features for
coreference resolution is offered by bengtson and roth (2008). durrett and klein (2013)
improved on prior work by introducing a large lexicalized feature set; subsequent work
has emphasized neural representations of entities and mentions (wiseman et al., 2015).

exercises

1. select an article from today   s news, and annotate coreference for the    rst twenty
noun phrases and possessive pronouns that appear in the article, include ones that
are nested within larger noun phrases. then specify the mention-pair training data
that would result from the    rst    ve of these candidate entity mentions.

2. using your annotations from the preceding problem, compute the following statis-

tics:

under contract with mit press, shared under cc-by-nc-nd license.

374

chapter 15. reference resolution

    the number of times new entities are introduced by each of the three types of
referring expressions: pronouns, proper nouns, and nominals. include    single-
ton    entities that are mentioned only once.

    for each type of referring expression, compute the fraction of mentions that are

anaphoric.

3. apply a simple heuristic to all pronouns in the article from the previous exercise:
link each pronoun to the closest preceding noun phrase that agrees in gender, num-
ber, animacy, and person. compute the following evaluation:

    true positive: a pronoun that is linked to a noun phrase with which it is coref-
erent, or is labeled as the    rst mention of an entity when in fact it does not
corefer with any preceding mention. in this case, non-referential pronouns can
be true positives if they are marked as having no antecedent.

    false positive: a pronoun that is linked to a noun phrase with which it is not
coreferent. this includes mistakenly linking singleton or non-referential pro-
nouns.

    false negative: a pronoun that has at least one antecedent, but is either labeled
as not having an antecednet, or is linked to mention with which it does not
corefer.

compute the f -measure for your method, and for a trivial baseline in which every
pronoun refers to the immediately preceding entity mention. are there any addi-
tional heuristics that would have improved the performance of this method?

4. durrett and klein (2013) compute the id203 of the gold coreference id91
by summing over all antecedent structures that are compatible with the id91.
for example, if there are three mentions of a single entity, m1, m2, m3, there are two
possible antecedent structures: a2 = 1, a3 = 1 and a2 = 1, a3 = 2. compute the
number of antecedent structures for a single entity with k mentions.

5. suppose that all mentions can be unambiguously divided into c classes, for exam-
ple by gender and number. further suppose that mentions from different classes
can never corefer. in a document with m mentions, give upper and lower bounds
on the total number of possible coreference id91s, in terms of the bell numbers
and the parameters m and c. compute numerical upper and lower bounds for the
case m = 4, c = 2.

6. lee et al. (2017) propose a model that considers all contiguous spans in a document

as possible mentions.

a) in a document of length m, how many mention pairs must be evaluated? (all

answers can be given in asymptotic, big-o notation.)

jacob eisenstein. draft of november 13, 2018.

15.4. evaluating coreference resolution

375

b) to make id136 more ef   cient, lee et al. (2017) restrict consideration to spans
of maximum length l (cid:28) m. under this restriction, how many mention pairs
must be evaluated?

c) to further improve id136, one might evaluate coreference only between
pairs of mentions whose endpoints are separated by a maximum of d tokens.
under this additional restriction, how many mention pairs must be evaluated?

7. in spanish, the subject can be omitted when it is clear from context, e.g.,

mam    feros.
(15.13) las ballenas
the whales
mammals.
whales are not    sh. they are mammals.

peces.
   sh.

son
are

son
are

no
no

resolution of such null subjects is facilitated by the spanish system of verb mor-
phology, which includes distinctive suf   xes for most combinations of person and
number. for example, the verb form son (   are   ) agrees with the third-person plural
pronouns ellos (masculine) and ellas (feminine), as well as the second-person plural
ustedes.

suppose that you are given the following components:

null subject j, according to the verb morphology.

    a system that automatically identi   es verbs with null subjects.
    a function c(j, p)     {0, 1} that indicates whether pronoun p is compatible with
    a trained mention-pair model, which computes scores   (wi, wj, j     i)     r for
all pairs of mentions i and j, scoring the pair by the antecedent mention wi, the
anaphor wj, and the distance j     i.

describe an integer linear program that simultaneously performs two tasks: resolv-
ing coreference among all entity mentions, and identifying suitable pronouns for all
null subjects. in the example above, your program should link the null subject with
las ballenas (   whales   ), and identify ellas as the correct pronoun. for simplicity, you
may assume that null subjects cannot be antecedents, and you need not worry about
the transitivity constraint described in    15.2.3.

8. use the policy gradient algorithm to compute the gradient for the following sce-

nario, based on the bell tree in figure 15.4:
    the gold id91 c    is {abigail, her},{she}.

under contract with mit press, shared under cc-by-nc-nd license.

376

chapter 15. reference resolution

    drawing a single sequence of actions (k = 1) from the current policy, you

obtain the following incremental id91s:

c(a1) ={abigail}
c(a1:2) ={abigail, she}
c(a1:3) ={abigail, she},{her}.

    at each mention t, the space of actions at includes merging the mention with
each existing cluster or with the empty cluster. the id203 of merging mt
with cluster c is proportional to the exponentiated score for the merged cluster,

p(merge(mt, c)))     exp   e(mt     c),

[15.33]

where   e(mt     c) is de   ned in equation 15.15.

compute the gradient    
(potential) cluster. explain the differences between the gradient-based update                  
and the incremental id88 update from this same example.

      l(  ) in terms of the loss (cid:96)(c(a)) and the features of each

      l(  )

9. as discussed in    15.1.1, some pronouns are not referential. in english, this occurs
frequently with the word it. download the text of alice in wonderland from nltk,
and examine the    rst ten appearances of it. for each occurrence:

    first, examine a    ve-token window around the word. in the    rst example, this

window is,

, but it had no

is there another pronoun that could be substituted for it? consider she, they,
and them. in this case, both she and they yield grammatical substitutions. what
about the other ten appearances of it?

    now, view an    fteen-word window for each example. based on this window,

mark whether you think the word it is referential.

how often does the substitution test predict whether it is referential?

10. now try to automate the test, using the google id165s corpus (brants and franz,
2006). speci   cally,    nd the count of each 5-gram containing it, and then compute
the counts of 5-grams in which it is replaced with other third-person pronouns: he,
she, they, her, him, them, herself, himself.
there are various ways to get these counts. one approach is to download the
raw data and search it; another is to construct web queries to https://books.
google.com/ngrams.

jacob eisenstein. draft of november 13, 2018.

15.4. evaluating coreference resolution

377

compare the ratio of the counts of the original 5-gram to the summed counts of
the 5-grams created by substitution. is this ratio a good predictor of whether it is
referential?

under contract with mit press, shared under cc-by-nc-nd license.

chapter 16

discourse

applications of natural language processing often concern multi-sentence documents:
from paragraph-long restaurant reviews, to 500-word newspaper articles, to 500-page
novels. yet most of the methods that we have discussed thus far are concerned with
individual sentences. this chapter discusses theories and methods for handling multi-
sentence linguistic phenomena, known collectively as discourse. there are diverse char-
acterizations of discourse structure, and no single structure is ideal for every computa-
tional application. this chapter covers some of the most well studied discourse repre-
sentations, while highlighting computational models for identifying and exploiting these
structures.

16.1 segments

a document or conversation can be viewed as a sequence of segments, each of which is
cohesive in its content and/or function. in wikipedia biographies, these segments often
pertain to various aspects to the subject   s life: early years, major events, impact on others,
and so on. this segmentation is organized around topics. alternatively, scienti   c research
articles are often organized by functional themes: the introduction, a survey of previous
research, experimental setup, and results.

written texts often mark segments with section headers and related formatting de-
vices. however, such formatting may be too coarse-grained to support applications such
as the retrieval of speci   c passages of text that are relevant to a query (hearst, 1997).
unformatted speech transcripts, such as meetings and lectures, are also an application
scenario for segmentation (carletta, 2007; glass et al., 2007; janin et al., 2003).

379

380

chapter 16. discourse

figure 16.1: smoothed cosine similarity among adjacent sentences in a news article. local
minima at m = 10 and m = 29 indicate likely segmentation points.

16.1.1 topic segmentation
a cohesive topic segment forms a uni   ed whole, using various linguistic devices: re-
peated references to an entity or event; the use of conjunctions to link related ideas; and
the repetition of meaning through lexical choices (halliday and hasan, 1976). each of
these cohesive devices can be measured, and then used as features for topic segmentation.
a classical example is the use of lexical cohesion in the texttiling method for topic seg-
mentation (hearst, 1997). the basic idea is to compute the textual similarity between each
pair of adjacent blocks of text (sentences or    xed-length units), using a formula such as
the smoothed cosine similarity of their bag-of-words vectors,

xm    xm+1

||xm||2    ||xm+1||2

k(cid:96)(sm+(cid:96) + sm   (cid:96)),

sm =

sm =

l(cid:88)(cid:96)=0

[16.1]

[16.2]

with k(cid:96) representing the value of a smoothing kernel of size l, e.g. k = [1, 0.5, 0.25](cid:62).
segmentation points are then identi   ed at local minima in the smoothed similarities s,
since these points indicate changes in the overall distribution of words in the text. an
example is shown in figure 16.1.

text segmentation can also be formulated as a probabilistic model, in which each seg-
ment has a unique language model that de   nes the id203 over the text in the seg-
ment (utiyama and isahara, 2001; eisenstein and barzilay, 2008; du et al., 2013).1 a good
1there is a rich literature on how latent variable models (such as id44) can track

jacob eisenstein. draft of november 13, 2018.

05101520253035sentence0.00.10.20.30.40.50.6cosine similarityoriginalsmoothing l=1smoothing l=316.2. entities and reference

381

segmentation achieves high likelihood by grouping segments with similar word distribu-
tions. this probabilistic approach can be extended to hierarchical topic segmentation, in
which each topic segment is divided into subsegments (eisenstein, 2009). all of these ap-
proaches are unsupervised. while labeled data can be obtained from well-formatted texts
such as textbooks, such annotations may not generalize to speech transcripts in alterna-
tive domains. supervised methods have been tried in cases where in-domain labeled data
is available, substantially improving performance by learning weights on multiple types
of features (galley et al., 2003).

16.1.2 functional segmentation

in some genres, there is a canonical set of communicative functions: for example, in sci-
enti   c research articles, one such function is to communicate the general background for
the article, another is to introduce a new contribution, or to describe the aim of the re-
search (teufel et al., 1999). a functional segmentation divides the document into con-
tiguous segments, sometimes called rhetorical zones, in which each sentence has the same
function. teufel and moens (2002) train a supervised classi   er to identify the functional
of each sentence in a set of scienti   c research articles, using features that describe the sen-
tence   s position in the text, its similarity to the rest of the article and title, tense and voice of
the main verb, and the functional role of the previous sentence. functional segmentation
can also be performed without supervision. noting that some types of wikipedia arti-
cles have very consistent functional segmentations (e.g., articles about cities or chemical
elements), chen et al. (2009) introduce an unsupervised model for functional segmenta-
tion, which learns both the language model associated with each function and the typical
patterning of functional segments across the article.

16.2 entities and reference

another dimension of discourse relates to which entities are mentioned throughout the
text, and how. consider the examples in figure 16.2: grosz et al. (1995) argue that the    rst
discourse is more coherent. do you agree? the examples differ in their choice of refer-
ring expressions for the protagonist john, and in the syntactic constructions in sentences
(b) and (d). the examples demonstrate the need for theoretical models to explain how
referring expressions are chosen, and where they are placed within sentences. such mod-
els can then be used to help interpret the overall structure of the discourse, to measure
discourse coherence, and to generate discourses in which referring expressions are used
coherently.

topics across documents (blei et al., 2003; blei, 2012).

under contract with mit press, shared under cc-by-nc-nd license.

382

chapter 16. discourse

(16.1)

a.

john went to his favorite music
store to buy a piano.

(16.2)

b. he had frequented the store for

many years.

c. he was excited that he could    -

nally buy a piano.

d. he arrived just as the store was

closing for the day

a.

b.

john went to his favorite music
store to buy a piano.
it was a store john had fre-
quented for many years.

c. he was excited that he could    -

d.

nally buy a piano.
it was closing just as john ar-
rived.

figure 16.2: two tellings of the same story (grosz et al., 1995). the discourse on the left
uses referring expressions coherently, while the one on the right does not.

16.2.1 centering theory
centering theory presents a uni   ed account of the relationship between discourse struc-
ture and entity reference (grosz et al., 1995). according to the theory, every utterance in
the discourse is characterized by a set of entities, known as centers.

    the forward-looking centers in utterance m are all the entities that are mentioned
in the utterance, cf (wm) = {e1, e2, . . . ,}. the forward-looking centers are partially
ordered by their syntactic prominence, favoring subjects over objects, and objects
over other positions (brennan et al., 1987). for example, in example (1.1a) of fig-
ure 16.2, the ordered list of forward-looking centers in the    rst utterance is john, the
music store, and the piano.

    the backward-looking center cb(wm) is the highest-ranked element in the set of
forward-looking centers from the previous utterance cf (wm   1) that is also men-
tioned in wm. in example (1.1b) of item 16.1, the backward looking center is john.

given these two de   nitions, centering theory makes the following predictions about

the form and position of referring expressions:

1. if a pronoun appears in the utterance wm, then the backward-looking center cb(wm)
must also be realized as a pronoun. this rule argues against the use of it to refer
to the piano store in example (16.2d), since john is the backward looking center of
(16.2d), and he is mentioned by name and not by a pronoun.

2. sequences of utterances should retain the same backward-looking center if possible,
and ideally, the backward-looking center should also be the top-ranked element in
the list of forward-looking centers. this rule argues in favor of the preservation of
john as the backward-looking center throughout example (16.1).

jacob eisenstein. draft of november 13, 2018.

16.2. entities and reference

383

skyler walter danger a guy

the door

you don   t know who you   re talk-
ing to,
so let me clue you in.
i am not in danger, skyler.
i am the danger.
a guy opens his door and gets
shot,
and you think that of me?
no. i am the one who knocks!

s

o
x
-
-

s
-

-

o
s
s
-

x
s

-

-
x
o
-

-
-

-

-
-
-
s

-
-

-

-
-
-
o

-
-

figure 16.3: the entity grid representation for a dialogue from the television show break-
ing bad.

centering theory uni   es aspects of syntax, discourse, and id2. however,
it can be dif   cult to clarify exactly how to rank the elements of each utterance, or even
how to partition a text or dialog into utterances (poesio et al., 2004).

16.2.2 the entity grid

one way to formalize the ideas of centering theory is to arrange the entities in a text or
conversation in an entity grid. this is a data structure with one row per sentence, and
one column per entity (barzilay and lapata, 2008). each cell c(m, i) can take the following
values:

c(m, i) =

s,
o,
x,
   ,

                                 

entity i is in subject position in sentence m
entity i is in object position in sentence m
entity i appears in sentence m, in neither subject nor object position
entity i does not appear in sentence m.

[16.3]
to populate the entity grid, syntactic parsing is applied to identify subject and object
positions, and coreference resolution is applied to link multiple mentions of a single entity.
an example is shown in figure 16.3.

after the grid is constructed, the coherence of a document can be measured by the
transitions between adjacent cells in each column. for example, the transition (s     s)
keeps an entity in subject position across adjacent sentences; the transition (o     s) pro-
motes an entity from object position to subject position; the transition (s        ) drops the
subject of one sentence from the next sentence. the probabilities of each transition can be

under contract with mit press, shared under cc-by-nc-nd license.

384

chapter 16. discourse

probabilities across all columns and all transitions,(cid:80)ne

estimated from labeled data, and an entity grid can then be scored by the sum of the log-
m=1 log p(c(m, i) | c(m     1, i)).
the resulting id203 can be used as a proxy for the coherence of a text. this has been
shown to be useful for a range of tasks: determining which of a pair of articles is more
readable (schwarm and ostendorf, 2005), correctly ordering the sentences in a scrambled
text (lapata, 2003), and disentangling multiple conversational threads in an online multi-
party chat (elsner and charniak, 2010).

i=1(cid:80)m

16.2.3

*formal semantics beyond the sentence level

an alternative view of the role of entities in discourse focuses on formal semantics, and the
construction of meaning representations for multi-sentence units. consider the following
two sentences (from bird et al., 2009):

(16.3)

a. angus owns a dog.
b.

it bit irene.

we would like to recover the formal semantic representation,

   x.dog(x)     own(angus, x)     bite(x, irene).

however, the semantic representations of each individual sentence are,

   x.dog(x)     own(angus, x)
bite(y, irene).

[16.4]

[16.5]
[16.6]

unifying these two representations into the form of equation 16.4 requires linking the
unbound variable y from [16.6] with the quanti   ed variable x in [16.5].2 discourse un-
derstanding therefore requires the reader to update a set of assignments, from variables
to entities. this update would (presumably) link the dog in the    rst sentence of [16.3]
with the unbound variable y in the second sentence, thereby licensing the conjunction in
[16.4].3 this basic idea is at the root of dynamic semantics (groenendijk and stokhof,
1991). segmented discourse representation theory links dynamic semantics with a set
of discourse relations, which explain how adjacent units of text are rhetorically or con-
ceptually related (lascarides and asher, 2007). the next section explores the theory of
discourse relations in more detail.

2groenendijk and stokhof (1991) treats the y variable in equation 16.6 as unbound. even if it were bound
locally with an existential quanti   er (   ybite(y, irene)), the variable would still need to be reconciled with
the quanti   ed variable in equation 16.5.

3this linking task is similar to coreference resolution (see chapter 15), but here the connections are be-

tween semantic variables, rather than spans of text.

jacob eisenstein. draft of november 13, 2018.

16.3. relations

16.3 relations

385

in dependency grammar, sentences are characterized by a graph (usually a tree) of syntac-
tic relations between words, such as nsubj and det. a similar idea can be applied at the
document level, identifying relations between discourse units, such as clauses, sentences,
or paragraphs. the task of discourse parsing involves identifying discourse units and
the relations that hold between them. these relations can then be applied to tasks such as
document classi   cation and summarization, as discussed in    16.3.4.
16.3.1 shallow discourse relations
the existence of discourse relations is hinted by discourse connectives, such as however,
moreover, meanwhile, and if . . . then. these connectives explicitly specify the relationship
between adjacent units of text: however signals a contrastive relationship, moreover signals
that the subsequent text elaborates or strengthens the point that was made immediately
beforehand, meanwhile indicates that two events are contemporaneous, and if . . . then sets
up a conditional relationship. discourse connectives can therefore be viewed as a starting
point for the analysis of discourse relations.

in lexicalized tree-adjoining grammar for discourse (d-ltag), each connective an-
chors a relationship between two units of text (webber, 2004). this model provides the
theoretical basis for the penn discourse treebank (pdtb), the largest corpus of discourse
relations in english (prasad et al., 2008). it includes a hierarchical inventory of discourse
relations (shown in table 16.1), which is created by abstracting the meanings implied by
the discourse connectives that appear in real texts (knott, 1996). these relations are then
annotated on the same corpus of news text used in the id32 (see    9.2.2), adding
the following information:

    each connective is annotated for the discourse relation or relations that it expresses,
if any     many discourse connectives have senses in which they do not signal a
discourse relation (pitler and nenkova, 2009).

    for each discourse relation, the two arguments of the relation are speci   ed as arg1
and arg2, where arg2 is constrained to be adjacent to the connective. these argu-
ments may be sentences, but they may also smaller or larger units of text.

    adjacent sentences are annotated for implicit discourse relations, which are not
marked by any connective. when a connective could be inserted between a pair
of sentence, the annotator supplies it, and also labels its sense (e.g., example 16.5).
in some cases, there is no relationship at all between a pair of adjacent sentences;
in other cases, the only relation is that the adjacent sentences mention one or more
shared entity. these phenomena are annotated as norel and entrel (entity rela-
tion), respectively.

under contract with mit press, shared under cc-by-nc-nd license.

386

chapter 16. discourse

    temporal

    asynchronous
    synchronous:

precedence, succession

    contingency

    cause: result, reason
    pragmatic cause:

justi   cation

    condition: hypothetical,
general, unreal present,
unreal past, real present,
real past

    pragmatic condition:

relevance, implicit
assertion

    comparison

    contrast: juxtaposition, opposition
    pragmatic contrast
    concession: expectation,

contra-expectation

    pragmatic concession

    expansion

    conjunction
    instantiation
    restatement: speci   cation,
equivalence, generalization

    alternative: conjunctive, disjunctive,

chosen alternative

    exception
    list

table 16.1: the hierarchy of discourse relation in the penn discourse treebank annota-
tions (prasad et al., 2008). for example, precedence is a subtype of synchronous,
which is a type of temporal relation.

examples of penn discourse treebank annotations are shown in (16.4). in (16.4), the
word therefore acts as an explicit discourse connective, linking the two adjacent units of
text. the treebank annotations also specify the    sense    of each relation, linking the con-
nective to a relation in the sense inventory shown in table 16.1: in (16.4), the relation is
pragmatic cause:justification because it relates to the author   s communicative in-
tentions. the word therefore can also signal causes in the external world (e.g., he was
therefore forced to relinquish his plan). in discourse sense classi   cation, the goal is to de-
termine which discourse relation, if any, is expressed by each connective. a related task
is the classi   cation of implicit discourse relations, as in (16.5). in this example, the re-
lationship between the adjacent sentences could be expressed by the connective because,
indicating a cause:reason relationship.

classifying explicit discourse relations and their arguments

as suggested by the examples above, many connectives can be used to invoke multiple
types of discourse relations. similarly, some connectives have senses that are unrelated
to discourse: for example, and functions as a discourse connective when it links propo-

jacob eisenstein. draft of november 13, 2018.

16.3. relations

387

(16.4)

. . . as this business of whaling has somehow come to be regarded among landsmen as a
rather unpoetical and disreputable pursuit; therefore, i am all anxiety to convince
ye, ye landsmen, of the injustice hereby done to us hunters of whales.

(16.5) but a few funds have taken other defensive steps. some have raised their cash
positions to record levels. implicit = because high cash positions help buffer a
fund when the market falls.

(16.6) michelle lives in a hotel room, and although she drives a canary-colored

porsche, she hasn   t time to clean or repair it.

(16.7) most oil companies, when they set exploration and production budgets for this

year, forecast revenue of $15 for each barrel of crude produced.

figure 16.4: example annotations of discourse relations. in the style of the penn discourse
treebank, the discourse connective is underlined, the    rst argument is shown in italics,
and the second argument is shown in bold. examples (16.5-16.7) are quoted from prasad
et al. (2008).

sitions, but not when it links noun phrases (lin et al., 2014). nonetheless, the senses of
explicitly-marked discourse relations in the id32 are relatively easy to classify,
at least at the coarse-grained level. when classifying the four top-level pdtb relations,
90% accuracy can be obtained simply by selecting the most common relation for each
connective (pitler and nenkova, 2009). at the more    ne-grained levels of the discourse
relation hierarchy, connectives are more ambiguous. this fact is re   ected both in the ac-
curacy of automatic sense classi   cation (versley, 2011) and in interannotator agreement,
which falls to 80% for level-3 discourse relations (prasad et al., 2008).

a more challenging task for explicitly-marked discourse relations is to identify the
scope of the arguments. discourse connectives need not be adjacent to arg1, as shown
in item 16.6, where arg1 follows arg2; furthermore, the arguments need not be contigu-
ous, as shown in (16.7). for these reasons, recovering the arguments of each discourse
connective is a challenging subtask. because intra-sentential arguments are often syn-
tactic constituents (see chapter 10), many approaches train a classi   er to predict whether
each constituent is an appropriate argument for each explicit discourse connective (well-
ner and pustejovsky, 2007; lin et al., 2014, e.g.,).

classifying implicit discourse relations
implicit discourse relations are considerably more dif   cult to classify and to annotate.4
most approaches are based on an encoding of each argument, which is then used as input

4in the dataset for the 2015 shared task on shallow discourse parsing, the interannotator agreement was
91% for explicit discourse relations and 81% for implicit relations, across all levels of detail (xue et al., 2015).

under contract with mit press, shared under cc-by-nc-nd license.

388

chapter 16. discourse

to a nonlinear classi   er:

z(i) =encode(w(i))

z(i+1) =encode(w(i+1))

  yi = argmax

y

  (y, z(i), z(i+1)).

[16.7]
[16.8]
[16.9]

this basic framework can be instantiated in several ways, including both feature-based
and neural encoders.

feature-based approaches each argument can be encoded into a vector of surface fea-
tures. the encoding typically includes lexical features (all words, or all content words, or
a subset of words such as the    rst three and the main verb), brown clusters of individ-
ual words (   14.4), and syntactic features such as terminal productions and dependency
arcs (pitler et al., 2009; lin et al., 2009; rutherford and xue, 2014). the classi   cation func-
tion then has two parts. first, it creates a joint feature vector by combining the encodings
of each argument, typically by computing the cross-product of all features in each encod-
ing:

f (y, z(i), z(i+1)) = {(a    b    y) : (z(i)

a z(i+1)

b

)}

[16.10]

the size of this feature set grows with the square of the size of the vocabulary, so it can be
helpful to select a subset of features that are especially useful on the training data (park
and cardie, 2012). after f is computed, any classi   er can be trained to compute the    nal
score,   (y, z(i), z(i+1)) =       f (y, z(i), z(i+1)).

neural network approaches
in neural network architectures, the encoder is learned
jointly with the classi   er as an end-to-end model. each argument can be encoded using
a variety of neural architectures (surveyed in    14.8): recursive (   10.6.1; ji and eisenstein,
2015), recurrent (   6.3; ji et al., 2016), and convolutional (   3.4; qin et al., 2017). the clas-
si   cation function can then be implemented as a feedforward neural network on the two
encodings (chapter 3; for examples, see rutherford et al., 2017; qin et al., 2017), or as a
simple bilinear product,   (y, z(i), z(i+1)) = (z(i))(cid:62)  yz(i+1) (ji and eisenstein, 2015). the
encoding model can be trained by id26 from the classi   cation objective, such
as the margin loss. rutherford et al. (2017) show that neural architectures outperform
feature-based approaches in most settings. while neural approaches require engineering
the network architecture (e.g., embedding size, number of hidden units in the classi   er),
feature-based approaches also require signi   cant engineering to incorporate linguistic re-
sources such as brown clusters and parse trees, and to select a subset of relevant features.

jacob eisenstein. draft of november 13, 2018.

16.3. relations

389

16.3.2 hierarchical discourse relations

in sentence parsing, adjacent phrases combine into larger constituents, ultimately pro-
ducing a single constituent for the entire sentence. the resulting tree structure enables
structured analysis of the sentence, with subtrees that represent syntactically coherent
chunks of meaning. rhetorical structure theory (rst) extends this style of hierarchical
analysis to the discourse level (mann and thompson, 1988).

the basic element of rst is the discourse unit, which refers to a contiguous span of
text. elementary discourse units (edus) are the atomic elements in this framework, and
are typically (but not always) clauses.5 each discourse relation combines two or more
adjacent discourse units into a larger, composite discourse unit; this process ultimately
unites the entire text into a tree-like structure.6

nuclearity in many discourse relations, one argument is primary. for example:

(16.8)

[lashawn loves animals]n
[she has nine dogs and one pig]s

in this example, the second sentence provides evidence for the point made in the    rst
sentence. the    rst sentence is thus the nucleus of the discourse relation, and the second
sentence is the satellite. the notion of nuclearity is similar to the head-modi   er structure
of id33 (see    11.1.1). however, in rst, some relations have multiple
nuclei. for example, the arguments of the contrast relation are equally important:

(16.9)

[the clash of ideologies survives this treatment]n
[but the nuance and richness of gorky   s individual characters have vanished in the scuf   e]n

7

relations that have multiple nuclei are called coordinating; relations with a single nu-
cleus are called subordinating. subordinating relations are constrained to have only two
arguments, while coordinating relations (such as conjunction) may have more than
two.

5details of discourse segmentation can be found in the rst annotation manual (carlson and marcu,

2001).

6while rst analyses are typically trees, this should not be taken as a strong theoretical commitment to

the principle that all coherent discourses have a tree structure. taboada and mann (2006) write:

it is simply the case that trees are convenient, easy to represent, and easy to understand. there
is, on the other hand, no theoretical reason to assume that trees are the only possible represen-
tation of discourse structure and of coherence relations.

the appropriateness of tree structures to discourse has been challenged, e.g., by wolf and gibson (2005), who
propose a more general graph-structured representation.

7from the rst treebank (carlson et al., 2002)

under contract with mit press, shared under cc-by-nc-nd license.

390

chapter 16. discourse

figure 16.5: a rhetorical structure theory analysis of a short movie review, adapted from
voll and taboada (2007). positive and :::::::::
negative sentiment words are underlined, indicat-
ing rst   s potential utility in document-level id31.

rst relations rhetorical structure theory features a large inventory of discourse rela-
tions, which are divided into two high-level groups: subject matter relations, and presen-
tational relations. presentational relations are organized around the intended beliefs of
the reader. for example, in (16.8), the second discourse unit provides evidence intended
to increase the reader   s belief in the proposition expressed by the    rst discourse unit, that
lashawn loves animals. in contrast, subject-matter relations are meant to communicate ad-
ditional facts about the propositions contained in the discourse units that they relate:

(16.10)

[the debt plan was rushed to completion]n
[in order to be announced at the meeting]s

8

in this example, the satellite describes a world state that is realized by the action described
in the nucleus. this relationship is about the world, and not about the author   s commu-
nicative intentions.

example figure 16.5 depicts an rst analysis of a paragraph from a movie review. asym-
metric (subordinating) relations are depicted with an arrow from the satellite to the nu-
cleus; symmetric (coordinating) relations are depicted with lines. the elementary dis-
course units 1f and 1g are combined into a larger discourse unit with the symmetric
conjunction relation. the resulting discourse unit is then the satellite in a justify
relation with 1e.

8from the rst treebank (carlson et al., 2002)

jacob eisenstein. draft of november 13, 2018.

	concession	justifyconjunctionelaboration	justifyconjunction[itcouldhavebeenagreatmovie]1a[itdoeshavebeautifulscenery,]1b[someofthebestsincelordoftherings.]1c[theactingiswelldone,]1d[andireallylikedthesonoftheleaderofthesamurai.]1e[hewasalikablechap,]1f[andi:::::hatedtoseehimdie.]1g[but,otherthanallthat,thismovieis::::::nothingmorethanhidden:::::rip-o   s.]1h16.3. relations

hierarchical discourse parsing

391

the goal of discourse parsing is to recover a hierarchical structural analysis from a doc-
ument text, such as the analysis in figure 16.5. for now, let   s assume a segmentation of
the document into elementary discourse units (edus); segmentation algorithms are dis-
cussed below. after segmentation, discourse parsing can be viewed as a combination of
two components: the discourse relation classi   cation techniques discussed in    16.3.1, and
algorithms for phrase-structure parsing, such as chart parsing and shift-reduce, which
were discussed in chapter 10.

both chart parsing and shift-reduce require encoding composite discourse units, ei-
ther in a discrete feature vector or a dense neural representation.9 some discourse parsers
rely on the strong compositionality criterion (marcu, 1996), which states the assumption
that a composite discourse unit can be represented by its nucleus. this criterion is used in
feature-based discourse parsing to determine the feature vector for a composite discourse
unit (hernault et al., 2010); it is used in neural approaches to setting the vector encod-
ing for a composite discourse unit equal to the encoding of its nucleus (ji and eisenstein,
2014). an alternative neural approach is to learn a composition function over the compo-
nents of a composite discourse unit (li et al., 2014), using a id56 (see
   14.8.3).

bottom-up discourse parsing assume a segmentation of the text into n elementary
discourse units with base representations {z(i)}n
i=1, and assume a composition function
compose(cid:0)z(i), z(j), (cid:96)(cid:1), which maps two encodings and a discourse relation (cid:96) into a new
encoding. the composition function can follow the strong compositionality criterion and
simply select the encoding of the nucleus, or it can do something more complex. we
also need a scoring function   (z(i,k), z(k,j), (cid:96)), which computes a scalar score for the (bi-
narized) discourse relation (cid:96) with left child covering the span i + 1 : k, and the right
child covering the span k + 1 : j. given these components, we can construct vector rep-
resentations for each span, and this is the basic idea underlying compositional vector
grammars (socher et al., 2013).

these same components can also be used in bottom-up parsing, in a manner that is
similar to the cky algorithm for weighted context-free grammars (see    10.1): compute
the score and best analysis for each possible span of increasing lengths, while storing
back-pointers that make it possible to recover the optimal parse of the entire input. how-
ever, there is an important distinction from cky parsing: for each labeled span (i, j, (cid:96)), we
must use the composition function to construct a representation z(i,j,(cid:96)). this representa-
tion is then used to combine the discourse unit spanning i + 1 : j in higher-level discourse
relations. the representation z(i,j,(cid:96)) depends on the entire substructure of the unit span-

9to use these algorithms, is also necessary to binarize all discourse relations during parsing, and then to

   unbinarize    them to reconstruct the desired structure (e.g., hernault et al., 2010).

under contract with mit press, shared under cc-by-nc-nd license.

392

chapter 16. discourse

ning i + 1 : j, and this violates the locality assumption that underlie cky   s optimality
guarantee. bottom-up parsing with recursively constructed span representations is gen-
erally not guaranteed to    nd the best-scoring discourse parse. this problem is explored
in an exercise at the end of the chapter.

transition-based discourse parsing one drawback of bottom-up parsing is its cubic
time complexity in the length of the input. for long documents, transition-based parsing
is an appealing alternative. the shift-reduce algorithm (see    10.6.2) can be applied to
discourse parsing fairly directly (sagae, 2009): the stack stores a set of discourse units and
their representations, and each action is chosen by a function of these representations.
this function could be a linear product of weights and features, or it could be a neural
network applied to encodings of the discourse units. the reduce action then performs
composition on the two discourse units at the top of the stack, yielding a larger composite
discourse unit, which goes on top of the stack. all of the techniques for integrating learn-
ing and transition-based parsing, described in    11.3, are applicable to discourse parsing.
segmenting discourse units

in rhetorical structure theory, elementary discourse units do not cross the sentence bound-
ary, so discourse segmentation can be performed within sentences, assuming the sentence
segmentation is given. the segmentation of sentences into elementary discourse units is
typically performed using features of the syntactic analysis (braud et al., 2017). one ap-
proach is to train a classi   er to determine whether each syntactic constituent is an edu,
using features such as the production, tree structure, and head words (soricut and marcu,
2003; hernault et al., 2010). another approach is to train a sequence labeling model, such
as a conditional random    eld (sporleder and lapata, 2005; xuan bach et al., 2012; feng
et al., 2014). this is done using the bio formalism for segmentation by sequence labeling,
described in    8.3.
16.3.3 argumentation
an alternative view of text-level relational structure focuses on argumentation (stab and
gurevych, 2014b). each segment (typically a sentence or clause) may support or rebut
another segment, creating a graph structure over the text. in the following example (from
peldszus and stede, 2013), segment s2 provides argumentative support for the proposi-
tion in the segment s1:

(16.11)

[we should tear the building down,]s1
[because it is full of asbestos]s2.

assertions may also support or rebut proposed links between two other assertions, cre-
ating a hypergraph, which is a generalization of a graph to the case in which edges can

jacob eisenstein. draft of november 13, 2018.

16.3. relations

393

join any number of vertices. this can be seen by introducing another sentence into the
example:

(16.12)

[in principle it is possible to clean it up,]s3
[but according to the mayor that is too expensive.]s4

s3 acknowledges the validity of s2, but undercuts its support of s1. this can be repre-
sented by introducing a hyperedge, (s3, s2, s1)undercut, indicating that s3 undercuts the
proposed relationship between s2 and s1. s4 then undercuts the relevance of s3.

argumentation mining is the task of recovering such structures from raw texts. at
present, annotations of argumentation structure are relatively small: stab and gurevych
(2014a) have annotated a collection of 90 persuasive essays, and peldszus and stede (2015)
have solicited and annotated a set of 112 paragraph-length    microtexts    in german.

16.3.4 applications of discourse relations
the predominant application of discourse parsing is to select content within a document.
in rhetorical structure theory, the nucleus is considered the more important element of
the relation, and is more likely to be part of a summary of the document; it may also
be more informative for document classi   cation. the d-ltag theory that underlies the
penn discourse treebank lacks this notion of nuclearity, but arguments may have varying
importance, depending on the relation type. for example, the span of text constituting
arg1 of an expansion relation is more likely to appear in a summary, while the sentence
constituting arg2 of an implicit relation is less likely (louis et al., 2010). discourse rela-
tions may also signal segmentation points in the document structure. explicit discourse
markers have been shown to correlate with changes in subjectivity, and identifying such
change points can improve document-level sentiment classi   cation, by helping the clas-
si   er to focus on the subjective parts of the text (trivedi and eisenstein, 2013; yang and
cardie, 2014).

extractive summarization

text summarization is the problem of converting a longer text into a shorter one, while
still conveying the key facts, events, ideas, and sentiments from the original. in extractive
summarization, the summary is a subset of the original text; in abstractive summariza-
tion, the summary is produced de novo, by id141 the original, or by    rst encoding
it into a semantic representation (see    19.2). the main strategy for extractive summa-
rization is to maximize coverage, choosing a subset of the document that best covers the
concepts mentioned in the document as a whole; typically, coverage is approximated by
bag-of-words overlap (nenkova and mckeown, 2012). coverage-based objectives can be
supplemented by hierarchical discourse relations, using the principle of nuclearity: in
any subordinating discourse relation, the nucleus is more critical to the overall meaning

under contract with mit press, shared under cc-by-nc-nd license.

394

chapter 16. discourse

of the text, and is therefore more important to include in an extractive summary (marcu,
1997a).10 this insight can be generalized from individual relations using the concept of
discourse depth (hirao et al., 2013): for each elementary discourse unit e, the discourse
depth de is the number of relations in which a discourse unit containing e is the satellite.
both discourse depth and nuclearity can be incorporated into extractive summariza-
tion, using constrained optimization. let xn be a bag-of-words vector representation of
elementary discourse unit n, let yn     {0, 1} indicate whether n is included in the summary,
and let dn be the depth of unit n. furthermore, let each discourse unit have a    head    h,
which is de   ned recursively:

    if a discourse unit is produced by a subordinating relation, then its head is the head

of the (unique) nucleus;

    if a discourse unit is produced by a coordinating relation, then its head is the head

of the left-most nucleus;

    for each elementary discourse unit, its parent   (n)     {   , 1, 2, . . . , n} is the head of

the smallest discourse unit containing n whose head is not n;

    if n is the head of the discourse unit spanning the whole document, then   (n) =    .

with these de   nitions in place, discourse-driven extractive summarization can be for-

malized as (hirao et al., 2013),

yn

   (xn,{x1:n})

dn

max

y={0,1}n

s.t.

n(cid:88)n=1
n(cid:88)n=1

yn(

v(cid:88)j=1

y  (n)     yn,

xn,j)     l

   n s.t.   (n) (cid:54)=    

[16.11]

where    (xn,{x1:n}) measures the coverage of elementary discourse unit n with respect
to the rest of the document, and(cid:80)v
j=1 xn,j is the number of tokens in xn. the    rst con-
straint ensures that the number of tokens in the summary has an upper bound l. the
second constraint ensures that no elementary discourse unit is included unless its parent
is also included. in this way, the discourse structure is used twice: to downweight the
contributions of elementary discourse units that are not central to the discourse, and to
ensure that the resulting structure is a subtree of the original discourse parse. the opti-

10conversely, the arguments of a multi-nuclear relation should either both be included in the summary,

or both excluded (durrett et al., 2016).

jacob eisenstein. draft of november 13, 2018.

16.3. relations

395

figure 16.6: a discourse depth tree (hirao et al., 2013) for the discourse parse from fig-
ure 16.5, in which each elementary discourse unit is connected to its parent. the discourse
units in one valid summary are underlined.

mization problem in 16.11 can be solved with integer id135, described in
   13.2.2.11

figure 16.6 shows a discourse depth tree for the rst analysis from figure 16.5, in
which each elementary discourse is connected to (and below) its parent. the underlined
discourse units in the    gure constitute the following summary:

(16.13)

it could have been a great movie, and i really liked the son of the leader of the
samurai. but, other than all that, this movie is nothing more than hidden rip-offs.

document classi   cation

hierarchical discourse structures lend themselves naturally to text classi   cation: in a sub-
ordinating discourse relation, the nucleus should play a stronger role in the classi   cation
decision than the satellite. various implementations of this idea have been proposed.

    focusing on within-sentence discourse relations and lexicon-based classi   cation (see
   4.1.2), voll and taboada (2007) simply ignore the text in the satellites of each dis-
course relation.

    at the document level, elements of each discourse relation argument can be reweighted,

favoring words in the nucleus, and disfavoring words in the satellite (heerschop
et al., 2011; bhatia et al., 2015). this approach can be applied recursively, computing
weights across the entire document. the weights can be relation-speci   c, so that the
features from the satellites of contrastive relations are discounted or even reversed.
    alternatively, the hierarchical discourse structure can de   ne the structure of a re-
cursive neural network (see    10.6.1). in this network, the representation of each
11formally, 16.11 is a special case of the knapsack problem, in which the goal is to    nd a subset of items

with maximum value, constrained by some maximum weight (cormen et al., 2009).

under contract with mit press, shared under cc-by-nc-nd license.

habdeid18396

chapter 16. discourse

discourse unit is computed from its arguments and from a parameter correspond-
ing to the discourse relation (ji and smith, 2017).

shallow, non-hierarchical discourse relations have also been applied to document clas-
si   cation. one approach is to impose a set of constraints on the analyses of individual
discourse units, so that adjacent units have the same polarity when they are connected
by a discourse relation indicating agreement, and opposite polarity when connected by a
contrastive discourse relation, indicating disagreement (somasundaran et al., 2009; zirn
et al., 2011). yang and cardie (2014) apply explicitly-marked relations from the penn
discourse treebank to the problem of sentence-level sentiment polarity classi   cation (see
   4.1). they impose the following soft constraints:

    when a contrast relation appears at the beginning of a sentence, the sentence

should have the opposite sentiment polarity as its predecessor.

    when an expansion or contingency appears at the beginning of a sentence, it

should have the same polarity as its predecessor.

    when a contrast relation appears within a sentence, the sentence should have

neutral polarity, since it is likely to express both sentiments.

these discourse-driven constraints are shown to improve performance on two datasets of
product reviews.

coherence

just as grammaticality is the property shared by well-structured sentences, coherence is
the property shared by well-structured discourses. one application of discourse process-
ing is to measure (and maximize) the coherence of computer-generated texts like transla-
tions and summaries (kibble and power, 2004). coherence assessment is also used to eval-
uate human-generated texts, such as student essays (e.g., miltsakaki and kukich, 2004;
burstein et al., 2013).

coherence subsumes a range of phenomena, many of which have been highlighted
earlier in this chapter: e.g., that adjacent sentences should be lexically cohesive (foltz
et al., 1998; ji et al., 2015; li and jurafsky, 2017), and that entity references should follow
the principles of centering theory (barzilay and lapata, 2008; nguyen and joty, 2017).
discourse relations also bear on the coherence of a text in a variety of ways:

    hierarchical discourse relations tend to have a    canonical ordering    of the nucleus
and satellite (mann and thompson, 1988): for example, in the elaboration rela-
tion from rhetorical structure theory, the nucleus always comes    rst, while in the
justification relation, the satellite tends to be    rst (marcu, 1997b).

jacob eisenstein. draft of november 13, 2018.

16.3. relations

397

    discourse relations should be signaled by connectives that are appropriate to the
semantic or functional relationship between the arguments: for example, a coherent
text would be more likely to use however to signal a comparison relation than a
temporal relation (kibble and power, 2004).

    discourse relations tend to be ordered in appear in predictable sequences: for ex-
ample, comparison relations tend to immediately precede contingency rela-
tions (pitler et al., 2008). this observation can be formalized by generalizing the
entity grid model (   16.2.2), so that each cell (i, j) provides information about the
role of the discourse argument containing a mention of entity j in sentence i (lin
et al., 2011). for example, if the    rst sentence is arg1 of a comparison relation, then
any entity mentions in the sentence would be labeled comp.arg1. this approach
can also be applied to rst discourse relations (feng et al., 2014).

datasets one dif   culty with evaluating metrics of discourse coherence is that human-
generated texts usually meet some minimal threshold of coherence. for this reason, much
of the research on measuring coherence has focused on synthetic data. a typical setting is
to permute the sentences of a human-written text, and then determine whether the origi-
nal sentence ordering scores higher according to the proposed coherence measure (barzi-
lay and lapata, 2008). there are also small datasets of human evaluations of the coherence
of machine summaries: for example, human judgments of the summaries from the partic-
ipating systems in the 2003 document understanding conference are available online.12
researchers from the educational testing service (an organization which administers sev-
eral national exams in the united states) have studied the relationship between discourse
coherence and student essay quality (burstein et al., 2003, 2010). a public dataset of es-
says from second-language learners, with quality annotations, has been made available by
researchers at cambridge university (yannakoudakis et al., 2011). at the other extreme,
louis and nenkova (2013) analyze the structure of professionally written scienti   c essays,
   nding that discourse relation transitions help to distinguish prize-winning essays from
other articles in the same genre.

additional resources

for a manuscript-length discussion of discourse processing, see stede (2011). article-
length surveys are offered by webber et al. (2012) and webber and joshi (2012).

12http://homepages.inf.ed.ac.uk/mlap/coherence/

under contract with mit press, shared under cc-by-nc-nd license.

398

exercises

chapter 16. discourse

1. some discourse connectives tend to occur between their arguments; others can pre-
cede both arguments, and a few can follow both arguments. indicate whether the
following connectives can occur between, before, and after their arguments: how-
ever, but, while (contrastive, not temporal), although, therefore, nonetheless.

2. this exercise is to be done in pairs. each participant selects an article from to-
day   s news, and replaces all mentions of individual people with special tokens like
person1, person2, and so on. the other participant should then use the rules
of centering theory to guess each type of referring expression: full name (captain
ahab), partial name (e.g., ahab), nominal (e.g., the ship   s captain), or pronoun. check
whether the predictions match the original text, and whether the text conforms to
the rules of centering theory.

3. in this exercise, you will produce a    gure similar to figure 16.1.

a) implement the smoothed cosine similarity metric from equation 16.2, using the

smoothing kernel k = [.5, .3, .15, .05].

b) download the text of a news article with at least ten paragraphs.
c) compute and plot the smoothed similarity s over the length of the article.
d) identify local minima in s as follows:    rst    nd all sentences m such that sm <
sm  1. then search among these points to    nd the    ve sentences with the lowest
sm.

e) how often do the    ve local minima correspond to paragraph boundaries?

at-k, where in this case, k = 5.

    the fraction of local minima that are paragraph boundaries is the precision-
    the fraction of paragraph boundaries which are local minima is the recall-
    compute precision-at-k and recall-at-k for k = 3 and k = 10.

at-k.

4. one way to formulate text segmentation as a probabilistic model is through the use
of the dirichlet compound multinomial (dcm) distribution, which computes the
id203 of a bag-of-words, dcm(x;   ), where the parameter    is a vector of
positive reals. this distribution can be con   gured to assign high likelihood to bag-
of-words vectors that are internally coherent, such that individual words appear re-
peatedly: for example, this behavior can be observed for simple parameterizations,
such as    =   1 with    < 1.
let     (i, j) represent the log-id203 of a segment wi+1:j under a dcm distribu-
tion with parameter   . give a dynamic program for segmenting a text into a total

jacob eisenstein. draft of november 13, 2018.

16.3. relations

399

of k segments maximizing the sum of log-probabilities(cid:80)k

k=1     (sk   1, sk), where
sk indexes the last token of segment k, and s0 = 0. the time complexity of your
dynamic program should not be worse than quadratic in the length of the input and
linear in the number of segments.

k=1 }l

k }k((cid:96))

5. building on the previous problem, you will now adapt the cky algorithm to per-
form hierarchical segmentation. de   ne a hierarchical segmentation as a set of seg-
mentations {{s((cid:96))
(cid:96)=1, where l is the segmentation depth. to ensure that the
segmentation is hierarchically valid, we require that each segmentation point s((cid:96))
k at
level (cid:96) is also a segmentation point at level (cid:96)     1, where (cid:96) > 1.
for simplicity, this problem focuses on binary hierarchical segmentation, so that
each segment at level (cid:96) > 1 has exactly 2 subsegments. de   ne the score of a hierar-
chical segmentation as the sum of the scores of all segments (at all levels), using the
the dcm log-probabilities from the previous problem as the segment scores. give a
cky-like recurrence such that the optimal    parse    of the text is the maximum log-
id203 binary segmentation with exactly l levels.

6. the entity grid representation of centering theory can be used to compute a score for
adjacent sentences, as described in    16.2.2. given a set of sentences, these scores can
be used to compute an optimal ordering. show that    nding the ordering with the
maximum log id203 is np-complete, by reduction from a well-known prob-
lem.

7. in    16.3.2, it is noted that bottom-up parsing with compositional vector representa-
tions of each span is not guaranteed to be optimal. in this exercise, you will construct
a minimal example proving this point. consider a discourse with four units, with
i=1. construct a scenario in which the parse selected by
base representations {z(i)}4
bottom-up parsing is not optimal, and give the precise mathematical conditions un-
der which this suboptimal parse is selected. you may ignore the relation labels (cid:96) for
the purpose of this example.

8. as noted in    16.3.3, arguments can described by hypergraphs, in which a segment
may undercut a proposed edge between two other segments. extend the model of
extractive summarization described in    16.3.4 to arguments, adding the follwoing
constraint: if segment i undercuts an argumentative relationship between j and k,
then i cannot be included in the summary unless both j and k are included. your so-
lution should take the form of a set of linear constraints on an integer linear program
    that is, each constraint can only involve addition and subtraction of variables.

in the next two exercises, you will explore the use of discourse connectives in a real corpus.
using nltk, acquire the brown corpus, and identify sentences that begin with any of the
following connectives: however, nevertheless, moreover, furthermore, thus.

under contract with mit press, shared under cc-by-nc-nd license.

400

chapter 16. discourse

9. both lexical consistency and discourse connectives contribute to the cohesion of a
text. we might therefore expect adjacent sentences that are joined by explicit dis-
course connectives to also have higher word overlap. using the brown corpus, test
this theory by computing the average cosine similarity between adjacent sentences
that are connected by one of the connectives mentioned above. compare this to the
average cosine similarity of all other adjacent sentences. if you know how, perform
a two-sample t-test to determine whether the observed difference is statistically sig-
ni   cant.

10. group the above connectives into the following three discourse relations:

    expansion: moreover, furthermore
    comparison: however, nevertheless
    contingency: thus

focusing on pairs of sentences which are joined by one of these    ve connectives,
build a classi   er to predict the discourse relation from the text of the two adjacent
sentences     taking care to ignore the connective itself. use the    rst 30000 sentences
of the brown corpus as the training set, and the remaining sentences as the test
set. compare the performance of your classi   er against simply choosing the most
common class. using a bag-of-words classi   er, it is hard to do much better than this
baseline, so consider more sophisticated alternatives!

jacob eisenstein. draft of november 13, 2018.

part iv

applications

401

chapter 17

information extraction

computers offer powerful capabilities for searching and reasoning about structured records
and relational data. some have argued that the most important limitation of arti   cial in-
telligence is not id136 or learning, but simply having too little knowledge (lenat et al.,
1990). natural language processing provides an appealing solution: automatically con-
struct a structured knowledge base by reading natural language text.

for example, many wikipedia pages have an    infobox    that provides structured in-
formation about an entity or event. an example is shown in figure 17.1a: each row rep-
resents one or more properties of the entity in the aeroplane over the sea, a record
album. the set of properties is determined by a prede   ned schema, which applies to all
record albums in wikipedia. as shown in figure 17.1b, the values for many of these    elds
are indicated directly in the    rst few sentences of text on the same wikipedia page.

the task of automatically constructing (or    populating   ) an infobox from text is an
example of information extraction. much of information extraction can be described in
terms of entities, relations, and events.

    entities are uniquely speci   ed objects in the world, such as people (jeff mangum),
places (athens, georgia), organizations (merge records), and times (february
10, 1998). chapter 8 described the task of id39, which labels
tokens as parts of entity spans. now we will see how to go further, linking each
entity mention to an element in a knowledge base.

    relations include a predicate and two arguments: for example, capital(georgia, atlanta).

    events involve multiple typed arguments. for example, the production and release

403

404

chapter 17. information extraction

(17.1)

(17.2)

(17.3)

in the aeroplane over the sea is the
second and    nal studio album by the
american indie rock band neutral milk
hotel.
it was released in the united states on
february 10, 1998 on merge records
and ::::may:::::
records in
the united kingdom.
:::jeff::::::::::
mangum moved from :::::::
athens,
georgia to denver, colorado to prepare
the bulk of the album   s material with
producer robert schneider, this time at
schneider   s newly created pet sounds
studio at the home of:::jim:::::::::
mcintyre.

1998 on ::::blue:::::rose::::::::

:::::::

(a) a wikipedia infobox

(b) the    rst few sentences of text. strings that
match    elds or    eld names in the infobox are
underlined; strings that mention other entities
are :::::wavy::::::::::
underlined.

figure 17.1: from the wikipedia page for the album    in the aeroplane over the sea   ,
retrieved october 26, 2017.

of the album described in figure 17.1 is described by the event,
(cid:104)title : in the aeroplane over the sea,
artist : neutral milk hotel,
release-date : 1998-feb-10, . . .(cid:105)

the set of arguments for an event type is de   ned by a schema. events often refer to
time-delimited occurrences: weddings, protests, purchases, terrorist attacks.

information extraction is similar to id14 (chapter 13): we may think
of predicates as corresponding to events, and the arguments as de   ning slots in the event
representation. however, the goals of information extraction are different. rather than
accurately parsing every sentence, information extraction systems often focus on recog-
nizing a few key relation or event types, or on the task of identifying all properties of a
given entity. information extraction is often evaluated by the correctness of the resulting
knowledge base, and not by how many sentences were accurately parsed. the goal is
sometimes described as macro-reading, as opposed to micro-reading, in which each sen-
tence must be analyzed correctly. macro-reading systems are not penalized for ignoring
dif   cult sentences, as long as they can recover the same information from other, easier-
to-read sources. however, macro-reading systems must resolve apparent inconsistencies

jacob eisenstein. draft of november 13, 2018.

17.1. entities

405

(was the album released on merge records or blue rose records?), requiring rea-
soning across the entire dataset.

in addition to the basic tasks of recognizing entities, relations, and events, information
extraction systems must handle negation, and must be able to distinguish statements of
fact from hopes, fears, hunches, and hypotheticals. finally, information extraction is of-
ten paired with the problem of id53, which requires accurately parsing a
query, and then selecting or generating a textual answer. id53 systems can
be built on knowledge bases that are extracted from large text corpora, or may attempt to
identify answers directly from the source texts.

17.1 entities

the starting point for information extraction is to identify mentions of entities in text.
consider the following example:

(17.4) the united states army captured a hill overlooking atlanta on may 14, 1864.

for this sentence, there are two goals:

1. identify the spans united states army, atlanta, and may 14, 1864 as entity mentions.
(the hill is not uniquely identi   ed, so it is not a named entity.) we may also want to
recognize the named entity types: organization, location, and date. this is named
entity recognition, and is described in chapter 8.

2. link these spans to entities in a knowledge base: u.s. army, atlanta, and 1864-

may-14. this task is known as entity linking.

the strings to be linked to entities are mentions     similar to the use of this term in
coreference resolution. in some formulations of the entity linking task, only named enti-
ties are candidates for linking. this is sometimes called named entity linking (ling et al.,
2015). in other formulations, such as wiki   cation (milne and witten, 2008), any string
can be a mention. the set of target entities often corresponds to wikipedia pages, and
wikipedia is the basis for more comprehensive knowledge bases such as yago (suchanek
et al., 2007), dbpedia (auer et al., 2007), and freebase (bollacker et al., 2008). entity link-
ing may also be performed in more    closed    settings, where a much smaller list of targets
is provided in advance. the system must also determine if a mention does not refer to
any entity in the knowledge base, sometimes called a nil entity (mcnamee and dang,
2009).

returning to (17.4), the three entity mentions may seem unambiguous. but the wikipedia

disambiguation page for the string atlanta says otherwise:1 there are more than twenty
1https://en.wikipedia.org/wiki/atlanta_(disambiguation), retrieved november 1, 2017.

under contract with mit press, shared under cc-by-nc-nd license.

406

chapter 17. information extraction

different towns and cities,    ve united states navy vessels, a magazine, a television show,
a band, and a singer     each prominent enough to have its own wikipedia page. we now
consider how to choose among these dozens of possibilities. in this chapter we will focus
on supervised approaches. unsupervised entity linking is closely related to the problem
of cross-document coreference resolution, where the task is to identify pairs of mentions
that corefer, across document boundaries (bagga and baldwin, 1998b; singh et al., 2011).

17.1.1 entity linking by learning to rank

entity linking is often formulated as a ranking problem,

  y = argmax
y   y(x)

  (y, x, c),

[17.1]

where y is a target entity, x is a description of the mention, y(x) is a set of candidate
entities, and c is a description of the context     such as the other text in the document,
or its metadata. the function    is a scoring function, which could be a linear model,
  (y, x, c) =       f (y, x, c), or a more complex function such as a neural network. in either
case, the scoring function can be learned by minimizing a margin-based ranking loss,

(cid:96)(  y, y(i), x(i), c(i)) =(cid:16)  (  y, x(i), c(i))       (y(i), x(i), c(i)) + 1(cid:17)+

,

[17.2]

where y(i) is the ground truth and   y (cid:54)= y(i) is the predicted target for mention x(i) in
context c(i) (joachims, 2002; dredze et al., 2010).

candidate identi   cation for computational tractability, it is helpful to restrict the set of
candidates, y(x). one approach is to use a name dictionary, which maps from strings
to the entities that they might mention. this mapping is many-to-many: a string such as
atlanta can refer to multiple entities, and conversely, an entity such as atlanta can be
referenced by multiple strings. a name dictionary can be extracted from wikipedia, with
links between each wikipedia entity page and the anchor text of all hyperlinks that point
to the page (bunescu and pasca, 2006; ratinov et al., 2011). to improve recall, the name
dictionary can be augmented by partial and approximate matching (dredze et al., 2010),
but as the set of candidates grows, the risk of false positives increases. for example, the
string atlanta is a partial match to the atlanta fed (a name for the federal reserve bank
of atlanta), and a noisy match (id153 of one) from atalanta (a heroine in greek
mythology and an italian soccer team).

features feature-based approaches to entity ranking rely on three main types of local
information (dredze et al., 2010):

jacob eisenstein. draft of november 13, 2018.

17.1. entities

407

    the similarity of the mention string to the canonical entity name, as quanti   ed by
string similarity. this feature would elevate the city atlanta over the basketball
team atlanta hawks for the string atlanta.

    the popularity of the entity, which can be measured by wikipedia page views or
id95 in the wikipedia link graph. this feature would elevate atlanta, geor-
gia over the unincorporated community of atlanta, ohio.

    the entity type, as output by the id39 system. this feature
would elevate the city of atlanta over the magazine atlanta in contexts where
the mention is tagged as a location.

in addition to these local features, the document context can also help. if jamaica is men-
tioned in a document about the caribbean, it is likely to refer to the island nation; in
the context of new york, it is likely to refer to the neighborhood in queens; in the con-
text of a menu, it might refer to a hibiscus tea beverage. such hints can be formalized
by computing the similarity between the wikipedia page describing each candidate en-
tity and the mention context c(i), which may include the bag-of-words representing the
document (dredze et al., 2010; hoffart et al., 2011) or a smaller window of text around
the mention (ratinov et al., 2011). for example, we can compute the cosine similarity
between bag-of-words vectors for the context and entity description, typically weighted
using inverse document frequency to emphasize rare words.2

neural entity linking an alternative approach is to compute the score for each entity
candidate using distributed vector representations of the entities, mentions, and context.
for example, for the task of entity linking in twitter, yang et al. (2016) employ the bilinear
scoring function,

(cid:62)
y   (y,x)x + v

(cid:62)
y   (y,c)c,

  (y, x, c) = v

[17.3]
with vy     rky as the vector embedding of entity y, x     rkx as the embedding of the
mention, c     rkc as the embedding of the context, and the matrices   (y,x) and   (y,c)
as parameters that score the compatibility of each entity with respect to the mention and
context. each of the vector embeddings can be learned from an end-to-end objective, or
pre-trained on unlabeled data.

    pretrained entity embeddings can be obtained from an existing knowledge base (bor-
des et al., 2011, 2013), or by running a id27 algorithm such as id97

(cid:80)n

(cid:16)

(cid:17)

2the document frequency of word j is df(j) = 1

, equal to the number of docu-
ments in which the word appears. the contribution of each word to the cosine similarity of two bag-of-
words vectors can be weighted by the inverse document frequency
df(j) , to emphasize rare
words (sp  arck jones, 1972).

df(j) or log

x(i)
j > 0

i=1   

n

1

1

under contract with mit press, shared under cc-by-nc-nd license.

408

chapter 17. information extraction

on the text of wikipedia, with hyperlinks substituted for the anchor text.3

    the embedding of the mention x can be computed by averaging the embeddings
of the words in the mention (yang et al., 2016), or by the compositional techniques
described in    14.8.

    the embedding of the context c can also be computed from the embeddings of the
words in the context. a denoising autoencoder learns a function from raw text to
dense k-dimensional vector encodings by minimizing a reconstruction loss (vin-
cent et al., 2010),

min
  g,  h

n(cid:88)i=1

||x(i)     g(h(   x(i);   h);   g)||2,

[17.4]

where   x(i) is a noisy version of the bag-of-words counts x(i), which is produced by
randomly setting some counts to zero; h : rv     rk is an encoder with parameters
  h; and g : rk     rv , with parameters   g. the encoder and decoder functions
are typically implemented as feedforward neural networks. to apply this model to
entity linking, each entity and context are initially represented by the encoding of
their bag-of-words vectors, h(e) and g(c), and these encodings are then    ne-tuned
from labeled data (he et al., 2013). the context vector c can also be obtained by
convolution (   3.4) on the embeddings of words in the document (sun et al., 2015),
or by examining metadata such as the author   s social network (yang et al., 2016).

the remaining parameters   (y,x) and   (y,c) can be trained by id26 from the
margin loss in equation 17.2.

17.1.2 collective entity linking

entity linking can be more accurate when it is performed jointly across a document. to
see why, consider the following lists:

(17.5)

a. california, oregon, washington
b. baltimore, washington, philadelphia
c. washington, adams, jefferson

in each case, the term washington refers to a different entity, and this reference is strongly
suggested by the other entries on the list. in the last list, all three names are highly am-
biguous     there are dozens of other adams and jefferson entities in wikipedia. but a

3pre-trained entity embeddings can be downloaded from https://code.google.com/archive/p/

id97/.

jacob eisenstein. draft of november 13, 2018.

17.1. entities

409

preference for coherence motivates collectively linking these references to the    rst three
u.s. presidents.

a general approach to collective entity linking is to introduce a compatibility score

  c(y). collective entity linking is then performed by optimizing the global objective,

  y = argmax
y   y(x)

  c(y) +

n(cid:88)i=1

  (cid:96)(y(i), x(i), c(i)),

[17.5]

can be computed in a number of different ways:

where y(x) is the set of all possible collective entity assignments for the mentions in x,
and   (cid:96) is the local scoring function for each entity i. the compatibility function is typically
j(cid:54)=i   c(y(i), y(j)). these scores

decomposed into a sum of pairwise scores,   c(y) =(cid:80)n
    wikipedia de   nes high-level categories for entities (e.g., living people, presidents of
the united states, states of the united states), and   c can reward entity pairs for the
number of categories that they have in common (cucerzan, 2007).

i=1(cid:80)n

    compatibility can be measured by the number of incoming hyperlinks shared by

the wikipedia pages for the two entities (milne and witten, 2008).

    in a neural architecture, the compatibility of two entities can be set equal to the inner

product of their embeddings,   c(y(i), y(j)) = vy(i)    vy(j).

    a non-pairwise compatibility score can be de   ned using a type of latent variable
model known as a probabilistic topic model (blei et al., 2003; blei, 2012). in this
framework, each latent topic is a id203 distribution over entities, and each
document has a id203 distribution over topics. each entity helps to determine
the document   s distribution over topics, and in turn these topics help to resolve am-
biguous entity mentions (newman et al., 2006). id136 can be performed using
the sampling techniques described in chapter 5.

unfortunately, collective entity linking is np-hard even for pairwise compatibility func-
tions, so exact optimization is almost certainly intractable. various approximate id136
techniques have been proposed, including integer id135 (cheng and roth,
2013), id150 (han and sun, 2012), and graph-based algorithms (hoffart et al.,
2011; han et al., 2011).

17.1.3

*pairwise ranking id168s

the id168 de   ned in equation 17.2 considers only the highest-scoring prediction
  y, but in fact, the true entity y(i) should outscore all other entities. a id168 based on
this idea would give a gradient against the features or representations of several entities,

under contract with mit press, shared under cc-by-nc-nd license.

410

chapter 17. information extraction

n     0
repeat

algorithm 18 warp approximate ranking loss
1: procedure warp(y(i), x(i))
2:
3:
4:
5:
6:
7:
8:
9:
10:

randomly sample y     y(x(i))
n     n + 1
if   (y, x(i)) + 1 >   (y(i), x(i)) then
r    (cid:4)|y(x(i))|/n(cid:5)
return lrank(r)    (  (y, x(i)) + 1       (y(i), x(i)))
until n     |y(x(i))|     1
return 0

(cid:46) check for margin violation
(cid:46) compute approximate rank

(cid:46) no violation found
(cid:46) return zero loss

not just the top-scoring prediction. usunier et al. (2009) de   ne a general ranking error
function,

lrank(k) =

k(cid:88)j=1

  j, with   1       2                0,

[17.6]

where k is equal to the number of labels ranked higher than the correct label y(i). this
function de   nes a class of ranking errors: if   j = 1 for all j, then the ranking error is
equal to the rank of the correct entity; if   1 = 1 and   j>1 = 0, then the ranking error is
one whenever the correct entity is not ranked    rst; if   j decreases smoothly with j, as in
  j = 1

j , then the error is between these two extremes.

this ranking error can be integrated into a margin objective. remember that large
margin classi   cation requires not only the correct label, but also that the correct label
outscores other labels by a substantial margin. a similar principle applies to ranking: we
want a high rank for the correct entity, and we want it to be separated from other entities
by a substantial margin. we therefore de   ne the margin-augmented rank,

r(y(i), x(i)) (cid:44) (cid:88)y   y(x(i))\y(i)

  (cid:16)1 +   (y, x(i))       (y(i), x(i))(cid:17) ,

[17.7]

where    (  ) is a delta function, and y(x(i)) \ y(i) is the set of all entity candidates minus
the true entity y(i). the margin-augmented rank is the rank of the true entity, after aug-
menting every other candidate with a margin of one, under the current scoring function
  . (the context c is omitted for clarity, and can be considered part of x.)

for each instance, a hinge loss is computed from the ranking error associated with this

jacob eisenstein. draft of november 13, 2018.

17.2. relations

margin-augmented rank, and the violation of the margin constraint,

(cid:96)(y(i), x(i)) =

lrank(r(y(i), x(i)))

r(y(i), x(i))

(cid:88)y   y(x)\y(i)(cid:16)  (y, x(i))       (y(i), x(i)) + 1(cid:17)+

,

411

[17.8]

the sum in equation 17.8 includes non-zero values for every label that is ranked at least as
high as the true entity, after applying the margin augmentation. dividing by the margin-
augmented rank of the true entity thus gives the average violation.

the objective in equation 17.8 is expensive to optimize when the label space is large,
as is usually the case for entity linking against large knowledge bases. this motivates a
randomized approximation called warp (weston et al., 2011), shown in algorithm 18. in
this procedure, we sample random entities until one violates the pairwise margin con-
straint,   (y, x(i)) + 1       (y(i), x(i)). the number of samples n required to    nd such
a violation yields an approximation of the margin-augmented rank of the true entity,
r(y(i), x(i))     (cid:106)|y(x)|
n (cid:107).
if a violation is found immediately, n = 1, the correct entity
probably ranks below many others, r     |y(x)|. if many samples are required before a
violation is found, n     |y(x)|, then the correct entity is probably highly ranked, r     1.
a computational advantage of warp is that it is not necessary to    nd the highest-scoring
label, which can impose a non-trivial computational cost when y(x(i)) is large. the objec-
tive is conceptually similar to the negative sampling objective in id97 (chapter 14),
which compares the observed word against randomly sampled alternatives.

17.2 relations

after identifying the entities that are mentioned in a text, the next step is to determine
how they are related. consider the following example:

(17.6) george bush traveled to france on thursday for a summit.

this sentence introduces a relation between the entities referenced by george bush and
france. in the automatic content extraction (ace) ontology (linguistic data consortium,
2005), the type of this relation is physical, and the subtype is located. this relation
would be written,

physical.located(george bush, france).

[17.9]

relations take exactly two arguments, and the order of the arguments matters.

in the ace datasets, relations are annotated between entity mentions, as in the exam-
ple above. relations can also hold between nominals, as in the following example from
the semeval-2010 shared task (hendrickx et al., 2009):

under contract with mit press, shared under cc-by-nc-nd license.

412

chapter 17. information extraction

those cancers were caused by radiation exposures
cause-effect
phone operator
instrument-agency
a factory manufactures suits
product-producer
a bottle of honey was weighed
content-container
letters from foreign countries
entity-origin
the boy went to bed
entity-destination
my apartment has a large kitchen
component-whole
there are many trees in the forest
member-collection
communication-topic the lecture was about semantics

table 17.1: relations and example sentences from the semeval-2010 dataset (hendrickx
et al., 2009)

(17.7) the cup contained tea from dried ginseng.

this sentence describes a relation of type entity-origin between tea and ginseng. nomi-
nal id36 is closely related to id14 (chapter 13). the main
difference is that id36 is restricted to a relatively small number of relation
types; for example, table 17.1 shows the ten relation types from semeval-2010.

17.2.1 pattern-based id36
early work on id36 focused on hand-crafted patterns (hearst, 1992). for
example, the appositive starbuck, a native of nantucket signals the relation entity-origin
between starbuck and nantucket. this pattern can be written as,

person , a native of location     entity-origin(person, location).

[17.10]

this pattern will be    triggered    whenever the literal string , a native of occurs between an
entity of type person and an entity of type location. such patterns can be generalized
beyond literal matches using techniques such as lemmatization, which would enable the
words (buy, buys, buying) to trigger the same patterns (see    4.3.1). a more aggressive
strategy would be to group all words in a id138 synset (   4.2), so that, e.g., buy and
purchase trigger the same patterns.

id36 patterns can be implemented in    nite-state automata (   9.1). if the
named entity recognizer is also a    nite-state machine, then the systems can be combined
by    nite-state transduction (hobbs et al., 1997). this makes it possible to propagate uncer-
tainty through the    nite-state cascade, and disambiguate from higher-level context. for
example, suppose the entity recognizer cannot decide whether starbuck refers to either a
person or a location; in the composed transducer, the relation extractor would be free
to select the person annotation when it appears in the context of an appropriate pattern.

jacob eisenstein. draft of november 13, 2018.

17.2. relations

17.2.2 id36 as a classi   cation task
id36 can be formulated as a classi   cation problem,

  r(i,j),(m,n) = argmax

r   r   (r, (i, j), (m, n), w),

413

[17.11]

where r     r is a relation type (possibly nil), wi+1:j is the span of the    rst argument, and
wm+1:n is the span of the second argument. the argument wm+1:n may appear before
or after wi+1:j in the text, or they may overlap; we stipulate only that wi+1:j is the    rst
argument of the relation. we now consider three alternatives for computing the scoring
function.

feature-based classi   cation

in a feature-based classi   er, the scoring function is de   ned as,

  (r, (i, j), (m, n), w) =       f (r, (i, j), (m, n), w),

[17.12]

with    representing a vector of weights, and f (  ) a vector of features. the pattern-based
methods described in    17.2.1 suggest several features:

    local features of wi+1:j and wm+1:n, including: the strings themselves; whether they
are recognized as entities, and if so, which type; whether the strings are present in a
gazetteer of entity names; each string   s syntactic head (   9.2.2).

    features of the span between the two arguments, wj+1:m or wn+1:i (depending on
which argument appears    rst): the length of the span; the speci   c words that appear
in the span, either as a literal sequence or a bag-of-words; the id138 synsets (   4.2)
that appear in the span between the arguments.
    features of the syntactic relationship between the two arguments, typically the de-
pendency path between the arguments (   13.2.1). example dependency paths are
shown in table 17.2.

kernels

suppose that the    rst line of table 17.2 is a labeled example, and the remaining lines are
instances to be classi   ed. a feature-based approach would have to decompose the depen-
dency paths into features that capture individual edges, with or without their labels, and
then learn weights for each of these features: for example, the second line contains identi-
cal dependencies, but different arguments; the third line contains a different in   ection of
the word travel; the fourth and    fth lines each contain an additional edge on the depen-
dency path; and the sixth example uses an entirely different path. rather than attempting
to create local features that capture all of the ways in which these dependencies paths

under contract with mit press, shared under cc-by-nc-nd license.

414

chapter 17. information extraction

1. george bush traveled to france
george bush    nsubj
2. ahab traveled to nantucket
ahab    nsubj
3. george bush will travel to france
george bush    nsubj
4. george bush wants to travel to france george bush    nsubj
5. ahab traveled to a city in france
6. we await ahab    s visit to france

ahab    nsubj
ahab    nmod:poss

traveled   obl

france

travel    obl
wants    xcomp
city    nmod
france

travel    obl
france

traveled    obl

visit    nmod

traveled    obl

france

nantucket

france

table 17.2: candidates instances for the physical.located relation, and their depen-
dency paths

are similar and different, we can instead de   ne a similarity function   , which computes a
score for any pair of instances,    : x    x     r+. the score for any pair of instances (i, j)
is   (x(i), x(j))     0, with   (i, j) being large when instances x(i) and x(j) are similar. if the
function    obeys a few key properties it is a valid id81.4

given a valid id81, we can build a non-linear classi   er without explicitly
de   ning a feature vector or neural network architecture. for a binary classi   cation prob-
lem y     {   1, 1}, we have the decision function,
n(cid:88)i=1

y(i)  (i)  (x(i), x))

  y =sign(b +

[17.13]

where b and {  (i)}n
i=1 are parameters that must be learned from the training set, under
the constraint    i,   (i)     0. intuitively, each   i speci   es the importance of the instance x(i)
towards the classi   cation rule. kernel-based classi   cation can be viewed as a weighted
form of the nearest-neighbor classi   er (hastie et al., 2009), in which test instances are
assigned the most common label among their near neighbors in the training set. this
results in a non-linear classi   cation boundary. the parameters are typically learned from
a margin-based objective (see    2.4), leading to the kernel support vector machine. to
generalize to multi-class classi   cation, we can train separate binary classi   ers for each
label (sometimes called one-versus-all), or train binary classi   ers for each pair of possible
labels (one-versus-one).

dependency kernels are particularly effective for id36, due to their abil-
ity to capture syntactic properties of the path between the two candidate arguments. one
class of dependency tree kernels is de   ned recursively, with the score for a pair of trees

4the gram matrix k arises from computing the id81 between all pairs in a set of instances. for
a valid kernel, the gram matrix must be symmetric (k = k(cid:62)) and positive semi-de   nite (   a, a(cid:62)ka     0).
for more on kernel-based classi   cation, see chapter 14 of murphy (2012).

jacob eisenstein. draft of november 13, 2018.

17.2. relations

415

equal to the similarity of the root nodes and the sum of similarities of matched pairs of
child subtrees (zelenko et al., 2003; culotta and sorensen, 2004). alternatively, bunescu
and mooney (2005) de   ne a id81 over sequences of unlabeled dependency
edges, in which the score is computed as a product of scores for each pair of words in the
sequence: identical words receive a high score, words that share a synset or part-of-speech
receive a small non-zero score (e.g., travel / visit), and unrelated words receive a score of
zero.

neural id36
convolutional neural networks (   3.4) were an early neural architecture for relation ex-
traction (zeng et al., 2014; dos santos et al., 2015). for the sentence (w1, w2, . . . , wm ),
obtain a matrix of id27s x, where xm     rk is the embedding of wm. now,
suppose the candidate arguments appear at positions a1 and a2; then for each word in
the sentence, its position with respect to each argument is m     a1 and m     a2. (following
zeng et al. (2014), this is a restricted version of the id36 task in which the
arguments are single tokens.) to capture any information conveyed by these positions,
the id27s are concatenated with vector encodings of the positional offsets,
x(p)
m   a1 and x(p)
m   a2. (for more on positional encodings, see    18.3.2.) the complete base
representation of the sentence is,

x(a1, a2) =         

x1
x(p)
1   a1
x(p)
1   a2

x2
x(p)
2   a1
x(p)
2   a2

       xm
       x(p)
       x(p)

m   a1
m   a2

          ,

[17.14]

where each column is a vertical concatenation of a id27, represented by the
column vector xm, and two positional encodings, specifying the position with respect to
a1 and a2. the matrix x(a1, a2) is then taken as input to a convolutional layer (see    3.4),
and max-pooling is applied to obtain a vector. the    nal scoring function is then,

  (r, i, j, x) =   r    maxpool(convnet(x(i, j);   )),

[17.15]

where    de   nes the parameters of the convolutional operator, and the   r de   nes a set of
weights for relation r. the model can be trained using a margin objective,

  r = argmax

r

  (r, i, j, x)

(cid:96) =(1 +   (  r, i, j, x)       (r, i, j, x))+.

[17.16]

[17.17]

recurrent neural networks (   6.3) have also been applied to id36, us-
ing a network such as a bidirectional lstm to encode the words or dependency path
between the two arguments. xu et al. (2015) segment each dependency path into left and

under contract with mit press, shared under cc-by-nc-nd license.

416

chapter 17. information extraction

travel    obl

travel    obl

wants    xcomp

wants and wants    xcomp

france is segmented into the
right subpaths: the path george bush    nsubj
france. in each path, a recurrent
subpaths, george bush    nsubj
neural network is run from the argument to the root word (in this case, wants). the    -
nal representation by max pooling (   3.4) across all the recurrent states along each path.
this process can be applied across separate    channels   , in which the inputs consist of em-
beddings for the words, parts-of-speech, dependency relations, and id138 hypernyms
(e.g., france-nation; see    4.2). to de   ne the model formally, let s(m) de   ne the successor
of word m in either the left or right subpath (in a dependency path, each word can have
a successor in at most one subpath). let x(c)
m indicate the embedding of word (or relation)
m in channel c, and let       h (c)
m indicate the associated recurrent states in the left
and right subtrees respectively. then the complete model is speci   ed as follows,

m and       h (c)

h(c)
s(m) =id56(x(c)

z(c) =maxpool(cid:16)      h (c)

i

s(m), h(c)
m )
,      h (c)

s(i), . . . ,      h (c)

root,      h (c)

j

,      h (c)

root(cid:17)
s(j), . . . ,      h (c)

  (r, i, j) =     (cid:104)z(word); z(pos); z(dependency); z(hypernym)(cid:105) .

[17.18]

[17.19]

[17.20]

note that z is computed by applying max-pooling to the matrix of horizontally concate-
nated vectors h, while    is computed from the vector of vertically concatenated vectors
z. xu et al. (2015) pass the score    through a softmax layer to obtain a id203
p(r | i, j, w), and train the model by regularized cross-id178. miwa and bansal (2016)
show that a related model can solve the more challenging    end-to-end    relation extrac-
tion task, in which the model must simultaneously detect entities and then extract their
relations.

17.2.3 knowledge base population
in many applications, what matters is not what fraction of sentences are analyzed cor-
rectly, but how much accurate knowledge can be extracted. knowledge base population
(kbp) refers to the task of    lling in wikipedia-style infoboxes, as shown in figure 17.1a.
knowledge base population can be decomposed into two subtasks: entity linking (de-
scribed in    17.1), and slot    lling (ji and grishman, 2011). slot    lling has two key dif-
ferences from the formulation of id36 presented above: the relations hold
between entities rather than spans of text, and the performance is evaluated at the type
level (on entity pairs), rather than on the token level (on individual sentences).

from a practical standpoint, there are three other important differences between slot

   lling and per-sentence id36.

    kbp tasks are often formulated from the perspective of identifying attributes of a
few    query    entities. as a result, these systems often start with an information

jacob eisenstein. draft of november 13, 2018.

17.2. relations

417

retrieval phase, in which relevant passages of text are obtained by search.

    for many entity pairs, there will be multiple passages of text that provide evidence.
slot    lling systems must aggregate this evidence to predict a single relation type (or
set of relations).

    labeled data is usually available in the form of pairs of related entities, rather than
annotated passages of text. training from such type-level annotations is a challenge:
two entities may be linked by several relations, or they may appear together in a
passage of text that nonetheless does not describe their relation to each other.

information retrieval is beyond the scope of this text (see manning et al., 2008). the re-
mainder of this section describes approaches to information fusion and learning from
type-level annotations.

information fusion

in knowledge base population, there will often be multiple pieces of evidence for (and
sometimes against) a single relation. for example, a search for the entity maynard jack-
son, jr. may return several passages that reference the entity atlanta:5

(17.8)

a. elected mayor of atlanta in 1973, maynard jackson was the    rst african

american to serve as mayor of a major southern city.

b. atlanta   s airport will be renamed to honor maynard jackson, the city   s    rst

black mayor.

c. born in dallas, texas in 1938, maynard holbrook jackson, jr. moved to

atlanta when he was 8.

d. maynard jackson has gone from one of the worst high schools in atlanta to

one of the best.

the    rst and second examples provide evidence for the relation mayor holding between
the entities atlanta and maynard jackson, jr.. the third example provides evidence
for a different relation between these same entities, lived-in. the fourth example poses
an entity linking problem, referring to maynard jackson high school. knowledge
base population requires aggregating this sort of textual evidence, and predicting the re-
lations that are most likely to hold.

one approach is to run a single-document id36 system (using the tech-
niques described in    17.2.2), and then aggregate the results (li et al., 2011). relations

5first

three

examples

from:

government-politics/maynard-jackson-1938-2003;
www.todayingeorgiahistory.org/content/maynard-jackson-elected

http://www.georgiaencyclopedia.org/articles/
2003;

jet magazine, november

10,

under contract with mit press, shared under cc-by-nc-nd license.

418

chapter 17. information extraction

that are detected with high con   dence in multiple documents are more likely to be valid,
motivating the heuristic,

  (r, e1, e2) =

n(cid:88)i=1

(p(r(e1, e2) | w(i)))  ,

[17.21]

where p(r(e1, e2) | w(i)) is the id203 of relation r between entities e1 and e2 condi-
tioned on the text w(i), and    (cid:29) 1 is a tunable hyperparameter. using this heuristic, it is
possible to rank all candidate relations, and trace out a precision-recall curve as more re-
lations are extracted.6 alternatively, features can be aggregated across multiple passages
of text, feeding a single type-level id36 system (wolfe et al., 2017).

precision can be improved by introducing constraints across multiple relations. for
example, if we are certain of the relation parent(e1, e2), then it cannot also be the case
that parent(e2, e1). integer id135 makes it possible to incorporate such
constraints into a global optimization (li et al., 2011). other pairs of relations have posi-
tive correlations, such mayor(e1, e2) and lived-in(e1, e2). compatibility across relation
types can be incorporated into probabilistic id114 (e.g., riedel et al., 2010).

distant supervision

id36 is    annotation hungry,    because each relation requires its own la-
beled data. rather than relying on annotations of individual documents, it would be
preferable to use existing knowledge resources     such as the many facts that are al-
ready captured in knowledge bases like dbpedia. however such annotations raise the
inverse of the information fusion problem considered above: the existence of the relation
mayor(maynard jackson jr., atlanta) provides only distant supervision for the
example texts in which this entity pair is mentioned.

one approach is to treat the entity pair as the instance, rather than the text itself (mintz
et al., 2009). features are then aggregated across all sentences in which both entities are
mentioned, and labels correspond to the relation (if any) between the entities in a knowl-
edge base, such as freebase. negative instances are constructed from entity pairs that are
not related in the knowledge base. in some cases, two entities are related, but the knowl-
edge base is missing the relation; however, because the number of possible entity pairs is
huge, these missing relations are presumed to be relatively rare. this approach is shown
in figure 17.2.

in multiple instance learning, labels are assigned to sets of instances, of which only
an unknown subset are actually relevant (dietterich et al., 1997; maron and lozano-p  erez,
1998). this formalizes the framework of distant supervision: the relation rel(a, b) acts

6the precision-recall curve is similar to the roc curve shown in figure 4.4, but it includes the precision

tp
tp+fp rather than the false positive rate

fp

fp+tn .

jacob eisenstein. draft of november 13, 2018.

17.2. relations

419

    label : mayor(atlanta, maynard jackson)

    elected mayor of atlanta in 1973, maynard jackson . . .
    atlanta   s airport will be renamed to honor maynard jackson, the city   s    rst black

    born in dallas, texas in 1938, maynard holbrook jackson, jr. moved to atlanta

mayor

when he was 8.

    label : mayor(new york, fiorello la guardia)

    fiorello la guardia was mayor of new york for three terms . . .
    fiorello la guardia, then serving on the new york city board of aldermen. . .

    born in dallas, texas in 1938, maynard holbrook jackson, jr. moved to atlanta

    label : born-in(dallas, maynard jackson)

when he was 8.

    maynard jackson was raised in dallas . . .
    label : nil(new york, maynard jackson)

    jackson married valerie richardson, whom he had met in new york. . .
    jackson was a member of the georgia and new york bars . . .

figure 17.2: four training instances for relation classi   cation using distant supervi-
sion mintz et al. (2009). the    rst two instances are positive for the mayor relation, and
the third instance is positive for the born-in relation. the fourth instance is a negative ex-
ample, constructed from a pair of entities (new york, maynard jackson) that do not
appear in any freebase relation. each instance   s features are computed by aggregating
across all sentences in which the two entities are mentioned.

as a label for the entire set of sentences mentioning entities a and b, even when only a
subset of these sentences actually describes the relation. one approach to multi-instance
learning is to introduce a binary latent variable for each sentence, indicating whether the
sentence expresses the labeled relation (riedel et al., 2010). a variety of id136 tech-
niques have been employed for this probabilistic model of id36: surdeanu
et al. (2012) use expectation maximization, riedel et al. (2010) use sampling, and hoff-
mann et al. (2011) use a custom graph-based algorithm. expectation maximization and
sampling are surveyed in chapter 5, and are covered in more detail by murphy (2012);
graph-based methods are surveyed by mihalcea and radev (2011).

17.2.4 id10
in classical id36, the set of relations is de   ned in advance, using a schema.
the relation for any pair of entities can then be predicted using multi-class classi   cation.
in id10 (openie), a relation can be any triple of text. the example

under contract with mit press, shared under cc-by-nc-nd license.

420

chapter 17. information extraction

relation ontology

task
propbank id14 verbnet
framenet id14 framenet
id36
slot    lling
id10

ace, tac, semeval, etc
ace, tac, semeval, etc
open

supervision
sentence
sentence
sentence
relation
seed relations or patterns

table 17.3: various id36 tasks and their properties. verbnet and framenet
are described in chapter 13. ace (linguistic data consortium, 2005), tac (mcnamee
and dang, 2009), and semeval (hendrickx et al., 2009) refer to shared tasks, each of which
involves an ontology of relation types.

sentence (17.8a) instantiates several    relations    of this sort, e.g.,

    (mayor of, maynard jackson, atlanta),
    (elected, maynard jackson, mayor of atlanta),
    (elected in, maynard jackson, 1973).

extracting such tuples can be viewed as a lightweight version of id14
(chapter 13), with only two argument types:    rst slot and second slot. the task is gen-
erally evaluated on the relation level, rather than on the level of sentences: precision is
measured by the number of extracted relations that are accurate, and recall is measured
by the number of true relations that were successfully extracted. openie systems are
trained from distant supervision or id64, rather than from labeled sentences.

an early example is the textrunner system (banko et al., 2007), which identi   es
relations with a set of handcrafted syntactic rules. the examples that are acquired from
the handcrafted rules are then used to train a classi   cation model that uses part-of-speech
patterns as features. finally, the relations that are extracted by the classi   er are aggre-
gated, removing redundant relations and computing the number of times that each rela-
tion is mentioned in the corpus. textrunner was the    rst in a series of systems that
performed increasingly accurate open id36 by incorporating more precise
linguistic features (etzioni et al., 2011), distant supervision from wikipedia infoboxes (wu
and weld, 2010), and better learning algorithms (zhu et al., 2009).

17.3 events

relations link pairs of entities, but many real-world situations involve more than two enti-
ties. consider again the example sentence (17.8a), which describes the event of an election,

jacob eisenstein. draft of november 13, 2018.

17.3. events

421

with four properties: the of   ce (mayor), the district (atlanta), the date (1973), and the
person elected (maynard jackson, jr.). in id37, a schema is provided for
each event type (e.g., an election, a terrorist attack, or a chemical reaction), indicating all
the possible properties of the event. the system is then required to    ll in as many of these
properties as possible (doddington et al., 2004).

id37 systems generally involve a retrieval component (   nding relevant
documents and passages of text) and an extraction component (determining the proper-
ties of the event based on the retrieved texts). early approaches focused on    nite-state pat-
terns for identify event properties (hobbs et al., 1997); such patterns can be automatically
induced by searching for patterns that are especially likely to appear in documents that
match the event query (riloff, 1996). contemporary approaches employ techniques that
are similar to framenet id14 (   13.2), such as id170 over
local and global features (li et al., 2013) and id182 (feng
et al., 2016). these methods detect whether an event is described in a sentence, and if so,
what are its properties.

event coreference because multiple sentences may describe unique properties of a sin-
gle event, event coreference is required to link event mentions across a single passage
of text, or between passages (humphreys et al., 1997). bejan and harabagiu (2014) de-
   ne event coreference as the task of identifying event mentions that share the same event
participants (i.e., the slot-   lling entities) and the same event properties (e.g., the time and
location), within or across documents. event coreference resolution can be performed us-
ing supervised learning techniques in a similar way to entity coreference, as described
in chapter 15: move left-to-right through the document, and use a classi   er to decide
whether to link each event reference to an existing cluster of coreferent events, or to cre-
ate a new cluster (ahn, 2006). each id91 decision is based on the compatibility of
features describing the participants and properties of the event. due to the dif   culty of
annotating large amounts of data for entity coreference, unsupervised approaches are es-
pecially desirable (chen and ji, 2009; bejan and harabagiu, 2014).

relations between events
just as entities are related to other entities, events may be
related to other events: for example, the event of winning an election both precedes and
causes the event of serving as mayor; moving to atlanta precedes and enables the event of
becoming mayor of atlanta; moving from dallas to atlanta prevents the event of later be-
coming mayor of dallas. as these examples show, events may be related both temporally
and causally. the timeml annotation scheme speci   es a set of six temporal relations
between events (pustejovsky et al., 2005), derived in part from interval algebra (allen,
1984). the timebank corpus provides timeml annotations for 186 documents (puste-
jovsky et al., 2003). methods for detecting these temporal relations combine supervised

under contract with mit press, shared under cc-by-nc-nd license.

422

chapter 17. information extraction

certain (ct)
probable (pr)
possible (ps)
underspeci   ed (u)

negative (-)
counterfact: ct-

positive (+)
fact: ct+
probable: pr+ not probable: pr-
possible: ps+ not possible: ps-
(na)

(na)

underspeci   ed (u)
certain, but unknown: ctu
(na)
(na)
unknown or uncommitted: uu

table 17.4: table of factuality values from the factbank corpus (saur     and pustejovsky,
2009). the entry (na) indicates that this combination is not annotated.

machine learning with temporal constraints, such as transitivity (e.g. mani et al., 2006;
chambers and jurafsky, 2008).

more recent annotation schemes and datasets combine temporal and causal relations (mirza

et al., 2014; dunietz et al., 2017): for example, the caters dataset includes annotations of
320    ve-sentence short stories (mostafazadeh et al., 2016). abstracting still further, pro-
cesses are networks of causal relations between multiple events. a small dataset of bi-
ological processes is annotated in the processbank dataset (berant et al., 2014), with the
goal of supporting automatic id53 on scienti   c textbooks.

17.4 hedges, denials, and hypotheticals

the methods described thus far apply to propositions about the way things are in the
real world. but natural language can also describe events and relations that are likely or
unlikely, possible or impossible, desired or feared. the following examples hint at the
scope of the problem (prabhakaran et al., 2010):

(17.9)

a. gm will lay off workers.
b. a spokesman for gm said gm will lay off workers.
c. gm may lay off workers.
d. the politician claimed that gm will lay off workers.
e. some wish gm would lay off workers.
f. will gm lay off workers?
g. many wonder whether gm will lay off workers.

accurate information extraction requires handling these extra-propositional aspects
of meaning, which are sometimes summarized under the terms modality and negation.7

7the classi   cation of negation as extra-propositional is controversial: packard et al. (2014) argue that
negation is a    core part of compositionally constructed logical-form representations.    negation is an element
of the id29 tasks discussed in chapter 12 and chapter 13     for example, negation markers are

jacob eisenstein. draft of november 13, 2018.

17.4. hedges, denials, and hypotheticals

423

modality refers to expressions of the speaker   s attitude towards her own statements, in-
cluding    degree of certainty, reliability, subjectivity, sources of information, and perspec-
tive    (morante and sporleder, 2012). various systematizations of modality have been
proposed (e.g., palmer, 2001), including categories such as future, interrogative, imper-
ative, conditional, and subjective. information extraction is particularly concerned with
negation and certainty. for example, saur     and pustejovsky (2009) link negation with
a modal calculus of certainty, likelihood, and possibility, creating the two-dimensional
schema shown in table 17.4. this is the basis for the factbank corpus, with annotations
of the factuality of all sentences in 208 documents of news text.

a related concept is hedging, in which speakers limit their commitment to a proposi-

tion (lakoff, 1973):

(17.10)

a. these results suggest that expression of c-jun, jun b and jun d genes might be
involved in terminal granulocyte differentiation. . . (morante and daelemans,
2009)

b. a whale is technically a mammal (lakoff, 1973)

in the    rst example, the hedges suggest and might communicate uncertainty; in the second
example, there is no uncertainty, but the hedge technically indicates that the evidence for
the proposition will not fully meet the reader   s expectations. hedging has been studied
extensively in scienti   c texts (medlock and briscoe, 2007; morante and daelemans, 2009),
where the goal of large-scale extraction of scienti   c facts is obstructed by hedges and spec-
ulation. still another related aspect of modality is evidentiality, in which speakers mark
the source of their information. in many languages, it is obligatory to mark evidentiality
through af   xes or particles (aikhenvald, 2004); while evidentiality is not grammaticalized
in english, authors are expected to express this information in contexts such as journal-
ism (kovach and rosenstiel, 2014) and wikipedia.8

methods for handling negation and modality generally include two phases:

1. detecting negated or uncertain events;

2. identifying scope of the negation or modal operator.

a considerable body of work on negation has employed rule-based techniques such
as id157 (chapman et al., 2001) to detect negated events. such techniques

treated as adjuncts in propbank id14. however, many of the id36 methods
mentioned in this chapter do not handle negation directly. a further consideration is that negation inter-
acts closely with aspects of modality that are generally not considered in propositional semantics, such as
certainty and subjectivity.

8https://en.wikipedia.org/wiki/wikipedia:veri   ability

under contract with mit press, shared under cc-by-nc-nd license.

424

chapter 17. information extraction

match lexical cues (e.g., norwood was not elected mayor), while avoiding    double nega-
tives    (e.g., surely all this is not without meaning). supervised techniques involve classi-
   ers over lexical and syntactic features (uzuner et al., 2009) and sequence labeling (prab-
hakaran et al., 2010).

the scope refers to the elements of the text whose propositional meaning is negated or
modulated (huddleston and pullum, 2005), as elucidated in the following example from
morante and sporleder (2012):

(17.11)

[ after his habit he said ] nothing, and after mine i asked no questions.
after his habit he said nothing, and [ after mine i asked ] no [ questions ].

in this sentence, there are two negation cues (nothing and no). each negates an event, in-
dicated by the underlined verbs said and asked, and each occurs within a scope: after his
habit he said and after mine i asked
questions. scope identi   cation is typically formal-
ized as sequence labeling problems, with each word token labeled as beginning, inside,
or outside of a cue, focus, or scope span (see    8.3). conventional sequence labeling ap-
proaches can then be applied, using surface features as well as syntax (velldal et al., 2012)
and semantic analysis (packard et al., 2014). labeled datasets include the bioscope corpus
of biomedical texts (vincze et al., 2008) and a shared task dataset of detective stories by
arthur conan doyle (morante and blanco, 2012).

17.5 id53 and machine reading

the victory of the watson question-answering system against three top human players on
the game show jeopardy! was a landmark moment for natural language processing (fer-
rucci et al., 2010). game show questions are usually answered by factoids: entity names
and short phrases.9 the task of factoid id53 is therefore closely related to
information extraction, with the additional problem of accurately parsing the question.

17.5.1 formal semantics

id29 is an effective method for question-answering in restricted domains
such as questions about geography and airline reservations (zettlemoyer and collins,
2005), and has also been applied in    open-domain    settings such as id53
on freebase (berant et al., 2013) and biomedical research abstracts (poon and domingos,
2009). one approach is to convert the question into a id198 expression that
returns a boolean value: for example, the question who is the mayor of the capital of georgia?

9the broader landscape of id53 includes    why    questions (why did ahab continue to pursue
the white whale?),    how questions    (how did queequeg die?), and requests for summaries (what was ishmael   s
attitude towards organized religion?). for more, see hirschman and gaizauskas (2001).

jacob eisenstein. draft of november 13, 2018.

17.5. id53 and machine reading

425

would be converted to,

  x.   y capital(georgia, y)     mayor(y, x).

[17.22]

this lambda expression can then be used to query an existing knowledge base, returning
   true    for all entities that satisfy it.

17.5.2 machine reading
recent work has focused on answering questions about speci   c textual passages, similar
to the reading comprehension examinations for young students (hirschman et al., 1999).
this task has come to be known as machine reading.

datasets

the machine reading problem can be formulated in a number of different ways. the most
important distinction is what form the answer should take.

    multiple-choice id53, as in the mctest dataset of stories (richard-
son et al., 2013) and the new york regents science exams (clark, 2015). in mctest,
the answer is deducible from the text alone, while in the science exams, the system
must make id136s using an existing model of the underlying scienti   c phenom-
ena. here is an example from mctest:

(17.12)

james the turtle was always getting into trouble. sometimes he   d reach into
the freezer and empty out all the food . . .
q: what is the name of the trouble making turtle?
(a) fries
(b) pudding
(c) james
(d) jane

    cloze-style       ll in the blank    questions, as in the id98/daily mail comprehension
task (hermann et al., 2015), the children   s book test (hill et al., 2016), and the who-
did-what dataset (onishi et al., 2016). in these tasks, the system must guess which
word or entity completes a sentence, based on reading a passage of text. here is an
example from who-did-what:

(17.13) q: tottenham manager juande ramos has hinted he will allow

to leave

if the bulgaria striker makes it clear he is unhappy. (onishi et al., 2016)

the query sentence may be selected either from the story itself, or from an external
summary. in either case, datasets can be created automatically by processing large

under contract with mit press, shared under cc-by-nc-nd license.

426

chapter 17. information extraction

quantities existing documents. an additional constraint is that that missing element
from the cloze must appear in the main passage of text: for example, in who-did-
what, the candidates include all entities mentioned in the main passage.
in the
id98/daily mail dataset, each entity name is replaced by a unique identi   er, e.g.,
entity37. this ensures that correct answers can only be obtained by accurately
reading the text, and not from external knowledge about the entities.

    extractive id53, in which the answer is drawn from the original text.
in wikiqa, answers are sentences (yang et al., 2015). in the stanford question an-
swering dataset (squad), answers are words or short phrases (rajpurkar et al.,
2016):

(17.14)

in metereology, precipitation is any product of the condensation of atmo-
spheric water vapor that falls under gravity.
q: what causes precipitation to fall? a: gravity

in both wikiqa and squad, the original texts are wikipedia articles, and the ques-
tions are generated by crowdworkers.

methods

a baseline method is to search the text for sentences or short passages that overlap with
both the query and the candidate answer (richardson et al., 2013). in example (17.12), this
baseline would select the correct answer, since james appears in a sentence that includes
the query terms trouble and turtle.

this baseline can be implemented as a neural architecture, using an attention mech-
anism (see    18.3.1), which scores the similarity of the query to each part of the source
text (chen et al., 2016). the    rst step is to encode the passage w(p) and the query w(q),
using two bidirectional lstms (   7.6).

h(q) =bilstm(w(q);   (q))
h(p) =bilstm(w(p);   (p)).

[17.23]
[17.24]

the query is represented by vertically concatenating the    nal states of the left-to-right

and right-to-left passes:

u =[         h(q)

mq ;          h(q)

0].

[17.25]

jacob eisenstein. draft of november 13, 2018.

17.5. id53 and machine reading

427

the attention vector is computed as a softmax over a vector of bilinear products, and

the expected representation is computed by summing over attention values,

(cid:62)
    m =(u(q))
   =softmax(     )

wah(p)
m

o =

m(cid:88)m=1

  mh(p)
m .

[17.26]
[17.27]

[17.28]

each candidate answer c is represented by a vector xc. assuming the candidate answers
are spans from the original text, these vectors can be set equal to the corresponding ele-
ment in h(p). the score for each candidate answer a is computed by the inner product,

  c = argmax

c

o    xc.

[17.29]

this architecture can be trained end-to-end from a loss based on the log-likelihood of the
correct answer. a number of related architectures have been proposed (e.g., hermann
et al., 2015; kadlec et al., 2016; dhingra et al., 2017; cui et al., 2017), and these methods are
surveyed by wang et al. (2017).

additional resources

the    eld of information extraction is surveyed in course notes by grishman (2012), and
more recently in a short survey paper (grishman, 2015). shen et al. (2015) survey the task
of entity linking, and ji and grishman (2011) survey work on knowledge base popula-
tion. this chapter   s discussion of non-propositional meaning was strongly in   uenced by
morante and sporleder (2012), who introduced a special issue of the journal computational
linguistics dedicated to recent work on modality and negation.

exercises

1. go to the wikipedia page for your favorite movie. for each record in the info box
(e.g., screenplay by: stanley kubrick), report whether there is a sentence in the ar-
ticle containing both the    eld and value (e.g., the screenplay was written by stanley
kubrick). if not, is there is a sentence in the article containing just the value? (for
records with more than one value, just use the    rst value.)

2. building on your answer in the previous question, report the dependency path be-

tween the head words of the    eld and value for at least three records.

3. consider the following heuristic for entity linking:

under contract with mit press, shared under cc-by-nc-nd license.

428

chapter 17. information extraction

choose the one whose name has the lowest id153 from the mention.

    among all entities that have the same type as the mention (e.g., loc, per),
    if more than one entity has the right type and the lowest id153 from the
    if no candidate entity has the right type, choose nil.

mention, choose the most popular one.

now suppose you have the following feature function:

f (y, x) = [edit-dist(name(y), x), same-type(y, x), popularity(y),    (y = nil)]

design a set of ranking weights    that match the heuristic. you may assume that
id153 and popularity are always in the range [0, 100], and that the nil entity
has values of zero for all features except    (y = nil).

4. now consider another heuristic:

and are the right type, choose the most popular one.

    among all candidate entities that have id153 zero from the mention,
    if no entity has id153 zero from the mention, choose the one with the
    if no entity has the right type, choose nil.

right type that is most popular, regardless of id153.

using the same features and assumptions from the previous problem, prove that
there is no set of weights that could implement this heuristic. then show that the
heuristic can be implemented by adding a single feature. your new feature should
consider only the id153.

5. download the reuters corpus in nltk, and iterate over the tokens in the corpus:

import nltk
nltk.corpus.download(   reuters   )
from nltk.corpus import reuters
for word in reuters.words():

#your code here

a) apply the pattern

to obtain candidates for the is-a relation,
e.g. is-a(romania, country). what are three pairs that this method identi-
   es correctly? what are three different pairs that it gets wrong?

, such as

b) design a pattern for the president relation, e.g. president(philippines, corazon aquino).

in this case, you may want to augment your pattern matcher with the ability
to match multiple token wildcards, perhaps using case information to detect
proper names. again, list three correct

jacob eisenstein. draft of november 13, 2018.

17.5. id53 and machine reading

429

c) preprocess the reuters data by running a named entity recognizer, replacing
tokens with named entity spans when applicable     e.g., your pattern can now
match on the united states if the ner system tags it. apply your president
matcher to this preprocessed data. does the accuracy improve? compare 20
randomly-selected pairs from this pattern and the one you designed in the pre-
vious part.

6. using the same nltk reuters corpus, apply distant supervision to build a training
set for detecting the relation between nations and their capitals. start with the fol-
lowing known relations: (japan, tokyo), (france, paris), (italy, rome). how
many positive and negative examples are you able to extract?

7. represent the dependency path x(i) as a sequence of words and dependency arcs
of length mi, ignoring the endpoints of the path. in example 1 of table 17.2, the
dependency path is,

x(1) = (    nsubj

, traveled,    obl

)

[17.30]

m is a word, then let pos(x(i)

if x(i)
chapter 8.
we can de   ne the following id81 over pairs of dependency paths (bunescu
and mooney, 2005):

m ) be its part-of-speech, using the tagset de   ned in

  (x(i), x(j)) =(cid:40)0,
(cid:81)mi
m ) =               

m , x(j)

c(x(i)

m=1 c(x(i)

m , x(j)

mi (cid:54)= mj
m ), mi = mj

m = x(j)
2, x(i)
m
1, x(i)
m (cid:54)= x(j)
m and pos(x(i)
0, otherwise.

m ) = pos(x(j)
m )

using this id81, compute the kernel similarities of example 1 from ta-
ble 17.2 with the other    ve examples.

8. continuing from the previous problem, suppose that the instances have the follow-

ing labels:

y2 = 1, y3 =    1, y4 =    1, y5 = 1, y6 = 1

[17.31]

equation 17.13 de   nes a kernel-based classi   cation in terms of parameters    and
b. using the above labels for y2, . . . , y6, identify the values of    and b under which
  y1 = 1. remember the constraint that   i     0 for all i.

under contract with mit press, shared under cc-by-nc-nd license.

430

chapter 17. information extraction

9. consider the neural qa system described in    17.5.2, but restrict the set of candidate
answers to words in the passage, and set each candidate answer embedding x equal
m , representing token m in the passage, so that   m = argmaxm o   h(p)
to the vector h(p)
m .
suppose the system selects answer   m, but the correct answer is m   . consider the
gradient of the margin loss with respect to the attention:

     m   .

        m        (cid:96)

a) prove that    (cid:96)
b) assuming that ||h   m|| = ||hm   ||, prove that    (cid:96)

     m        0. explain in
words what this means about how the attention is expected to change after a
gradient-based update.

        m     0 and    (cid:96)

jacob eisenstein. draft of november 13, 2018.

chapter 18

machine translation

machine translation (mt) is one of the    holy grail    problems in arti   cial intelligence,
with the potential to transform society by facilitating communication between people
anywhere in the world. as a result, mt has received signi   cant attention and funding
since the early 1950s. however, it has proved remarkably challenging, and while there
has been substantial progress towards usable mt systems     especially for high-resource
language pairs like english-french     we are still far from translation systems that match
the nuance and depth of human translations.

18.1 machine translation as a task

machine translation can be formulated as an optimization problem:

  w(t) = argmax

  (w(s), w(t)),

w(t)

[18.1]

where w(s) is a sentence in a source language, w(t) is a sentence in the target language,
and    is a scoring function. as usual, this formalism requires two components: a decod-
ing algorithm for computing   w(t), and a learning algorithm for estimating the parameters
of the scoring function   .

decoding is dif   cult for machine translation because of the huge space of possible
translations. we have faced large label spaces before: for example, in sequence labeling,
the set of possible label sequences is exponential in the length of the input. in these cases,
it was possible to search the space quickly by introducing locality assumptions: for ex-
ample, that each tag depends only on its predecessor, or that each production depends
only on its parent. in machine translation, no such locality assumptions seem possible:
human translators reword, reorder, and rearrange words; they replace single words with
multi-word phrases, and vice versa. this    exibility means that in even relatively simple

431

432

chapter 18. machine translation

figure 18.1: the vauquois pyramid

translation models, decoding is np-hard (knight, 1999). approaches for dealing with this
complexity are described in    18.4.

estimating translation models is dif   cult as well. labeled translation data usually

comes in the form parallel sentences, e.g.,

w(s) =a vinay le gusta las manzanas.
w(t) =vinay likes apples.

a useful feature function would note the translation pairs (gusta, likes), (manzanas, apples),
and even (vinay, vinay). but this word-to-word alignment is not given in the data. one
solution is to treat this alignment as a latent variable; this is the approach taken by clas-
sical id151 (smt) systems, described in    18.2. another solution
is to model the relationship between w(t) and w(s) through a more complex and expres-
sive function; this is the approach taken by id4 (id4) systems,
described in    18.3.

the vauquois pyramid is a theory of how translation should be done. at the lowest
level, the translation system operates on individual words, but the horizontal distance
at this level is large, because languages express ideas differently. if we can move up the
triangle to syntactic structure, the distance for translation is reduced; we then need only
produce target-language text from the syntactic representation, which can be as simple
as reading off a tree. further up the triangle lies semantics; translating between semantic
representations should be easier still, but mapping between semantics and surface text is a
dif   cult, unsolved problem. at the top of the triangle is interlingua, a semantic represen-
tation that is so generic that it is identical across all human languages. philosophers de-
bate whether such a thing as interlingua is really possible (e.g., derrida, 1985). while the
   rst-order logic representations discussed in chapter 12 might be thought to be language
independent, they are built on an inventory of predicates that are suspiciously similar to
english words (nirenburg and wilks, 2001). nonetheless, the idea of linking translation

jacob eisenstein. draft of november 13, 2018.

sourcetargettextsyntaxsemanticsinterlingua18.1. machine translation as a task

433

adequate? fluent?
to vinay it like python
yes
vinay debugs memory leaks no
vinay likes python
yes

no
yes
yes

table 18.1: adequacy and    uency for translations of the spanish sentence a vinay le gusta
python.

and semantic understanding may still be a promising path, if the resulting translations
better preserve the meaning of the original text.

18.1.1 evaluating translations
there are two main criteria for a translation, summarized in table 18.1.

    adequacy: the translation w(t) should adequately re   ect the linguistic content of
w(s). for example, if w(s) = a vinay le gusta python, the reference translation is
w(t) = vinay likes python. however, the gloss, or word-for-word translation w(t) =
to vinay it like python is also considered adequate because it contains all the relevant
content. the output w(t) = vinay debugs memory leaks is not adequate.

    fluency: the translation w(t) should read like    uent text in the target language. by
this criterion, the gloss w(t) = to vinay it like python will score poorly, and w(t) =
vinay debugs memory leaks will be preferred.

automated evaluations of machine translations typically merge both of these criteria,
by comparing the system translation with one or more reference translations, produced
by professional human translators. the most popular quantitative metric is id7 (bilin-
gual evaluation understudy; papineni et al., 2002), which is based on id165 precision:
what fraction of id165s in the system translation appear in the reference? speci   cally,
for each id165 length, the precision is de   ned as,

pn =

number of id165s appearing in both reference and hypothesis translations

number of id165s appearing in the hypothesis translation

.

[18.2]

the id165 precisions for three hypothesis translations are shown in figure 18.2.

the id7 score is then based on the average, exp 1

n=1 log pn. two modi   cations
of equation 18.2 are necessary: (1) to avoid computing log 0, all precisions are smoothed
to ensure that they are positive; (2) each id165 in the reference can be used at most
once, so that to to to to to to does not achieve p1 = 1 against the reference to be or not to
be. furthermore, precision-based metrics are biased in favor of short translations, which

n(cid:80)n

under contract with mit press, shared under cc-by-nc-nd license.

434

chapter 18. machine translation

translation

p1

p2

p3

p4 bp id7

reference vinay likes programming in python

sys1
sys2
sys3

to vinay it like to program python
vinay likes python
vinay likes programming in his pajamas

2
7
3
3
4
6

0
1
2
3
5

0
0
2
4

0
0
1
3

1
.51
1

.21
.33
.76

figure 18.2: a reference translation and three system outputs. for each output, pn indi-
cates the precision at each id165, and bp indicates the brevity penalty.

can achieve high scores by minimizing the denominator in [18.2]. to avoid this issue, a
brevity penalty is applied to translations that are shorter than the reference. this penalty
is indicated as    bp    in figure 18.2.

automated metrics like id7 have been validated by correlation with human judg-
ments of translation quality. nonetheless, it is not dif   cult to construct examples in which
the id7 score is high, yet the translation is dis   uent or carries a completely different
meaning from the original. to give just one example, consider the problem of translating
pronouns. because pronouns refer to speci   c entities, a single incorrect pronoun can oblit-
erate the semantics of the original sentence. existing state-of-the-art systems generally
do not attempt the reasoning necessary to correctly resolve pronominal anaphora (hard-
meier, 2012). despite the importance of pronouns for semantics, they have a marginal
impact on id7, which may help to explain why existing systems do not make a greater
effort to translate them correctly.

fairness and bias the problem of pronoun translation intersects with issues of fairness
and bias. in many languages, such as turkish, the third person singular pronoun is gender
neutral. today   s state-of-the-art systems produce the following turkish-english transla-
tions (caliskan et al., 2017):

(18.1) o
he
(18.2) o

she

bir
is a
bir
is a

doktor.
doctor.
hem  sire.
nurse.

the same problem arises for other professions that have stereotypical genders, such as
engineers, soldiers, and teachers, and for other languages that have gender-neutral pro-
nouns. this bias was not directly programmed into the translation model; it arises from
statistical tendencies in existing datasets. this highlights a general problem with data-
driven approaches, which can perpetuate biases that negatively impact disadvantaged

jacob eisenstein. draft of november 13, 2018.

18.1. machine translation as a task

435

groups. worse, machine learning can amplify biases in data (bolukbasi et al., 2016): if a
dataset has even a slight tendency towards men as doctors, the resulting translation model
may produce translations in which doctors are always he, and nurses are always she.

other metrics a range of other automated metrics have been proposed for machine
translation. one potential weakness of id7 is that it only measures precision; meteor
is a weighted f -measure, which is a combination of recall and precision (see    4.4.1).
translation error rate (ter) computes the string id153 (see    9.1.4) between the
reference and the hypothesis (snover et al., 2006). for language pairs like english and
japanese, there are substantial differences in word order, and word order errors are not
suf   ciently captured by id165 based metrics. the ribes metric applies rank correla-
tion to measure the similarity in word order between the system and reference transla-
tions (isozaki et al., 2010).

18.1.2 data

data-driven approaches to machine translation rely primarily on parallel corpora, which
are translations at the sentence level. early work focused on government records, in which
   ne-grained of   cial translations are often required. for example, the ibm translation sys-
tems were based on the proceedings of the canadian parliament, called hansards, which
are recorded in english and french (brown et al., 1990). the growth of the european
union led to the development of the europarl corpus, which spans 21 european lan-
guages (koehn, 2005). while these datasets helped to launch the    eld of machine transla-
tion, they are restricted to narrow domains and a formal speaking style, limiting their ap-
plicability to other types of text. as more resources are committed to machine translation,
new translation datasets have been commissioned. this has broadened the scope of avail-
able data to news,1 movie subtitles,2 social media (ling et al., 2013), dialogues (fordyce,
2007), ted talks (paul et al., 2010), and scienti   c research articles (nakazawa et al., 2016).
despite this growing set of resources, the main bottleneck in machine translation data
is the need for parallel corpora that are aligned at the sentence level. many languages have
sizable parallel corpora with some high-resource language, but not with each other. the
high-resource language can then be used as a    pivot    or    bridge    (boitet, 1988; utiyama
and isahara, 2007): for example, de gispert and marino (2006) use spanish as a bridge for
translation between catalan and english. for most of the 6000 languages spoken today,
the only source of translation data remains the judeo-christian bible (resnik et al., 1999).
while relatively small, at less than a million tokens, the bible has been translated into
more than 2000 languages, far outpacing any other corpus. some research has explored

1https://catalog.ldc.upenn.edu/ldc2010t10,

http://www.statmt.org/wmt15/

translation-task.html

2http://opus.nlpl.eu/

under contract with mit press, shared under cc-by-nc-nd license.

436

chapter 18. machine translation

the possibility of automatically identifying parallel sentence pairs from unaligned parallel
texts, such as web pages and wikipedia articles (kilgarriff and grefenstette, 2003; resnik
and smith, 2003; adafre and de rijke, 2006). another approach is to create large parallel
corpora through id104 (zaidan and callison-burch, 2011).

18.2 id151

the previous section introduced adequacy and    uency as the two main criteria for ma-
chine translation. a natural modeling approach is to represent them with separate scores,

  (w(s), w(t)) =   a(w(s), w(t)) +   f (w(t)).

[18.3]

the    uency score   f need not even consider the source sentence; it only judges w(t) on
whether it is    uent in the target language. this decomposition is advantageous because
it makes it possible to estimate the two scoring functions on separate data. while the
adequacy model must be estimated from aligned sentences     which are relatively expen-
sive and rare     the    uency model can be estimated from monolingual text in the target
language. large monolingual corpora are now available in many languages, thanks to
resources such as wikipedia.

an elegant justi   cation of the decomposition in equation 18.3 is provided by the noisy

channel model, in which each scoring function is a log id203:

  a(w(s), w(t)) (cid:44) log ps|t (w(s) | w(t))

  f (w(t)) (cid:44) log pt (w(t))

  (w(s), w(t)) = log ps|t (w(s) | w(t)) + log pt (w(t)) = log ps,t (w(s), w(t)).

[18.4]
[18.5]
[18.6]

by setting the scoring functions equal to the logarithms of the prior and likelihood, their
sum is equal to log ps,t , which is the logarithm of the joint id203 of the source and
target. the sentence   w(t) that maximizes this joint id203 is also the maximizer of the
id155 pt|s, making it the most likely target language sentence, condi-
tioned on the source.

the id87 can be justi   ed by a generative story. the target text is orig-
inally generated from a id203 model pt . it is then encoded in a    noisy channel   
ps|t , which converts it to a string in the source language. in decoding, we apply bayes   
rule to recover the string w(t) that is maximally likely under the id155
pt|s. under this interpretation, the target id203 pt is just a language model, and
can be estimated using any of the techniques from chapter 6. the only remaining learning
problem is to estimate the translation model ps|t .

jacob eisenstein. draft of november 13, 2018.

18.2. id151

437

figure 18.3: an example word-to-word alignment

18.2.1 statistical translation modeling
the simplest decomposition of the translation model is word-to-word: each word in the
source should be aligned to a word in the translation. this approach presupposes an
alignment a(w(s), w(t)), which contains a list of pairs of source and target tokens. for
example, given w(s) = a vinay le gusta python and w(t) = vinay likes python, one possible
word-to-word alignment is,

a(w(s), w(t)) = {(a,    ), (vinay, vinay), (le, likes), (gusta, likes), (python,python)}.

[18.7]

this alignment is shown in figure 18.3. another, less promising, alignment is:

a(w(s), w(t)) = {(a, vinay), (vinay, likes), (le, python), (gusta,    ), (python,    )}.

[18.8]

each alignment contains exactly one tuple for each word in the source, which serves to
explain how the source word could be translated from the target, as required by the trans-
lation id203 ps|t . if no appropriate word in the target can be identi   ed for a source
word, it is aligned to         as is the case for the spanish function word a in the example,
which glosses to the english word to. words in the target can align with multiple words
in the source, so that the target word likes can align to both le and gusta in the source.

the joint id203 of the alignment and the translation can be de   ned conveniently

as,

p(w(s),a | w(t)) =

=

p(w(s)

m , am | w(t)

am, m, m (s), m (t))

p(am | m, m (s), m (t))    p(w(s)

m | w(t)
am).

[18.9]

[18.10]

m (s)(cid:89)m=1
m (s)(cid:89)m=1

this id203 model makes two key assumptions:

under contract with mit press, shared under cc-by-nc-nd license.

avinaylegustapythonvinaylikespythonm (s)(cid:89)m=1

m (s)(cid:89)m=1

438

chapter 18. machine translation

    the alignment id203 factors across tokens,

p(a | w(s), w(t)) =

p(am | m, m (s), m (t)).

[18.11]

this means that each alignment decision is independent of the others, and depends
only on the index m, and the sentence lengths m (s) and m (t).

    the translation id203 also factors across tokens,

p(w(s) | w(t),a) =

p(w(s)

m | w(t)
am),

[18.12]

so that each word in w(s) depends only on its aligned word in w(t). this means that
translation is word-to-word, ignoring context. the hope is that the target language
model p(w(t)) will correct any dis   uencies that arise from word-to-word translation.

to translate with such a model, we could sum or max over all possible alignments,

p(w(s), w(t)) =(cid:88)a

p(w(s), w(t),a)

=p(w(t))(cid:88)a
p(a)    p(w(s) | w(t),a)
   p(w(t)) maxa p(a)    p(w(s) | w(t),a).

[18.13]

[18.14]

[18.15]

the term p(a) de   nes the prior id203 over alignments. a series of alignment
models with increasingly relaxed independence assumptions was developed by researchers
at ibm in the 1980s and 1990s, known as ibm models 1-6 (och and ney, 2003).
ibm
model 1 makes the strongest independence assumption:
1

p(am | m, m (s), m (t)) =

.

m (t)

[18.16]

in this model, every alignment is equally likely. this is almost surely wrong, but it re-
sults in a convex learning objective, yielding a good initialization for the more complex
alignment models (brown et al., 1993; koehn, 2009).

18.2.2 estimation
let us de   ne the parameter   u   v as the id203 of translating target word u to source
word v. if word-to-word alignments were annotated, these probabilities could be com-
puted from relative frequencies,

    u   v =

count(u, v)
count(u)

,

[18.17]

jacob eisenstein. draft of november 13, 2018.

18.2. id151

439

where count(u, v) is the count of instances in which word v was aligned to word u in
the training set, and count(u) is the total count of the target word u. the smoothing
techniques mentioned in chapter 6 can help to reduce the variance of these id203
estimates.

conversely, if we had an accurate translation model, we could estimate the likelihood

of each alignment decision,

qm(am | w(s), w(t))     p(am | m, m (s), m (t))    p(w(s)

m | w(t)
am),

[18.18]

where qm(am | w(s), w(t)) is a measure of our con   dence in aligning source word w(s)
m
to target word w(t)
am. the relative frequencies could then be computed from the expected
counts,

eq [count(u, v)]

    u   v =

count(u)
qm(am | w(s), w(t))      (w(s)

m = v)      (w(t)

am = u).

[18.19]

[18.20]

eq [count(u, v)] =(cid:88)m

the id83 proceeds by iteratively updating qm
and     . the algorithm is described in general form in chapter 5. for statistical machine
translation, the steps of the algorithm are:

1. e-step: update beliefs about word alignment using equation 18.18.

2. m-step: update the translation model using equations 18.19 and 18.20.

as discussed in chapter 5, the expectation maximization algorithm is guaranteed to con-
verge, but not to a global optimum. however, for ibm model 1, it can be shown that em
optimizes a convex objective, and global optimality is guaranteed. for this reason, ibm
model 1 is often used as an initialization for more complex alignment models. for more
detail, see koehn (2009).

18.2.3 phrase-based translation

real translations are not word-to-word substitutions. one reason is that many multiword
expressions are not translated literally, as shown in this example from french:

(18.3) nous

allons
will

prendre
take

un
a

verre
glass

we
we   ll have a drink

under contract with mit press, shared under cc-by-nc-nd license.

440

chapter 18. machine translation

figure 18.4: a phrase-based alignment between french and english, corresponding to
example (18.3)

the line we will take a glass is the word-for-word gloss of the french sentence; the transla-
tion we   ll have a drink is shown on the third line. such examples are dif   cult for word-to-
word translation models, since they require translating prendre to have and verre to drink.
these translations are only correct in the context of these speci   c phrases.

phrase-based translation generalizes on word-based models by building translation
tables and alignments between multiword spans. (these    phrases    are not necessarily
syntactic constituents like the noun phrases and verb phrases described in chapters 9 and
10.) the generalization from word-based translation is surprisingly straightforward: the
translation tables can now condition on multi-word units, and can assign probabilities to
multi-word units; alignments are mappings from spans to spans, ((i, j), (k, (cid:96))), so that

p(w(s) | w(t),a) = (cid:89)((i,j),(k,(cid:96)))   a

pw(s)|w(t)({w(s)

i+1, w(s)

i+2, . . . , w(s)

j } | {w(t)

k+1, w(t)

k+2, . . . , w(t)

(cid:96) }).
[18.21]

the phrase alignment ((i, j), (k, (cid:96))) indicates that the span w(s)
i+1:j is the translation of the
span w(t)
k+1:(cid:96). an example phrasal alignment is shown in figure 18.4. note that the align-
ment set a is required to cover all of the tokens in the source, just as in word-based trans-
lation. the id203 model pw(s)|w(t) must now include translations for all phrase pairs,
which can be learned from expectation-maximization just as in word-based statistical ma-
chine translation.

jacob eisenstein. draft of november 13, 2018.

nousallonsprendreuneverrewe   llhaveadrink18.2. id151

441

*syntax-based translation

18.2.4
the vauquois pyramid (figure 18.1) suggests that translation might be easier if we take a
higher-level view. one possibility is to incorporate the syntactic structure of the source,
the target, or both. this is particularly promising for language pairs that consistent syn-
tactic differences. for example, english adjectives almost always precede the nouns that
they modify, while in romance languages such as french and spanish, the adjective often
follows the noun: thus, angry    sh would translate to pez (   sh) enojado (angry) in spanish.
in word-to-word translation, these reorderings cause the alignment model to be overly
permissive. it is not that the order of any pair of english words can be reversed when
translating into spanish, but only adjectives and nouns within a noun phrase. similar
issues arise when translating between verb-   nal languages such as japanese (in which
verbs usually follow the subject and object), verb-initial languages like tagalog and clas-
sical arabic, and verb-medial languages such as english.

an elegant solution is to link parsing and translation in a synchronous context-free
grammar (sid18; chiang, 2007).3 an sid18 is a set of productions of the form x     (  ,   ,   ),
where x is a non-terminal,    and    are sequences of terminals or non-terminals, and    
is a one-to-one alignment of items in    with items in   . english-spanish adjective-noun
ordering can be handled by a set of synchronous productions, e.g.,

np     (det1 nn2 jj3, det1 jj3 nn2),

[18.22]

with subscripts indicating the alignment between the spanish (left) and english (right)
parts of the right-hand side. terminal productions yield translation pairs,

jj     (enojado1,

angry1).

[18.23]

a synchronous derivation begins with the start symbol s, and derives a pair of sequences
of terminal symbols.

given an sid18 in which each production yields at most two symbols in each lan-
guage (chomsky normal form; see    9.2.1), a sentence can be parsed using only the cky
algorithm (chapter 10). the resulting derivation also includes productions in the other
language, all the way down to the surface form. therefore, sid18s make translation very
similar to parsing. in a weighted sid18, the log id203 log ps|t can be computed from
the sum of the log-probabilities of the productions. however, combining sid18s with a
target language model is computationally expensive, necessitating approximate search
algorithms (huang and chiang, 2007).

synchronous context-free grammars are an example of tree-to-tree translation, be-
cause they model the syntactic structure of both the target and source language. in string-
to-tree translation, string elements are translated into constituent tree fragments, which
3earlier approaches to syntactic machine translation includes syntax-driven transduction (lewis ii and

stearns, 1968) and stochastic inversion transduction grammars (wu, 1997).

under contract with mit press, shared under cc-by-nc-nd license.

442

chapter 18. machine translation

are then assembled into a translation (yamada and knight, 2001; galley et al., 2004); in
tree-to-string translation, the source side is parsed, and then transformed into a string on
the target side (liu et al., 2006). a key question for syntax-based translation is the extent
to which we phrasal constituents align across translations (fox, 2002), because this gov-
erns the extent to which we can rely on monolingual parsers and treebanks. for more on
syntax-based machine translation, see the monograph by williams et al. (2016).

18.3 id4

neural network models for machine translation are based on the encoder-decoder archi-
tecture (cho et al., 2014). the encoder network converts the source language sentence into
a vector or matrix representation; the decoder network then converts the encoding into a
sentence in the target language.

z =encode(w(s))

w(t) | w(s)    decode(z),

[18.24]
[18.25]

where the second line means that the function decode(z) de   nes the conditional proba-
bility p(w(t) | w(s)).

the decoder is typically a recurrent neural network, which generates the target lan-
guage sentence one word at a time, while recurrently updating a hidden state. the en-
coder and decoder networks are trained end-to-end from parallel sentences. if the output
layer of the decoder is a logistic function, then the entire architecture can be trained to
maximize the conditional log-likelihood,

p(w(t)

m (t)(cid:88)m=1
1:m   1, w(s))     exp(cid:16)  

log p(w(t) | w(s)) =
m | w(t)
m   1 is a recurrent function of the previously generated text

m | w(t)
m   1(cid:17)
m    h(t)

1:m   1, z)

[18.27]

[18.26]

w(t)

p(w(t)

where the hidden state h(t)
w(t)

1:m   1 and the encoding z. the second line is equivalent to writing,

[18.28]
where        r(v (t)  k) is the matrix of output word vectors for the v (t) words in the target
language vocabulary.

1:m   1, w(s)     softmax(cid:16)      h(t)

m   1(cid:17) ,

m | w(t)
w(t)

the simplest encoder-decoder architecture is the sequence-to-sequence model (sutskever

et al., 2014). in this model, the encoder is set to the    nal hidden state of a long short-term

jacob eisenstein. draft of november 13, 2018.

18.3. id4

443

figure 18.5: a deep bidirectional lstm encoder

memory (lstm) (see    6.3.3) on the source sentence:
m , h(s)

m   1)

m =lstm(x(s)
h(s)
z (cid:44)h(s)

m (s),

[18.29]
[18.30]

where x(s)
the initial hidden state for the decoder lstm:

m is the embedding of source language word w(s)

m . the encoding then provides

h(t)
0 =z
h(t)
m =lstm(x(t)

m , h(t)

m   1),

[18.31]
[18.32]

where x(t)

m is the embedding of the target language word w(t)
m .

sequence-to-sequence translation is nothing more than wiring together two lstms:
one to read the source, and another to generate the target. to make the model work well,
some additional tweaks are needed:

    most notably, the model works much better if the source sentence is reversed, read-
ing from the end of the sentence back to the beginning. in this way, the words at the
beginning of the source have the greatest impact on the encoding z, and therefore
impact the words at the beginning of the target sentence. later work on more ad-
vanced encoding models, such as neural attention (see    18.3.1), has eliminated the
need for reversing the source sentence.
    the encoder and decoder can be implemented as deep lstms, with multiple layers
m at layer i is treated

of hidden states. as shown in figure 18.5, each hidden state h(s,i)

under contract with mit press, shared under cc-by-nc-nd license.

h(s,d)m   1h(s,d)mh(s,d)m+1.........h(s,2)m   1h(s,2)mh(s,2)m+1h(s,1)m   1h(s,1)mh(s,1)m+1x(s)m   1x(s)mx(s)m+1444

chapter 18. machine translation

as the input to an lstm at layer i + 1:

h(s,1)
m =lstm(x(s)
=lstm(h(s,i)
h(s,i+1)

m , h(s)
m , h(s,i+1)

m   1)
m   1

m

),

   i     1.

[18.33]
[18.34]

the original work on sequence-to-sequence translation used four layers; in 2016,
google   s commercial machine translation system used eight layers (wu et al., 2016).4
    signi   cant improvements can be obtained by creating an ensemble of translation
models, each trained from a different random initialization. for an ensemble of size
n, the per-token decoding id203 is set equal to,

p(w(t) | z, w(t)

1:m   1) =

1
n

n(cid:88)i=1

pi(w(t) | z, w(t)

1:m   1),

[18.35]

where pi is the decoding id203 for model i. each translation model in the
ensemble includes its own encoder and decoder networks.

    the original sequence-to-sequence model used a fairly standard training setup: stochas-

tic id119 with an exponentially decreasing learning rate after the    rst    ve
epochs; mini-batches of 128 sentences, chosen to have similar length so that each
sentence on the batch will take roughly the same amount of time to process; gradi-
ent clipping (see    3.3.4) to ensure that the norm of the gradient never exceeds some
prede   ned value.

18.3.1 neural attention
the sequence-to-sequence model discussed in the previous section was a radical depar-
ture from id151, in which each word or phrase in the target lan-
guage is conditioned on a single word or phrase in the source language. both approaches
have advantages. statistical translation leverages the idea of compositionality     transla-
tions of large units should be based on the translations of their component parts     and
this seems crucial if we are to scale translation to longer units of text. but the translation
of each word or phrase often depends on the larger context, and encoder-decoder models
capture this context at the sentence level.

is it possible for translation to be both contextualized and compositional? one ap-
proach is to augment neural translation with an attention mechanism. the idea of neural
attention was described in    17.5, but its application to translation bears further discus-
sion. in general, attention can be thought of as using a query to select from a memory
of key-value pairs. however, the query, keys, and values are all vectors, and the entire

4google reports that this system took six days to train for english-french translation, using 96 nvidia

k80 gpus, which would have cost roughly half a million dollars at the time.

jacob eisenstein. draft of november 13, 2018.

18.3. id4

445

figure 18.6: a general view of neural attention. the dotted box indicates that each   m   n
can be viewed as a gate on value n.

operation is differentiable. for each key n in the memory, we compute a score     (m, n)
with respect to the query m. that score is a function of the compatibility of the key and
the query, and can be computed using a small feedforward neural network. the vector
of scores is passed through an activation function, such as softmax. the output of this
activation function is a vector of non-negative numbers [  m   1,   m   2, . . . ,   m   n ](cid:62), with
length n equal to the size of the memory. each value in the memory vn is multiplied by
the attention   m   n; the sum of these scaled values is the output. this process is shown in
figure 18.6. in the extreme case that   m   n = 1 and   m   n(cid:48) = 0 for all other n(cid:48), then the
attention mechanism simply selects the value vn from the memory.

neural attention makes it possible to integrate alignment into the encoder-decoder ar-
chitecture. rather than encoding the entire source sentence into a    xed length vector z,
it can be encoded into a matrix z     rk  m (s), where k is the dimension of the hidden
state, and m (s) is the number of tokens in the source input. each column of z represents
the state of a recurrent neural network over the source sentence. these vectors are con-
structed from a bidirectional lstm (see    7.6), which can be a deep network as shown in
figure 18.5. these columns are both the keys and the values in the attention mechanism.
at each step m in decoding, the attentional state is computed by executing a query,

which is equal to the state of the decoder, h(t)

m . the resulting compatibility scores are,

    (m, n) =v      tanh(    [h(t)

m ; h(s)

n ]).

[18.36]

the function    is thus a two layer feedforward neural network, with weights v   on the
output layer, and weights      on the input layer. to convert these scores into atten-
tion weights, we apply an activation function, which can be vector-wise softmax or an
element-wise sigmoid:

softmax attention

  m   n =

exp     (m, n)
(cid:80)m (s)
n(cid:48)=1 exp     (m, n(cid:48))

[18.37]

under contract with mit press, shared under cc-by-nc-nd license.

outputactivation  query    keyvalue446

chapter 18. machine translation

sigmoid attention

  m   n =    (    (m, n))

[18.38]

the attention    is then used to compute a context vector cm by taking a weighted

average over the columns of z,

cm =

m (s)(cid:88)n=1

  m   nzn,

[18.39]

where   m   n     [0, 1] is the amount of attention from word m of the target to word n of the
source. the context vector can be incorporated into the decoder   s word output id203
model, by adding another layer to the decoder (luong et al., 2015):

  h(t)

m ; cm](cid:17)
m = tanh(cid:16)  c[h(t)
m(cid:19) .
m+1      h(t)

1:m, w(s))     exp(cid:18)  

w(t)

p(w(t)

m+1 | w(t)

[18.40]

[18.41]

here the decoder state h(t)
to compute a    nal output vector   h(t)
decoder recurrence in a similar manner (bahdanau et al., 2014).

m is concatenated with the context vector, forming the input
m . the context vector can be incorporated into the

*id4 without recurrence

18.3.2
in the encoder-decoder model, attention   s    keys and values    are the hidden state repre-
sentations in the encoder network, z, and the    queries    are state representations in the
decoder network h(t). it is also possible to completely eliminate recurrence from neural
translation, by applying self-attention (lin et al., 2017; kim et al., 2017) within the en-
coder and decoder, as in the transformer architecture (vaswani et al., 2017). for level i,
the basic equations of the encoder side of the transformer are:

z(i)
m =

m (s)(cid:88)n=1

m   n(  vh(i   1)
  (i)
m =  2 relu(cid:16)  1z(i)

n

h(i)

m + b1(cid:17) + b2.

)

[18.42]

[18.43]

for each token m at level i, we compute self-attention over the entire source sentence.
the keys, values, and queries are all projections of the vector h(i   1): for example, in equa-
tion 18.42, the value vn is the projection   vh(i   1)
m   n are com-
n
puted using a scaled form of softmax attention,

. the attention scores   (i)

  m   n     exp(    (m, n)/m ),

[18.44]

jacob eisenstein. draft of november 13, 2018.

18.3. id4

447

m from h(i   1). the key, value,
figure 18.7: the transformer encoder   s computation of z(i)
and query are shown for token m     1. for example,   (i)
   (m, m     1) is computed from
the key   kh(i   1)
m , and the gate   (i)
m   m   1 operates on the value
  vh(i   1)
m   1 . the    gure shows a minimal version of the architecture, with a single atten-
tion head. with multiple heads, it is possible to attend to different properties of multiple
words.

m   1 and the query   qh(i   1)

where m is the length of the input. this encourages the attention to be more evenly
dispersed across the input. self-attention is applied across multiple    heads   , each using
different projections of h(i   1) to form the keys, values, and queries. this architecture is
shown in figure 18.7. the output of the self-attentional layer is the representation z(i)
m ,
which is then passed through a two-layer feed-forward network, yielding the input to the
next layer, h(i). this self-attentional architecture can be applied in the decoder as well,
but this requires that there is zero attention to future words:   m   n = 0 for all n > m.

to ensure that information about word order in the source is integrated into the model,
the encoder augments the base layer of the network with positional encodings of the
indices of each word in the source. these encodings are vectors for each position m    
{1, 2, . . . , m}. the transformer sets these encodings equal to a set of sinusoidal functions
of m,

e2i   1(m) = sin(m/(10000

e2i(m) = cos(m/(10000

2i
ke ))
2i
ke )),

   i     {1, 2, . . . , ke/2}

[18.45]

[18.46]

where e2i(m) is the value at element 2i of the encoding for index m. as we progress
through the encoding, the sinusoidal functions have progressively narrower bandwidths.
this enables the model to learn to attend by relative positions of words. the positional
encodings are concatenated with the id27s xm at the base layer of the model.5
5the transformer architecture relies on several additional tricks, including layer id172 (see

under contract with mit press, shared under cc-by-nc-nd license.

z(i)  (i)m     (i)  (m,  )h(i   1)m   1mm+1kqv448

chapter 18. machine translation

figure 18.8: translation with unknown words. the system outputs unk to indicate words
that are outside its vocabulary. figure adapted from luong et al. (2015).

convolutional neural networks (see    3.4) have also been applied as encoders in neu-
ral machine translation (gehring et al., 2017). for each word w(s)
m , a convolutional network
computes a representation h(s)
m from the embeddings of the word and its neighbors. this
procedure is applied several times, creating a deep convolutional network. the recurrent
decoder then computes a set of attention weights over these convolutional representa-
tions, using the decoder   s hidden state h(t) as the queries. this attention vector is used
to compute a weighted average over the outputs of another convolutional neural network
of the source, yielding an averaged representation cm, which is then fed into the decoder.
as with the transformer, speed is the main advantage over recurrent encoding models;
another similarity is that word order information is approximated through the use of po-
sitional encodings.6

18.3.3 out-of-vocabulary words
thus far, we have treated translation as a problem at the level of words or phrases. for
words that do not appear in the training data, all such models will struggle. there are
two main reasons for the presence of out-of-vocabulary (oov) words:

    new proper nouns, such as family names or organizations, are constantly arising    
particularly in the news domain. the same is true, to a lesser extent, for technical
terminology. this issue is shown in figure 18.8.

    in many languages, words have complex internal structure, known as morphology.
an example is german, which uses compounding to form nouns like abwasserbe-
handlungsanlage (sewage water treatment plant; example from sennrich et al. (2016)).
   3.3.4), residual connections around the nonlinear activations (see    3.2.2), and a non-monotonic learning
rate schedule.

6a recent evaluation found that best performance was obtained by using a recurrent network for the
decoder, and a transformer for the encoder (chen et al., 2018). the transformer was also found to signi   cantly
outperform a convolutional neural network.

jacob eisenstein. draft of november 13, 2018.

source:theecotaxporticoinpont-de-buiswastakendownonthursdaymorningreference:leportique  ecotaxedepont-de-buisa  et  ed  emont  ejeudimatinsystem:leunkdeunk`aunka  et  eprislejeudimatin18.4. decoding

449

while compounds could in principle be addressed by better id121 (see    8.4),
other morphological processes involve more complex transformations of subword
units.

names and technical terms can be handled in a postprocessing step: after    rst identi-
fying alignments between unknown words in the source and target, we can look up each
aligned source word in a dictionary, and choose a replacement (luong et al., 2015). if the
word does not appear in the dictionary, it is likely to be a proper noun, and can be copied
directly from the source to the target. this approach can also be integrated directly into
the translation model, rather than applying it as a postprocessing step (jean et al., 2015).
words with complex internal structure can be handled by translating subword units
rather than entire words. a popular technique for identifying subword units is byte-pair
encoding (bpe; gage, 1994; sennrich et al., 2016). the initial vocabulary is de   ned as the
set of characters used in the text. the most common character bigram is then merged into
a new symbol, the vocabulary is updated, and the merging operation is applied again. for
example, given the dictionary {   sh,    shed, want, wanted, bike, biked}, we would    rst form
the subword unit ed, since this character bigram appears in three of the six words. next,
there are several bigrams that each appear in a pair of words:    , is, sh, wa, an, etc. these can
be merged in any order. by iterating this process, we eventually reach the segmentation,
{   sh,    sh+ed, want, want+ed, bik+e, bik+ed}. at this point, there are no bigrams that appear
more than once. in real data, merging is performed until the number of subword units
reaches some prede   ned threshold, such as 104.

each subword unit is treated as a token for translation, in both the encoder (source
side) and decoder (target side). bpe can be applied jointly to the union of the source and
target vocabularies, identifying subword units that appear in both languages. for lan-
guages that have different scripts, such as english and russian, id68 between
the scripts should be applied    rst.7

18.4 decoding

given a trained translation model, the decoding task is:

  w(t) = argmax
w   v   

  (w, w(s)),

[18.47]

it is not possible to
where w(t) is a sequence of tokens from the target vocabulary v.
ef   ciently obtain exact solutions to the decoding problem, for even minimally effective

7id68 is crucial for converting names and other foreign words between languages that do not
share a single script, such as english and japanese. it is typically approached using the    nite-state methods
discussed in chapter 9 (knight and graehl, 1998).

under contract with mit press, shared under cc-by-nc-nd license.

450

chapter 18. machine translation

models in either statistical or id4. today   s state-of-the-art transla-
tion systems use id125 (see    11.3.1), which is an incremental decoding algorithm
that maintains a small constant number of competitive hypotheses. such greedy approxi-
mations are reasonably effective in practice, and this may be in part because the decoding
objective is only loosely correlated with measures of translation quality, so that exact op-
timization of [18.47] may not greatly improve the resulting translations.

decoding in id4 is simpler than in phrase-based statistical ma-

chine translation.8 the scoring function    is de   ned,

m (t)(cid:88)m=1

  (w(t), w(s)) =

  (w(t)

m ; w(t)

1:m   1, z)

  (w(t); w(t)

1:m   1, z) =  

m    h(t)
w(t)

m     log(cid:88)w   v

exp(cid:16)  w    h(t)
m(cid:17) ,

[18.48]

[18.49]

where z is the encoding of the source sentence w(s), and h(t)
z and the decoding history w(t)
model, where z is a matrix encoding of the source.

m is a function of the encoding
1:m   1. this formulation subsumes the attentional translation

now consider the incremental decoding algorithm,

  w(t)

m = argmax

w   v   (w;   w(t)

1:m   1, z), m = 1, 2, . . .

[18.50]

1:m   1.

this algorithm selects the best target language word at position m, assuming that it has
already generated the sequence   w(t)
(termination can be handled by augmenting
the vocabulary v with a special end-of-sequence token, (cid:4).) the incremental algorithm
is likely to produce a suboptimal solution to the optimization problem de   ned in equa-
tion 18.47, because selecting the highest-scoring word at position m can set the decoder
on a    garden path,    in which there are no good choices at some later position n > m. we
might hope for some id145 solution, as in sequence labeling (   7.3). but
the viterbi algorithm and its relatives rely on a markov decomposition of the objective
function into a sum of local scores: for example, scores can consider locally adjacent tags
(ym, ym   1), but not the entire tagging history y1:m. this decomposition is not applicable
to recurrent neural networks, because the hidden state h(t)
m is impacted by the entire his-
tory w(t)
1:m; this sensitivity to long-range context is precisely what makes recurrent neural
networks so effective.9 in fact, it can be shown that decoding from any recurrent neural
network is np-complete (siegelmann and sontag, 1995; chen et al., 2018).

8for more on decoding in phrase-based statistical models, see koehn (2009).
9note that this problem does not impact id56-based sequence labeling models (see    7.6). this is because

the tags produced by these models do not affect the recurrent state.

jacob eisenstein. draft of november 13, 2018.

18.5. training towards the evaluation metric

451

id125 id125 is a general technique for avoiding search errors when ex-
haustive search is impossible; it was    rst discussed in    11.3.1. id125 can be seen
as a variant of the incremental decoding algorithm sketched in equation 18.50, but at
each step m, a set of k different hypotheses are kept on the beam. for each hypothesis
k     {1, 2, . . . , k}, we compute both the current score(cid:80)m (t)
k,1:m   1, z) as well as
the current hidden state h(t)
k . at each step in the id125, the k top-scoring children
of each hypothesis currently on the beam are    expanded   , and the beam is updated. for
a detailed description of id125 for id56 decoding, see graves (2012).

m=1   (w(t)

k,m; w(t)

learning and search conventionally, the learning algorithm is trained to predict the
right token in the translation, conditioned on the translation history being correct. but
if decoding must be approximate, then we might do better by modifying the learning
algorithm to be robust to errors in the translation history. scheduled sampling does this
by training on histories that sometimes come from the ground truth, and sometimes come
from the model   s own output (bengio et al., 2015).10 as training proceeds, the training
wheels come off: we increase the fraction of tokens that come from the model rather than
the ground truth. another approach is to train on an objective that relates directly to beam
search performance (wiseman et al., 2016). id23 has also been applied
to decoding of id56-based translation models, making it possible to directly optimize
translation metrics such as id7 (ranzato et al., 2016).

18.5 training towards the evaluation metric

in likelihood-based training, the objective is the maximize the id203 of a parallel
corpus. however, translations are not evaluated in terms of likelihood: metrics like id7
consider only the correctness of a single output translation, and not the range of prob-
abilities that the model assigns. it might therefore be better to train translation models
to achieve the highest id7 score possible     to the extent that we believe id7 mea-
sures translation quality. unfortunately, id7 and related metrics are not friendly for
optimization: they are discontinuous, non-differentiable functions of the parameters of
the translation model.

consider an error function    (   w(t), w(t)), which measures the discrepancy between the
system translation   w(t) and the reference translation w(t); this function could be based on
id7 or any other metric on translation quality. one possible criterion would be to select

10scheduled sampling builds on earlier work on learning to search (daum  e iii et al., 2009; ross et al.,

2011), which are also described in    15.2.4.

under contract with mit press, shared under cc-by-nc-nd license.

452

chapter 18. machine translation

the parameters    that minimize the error of the system   s preferred translation,

  w(t) = argmax

  (w(t), w(s);   )

w(t)

     = argmin

   (   w(t), w(s))

  

[18.51]

[18.52]

however, identifying the top-scoring translation   w(t) is usually intractable, as described
in the previous section. in minimum error-rate training (mert),   w(t) is selected from a
set of candidate translations y(w(s)); this is typically a strict subset of all possible transla-
tions, so that it is only possible to optimize an approximation to the true error rate (och
and ney, 2003).

a further issue is that the objective function in equation 18.52 is discontinuous and
non-differentiable, due to the argmax over translations: an in   nitesimal change in the
parameters    could cause another translation to be selected, with a completely different
error. to address this issue, we can instead minimize the risk, which is de   ned as the
expected error rate,

r(  ) =e   w(t)|w(s);  [   (   w(t), w(t))]

= (cid:88)  w(t)   y(w(s))

p(   w(t) | w(s))       (   w(t), w(t)).

[18.53]

[18.54]

minimum risk training minimizes the sum of r(  ) across all instances in the training set.

the risk can be generalized by exponentiating the translation probabilities,

  p(w(t);   ,   )    (cid:16)p(w(t) | w(s);   )(cid:17)  

  r(  ) = (cid:88)  w(t)   y(w(s))

  p(   w(t) | w(s);   ,   )       (   w(t), w(t))

[18.55]

[18.56]

where y(w(s)) is now the set of all possible translations for w(s). exponentiating the prob-
abilities in this way is known as annealing (smith and eisner, 2006). when    = 1, then
  r(  ) = r(  ); when    =    , then   r(  ) is equivalent to the sum of the errors of the maxi-
mum id203 translations for each sentence in the dataset.

clearly the set of candidate translations y(w(s)) is too large to explicitly sum over.
because the error function     generally does not decompose into smaller parts, there is
no ef   cient id145 solution to sum over this set. we can approximate
the sum(cid:80)   w(t)   y(w(s)) with a sum over a    nite number of samples, {w(t)
2 , . . . , w(t)
k }.
if these samples were drawn uniformly at random, then the (annealed) risk would be

1 , w(t)

jacob eisenstein. draft of november 13, 2018.

18.5. training towards the evaluation metric

453

approximated as (shen et al., 2016),

  p(w(t)
k

| w(s);   ,   )       (w(t)

k , w(t))

  r(  )    

z =

1
z

k(cid:88)k=1
k(cid:88)k=1

  p(w(t)
k

| w(s);   ,   ).

[18.57]

[18.58]

shen et al. (2016) report that performance plateaus at k = 100 for minimum risk training
of id4.

uniform sampling over the set of all possible translations is undesirable, because most
translations have very low id203. a solution from monte carlo estimation is impor-
tance sampling, in which we draw samples from a proposal distribution q(w(s)). this
distribution can be set equal to the current translation model p(w(t) | w(s);   ). each sam-
ple is then weighted by an importance score,   k =
. the effect of this weighting
is to correct for any mismatch between the proposal distribution q and the true distribu-
tion   p. the risk can then be approximated as,

k |w(s))
k ;w(s))

  p(w(t)
q(w(t)

w(t)
k    q(w(s))
  p(w(t)
| w(s))
k
  k =
q(w(t)
k ; w(s))
1
k=1   k

  r(  )    

k(cid:88)k=1

(cid:80)k

  k       (w(t)

k , w(t)).

[18.59]

[18.60]

[18.61]

importance sampling will generally give a more accurate approximation than uniform
sampling. the only formal requirement is that the proposal assigns non-zero id203
to every w(t)     y(w(s)). for more on importance sampling and related methods, see
robert and casella (2013).

additional resources

a complete textbook on machine translation is available from koehn (2009). while this
book precedes recent work on neural translation, a more recent draft chapter on neural
translation models is also available (koehn, 2017). neubig (2017) provides a compre-
hensive tutorial on id4, starting from    rst principles. the course
notes from cho (2015) are also useful. several id4 libraries are
available: lamtram is an implementation of id4 in dynet (neu-
big et al., 2017); openid4 (klein et al., 2017) and fairseq are available in pytorch;

under contract with mit press, shared under cc-by-nc-nd license.

454

chapter 18. machine translation

tensor2tensor is an implementation of several of the google translation models in ten-
sorflow (abadi et al., 2016).

literary translation is especially challenging, even for expert human translators. mes-

sud (2014) describes some of these issues in her review of an english translation of l     etranger,
the 1942 french novel by albert camus.11 she compares the new translation by sandra
smith against earlier translations by stuart gilbert and matthew ward, focusing on the
dif   culties presented by a single word in the    rst sentence:

then, too, smith has reconsidered the book   s famous opening. camus   s
original is deceptively simple:    aujourd   hui, maman est morte.    gilbert in   u-
enced generations by offering us    mother died today      inscribing in meur-
sault [the narrator] from the outset a formality that could be construed as
heartlessness. but maman, after all, is intimate and affectionate, a child   s name
for his mother. matthew ward concluded that it was essentially untranslatable
(   mom    or    mummy    being not quite apt), and left it in the original french:
   maman died today.    there is a clear logic in this choice; but as smith has
explained, in an interview in the guardian, maman    didn   t really tell the reader
anything about the connotation.    she, instead, has translated the sentence as
   my mother died today.   

i chose    my mother    because i thought about how someone would
tell another person that his mother had died. meursault is speaking
to the reader directly.    my mother died today    seemed to me the
way it would work, and also implied the closeness of    maman    you
get in the french.

elsewhere in the book, she has translated maman as    mama        again, striving
to come as close as possible to an actual, colloquial word that will carry the
same connotations as maman does in french.

the passage is a reminder that while the quality of machine translation has improved
dramatically in recent years, expert human translations draw on considerations that are
beyond the ken of any contemporary computational approach.

exercises

1. using google translate or another online service, translate the following example

into two different languages of your choice:

11the book review is currently available online at http://www.nybooks.com/articles/2014/06/

05/camus-new-letranger/.

jacob eisenstein. draft of november 13, 2018.

18.5. training towards the evaluation metric

455

(18.4)

it is not down on any map; true places never are.

then translate each result back into english. which is closer to the original? can
you explain the differences?

2. compute the unsmoothed id165 precisions p1 . . . p4 for the two back-translations
in the previous problem, using the original source as the reference. your id165s
should include punctuation, and you should segment conjunctions like it   s into two
tokens.

3. you are given the following dataset of translations from    simple    to    dif   cult    en-

glish:

(18.5)

a. kids

children

like
adore

cats.
felines.

b. cats

felines

hats.
fedoras.

estimate a word-to-word statistical translation model from simple english (source)
to dif   cult english (target), using the expectation-maximization as described in    18.2.2.
compute two iterations of the algorithm by hand, starting from a uniform transla-
m (t) .
tion model, and using the simple alignment model p(am | m, m (s), m (t)) = 1
hint: in the    nal m-step, you will want to switch from fractions to decimals.

4. building on the previous problem, what will be the converged translation proba-
bility table? can you state a general condition about the data, under which this
translation model will fail in the way that it fails here?

5. propose a simple alignment model that would make it possible to recover the correct

translation probabilities from the toy dataset in the previous two problems.

6. let (cid:96)(t)

m+1 represent the loss at word m + 1 of the target, and let h(s)

n represent the hid-
den state at word n of the source. write the expression for the derivative    (cid:96)(t)
in the
m+1
   h(s)
n
sequence-to-sequence translation model expressed in equations [18.29-18.32]. you
may assume that both the encoder and decoder are one-layer lstms. in general,
how many terms are on the shortest id26 path from (cid:96)(t)

m+1 to h(s)
n ?

7. now consider the neural attentional model from    18.3.1, with sigmoid attention.
the derivative    (cid:96)(t)
is the sum of many paths through the computation graph;
identify the shortest such path. you may assume that the initial state of the decoder
recurrence h(t)
0

is not tied to the    nal state of the encoder recurrence h(s)

m+1
   zn

m (s).

under contract with mit press, shared under cc-by-nc-nd license.

456

chapter 18. machine translation

8. apply byte-pair encoding for the vocabulary it, unit, unite, until no bigram appears

more than once.

9. this problem relates to the complexity of machine translation. suppose you have
an oracle that returns the list of words to include in the translation, so that your
only task is to order the words. furthermore, suppose that the scoring function
m   1). show that the problem
of    nding the optimal translation is np-complete, by reduction from a well-known
problem.

over orderings is a sum over bigrams,(cid:80)m

m=1   (w(t)

m , w(t)

10. hand-design an attentional recurrent translation model that simply copies the input
from the source to the target. you may assume an arbitrarily large hidden state, and
you may assume that there is a    nite maximum input length m. specify all the
weights such that the maximum id203 translation of any source is the source
itself. hint: it is simplest to use the elman recurrence hm = f (  hm   1 + xm) rather
than an lstm.

11. give a synchronized derivation (   18.2.4) for the spanish-english translation,

(18.6) el

pez
   sh

enojado
angry

atacado.
the
attacked.
the angry    sh attacked.

as above, the second line shows a word-for-word gloss, and the third line shows
the desired translation. use the synchronized production rule in [18.22], and design
the other production rules necessary to derive this sentence pair. you may derive
(atacado, attacked) directly from vp.

jacob eisenstein. draft of november 13, 2018.

chapter 19

text generation

in many of the most interesting problems in natural language processing, language is
the output. the previous chapter described the speci   c case of machine translation, but
there are many other applications, from summarization of research articles, to automated
journalism, to dialogue systems. this chapter emphasizes three main scenarios: data-to-
text, in which text is generated to explain or describe a structured record or unstructured
perceptual input; text-to-text, which typically involves fusing information from multiple
linguistic sources into a single coherent summary; and dialogue, in which text is generated
as part of an interactive conversation with one or more human participants.

19.1 data-to-text generation

in data-to-text generation, the input ranges from structured records, such as the descrip-
tion of an weather forecast (as shown in figure 19.1), to unstructured perceptual data,
such as a raw image or video; the output may be a single sentence, such as an image cap-
tion, or a multi-paragraph argument. despite this diversity of conditions, all data-to-text
systems share some of the same challenges (reiter and dale, 2000):

    determining what parts of the data to describe;
    planning a presentation of this information;
    lexicalizing the data into words and phrases;
    organizing words and phrases into well-formed sentences and paragraphs.

the earlier stages of this process are sometimes called content selection and text plan-
ning; the later stages are often called surface realization.

early systems for data-to-text generation were modular, with separate software com-
ponents for each task. arti   cial intelligence planning algorithms can be applied to both

457

458

chapter 19. text generation

cloudy, with temperatures between 10 and 20 degrees. south wind around 20 mph.

figure 19.1: an example input-output pair for the task of generating text descriptions of
weather forecasts (adapted from konstas and lapata, 2013).

the high-level information structure and the organization of individual sentences, ensur-
ing that communicative goals are met (mckeown, 1992; moore and paris, 1993). surface
realization can be performed by grammars or templates, which link speci   c types of data
to candidate words and phrases. a simple example template is offered by wiseman et al.
(2017), for generating descriptions of basketball games:

(19.1) the <team1> (<wins1>-losses1) defeated the <team2> (<wins2>-<losses2>),

<pts1>-<pts2>.
the new york knicks (45-5) defeated the boston celtics (11-38), 115-79.

for more complex cases, it may be necessary to apply morphological in   ections such as
pluralization and tense marking     even in the simple example above, languages such
as russian would require case marking suf   xes for the team names. such in   ections can
be applied as a postprocessing step. another dif   cult challenge for surface realization is
the generation of varied referring expressions (e.g., the knicks, new york, they), which is
critical to avoid repetition. as discussed in    16.2.1, the form of referring expressions is
constrained by the discourse and information structure.

an example at the intersection of rule-based and statistical techniques is the nitro-
gen system (langkilde and knight, 1998). the input to nitrogen is an abstract meaning
representation (amr; see    13.3) of semantic content to be expressed in a single sentence.
in data-to-text scenarios, the id15 is the output of a higher-
level text planning stage. a set of rules then converts the id15
into various sentence plans, which may differ in both the high-level structure (e.g., active
versus passive voice) as well as the low-level details (e.g., word and phrase choice). some
examples are shown in figure 19.2. to control the combinatorial explosion in the number
of possible realizations for any given meaning, the sentence plans are uni   ed into a single
   nite-state acceptor, in which word tokens are represented by arcs (see    9.1.1). a bigram
jacob eisenstein. draft of november 13, 2018.

temperaturetimeminmeanmax06:00-21:0091521cloudskycovertimepercent(%)06:00-09:0025-5009:00-12:0050-75windspeedtimeminmeanmax06:00-21:00152030winddirectiontimemode06:00-21:00s19.1. data-to-text generation

459

(a / admire-01

:arg0 (v / visitor

:arg1-of (c / arrive-01

:arg4 (j / japan)))

:arg1 (m / "mount fuji"))

fuji.

    visitors who came to japan admire mount
    visitors who came in japan admire mount
    mount fuji is admired by the visitor who

fuji.

came in japan.

figure 19.2: id15 and candidate surface realizations from the
nitrogen system. example adapted from langkilde and knight (1998).

language model is then used to compute weights on the arcs, so that the shortest path is
also the surface realization with the highest bigram language model id203.

more recent systems are uni   ed models that are trained end-to-end using backpropa-
gation. data-to-text generation shares many properties with machine translation, includ-
ing a problem of alignment: labeled examples provide the data and the text, but they do
not specify which parts of the text correspond to which parts of the data. for example, to
learn from figure 19.1, the system must align the word cloudy to records in cloud sky
cover, the phrases 10 and 20 degrees to the min and max    elds in temperature, and
so on. as in machine translation, both latent variables and neural attention have been
proposed as solutions.

19.1.1 latent data-to-text alignment
given a dataset of texts and associated records {(w(i), y(i))}n
model   , so that

i=1, our goal is to learn a

  w = argmax
w   v   

  (w, y;   ),

[19.1]

    is the set of strings over a discrete vocabulary, and    is a vector of parameters.
where v
the relationship between w and y is complex: the data y may contain dozens of records,
and w may extend to several sentences. to facilitate learning and id136, it would be
helpful to decompose the scoring function    into subcomponents. this would be possi-
ble if given an alignment, specifying which element of y is expressed in each part of w.
speci   cally, let zm indicates the record aligned to word m. for example, in figure 19.1, z1
might specify that the word cloudy is aligned to the record cloud-sky-cover:percent.
the score for this alignment would then be given by the weight on features such as

(cloudy, cloud-sky-cover:percent).

[19.2]

in general, given an observed set of alignments, the score for a generation can be

under contract with mit press, shared under cc-by-nc-nd license.

460

chapter 19. text generation

written as sum of local scores (angeli et al., 2010):

  (w, y;   ) =

m(cid:88)m=1

  w,y(wm, yzm) +   w(wm, wm   1) +   z(zm, zm   1),

[19.3]

where   w can represent a bigram language model, and   z can be tuned to reward coher-
ence, such as the use of related records in nearby words. 1 the parameters of this model
could be learned from labeled data {(w(i), y(i), z(i))}n
i=1. however, while several datasets
include structured records and natural language text (barzilay and mckeown, 2005; chen
and mooney, 2008; liang and klein, 2009), the alignments between text and records are
usually not available.2 one solution is to model the problem probabilistically, treating the
alignment as a latent variable (liang et al., 2009; konstas and lapata, 2013). the model
can then be estimated using expectation maximization or sampling (see chapter 5).

19.1.2 neural data-to-text generation
the encoder-decoder model and neural attention were introduced in    18.3 as methods
for id4. they can also be applied to data-to-text generation, with
the data acting as the source language (mei et al., 2016). in id4,
the attention mechanism linked words in the source to words in the target; in data-to-
text generation, the attention mechanism can link each part of the generated text back
to a record in the data. the biggest departure from translation is in the encoder, which
depends on the form of the data.

data encoders

in some types of structured records, all values are drawn from discrete sets. for example,
the birthplace of an individual is drawn from a discrete set of possible locations; the diag-
nosis and treatment of a patient are drawn from an exhaustive list of clinical codes (john-
son et al., 2016). in such cases, vector embeddings can be estimated for each    eld and
possible value: for example, a vector embedding for the    eld birthplace, and another
embedding for the value berkeley california (bordes et al., 2011). the table of such
embeddings serves as the encoding of a structured record (he et al., 2017). it is also possi-
ble to compress the entire table into a single vector representation, by pooling across the
embeddings of each    eld and value (lebret et al., 2016).

1more expressive decompositions of    are possible. for example, wong and mooney (2007) use a syn-
chronous context-free grammar (see    18.2.4) to    translate    between a meaning representation and natural
language text.

2an exception is a dataset of records and summaries from american football games, containing annota-

tions of alignments between sentences and records (snyder and barzilay, 2007).

jacob eisenstein. draft of november 13, 2018.

19.1. data-to-text generation

461

figure 19.3: examples of the image captioning task, with attention masks shown for each
of the underlined words (xu et al., 2015).

sequences some types of structured records have a natural ordering, such as events in
a game (chen and mooney, 2008) and steps in a recipe (tutin and kittredge, 1992). for
example, the following records describe a sequence of events in a robot soccer match (mei
et al., 2016):

pass(arg1 = purple6, arg2 = purple3)
kick(arg1 = purple3)
badpass(arg1 = purple3, arg2 = pink9).

each event is a single record, and can be encoded by a concatenation of vector represen-
tations for the event type (e.g., pass), the    eld (e.g., arg1), and the values (e.g., purple3),
e.g.,

x =(cid:2)upass, uarg1, upurple6, uarg2, upurple3(cid:3) .

[19.4]
this encoding can then act as the input layer for a recurrent neural network, yielding a
r=1, where r indexes over records. interestingly,
sequence of vector representations {zr}r
this sequence-based approach can work even in cases where there is no natural ordering
over the records, such as the weather data in figure 19.1 (mei et al., 2016).

images another    avor of data-to-text generation is the generation of text captions for
images. examples from this task are shown in figure 19.3. images are naturally repre-
sented as tensors: a color image of 320    240 pixels would be stored as a tensor with
320    240    3 intensity values. the dominant approach to image classi   cation is to en-
code images as vectors using a combination of convolution and pooling (krizhevsky et al.,

under contract with mit press, shared under cc-by-nc-nd license.

462

chapter 19. text generation

figure 19.4: neural attention in text generation. figure adapted from mei et al. (2016).

2012). chapter 3 explains how to use convolutional networks for text; for images, convo-
lution is applied across the vertical, horizontal, and color dimensions. by pooling the re-
sults of successive convolutions, the image is converted to a vector representation, which
can then be fed directly into the decoder as the initial state (vinyals et al., 2015), just as
in the sequence-to-sequence translation model (see    18.3). alternatively, one can apply
a set of convolutional networks, yielding vector representations for different parts of the
image, which can then be combined using neural attention (xu et al., 2015).

attention
r=1 and a decoder state hm, an attention vector
given a set of embeddings of the data {zr}r
over the data can be computed using the same techniques as in machine translation (see
   18.3.1). when generating word m of the output, attention is computed over the records,
[19.5]
[19.6]

    (m, r) =        f (    [hm; zr])

  m =g ([    (m, 1),     (m, 2), . . . ,     (m, r)])

cm =

  m   rzr,

[19.7]

r(cid:88)r=1

where f is an elementwise nonlinearity such as tanh or relu, and g is a either softmax or
elementwise sigmoid. the weighted sum cm can then be included in the recurrent update
to the decoder state, or in the emission probabilities, as described in    18.3.1. figure 19.4
shows the attention to components of a weather record, while generating the text shown
on the x-axis.

adapting this architecture to image captioning is straightforward. a convolutional
neural networks is applied to a set of image locations, and the output at each location (cid:96) is
represented with a vector z(cid:96). attention can then be computed over the image locations,
as shown in the right panels of each pair of images in figure 19.3.

jacob eisenstein. draft of november 13, 2018.

a20%chanceofshowersandthunderstormsafternoon.mostlycloudywithahighnear71.id-0: temperature(min=52,max=71,mean=63)id-2: windspeed(min=8,mean=17,max=23)id-5: skycover(mode=50-75)id-10: precipchance(min=19,mean=32,max=73)id-15: thunderchance(mode=schc)19.1. data-to-text generation

463

various modi   cations to this basic mechanism have been proposed. in coarse-to-   ne
attention (mei et al., 2016), each record receives a global attention ar     [0, 1], which is
independent of the decoder state. this global attention, which represents the overall
importance of the record, is multiplied with the decoder-based attention scores, before
computing the    nal normalized attentions. in structured attention, the attention vector
  m      can include structural biases, which can favor assigning higher attention values to
contiguous segments or to dependency subtrees (kim et al., 2017). structured attention
vectors can be computed by running the forward-backward algorithm to obtain marginal
attention probabilities (see    7.5.3). because each step in the forward-backward algorithm
is differentiable, it can be encoded in a computation graph, and end-to-end learning can
be performed by id26.

decoder

given the encoding, the decoder can function just as in id4 (see
   18.3.1), using the attention-weighted encoder representation in the decoder recurrence
and/or output computation. as in machine translation, id125 can help to avoid
search errors (lebret et al., 2016).

many applications require generating words that do not appear in the training vocab-
ulary. for example, a weather record may contain a previously unseen city name; a sports
record may contain a previously unseen player name. such tokens can be generated in the
text by copying them over from the input (e.g., gulcehre et al., 2016).3 first introduce an
additional variable sm     {gen, copy}, indicating whether token w(t)
m should be generated
or copied. the decoder id203 is then,

p(w(t) | w(t)

1:m   1, z, sm) =(cid:40)softmax(  w(t)    h(t)

r=1   (cid:16)w(s)
(cid:80)r

r = w(t)(cid:17)      m   r,

m   1),

sm = gen
sm = copy,

[19.8]

where   (w(s)
r = w(t)) is an indicator function, taking the value 1 iff the text of the record
w(s)
is identical to the target word w(t). the id203 of copying record r from the source
r
is    (sm = copy)      m   r, the product of the copy id203 by the local attention. note
that in this model, the attention weights   m are computed from the previous decoder state
hm   1. the computation graph therefore remains a feedforward network, with recurrent
paths such as h(t)

m   1       m     w(t)

m     h(t)
m .

to facilitate end-to-end training, the switching variable sm can be represented by a
gate   m, which is computed from a two-layer feedforward network, whose input consists
of the concatenation of the decoder state h(t)
m   1 and the attention-weighted representation
3a number of variants of this strategy have been proposed (e.g., gu et al., 2016; merity et al., 2017). see

wiseman et al. (2017) for an overview.

under contract with mit press, shared under cc-by-nc-nd license.

464

chapter 19. text generation

of the data, cm =(cid:80)r

r=1   m   rzr,

  m =   (  (2)f (  (1)[h(t)

m   1; cm])).

[19.9]

the full generative id203 at token m is then,

p(w(t) | w(t)

1:m, z) =  m   

exp   w(t)    h(t)
m   1
(cid:80)v
j=1 exp   j    h(t)
m   1
(cid:124)

(cid:123)(cid:122)

generate

(cid:125)

+(1       m)   

r(cid:88)r=1
(cid:124)

  (w(s)

.

r = w(t))      m   r
(cid:125)

(cid:123)(cid:122)

copy

[19.10]

19.2 text-to-text generation

text-to-text generation includes problems of summarization and simpli   cation:

    reading a novel and outputting a paragraph-long summary of the plot;4
    reading a set of blog posts about politics, and outputting a bullet list of the various
issues and perspectives;

    reading a technical research article about the long-term health consequences of drink-
ing kombucha, and outputting a summary of the article in language that non-experts
can understand.

these problems can be approached in two ways: through the encoder-decoder architec-
ture discussed in the previous section, or by operating directly on the input text.

19.2.1 neural abstractive summarization
sentence summarization is the task of shortening a sentence while preserving its mean-
ing, as in the following examples (knight and marcu, 2000; rush et al., 2015):

(19.2)

a. the documentation is typical of epson quality: excellent.

documentation is excellent.

b. russian defense minister ivanov called sunday for the creation of a joint front

for combating global terrorism.
russia calls for joint front against terrorism.

4in    16.3.4, we encountered a special case of single-document summarization, which involved extract-
ing the most important sentences or discourse units. we now consider the more challenging problem of
abstractive summarization, in which the summary can include words that do not appear in the original text.

jacob eisenstein. draft of november 13, 2018.

19.2. text-to-text generation

465

sentence summarization is closely related to sentence compression, in which the sum-
mary is produced by deleting words or phrases from the original (clarke and lapata,
2008). but as shown in (19.2b), a sentence summary can also introduce new words, such
as against, which replaces the phrase for combatting.

sentence summarization can be treated as a machine translation problem, using the at-
tentional encoder-decoder translation model discussed in    18.3.1 (rush et al., 2015). the
longer sentence is encoded into a sequence of vectors, one for each token. the decoder
then computes attention over these vectors when updating its own recurrent state. as
with data-to-text generation, it can be useful to augment the encoder-decoder model with
the ability to copy words directly from the source. rush et al. (2015) train this model by
building four million sentence pairs from news articles. in each pair, the longer sentence is
the    rst sentence of the article, and the summary is the article headline. sentence summa-
rization can also be trained in a semi-supervised fashion, using a probabilistic formulation
of the encoder-decoder model called a variational autoencoder (miao and blunsom, 2016,
also see    14.8.2).
by maintaining a vector of the sum total of all attention values thus far, tm = (cid:80)m

when summarizing longer documents, an additional concern is that the summary not
be repetitive: each part of the summary should cover new ground. this can be addressed
n=1   n.
this total can be used as an additional input to the computation of the attention weights,

  m   n     exp(cid:16)v      tanh(    [h(t)

m ; h(s)

n ; tm])(cid:17) ,

[19.11]

which enables the model to learn to prefer parts of the source which have not been at-
tended to yet (tu et al., 2016). to further encourage diversity in the generated summary,
see et al. (2017) introduce a coverage loss to the objective function,

(cid:96)m =

min(  m   n, tm   n).

[19.12]

m (s)(cid:88)n=1

this loss will be low if   massigns little attention to words that already have large values in
tm.coverage loss is similar to the concept of marginal relevance, in which the reward for
adding new content is proportional to the extent to which it increases the overall amount
of information conveyed by the summary (carbonell and goldstein, 1998).

19.2.2 sentence fusion for id57
in id57, the goal is to produce a summary that covers the
content of several documents (mckeown et al., 2002). one approach to this challenging
problem is to identify sentences across multiple documents that relate to a single theme,
and then to fuse them into a single sentence (barzilay and mckeown, 2005). as an exam-
ple, consider the following two sentences (mckeown et al., 2010):

under contract with mit press, shared under cc-by-nc-nd license.

466

chapter 19. text generation

(19.3)

a. palin actually turned against the bridge project only after it became a national

symbol of wasteful spending.

b. ms. palin supported the bridge project while running for governor, and

abandoned it after it became a national scandal.

an intersection preserves only the content that is present in both sentences:

(19.4) palin turned against the bridge project after it became a national scandal.

a union includes information from both sentences:

(19.5) ms. palin supported the bridge project while running for governor, but turned
against it when it became a national scandal and a symbol of wasteful spending.

max

y (cid:88)i,j,r
s.t. y     c,

id33 is often used as a technique for sentence fusion. after parsing
each sentence, the resulting dependency trees can be aggregated into a lattice (barzilay
and mckeown, 2005) or a graph structure (filippova and strube, 2008), in which identical
or closely related words (e.g., palin, bridge, national) are fused into a single node. the
resulting graph can then be pruned back to a tree by solving an integer linear program
(see    13.2.2),

  (i r       j, w;   )    yi,j,r

[19.13]

[19.14]
where the variable yi,j,r     {0, 1} indicates whether there is an edge from i to j of type r,
the score of this edge is   (i r       j, w;   ), and c is a set of constraints, which ensures that y
forms a valid dependency graph. as usual, w is the list of words in the graph, and    is a
vector of parameters. the score   (i r       j, w;   ) re   ects the    importance    of the modi   er
j to the overall meaning: in intersective fusion, this score indicates the extent to which
the content in this edge is expressed in all sentences; in union fusion, the score indicates
whether the content in the edge is expressed in any sentence. the constraint set c can
impose additional linguistic constraints: for example, ensuring that coordinated nouns
are suf   ciently similar. the resulting tree must then be linearized into a sentence. lin-
earization is like the inverse of id33: instead of parsing from a sequence
of tokens into a tree, we must convert the tree back into a sequence of tokens. this is
typically done by generating a set of candidate linearizations, and choosing the one with
the highest score under a language model (langkilde and knight, 1998; song et al., 2016).

19.3 dialogue

dialogue systems are capable of conversing with a human interlocutor, often to per-
form some task (grosz, 1979), but sometimes just to chat (weizenbaum, 1966). while re-

jacob eisenstein. draft of november 13, 2018.

19.3. dialogue

467

(19.6) a: i want to order a pizza.

b: what toppings?
a: anchovies.
b: ok, what address?
a: the college of computing
building.
b: please con   rm: one pizza
with artichokes, to be delivered
to the college of computing
building.
a: no.
b: what toppings?
. . .

figure 19.5: an example dialogue and the associated    nite-state model. in the    nite-state
model, small caps indicates that the user must provide information of this type in their
answer.

search on dialogue systems goes back several decades (carbonell, 1970; winograd, 1972),
commercial systems such as alexa and siri have recently brought this technology into
widespread use. nonetheless, there is a signi   cant gap between research and practice:
many practical dialogue systems remain scripted and in   exible, while research systems
emphasize abstractive text generation,    on-the-   y    decision making, and probabilistic
reasoning about the user   s intentions.

19.3.1 finite-state and agenda-based dialogue systems

finite-state automata were introduced in chapter 9 as a formal model of computation,
in which string inputs and outputs are linked to transitions between a    nite number of
discrete states. this model naturally    ts simple task-oriented dialogues, such as the one
shown in the left panel of figure 19.5. this (somewhat frustrating) dialogue can be repre-
sented with a    nite-state transducer, as shown in the right panel of the    gure. the accept-
ing state is reached only when the two needed pieces of information are provided, and the
human user con   rms that the order is correct. in this simple scenario, the topping and
address are the two slots associated with the activity of ordering a pizza, which is called
a frame. frame representations can be hierarchical: for example, an address could have
slots of its own, such as street and city.

in the example dialogue in figure 19.5, the user provides the precise inputs that are
needed in each turn (e.g., anchovies; the college of computing building). some users may

under contract with mit press, shared under cc-by-nc-nd license.

q0startq1q2q3q4q5q6whattoppings?toppingwhataddress?addresscon   rm?noyes468

chapter 19. text generation

prefer to communicate more naturally, with phrases like i   d, uh, like some anchovies please.
one approach to handling such utterances is to design a custom grammar, with non-
terminals for slots such as topping and location. however, context-free parsing of
unconstrained speech input is challenging. a more lightweight alternative is bio-style
sequence labeling (see    8.3), e.g.:
and
(19.7)
o

anchovies
b-topping

computing
i-addr

the
b-addr

college
i-addr

of
i-addr

please
o

bring
o

it
o

to
o

,
o

like
o

i   d
o
building
i-addr

.
o

the tagger can be driven by a bi-directional recurrent neural network, similar to recurrent
approaches to id14 described in    13.2.3.

the input in (19.7) could not be handled by the    nite-state system from figure 19.5,
which forces the user to provide the topping    rst, and then the location. in this sense, the
   initiative    is driven completely by the system. agenda-based dialogue systems extend
   nite-state architectures by attempting to recognize all slots that are    lled by the user   s re-
ply, thereby handling these more complex examples. agenda-based systems dynamically
pose additional questions until the frame is complete (bobrow et al., 1977; allen et al.,
1995; rudnicky and xu, 1999). such systems are said to be mixed-initiative, because both
the user and the system can drive the direction of the dialogue.

19.3.2 id100
the task of dynamically selecting the next move in a conversation is known as dialogue
management. this problem can be framed as a markov decision process, which is a
theoretical model that includes a discrete set of states, a discrete set of actions, a function
that computes the id203 of transitions between states, and a function that computes
the cost or reward of action-state pairs. let   s see how each of these elements pertains to
the pizza ordering dialogue system.

    each state is a tuple of information about whether the topping and address are

known, and whether the order has been con   rmed. for example,

(known topping, unknown address, not confirmed)

[19.15]

is a possible state. any state in which the pizza order is con   rmed is a terminal
state, and the markov decision process stops after entering such a state.

    the set of actions includes querying for the topping, querying for the address, and
requesting con   rmation. each action induces a id203 distribution over states,
p(st | at, st   1). for example, requesting con   rmation of the order is not likely to
jacob eisenstein. draft of november 13, 2018.

19.3. dialogue

469

result in a transition to the terminal state if the topping is not yet known. this
id203 distribution over state transitions may be learned from data, or it may
be speci   ed in advance.

    each state-action-state tuple earns a reward, ra(st, st+1). in the context of the pizza

ordering system, a simple reward function would be,

ra(st, st   1) =               

a = confirm, st = (*, *, confirmed)

0,
   10, a = confirm, st = (*, *, not confirmed)
   1,

a (cid:54)= confirm

[19.16]

this function assigns zero reward for successful transitions to the terminal state, a
large negative reward to a rejected request for con   rmation, and a small negative re-
ward for every other type of action. the system is therefore rewarded for reaching
the terminal state in few steps, and penalized for prematurely requesting con   rma-
tion.

in a markov decision process, a policy is a function    : s     a that maps from states
to actions (see    15.2.4). the value of a policy is the expected sum of discounted rewards,
e  [(cid:80)t
t=1   trat(st, st+1)], where    is the discount factor,        [0, 1). discounting has the
effect of emphasizing rewards that can be obtained immediately over less certain rewards
in the distant future.

an optimal policy can be obtained by id145, by iteratively updating
the value function v (s), which is the expectation of the cumulative reward from s under
the optimal action a,

v (s)     max

a   a (cid:88)s(cid:48)   s

(cid:48)

p(s

(cid:48)
| s, a)[ra(s, s

) +   v (s

(cid:48)

)].

[19.17]

the value function v (s) is computed in terms of v (s(cid:48)) for all states s(cid:48)
    s. a series
of iterative updates to the value function will eventually converge to a stationary point.
this algorithm is known as value iteration. given the converged value function v (s), the
optimal action at each state is the argmax,

  (s) = argmax

a   a (cid:88)s(cid:48)   s

(cid:48)
p(s

(cid:48)
| s, a)[ra(s, s

(cid:48)
) +   v (s

)].

[19.18]

value iteration and related algorithms are described in detail by sutton and barto (1998).
for applications to dialogue systems, see levin et al. (1998) and walker (2000).

the markov decision process framework assumes that the current state of the dialogue
is known. in reality, the system may misinterpret the user   s statements     for example,
believing that a speci   cation of the delivery location (peachtree) is in fact a speci   cation

under contract with mit press, shared under cc-by-nc-nd license.

470

chapter 19. text generation

of the topping (peaches). in a partially observable markov decision process (pomdp),
the system receives an observation o, which is probabilistically conditioned on the state,
p(o | s). it must therefore maintain a distribution of beliefs about which state it is in, with
qt(s) indicating the degree of belief that the dialogue is in state s at time t. the pomdp
formulation can help to make dialogue systems more robust to errors, particularly in the
context of spoken language dialogues, where the speech itself may be misrecognized (roy
et al., 2000; williams and young, 2007). however,    nding the optimal policy in a pomdp
is computationally intractable, requiring additional approximations.

19.3.3 neural chatbots
it   s easier to talk when you don   t need to get anything done. chatbots are systems that
parry the user   s input with a response that keeps the conversation going. they can be
built from the encoder-decoder architecture discussed in    18.3 and    19.1.2: the encoder
converts the user   s input into a vector, and the decoder produces a sequence of words as a
response. for example, shang et al. (2015) apply the attentional encoder-decoder transla-
tion model, training on a dataset of posts and responses from the chinese microblogging
platform sina weibo.5 this approach is capable of generating replies that relate themati-
cally to the input, as shown in the following examples (translated from chinese by shang
et al., 2015).

(19.8)

a. a: high fever attacks me every new year   s day.

b: get well soon and stay healthy!

b. a: i gain one more year. grateful to my group, so happy.

b: getting old now. time has no mercy.

while encoder-decoder models can generate responses that make sense in the con-
text of the immediately preceding turn, they struggle to maintain coherence over longer
conversations. one solution is to model the dialogue context recurrently. this creates
a hierarchical recurrent network, including both word-level and turn-level recurrences.
the turn-level hidden state is then used as additional context in the decoder (serban et al.,
2016).

an open question is how to integrate the encoder-decoder architecture into task-oriented

dialogue systems. neural chatbots can be trained end-to-end: the user   s turn is analyzed
by the encoder, and the system output is generated by the decoder. this architecture
can be trained by log-likelihood using id26 (e.g., sordoni et al., 2015; serban
et al., 2016), or by more elaborate objectives, using id23 (li et al., 2016).
in contrast, the task-oriented dialogue systems described in    19.3.1 typically involve a
5twitter is also frequently used for construction of dialogue datasets (ritter et al., 2011; sordoni et al.,
2015). another source is technical support chat logs from the ubuntu linux distribution (uthus and aha,
2013; lowe et al., 2015).

jacob eisenstein. draft of november 13, 2018.

19.3. dialogue

471

set of specialized modules: one for recognizing the user input, another for deciding what
action to take, and a third for arranging the text of the system output.

recurrent neural network decoders can be integrated into markov decision process
dialogue systems, by conditioning the decoder on a representation of the information
that is to be expressed in each turn (wen et al., 2015). speci   cally, the long short-term
memory (lstm;    6.3) architecture is augmented so that the memory cell at turn m takes
an additional input dm, which is a representation of the slots and values to be expressed
in the next turn. however, this approach still relies on additional modules to recognize
the user   s utterance and to plan the overall arc of the dialogue.

another promising direction is to create embeddings for the elements in the domain:
for example, the slots in a record and the entities that can    ll them. the encoder then
encodes not only the words of the user   s input, but the embeddings of the elements that
the user mentions. similarly, the decoder is endowed with the ability to refer to speci   c
elements in the knowledge base. he et al. (2017) show that such a method can learn to
play a collaborative dialogue game, in which both players are given a list of entities and
their properties, and the goal is to    nd an entity that is on both players    lists.

additional resources

gatt and krahmer (2018) provide a comprehensive recent survey on text generation. for
a book-length treatment of earlier work, see reiter and dale (2000). for a survey on image
captioning, see bernardi et al. (2016); for a survey of pre-neural approaches to dialogue
systems, see rieser and lemon (2011). dialogue acts were introduced in    8.6 as a label-
ing scheme for human-human dialogues; they also play a critical in task-based dialogue
systems (e.g., allen et al., 1996). the incorporation of theoretical models of dialogue into
computational systems is reviewed by jurafsky and martin (2009, chapter 24).

while this chapter has focused on the informative dimension of text generation, an-
other line of research aims to generate text with con   gurable stylistic properties (walker
et al., 1997; mairesse and walker, 2011; ficler and goldberg, 2017; hu et al., 2017). this
chapter also does not address the generation of creative text such as narratives (riedl and
young, 2010), jokes (ritchie, 2001), poems (colton et al., 2012), and song lyrics (gonc  alo oliveira
et al., 2007).

exercises

1. find an article about a professional basketball game, with an associated    box score   
of statistics. which are the    rst three elements in the box score that are expressed
in the article? can you identify template-based patterns that express these elements
of the record? now    nd a second article about a different basketball game. does it

under contract with mit press, shared under cc-by-nc-nd license.

472

chapter 19. text generation

mention the same    rst three elements of the box score? do your templates capture
how these elements are expressed in the text?

2. this exercise is to be done by a pair of students. one student should choose an article
from the news or from wikipedia, and manually perform id14
(srl) on three short sentences or clauses.
(see chapter 13 for a review of srl.)
identify the main the semantic relation and its arguments and adjuncts. pass this
structured record     but not the original sentence     to the other student, whose
job is to generate a sentence expressing the semantics. then reverse roles, and try
to regenerate three sentences from another article, based on the predicate-argument
semantics.

3. compute the id7 scores (see    18.1.1) for the generated sentences in the previous

problem, using the original article text as the reference.

4. align each token in the text of figure 19.1 to a speci   c single record in the database,
or to the null record    . for example, the tokens south wind would align to the record
mode=s. how often is each token aligned
wind direction:
to the same record as the previous token? how many transitions are there? how
might a system learn to output 10 degrees for the record min=9?

06:00-21:00:

5. in sentence compression and fusion, we may wish to preserve contiguous sequences
of tokens (id165s) and/or dependency edges. find    ve short news articles with
headlines. for each headline, compute the fraction of bigrams that appear in the
main text of the article. then do a manual depenency parse of the headline. for
each dependency edge, count how often it appears as a dependency edge in the
main text. you may use an automatic dependency parser to assist with this exercise,
but check the output, and focus on ud 2.0 dependency grammar, as described in
chapter 11.

6.    19.2.2 presents the idea of generating text from dependency trees, which requires
linearization. sometimes there are multiple ways that a dependency tree can be
linearized. for example:

(19.9)

a. the sick kids stayed at home in bed.
b. the sick kids stayed in bed at home.

both sentences have an identical dependency parse: both home and bed are (oblique)
dependents of stayed.
identify two more english dependency trees that can each be linearized in more than
one way, and try to use a different pattern of variation in each tree. as usual, specify
your trees in the universal dependencies 2 style, which is described in chapter 11.

jacob eisenstein. draft of november 13, 2018.

19.3. dialogue

473

7. in    19.3.2, we considered a pizza delivery service. let   s simplify the problem to
take-out, where it is only necessary to determine the topping and con   rm the order.
the state is a tuple in which the    rst element is t if the topping is speci   ed and ?
otherwise, and the second element is either yes or no, depending on whether the
order has been con   rmed. the actions are topping? (request information about
the topping) and confirm? (request con   rmation). the state transition function is:

p(st | st   1 = (?, no), a = topping?) =(cid:40)0.9,
p(st | st   1 = (?, no), a = confirm?) =(cid:110)1,
p(st | st   1 = (t, no), a = topping?) =(cid:110)1,
p(st | st   1 = (t, no), a = confirm?) =(cid:40)0.9,

0.1,

0.1,

st = (t, no)
st = (?, no).

st = (?, no).

st = (t, no).

st = (t, yes)
st = (t, no).

[19.19]

[19.20]

[19.21]

[19.22]

using the reward function de   ned in equation 19.16, the discount    = 0.9, and the
initialization v (s) = 0, execute three iterations of equation 19.17. after these three
iterations, compute the optimal action in each state. you can assume that for the
terminal states, v (*, yes) = 0, so you only need to compute the values for non-
terminal states, v (?, no) and v (t, no).

8. there are several toolkits that allow you to train encoder-decoder translation models
   out of the box   , such as fairseq (gehring et al., 2017), xid4 (neubig et al., 2018),
tensor2tensor (vaswani et al., 2018), and openid4 (klein et al., 2017).6 use one
of these toolkits to train a chatbot dialogue system, using either the nps dialogue
corpus that comes with nltk (forsyth and martell, 2007), or, if you are feeling more
ambitious, the ubuntu dialogue corpus (lowe et al., 2015).

6https://github.com/facebookresearch/fairseq;

https://github.com/neulab/xid4;

https://github.com/tensorflow/tensor2tensor; http://openid4.net/

under contract with mit press, shared under cc-by-nc-nd license.

appendix a

id203

id203 theory provides a way to reason about random events. the sorts of random
events that are typically used to explain id203 theory include coin    ips, card draws,
and the weather. it may seem odd to think about the choice of a word as akin to the    ip of
a coin, particularly if you are the type of person to choose words carefully. but random or
not, language has proven to be extremely dif   cult to model deterministically. id203
offers a powerful tool for modeling and manipulating linguistic data.

id203 can be thought of in terms of random outcomes: for example, a single coin
   ip has two possible outcomes, heads or tails. the set of possible outcomes is the sample
space, and a subset of the sample space is an event. for a sequence of two coin    ips,
there are four possible outcomes, {hh, ht, t h, t t}, representing the ordered sequences
heads-head, heads-tails, tails-heads, and tails-tails. the event of getting exactly one head
includes two outcomes: {ht, t h}.

formally, a id203 is a function from events to the interval between zero and one:
pr : f     [0, 1], where f is the set of possible events. an event that is certain has proba-
bility one; an event that is impossible has id203 zero. for example, the id203
of getting fewer than three heads on two coin    ips is one. each outcome is also an event
(a set with exactly one element), and for two    ips of a fair coin, the id203 of each
outcome is,

pr({hh}) = pr({ht}) = pr({t h}) = pr({t t}) =

1
4

.

[a.1]

a.1 probabilities of event combinations

because events are sets of outcomes, we can use set-theoretic operations such as comple-
ment, intersection, and union to reason about the probabilities of events and their combi-
nations.

475

476

appendix a. id203

for any event a, there is a complement   a, such that:
    the id203 of the union a       a is pr(a       a) = 1;
    the intersection a       a =     is the empty set, and pr(a       a) = 0.

in the coin    ip example, the event of obtaining a single head on two    ips corresponds to
the set of outcomes {ht, t h}; the complement event includes the other two outcomes,
{t t, hh}.
a.1.1 probabilities of disjoint events
when two events have an empty intersection, a    b =    , they are disjoint. the probabil-
ity of the union of two disjoint events is equal to the sum of their probabilities,

[a.2]
this is the third axiom of id203, and it can be generalized to any countable sequence
of disjoint events.

a     b =         pr(a     b) = pr(a) + pr(b).

in the coin    ip example, this axiom can derive the id203 of the event of getting a
single head on two    ips. this event is the set of outcomes {ht, t h}, which is the union
of two simpler events, {ht, t h} = {ht}     {t h}. the events {ht} and {t h} are
disjoint. therefore,

[a.3]

[a.4]

[a.5]

pr({ht, t h}) = pr({ht}     {t h}) = pr({ht}) + pr({t h})

=

+

=

1
4

1
4

1
2

.

in the general, the id203 of the union of two events is,

pr(a     b) = pr(a) + pr(b)     pr(a     b).

this can be seen visually in figure a.1, and it can be derived from the third axiom of
id203. consider an event that includes all outcomes in b that are not in a, denoted
as b     (a     b). by construction, this event is disjoint from a. we can therefore apply the
additive rule,

pr(a     b) = pr(a) + pr(b     (a     b)).

[a.6]

furthermore, the event b is the union of two disjoint events: a     b and b     (a     b).
[a.7]

pr(b) = pr(b     (a     b)) + pr(a     b).

reorganizing and subtituting into equation a.6 gives the desired result:

pr(b     (a     b)) = pr(b)     pr(a     b)

pr(a     b) = pr(a) + pr(b)     pr(a     b).

[a.8]
[a.9]

jacob eisenstein. draft of november 13, 2018.

a.2. id155 and bayes    rule

477

figure a.1: a visualization of the id203 of non-disjoint events a and b.

a.1.2 law of total id203
a set of events b = {b1, b2, . . . , bn} is a partition of the sample space iff each pair of
events is disjoint (bi     bj =    ), and the union of the events is the entire sample space.
the law of total id203 states that we can marginalize over these events as follows,

pr(a) = (cid:88)bn   b

pr(a     bn).

[a.10]

for any event b, the union b       b is a partition of the sample space. therefore, a special
case of the law of total id203 is,

pr(a) = pr(a     b) + pr(a       b).

[a.11]

a.2 id155 and bayes    rule

a id155 is an expression like pr(a | b), which is the id203 of
the event a, assuming that event b happens too. for example, we may be interested
in the id203 of a randomly selected person answering the phone by saying hello,
conditioned on that person being a speaker of english. id155 is de   ned
as the ratio,

pr(a | b) =

pr(a     b)

pr(b)

.

[a.12]

the chain rule of id203 states that pr(a    b) = pr(a | b)   pr(b), which is just
under contract with mit press, shared under cc-by-nc-nd license.

aba   b478

appendix a. id203

a rearrangement of terms from equation a.12. the chain rule can be applied repeatedly:

pr(a     b     c) = pr(a | b     c)    pr(b     c)

= pr(a | b     c)    pr(b | c)    pr(c).

bayes    rule (sometimes called bayes    law or bayes    theorem) gives us a way to convert
between pr(a | b) and pr(b | a). it follows from the de   nition of id155
and the chain rule:

pr(a | b) =

pr(a     b)

pr(b)

=

pr(b | a)    pr(a)

pr(b)

[a.13]

each term in bayes rule has a name, which we will occasionally use:

    pr(a) is the prior, since it is the id203 of event a without knowledge about

whether b happens or not.

    pr(b | a) is the likelihood, the id203 of event b given that event a has oc-

curred.

    pr(a | b) is the posterior, the id203 of event a with knowledge that b has

occurred.

example the classic examples for bayes    rule involve tests for rare diseases, but man-
ning and sch   utze (1999) reframe this example in a linguistic setting. suppose that you are
is interested in a rare syntactic construction, such as parasitic gaps, which occur on average
once in 100,000 sentences. here is an example of a parasitic gap:

(a.1) which class did you attend without registering for ?

lana linguist has developed a complicated pattern matcher that attempts to identify

sentences with parasitic gaps. it   s pretty good, but it   s not perfect:

    if a sentence has a parasitic gap, the pattern matcher will    nd it with id203

0.95. (this is the recall, which is one minus the false negative rate.)

    if the sentence doesn   t have a parasitic gap, the pattern matcher will wrongly say it
does with id203 0.005. (this is the false positive rate, which is one minus the
precision.)

suppose that lana   s pattern matcher says that a sentence contains a parasitic gap. what
is the id203 that this is true?

jacob eisenstein. draft of november 13, 2018.

a.3. independence

479

let g be the event of a sentence having a parasitic gap, and t be the event of the test
being positive. we are interested in the id203 of a sentence having a parasitic gap
given that the test is positive. this is the id155 pr(g | t ), and it can be
computed by bayes    rule:

pr(g | t ) =

pr(t | g)    pr(g)

pr(t )

.

[a.14]

we already know both terms in the numerator: pr(t | g) is the recall, which is 0.95; pr(g)
is the prior, which is 10   5.

we are not given the denominator, but it can be computed using tools developed ear-

lier in this section. first apply the law of total id203, using the partition {g,  g}:

pr(t ) = pr(t     g) + pr(t       g).

[a.15]

this says that the id203 of the test being positive is the sum of the id203 of a
true positive (t     g) and the id203 of a false positive (t       g). the id203 of
each of these events can be computed using the chain rule:

pr(t     g) = pr(t | g)    pr(g) = 0.95    10
pr(t       g) = pr(t |   g)    pr(  g) = 0.005    (1     10

   5)     0.005

   5

pr(t ) = pr(t     g) + pr(t       g)

   5 + 0.005.

=0.95    10

plugging these terms into bayes    rule gives the desired posterior id203,

pr(g | t ) =

pr(t | g) pr(g)

pr(t )

0.95    10   5

=

0.95    10   5 + 0.005    (1     10   5)
   0.002.

[a.16]
[a.17]
[a.18]
[a.19]

[a.20]

[a.21]

[a.22]

lana   s pattern matcher seems accurate, with false positive and false negative rates
below 5%. yet the extreme rarity of the phenomenon means that a positive result from the
detector is most likely to be wrong.

a.3 independence

two events are independent if the id203 of their intersection is equal to the product
of their probabilities: pr(a     b) = pr(a)    pr(b). for example, for two    ips of a fair
under contract with mit press, shared under cc-by-nc-nd license.

480

appendix a. id203

coin, the id203 of getting heads on the    rst    ip is independent of the id203 of
getting heads on the second    ip:

pr({ht, hh}) = pr(ht ) + pr(hh) =
pr({hh, t h}) = pr(hh) + pr(t h) =

pr({ht, hh})    pr({hh, t h}) =

1
4
pr({ht, hh}     {hh, t h}) = pr(hh) =

1
2   

1
2

=

1
4

1
4
1
4

+

+

1
4
1
4

=

=

1
2
1
2

[a.23]

[a.24]

[a.25]

[a.26]

[a.27]
if pr(a     b | c) = pr(a | c)    pr(b | c), then the events a and b are conditionally
independent, written a     b | c. conditional independence plays a important role in
probabilistic models such as na    ve bayes chapter 2.

= pr({ht, hh})    pr({hh, t h}).

a.4 random variables

random variables are functions from events to rn, where r is the set of real numbers.
this subsumes several useful special cases:

4 + 1

4 + 1

4.
4 = 3

    an indicator random variable is a function from events to the set {0, 1}. in the coin
   ip example, we can de   ne y as an indicator random variable, taking the value
1 when the coin has come up heads on at least one    ip. this would include the
outcomes {hh, ht, t h}. the id203 pr(y = 1) is the sum of the probabilities
of these outcomes, pr(y = 1) = 1
    a discrete random variable is a function from events to a discrete subset of r. con-
sider the coin    ip example: the number of heads on two    ips, x, can be viewed as a
discrete random variable, x     0, 1, 2. the event id203 pr(x = 1) can again be
computed as the sum of the probabilities of the events in which there is one head,
{ht, t h}, giving pr(x = 1) = 1
each possible value of a random variable is associated with a subset of the sample
space.
in the coin    ip example, x = 0 is associated with the event {t t}, x = 1 is
associated with the event {ht, t h}, and x = 2 is associated with the event {hh}.
assuming a fair coin, the probabilities of these events are, respectively, 1/4, 1/2, and 1/4.
this list of numbers represents the id203 distribution over x, written px, which
maps from the possible values of x to the non-negative reals. for a speci   c value x, we
write px (x), which is equal to the event id203 pr(x = x).1 the function px is called
1in general, capital letters (e.g., x) refer to random variables, and lower-case letters (e.g., x) refer to

2.
4 = 1

4 + 1

speci   c values. when the distribution is clear from context, i will simply write p(x).

jacob eisenstein. draft of november 13, 2018.

a.5. expectations

481

a id203 mass function (pmf) if x is discrete; it is called a id203 density function
(pdf) if x is continuous. in either case, the function must sum to one, and all values must
be non-negative:

(cid:90)x
px (x)dx =1
   x, px (x)    0.

[a.28]

[a.29]

probabilities over multiple random variables can written as joint probabilities, e.g.,
pa,b(a, b) = pr(a = a     b = b). several properties of event probabilities carry over to
id203 distributions over random variables:
    the marginal id203 distribution is pa(a) =(cid:80)b pa,b(a, b).
    the id155 distribution is pa|b(a | b) =
    random variables a and b are independent iff pa,b(a, b) = pa(a)    pb(b).

pa,b(a,b)

pb(b)

.

a.5 expectations

sometimes we want the expectation of a function, such as e[g(x)] = (cid:80)x   x g(x)p(x).

expectations are easiest to think about in terms of id203 distributions over discrete
events:

    if it is sunny, lucia will eat three ice creams.
    if it is rainy, she will eat only one ice cream.
    there   s a 80% chance it will be sunny.
    the expected number of ice creams she will eat is 0.8    3 + 0.2    1 = 2.6.

if the random variable x is continuous, the expectation is an integral:

e[g(x)] =(cid:90)x

g(x)p(x)dx

[a.30]

for example, a fast food restaurant in quebec has a special offer for cold days: they give
a 1% discount on poutine for every degree below zero. assuming a thermometer with
in   nite precision, the expected price would be an integral over all possible temperatures,

e[price(x)] =(cid:90)x

min(1, 1 + x)    original-price    p(x)dx.

[a.31]

under contract with mit press, shared under cc-by-nc-nd license.

482

appendix a. id203

a.6 modeling and estimation

probabilistic models provide a principled way to reason about random events and ran-
dom variables. let   s consider the coin toss example. each toss can be modeled as a ran-
dom event, with id203    of the event h, and id203 1        of the complementary
event t . if we write a random variable x as the total number of heads on three coin
   ips, then the distribution of x depends on   . in this case, x is distributed as a binomial
random variable, meaning that it is drawn from a binomial distribution, with parameters
(  , n = 3). this is written,

x     binomial(  , n = 3).

[a.32]

the properties of the binomial distribution enable us to make statements about the x,
such as its expected value and the likelihood that its value will fall within some interval.
now suppose that    is unknown, but we have run an experiment, in which we exe-
cuted n trials, and obtained x heads. we can estimate    by the principle of maximum
likelihood:

     = argmax

  

px (x;   , n ).

[a.33]

this says that the estimate      should be the value that maximizes the likelihood of the
data. the semicolon indicates that    and n are parameters of the id203 function.
the likelihood px (x;   , n ) can be computed from the binomial distribution,

px (x;   , n ) =

n !

x!(n     x)!

  x(1       )n   x.

[a.34]

this likelihood is proportional to the product of the id203 of individual out-
comes: for example, the sequence t, h, h, t, h would have id203   3(1       )2. the
term n !
x!(n   x)! arises from the many possible orderings by which we could obtain x heads
on n trials. this term does not depend on   , so it can be ignored during estimation.

in practice, we maximize the log-likelihood, which is a monotonic function of the like-
lihood. under the binomial distribution, the log-likelihood is a convex function of    (see

jacob eisenstein. draft of november 13, 2018.

a.6. modeling and estimation

   2.4), so it can be maximized by taking the derivative and setting it equal to zero.

(cid:96)(  ) =x log    + (n     x) log(1       )
   (cid:96)(  )

n     x
1       

=

=

     
n     x
1       
n     x
x
n
x     1 =
     =

=

x
      
x
  
1       
  
1
       1
x
n

.

483

[a.35]

[a.36]

[a.37]

[a.38]

[a.39]

[a.40]

in this case, the maximum likelihood estimate is equal to x

n , the fraction of trials that
came up heads. this intuitive solution is also known as the relative frequency estimate,
since it is equal to the relative frequency of the outcome.

is id113 always the right choice? suppose you conduct one
trial, and get heads. would you conclude that    = 1, meaning that the coin is guaran-
teed to come up heads? if not, then you must have some prior expectation about   . to
incorporate this prior information, we can treat    as a random variable, and use bayes   
rule:

p(   | x; n ) =

p(x |   )    p(  )

p(x)

   p(x |   )    p(  )
     = argmax

p(x |   )    p(  ).

  

[a.41]

[a.42]
[a.43]

this it the maximum a posteriori (map) estimate. given a form for p(  ), you can de-
rive the map estimate using the same approach that was used to derive the maximum
likelihood estimate.

additional resources

a good introduction to id203 theory is offered by manning and sch   utze (1999),
which helped to motivate this section. for more detail, sharon goldwater provides an-
other useful reference, http://homepages.inf.ed.ac.uk/sgwater/teaching/general/
id203.pdf. a historical and philosophical perspective on id203 is offered
by diaconis and skyrms (2017).

under contract with mit press, shared under cc-by-nc-nd license.

appendix b

numerical optimization

unconstrained numerical optimization involves solving problems of the form,

f (x),

min
x   rd

[b.1]

where x     rd is a vector of d real numbers.

differentiation is fundamental to numerical optimization. suppose that at some x   ,
= 0. then x    is said to be a
every partial derivative of f is equal to 0: formally,    f
critical point of f. if f is a convex function (de   ned in    2.4), then the value of f (x   ) is
equal to the global minimum of f iff x    is a critical point of f.
as an example, consider the convex function f (x) = (x   2)2 +3, shown in figure b.1a.
the derivative is    f
   x = 2x   4. a unique minimum can be obtained by setting the derivative
equal to zero and solving for x, obtaining x    = 2. now consider the multivariate convex
2||x     [2, 1](cid:62)
function f (x) = 1
||2, where ||x||2 is the squared euclidean norm. the partial

   xi(cid:12)(cid:12)(cid:12)x   

(a) the function f (x) = (x     2)2 + 3

(b) the function f (x) = |x|     2 cos(x)

figure b.1: two functions with unique global minima

485

42024x10203040(x2)2+3201001020x01020|x|2cos(x)486

derivatives are,

appendix b. numerical optimization

   d
   x1
   d
   x2

= x1     2
= x2     1

[b.2]

[b.3]

the unique minimum is x    = [2, 1](cid:62).

for non-convex functions, critical points are not necessarily global minima. a local
minimum x    is a point at which the function takes a smaller value than at all nearby
neighbors: formally, x    is a local minimum if there is some positive   such that f (x   )    
f (x) for all x within distance   of x   . figure b.1b shows the function f (x) = |x|    2 cos(x),
which has many local minima, as well as a unique global minimum at x = 0. a critical
point may also be the local or global maximum of the function; it may be a saddle point,
which is a minimum with respect to at least one coordinate, and a maximum with respect
at least one other coordinate; it may be an in   ection point, which is neither or a minimum
nor maximum. when available, the second derivative of f can help to distinguish these
cases.

b.1 id119
for many convex functions, it is not possible to solve for x    in closed form. in gradient
descent, we compute a series of solutions, x(0), x(1), . . . by taking steps along the local
gradient    x(t)f, which is the vector of partial derivatives of the function f, evaluated at
the point x(t). each solution x(t+1) is computed,

x(t+1)    x(t)       (t)   x(t)f.

[b.4]

where   (t) > 0 is a step size. if the step size is chosen appropriately, this procedure will
   nd the global minimum of a differentiable convex function. for non-convex functions,
id119 will    nd a local minimum. the extension to non-differentiable convex
functions is discussed in    2.4.
b.2 constrained optimization

optimization must often be performed under constraints: for example, when optimizing
the parameters of a id203 distribution, the probabilities of all events must sum to
one. constrained optimization problems can be written,

x

f (x)

min
s.t. gc(x)     0,

   c = 1, 2, . . . , c

[b.5]

[b.6]

jacob eisenstein. draft of november 13, 2018.

b.3. example: passive-aggressive online learning

487

where each gc(x) is a scalar function of x. for example, suppose that x must be non-
negative, and that its sum cannot exceed a budget b. then there are d + 1 inequality
constraints,

gi(x) =     xi,
gd+1(x) =     b +

   i = 1, 2, . . . , d

xi.

d(cid:88)i=1

[b.7]

[b.8]

inequality constraints can be combined with the original objective function f by form-

ing a lagrangian,

l(x,   ) = f (x) +

  cgc(x),

[b.9]

where   c is a lagrange multiplier. for any lagrangian, there is a corresponding dual
form, which is a function of   :

the lagrangian l can be referred to as the primal form.

d(  ) = min

x

l(x,   ).

[b.10]

b.3 example: passive-aggressive online learning

sometimes it is possible to solve a constrained optimization problem by manipulating the
lagrangian. one example is maximum-likelihood estimation of a na    ve bayes id203
model, as described in    2.2.3. in that case, it is unnecessary to explicitly compute the
lagrange multiplier. another example is illustrated by the passive-aggressive algorithm
for online learning (crammer et al., 2006). this algorithm is similar to the id88, but
the goal at each step is to make the most conservative update that gives zero margin loss
on the current example.1 each update can be formulated as a constrained optimization
over the weights   :

c(cid:88)c=1

min

1

2||         (i   1)||2

  
s.t. (cid:96)(i)(  ) = 0

[b.11]

[b.12]
where   (i   1) is the previous set of weights, and (cid:96)(i)(  ) is the margin loss on instance i. as
in    2.4.1, this loss is de   ned as,

(cid:96)(i)(  ) = 1           f (x(i), y(i)) + max
y(cid:54)=y(i)

      f (x(i), y).

[b.13]

1this is the basis for the name of the algorithm: it is passive when the loss is zero, but it aggressively

moves to make the loss zero when necessary.

under contract with mit press, shared under cc-by-nc-nd license.

488

appendix b. numerical optimization

when the margin loss is zero for   (i   1), the optimal solution is       =   (i   1), so we will

focus on the case where (cid:96)(i)(  (i   1)) > 0. the lagrangian for this problem is,

l(  ,   ) =

1

2||         (i   1)||2 +   (cid:96)(i)(  ),

holding    constant, we can solve for    by differentiating,

     l =         (i   1) +   
=  (i   1) +     ,

  

   

   
     

(cid:96)(i)(  )

[b.14]

[b.15]

[b.16]

where    = f (x(i), y(i))     f (x(i),   y) and   y = argmaxy(cid:54)=y(i)       f (x(i), y).
the lagrange multiplier    acts as the learning rate in a id88-style update to   .
we can solve for    by plugging       back into the lagrangian, obtaining the dual function,

d(  ) =

1

  2

2||  (i   1) +            (i   1)||2 +   (1     (  (i   1) +     )      )
2 ||  ||2       2||  ||2 +   (1       (i   1)      )
2 ||  ||2 +   (cid:96)(i)(  (i   1)).
=    

  2

=

differentiating and solving for   ,

   d
     
   
  

=       ||  ||2 + (cid:96)(i)(  (i   1))
(cid:96)(i)(  (i   1))

=

.

||  ||2

[b.17]

[b.18]

[b.19]

[b.20]

[b.21]

the complete update equation is therefore:

   

  

=   (i   1) +

(cid:96)(i)(  (i   1))

||f (x(i), y(i))     f (x(i),   y)||2

(f (x(i), y(i))     f (x(i),   y)).

[b.22]

this learning rate makes intuitive sense. the numerator grows with the loss; the denom-
inator grows with the norm of the difference between the feature vectors associated with
the correct and predicted label. if this norm is large, then the step with respect to each
feature should be small, and vice versa.

jacob eisenstein. draft of november 13, 2018.

bibliography

(1992).

(1996).

(1997).

(1998).

(1999).

(2000).

(2001).

(2002).

(2002).

(2003).

(2003).

(2004).

(2004).

(2005).

(2005).

(2006).

(2006).

(2006).

(2007).

489

490

(2007).

(2007).

(2007).

(2008).

(2008).

(2008).

(2009).

(2009).

(2009).

(2009).

(2010).

(2010).

(2010).

(2010).

(2010).

(2011).

(2011).

(2011).

(2011).

(2011).

(2012).

(2012).

(2012).

(2012).

(2012).

bibliography

jacob eisenstein. draft of november 13, 2018.

bibliography

491

(2012).

(2013).

(2013).

(2013).

(2013).

(2013).

(2014).

(2014).

(2014).

(2014).

(2014).

(2014).

(2015).

(2015).

(2015).

(2015).

(2015).

(2015).

(2016).

(2016).

(2016).

(2016).

(2016).

(2016).

(2017).

under contract with mit press, shared under cc-by-nc-nd license.

bibliography

492

(2017).

(2017).

(2017).

(2017).

(2018).

(2018).

(2018).

abadi, m., a. agarwal, p. barham, e. brevdo, z. chen, c. citro, g. s. corrado, a. davis,
j. dean, m. devin, s. ghemawat, i. j. goodfellow, a. harp, g. irving, m. isard, y. jia,
r. j  ozefowicz, l. kaiser, m. kudlur, j. levenberg, d. man  e, r. monga, s. moore,
d. g. murray, c. olah, m. schuster, j. shlens, b. steiner, i. sutskever, k. talwar, p. a.
tucker, v. vanhoucke, v. vasudevan, f. b. vi  egas, o. vinyals, p. warden, m. watten-
berg, m. wicke, y. yu, and x. zheng (2016). tensor   ow: large-scale machine learning
on heterogeneous distributed systems. corr abs/1603.04467.

abend, o. and a. rappoport (2017). the state of the art in semantic representation. see

acl (2017).

abney, s., r. e. schapire, and y. singer (1999). boosting applied to tagging and pp attach-

ment. see emn (1999), pp. 132   134.

abney, s. p. (1987). the english noun phrase in its sentential aspect. ph. d. thesis, mas-

sachusetts institute of technology.

abney, s. p. and m. johnson (1991). memory requirements and local ambiguities of pars-

ing strategies. journal of psycholinguistic research 20(3), 233   250.

adafre, s. f. and m. de rijke (2006). finding similar sentences across multiple languages
in proceedings of the workshop on new text wikis and blogs and other

in wikipedia.
dynamic text sources.

ahn, d. (2006). the stages of event extraction. in proceedings of the workshop on annotating
and reasoning about time and events, pp. 1   8. association for computational linguistics.

aho, a. v., m. s. lam, r. sethi, and j. d. ullman (2006). compilers: principles, techniques,

& tools (2nd ed.). addison-wesley publishing company.

aikhenvald, a. y. (2004). evidentiality. oxford university press.

jacob eisenstein. draft of november 13, 2018.

bibliography

493

akaike, h. (1974). a new look at the statistical model identi   cation. ieee transactions on

automatic control 19(6), 716   723.

akmajian, a., r. a. demers, a. k. farmer, and r. m. harnish (2010). linguistics: an

introduction to language and communication (sixth ed.). cambridge, ma: mit press.

alfano, m., d. hovy, m. mitchell, and m. strube (2018). proceedings of the second acl
in proceedings of the second acl
workshop on ethics in natural language processing.
workshop on ethics in natural language processing. association for computational lin-
guistics.

alfau, f. (1999). chromos. dalkey archive press.

allauzen, c., m. riley, j. schalkwyk, w. skut, and m. mohri (2007). openfst: a gen-
eral and ef   cient weighted    nite-state transducer library. in international conference on
implementation and application of automata, pp. 11   23. springer.

allen, j. f. (1984). towards a general theory of action and time. arti   cial intelligence 23(2),

123   154.

allen, j. f., b. w. miller, e. k. ringger, and t. sikorski (1996). a robust system for natural
spoken dialogue. in proceedings of the association for computational linguistics (acl), pp.
62   70.

allen, j. f., l. k. schubert, g. ferguson, p. heeman, c. h. hwang, t. kato, m. light,
n. martin, b. miller, m. poesio, and d. traum (1995). the trains project: a case
study in building a conversational planning agent. journal of experimental & theoretical
arti   cial intelligence 7(1), 7   48.

alm, c. o., d. roth, and r. sproat (2005). emotions from text: machine learning for

text-based emotion prediction. see emn (2005), pp. 579   586.

alu    sio, s., j. pelizzoni, a. marchi, l. de oliveira, r. manenti, and v. marquiaf  avel (2003).
an account of the challenge of tagging a reference corpus for brazilian portuguese.
computational processing of the portuguese language, 194   194.

anand, p., m. walker, r. abbott, j. e. fox tree, r. bowmani, and m. minor (2011). cats rule
and dogs drool!: classifying stance in online debate. in proceedings of the 2nd workshop
on computational approaches to subjectivity and id31, pp. 1   9. association
for computational linguistics.

anandkumar, a. and r. ge (2016). ef   cient approaches for escaping higher order saddle
points in non-id76. in proceedings of the conference on learning theory
(colt), pp. 81   102.

under contract with mit press, shared under cc-by-nc-nd license.

494

bibliography

anandkumar, a., r. ge, d. hsu, s. m. kakade, and m. telgarsky (2014). tensor decompo-
sitions for learning latent variable models. the journal of machine learning research 15(1),
2773   2832.

ando, r. k. and t. zhang (2005). a framework for learning predictive structures from
multiple tasks and unlabeled data. the journal of machine learning research 6, 1817   
1853.

andor, d., c. alberti, d. weiss, a. severyn, a. presta, k. ganchev, s. petrov, and
m. collins (2016). globally normalized transition-based neural networks. see acl (2016),
pp. 2442   2452.

angeli, g., p. liang, and d. klein (2010). a simple domain-independent probabilistic

approach to generation. see emn (2010), pp. 502   512.

antol, s., a. agrawal, j. lu, m. mitchell, d. batra, c. lawrence zitnick, and d. parikh

(2015). vqa: visual id53. see icc (2015), pp. 2425   2433.

aronoff, m. (1976). word formation in generative grammar. mit press.

arora, s. and b. barak (2009). computational complexity: a modern approach. cambridge

university press.

arora, s., r. ge, y. halpern, d. mimno, a. moitra, d. sontag, y. wu, and m. zhu (2013).
a practical algorithm for id96 with provable guarantees. see icm (2013), pp.
280   288.

arora, s., y. li, y. liang, t. ma, and a. risteski (2018). id202ic structure of word
senses, with applications to polysemy. transactions of the association of computational
linguistics 6, 483   495.

artstein, r. and m. poesio (2008). inter-coder agreement for computational linguistics.

computational linguistics 34(4), 555   596.

artzi, y. and l. zettlemoyer (2013). weakly supervised learning of semantic parsers for
mapping instructions to actions. transactions of the association for computational linguis-
tics 1, 49   62.

attardi, g. (2006). experiments with a multilanguage non-projective dependency parser.

in proceedings of the conference on natural language learning (conll), pp. 166   170.

auer, p. (2013). code-switching in conversation: language, interaction and identity. routledge.

auer, s., c. bizer, g. kobilarov, j. lehmann, r. cyganiak, and z. ives (2007). dbpedia: a

nucleus for a web of open data. the semantic web, 722   735.

jacob eisenstein. draft of november 13, 2018.

bibliography

495

austin, j. l. (1962). how to do things with words. oxford university press.

aw, a., m. zhang, j. xiao, and j. su (2006). a phrase-based statistical model for sms text

id172. see acl (2006), pp. 33   40.

ba, j. l., j. r. kiros, and g. e. hinton (2016). layer id172.

arxiv:1607.06450.

arxiv preprint

bagga, a. and b. baldwin (1998a). algorithms for scoring coreference chains. in proceed-

ings of the language resources and evaluation conference, pp. 563   566.

bagga, a. and b. baldwin (1998b). entity-based cross-document coreferencing using the

vector space model. see col (1998), pp. 79   85.

bahdanau, d., k. cho, and y. bengio (2014). id4 by jointly learn-

ing to align and translate. see nip (2014).

baldwin, t. and s. n. kim (2010). multiword expressions. in handbook of natural language

processing, volume 2, pp. 267   292. boca raton, usa: crc press.

balle, b., a. quattoni, and x. carreras (2011). a spectral learning algorithm for    nite state
transducers. in proceedings of the european conference on machine learning and principles
and practice of knowledge discovery in databases (ecml), pp. 156   171.

banarescu, l., c. bonial, s. cai, m. georgescu, k. grif   tt, u. hermjakob, k. knight,
p. koehn, m. palmer, and n. schneider (2013, august). abstract meaning represen-
tation for sembanking. in proceedings of the 7th linguistic annotation workshop and in-
teroperability with discourse, so   a, bulgaria, pp. 178   186. association for computational
linguistics.

banko, m., m. j. cafarella, s. soderland, m. broadhead, and o. etzioni (2007). open

information extraction from the web. see ijc (2007), pp. 2670   2676.

bansal, n., a. blum, and s. chawla (2004). correlation id91. machine learning 56(1-

3), 89   113.

barber, d. (2012). bayesian reasoning and machine learning. cambridge university press.

barman, u., a. das, j. wagner, and j. foster (2014). code mixing: a challenge for language
in proceedings of the first workshop on
identi   cation in the language of social media.
computational approaches to code switching, pp. 13   23. association for computational
linguistics.

baron, a. and p. rayson (2008). vard2: a tool for dealing with spelling variation in his-

torical corpora. in postgraduate conference in corpus linguistics.

under contract with mit press, shared under cc-by-nc-nd license.

496

bibliography

baroni, m., r. bernardi, and r. zamparelli (2014). frege in space: a program for compo-

sitional id65. linguistic issues in language technologies.

barzilay, r. and m. lapata (2008). modeling local coherence: an entity-based approach.

computational linguistics 34(1), 1   34.

barzilay, r. and k. r. mckeown (2005). sentence fusion for multidocument news summa-

rization. computational linguistics 31(3), 297   328.

beesley, k. r. and l. karttunen (2003). finite-state morphology. stanford, ca: center for

the study of language and information.

bejan, c. a. and s. harabagiu (2014). unsupervised event coreference resolution. compu-

tational linguistics 40(2), 311   347.

bell, e. t. (1934). exponential numbers. the american mathematical monthly 41(7), 411   419.

bender, e. m. (2013). linguistic fundamentals for natural language processing: 100 essentials
from morphology and syntax, volume 6 of synthesis lectures on human language technolo-
gies. morgan & claypool publishers.

bengio, s., o. vinyals, n. jaitly, and n. shazeer (2015). scheduled sampling for sequence

prediction with recurrent neural networks. see nip (2015), pp. 1171   1179.

bengio, y., r. ducharme, p. vincent, and c. janvin (2003). a neural probabilistic language

model. the journal of machine learning research 3, 1137   1155.

bengio, y., p. simard, and p. frasconi (1994). learning long-term dependencies with gra-

dient descent is dif   cult. ieee transactions on neural networks 5(2), 157   166.

bengtson, e. and d. roth (2008). understanding the value of features for coreference

resolution. see emn (2008), pp. 294   303.

benjamini, y. and y. hochberg (1995). controlling the false discovery rate: a practical and
journal of the royal statistical society. series b

powerful approach to multiple testing.
(methodological), 289   300.

berant, j., a. chou, r. frostig, and p. liang (2013). id29 on freebase from

question-answer pairs. see emn (2013), pp. 1533   1544.

berant, j., v. srikumar, p.-c. chen, a. vander linden, b. harding, b. huang, p. clark, and
c. d. manning (2014). modeling biological processes for reading comprehension. see
emn (2014).

berg-kirkpatrick, t., a. bouchard-c  ot  e, j. denero, and d. klein (2010). painless unsuper-

vised learning with features. see naa (2010), pp. 582   590.

jacob eisenstein. draft of november 13, 2018.

bibliography

497

berg-kirkpatrick, t., d. burkett, and d. klein (2012). an empirical investigation of statis-

tical signi   cance in nlp. see emn (2012), pp. 995   1005.

berger, a. l., v. j. d. pietra, and s. a. d. pietra (1996). a maximum id178 approach to

natural language processing. computational linguistics 22(1), 39   71.

bergsma, s., d. lin, and r. goebel (2008). distributional identi   cation of non-referential

pronouns. see acl (2008), pp. 10   18.

bernardi, r., r. cakici, d. elliott, a. erdem, e. erdem, n. ikizler-cinbis, f. keller, a. mus-
cat, and b. plank (2016). automatic description generation from images: a survey of
models, datasets, and evaluation measures. journal of arti   cial intelligence research 55,
409   442.

bertsekas, d. p. (2012). incremental gradient, subgradient, and proximal methods for con-
vex optimization: a survey. in s. sra, s. nowozin, and s. j. wright (eds.), optimization
for machine learning. mit press.

bhatia, p., r. guthrie, and j. eisenstein (2016). morphological priors for probabilistic neu-

ral id27s. see emn (2016).

bhatia, p., y. ji, and j. eisenstein (2015). better document-level id31 from rst

discourse parsing. see emn (2015).

biber, d. (1991). variation across speech and writing. cambridge university press.

bird, s., e. klein, and e. loper (2009). natural language processing with python. california:

o   reilly media.

bishop, c. m. (2006). pattern recognition and machine learning. springer.

bj  orkelund, a. and p. nugues (2011). exploring lexicalized features for coreference reso-

lution. see con (2011), pp. 45   50.

blackburn, p. and j. bos (2005). representation and id136 for natural language: a    rst

course in computational semantics. csli.

blei, d. m. (2012). probabilistic topic models. communications of the acm 55(4), 77   84.

blei, d. m. (2014). build, compute, critique, repeat: data analysis with latent variable

models. annual review of statistics and its application 1, 203   232.

blei, d. m., a. y. ng, and m. i. jordan (2003). id44. the journal of

machine learning research 3, 993   1022.

blitzer, j., m. dredze, and f. pereira (2007). biographies, bollywood, boom-boxes and

blenders: id20 for sentiment classi   cation. see acl (2007), pp. 440   447.

under contract with mit press, shared under cc-by-nc-nd license.

498

bibliography

blum, a. and t. mitchell (1998). combining labeled and unlabeled data with co-training.

in proceedings of the conference on learning theory (colt), pp. 92   100.

bobrow, d. g., r. m. kaplan, m. kay, d. a. norman, h. thompson, and t. winograd

(1977). gus, a frame-driven dialog system. arti   cial intelligence 8(2), 155   173.

bohnet, b. (2010). very high accuracy and fast id33 is not a contradiction.

see col (2010), pp. 89   97.

boitet, c. (1988). pros and cons of the pivot and transfer approaches in multilingual ma-

chine translation. readings in machine translation, 273   279.

bojanowski, p., e. grave, a. joulin, and t. mikolov (2017). enriching word vectors with
subword information. transactions of the association for computational linguistics 5, 135   
146.

bollacker, k., c. evans, p. paritosh, t. sturge, and j. taylor (2008). freebase: a collabora-
tively created graph database for structuring human knowledge. in proceedings of the
acm international conference on management of data (sigmod), pp. 1247   1250. acm.

bolukbasi, t., k.-w. chang, j. y. zou, v. saligrama, and a. t. kalai (2016). man is to
computer programmer as woman is to homemaker? debiasing id27s. in
neural information processing systems (nips), pp. 4349   4357.

bordes, a., n. usunier, a. garcia-duran, j. weston, and o. yakhnenko (2013). translating

embeddings for modeling multi-relational data. see nip (2013), pp. 2787   2795.

bordes, a., j. weston, r. collobert, y. bengio, et al. (2011). learning structured embed-
dings of knowledge bases. in proceedings of the national conference on arti   cial intelligence
(aaai), pp. 301   306.

borges, j. l. (1993). other inquisitions 1937   1952. university of texas press. translated by

ruth l. c. simms.

botha, j. a. and p. blunsom (2014). compositional morphology for word representations

and language modelling. see icm (2014).

bottou, l. (2012). stochastic id119 tricks. in neural networks: tricks of the trade,

pp. 421   436. springer.

bottou, l., f. e. curtis, and j. nocedal (2016). optimization methods for large-scale ma-

chine learning. arxiv preprint arxiv:1606.04838.

bowman, s. r., l. vilnis, o. vinyals, a. dai, r. jozefowicz, and s. bengio (2016). gen-
erating sentences from a continuous space. in proceedings of the conference on natural
language learning (conll), pp. 10   21.

jacob eisenstein. draft of november 13, 2018.

bibliography

499

boyd, d. and k. crawford (2012). critical questions for big data. information, communica-

tion & society 15(5), 662   679.

boyd, s. and l. vandenberghe (2004). id76. new york: cambridge uni-

versity press.

boydstun, a. e. (2013). making the news: politics, the media, and agenda setting. university

of chicago press.

branavan, s., h. chen, j. eisenstein, and r. barzilay (2009). learning document-level
journal of arti   cial intelligence re-

semantic properties from free-text annotations.
search 34(2), 569   603.

branavan, s. r., h. chen, l. s. zettlemoyer, and r. barzilay (2009). id23

for mapping instructions to actions. see acl (2009), pp. 82   90.

brants, t. and a. franz (2006). the google 1t 5-gram corpus. ldc2006t13.

braud, c., o. lacroix, and a. s  gaard (2017). does syntax help discourse segmentation?

not so much. see emn (2017), pp. 2432   2442.

brennan, s. e., m. w. friedman, and c. j. pollard (1987). a centering approach to pro-
nouns. in proceedings of the association for computational linguistics (acl), pp. 155   162.

briscoe, t.

(2011).

introduction to formal

semantics

for natural

language.

https://www.cl.cam.ac.uk/teaching/1011/l107/semantics.pdf.

brown, p. f., j. cocke, s. a. d. pietra, v. j. d. pietra, f. jelinek, j. d. lafferty, r. l. mercer,
and p. s. roossin (1990). a statistical approach to machine translation. computational
linguistics 16(2), 79   85.

brown, p. f., p. v. desouza, r. l. mercer, v. j. d. pietra, and j. c. lai (1992). class-based

id165 models of natural language. computational linguistics 18(4), 467   479.

brown, p. f., v. j. d. pietra, s. a. d. pietra, and r. l. mercer (1993). the mathematics
of id151: parameter estimation. computational linguistics 19(2),
263   311.

brun, c. and c. roux (2014). d  ecomposition des    hash tags    pour l   am  elioration de la
classi   cation en polarit  e des    tweets   . proceedings of traitement automatique des langues
naturelles, 473   478.

bruni, e., n.-k. tran, and m. baroni (2014). multimodal id65. journal

of arti   cial intelligence research 49(2014), 1   47.

under contract with mit press, shared under cc-by-nc-nd license.

500

bibliography

brutzkus, a., a. globerson, e. malach, and s. shalev-shwartz (2018). sgd learns over-
parameterized networks that provably generalize on linearly separable data. see icl
(2018).

bullinaria, j. a. and j. p. levy (2007). extracting semantic representations from word co-

occurrence statistics: a computational study. behavior research methods 39(3), 510   526.

bunescu, r. c. and r. j. mooney (2005). a shortest path dependency kernel for relation

extraction. see emn (2005), pp. 724   731.

bunescu, r. c. and m. pasca (2006). using encyclopedic knowledge for named entity

disambiguation. see eac (2006), pp. 9   16.

burstein, j., d. marcu, and k. knight (2003). finding the write stuff: automatic identi-

   cation of discourse structure in student essays. ieee intelligent systems 18(1), 32   39.

burstein, j., j. tetreault, and s. andreyev (2010). using entity-based features to model
coherence in student essays. in human language technologies: the 2010 annual conference
of the north american chapter of the association for computational linguistics, pp. 681   684.
association for computational linguistics.

burstein, j., j. tetreault, and m. chodorow (2013). holistic discourse coherence annotation

for noisy essay writing. dialogue & discourse 4(2), 34   52.

cai, q. and a. yates (2013). large-scale id29 via schema matching and lexicon

extension. see acl (2013), pp. 423   433.

caliskan, a., j. j. bryson, and a. narayanan (2017). semantics derived automatically from

language corpora contain human-like biases. science 356(6334), 183   186.

canny, j. (1987). a computational approach to edge detection. in readings in computer

vision, pp. 184   203. elsevier.

capp  e, o. and e. moulines (2009). on-line expectation   maximization algorithm for latent
data models. journal of the royal statistical society: series b (statistical methodology) 71(3),
593   613.

carbonell, j. and j. goldstein (1998). the use of mmr, diversity-based reranking for re-
ordering documents and producing summaries. in proceedings of acm sigir conference
on research and development in information retrieval.

carbonell, j. r. (1970). mixed-initiative man-computer instructional dialogues. technical

report, bolt beranek and newman.

cardie, c. and k. wagstaff (1999). noun phrase coreference as id91. see emn (1999),

pp. 82   89.

jacob eisenstein. draft of november 13, 2018.

bibliography

501

carletta, j. (1996). assessing agreement on classi   cation tasks: the kappa statistic. com-

putational linguistics 22(2), 249   254.

carletta, j. (2007). unleashing the killer corpus: experiences in creating the multi-

everything ami meeting corpus. language resources and evaluation 41(2), 181   190.

carlson, l. and d. marcu (2001). discourse tagging reference manual. technical report

isi-tr-545, information sciences institute.

carlson, l., m. e. okurowski, and d. marcu (2002). rst discourse treebank. linguistic

data consortium, university of pennsylvania.

carpenter, b. (1997). type-logical semantics. cambridge, ma: mit press.

carreras, x., m. collins, and t. koo (2008). tag, id145, and the percep-
tron for ef   cient, feature-rich parsing. in proceedings of the conference on natural language
learning (conll), pp. 9   16.

carreras, x. and l. m`arquez (2005). introduction to the conll-2005 shared task: semantic
role labeling. in proceedings of the ninth conference on computational natural language
learning, pp. 152   164. association for computational linguistics.

carroll, l. (1865). alice   s adventures in wonderland. london: macmillan.

carroll, l. (1917). through the looking glass: and what alice found there. chicago: rand,

mcnally.

chambers, n. and d. jurafsky (2008).

jointly combining implicit constraints improves

temporal ordering. see emn (2008), pp. 698   706.

chang, k.-w., a. krishnamurthy, a. agarwal, h. daume iii, and j. langford (2015).

learning to search better than your teacher. see icm (2015).

chang, m.-w., l. ratinov, and d. roth (2007). guiding semi-supervision with constraint-

driven learning. see acl (2007), pp. 280   287.

chang, m.-w., l.-a. ratinov, n. rizzolo, and d. roth (2008). learning and id136 with
constraints. in proceedings of the national conference on arti   cial intelligence (aaai), pp.
1513   1518.

chapman, w. w., w. bridewell, p. hanbury, g. f. cooper, and b. g. buchanan (2001). a
simple algorithm for identifying negated    ndings and diseases in discharge summaries.
journal of biomedical informatics 34(5), 301   310.

charniak, e. (1997). statistical techniques for natural language parsing. ai magazine 18(4),

33   43.

under contract with mit press, shared under cc-by-nc-nd license.

502

bibliography

charniak, e. and m. johnson (2005). coarse-to-   ne n-best parsing and maxent discrimi-

native reranking. see acl (2005), pp. 173   180.

chelba, c. and a. acero (2006). adaptation of maximum id178 capitalizer: little data

can help a lot. computer speech & language 20(4), 382   399.

chelba, c., t. mikolov, m. schuster, q. ge, t. brants, p. koehn, and t. robinson (2013).
one billion word benchmark for measuring progress in statistical id38.
arxiv preprint arxiv:1312.3005.

chen, d., j. bolton, and c. d. manning (2016). a thorough examination of the id98/daily

mail reading comprehension task. see acl (2016).

chen, d. and c. d. manning (2014). a fast and accurate dependency parser using neural

networks. see emn (2014), pp. 740   750.

chen, d. l. and r. j. mooney (2008). learning to sportscast: a test of grounded language

acquisition. see icm (2008), pp. 128   135.

chen, h., s. branavan, r. barzilay, and d. r. karger (2009). content modeling using latent

permutations. journal of arti   cial intelligence research 36(1), 129   163.

chen, m., z. xu, k. weinberger, and f. sha (2012). marginalized denoising autoencoders

for id20. see icm (2012).

chen, m. x., o. firat, a. bapna, m. johnson, w. macherey, g. foster, l. jones, n. parmar,
m. schuster, z. chen, y. wu, and m. hughes (2018). the best of both worlds: combining
recent advances in id4. see acl (2018).

chen, s. f. and j. goodman (1999). an empirical study of smoothing techniques for lan-

guage modeling. computer speech & language 13(4), 359   393.

chen, t. and c. guestrin (2016). xgboost: a scalable tree boosting system. in proceedings

of knowledge discovery and data mining (kdd), pp. 785   794.

chen, x., x. qiu, c. zhu, p. liu, and x. huang (2015). long short-term memory neural

networks for chinese id40. see emn (2015), pp. 1197   1206.

chen, y., s. gilroy, a. malletti, k. knight, and j. may (2018). recurrent neural networks

as weighted language recognizers. see naa (2018).

chen, z. and h. ji (2009). graph-based event coreference resolution.

in proceedings of
the 2009 workshop on graph-based methods for natural language processing, pp. 54   57.
association for computational linguistics.

jacob eisenstein. draft of november 13, 2018.

bibliography

503

cheng, x. and d. roth (2013). relational id136 for wiki   cation. see emn (2013), pp.

1787   1796.

chiang, d. (2007). hierarchical phrase-based translation. computational linguistics 33(2),

201   228.

chiang, d., j. graehl, k. knight, a. pauls, and s. ravi (2010). bayesian id136 for

   nite-state transducers. see naa (2010), pp. 447   455.

chinchor, n. and p. robinson (1997). muc-7 named entity task de   nition. in proceedings

of the 7th conference on message understanding, volume 29.

cho, k. (2015). natural

corr abs/1511.07916.

language understanding with distributed representation.

cho, k., b. van merri  enboer, c. gulcehre, d. bahdanau, f. bougares, h. schwenk, and
y. bengio (2014). learning phrase representations using id56 encoder-decoder for statis-
tical machine translation. see emn (2014).

chomsky, n. (1957). syntactic structures. the hague: mouton & co.

chomsky, n. (1982). some concepts and consequences of the theory of government and binding,

volume 6. mit press.

choromanska, a., m. henaff, m. mathieu, g. b. arous, and y. lecun (2015). the loss
surfaces of multilayer networks. in proceedings of arti   cial intelligence and statistics (ais-
tats), pp. 192   204.

christodoulopoulos, c., s. goldwater, and m. steedman (2010). two decades of unsuper-

vised pos induction: how far have we come? see emn (2010), pp. 575   584.

chu, y.-j. and t.-h. liu (1965). on shortest arborescence of a directed graph. scientia

sinica 14(10), 1396   1400.

chung, c. and j. w. pennebaker (2007). the psychological functions of function words.
in k. fiedler (ed.), social communication, pp. 343   359. new york and hove: psychology
press.

church, k. (2011). a pendulum swung too far. linguistic issues in language technology 6(5),

1   27.

church, k. w. (2000). empirical estimates of adaptation: the chance of two noriegas is

closer to p/2 than p2. see col (2000), pp. 180   186.

church, k. w. and p. hanks (1990). word association norms, mutual information, and

id69. computational linguistics 16(1), 22   29.

under contract with mit press, shared under cc-by-nc-nd license.

504

bibliography

ciaramita, m. and m. johnson (2003). supersense tagging of unknown nouns in id138.
in proceedings of empirical methods for natural language processing (emnlp), pp. 168   
175.

clark, k. and c. d. manning (2015). entity-centric coreference resolution with model

stacking. see acl (2015), pp. 1405   1415.

clark, k. and c. d. manning (2016). improving coreference resolution by learning entity-

level distributed representations. see acl (2016).

clark, p. (2015). elementary school science and math tests as a driver for ai: take the aristo
in proceedings of the national conference on arti   cial intelligence (aaai), pp.

challenge!
4019   4021.

clarke, j., d. goldwasser, m.-w. chang, and d. roth (2010). driving id29

from the world   s response. see con (2010), pp. 18   27.

clarke, j. and m. lapata (2008). global id136 for sentence compression: an integer

id135 approach. journal of arti   cial intelligence research 31, 399   429.

cohen, j. (1960). a coef   cient of agreement for nominal scales. educational and psychologi-

cal measurement 20(1), 37   46.

cohen, s. (2016). bayesian analysis in natural language processing. synthesis lectures on

human language technologies. san rafael, ca: morgan & claypool publishers.

cohen, s. b., k. stratos, m. collins, d. p. foster, and l. ungar (2014). spectral learning of
latent-variable pid18s: algorithms and sample complexity. journal of machine learning
research 15, 2399   2449.

collier, n., c. nobata, and j.-i. tsujii (2000). extracting the names of genes and gene

products with a hidden markov model. see col (2000), pp. 201   207.

collins, m. (1997). three generative, lexicalised models for statistical parsing. see acl

(1997), pp. 16   23.

collins, m. (2002). discriminative training methods for id48: theory

and experiments with id88 algorithms. see emn (2002), pp. 1   8.

collins, m. and t. koo (2005). discriminative reranking for natural language parsing.

computational linguistics 31(1), 25   70.

collins, m. and b. roark (2004). incremental parsing with the id88 algorithm. see

acl (2004).

jacob eisenstein. draft of november 13, 2018.

bibliography

505

collobert, r., k. kavukcuoglu, and c. farabet (2011). torch7: a matlab-like environment

for machine learning. technical report epfl-conf-192376, epfl.

collobert, r. and j. weston (2008). a uni   ed architecture for natural language processing:

deep neural networks with multitask learning. see icm (2008), pp. 160   167.

collobert, r., j. weston, l. bottou, m. karlen, k. kavukcuoglu, and p. kuksa (2011). nat-
ural language processing (almost) from scratch. journal of machine learning research 12,
2493   2537.

colton, s., j. goodwin, and t. veale (2012). full-face poetry generation. in proceedings of

the international conference on computational creativity, pp. 95   102.

conneau, a., d. kiela, h. schwenk, l. barrault, and a. bordes (2017). supervised learning
of universal sentence representations from natural language id136 data. see emn
(2017), pp. 681   691.

cormen, t. h., c. e. leiserson, r. l. rivest, and c. stein (2009). introduction to algorithms

(third ed.). mit press.

cotterell, r., h. sch   utze, and j. eisner (2016). morphological smoothing and extrapolation

of id27s. see acl (2016), pp. 1651   1660.

coviello, l., y. sohn, a. d. kramer, c. marlow, m. franceschetti, n. a. christakis, and
j. h. fowler (2014). detecting emotional contagion in massive social networks. plos
one 9(3), e90315.

covington, m. a. (2001). a fundamental algorithm for id33. in proceedings

of the 39th annual acm southeast conference, pp. 95   102.

crammer, k., o. dekel, j. keshet, s. shalev-shwartz, and y. singer (2006, december).
online passive-aggressive algorithms. the journal of machine learning research 7, 551   
585.

crammer, k. and y. singer (2001). pranking with ranking. in neural information processing

systems (nips), pp. 641   647.

crammer, k. and y. singer (2003). ultraconservative online algorithms for multiclass

problems. the journal of machine learning research 3, 951   991.

creutz, m. and k. lagus (2007). unsupervised models for morpheme segmentation and
morphology learning. acm transactions on speech and language processing (tslp) 4(1),
3.

cross, j. and l. huang (2016). span-based constituency parsing with a structure-label

system and provably optimal dynamic oracles. see emn (2016), pp. 1   11.

under contract with mit press, shared under cc-by-nc-nd license.

506

bibliography

cucerzan, s. (2007). large-scale named entity disambiguation based on wikipedia data.

see emn (2007).

cui, h., r. sun, k. li, m.-y. kan, and t.-s. chua (2005). id53 passage
retrieval using dependency relations. in proceedings of acm sigir conference on research
and development in information retrieval, pp. 400   407.

cui, y., z. chen, s. wei, s. wang, t. liu, and g. hu (2017). attention-over-attention neural

networks for reading comprehension. see acl (2017).

culotta, a. and j. sorensen (2004). dependency tree kernels for id36. see

acl (2004).

culotta, a., m. wick, and a. mccallum (2007). first-order probabilistic models for coref-

erence resolution. see naa (2007), pp. 81   88.

curry, h. b. and r. feys (1958). combinatory logic, volume i. amsterdam: north holland.

danescu-niculescu-mizil, c., m. sudhof, d. jurafsky, j. leskovec, and c. potts (2013). a
computational approach to politeness with application to social factors. see acl (2013),
pp. 250   259.

das, d., d. chen, a. f. martins, n. schneider, and n. a. smith (2014). frame-semantic

parsing. computational linguistics 40(1), 9   56.

daum  e iii, h. (2007). frustratingly easy id20. see acl (2007).

daum  e iii, h., j. langford, and d. marcu (2009). search-based id170.

machine learning 75(3), 297   325.

daum  e iii, h. and d. marcu (2005). a large-scale exploration of effective global features

for a joint entity detection and tracking model. see emn (2005), pp. 97   104.

dauphin, y. n., r. pascanu, c. gulcehre, k. cho, s. ganguli, and y. bengio (2014). iden-
tifying and attacking the saddle point problem in high-dimensional non-convex opti-
mization. see nip (2014), pp. 2933   2941.

davidson, d. (1967). the logical form of action sentences. in n. rescher (ed.), the logic of

decision and action. pittsburgh: university of pittsburgh press.

de gispert, a. and j. b. marino (2006). catalan-english id151
without parallel corpus: bridging through spanish. in proc. of 5th international conference
on language resources and evaluation (lrec), pp. 65   68. citeseer.

jacob eisenstein. draft of november 13, 2018.

bibliography

507

de marneffe, m.-c. and c. d. manning (2008). the stanford typed dependencies represen-
tation. in coling 2008: proceedings of the workshop on cross-framework and cross-domain
parser evaluation, pp. 1   8. association for computational linguistics.

dean, j. and s. ghemawat (2008). mapreduce: simpli   ed data processing on large clusters.

communications of the acm 51(1), 107   113.

deerwester, s. c., s. t. dumais, t. k. landauer, g. w. furnas, and r. a. harshman (1990).
indexing by latent semantic analysis. journal of the american society for information sci-
ence 41(6), 391   407.

dehdari, j. (2014). a neurophysiologically-inspired statistical language model. ph. d. thesis,

the ohio state university.

deisenroth, m. p., a. a. faisal, and c. s. ong (2018). mathematics for machine learning.

cambridge university press.

dempster, a. p., n. m. laird, and d. b. rubin (1977). maximum likelihood from incom-
plete data via the em algorithm. journal of the royal statistical society. series b (method-
ological), 1   38.

denis, p. and j. baldridge (2007). a ranking approach to pronoun resolution. see ijc (2007).

denis, p. and j. baldridge (2008). specialized models and ranking for coreference resolu-

tion. see emn (2008), pp. 660   669.

denis, p. and j. baldridge (2009). global joint models for coreference resolution and named

entity classi   cation. procesamiento del lenguaje natural 42.

derrida, j. (1985). des tours de babel. in j. graham (ed.), difference in translation. ithaca,

ny: cornell university press.

dhingra, b., h. liu, z. yang, w. w. cohen, and r. salakhutdinov (2017). gated-attention

readers for text comprehension. see acl (2017).

diaconis, p. and b. skyrms (2017). ten great ideas about chance. princeton university

press.

dietterich, t. g. (1998). approximate statistical tests for comparing supervised classi   ca-

tion learning algorithms. neural computation 10(7), 1895   1923.

dietterich, t. g., r. h. lathrop, and t. lozano-p  erez (1997). solving the multiple instance

problem with axis-parallel rectangles. arti   cial intelligence 89(1), 31   71.

under contract with mit press, shared under cc-by-nc-nd license.

508

bibliography

dimitrova, l., n. ide, v. petkevic, t. erjavec, h. j. kaalep, and d. tu   s (1998). multext-
east: parallel and comparable corpora and lexicons for six central and eastern european
languages. see col (1998), pp. 315   319.

doddington, g. r., a. mitchell, m. a. przybocki, l. a. ramshaw, s. strassel, and r. m.
weischedel (2004). the automatic content extraction (ace) program-tasks, data, and
evaluation. in proceedings of the language resources and evaluation conference, pp. 837   
840.

dos santos, c., b. xiang, and b. zhou (2015). classifying relations by ranking with convo-

lutional neural networks. see acl (2015), pp. 626   634.

dowty, d. (1991). thematic proto-roles and argument selection. language, 547   619.

dredze, m., p. mcnamee, d. rao, a. gerber, and t. finin (2010). entity disambiguation

for knowledge base population. see col (2010), pp. 277   285.

dredze, m., m. j. paul, s. bergsma, and h. tran (2013). carmen: a twitter geolocation
system with applications to public health. in aaai workshop on expanding the boundaries
of health informatics using ai (hiai), pp. 20   24.

dreyfus, h. l. (1992). what computers still can   t do: a critique of arti   cial reason. mit press.

dror, r., g. baumer, m. bogomolov, and r. reichart (2017). replicability analysis for
natural language processing: testing signi   cance with multiple datasets. transactions of
the association for computational linguistics 5, 471   486.

dror, r., g. baumer, s. shlomov, and r. reichart (2018). the hitchhiker   s guide to testing

statistical signi   cance in natural language processing. see acl (2018), pp. 1383   1392.

du, l., w. buntine, and m. johnson (2013). topic segmentation with a structured topic

model. see naa (2013), pp. 190   200.

duchi, j., e. hazan, and y. singer (2011). adaptive subgradient methods for online learn-
ing and stochastic optimization. the journal of machine learning research 12, 2121   2159.

dunietz, j., l. levin, and j. carbonell (2017). the because corpus 2.0: annotating causality

and overlapping relations. in proceedings of the linguistic annotation workshop.

durrett, g., t. berg-kirkpatrick, and d. klein (2016). learning-based single-document
summarization with compression and anaphoricity constraints. see acl (2016), pp.
1998   2008.

durrett, g. and d. klein (2013). easy victories and uphill battles in coreference resolution.

see emn (2013).

jacob eisenstein. draft of november 13, 2018.

bibliography

509

durrett, g. and d. klein (2015). neural crf parsing. see acl (2015).

dyer, c., m. ballesteros, w. ling, a. matthews, and n. a. smith (2015). transition-based

id33 with stack long short-term memory. see acl (2015), pp. 334   343.

dyer, c., a. kuncoro, m. ballesteros, and n. a. smith (2016). recurrent neural network

grammars. see naa (2016), pp. 199   209.

edmonds, j. (1967). optimum branchings.

standards b 71(4), 233   240.

journal of research of the national bureau of

efron, b. and r. j. tibshirani (1993). an introduction to the bootstrap: monographs on
statistics and applied id203, vol. 57. new york and london: chapman and hall/crc.

eisenstein, j. (2009). hierarchical text segmentation from multi-scale lexical cohesion. see

naa (2009).

eisenstein, j. and r. barzilay (2008). bayesian unsupervised topic segmentation. see emn

(2008).

eisner, j. (1997). state-of-the-art algorithms for minimum spanning trees: a tutorial dis-
cussion. https://www.cs.jhu.edu/  jason/papers/eisner.mst-tutorial.
pdf.

eisner, j. (2000). bilexical grammars and their cubic-time parsing algorithms. in advances

in probabilistic and other parsing technologies, pp. 29   61. springer.

eisner, j. (2002). parameter estimation for probabilistic    nite-state transducers. see acl

(2002), pp. 1   8.

eisner, j. (2016). inside-outside and forward-backward algorithms are just backprop. in

proceedings of the workshop on id170 for nlp, pp. 1   17.

eisner, j. m. (1996). three new probabilistic models for id33: an explo-

ration. see col (1996), pp. 340   345.

ekman, p. (1992). are there basic emotions? psychological review 99(3), 550   553.

elman, j. l. (1990). finding structure in time. cognitive science 14(2), 179   211.

elman, j. l., e. a. bates, m. h. johnson, a. karmiloff-smith, d. parisi, and k. plunkett
(1998). rethinking innateness: a connectionist perspective on development, volume 10. mit
press.

elsner, m. and e. charniak (2010). disentangling chat. computational linguistics 36(3),

389   409.

under contract with mit press, shared under cc-by-nc-nd license.

510

bibliography

esuli, a. and f. sebastiani (2006). sentiid138: a publicly available lexical resource for
opinion mining. in proceedings of the language resources and evaluation conference, pp.
417   422.

etzioni, o., a. fader, j. christensen, s. soderland, and m. mausam (2011). open informa-

tion extraction: the second generation. see ijc (2011), pp. 3   10.

faruqui, m., j. dodge, s. k. jauhar, c. dyer, e. hovy, and n. a. smith (2015). retro   tting

word vectors to semantic lexicons. see naa (2015).

faruqui, m. and c. dyer (2014). improving vector space word representations using mul-

tilingual correlation. see eac (2014), pp. 462   471.

faruqui, m., r. mcdonald, and r. soricut (2016). morpho-syntactic lexicon generation
using graph-based semi-supervised learning. transactions of the association for computa-
tional linguistics 4, 1   16.

faruqui, m., y. tsvetkov, p. rastogi, and c. dyer (2016). problems with evaluation of word
embeddings using word similarity tasks. in proceedings of the 1st workshop on evaluating
vector-space representations for nlp, pp. 30   35. association for computational linguis-
tics.

fellbaum, c. (2010). id138. springer.

feng, v. w., z. lin, and g. hirst (2014). the impact of deep hierarchical discourse struc-

tures in the evaluation of text coherence. see col (2014), pp. 940   949.

feng, x., l. huang, d. tang, h. ji, b. qin, and t. liu (2016). a language-independent

neural network for id37. see acl (2016), pp. 66   71.

fernandes, e. r., c. n. dos santos, and r. l. milidi   u (2014). latent trees for coreference

resolution. computational linguistics.

ferrucci, d., e. brown, j. chu-carroll, j. fan, d. gondek, a. a. kalyanpur, a. lally, j. w.
murdock, e. nyberg, j. prager, et al. (2010). building watson: an overview of the
deepqa project. ai magazine 31(3), 59   79.

ficler, j. and y. goldberg (2017). controlling linguistic style aspects in neural language
generation. in proceedings of the workshop on stylistic variation, pp. 94   104. association
for computational linguistics.

filippova, k. and m. strube (2008). sentence fusion via dependency graph compression.

see emn (2008), pp. 177   185.

fillmore, c. j. (1968). the case for case.

in e. bach and r. harms (eds.), universals in

linguistic theory. holt, rinehart, and winston.

jacob eisenstein. draft of november 13, 2018.

bibliography

511

fillmore, c. j. (1976). frame semantics and the nature of language. annals of the new york

academy of sciences 280(1), 20   32.

fillmore, c. j. and c. baker (2009). a frames approach to semantic analysis. in the oxford

handbook of linguistic analysis. oxford university press.

finkel, j. r., t. grenager, and c. manning (2005).

incorporating non-local information

into information extraction systems by id150. see acl (2005), pp. 363   370.

finkel, j. r., t. grenager, and c. d. manning (2007). the in   nite tree. see acl (2007), pp.

272   279.

finkel, j. r., a. kleeman, and c. d. manning (2008). ef   cient, feature-based, conditional

random    eld parsing. see acl (2008), pp. 959   967.

finkel, j. r. and c. manning (2009). hierarchical bayesian id20. see naa

(2009), pp. 602   610.

finkel, j. r. and c. d. manning (2008). enforcing transitivity in coreference resolution.

see acl (2008), pp. 45   48.

finkelstein, l., e. gabrilovich, y. matias, e. rivlin, z. solan, g. wolfman, and e. ruppin
(2002). placing search in context: the concept revisited. acm transactions on information
systems 20(1), 116   131.

firth, j. r. (1957). papers in linguistics 1934-1951. oxford university press.

flanigan, j., s. thomson, j. carbonell, c. dyer, and n. a. smith (2014). a discriminative
graph-based parser for the id15. see acl (2014), pp. 1426   
1436.

foltz, p. w., w. kintsch, and t. k. landauer (1998). the measurement of textual coherence

with latent semantic analysis. discourse processes 25(2-3), 285   307.

fordyce, c. (2007). overview of the iwslt 2007 evaluation campaign. in international

workshop on spoken language translation (iwslt) 2007.

forsyth, e. n. and c. h. martell (2007). lexical and discourse analysis of online chat

dialog. in international conference on semantic computing, pp. 19   26. ieee.

fort, k., g. adda, and k. b. cohen (2011). amazon mechanical turk: gold mine or coal

mine? computational linguistics 37(2), 413   420.

fox, h. (2002). phrasal cohesion and id151. see emn (2002), pp.

304   3111.

under contract with mit press, shared under cc-by-nc-nd license.

512

bibliography

francis, w. and h. kucera (1982). frequency analysis of english usage. houghton mif   in

company.

francis, w. n. (1964). a standard sample of present-day english for use with digital
computers. report to the u.s of   ce of education on cooperative research project no.
e-007.

freund, y. and r. e. schapire (1999). large margin classi   cation using the id88

algorithm. machine learning 37(3), 277   296.

fromkin, v., r. rodman, and n. hyams (2013). an introduction to language. cengage

learning.

fundel, k., r. k   uffner, and r. zimmer (2007). relex     id36 using depen-

dency parse trees. bioinformatics 23(3), 365   371.

gabow, h. n., z. galil, t. spencer, and r. e. tarjan (1986). ef   cient algorithms for    nding
minimum spanning trees in undirected and directed graphs. combinatorica 6(2), 109   
122.

gabrilovich, e. and s. markovitch (2007). computing semantic relatedness using
wikipedia-based explicit semantic analysis. in proceedings of the international joint con-
ference on arti   cial intelligence (ijcai), volume 7, pp. 1606   1611.

gage, p. (1994). a new algorithm for data compression. the c users journal 12(2), 23   38.

gale, w. a., k. w. church, and d. yarowsky (1992). one sense per discourse.

in pro-
ceedings of the workshop on speech and natural language, pp. 233   237. association for
computational linguistics.

galley, m., m. hopkins, k. knight, and d. marcu (2004). what   s in a translation rule? see

naa (2004), pp. 273   280.

galley, m., k. r. mckeown, e. fosler-lussier, and h. jing (2003). discourse segmentation

of multi-party conversation. see acl (2003).

ganchev, k. and m. dredze (2008). small statistical models by random feature mixing. in
proceedings of workshop on mobile language processing, pp. 19   20. association of compu-
tational linguistics.

ganchev, k., j. grac  a, j. gillenwater, and b. taskar (2010). posterior id173 for
structured latent variable models. the journal of machine learning research 11, 2001   
2049.

jacob eisenstein. draft of november 13, 2018.

bibliography

513

ganin, y., e. ustinova, h. ajakan, p. germain, h. larochelle, f. laviolette, m. marchand,
and v. lempitsky (2016). domain-adversarial training of neural networks. the journal
of machine learning research 17(59), 1   35.

gao, j., g. andrew, m. johnson, and k. toutanova (2007). a comparative study of param-
eter estimation methods for statistical natural language processing. see acl (2007), pp.
824   831.

garg, n., l. schiebinger, d. jurafsky, and j. zou (2018). id27s quantify
100 years of gender and ethnic stereotypes. proceedings of the national academy of sci-
ences 115(16), e3635   e3644.

gatt, a. and e. krahmer (2018). survey of the state of the art in natural language genera-
tion: core tasks, applications and evaluation. journal of arti   cial intelligence research 61,
65   170.

ge, d., x. jiang, and y. ye (2011). a note on the complexity of lp minimization. mathemat-

ical programming 129(2), 285   299.

ge, n., j. hale, and e. charniak (1998). a statistical approach to id2. in

proceedings of the sixth workshop on very large corpora, volume 71, pp. 76.

ge, r., f. huang, c. jin, and y. yuan (2015). escaping from saddle points     online stochas-
tic gradient for tensor decomposition. in p. gr   unwald, e. hazan, and s. kale (eds.),
proceedings of the conference on learning theory (colt).

ge, r. and r. j. mooney (2005). a statistical semantic parser that integrates syntax and
in proceedings of the conference on natural language learning (conll), pp.

semantics.
9   16.

geach, p. t. (1962). reference and generality: an examination of some medieval and modern

theories. cornell university press.

gehring, j., m. auli, d. grangier, d. yarats, and y. n. dauphin (2017). convolutional

sequence to sequence learning. see icm (2017), pp. 1243   1252.

gildea, d. and d. jurafsky (2002). automatic labeling of semantic roles. computational

linguistics 28(3), 245   288.

gimpel, k., n. schneider, b. o   connor, d. das, d. mills, j. eisenstein, m. heilman, d. yo-
gatama, j. flanigan, and n. a. smith (2011). part-of-speech tagging for twitter: anno-
tation, features, and experiments. see acl (2011), pp. 42   47.

glass, j., t. j. hazen, s. cyphers, i. malioutov, d. huynh, and r. barzilay (2007). recent
progress in the mit spoken lecture processing project. in eighth annual conference of the
international speech communication association.

under contract with mit press, shared under cc-by-nc-nd license.

514

bibliography

glorot, x. and y. bengio (2010). understanding the dif   culty of training deep feedforward
neural networks. in proceedings of arti   cial intelligence and statistics (aistats), pp. 249   
256.

glorot, x., a. bordes, and y. bengio (2011). deep sparse recti   er networks. see ais (2011),

pp. 315   323.

godfrey, j. j., e. c. holliman, and j. mcdaniel (1992). switchboard: telephone speech
corpus for research and development. in proceedings of the international conference on
acoustics, speech, and signal processing (icassp), pp. 517   520. ieee.

goldberg, y. (2017a,

june). an adversarial review of    adversarial generation of
natural language   . https://medium.com/@yoav.goldberg/an-adversarial-review-of-
adversarial-generation-of-natural-language-409ac3378bd7.

goldberg, y. (2017b). neural network methods for natural language processing. synthesis

lectures on human language technologies. morgan & claypool publishers.

goldberg, y. and m. elhadad (2010). an ef   cient algorithm for easy-   rst non-directional

id33. see naa (2010), pp. 742   750.

goldberg, y. and j. nivre (2012). a dynamic oracle for arc-eager id33. see

col (2012), pp. 959   976.

goldberg, y., k. zhao, and l. huang (2013). ef   cient implementation of beam-search

incremental parsers. see acl (2013), pp. 628   633.

goldwater, s. and t. grif   ths (2007). a fully bayesian approach to unsupervised part-of-

speech tagging. see acl (2007).

gonc  alo oliveira, h. r., f. a. cardoso, and f. c. pereira (2007). tra-la-lyrics: an approach
to generate text based on rhythm. in proceedings of the 4th. international joint workshop
on computational creativity. a. cardoso and g. wiggins.

goodfellow, i., y. bengio, and a. courville (2016). deep learning. mit press.

goodman, j. t. (2001). a bit of progress in id38. computer speech & lan-

guage 15(4), 403   434.

gouws, s., d. metzler, c. cai, and e. hovy (2011). contextual bearing on linguistic varia-
tion in social media. in proceedings of the workshop on language and social media. associ-
ation for computational linguistics.

goyal, a., h. daum  e iii, and s. venkatasubramanian (2009). streaming for large scale

nlp: id38. see naa (2009), pp. 512   520.

jacob eisenstein. draft of november 13, 2018.

bibliography

515

graves, a. (2012). sequence transduction with recurrent neural networks. see icm (2012).

graves, a. and n. jaitly (2014). towards end-to-end id103 with recurrent

neural networks. see icm (2014), pp. 1764   1772.

graves, a. and j. schmidhuber (2005). framewise phoneme classi   cation with bidirec-

tional lstm and other neural network architectures. neural networks 18(5), 602   610.

grice, h. p. (1975). logic and conversation. in p. cole and j. l. morgan (eds.), syntax and

semantics volume 3: speech acts, pp. 41   58. academic press.

grishman, r. (2012). information extraction: capabilities and challenges. notes prepared
for the 2012 international winter school in language and speech technologies, rovira
i virgili university, tarragona, spain.

grishman, r. (2015). information extraction. ieee intelligent systems 30(5), 8   15.

grishman, r., c. macleod, and j. sterling (1992). evaluating parsing strategies using
standardized parse    les. in proceedings of the third conference on applied natural language
processing, pp. 156   161. association for computational linguistics.

grishman, r. and b. sundheim (1996). message understanding conference-6: a brief

history. see col (1996), pp. 466   471.

groenendijk, j. and m. stokhof (1991). dynamic predicate logic. linguistics and philoso-

phy 14(1), 39   100.

grosz, b. j. (1979). focusing and description in natural language dialogues. technical

report, sri international.

grosz, b. j., s. weinstein, and a. k. joshi (1995). centering: a framework for modeling

the local coherence of discourse. computational linguistics 21(2), 203   225.

gu, j., z. lu, h. li, and v. o. li (2016). incorporating copying mechanism in sequence-to-

sequence learning. see acl (2016), pp. 1631   1640.

gulcehre, c., s. ahn, r. nallapati, b. zhou, and y. bengio (2016). pointing the unknown

words. see acl (2016), pp. 140   149.

gutmann, m. u. and a. hyv  arinen (2012). noise-contrastive estimation of unnormalized
statistical models, with applications to natural image statistics. the journal of machine
learning research 13(1), 307   361.

haghighi, a. and d. klein (2007). unsupervised coreference resolution in a nonparametric

bayesian model. see acl (2007).

under contract with mit press, shared under cc-by-nc-nd license.

516

bibliography

haghighi, a. and d. klein (2009). simple coreference resolution with rich syntactic and

semantic features. see emn (2009), pp. 1152   1161.

haghighi, a. and d. klein (2010). coreference resolution in a modular, entity-centered

model. see naa (2010), pp. 385   393.

haji  c, j. and b. hladk  a (1998). tagging in   ective languages: prediction of morphological
categories for a rich, structured tagset. in proceedings of the association for computational
linguistics (acl), pp. 483   490.

halliday, m. and r. hasan (1976). cohesion in english. london: longman.

hammerton, j. (2003). id39 with long short-term memory. in pro-

ceedings of the conference on natural language learning (conll), pp. 172   175.

han, x. and l. sun (2012). an entity-topic model for entity linking. see emn (2012), pp.

105   115.

han, x., l. sun, and j. zhao (2011). collective entity linking in web text: a graph-based
method. in proceedings of acm sigir conference on research and development in informa-
tion retrieval, pp. 765   774.

hannak, a., e. anderson, l. f. barrett, s. lehmann, a. mislove, and m. riedewald (2012).
tweetin   in the rain: exploring societal-scale effects of weather on mood. in proceedings
of the international conference on web and social media (icwsm).

hardmeier, c. (2012). discourse in id151. a survey and a case

study. discours (11).

haspelmath, m. and a. sims (2013). understanding morphology. routledge.

hastie, t., r. tibshirani, and j. friedman (2009). the elements of statistical learning (second

ed.). new york: springer.

hatzivassiloglou, v. and k. r. mckeown (1997). predicting the semantic orientation of

adjectives. see acl (1997), pp. 174   181.

hayes, a. f. and k. krippendorff (2007). answering the call for a standard reliability

measure for coding data. communication methods and measures 1(1), 77   89.

he, h., a. balakrishnan, m. eric, and p. liang (2017). learning symmetric collaborative
dialogue agents with dynamic id13 embeddings. see acl (2017), pp. 1766   
1776.

he, k., x. zhang, s. ren, and j. sun (2015). delving deep into recti   ers: surpassing

human-level performance on id163 classi   cation. see icc (2015), pp. 1026   1034.

jacob eisenstein. draft of november 13, 2018.

bibliography

517

he, k., x. zhang, s. ren, and j. sun (2016). deep residual learning for image recognition.

in proceedings of the international conference on id161 (iccv), pp. 770   778.

he, l., k. lee, m. lewis, and l. zettlemoyer (2017). deep id14: what

works and what   s next. see acl (2017).

he, z., s. liu, m. li, m. zhou, l. zhang, and h. wang (2013). learning entity representa-

tion for entity disambiguation. see acl (2013), pp. 30   34.

hearst, m. a. (1992). automatic acquisition of hyponyms from large text corpora. see col

(1992), pp. 539   545.

hearst, m. a. (1997). texttiling: segmenting text into multi-paragraph subtopic passages.

computational linguistics 23(1), 33   64.

heerschop, b., f. goossen, a. hogenboom, f. frasincar, u. kaymak, and f. de jong (2011).
polarity analysis of texts using discourse structure. in proceedings of the 20th acm inter-
national conference on information and knowledge management, pp. 1061   1070. acm.

henderson, j. (2004). discriminative training of a neural network statistical parser. see

acl (2004), pp. 95   102.

hendrickx, i., s. n. kim, z. kozareva, p. nakov, d.   o s  eaghdha, s. pad  o, m. pennacchiotti,
l. romano, and s. szpakowicz (2009). semeval-2010 task 8: multi-way classi   cation of
semantic relations between pairs of nominals. in proceedings of the workshop on semantic
evaluations: recent achievements and future directions, pp. 94   99. association for com-
putational linguistics.

hermann, k. m., t. kocisky, e. grefenstette, l. espeholt, w. kay, m. suleyman, and
p. blunsom (2015). teaching machines to read and comprehend. in advances in neu-
ral information processing systems, pp. 1693   1701.

hernault, h., h. prendinger, d. a. duverle, and m. ishizuka (2010). hilda: a discourse

parser using support vector machine classi   cation. dialogue and discourse 1(3), 1   33.

hill, f., a. bordes, s. chopra, and j. weston (2016). the goldilocks principle: reading

children   s books with explicit memory representations. see icl (2016).

hill, f., k. cho, and a. korhonen (2016). learning distributed representations of sentences

from unlabelled data. see naa (2016).

hill, f., r. reichart, and a. korhonen (2015). siid113x-999: evaluating semantic models

with (genuine) similarity estimation. computational linguistics 41(4), 665   695.

hindle, d. and m. rooth (1993). structural ambiguity and lexical relations. computational

linguistics 19(1), 103   120.

under contract with mit press, shared under cc-by-nc-nd license.

518

bibliography

hirao, t., y. yoshida, m. nishino, n. yasuda, and m. nagata (2013). single-document

summarization as a tree knapsack problem. see emn (2013), pp. 1515   1520.

hirschman, l. and r. gaizauskas (2001). natural language id53: the view

from here. natural language engineering 7(4), 275   300.

hirschman, l., m. light, e. breck, and j. d. burger (1999). deep read: a reading compre-
hension system. in proceedings of the association for computational linguistics (acl), pp.
325   332.

hobbs, j. r. (1978). resolving pronoun references. lingua 44(4), 311   338.

hobbs, j. r., d. appelt, j. bear, d. israel, m. kameyama, m. stickel, and m. tyson (1997).
fastus: a cascaded    nite-state transducer for extracting information from natural-
language text. finite-state language processing, 383   406.

hochreiter, s. and j. schmidhuber (1997). long short-term memory. neural computa-

tion 9(8), 1735   1780.

hockenmaier, j. and m. steedman (2007). id35bank: a corpus of id35 derivations and de-
pendency structures extracted from the id32. computational linguistics 33(3),
355   396.

hoffart, j., m. a. yosef, i. bordino, h. f   urstenau, m. pinkal, m. spaniol, b. taneva,
s. thater, and g. weikum (2011). robust disambiguation of named entities in text.
see emn (2011), pp. 782   792.

hoffmann, r., c. zhang, x. ling, l. zettlemoyer, and d. s. weld (2011). knowledge-based
weak supervision for information extraction of overlapping relations. see acl (2011), pp.
541   550.

holmstrom, l. and p. koistinen (1992). using additive noise in back-propagation training.

ieee transactions on neural networks 3(1), 24   38.

hovy, d., s. spruit, m. mitchell, e. m. bender, m. strube, and h. wallach (2017). proceed-
ings of the    rst acl workshop on ethics in natural language processing. in proceedings of
the first acl workshop on ethics in natural language processing. association for compu-
tational linguistics.

hovy, e. and j. lavid (2010). towards a    science    of corpus annotation: a new method-
ological challenge for corpus linguistics. international journal of translation 22(1), 13   36.

hsu, d., s. m. kakade, and t. zhang (2012). a spectral algorithm for learning hidden

markov models. journal of computer and system sciences 78(5), 1460   1480.

jacob eisenstein. draft of november 13, 2018.

bibliography

519

hu, m. and b. liu (2004). mining and summarizing customer reviews. in proceedings of

knowledge discovery and data mining (kdd), pp. 168   177.

hu, z., z. yang, x. liang, r. salakhutdinov, and e. p. xing (2017). toward controlled

generation of text. see icm (2017), pp. 1587   1596.

huang, f. and a. yates (2012). biased representation learning for id20. see

emn (2012), pp. 1313   1323.

huang, l. and d. chiang (2007). forest rescoring: faster decoding with integrated lan-

guage models. see acl (2007), pp. 144   151.

huang, l., s. fayong, and y. guo (2012). structured id88 with inexact search. see

naa (2012), pp. 142   151.

huang, y. (2015). pragmatics (second ed.). oxford textbooks in linguistics. oxford uni-

versity press.

huang, z., w. xu, and k. yu (2015). bidirectional lstm-crf models for sequence tagging.

arxiv preprint arxiv:1508.01991.

huddleston, r. and g. k. pullum (2005). a student   s introduction to english grammar. cam-

bridge university press.

huffman, d. a. (1952). a method for the construction of minimum-redundancy codes.

proceedings of the ire 40(9), 1098   1101.

humphreys, k., r. gaizauskas, and s. azzam (1997). event coreference for information
extraction. in proceedings of a workshop on operational factors in practical, robust anaphora
resolution for unrestricted texts, pp. 75   81. association for computational linguistics.

ide, n. and y. wilks (2006). making sense about sense. in id51, pp.

47   73. springer.

ioffe, s. and c. szegedy (2015). batch id172: accelerating deep network training

by reducing internal covariate shift. see icm (2015), pp. 448   456.

isozaki, h., t. hirao, k. duh, k. sudoh, and h. tsukada (2010). automatic evaluation of

translation quality for distant language pairs. see emn (2010), pp. 944   952.

ivanova, a., s. oepen, l.   vrelid, and d. flickinger (2012). who did what to whom? a
contrastive study of syntacto-semantic dependencies. in proceedings of the sixth linguis-
tic annotation workshop, pp. 2   11. association for computational linguistics.

iyyer, m., v. manjunatha, j. boyd-graber, and h. daum  e iii (2015). deep unordered com-

position rivals syntactic methods for text classi   cation. see acl (2015), pp. 1681   1691.

under contract with mit press, shared under cc-by-nc-nd license.

520

bibliography

james, g., d. witten, t. hastie, and r. tibshirani (2013). an introduction to statistical learn-

ing, volume 112. springer.

janin, a., d. baron, j. edwards, d. ellis, d. gelbart, n. morgan, b. peskin, t. pfau,
in proceedings of the

e. shriberg, a. stolcke, et al. (2003). the icsi meeting corpus.
international conference on acoustics, speech, and signal processing (icassp).

jean, s., k. cho, r. memisevic, and y. bengio (2015). on using very large target vocabulary

for id4. see acl (2015), pp. 1   10.

jeong, m., c.-y. lin, and g. g. lee (2009). semi-supervised speech act recognition in

emails and forums. see emn (2009), pp. 1250   1259.

ji, h. and r. grishman (2011). knowledge base population: successful approaches and

challenges. see acl (2011), pp. 1148   1158.

ji, y., t. cohn, l. kong, c. dyer, and j. eisenstein (2015). document context language
models. in international conference on learning representations, workshop track, volume
abs/1511.03962.

ji, y. and j. eisenstein (2014). representation learning for text-level discourse parsing. see

acl (2014).

ji, y. and j. eisenstein (2015). one vector is not enough: entity-augmented distributional
semantics for discourse relations. transactions of the association for computational lin-
guistics (tacl).

ji, y., g. haffari, and j. eisenstein (2016). a latent variable recurrent neural network for

discourse relation language models. see naa (2016).

ji, y. and n. a. smith (2017). neural discourse structure for text categorization. see acl

(2017), pp. 996   1005.

ji, y., c. tan, s. martschat, y. choi, and n. a. smith (2017). dynamic entity representations

in neural language models. see emn (2017), pp. 1831   1840.

jiang, l., m. yu, m. zhou, x. liu, and t. zhao (2011). target-dependent twitter sentiment

classi   cation. see acl (2011), pp. 151   160.

jing, h. (2000). sentence reduction for automatic text summarization. in proceedings of
the sixth conference on applied natural language processing, pp. 310   315. association for
computational linguistics.

joachims, t. (2002). optimizing search engines using clickthrough data. in proceedings of

knowledge discovery and data mining (kdd), pp. 133   142.

jacob eisenstein. draft of november 13, 2018.

bibliography

521

jockers, m. l. (2015). revealing sentiment and plot arcs with the syuzhet package. http:

//www.matthewjockers.net/2015/02/02/syuzhet/.

johnson, a. e., t. j. pollard, l. shen, h. l. li-wei, m. feng, m. ghassemi, b. moody,
p. szolovits, l. a. celi, and r. g. mark (2016). mimic-iii, a freely accessible critical care
database. scienti   c data 3, 160035.

johnson, m. (1998). pid18 models of linguistic tree representations. computational lin-

guistics 24(4), 613   632.

johnson, r. and t. zhang (2017). deep pyramid convolutional neural networks for text

categorization. see acl (2017), pp. 562   570.

joshi, a. k. (1985). id34s: how much context-sensitivity is required
to provide reasonable structural descriptions? in natural language processing     theoret-
ical, computational and psychological perspectives. new york, ny: cambridge university
press.

joshi, a. k. and y. schabes (1997). tree-adjoining grammars. in handbook of formal lan-

guages, pp. 69   123. springer.

joshi, a. k., k. v. shanker, and d. weir (1991). the convergence of mildly context-sensitive
grammar formalisms. in foundational issues in natural language processing. cambridge
ma: mit press.

jozefowicz, r., o. vinyals, m. schuster, n. shazeer, and y. wu (2016). exploring the limits

of id38. arxiv preprint arxiv:1602.02410.

jozefowicz, r., w. zaremba, and i. sutskever (2015). an empirical exploration of recurrent

network architectures. see icm (2015), pp. 2342   2350.

jurafsky, d. (1996). a probabilistic model of lexical and syntactic access and disambigua-

tion. cognitive science 20(2), 137   194.

jurafsky, d. and j. h. martin (2009). speech and language processing (second ed.). prentice

hall.

jurafsky, d. and j. h. martin (2019). speech and language processing (third ed.). prentice

hall.

kadlec, r., m. schmid, o. bajgar, and j. kleindienst (2016). text understanding with the

attention sum reader network. see acl (2016), pp. 908   918.

kalchbrenner, n. and p. blunsom (2013). recurrent convolutional neural networks for dis-
course compositionality. in proceedings of the workshop on continuous vector space models
and their compositionality, pp. 119   126. association for computational linguistics.

under contract with mit press, shared under cc-by-nc-nd license.

522

bibliography

kalchbrenner, n., l. espeholt, k. simonyan, a. v. d. oord, a. graves, and k. kavukcuoglu

(2016). id4 in linear time. arxiv preprint arxiv:1610.10099.

kalchbrenner, n., e. grefenstette, and p. blunsom (2014). a convolutional neural network

for modelling sentences. see acl (2014), pp. 655   665.

karlsson, f. (2007). constraints on multiple center-embedding of clauses. journal of lin-

guistics 43(2), 365   392.

kate, r. j., y. w. wong, and r. j. mooney (2005). learning to transform natural to formal

languages. in proceedings of the national conference on arti   cial intelligence (aaai).

kawaguchi, k., l. p. kaelbling, and y. bengio (2017). generalization in deep learning.

arxiv preprint arxiv:1710.05468.

kehler, a. (2007). rethinking the smash approach to pronoun interpretation. in interdis-
ciplinary perspectives on reference processing, new directions in cognitive science series,
pp. 95   122. oxford university press.

kibble, r. and r. power (2004). optimizing referential coherence in text generation. com-

putational linguistics 30(4), 401   416.

kilgarriff, a. (1997). i don   t believe in word senses. computers and the humanities 31(2),

91   113.

kilgarriff, a. and g. grefenstette (2003). introduction to the special issue on the web as

corpus. computational linguistics 29(3), 333   347.

kim, m.-j. (2002). does korean have adjectives? mit working papers in linguistics 43,

71   89.

kim, s.-m. and e. hovy (2006, july). extracting opinions, opinion holders, and topics
in proceedings of the workshop on sentiment and

expressed in online news media text.
subjectivity in text, pp. 1   8. association for computational linguistics.

kim, y. (2014). convolutional neural networks for sentence classi   cation. see emn (2014),

pp. 1746   1751.

kim, y., c. denton, l. hoang, and a. m. rush (2017). structured attention networks. see

icl (2017).

kim, y., y. jernite, d. sontag, and a. m. rush (2016). character-aware neural language

models. see aaa (2016).

kingma, d. and j. ba (2014). adam: a method for stochastic optimization. arxiv preprint

arxiv:1412.6980.

jacob eisenstein. draft of november 13, 2018.

bibliography

523

kiperwasser, e. and y. goldberg (2016). simple and accurate id33 using
bidirectional lstm feature representations. transactions of the association for computa-
tional linguistics 4, 313   327.

kipper-schuler, k. (2005). verbnet: a broad-coverage, comprehensive verb lexicon. ph. d.

thesis, computer and information science, university of pennsylvania.

kiros, r., r. salakhutdinov, and r. zemel (2014). multimodal neural language models.

see icm (2014), pp. 595   603.

kiros, r., y. zhu, r. salakhudinov, r. s. zemel, a. torralba, r. urtasun, and s. fidler

(2015). skip-thought vectors. see nip (2015).

klein, d. and c. d. manning (2003). accurate unlexicalized parsing. see acl (2003), pp.

423   430.

klein, d. and c. d. manning (2004). corpus-based induction of syntactic structure: mod-

els of dependency and constituency. see acl (2004).

klein, g., y. kim, y. deng, j. senellart, and a. m. rush (2017). openid4: open-source

toolkit for id4. arxiv preprint arxiv:1701.02810.

klementiev, a., i. titov, and b. bhattarai (2012). inducing crosslingual distributed repre-

sentations of words. see col (2012), pp. 1459   1474.

klenner, m. (2007). enforcing consistency on coreference sets. in recent advances in natu-

ral language processing (ranlp), pp. 323   328.

knight, k. (1999). decoding complexity in word-replacement translation models. compu-

tational linguistics 25(4), 607   615.

knight, k. and j. graehl (1998). machine id68. computational linguistics 24(4),

599   612.

knight, k. and d. marcu (2000). statistics-based summarization-step one: sentence com-
in proceedings of the national conference on arti   cial intelligence (aaai), pp.

pression.
703   710.

knight, k. and j. may (2009). applications of weighted automata in natural language

processing. in handbook of weighted automata, pp. 571   596. springer.

knott, a. (1996). a data-driven methodology for motivating a set of coherence relations. ph. d.

thesis, the university of edinburgh.

koehn, p. (2005). europarl: a parallel corpus for id151. in mt

summit, volume 5, pp. 79   86.

under contract with mit press, shared under cc-by-nc-nd license.

524

bibliography

koehn, p. (2009). id151. cambridge university press.

koehn, p. (2017). id4. arxiv preprint arxiv:1709.07809.

konstas, i. and m. lapata (2013). a global model for concept-to-text generation. journal

of arti   cial intelligence research 48, 305   346.

koo, t., x. carreras, and m. collins (2008). simple semi-supervised id33.

see acl (2008), pp. 595   603.

koo, t. and m. collins (2005). hidden-variable models for discriminative reranking. see

emn (2005), pp. 507   514.

koo, t. and m. collins (2010). ef   cient third-order dependency parsers. see acl (2010).

koo, t., a. globerson, x. carreras, and m. collins (2007). id170 models

via the matrix-tree theorem. see emn (2007), pp. 141   150.

kovach, b. and t. rosenstiel (2014). the elements of journalism: what newspeople should know

and the public should expect. three rivers press.

krishnamurthy, j. (2016). probabilistic models for learning a semantic parser lexicon. see

naa (2016), pp. 606   616.

krishnamurthy, j. and t. m. mitchell (2012). weakly supervised training of semantic

parsers. see emn (2012), pp. 754   765.

krizhevsky, a., i. sutskever, and g. e. hinton (2012). id163 classi   cation with deep
in neural information processing systems (nips), pp.

convolutional neural networks.
1097   1105.

k   ubler, s., r. mcdonald, and j. nivre (2009). id33. synthesis lectures on

human language technologies 1(1), 1   127.

kuhlmann, m. and j. nivre (2010). transition-based techniques for non-projective depen-

dency parsing. northern european journal of language technology (nejlt) 2(1), 1   19.

kummerfeld, j. k., t. berg-kirkpatrick, and d. klein (2015). an empirical analysis of

optimization for max-margin nlp. see emn (2015).

kwiatkowski, t., s. goldwater, l. zettlemoyer, and m. steedman (2012). a probabilistic
model of syntactic and semantic acquisition from child-directed utterances and their
meanings. see eac (2012), pp. 234   244.

lafferty, j., a. mccallum, and f. pereira (2001). conditional random    elds: probabilistic
models for segmenting and labeling sequence data. in proceedings of the international
conference on machine learning (icml).

jacob eisenstein. draft of november 13, 2018.

bibliography

525

lakoff, g. (1973). hedges: a study in meaning criteria and the logic of fuzzy concepts.

journal of philosophical logic 2(4), 458   508.

lample, g., m. ballesteros, s. subramanian, k. kawakami, and c. dyer (2016). neural

architectures for id39. see naa (2016), pp. 260   270.

langkilde, i. and k. knight (1998). generation that exploits corpus-based statistical
knowledge. in proceedings of the association for computational linguistics (acl), pp. 704   
710.

lapata, m. (2003). probabilistic text structuring: experiments with sentence ordering. see

acl (2003), pp. 545   552.

lappin, s. and h. j. leass (1994). an algorithm for pronominal id2.

computational linguistics 20(4), 535   561.

lari, k. and s. j. young (1990). the estimation of stochastic context-free grammars using

the inside-outside algorithm. computer speech & language 4(1), 35   56.

lascarides, a. and n. asher (2007). segmented discourse representation theory: dynamic

semantics with discourse structure. in computing meaning, pp. 87   124. springer.

law, e. and l. v. ahn (2011). human computation. synthesis lectures on arti   cial intelli-

gence and machine learning 5(3), 1   121.

lebret, r., d. grangier, and m. auli (2016). neural text generation from structured data

with application to the biography domain. see emn (2016), pp. 1203   1213.

lecun, y., l. bottou, g. b. orr, and k.-r. m   uller (1998). ef   cient backprop. in neural

networks: tricks of the trade, pp. 9   50. springer.

lee, c. m. and s. s. narayanan (2005). toward detecting emotions in spoken dialogs.

ieee transactions on speech and audio processing 13(2), 293   303.

lee, h., a. chang, y. peirsman, n. chambers, m. surdeanu, and d. jurafsky (2013). de-
terministic coreference resolution based on entity-centric, precision-ranked rules. com-
putational linguistics 39(4), 885   916.

lee, h., y. peirsman, a. chang, n. chambers, m. surdeanu, and d. jurafsky (2011). stan-
ford   s multi-pass sieve coreference resolution system at the conll-2011 shared task. see
con (2011), pp. 28   34.

lee, k., l. he, m. lewis, and l. zettlemoyer (2017). end-to-end neural coreference reso-

lution. see emn (2017).

under contract with mit press, shared under cc-by-nc-nd license.

526

bibliography

lee, k., l. he, and l. zettlemoyer (2018). higher-order coreference resolution with coarse-

to-   ne id136. see naa (2018), pp. 687   692.

lenat, d. b., r. v. guha, k. pittman, d. pratt, and m. shepherd (1990). cyc: toward

programs with common sense. communications of the acm 33(8), 30   49.

lesk, m. (1986). automatic sense disambiguation using machine readable dictionaries:
how to tell a pine cone from an ice cream cone. in proceedings of the 5th annual interna-
tional conference on systems documentation, pp. 24   26. acm.

levesque, h. j., e. davis, and l. morgenstern (2011). the winograd schema challenge. in

aaai spring symposium: logical formalizations of commonsense reasoning.

levin, e., r. pieraccini, and w. eckert (1998). using markov decision process for learning
dialogue strategies. in proceedings of the international conference on acoustics, speech and
signal processing, volume 1, pp. 201   204. ieee.

levy, o. and y. goldberg (2014). dependency-based id27s. see acl (2014),

pp. 302   308.

levy, o., y. goldberg, and i. dagan (2015).

improving distributional similarity with
lessons learned from id27s. transactions of the association for computational
linguistics 3, 211   225.

levy, r. and c. manning (2009). an informal introduction to computational seman-
tics. http://idiom.ucsd.edu/  rlevy/teaching/winter2009/ligncse256/
lectures/lecture_14_compositional_semantics.pdf.

lewis, m. and m. steedman (2013). combined distributional and logical semantics. trans-

actions of the association for computational linguistics 1, 179   192.

lewis ii, p. m. and r. e. stearns (1968). syntax-directed transduction.

acm 15(3), 465   488.

journal of the

li, j. and d. jurafsky (2015). do multi-sense embeddings improve natural language un-

derstanding? see emn (2015), pp. 1722   1732.

li, j. and d. jurafsky (2017). neural net models of open-domain discourse coherence. see

emn (2017), pp. 198   209.

li, j., r. li, and e. hovy (2014). recursive deep models for discourse parsing. see emn

(2014).

li, j., m.-t. luong, and d. jurafsky (2015). a hierarchical neural autoencoder for para-

graphs and documents. see emn (2015).

jacob eisenstein. draft of november 13, 2018.

bibliography

527

li, j., t. luong, d. jurafsky, and e. hovy (2015). when are tree structures necessary for

deep learning of representations? see emn (2015), pp. 2304   2314.

li, j., w. monroe, a. ritter, d. jurafsky, m. galley, and j. gao (2016). deep reinforcement

learning for dialogue generation. see emn (2016), pp. 1192   1202.

li, q., s. anzaroot, w.-p. lin, x. li, and h. ji (2011). joint id136 for cross-document
information extraction. in proceedings of the international conference on information and
knowledge management (cikm), pp. 2225   2228.

li, q., h. ji, and l. huang (2013). joint event extraction via id170 with

global features. see acl (2013), pp. 73   82.

liang, p. (2005). semi-supervised learning for natural language. master   s thesis, mas-

sachusetts institute of technology.

liang, p., a. bouchard-c  ot  e, d. klein, and b. taskar (2006). an end-to-end discriminative

approach to machine translation. see acl (2006), pp. 761   768.

liang, p., m. jordan, and d. klein (2009). learning semantic correspondences with less

supervision. see acl (2009), pp. 91   99.

liang, p., m. i. jordan, and d. klein (2013). learning dependency-based compositional

semantics. computational linguistics 39(2), 389   446.

liang, p. and d. klein (2009). online em for unsupervised models. see naa (2009), pp.

611   619.

liang, p., s. petrov, m. i. jordan, and d. klein (2007). the in   nite pid18 using hierarchical

dirichlet processes. see emn (2007), pp. 688   697.

liang, p. and c. potts (2015). bringing machine learning and id152

together. annual review of linguistics 1(1), 355   376.

lieber, r. (2015). introducing morphology. cambridge university press.

lin, d. (1998). automatic retrieval and id91 of similar words. see col (1998), pp.

768   774.

lin, j. and c. dyer (2010). data-intensive text processing with mapreduce. synthesis

lectures on human language technologies 3(1), 1   177.

lin, z., m. feng, c. n. d. santos, m. yu, b. xiang, b. zhou, and y. bengio (2017). a

structured self-attentive sentence embedding. see icl (2017).

lin, z., m.-y. kan, and h. t. ng (2009). recognizing implicit discourse relations in the

penn discourse treebank. see emn (2009), pp. 343   351.

under contract with mit press, shared under cc-by-nc-nd license.

528

bibliography

lin, z., h. t. ng, and m.-y. kan (2011). automatically evaluating text coherence using

discourse relations. see acl (2011), pp. 997   1006.

lin, z., h. t. ng, and m.-y. kan (2014). a pdtb-styled end-to-end discourse parser. natural

language engineering 20(2), 151   184.

ling, w., c. dyer, a. black, and i. trancoso (2015). two/too simple adaptations of

id97 for syntax problems. see naa (2015).

ling, w., t. lu    s, l. marujo, r. f. astudillo, s. amir, c. dyer, a. w. black, and i. trancoso
(2015). finding function in form: compositional character models for open vocabulary
word representation. see emn (2015).

ling, w., g. xiang, c. dyer, a. black, and i. trancoso (2013). microblogs as parallel cor-

pora. see acl (2013).

ling, x., s. singh, and d. s. weld (2015). design challenges for entity linking. transactions

of the association for computational linguistics 3, 315   328.

linguistic data consortium (2005). ace (automatic content extraction) english annota-
tion guidelines for relations. technical report version 5.8.3, linguistic data consor-
tium.

liu, b. (2015). id31: mining opinions, sentiments, and emotions. cambridge

university press.

liu, d. c. and j. nocedal (1989). on the limited memory bfgs method for large scale

optimization. mathematical programming 45(1-3), 503   528.

liu, y., q. liu, and s. lin (2006). tree-to-string alignment template for statistical machine

translation. see acl (2006), pp. 609   616.

loper, e. and s. bird (2002). nltk: the natural language toolkit.

in proceedings of the
workshop on effective tools and methodologies for teaching natural language processing and
computational linguistics, pp. 63   70. association for computational linguistics.

louis, a., a. joshi, and a. nenkova (2010). discourse indicators for content selection
in summarization. in proceedings of the special interest group on discourse and dialogue
(sigdial), pp. 147   156.

louis, a. and a. nenkova (2013). what makes writing great?    rst experiments on article
quality prediction in the science journalism domain. transactions of the association for
computational linguistics 1, 341   352.

loveland, d. w. (2016). automated theorem proving: a logical basis. elsevier.

jacob eisenstein. draft of november 13, 2018.

bibliography

529

lowe, r., n. pow, i. v. serban, and j. pineau (2015). the ubuntu dialogue corpus: a
large dataset for research in unstructured multi-turn dialogue systems. in proceedings
of the special interest group on discourse and dialogue (sigdial).

luo, x. (2005). on coreference resolution performance metrics. see emn (2005), pp. 25   32.

luo, x., a. ittycheriah, h. jing, n. kambhatla, and s. roukos (2004). a mention-

synchronous coreference resolution algorithm based on the bell tree. see acl (2004).

luong, m.-t., r. socher, and c. d. manning (2013). better word representations with

id56s for morphology.

luong, t., h. pham, and c. d. manning (2015). effective approaches to attention-based

id4. see emn (2015), pp. 1412   1421.

luong, t., i. sutskever, q. le, o. vinyals, and w. zaremba (2015). addressing the rare

word problem in id4. see acl (2015), pp. 11   19.

maas, a. l., a. y. hannun, and a. y. ng (2013). recti   er nonlinearities improve neural

network acoustic models. see icm (2013).

magerman, d. m. (1995). statistical decision-tree models for parsing. in proceedings of the

association for computational linguistics (acl), pp. 276   283.

mairesse, f. and m. a. walker (2011). controlling user perceptions of linguistic style:

trainable generation of personality traits. computational linguistics 37(3), 455   488.

mani, i., m. verhagen, b. wellner, c. m. lee, and j. pustejovsky (2006). machine learning

of temporal relations. see acl (2006), pp. 753   760.

mann, w. c. and s. a. thompson (1988). rhetorical structure theory: toward a functional

theory of text organization. text 8(3), 243   281.

manning, c. d. (2015). last words: computational linguistics and deep learning. compu-

tational linguistics 41(4), 701   707.

manning, c. d., p. raghavan, h. sch   utze, et al. (2008). introduction to information retrieval,

volume 1. cambridge university press.

manning, c. d. and h. sch   utze (1999). foundations of statistical natural language process-

ing. cambridge, massachusetts: mit press.

marcu, d. (1996). building up rhetorical structure trees.

conference on arti   cial intelligence, pp. 1069   1074.

in proceedings of the national

marcu, d. (1997a). from discourse structures to text summaries.

workshop on intelligent scalable text summarization.

in proceedings of the

under contract with mit press, shared under cc-by-nc-nd license.

530

bibliography

marcu, d. (1997b). from local to global coherence: a bottom-up approach to text plan-
ning. in proceedings of the national conference on arti   cial intelligence (aaai), pp. 629   635.

marcus, m. p., m. a. marcinkiewicz, and b. santorini (1993). building a large annotated

corpus of english: the id32. computational linguistics 19(2), 313   330.

maron, o. and t. lozano-p  erez (1998). a framework for multiple-instance learning. in

neural information processing systems (nips), pp. 570   576.

m  arquez, g. g. (1970). one hundred years of solitude. harper & row. english translation

by gregory rabassa.

martins, a. f. t., n. a. smith, and e. p. xing (2009). concise integer id135

formulations for id33. see acl (2009), pp. 342   350.

martins, a. f. t., n. a. smith, e. p. xing, p. m. q. aguiar, and m. a. t. figueiredo (2010).
turbo parsers: id33 by approximate variational id136. see emn
(2010), pp. 34   44.

matsuzaki, t., y. miyao, and j. tsujii (2005). probabilistic id18 with latent annotations. see

acl (2005), pp. 75   82.

matthiessen, c. and j. a. bateman (1991). text generation and systemic-functional linguistics:

experiences from english and japanese. pinter publishers.

mccallum, a. and w. li (2003). early results for id39 with condi-
tional random    elds, feature induction and web-enhanced lexicons. see naa (2003), pp.
188   191.

mccallum, a. and b. wellner (2004). conditional models of identity uncertainty with
in neural information processing systems (nips), pp.

application to noun coreference.
905   912.

mcdonald, r., k. crammer, and f. pereira (2005). online large-margin training of depen-

dency parsers. see acl (2005), pp. 91   98.

mcdonald, r., k. hannan, t. neylon, m. wells, and j. reynar (2007). structured models

for    ne-to-coarse id31. see acl (2007).

mcdonald, r. and f. pereira (2006). online learning of approximate id33

algorithms. see eac (2006).

mckeown, k. (1992). text generation. cambridge university press.

mckeown, k., s. rosenthal, k. thadani, and c. moore (2010). time-ef   cient creation of

an accurate sentence fusion corpus. see naa (2010), pp. 317   320.

jacob eisenstein. draft of november 13, 2018.

bibliography

531

mckeown, k. r., r. barzilay, d. evans, v. hatzivassiloglou, j. l. klavans, a. nenkova,
c. sable, b. schiffman, and s. sigelman (2002). tracking and summarizing news on
in proceedings of the second international
a daily basis with columbia   s newsblaster.
conference on human language technology research, pp. 280   285.

mcnamee, p. and h. t. dang (2009). overview of the tac 2009 knowledge base population

track. in text analysis conference (tac), volume 17, pp. 111   113.

medlock, b. and t. briscoe (2007). weakly supervised learning for hedge classi   cation in

scienti   c literature. see acl (2007), pp. 992   999.

mei, h., m. bansal, and m. r. walter (2016). what to talk about and how? selective

generation using lstms with coarse-to-   ne alignment. see naa (2016), pp. 720   730.

merity, s., n. s. keskar, and r. socher (2018). regularizing and optimizing lstm lan-

guage models. see icl (2018).

merity, s., c. xiong, j. bradbury, and r. socher (2017). pointer sentinel mixture models.

see icl (2017).

messud, c. (2014, june). a new    l     etranger   . new york review of books.

miao, y. and p. blunsom (2016). language as a latent variable: discrete generative models

for sentence compression. see emn (2016), pp. 319   328.

miao, y., l. yu, and p. blunsom (2016). neural variational id136 for text processing. in

proceedings of the international conference on machine learning (icml).

mihalcea, r., t. a. chklovski, and a. kilgarriff (2004). the senseval-3 english lexical
sample task. in proceedings of senseval-3, barcelona, spain, pp. 25   28. association for
computational linguistics.

mihalcea, r. and d. radev (2011). graph-based natural language processing and information

retrieval. cambridge university press.

mikolov, t., k. chen, g. corrado, and j. dean (2013). ef   cient estimation of word repre-
sentations in vector space. in proceedings of international conference on learning represen-
tations.

mikolov, t., a. deoras, d. povey, l. burget, and j. cernocky (2011). strategies for training
large scale neural network language models. in proceedings of the workshop on automatic
id103 and understanding (asru), pp. 196   201.

mikolov, t., m. kara     at, l. burget, j. cernock`y, and s. khudanpur (2010). recurrent

neural network based language model. in interspeech, pp. 1045   1048.

under contract with mit press, shared under cc-by-nc-nd license.

532

bibliography

mikolov, t., i. sutskever, k. chen, g. s. corrado, and j. dean (2013). distributed rep-
in advances in neural

resentations of words and phrases and their compositionality.
information processing systems, pp. 3111   3119.

mikolov, t., w.-t. yih, and g. zweig (2013). linguistic regularities in continuous space

word representations. see naa (2013), pp. 746   751.

mikolov, t. and g. zweig. context dependent recurrent neural network language model.

in proceedings of spoken language technology (slt), pp. 234   239.

miller, g. a., g. a. heise, and w. lichten (1951). the intelligibility of speech as a function

of the context of the test materials. journal of experimental psychology 41(5), 329.

miller, m., c. sathi, d. wiesenthal, j. leskovec, and c. potts (2011). sentiment    ow
in proceedings of the international conference on web and

through hyperlink networks.
social media (icwsm).

miller, s., j. guinness, and a. zamanian (2004). name tagging with word clusters and

discriminative training. see naa (2004), pp. 337   342.

milne, d. and i. h. witten (2008). learning to link with wikipedia. in proceedings of the
international conference on information and knowledge management (cikm), pp. 509   518.

miltsakaki, e. and k. kukich (2004). evaluation of text coherence for electronic essay

scoring systems. natural language engineering 10(1), 25   55.

minka, t. p. (1999). from id48 to linear dynamical systems. tech. rep.

531, vision and modeling group of media lab, mit.

minsky, m. (1974). a framework for representing knowledge. technical report 306, mit

ai laboratory.

minsky, m. and s. papert (1969). id88s. mit press.

mintz, m., s. bills, r. snow, and d. jurafsky (2009). distant supervision for relation ex-

traction without labeled data. see acl (2009), pp. 1003   1011.

mirza, p., r. sprugnoli, s. tonelli, and m. speranza (2014). annotating causality in the
in proceedings of the eacl 2014 workshop on computational ap-

tempeval-3 corpus.
proaches to causality in language (catocl), pp. 10   19.

misra, d. k. and y. artzi (2016). neural shift-reduce id35 id29. see emn (2016).

mitchell, j. and m. lapata (2010). composition in distributional models of semantics.

cognitive science 34(8), 1388   1429.

jacob eisenstein. draft of november 13, 2018.

bibliography

533

miwa, m. and m. bansal (2016). end-to-end id36 using lstms on sequences

and tree structures. see acl (2016), pp. 1105   1116.

mnih, a. and g. hinton (2007). three new id114 for statistical language mod-
elling. in proceedings of the international conference on machine learning (icml), pp. 641   
648.

mnih, a. and g. e. hinton (2008). a scalable hierarchical distributed language model. in

neural information processing systems (nips), pp. 1081   1088.

mnih, a. and y. w. teh (2012). a fast and simple algorithm for training neural probabilistic

language models. see icm (2012).

mohammad, s. m. and p. d. turney (2013). id104 a word   emotion association

lexicon. computational intelligence 29(3), 436   465.

mohri, m., f. pereira, and m. riley (2002). weighted    nite-state transducers in speech

recognition. computer speech & language 16(1), 69   88.

mohri, m., a. rostamizadeh, and a. talwalkar (2012). foundations of machine learning.

mit press.

montague, r. (1973). the proper treatment of quanti   cation in ordinary english. in ap-

proaches to natural language, pp. 221   242. springer.

moore, j. d. and c. l. paris (1993). planning text for advisory dialogues: capturing inten-

tional and rhetorical information. computational linguistics 19(4), 651   694.

morante, r. and e. blanco (2012). *sem 2012 shared task: resolving the scope and focus of
negation. in proceedings of the first joint conference on lexical and computational semantics,
pp. 265   274. association for computational linguistics.

morante, r. and w. daelemans (2009). learning the scope of hedge cues in biomedical
in proceedings of the workshop on current trends in biomedical natural language

texts.
processing, pp. 28   36. association for computational linguistics.

morante, r. and c. sporleder (2012). modality and negation: an introduction to the

special issue. computational linguistics 38(2), 223   260.

mostafazadeh, n., a. grealish, n. chambers, j. allen, and l. vanderwende (2016).
caters: causal and temporal relation scheme for semantic annotation of event struc-
tures. in proceedings of the fourth workshop on events, pp. 51   61. association for com-
putational linguistics.

mueller, t., h. schmid, and h. sch   utze (2013). ef   cient higher-order crfs for morpho-

logical tagging. see emn (2013), pp. 322   332.

under contract with mit press, shared under cc-by-nc-nd license.

534

bibliography

m   uller, c. and m. strube (2006). multi-level annotation of linguistic data with mmax2.
corpus technology and language pedagogy: new resources, new tools, new methods 3, 197   
214.

muralidharan, a. and m. a. hearst (2013). supporting exploratory text analysis in litera-

ture study. literary and linguistic computing 28(2), 283   295.

murphy, k. p. (2012). machine learning: a probabilistic perspective. the mit press.

nakagawa, t., k. inui, and s. kurohashi (2010). dependency tree-based sentiment classi-

   cation using crfs with hidden variables. see naa (2010), pp. 786   794.

nakazawa, t., m. yaguchi, k. uchimoto, m. utiyama, e. sumita, s. kurohashi, and h. isa-
hara (2016). aspec: asian scienti   c paper excerpt corpus. see lre (2016), pp. 2204   2208.

navigli, r. (2009). id51: a survey. acm computing surveys 41(2),

10.

neal, r. m. and g. e. hinton (1998). a view of the em algorithm that justi   es incremental,

sparse, and other variants. in learning in id114, pp. 355   368. springer.

nenkova, a. and k. mckeown (2012). a survey of text summarization techniques. in

mining text data, pp. 43   76. springer.

neubig, g. (2017). id4 and sequence-to-sequence models: a tu-

torial. arxiv preprint arxiv:1703.01619.

neubig, g., c. dyer, y. goldberg, a. matthews, w. ammar, a. anastasopoulos, m. balles-
teros, d. chiang, d. clothiaux, t. cohn, k. duh, m. faruqui, c. gan, d. garrette,
y. ji, l. kong, a. kuncoro, g. kumar, c. malaviya, p. michel, y. oda, m. richardson,
n. saphra, s. swayamdipta, and p. yin (2017). dynet: the dynamic neural network
toolkit.

neubig, g., y. goldberg, and c. dyer (2017). on-the-   y operation batching in dynamic

computation graphs. see nip (2017).

neubig, g., m. sperber, x. wang, m. felix, a. matthews, s. padmanabhan, y. qi, d. s.
sachan, p. arthur, p. godard, j. hewitt, r. riad, and l. wang (2018). xid4: the ex-
tensible id4 toolkit. in conference of the association for machine
translation in the americas (amta).

neuhaus, p. and n. br  oker (1997). the complexity of recognition of linguistically ade-
quate dependency grammars. in proceedings of the european chapter of the association for
computational linguistics (eacl), pp. 337   343.

jacob eisenstein. draft of november 13, 2018.

bibliography

535

newman, d., c. chemudugunta, and p. smyth (2006). statistical entity-topic models. in

proceedings of knowledge discovery and data mining (kdd), pp. 680   686.

ng, v. (2010). supervised noun phrase coreference research: the    rst    fteen years. see

acl (2010), pp. 1396   1411.

nguyen, d. and a. s. dogru  oz (2013). word level language identi   cation in online multi-

lingual communication. see emn (2013).

nguyen, d. t. and s. joty (2017). a neural local coherence model. see acl (2017), pp.

1320   1330.

nigam, k., a. k. mccallum, s. thrun, and t. mitchell (2000). text classi   cation from

labeled and unlabeled documents using em. machine learning 39(2-3), 103   134.

nirenburg, s. and y. wilks (2001). what   s in a symbol: ontology, representation and lan-

guage. journal of experimental & theoretical arti   cial intelligence 13(1), 9   23.

nivre, j. (2008). algorithms for deterministic incremental id33. computa-

tional linguistics 34(4), 513   553.

nivre, j., m.-c. de marneffe, f. ginter, y. goldberg, j. haji  c, c. d. manning, r. mcdonald,
s. petrov, s. pyysalo, n. silveira, r. tsarfaty, and d. zeman (2016). universal depen-
dencies v1: a multilingual treebank collection. see lre (2016).

nivre, j. and j. nilsson (2005). pseudo-projective id33. see acl (2005), pp.

99   106.

novikoff, a. b. (1962). on convergence proofs on id88s. in proceedings of the sym-

posium on the mathematical theory of automata, volume 12, pp. 615      622.

och, f. j. and h. ney (2003). a systematic comparison of various statistical alignment

models. computational linguistics 29(1), 19   51.

o   connor, b., m. krieger, and d. ahn (2010). tweetmotif: exploratory search and topic
summarization for twitter. in proceedings of the international conference on web and social
media (icwsm), pp. 384   385.

oepen, s., m. kuhlmann, y. miyao, d. zeman, d. flickinger, j. hajic, a. ivanova, and
y. zhang (2014). semeval 2014 task 8: broad-coverage semantic id33.
in proceedings of the 8th international workshop on semantic evaluation (semeval 2014), pp.
63   72.

o   azer, k. and `i. kuru  oz (1994). tagging and id60 of turkish
text. in proceedings of the fourth conference on applied natural language processing, pp. 144   
149.

under contract with mit press, shared under cc-by-nc-nd license.

536

bibliography

ohta, t., y. tateisi, and j.-d. kim (2002). the genia corpus: an annotated research abstract
corpus in molecular biology domain. in proceedings of the second international conference
on human language technology research, pp. 82   86. morgan kaufmann publishers inc.

onishi, t., h. wang, m. bansal, k. gimpel, and d. mcallester (2016). who did what: a

large-scale person-centered cloze dataset. see emn (2016), pp. 2230   2235.

owoputi, o., b. o   connor, c. dyer, k. gimpel, n. schneider, and n. a. smith (2013).
improved part-of-speech tagging for online conversational text with word clusters. see
naa (2013), pp. 380   390.

packard, w., e. m. bender, j. read, s. oepen, and r. dridan (2014). simple negation scope
resolution through deep parsing: a semantic solution to a semantic problem. see acl
(2014), pp. 69   78.

paice, c. d. (1990). another stemmer. in acm sigir forum, volume 24, pp. 56   61.

pak, a. and p. paroubek (2010). twitter as a corpus for id31 and opinion
mining. in proceedings of the language resources and evaluation conference, pp. 1320   1326.

palmer, f. r. (2001). mood and modality. cambridge university press.

palmer, m., d. gildea, and p. kingsbury (2005). the proposition bank: an annotated

corpus of semantic roles. computational linguistics 31(1), 71   106.

pan, s. j. and q. yang (2010). a survey on id21. ieee transactions on knowledge

and data engineering 22(10), 1345   1359.

pang, b. and l. lee (2004). a sentimental education: id31 using subjectivity

summarization based on minimum cuts. see acl (2004), pp. 271   278.

pang, b. and l. lee (2005). seeing stars: exploiting class relationships for sentiment cate-

gorization with respect to rating scales. see acl (2005), pp. 115   124.

pang, b. and l. lee (2008). opinion mining and id31. foundations and trends

in information retrieval 2(1-2), 1   135.

pang, b., l. lee, and s. vaithyanathan (2002). thumbs up?: sentiment classi   cation using

machine learning techniques. see emn (2002), pp. 79   86.

papineni, k., s. roukos, t. ward, and w.-j. zhu (2002). id7: a method for automatic

evaluation of machine translation. see acl (2002), pp. 311   318.

park, j. and c. cardie (2012). improving implicit discourse relation recognition through

feature set optimization. see sig (2012), pp. 108   112.

jacob eisenstein. draft of november 13, 2018.

bibliography

537

parsons, t. (1990). events in the semantics of english, volume 5. mit press.

pascanu, r., t. mikolov, and y. bengio (2013). on the dif   culty of training recurrent neural

networks. see icm (2013), pp. 1310   1318.

paul, m., m. federico, and s. st   uker (2010). overview of the iwslt 2010 evaluation cam-

paign. in international workshop on spoken language translation (iwslt) 2010.

pedersen, t., s. patwardhan, and j. michelizzi (2004). id138::similarity - measuring the

relatedness of concepts. see naa (2004), pp. 38   41.

pedregosa, f., g. varoquaux, a. gramfort, v. michel, b. thirion, o. grisel, m. blon-
del, p. prettenhofer, r. weiss, v. dubourg, j. vanderplas, a. passos, d. cournapeau,
m. brucher, m. perrot, and e. duchesnay (2011). scikit-learn: machine learning in
python. journal of machine learning research 12, 2825   2830.

pei, w., t. ge, and b. chang (2015). an effective neural network model for graph-based

id33. see acl (2015), pp. 313   322.

peldszus, a. and m. stede (2013). from argument diagrams to argumentation mining
in texts: a survey. international journal of cognitive informatics and natural intelligence
(ijcini) 7(1), 1   31.

peldszus, a. and m. stede (2015). an annotated corpus of argumentative microtexts. in

proceedings of the first conference on argumentation.

peng, f., f. feng, and a. mccallum (2004). chinese segmentation and new word detec-
in proceedings of the international conference on

tion using conditional random    elds.
computational linguistics (coling), pp. 562   568.

pennington, j., r. socher, and c. manning (2014). glove: global vectors for word repre-

sentation. see emn (2014), pp. 1532   1543.

pereira, f. and y. schabes (1992).

inside-outside reestimation from partially bracketed
in proceedings of the association for computational linguistics (acl), pp. 128   

corpora.
135.

pereira, f. c. n. and s. m. shieber (2002). prolog and natural-language analysis. microtome

publishing.

peters, m. e., m. neumann, m. iyyer, m. gardner, c. clark, k. lee, and l. zettlemoyer

(2018). deep contextualized word representations. see naa (2018).

peterson, w. w., t. g. birdsall, and w. c. fox (1954). the theory of signal detectability.

transactions of the ire professional group on id205 4(4), 171   212.

under contract with mit press, shared under cc-by-nc-nd license.

538

bibliography

petrov, s., l. barrett, r. thibaux, and d. klein (2006). learning accurate, compact, and

interpretable tree annotation. see acl (2006).

petrov, s., d. das, and r. mcdonald (2012). a universal part-of-speech tagset. in proceed-

ings of the language resources and evaluation conference.

petrov, s. and r. mcdonald (2012). overview of the 2012 shared task on parsing the web.

in notes of the first workshop on syntactic analysis of non-canonical language (sancl).

pinker, s. (2003). the language instinct: how the mind creates language. penguin uk.

pinter, y., r. guthrie, and j. eisenstein (2017). mimicking id27s using sub-

word id56s. see emn (2017).

pitler, e., a. louis, and a. nenkova (2009). automatic sense prediction for implicit dis-

course relations in text. see acl (2009).

pitler, e. and a. nenkova (2009). using syntax to disambiguate explicit discourse connec-

tives in text. see acl (2009), pp. 13   16.

pitler, e., m. raghupathy, h. mehta, a. nenkova, a. lee, and a. joshi (2008). easily iden-
ti   able discourse relations. in proceedings of the international conference on computational
linguistics (coling), pp. 87   90.

plank, b., a. s  gaard, and y. goldberg (2016). multilingual part-of-speech tagging with

bidirectional long short-term memory models and auxiliary loss. see acl (2016).

poesio, m., r. stevenson, b. di eugenio, and j. hitzeman (2004). centering: a parametric

theory and its instantiations. computational linguistics 30(3), 309   363.

polanyi, l. and a. zaenen (2006). contextual valence shifters. in computing attitude and

affect in text: theory and applications. springer.

ponzetto, s. p. and m. strube (2006). exploiting id14, id138 and

wikipedia for coreference resolution. see naa (2006), pp. 192   199.

ponzetto, s. p. and m. strube (2007). knowledge derived from wikipedia for computing

semantic relatedness. journal of arti   cial intelligence research 30, 181   212.

poon, h. and p. domingos (2008). joint unsupervised coreference resolution with markov

logic. see emn (2008), pp. 650   659.

poon, h. and p. domingos (2009). unsupervised id29. see emn (2009), pp.

1   10.

popel, m., d. marecek, j. step  anek, d. zeman, and z. zabokrtsk`y (2013). coordination

structures in dependency treebanks. see acl (2013), pp. 517   527.

jacob eisenstein. draft of november 13, 2018.

bibliography

539

popescu, a.-m., o. etzioni, and h. kautz (2003). towards a theory of natural language

interfaces to databases. in proceedings of intelligent user interfaces (iui), pp. 149   157.

poplack, s. (1980). sometimes i   ll start a sentence in spanish y termino en espa   nol: toward

a typology of code-switching. linguistics 18(7-8), 581   618.

porter, m. f. (1980). an algorithm for suf   x stripping. program 14(3), 130   137.

prabhakaran, v., o. rambow, and m. diab (2010). automatic committed belief tagging.

see col (2010), pp. 1014   1022.

pradhan, s., x. luo, m. recasens, e. hovy, v. ng, and m. strube (2014). scoring corefer-
ence partitions of predicted mentions: a reference implementation. see acl (2014), pp.
30   35.

pradhan, s., l. ramshaw, m. marcus, m. palmer, r. weischedel, and n. xue (2011).
conll-2011 shared task: modeling unrestricted coreference in ontonotes. see con
(2011), pp. 1   27.

pradhan, s., w. ward, k. hacioglu, j. h. martin, and d. jurafsky (2005). semantic role

labeling using different syntactic views. see acl (2005), pp. 581   588.

prasad, r., n. dinesh, a. lee, e. miltsakaki, l. robaldo, a. joshi, and b. webber (2008).
the penn discourse treebank 2.0. in proceedings of the language resources and evaluation
conference.

punyakanok, v., d. roth, and w.-t. yih (2008). the importance of syntactic parsing and

id136 in id14. computational linguistics 34(2), 257   287.

pustejovsky, j., p. hanks, r. saur    , a. see, r. gaizauskas, a. setzer, d. radev, b. sundheim,
d. day, l. ferro, et al. (2003). the timebank corpus. in proceedings of corpus linguistics,
pp. 647   656.

pustejovsky, j., b. ingria, r. sauri, j. castano, j. littman, r. gaizauskas, a. setzer, g. katz,
and i. mani (2005). the speci   cation language timeml. in the language of time: a reader,
pp. 545   557. oxford university press.

qin, l., z. zhang, h. zhao, z. hu, and e. xing (2017). adversarial connective-exploiting

networks for implicit discourse relation classi   cation. see acl (2017), pp. 1006   1017.

qiu, g., b. liu, j. bu, and c. chen (2011). opinion word expansion and target extraction

through double propagation. computational linguistics 37(1), 9   27.

quattoni, a., s. wang, l.-p. morency, m. collins, and t. darrell (2007). hidden conditional

random    elds. ieee transactions on pattern analysis and machine intelligence 29(10).

under contract with mit press, shared under cc-by-nc-nd license.

540

bibliography

rahman, a. and v. ng (2011). narrowing the modeling gap: a cluster-ranking approach

to coreference resolution. journal of arti   cial intelligence research 40, 469   521.

rajpurkar, p., j. zhang, k. lopyrev, and p. liang (2016). squad: 100,000+ questions for

machine comprehension of text. see emn (2016), pp. 2383   2392.

ranzato, m., s. chopra, m. auli, and w. zaremba (2016). sequence level training with

recurrent neural networks. see icl (2016).

rao, d., d. yarowsky, a. shreevats, and m. gupta (2010). classifying latent user attributes

in twitter. in proceedings of workshop on search and mining user-generated contents.

ratinov, l. and d. roth (2009). design challenges and misconceptions in named entity
recognition. in proceedings of the conference on natural language learning (conll), pp.
147   155.

ratinov, l., d. roth, d. downey, and m. anderson (2011). local and global algorithms

for disambiguation to wikipedia. see acl (2011), pp. 1375   1384.

ratliff, n. d., j. a. bagnell, and m. zinkevich (2007). (approximate) subgradient methods
for id170. in proceedings of arti   cial intelligence and statistics (aistats),
pp. 380   387.

ratnaparkhi, a. (1996). a maximum id178 model for part-of-speech tagging. in pro-

ceedings of empirical methods for natural language processing (emnlp), pp. 133   142.

ratnaparkhi, a., j. reynar, and s. roukos (1994). a maximum id178 model for preposi-
tional phrase attachment. in proceedings of the workshop on human language technology,
pp. 250   255.

read, j. (2005). using emoticons to reduce dependency in machine learning techniques for

sentiment classi   cation. in proceedings of the acl student research workshop, pp. 43   48.

reisinger, d., r. rudinger, f. ferraro, c. harman, k. rawlins, and b. v. durme (2015).
semantic proto-roles. transactions of the association for computational linguistics 3, 475   
488.

reisinger, j. and r. j. mooney (2010). multi-prototype vector-space models of word mean-

ing. see naa (2010), pp. 109   117.

reiter, e. and r. dale (2000). building id86 systems. cambridge

university press.

resnik, p., m. b. olsen, and m. diab (1999). the bible as a parallel corpus: annotating the

   book of 2000 tongues   . computers and the humanities 33(1-2), 129   153.

jacob eisenstein. draft of november 13, 2018.

bibliography

541

resnik, p. and n. a. smith (2003). the web as a parallel corpus. computational linguis-

tics 29(3), 349   380.

ribeiro, f. n., m. ara   ujo, p. gonc  alves, m. a. gonc  alves, and f. benevenuto (2016).
sentibench-a benchmark comparison of state-of-the-practice id31 meth-
ods. epj data science 5(1), 1   29.

richardson, m., c. j. burges, and e. renshaw (2013). mctest: a challenge dataset for the

open-domain machine comprehension of text. see emn (2013), pp. 193   203.

riedel, s., l. yao, and a. mccallum (2010). modeling relations and their mentions without
labeled text. in proceedings of the european conference on machine learning and principles
and practice of knowledge discovery in databases (ecml), pp. 148   163.

riedl, m. o. and r. m. young (2010). narrative planning: balancing plot and character.

journal of arti   cial intelligence research 39, 217   268.

rieser, v. and o. lemon (2011). id23 for adaptive dialogue systems: a data-
driven methodology for dialogue management and id86. springer sci-
ence & business media.

riloff, e. (1996). automatically generating extraction patterns from untagged text.
proceedings of the national conference on arti   cial intelligence (aaai), pp. 1044   1049.

in

riloff, e. and j. wiebe (2003). learning extraction patterns for subjective expressions. in
proceedings of the 2003 conference on empirical methods in natural language processing, pp.
105   112. association for computational linguistics.

ritchie, g. (2001). current directions in computational humour. arti   cial intelligence re-

view 16(2), 119   135.

ritter, a., c. cherry, and w. b. dolan (2011). data-driven response generation in social

media. see emn (2011), pp. 583   593.

ritter, a., s. clark, mausam, and o. etzioni (2011). id39 in tweets:

an experimental study. see emn (2011).

roark, b., m. saraclar, and m. collins (2007). discriminative id165 id38.

computer speech & language 21(2), 373   392.

robert, c. and g. casella (2013). monte carlo statistical methods. springer science & busi-

ness media.

rosenfeld, r. (1996). a maximum id178 approach to adaptive statistical language mod-

elling. computer speech & language 10(3), 187   228.

under contract with mit press, shared under cc-by-nc-nd license.

542

bibliography

ross, s., g. gordon, and d. bagnell (2011). a reduction of imitation learning and struc-

tured prediction to no-regret online learning. see ais (2011), pp. 627   635.

roy, n., j. pineau, and s. thrun (2000). spoken dialogue management using probabilistic
reasoning. in proceedings of the association for computational linguistics (acl), pp. 93   
100.

rudinger, r., j. naradowsky, b. leonard, and b. van durme (2018). gender bias in coref-

erence resolution. see naa (2018).

rudnicky, a. and w. xu (1999). an agenda-based dialog management architecture for
spoken language systems. in ieee automatic id103 and understanding work-
shop, volume 13.

rush, a. m., s. chopra, and j. weston (2015). a neural attention model for abstractive

sentence summarization. see emn (2015), pp. 379   389.

rush, a. m., d. sontag, m. collins, and t. jaakkola (2010). on id209 and
id135 relaxations for natural language processing. see emn (2010), pp.
1   11.

russell, s. j. and p. norvig (2009). arti   cial intelligence: a modern approach (3rd ed.). prentice

hall.

rutherford, a., v. demberg, and n. xue (2017). a systematic study of neural discourse
models for implicit discourse relation. in proceedings of the european chapter of the asso-
ciation for computational linguistics (eacl), pp. 281   291.

rutherford, a. t. and n. xue (2014). discovering implicit discourse relations through

brown cluster pair representation and coreference patterns. see eac (2014).

sag, i. a., t. baldwin, f. bond, a. copestake, and d. flickinger (2002). multiword expres-
sions: a pain in the neck for nlp. in international conference on intelligent text processing
and computational linguistics, pp. 1   15. springer.

sagae, k. (2009). analysis of discourse structure with syntactic dependencies and data-
driven id132. in proceedings of the 11th international conference on parsing
technologies, pp. 81   84.

santos, c. d. and b. zadrozny (2014). learning character-level representations for part-

of-speech tagging. see icm (2014), pp. 1818   1826.

sato, m.-a. and s. ishii (2000). on-line em algorithm for the normalized gaussian network.

neural computation 12(2), 407   432.

jacob eisenstein. draft of november 13, 2018.

bibliography

543

saur    , r. and j. pustejovsky (2009). factbank: a corpus annotated with event factuality.

language resources and evaluation 43(3), 227.

saxe, a. m., j. l. mcclelland, and s. ganguli (2014). exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. in proceedings of the international
conference on learning representations (iclr).

schank, r. c. and r. abelson (1977). scripts, goals, plans, and understanding. hillsdale, nj:

erlbaum.

schapire, r. e. and y. singer (2000). boostexter: a boosting-based system for text catego-

rization. machine learning 39(2-3), 135   168.

schaul, t., s. zhang, and y. lecun (2013). no more pesky learning rates. see icm (2013),

pp. 343   351.

schnabel, t., i. labutov, d. mimno, and t. joachims (2015). evaluation methods for unsu-

pervised id27s. see emn (2015), pp. 298   307.

schneider, n., j. flanigan, and t. o   gorman (2015). the logic of amr: practical, uni   ed,

graph-based sentence semantics for nlp. see naa (2015), pp. 4   5.

sch   utze, h. (1998). automatic word sense discrimination. computational linguistics 24(1),

97   123.

schwarm, s. e. and m. ostendorf (2005). reading level assessment using support vector

machines and statistical language models. see acl (2005), pp. 523   530.

see, a., p. j. liu, and c. d. manning (2017). get to the point: summarization with pointer-

generator networks. see acl (2017), pp. 1073   1083.

sennrich, r., b. haddow, and a. birch (2016). id4 of rare words

with subword units. see acl (2016), pp. 1715   1725.

serban, i. v., a. sordoni, y. bengio, a. c. courville, and j. pineau (2016). building end-
to-end dialogue systems using generative hierarchical neural network models. see aaa
(2016), pp. 3776   3784.

settles, b. (2012). active learning. synthesis lectures on arti   cial intelligence and machine

learning 6(1), 1   114.

shang, l., z. lu, and h. li (2015). neural responding machine for short-text conversation.

see acl (2015), pp. 1577   1586.

shen, d. and m. lapata (2007). using semantic roles to improve id53. see

emn (2007), pp. 12   21.

under contract with mit press, shared under cc-by-nc-nd license.

544

bibliography

shen, s., y. cheng, z. he, w. he, h. wu, m. sun, and y. liu (2016). minimum risk training

for id4. see acl (2016), pp. 1683   1692.

shen, w., j. wang, and j. han (2015). entity linking with a knowledge base: issues, tech-
niques, and solutions. ieee transactions on knowledge and data engineering 27(2), 443   
460.

shieber, s. m. (1985). evidence against the context-freeness of natural language. linguistics

and philosophy 8(3), 333   343.

siegelmann, h. t. and e. d. sontag (1995). on the computational power of neural nets.

journal of computer and system sciences 50(1), 132   150.

singh, s., a. subramanya, f. pereira, and a. mccallum (2011). large-scale cross-
document coreference using distributed id136 and id187. see acl
(2011), pp. 793   803.

sipser, m. (2012). introduction to the theory of computation. cengage learning.

smith, d. a. and j. eisner (2006). minimum risk annealing for training id148.

see acl (2006), pp. 787   794.

smith, d. a. and j. eisner (2008). id33 by belief propagation. see emn

(2008), pp. 145   156.

smith, d. a. and n. a. smith (2007). probabilistic models of nonprojective dependency

trees. see emn (2007), pp. 132   140.

smith, n. a. (2011). linguistic structure prediction. synthesis lectures on human language

technologies 4(2), 1   274.

snover, m., b. dorr, r. schwartz, l. micciulla, and j. makhoul (2006). a study of transla-
tion edit rate with targeted human annotation. in proceedings of association for machine
translation in the americas (amta).

snow, r., b. o   connor, d. jurafsky, and a. y. ng (2008). cheap and fast   but is it good?:
evaluating non-expert annotations for natural language tasks. see emn (2008), pp. 254   
263.

snyder, b. and r. barzilay (2007). database-text alignment via structured multilabel clas-

si   cation. see ijc (2007), pp. 1713   1718.

socher, r., j. bauer, c. d. manning, and a. y. ng (2013). parsing with compositional vector

grammars. see acl (2013).

jacob eisenstein. draft of november 13, 2018.

bibliography

545

socher, r., b. huval, c. d. manning, and a. y. ng (2012). semantic compositionality

through recursive matrix-vector spaces. see emn (2012), pp. 1201   1211.

socher, r., a. perelygin, j. y. wu, j. chuang, c. d. manning, a. y. ng, and c. potts (2013).
recursive deep models for semantic compositionality over a sentiment treebank. see
emn (2013).

s  gaard, a. (2013). semi-supervised learning and id20 in natural language

processing. synthesis lectures on human language technologies 6(2), 1   103.

solorio, t. and y. liu (2008). learning to predict code-switching points. see emn (2008),

pp. 973   981.

somasundaran, s., g. namata, j. wiebe, and l. getoor (2009). supervised and unsuper-
vised methods in employing discourse relations for improving opinion polarity classi-
   cation. see emn (2009).

somasundaran, s. and j. wiebe (2009). recognizing stances in online debates. see acl

(2009), pp. 226   234.

song, l., b. boots, s. m. siddiqi, g. j. gordon, and a. j. smola (2010). hilbert space
embeddings of id48. in proceedings of the international conference on
machine learning (icml), pp. 991   998.

song, l., y. zhang, x. peng, z. wang, and d. gildea (2016). amr-to-text generation as a

traveling salesman problem. see emn (2016), pp. 2084   2089.

soon, w. m., h. t. ng, and d. c. y. lim (2001). a machine learning approach to corefer-

ence resolution of noun phrases. computational linguistics 27(4), 521   544.

sordoni, a., m. galley, m. auli, c. brockett, y. ji, m. mitchell, j.-y. nie, j. gao, and b. dolan
(2015). a neural network approach to context-sensitive generation of conversational
responses. see naa (2015).

soricut, r. and d. marcu (2003). sentence level discourse parsing using syntactic and

lexical information. see naa (2003), pp. 149   156.

sowa, j. f. (2000). id99: logical, philosophical, and computational founda-

tions. paci   c grove, ca: brooks/cole.

sp  arck jones, k. (1972). a statistical interpretation of term speci   city and its application

in retrieval. journal of documentation 28(1), 11   21.

spitkovsky, v. i., h. alshawi, d. jurafsky, and c. d. manning (2010). viterbi training

improves unsupervised id33. see con (2010), pp. 9   17.

under contract with mit press, shared under cc-by-nc-nd license.

546

bibliography

sporleder, c. and m. lapata (2005). discourse chunking and its application to sentence

compression. see emn (2005), pp. 257   264.

sproat, r., a. black, s. chen, s. kumar, m. ostendorf, and c. richards (2001). normaliza-

tion of non-standard words. computer speech & language 15(3), 287   333.

sproat, r., w. gale, c. shih, and n. chang (1996). a stochastic    nite-state word-

segmentation algorithm for chinese. computational linguistics 22(3), 377   404.

sra, s., s. nowozin, and s. j. wright (2012). optimization for machine learning. mit press.

srivastava, n., g. hinton, a. krizhevsky, i. sutskever, and r. salakhutdinov (2014).
dropout: a simple way to prevent neural networks from over   tting. the journal of
machine learning research 15(1), 1929   1958.

srivastava, r. k., k. greff, and j. schmidhuber (2015). training very deep networks. see

nip (2015), pp. 2377   2385.

stab, c. and i. gurevych (2014a). annotating argument components and relations in per-

suasive essays. see col (2014), pp. 1501   1510.

stab, c. and i. gurevych (2014b). identifying argumentative discourse structures in per-

suasive essays. see emn (2014), pp. 46   56.

stede, m. (2011, nov). discourse processing, volume 4 of synthesis lectures on human lan-

guage technologies. morgan & claypool publishers.

steedman, m. and j. baldridge (2011). id35.

transformational syntax: formal and explicit models of grammar. wiley-blackwell.

in non-

stenetorp, p., s. pyysalo, g. topi  c, t. ohta, s. ananiadou, and j. tsujii (2012). brat: a

web-based tool for nlp-assisted text annotation. see eac (2012), pp. 102   107.

stern, m., j. andreas, and d. klein (2017). a minimal span-based neural constituency

parser. see acl (2017).

stolcke, a., k. ries, n. coccaro, e. shriberg, r. bates, d. jurafsky, p. taylor, r. martin,
c. van ess-dykema, and m. meteer (2000). dialogue act modeling for automatic tag-
ging and recognition of conversational speech. computational linguistics 26(3), 339   373.

stone, p. j. (1966). the general inquirer: a computer approach to content analysis. the mit

press.

stoyanov, v., n. gilbert, c. cardie, and e. riloff (2009). conundrums in noun phrase
coreference resolution: making sense of the state-of-the-art. see acl (2009), pp. 656   664.

jacob eisenstein. draft of november 13, 2018.

bibliography

547

strang, g. (2016).

introduction to id202 (fifth ed.). wellesley, ma: wellesley-

cambridge press.

strubell, e., p. verga, d. belanger, and a. mccallum (2017). fast and accurate entity recog-

nition with iterated dilated convolutions. see emn (2017).

suchanek, f. m., g. kasneci, and g. weikum (2007). yago: a core of semantic knowledge.

in proceedings of the conference on world-wide web (www), pp. 697   706.

sun, x., t. matsuzaki, d. okanohara, and j. tsujii (2009). latent variable id88 algo-
rithm for structured classi   cation. in proceedings of the international joint conference on
arti   cial intelligence (ijcai), pp. 1236   1242.

sun, y., l. lin, d. tang, n. yang, z. ji, and x. wang (2015). modeling mention, context and
entity with neural networks for entity disambiguation. in proceedings of the international
joint conference on arti   cial intelligence (ijcai), pp. 1333   1339.

sundermeyer, m., r. schl   uter, and h. ney (2012). lstm neural networks for language
modeling. in proceedings of the international speech communication association (inter-
speech).

surdeanu, m., j. tibshirani, r. nallapati, and c. d. manning (2012). multi-instance multi-

label learning for id36. see emn (2012), pp. 455   465.

sutskever, i., o. vinyals, and q. v. le (2014). sequence to sequence learning with neural

networks. see nip (2014), pp. 3104   3112.

sutton, r. s. and a. g. barto (1998). id23: an introduction, volume 1. mit

press cambridge.

sutton, r. s., d. a. mcallester, s. p. singh, and y. mansour (2000). id189
for id23 with function approximation. in neural information process-
ing systems (nips), pp. 1057   1063.

suzuki, j., s. takase, h. kamigaito, m. morishita, and m. nagata (2018). an empirical
study of building a strong baseline for constituency parsing. see acl (2018), pp. 612   
618.

sweeney, l. (2013). discrimination in online ad delivery. queue 11(3), 10.

taboada, m., j. brooke, m. to   loski, k. voll, and m. stede (2011). lexicon-based methods

for id31. computational linguistics 37(2), 267   307.

taboada, m. and w. c. mann (2006). rhetorical structure theory: looking back and mov-

ing ahead. discourse studies 8(3), 423   459.

under contract with mit press, shared under cc-by-nc-nd license.

548

bibliography

t  ackstr  om, o., k. ganchev, and d. das (2015). ef   cient id136 and structured learning
for id14. transactions of the association for computational linguistics 3,
29   41.

t  ackstr  om, o., r. mcdonald, and j. uszkoreit (2012). cross-lingual word clusters for

direct transfer of linguistic structure. see naa (2012), pp. 477   487.

tang, d., b. qin, and t. liu (2015). document modeling with gated recurrent neural

network for sentiment classi   cation. see emn (2015), pp. 1422   1432.

taskar, b., c. guestrin, and d. koller (2003). max-margin markov networks. in neural

information processing systems (nips).

tausczik, y. r. and j. w. pennebaker (2010). the psychological meaning of words: liwc
and computerized text analysis methods. journal of language and social psychology 29(1),
24   54.

teh, y. w. (2006). a hierarchical bayesian language model based on pitman-yor processes.

see acl (2006), pp. 985   992.

tesni`ere, l. (1966).   el  ements de syntaxe structurale (second ed.). paris: klincksieck.

teufel, s., j. carletta, and m. moens (1999). an annotation scheme for discourse-level
argumentation in research articles. in proceedings of the european chapter of the association
for computational linguistics (eacl), pp. 110   117.

teufel, s. and m. moens (2002). summarizing scienti   c articles: experiments with rele-

vance and rhetorical status. computational linguistics 28(4), 409   445.

thomas, m., b. pang, and l. lee (2006). get out the vote: determining support or opposi-
tion from congressional    oor-debate transcripts. in proceedings of empirical methods for
natural language processing (emnlp), pp. 327   335.

tibshirani, r. (1996). regression shrinkage and selection via the lasso. journal of the royal

statistical society. series b (methodological), 267   288.

titov, i. and j. henderson (2007). constituent parsing with incremental sigmoid belief

networks. see acl (2007), pp. 632   639.

toutanova, k., d. klein, c. d. manning, and y. singer (2003). feature-rich part-of-speech

tagging with a cyclic dependency network. see naa (2003).

trivedi, r. and j. eisenstein (2013). discourse connectors for latent subjectivity in senti-

ment analysis. see naa (2013), pp. 808   813.

jacob eisenstein. draft of november 13, 2018.

bibliography

549

tromble, r. w. and j. eisner (2006). a fast    nite-state relaxation method for enforcing

global constraints on sequence decoding. see naa (2006).

tsochantaridis, i., t. hofmann, t. joachims, and y. altun (2004). support vector machine
learning for interdependent and structured output spaces. in proceedings of the interna-
tional conference on machine learning (icml).

tsvetkov, y., m. faruqui, w. ling, g. lample, and c. dyer (2015). evaluation of word

vector representations by subspace alignment. see emn (2015), pp. 2049   2054.

tu, z., z. lu, y. liu, x. liu, and h. li (2016). modeling coverage for neural machine

translation. see acl (2016), pp. 76   85.

turian, j., l. ratinov, and y. bengio (2010). word representations: a simple and general

method for semi-supervised learning. see acl (2010), pp. 384   394.

turing, a. m. (2009). computing machinery and intelligence. in r. epstein, g. roberts,

and g. beber (eds.), parsing the turing test, pp. 23   65. springer.

turney, p. d. and p. pantel (2010). from frequency to meaning: vector space models of

semantics. journal of arti   cial intelligence research 37, 141   188.

tutin, a. and r. kittredge (1992). lexical choice in context: generating procedural texts.

see col (1992), pp. 763   769.

twain, m. (1997). a tramp abroad. new york: penguin.

tzeng, e., j. hoffman, t. darrell, and k. saenko (2015). simultaneous deep transfer across

domains and tasks. see icc (2015), pp. 4068   4076.

usunier, n., d. buffoni, and p. gallinari (2009). ranking with ordered weighted pairwise

classi   cation. see icm (2009), pp. 1057   1064.

uthus, d. c. and d. w. aha (2013). the ubuntu chat corpus for multiparticipant chat

analysis. in aaai spring symposium: analyzing microtext, volume 13, pp. 01.

utiyama, m. and h. isahara (2001). a statistical model for domain-independent text seg-

mentation. see acl (2001), pp. 499   506.

utiyama, m. and h. isahara (2007). a comparison of pivot methods for phrase-based

id151. see naa (2007), pp. 484   491.

uzuner,   o., x. zhang, and t. sibanda (2009). machine learning and rule-based approaches
to assertion classi   cation. journal of the american medical informatics association 16(1),
109   115.

under contract with mit press, shared under cc-by-nc-nd license.

550

bibliography

vadas, d. and j. r. curran (2011). parsing noun phrases in the id32. computa-

tional linguistics 37(4), 753   809.

van eynde, f. (2006). np-internal agreement and the structure of the noun phrase. journal

of linguistics 42(1), 139   186.

van gael, j., a. vlachos, and z. ghahramani (2009). the in   nite id48 for unsupervised

id52. see emn (2009), pp. 678   687.

vaswani, a., s. bengio, e. brevdo, f. chollet, a. n. gomez, s. gouws, l. jones, l. kaiser,
n. kalchbrenner, n. parmar, r. sepassi, n. shazeer, and j. uszkoreit (2018). ten-
sor2tensor for id4. corr abs/1803.07416.

vaswani, a., n. shazeer, n. parmar, j. uszkoreit, l. jones, a. n. gomez,   . kaiser, and

i. polosukhin (2017). attention is all you need. see nip (2017), pp. 6000   6010.

velldal, e., l.   vrelid, j. read, and s. oepen (2012). speculation and negation: rules,

rankers, and the role of syntax. computational linguistics 38(2), 369   410.

versley, y. (2011). towards    ner-grained tagging of discourse connectives. in proceedings
of the workshop beyound semantics: corpus-based investigations of pragmatic and discourse
phenomena, pp. 2   63.

vilain, m., j. burger, j. aberdeen, d. connolly, and l. hirschman (1995). a model-
in proceedings of the 6th conference on message

theoretic coreference scoring scheme.
understanding, pp. 45   52. association for computational linguistics.

vincent, p., h. larochelle, i. lajoie, y. bengio, and p.-a. manzagol (2010). stacked de-
noising autoencoders: learning useful representations in a deep network with a local
denoising criterion. the journal of machine learning research 11, 3371   3408.

vincze, v., g. szarvas, r. farkas, g. m  ora, and j. csirik (2008). the bioscope corpus:
biomedical texts annotated for uncertainty, negation and their scopes. bmc bioinformat-
ics 9(11), s9.

vinyals, o., a. toshev, s. bengio, and d. erhan (2015). show and tell: a neural image cap-
tion generator. in id161 and pattern recognition (cvpr), 2015 ieee conference
on, pp. 3156   3164. ieee.

viterbi, a. (1967). error bounds for convolutional codes and an asymptotically optimum

decoding algorithm. ieee transactions on id205 13(2), 260   269.

voll, k. and m. taboada (2007). not all words are created equal: extracting semantic
orientation as a function of adjective relevance. in proceedings of australian conference
on arti   cial intelligence.

jacob eisenstein. draft of november 13, 2018.

bibliography

551

wager, s., s. wang, and p. s. liang (2013). dropout training as adaptive id173.

see nip (2013), pp. 351   359.

wainwright, m. j. and m. i. jordan (2008). id114, exponential families, and

variational id136. foundations and trends r(cid:13) in machine learning 1(1-2), 1   305.

walker, m. a. (2000). an application of id23 to dialogue strategy selec-
tion in a spoken dialogue system for email. journal of arti   cial intelligence research 12,
387   416.

walker, m. a., j. e. cahn, and s. j. whittaker (1997). improvising linguistic style: social
and affective bases for agent personality. in proceedings of the    rst international conference
on autonomous agents, pp. 96   105. acm.

wang, c., n. xue, and s. pradhan (2015). a transition-based algorithm for amr parsing.

see naa (2015), pp. 366   375.

wang, h., t. onishi, k. gimpel, and d. mcallester (2017). emergent predication structure
in hidden state vectors of neural readers. in proceedings of the 2nd workshop on represen-
tation learning for nlp, pp. 26   36.

weaver, w. (1955). translation. machine translation of languages 14, 15   23.

webber, b. (2004). d-ltag: extending lexicalized tag to discourse. cognitive sci-

ence 28(5), 751   779.

webber, b., m. egg, and v. kordoni (2012). discourse structure and language technology.

journal of natural language engineering 18(4), 437   490.

webber, b. and a. joshi (2012). discourse structure and computation: past, present and
future. in proceedings of the acl-2012 special workshop on rediscovering 50 years of dis-
coveries, pp. 42   54. association for computational linguistics.

wei, g. c. and m. a. tanner (1990). a monte carlo implementation of the em algorithm
journal of the american statistical

and the poor man   s data augmentation algorithms.
association 85(411), 699   704.

weinberger, k., a. dasgupta, j. langford, a. smola, and j. attenberg (2009). feature

hashing for large scale multitask learning. see icm (2009), pp. 1113   1120.

weizenbaum, j. (1966). eliza   a computer program for the study of natural language

communication between man and machine. communications of the acm 9(1), 36   45.

wellner, b. and j. pustejovsky (2007). automatically identifying the arguments of dis-

course connectives. see emn (2007), pp. 92   101.

under contract with mit press, shared under cc-by-nc-nd license.

552

bibliography

wen, t.-h., m. gasic, n. mrk  si  c, p.-h. su, d. vandyke, and s. young (2015). semantically
conditioned lstm-based id86 for spoken dialogue systems. see
emn (2015), pp. 1711   1721.

weston, j., s. bengio, and n. usunier (2011). wsabie: scaling up to large vocabulary image

annotation. see ijc (2011), pp. 2764   2770.

wiebe, j., t. wilson, and c. cardie (2005). annotating expressions of opinions and emo-

tions in language. language resources and evaluation 39(2), 165   210.

wieting, j., m. bansal, k. gimpel, and k. livescu (2016a). charagram: embedding

words and sentences via character id165s. see emn (2016), pp. 1504   1515.

wieting, j., m. bansal, k. gimpel, and k. livescu (2016b). towards universal paraphrastic

sentence embeddings. see icl (2016).

williams, j. d. and s. young (2007). partially observable id100 for

spoken id71. computer speech & language 21(2), 393   422.

williams, p., r. sennrich, m. post, and p. koehn (2016). syntax-based statistical machine

translation. synthesis lectures on human language technologies 9(4), 1   208.

wilson, t., j. wiebe, and p. hoffmann (2005). recognizing contextual polarity in phrase-

level id31. see emn (2005), pp. 347   354.

winograd, t. (1972). understanding natural language. cognitive psychology 3(1), 1   191.

wiseman, s., a. m. rush, and s. m. shieber (2016). learning global features for corefer-

ence resolution. see naa (2016), pp. 994   1004.

wiseman, s., s. shieber, and a. rush (2017). challenges in data-to-document generation.

see emn (2017), pp. 2253   2263.

wiseman, s. j., a. m. rush, s. m. shieber, and j. weston (2015). learning anaphoricity and

antecedent ranking features for coreference resolution. see acl (2015).

wolf, f. and e. gibson (2005). representing discourse coherence: a corpus-based study.

computational linguistics 31(2), 249   287.

wolfe, t., m. dredze, and b. van durme (2017). pocket knowledge base population. see

acl (2017), pp. 305   310.

wong, y. w. and r. mooney (2007). generation by inverting a semantic parser that uses

id151. see naa (2007), pp. 172   179.

wong, y. w. and r. j. mooney (2006). learning for id29 with statistical ma-

chine translation. see naa (2006), pp. 439   446.

jacob eisenstein. draft of november 13, 2018.

bibliography

553

wu, b. y. and k.-m. chao (2004). spanning trees and optimization problems. crc press.

wu, d. (1997). stochastic inversion transduction grammars and bilingual parsing of par-

allel corpora. computational linguistics 23(3), 377   403.

wu, f. and d. s. weld (2010). id10 using wikipedia. see acl (2010),

pp. 118   127.

wu, x., r. ward, and l. bottou (2018). wngrad: learn the learning rate in gradient

descent. arxiv preprint arxiv:1803.02865.

wu, y., m. schuster, z. chen, q. v. le, m. norouzi, w. macherey, m. krikun, y. cao,
q. gao, k. macherey, j. klingner, a. shah, m. johnson, x. liu,   ukasz kaiser, s. gouws,
y. kato, t. kudo, h. kazawa, k. stevens, g. kurian, n. patil, w. wang, c. young,
j. smith, j. riesa, a. rudnick, o. vinyals, g. corrado, m. hughes, and j. dean (2016).
google   s id4 system: bridging the gap between human and ma-
chine translation. corr abs/1609.08144.

xia, f. (2000). the part-of-speech tagging guidelines for the penn chinese treebank (3.0).
technical report, university of pennsylvania institute for research in cognitive science.

xu, k., j. ba, r. kiros, k. cho, a. courville, r. salakhudinov, r. zemel, and y. bengio
(2015). show, attend and tell: neural image id134 with visual attention.
see icm (2015), pp. 2048   2057.

xu, w., x. liu, and y. gong (2003). document id91 based on non-negative matrix
factorization. in proceedings of acm sigir conference on research and development in in-
formation retrieval, pp. 267   273.

xu, y., l. mou, g. li, y. chen, h. peng, and z. jin (2015). classifying relations via long
short term memory networks along shortest dependency paths. see emn (2015), pp.
1785   1794.

xuan bach, n., n. l. minh, and a. shimazu (2012). a reranking model for discourse

segmentation using subtree features. see sig (2012).

xue, n. et al. (2003). chinese id40 as character tagging. computational

linguistics and chinese language processing 8(1), 29   48.

xue, n., h. t. ng, s. pradhan, r. prasad, c. bryant, and a. t. rutherford (2015). the
conll-2015 shared task on shallow discourse parsing. in proceedings of the conference
on natural language learning (conll).

yamada, h. and y. matsumoto (2003). statistical dependency analysis with support vector
machines. in proceedings of the 8th international workshop on parsing technologies (iwpt),
pp. 195   206.

under contract with mit press, shared under cc-by-nc-nd license.

554

bibliography

yamada, k. and k. knight (2001). a syntax-based statistical translation model. see acl

(2001), pp. 523   530.

yang, b. and c. cardie (2014). context-aware learning for sentence-level sentiment anal-

ysis with posterior id173. see acl (2014).

yang, y., m.-w. chang, and j. eisenstein (2016). toward socially-infused information

extraction: embedding authors, mentions, and entities. see emn (2016).

yang, y. and j. eisenstein (2013). a log-linear model for unsupervised text id172.

see emn (2013).

yang, y. and j. eisenstein (2015). unsupervised multi-id20 with feature

embeddings. see naa (2015).

yang, y., w.-t. yih, and c. meek (2015). wikiqa: a challenge dataset for open-domain

id53. see emn (2015), pp. 2013   2018.

yannakoudakis, h., t. briscoe, and b. medlock (2011). a new dataset and method for

automatically grading esol texts. see acl (2011), pp. 180   189.

yarowsky, d. (1995). unsupervised id51 rivaling supervised meth-
in proceedings of the association for computational linguistics (acl), pp. 189   196.

ods.
association for computational linguistics.

yee, l. c. and t. y. jones (2012, march). apple ceo in china mission to clear up problems.

reuters. retrieved on march 26, 2017.

yi, y., c.-y. lai, s. petrov, and k. keutzer (2011). ef   cient parallel cky parsing on gpus. in

proceedings of the 12th international conference on parsing technologies, pp. 175   185.

yu, c.-n. j. and t. joachims (2009). learning structural id166s with latent variables. see

icm (2009), pp. 1169   1176.

yu, f. and v. koltun (2016). multi-scale context aggregation by dilated convolutions. see

icl (2016).

zaidan, o. f. and c. callison-burch (2011). id104 translation: professional qual-

ity from non-professionals. see acl (2011), pp. 1220   1229.

zaremba, w., i. sutskever, and o. vinyals. recurrent neural network id173. arxiv

preprint arxiv:1409.2329.

zeiler, m. d. (2012). adadelta: an adaptive learning rate method.

arxiv:1212.5701.

arxiv preprint

jacob eisenstein. draft of november 13, 2018.

bibliography

555

zelenko, d., c. aone, and a. richardella (2003). kernel methods for id36.

the journal of machine learning research 3, 1083   1106.

zelle, j. m. and r. j. mooney (1996). learning to parse database queries using induc-
tive logic programming. in proceedings of the national conference on arti   cial intelligence
(aaai), pp. 1050   1055.

zeng, d., k. liu, s. lai, g. zhou, and j. zhao (2014). relation classi   cation via convolu-

tional deep neural network. see col (2014), pp. 2335   2344.

zettlemoyer, l. s. and m. collins (2005). learning to map sentences to logical form: struc-
tured classi   cation with probabilistic categorial grammars. in proceedings of uncertainty
in arti   cial intelligence (uai).

zhang, c., s. bengio, m. hardt, b. recht, and o. vinyals (2017). understanding deep

learning requires rethinking generalization. see icl (2017).

zhang, x., j. zhao, and y. lecun (2015). character-level convolutional networks for text

classi   cation. see nip (2015), pp. 649   657.

zhang, y. and s. clark (2008). a tale of two parsers: investigating and combining graph-
based and transition-based id33 using beam-search. see emn (2008),
pp. 562   571.

zhang, y., t. lei, r. barzilay, t. jaakkola, and a. globerson (2014). steps to excellence:
simple id136 with re   ned scoring of dependency trees. see acl (2014), pp. 197   207.

zhang, y. and j. nivre (2011). transition-based id33 with rich non-local

features. see acl (2011), pp. 188   193.

zhang, z. (2017). a note on counting dependency trees. arxiv preprint arxiv:1708.08789.

zhao, j., t. wang, m. yatskar, e. v. ordonez, and k.-w. chang (2018). gender bias in

coreference resolution: evaluation and debiasing methods. see naa (2018).

zhou, j. and w. xu (2015). end-to-end learning of id14 using recurrent

neural networks. see acl (2015), pp. 1127   1137.

zhu, j., z. nie, x. liu, b. zhang, and j.-r. wen (2009). statsnowball: a statistical approach
in proceedings of the conference on world-wide web

to extracting entity relationships.
(www), pp. 101   110.

zhu, x., z. ghahramani, and j. d. lafferty (2003). semi-supervised learning using gaus-
sian    elds and id94. in proceedings of the international conference on ma-
chine learning (icml), pp. 912   919.

under contract with mit press, shared under cc-by-nc-nd license.

556

bibliography

zhu, x. and a. b. goldberg (2009). introduction to semi-supervised learning. synthesis

lectures on arti   cial intelligence and machine learning 3(1), 1   130.

zipf, g. k. (1949). human behavior and the principle of least effort. addison-wesley.

zirn, c., m. niepert, h. stuckenschmidt, and m. strube (2011). fine-grained sentiment

analysis with structural features. see ijc (2011), pp. 336   344.

zou, w. y., r. socher, d. cer, and c. d. manning (2013). bilingual id27s for

phrase-based machine translation. see emn (2013), pp. 1393   1398.

jacob eisenstein. draft of november 13, 2018.

index

  -conversion, 295
  -reduction,   -conversion, 293
id165, 24

language model, 198

p-value, 84

one-tailed, 85

f -measure, 82
balanced, 83
macro, 82
micro, 83

id138, 9, 74
id7, 433
meteor, 435
ribes, 435

semantics

extra-propositional, 422

ablation test, 84
id15

(amr), 307, 319

accuracy, 23, 81
action, in id23, 365
adagrad, 40, 61
adequacy, in translation, 433
adjectives, 177
adjuncts, 306
adpositions, 178
adverbs, 177
af   x, 195

in   ectional, 78

agent (thematic role), 308

alignment

in machine translation, 432, 437
in id29, 321
in text generation, 459

amazon mechanical turk, 91
ambiguity, 210, 218
attachment, 257
derivational, 222
spurious, 222, 269, 298
syntactic, 229
anaphoricity, 360
anchored productions, 232
animacy, 307
annealing, 452
antecedent mention, 351, 359
antonymy, 74, 281
apophony, 194
area under the curve (auc), 83
argumentation, 392
arguments, 403
article, 182
aspect, 177
aspect-based opinion mining, 71
autoencoder, 345

denoising, 345, 408
variational, 465

automated theorem provers, 288
automatic differentiation, 56
auxiliary verbs, 178
average mutual information, 333
averaged id88, 27

557

558

backchannel, 187
backoff, 130

katz, 130

id26, 55

through time, 136

backward recurrence, 165, 166
backward-looking center, 382
bag of words, 13
balanced test set, 81
batch id172, 61
baum-welch algorithm, 170
bayes    rule, 478
bayesian nonparametrics, 103, 249
beam sampling, 172
id125, 272

in coreference resolution, 363
in machine translation, 450, 451

bell number, 362
bias, 23
id160, 23, 127, 129
bigrams, 24, 70
bilinear product, 330
binarization, 211, 228
binomial

distribution, 85
random variable, 482
test, 84

bio notation, 184, 317
biomedical natural language processing,

183

bonferroni correction, 87
boolean semiring, 199
boosting, 48
bootstrap samples, 86
brevity penalty, 434
brown clusters, 327, 332
byte-pair encoding, 343, 449

c-command, 353
case marking, 182, 219
catalan number, 225

bibliography

cataphora, 352
center embedding, 207
centering theory, 355, 382
character-level language models, 141
chatbots, 470
chomsky normal form (cnf), 211
chu-liu-edmonds algorithm, 264
cky algorithm, 226
class imbalance, 81
classi   cation, 13

large margin, 30
lexicon-based, 72
weights, 14

closed-vocabulary, 141
closure, of classes of formal languages,

192

cluster ranking, 362
id91, 96

id116, 96
exchange, 333
hierarchical, 332
soft, 97

co-hypernymy, 281
co-training, 108
code switching, 180, 186
cohen   s kappa, 90
coherence, 396
cohesion, 379, 400
collective entity linking, 408
collocation extraction, 344
id35, 220
complement clause, 213
complement event (id203), 476
composition (id35), 221
compositional vector grammars, 391
compositionality, 2, 7, 10, 291, 341
computation graph, 48, 55

dynamic, 57

computational linguistics (versus

natural language processing), 1

computational social science, 5

jacob eisenstein. draft of november 13, 2018.

bibliography

559

concept (in abstract meaning
representation), 319

conditional independence, 154, 480
conditional random    eld, 162
con   dence interval, 86
con   guration (transition-based parsing),

269

consistency, in logic, 291
constants, in logic, 286
constituents, 212

split, 312

constrained optimization, 315
content selection, 457
content words, 178
context-free grammars, 208

probabilistic (pid18s), 235
synchronous, 441
weighted, 218, 227, 233

context-free languages, 207, 208
context-free production, 209

recursive, 209
unary, 210

context-sensitive languages, 218
continuous bag-of-words (cbow), 334
contradiction, 346
conversational turns, 187
convexity, 29, 58, 300, 482, 485

biconvexity, 102

convolution

dilated, 64, 185
narrow, 63
one-dimensional, 63
wide, 63

convolutional neural networks, 62

in machine translation, 447
in id36, 415
in id14, 318
in sequence labeling, 170

cooperative principle, 351
coordinate ascent, 102
coordinating conjunctions, 178

copula, 177, 217, 260
coreference resolution, 347, 351

cross-document, 406

coreferent, 351
cosine similarity, 339, 380
cost function, 31
cost-augmented decoding, 32
coverage loss, 465
critical point, 58, 485
cross-id178, 53, 416
cross-serial dependencies, 219
cross-validation, 24
id104, 91
cumulative id203 distribution, 86

decidability, 291
id90, 48
decoding

cost-augmented, 161
in conditional random    elds, 163

de   niteness, 183
delta function, 22
denotation, 286
dependency, 258
grammar, 257
graph, 258
labels, 259
path, 75, 314, 413
syntactic, 258

id33, 257

arc-eager, 269, 270
arc-factored, 263
arc-standard, 269
pseudo-projective, 272
second-order, 263
third-order, 264

derivation

in context-free languages, 209
in id33, 269
in id29, 293

determiner, 179

under contract with mit press, shared under cc-by-nc-nd license.

560

bibliography

phrase, 215

development set, 23, 81
dialogue acts, 90, 187, 471
dialogue management, 468
dialogue systems, 125, 466

agenda-based, 468
mixed-initiative, 468
digital humanities, 5, 69
dirichlet compound multinomial, 398
dirichlet distribution, 115
discounting, 130
absolute, 130

discourse, 379

connectives, 385
depth, 394
parsing, 385
segment, 379
unit, 389

discourse relations, 347, 384

coordinating, 389
implicit, 385
sense classi   cation, 386
subordinating, 389

distant supervision, 118, 418, 419
distributed semantics, 327
distributional

hypothesis, 325, 326
semantics, 10, 327
statistics, 75, 248, 326
document frequency, 407
id20, 95, 111

by projection, 112

dropout, 57, 137
id209, 317
id145, 149

e-step, 99
early stopping, 27, 61
early update, 276
id153, 201, 435
effective counts, 129

elementwise nonlinearity, 50
elman unit, 135
elmo (embeddings from language

models), 340

embedding, 167
emotion, 71
empirical bayes, 116
empty string, 192
encoder-decoder model, 345, 442, 460
id108, 48, 318, 444
entailment, 290, 346
entity, 403

embeddings, 407
grid, 383
linking, 351, 403, 405, 416
linking, collective, 409
nil, 405

id178, 42, 99
estimation, 482
europarl corpus, 435
evaluation

extrinsic, 139, 338
intrinsic, 139, 338

event, 403

coreference, 421
detection, 421

event semantics, 305
events, in id203, 475

disjoint, 476

evidentiality, 182, 423
expectation, 481
expectation-maximization, 97

hard, 102
in id38, 131
in machine translation, 439
incremental, 102
online, 102

explicit semantic analysis, 328

factoid question, 424
factor graph, 163

jacob eisenstein. draft of november 13, 2018.

bibliography

factuality, 423
fairness and bias, 5

in machine translation, 434
in id27s, 340

false discovery rate, 87
false negative, 81

rate, 478

false positive, 81, 479

rate, 83, 478

feature

co-adaptation, 58
function, 14
hashing, 80
noising, 58
selection, 41

features, 6

bilexical, 266
collocation, 75
emission, 148
lexical, 47
offset, 15
pivot, 113
transition, 148

   nite state

acceptor, 193
acceptor, chain, 205
acceptors, weighted, 197
automata, 193
automaton, deterministic, 194
composition, 204
transducers, 196, 201

   nite-state

transduction, 192

   uency, 125, 433
formal language theory, 191
forward

recurrence, 164
variable, 164, 166

forward-backward algorithm, 165, 206,

240

forward-looking centers, 382

561

frame, 310

element, 310
in dialogue systems, 467

framenet, 310
frobenius norm, 57, 105
function words, 178
function, in logic, 289
functional segmentation, 379, 381

garden path sentence, 146
gazetteer, 184, 357, 413
generalization, 27

of neural networks, 59

generalized linear models, 42
generative model

for classi   cation, 17
for coreference, 363
for interpolated id38,

131

for parsing, 235
for sequence labeling, 154

id150, 115, 409

collapsed, 116
gloss, 125, 179, 433
government and binding theory, 353
gradient, 29

clipping, 60
descent, 38
exploding, 137
vanishing, 51, 137

gram matrix, 414
grammar equivalence, 210
grammar induction, 241
grammaticality, 396
graphical model, 154
graphics processing units (gpus), 170,

185

grid search, 23

hamming cost, 161
hansards corpus, 435

under contract with mit press, shared under cc-by-nc-nd license.

562

bibliography

hanzi, 77
head rules, for lexicalized id18s, 246, 257
head word, 212, 244, 257, 353

of a dependency edge, 258

hedging, 423
held-out data, 139
hessian matrix, 39
id48, 154
hierarchical recurrent network, 470
highway network, 52
holonymy, 75
homonymy, 73
human computation, 91
hypergraph, 392
hypernymy, 75, 281
hyperparameter, 22
hyponymy, 75

illocutionary force, 187
importance sampling, 453
importance score, 453
independent and identically distributed

(iid), 17

id136

in id170, 147
logical, 285
rules for id118, 288

in   ection point, 486
information extraction, 403

open, 419

information retrieval, 5, 416
inside recurrence, 235   237
inside-outside algorithm, 240, 249
instance labels, 16
instance, in abstract meaning

representation, 319

integer id135, 315
in coreference resolution, 362
in entity linking, 409
in extractive summarization, 395
in sentence compression, 466

inter-annotator agreement, 90
interjections, 177
interlingua, 432
interpolation, 131, 199
interval algebra, 421
inverse document frequency, 407
inversion (of    nite state automata), 203
irrealis, 70

jensen   s inequality, 99

kalman smoother, 172
kernel, 48, 414
kleene star, 192
knapsack problem, 395
knowledge base, 403
population, 416

label bias problem, 275, 280
label propagation, 110, 120
lagrangian, 487
id198, 292
lambda expressions, 292
language model, 4, 126
id44, 380
latent semantic analysis, 328, 329
latent variable, 98, 206, 299, 419, 432

conditional random    eld, 301
in parsing, 249
id88, 207, 300, 360
layer id172, 61, 447
learning

active, 118
batch, 25
constraint-driven, 118
deep, 47
discriminative, 25
multiple instance, 118, 418
multitask, 118
online, 25, 39
reinforcement, 365, 451
semi-supervised, 76, 95, 339

jacob eisenstein. draft of november 13, 2018.

bibliography

563

to search, 257, 277, 366
transfer, 118
unsupervised, 71, 95

learning rate, 38
least squares, 72
leave-one-out cross-validation, 24
lemma, 73, 202
lemmatization, 79
lexical entry, 293
lexical unit, 310
lexicalization

in parsing, 245
in text generation, 457

lexicalized tree-adjoining grammar for

discourse (d-ltag), 385

lexicon, 293

in id35,

221

lexicon-based classi   cation, 72
seed, 73

light verb, 321
linear separability, 25
linearization, 466, 472
link function, 42
literal character, 192
local optimum, 102, 486
locally-normalized objective, 275
log-bilinear language model, 342
log-likelihood

conditional, 52

id148, 42
logic, 287

   rst-order, 288
higher-order, 289
propositional, 287

logistic function, 42
long short-term memory (lstm), 52,

135, 137, 181, 442
bidirectional, 169, 445
deep, 443
lstm-crf, 169, 318

memory cell, 137
lookup layer, 53, 135
loss

function, 27
hinge, 29
logistic, 36
warp, 411
zero-one, 28

machine learning, 2
supervised, 16
theory, 26

machine reading, 425

macro, 404
micro, 404

machine translation, 125

neural, 432
statistical, 432

margin, 26, 30

functional, 32
geometric, 32

marginal relevance, 465
marginalization, 477
markable, 358
markov assumption, 154
markov blanket, 154
id115 (mcmc),

103, 115, 172

markov decision process, 468

partially-observable (pomdp), 470

markov random    elds, 162
markovization
vertical, 244

matrix-tree theorem, 268
max-margin markov network, 161
max-product algorithm, 157
maximum a posteriori, 22, 483
maximum conditional likelihood, 36
maximum directed spanning tree, 264
maximum id178, 42
maximum likelihood, 17, 21, 482

under contract with mit press, shared under cc-by-nc-nd license.

564

bibliography

mcnemar   s test, 84
meaning representation, 285
membership problem, 191
mention

in coreference resolution, 351
in entity linking, 403
in information extraction, 405

mention ranking, 360
mention-pair model, 359
meronymy, 75
method of moments, 117
mildly context-sensitive languages, 219
minibatch, 39
minimization of    nite-state automata,

196

minimum error-rate training (mert),

452

minimum risk training, 452
modality, 422
model, 8, 38
modi   er, 258
modus ponens, 288, 303
moment-matching, 42
monomorphemic, 196
morpheme, 6, 141, 195
morphological segmentation, 159
morphology, 79, 158, 194, 342, 448

derivational, 194
in   ectional, 177, 194, 202

morphosyntactic, 176

attributes, 180
morphotactics, 195
multi-view learning, 108
multiclass classi   cation
one-versus-all, 414
one-versus-one, 414

multinomial

distribution, 18
na    ve bayes, 19

na    ve bayes, 17

name dictionary, 406
named entity, 183

linking, 405
recognition, 169, 403, 405
types, 405

nearest-neighbor, 48, 414
negation, 70, 422

scope, 423

negative sampling, 336, 337, 411
neo-davidsonian event semantics, 306
neural attention, 371, 426, 443, 444, 460

coarse-to-   ne, 463
structured, 463
neural gate, 52, 445
neural network, 48
adversarial, 113
bidirectional recurrent, 168
convolutional, 53, 62, 185, 415
feedforward, 50
recurrent, 134, 415
recursive, 250, 343, 346, 395
noise-contrastive estimation, 135
id87, 126, 436
nominal modi   er, 215
nominals, 351, 357
non-terminals, 209
id172, 78
noun phrase, 2, 212
nouns, 176
np-hard, 41, 409
nucleus, in rst, 389
null hypothesis, 84
null subjects, 375
numerals, 179

one-hot vector, 53
online support vector machine, 31
ontology, 9
open word classes, 176
opinion polarity, 69
optimization

jacob eisenstein. draft of november 13, 2018.

bibliography

batch, 38
combinatorial, 8
constrained, 33
convex, 38
numerical, 8
quasi-newton, 39

oracle, 275

dynamic, 277
in learning to search, 366

orthography, 196, 204
orthonormal matrix, 60
out-of-vocabulary words, 181
outside recurrence, 237, 240
over   tting, 23, 27

in neural networks, 59

overgeneration, 203, 213

parallel corpus, 435
parameters, 482
paraphase, 346
parent annotation, 244
parsing, 209

agenda-based, 279
chart, 226
easy-   rst, 279
graph-based, 263
transition-based, 225

part-of-speech, 6, 175

tagging, 145
particle, 179, 217
partition, 477
partition function, 164
passive-aggressive, 43, 487
path, in an fsa, 193
penn discourse treebank (pdtb), 385
id32, 140, 159, 180, 212, 238
id88, 25

incremental, 277, 364
multilayer, 50
structured, 160

perplexity, 140

565

phonology, 196
phrase, 212
phrase-structure grammar, 212
pointwise mutual information, 281, 329,

330

in collocation identi   cation, 344
positive, 331
shifted positive, 337

policy, 276, 365, 469
policy gradient, 366
polysemy, 74
pooling, 63, 65, 372, 460
positional encodings

in machine translation, 447
in id36, 415

power law, 2
pragmatics, 351
precision, 82, 478
at-k, 83, 398
labeled, 230
unlabeled, 230

precision-recall curve, 83, 418
predicate, 403
predicative adjectives, 217
predictive likelihood, 103
prepositional phrase, 2, 217
primal form, 487
prior expectation, 483
probabilistic models, 482
probabilistic topic model, 5, 409
id203

chain rule, 477
conditional, 35, 477, 481
density function, 481
distribution, 480
joint, 17, 35, 481
likelihood, 478
marginal, 481
mass function, 85, 481
posterior, 478
prior, 22, 36, 478

under contract with mit press, shared under cc-by-nc-nd license.

566

bibliography

simplex, 18

processes, 422
productivity, 195
projectivity, 261
pronominal id2, 351
pronoun, 178

re   exive, 353

propbank, 310
proper nouns, 177
proposal distribution, 453
propositions, 286, 287, 422
id144, 187
proto-roles, 309
pumping lemma, 207
pushdown automata, 209, 251

quadratic program, 34
quanti   er, 289

existential, 289
universal, 289

id53, 346, 405

cloze, 425
extractive, 426
multiple-choice, 425

random outcomes, 475
random variable, 480

discrete, 480
indicator, 480

ranking, 406
loss, 406
recall, 82, 478
at-k, 398
labeled, 230
unlabeled, 230

reference translations, 433
referent, 351

generic, 356

referring expressions, 351, 381, 458
regression, 72
linear, 72
logistic, 35
ridge, 72

regular expression, 192
regular language, 192
id173, 23, 32
rei   cation (events), 305
relation

extraction, 277
logical, 286, 295

id36, 411
relations

in information extraction, 403

relative frequency estimate, 21, 126, 483
reranking, 250
residual networks, 52
retro   tting, 343
rhetorical structure theory (rst), 389
rhetorical zones, 381
risk, 452
roll-in, roll-out, 366

saddle point, 58, 486
sample space, 475
satellite, in rst, 389
satisfaction, 290
scheduled sampling, 451
schema, 403, 404, 419
search error, 252, 363
segmented discourse representation

receiver operating characteristic (roc),

theory (sdrt), 384

83

recti   ed linear unit (relu), 51

leaky, 51

reference arguments, 322
reference resolution, 351

self-attention, 446
self-training, 108
semantic concordance, 76
id29, 291
semantic role, 306

jacob eisenstein. draft of november 13, 2018.

bibliography

567

id14, 306, 412, 420
semantics, 285

dynamic, 302, 384
in parsing, 242
lexical, 73
model-theoretic, 286
underspeci   cation, 302

semi-supervised learning, 105
semiring, 199

algebra, 172
expectation, 207
tropical, 173

sentence compression, 465
sentence fusion, 465
sentence, in logic, 289
id31, 69
lexicon-based, 70
targeted, 71

sentiment lexicon, 16
sequence-to-sequence model, 442
id132, 251
shortest-path algorithm, 197
sigmoid, 49
simplex, 115
singular value decomposition, 60, 104

truncated, 104, 330

singular vectors, 60
skipgram id27s, 335
slack variables, 34
slot    lling, 416
slots, in dialogue systems, 467
smooth functions, 29
smoothing, 22, 129

jeffreys-perks, 129
kneser-ney, 133
laplace, 22, 129
lidstone, 129

softmax, 49, 134, 416

hierarchical, 135, 336

source language, 431
spanning tree, 258

sparse matrix, 330
sparsity, 41
spectral learning, 117
speech acts, 187
id103, 125
squashing function, 51, 135
stand-off annotations, 90
stanford natural language id136

corpus, 346

statistical signi   cance, 84
id30, 7, 78, 195
step size, 486
stochastic id119, 29, 35, 39
stopwords, 80
string, in formal language theory, 191
string-to-tree translation, 441
strong compositionality criterion, 391
structure induction, 170
structure prediction, 15
subgradient, 29, 41
subjectivity detection, 70
subordinating conjunctions, 178
sum-product algorithm, 164
summarization, 125, 393
abstractive, 393, 464
extractive, 393
multi-document, 465
of sentences, 464

supersenses, 339
support vector machine, 34

kernel, 48, 414
structured, 161
support vectors, 34
surface form, 203
surface realization, 457
synonymy, 74, 281, 325
synset, 74, 369
syntactic path, 313
syntactic-semantic grammar, 293
syntax, 175, 211, 285

under contract with mit press, shared under cc-by-nc-nd license.

568

bibliography

tagset, 176
tanh activation function, 51
target language, 431
tense, 177
terminal symbols, 209
test set, 23, 95
test statistic, 84
id111, 5
text planning, 457
thematic roles, 307
third axiom of id203, 476
timeml, 421
id121, 77, 185
tokens and types, 19
topic segmentation, 379

hierarchical, 381

trace, 222
training set, 17, 95
transformer architecture, 446
transition system, 269

for context-free grammars, 251

transitive closure, 361
translation error rate (ter), 435
translation model, 126
id68, 449
tree-adjoining grammar, 220
tree-to-string translation, 442
tree-to-tree translation, 441
treebank, 238
trellis, 150, 205
trigrams, 24
trilexical dependencies, 248
tropical semiring, 199
true negative, 81
true positive, 81, 479

rate, 83

truth conditions, 290
tuning set, see development set, 81
turing test, 3
two-tailed test, 86, 92
type systems, 295

type-raising, 221, 295

unary closure, 228
undercutting, in argumentation, 393, 399
under   tting, 23
under   ow, 17
undergeneration, 203, 213
universal dependencies, 176, 257
unseen word, 169
utterances, 187

validation function, 301
validity, in logic, 290
value iteration, 469
variable, 319

bound, 289
free, 289

variance, 22, 87
vauquois pyramid, 432
verb phrase, 213
verbnet, 308
verbs, 177
viterbi

algorithm, 148
variable, 149

volition, 307

weight decay, 57
wiki   cation, 405
winograd schemas, 3
word

embeddings, 47, 53, 135, 136, 327,

334

embeddings,    ne-tuned, 340
embeddings, pre-trained, 339
representations, 326
sense disambiguation, 73
senses, 73, 309
tokens, 77

world model, 286

builder, 291
checker, 291

jacob eisenstein. draft of november 13, 2018.

bibliography

yield, 209

569

zipf   s law, 143

under contract with mit press, shared under cc-by-nc-nd license.

