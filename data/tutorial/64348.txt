discriminative sequence 

models
yasemin altun

google research, zurich

id48 (id48s)

    generative framework p(x, y)

tags

words

    bayes net: independence assumptions. factorization of 

the joint id203 to local conditional probabilities

     (words are generated from tags)
     

id48 (id48s)

    advantages: efficient algorithms

    decoding: viterbi  
    supervised learning: closed form solution: relative 

frequency.

    unsupervised: forward-backward.

    disadvantages: 

    harder than the problem in hand p(x,y) = p(y|x)p(x)
    questionable conditional independence assumptions
    overlapping features problematic (board)
     

maximum id178 models

    id28 (misnomer in nlp)

    training: id76 (board)
    advantages: since conditioning x, overlapping features
        f1(xt,yt) := is yt = n and xt = 'running'
        f2(xt,yt) := is yt = n and xt ends with 'ing'
    disadvantages: no dependency across tags, no closed 

form solution

maximum id178 markov models 
(memms)

    include dependencies between tags

    include transition features
     f1(xt,yt,yt-1):= is yt = n and yt-1 = 'adj'
    learning: id76  (board if there is time)
    decoding: viterbi by replacing 
      with 

memms

    advantages: 

factorization)
    disadvantages:

    tag dependencies
    no closed form for learning but z is local (due to 

    label bias problem: states with low-id178 
transition distributions effectively ignore the 
observations.

id49

    instead of bayes nets (directed graphs), model p(y|x) 

with markov random fields (mrfs)

    hammersly-clifford theorem
     

    id28 with huge label set 
    learning: expectations (on the board)

sequence id88: 
margin interpretation

    generalized id88 learning

    on-line learning: present one example at a time
    update, if prediction differs from correct output

   for each instance (x,y)
  

find best tag

sequence id88: 
approx to crf

    in crfs, compute expectations ( sum_y ) over all 

training instances, then make an update

    approximation of sequential log-loss

    viterbi assumption: distribution over y is peaked so 

that max_y dominates the sum_y therefore the

     viterbi labeling is good enough
    online updates: consider one training instance at

 a time
    sparse models wrt features (in practice)

id166 struct

separation margin

maximize minimum separation margin

id166 struct

   

   
   

   

id166 struct

    advantages: 

    cost sensitivity
    for some graphs, argmax easier than expectation
    solution sparse wrt support vectors

    disadvantages:

    more difficult to parallelize

wrap-up

    id48s, maxent (lr), memm, crf, 
sequence id88, id166 struct

    comparative discussion 
    all generalize to arbitrary graphs

