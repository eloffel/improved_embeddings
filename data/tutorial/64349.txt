id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith	
   

	
   

heidelberg	
   university,	
   november	
   2014	
   

introduc@on	
   

mo@va@on	
   

       sta@s@cal	
   methods	
   in	
   nlp	
   arrived	
   ~20	
   years	
   ago	
   
       mercer	
   was	
   right:	
   	
      there's	
   no	
   data	
   like	
   more	
   

and	
   now	
   dominate.	
   

data.   	
   
       and	
   there's	
   more	
   and	
   more	
   data.	
   

       lots	
   of	
   new	
   applica@ons	
   and	
   new	
   sta@s@cal	
   

techniques	
      	
   it's	
   formidable	
   to	
   learn	
   and	
   keep	
   up	
   
with	
   all	
   of	
   them.	
   

thesis	
   

       most	
   of	
   the	
   main	
   ideas	
   are	
   related	
   and	
   similar	
   to	
   

each	
   other.	
   
       di   erent	
   approaches	
   to	
   decoding.	
   
       di   erent	
   learning	
   criteria.	
   
       supervised	
   and	
   unsupervised	
   learning.	
   

       umbrella:	
   	
   probabilis@c	
   reasoning	
   about	
   discrete	
   

linguis@c	
   structures.	
   

	
   
       this	
   is	
   good	
   news!	
   

plan	
   

1.	
    graphical	
   models	
   and	
   id136	
    monday	
   
tuesday	
   
2.	
    decoding	
   and	
   structures	
   
3.	
   
wednesday	
   
thursday	
   
4.	
    hidden	
   variables	
   

supervised	
   learning	
   

exhorta@ons	
   

       the	
   content	
   is	
   formal,	
   but	
   the	
   style	
   doesn't	
   

need	
   to	
   be.	
   

       ask	
   ques@ons!	
   

	
   

      help	
   me	
      nd	
   the	
   right	
   pace.	
   
      lecture	
   4	
   can	
   be	
   dropped/reduced	
   if	
   needed.	
   

lecture	
   1:	
   	
   graphical	
   models	
   	
   

and	
   id136	
   

random	
   variables	
   

       id203	
   distribu@ons	
   usually	
   de   ned	
   by	
   events	
   
       events	
   are	
   complicated!	
   

       we	
   tend	
   to	
   group	
   events	
   by	
   a(ributes	
   
       person	
      	
   age,	
   grade,	
   haircolor	
   

       random	
   variables	
   formalize	
   a_ributes:	
   

          grade	
   =	
   a   	
   is	
   shorthand	
   for	
   event	
   
       proper@es	
   of	
   random	
   variable	
   x:	
   
       val(x)	
   =	
   possible	
   values	
   of	
   x	
   
 x val(x)
       for	
   discrete	
   (categorical):	
   
  p (x = x)dx = 1
       for	
   con@nuous:	
   
       nonnega@vity:	
   
   x     val(x), p (x = x)   0

p (x = x) = 1

{      : fgrade( ) = a}

condi@onal	
   probabili@es	
   

       aeer	
   learning	
   that	
     	
   is	
   true,	
   how	
   do	
   we	
   feel	
   
about	
     ?  p(   |   ) 	
   
  	
   

  	
   	
   

       	
   	
   

  	
   

chain	
   rule	
   

p (       ) = p ( )p (    |  )

p ( 1             k) = p ( 1)p ( 2 |  1)       p ( k |  1   . . .    k 1)

bayes	
   rule	
   

likelihood	
   

prior	
   

posterior	
   

normaliza@on	
   constant	
   

  	
   is	
   an	
      external	
   event   	
   

independence	
   

         	
   and	
     	
   are	
   independent	
   if	
   p(  |  )	
   =	
   p(  )	
   

p	
       (      	
     )	
   

       proposi4on:	
   	
     	
   and	
     	
   are	
   independent	
   if	
   and	
   

only	
   if	
   p(       )	
   =	
   p(  )	
   p(  )	
   	
   

condi4onal	
   independence	
   

       independence	
   is	
   rarely	
   true.	
   

         	
   and	
     	
   are	
   condi4onally	
   independent	
   given	
     	
   if	
   

p(   |	
            )	
   =	
   p(   |	
     )	
   

p	
       (      	
      |   )	
   

proposi4on:	
   	
   p	
       (      	
      |   )	
   if	
   and	
   only	
   if	
   	
   
	
   p(        |	
     )	
   =p	
   (   |	
     )	
   p(   |	
     )	
   	
   

joint	
   distribu@on	
   and	
   marginaliza@on	
   

       compute	
   the	
   marginal	
   

over	
   each	
   individual	
   
random	
   variable?	
   

p (grade, intelligence) =

intelligence	
   
=	
   very	
   high	
   

intelligence	
   
=	
   high	
   

0.70	
   

0.15	
   

0.10	
   

0.05	
   

grade	
   =	
   a	
   

grade	
   =	
   b	
   

marginaliza@on:	
   	
   general	
   case	
   

p(x1 = x) = xx22val(x2)

       xxn2val(xn)

p (x1 = x, x2 = x2, . . . , xn = xn)

how	
   many	
   terms?	
   

basic	
   concepts	
   so	
   far	
   
       atomic	
   outcomes:	
   	
   assignment	
   of	
   x1,   ,xn	
   to	
   	
   

x1,   ,	
   xn	
   

       condi@onal	
   id203:	
   p(x,	
   y)	
   =	
   p(x)	
   p(y|x)	
   
       bayes	
   rule:	
   p(x|y)	
   =	
   p(y|x)	
   p(x)	
   /	
   p(y)	
   
       chain	
   rule:	
   	
   p(x1,   ,xn)	
   =	
   p(x1)	
   p(x2|x1)	
   	
   

	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
       p(xk|x1,   ,xk-     1)	
   

       marginals:	
   	
   deriving	
   p(x	
   =	
   x)	
   from	
   p(x,	
   y)	
   

sets	
   of	
   variables	
   

       sets	
   of	
   variables	
   x,	
   y,	
   z	
   
       x	
   is	
   independent	
   of	
   y	
   given	
   z	
   if	
   

	
   p	
       (x=x	
      	
   y=y|z=z),	
   	
   
	
      	
   x   val(x),	
   y   val(y),	
   z   val(z)	
   

       shorthand:	
   

       condi@onal	
   independence:	
   p	
      	
   (x	
      	
   y	
   |	
   z)	
   
       for	
   p	
      	
   (x	
      	
   y	
   |	
      ),	
   write	
   p	
      	
   (x	
      	
   y)	
   

       proposi4on:	
   p	
   sa@s   es	
   (x	
      	
   y	
   |	
   z)	
   if	
   and	
   only	
   if	
   

p(x,y|z)	
   =	
   p(x|z)	
   p(y|z)	
   

factor	
   graphs	
   

factor	
   graphs	
   

       random	
   variable	
   nodes	
   (circles)	
   
       factor	
   nodes	
   (squares)	
   
       edge	
   between	
   variable	
   and	
   factor	
   if	
   
the	
   factor	
   depends	
   on	
   that	
   variable.	
   
       the	
   graph	
   is	
   bipar@te.	
   

       a	
   factor	
   is	
   a	
   func@on	
   from	
   tuples	
   of	
   

r.v.	
   values	
   to	
   nonnega@ve	
   
numbers.	
   

x1	
   

x2	
   

x3	
   

  1	
   

  2	
   

  3	
   

  4	
   

p (x = x) /yj

 j(xj)

two	
   kinds	
   of	
   factors	
   

       condi@onal	
   id203	
   tables	
   

      e.g.,	
   p(x2	
   |	
   x1,	
   x3)	
   
      leads	
   to	
   bayesian	
   networks,	
   causal	
   
explana@ons	
   

       poten@al	
   func@ons	
   

      arbitrary	
   posi@ve	
   scores	
   
      leads	
   to	
   markov	
   networks	
   

x1	
   

x2	
   

x3	
   

  1	
   

  2	
   

  3	
   

  4	
   

example:	
   	
   bayesian	
   network	
   

       the	
      u	
   causes	
   sinus	
   

in   amma@on	
   

       allergies	
   also	
   cause	
   
sinus	
   in   amma@on	
   
       sinus	
   in   amma@on	
   
causes	
   a	
   runny	
   nose	
   
       sinus	
   in   amma@on	
   
causes	
   headaches	
   

flu	
   

all.	
   

s.i.	
   

r.n.	
   

h	
   

       the	
      u	
   causes	
   sinus	
   

in   amma@on	
   

       allergies	
   also	
   cause	
   
sinus	
   in   amma@on	
   
       sinus	
   in   amma@on	
   
causes	
   a	
   runny	
   nose	
   
       sinus	
   in   amma@on	
   
causes	
   headaches	
   

  f	
   

  a	
   

flu	
   

  fas	
   

all.	
   

  sr	
   

r.n.	
   

s.i.	
   

  sh	
   

h.	
   

   some	
   local	
   

con   gura@ons	
   are	
   
more	
   likely	
   than	
   

others.   	
   

f	
      f(f)	
   
0	
   
1	
   

a	
      a(a)	
   
0	
   
1	
   

f	
    a s	
      fas(f,a,s)	
   

0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

s	
   
0	
   
0	
   
1	
   
1	
   

r	
      sr(s,	
   r)	
   
0	
   
1	
   
0	
   
1	
   

s	
    h	
      sh(s,	
   h)	
   
0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

  f	
   

  a	
   

flu	
   

  fas	
   

all.	
   

  sr	
   

r.n.	
   

s.i.	
   

  sh	
   

h.	
   

   some	
   local	
   

con   gura@ons	
   are	
   
more	
   likely	
   than	
   

others.   	
   

example:	
   	
   markov	
   network	
   

       swinging	
   couples	
   or	
   confused	
   students	
   

a	
      	
   c	
   |	
   b,	
   d	
   
b	
      	
   d	
   |	
   a,	
   c	
   
  	
   b	
      	
   d	
   
  	
   a	
      	
   c	
   

b	
   

a	
   

c	
   

d	
   

example:	
   	
   markov	
   network	
   

vertex.	
   

       each	
   random	
   variable	
   is	
   a	
   
       undirected	
   edges.	
   
       factors	
   are	
   associated	
   

with	
   subsets	
   of	
   nodes	
   that	
   
form	
   cliques.	
   
      a	
   factor	
   maps	
   assignments	
   
of	
   its	
   nodes	
   to	
   nonnega@ve	
   
values.	
   

b	
   

a	
   

c	
   

d	
   

       in	
   this	
   example,	
   
associate	
   a	
   factor	
   
with	
   each	
   edge.	
   
      could	
   also	
   have	
   
factors	
   for	
   single	
   
nodes!	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

a	
   

c	
   

d	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

example:	
   	
   markov	
   network	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

=	
   7,201,840	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

example:	
   	
   markov	
   network	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

=	
   7,201,840	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

p(0,	
   1,	
   1,	
   0)	
   
=	
   5,000,000	
   /	
   z	
   
=	
   0.69	
   

example:	
   	
   markov	
   network	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

=	
   7,201,840	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

p(1,	
   1,	
   0,	
   0)	
   
=	
   10	
   /	
   z	
   
=	
   0.0000014	
   

independence	
   and	
   structure	
   
       there's	
   a	
   lot	
   of	
   theory	
   about	
   how	
   bns	
   and	
   

mns	
   encode	
   condi@onal	
   independence	
   
assump@ons.	
   
      bns:	
   	
   a	
   variable	
   x	
   is	
   independent	
   of	
   its	
   non-     
descendants	
   given	
   its	
   parents.	
   
      mns:	
   	
   condi@onal	
   independence	
   derived	
   from	
   
   markov	
   blanket   	
   and	
   separa,on	
   proper@es.	
   
      local	
   con   gura@ons	
   can	
   be	
   used	
   to	
   check	
   all	
   
condi@onal	
   independence	
   ques@ons;	
   almost	
   no	
   
need	
   to	
   look	
   at	
   the	
   values	
   in	
   the	
   factors!	
   

independence	
   spectrum	
   

various	
   graphs	
   

 i(xi)

 (x)

everything	
   is	
   dependent	
   

yi

full	
   independence	
   
assump@ons	
   

products	
   of	
   factors	
   

       given	
   two	
   factors	
   with	
   di   erent	
   scopes,	
   we	
   

can	
   calculate	
   a	
   new	
   factor	
   equal	
   to	
   their	
   
products.	
   

 product(x   y) =  1(x)     2(y)

products	
   of	
   factors	
   

       given	
   two	
   factors	
   with	
   di   erent	
   scopes,	
   we	
   

can	
   calculate	
   a	
   new	
   factor	
   equal	
   to	
   their	
   
products.	
   

a	
    b	
    c	
      3(a,	
   b,	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

. 

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

= 

0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

c)	
   
3000	
   
30	
   
5	
   
500	
   
100	
   
1	
   
10	
   
1000	
   

factor	
   maximiza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   
  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   maximiza@on:	
   

   (x) = max

y

 (x, y )

       we	
   can	
   refer	
   to	
   this	
   new	
   factor	
   by	
   maxy	
     .	
   	
   

factor	
   maximiza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   
  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   maximiza@on:	
   

	
   

   (x) = max

 (x, y )

y

a	
   
0	
   
0	
   
0	
   
0	
   
1	
   
1	
   
1	
   
1	
   

b	
   
0	
   
0	
   
1	
   
1	
   
0	
   
0	
   
1	
   
1	
   

c	
      	
   (a,	
   b,	
   c)	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   

0.9	
   
0.3	
   
1.1	
   
1.7	
   
0.4	
   
0.7	
   
1.1	
   
0.2	
   

a	
    c	
      (a,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1.1	
   
1.7	
   
1.1	
   
0.7	
   

b=1	
   
b=1	
   
b=1	
   
b=0	
   

   maximizing	
   out   	
   b	
   

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

a	
    c	
      (a,	
   c)	
   
2.0	
   
0	
    0	
   
0	
    1	
   
2.0	
   
1.5	
   
1	
    0	
   
1	
    1	
   
0.9	
   

   summing	
   out   	
   b	
   

a	
   
0	
   
0	
   
0	
   
0	
   
1	
   
1	
   
1	
   
1	
   

b	
   
0	
   
0	
   
1	
   
1	
   
0	
   
0	
   
1	
   
1	
   

c	
      	
   (a,	
   b,	
   c)	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   

0.9	
   
0.3	
   
1.1	
   
1.7	
   
0.4	
   
0.7	
   
1.1	
   
0.2	
   

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

a	
    b	
      (a,	
   b)	
   
1.2	
   
0	
    0	
   
0	
    1	
   
2.8	
   
1.1	
   
1	
    0	
   
1	
    1	
   
1.3	
   

   summing	
   out   	
   c	
   

a	
   
0	
   
0	
   
0	
   
0	
   
1	
   
1	
   
1	
   
1	
   

b	
   
0	
   
0	
   
1	
   
1	
   
0	
   
0	
   
1	
   
1	
   

c	
      	
   (a,	
   b,	
   c)	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   

0.9	
   
0.3	
   
1.1	
   
1.7	
   
0.4	
   
0.7	
   
1.1	
   
0.2	
   

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

       we	
   can	
   refer	
   to	
   this	
   new	
   factor	
   by	
      y	
     .	
   	
   

marginalizing	
   everything?	
   

       take	
   a	
   factor	
   graph   s	
      everything	
   factor   	
   by	
   

mul@plying	
   all	
   of	
   its	
   factors.	
   

       sum	
   out	
   all	
   the	
   variables	
   (one	
   by	
   one).	
   

       what	
   do	
   you	
   get?	
   

factors	
   are	
   like	
   numbers	
   

       products	
   are	
   commuta@ve:	
     1	
        2	
   	
   =	
     2	
        1	
   
       products	
   are	
   associa@ve:	
   	
   
(  1	
        2)      3	
   =	
     1	
      (  2     3)	
   
       sums	
   are	
   commuta@ve:	
   	
      x	
      y	
     	
   =	
      y	
      x	
     	
   
(max,	
   too).	
   
       distribu@vity	
   of	
   mul@plica@on	
   over	
   
marginaliza@on	
   and	
   maximiza@on:	
   
x        scope( 1)    x

( 1     2) =  1    x
 2
( 1     2) =  1    max

max
x

x

 2

id136	
   

querying	
   the	
   model	
   
       id136	
   (e.g.,	
   do	
   you	
   

have	
   allergies	
   or	
   the	
   
   u?)	
   

       what's	
   the	
   best	
   

explana@on	
   for	
   your	
   
symptoms?	
   

       ac@ve	
   data	
   collec@on	
   
(what	
   is	
   the	
   next	
   best	
   
r.v.	
   to	
   observe?)	
   

  f	
   

  a	
   

flu	
   

  fas	
   

all.	
   

  sr	
   

r.n.	
   

s.i.	
   

  sh	
   

h.	
   

a	
   bigger	
   example:	
   	
   your	
   car	
   

       the	
   car	
   doesn't	
   start.	
   
       what	
   do	
   we	
   conclude	
   
about	
   the	
   ba_ery	
   age?	
   

       18	
   random	
   variables	
   
       218	
   possible	
   scenarios	
   
	
   

id136:	
   	
   an	
   ubiquitous	
   obstacle	
   
       decoding	
   is	
   id136	
   (lecture	
   2).	
   
       learning	
   is	
   id136	
   (lectures	
   3	
   and	
   4).	
   

       exact	
   id136	
   is	
   #p-     complete.	
   

      even	
   approxima@ons	
   within	
   a	
   given	
   absolute	
   or	
   
rela@ve	
   error	
   are	
   hard.	
   

	
   

probabilis@c	
   id136	
   problems	
   

given	
   values	
   for	
   some	
   random	
   variables	
   (x	
       v)	
      	
   
       most	
   probable	
   explana@on:	
   	
   what	
   are	
   the	
   most	
   probable	
   values	
   of	
   the	
   rest	
   

of	
   the	
   r.v.s	
   v	
   \	
   x?	
   

	
   
(more	
   generally	
      )	
   
       maximum	
   a	
   posteriori	
   (map):	
   what	
   are	
   the	
   most	
   probable	
   values	
   of	
   some	
   

other	
   r.v.s,	
   y	
       (v	
   \	
   x)?	
   

	
   
       random	
   sampling	
   from	
   the	
   posterior	
   over	
   values	
   of	
   y	
   
       full	
   posterior	
   over	
   values	
   of	
   y	
   
       marginal	
   probabili@es	
   from	
   the	
   posterior	
   over	
   y	
   
       minimum	
   bayes	
   risk:	
   	
   what	
   is	
   the	
   y	
   with	
   the	
   lowest	
   expected	
   cost?	
   
       cost-     augmented	
   decoding:	
   	
   what	
   is	
   the	
   most	
   dangerous	
   y?	
   

approaches	
   to	
   id136	
   

id136	
   

exact	
   

approximate	
   

variable	
   

elimina@on	
   

ilp	
   

randomized	
   

determinis@c	
   

dynamic	
   

program'ng	
   

mcmc	
   

importance	
   
sampling	
   

randomized	
   

search	
   

varia@onal	
   

loopy	
   belief	
   
propaga@on	
   

lp	
   

relaxa@ons	
   

local	
   search	
   

gibbs	
   

simulated	
   
annealing	
   

mean	
      eld	
   

dual	
   

decomp.	
   

beam	
   search	
   

today	
   

tomorrow	
   

hard	
   id136	
   methods;	
   soe	
   id136	
   methods;	
   methods	
   for	
   both	
   

exact	
   marginal	
   for	
   y	
   

       this	
   will	
   be	
   a	
   generaliza@on	
   of	
   algorithms	
   you	
   

may	
   already	
   have	
   seen:	
   	
   the	
   forward	
   and	
   
backward	
   algorithms.	
   

       the	
   general	
   name	
   is	
   variable	
   elimina@on.	
   
       aeer	
   we	
   see	
   it	
   for	
   the	
   marginal,	
   we'll	
   see	
   how	
   

to	
   use	
   it	
   for	
   the	
   map.	
   

id136	
   example	
   
       goal:	
   	
   p(d)	
   
	
   

a	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

b	
   

0	
   
0	
   
1	
   
1	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

0	
   
1	
   
0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

id136	
   example	
   
       let   s	
   calculate	
   

p(b)	
      rst.	
   

	
   

a	
   

b	
   

c	
   

d	
   

id136	
   example	
   
       let   s	
   calculate	
   

a	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

p(b)	
      rst.	
   

	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

p (b) =  a val(a)

p (a = a)p (b | a = a)

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

id136	
   example	
   
       let   s	
   calculate	
   

a	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

p(b)	
      rst.	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

p (a = a)p (b | a = a)

p (b) =  a val(a)
       note:	
   	
   c	
   and	
   d	
   
don   t	
   ma_er.	
   

	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

id136	
   example	
   
       let   s	
   calculate	
   

p(b)	
      rst.	
   

p (b) =  a val(a)

	
   
b	
    p(b)	
   =	
   
  b(b)	
   

0	
   
1	
   

p (a = a)p (b | a = a)
b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

a	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

id136	
   example	
   

       new	
   model	
   in	
   

which	
   a	
   is	
   
eliminated;	
   
de   nes	
   	
   
p(b,	
   c,	
   d)	
   

	
   

	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

b	
    p(b)	
   =	
   
  b(b)	
   

0	
   
1	
   

b	
   

c	
   

d	
   

b	
   

0	
   
0	
   
1	
   
1	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

0	
   
1	
   
0	
   
1	
   

id136	
   example	
   
       same	
   thing	
   to	
   
eliminate	
   b.	
   

p (c) =  b val(b)

p (b = b)p (c | b = b)

	
   
c	
    p(c)	
   =	
   
  c(c)	
   

0	
   
1	
   

b	
    p(b)	
   =	
   
  b(b)	
   

0	
   
1	
   

b	
   

0	
   
0	
   
1	
   
1	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

0	
   
1	
   
0	
   
1	
   

b	
   

c	
   

d	
   

id136	
   example	
   

       new	
   model	
   in	
   

which	
   b	
   is	
   
eliminated;	
   
de   nes	
   	
   
p(c,	
   d)	
   

	
   

	
   

c	
    p(c)	
   =	
   
  c(c)	
   

0	
   
1	
   

c	
   

d	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

simple	
   id136	
   example	
   

       last	
   step	
   to	
   get	
   p(d):	
   
p (d) =  c val(c)

d	
    p(d)	
   =	
   
  d(d)	
   

0	
   
1	
   

c	
    p(c)	
   =	
   
  c(c)	
   

0	
   
1	
   

p (c = c)p (d | c = c)

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

d	
   

simple	
   id136	
   example	
   

       no@ce	
   that	
   the	
   same	
   step	
   happened	
   for	
   each	
   

random	
   variable:	
   
      we	
   created	
   a	
   new	
   factor	
   over	
   the	
   variable	
   and	
   its	
   
   successor   	
   
      we	
   summed	
   out	
   (marginalized)	
   the	
   variable.	
   

p (d) =  a val(a)  b val(b)  c val(c)

=  c val(c)

p (d | c = c)  b val(b)

p (a = a)p (b = b | a = a)p (c = c | b = b)p (d | c = c)
p (a = a)p (b = b | a = a)

p (c = c | b = b)  a val(a)

that	
   was	
   variable	
   elimina@on	
   
       we	
   reused	
   computa@on	
   from	
   previous	
   steps	
   
and	
   avoided	
   doing	
   the	
   same	
   work	
   more	
   than	
   
once.	
   
      dynamic	
   programming	
     	
   la	
   forward	
   algorithm!	
   

       we	
   exploited	
   the	
   graph	
   structure	
   (each	
   
subexpression	
   only	
   depends	
   on	
   a	
   small	
   
number	
   of	
   variables).	
   

       exponen@al	
   blowup	
   avoided!	
   

what	
   remains	
   
       variable	
   elimina@on	
   in	
   general	
   
       the	
   maximiza@on	
   version	
   (for	
   map	
   id136)	
   
       a	
   bit	
   about	
   approximate	
   id136	
   

elimina@ng	
   one	
   variable	
   

input:	
   	
   set	
   of	
   factors	
     ,	
   variable	
   z	
   to	
   eliminate	
   
output:	
   	
   new	
   set	
   of	
   factors	
     	
   
	
   
1.   let	
     '	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
2.   let	
     	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
3.   let	
     	
   be	
      z	
             '	
     	
   
4.   return	
     	
      	
   {  }	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   eliminate	
   h.	
   

flu	
   

  fas	
   

all.	
   

  sr	
   

r.n.	
   

s.i.	
   

  sh	
   

h.	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   eliminate	
   h.	
   
1.   	
     '	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
             '	
     	
   
4.   return	
     	
      	
   {  }	
   

flu	
   

  fas	
   

all.	
   

  sr	
   

r.n.	
   

s.i.	
   

  sh	
   

h.	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   eliminate	
   h.	
   
1.   	
     '	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
     sh	
   
4.   return	
     	
      	
   {  }	
   

flu	
   

  fas	
   

all.	
   

  sr	
   

r.n.	
   

s.i.	
   

  sh	
   

h.	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   eliminate	
   h.	
   
1.   	
     '	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
     sh	
   
4.   return	
     	
      	
   {  }	
   

flu	
   

  fas	
   

all.	
   

s.i.	
   

  sh	
   

  sr	
   

r.n.	
   

s	
    h	
      sh(s,	
   h)	
   
0	
   
0	
   
1	
   
1	
   

0.8	
   
0.2	
   
0.1	
   
0.9	
   

0	
   
1	
   
0	
   
1	
   

h.	
   

s	
   
0	
   
1	
   

  (s)	
   
1.0	
   
1.0	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   eliminate	
   h.	
   
1.   	
     '	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
     sh	
   
4.   return	
     	
      	
   {  }	
   

flu	
   

  fas	
   

all.	
   

s.i.	
   

  	
   

  sr	
   

r.n.	
   

s	
    h	
      sh(s,	
   h)	
   
0	
   
0	
   
1	
   
1	
   

0.8	
   
0.2	
   
0.1	
   
0.9	
   

0	
   
1	
   
0	
   
1	
   

s	
   
0	
   
1	
   

  (s)	
   
1.0	
   
1.0	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   eliminate	
   h.	
   
       we	
   can	
   actually	
   ignore	
   the	
   
new	
   factor,	
   equivalently	
   just	
   
dele@ng	
   h!	
   
       why?	
   
       in	
   some	
   cases	
   elimina@ng	
   a	
   

variable	
   is	
   really	
   easy!	
   

flu	
   

  fas	
   

all.	
   

s.i.	
   

  sr	
   

r.n.	
   

s	
   
0	
   
1	
   

  (s)	
   
1.0	
   
1.0	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       h	
   is	
   already	
   eliminated.	
   
       let's	
   now	
   eliminate	
   s.	
   

flu	
   

  fas	
   

all.	
   

s.i.	
   

  sr	
   

r.n.	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   s.	
   
1.   	
     '	
   =	
   {  sr,	
     fas}	
   
2.   	
     	
   =	
   {  f,	
     a}	
   
3.     far	
   =	
      s	
             '	
     	
   
4.   return	
     	
      	
   {  far}	
   

flu	
   

  fas	
   

all.	
   

s.i.	
   

  sr	
   

r.n.	
   

example	
   

  f	
   

  a	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   s.	
   
1.   	
     '	
   =	
   {  sr,	
     fas}	
   
2.   	
     	
   =	
   {  f,	
     a}	
   
3.     far	
   =	
      s	
     sr	
        	
     fas	
   
4.   return	
     	
      	
   {  far}	
   

flu	
   

  fas	
   

all.	
   

s.i.	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  far	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   s.	
   
1.   	
     '	
   =	
   {  sr,	
     fas}	
   
2.   	
     	
   =	
   {  f,	
     a}	
   
3.     far	
   =	
      s	
     sr	
        	
     fas	
   
4.   return	
     	
      	
   {  far}	
   

flu	
   

r.n.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       finally,	
   eliminate	
   a.	
   

flu	
   

r.n.	
   

  a	
   

all.	
   

  far	
   

  a	
   

all.	
   

  far	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   a.	
   
1.   	
     '	
   =	
   {  a,	
     far}	
   
2.   	
     	
   =	
   {  f}	
   
3.     fr	
   =	
      a	
     a	
        	
     far	
   
4.   return	
     	
      	
   {  fr}	
   

flu	
   

r.n.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   a.	
   
1.   	
     '	
   =	
   {  a,	
     far}	
   
2.   	
     	
   =	
   {  f}	
   
3.     fr	
   =	
      a	
     a	
        	
     far	
   
4.   return	
     	
      	
   {  fr}	
   

flu	
   

  fr	
   

r.n.	
   

chain,	
   again	
   
       goal:	
   	
   p(d)	
   
       earlier,	
   we	
   

eliminated	
   a,	
   
then	
   b,	
   then	
   
c.	
   

	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

b	
   

0	
   
0	
   
1	
   
1	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

0	
   
1	
   
0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

a	
   

0	
   
0	
   
1	
   
1	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

0	
   
1	
   
0	
   
1	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

chain,	
   again	
   
       goal:	
   	
   p(d)	
   
       earlier,	
   we	
   

eliminated	
   a,	
   
then	
   b,	
   then	
   
c.	
   

       let   s	
   start	
   

with	
   c.	
   

	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

b	
   

0	
   
0	
   
1	
   
1	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

0	
   
1	
   
0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

a	
   

0	
   
0	
   
1	
   
1	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

0	
   
1	
   
0	
   
1	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

chain,	
   again	
   
       goal:	
   	
   p(d)	
   
       earlier,	
   we	
   

eliminated	
   a,	
   
then	
   b,	
   then	
   
c.	
   

       let   s	
   start	
   

with	
   c.	
   

	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

b	
   

0	
   
0	
   
1	
   
1	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

0	
   
1	
   
0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

a	
   

0	
   
0	
   
1	
   
1	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

0	
   
1	
   
0	
   
1	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

chain,	
   again	
   
       elimina@ng	
   c.	
   
	
   
b	
   

c	
    p(c	
   |	
   b)	
   =	
   
  bc(b,	
   c)	
   

c	
    d	
    p(d	
   |	
   c)	
   =	
   
  cd(c,	
   d)	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

0	
   
0	
   
1	
   
1	
   

0	
   
1	
   
0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

b	
    c	
    d	
      bcd(b,	
   c,	
   d)	
   
0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

chain,	
   again	
   
       elimina@ng	
   c.	
   
	
   

b	
    c	
    d	
      bcd(b,	
   c,	
   d)	
   
0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

a	
   

b	
   

c	
   

d	
   

  (b,	
   d)	
   

b	
    d	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

chain,	
   again	
   
       elimina@ng	
   b	
   will	
   be	
   

similarly	
   complex.	
   

	
   

a	
    p(a)	
   =	
   
  a(a)	
   

0	
   
1	
   

b	
    p(b	
   |	
   a)	
   =	
   
  ab(a,	
   b)	
   

0	
   
1	
   
0	
   
1	
   

  (b,	
   d)	
   

a	
   

0	
   
0	
   
1	
   
1	
   

b	
    d	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

a	
   

b	
   

d	
   

variable	
   elimina@on:	
   	
   comments	
   
       can	
   prune	
   away	
   all	
   non-     ancestors	
   of	
   the	
   query	
   

variables.	
   

       ordering	
   makes	
   a	
   di   erence!	
   

what	
   about	
   evidence?	
   

       so	
   far,	
   we've	
   just	
   considered	
   the	
   posterior/

marginal	
   p(y).	
   

       next:	
   	
   condi@onal	
   distribu@on	
   p(y	
   |	
   x	
   =	
   x).	
   

       it's	
   almost	
   the	
   same:	
   	
   the	
   addi@onal	
   step	
   is	
   to	
   

reduce	
   factors	
   to	
   respect	
   the	
   evidence.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  sh	
   

h.	
   

s	
    r	
      sr	
   (s,	
   r)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

s	
    r	
      's	
   (s)	
   

s	
    r	
      's	
   (s)	
   

0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

0	
    1	
   
1	
    1	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let's	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

  fa
s	
   

s.i.	
   

flu	
   

  's	
   

  a	
   

all.	
   

  sh	
   

h.	
   

s	
    r	
      's	
   (s)	
   

0	
    1	
   
1	
    1	
   

example	
   

  f	
   

  a	
   

all.	
   

  sh	
   

  fa
s	
   

s.i.	
   

flu	
   

  's	
   

h.	
   

eliminate	
   h.	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

  a	
   

all.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

  fa
s	
   

s.i.	
   

flu	
   

  's	
   

eliminate	
   s.	
   

example	
   

  f	
   

  a	
   

flu	
   

  fa	
   

all.	
   

eliminate a. 

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

example	
   

  f	
   

flu	
   

  f	
   

take	
      nal	
   product.	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

example	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor.	
   

  f	
        f	
   

addi@onal	
   comments	
   

       run@me	
   depends	
   on	
   the	
   size	
   of	
   the	
   intermediate	
   
       hence,	
   variable	
   elimina@on	
   ordering	
   ma_ers	
   a	
   

factors.	
   

lot.	
   
       but	
   it's	
   np-     hard	
   to	
      nd	
   the	
   best	
   one.	
   
       for	
   mns,	
   chordal	
   graphs	
   permit	
   id136	
   in	
   @me	
   
       for	
   bns,	
   polytree	
   structures	
   do	
   the	
   same.	
   

linear	
   in	
   the	
   size	
   of	
   the	
   original	
   factors.	
   

       if	
   you	
   can	
   avoid	
      big   	
   intermediate	
   factors,	
   you	
   

can	
   make	
   id136	
   linear	
   in	
   the	
   size	
   of	
   the	
   
original	
   factors.	
   

variable	
   elimina@on	
   for	
   	
   

condi@onal	
   probabili@es	
   p(y	
   |	
   x	
   =	
   x)	
   

input:	
   	
   graphical	
   model	
   on	
   v,	
   set	
   of	
   query	
   variables	
   

y,	
   evidence	
   x	
   =	
   x	
   	
   

output:	
   	
   factor	
     	
   and	
   scalar	
     	
   
1.   	
     	
   =	
   factors	
   in	
   the	
   model	
   
2.   reduce	
   factors	
   in	
     	
   by	
   x	
   =	
   x	
   
3.   choose	
   variable	
   ordering	
     	
   on	
   z	
   	
   =	
   v	
   \	
   y	
   \	
   x	
   
4.     	
   =	
   variable-     elimina@on(  ,	
   z,	
     )	
   
5.     	
   =	
      z   val(z)	
     (z)	
   
6.   return	
     ,	
     	
   

	
   

ge(cid:137)ng	
   back	
   to	
   nlp	
   

       tradi@onal	
   structured	
   nlp	
   models	
   were	
   
some@mes	
   chosen	
   for	
   these	
   proper@es.	
   
      id48s,	
   pid18s	
   (with	
   a	
   li_le	
   work)	
   
      but	
   not:	
   	
   ibm	
   model	
   3	
   

       to	
   decode,	
   we	
   need	
   map	
   id136	
   for	
   

decoding!	
   

       when	
   models	
   get	
   complicated,	
   need	
   

approxima@ons!	
   

from	
   marginals	
   to	
   map	
   
       replace	
   factor	
   marginaliza@on	
   steps	
   with	
   

maximiza,on.	
   
       add	
   bookkeeping	
   to	
   keep	
   track	
   of	
   the	
   maximizing	
   

       add	
   a	
   traceback	
   at	
   the	
   end	
   to	
   recover	
   the	
   

values.	
   

solu@on.	
   

       this	
   is	
   analogous	
   to	
   the	
   connec@on	
   between	
   the	
   

forward	
   algorithm	
   and	
   the	
   viterbi	
   algorithm.	
   
       ordering	
   challenge	
   is	
   the	
   same.	
   

variable	
   elimina@on	
   

(max-     product	
   version	
   with	
   decoding)	
   

input:	
   	
   set	
   of	
   factors	
     ,	
   ordered	
   list	
   of	
   variables	
   

z	
   to	
   eliminate	
   

output:	
   	
   new	
   factor	
   
1.   for	
   each	
   zi	
      	
   z	
   (in	
   order):	
   

       let	
   (  ,	
     zi)	
   =	
   eliminate-     one(  ,	
   zi) 	
   	
   
2.   return	
             	
     ,	
   traceback({  zi})	
   

elimina@ng	
   one	
   variable	
   

(max-     product	
   version	
   with	
   bookkeeping)	
   
input:	
   	
   set	
   of	
   factors	
     ,	
   variable	
   z	
   to	
   eliminate	
   
output:	
   	
   new	
   set	
   of	
   factors	
     	
   
	
   
1.   let	
     '	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
2.   let	
     	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
3.   let	
     	
   be	
   maxz	
             '	
     	
   
       let	
      be           '	
     	
   (bookkeeping) 	
   
4.   return	
     	
      	
   {  },	
     	
   	
   

traceback	
   

input:	
   	
   sequence	
   of	
   factors	
   with	
   associated	
   

variables:	
   	
   (  z1,	
      ,	
     zk)	
   

output:	
   	
   z*	
   
       each	
     z	
   is	
   a	
   factor	
   with	
   scope	
   including	
   z	
   and	
   
variables	
   eliminated	
   a<er	
   z.	
   
       work	
   backwards	
   from	
   i	
   =	
   k	
   to	
   1:	
   
      let	
   zi	
   =	
   arg	
   maxz	
     zi(z,	
   zi+1,	
   zi+2,	
      ,	
   zk)	
   

       return	
   z	
   

about	
   the	
   traceback	
   

       no	
   extra	
   (asympto@c)	
   expense.	
   

      linear	
   traversal	
   over	
   the	
   intermediate	
   factors.	
   
       the	
   factor	
   opera@ons	
   for	
   both	
   sum-     product	
   
ve	
   and	
   max-     product	
   ve	
   can	
   be	
   generalized.	
   
      example:	
   	
   get	
   the	
   k	
   most	
   likely	
   assignments	
   

variable	
   elimina@on	
   tips	
   

       any	
   ordering	
   will	
   be	
   correct.	
   
       most	
   orderings	
   will	
   be	
   too	
   expensive.	
   
       there	
   are	
   heuris@cs	
   for	
   choosing	
   an	
   ordering.	
   

      if	
   the	
   graph	
   is	
   chain-     like,	
   work	
   from	
   one	
   end	
   
toward	
   the	
   other.	
   

(rocket	
   science:	
   	
   true	
   map)	
   

       evidence:	
   	
   x	
   =	
   x	
   
       query:	
   	
   y	
   
       other	
   variables:	
   	
   z	
   =	
   v	
   \	
   x	
   \	
   y	
   

y  = arg max

y   val(y )

p (y = y | x = x)

= arg max

p (y = y, z = z | x = x)
       first,	
   marginalize	
   out	
   z,	
   then	
   do	
   map	
   id136	
   over	
   y	
   

y   val(y )  z   val(z)

given	
   x	
   =	
   x	
   

       this	
   is	
   not	
   usually	
   a_empted	
   in	
   nlp,	
   with	
   some	
   excep@ons.	
   

par@ng	
   shots	
   

       you	
   will	
   probably	
   never	
   implement	
   the	
   
general	
   variable	
   elimina@on	
   algorithm.	
   

       you	
   will	
   rarely	
   use	
   exact	
   id136.	
   
       understand	
   the	
   id136	
   problem	
   would	
   look	
   

like	
   in	
   exact	
   form;	
   then	
   approximate.	
   
      some@mes	
   you	
   get	
   lucky.	
   
      you   ll	
   appreciate	
   be_er	
   approxima@ons	
   as	
   they	
   
come	
   along.	
   

