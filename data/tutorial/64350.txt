id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith	
   

	
   

heidelberg	
   university,	
   november	
   2014	
   

two	
   meanings	
   of	
      structure   	
   

       yesterday:	
   	
   structure	
   of	
   a	
   graph	
   for	
   modeling	
   a	
   

collechon	
   of	
   random	
   variables	
   together.	
   

       today:	
   	
   linguishc	
   structure.	
   

      sequence	
   labelings	
   (pos,	
   iob	
   chunkings,	
      )	
   
      parse	
   trees	
   (phrase-     structure,	
   dependency,	
      )	
   
      alignments	
   (word,	
   phrase,	
   tree,	
      )	
   
      predicate-     argument	
   structures	
   
      text-     to-     text	
   (translahon,	
   paraphrase,	
   answers,	
      )	
   

a	
   useful	
   abstrachon?	
   

       i	
   think	
   so.	
   
       brings	
   out	
   commonalihes:	
   

      modeling	
   formalisms	
   (e.g.,	
   linear	
   models	
   with	
   
features)	
   
      learning	
   algorithms	
   (lectures	
   3-     4)	
   
      generic	
   id136	
   algorithms	
   

       permits	
   sharing	
   across	
   a	
   wider	
   space	
   of	
   
       disadvantage:	
   	
   hides	
   engineering	
   details.	
   

problems.	
   

familiar	
   example:	
   	
   	
   

hidden	
   markov	
   models	
   

hidden	
   markov	
   model	
   
       x	
   and	
   y	
   are	
   both	
   sequences	
   of	
   symbols	
   

      x	
   is	
   a	
   sequence	
   from	
   the	
   vocabulary	
     	
   
      y	
   is	
   a	
   sequence	
   from	
   the	
   state	
   space	
     	
   
p(x = x, y = y) =   n   i=1
      transihons	
   p(y   	
   |	
   y)	
   

p(xi | yi)p(yi | yi 1)    p(stop | yn)

       parameters:	
   

       including	
   p(stop	
   |	
   y),	
   p(y	
   |	
   start)	
   

      emissions	
   p(x	
   |	
   y)	
   

hidden	
   markov	
   model	
   

       the	
   joint	
   model   s	
   independence	
   assumphons	
   
are	
   easy	
   to	
   capture	
   with	
   a	
   bayesian	
   network.	
   

p(x = x, y = y) =   n   i=1

p(xi | yi)p(yi | yi 1)    p(stop | yn)

y0	
   

   	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

yn	
   

xn	
   

stop	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

y0	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

stop	
   

x1	
   =	
   
x1	
   

x2	
   =	
   
x2	
   

x3	
   =	
   
x3	
   

xn	
   =	
   
xn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

       factor	
   graph:	
   

y0	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

stop	
   

x1	
   =	
   
x1	
   

x2	
   =	
   
x2	
   

x3	
   =	
   
x3	
   

xn	
   =	
   
xn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

       factor	
   graph	
   acer	
   reducing	
   factors	
   to	
   respect	
   

evidence:	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

       clever	
   ordering	
   should	
   be	
   apparent!	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
   take	
   a	
   product	
   of	
   
three	
   relevant	
   factors.	
   
       p(y1	
   |	
   start)	
   
         (y1)	
   =	
   reduced	
   p(x1	
   |	
   y1)	
   
       p(y2	
   |	
   y1)	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
      rst	
   take	
   a	
   product	
   
of	
   two	
   factors	
   that	
   only	
   involve	
   y1.	
   
	
   

p(y1	
   |	
   start)	
   
	
   

y1	
   
y2	
   
   	
   
y|  |	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

y1	
   
y2	
   
   	
   
y|  |	
   

  (y1)	
   =	
   reduced	
   p(x1	
   |	
   y1)	
   
	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
      rst	
   take	
   a	
   product	
   
of	
   two	
   factors	
   that	
   only	
   involve	
   y1.	
   
       this	
   is	
   the	
   viterbi	
   id203	
   vector	
   for	
   y1.	
   

	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

y1	
   
y2	
   
   	
   
y|  |	
   
  1(y1)	
   
	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
      rst	
   take	
   a	
   product	
   
of	
   two	
   factors	
   that	
   only	
   involve	
   y1.	
   
       this	
   is	
   the	
   viterbi	
   id203	
   vector	
   for	
   y1.	
   
       eliminahng	
   y1	
   equates	
   to	
   solving	
   the	
   viterbi	
   
probabilihes	
   for	
   y2.	
   
	
   

y1	
   
y2	
   
   	
   
y|  |	
   
  1(y1)	
   
	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

y1	
   

y2	
   

   	
   

y|  |	
   

p(y2	
   |	
   y1)	
   
	
   

hidden	
   markov	
   model	
   
       product	
   of	
   all	
   factors	
   involving	
   y1,	
   then	
   
reduce.	
   
         2(y2)	
   =	
   maxy   val(y1)	
   (  1(y)	
      	
   p(y2	
   |	
   y))	
   
       this	
   factor	
   holds	
   viterbi	
   probabilihes	
   for	
   y2.	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y2,	
   we	
   take	
   a	
   product	
   of	
   
the	
   analogous	
   two	
   relevant	
   factors.	
   
       then	
   reduce.	
   

         3(y3)	
   =	
   maxy   val(y2)	
   (  2(y)	
      	
   p(y3	
   |	
   y))	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       at	
   the	
   end,	
   we	
   have	
   one	
      nal	
   factor	
   with	
   one	
   

row,	
     n+1.	
   

       this	
   is	
   the	
   score	
   of	
   the	
   best	
   sequence.	
   
       use	
   backtrace	
   to	
   recover	
   values.	
   

yn	
   

why	
   think	
   this	
   way?	
   
       easy	
   to	
   see	
   how	
   to	
   generalize	
   id48s.	
   

      more	
   evidence	
   
      more	
   factors	
   
      more	
   hidden	
   structure	
   
      more	
   dependencies	
   

       probabilishc	
   interpretahon	
   of	
   factors	
   is	
   not	
   

central	
   to	
      nding	
   the	
      best   	
   y	
      	
   
      many	
   factors	
   are	
   not	
   condihonal	
   id203	
   
tables.	
   

generalizahon	
   example	
   1	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

       each	
   word	
   also	
   depends	
   on	
   previous	
   state.	
   
	
   

generalizahon	
   example	
   1	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

       each	
   word	
   also	
   depends	
   on	
   previous	
   state.	
   
	
   

generalizahon	
   example	
   2	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

          trigram   	
   id48	
   
	
   

generalizahon	
   example	
   2	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

          trigram   	
   id48	
   
	
   

generalizahon	
   example	
   3	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

       aggregate	
   bigram	
   model	
   (saul	
   and	
   pereira,	
   

1997)	
   

generalizahon	
   example	
   3	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

       aggregate	
   bigram	
   model	
   (saul	
   and	
   pereira,	
   

1997)	
   

	
   

general	
   decoding	
   problem	
   
       two	
   structured	
   random	
   variables,	
   x	
   and	
   y.	
   
      somehmes	
   described	
   as	
   collec,ons	
   of	
   random	
   
variables.	
   

          decode   	
   observed	
   value	
   x	
   =	
   x	
   into	
   some	
   

value	
   of	
   y.	
   

       usually,	
   we	
   seek	
   to	
   maximize	
   some	
   score.	
   

      e.g.,	
   map	
   id136	
   from	
   yesterday.	
   

linear	
   models	
   

       de   ne	
   a	
   feature	
   vector	
   funchon	
   g	
   that	
   maps	
   (x,	
   y)	
   pairs	
   
       score	
   is	
   linear	
   in	
   g(x,	
   y).	
   

into	
   d-     dimensional	
   real	
   space.	
   

score(x, y) = w   g(x, y)

y  = arg max
y   yx

w   g(x, y)

       results:	
   	
   	
   

       decoding	
   seeks	
   y	
   to	
   maximize	
   the	
   score.	
   
       learning	
   seeks	
   w	
   to	
      	
   do	
   something	
   we   ll	
   talk	
   about	
   later.	
   

       extremely	
   general!	
   

generic	
   noisy	
   channel	
   as	
   linear	
   model	
   

  y = arg max

y

= arg max
= arg max

y

y

= arg max

y

log (p(y)    p(x | y))
log p(y) + log p(x | y)
wy + wx|y
w g(x, y)

       of	
   course,	
   the	
   two	
   id203	
   terms	
   are	
   

typically	
   composed	
   of	
      smaller   	
   factors;	
   each	
   
can	
   be	
   understood	
   as	
   an	
   exponenhated	
   
weight.	
   

max	
   ent	
   models	
   as	
   linear	
   models	
   

  y = arg max

y

= arg max

y

= arg max

y

= arg max

y

exp w g(x, y)

log p(y | x)
log
w g(x, y)   log z(x)
w g(x, y)

z(x)

id48s	
   as	
   linear	
   models	
   

  y = arg max

log p(x, y)

y

= arg max

= arg max

= arg max

y   n   i=1
y   n   i=1
y    y,x

log p(xi | yi) + log p(yi | yi 1)    + log p(stop | yn)
wyi   xi + wyi 1   yi    + wyn   stop
wy   xfreq(y     x; y, x) +   y,y   

wy   y   freq(y   y   ; y)

= arg max

w   g(x, y)

y

running	
   example	
   

       iob	
   sequence	
   labeling,	
   here	
   applied	
   to	
   ner	
   
       ocen	
   solved	
   with	
   id48s,	
   crfs,	
   m3ns	
      	
   

(what	
   is	
   not	
   a	
   linear	
   model?)	
   

       models	
   with	
   hidden	
   variables	
   
y  z
p(y | x) = arg max

	
   
	
   
(also	
      neural   	
   models)	
   

arg max

y

p(y, z | x)

       models	
   based	
   on	
   non-     linear	
   kernels	
   

arg max

y

w g(x, y) = arg max

y

n i=1

 ik ( xi, yi   , x, y   )

decoding	
   

       for	
   id48s,	
   the	
   decoding	
   algorithm	
   we	
   usually	
   

think	
   of	
      rst	
   is	
   the	
   viterbi	
   algorithm.	
   	
   	
   
      this	
   is	
   just	
   one	
   example.	
   

       we	
   will	
   view	
   decoding	
   in	
      ve	
   di   erent	
   ways.	
   

      sequence	
   models	
   as	
   a	
   running	
   example.	
   
      these	
   views	
   are	
   not	
   just	
   for	
   id48s.	
   
      somehmes	
   they	
   will	
   lead	
   us	
   back	
   to	
   viterbi!	
   

five	
   views	
   of	
   decoding	
   

id136	
   in	
   a	
   
probabilishc	
   

graphical	
   model!	
   

x	
      	
   y	
   

1.	
   	
   probabilishc	
   graphical	
   models	
   
       view	
   the	
   linguishc	
   structure	
   as	
   a	
   collechon	
   of	
   

random	
   variables	
   that	
   are	
   interdependent.	
   

       represent	
   interdependencies	
   as	
   a	
   directed	
   or	
   

undirected	
   graphical	
   model.	
   

       condihonal	
   id203	
   tables	
   (bns)	
   or	
   factors	
   

(mns)	
   encode	
   the	
   id203	
   distribuhon.	
   

id136	
   in	
   graphical	
   models	
   
       general	
   algorithm	
   for	
   exact	
   map	
   id136:	
   	
   

variable	
   eliminahon.	
   
      iterahvely	
   solve	
   for	
   the	
   best	
   values	
   of	
   each	
   
variable	
   condihoned	
   on	
   values	
   of	
      preceding   	
   
neighbors.	
   
      then	
   trace	
   back.	
   

the	
   viterbi	
   algorithm	
   is	
   an	
   instance	
   of	
   

max-     product	
   variable	
   eliminahon!	
   

map	
   is	
   linear	
   decoding	
   

       bayesian	
   network:	
   

       markov	
   network:	
   

 i
log p(xi | parents(xi))
+ j
log p(yj | parents(yj))
 c

log  c ({xi}i c,{yj}j c)

       this	
   only	
   works	
   if	
   every	
   variable	
   is	
   in	
   x	
   or	
   y.	
   

id136	
   in	
   graphical	
   models	
   

       remember:	
   	
   more	
   edges	
   make	
   id136	
   more	
   

expensive.	
   
      fewer	
   edges	
   means	
   stronger	
   independence.	
   

       really	
   pleasant:	
   

id136	
   in	
   graphical	
   models	
   

       remember:	
   	
   more	
   edges	
   make	
   id136	
   more	
   

expensive.	
   
      fewer	
   edges	
   means	
   stronger	
   independence.	
   

       really	
   unpleasant:	
   

id136	
   in	
   a	
   
probabilishc	
   

graphical	
   model!	
   

integer	
   linear	
   
programming!	
   

x	
      	
   y	
   

   parts   	
   

       assume	
   that	
   feature	
   funchon	
   g	
   breaks	
   down	
   

into	
   local	
   parts.	
   

#parts(x) i=1

g(x, y) =

f( i(x, y))

	
   
       each	
   part	
   has	
   an	
   alphabet	
   of	
   possible	
   values.	
   

      decoding	
   is	
   choosing	
   values	
   for	
   all	
   parts,	
   with	
   
consistency	
   constraints.	
   
      (in	
   the	
   graphical	
   models	
   view,	
   a	
   part	
   corresponds	
   
to	
   a	
   factor	
   assignment.)	
   

example	
   

       one	
   part	
   per	
   word,	
   each	
   is	
   in	
   {b,	
   i,	
   o}	
   
       no	
   features	
   look	
   at	
   mulhple	
   parts	
   

      fast	
   id136	
   
      not	
   very	
   expressive	
   

example	
   

       one	
   part	
   per	
   bigram,	
   each	
   is	
   in	
   {bb,	
   bi,	
   bo,	
   
       features	
   and	
   constraints	
   can	
   look	
   at	
   pairs	
   	
   

ib,	
   ii,	
   io,	
   ob,	
   oo}	
   

      slower	
   id136	
   
      a	
   bit	
   more	
   expressive	
   

geometric	
   view	
   

       let	
   zi,  	
   be	
   1	
   if	
   part	
   i	
   takes	
   value	
     	
   and	
   0	
   
otherwise.	
   
       z	
   is	
   a	
   vector	
   in	
   {0,	
   1}n	
   	
   

      n	
   =	
   total	
   number	
   of	
   localized	
   part	
   values	
   
      each	
   z	
   is	
   a	
   vertex	
   of	
   the	
   unit	
   cube	
   

score	
   is	
   linear	
   in	
   z	
   

arg max

y

w   g(x, y) = arg max

not	
   really	
   
equal;	
   need	
   
to	
   transform	
   
back	
   to	
   get	
   y	
   

f( i(x, y))

     values( i)
     values( i)

f( )1{ i(x, y) =  }

f( )zi, 

y

y

w   

w   

#parts(x)   i=1
#parts(x)   i=1
#parts(x)   i=1
z zx w   fx    z

w   fxz

w   

= arg max

= arg max
z zx
= arg max
z zx
= arg max

polyhedra	
   

       not	
   all	
   verhces	
   of	
   the	
   n-     dimensional	
   unit	
   cube	
   

sahsfy	
   the	
   constraints.	
   
      e.g.,	
   can   t	
   have	
   z1,bi	
   =	
   1 and	
   z2,bi	
   =	
   1	
   

       somehmes	
   we	
   can	
   write	
   down	
   a	
   small	
   

(polynomial	
   number)	
   of	
   linear	
   constraints	
   on	
   
z.	
   

       result:	
   	
   linear	
   objechve,	
   linear	
   constraints,	
   

integer	
   constraints	
      	
   

integer	
   linear	
   programming	
   

features.	
   

       very	
   easy	
   to	
   add	
   new	
   constraints	
   and	
   non-     local	
   
       many	
   decoding	
   problems	
   have	
   been	
   mapped	
   to	
   
ilp	
   (sequence	
   labeling,	
   parsing,	
      ),	
   but	
   it   s	
   not	
   
always	
   trivial.	
   	
   

       np-     hard	
   in	
   general.	
   

prachce	
   (e.g.,	
   cplex)	
   

       but	
   there	
   are	
   packages	
   that	
   ocen	
   work	
   well	
   in	
   
       specialized	
   algorithms	
   in	
   some	
   cases	
   
       lp	
   relaxahon	
   for	
   approximate	
   soluhons	
   

remark	
   

       graphical	
   models	
   assumed	
   a	
   probabilishc	
   

interpretahon	
   
      though	
   they	
   are	
   not	
   always	
   learned	
   using	
   a	
   
probabilishc	
   interpretahon!	
   

       the	
   polytope	
   view	
   is	
   agnoshc	
   about	
   how	
   you	
   

interpret	
   the	
   weights.	
   
      it	
   only	
   says	
   that	
   the	
   decoding	
   problem	
   is	
   an	
   ilp.	
   

id136	
   in	
   a	
   
probabilishc	
   

graphical	
   model!	
   

integer	
   linear	
   
programming!	
   

x	
      	
   y	
   

parsing	
   (with	
   

weights)!	
   

grammars	
   

       grammars	
   are	
   ocen	
   associated	
   with	
   natural	
   

language	
   parsing,	
   but	
   they	
   are	
   extremely	
   
powerful	
   for	
   imposing	
   constraints.	
   

       we	
   can	
   add	
   weights	
   to	
   them.	
   

(closely	
   connected	
   to	
   wfsas)	
   

       id48s	
   are	
   a	
   kind	
   of	
   weighted	
   regular	
   grammar	
   
       pid18s	
   are	
   a	
   kind	
   of	
   weighted	
   id18	
   
       many,	
   many	
   more.	
   

       weighted	
   parsing:	
   	
      nd	
   the	
   maximum-     weighted	
   

derivahon	
   for	
   a	
   string	
   x.	
   	
   

decoding	
   as	
   weighted	
   parsing	
   

       every	
   valid	
   y	
   is	
   a	
   grammahcal	
   derivahon	
   

(parse)	
   for	
   x.	
   
      id48:	
   	
   sequence	
   of	
      grammahcal   	
   states	
   is	
   one	
   
allowed	
   by	
   the	
   transihon	
   table.	
   

       augment	
   parsing	
   algorithms	
   with	
   weights	
   and	
   

   nd	
   the	
   best	
   parse.	
   	
   

the	
   viterbi	
   algorithm	
   is	
   an	
   instance	
   of	
   
recognihon	
   by	
   a	
   weighted	
   grammar!	
   

bio	
   tagging	
   as	
   a	
   id18	
   

       weighted	
   (or	
   probabilishc)	
   cky	
   is	
   a	
   dynamic	
   

programming	
   algorithm	
   very	
   similar	
   in	
   
structure	
   to	
   classical	
   cky.	
   

id136	
   in	
   a	
   
probabilishc	
   

graphical	
   model!	
   

integer	
   linear	
   
programming!	
   

parsing	
   (with	
   

weights)!	
   

x	
      	
   y	
   

shortest	
   

(hyper)path!	
   

best	
   path	
   

       general	
   idea:	
   	
   take	
   x	
   and	
   build	
   a	
   graph.	
   
       score	
   of	
   a	
   path	
   factors	
   into	
   the	
   edges.	
   
arg max

w   g(x, y) = arg max

y

w     e edges

y

       decoding	
   is	
      nding	
   the	
   best	
   path.	
   

f(e)1{e is crossed by y   s path}

the	
   viterbi	
   algorithm	
   is	
   an	
   instance	
   of	
   

   nding	
   a	
   best	
   path!	
   

	
   

   lauce   	
   view	
   of	
   viterbi	
   

minimum	
   cost	
   hyperpath	
   

       general	
   idea:	
   	
   take	
   x	
   and	
   build	
   a	
   hypergraph.	
   
       score	
   of	
   a	
   hyperpath	
   factors	
   into	
   the	
   

hyperedges.	
   

       decoding	
   is	
      nding	
   the	
   best	
   hyperpath.	
   

       this	
   connechon	
   was	
   elucidated	
   by	
   klein	
   and	
   

manning	
   (2002).	
   

	
   

parsing	
   as	
   a	
   hypergraph	
   

parsing	
   as	
   a	
   hypergraph	
   

cf.    dean for democracy    

parsing	
   as	
   a	
   hypergraph	
   

forced to work on his thesis, sunshine streaming in 
the window, mike experienced a     

parsing	
   as	
   a	
   hypergraph	
   

forced to work on his thesis, sunshine streaming in 
the window, mike began to     

why	
   hypergraphs?	
   

       useful,	
   compact	
   encoding	
   of	
   the	
   hypothesis	
   

space.	
   
      build	
   hypothesis	
   space	
   using	
   local	
   features,	
   maybe	
   
do	
   some	
      ltering.	
   
      pass	
   it	
   o   	
   to	
   another	
   module	
   for	
   more	
      ne-     
grained	
   scoring	
   with	
   richer	
   or	
   more	
   expensive	
   
features.	
   

id136	
   in	
   a	
   
probabilishc	
   

graphical	
   model!	
   

integer	
   linear	
   
programming!	
   

weighted	
   logic	
   
programming!	
   

parsing	
   (with	
   

weights)!	
   

x	
      	
   y	
   

shortest	
   path!	
   

logic	
   programming	
   

       start	
   with	
   a	
   set	
   of	
   axioms	
   and	
   a	
   set	
   of	
   id136	
   

rules.	
   

       the	
   goal	
   is	
   to	
   prove	
   a	
   speci   c	
   theorem,	
   goal.	
   
       many	
   approaches,	
   but	
   we	
   assume	
   a	
   deduc,ve	
   

approach.	
   
       start	
   with	
   axioms,	
   iterahvely	
   produce	
   more	
   theorems.	
   

weighted	
   deduchon	
   

       twist:	
   	
   axioms	
   have	
   weights.	
   
       want	
   the	
   proof	
   of	
   goal with	
   the	
   best	
   score:	
   

arg max

y

w   g(x, y) = arg max

y

w     a axioms

f(a)freq(a; y)

       note	
   that	
   axioms	
   can	
   be	
   used	
   more	
   than	
   once	
   

in	
   a	
   proof	
   (y).	
   

weighted	
   deduchon	
   

       shieber,	
   schabes,	
   and	
   pereira	
   (1995):	
   	
   many	
   
parsing	
   algorithms	
   can	
   be	
   understood	
   in	
   the	
   
same	
   deduchve	
   logic	
   framework.	
   

       goodman	
   (1999):	
   	
   add	
   weights,	
   get	
   many	
   

useful	
   nlp	
   algorithms.	
   

       eisner,	
   goldlust,	
   and	
   smith	
   (2004,	
   2005):	
   	
   

semiring-     generic	
   algorithms,	
   dyna.	
   

dynamic	
   programming	
   
       most	
   views	
   (excephon	
   is	
   polytopes)	
   can	
   be	
   
understood	
   as	
   dp	
   algorithms.	
   
       the	
   low-     level	
   procedures	
   we	
   use	
   are	
   ocen	
   dp.	
   
       even	
   dp	
   is	
   too	
   high-     level	
   to	
   know	
   the	
   best	
   way	
   to	
   
       dp	
   does	
   not	
   imply	
   polynomial	
   hme	
   and	
   space!	
   

implement.	
   	
   

       most	
   common	
   approximahons	
   when	
   the	
   desired	
   state	
   
space	
   is	
   too	
   big:	
   	
   beam	
   search,	
   cube	
   pruning,	
   agendas	
   
with	
   early	
   stopping,	
   ...	
   

       other	
   views	
   suggest	
   others.	
   

summary	
   

       decoding	
   is	
   the	
   general	
   problem	
   of	
   choosing	
   a	
   

complex	
   structure.	
   
      linguishc	
   analysis,	
   machine	
   translahon,	
   speech	
   
recognihon,	
      	
   
      stahshcal	
   models	
   are	
   usually	
   involved	
   (not	
   
necessarily	
   probabilishc).	
   

       no	
   perfect	
   general	
   view,	
   but	
   much	
   can	
   be	
   

gained	
   through	
   a	
   combinahon	
   of	
   views.	
   

