id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith	
   

	
   

heidelberg	
   university,	
   november	
   2014	
   

learning	
   

       assume	
   a	
   collec@on	
   of	
   n	
   pairs	
   (x,	
   y);	
   

supervised	
   learning	
   with	
   complete	
   data.	
   

loss	
   

       let	
   h	
   be	
   a	
   hypothesis	
   (an	
   instan@ated,	
   predic@ve	
   
       loss(x,	
   y;	
   h)	
   =	
   a	
   measure	
   of	
   how	
   badly	
   h	
   performs	
   

model).	
   

on	
   input	
   x	
   if	
   y	
   is	
   the	
   correct	
   output.	
   

       how	
   to	
   decide	
   what	
      loss   	
   should	
   be?	
   

1.    computa@onal	
   expense	
   
2.    knowledge	
   of	
   actual	
   costs	
   of	
   errors	
   
3.   

formal	
   founda@ons	
   enabling	
   theore@cal	
   guarantees	
   

risk	
   

       there	
   is	
   some	
   true	
   distribu@on	
   p*	
   over	
   input,	
   

output	
   pairs	
   (x,	
   y).	
   

       under	
   that	
   distribu@on,	
   what	
   do	
   we	
   expect	
   h   s	
   

loss	
   to	
   be?	
   

ep (x,y )[loss(x, y ; h)]

       we	
   don   t	
   have	
   p*,	
   but	
   we	
   have	
   the	
   empirical	
   

distribu@on,	
   giving	
   empirical	
   risk:	
   
1
n

e  p(x,y )[loss(x, y ; h)] =

loss(xi, yi; h)

n i=1

empirical	
   risk	
   minimiza@on	
   

       provides	
   a	
   criterion	
   to	
   decide	
   on	
   h:	
   

1
n

min
h h

n i=1

loss(xi, yi; h)

       background	
   preferences	
   over	
   h	
   can	
   be	
   
included	
   in	
   regularized	
   empirical	
   risk	
   
minimiza@on:	
   

1
n

min
h h

n i=1

loss(xi, yi; h) + r(h)

parametric	
   assump@ons	
   

       typically	
   we	
   do	
   not	
   move	
   in	
      h-     space,   	
   but	
   

rather	
   in	
   the	
   space	
   of	
   con@nuously-     
parameterized	
   predictors.	
   

1
n

1
n

min
h h

min
w rd

n i=1
n i=1

loss(xi, yi; h) + r(h)

loss(xi, yi; hw) + r(w)

three	
   kinds	
   of	
   loss	
   func@ons	
   

       error	
   

       log	
   loss	
   

       hinge	
   loss	
   

      could	
   be	
   zero-     one,	
   or	
   task-     speci   c.	
   
      mean	
   squared	
   error	
   makes	
   sense	
   for	
   con$nuous	
   
predic@ons	
   and	
   is	
   used	
   in	
   regression.	
   

      probabilis@c	
   interpreta@on	
   (   likelihood   )	
   

      geometric	
   interpreta@on	
   (   margin   )	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

       maximum	
   likelihood	
   es@ma@on:	
   	
   	
   

loss(x, y; hw) =   log pw(x, y)
r(w)	
   is	
   0	
   for	
   models	
   in	
   the	
   family,	
   +   	
   for	
   
other	
   models.	
   
       maximum	
   a	
   posteriori	
   (map)	
   es@ma@on:	
   
       oeen	
   called	
   genera6ve	
   modeling.	
   

r(w)	
   is	
      log	
   p(w)	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

examples:	
   
       n-     gram	
   language	
   models	
   
       supervised	
   id48	
   taggers	
   
       charniak,	
   collins,	
   and	
   stanford	
   parsers	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

computa@onally	
      	
   
       convex	
   and	
   di   eren@able.	
   
       closed	
   form	
   for	
   directed,	
   mul@nomial-     based	
   models	
   pw.	
   
       count	
   and	
   normalize!	
   
      
in	
   other	
   cases,	
   requires	
   posterior	
   id136,	
   which	
   can	
   be	
   
expensive	
   depending	
   on	
   the	
   model   s	
   structure.	
   
       linear	
   decoding	
   (for	
   some	
   parameteriza@ons).	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

error	
      	
   
       no	
   no@on	
   of	
   error.	
   
       learner	
   wins	
   by	
   moving	
   as	
   much	
   id203	
   

mass	
   as	
   possible	
   to	
   training	
   examples.	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)
guarantees   	
   
       consistency:	
   	
   if	
   the	
   true	
   model	
   is	
   in	
   the	
   right	
   

family,	
   enough	
   data	
   will	
   lead	
   you	
   to	
   it.	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

di   erent	
   parameteriza@ons	
      	
   
       mul@nomials	
   (bn-     like):	
   

       global	
   log-     linear	
   (mn-     like):	
   

       locally	
   normalized	
   log-     linear:	
   

freq(e; x, y) log pe

  e
            we
 w   g(x, y) + log x ,y 
freq(e; x, y)    w   g(e)   log    e    c(e)

    e

exp w   g(x , y )

exp w   g(e )      

re   ec@ons	
   on	
   genera@ve	
   models	
   
       most	
   early	
   solu@ons	
   are	
   genera@ve.	
   
       most	
   unsupervised	
   approaches	
   are	
   genera@ve.	
   
       some	
   people	
   only	
   believe	
   in	
   genera@ve	
   models.	
   
       some@mes	
   es@mators	
   are	
   not	
   as	
   easy	
   as	
   they	
   
       advice:	
   	
   start	
   here	
   if	
   there   s	
   a	
   sensible	
   genera@ve	
   
story.	
   
       you	
   can	
   always	
   use	
   a	
      bemer   	
   loss	
   func@on	
   with	
   the	
   

seem	
   (   de   ciency   ).	
   

same	
   linear	
   model	
   later	
   on.	
   

zero-     one	
   loss	
   

loss(xi, yi; hw) + r(w)

1
n

min
w rd

n i=1

loss(x, y; hw) = 1{hw(x)  = y}

zero-     one	
   loss	
   

loss(xi, yi; hw) + r(w)

1
n

min
w rd

n i=1

loss(x, y; hw) = 1{hw(x)  = y}

computa@onally:	
   	
   	
   
       piecewise	
   constant.	
   	
   l   	
   
error:	
   	
   j   	
   
guarantees:	
   	
   none	
   

error	
   as	
   loss	
   

loss(xi, yi; hw) + r(w)

1
n

min
w rd

n i=1

loss(x, y; hw) = error(hw(x); y)

generalizes	
   zero-     one,	
   same	
   di   cul@es.	
   
example:	
   	
   id7-     score	
   maximiza@on	
   in	
   machine	
   

transla@on,	
   with	
      mert   	
   line	
   search.	
   

comparison	
   

computa@on	
   

genera6ve	
   (log	
   loss)	
   
convex	
   op@miza@on.	
    op@mizing	
   a	
   

error	
   as	
   loss	
   

error-     awareness	
    none	
   
guarantees	
   

consistency.	
   

piecewise	
   constant	
   
func@on.	
   
j   	
   
none.	
   

discrimina@ve	
   learning	
   

       various	
   loss	
   func@ons	
   between	
   log	
   loss	
   and	
   

error.	
   

       three	
   commonly	
   used	
   in	
   nlp:	
   

      condi@onal	
   log	
   loss	
   (   max	
   ent,   	
   crfs)	
   
      hinge	
   loss	
   (structural	
   id166s)	
   
      id88   s	
   loss	
   

       we   ll	
   discuss	
   each,	
   compare,	
   and	
   unify.	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)
       can	
   be	
   understood	
   as	
   a	
   genera@ve	
   model	
   over	
   

y,	
   but	
   does	
   not	
   model	
   x.	
   
         condi@onal   	
   model	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

examples:	
   
       logis@c	
   regression	
   (for	
   classi   ca@on)	
   
       memms	
   
       crfs	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

computa@onally	
      	
   
       convex	
   and	
   di   eren@able.	
   
       requires	
   posterior	
   id136,	
   which	
   can	
   be	
   

expensive	
   depending	
   on	
   the	
   model   s	
   structure.	
   
       linear	
   decoding	
   (for	
   some	
   parameteriza@ons).	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

error	
      	
   
       no	
   no@on	
   of	
   error.	
   
       learner	
   wins	
   by	
   moving	
   as	
   much	
   id203	
   
mass	
   as	
   possible	
   to	
   training	
   examples   	
   correct	
   
outputs.	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)
guarantees   	
   
       consistency:	
   	
   if	
   the	
   true	
   condi@onal	
   model	
   is	
   
in	
   the	
   right	
   family,	
   enough	
   data	
   will	
   lead	
   you	
   
to	
   it.	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

di   erent	
   parameteriza@ons	
      	
   
       global	
   log-     linear	
   (crf):	
   

       locally	
   normalized	
   log-     linear	
   (memm):	
   

 w   g(x, y) + log y 
freq(e; x, y)    w   g(e)   log    e    c(e)

    e

exp w   g(x , y )

exp w   g(e )      

comparing	
   the	
   two	
   log	
   losses	
   

parameteriza@on	
   

-     log	
   pw(x,	
   y)	
   

usually	
   
mul@nomials	
   (bn-     
like).	
   

-     log	
   pw(y	
   |	
   x)	
   
almost	
   always	
   log-     
linear	
   (mn-     like).	
   

under	
   the	
   usual	
   parameteriza@on	
      	
   
computa@on	
   

count	
   and	
   
normalize.	
   
none.	
   

error-     awareness	
   

convex	
   
op@miza@on.	
   
aware	
   of	
   the	
   y-     
predic@on	
   task,	
   
(approximates	
   zero-     
one).	
   

guarantees	
   

consistency	
   of	
   joint.	
    consistency	
   of	
   cond.	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

w   g(x, y ) + error(y , y)

y 

loss(x, y; hw) =  w   g(x, y) + max
       penalizes	
   the	
   model	
   for	
   lerng	
   compe@tors	
   
get	
   close	
   (in	
   terms	
   of	
   score)	
   to	
   the	
   correct	
   
answer	
   y.	
   
      can	
   penalize	
   them	
   in	
   propor@on	
   to	
   their	
   error.	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =  w   g(x, y) + max
examples	
      	
   
       id88	
   (including	
   collins   	
   structured	
   version)	
   

w   g(x, y ) + error(y , y)

y 

       classic	
   version	
   ignores	
   error	
   term	
   

       id166	
   and	
   some	
   structured	
   variants:	
   

       max-     margin	
   markov	
   networks	
   (taskar	
   et	
   al.)	
   
       mira	
   (1-     best,	
   k-     best)	
   

	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

w   g(x, y ) + error(y , y)

y 

loss(x, y; hw) =  w   g(x, y) + max
computa@onally	
      	
   
       convex,	
   not	
   everywhere	
   di   eren@able.	
   
       many	
   specialized	
   techniques	
   now	
   available.	
   
       requires	
   map	
   or	
      cost-     augmented   	
   map	
   
       linear	
   decoding.	
   
	
   

id136.	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

y 

w   g(x, y ) + error(y , y)

loss(x, y; hw) =  w   g(x, y) + max
error	
      	
   
       built	
   in.	
   
       most	
   convenient	
   when	
   error	
   func@on	
   factors	
   

similarly	
   to	
   features	
   g.	
   

	
   

w   g(x, y ) + error(y , y)

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =  w   g(x, y) + max
guarantees	
      	
   
       generaliza@on	
   bounds.	
   

y 

      not	
   clear	
   how	
   seriously	
   to	
   take	
   these	
   in	
   nlp;	
   may	
   
not	
   be	
   @ght	
   enough	
   to	
   be	
   meaningful.	
   

       oeen	
   you	
   will	
      nd	
   convergence	
   guarantees	
   

(and	
   rates)	
   for	
   op@miza@on	
   techniques.	
   

	
   
	
   

they	
   are	
   all	
   related	
   
exp     w    (g(x, y )   g(x, y)) +    error(y , y)      

1
 

log   y 

condi@onal	
   log	
   loss	
   (la   erty	
   et	
   al.,	
   2001)	
   
id88   s	
   hinge	
   loss	
   (collins,	
   2002)	
   
structural	
   id166   s	
   hinge	
   loss	
   (taskar	
   et	
   al.,	
   2004)	
   
soemax-     margin	
   (gimpel	
   and	
   smith,	
   2010)	
   

  	
   
1	
   
   	
   
   	
   
1	
   

  	
   
0	
   
0	
   
>	
   0	
   
1	
   

crfs,	
   max	
   margin,	
   or	
   id88?	
   
       for	
   supervised	
   problems,	
   we	
   do	
   not	
   expect	
   
       id88	
   is	
   easiest	
   to	
   implement.	
   

large	
   di   erences.	
   

      with	
   cost-     augmented	
   id136,	
   it	
   should	
   get	
   
bemer	
   and	
   begins	
   to	
   approach	
   mira	
   and	
   m3ns.	
   

       crfs	
   are	
   best	
   for	
   id203	
   fe@shists.	
   

      probably	
   most	
   appropriate	
   if	
   you	
   are	
   extending	
   
with	
   latent	
   variables;	
   the	
   jury	
   is	
   out.	
   

       not	
   yet	
      plug	
   and	
   play.   	
   

r(w)	
   

       regulariza@on	
   term	
      	
   avoid	
   over   rng	
   

      usually	
   means	
      avoid	
   large	
   magnitudes	
   in	
   w   	
   
	
   

       (log)	
   prior	
      	
   respect	
   background	
   beliefs	
   about	
   

the	
   predictor	
   hw	
   

r(w)	
   

       usual	
   star@ng	
   point:	
   	
   squared	
   l2	
   norm	
   

      computa@onally	
   convenient	
   (it   s	
   strongly	
   convex,	
   
it	
   is	
   its	
   own	
   fenchel	
   conjugate,	
      )	
   
      probabilis@c	
   view:	
   	
   gaussian	
   prior	
   on	
   weights	
   
(chen	
   and	
   rosenfeld,	
   2000)	
   
      geometric	
   view:	
   	
   euclidean	
   distance	
   	
   
(original	
   regulariza@on	
   method	
   in	
   id166s)	
   
      only	
   one	
   hyperparameter	
   
r(w) =   w 2

w2
j

2 =   j

r(w)	
   
       another	
   op@on:	
   	
   l1-     norm	
   

      computa@onally	
   less	
   convenient	
   (not	
   everywhere	
   
di   eren@able)	
   
      probabilis@c	
   view:	
   	
   laplacian	
   prior	
   on	
   weights	
   
(originally	
   proposed	
   as	
      lasso   	
   in	
   regression)	
   
      sparsity	
   inducing	
   (   free   	
   feature	
   selec@on)	
   

r(w) =   w 1 =   j

|wj|

r(w)	
   

       lots	
   of	
   amen@on	
   to	
   this	
   in	
   machine	
   learning.	
   
          structured	
   sparsity   	
   

      want	
   groups	
   of	
   features	
   to	
   go	
   to	
   zero,	
   or	
   group-     
internal	
   sparsity,	
   or	
      	
   

       interpola@on	
   between	
   l1	
   and	
   l2	
      	
      elas@c	
   net   	
   
       this	
   is	
   not	
   yet	
      plug	
   and	
   play.   	
   

      sparsity	
   but	
   maybe	
   bemer	
   behaved	
   

      op@miza@on	
   algorithm	
   is	
   heavily	
   a   ected.	
   	
   

map	
   learning	
   is	
   id136	
   

       seeking	
      most	
   probable	
   explana@on   	
   of	
   the	
   

data,	
   in	
   terms	
   of	
   w.	
   
      explain	
   the	
   data:	
   	
   p(x,	
   y	
   |	
   w)	
   
      not	
   too	
   surprising:	
   	
   p(w)	
   

       if	
   we	
   think	
   of	
      w   	
   as	
   another	
   random	
   

variable,	
   map	
   learning	
   is	
   map	
   id136.	
   
      looks	
   very	
   di   erent	
   from	
   decoding!	
   
      but	
   at	
   a	
   high	
   level	
   of	
   abstrac@on,	
   it	
   is	
   the	
   same.	
   

map	
   learning	
   as	
   a	
   graphical	
   model	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(y)	
   

r	
   

w	
   

y	
   

pw(x	
   |	
   y)	
   

x	
   

       this	
   is	
   a	
   view	
   of	
   learning	
   a	
      noisy	
   channel   	
   

model.	
   

map	
   learning	
   as	
   a	
   graphical	
   model	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(y	
   |	
   x)	
   

r	
   

w	
   

y	
   

x	
   

       this	
   is	
   a	
   view	
   of	
   learning	
   in	
   a	
   crf.	
   

map	
   es@ma@on	
   for	
   crfs	
   

maxw	
   p(w	
   |	
   x,	
   y),	
   which	
   is	
   map	
   id136	
   

iterate	
   to	
   obtain	
   gradient:	
   
	
   

su   cient	
   sta@s@cs	
   from	
   p(y	
   |	
   x,	
   w),	
   obtained	
   by	
   
posterior	
   id136	
   

how	
   to	
   think	
   about	
   op@miza@on	
   
       depending	
   on	
   your	
   choice	
   of	
   loss	
   and	
   r,	
   
di   erent	
   approaches	
   become	
   available.	
   
      learning	
   algorithms	
   can	
   interact	
   with	
   id136/
decoding	
   algorithms,	
   too.	
   

       in	
   nlp	
   today,	
   it	
   is	
   probably	
   more	
   important	
   to	
   

focus	
   on	
   the	
   features,	
   error	
   func@on,	
   and	
   
prior	
   knowledge.	
   
      decide	
   what	
   you	
   want,	
   and	
   then	
   use	
   the	
   best	
   
available	
   op@miza@on	
   technique.	
   

key	
   techniques	
   

       quasi-     newton	
      	
   batch	
   method	
   for	
   di   eren@able	
   

ascent	
   

       stochas@c	
   subgradient	
   ascent	
      	
   online	
   

loss	
   func@ons	
   
       lbfgs,	
   owlqn	
   when	
   using	
   l1	
   regulariza@on	
   	
   
       generalizes	
   id88,	
   mira,	
   stochas@c	
   gradient	
   
       some@mes	
   sensi@ve	
   to	
   step	
   size	
   
       can	
   oeen	
   use	
      mini-     batches   	
   to	
   speed	
   up	
   
       recent	
   trick:	
   	
   adagrad,	
   which	
   adapts	
   the	
   step	
   size	
   

convergence	
   

       for	
   error	
   minimiza@on:	
   	
   randomiza@on	
   

pixalls	
   

       engineering	
   learning	
   procedures	
   is	
   temp@ng	
   and	
   
may	
   help	
   you	
   get	
   bemer	
   performance.	
   
       without	
   at	
   least	
   some	
   analysis	
   in	
   terms	
   of	
   loss,	
   error,	
   

and	
   regulariza@on,	
   it   s	
   unlikely	
   to	
   be	
   useful	
   outside	
   
your	
   problem/dataset.	
   

       when	
   randomiza@on	
   is	
   involved,	
   look	
   at	
   variance	
   
       always	
   tune	
   hyperparameters	
   (e.g.,	
   

across	
   runs	
   (clark	
   et	
   al.,	
   2011)	
   

regulariza@on	
   strength)	
   on	
   development	
   data!	
   

major	
   topics	
   in	
   current	
   work	
   

       coping	
   with	
   approximate	
   id136	
   
       exploi@ng	
   incomplete	
   data	
   

      semisupervised	
   learning	
   
      crea@ng	
   features	
   from	
   raw	
   text	
   
      latent	
   variable	
   models	
   (discussed	
   tomorrow)	
   

       feature	
   management	
   
      structured	
   sparsity	
   (r)	
   

