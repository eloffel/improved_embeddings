id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith	
   

	
   

heidelberg	
   university,	
   november	
   2014	
   

random	
   variables	
   in	
   decoding	
   

parameters	
   (w)	
   

inputs	
   (x)	
   

outputs	
   (y)	
   

random	
   variables	
   in	
   	
   
supervised	
   learning	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

hidden	
   variables	
   are	
   di   erent	
   

       we	
   use	
   the	
   term	
   "hidden	
   variable"	
   (or	
   "latent	
   
variable")	
   to	
   refer	
   to	
   something	
   we	
   never	
   see.	
   
      not	
   even	
   in	
   training.	
   
      somekmes	
   we	
   believe	
   they	
   are	
   real.	
   
      somekmes	
   we	
   believe	
   they	
   only	
   approximate	
   
reality.	
   

random	
   variables	
   in	
   decoding	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

latent	
   (z)	
   

random	
   variables	
   in	
   	
   
supervised	
   learning	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

latent	
   (z)	
   

latent	
   variables	
   and	
   id136	
   
       both	
   learning	
   and	
   decoding	
   can	
   be	
   skll	
   be	
   

understood	
   as	
   id136	
   problems.	
   

       usually	
   "mixed":	
   

      some	
   variables	
   are	
   gepng	
   maximized	
   
      some	
   variables	
   are	
   gepng	
   summed	
   	
   

word	
   alignments	
   

prototypical	
   hidden	
   variable.	
   

       since	
   ibm	
   model	
   1,	
   word	
   alignments	
   have	
   been	
   the	
   
       ulkmately,	
   in	
   translakon,	
   we	
   do	
   not	
   care	
   what	
   they	
   
       current	
   approach:	
   	
   learn	
   the	
   word	
   alignments	
   

are.	
   

unsupervised,	
   then	
      x	
   them	
   to	
   their	
   most	
   likely	
   values.	
   
       then	
   construct	
   models	
   for	
   translakon.	
   
       alignment	
   on	
   its	
   own:	
   	
   unsupervised	
   problem.	
   
       mt	
   on	
   its	
   own:	
   	
   supervised	
   problem.	
   
       mt	
   +	
   alignment:	
   	
   supervised	
   problem	
   with	
   latent	
   

variables.	
   

alignments	
   in	
   text-     to-     text	
   problems	
   
       wang	
   et	
   al.	
   (2007):	
   	
   "jeopardy"	
   model	
   for	
   

answer	
   ranking	
   in	
   qa.	
   
      align	
   queskons	
   to	
   answers.	
   
      similar	
   model	
   for	
   paraphrase	
   deteckon	
   (das	
   and	
   
smith,	
   2009)	
   

latent	
   annotakons	
   in	
   parsing	
   

       treebank	
   categories	
   (n,	
   nn,	
   np,	
   etc.)	
   are	
   too	
   

coarse-     grained.	
   
      lexicalizakon	
   (collins,	
   eisner)	
   
      johnson   s	
   (1998)	
   parent	
   annotakon	
   
      klein	
   and	
   manning	
   (2003)	
   parser	
   

       treat	
   the	
   true,	
      ne-     grained	
   category	
   as	
   

hidden,	
   and	
   infer	
   it	
   from	
   data.	
   
      matsuzaki,	
   petrov,	
   dreyer,	
   many	
   others.	
   

richer	
   formalisms	
   

       cohn	
   et	
   al.	
   (2009):	
   	
   tree	
   subsktukon	
   grammar.	
   

      derived	
   tree	
   is	
   observed	
   (output	
   variable).	
   
      derivakon	
   tree	
   (segmentakon	
   into	
   elementary	
   
trees)	
   is	
   hidden.	
   

       zeelemoyer	
   and	
   collins	
   (2005	
   and	
   later):	
   	
   infer	
   
id35	
   syntax	
   from	
      rst-     order	
   logical	
   expressions	
   
and	
   sentences.	
   

       liang	
   et	
   al.	
   (2011):	
   	
   infer	
   semankc	
   

representakon	
   from	
   text	
   and	
   database.	
   

topic	
   models	
   

       infer	
   topics	
   (or	
   topic	
   blends)	
   in	
   documents.	
   
       latent	
   dirichlet	
   allocakon	
   (blei	
   et	
   al.,	
   2003)	
   is	
   

a	
   great	
   example.	
   
      somekmes	
   augmented	
   with	
   an	
   output	
   variable	
   
(blei	
   and	
   mcauli   e,	
   2007)	
      	
   "supervised"	
   lda.	
   
      many	
   extensions!	
   

unsupervised	
   nlp	
   

       id91	
   (brown,	
   1992,	
   many	
   more)	
   
       pos	
   tagging	
   (merialdo,	
   1994,	
   many	
   more)	
   
       parsing	
   (pereira	
   and	
   schabes,	
   klein	
   and	
   manning,	
      )	
   
       segmentakon	
   (word	
      	
   goldwater;	
   discourse	
      	
   
eisenstein)	
   
       morphology	
   
       lexical	
   semankcs	
   
       syntax-     semankcs	
   correspondences	
   
       senkment	
   analysis	
   
       coreference	
   resolukon	
   
       word,	
   phrase,	
   and	
   tree	
   alignment	
   
	
   

supervised	
   or	
   unsupervised?	
   

       depends	
   on	
   the	
   task,	
   not	
   the	
   model.	
   

      i	
   say	
   "unsupervised"	
   when	
   the	
   output	
   variables	
   
are	
   hidden	
   at	
   training	
   kme.	
   

random	
   variables	
   	
   

in	
   unsupervised	
   learning	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

latent	
   (z)	
   

probabiliskc	
   view	
   

       the	
   usual	
   starkng	
   point	
   for	
   hidden	
   variables	
   is	
   

maximum	
   likelihood.	
   
      "input"	
   and	
   "output"	
   do	
   not	
   maeer;	
   only	
   
observed/latent.	
   

random	
   variables	
   	
   

in	
   general	
   probabiliskc	
   modeling	
   

parameters	
   (w)	
   

visible	
   (v)	
   

latent	
   (l)	
   

empirical	
   risk	
   view	
   

min
w rd

1

n  i

loss(vi; hw) + r(w)

loss(v; hw) =   log pw(v)
=   log  

pw(v,  )

       log-     loss	
   

is	
   a	
   negated	
   log	
   prior).	
   

       equates	
   to	
   maximum	
   marginal	
   likelihood	
   (or	
   map	
   if	
   r(w)	
   
       unlike	
   loss	
   funckons	
   in	
   lecture	
   3,	
   this	
   is	
   not	
   convex!	
   
       em	
   seeks	
   to	
   solve	
   this	
   problem	
   (but	
   it   s	
   not	
   the	
   only	
   way).	
   
       regularizakon	
   decisions	
   are	
   orthogonal.	
   

opkmizing	
   the	
   marginal	
   log-     loss	
   
       em	
   as	
   id136	
   
       em	
   as	
   opkmizakon	
   
       direct	
   opkmizakon	
   

generic	
   em	
   algorithm	
   
       input:	
   	
   w(0)	
   and	
   observakons	
   v1,	
   v2,	
      	
   vn	
   
       output:	
   	
   learned	
   w	
   
       t	
   =	
   0	
   
       repeat	
   unkl	
   w(t)	
      	
   w(t-     1):	
   

       e	
   step:	
   
       m	
   step:	
   
       ++t	
   	
   	
   

       return	
   w(t)	
   

   i,    ,

q(t)
i ( )   pw(t)(  | vi)
w  i   
q(t)
i ( ) log pw(vi,  )

w(t+1)   max

map	
   learning	
   as	
   a	
   graphical	
   model	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

       combined	
   id136	
   (max	
   over	
   w,	
   sum	
   over	
   l)	
   is	
   very	
   
hard.	
   
       if	
   w	
   were	
      xed,	
   gepng	
   the	
   posterior	
   over	
   l	
   wouldn   t	
   be	
   so	
   
       if	
   l	
   were	
      xed,	
   maximizing	
   over	
   w	
   wouldn   t	
   be	
   so	
   bad.	
   

bad.	
   

map	
   learning	
   as	
   a	
   graphical	
   model	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

pw(v	
   |	
   l)	
   

v	
   

m	
   step	
   

e	
   step	
   
	
   
	
   

baum-     welch	
   (em	
   for	
   id48s)	
   

as	
   an	
   example	
   

       e	
   step:	
   	
   forward-     backward	
   algorithm	
   (on	
   each	
   

eliminakon.	
   

example).	
   
       this	
   is	
   exact	
   marginal	
   id136	
   by	
   variable	
   
       the	
   structure	
   of	
   the	
   graphical	
   model	
   lets	
   us	
   do	
   this	
   by	
   
       the	
   marginals	
   are	
   probabilikes	
   of	
   transikon	
   and	
   

dynamic	
   programming.	
   

emission	
   events	
   at	
   each	
   posikon.	
   

       m	
   step:	
   	
   id113	
   based	
   on	
   sos	
   event	
   counts.	
   

       relakve	
   frequency	
   eskmakon	
   accomplishes	
   id113	
   for	
   

mulknomials.	
   

baum-     welch	
   as	
   a	
   graphical	
   model	
   

parameters	
   (w)	
   

r	
   

emit	
   

transit	
   

x1	
   

x2	
   

x3	
   

visible	
   (v)	
   

xn	
   

y1	
   

y2	
   

y3	
   

yn	
   

latent	
   (l)	
   

ackve	
   trail!	
   

parameters	
   (w)	
   

r	
   

emit	
   

transit	
   

x1	
   

x2	
   

x3	
   

visible	
   (v)	
   

xn	
   

y1	
   

y2	
   

y3	
   

yn	
   

latent	
   (l)	
   

no	
   ackve	
   trail	
   in	
   all-     visible	
   case	
   

parameters	
   (w)	
   

r	
   

emit	
   

transit	
   

x1	
   

x2	
   

x3	
   

visible	
   (v)	
   

xn	
   

y1	
   

y2	
   

y3	
   

yn	
   

visible	
   (v)	
   

why	
   latent	
   variables	
   	
   
make	
   learning	
   hard	
   
       new	
   intuikon:	
   	
   parameters	
   are	
   now	
   

interdependent	
   that	
   were	
   not	
   interdependent	
   
in	
   the	
   fully-     visible	
   case.	
   

       it	
   all	
   goes	
   back	
   to	
   the	
   structural	
   properkes	
   of	
   
the	
   graphical	
   model	
   (in	
   this	
   case,	
   ackve	
   trails).	
   

"viterbi"	
   learning	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

       approximate	
   joint	
   map	
   id136	
   over	
   w	
   and	
   

l	
   (most	
   probable	
   explanakon	
   id136).	
   
log pw(v,  )

       loss	
   funckon:	
   

loss(v; hw) =   max

 

condikonal	
   models	
   

       em	
   is	
   usually	
   closely	
   associated	
   with	
   fully	
   

generakve	
   approaches.	
   

       it	
   generalizes	
   to	
   log-     linear	
   models	
   and	
   with	
   

condikonal	
   models.	
   
      locally	
   normalized	
   models	
   give	
      exibility	
   without	
   
requiring	
   global	
   id136	
   (eisner,	
   2002).	
   
      hidden	
   variable	
   crfs	
   (quaeoni	
   et	
   al.,	
   2007)	
   are	
   
very	
   powerful.	
   

	
   

learning	
   condikonal	
   	
   
hidden	
   variable	
   models	
   

vin	
   

distribukon	
   over	
   vin	
   is	
   not	
   modeled	
   

vin	
   

r	
   

w	
   

l	
   

r	
   

w	
   

vout	
   

vout	
   

standard	
   condikonal	
   model	
   (e.g.,	
   crf)	
   

opkmizakon	
   for	
   hidden	
   variables	
   
       we   ve	
   described	
   hidden	
   variable	
   learning	
   as	
   

id136	
   problems.	
   

       it	
   is	
   more	
   prackcal	
   to	
   think	
   about	
   this	
   as	
   

op.miza.on.	
   

       em	
   can	
   be	
   understood	
   from	
   an	
   opkmizakon	
   

framework,	
   as	
   well.	
   

em	
   and	
   likelihood	
   

 (w) =  i

log  

pw(vi,  )

       the	
   conneckon	
   between	
   the	
   goal	
   above	
   and	
   
the	
   em	
   procedure	
   is	
   not	
   immediately	
   clear.	
   

	
   

opkmizakon	
   view	
   of	
   em	
   

qi( ) log qi( ) +    

qi( ) log pw(  | vi) + log pw(vi)   

   i       
       a	
   funckon	
   of	
   w	
   and	
   the	
   colleckon	
   of	
   qi.	
   
       claim:	
   	
   em	
   performs	
   coordinate	
   ascent	
   on	
   this	
   

funckon.	
   

opkmizakon	
   view	
   of	
   em	
   

   i       

qi( ) log qi( ) +    

 (w)

qi( ) log pw(  | vi) + log pw(vi)   

       the	
   third	
   term	
   is	
   our	
   actual	
   goal,	
     .	
   	
   it	
   only	
   

depends	
   on	
   w	
   (not	
   the	
   qi).	
   	
   

   i       

opkmizakon	
   view	
   of	
   em	
   

 (w)

qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       the	
   laeer	
   two	
   terms	
   together	
   are	
   precisely	
   what	
   
we	
   maximize	
   on	
   the	
   m	
   step,	
   given	
   the	
   current	
   qi.	
   
       this	
   is	
   a	
   concave	
   problem	
   and	
   we	
   solve	
   it	
   exactly.	
   

   i       

opkmizakon	
   view	
   of	
   em	
   

 (w)

qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       concern:	
   	
   is	
   the	
   m	
   step	
   improving	
   term	
   2	
   at	
   

the	
   expense	
   of	
     	
   (term	
   3)?	
   
      no.	
   

q(t)
i ( ) log pw(  | vi)

q(t)

the	
   m	
   step	
   
i ( ) log pw(vi,  )   i   

iterakon	
   to	
   iterakon:	
   

 (w) =  i   
       last	
   term	
   is	
   also	
   not	
   gepng	
   any	
   worse	
   from	
   
  i   
=   i   
=  i
    0

i ( ) log pw(t+1)(  | vi) + i   
i (  )   pw(t+1)(   | vi))

i ( ) log pw(t+1)(  | vi) + i   

d(q(t)

q(t)

q(t)

q(t)
i ( ) log pw(t)(  | vi)
q(t)
i ( ) log q(t)
i ( )

the	
   m	
   step	
   

       each	
   m	
   step,	
   once	
   qi	
   is	
      xed,	
   maximizes	
   a	
   
bound	
   on	
   the	
   log-     likelihood	
     .	
   
      for	
      xed	
   qi,	
   this	
   is	
   a	
   concave	
   problem	
   we	
   can	
   
solve	
   in	
   closed	
   form	
   in	
   many	
   cases.	
   

       what	
   about	
   the	
   e	
   step?	
   

opkmizakon	
   view	
   of	
   em	
   

 (w)

 d(qi(  )   pw(   | vi))
   i       
qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       e	
   step	
   considers	
   the	
      rst	
   two	
   terms.	
   
       sets	
   each	
   qi	
   to	
   be	
   equal	
   to	
   the	
   posterior	
   under	
   
the	
   current	
   model.	
   	
   

coordinate	
   ascent	
   

 d(qi(  )   pw(   | vi))
   i       
qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       e	
   step	
      xes	
   w	
   and	
   solves	
   for	
   the	
   qi.	
   
       m	
   step	
      xes	
   all	
   qi	
   and	
   solves	
   for	
   w.	
   

things	
   people	
   forget	
   about	
   em	
   

       mulkple	
   random	
   starts	
   (or	
   non-     random	
   

starts),	
   select	
      nal	
   model	
   using	
   likelihood	
   on	
   
development	
   data.	
   

       variants	
   may	
   help	
   avoid	
   local	
   opkma	
      	
   
	
   

variants	
   of	
   em	
   

       "online"	
   variants	
   where	
   we	
   do	
   an	
   e	
   step	
   on	
   

one	
   or	
   a	
   mini-     batch	
   of	
   examples	
   are	
   skll	
   
coordinate	
   ascent	
   (neal	
   and	
   hinton,	
   1998).	
   
       determiniskc	
   annealing:	
   	
      aeen	
   out	
   the	
   qi,	
   
making	
   the	
   funckon	
   closer	
   to	
   concave.	
   
       stochaskc	
   variant:	
   	
   use	
   randomized	
   
approximate	
   id136	
   for	
   e	
   step.	
   
       "generalized"	
   em:	
   	
   improve	
   w	
   but	
   don   t	
   

bother	
   opkmizing	
   completely.	
   

direct	
   opkmizakon	
   

       an	
   alternakve	
   to	
   em:	
   	
   apply	
   stochaskc	
   

gradient	
   ascent	
   or	
   quasi-     newton	
   methods	
   
directly	
   to	
     .	
   

       typically	
   done	
   for	
   mn-     like	
   models	
   with	
   

features,	
   e.g.,	
   latent-     variable	
   crfs.	
   
      gradient	
   is	
   a	
   di   erence	
   of	
   feature	
   expectakons.	
   
      requires	
   marginal	
   id136.	
   

summary	
   

       em:	
   	
   many	
   ways	
   to	
   understand	
   it.	
   	
   

       the	
   guarantee:	
   	
   each	
   round	
   will	
   improve	
   the	
   
       that   s	
   about	
   as	
   much	
   as	
   we	
   can	
   say.	
   

likelihood.	
   

       somekmes	
   it	
   works.	
   

       smart	
   inikalizers	
   
       lots	
   of	
   bias	
   inherent	
   in	
   the	
   model	
   structure/

assumpkons	
   

       beeer	
   to	
   understand	
   the	
   more	
   general	
   problem	
   

of	
   opkmizing	
   a	
   model	
   given	
   data.	
   

bayesian	
   modeling	
   

in	
   stakskcs	
      	
   

       beeer	
   to	
   assign	
   f.	
   and	
   b.	
   to	
   analyses,	
   not	
   people.	
   
       frequen/st	
   analysis	
   (most	
   of	
   science	
   today):	
   	
   parameters	
   
are	
      xed	
   and	
   unknown;	
   we	
   gain	
   informakon	
   by	
   repeated	
   
experiments	
   
       point	
   eskmates,	
   standard	
   errors,	
   con   dence	
   intervals	
   (   in	
   p%	
   of	
   

experiments,	
   the	
   interval	
   will	
   cover	
   the	
   true	
        ),	
   hypothesis	
   
tests	
   with	
     	
      xed	
   in	
   advance,	
   reason	
   about	
   p(data	
   |	
   h0)	
   

       bayesian	
   analysis:	
   	
   treat	
   unknown	
   parameters	
   

probabiliskcally;	
   update	
   beliefs	
   as	
   evidence	
   arrives	
   
       start	
   with	
   p(  )	
   and	
   infer	
   p(  	
   |	
   data),	
   means	
   and	
   quankles	
   of	
   the	
   
posterior	
   over	
     ,	
   intervals	
   corresponding	
   to	
      p%	
   belief   	
   that	
     	
   is	
   
in	
   the	
   interval	
   

the	
   aerackon	
   of	
   bayesian	
   thinking	
   
       write	
   down	
   your	
   model	
   declarakvely,	
   worry	
   later	
   
about	
   how	
   to	
      t	
   it	
   from	
   data.	
   
       prior	
   encodes	
   prior	
   knowledge.	
   

least	
   we	
   think	
   we	
   do!	
   

       we	
   have	
   lots	
   of	
   this	
   when	
   it	
   comes	
   to	
   language,	
   or	
   at	
   
       manage	
   uncertainty	
   about	
   the	
   model	
   the	
   same	
   
       bayesian	
   methods	
   are	
   strongly	
   associated	
   with:	
   

way	
   we	
   manage	
   uncertainty	
   about	
   the	
   data.	
   

       unsupervised	
   (and	
   latent	
   variable)	
   learning	
   
       generakve	
   models	
   

evolving	
   de   nikons	
   

       id113	
   (not	
   bayesian):	
   
       maximum	
   a	
   posteriori	
   eskmakon:	
   

       compukng	
   the	
   posterior	
   over	
   the	
   parameters	
   

(fully	
   bayesian):	
   

       empirical	
   bayesian:	
   

map	
   learning	
   as	
   a	
   graphical	
   model	
   

p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

       combined	
   id136	
   (max	
   over	
   w,	
   sum	
   over	
   l)	
   is	
   very	
   hard.	
   
       if	
   w	
   were	
      xed,	
   gepng	
   the	
   posterior	
   over	
   l	
   wouldn   t	
   be	
   so	
   bad.	
   
       if	
   l	
   were	
      xed,	
   maximizing	
   over	
   w	
   wouldn   t	
   be	
   so	
   bad.	
   

          standard	
   em    doesn   t	
   have	
   p(w);	
   it   s	
   very	
   simple	
   to	
   add	
   

and	
   useful	
   in	
   prackce.	
   

id113	
   

w	
   

l	
   

r	
   

w	
   

l	
   

map	
   

v	
   

v	
   

r	
   

w	
   

l	
   

fully	
   
bayesian	
   

v	
   

r	
   

w	
   

l	
   

empirical	
   bayesian	
   

v	
   

mulknomials	
   

       let   s	
   assume	
   discrete	
   distribukons	
   that	
   simply	
   

assign	
   probabilikes	
   to	
      nite	
   sets	
   of	
   events.	
   
      n-     gram	
   models,	
   id48s,	
   pid18s,	
      	
   

distribukons	
   over	
   mulknomials	
   

       you	
   can	
   think	
   of	
   a	
   mulknomial	
   distribukon	
   over	
   d	
   

events	
   as	
   a	
   point	
   in	
   the	
   (d-     1)	
   simplex.	
   

[1,	
   0,	
   0,	
   0]	
   

[0,	
   0,	
   0,	
   1]	
   

[0,	
   1,	
   0,	
   0]	
   

[0,	
   0,	
   1,	
   0]	
   

       to	
   randomly	
   pick	
   a	
   point	
   in	
   this	
   space,	
   we	
   need	
   a	
   

con/nuous	
   distribukon	
   over	
   the	
   simplex.	
   

dirichlet	
   distribukon	
   

       a	
   distribukon	
   over	
   the	
   d-     
event	
   id203	
   simplex.	
   

       parameters:	
   	
     ,	
   the	
   mean	
   of	
   
the	
   dirichlet,	
   and	
     ,	
   the	
   
concentrakon	
   around	
   that	
   
mean	
   (large	
     	
   means	
   
smaller	
   variance).	
   

       beta	
   funckon:	
   
       gamma	
   funckon	
   

(generalized	
   factorial):	
   

p(  |  ,    ) =

1

b(    )

       i 1
i

d i=1

 (    i)

 ( )

b(    ) =

d i=1
 (a) =        

0

ta 1e adt

dirichlet,	
   d=3	
   

(various	
   parameter	
   sepngs)	
   

from	
   answers.com	
   

dirichlet,	
   d=	
   3	
   

(di   erent	
      means   	
   and	
      variances   )	
   

       from	
   liang	
   and	
   klein,	
   2007	
   

sampling	
   from	
   a	
   dirichlet	
   

       for	
   i	
   from	
   1	
   to	
   do,	
   sample	
   vi	
   from	
   a	
   gamma	
   
distribukon	
   with	
   shape	
       i	
   and	
   scale	
   1.	
   
       renormalize	
   the	
   vector	
   v	
   to	
   obtain	
     .	
   	
   	
   

map	
   with	
   a	
   dirichlet	
   

       recall	
   that	
   we	
   can	
   use	
   a	
   prior	
   to	
      smooth   	
   an	
   eskmate.	
   
       for	
   a	
   mulknomial	
     	
   with	
   dirichlet	
   prior	
       	
   >	
   1,	
   this	
   equates	
   
to	
   adding	
   pseudocounts	
   to	
   the	
   vector	
   of	
   observed	
   counts.	
   
     i = ni +     i   1
n +     d
d i=1

       as	
   counts	
   become	
   large,	
   prior	
   maeers	
   less.	
   
       closed	
   form!	
   
       regularizer	
   view:	
   

       flat	
   prior:	
   	
     	
   =	
   d,	
   all	
     i	
   =	
   1/d	
   (equates	
   to	
   id113)	
   
       sparse	
   prior	
   (encourages	
   most	
     i	
   to	
   go	
   to	
   zero),	
   but	
   now	
   

(    i   1) log    i

r( ) =  

it   s	
   not	
   closed	
   form.	
   

mixture	
   of	
   unigrams	
   

       the	
   generakve	
   story	
   for	
   a	
   classical	
   document-     
id91	
   model	
   would	
   be	
   something	
   like	
   this	
   
(nigam	
   et	
   al.,	
   2000):	
   

       for	
   i	
   =	
   1	
   ...	
   m	
   (number	
   of	
   documents):	
   

       draw	
   a	
   document	
   length	
   ni	
   from	
   some	
   distribukon.	
   
       draw	
   a	
   topic	
   zi	
   for	
   the	
   document	
   from	
   a	
   mulknomial	
   over	
   
topics,	
     .	
   
       for	
   j	
   =	
   1	
   ...	
   ni:	
   

       draw	
   word	
   wij	
   from	
   the	
   mulknomial	
     z.	
   

       nigam	
   et	
   al.	
   learned	
   this	
   using	
   em.	
   

mixture	
   of	
   unigrams	
   

  	
   

mixture	
   
coe   cients	
   

zi	
   

wi,j	
   

  z	
   

words	
   

ni	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

zi	
   is	
   somekmes	
   called	
   the	
   topic	
   of	
   
document	
   i.	
   
a	
   topic	
   z	
   is	
   de   ned	
   by	
   a	
   unigram	
   
distribukon	
     z.	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

a	
   word	
   id91	
   model	
   

  	
   

mixture	
   
coe   cients	
   

zi,j	
   

wi,j	
   

  z	
   

words	
   

ni	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

problem:	
   	
   all	
   words	
   are	
   the	
   same;	
   
document	
   informakon	
   is	
   irrelevant.	
   
(this	
   is	
   exactly	
   a	
   zero-     order	
   id48.)	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

probabiliskc	
   lsi	
   (hofmann,	
   1996)	
   

this	
   has	
   very	
   liele	
   
to	
   do	
   with	
   latent	
   
seman/c	
   indexing,	
   
except	
   that	
   it   s	
   a	
   
probabiliskc	
   model	
   
trying	
   to	
   perform	
   a	
   
similar	
   task.	
   

  i	
   

zi,j	
   

wi,j	
   

words	
   

ni	
   

  z	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

word	
   id91;	
   documents	
   correspond	
   
to	
   distribu.ons	
   over	
   topics.	
   
problem:	
   	
   can   t	
   describe	
   new	
   documents!	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

latent	
   dirichlet	
   allocakon	
   	
   

(blei	
   et	
   al.,	
   2003)	
   

  ,	
     	
   

  i	
   

zi,j	
   

wi,j	
   

words	
   

ni	
   

  z	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

documents	
   are	
   mixtures	
   of	
   topics,	
   but	
   a	
   
prior	
   over	
   those	
   mixtures	
   lets	
   us	
   reason	
   
about	
   new	
   documents,	
   too.	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

lda	
   on	
   acl	
   papers	
   (gimpel,	
   2006)	
   

smoothed	
   latent	
   dirichlet	
   allocakon	
   	
   

(blei	
   et	
   al.,	
   2003)	
   

  ,	
     	
   

  i	
   

zi,j	
   

wi,j	
   

words	
   

ni	
   

     ,	
   
     	
   

  z	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

topic	
   models	
   beyond	
   lda	
   

       small	
   industry	
   in	
   variakons	
   on	
   topic	
   models,	
   usually	
   adding	
   
       examples:	
   	
   	
   

more	
   evidence	
   to	
   be	
   explained	
   by	
   the	
   topics.	
   

document	
   category.	
   

explained	
   by	
   more	
   draws	
   from	
     i	
   

       supervised	
   lda	
   (blei	
   and	
   mcauli   e,	
   2007)	
   adds	
   an	
   observed	
   
       link	
   lda	
   (erosheva	
   et	
   al.,	
   2004)	
   adds	
   citakons	
   to	
   the	
   document,	
   
       author	
   topic	
   model	
   (rosen-     zvi	
   et	
   al.,	
   2004)	
   adds	
   authors.	
   
       correlated	
   topic	
   model	
   (blei	
   and	
   la   erty,	
   2006)	
   lets	
   di   erent	
   
       comment	
   lda	
   (yano	
   et	
   al.,	
   2009)	
   generates	
   comments	
   from	
   a	
   

topics	
   correlate	
   more	
      exibly	
   through	
   a	
   di   erent	
   prior.	
   

di   erent	
   set	
   of	
   unigram	
   models	
   but	
   the	
   same	
   topics.	
   

where	
   is	
   the	
   structure?	
   
       through	
   the	
   topics	
   and	
     i,	
   words	
   in	
   a	
   
document	
   become	
   interdependent.	
   
      kind	
   of	
   a	
   joint	
   document/word	
   id91.	
   

       not	
   really	
   discrete	
   structure	
   the	
   way	
   we   ve	
   

mostly	
   discussed	
   in	
   this	
   class,	
   though.	
   

       lda	
   is	
   a	
      bayesian	
   zero-     order	
   id48.   	
   

where	
   is	
   the	
   predickon?	
   

       topics	
   are	
   hard	
   to	
   evaluate;	
   no	
   gold	
   standard.	
   
       this	
   is	
   either	
   an	
   open	
   problem	
   or	
   the	
   nail	
   in	
   
the	
   co   n,	
   depending	
   on	
   your	
   point	
   of	
   view.	
   

hidden	
   markov	
   models	
   

  ,	
     	
   

  y	
   

k	
   

yi,j-     1	
   

yi,j	
   

xi,j	
   

bayesian	
   id48	
   
(goldwater	
   and	
   
gri   ths,	
   2007)	
   

ni	
   
m	
   

     ,	
   
     	
   

  y	
   

k	
   

  y	
   

k	
   

yi,j-     1	
   

yi,j	
   

xi,j	
   

unsupervised	
   id48	
   
(merialdo,	
   1994)	
   

ni	
   
m	
   

  y	
   

k	
   

m	
   =	
   number	
   of	
   sequences	
   
ni	
   =	
   number	
   of	
   words	
   in	
   sequence	
   i	
   
k	
   =	
   number	
   of	
   states	
   

the	
   engineering	
   part	
   

       typically	
   approximate	
   id136	
   is	
   required.	
   

       markov	
   chain	
   monte	
   carlo	
   (e.g.,	
   gibbs	
   sampling)	
   
       variakonal	
   id136	
   (e.g.,	
   mean-        eld)	
   

       graphical	
   model	
   view	
   is	
   really	
   helpful	
   when	
   designing	
   
       learning:	
   	
   approximate	
   id136	
   +	
   opkmizakon	
   of	
   

id136	
   algorithms	
   for	
   your	
   bayesian	
   model!	
   

hyperparameters	
   (for	
   lda,	
   usually	
     	
   and	
     ;	
     	
   is	
   osen	
   
assumed	
   uniform).	
   
       stochaskc	
   or	
   variakonal	
   em,	
   depending	
   on	
   your	
   choice	
   of	
   
       full	
   bayesian:	
   	
      x	
   the	
   prior	
   and	
   do	
   id136	
   on	
   all	
   your	
   

approximate	
   id136.	
   

data.	
   
       implicakons	
   for	
   train/test	
   methodology?	
   

sketch	
   of	
   gibbs	
   sampling	
   

       mcmc:	
   	
   design	
   (on	
   paper)	
   a	
   graph	
   where	
   each	
   

con   gurakon	
   from	
   val(v)	
   is	
   a	
   node.	
   
      transikons	
   in	
   the	
   graph	
   designed	
   to	
   give	
   a	
   markov	
   
chain	
   whose	
   stakonary	
   distribukon	
   is	
   the	
   
posterior.	
   

       simulate	
   a	
   random	
   walk	
   in	
   the	
   graph.	
   
       if	
   you	
   walk	
   long	
   enough,	
   your	
   posikon	
   is	
   

distributed	
   according	
   to	
   p(v).	
   

	
   

transikons	
   in	
   gibbs	
   sampling	
   
       a	
   transikon	
   in	
   the	
   markov	
   chain	
   equates	
   to	
   
changing	
   a	
   subset	
   of	
   the	
   random	
   variables.	
   
       gibbs:	
   	
   resample	
   vi   s	
   value	
   according	
   to	
   	
   
p(vi	
   |	
   v	
   \	
   {vi}).	
   
       only	
   need	
   the	
   local	
   factors	
   that	
   a   ect	
   vi:	
   	
   take	
   
product,	
   marginalize,	
   and	
   randomly	
   choose	
   new	
   
value.	
   

       simply	
   lock	
   evidence	
   variables	
   x.	
   
       maximizing	
   version	
   gradually	
   shiss	
   sampler	
   in	
   

favor	
   of	
   most	
   probable	
   value	
   for	
   vi.	
   

sketch	
   of	
   	
   

mean	
   field	
   variakonal	
   id136	
   
       id136	
   with	
   our	
   distribukon	
   p	
   is	
   hard.	
   
       choose	
   an	
      easier   	
   distribukon	
   family,	
   q.	
   	
   
then	
      nd:	
   

arg min
q q

d(q p )

       usually	
   iterakve	
   methods	
   are	
   required	
   to	
         t   	
   
q	
   to	
   p.	
   
      these	
   osen	
   resemble	
   familiar	
   learning	
   algorithms	
   
like	
   em!	
   

energy	
   funckonal	
   

d(q(y )   p (y | x = x)) = eq[log q(y )]   eq[log p (y | x = x)]

log p (x = x)

=  h(q(y ))   (eq[log p (x = x, y )]   log p (x = x))

=  h(q(y ))          
eq[log  |x]   log p (x = x)      
= d(q(y )   p (y | x = x)) + h(q(y )) +    
      

eq[log  |x]

 

 
       expectakons	
   under	
   simpler	
   distribukon	
   family,	
   q.	
   

maximize this

constant

      

 

 

       every	
   element	
   of	
   q is	
   an	
   approximate	
   solukon.	
   
       we	
   try	
   to	
      nd	
   the	
   best	
   one.	
   

variakonal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1
family of functions g  (x) 

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

x

variakonal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

       further,	
   for	
   any	
   x,	
   there	
   is	
   some	
     	
   where	
   the	
   

x

bound	
   is	
   kght.	
   	
   
        	
   is	
   called	
   a	
   varia/onal	
   parameter.	
   

tangent:	
   	
   variakonal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

       further,	
   for	
   any	
   x,	
   there	
   is	
   some	
     	
   where	
   the	
   

x

bound	
   is	
   kght.	
   	
   
        	
   is	
   called	
   a	
   varia/onal	
   parameter.	
   

tangent:	
   	
   variakonal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

       further,	
   for	
   any	
   x,	
   there	
   is	
   some	
     	
   where	
   the	
   

x

bound	
   is	
   kght.	
   	
   
        	
   is	
   called	
   a	
   varia/onal	
   parameter.	
   

       for	
   us,	
   log	
   p(x	
   =	
   x)	
   is	
   like	
   -     ln(x),	
   and	
   q	
   is	
   like	
     .	
   	
   

structured	
   variakonal	
   approach	
   
       maximize	
   the	
   energy	
   funckonal	
   over	
   a	
   family	
   

q that	
   is	
   well-     de   ned.	
   
      a	
   graphical	
   model!	
   
      probably	
   not	
   an	
   i-     map	
   for	
   p.	
   	
   (bound	
   isn   t	
   kght.)	
   
       simpler	
   structures	
   lead	
   to	
   easier	
   id136.	
   

      mean	
      eld	
   is	
   the	
   simplest:	
   

q(v ) =  i

qi(vi)

going	
   nonparametric	
   

       how	
   many	
   topics	
   or	
   states?	
   
       nonparametric:	
   	
   let	
   the	
   data	
   decide.	
   

       not	
   necessarily	
   bayesian	
   or	
   even	
   probabiliskc!	
   
       more	
   data	
   juskfy	
   more	
   parameters.	
   

       most	
   common	
   nonparametric	
   and	
   bayesian	
   tools	
   

in	
   nlp	
   are	
   based	
   on	
   the	
   dirichlet	
   process.	
   
       dp	
   is	
   not	
   the	
   same	
   as	
   the	
   dirichlet	
   distribu.on.	
   

       you	
   can	
   be	
   bayesian	
   without	
   being	
   

nonparametric,	
   and	
   you	
   can	
   be	
   nonparametric	
   
without	
   being	
   bayesian!	
   

remember	
   

          bayesian   	
   describes	
   your	
   model,	
   not	
   you!	
   
       approximate	
   id136	
   is	
   necessary	
   in	
   

bayesian	
   modeling,	
   but	
   useful	
   elsewhere,	
   too!	
   

