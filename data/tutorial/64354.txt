id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith,	
   carnegie	
   mellon	
   university	
   

	
   

2012	
   interna@onal	
   summer	
   school	
   in	
   
language	
   and	
   speech	
   technologies	
   	
   

introduc@on	
   

mo@va@on	
   

       sta@s@cal	
   methods	
   in	
   nlp	
   arrived	
   ~20	
   years	
   ago	
   
       mercer	
   was	
   right:	
   	
      there   s	
   no	
   data	
   like	
   more	
   

and	
   now	
   dominate.	
   

data.   	
   
       and	
   there   s	
   more	
   and	
   more	
   data.	
   

       lots	
   of	
   new	
   applica@ons	
   and	
   new	
   sta@s@cal	
   
       my	
   goal	
   is	
   to	
   synthesize	
   ideas	
   you	
   may	
   have	
   seen	
   

techniques.	
   

before	
   ...	
   

thesis	
   

       most	
   of	
   the	
   main	
   ideas	
   are	
   related	
   and	
   similar	
   to	
   

each	
   other.	
   
       di   erent	
   approaches	
   to	
   decoding.	
   
       di   erent	
   learning	
   criteria.	
   
       supervised	
   and	
   unsupervised	
   learning.	
   

       umbrella:	
   	
   probabilis@c	
   reasoning	
   about	
   discrete	
   

linguis@c	
   structures.	
   

	
   
       this	
   is	
   good	
   news!	
   

introduc@on	
   

       noah	
      	
   professor	
   at	
   cmu	
   since	
   2006	
   

       language	
   technologies	
   ins@tute	
   
       machine	
   learning	
   department	
   
       linguis   c	
   structure	
   predic   on	
   (2011)	
   
       courses:	
   	
      language	
   and	
   sta@s@cs	
   ii,   	
      probabilis@c	
   

graphical	
   models,   	
      structured	
   predic@on,   	
   
   algorithms	
   for	
   natural	
   language	
   processing   	
   at	
   cmu	
   

       this	
   course	
   was	
   codesigned	
   with	
   shay	
   cohen,	
   

now	
   at	
   columbia	
   university.	
   

plan	
   

1.	
    graphical	
   models	
   
2.	
    probabilis@c	
   id136	
   
3.	
    decoding	
   and	
   structures	
   
4.	
   
5.	
    hidden	
   variables	
   
6.	
   

the	
   bayesian	
   approach	
   

supervised	
   learning	
   

m	
   8:00-     9:30	
   
m	
   13:30-     15:00	
   
t	
   8:00-     9:30	
   
t	
   14:30-     16:00	
   
w	
   8:00-     9:30	
   
w	
   13:30-     15:00	
   

exhorta@ons	
   

       the	
   content	
   is	
   formal,	
   but	
   the	
   style	
   doesn   t	
   
       ask	
   ques@ons!	
   

need	
   to	
   be.	
   

      help	
   me	
      nd	
   the	
   right	
   pace.	
   
      lecture	
   6	
   can	
   be	
   dropped	
   if	
   we	
   need	
   to	
   slow	
   
down.	
   
       the	
   course	
   starts	
   in	
   machine	
   learning	
   and	
   

moves	
   toward	
   nlp.	
   	
   	
   
      be	
   pa@ent.	
   

	
   

lecture	
   1:	
   	
   graphical	
   models	
   

random	
   variables	
   

       id203	
   distribu@ons	
   usually	
   de   ned	
   by	
   events	
   
       events	
   are	
   complicated!	
   

       we	
   tend	
   to	
   group	
   events	
   by	
   a-ributes	
   
       person	
      	
   age,	
   grade,	
   haircolor	
   

       random	
   variables	
   formalize	
   afributes:	
   

          grade	
   =	
   a   	
   is	
   shorthand	
   for	
   event	
   
       proper@es	
   of	
   random	
   variable	
   x:	
   
       val(x)	
   =	
   possible	
   values	
   of	
   x	
   
 x val(x)
       for	
   discrete	
   (categorical):	
   
  p (x = x)dx = 1
       for	
   con@nuous:	
   
       nonnega@vity:	
   
   x     val(x), p (x = x)   0

p (x = x) = 1

{      : fgrade( ) = a}

condi@onal	
   probabili@es	
   

       ajer	
   learning	
   that	
     	
   is	
   true,	
   how	
   do	
   we	
   feel	
   
about	
     ?  p(   |   ) 	
   
  	
   

  	
   	
   

       	
   	
   

  	
   

chain	
   rule	
   

p (       ) = p ( )p (    |  )

p ( 1             k) = p ( 1)p ( 2 |  1)       p ( k |  1   . . .    k 1)

bayes	
   rule	
   

likelihood	
   

prior	
   

posterior	
   

normaliza@on	
   constant	
   

  	
   is	
   an	
      external	
   event   	
   

independence	
   

         	
   and	
     	
   are	
   independent	
   if	
   p(  |  )	
   =	
   p(  )	
   

p	
       (      	
     )	
   

       proposi8on:	
   	
     	
   and	
     	
   are	
   independent	
   if	
   and	
   

only	
   if	
   p(       )	
   =	
   p(  )	
   p(  )	
   	
   

condi8onal	
   independence	
   

       independence	
   is	
   rarely	
   true.	
   

         	
   and	
     	
   are	
   condi8onally	
   independent	
   given	
     	
   if	
   

p(   |	
            )	
   =	
   p(   |	
     )	
   

p	
       (      	
      |   )	
   

proposi8on:	
   	
   p	
       (      	
      |   )	
   if	
   and	
   only	
   if	
   	
   
	
   p(        |	
     )	
   =p	
   (   |	
     )	
   p(   |	
     )	
   	
   

joint	
   distribu@on	
   and	
   marginaliza@on	
   

       compute	
   the	
   marginal	
   

over	
   each	
   individual	
   
random	
   variable?	
   

p (grade, intelligence) =

intelligence	
   
=	
   very	
   high	
   

intelligence	
   
=	
   high	
   

0.70	
   

0.15	
   

0.10	
   

0.05	
   

grade	
   =	
   a	
   

grade	
   =	
   b	
   

marginaliza@on:	
   	
   general	
   case	
   

p (xi = x) =

p (x1 = x1, x2 = x2, . . . , xi = x, . . . , xn = xn)

x1 val(x1),x2 val(x2),...,xi 1 val(xi 1),xi+1 val(xi+1),...,xn val(xn)

 

p (xi = x) =

 

x1,x2,...,xi 1,xi+1,...,xn

p (x1, x2, . . . , xi, . . . , xn)

how	
   many	
   terms?	
   

basic	
   concepts	
   so	
   far	
   

       atomic	
   outcomes:	
   	
   assignment	
   of	
   x1,   ,xn	
   to	
   	
   
x1,   ,	
   xn	
   

       condi@onal	
   id203:	
   p(x,	
   y)	
   =	
   p(x)	
   p(y|x)	
   

       bayes	
   rule:	
   p(x|y)	
   =	
   p(y|x)	
   p(x)	
   /	
   p(y)	
   
       chain	
   rule:	
   	
   p(x1,   ,xn)	
   =	
   p(x1)	
   p(x2|x1)	
   	
   
	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
   	
       p(xk|x1,   ,xk-     1)	
   

sets	
   of	
   variables	
   

       sets	
   of	
   variables	
   x,	
   y,	
   z	
   
       x	
   is	
   independent	
   of	
   y	
   given	
   z	
   if	
   
       p	
       (x=x	
      	
   y=y|z=z),	
   	
   
   	
   x   val(x),	
   y   val(y),	
   z   val(z)	
   

       shorthand:	
   

       condi8onal	
   independence:	
   p	
      	
   (x	
      	
   y	
   |	
   z)	
   
       for	
   p	
      	
   (x	
      	
   y	
   |	
      ),	
   write	
   p	
      	
   (x	
      	
   y)	
   

       proposi8on:	
   p	
   sa@s   es	
   (x	
      	
   y	
   |	
   z)	
   if	
   and	
   only	
   if	
   

p(x,y|z)	
   =	
   p(x|z)	
   p(y|z)	
   

free	
   parameters	
   

       consider	
   assigning	
   a	
   value	
   to	
   p(x	
   =	
   x)	
   for	
   each	
   

x	
   in	
   val(x).	
   	
   how	
   many	
   free	
   parameters,	
   if	
   
   val(x)   	
   =	
   k?	
   

       now	
   consider	
   p(x1,	
   x2,	
      ,	
   xn).	
   	
   how	
   many?	
   

       can	
   we	
   do	
   it	
   with	
   fewer	
   parameters?	
   	
   
	
   

(marginal)	
   independence	
   
       let   s	
   make	
   a	
   very	
   strong	
   independence	
   

assump@on:	
   

   y   x,	
   z   x,	
   y	
      	
   z	
   

       joint	
   distribu@on:	
   

p (x) =

p (xi)

n i=1

       how	
   many	
   free	
   parameters	
   now?	
   

independence	
   spectrum	
   

various	
   bayesian	
   networks	
   

full	
   independence	
   assump@ons	
   

everything	
   is	
   dependent	
   

causal	
   structure	
   

       the	
      u	
   causes	
   sinus	
   

in   amma@on	
   

       allergies	
   also	
   cause	
   
sinus	
   in   amma@on	
   
       sinus	
   in   amma@on	
   
causes	
   a	
   runny	
   nose	
   
       sinus	
   in   amma@on	
   
causes	
   headaches	
   

flu	
   

all.	
   

s.i.	
   

r.n.	
   

h	
   

querying	
   the	
   model	
   

       id136	
   (e.g.,	
   do	
   
you	
   have	
   allergies?)	
   

       what   s	
   the	
   best	
   

explana@on?	
   

       ac@ve	
   data	
   collec@on	
   
(what	
   is	
   the	
   next	
   best	
   
r.v.	
   to	
   observe?)	
   

flu	
   

all.	
   

s.i.	
   

r.n.	
   

h.	
   

a	
   bigger	
   example:	
   	
   your	
   car	
   

       the	
   car	
   doesn   t	
   start.	
   
       what	
   do	
   we	
   conclude	
   
about	
   the	
   bafery	
   age?	
   

       18	
   random	
   variables	
   
       marginaliza@on	
   will	
   

have	
   218	
   terms!	
   

	
   

factored	
   joint	
   distribu@on	
   

       want:	
   

p(f,	
   a,	
   s,	
   r,	
   h)	
   	
   
=	
   p(f)	
   
	
   	
   	
   p(a)	
   
	
   	
   	
   p(s	
   |	
   f,	
   a)	
   
	
   	
   	
   p(r	
   |	
   s)	
   
	
   	
   	
   p(h	
   |	
   s)	
   

       how	
   many	
   parameters?	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

the	
   bn	
   independence	
   assump@on	
   

       local	
   markov	
   assump8on:	
   	
   a	
   variable	
   x	
   is	
   

independent	
   of	
   its	
   non-     descendants	
   given	
   its	
   
parents	
   (and	
   only	
   its	
   parents).	
   

x	
      	
   nondescendants(x)	
   |	
   parents(x)	
   

	
   

what   s	
   independent?	
   

       f	
      	
   a	
   |	
      	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

what   s	
   independent?	
   

       f	
      	
   a	
   |	
      	
   
       a	
      	
   f	
   |	
      	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

what   s	
   independent?	
   

       f	
      	
   a	
   |	
      	
   
       a	
      	
   f	
   |	
      	
   
       s?	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

what   s	
   independent?	
   

       f	
      	
   a	
   |	
      	
   
       a	
      	
   f	
   |	
      	
   
       s?	
   
       r	
      	
   {f,	
   a,	
   h}	
   |	
   s	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

what   s	
   independent?	
   

       f	
      	
   a	
   |	
      	
   
       a	
      	
   f	
   |	
      	
   
       s?	
   
       r	
      	
   {f,	
   a,	
   h}	
   |	
   s	
   
       h	
      	
   {f,	
   a,	
   r}	
   |	
   s	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

new	
   edge:	
   	
   what   s	
   independent?	
   
       f	
      	
   a	
   |	
      	
   
       a	
      	
   f	
   |	
      	
   
       s?	
   
       r	
      	
   {f,	
   a,	
   h}	
   |	
   s,	
   f	
   
       h	
      	
   {f,	
   a,	
   r}	
   |	
   s	
   

p(s	
   |	
   f,	
   a)	
   

p(r	
   |	
   s,	
   f)	
   

p(h	
   |	
   s)	
   

p(a)	
   

p(f)	
   

r.n.	
   

all.	
   

s.i.	
   

flu	
   

h.	
   

       f	
      	
   a	
   |	
   s	
   ?	
   

a	
   puzzle	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

a	
   puzzle	
   

       f	
      	
   a	
   |	
   s	
   ?	
   

p(s|f,	
   a)	
   

f	
   =	
   true,	
   
a	
   =	
   true	
   	
   

true	
   
false	
   

0	
   
1	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

f	
   =	
   true,	
   
a	
   =	
   false	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   true	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   false	
   
0	
   
1	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

a	
   puzzle	
   

true	
   
false	
   

0.2	
   
0.8	
   

       f	
      	
   a	
   |	
   s	
   ?	
   

true	
   
false	
   

0.2	
   
0.8	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s|f,	
   a)	
   

f	
   =	
   true,	
   
a	
   =	
   true	
   	
   

true	
   
false	
   

0	
   
1	
   

f	
   =	
   true,	
   
a	
   =	
   false	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   true	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   false	
   
0	
   
1	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

a	
   puzzle	
   

true	
   
false	
   

0.2	
   
0.8	
   

       f	
      	
   a	
   |	
   s	
   ?	
   

true	
   
false	
   

0.2	
   
0.8	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

p(s|f,	
   a)	
   

f	
   =	
   true,	
   
a	
   =	
   true	
   	
   

f	
   =	
   true,	
   
a	
   =	
   false	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   true	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   false	
   
0	
   
1	
   

0	
   
1	
   

true	
   
	
   
false	
   
       p(f	
   =	
   true)	
   =	
   0.2	
   
       p(f	
   =	
   true	
   |	
   s	
   =	
   true)	
   =	
   0.5	
   
       p(f	
   =	
   true	
   |	
   s	
   =	
   true,	
   a	
   =	
   true)	
   =	
   0	
   

p(r	
   |	
   s)	
   

r.n.	
   

s.i.	
   

p(h	
   |	
   s)	
   

h.	
   

a	
   puzzle	
   

true	
   
false	
   

0.2	
   
0.8	
   

       f	
      	
   a	
   |	
   s	
   ?	
   

true	
   
false	
   

0.2	
   
0.8	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(s|f,	
   a)	
   

f	
   =	
   true,	
   
a	
   =	
   true	
   	
   

f	
   =	
   true,	
   
a	
   =	
   false	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   true	
   
1	
   
0	
   

f	
   =	
   false,	
   
a	
   =	
   false	
   
0	
   
1	
   

  	
   
1	
   -     	
     	
   

true	
   
	
   
false	
   
       p(f	
   =	
   true)	
   =	
   0.2	
   
       p(f	
   =	
   true	
   |	
   s	
   =	
   true)	
   =	
   (  	
   +	
   4)/(  	
   +	
   8)	
   
       p(f	
   =	
   true	
   |	
   s	
   =	
   true,	
   a	
   =	
   true)	
   =	
     	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

a	
   puzzle	
   

       f	
      	
   a	
   |	
   s	
   ?	
   

       in	
   general,	
   no.	
   

      this	
   independence	
   
statement	
   does	
   not	
   
follow	
   from	
   the	
   local	
   
markov	
   assump@on.	
   	
   

         	
   (f	
      	
   a	
   |	
   s)	
   

p(f)	
   

flu	
   

p(a)	
   

all.	
   

p(s	
   |	
   f,	
   a)	
   

s.i.	
   

p(r	
   |	
   s)	
   

r.n.	
   

p(h	
   |	
   s)	
   

h.	
   

recipe	
   for	
   a	
   bayesian	
   network	
   

       set	
   of	
   random	
   variables	
   x	
   
       directed	
   acyclic	
   graph	
   (each	
   xi	
   is	
   a	
   vertex)	
   
       condi@onal	
   id203	
   tables,	
   p(x	
   |	
   parents(x))	
   
       joint	
   distribu@on:	
   

p (x) =

p (xi | parents(xi))

n i=1

       local	
   markov	
   assump@on	
   

       a	
   variable	
   x	
   is	
   independent	
   of	
   its	
   non-     descendants	
   

given	
   its	
   parents	
   (and	
   only	
   its	
   parents).	
   

x	
      	
   nondescendants(x)	
   |	
   parents(x)	
   

	
   

ques@ons	
   

1.    given	
   a	
   bn,	
   what	
   distribu@ons	
   can	
   be	
   

represented?	
   

2.    given	
   a	
   distribu@on,	
   what	
   bns	
   can	
   represent	
   

it?	
   

3.    in	
   addi@on	
   to	
   the	
   local	
   markov	
   assump@on,	
   
what	
   other	
   independence	
   assump@ons	
   are	
   
encoded	
   in	
   a	
   given	
   bn?	
   

representation theorem

the conditional 
independencies in our bn 
are a subset of the 
independencies in p.

ques@ons	
   

1.    given	
   a	
   bn,	
   what	
   distribu@ons	
   can	
   be	
   

represented?	
   

2.    given	
   a	
   distribu@on,	
   what	
   bns	
   can	
   represent	
   

it?	
   

3.    in	
   addi@on	
   to	
   the	
   local	
   markov	
   assump@on,	
   
what	
   other	
   independence	
   assump@ons	
   are	
   
encoded	
   in	
   a	
   given	
   bn?	
   

independencies	
   

       local	
   markov	
   assump@on:	
   	
   	
   

xi	
      	
   nondescendants(xi)	
   |	
   parents(xi)	
   

       are	
   there	
   other	
   independencies	
   that	
   we	
   can	
   

derive?	
   
      yes.	
   
      let   s	
   consider	
   some	
   three-     node	
   bayesian	
   
networks.	
   

three-     node	
   bns	
   

       indirect	
   causal	
   e   ect	
   	
   
       indirect	
   eviden@al	
   e   ect	
   
       common	
   cause	
   

(x	
      	
   y	
   |	
   z),	
     (x	
      	
   y)	
   

       common	
   e   ect	
   	
   

(v-     structure)	
   
(x	
      	
   y),	
     (x	
      	
   y	
   |	
   z)	
   	
   

x	
   

x	
   

x	
   

x	
   

z	
   

z	
   

z	
   

z	
   

y	
   

y	
   

y	
   

y	
   

v-     structures,	
   or	
   colliders	
   

       let	
   z	
   =	
   x	
      	
   y.	
   

      yes,	
   random	
   variables	
   can	
   be	
   determinis@c	
   
func@ons!	
   

       in	
   this	
   case,	
   if	
   i	
   know	
   z,	
   then	
   x	
   and	
   y	
   are	
   

dependent,	
   because	
   they	
   cannot	
   be	
   equal!	
   

         (x	
      	
   y	
   |	
   z)	
   	
   

x	
   

y	
   

z	
   

what	
   we	
   want	
   

      a	
   general	
   test	
   for	
   condi@onal	
   independence	
   in	
   
a	
   bayesian	
   network!	
   

      surprisingly	
   enough,	
   we	
   can	
   characterize	
   all	
   
independence	
   assump@ons	
   in	
   a	
   bayesian	
   
network	
   based	
   on	
   the	
   simple	
   constructs	
   of	
   
three-     node	
   bns	
   

observa@ons	
   and	
   condi@onal	
   

independence	
   

       note:	
   when	
   we	
   observe	
   a	
   certain	
   outcome	
   of	
   

a	
   variable,	
   we	
   condi@on	
   on	
   its	
   value	
   

          x	
   and	
   y	
   are	
   independent	
   when	
   we	
   observe	
   
z   :	
   	
   	
   x	
      	
   y	
   |	
   z	
   

ac@ve	
   trails,	
   formalized	
   

      trail:	
   	
   undirected	
   path	
   that	
   doesn  t	
   visit	
   any	
   
nodes	
   more	
   than	
   once	
   
      a	
   trail	
   x1	
      	
   x2	
      	
      	
      	
   xk	
   is	
   an	
   ac8ve	
   trail	
   if,	
   for	
   
each	
   consecu@ve	
   triplet	
   in	
   the	
   trail:	
   
      xi-     1	
      	
   xi	
      	
   xi+1	
   and	
   xi	
   is	
   not	
   observed.	
   
      xi-     1	
      	
   xi	
      	
   xi+1	
   and	
   xi	
   is	
   not	
   observed.	
   
      xi-     1	
      	
   xi	
      	
   xi+1	
   and	
   xi	
   is	
   not	
   observed.	
   
      xi-     1	
      	
   xi	
      	
   xi+1	
   and	
   xi	
   (or	
   one	
   of	
   its	
   descendents)	
   is	
   
observed.	
   

d-     separa@on	
   

      three	
   sets	
   of	
   nodes:	
   	
   x,	
   y,	
   and	
   observed	
   nodes	
   
z	
   
      x	
   and	
   y	
   are	
   d-     separated	
   given	
   z	
   if	
   there	
   is	
   no	
   
ac@ve	
   trail	
   from	
   any	
   x	
   (cid:15464)	
   x	
   to	
   any	
   y	
   (cid:15464)	
   y	
   given	
   z.	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

       if	
   i	
   observe	
   nothing,	
   then	
   a	
      	
   h.	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

       if	
   i	
   observe	
   c,	
   then	
   a	
      	
   h.	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

       if	
   i	
   observe	
   c	
   and	
   f,	
   then	
     (a	
      	
   h).	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

       if	
   i	
   observe	
   c	
   and	
   f,	
   then	
     (a	
      	
   h).	
   

      but	
   if	
   i	
   observe	
   b,	
   d,	
   e,	
   and/or	
   g,	
   then	
   a	
      	
   h.	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

       if	
   i	
   observe	
   c	
   and	
   f,	
   then	
     (a	
      	
   h).	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

       if	
   i	
   observe	
   c	
   and	
   f   ,	
   then	
     (a	
      	
   h).	
   

another	
   example	
   

a	
   

b	
   

c	
   

d	
   

e	
   

g	
   

h	
   

f	
   

f   	
   

f      	
   

       if	
   i	
   observe	
   c	
   and	
   f      ,	
   then	
     (a	
      	
   h).	
   

intui@on	
   

       two	
   variables	
   can	
   be	
   dependent	
   if	
   there	
   is	
   a	
   

trail	
   between	
   them.	
   
         flow	
   of	
   in   uence   	
   along	
   ac@ve	
   trails	
   

       d-     separa@on	
   gives	
   us	
   a	
   way	
   to	
   think	
   about	
   

how	
   that	
         ow	
   of	
   in   uence   	
   could	
   be	
   blocked.	
   
      no	
   ac@ve	
   trail	
      	
   d-     separa@on	
      	
   no	
   dependence	
   

	
   

where	
   we	
   are	
   
       d-     separa@on	
   and	
   independence	
   

      d-     separa@on	
   is	
   a	
   sound	
   procedure	
   for	
      nding	
   
independencies:	
   	
   i(g)	
      	
   i(p)	
   
      we	
   can	
      nd	
   a	
   distribu@on	
   respec@ng	
   any	
   such	
   
independency.	
   
      almost	
   all	
   independencies	
   can	
   be	
   read	
   from	
   the	
   
graph	
   without	
   recourse	
   to	
   the	
   condi@onal	
   
id203	
   tables.	
   	
   i(g)	
      	
   i(p).	
   
       some@mes	
   independencies	
   can	
   happen	
   as	
   an	
   accident	
   
based	
   on	
   the	
   probabili@es!	
   

markov	
   networks	
   

perfect	
   maps	
   (p-     maps)	
   

      a	
   graph	
   g	
   is	
   a	
   p-     map	
   for	
   a	
   distribu@on	
   p	
   if	
   i(g)	
   
=	
   i(p).	
   

      can	
   we	
   always	
   construct	
   one?	
   

mo@va@ng	
   example:	
   	
   	
   

no	
   bayesian	
   network	
   is	
   a	
   p-     map	
   

       swinging	
   couples	
   or	
   misunderstanding	
   

students	
   

i(p):	
   
       a	
      	
   c	
   |	
   b,	
   d	
   
       b	
      	
   d	
   |	
   a,	
   c	
   
         	
   b	
      	
   d	
   
         	
   a	
      	
   c	
   

b	
   

a	
   

c	
   

d	
   

fails	
   to	
   capture:	
   
b	
      	
   d	
   |	
   a,	
   c	
   
	
   

b	
   

c	
   

d	
   

a	
   

fails	
   to	
   capture:	
   
  	
   b	
      	
   d	
   
	
   

       alice	
   only	
   talks	
   to	
   bob	
   and	
   debbie;	
   bob	
   only	
   talks	
   to	
   charles	
   and	
   alice;	
   charles	
   

only	
   talks	
   to	
   bob	
   and	
   debbie;	
   debbie	
   only	
   talks	
   to	
   alice	
   and	
   charles	
   

mo@va@ng	
   example:	
   	
   	
   

this	
   markov	
   network	
   is	
   a	
   p-     map!	
   

       swinging	
   couples	
   or	
   misunderstanding	
   

students	
   

i(p):	
   
       a	
      	
   c	
   |	
   b,	
   d	
   
       b	
      	
   d	
   |	
   a,	
   c	
   
         	
   b	
      	
   d	
   
         	
   a	
      	
   c	
   

b	
   

a	
   

c	
   

d	
   

markov	
   networks	
   

vertex.	
   

       each	
   random	
   variable	
   is	
   a	
   
       undirected	
   edges.	
   
       factors	
   are	
   associated	
   

with	
   subsets	
   of	
   nodes	
   that	
   
form	
   cliques.	
   
      a	
   factor	
   maps	
   assignments	
   
of	
   its	
   nodes	
   to	
   nonnega@ve	
   
values.	
   

b	
   

a	
   

c	
   

d	
   

markov	
   networks	
   

       in	
   this	
   example,	
   
associate	
   a	
   factor	
   
with	
   each	
   edge.	
   
      could	
   also	
   have	
   
factors	
   for	
   single	
   
nodes!	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

a	
   

c	
   

d	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

=	
   7,201,840	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

=	
   7,201,840	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

p(0,	
   1,	
   1,	
   0)	
   
=	
   5,000,000	
   /	
   z	
   
=	
   0.69	
   

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

=	
   7,201,840	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

p(1,	
   1,	
   0,	
   0)	
   
=	
   10	
   /	
   z	
   
=	
   0.0000014	
   

markov	
   networks	
   
(general	
   form)	
   

       let	
   di	
   denote	
   the	
   set	
   of	
   variables	
   (subset	
   of	
   x)	
   
in	
   the	
   ith	
   clique.	
   
       id203	
   distribu@on	
   is	
   a	
   gibbs	
   distribu@on:	
   

z

p (x) = u(x)
m   i=1

u(x) =

z =  x val(x)

 i(di)

u(x)

