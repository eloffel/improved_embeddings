id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith,	
   carnegie	
   mellon	
   university	
   

	
   

2012	
   interna@onal	
   summer	
   school	
   in	
   
language	
   and	
   speech	
   technologies	
   	
   

slides	
   online!	
   
       hep://@nyurl.com/psnlp2012	
   

       (i   ll	
   post	
   the	
   slides	
   aker	
   each	
   lecture.)	
   

markov	
   networks	
   

vertex.	
   

       each	
   random	
   variable	
   is	
   a	
   
       undirected	
   edges.	
   
       factors	
   are	
   associated	
   

with	
   subsets	
   of	
   nodes	
   that	
   
form	
   cliques.	
   
      a	
   factor	
   maps	
   assignments	
   
of	
   its	
   nodes	
   to	
   nonnega@ve	
   
values.	
   

b	
   

a	
   

c	
   

d	
   

markov	
   networks	
   

       in	
   this	
   example,	
   
associate	
   a	
   factor	
   
with	
   each	
   edge.	
   
      could	
   also	
   have	
   
factors	
   for	
   single	
   
nodes!	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

a	
   

c	
   

d	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

= 7,201,840 

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

p(0, 1, 1, 0) 
= 5,000,000 / z 
= 0.69 

markov	
   networks	
   

       id203	
   distribu@on:	
   
p (a, b, c, d)    1(a, b) 2(b, c) 3(c, d) 4(a, d)
p (a, b, c, d) =

 1(a, b) 2(b, c) 3(c, d) 4(a, d)
 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 1(a , b ) 2(b , c ) 3(c , d ) 4(a , d )

 a ,b ,c ,d 
z =  a ,b ,c ,d 

= 7,201,840 

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

c	
    d	
      3(c,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1	
   
100	
   
100	
   
1	
   

a	
    d	
      4(a,	
   d)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

b	
   

a	
   

c	
   

d	
   

p(1, 1, 0, 0) 
= 10 / z 
= 0.0000014 

markov	
   networks	
   
(general	
   form)	
   

       let	
   di	
   denote	
   the	
   set	
   of	
   variables	
   (subset	
   of	
   x)	
   
in	
   the	
   ith	
   clique.	
   
       id203	
   distribu@on	
   is	
   a	
   gibbs	
   distribu@on:	
   

z

p (x) = u(x)
m   i=1

u(x) =

z =  x val(x)

 i(di)

u(x)

notes	
   

       z	
   might	
   be	
   hard	
   to	
   calculate.	
   

         normaliza@on	
   constant   	
   
         par@@on	
   func@on   	
   

       can	
   get	
   e   cient	
   calcula@on	
   in	
   some	
   cases.	
   
      this	
   is	
   an	
   id136	
   problem;	
   it   s	
   equivalent	
   to	
   
marginalizing	
   over	
   everything.	
   

       ra#os	
   of	
   probabili@es	
   are	
   easy.	
   

p (x)
p (x )

= u(x)/z
u(x )/z

= u(x)
u(x )

independence	
   in	
   markov	
   networks	
   
       given	
   a	
   set	
   of	
   observed	
   nodes	
   z,	
   a	
   path	
   x1-     x2-     
x3-        -     xk	
   is	
   ac1ve	
   if	
   no	
   nodes	
   on	
   the	
   path	
   are	
   
observed.	
   	
   

independence	
   in	
   markov	
   networks	
   
       given	
   a	
   set	
   of	
   observed	
   nodes	
   z,	
   a	
   path	
   x1-     x2-     
x3-        -     xk	
   is	
   ac1ve	
   if	
   no	
   nodes	
   on	
   the	
   path	
   are	
   
observed.	
   	
   
       two	
   sets	
   of	
   nodes	
   x	
   and	
   y	
   in	
   h	
   are	
   separated	
   
given	
   z	
   if	
   there	
   is	
   no	
   ac@ve	
   path	
   between	
   any	
   
xi	
      	
   x	
   and	
   any	
   yi	
      	
   y.	
   
      denoted:	
   	
   seph(x,	
   y	
   |	
   z)	
   

independence	
   in	
   markov	
   networks	
   
       given	
   a	
   set	
   of	
   observed	
   nodes	
   z,	
   a	
   path	
   x1-     x2-     
x3-        -     xk	
   is	
   ac1ve	
   if	
   no	
   nodes	
   on	
   the	
   path	
   are	
   
observed.	
   	
   
       two	
   sets	
   of	
   nodes	
   x	
   and	
   y	
   in	
   h	
   are	
   separated	
   
given	
   z	
   if	
   there	
   is	
   no	
   ac@ve	
   path	
   between	
   any	
   
xi	
      	
   x	
   and	
   any	
   yi	
      	
   y.	
   
      denoted:	
   	
   seph(x,	
   y	
   |	
   z)	
   

       global	
   markov	
   assump@on:	
   	
   	
   
seph (x,	
   y	
   |	
   z)	
      	
   x	
      	
   y	
   |	
   z	
   

representa@on	
   theorems	
   

       bayesian	
   networks	
      	
   

the	
   bayesian	
   network	
   graph   s	
   
independencies	
   are	
   a	
   subset	
   of	
   
those	
   in	
   p.	
   

p (x) =

n i=1

p (xi | parents(xi))

      independencies	
   give	
   you	
   the	
   bayesian	
   network.	
   
      bayesian	
   network	
   reveals	
   independencies.	
   

representa@on	
   theorems	
   

       bayesian	
   networks	
      	
   

the	
   bayesian	
   network	
   graph   s	
   
independencies	
   are	
   a	
   subset	
   of	
   
those	
   in	
   p.	
   

       markov	
   networks	
      	
   

p (x) =

n i=1

p (xi | parents(xi))

representa@on	
   theorems	
   

       bayesian	
   networks	
      	
   

the	
   bayesian	
   network	
   graph   s	
   
independencies	
   are	
   a	
   subset	
   of	
   
those	
   in	
   p.	
   

       markov	
   networks	
      	
   

p (x) =

n i=1

p (xi | parents(xi))

the	
   markov	
   network	
   
graph   s	
   independencies	
   
are	
   a	
   subset	
   of	
   those	
   in	
   p.	
   

***	
   

p (x) =

 i(di)

1
z

m i=1

hammersley-     cli   ord	
   theorem	
   
       other	
   direc@on	
   succeeds	
   if	
   p(x)	
   >	
   0	
   for	
   all	
   x.	
   
       hammersley-     cli   ord	
   theorem	
   

the	
   markov	
   network	
   
graph   s	
   independencies	
   
are	
   a	
   subset	
   of	
   those	
   in	
   p	
   
and	
   p	
   is	
   nonnega#ve.	
   

p (x) =

 i(di)

1
z

m i=1

completeness	
   of	
   separa@on	
   
       for	
   almost	
   all	
   p	
   that	
   factorize,	
   i(h)	
   =	
   i(p).	
   

         almost	
   all   	
   is	
   the	
   same	
   hedge	
   as	
   in	
   the	
   bayesian	
   
network	
   case.	
   	
   a	
   measure-     zero	
   set	
   of	
   
parameteriza@ons	
   might	
   make	
   stronger	
   
independence	
   assump@ons	
   than	
   p	
   does.	
   

graphs	
   and	
   independencies	
   

bayesian	
   
networks	
   
local	
   markov	
   
assump@on	
   

markov	
   
networks	
   

?	
   

d-     separa@on	
   

separa@on	
   

local	
   

independencies	
   

global	
   

independencies	
   

       with id110s, we had the local 
       is there anything similar in markov networks?

markov assumptions

local	
   independence	
   assump@ons	
   in	
   

markov	
   networks	
   

       separa1on	
   de   nes	
   global	
   independencies.	
   

t1	
   

t2	
   

t3	
   

t5	
   

t4	
   

t6	
   

t7	
   

t8	
   

t9	
   

local	
   independence	
   assump@ons	
   in	
   

markov	
   networks	
   

       pairwise	
   markov	
   independence:	
   	
   pairs	
   of	
   non-     

adjacent	
   variables	
   are	
   independent	
   given	
   
everything	
   else.	
   

t1	
   

t2	
   

t3	
   

t5	
   

t4	
   

t6	
   

t7	
   

t8	
   

t9	
   

local	
   independence	
   assump@ons	
   in	
   

markov	
   networks	
   
       markov	
   blanket:	
   	
   each	
   variable	
   is	
   

independent	
   of	
   the	
   rest	
   given	
   its	
   neighbors.	
   

t1	
   

t2	
   

t3	
   

t5	
   

t4	
   

t6	
   

t7	
   

t8	
   

t9	
   

local	
   independence	
   assump@ons	
   in	
   

markov	
   networks	
   

       separa@on:	
   	
   	
   

seph (w,	
   y	
   |	
   z)	
      	
   w	
      	
   y	
   |	
   z	
   

       pairwise	
   markov:	
   	
   	
   
a	
      	
   b	
   |	
   x	
   \	
   {a,	
   b}	
   

       markov	
   blanket:	
   	
   	
   

a	
      	
   x	
   \	
   neighbors(a)	
   |	
   neighbors(a)	
   

soundness	
   

       for	
   a	
   posi@ve	
   distribu@on	
   p,	
   the	
   three	
   statements	
   

are	
   equivalent:	
   
       p	
   entails	
   the	
   global	
   independencies	
   of	
   h	
   (strongest)	
   	
   
       p	
   entails	
   the	
   markov	
   blanket	
   independencies	
   of	
   h 
       p	
   entails	
   the	
   pairwise	
   independencies	
   of	
   h	
   (weakest)	
   
       for	
   nonposi@ve	
   distribu@ons,	
   we	
   can	
      nd	
   cases	
   
that	
   sa@sfy	
   each	
   property,	
   but	
   not	
   the	
   stronger	
   
one!	
   
       examples	
   in	
   k&f	
   4.3.	
   

bayesian	
   networks	
   	
   
and	
   markov	
   networks	
   

local	
   

independencies	
   

global	
   

independencies	
   

rela1ve	
   

advantages	
   

bayesian	
   networks	
   

markov	
   networks	
   

local	
   markov	
   assump@on	
   

pairwise;	
   markov	
   blanket	
   

d-     separa@on	
   

separa@on	
   

       v-     structures	
   handled	
   elegantly	
   
       cpds	
   are	
   condi@onal	
   
probabili@es	
   
       id203	
   of	
   full	
   instan@a@on	
   
is	
   easy	
   (no	
   par@@on	
   func@on)	
   

       cycles	
   allowed	
   
       perfect	
   maps	
   for	
   swinging	
   
couples	
   

bayesian	
   
network	
   

moralize	
   

triangulate	
   

markov	
   
network	
   

from	
   bayesian	
   networks	
   to	
   markov	
   

networks	
   

       each	
   cpt	
   can	
   be	
   thought	
   of	
   as	
   a	
   factor	
   
       requires	
   us	
   to	
   connect	
   all	
   parents	
   of	
   each	
   
node	
   together	
   
      also	
   called	
      moraliza@on   	
   

25

from	
   markov	
   networks	
   to	
   bayesian	
   

networks	
   

      conversion	
   from	
   mn	
   to	
   bn	
   requires	
   
triangula@on.	
   	
   
      may	
   lose	
   some	
   independence	
   informa@on.	
   
      may	
   involve	
   a	
   lot	
   of	
   addi@onal	
   edges.	
   

summary	
   

       bns	
   and	
   mns	
   o   er	
   a	
   way	
   to	
   encode	
   a	
   set	
   of	
   

independence	
   assump@ons	
   

       there	
   is	
   a	
   way	
   to	
   transform	
   from	
   one	
   to	
   
another,	
   but	
   it	
   can	
   be	
   at	
   the	
   cost	
   of	
   losing	
   
independence	
   assump@ons	
   
       this	
   akernoon:	
   	
   id136	
   

27	
   

lecture	
   2:	
   	
   id136	
   

id136:	
   	
   an	
   ubiquitous	
   obstacle	
   
       decoding	
   is	
   id136.	
   
       subrou@nes	
   for	
   learning	
   are	
   id136.	
   
       learning	
   is	
   id136.	
   

       exact	
   id136	
   is	
   #p-     complete.	
   

      even	
   approxima@ons	
   within	
   a	
   given	
   absolute	
   or	
   
rela@ve	
   error	
   are	
   hard.	
   

	
   

probabilis@c	
   id136	
   problems	
   

given	
   values	
   for	
   some	
   random	
   variables	
   (x	
       v)	
      	
   
       most	
   probable	
   explana@on:	
   	
   what	
   are	
   the	
   most	
   probable	
   values	
   of	
   the	
   rest	
   

of	
   the	
   r.v.s	
   v	
   \	
   x?	
   

	
   
(more	
   generally	
      )	
   
       maximum	
   a	
   posteriori	
   (map):	
   what	
   are	
   the	
   most	
   probable	
   values	
   of	
   some	
   

other	
   r.v.s,	
   y	
       (v	
   \	
   x)?	
   

	
   
       random	
   sampling	
   from	
   the	
   posterior	
   over	
   values	
   of	
   y	
   
       full	
   posterior	
   over	
   values	
   of	
   y	
   
       marginal	
   probabili@es	
   from	
   the	
   posterior	
   over	
   y	
   
       minimum	
   bayes	
   risk:	
   	
   what	
   is	
   the	
   y	
   with	
   the	
   lowest	
   expected	
   cost?	
   
       cost-     augmented	
   decoding:	
   	
   what	
   is	
   the	
   most	
   dangerous	
   y?	
   

approaches	
   to	
   id136	
   

id136	
   

exact	
   

approximate	
   

variable	
   

elimina@on	
   

ilp	
   

randomized	
   

determinis@c	
   

dynamic	
   

program   ng	
   

mcmc	
   

importance	
   
sampling	
   

randomized	
   

search	
   

varia@onal	
   

loopy	
   belief	
   
propaga@on	
   

lp	
   

relaxa@ons	
   

local	
   search	
   

gibbs	
   

simulated	
   
annealing	
   

mean	
      eld	
   

dual	
   

decomp.	
   

beam	
   search	
   

today	
   

lecture	
   6,	
   if	
   @me	
   

exact	
   marginal	
   for	
   y	
   

       this	
   will	
   be	
   a	
   generaliza@on	
   of	
   algorithms	
   you	
   

already	
   know:	
   	
   the	
   forward	
   and	
   backward	
   
algorithms.	
   

       the	
   general	
   name	
   is	
   variable	
   elimina@on.	
   
       aker	
   we	
   see	
   it	
   for	
   the	
   marginal,	
   we   ll	
   see	
   how	
   

to	
   use	
   it	
   for	
   the	
   map.	
   

simple	
   id136	
   example	
   
       goal:	
   	
   p(d)	
   
	
   

0	
    1	
   

p(b	
   |a)	
   
0	
   
1	
   

0	
    1	
   

p(c	
   |b)	
   
0	
   
1	
   

0	
   
1	
   

a	
   

b	
   

c	
   

0	
    1	
   

d	
   

p(d	
   |c)	
   
0	
   
1	
   

simple	
   id136	
   example	
   
       let   s	
   calculate	
   p(b)	
   from	
   

things	
   we	
   have.	
   

0	
    1	
   

p(b	
   |a)	
   
0	
   
1	
   

0	
    1	
   

p(c	
   |b)	
   
0	
   
1	
   

0	
   
1	
   

a	
   

b	
   

c	
   

0	
    1	
   

d	
   

p(d	
   |c)	
   
0	
   
1	
   

simple	
   id136	
   example	
   
       let   s	
   calculate	
   p(b)	
   from	
   

things	
   we	
   have.	
   

p (b) =  a val(a)

p (a = a)p (b | a = a)

a	
   

b	
   

c	
   

d	
   

simple	
   id136	
   example	
   
       let   s	
   calculate	
   p(b)	
   from	
   

things	
   we	
   have.	
   

p (a = a)p (b | a = a)

p (b) =  a val(a)
       note	
   that	
   c	
   and	
   d	
   do	
   not	
   

maeer.	
   

a	
   

b	
   

c	
   

d	
   

simple	
   id136	
   example	
   
       let   s	
   calculate	
   p(b)	
   from	
   

things	
   we	
   have.	
   

p (b) =  a val(a)

p (a = a)p (b | a = a)

0	
   
1	
   

0	
    1	
   

t 

p(b	
   |	
   a)	
   
0	
   
1	
   

= 

0	
   
1	
   

a	
   

b	
   

c	
   

d	
   

simple	
   id136	
   example	
   

       we	
   now	
   have	
   a	
   bayesian	
   
network	
   for	
   the	
   marginal	
   
distribu@on	
   p(b,	
   c,	
   d).	
   

0	
    1	
   

p(c	
   |b)	
   
0	
   
1	
   

0	
   
1	
   

b	
   

c	
   

0	
    1	
   

d	
   

p(d	
   |c)	
   
0	
   
1	
   

simple	
   id136	
   example	
   

       we	
   can	
   repeat	
   the	
   same	
   
process	
   to	
   calculate	
   p(c).	
   
p (c) =  b val(b)
       we	
   already	
   have	
   p(b)!	
   

p (b = b)p (c | b = b)

b	
   

c	
   

d	
   

simple	
   id136	
   example	
   

       we	
   can	
   repeat	
   the	
   same	
   
process	
   to	
   calculate	
   p(c).	
   
p (c) =  b val(b)

p (b = b)p (c | b = b)

0	
   
1	
   

0	
    1	
   

t 

p(c	
   |	
   b)	
   
0	
   
1	
   

= 

0	
   
1	
   

b	
   

c	
   

d	
   

simple	
   id136	
   example	
   

       we	
   now	
   have	
   p(c,	
   d).	
   
       marginalizing	
   out	
   a	
   and	
   b	
   

happened	
   in	
   two	
   steps,	
   
and	
   we	
   are	
   exploi@ng	
   the	
   
bayesian	
   network	
   
structure.	
   

0	
   
1	
   

c	
   

0	
    1	
   

d	
   

p(d	
   |c)	
   
0	
   
1	
   

simple	
   id136	
   example	
   

       last	
   step	
   to	
   get	
   p(d):	
   
p (d) =  c val(c)

p (c = c)p (d | c = c)

0	
   
1	
   

t 

p(d	
   |	
   c)	
    0	
    1	
   
0	
   
1	
   

= 

0	
   
1	
   

d	
   

simple	
   id136	
   example	
   

       no@ce	
   that	
   the	
   same	
   step	
   happened	
   for	
   each	
   

random	
   variable:	
   
      we	
   created	
   a	
   new	
   cpd	
   over	
   the	
   variable	
   and	
   its	
   
   successor   	
   
      we	
   summed	
   out	
   (marginalized)	
   the	
   variable.	
   

p (d) =  a val(a)  b val(b)  c val(c)

=  c val(c)

p (d | c = c)  b val(b)

p (a = a)p (b = b | a = a)p (c = c | b = b)p (d | c = c)
p (a = a)p (b = b | a = a)

p (c = c | b = b)  a val(a)

that	
   was	
   variable	
   elimina@on	
   
       we	
   reused	
   computa@on	
   from	
   previous	
   steps	
   
and	
   avoided	
   doing	
   the	
   same	
   work	
   more	
   than	
   
once.	
   
      dynamic	
   programming	
     	
   la	
   forward	
   algorithm!	
   

       we	
   exploited	
   the	
   bayesian	
   network	
   structure	
   
(each	
   subexpression	
   only	
   depends	
   on	
   a	
   small	
   
number	
   of	
   variables).	
   

       exponen@al	
   blowup	
   avoided!	
   

what	
   remains	
   

       some	
   machinery	
   
       variable	
   elimina@on	
   in	
   general	
   
       the	
   maximiza@on	
   version	
   (for	
   map	
   id136)	
   
       a	
   bit	
   about	
   approximate	
   id136	
   

factor	
   graphs	
   

       variable	
   nodes	
   (circles)	
   
       factor	
   nodes	
   (squares)	
   

       can	
   be	
   mn	
   factors	
   or	
   bn	
   condi@onal	
   

id203	
   distribu@ons!	
   

       edge	
   between	
   variable	
   and	
   factor	
   if	
   
the	
   factor	
   depends	
   on	
   that	
   variable.	
   

       the	
   graph	
   is	
   bipar@te.	
   

x	
   

y	
   

z	
   

  1	
   

  2	
   

  3	
   

  4	
   

products	
   of	
   factors	
   

       given	
   two	
   factors	
   with	
   di   erent	
   scopes,	
   we	
   

can	
   calculate	
   a	
   new	
   factor	
   equal	
   to	
   their	
   
products.	
   

 product(x   y) =  1(x)     2(y)

products	
   of	
   factors	
   

       given	
   two	
   factors	
   with	
   di   erent	
   scopes,	
   we	
   

can	
   calculate	
   a	
   new	
   factor	
   equal	
   to	
   their	
   
products.	
   

a	
    b	
    c	
      3(a,	
   b,	
   

a	
    b	
      1(a,	
   b)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

30	
   
5	
   
1	
   
10	
   

. 

b	
    c	
      2(b,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

100	
   
1	
   
1	
   
100	
   

= 

0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

c)	
   
3000	
   
30	
   
5	
   
500	
   
100	
   
1	
   
10	
   
1000	
   

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

p(c	
   |	
   a,	
   b)	
   
0	
   
1	
   

0,	
   0	
    0,	
   1	
    1,	
   0	
    1,1	
   
0.1	
   
0.5	
   
0.5	
   
0.9	
   

0.2	
   
0.8	
   

0.4	
   
0.6	
   

   summing out    b 

a	
    c	
      (a,	
   c)	
   
0.9	
   
0	
    0	
   
0	
    1	
   
0.3	
   
1.1	
   
1	
    0	
   
1	
    1	
   
1.7	
   

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

p(c	
   |	
   a,	
   b)	
   
0	
   
1	
   

0,	
   0	
    0,	
   1	
    1,	
   0	
    1,1	
   
0.1	
   
0.5	
   
0.5	
   
0.9	
   

0.2	
   
0.8	
   

0.4	
   
0.6	
   

   summing out    c 

a	
    b	
      (a,	
   b)	
   
1	
   
0	
    0	
   
0	
    1	
   
1	
   
1	
   
1	
    0	
   
1	
    1	
   
1	
   

factor	
   marginaliza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   

  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   marginaliza@on:	
   

   (x) =  y val(y )

 (x, y)

       we	
   can	
   refer	
   to	
   this	
   new	
   factor	
   by	
      y	
     .	
   	
   

marginalizing	
   everything?	
   

       take	
   a	
   markov	
   network   s	
      product	
   factor   	
   by	
   

mul@plying	
   all	
   of	
   its	
   factors.	
   

       sum	
   out	
   all	
   the	
   variables	
   (one	
   by	
   one).	
   

       what	
   do	
   you	
   get?	
   

factors	
   are	
   like	
   numbers	
   

       products	
   are	
   commuta@ve:	
     1	
        2	
   	
   =	
     2	
        1	
   
       products	
   are	
   associa@ve:	
   	
   
(  1	
        2)      3	
   =	
     1	
      (  2     3)	
   
       sums	
   are	
   commuta@ve:	
   	
      x	
      y	
     	
   =	
      y	
      x	
     	
   
       distribu@vity	
   of	
   multliplica@on	
   over	
   
x        scope( 1)    x

( 1     2) =  1    x

summa@on:	
   

 2

elimina@ng	
   one	
   variable	
   

input:	
   	
   set	
   of	
   factors	
     ,	
   variable	
   z	
   to	
   eliminate	
   
output:	
   	
   new	
   set	
   of	
   factors	
     	
   
	
   
1.   let	
        	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
2.   let	
     	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
3.   let	
     	
   be	
      z	
                	
     	
   
4.   return	
     	
      	
   {  }	
   

example	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   

flu	
   

all.	
   

s.i.	
   

r.n.	
   

h.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  sh	
   

h.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   
1.   	
        	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
                	
     	
   
4.   return	
     	
      	
   {  }	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  sh	
   

h.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   
1.   	
        	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
     sh	
   
4.   return	
     	
      	
   {  }	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  sh	
   

h.	
   

example	
   

  f	
   

  a	
   

all.	
   

  sh	
   

h.	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   
1.   	
        	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
     sh	
   
4.   return	
     	
      	
   {  }	
   

p(h	
   |	
   s)	
    0	
   
0	
   
1	
   

0.8	
   
0.2	
   

1	
   

0.1	
   
0.9	
   

s	
   
0	
   
1	
   

  (s)	
   

1.0	
   
1.0	
   

example	
   

  f	
   

  a	
   

all.	
   

  	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   
1.   	
        	
   =	
   {  sh}	
   
2.   	
     	
   =	
   {  f,	
     a,	
     fas,	
     sr}	
   
3.     	
   =	
      h	
     sh	
   
4.   return	
     	
      	
   {  }	
   

p(h	
   |	
   s)	
    0	
   
0	
   
1	
   

0.8	
   
0.2	
   

1	
   

0.1	
   
0.9	
   

s	
   
0	
   
1	
   

  (s)	
   

1.0	
   
1.0	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   eliminate	
   h.	
   
       we	
   can	
   actually	
   ignore	
   the	
   
new	
   factor,	
   equivalently	
   just	
   
dele@ng	
   h!	
   
       why?	
   
       in	
   some	
   cases	
   elimina@ng	
   a	
   

variable	
   is	
   really	
   easy!	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  fa
s	
   

s.i.	
   

s	
   
0	
   
1	
   

  (s)	
   

1.0	
   
1.0	
   

variable	
   elimina@on	
   

input:	
   	
   set	
   of	
   factors	
     ,	
   ordered	
   list	
   of	
   variables	
   

z	
   to	
   eliminate	
   

output:	
   	
   new	
   factor	
     	
   
1.   for	
   each	
   zi	
      	
   z	
   (in	
   order):	
   
       let	
     	
   =	
   eliminate-     one(  ,	
   zi)	
   	
   

2.   return	
             	
     	
   	
   
	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       h	
   is	
   already	
   eliminated.	
   
       let   s	
   now	
   eliminate	
   s.	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  fa
s	
   

s.i.	
   

  a	
   

all.	
   

  fa
s	
   

s.i.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   s.	
   
1.   	
        	
   =	
   {  sr,	
     fas}	
   
2.   	
     	
   =	
   {  f,	
     a}	
   
3.     far	
   =	
      s	
                	
     	
   
4.   return	
     	
      	
   {  far}	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  fa
s	
   

s.i.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   s.	
   
1.   	
        	
   =	
   {  sr,	
     fas}	
   
2.   	
     	
   =	
   {  f,	
     a}	
   
3.     far	
   =	
      s	
     sr	
        	
     fas	
   
4.   return	
     	
      	
   {  far}	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  far	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   s.	
   
1.   	
        	
   =	
   {  sr,	
     fas}	
   
2.   	
     	
   =	
   {  f,	
     a}	
   
3.     far	
   =	
      s	
     sr	
        	
     fas	
   
4.   return	
     	
      	
   {  far}	
   

flu	
   

r.n.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       finally,	
   eliminate	
   a.	
   

flu	
   

r.n.	
   

  a	
   

all.	
   

  far	
   

  a	
   

all.	
   

  far	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   a.	
   
1.   	
        	
   =	
   {  a,	
     far}	
   
2.   	
     	
   =	
   {  f}	
   
3.     fr	
   =	
      a	
     a	
        	
     far	
   
4.   return	
     	
      	
   {  fr}	
   

flu	
   

r.n.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       elimina@ng	
   a.	
   
1.   	
        	
   =	
   {  a,	
     far}	
   
2.   	
     	
   =	
   {  f}	
   
3.     fr	
   =	
      a	
     a	
        	
     far	
   
4.   return	
     	
      	
   {  fr}	
   

flu	
   

  fr	
   

r.n.	
   

markov	
   chain,	
   again	
   

       earlier,	
   we	
   eliminated	
   

a,	
   then	
   b,	
   then	
   c.	
   

	
   

0	
    1	
   

0	
    1	
   

p(b	
   |a)	
   
0	
   
1	
   

p(c	
   |b)	
   
0	
   
1	
   

0	
   
1	
   

a	
   

b	
   

c	
   

0	
    1	
   

d	
   

p(d	
   |c)	
   
0	
   
1	
   

markov	
   chain,	
   again	
   

       now	
   let   s	
   start	
   by	
   

elimina@ng	
   c.	
   

	
   

0	
    1	
   

0	
    1	
   

p(b	
   |a)	
   
0	
   
1	
   

p(c	
   |b)	
   
0	
   
1	
   

0	
   
1	
   

a	
   

b	
   

c	
   

0	
    1	
   

d	
   

p(d	
   |c)	
   
0	
   
1	
   

markov	
   chain,	
   again	
   

       now	
   let   s	
   start	
   by	
   

elimina@ng	
   c.	
   
p(d	
   |c)	
   
0	
   
1	
   

. 

0	
    1	
   

	
   

p(c	
   |b)	
   
0	
   
1	
   

0	
    1	
   

= 

b	
    c	
    d	
         (b,	
   c,	
   d)	
   
0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

a	
   

b	
   

c	
   

d	
   

markov	
   chain,	
   again	
   

       now	
   let   s	
   start	
   by	
   

elimina@ng	
   c.	
   

	
   

  c 

b	
    c	
    d	
         (b,	
   c,	
   d)	
   
0	
    0	
    0	
   
0	
    0	
    1	
   
0	
    1	
    0	
   
0	
    1	
    1	
   
1	
    0	
    0	
   
1	
    0	
    1	
   
1	
    1	
    0	
   
1	
    1	
    1	
   

  (b,	
   d)	
   

= 

b	
    d	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

a	
   

b	
   

c	
   

d	
   

markov	
   chain,	
   again	
   

       elimina@ng	
   b	
   will	
   be	
   

similarly	
   complex.	
   

	
   

  (b,	
   d)	
   

b	
    d	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

a	
   

b	
   

d	
   

variable	
   elimina@on:	
   	
   comments	
   
       can	
   prune	
   away	
   all	
   non-     ancestors	
   of	
   the	
   query	
   

variables.	
   

       ordering	
   makes	
   a	
   di   erence!	
   
       works	
   for	
   markov	
   networks	
   and	
   bayesian	
   

networks.	
   
      factors	
   need	
   not	
   be	
   cpds	
   and,	
   in	
   general,	
   new	
   
factors	
   won   t	
   be.	
   

what	
   about	
   evidence?	
   

       so	
   far,	
   we   ve	
   just	
   considered	
   the	
   posterior/

marginal	
   p(y).	
   

       next:	
   	
   condi@onal	
   distribu@on	
   p(y	
   |	
   x	
   =	
   x).	
   

       it   s	
   almost	
   the	
   same:	
   	
   the	
   addi@onal	
   step	
   is	
   to	
   

reduce	
   factors	
   to	
   respect	
   the	
   evidence.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

0	
    1	
   

p(r	
   |	
   s)	
   
0	
   
1	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

  a	
   

all.	
   

  sh	
   

h.	
   

example	
   

  f	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

0	
    1	
   

p(r	
   |	
   s)	
   
0	
   
1	
   

s	
    r	
      sr	
   (s,	
   r)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

  a	
   

all.	
   

  sh	
   

h.	
   

  a	
   

all.	
   

  sh	
   

h.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

0	
    1	
   

p(r	
   |	
   s)	
   
0	
   
1	
   

s	
    r	
      sr	
   (s,	
   r)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

s	
    r	
         s	
   
(s)	
   

0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

0	
    1	
   

p(r	
   |	
   s)	
   
0	
   
1	
   

s	
    r	
      sr	
   (s,	
   r)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

  fa
s	
   

s.i.	
   

flu	
   

  sr	
   

r.n.	
   

s	
    r	
         s	
   
(s)	
   

0	
    1	
   
1	
    1	
   

  a	
   

all.	
   

  sh	
   

h.	
   

example	
   

  f	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       let   s	
   reduce	
   to	
   r	
   =	
   
true	
   (runny	
   nose).	
   

  fa
s	
   

s.i.	
   

flu	
   

     s	
   

s	
    r	
         s	
   
(s)	
   

0	
    1	
   
1	
    1	
   

  a	
   

all.	
   

  sh	
   

h.	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

example	
   

  f	
   

  fa
s	
   

s.i.	
   

flu	
   

     s	
   

  a	
   

all.	
   

  sh	
   

h.	
   

h can be pruned 
for the same reasons 
as before. 

example	
   

  f	
   

  a	
   

all.	
   

  fa
s	
   

s.i.	
   

flu	
   

     s	
   

eliminate s. 

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

example	
   

  f	
   

  a	
   

flu	
   

  fa	
   

all.	
   

eliminate a. 

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

example	
   

  f	
   

flu	
   

  f	
   

take final product. 

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor	
   (for	
   f).	
   

example	
   

       query:	
   	
   	
   

p(flu	
   |	
   runny	
   nose)	
   

       now	
   run	
   variable	
   
elimina@on	
   all	
   the	
   
way	
   down	
   to	
   one	
   
factor.	
   

  f	
        f	
   

variable	
   elimina@on	
   for	
   	
   
condi@onal	
   probabili@es	
   

input:	
   	
   graphical	
   model	
   on	
   v,	
   set	
   of	
   query	
   variables	
   

y,	
   evidence	
   x	
   =	
   x	
   	
   

output:	
   	
   factor	
     	
   and	
   scalar	
     	
   
1.   	
     	
   =	
   factors	
   in	
   the	
   model	
   
2.   reduce	
   factors	
   in	
     	
   by	
   x	
   =	
   x	
   
3.   choose	
   variable	
   ordering	
   on	
   z	
   	
   =	
   v	
   \	
   y	
   \	
   x	
   
4.     	
   =	
   variable-     elimina@on(  ,	
   z)	
   
5.     	
   =	
      z   val(z)	
     (z)	
   
6.   return	
     ,	
     	
   

	
   

note	
   

       for	
   bayesian	
   networks,	
   the	
      nal	
   factor	
   will	
   be	
   

p(y,	
   x	
   =	
   x)	
   and	
   the	
   sum	
     	
   =	
   p(x	
   =	
   x).	
   

       this	
   equates	
   to	
   a	
   gibbs	
   distribu@on	
   with	
   

par@@on	
   func@on	
   =	
     .	
   

variable	
   elimina@on	
   

       in	
   general,	
   exponen@al	
   requirements	
   in	
   induced	
   
width	
   corresponding	
   to	
   the	
   ordering	
   you	
   choose.	
   

       it   s	
   np-     hard	
   to	
      nd	
   the	
   best	
   elimina@on	
   

ordering.	
   

       if	
   you	
   can	
   avoid	
      big   	
   intermediate	
   factors,	
   you	
   

can	
   make	
   id136	
   linear	
   in	
   the	
   size	
   of	
   the	
   
original	
   factors.	
   

addi@onal	
   comments	
   
       run@me	
   depends	
   on	
   the	
   size	
   of	
   the	
   

intermediate	
   factors.	
   

       hence,	
   variable	
   elimina@on	
   ordering	
   maeers	
   a	
   

lot.	
   
      but	
   it   s	
   np-     hard	
   to	
      nd	
   the	
   best	
   one.	
   
      for	
   mns,	
   chordal	
   graphs	
   permit	
   id136	
   in	
   @me	
   
linear	
   in	
   the	
   size	
   of	
   the	
   original	
   factors.	
   
      for	
   bns,	
   polytree	
   structures	
   do	
   the	
   same.	
   

ge(cid:132)ng	
   back	
   to	
   nlp	
   

       tradi@onal	
   structured	
   nlp	
   models	
   were	
   

some@mes	
   subconsciously	
   chosen	
   for	
   these	
   
proper@es.	
   
      id48s,	
   pid18s	
   (with	
   a	
   liele	
   work)	
   
      but	
   not:	
   	
   ibm	
   model	
   3	
   

       need	
   map	
   id136	
   for	
   decoding!	
   
       need	
   approximate	
   id136	
   for	
   complex	
   

models!	
   

from	
   marginals	
   to	
   map	
   
       replace	
   factor	
   marginaliza@on	
   steps	
   with	
   

maximiza#on.	
   
       add	
   bookkeeping	
   to	
   keep	
   track	
   of	
   the	
   maximizing	
   

       add	
   a	
   traceback	
   at	
   the	
   end	
   to	
   recover	
   the	
   

values.	
   

solu@on.	
   

       this	
   is	
   analogous	
   to	
   the	
   connec@on	
   between	
   the	
   

forward	
   algorithm	
   and	
   the	
   viterbi	
   algorithm.	
   
       ordering	
   challenge	
   is	
   the	
   same.	
   

factor	
   maximiza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   
  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   maximiza@on:	
   

   (x) = max

y

 (x, y )

       we	
   can	
   refer	
   to	
   this	
   new	
   factor	
   by	
   maxy	
     .	
   	
   

factor	
   maximiza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   
  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   maximiza@on:	
   

	
   

   (x) = max

 (x, y )

y

a	
   
0	
   
0	
   
0	
   
0	
   
1	
   
1	
   
1	
   
1	
   

b	
   
0	
   
0	
   
1	
   
1	
   
0	
   
0	
   
1	
   
1	
   

c	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   

  	
   (a,	
   b,	
   c)	
   

0.9	
   
0.3	
   
1.1	
   
1.7	
   
0.4	
   
0.7	
   
1.1	
   
0.2	
   

a	
    c	
      (a,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1.1	
    b=1	
   
1.7	
    b=1	
   
1.1	
    b=1	
   
0.7	
    b=0	
   

   maximizing out    b 

distribu@ve	
   property	
   

elimina@on:	
   

       a	
   useful	
   property	
   we	
   exploited	
   in	
   variable	
   
( 1     2) =  1    x
x        scope( 1)    x
       under	
   the	
   same	
   condi@ons,	
   factor	
   

mul@plica@on	
   distributes	
   over	
   max,	
   too:	
   

 2

max
x

( 1     2) =  1    max

x

 2

