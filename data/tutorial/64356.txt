id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith,	
   carnegie	
   mellon	
   university	
   

	
   

2012	
   interna@onal	
   summer	
   school	
   in	
   
language	
   and	
   speech	
   technologies	
   	
   

quick	
   recap	
   

       yesterday:	
   

      bayesian	
   networks	
   and	
   some	
   formal	
   proper@es	
   
      markov	
   networks	
   and	
   some	
   formal	
   proper@es	
   
      exact	
   marginal	
   id136	
   using	
   variable	
   
elimina@on	
   
       sum-     product	
   version	
   
       beginnings	
   of	
   the	
   max-     product	
   version	
   

variable	
   elimina@on	
   for	
   	
   
condi@onal	
   probabili@es	
   

input:	
   	
   graphical	
   model	
   on	
   v,	
   set	
   of	
   query	
   variables	
   

y,	
   evidence	
   x	
   =	
   x	
   	
   

output:	
   	
   factor	
     	
   and	
   scalar	
     	
   
1.   	
     	
   =	
   factors	
   in	
   the	
   model	
   
2.   reduce	
   factors	
   in	
     	
   by	
   x	
   =	
   x	
   
3.   choose	
   variable	
   ordering	
   on	
   z	
   	
   =	
   v	
   \	
   y	
   \	
   x	
   
4.     	
   =	
   variable-     elimina@on(  ,	
   z)	
   
5.     	
   =	
      z   val(z)	
     (z)	
   
6.   return	
     ,	
     	
   

	
   

from	
   marginals	
   to	
   map	
   
       replace	
   factor	
   marginaliza@on	
   steps	
   with	
   

maximiza&on.	
   
       add	
   bookkeeping	
   to	
   keep	
   track	
   of	
   the	
   maximizing	
   

       add	
   a	
   traceback	
   at	
   the	
   end	
   to	
   recover	
   the	
   

values.	
   

solu@on.	
   

       this	
   is	
   analogous	
   to	
   the	
   connec@on	
   between	
   the	
   

forward	
   algorithm	
   and	
   the	
   viterbi	
   algorithm.	
   
       ordering	
   challenge	
   is	
   the	
   same.	
   

factor	
   maximiza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   
  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   maximiza@on:	
   

   (x) = max

y

 (x, y )

       we	
   can	
   refer	
   to	
   this	
   new	
   factor	
   by	
   maxy	
     .	
   	
   

factor	
   maximiza@on	
   

       given	
   x	
   and	
   y	
   (y	
      	
   x),	
   we	
   can	
   turn	
   a	
   factor	
   	
   
  (x,	
   y)	
   into	
   a	
   factor	
     (x)	
   via	
   maximiza@on:	
   

	
   

   (x) = max

 (x, y )

y

a	
   
0	
   
0	
   
0	
   
0	
   
1	
   
1	
   
1	
   
1	
   

b	
   
0	
   
0	
   
1	
   
1	
   
0	
   
0	
   
1	
   
1	
   

c	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   
0	
   
1	
   

  	
   (a,	
   b,	
   c)	
   

0.9	
   
0.3	
   
1.1	
   
1.7	
   
0.4	
   
0.7	
   
1.1	
   
0.2	
   

a	
    c	
      (a,	
   c)	
   
0	
    0	
   
0	
    1	
   
1	
    0	
   
1	
    1	
   

1.1	
    b=1	
   
1.7	
    b=1	
   
1.1	
    b=1	
   
0.7	
    b=0	
   

   maximizing out    b 

distribu@ve	
   property	
   

elimina@on:	
   

       a	
   useful	
   property	
   we	
   exploited	
   in	
   variable	
   
( 1     2) =  1    x
x        scope( 1)    x
       under	
   the	
   same	
   condi@ons,	
   factor	
   

mul@plica@on	
   distributes	
   over	
   max,	
   too:	
   

 2

max
x

( 1     2) =  1    max

x

 2

traceback	
   

input:	
   	
   sequence	
   of	
   factors	
   with	
   associated	
   

variables:	
   	
   (  z1,	
      ,	
     zk)	
   

output:	
   	
   z*	
   
       each	
     z	
   is	
   a	
   factor	
   with	
   scope	
   including	
   z	
   and	
   
variables	
   eliminated	
   a)er	
   z.	
   
       work	
   backwards	
   from	
   i	
   =	
   k	
   to	
   1:	
   
      let	
   zi	
   =	
   arg	
   maxz	
     zi(z,	
   zi+1,	
   zi+2,	
      ,	
   zk)	
   

       return	
   z	
   

about	
   the	
   traceback	
   

       no	
   extra	
   (asympto@c)	
   expense.	
   

      linear	
   traversal	
   over	
   the	
   intermediate	
   factors.	
   
       the	
   factor	
   opera@ons	
   for	
   both	
   sum-     product	
   
ve	
   and	
   max-     product	
   ve	
   can	
   be	
   generalized.	
   
      example:	
   	
   get	
   the	
   k	
   most	
   likely	
   assignments	
   

elimina@ng	
   one	
   variable	
   

(max-     product	
   version	
   with	
   bookkeeping)	
   
input:	
   	
   set	
   of	
   factors	
     ,	
   variable	
   z	
   to	
   eliminate	
   
output:	
   	
   new	
   set	
   of	
   factors	
     	
   
	
   
1.   let	
        	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
2.   let	
     	
   =	
   {  	
      	
     	
   |	
   z	
      	
   scope(  )}	
   
3.   let	
     	
   be	
   maxz	
                	
     	
   
       let	
      be              	
     	
   (bookkeeping) 	
   
4.   return	
     	
      	
   {  },	
     	
   	
   

variable	
   elimina@on	
   

(max-     product	
   version	
   with	
   decoding)	
   

input:	
   	
   set	
   of	
   factors	
     ,	
   ordered	
   list	
   of	
   variables	
   

z	
   to	
   eliminate	
   

output:	
   	
   new	
   factor	
   
1.   for	
   each	
   zi	
      	
   z	
   (in	
   order):	
   

       let	
   (  ,	
     zi)	
   =	
   eliminate-     one(  ,	
   zi) 	
   	
   
2.   return	
             	
     ,	
   traceback({  zi})	
   

variable	
   elimina@on	
   tips	
   

       any	
   ordering	
   will	
   be	
   correct.	
   
       most	
   orderings	
   will	
   be	
   too	
   expensive.	
   
       there	
   are	
   heuris@cs	
   for	
   choosing	
   an	
   ordering	
   
(you	
   are	
   welcome	
   to	
      nd	
   them	
   and	
   test	
   them	
   
out).	
   

(rocket	
   science:	
   	
   true	
   map)	
   

       evidence:	
   	
   x	
   =	
   x	
   
       query:	
   	
   y	
   
       other	
   variables:	
   	
   z	
   =	
   v	
   \	
   x	
   \	
   y	
   

y  = arg max

y   val(y )

p (y = y | x = x)

= arg max

p (y = y, z = z | x = x)
       first,	
   marginalize	
   out	
   z,	
   then	
   do	
   map	
   id136	
   over	
   y	
   

y   val(y )  z   val(z)

given	
   x	
   =	
   x	
   

       this	
   is	
   not	
   usually	
   apempted	
   in	
   nlp,	
   with	
   some	
   excep@ons.	
   

par@ng	
   shots	
   

       you	
   will	
   probably	
   never	
   implement	
   the	
   
general	
   variable	
   elimina@on	
   algorithm.	
   

       you	
   will	
   rarely	
   use	
   exact	
   id136.	
   

       there	
   is	
   value	
   in	
   understanding	
   the	
   problem	
   

that	
   approxima@on	
   methods	
   are	
   trying	
   to	
   
solve,	
   and	
   what	
   an	
   exact	
   (if	
   intractable)	
   
solu@on	
   would	
   look	
   like!	
   

lecture	
   3:	
   	
   	
   

structures	
   and	
   decoding	
   

two	
   meanings	
   of	
      structure   	
   

       yesterday:	
   	
   structure	
   of	
   a	
   graph	
   for	
   modeling	
   a	
   

collec@on	
   of	
   random	
   variables	
   together.	
   

       today:	
   	
   linguis@c	
   structure.	
   

      sequence	
   labelings	
   (pos,	
   iob	
   chunkings,	
      )	
   
      parse	
   trees	
   (phrase-     structure,	
   dependency,	
      )	
   
      alignments	
   (word,	
   phrase,	
   tree,	
      )	
   
      predicate-     argument	
   structures	
   
      text-     to-     text	
   (transla@on,	
   paraphrase,	
   answers,	
      )	
   

a	
   useful	
   abstrac@on?	
   

       i	
   think	
   so.	
   
       brings	
   out	
   commonali@es:	
   

      modeling	
   formalisms	
   (e.g.,	
   linear	
   models	
   with	
   
features)	
   
      learning	
   algorithms	
   (lectures	
   4-     6)	
   
      generic	
   id136	
   algorithms	
   

       permits	
   sharing	
   across	
   a	
   wider	
   space	
   of	
   
       disadvantage:	
   	
   hides	
   engineering	
   details.	
   

problems.	
   

familiar	
   example:	
   	
   	
   

hidden	
   markov	
   models	
   

hidden	
   markov	
   model	
   
       x	
   and	
   y	
   are	
   both	
   sequences	
   of	
   symbols	
   

      x	
   is	
   a	
   sequence	
   from	
   the	
   vocabulary	
     	
   
      y	
   is	
   a	
   sequence	
   from	
   the	
   state	
   space	
     	
   
p(x = x, y = y) =   n   i=1
      transi@ons	
   p(y   	
   |	
   y)	
   

p(xi | yi)p(yi | yi 1)    p(stop | yn)

       parameters:	
   

       including	
   p(stop	
   |	
   y),	
   p(y	
   |	
   start)	
   

      emissions	
   p(x	
   |	
   y)	
   

hidden	
   markov	
   model	
   

       the	
   joint	
   model   s	
   independence	
   assump@ons	
   
are	
   easy	
   to	
   capture	
   with	
   a	
   bayesian	
   network.	
   

p(x = x, y = y) =   n   i=1

p(xi | yi)p(yi | yi 1)    p(stop | yn)

y0	
   

   	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

yn	
   

xn	
   

stop	
   

hidden	
   markov	
   model	
   
       the	
   joint	
   model	
   instan@ates	
   dynamic	
   

bayesian	
   networks.	
   

p(x = x, y = y) =   n   i=1

p(xi | yi)p(yi | yi 1)    p(stop | yn)

y0	
   

yi-     1	
   

yi	
   

xi	
   

template	
   that	
   
gets	
   copied	
   as	
   
many	
   @mes	
   as	
   
needed	
   

hidden	
   markov	
   model	
   

       given	
   x   s	
   value	
   as	
   evidence,	
   the	
   dynamic	
   part	
   

becomes	
   unnecessary,	
   since	
   we	
   know	
   n.	
   

p(x = x, y = y) =   n   i=1

p(xi | yi)p(yi | yi 1)    p(stop | yn)

y0	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

stop	
   

x1	
   =	
   
x1	
   

x2	
   =	
   
x2	
   

x3	
   =	
   
x3	
   

xn	
   =	
   
xn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

y0	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

stop	
   

x1	
   =	
   
x1	
   

x2	
   =	
   
x2	
   

x3	
   =	
   
x3	
   

xn	
   =	
   
xn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

       factor	
   graph:	
   

y0	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

stop	
   

x1	
   =	
   
x1	
   

x2	
   =	
   
x2	
   

x3	
   =	
   
x3	
   

xn	
   =	
   
xn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

       factor	
   graph	
   axer	
   reducing	
   factors	
   to	
   respect	
   

evidence:	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       the	
   usual	
   id136	
   problem	
   is	
   to	
      nd	
   the	
   

most	
   probable	
   value	
   of	
   y	
   given	
   x	
   =	
   x.	
   

       clever	
   ordering	
   should	
   be	
   apparent!	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
   take	
   a	
   product	
   of	
   
three	
   relevant	
   factors.	
   
       p(y1	
   |	
   start)	
   
         (y1)	
   =	
   reduced	
   p(x1	
   |	
   y1)	
   
       p(y2	
   |	
   y1)	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
      rst	
   take	
   a	
   product	
   
of	
   two	
   factors	
   that	
   only	
   involve	
   y1.	
   
	
   

p(y1	
   |	
   start)	
   
	
   

y1	
   
y2	
   
   	
   
y|  |	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

y1	
   
y2	
   
   	
   
y|  |	
   

  (y1)	
   =	
   reduced	
   p(x1	
   |	
   y1)	
   
	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
      rst	
   take	
   a	
   product	
   
of	
   two	
   factors	
   that	
   only	
   involve	
   y1.	
   
       this	
   is	
   the	
   viterbi	
   id203	
   vector	
   for	
   y1.	
   

	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

y1	
   
y2	
   
   	
   
y|  |	
   
  1(y1)	
   
	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y1,	
   we	
      rst	
   take	
   a	
   product	
   
of	
   two	
   factors	
   that	
   only	
   involve	
   y1.	
   
       this	
   is	
   the	
   viterbi	
   id203	
   vector	
   for	
   y1.	
   
       elimina@ng	
   y1	
   equates	
   to	
   solving	
   the	
   viterbi	
   
probabili@es	
   for	
   y2.	
   
	
   

y1	
   
y2	
   
   	
   
y|  |	
   
  1(y1)	
   
	
   

y1	
   

y2	
   

y3	
   

   	
   

yn	
   

y1	
   

y2	
   

   	
   

y|  |	
   

p(y2	
   |	
   y1)	
   
	
   

hidden	
   markov	
   model	
   
       product	
   of	
   all	
   factors	
   involving	
   y1,	
   then	
   
reduce.	
   
         2(y2)	
   =	
   maxy   val(y1)	
   (  1(y)	
      	
   p(y2	
   |	
   y))	
   
       this	
   factor	
   holds	
   viterbi	
   probabiliiesy	
   for	
   y2.	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       when	
   we	
   eliminate	
   y2,	
   we	
   take	
   a	
   product	
   of	
   
the	
   analogous	
   two	
   relevant	
   factors.	
   
       then	
   reduce.	
   

         3(y3)	
   =	
   maxy   val(y2)	
   (  2(y)	
      	
   p(y3	
   |	
   y))	
   

y2	
   

y3	
   

   	
   

yn	
   

hidden	
   markov	
   model	
   

       at	
   the	
   end,	
   we	
   have	
   one	
      nal	
   factor	
   with	
   one	
   

row,	
     n+1.	
   

       this	
   is	
   the	
   score	
   of	
   the	
   best	
   sequence.	
   
       use	
   backtrace	
   to	
   recover	
   values.	
   

yn	
   

why	
   think	
   this	
   way?	
   
       easy	
   to	
   see	
   how	
   to	
   generalize	
   id48s.	
   

      more	
   evidence	
   
      more	
   factors	
   
      more	
   hidden	
   structure	
   
      more	
   dependencies	
   

       probabilis@c	
   interpreta@on	
   of	
   factors	
   is	
   not	
   

central	
   to	
      nding	
   the	
      best    y	
      	
   
      many	
   factors	
   are	
   not	
   condi@onal	
   id203	
   
tables.	
   

generaliza@on	
   example	
   1	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

       each	
   word	
   also	
   depends	
   on	
   previous	
   state.	
   
	
   

generaliza@on	
   example	
   2	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

          trigram   	
   id48	
   
	
   

generaliza@on	
   example	
   3	
   

y1	
   

x1	
   

y2	
   

x2	
   

y3	
   

x3	
   

y4	
   

x4	
   

y5	
   

x5	
   

       aggregate	
   bigram	
   model	
   (saul	
   and	
   pereira,	
   

1997)	
   

general	
   decoding	
   problem	
   
       two	
   structured	
   random	
   variables,	
   x	
   and	
   y.	
   
      some@mes	
   described	
   as	
   collec&ons	
   of	
   random	
   
variables.	
   

          decode   	
   observed	
   value	
   x	
   =	
   x	
   into	
   some	
   

value	
   of	
   y.	
   

       usually,	
   we	
   seek	
   to	
   maximize	
   some	
   score.	
   

      e.g.,	
   map	
   id136	
   from	
   yesterday.	
   

linear	
   models	
   

       de   ne	
   a	
   feature	
   vector	
   func@on	
   g	
   that	
   maps	
   (x,	
   y)	
   pairs	
   
       score	
   is	
   linear	
   in	
   g(x,	
   y).	
   

into	
   d-     dimensional	
   real	
   space.	
   

score(x, y) = w   g(x, y)

y  = arg max
y   yx

w   g(x, y)

       results:	
   	
   	
   

       decoding	
   seeks	
   y	
   to	
   maximize	
   the	
   score.	
   
       learning	
   seeks	
   w	
   to	
      	
   do	
   something	
   we   ll	
   talk	
   about	
   later.	
   

       extremely	
   general!	
   

generic	
   noisy	
   channel	
   as	
   linear	
   model	
   

  y = arg max

y

= arg max
= arg max

y

y

= arg max

y

log (p(y)    p(x | y))
log p(y) + log p(x | y)
wy + wx|y
w g(x, y)

       of	
   course,	
   the	
   two	
   id203	
   terms	
   are	
   

typically	
   composed	
   of	
      smaller   	
   factors;	
   each	
   
can	
   be	
   understood	
   as	
   an	
   exponen@ated	
   
weight.	
   

max	
   ent	
   models	
   as	
   linear	
   models	
   

  y = arg max

y

= arg max

y

= arg max

y

= arg max

y

exp w g(x, y)

log p(y | x)
log
w g(x, y)   log z(x)
w g(x, y)

z(x)

id48s	
   as	
   linear	
   models	
   

  y = arg max

log p(x, y)

y

= arg max

= arg max

= arg max

y   n   i=1
y   n   i=1
y    y,x

log p(xi | yi) + log p(yi | yi 1)    + log p(stop | yn)
wyi   xi + wyi 1   yi    + wyn   stop
wy   xfreq(y     x; y, x) +   y,y   

wy   y   freq(y   y   ; y)

= arg max

w   g(x, y)

y

running	
   example	
   

       iob	
   sequence	
   labeling,	
   here	
   applied	
   to	
   ner	
   
       oxen	
   solved	
   with	
   id48s,	
   crfs,	
   m3ns	
      	
   

(what	
   is	
   not	
   a	
   linear	
   model?)	
   

       models	
   with	
   hidden	
   variables	
   
y  z
p(y | x) = arg max
       models	
   based	
   on	
   non-     linear	
   kernels	
   

arg max

y

p(y, z | x)

arg max

y

w g(x, y) = arg max

y

n i=1

 ik ( xi, yi   , x, y   )

decoding	
   

       for	
   id48s,	
   the	
   decoding	
   algorithm	
   we	
   usually	
   

think	
   of	
      rst	
   is	
   the	
   viterbi	
   algorithm.	
   	
   	
   
      this	
   is	
   just	
   one	
   example.	
   

       we	
   will	
   view	
   decoding	
   in	
      ve	
   di   erent	
   ways.	
   

      sequence	
   models	
   as	
   a	
   running	
   example.	
   
      these	
   views	
   are	
   not	
   just	
   for	
   id48s.	
   
      some@mes	
   they	
   will	
   lead	
   us	
   back	
   to	
   viterbi!	
   

five	
   views	
   of	
   decoding	
   

1.	
   	
   probabilis@c	
   graphical	
   models	
   
       view	
   the	
   linguis@c	
   structure	
   as	
   a	
   collec@on	
   of	
   

random	
   variables	
   that	
   are	
   interdependent.	
   

       represent	
   interdependencies	
   as	
   a	
   directed	
   or	
   

undirected	
   graphical	
   model.	
   

       condi@onal	
   id203	
   tables	
   (bns)	
   or	
   factors	
   

(mns)	
   encode	
   the	
   id203	
   distribu@on.	
   

id136	
   in	
   graphical	
   models	
   
       general	
   algorithm	
   for	
   exact	
   map	
   id136:	
   	
   

variable	
   elimina@on.	
   
      itera@vely	
   solve	
   for	
   the	
   best	
   values	
   of	
   each	
   
variable	
   condi@oned	
   on	
   values	
   of	
      preceding   	
   
neighbors.	
   
      then	
   trace	
   back.	
   

the	
   viterbi	
   algorithm	
   is	
   an	
   instance	
   of	
   

max-     product	
   variable	
   elimina@on!	
   

map	
   is	
   linear	
   decoding	
   

       bayesian	
   network:	
   

       markov	
   network:	
   

 i
log p(xi | parents(xi))
+ j
log p(yj | parents(yj))
 c

log  c ({xi}i c,{yj}j c)

       this	
   only	
   works	
   if	
   every	
   variable	
   is	
   in	
   x	
   or	
   y.	
   

id136	
   in	
   graphical	
   models	
   

       remember:	
   	
   more	
   edges	
   make	
   id136	
   more	
   

expensive.	
   
      fewer	
   edges	
   means	
   stronger	
   independence.	
   

       really	
   pleasant:	
   

id136	
   in	
   graphical	
   models	
   

       remember:	
   	
   more	
   edges	
   make	
   id136	
   more	
   

expensive.	
   
      fewer	
   edges	
   means	
   stronger	
   independence.	
   

       really	
   unpleasant:	
   

2.	
   	
   polytopes	
   

   parts   	
   

       assume	
   that	
   feature	
   func@on	
   g	
   breaks	
   down	
   

into	
   local	
   parts.	
   

#parts(x) i=1

g(x, y) =

f( i(x, y))

	
   
       each	
   part	
   has	
   an	
   alphabet	
   of	
   possible	
   values.	
   

      decoding	
   is	
   choosing	
   values	
   for	
   all	
   parts,	
   with	
   
consistency	
   constraints.	
   
      (in	
   the	
   graphical	
   models	
   view,	
   a	
   part	
   is	
   a	
   clique.)	
   

example	
   

       one	
   part	
   per	
   word,	
   each	
   is	
   in	
   {b,	
   i,	
   o}	
   
       no	
   features	
   look	
   at	
   mul@ple	
   parts	
   

      fast	
   id136	
   
      not	
   very	
   expressive	
   

example	
   

       one	
   part	
   per	
   bigram,	
   each	
   is	
   in	
   {bb,	
   bi,	
   bo,	
   
       features	
   and	
   constraints	
   can	
   look	
   at	
   pairs	
   	
   

ib,	
   ii,	
   io,	
   ob,	
   oo}	
   

      slower	
   id136	
   
      a	
   bit	
   more	
   expressive	
   

geometric	
   view	
   

       let	
   zi,  	
   be	
   1	
   if	
   part	
   i	
   takes	
   value	
     	
   and	
   0	
   
otherwise.	
   
       z	
   is	
   a	
   vector	
   in	
   {0,	
   1}n	
   	
   

      n	
   =	
   total	
   number	
   of	
   localized	
   part	
   values	
   
      each	
   z	
   is	
   a	
   vertex	
   of	
   the	
   unit	
   cube	
   

score	
   is	
   linear	
   in	
   z	
   

arg max

y

w   g(x, y) = arg max

not	
   really	
   
equal;	
   need	
   
to	
   transform	
   
back	
   to	
   get	
   y	
   

f( i(x, y))

     values( i)
     values( i)

f( )1{ i(x, y) =  }

f( )zi, 

y

y

w   

w   

#parts(x)   i=1
#parts(x)   i=1
#parts(x)   i=1
z zx w   fx    z

w   fxz

w   

= arg max

= arg max
z zx
= arg max
z zx
= arg max

polyhedra	
   

       not	
   all	
   ver@ces	
   of	
   the	
   n-     dimensional	
   unit	
   cube	
   

sa@sfy	
   the	
   constraints.	
   
      e.g.,	
   can   t	
   have	
   z1,bi	
   =	
   1 and	
   z2,bi	
   =	
   1	
   

       some@mes	
   we	
   can	
   write	
   down	
   a	
   small	
   

(polynomial	
   number)	
   of	
   linear	
   constraints	
   on	
   
z.	
   

       result:	
   	
   linear	
   objec@ve,	
   linear	
   constraints,	
   

integer	
   constraints	
      	
   

integer	
   linear	
   programming	
   

features.	
   

       very	
   easy	
   to	
   add	
   new	
   constraints	
   and	
   non-     local	
   
       many	
   decoding	
   problems	
   have	
   been	
   mapped	
   to	
   
ilp	
   (sequence	
   labeling,	
   parsing,	
      ),	
   but	
   it   s	
   not	
   
always	
   trivial.	
   	
   

       np-     hard	
   in	
   general.	
   

prac@ce	
   (e.g.,	
   cplex)	
   

       but	
   there	
   are	
   packages	
   that	
   oxen	
   work	
   well	
   in	
   
       specialized	
   algorithms	
   in	
   some	
   cases	
   
       lp	
   relaxa@on	
   for	
   approximate	
   solu@ons	
   

remark	
   

       graphical	
   models	
   assumed	
   a	
   probabilis@c	
   

interpreta@on	
   
      though	
   they	
   are	
   not	
   always	
   learned	
   using	
   a	
   
probabilis@c	
   interpreta@on!	
   

       the	
   polytope	
   view	
   is	
   agnos@c	
   about	
   how	
   you	
   

interpret	
   the	
   weights.	
   
      it	
   only	
   says	
   that	
   the	
   decoding	
   problem	
   is	
   an	
   ilp.	
   

