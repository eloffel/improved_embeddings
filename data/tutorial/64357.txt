id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith,	
   carnegie	
   mellon	
   university	
   

	
   

2012	
   interna@onal	
   summer	
   school	
   in	
   
language	
   and	
   speech	
   technologies	
   	
   

slides	
   online!	
   
       hep://@nyurl.com/psnlp2012	
   

       (i   ll	
   post	
   the	
   slides	
   aker	
   each	
   lecture.)	
   

where	
   we	
   lek	
   o   	
   

       graphical	
   models	
      	
   id136	
   ...	
      max   	
   

id136	
   and	
   decoding	
   with	
   linear	
   models.	
   

       five	
   views	
   of	
   decoding:	
   

1.    probabilis@c	
   graphical	
   models	
   
2.    polytopes	
   and	
   integer	
   linear	
   programming	
   
3.    ?	
   
4.    ?	
   
5.    ?	
   

3.	
   	
   weighted	
   parsing	
   

grammars	
   

       grammars	
   are	
   oken	
   associated	
   with	
   natural	
   

language	
   parsing,	
   but	
   they	
   are	
   extremely	
   
powerful	
   for	
   imposing	
   constraints.	
   

       we	
   can	
   add	
   weights	
   to	
   them.	
   

(closely	
   connected	
   to	
   wfsas)	
   

       id48s	
   are	
   a	
   kind	
   of	
   weighted	
   regular	
   grammar	
   
       pid18s	
   are	
   a	
   kind	
   of	
   weighted	
   id18	
   
       many,	
   many	
   more.	
   

       weighted	
   parsing:	
   	
      nd	
   the	
   maximum-     weighted	
   

deriva@on	
   for	
   a	
   string	
   x.	
   	
   

decoding	
   as	
   weighted	
   parsing	
   

       every	
   valid	
   y	
   is	
   a	
   gramma@cal	
   deriva@on	
   

(parse)	
   for	
   x.	
   
      id48:	
   	
   sequence	
   of	
      gramma@cal   	
   states	
   is	
   one	
   
allowed	
   by	
   the	
   transi@on	
   table.	
   

       augment	
   parsing	
   algorithms	
   with	
   weights	
   and	
   

   nd	
   the	
   best	
   parse.	
   	
   

the	
   viterbi	
   algorithm	
   is	
   an	
   instance	
   of	
   
recogni@on	
   by	
   a	
   weighted	
   grammar!	
   

bio	
   tagging	
   as	
   a	
   id18	
   

       weighted	
   (or	
   probabilis@c)	
   cky	
   is	
   a	
   dynamic	
   

programming	
   algorithm	
   very	
   similar	
   in	
   
structure	
   to	
   classical	
   cky.	
   

4.	
   	
   paths	
   and	
   hyperpaths	
   

best	
   path	
   

       general	
   idea:	
   	
   take	
   x	
   and	
   build	
   a	
   graph.	
   
       score	
   of	
   a	
   path	
   factors	
   into	
   the	
   edges.	
   
arg max

w   g(x, y) = arg max

y

w     e edges

y

       decoding	
   is	
      nding	
   the	
   best	
   path.	
   

f(e)1{e is crossed by y   s path}

the	
   viterbi	
   algorithm	
   is	
   an	
   instance	
   of	
   

   nding	
   a	
   best	
   path!	
   

	
   

   lagce   	
   view	
   of	
   viterbi	
   

minimum	
   cost	
   hyperpath	
   

       general	
   idea:	
   	
   take	
   x	
   and	
   build	
   a	
   hypergraph.	
   
       score	
   of	
   a	
   hyperpath	
   factors	
   into	
   the	
   

hyperedges.	
   

       decoding	
   is	
      nding	
   the	
   best	
   hyperpath.	
   

       this	
   connec@on	
   was	
   elucidated	
   by	
   klein	
   and	
   

manning	
   (2002).	
   

	
   

parsing	
   as	
   a	
   hypergraph	
   

parsing	
   as	
   a	
   hypergraph	
   

cf.    dean for democracy    

parsing	
   as	
   a	
   hypergraph	
   

forced to work on his thesis, sunshine streaming in 
the window, mike experienced a     

parsing	
   as	
   a	
   hypergraph	
   

forced to work on his thesis, sunshine streaming in 
the window, mike began to     

why	
   hypergraphs?	
   

       useful,	
   compact	
   encoding	
   of	
   the	
   hypothesis	
   

space.	
   
      build	
   hypothesis	
   space	
   using	
   local	
   features,	
   maybe	
   
do	
   some	
      ltering.	
   
      pass	
   it	
   o   	
   to	
   another	
   module	
   for	
   more	
      ne-     
grained	
   scoring	
   with	
   richer	
   or	
   more	
   expensive	
   
features.	
   

5.	
   	
   weighted	
   logic	
   programming	
   

logic	
   programming	
   

       start	
   with	
   a	
   set	
   of	
   axioms	
   and	
   a	
   set	
   of	
   id136	
   

rules.	
   

       the	
   goal	
   is	
   to	
   prove	
   a	
   speci   c	
   theorem,	
   goal.	
   
       many	
   approaches,	
   but	
   we	
   assume	
   a	
   deduc.ve	
   

approach.	
   
       start	
   with	
   axioms,	
   itera@vely	
   produce	
   more	
   theorems.	
   

weighted	
   logic	
   programming	
   

       twist:	
   	
   axioms	
   have	
   weights.	
   
       want	
   the	
   proof	
   of	
   goal with	
   the	
   best	
   score:	
   

arg max

y

w   g(x, y) = arg max

y

w     a axioms

f(a)freq(a; y)

       note	
   that	
   axioms	
   can	
   be	
   used	
   more	
   than	
   once	
   

in	
   a	
   proof	
   (y).	
   

whence	
   wlp?	
   

       shieber,	
   schabes,	
   and	
   pereira	
   (1995):	
   	
   many	
   
parsing	
   algorithms	
   can	
   be	
   understood	
   in	
   the	
   
same	
   deduc@ve	
   logic	
   framework.	
   

       goodman	
   (1999):	
   	
   add	
   weights,	
   get	
   many	
   

useful	
   nlp	
   algorithms.	
   

       eisner,	
   goldlust,	
   and	
   smith	
   (2004,	
   2005):	
   	
   

semiring-     generic	
   algorithms,	
   dyna.	
   

dynamic	
   programming	
   
       most	
   views	
   (excep@on	
   is	
   polytopes)	
   can	
   be	
   
understood	
   as	
   dp	
   algorithms.	
   
       the	
   low-     level	
   procedures	
   we	
   use	
   are	
   oken	
   dp.	
   
       even	
   dp	
   is	
   too	
   high-     level	
   to	
   know	
   the	
   best	
   way	
   to	
   
       dp	
   does	
   not	
   imply	
   polynomial	
   @me	
   and	
   space!	
   

implement.	
   	
   

       most	
   common	
   approxima@ons	
   when	
   the	
   desired	
   state	
   
space	
   is	
   too	
   big:	
   	
   beam	
   search,	
   cube	
   pruning,	
   agendas	
   
with	
   early	
   stopping,	
   ...	
   

       other	
   views	
   suggest	
   others.	
   

summary	
   

       decoding	
   is	
   the	
   general	
   problem	
   of	
   choosing	
   a	
   

complex	
   structure.	
   
      linguis@c	
   analysis,	
   machine	
   transla@on,	
   speech	
   
recogni@on,	
      	
   
      sta@s@cal	
   models	
   are	
   usually	
   involved	
   (not	
   
necessarily	
   probabilis@c).	
   

       no	
   perfect	
   general	
   view,	
   but	
   much	
   can	
   be	
   

gained	
   through	
   a	
   combina@on	
   of	
   views.	
   

lecture	
   4:	
   	
   supervised	
   learning	
   

quick	
   recap	
   

       graphical	
   models	
   
       id136	
   
       decoding	
   for	
   models	
   of	
   structure	
   

       finally,	
   we	
   get	
   to	
   learning.	
   

      today,	
   assume	
   a	
   collec@on	
   of	
   n	
   pairs	
   (x,	
   y);	
   
supervised	
   learning	
   with	
   complete	
   data.	
   

loss	
   

       let	
   h	
   be	
   a	
   hypothesis	
   (an	
   instan@ated,	
   predic@ve	
   
       loss(x,	
   y;	
   h)	
   =	
   a	
   measure	
   of	
   how	
   badly	
   h	
   performs	
   

model).	
   

on	
   input	
   x	
   if	
   y	
   is	
   the	
   correct	
   output.	
   

       how	
   to	
   decide	
   what	
      loss   	
   should	
   be?	
   

1.    computa@onal	
   expense	
   
2.    knowledge	
   of	
   actual	
   costs	
   of	
   errors	
   
3.   

formal	
   founda@ons	
   enabling	
   theore@cal	
   guarantees	
   

risk	
   

       there	
   is	
   some	
   true	
   distribu@on	
   p*	
   over	
   input,	
   

output	
   pairs	
   (x,	
   y).	
   

       under	
   that	
   distribu@on,	
   what	
   do	
   we	
   expect	
   

h   s	
   loss	
   to	
   be?	
   

ep (x,y )[loss(x, y ; h)]

       we	
   don   t	
   have	
   p*,	
   but	
   we	
   have	
   the	
   empirical	
   

distribu@on,	
   giving	
   empirical	
   risk:	
   
1
n

e  p(x,y )[loss(x, y ; h)] =

loss(xi, yi; h)

n i=1

empirical	
   risk	
   minimiza@on	
   

       provides	
   a	
   criterion	
   to	
   decide	
   on	
   h:	
   

1
n

min
h h

n i=1

loss(xi, yi; h)

       background	
   preferences	
   over	
   h	
   can	
   be	
   
included	
   in	
   regularized	
   empirical	
   risk	
   
minimiza@on:	
   

1
n

min
h h

n i=1

loss(xi, yi; h) + r(h)

parametric	
   assump@ons	
   

       typically	
   we	
   do	
   not	
   move	
   in	
      h-     space,   	
   but	
   

rather	
   in	
   the	
   space	
   of	
   con@nuously-     
parameterized	
   predictors.	
   

1
n

1
n

min
h h

min
w rd

n i=1
n i=1

loss(xi, yi; h) + r(h)

loss(xi, yi; hw) + r(w)

three	
   kinds	
   of	
   loss	
   func@ons	
   

       error	
   

       log	
   loss	
   

       hinge	
   loss	
   

      could	
   be	
   zero-     one,	
   or	
   task-     speci   c.	
   
      mean	
   squared	
   error	
   makes	
   sense	
   for	
   con.nuous	
   
predic@ons	
   and	
   is	
   used	
   in	
   regression.	
   

      probabilis@c	
   interpreta@on	
   (   likelihood   )	
   

      geometric	
   interpreta@on	
   (   margin   )	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

       maximum	
   likelihood	
   es@ma@on:	
   	
   	
   

loss(x, y; hw) =   log pw(x, y)
r(w)	
   is	
   0	
   for	
   models	
   in	
   the	
   family,	
   +   	
   for	
   
other	
   models.	
   
       maximum	
   a	
   posteriori	
   (map)	
   es@ma@on:	
   
       oken	
   called	
   genera6ve	
   modeling.	
   

r(w)	
   is	
      log	
   p(w)	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

examples:	
   
       n-     gram	
   language	
   models	
   
       supervised	
   id48	
   taggers	
   
       charniak,	
   collins,	
   and	
   stanford	
   parsers	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

computa@onally	
      	
   
       convex	
   and	
   di   eren@able.	
   
       closed	
   form	
   for	
   directed,	
   mul@nomial-     based	
   models	
   pw.	
   
       count	
   and	
   normalize!	
   
      
in	
   other	
   cases,	
   requires	
   posterior	
   id136,	
   which	
   can	
   be	
   
expensive	
   depending	
   on	
   the	
   model   s	
   structure.	
   
       linear	
   decoding	
   (for	
   some	
   parameteriza@ons).	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

error	
      	
   
       no	
   no@on	
   of	
   error.	
   
       learner	
   wins	
   by	
   moving	
   as	
   much	
   id203	
   

mass	
   as	
   possible	
   to	
   training	
   examples.	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)
guarantees   	
   
       consistency:	
   	
   if	
   the	
   true	
   model	
   is	
   in	
   the	
   right	
   

family,	
   enough	
   data	
   will	
   lead	
   you	
   to	
   it.	
   

log	
   loss	
   (first	
   version)	
   
n i=1

loss(xi, yi; hw) + r(w)

1
n

min
w rd

loss(x, y; hw) =   log pw(x, y)

di   erent	
   parameteriza@ons	
      	
   
       mul@nomials	
   (bn-     like):	
   

       global	
   log-     linear	
   (mn-     like):	
   

       locally	
   normalized	
   log-     linear:	
   

freq(e; x, y) log pe

  e
            we
 w   g(x, y) + log x ,y 
freq(e; x, y)    w   g(e)   log    e    c(e)

    e

exp w   g(x , y )

exp w   g(e )      

re   ec@ons	
   on	
   genera@ve	
   models	
   
       most	
   early	
   solu@ons	
   are	
   genera@ve.	
   
       most	
   unsupervised	
   approaches	
   are	
   genera@ve.	
   
       some	
   people	
   only	
   believe	
   in	
   genera@ve	
   models.	
   
       some@mes	
   es@mators	
   are	
   not	
   as	
   easy	
   as	
   they	
   

seem	
   (   de   ciency   ).	
   

       start	
   here	
   if	
   there   s	
   a	
   sensible	
   genera@ve	
   story.	
   
       you	
   can	
   always	
   use	
   a	
      beeer   	
   loss	
   func@on	
   with	
   the	
   

same	
   linear	
   model	
   later	
   on.	
   

zero-     one	
   loss	
   

loss(xi, yi; hw) + r(w)

1
n

min
w rd

n i=1

loss(x, y; hw) = 1{hw(x)  = y}

zero-     one	
   loss	
   

loss(xi, yi; hw) + r(w)

1
n

min
w rd

n i=1

loss(x, y; hw) = 1{hw(x)  = y}

computa@onally:	
   	
   	
   
       piecewise	
   constant.	
   	
   l   	
   
error:	
   	
   j   	
   
guarantees:	
   	
   none	
   

error	
   as	
   loss	
   

loss(xi, yi; hw) + r(w)

1
n

min
w rd

n i=1

loss(x, y; hw) = error(hw(x); y)

generalizes	
   zero-     one,	
   same	
   di   cul@es.	
   
example:	
   	
   id7-     score	
   maximiza@on	
   in	
   machine	
   

transla@on,	
   with	
      mert   	
   line	
   search.	
   

comparison	
   

computa@on	
   

genera6ve	
   (log	
   loss)	
   
convex	
   op@miza@on.	
    op@mizing	
   a	
   

error	
   as	
   loss	
   

error-     awareness	
    none	
   
guarantees	
   

consistency.	
   

piecewise	
   constant	
   
func@on.	
   
j   	
   
none.	
   

discrimina@ve	
   learning	
   

       various	
   loss	
   func@ons	
   between	
   log	
   loss	
   and	
   

error.	
   

       three	
   commonly	
   used	
   in	
   nlp:	
   

      condi@onal	
   log	
   loss	
   (   max	
   ent,   	
   crfs)	
   
      hinge	
   loss	
   (structural	
   id166s)	
   
      id88   s	
   loss	
   

       we   ll	
   discuss	
   each,	
   compare,	
   and	
   unify.	
   

