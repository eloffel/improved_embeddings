id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith,	
   carnegie	
   mellon	
   university	
   

	
   

2012	
   interna@onal	
   summer	
   school	
   in	
   
language	
   and	
   speech	
   technologies	
   	
   

where	
   we	
   led	
   o   	
   

       graphical	
   models	
      	
   id136	
   ...	
      max   	
   

id136	
   and	
   decoding	
   with	
   linear	
   models	
   
(   ve	
   views).	
   

       supervised	
   learning:	
   

      log	
   loss	
   
      error	
   as	
   loss	
   

comparison	
   

computa@on	
   

genera&ve	
   (log	
   loss)	
   
convex	
   op@miza@on.	
    op@mizing	
   a	
   

error	
   as	
   loss	
   

error-     awareness	
    none	
   
guarantees	
   

consistency.	
   

piecewise	
   constant	
   
func@on.	
   
j   	
   
none.	
   

discrimina@ve	
   learning	
   

       various	
   loss	
   func@ons	
   between	
   log	
   loss	
   and	
   

error.	
   

       three	
   commonly	
   used	
   in	
   nlp:	
   

      condi@onal	
   log	
   loss	
   (   max	
   ent,   	
   crfs)	
   
      hinge	
   loss	
   (structural	
   id166s)	
   
      id88   s	
   loss	
   

       we   ll	
   discuss	
   each,	
   compare,	
   and	
   unify.	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)
       can	
   be	
   understood	
   as	
   a	
   genera@ve	
   model	
   over	
   

y,	
   but	
   does	
   not	
   model	
   x.	
   
          condi@onal   	
   model	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

examples:	
   
       logis@c	
   regression	
   (for	
   classi   ca@on)	
   
       memms	
   
       crfs	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

computa@onally	
      	
   
       convex	
   and	
   di   eren@able.	
   
       requires	
   posterior	
   id136,	
   which	
   can	
   be	
   

expensive	
   depending	
   on	
   the	
   model   s	
   structure.	
   
       linear	
   decoding	
   (for	
   some	
   parameteriza@ons).	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

error	
      	
   
       no	
   no@on	
   of	
   error.	
   
       learner	
   wins	
   by	
   moving	
   as	
   much	
   id203	
   

mass	
   as	
   possible	
   to	
   training	
   examples   	
   correct	
   
outputs.	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)
guarantees   	
   
       consistency:	
   	
   if	
   the	
   true	
   condi@onal	
   model	
   is	
   
in	
   the	
   right	
   family,	
   enough	
   data	
   will	
   lead	
   you	
   
to	
   it.	
   

log	
   loss	
   (second	
   version)	
   

1
n

min
w rd

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =   log pw(y | x)

di   erent	
   parameteriza@ons	
      	
   
       global	
   log-     linear	
   (crf):	
   

       locally	
   normalized	
   log-     linear	
   (memm):	
   

 w   g(x, y) + log y 
freq(e; x, y)    w   g(e)   log    e    c(e)

    e

exp w   g(x , y )

exp w   g(e )      

comparing	
   the	
   two	
   log	
   losses	
   

parameteriza@on	
   

-     log	
   pw(x,	
   y)	
   

usually	
   
mul@nomials	
   (bn-     
like).	
   

-     log	
   pw(y	
   |	
   x)	
   
almost	
   always	
   log-     
linear	
   (mn-     like).	
   

under	
   the	
   usual	
   parameteriza@on	
      	
   
computa@on	
   

count	
   and	
   
normalize.	
   
none.	
   

error-     awareness	
   

convex	
   
op@miza@on.	
   
aware	
   of	
   the	
   y-     
predic@on	
   task,	
   
(approximates	
   zero-     
one).	
   

guarantees	
   

consistency	
   of	
   joint.	
    consistency	
   of	
   cond.	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

w   g(x, y ) + error(y , y)

y 

loss(x, y; hw) =  w   g(x, y) + max
       penalizes	
   the	
   model	
   for	
   lebng	
   compe@tors	
   
get	
   close	
   (in	
   terms	
   of	
   score)	
   to	
   the	
   correct	
   
answer	
   y.	
   
      can	
   penalize	
   them	
   in	
   propor@on	
   to	
   their	
   error.	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =  w   g(x, y) + max
examples	
      	
   
       id88	
   (including	
   collins   	
   structured	
   version)	
   

w   g(x, y ) + error(y , y)

y 

       classic	
   version	
   ignores	
   error	
   term	
   

       id166	
   and	
   some	
   structured	
   variants:	
   

       max-     margin	
   markov	
   networks	
   (taskar	
   et	
   al.)	
   
       mira	
   (1-     best,	
   k-     best)	
   

	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

w   g(x, y ) + error(y , y)

y 

loss(x, y; hw) =  w   g(x, y) + max
computa@onally	
      	
   
       convex,	
   not	
   everywhere	
   di   eren@able.	
   
       many	
   specialized	
   techniques	
   now	
   available.	
   
       requires	
   map	
   or	
      cost-     augmented   	
   map	
   
       linear	
   decoding.	
   
	
   

id136.	
   

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

y 

w   g(x, y ) + error(y , y)

loss(x, y; hw) =  w   g(x, y) + max
error	
      	
   
       built	
   in.	
   
       most	
   convenient	
   when	
   error	
   func@on	
   factors	
   

similarly	
   to	
   features	
   g.	
   

	
   

w   g(x, y ) + error(y , y)

hinge	
   loss	
   

min
w rd

1
n

n i=1

loss(xi, yi; hw) + r(w)

loss(x, y; hw) =  w   g(x, y) + max
guarantees	
      	
   
       generaliza@on	
   bounds.	
   

y 

      not	
   clear	
   how	
   seriously	
   to	
   take	
   these	
   in	
   nlp;	
   may	
   
not	
   be	
   @ght	
   enough	
   to	
   be	
   meaningful.	
   

       oden	
   you	
   will	
      nd	
   convergence	
   guarantees	
   

for	
   op@miza@on	
   techniques.	
   

	
   
	
   

they	
   are	
   all	
   related	
   
exp     w    (g(x, y )   g(x, y)) +    error(y , y)      

1
 

log   y 

condi@onal	
   log	
   loss	
   
id88   s	
   hinge	
   loss	
   
structural	
   id166   s	
   hinge	
   loss	
   
sodmax-     margin	
   (gimpel	
   and	
   smith,	
   2010)	
   

  	
   
1	
   
   	
   
   	
   
1	
   

  	
   
0	
   
0	
   
>	
   0	
   
1	
   

crfs,	
   max	
   margin,	
   or	
   id88?	
   
       for	
   supervised	
   problems,	
   we	
   do	
   not	
   expect	
   

large	
   di   erences.	
   

       id88	
   is	
   easiest	
   to	
   implement.	
   

      with	
   cost-     augmented	
   id136,	
   it	
   should	
   get	
   
beher	
   and	
   begins	
   to	
   approach	
   mira	
   and	
   m3ns.	
   

       crfs	
   are	
   best	
   for	
   id203	
   fe@shists.	
   

      probably	
   most	
   appropriate	
   if	
   you	
   are	
   extending	
   
with	
   latent	
   variables;	
   the	
   jury	
   is	
   out.	
   

       not	
   yet	
      plug	
   and	
   play..   	
   

r(w)	
   

       regulariza@on	
   term	
      	
   avoid	
   over   bng	
   

      usually	
   means	
      avoid	
   large	
   magnitudes	
   in	
   w   	
   
	
   

       (log)	
   prior	
      	
   respect	
   background	
   beliefs	
   about	
   

the	
   predictor	
   hw	
   

r(w)	
   

       usual	
   star@ng	
   point:	
   	
   squared	
   l2	
   norm	
   

      computa@onally	
   convenient	
   (it   s	
   strongly	
   convex,	
   
it	
   is	
   its	
   own	
   fenchel	
   conjugate,	
      )	
   
      probabilis@c	
   view:	
   	
   gaussian	
   prior	
   on	
   weights	
   
(chen	
   and	
   rosenfeld,	
   2000)	
   
      geometric	
   view:	
   	
   euclidean	
   distance	
   	
   
(original	
   regulariza@on	
   method	
   in	
   id166s)	
   
      only	
   one	
   hyperparameter	
   
r(w) =   w 2

w2
j

2 =   j

r(w)	
   
       another	
   op@on:	
   	
   l1-     norm	
   

      computa@onally	
   less	
   convenient	
   (not	
   everywhere	
   
di   eren@able)	
   
      probabilis@c	
   view:	
   	
   laplacian	
   prior	
   on	
   weights	
   
(originally	
   proposed	
   as	
      lasso   	
   in	
   regression)	
   
      sparsity	
   inducing	
   (   free   	
   feature	
   selec@on)	
   

r(w) =   w 1 =   j

|wj|

r(w)	
   

       lots	
   of	
   ahen@on	
   to	
   this	
   in	
   machine	
   learning.	
   
          structured	
   sparsity   	
   

      want	
   groups	
   of	
   features	
   to	
   go	
   to	
   zero,	
   or	
   group-     
internal	
   sparsity,	
   or	
      	
   

       interpola@on	
   between	
   l1	
   and	
   l2	
      	
      elas@c	
   
net   	
   
      sparsity	
   but	
   maybe	
   beher	
   behaved	
   

       this	
   is	
   not	
   yet	
      plug	
   and	
   play.   	
   

      op@miza@on	
   algorithm	
   is	
   heavily	
   a   ected.	
   	
   

map	
   learning	
   is	
   id136	
   

       seeking	
      most	
   probable	
   explana@on   	
   of	
   the	
   

data,	
   in	
   terms	
   of	
   w.	
   
      explain	
   the	
   data:	
   	
   p(x,	
   y	
   |	
   w)	
   
      not	
   too	
   surprising:	
   	
   p(w)	
   

       if	
   we	
   think	
   of	
      w   	
   as	
   another	
   random	
   

variable,	
   map	
   learning	
   is	
   map	
   id136.	
   
      looks	
   very	
   di   erent	
   from	
   decoding!	
   
      but	
   at	
   a	
   high	
   level	
   of	
   abstrac@on,	
   it	
   is	
   the	
   same.	
   

map	
   learning	
   as	
   a	
   graphical	
   

model	
   

exp    r(w) 
= p(w) 

pw(y) 

r	
   

w	
   

y	
   

pw(x | y) 

x	
   

       this	
   is	
   a	
   view	
   of	
   learning	
   a	
      noisy	
   channel   	
   

model.	
   

map	
   learning	
   as	
   a	
   graphical	
   

exp    r(w) 
= p(w) 

model	
   
pw(y | x) 

r	
   

w	
   

y	
   

x	
   

       this	
   is	
   a	
   view	
   of	
   learning	
   in	
   a	
   crf.	
   

map	
   es@ma@on	
   for	
   crfs	
   

maxw	
   p(w	
   |	
   x,	
   y),	
   which	
   is	
   map	
   id136	
   

iterate	
   to	
   obtain	
   gradient:	
   
	
   

su   cient	
   sta@s@cs	
   from	
   p(y	
   |	
   x,	
   w),	
   obtained	
   by	
   
posterior	
   id136	
   

how	
   to	
   think	
   about	
   op@miza@on	
   
       depending	
   on	
   your	
   choice	
   of	
   loss	
   and	
   r,	
   
di   erent	
   approaches	
   become	
   available.	
   
      learning	
   algorithms	
   can	
   interact	
   with	
   id136/
decoding	
   algorithms,	
   too.	
   

       in	
   nlp	
   today,	
   it	
   is	
   probably	
   more	
   important	
   to	
   

focus	
   on	
   the	
   features,	
   error	
   func@on,	
   and	
   
prior	
   knowledge.	
   
      decide	
   what	
   you	
   want,	
   and	
   then	
   use	
   the	
   best	
   
available	
   op@miza@on	
   technique.	
   

key	
   techniques	
   

       quasi-     newton	
      	
   batch	
   method	
   for	
   di   eren@able	
   

loss	
   func@ons	
   
       lbfgs,	
   owlqn	
   when	
   using	
   l1	
   regulariza@on	
   	
   

       stochas@c	
   subgradient	
   ascent	
      	
   online	
   

ascent	
   

       generalizes	
   id88,	
   mira,	
   stochas@c	
   gradient	
   
       some@mes	
   sensi@ve	
   to	
   step	
   size	
   
       can	
   oden	
   use	
      mini-     batches   	
   to	
   speed	
   up	
   
       for	
   error	
   minimiza@on:	
   	
   randomiza@on	
   

convergence	
   

pipalls	
   

       engineering	
   online	
   learning	
   procedures	
   is	
   

temp@ng	
   and	
   may	
   help	
   you	
   get	
   beher	
   
performance.	
   
       without	
   at	
   least	
   some	
   analysis	
   in	
   terms	
   of	
   loss,	
   error,	
   
and	
   regulariza@on,	
   it   s	
   unlikely	
   to	
   be	
   useful	
   outside	
   
your	
   problem/dataset.	
   

       when	
   randomiza@on	
   is	
   involved,	
   look	
   at	
   variance	
   
       always	
   tune	
   hyperparameters	
   (e.g.,	
   

across	
   runs	
   (clark	
   et	
   al.,	
   2011)	
   

regulariza@on	
   strength)	
   on	
   development	
   data!	
   

major	
   topics	
   in	
   current	
   work	
   

       coping	
   with	
   approximate	
   id136	
   
       exploi@ng	
   incomplete	
   data	
   

      semisupervised	
   learning	
   
      crea@ng	
   features	
   from	
   raw	
   text	
   
      latent	
   variable	
   models	
   (discussed	
   tomorrow)	
   

       feature	
   management	
   
      structured	
   sparsity	
   (r)	
   

