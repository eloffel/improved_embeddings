id203	
   and	
   structure	
   in	
   
natural	
   language	
   processing	
   

noah	
   smith,	
   carnegie	
   mellon	
   university	
   

	
   

2012	
   interna@onal	
   summer	
   school	
   in	
   
language	
   and	
   speech	
   technologies	
   	
   

(logical)	
   lecture	
   5:	
   	
   	
   
hidden	
   variables	
   

random	
   variables	
   in	
   decoding	
   

parameters	
   (w)	
   

inputs	
   (x)	
   

outputs	
   (y)	
   

random	
   variables	
   in	
   	
   
supervised	
   learning	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

hidden	
   variables	
   are	
   di   erent	
   

       we	
   use	
   the	
   term	
      hidden	
   variable   	
   (or	
      latent	
   
variable   )	
   to	
   refer	
   to	
   something	
   we	
   never	
   see.	
   
      not	
   even	
   in	
   training.	
   
      some@mes	
   we	
   believe	
   they	
   are	
   real.	
   
      some@mes	
   we	
   believe	
   they	
   only	
   approximate	
   
reality.	
   

random	
   variables	
   in	
   decoding	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

latent	
   (z)	
   

random	
   variables	
   in	
   	
   
supervised	
   learning	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

latent	
   (z)	
   

latent	
   variables	
   and	
   id136	
   
       both	
   learning	
   and	
   decoding	
   can	
   be	
   s@ll	
   be	
   

understood	
   as	
   id136	
   problems.	
   

       usually	
      mixed   :	
   

      some	
   variables	
   are	
   gerng	
   maximized	
   
      some	
   variables	
   are	
   gerng	
   summed	
   	
   

word	
   alignments	
   

prototypical	
   hidden	
   variable.	
   

       since	
   ibm	
   model	
   1,	
   word	
   alignments	
   have	
   been	
   the	
   
       ul@mately,	
   in	
   transla@on,	
   we	
   do	
   not	
   care	
   what	
   they	
   
       current	
   approach:	
   	
   learn	
   the	
   word	
   alignments	
   

are.	
   

unsupervised,	
   then	
      x	
   them	
   to	
   their	
   most	
   likely	
   values.	
   
       then	
   construct	
   models	
   for	
   transla@on.	
   
       alignment	
   on	
   its	
   own:	
   	
   unsupervised	
   problem.	
   
       mt	
   on	
   its	
   own:	
   	
   supervised	
   problem.	
   
       mt	
   +	
   alignment:	
   	
   supervised	
   problem	
   with	
   latent	
   

variables.	
   

alignments	
   in	
   text-     to-     text	
   problems	
   
       wang	
   et	
   al.	
   (2007):	
   	
      jeopardy   	
   model	
   for	
   

answer	
   ranking	
   in	
   qa.	
   
      align	
   ques@ons	
   to	
   answers.	
   
      similar	
   model	
   for	
   paraphrase	
   detec@on	
   (das	
   and	
   
smith,	
   2009)	
   

latent	
   annota@ons	
   in	
   parsing	
   

       treebank	
   categories	
   (n,	
   nn,	
   np,	
   etc.)	
   are	
   too	
   

coarse-     grained.	
   
      lexicaliza@on	
   (collins,	
   eisner)	
   
      johnson   s	
   (1998)	
   parent	
   annota@on	
   
      klein	
   and	
   manning	
   (2003)	
   parser	
   

       treat	
   the	
   true,	
      ne-     grained	
   category	
   as	
   

hidden,	
   and	
   infer	
   it	
   from	
   data.	
   
      matsuzaki,	
   petrov,	
   dreyer,	
   many	
   others.	
   

richer	
   formalisms	
   

       cohn	
   et	
   al.	
   (2009):	
   	
   tree	
   subs@tu@on	
   grammar.	
   

      derived	
   tree	
   is	
   observed	
   (output	
   variable).	
   
      deriva@on	
   tree	
   (segmenta@on	
   into	
   elementary	
   
trees)	
   is	
   hidden.	
   

       zedlemoyer	
   and	
   collins	
   (2005	
   and	
   later):	
   	
   infer	
   
id35	
   syntax	
   from	
      rst-     order	
   logical	
   expressions	
   
and	
   sentences.	
   

       liang	
   et	
   al.	
   (2011):	
   	
   infer	
   seman@c	
   

representa@on	
   from	
   text	
   and	
   database.	
   

topic	
   models	
   

       infer	
   topics	
   (or	
   topic	
   blends)	
   in	
   documents.	
   
       latent	
   dirichlet	
   alloca@on	
   (blei	
   et	
   al.,	
   2003)	
   is	
   

a	
   great	
   example.	
   
      some@mes	
   augmented	
   with	
   an	
   output	
   variable	
   
(blei	
   and	
   mcauli   e,	
   2007)	
      	
      supervised   	
   lda.	
   
      many	
   extensions!	
   

unsupervised	
   nlp	
   

       id91	
   (brown,	
   1992,	
   many	
   more)	
   
       pos	
   tagging	
   (merialdo,	
   1994,	
   many	
   more)	
   
       parsing	
   (pereira	
   and	
   schabes,	
   klein	
   and	
   manning,	
      )	
   
       segmenta@on	
   (word	
      	
   goldwater;	
   discourse	
      	
   
eisenstein)	
   
       morphology	
   
       lexical	
   seman@cs	
   
       syntax-     seman@cs	
   correspondences	
   
       sen@ment	
   analysis	
   
       coreference	
   resolu@on	
   
       word,	
   phrase,	
   and	
   tree	
   alignment	
   
	
   

supervised	
   or	
   unsupervised?	
   

       depends	
   on	
   the	
   task,	
   not	
   the	
   model.	
   

      i	
   say	
      unsupervised   	
   when	
   the	
   output	
   variables	
   
are	
   hidden	
   at	
   training	
   @me.	
   

random	
   variables	
   	
   

in	
   unsupervised	
   learning	
   

parameters	
   (w)	
   

outputs	
   (y)	
   

inputs	
   (x)	
   

latent	
   (z)	
   

probabilis@c	
   view	
   

       the	
   usual	
   star@ng	
   point	
   for	
   hidden	
   variables	
   is	
   

maximum	
   likelihood.	
   
          input   	
   and	
      output   	
   do	
   not	
   mader;	
   only	
   
observed/latent.	
   

random	
   variables	
   	
   

in	
   probabilis@c	
   learning	
   

parameters	
   (w)	
   

visible	
   (v)	
   

latent	
   (l)	
   

empirical	
   risk	
   view	
   

min
w rd

1

n  i

loss(vi; hw) + r(w)

loss(v; hw) =   log pw(v)
=   log  

pw(v,  )

       log-     loss	
   

is	
   a	
   negated	
   log	
   prior).	
   

       equates	
   to	
   maximum	
   marginal	
   likelihood	
   (or	
   map	
   if	
   r(w)	
   
       unlike	
   loss	
   func@ons	
   in	
   lecture	
   4,	
   this	
   is	
   not	
   convex!	
   
       em	
   seeks	
   to	
   solve	
   this	
   problem	
   (but	
   it   s	
   not	
   the	
   only	
   way).	
   
       regulariza@on	
   decisions	
   are	
   orthogonal.	
   

op@mizing	
   the	
   marginal	
   log-     loss	
   
       em	
   as	
   id136	
   
       em	
   as	
   op@miza@on	
   
       direct	
   op@miza@on	
   

generic	
   em	
   algorithm	
   
       input:	
   	
   w(0)	
   and	
   observa@ons	
   v1,	
   v2,	
      	
   vn	
   
       output:	
   	
   learned	
   w	
   
       t	
   =	
   0	
   
       repeat	
   un@l	
   w(t)	
      	
   w(t-     1):	
   

       e	
   step:	
   
       m	
   step:	
   
       ++t	
   	
   	
   

       return	
   w(t)	
   

   i,    ,

q(t)
i ( )   pw(t)(  | vi)
w  i   
q(t)
i ( ) log pw(vi,  )

w(t+1)   max

map	
   learning	
   as	
   a	
   graphical	
   

model	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

       combined	
   id136	
   (max	
   over	
   w,	
   sum	
   over	
   l)	
   is	
   very	
   
hard.	
   
       if	
   w	
   were	
      xed,	
   gerng	
   the	
   posterior	
   over	
   l	
   wouldn   t	
   be	
   
       if	
   l	
   were	
      xed,	
   maximizing	
   over	
   w	
   wouldn   t	
   be	
   so	
   bad.	
   

so	
   bad.	
   

map	
   learning	
   as	
   a	
   graphical	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

model	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

pw(v	
   |	
   l)	
   

v	
   

m	
   step	
   

e	
   step	
   
	
   
	
   

baum-     welch	
   (em	
   for	
   id48s)	
   

as	
   an	
   example	
   

       e	
   step:	
   	
   forward-     backward	
   algorithm	
   (on	
   each	
   

elimina@on.	
   

example).	
   
       this	
   is	
   exact	
   marginal	
   id136	
   by	
   variable	
   
       the	
   structure	
   of	
   the	
   graphical	
   model	
   lets	
   us	
   do	
   this	
   by	
   
       the	
   marginals	
   are	
   probabili@es	
   of	
   transi@on	
   and	
   

dynamic	
   programming.	
   

emission	
   events	
   at	
   each	
   posi@on.	
   

       m	
   step:	
   	
   id113	
   based	
   on	
   soq	
   event	
   counts.	
   

       rela@ve	
   frequency	
   es@ma@on	
   accomplishes	
   id113	
   for	
   

mul@nomials.	
   

baum-     welch	
   as	
   a	
   graphical	
   model	
   

parameters	
   (w)	
   

r	
   

emit	
   

transit	
   

x1	
   

x2	
   

x3	
   

visible	
   (v)	
   

xn	
   

y1	
   

y2	
   

y3	
   

yn	
   

latent	
   (l)	
   

ac@ve	
   trail!	
   

parameters	
   (w)	
   

r	
   

emit	
   

transit	
   

x1	
   

x2	
   

x3	
   

visible	
   (v)	
   

xn	
   

y1	
   

y2	
   

y3	
   

yn	
   

latent	
   (l)	
   

no	
   ac@ve	
   trail	
   in	
   all-     visible	
   case	
   

parameters	
   (w)	
   

r	
   

emit	
   

transit	
   

x1	
   

x2	
   

x3	
   

visible	
   (v)	
   

xn	
   

y1	
   

y2	
   

y3	
   

yn	
   

visible	
   (v)	
   

why	
   latent	
   variables	
   	
   
make	
   learning	
   hard	
   
       new	
   intui@on:	
   	
   parameters	
   are	
   now	
   

interdependent	
   that	
   were	
   not	
   interdependent	
   
in	
   the	
   fully-     visible	
   case.	
   

       it	
   all	
   goes	
   back	
   to	
   ac@ve	
   trails.	
   

   viterbi   	
   learning	
   

exp	
      r(w)	
   
=	
   p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

       approximate	
   joint	
   map	
   id136	
   over	
   w	
   and	
   

l	
   (most	
   probable	
   explana@on	
   id136).	
   
log pw(v,  )

       loss	
   func@on:	
   

loss(v; hw) =   max

 

condi@onal	
   models	
   

       em	
   is	
   usually	
   closely	
   associated	
   with	
   fully	
   

genera@ve	
   approaches.	
   

       you	
   can	
   do	
   the	
   same	
   things	
   with	
   log-     linear	
   

models	
   and	
   with	
   condi@onal	
   models.	
   
      locally	
   normalized	
   models	
   give	
      exibility	
   without	
   
requiring	
   global	
   id136	
   (eisner,	
   2002).	
   
      hidden	
   variable	
   crfs	
   (quadoni	
   et	
   al.,	
   2007)	
   are	
   
very	
   powerful.	
   

	
   

learning	
   condi@onal	
   	
   
hidden	
   variable	
   models	
   

vin	
   

distribu@on	
   over	
   vin	
   is	
   not	
   modeled	
   

vin	
   

r	
   

w	
   

l	
   

r	
   

w	
   

vout	
   

vout	
   

standard	
   condi@onal	
   model	
   (e.g.,	
   crf)	
   

op@miza@on	
   for	
   hidden	
   variables	
   
       we   ve	
   described	
   hidden	
   variable	
   learning	
   as	
   

id136	
   problems.	
   

       it	
   is	
   more	
   prac@cal,	
   of	
   course,	
   to	
   think	
   about	
   

this	
   as	
   op.miza.on.	
   

       em	
   can	
   be	
   understood	
   from	
   an	
   op@miza@on	
   

framework,	
   as	
   well.	
   

em	
   and	
   likelihood	
   

 (w) =  i

log  

pw(vi,  )

       the	
   connec@on	
   between	
   the	
   goal	
   above	
   and	
   
the	
   em	
   procedure	
   is	
   not	
   immediately	
   clear.	
   

	
   

op@miza@on	
   view	
   of	
   em	
   

qi( ) log qi( ) +    

qi( ) log pw(  | vi) + log pw(vi)   

   i       
       a	
   func@on	
   of	
   w	
   and	
   the	
   collec@on	
   of	
   qi.	
   
       claim:	
   	
   em	
   performs	
   coordinate	
   ascent	
   on	
   this	
   

func@on.	
   

op@miza@on	
   view	
   of	
   em	
   

   i       

qi( ) log qi( ) +    

 (w)

qi( ) log pw(  | vi) + log pw(vi)   

       the	
   third	
   term	
   is	
   our	
   actual	
   goal,	
     .	
   	
   it	
   only	
   

depends	
   on	
   w	
   (not	
   the	
   qi).	
   	
   

   i       

op@miza@on	
   view	
   of	
   em	
   

 (w)

qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       the	
   lader	
   two	
   terms	
   together	
   are	
   precisely	
   what	
   
we	
   maximize	
   on	
   the	
   m	
   step,	
   given	
   the	
   current	
   qi.	
   
       this	
   is	
   a	
   concave	
   problem	
   and	
   we	
   solve	
   it	
   exactly.	
   

   i       

op@miza@on	
   view	
   of	
   em	
   

 (w)

qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       concern:	
   	
   is	
   the	
   m	
   step	
   improving	
   term	
   2	
   at	
   

the	
   expense	
   of	
     	
   (term	
   3)?	
   
      no.	
   

q(t)
i ( ) log pw(  | vi)

q(t)

the	
   m	
   step	
   
i ( ) log pw(vi,  )   i   

itera@on	
   to	
   itera@on:	
   

 (w) =  i   
       last	
   term	
   is	
   also	
   not	
   gerng	
   any	
   worse	
   from	
   
  i   
=   i   
=  i
    0

i ( ) log pw(t+1)(  | vi) + i   
i (  )   pw(t+1)(   | vi))

i ( ) log pw(t+1)(  | vi) + i   

d(q(t)

q(t)

q(t)

q(t)
i ( ) log pw(t)(  | vi)
q(t)
i ( ) log q(t)
i ( )

the	
   m	
   step	
   

       each	
   m	
   step,	
   once	
   qi	
   is	
      xed,	
   maximizes	
   a	
   
bound	
   on	
   the	
   log-     likelihood	
     .	
   
      for	
      xed	
   qi,	
   this	
   is	
   a	
   concave	
   problem	
   we	
   can	
   
solve	
   in	
   closed	
   form	
   in	
   many	
   cases.	
   

       what	
   about	
   the	
   e	
   step?	
   

op@miza@on	
   view	
   of	
   em	
   

 (w)

 d(qi(  )   pw(   | vi))
   i       
qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       e	
   step	
   considers	
   the	
      rst	
   two	
   terms.	
   
       sets	
   each	
   qi	
   to	
   be	
   equal	
   to	
   the	
   posterior	
   under	
   
the	
   current	
   model.	
   	
   

coordinate	
   ascent	
   

 d(qi(  )   pw(   | vi))
   i       
qi( ) log pw(  | vi) + log pw(vi)   
qi( ) log qi( ) +    
  
qi( ) log pw(vi,  )

       e	
   step	
      xes	
   w	
   and	
   solves	
   for	
   the	
   qi.	
   
       m	
   step	
      xes	
   all	
   qi	
   and	
   solves	
   for	
   w.	
   

things	
   people	
   forget	
   about	
   em	
   

       mul@ple	
   random	
   starts	
   (or	
   non-     random	
   

starts),	
   select	
   using	
   likelihood	
   on	
   development	
   
data.	
   

       variants	
   may	
   help	
   avoid	
   local	
   op@ma	
      	
   
	
   

variants	
   of	
   em	
   

          online   	
   variants	
   where	
   we	
   do	
   an	
   e	
   step	
   on	
   

one	
   or	
   a	
   mini-     batch	
   of	
   examples	
   are	
   s@ll	
   
coordinate	
   ascent	
   (neal	
   and	
   hinton,	
   1998).	
   
       determinis@c	
   annealing:	
   	
      aden	
   out	
   the	
   qi,	
   
making	
   the	
   func@on	
   closer	
   to	
   concave.	
   
       stochas@c	
   variant:	
   	
   use	
   randomized	
   
approximate	
   id136	
   for	
   e	
   step.	
   
          generalized   	
   em:	
   	
   improve	
   w	
   but	
   don   t	
   

bother	
   op@mizing	
   completely.	
   

direct	
   op@miza@on	
   

       an	
   alterna@ve	
   to	
   em:	
   	
   apply	
   stochas@c	
   

gradient	
   ascent	
   or	
   quasi-     newton	
   methods	
   
directly	
   to	
     .	
   

       typically	
   done	
   for	
   mn-     like	
   models	
   with	
   

features,	
   e.g.,	
   latent-     variable	
   crfs.	
   
      gradient	
   is	
   a	
   di   erence	
   of	
   feature	
   expecta@ons.	
   
      requires	
   marginal	
   id136.	
   

summary	
   

       em:	
   	
   many	
   ways	
   to	
   understand	
   it.	
   	
   

      the	
   guarantee:	
   	
   each	
   round	
   will	
   improve	
   the	
   
likelihood.	
   
      that   s	
   about	
   as	
   much	
   as	
   we	
   can	
   say.	
   

       some@mes	
   it	
   works.	
   

      smart	
   ini@alizers	
   
      lots	
   of	
   bias	
   inherent	
   in	
   the	
   model	
   structure/
assump@ons	
   

(logical)	
   lecture	
   6:	
   
bayesian	
   approach	
   

outline	
   

       de   ning	
      bayesian   	
   
       parametric	
   bayesian	
   models	
   

      latent	
   dirichlet	
   alloca@on	
   (blei	
   et	
   al.,	
   2003)	
   
      bayesian	
   id48	
   (goldwater	
   and	
   gri   ths,	
   2007)	
   

       a	
   lidle	
   bit	
   about	
   id136	
   

in	
   sta@s@cs	
      	
   

       beder	
   to	
   assign	
   f.	
   and	
   b.	
   to	
   analyses,	
   not	
   people.	
   
       frequen/st	
   analysis	
   (most	
   of	
   science	
   today):	
   	
   parameters	
   
are	
      xed	
   and	
   unknown;	
   we	
   gain	
   informa@on	
   by	
   repeated	
   
experiments	
   
       point	
   es@mates,	
   standard	
   errors,	
   con   dence	
   intervals	
   (   in	
   p%	
   of	
   

experiments,	
   the	
   interval	
   will	
   cover	
   the	
   true	
        ),	
   hypothesis	
   
tests	
   with	
     	
      xed	
   in	
   advance,	
   reason	
   about	
   p(data	
   |	
   h0)	
   

       bayesian	
   analysis:	
   	
   treat	
   unknown	
   parameters	
   

probabilis@cally;	
   update	
   beliefs	
   as	
   evidence	
   arrives	
   
       start	
   with	
   p(  )	
   and	
   infer	
   p(  	
   |	
   data),	
   means	
   and	
   quan@les	
   of	
   the	
   
posterior	
   over	
     ,	
   intervals	
   corresponding	
   to	
      p%	
   belief   	
   that	
     	
   is	
   
in	
   the	
   interval	
   

the	
   adrac@on	
   of	
   bayesian	
   thinking	
   
       write	
   down	
   your	
   model	
   declara@vely,	
   worry	
   later	
   
about	
   how	
   to	
      t	
   it	
   from	
   data.	
   
       prior	
   encodes	
   prior	
   knowledge.	
   

least	
   we	
   think	
   we	
   do!	
   

       we	
   have	
   lots	
   of	
   this	
   when	
   it	
   comes	
   to	
   language,	
   or	
   at	
   
       manage	
   uncertainty	
   about	
   the	
   model	
   the	
   same	
   
       bayesian	
   methods	
   are	
   strongly	
   associated	
   with:	
   

way	
   we	
   manage	
   uncertainty	
   about	
   the	
   data.	
   

       unsupervised	
   (and	
   latent	
   variable)	
   learning	
   
       genera@ve	
   models	
   

evolving	
   de   ni@ons	
   

       id113	
   (not	
   bayesian):	
   
       maximum	
   a	
   posteriori	
   es@ma@on:	
   

       compu@ng	
   the	
   posterior	
   over	
   the	
   parameters	
   

(fully	
   bayesian):	
   

       empirical	
   bayesian:	
   

map	
   learning	
   as	
   a	
   graphical	
   

model	
   

p(w)	
   

pw(l)	
   

r	
   

w	
   

l	
   

pw(v	
   |	
   l)	
   

v	
   

       combined	
   id136	
   (max	
   over	
   w,	
   sum	
   over	
   l)	
   is	
   very	
   hard.	
   
       if	
   w	
   were	
      xed,	
   gerng	
   the	
   posterior	
   over	
   l	
   wouldn   t	
   be	
   so	
   bad.	
   
       if	
   l	
   were	
      xed,	
   maximizing	
   over	
   w	
   wouldn   t	
   be	
   so	
   bad.	
   
          standard	
   em   	
   doesn   t	
   have	
   p(w);	
   it   s	
   very	
   simple	
   to	
   add	
   

and	
   useful	
   in	
   prac@ce.	
   

id113	
   

w	
   

l	
   

r	
   

w	
   

l	
   

map	
   

v	
   

v	
   

r	
   

w	
   

l	
   

fully	
   
bayesian	
   

v	
   

r	
   

w	
   

l	
   

empirical	
   bayesian	
   

v	
   

mul@nomials	
   

       let   s	
   assume	
   discrete	
   distribu@ons	
   that	
   
simply	
   assign	
   probabili@es	
   to	
      nite	
   sets	
   of	
   
events.	
   
      n-     gram	
   models,	
   id48s,	
   pid18s,	
      	
   

distribu@ons	
   over	
   mul@nomials	
   

       you	
   can	
   think	
   of	
   a	
   mul@nomial	
   distribu@on	
   over	
   d	
   

events	
   as	
   a	
   point	
   in	
   the	
   (d-     1)	
   simplex.	
   

[1,	
   0,	
   0,	
   0]	
   

[0,	
   0,	
   0,	
   1]	
   

[0,	
   1,	
   0,	
   0]	
   

[0,	
   0,	
   1,	
   0]	
   

       to	
   randomly	
   pick	
   a	
   point	
   in	
   this	
   space,	
   we	
   need	
   a	
   

con/nuous	
   distribu@on	
   over	
   the	
   simplex.	
   

dirichlet	
   distribu@on	
   

       a	
   distribu@on	
   over	
   the	
   d-     
event	
   id203	
   simplex.	
   

       parameters:	
   	
     ,	
   the	
   mean	
   of	
   
the	
   dirichlet,	
   and	
     ,	
   the	
   
concentra@on	
   around	
   that	
   
mean	
   (large	
     	
   means	
   
smaller	
   variance).	
   

       beta	
   func@on:	
   
       gamma	
   func@on	
   

(generalized	
   factorial):	
   

p(  |  ,    ) =

1

b(    )

       i 1
i

d i=1

 (    i)

 ( )

b(    ) =

d i=1
 (a) =        

0

ta 1e adt

dirichlet,	
   d=3	
   

(various	
   parameter	
   serngs)	
   

from	
   answers.com	
   

dirichlet,	
   d=	
   3	
   

(di   erent	
      means   	
   and	
      variances   )	
   

       from	
   liang	
   and	
   klein,	
   2007	
   

sampling	
   from	
   a	
   dirichlet	
   

       for	
   i	
   from	
   1	
   to	
   do,	
   sample	
   vi	
   from	
   a	
   gamma	
   
distribu@on	
   with	
   shape	
       i	
   and	
   scale	
   1.	
   
       renormalize	
   the	
   vector	
   v	
   to	
   obtain	
     .	
   	
   	
   

map	
   with	
   a	
   dirichlet	
   

       recall	
   that	
   we	
   can	
   use	
   a	
   prior	
   to	
      smooth   	
   an	
   es@mate.	
   
       for	
   a	
   mul@nomial	
     	
   with	
   dirichlet	
   prior	
       	
   >	
   1,	
   this	
   equates	
   
to	
   adding	
   pseudocounts	
   to	
   the	
   vector	
   of	
   observed	
   counts.	
   
     i = ni +     i   1
n +     d
d i=1

       as	
   counts	
   become	
   large,	
   prior	
   maders	
   less.	
   
       closed	
   form!	
   
       regularizer	
   view:	
   

       flat	
   prior:	
   	
     	
   =	
   d,	
   all	
     i	
   =	
   1/d	
   (equates	
   to	
   id113)	
   
       sparse	
   prior	
   (encourages	
   most	
     i	
   to	
   go	
   to	
   zero),	
   but	
   now	
   

(    i   1) log    i

r( ) =  

it   s	
   not	
   closed	
   form.	
   

mixture	
   of	
   unigrams	
   

       the	
   genera@ve	
   story	
   for	
   a	
   classical	
   document-     
id91	
   model	
   would	
   be	
   something	
   like	
   this	
   
(nigam	
   et	
   al.,	
   2000):	
   

       for	
   i	
   =	
   1	
   ...	
   m	
   (number	
   of	
   documents):	
   

       draw	
   a	
   document	
   length	
   ni	
   from	
   some	
   distribu@on.	
   
       draw	
   a	
   topic	
   zi	
   for	
   the	
   document	
   from	
   a	
   mul@nomial	
   over	
   
topics,	
     .	
   
       for	
   j	
   =	
   1	
   ...	
   ni:	
   

       draw	
   word	
   wij	
   from	
   the	
   mul@nomial	
     z.	
   

       nigam	
   et	
   al.	
   learned	
   this	
   using	
   em.	
   

mixture	
   of	
   unigrams	
   

  	
   

mixture	
   
coe   cients	
   

zi	
   

wi,j	
   

  z	
   

words	
   

ni	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

zi	
   is	
   some@mes	
   called	
   the	
   topic	
   of	
   
document	
   i.	
   
a	
   topic	
   z	
   is	
   de   ned	
   by	
   a	
   unigram	
   
distribu@on	
     z.	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

a	
   word	
   id91	
   model	
   

  	
   

mixture	
   
coe   cients	
   

zi,j	
   

wi,j	
   

  z	
   

words	
   

ni	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

problem:	
   	
   all	
   words	
   are	
   the	
   same;	
   
document	
   informa@on	
   is	
   irrelevant.	
   
(this	
   is	
   exactly	
   a	
   zero-     order	
   id48.)	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

probabilis@c	
   lsi	
   (hofmann,	
   1996)	
   

this	
   has	
   very	
   lidle	
   
to	
   do	
   with	
   latent	
   
seman/c	
   indexing,	
   
except	
   that	
   it   s	
   a	
   
probabilis@c	
   model	
   
trying	
   to	
   perform	
   a	
   
similar	
   task.	
   

  i	
   

zi,j	
   

wi,j	
   

words	
   

ni	
   

  z	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

word	
   id91;	
   documents	
   correspond	
   
to	
   distribu.ons	
   over	
   topics.	
   
problem:	
   	
   can   t	
   describe	
   new	
   documents!	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

latent	
   dirichlet	
   alloca@on	
   	
   

(blei	
   et	
   al.,	
   2003)	
   

  ,	
     	
   

  i	
   

zi,j	
   

wi,j	
   

words	
   

ni	
   

  z	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

documents	
   are	
   mixtures	
   of	
   topics,	
   but	
   a	
   
prior	
   over	
   those	
   mixtures	
   lets	
   us	
   reason	
   
about	
   new	
   documents,	
   too.	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

lda	
   on	
   acl	
   papers	
   (gimpel,	
   2006)	
   

smoothed	
   latent	
   dirichlet	
   alloca@on	
   	
   

(blei	
   et	
   al.,	
   2003)	
   

  ,	
     	
   

  i	
   

zi,j	
   

wi,j	
   

words	
   

ni	
   

     ,	
   
     	
   

  z	
   

documents	
   

m	
   

components	
   of	
   
the	
   mixture	
   

k	
   

m	
   =	
   number	
   of	
   documents	
   
ni	
   =	
   number	
   of	
   words	
   in	
   document	
   i	
   
k	
   =	
   number	
   of	
   mixture	
   components	
   

topic	
   models	
   beyond	
   lda	
   

       small	
   industry	
   in	
   varia@ons	
   on	
   topic	
   models,	
   usually	
   adding	
   
       examples:	
   	
   	
   

more	
   evidence	
   to	
   be	
   explained	
   by	
   the	
   topics.	
   

document	
   category.	
   

explained	
   by	
   more	
   draws	
   from	
     i	
   

       supervised	
   lda	
   (blei	
   and	
   mcauli   e,	
   2007)	
   adds	
   an	
   observed	
   
       link	
   lda	
   (erosheva	
   et	
   al.,	
   2004)	
   adds	
   cita@ons	
   to	
   the	
   document,	
   
       author	
   topic	
   model	
   (rosen-     zvi	
   et	
   al.,	
   2004)	
   adds	
   authors.	
   
       correlated	
   topic	
   model	
   (blei	
   and	
   la   erty,	
   2006)	
   lets	
   di   erent	
   
       comment	
   lda	
   (yano	
   et	
   al.,	
   2009)	
   generates	
   comments	
   from	
   a	
   

topics	
   correlate	
   more	
      exibly	
   through	
   a	
   di   erent	
   prior.	
   

di   erent	
   set	
   of	
   unigram	
   models	
   but	
   the	
   same	
   topics.	
   

where	
   is	
   the	
   structure?	
   
       through	
   the	
   topics	
   and	
     i,	
   words	
   in	
   a	
   
document	
   become	
   interdependent.	
   
      kind	
   of	
   a	
   joint	
   document/word	
   id91.	
   

       not	
   really	
   discrete	
   structure	
   the	
   way	
   we   ve	
   

mostly	
   discussed	
   in	
   this	
   class,	
   though.	
   

       lda	
   is	
   a	
      bayesian	
   zero-     order	
   id48.   	
   

where	
   is	
   the	
   predic@on?	
   

       topics	
   are	
   hard	
   to	
   evaluate;	
   no	
   gold	
   standard.	
   
       this	
   is	
   either	
   an	
   open	
   problem	
   or	
   the	
   nail	
   in	
   
the	
   co   n,	
   depending	
   on	
   your	
   point	
   of	
   view.	
   

hidden	
   markov	
   models	
   

  ,	
     	
   

  y	
   

k	
   

yi,j-     1	
   

yi,j	
   

xi,j	
   

bayesian	
   id48	
   
(goldwater	
   and	
   
gri   ths,	
   2007)	
   

ni	
   
m	
   

     ,	
   
     	
   

  y	
   

k	
   

  y	
   

k	
   

yi,j-     1	
   

yi,j	
   

xi,j	
   

unsupervised	
   id48	
   
(merialdo,	
   1994)	
   

ni	
   
m	
   

  y	
   

k	
   

m	
   =	
   number	
   of	
   sequences	
   
ni	
   =	
   number	
   of	
   words	
   in	
   sequence	
   i	
   
k	
   =	
   number	
   of	
   states	
   

the	
   engineering	
   part	
   
       typically	
   approximate	
   id136	
   is	
   required.	
   

       markov	
   chain	
   monte	
   carlo	
   (e.g.,	
   gibbs	
   sampling)	
   
       varia@onal	
   id136	
   (e.g.,	
   mean-        eld)	
   

       graphical	
   model	
   view	
   is	
   really	
   helpful	
   when	
   designing	
   
       learning:	
   	
   approximate	
   id136	
   +	
   op@miza@on	
   of	
   

id136	
   algorithms	
   for	
   your	
   bayesian	
   model!	
   

hyperparameters	
   (for	
   lda,	
   usually	
     	
   and	
     ;	
     	
   is	
   oqen	
   
assumed	
   uniform).	
   
       stochas@c	
   or	
   varia@onal	
   em,	
   depending	
   on	
   your	
   choice	
   of	
   
       full	
   bayesian:	
   	
      x	
   the	
   prior	
   and	
   do	
   id136	
   on	
   all	
   your	
   

approximate	
   id136.	
   

data.	
   
       implica@ons	
   for	
   train/test	
   methodology?	
   

sketch	
   of	
   gibbs	
   sampling	
   

       mcmc:	
   	
   design	
   (on	
   paper)	
   a	
   graph	
   where	
   each	
   

con   gura@on	
   from	
   val(v)	
   is	
   a	
   node.	
   
      transi@ons	
   in	
   the	
   graph	
   designed	
   to	
   give	
   a	
   markov	
   
chain	
   whose	
   sta@onary	
   distribu@on	
   is	
   the	
   
posterior.	
   

       simulate	
   a	
   random	
   walk	
   in	
   the	
   graph.	
   
       if	
   you	
   walk	
   long	
   enough,	
   your	
   posi@on	
   is	
   

distributed	
   according	
   to	
   p(v).	
   

	
   

transi@ons	
   in	
   gibbs	
   sampling	
   
       a	
   transi@on	
   in	
   the	
   markov	
   chain	
   equates	
   to	
   
changing	
   a	
   subset	
   of	
   the	
   random	
   variables.	
   
       gibbs:	
   	
   resample	
   vi   s	
   value	
   according	
   to	
   	
   
p(vi	
   |	
   v	
   \	
   {vi}).	
   
       only	
   need	
   the	
   local	
   factors	
   that	
   a   ect	
   vi:	
   	
   take	
   
product,	
   marginalize,	
   and	
   randomly	
   choose	
   new	
   
value.	
   

       simply	
   lock	
   evidence	
   variables	
   x.	
   
       maximizing	
   version	
   gradually	
   shiqs	
   sampler	
   in	
   

favor	
   of	
   most	
   probable	
   value	
   for	
   vi.	
   

sketch	
   of	
   	
   

mean	
   field	
   varia@onal	
   id136	
   
       id136	
   with	
   our	
   distribu@on	
   p	
   is	
   hard.	
   
       choose	
   an	
      easier   	
   distribu@on	
   family,	
   q.	
   	
   
then	
      nd:	
   

arg min
q q

d(q p )

       usually	
   itera@ve	
   methods	
   are	
   required	
   to	
         t   	
   
q	
   to	
   p.	
   
      these	
   oqen	
   resemble	
   familiar	
   learning	
   algorithms	
   
like	
   em!	
   

energy	
   func@onal	
   

d(q(y )   p (y | x = x)) = eq[log q(y )]   eq[log p (y | x = x)]

log p (x = x)

=  h(q(y ))   (eq[log p (x = x, y )]   log p (x = x))

=  h(q(y ))          
eq[log  |x]   log p (x = x)      
= d(q(y )   p (y | x = x)) + h(q(y )) +    
      

eq[log  |x]

 

 
       expecta@ons	
   under	
   simpler	
   distribu@on	
   family,	
   q.	
   

maximize this

constant

      

 

 

       every	
   element	
   of	
   q is	
   an	
   approximate	
   solu@on.	
   
       we	
   try	
   to	
      nd	
   the	
   best	
   one.	
   

varia@onal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1
family of functions g  (x) 

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

x

varia@onal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

       further,	
   for	
   any	
   x,	
   there	
   is	
   some	
     	
   where	
   the	
   

x

bound	
   is	
   @ght.	
   	
   
        	
   is	
   called	
   a	
   varia/onal	
   parameter.	
   

tangent:	
   	
   varia@onal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

       further,	
   for	
   any	
   x,	
   there	
   is	
   some	
     	
   where	
   the	
   

x

bound	
   is	
   @ght.	
   	
   
        	
   is	
   called	
   a	
   varia/onal	
   parameter.	
   

tangent:	
   	
   varia@onal	
   methods	
   

       this	
   is	
   a	
   simple	
   example.	
   
       for	
   any	
     	
   and	
   any	
   x:	
   

)
x
(
n
l
-

  ln(x)       x + ln( ) + 1

4

3

2

1

0

0.0

0.2

0.4

0.6

0.8

1.0

       further,	
   for	
   any	
   x,	
   there	
   is	
   some	
     	
   where	
   the	
   

x

bound	
   is	
   @ght.	
   	
   
        	
   is	
   called	
   a	
   varia/onal	
   parameter.	
   

       for	
   us,	
   log	
   p(x	
   =	
   x)	
   is	
   like	
   -     ln(x),	
   and	
   q	
   is	
   like	
     .	
   	
   

structured	
   varia@onal	
   approach	
   
       maximize	
   the	
   energy	
   func@onal	
   over	
   a	
   family	
   

q that	
   is	
   well-     de   ned.	
   
      a	
   graphical	
   model!	
   
      probably	
   not	
   an	
   i-     map	
   for	
   p.	
   	
   (bound	
   isn   t	
   @ght.)	
   
       simpler	
   structures	
   lead	
   to	
   easier	
   id136.	
   

      mean	
      eld	
   is	
   the	
   simplest:	
   

q(v ) =  i

qi(vi)

going	
   nonparametric	
   

       how	
   many	
   topics	
   or	
   states?	
   
       nonparametric:	
   	
   let	
   the	
   data	
   decide.	
   

       not	
   necessarily	
   bayesian	
   or	
   even	
   probabilis@c!	
   
       more	
   data	
   jus@fy	
   more	
   parameters.	
   

       most	
   common	
   nonparametric	
   and	
   bayesian	
   tools	
   

in	
   nlp	
   are	
   based	
   on	
   the	
   dirichlet	
   process.	
   
       dp	
   is	
   not	
   the	
   same	
   as	
   the	
   dirichlet	
   distribu.on.	
   

       you	
   can	
   be	
   bayesian	
   without	
   being	
   

nonparametric,	
   and	
   you	
   can	
   be	
   nonparametric	
   
without	
   being	
   bayesian!	
   

remember	
   

          bayesian   	
   describes	
   your	
   model,	
   not	
   you!	
   
       approximate	
   id136	
   is	
   necessary	
   in	
   

bayesian	
   modeling,	
   but	
   useful	
   elsewhere,	
   too!	
   

