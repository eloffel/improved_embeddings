nlp	
   demys*   ed	
   

noah	
   smith	
   

language	
   technologies	
   ins*tute	
   
machine	
   learning	
   department	
   
school	
   of	
   computer	
   science	
   
carnegie	
   mellon	
   university	
   

nasmith@cs.cmu.edu	
   

nlp?	
   

outline	
   

1.    automa*cally	
   categorizing	
   documents	
   
2.    decoding	
   sequences	
   of	
   words	
   
3.    id91	
   documents	
   and/or	
   words	
   

categorizing	
   documents:	
   	
   examples	
   

       mosteller	
   and	
   wallace	
   (1964):	
   	
   authorship	
   of	
   the	
   

federalist	
   papers	
   

       news	
   categories:	
   	
   u.s.,	
   world,	
   sports,	
   religion,	
   

business,	
   technology,	
   entertainment,	
   ...	
   

       how	
   posi*ve	
   or	
   nega*ve	
   is	
   a	
   review	
   of	
   a	
      lm	
   or	
   

restaurant?	
   
is	
   a	
   given	
   email	
   message	
   spam?	
   

      
       what	
   is	
   the	
   reading	
   level	
   of	
   a	
   piece	
   of	
   text?	
   
       how	
   in   uen*al	
   will	
   a	
   research	
   paper	
   be?	
   
       will	
   a	
   congressional	
   bill	
   pass	
   commizee?	
   

the	
   vision	
   
       human	
   experts	
   label	
   some	
   data	
   
       feed	
   the	
   data	
   to	
   a	
   learning	
   algorithm	
   that	
   
constructs	
   an	
   automa*c	
   labeling	
   func*on	
   
       apply	
   that	
   func*on	
   to	
   as	
   much	
   data	
   as	
   you	
   

want!	
   

documents	
   d.	
   

basic	
   recipe	
   for	
   document	
   categoriza*on	
   
1.    obtain	
   a	
   pool	
   of	
   correctly	
   categorized	
   
2.    de   ne	
   a	
   func*on	
   f	
   from	
   documents	
   to	
   feature	
   
3.    de   ne	
   a	
   parameterized	
   func*on	
   hw	
   from	
   
4.    select	
   h   s	
   parameters	
   w	
   using	
   a	
   training	
   sample	
   
5.    es*mate	
   performance	
   on	
   a	
   held-     out	
   sample	
   

feature	
   vectors	
   to	
   categories.	
   

vectors.	
   	
   

from	
   d.	
   

from	
   d.	
   

1.	
   	
   obtain	
   categorized	
   documents	
   

spinoza,	
   17th	
   century	
   ra*onalist	
   

2.	
   	
   de   ne	
   the	
   feature	
   vector	
   func*on	
   
       simplest	
   choice:	
   	
   one	
   dimension	
   per	
   word,	
   and	
   
       twists:	
   	
   	
   

let	
   [	
   f(d)	
   ]j	
   be	
   the	
   count	
   of	
   wj	
   in	
   d.	
   

or	
   taking	
   a	
   log.	
   

       monotonic	
   transforms,	
   like	
   dividing	
   by	
   the	
   length	
   of	
   d	
   
       increase	
   the	
   weights	
   of	
   words	
   that	
   occur	
   in	
   fewer	
   
       n-     grams	
   
       count	
   specially	
   de   ned	
   groupings	
   of	
   words	
   
       sta*s*cal	
   tests	
   to	
   select	
   words	
   likely	
   to	
   be	
   informa*ve	
   

documents	
   (   inverse	
   document	
   frequency   )	
   

documents	
   d.	
   

basic	
   recipe	
   for	
   document	
   categoriza*on	
   
1.    obtain	
   a	
   pool	
   of	
   correctly	
   categorized	
   
2.    de   ne	
   a	
   func*on	
   f	
   from	
   documents	
   to	
   feature	
   
3.    de   ne	
   a	
   parameterized	
   func*on	
   hw	
   from	
   
4.    select	
   h   s	
   parameters	
   w	
   using	
   a	
   training	
   sample	
   
5.    es*mate	
   performance	
   on	
   a	
   held-     out	
   sample	
   

feature	
   vectors	
   to	
   categories.	
   

vectors.	
   	
   

from	
   d.	
   

from	
   d.	
   

3.	
   	
   de   ne	
   a	
   func*on	
   	
   

from	
   feature	
   vectors	
   to	
   categories	
   

       simplest	
   choice:	
   	
   linear	
   model	
   

c

hw(d) = arg max

w>c f (d) + wbias
wc	
   is	
   the	
   vector	
   of	
   coe   cients	
   associa*ng	
   each	
   
feature	
   with	
   class	
   c	
   (can	
   be	
   posi*ve	
   or	
   nega*ve).	
   
      advantage:	
   	
   interpretability	
   
      advantage:	
   	
   computa*onal	
   e   ciency	
   

c

       some	
   alterna*ves:	
   	
   k-     nearest	
   neighbors,	
   

decision	
   trees,	
   neural	
   networks,	
   ...	
   

4.	
   	
   select	
   parameters	
   using	
   data	
   

       also	
   known	
   as	
      machine	
   learning.   	
   
       many	
   learning	
   op*ons	
   for	
   linear	
   classi   ers!	
   

nb	
   

lr	
   

probabilis3c	
   
interpreta3on	
   

discrimina3ve	
   

id166	
   

id88	
   

4.	
   	
   select	
   parameters	
   using	
   data	
   

op*miza*on	
   view	
   of	
   learning:	
   
1

  w = arg min
w

r(w) +

|dtrain| xd2dtrain

l(d; w)

	
   
   regulariza*on   	
   to	
   avoid	
   over   jng	
   
	
   
typical	
   loss	
   func*ons	
   for	
   linear	
   models	
   are	
   convex	
   and	
   
can	
   be	
   e   ciently	
   op*mized	
   using	
   online	
   or	
   batch	
   
itera*ve	
   algorithms	
   with	
   convergence	
   guarantees.	
   

training	
   data	
   

   empirical	
   risk   	
   =	
   average	
   loss	
   over	
   

4.	
   	
   select	
   parameters	
   using	
   data	
   

	
   	
   

labels?	
   

considera*ons:
       do	
   you	
   want	
   posterior	
   probabili*es,	
   or	
   just	
   
       what	
   methods	
   do	
   you	
   understand	
   well	
   enough	
   
       what	
   methods	
   will	
   your	
   readers	
   understand?	
   
       what	
   implementa*ons	
   are	
   available?	
   

to	
   	
   explain	
   in	
   your	
   paper?	
   

       cost,	
   scalability,	
   programming	
   language,	
   compa*bility	
   

with	
   your	
   work   ow,	
   ...	
   

       how	
   well	
   does	
   it	
   work	
   (on	
   held-     out	
   data)?	
   
	
   
	
   

5.	
   	
   es*mate	
   performance	
   
       always,	
   always,	
   always	
   use	
   held-     out	
   data.	
   
      mul*ple	
   rounds	
   of	
   tests?	
   	
   fresh	
   tes*ng	
   data!	
   

       consider	
   the	
      most	
   frequent	
   class   	
   baseline.	
   
       consider	
   inter-     annotator	
   agreement.	
   
       what	
   to	
   measure?	
   

      accuracy	
   
      when	
   one	
   class	
   is	
   special:	
   	
   precision/recall	
   

5.	
   	
   es*mate	
   performance	
   

precision	
   

recall	
   

hw(d) = arg max

c

w>c f (d) + wbias

c

documents	
   d.	
   

basic	
   recipe	
   for	
   document	
   categoriza*on	
   
1.    obtain	
   a	
   pool	
   of	
   correctly	
   categorized	
   
2.    de   ne	
   a	
   func*on	
   f	
   from	
   documents	
   to	
   feature	
   
3.    de   ne	
   a	
   parameterized	
   func*on	
   hw	
   from	
   
4.    select	
   h   s	
   parameters	
   w	
   using	
   a	
   training	
   sample	
   
5.    es*mate	
   performance	
   on	
   a	
   held-     out	
   sample	
   

feature	
   vectors	
   to	
   categories.	
   

vectors.	
   	
   

from	
   d.	
   

from	
   d.	
   

outline	
   

        automa*cally	
   categorizing	
   documents	
   
2.    decoding	
   sequences	
   of	
   words	
   
3.    id91	
   documents	
   and/or	
   words	
   

decoding	
   word	
   sequences:	
   	
   examples	
   
       categorizing	
   each	
   word	
   by	
   its	
   part-     of-     speech	
   

or	
   seman*c	
   class	
   

       recognizing	
   men*ons	
   of	
   named	
   en**es	
   
       segmen*ng	
   a	
   document	
   into	
   parts	
   
       parsing	
   a	
   sentence	
   into	
   a	
   gramma*cal	
   or	
   

seman*c	
   structure	
   

high-     level	
   view	
   

d	
   

x	
   

c	
   

y2	
   

y3	
   

classi   ca*on	
   

y1	
   

structured	
   predic*on	
   

...	
   

yn	
   

possible	
   lines	
   of	
   azack	
   

1.    transform	
   into	
   a	
   sequence	
   of	
   classi   ca*on	
   

problems	
   (see	
   part	
   1).	
   

2.    transform	
   into	
   a	
   sequence	
   labeling	
   problem	
   

and	
   use	
   a	
   variant	
   of	
   the	
   viterbi	
   algorithm.	
   

3.    design	
   a	
   representa*on,	
   predic*on	
   

algorithm,	
   and	
   learning	
   algorithm	
   for	
   your	
   
par*cular	
   problem.	
   

shameless	
   self-     promo*on	
   

series issn: 1947-4040

synthesis lectures on
human language  technologies
series editor: graeme hirst, university of toronto
linguistic structure prediction
noah a. smith, carnegie mellon university
a major part of natural language processing now depends on the use of text data to build linguistic
analyzers. we consider statistical, computational approaches to modeling linguistic structure. we seek
to unify across many approaches and many kinds of linguistic structures. assuming a basic understanding
of natural language processing and/or machine learning, we seek to bridge the gap between the two    elds.
approaches to decoding (i.e., carrying out linguistic structure prediction) and supervised and unsupervised
learning of models that predict discrete structures as outputs are the focus. we also survey natural language
processing problems to which these methods are being applied, and we address related topics in probabilistic
id136, optimization, and experimental methodology.

$56.43	
   on	
   amazon.com	
   
	
   
possibly	
   free	
   in	
   
electronic	
   form,	
   through	
   
your	
   university   s	
   library	
   

s
m
i
t
h

 

l
i
n
g
u
i
s
t
i
c
s
t
r
u
c
t
u
r
e
p
r
e
d
i
c
t
i
o
n

 

&

cm& morgan      claypool  publishers
linguistic structure
prediction

noah a. smith

about synthesis
this volume is a printed version of a work that appears in the synthesis
digital library of engineering and computer science. synthesis lectures
provide concise, original presentations of important research and development
topics, published quickly, in digital and print formats. for more information
visit www.morganclaypool.com
&

morgan      claypool  publishers

w w w . m o r g a n c l a y p o o l . c o m

isbn: 978-1-60845-405-1
90000

9 781608 454051

m
o
r
g
a
n
&
c
l
a
y
p
o
o
l

synthesis lectures on
human language  technologies
graeme hirst, series editor

lines	
   of	
   azack	
   

1.    reduce	
   to	
   a	
   sequence	
   of	
   classi   ca*on	
   

problems	
   (see	
   part	
   1).	
   

2.    reduce	
   to	
   a	
   sequence	
   labeling	
   problem	
   and	
   

use	
   a	
   variant	
   of	
   the	
   viterbi	
   algorithm.	
   

3.    design	
   a	
   representa*on,	
   predic*on	
   

algorithm,	
   and	
   learning	
   algorithm	
   for	
   your	
   
problem.	
   

sequence	
   labeling	
   
       input:	
   	
   sequence	
   of	
   symbols	
   x1	
   x2	
   ...	
   xl	
   
       output:	
   	
   sequence	
   of	
   labels	
   y1	
   y2	
   ...	
   yl	
   each	
      	
     	
   
	
   
predic*on	
   rule:	
   
	
   
	
   
	
   
problem:	
   	
   there	
   are	
   o(|  |l)	
   choices	
   for	
   y1	
   y2	
   ...	
   yl	
   !	
   
	
   

hw(x) = arg max

y

w>f (x1 . . . xl, y1 . . . yl)

sequence	
   labeling	
   with	
   local	
   features	
   

a	
   key	
   assump*on	
   about	
   f	
   allows	
   us	
   to	
   solve	
   the	
   
problem	
   exactly,	
   in	
   o(|  |2	
   l)	
   *me	
   and	
   o(|  |l)	
   
space.	
   

hw(x) = arg max

y

= arg max

y

w>f (x1 . . . xl, y1 . . . yl)

w> l 1x`=1

f local (x1 . . . xl, y`y`+1)!

y   l = arg max
yl2   

= w> l 2x`=1

w> l 2x`=1
f local (x1 . . . xl, y   ` y   `+1)! + arg max

f local (x1 . . . xl, y   ` y   `+1)! + w>f local (x1 . . . xl, y   l 1yl)
w>f local (x1 . . . xl, y   l 1yl)

if	
   i	
   knew	
   the	
   best	
   label	
   sequence	
   for	
   x1	
   ...	
   xl	
      	
   1,	
   then	
   yl	
   would	
   be	
   easy.	
   
	
   
that	
   decision	
   would	
   depend	
   only	
   on	
   state	
   l	
      	
   1.	
   
	
   
	
   
	
   
	
   
	
   
i	
   don   t	
   know	
   that	
   best	
   sequence,	
   but	
   there	
   are	
   only	
   |  |	
   op*ons	
   at	
   l	
      	
   1.	
   
	
   
so	
   i	
   only	
   need	
   the	
   score	
   of	
   the	
   best	
   sequence	
   up	
   to	
   l	
      	
   1,	
   for	
   each	
   possible	
   
label	
   at	
   l	
      	
   1.	
   	
   	
   call	
   this	
   v[l	
      	
   1,	
   y]	
   for	
   y	
      	
     .	
   	
   from	
   this,	
   i	
   can	
   score	
   each	
   label	
   
at	
   l,	
   for	
   each	
   hypothe*cal	
   label	
   at	
   l	
      	
   1.	
   
	
   
score	
   of	
   the	
   best	
   sequences	
   up	
   to	
   l	
      	
   1	
   relies	
   similarly	
   on	
   score	
   of	
   the	
   best	
   
sequences	
   up	
   to	
   l	
      	
   2.	
   	
   	
   dizo,	
   at	
   every	
   other	
   *mestep	
   l	
      	
   2,	
   l	
      	
   3,	
   ...	
   1.	
   	
   

yl2   

(featurized)	
   viterbi	
   algorithm	
   
       precompute	
   v[*,	
   *]	
   from	
   ler	
   to	
   right.	
   	
   v[1,	
   *]	
   =	
   0.	
   	
   

v [`, y] =

for	
          	
   =	
   2	
   to	
   l,	
   for	
   each	
   y	
   in	
     :	
   
for	
          	
   =	
   l	
   -     	
   1	
   to	
   1:	
   

max
y02   
b[`, y] = arg max
y02   

y   l = arg max

v [l, y]

	
   

y

y   ` = b[` + 1, y   `+1]

v [`   1, y0] + w>f local (x1 . . . xl, y0y)
v [`   1, y0] + w>f local (x1 . . . xl, y0y)

       backtrack	
   and	
   select	
   the	
   labels	
   from	
   right	
   to	
   ler.	
   

part	
   of	
   speech	
   tagging	
   

arer	
   paying	
   the	
   medical	
   bills	
   ,	
   frances	
   was	
   nearly	
   broke	
   .	
   
	
   	
   	
   rb	
   	
   	
   	
   	
   	
   vbg	
   	
   	
   	
   	
   dt	
   	
   	
   	
   	
   	
   	
   jj	
   	
   	
   	
   	
   	
   	
   	
   nns	
   	
   ,	
   	
   	
   	
   	
   nnp	
   	
   	
   	
   	
   vbz	
   	
   	
   	
   	
   rb	
   	
   	
   	
   	
   	
   	
   	
   jj	
   	
   	
   	
   	
   	
   .	
   
	
   
       adverb	
   (rb)	
   
       verb	
   (vbg,	
   vbz,	
   and	
   others)	
   
       determiner	
   (dt)	
   
       adjec*ve	
   (jj)	
   
       noun	
   (nn,	
   nns,	
   nnp,	
   and	
   others)	
   
       punctua*on	
   (.,	
   ,,	
   and	
   others)	
   

named	
   en*ty	
   recogni*on	
   

with	
   commander	
   chris	
   ferguson	
   at	
   the	
   helm	
   ,	
   

atlan*s	
   touched	
   down	
   at	
   kennedy	
   space	
   center	
   .	
   

named	
   en*ty	
   recogni*on	
   

o	
   

i-     person	
   

i-     person	
   

b-     person	
   

o	
    o	
   
with	
   commander	
   chris	
   ferguson	
   at	
   the	
   helm	
   ,	
   
i-     place	
   
b-     space-     shuzle	
   
o	
   
atlan*s	
   touched	
   down	
   at	
   kennedy	
   space	
   center	
   .	
   

b-     place	
   

o	
    o	
   

i-     place	
   

o	
   

o	
   

o	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

word	
   alignment	
   

mr.	
   president	
   ,	
   noah   s	
   ark	
   was	
      lled	
   not	
   with	
   produc*on	
   factors	
   ,	
   but	
   with	
   living	
   creatures.	
   

	
   
	
   
	
   
	
   

null	
   	
   	
   	
   	
   	
   noahs	
   arche	
   war	
   nicht	
   voller	
   produc*onsfactoren	
   ,	
   sondern	
   gesch  pfe	
   .	
   

basic	
   recipe	
   for	
   document	
   categoriza*on	
   
sequence	
   labeling	
   
1.    obtain	
   a	
   pool	
   of	
   correctly	
   labeled	
   sequences	
   d.	
   
2.    de   ne	
   a	
   locally	
   factored	
   func*on	
   f	
   from	
   

sequences	
   and	
   labelings	
   to	
   feature	
   vectors.	
   	
   

3.    de   ne	
   a	
   parameterized	
   func*on	
   hw	
   from	
   
4.    select	
   h   s	
   parameters	
   w	
   using	
   a	
   training	
   sample	
   

feature	
   vectors	
   to	
   labelings.	
   

5.    es*mate	
   performance	
   on	
   a	
   held-     out	
   sample	
   

from	
   d.	
   

from	
   d.	
   

structured	
   learners	
   generalize	
   	
   
linear	
   classi   ca*on	
   learners!	
   

       hidden	
   markov	
   models	
      	
   na  ve	
   bayes	
   
       condi*onal	
   random	
      elds	
      	
   logis*c	
   regression	
   
       structured	
   id88	
      	
   id88	
   
       structured	
   id166	
      	
   support	
   vector	
   machine	
   

addi*onal	
   notes	
   

       outputs	
   that	
   are	
   trees,	
   graphs,	
   logical	
   forms,	
   

other	
   strings	
   ...	
   
parse	
   trees	
   (phrase	
   structure,	
   dependencies)	
   
coreference	
   rela*onships	
   among	
   en*ty	
   men*ons	
   
(and	
   pronouns)	
   
a	
   huge	
   range	
   of	
   seman*c	
   analyses	
   

       evalua*on?	
   

dependency	
   parse	
   

frame-     seman*c	
   parse	
   

run	
   our	
   parsers!	
   

http://demo.ark.cs.cmu.edu/parse 

outline	
   

        automa*cally	
   categorizing	
   documents	
   
        decoding	
   sequences	
   of	
   words	
   
3.    id91	
   documents	
   and/or	
   words	
   

id91	
   real	
   data	
   

k-     means	
   

given:	
   	
   points	
   {x1,	
      ,	
   xn},	
   k	
   (number	
   of	
   clusters)	
   
	
   
1.   arbitrarily	
   select	
     1,	
      ,	
     k.	
   
2.   assign	
   each	
   xi	
   to	
   the	
   nearest	
     j.	
   
3.   select	
   each	
     j	
   to	
   be	
   the	
   mean	
   of	
   all	
   xi	
   assigned	
   
to	
   it.	
   
4.   if	
   all	
     j	
   have	
   converged	
   stop;	
   else	
   go	
   to	
   2.	
   

k-     means,	
   visualized	
   

k-     means,	
   visualized	
   

k-     means,	
   visualized	
   

k-     means,	
   visualized	
   

k-     means,	
   visualized	
   

k-     means,	
   visualized	
   

k-     means	
   for	
   text?	
   

      use	
   the	
   same	
   f	
   we	
   might	
   use	
   for	
   classi   ca*on.	
   

       documents	
   

       words	
   

      use	
      context   	
   vectors	
   ...	
   

where   s	
   the	
   beef?	
   

chicken	
   

hypothe*cal	
   counts	
   based	
   on	
   syntac*c	
   dependencies	
   

modi   ed-     by-     
ferocious(adj)	
   

subject-     of-     
devour(v)	
   

object-     of-     
pet(v)	
   

modi   ed-     by-     
african(adj)	
   

modi   ed-     by-     
big(adj)	
   

lion	
   
dog	
   
cat	
   
elephant	
   
   	
   

15	
   
7	
   
1	
   
0	
   

5	
   
3	
   
1	
   
0	
   

0	
   
8	
   
6	
   
0	
   

6	
   
0	
   
1	
   
10	
   

15	
   
12	
   
9	
   
15	
   

brown	
   id91	
   

given:	
   	
   corpus	
   of	
   length	
   n,	
   k	
   
	
   
1.    assign	
   each	
   word	
   to	
   its	
   cluster	
   (v	
   clusters)	
   
2.    repeat	
   v	
      	
   k	
   *mes:	
   

       find	
   the	
   single	
   merge	
   (cj,	
   ck)	
   that	
   results	
   in	
   a	
   
new	
   id91	
   with	
   the	
   highest	
   quality	
   score	
   
       prepend	
   cj   s	
   bitstring	
   with	
   0	
   and	
   ck   s	
   with	
   1	
   
(and	
   the	
   same	
   for	
   all	
   their	
   descendents)	
   

mini-     example	
   

bitstrings	
   that	
   share	
   a	
   pre   x	
   are	
   in	
   the	
   same	
   cluster,	
   
at	
   some	
   level	
   of	
   granularity.	
   

clusters	
   from	
   brown	
   et	
   al.	
   (1992)	
   

clusters	
   from	
   owopu*	
   et	
   al.	
   (2013)	
   

(56m	
   tweets)	
   
lmao	
   lmfao	
   lmaoo	
   lmaooo	
   hahahahaha	
   lool	
   
c(cid:127)u	
   ro   	
   loool	
   lmfaoo	
   lmfaooo	
   lmaoooo	
   lmbo	
   
lololol	
   	
   
haha	
   hahaha	
   hehe	
   hahahaha	
   hahah	
   aha	
   
hehehe	
   ahaha	
   hah	
   hahahah	
   kk	
   hahaa	
   ahah	
   
yes	
   yep	
   yup	
   nope	
   yess	
   yesss	
   yessss	
   ofcourse	
   
yeap	
   likewise	
   yepp	
   yesh	
   yw	
   yuup	
   yus	
   
yeah	
   yea	
   nah	
   naw	
   yeahh	
   nooo	
   yeh	
   noo	
   
noooo	
   yeaa	
   ikr	
   nvm	
   yeahhh	
   nahh	
   nooooo	
   	
   
smh	
   jk	
   #fail	
   #random	
   #fact	
   sm(cid:129)	
   #smh	
   
#winning	
   #realtalk	
   smdh	
   #dead	
   #justsaying	
   	
   

acronyms	
   for	
   
laughter	
   

onomatopoeic	
   	
   
laugher	
   

a   rma*ve	
   

nega*ve	
   

metacomment	
   

clusters	
   from	
   owopu*	
   et	
   al.	
   (2013)	
   

second	
   person	
   
pronoun	
   

preposi*ons	
   

   contrac*ons   	
   

going	
   to	
   

so+	
   

(56m	
   tweets)	
   
u	
   yu	
   yuh	
   yhu	
   uu	
   yuu	
   yew	
   y0u	
   yuhh	
   youh	
   yhuu	
   
iget	
   yoy	
   yooh	
   yuo	
   yue	
   juu	
   dya	
   youz	
   yyou	
   	
   
w	
   fo	
   fa	
   fr	
   fro	
   ov	
   fer	
      r	
   whit	
   abou	
   ar	
   serie	
   fore	
   
fah	
   fuh	
   w/her	
   w/that	
   fron	
   isn	
   agains	
   	
   
tryna	
   gon	
      nna	
   bouta	
   trynna	
   bouza	
   gne	
      na	
   
gonn	
   tryina	
   fenna	
   qone	
   trynaa	
   qon	
   	
   
gonna	
   gunna	
   gona	
   gna	
   guna	
   gnna	
   ganna	
   
qonna	
   gonnna	
   gana	
   qunna	
   gonne	
   goona	
   	
   
soo	
   sooo	
   soooo	
   sooooo	
   soooooo	
   sooooooo	
   
soooooooo	
   sooooooooo	
   soooooooooo	
   	
   

clusters	
   from	
   owopu*	
   et	
   al.	
   (2013)	
   

mischevious	
   

happy	
   

sad	
   

love	
   

f-     word	
   +	
   ing	
   

(56m	
   tweets)	
   
;)	
   :p	
   :-     )	
   xd	
   ;-     )	
   ;d	
   (;	
   :3	
   ;p	
   =p	
   :-     p	
   =))	
   ;]	
   xdd	
   #gno	
   
xddd	
   >:)	
   ;-     p	
   >:d	
   8-     )	
   ;-     d	
   	
   
:)	
   (:	
   =)	
   :))	
   :]	
   :   )	
   =]	
   ^_^	
   :)))	
   ^.^	
   [:	
   ;))	
   ((:	
   ^__^	
   (=	
   ^-     
^	
   :))))	
   
:(	
   :/	
   -     _-     	
   -     .-     	
   :-     (	
   :   (	
   d:	
   :|	
   :s	
   -     __-     	
   =(	
   =/	
   >.<	
   -     ___-     	
   :-     /	
   
</3	
   :\	
   -     ____-     	
   ;(	
   /:	
   :((	
   >_<	
   =[	
   :[	
   #fml	
   	
   
<3	
   xoxo	
   <33	
   xo	
   <333	
   #love	
   s2	
   <url-     
twi**on.com>	
   #neversaynever	
   <3333	
   	
   
fucking	
   fuckin	
   freaking	
   bloody	
   freakin	
   friggin	
   
e   n	
   e   ng	
   fuckn	
   fucken	
   frickin	
   fukin	
   f'n	
   fckn	
   
   ippin	
   (cid:137)n	
   motherfucking	
   fckin	
   f*cking	
   fricken	
   
fukn	
   fuccin	
   fcking	
   fukkin	
   

browse	
   our	
   twizer	
   clusters!	
   

http://www.ark.cs.cmu.edu/tweetnlp/
cluster_viewer.html  

addi*onal	
   notes	
   

       sor	
   id91	
   allows	
   items	
   to	
   have	
   mixed	
   

membership	
   in	
   di   erent	
   clusters.	
   
      typically	
   accomplished	
   with	
   probabilis*c	
   models	
   
      latent	
   dirichlet	
   alloca*on	
   is	
   a	
   popular	
   and	
   
bayesian	
   model	
   

       evalua*on?	
   
       one	
   view	
   of	
   clusters:	
   	
   feature	
   crea*on!	
   

summary	
   

(5	
   steps:	
   	
   data,	
   features,	
   

predic3on	
   func3on,	
   
learning,	
   evalua3on)	
   

supervised	
   
classi   ca*on	
   

local	
   factoring	
   +	
   

dynamic	
   programming	
   

structured	
   
predic*on	
   

alterna3ng	
   
or	
   greedy	
   
op3miza3on	
   

unsupervised	
   

id91	
   

