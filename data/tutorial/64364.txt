id170

for natural language processing

noah a. smith

assistant professor

carnegie mellon university

nasmith@cs.cmu.edu

june 14, 2009

1 / 169

a very long relationship

natural language processing (nlp) and machine learning
(ml) go back to the 1940s.

nlp to ml:    you give me elegant, well-founded solutions to my
problems.   

ml to nlp:    you give meaning to my math. you come with
data.   

but we are seeing signs of strain.

2 / 169

grievances from nlp

    scalability:    i asked you to use all of the data. why can   t you

ever    nish a job?   

    simplistic models:    stop assuming things!   
    in   delity:    why are you always thinking about classi   cation?   

3 / 169

grievances from ml

    data incomplete:    why can   t you just tell me what you

want?   

    evaluation criteria unclear:    you keep changing your mind!   
    in   delity:    why are you always thinking about linguistics?   

4 / 169

this marriage can survive, if both parties learn to understand each
other better.

this tutorial is meant to provide a bit of marriage counseling to
nlp and ml.

5 / 169

where we   re going

goals:

    discuss several linguistic analysis problems that are examples

of id170.

    present algorithmic tools used for making structured

predictions (decoding).

    present the dominant techniques used in nlp for learning to

make id170s from

    complete data (supervisedly) and
    incomplete data (unsupervisedly).

6 / 169

where we   re not going

i won   t be presenting any experimental comparisons.

i won   t go into much detail on datasets, annotation conventions, or
linguistic theory (see the references).

i won   t go into much detail on implementation (features, tricks,
data structures)   you won   t be able to walk away from this
tutorial and start writing code.

7 / 169

managing expectations

hopefully after this tutorial you will:

    be (even) more excited about nlp as a playground for ml.
    understand id170 better, and have a broader

view of what it encompasses.

    have higher expectations for what (structured) ml should be

able to do.

8 / 169

what   s id170?

unfortunately, there is no widely agreed-upon de   nition!

two versions, proposed by [daum  e, 2006]:

1 discrete output representable by (collections of)

variable-length vectors in {0, 1, . . . , m}l.

2 additionally, id168 does not decompose into parts.

here we take the view that    you know it when you see it    and
assume that there may be more than one interesting or useful loss
function. (often our id168s will decompose.)

9 / 169

representations & data

10 / 169

don   t think about a bag of words

language and text have structure.

put another way, the words in a document are not iid.

linguistics o   ers many di   erent theories about the relationships
among bits of text; we will see some of them.

11 / 169

12 / 169

13 / 169

some notation

x: set of possible inputs, often x =      

y: set of possible outputs

14 / 169

where are the words?

problem: segmentation into words (or sentences)

15 / 169

            ,               ,                        ,         372   ,                                                ,                                                               ,                                                                                 [            ][,][   ][   ][         ][,][      ][            ][      ][,][      ][   ][372   ][,][   ][         ][      ][   ][      ][   ][      ][      ][      ][,][   ][      ][      ][   ][   ][      ][   ][   ][      ][      ][   ][      ][   ][      ][,][   ][   [               ][   ][      ][   ][            ][   ][   ][      ][      ][   ][      ][      ][   ]where are the words?

problem: segmentation into words (or sentence)s
x:       (character sequences)
y:       (word sequences)

mostly trivial for english (id121), though sentence
segmentation is a bit harder [ratnaparkhi, 1996].

16 / 169

the problem with words

words proliferate like cockroaches, and they tend to follow zip   an
distributions.

an early step in most language processing is trying to make words
more manageable.

17 / 169

the problem with words

18 / 169

2468102e+064e+066e+068e+06words in 10-k reports, 2001-5rankfrequencythe problem with words

19 / 169

0204060801000e+004e+068e+06words in 10-k reports, 2001-5rankfrequencythe problem with words

20 / 169

020040060080010000e+004e+068e+06words in 10-k reports, 2001-5rankfrequencythe problem with words

21 / 169

0200004000060000800001000001200000e+004e+068e+06words in 10-k reports, 2001-5rankfrequencyproblem: projection into a simpler vocabulary

what   s in a word?

22 / 169

the angle of cats' ears is an important clue to their mood.the angle of cats ' ears is an important clue to their mood .surface string:tokenized:the angl of cat ' ear is an import clue to their mood .stemmed:the angle of cat ' ear is an important clue to their mood .lemmatized:what   s in a word?

x:       (surface word sequence)
y:       (canonical word sequence)

    id30: strip su   xes, usually based on some rules

[porter, 1980]

    lemmatization: reduce words to stems (gave, given, give    

give)

23 / 169

extreme id121: parts of speech

problem: label each word with its part-of-speech

x:       (surface word sequence)
y:       (part-of-speech tag sequence)

24 / 169

the angle of cats ' ears is an important clue to their mood   det.    noun      p.    pln.  pos. pln.  v.  det.    adj.                noun  p.   pos.pr.   noun  why structure   s required

independent classi   cation of each word, without considering the
classes of its neighbors, gets 90% with enough data.

syntactic context is an important clue for disambiguating words.

    leaves is probably a verb if it   s followed by a proper noun
    bear is probably a noun if it   s preceded by a determiner

sequence (i.e., structured) models can achieve around 97% on this
task [toutanova et al., 2003, shen et al., 2007].

25 / 169

learning from data

id52 is perhaps the simplest example (and one of the
earliest) where it makes sense to have humans label text, then
perform supervised learning.

pos conventions are not universal!

annotators do not always agree!

26 / 169

morphology: dirty words

if you only speak english, this may come as a surprise.

problem: id60 (breaking words into
meaningful morphemes, optionally with tags)

27 / 169

                                        .nnc         pad   pau   paudan      nnc      pca   pauxpcaxvj   efn   sfn.adv      vx   ecs   ssy.morphology: dirty words

if you only speak english, this may come as a surprise.

problem: id60 (breaking words into
meaningful morphemes)
x:       (surface word sequence)
y:       (morpheme sequence)

28 / 169

agglutinative morphology

uygarla  st  ramad  klar  m  zdanm    ss  n  zcas  na

   (behaving) as if you are among those whom we could not civilize   

(more than 60 million people speak this language.)

29 / 169

interesting substrings: chunks

problem:    nd speci   c kinds of substrings

30 / 169

npppnpppthe angle of cats' ears is an important clue to their mood   the angle of cats' ears is an important clue to their mood   b-np  i-np      b-pp  i-pp     i-pp   o  b-np    i-np              i-np  b-pp  i-pp     i-ppinteresting substrings: chunks

problem:    nd speci   c kinds of substrings
x:       (sentence)
y: ((      {b, i})     {o})   

(   i-o-b    labels)

examples:

    base noun phrase chunking
       id66    (includes prepositional phrases and verb

groups)

    id39

31 / 169

base noun phrase chunking

32 / 169

npnpnpnpthe angle of cats' ears is an important clue to their mood   id39

33 / 169

on the streets around fatemi square, near the headquarters of the leading opposition candidate, mir hussein moussavi, riot police of   cers dressed in robocop gear roared down the sidewalks on motorcycles to disperse and intimidate the clots of pedestrians who gathered to share rumors and dismay.id39

34 / 169

on the streets around fatemi square, near the headquarters of the leading opposition candidate, mir hussein moussavi, riot police of   cers dressed in robocop gear roared down the sidewalks on motorcycles to disperse and intimidate the clots of pedestrians who gathered to share rumors and dismay.extreme chunks: parsing

problem:    nd compositional phrases from the whole sentence
down to the words

(cid:104)(cid:104)s, 1, 12(cid:105), (cid:104)np, 1, 5(cid:105), (cid:104)pp, 3, 5(cid:105), (cid:104)np, 4, 5(cid:105), (cid:104)vp, 6, 12(cid:105), (cid:104)np, 7,
12(cid:105), (cid:104)pp, 10, 12(cid:105), (cid:104)np, 11, 12(cid:105)(cid:105)

35 / 169

the angle of cats' ears is an important clue to their mood   snpnpppnpvpnppp         1         2       3     4         5      6   7            8              9    10    11      12extreme chunks: parsing

problem:    nd compositional phrases from the whole sentence
down to the words
x:       (sentence)
y: 2    n2 (phrase structure)

underlying this, often, is some context-free grammar.

36 / 169

nl vs. pl

undergrads who have taken programming languages often want to
use the same tools to process natural language. the problem   and
the reason nlp is married to ml and not to pl   is that natural
language syntax is riddled with ambiguities.

little hope given brain-damaged woman

37 / 169

little hope given brain-damaged
woman

    after terry   s accident, the doctor gave her family the bad

news.

38 / 169

little hope given brain-damaged
woman

    after terry   s accident, the doctor gave her family the bad

news.

    by september, mccain   s campaign sta    were not optimistic.

39 / 169

little hope given brain-damaged
woman

    after terry   s accident, the doctor gave her family the bad

news.

    by september, mccain   s campaign sta    were not optimistic.
    it was a strange christmas for little hope.

40 / 169

little hope given brain-damaged
woman

    after terry   s accident, the doctor gave her family the bad

news.

    by september, mccain   s campaign sta    were not optimistic.
    it was a strange christmas for little hope.
    if only little hope had used her gift for good rather than evil ...

41 / 169

alternative to phrases: dependency
parsing

not everyone agrees on parsing conventions!

dependency syntax focuses on relationships among words.

42 / 169

$ the angle of cats' ears is an important clue to their mood        0    1          2     3     4          5      6   7           8            9       10    11       12alternative to phrases: dependency
parsing

x:       (sentence)
y: y     {0, 1, 2, . . . , n}     2{0,1,...,n}

43 / 169

two versions of id33

projective trees correspond to derivations in a special kind of
context-free grammar [gaifman, 1965].

nonprojective trees don   t correspond to cf parses:

nonprojectivity is more important in some languages than others.

44 / 169

$ a talk is scheduled on cats' ears today45 / 169

interlude: linguistic pipeline

as we descend the iceberg, we get closer to the physical, cultural,
non-linguistic world where language gets used.

46 / 169

id102 (physical properties of speech)

phonology (units of sound)

wwwwwwwwwwwwww

orthography (units of writing)

wwwwwwwwwwwwwwwww

morphology (structure of words)

syntax (structure of sentences)

semantics (literal meaning of words and utterances)

pragmatics (acts of communication)

discourse (connected series of utterances)

47 / 169

problem: convert a sentence into a canonical meaning
representation language

meaning

48 / 169

robin swam across the river and delivered the message.robin swam across the river and delivered the message.agentagentpatientmedium   r, m, p1, p2 swimto(robin, p)     river(r)     across(r, p1, p2)      deliver(robin, m)     message(m)semantic roles:   rst order logic:meaning

problem: convert a sentence into a canonical meaning
representation language
x:       (sentence)

y: no consensus yet!

    identifying semantic roles of a verb [palmer et al., 2005]
       rst order logic expressions [zettlemoyer and collins, 2005]
    problem-speci   c meaning language [thompson et al., 1997]

49 / 169

problem: which real-world entities are mentioned and where?

grounding

50 / 169

the princeton and yale graduate has more than 16 years of federal opinions with which to gauge her proficiency as an arbiter. she spent six years as a district judge and a decade on the 2nd u.s. circuit court of appeals, but the 2001 comment promises to be a focal point of her confirmation.  conservatives such as talk radio host rush limbaugh have called her a "reverse racist." limbaugh further denounced president obama as "the greatest living example of a reverse racist."grounding

problem: which real-world entities are mentioned and where?

51 / 169

the princeton and yale graduate has more than 16 years of federal opinions with which to gauge her proficiency as an arbiter. she spent six years as a district judge and a decade on the 2nd u.s. circuit court of appeals, but the 2001 comment promises to be a focal point of her confirmation.  conservatives such as talk radio host rush limbaugh have called her a "reverse racist." limbaugh further denounced president obama as "the greatest living example of a reverse racist."grounding

problem: which real-world entities are mentioned and where in
text?

    entity detection (often seen as a kind of chunking)
    coreference resolution: which referring expressions corefer?
    grounding in ontologies

52 / 169

another dimension: multiple languages

parallel and comparable corpora are a fascinating type of data.

    sentence alignment (not widely studied now)
    word alignment
    phrase (chunk) alignment
    tree alignment
    bilingual parsing
    automatic bilingual dictionary construction

major application: machine translation

53 / 169

debates in nlp

all of these tasks are not universally seen as important!

as noted, sentence segmentation and part-of-speech tagging are
mostly    solved    (for english).

people who work on applications always question whether a
particular level of analysis helps their application, and whether the
useful ones deserve intrinsic evaluation (e.g., word alignment).

54 / 169

nlp problems on the frontier of
id170

    finding the entire predicate-argument structure of a sentence
    tasks requiring generation of text from deeper representations

    id53
    translation
       simpli   cation   

    learning about linguistic types   lexicons and ontologies
    representing discourse structure and dialog structure
    learning world knowledge from text

55 / 169

summary of some structures

nlp problem
id40
id121, tagging
morphological parsing
chunking
phrase-structure parsing
id33

x     . . . y     . . .
     
     
     
     
     
     

     
  |x|
     
((      {b, i})     {o})|x|
2    n2
{0, . . . ,|x|}     2{1,...,|x|}

56 / 169

decoding

57 / 169

x: set of possible inputs, often x =      
x: random variable taking values x     x

y: set of possible outputs
y: random variable taking values y     y
h: prediction function x     y

a decoder is an implementation of h.

notation

58 / 169

   id170    is the 2009 metaphor of choice, but before
that, circa 1999, it was    source-channel models.   

why    decoder   ?

source-channel model:

p(x = x, y = y) = p(y = y)

   p(x = x | y = y)

(cid:124)

(cid:123)(cid:122)

source

(cid:125)

(cid:124)

(cid:123)(cid:122)

channel

(cid:125)

source-channel decoding:

h(x) = argmax

y    y

= argmax

y   y

= argmax

y   y

p(y | x)
p(y)    p(x | y)

p(x)

p(y)    p(x | y)

(1)

(2)

59 / 169

decoding de   ned

generic de   nition:

h(x) = argmin

y   y

ey   p(y|x)[(cid:96)(y, x, y)]

common case when probabilistic models are used:

(cid:96)(y, x, y   ) = 1       (y, y   )

h(x) = argmax

y   y

p(y | x)

(3)

(4)

(5)

60 / 169

linear models

let g : x    y     rd denote a feature vector function that
embeds input-output pairs in euclidean space.
linear models de   ne a score parameterized by weights w     rd:

w(cid:62)g(x, y)

decoding with a linear model means    nding

h(x) = argmax

y   y

w(cid:62)g(x, y)

(6)

(7)

61 / 169

simplest recipe for id170

    input and output spaces x and y
    a feature representation g (linear model)
    a decoder h : x     y
    a method for learning the parameters w

62 / 169

on    local    features

e   ciency of decoding hinges crucially on g.

speci   cally, we often assume a speci   c structure to g: that it can
be calculated based on local parts of the structure.

this implies independence assumptions in the scoring function
w(cid:62)g.
extreme version, when y = (cid:104)y1, . . . , yn(cid:105) is a sequence:

n(cid:88)

g(x, y) =

f(x, yi, i)

(8)

(this is a non-structured classi   er for each yi!)

i=1

63 / 169

id145

combinatorial optimization problems with optimal substructure
and that break down into parts that are densely shared can often
be solved e   ciently by id145.

this relies on certain factoring properties of g. assume y breaks
into    local parts    {  i(y)}i:

g(x, y) =(cid:88)

i

key example: viterbi algorithm.

f(x,   i(y))

(9)

64 / 169

(classical) viterbi algorithm

(cid:105), each xi       

input: x = (cid:104)x1, x2, . . . , xn, (cid:56)(cid:124)(cid:123)(cid:122)(cid:125)
output: y = (cid:104) (cid:3)(cid:124)(cid:123)(cid:122)(cid:125)

xn+1

y0

, y1, y2, . . . , yn, (cid:56)(cid:124)(cid:123)(cid:122)(cid:125)
n+1(cid:89)

yn+1

assumption (id48): p(x, y) =

(cid:105), each yi       

p(yi | yi   1)p(xi | yi)

i=1

max
y   y

p(x, y) = v ((cid:56), n + 1)

v (y, i) = max
y(cid:48)     

v (y(cid:48), i     1)    p(y | y(cid:48))    p(xi | y)

v ((cid:3), 0) = 1

(10)

65 / 169

viterbi, visualized

66 / 169

yy'..................p(y'|y) p(x1|y')x1xn...input: x = (cid:104)x1, x2, . . . , xn, (cid:56)(cid:124)(cid:123)(cid:122)(cid:125)
output: y = (cid:104) (cid:3)(cid:124)(cid:123)(cid:122)(cid:125)

xn+1

y0

(classical) viterbi algorithm

(cid:105), each xi       

yn+1

(cid:105), each yi       

, y1, y2, . . . , yn, (cid:56)(cid:124)(cid:123)(cid:122)(cid:125)
n+1(cid:89)
(cid:124) (cid:123)(cid:122) (cid:125)
p(yi | yi   1)p(xi | yi)
(cid:123)(cid:122)
(cid:124)
exp w(cid:62)pn+1

exp w(cid:62)g(x,y)

p(x, y)

i=1 f (x,yi,yi   1,i)

(cid:125)

=

i=1

assumption (id48):

max
y   y

p(x, y) = v ((cid:56), n + 1)

v (y, i) = max
y(cid:48)     

v (y(cid:48), i     1)    p(y | y(cid:48))    p(xi | y)

v ((cid:3), 0) = 1

(11)

67 / 169

(generalized) viterbi algorithm

(cid:105), each xi       

input: x = (cid:104)x1, x2, . . . , xn, (cid:56)(cid:124)(cid:123)(cid:122)(cid:125)
output: y = (cid:104) (cid:3)(cid:124)(cid:123)(cid:122)(cid:125)

xn+1

y0

, y1, y2, . . . , yn, (cid:56)(cid:124)(cid:123)(cid:122)(cid:125)
n+1(cid:88)

yn+1

assumption: g(x, y) =

f(x, yi, yi   1, i)

(cid:105), each yi       

i=1

max
y   y

w(cid:62)g(x, y) = log v ((cid:56), n + 1)
v ((cid:3), 0) = 1

v (y, i) = max
y(cid:48)     

v (y(cid:48), i     1)    exp(w(cid:62)f(x, y, y(cid:48), i))

(12)

68 / 169

what features are    local   ?

generalized viterbi, as described, permits features that look at any
part of the input (words, word shape, spelling features, etc.) and
any two adjacent output symbols: f(x, yi, yi   1, i).

some things it still can   t do:

    three consecutive output symbols.
    output symbols for two instances of the same word.
    how many times have i seen output symbol y?

69 / 169

other dp algorithms

    monotonic sequence alignment/id153

[levenshtein, 1965]

    probabilistic earley   s and cky (weighted id18 parsing)
    projective id33 [eisner, 1996]
    parsing with other formalisms (combinatory categorial

grammar, id34, etc.)

70 / 169

generic id145

this technique is so beloved by nlp that there are now:

    generalizations of logic programming for dp [goodman, 1999]
    semiring-independent solvers [eisner et al., 2005]
    connections to hypergraph search [klein and manning, 2001]
    generalizations of a    for hypergraphs

[klein and manning, 2003]

    algorithms for including non-factoring features

[chiang, 2007, gimpel and smith, 2009]

bottom line: great way to think about decoders, often a good way
to implement them.

71 / 169

structures are graphs

we mention two interesting cases where decoding can be reduced
to well-known algorithms for graphs.

72 / 169

1, x(cid:48)

maximum weighted bipartite matching
m(cid:105)
2, . . . , x(cid:48)

input: two sentences x = (cid:104)x1, x2, . . . , xn(cid:105) and
x(cid:48) = (cid:104)x(cid:48)
output: y is a matching of {1, . . . , n} to {1, . . . , m}; each xi and
each x(cid:48)

j matches to at most one word in the other sequence

73 / 169

maximum weighted bipartite matching

1, x(cid:48)

m(cid:105)
2, . . . , x(cid:48)

input: two sentences x = (cid:104)x1, x2, . . . , xn(cid:105) and
x(cid:48) = (cid:104)x(cid:48)
output: y is a matching of {1, . . . , n} to {1, . . . , m}; each xi and
each x(cid:48)

j matches to at most one word in the other sequence

assumption: g(x, x(cid:48), y) = (cid:88)

f(x, x(cid:48), i, j)

(cid:104)i,j(cid:105)   y

solution: hungarian algorithm [kuhn, 1955], with
o((n + m)2 log(n + m) + nm(n + m)) runtime

application: word alignment [melamed, 2001]

74 / 169

maximum weighted (directed) spanning
tree

input: sentence x = (cid:104) (cid:3)(cid:124)(cid:123)(cid:122)(cid:125)

x0

, x1, . . . , xn(cid:105)

output: y     {0, 1, . . . , n}     2{1,...,n} de   nes a 0-arborescence,
i.e., a directed spanning tree with x0 as the root and x1, . . . , xn as
vertices; y(i) denotes y   s parent

n(cid:88)

assumption: g(x, y) =

f(x, i, y(i))

i=1

solution: chu-liu-edmonds algorithm
[chu and liu, 1965, edmonds, 1967] adapted by [tarjan, 1977],
with o(n2) runtime

applicaton: nonprojective id33
[mcdonald et al., 2005]

75 / 169

id33 features

the spanning tree approach works when we have only f(x, i, y(i)).

    parent-child features (words, word classes, lemmas)
    context features (words on either side of the parent or child)
    distance features

non-local:

    sibling features
    grandparent/grandchild features
    valency features (how many children?)
    phrase features

76 / 169

other approaches

    integer id135 [germann et al., 2001,

roth and yih, 2004, martins et al., 2009]

    reranking: replace y with the k best solutions from a simpler

model [collins, 2000, charniak and johnson, 2005]

    stacking [kou and cohen, 2007]
    belief propagation for structures [smith and eisner, 2008]
    id115 methods [finkel et al., 2006]
    search [daum  e, 2006]

note that many such approaches are tightly linked to speci   c kinds
of learning algorithms.

77 / 169

current hot topics

    coarse-to-   ne decoding [charniak and johnson, 2005]
    decoding multiple structures at once (   joint    id136)

[cohen and smith, 2007], among others

    generic tools for building decoders using dp, ilp, ...

78 / 169

break #1 (10 minutes)

next up: supervised structured natural language processing

79 / 169

supervised structured nlp

80 / 169

recap

    we   ve seen a bunch of nlp problems presented as structured

prediction problems.

    we   ve discussed the (generic) problem of decoding (i.e.,

making a prediction).

    next: how to learn the prediction model from labeled data.

81 / 169

learning setting

training data:   d = (cid:104)(cid:104)  x1,   y1(cid:105),(cid:104)  x2,   y2(cid:105) . . . ,(cid:104)  x   n ,   y   n(cid:105)(cid:105)

testing data:

  d = (cid:104)(cid:104)   xi,   yi(cid:105)(cid:105)   n

i=1

remember that the annotations   yi depend heavily on conventions
and are often subject to debate!

82 / 169

id168s

(cid:96) : y    x    y     r   0
(cid:96)(y, x, y   ) is the cost when h(x) = y but the correct answer is y   .

training usually looks like:

  n(cid:88)

i=1

min
w

(cid:96)(h(  xi),   xi,   yi) + modelcomplexity(w)

(13)

considerable e   ort has gone into making the id168 used in
training look like the evaluation function we care about on test
data.

83 / 169

id168s in nlp

nlp makes this harder with evaluation-time cost functions that:

    are not formally well-de   ned
    are not widely agreed upon
    are not unique and involve trade-o   s
    are extrinsic (embedded in systems)
    change frequently
    require humans

84 / 169

id168s in nlp

for intrinsic evaluation:

    tagging: count of words wrongly tagged
    chunking: f1 of identi   ed chunks, by chunk type
    parsing: count of incorrect phrases in a parse tree or words

wrongly attached

    coreference: precision and recall

admittedly, we   d rather see how these a   ect performance of real
systems. but extrinsic evaluations are expensive.

85 / 169

generative models

basic idea: de   ne a stochastic process that can produce x    y (or
some appropriate subset), then estimate parameters using
maximum likelihood (id113) or maximum a posteriori (map).

for nlp models, this commonly means a probabilistic grammar
(e.g., id48 or pid18).

challenge: predicting each piece of structure exactly once limits
the e   ective features g if we want to use simple estimators (   count
and normalize   ). there are also generative arbitrary-feature
models [rosenfeld, 1997, smith et al., 2007].

86 / 169

example: id48

87 / 169

1example: id48

88 / 169

y112example: id48

89 / 169

y1y1x1123example: id48

90 / 169

y1y1x1y1x1y21234example: id48

91 / 169

y1y1x1y1x1y2y1x1y2x212345example: id48

92 / 169

2n + 2y1x1y2x2ynxn......generative models of structure

pros: easy to train

cons: major restrictions on features due to markovian
independence assumptions, log-loss on x    y

bottom line: least satisfying from a machine learning perspective,
but nlp makes heavy use of them anyway.

93 / 169

why be generative?

if our goal is a particular type of prediction (here, from x to y),
there   s no reason to learn a distribution over x.

(of course, if we want our model to do many types of prediction,
then a generative model may be ideal.)

discriminative methods focus on the decoding function h.

94 / 169

discrimative structure models (take 1)

early approaches [ratnaparkhi et al., 1994, mccallum et al., 2000]
broke the output y     y into    parts    that could be built
incrementally, e.g., using probabilistic automata.

each    part    has its own discriminative classi   cation model that
depends on earlier decisions (trained using multinomials,
multinomial id28, id166s, id90, etc.).

classic case: magerman   s decision tree parser [magerman, 1995].

95 / 169

decoding with local decisions

generally state-space search or id145 implements
h.

greedy version (e.g., [nivre and scholz, 2004]):

h(x) = h(cid:96)(x, h(cid:96)   1(x,       h2(x, h1(x))       ))

(14)

96 / 169

example:    maximum id178 markov
model   

97 / 169

321y1x1y1x1y2x2n + 1y1x1y2x2ynxn......what   s wrong?

this formulation might lead to label bias [la   erty et al., 2001]:
there   s no notion of    you made a bad choice earlier, and now there
are no good options.    (this is not a problem with search or
id136; it is a problem with the model.)

training doesn   t match testing: training always assumes    earlier   
h steps were correct.

expressive power: some h cannot be represented when we make
the model incremental [smith and johnson, 2007].

[daum  e, 2006] o   ers a way to train these that tries to take
possible later decisions into account.

98 / 169

local classi   ers

pros: easy to train, approximate factored id168s, rich
   history-facing    features

cons: lots of approximations (loss, decoding during training), label
bias problem, expressive power

bottom line: least satisfying analytically, but nlp makes heavy use
of them anyway. a widely held view is that richer features often
make up for the    aws.

99 / 169

discriminative structure models (take 2)

want a global score (like generative models), discriminatively
trained (like local classi   ers), but with a single id168 on y.

solution: conditional random    elds [la   erty et al., 2001]

100 / 169

id49

id148 over y given evidence in x:

log p(y | x)     w(cid:62)g(x, y)
p(y | x) = ew(cid:62)g(x,y)
(cid:16)
z(w, x)
(cid:32)(cid:88)

= exp

(cid:32)

p(y | x) = exp

w(cid:62)

i

    exponentiated score
    partition function

(cid:17)

w(cid:62)g(x, y)     log z(w, x)

(cid:33)

f(x,   i(y))

    log z(w, x)

assume y breaks into    local parts    {  i(y)}i:

(15)

(16)

(17)

(cid:33)

(18)

101 / 169

crfs for sequence labeling

102 / 169

1y1x1y2x2ynxn......training crfs

crfs are (conditional) probabilistic models; m(c)le requires:

  n(cid:89)

i=1

max
w

p(  yi |   xi)

  n(cid:88)

i=1

w(cid:62)

         n(cid:88)

i=1

    max

w

    max

w

log p(  yi |   xi)

g(  xi,   yi)

(19)

(20)

log z(w,   xi)

(21)

  n(cid:88)

i=1

          

there is no closed-form solution. we must use iterative
optimization routines.

103 / 169

crfs: implementation

          

g(  xi,   yi)

         n(cid:88)

i=1

  n(cid:88)

i=1

w(cid:62)

max
w

log z(w,   xi)

(22)

fortunately, the above is concave and di   erentiable with respect
to w.

  n(cid:88)

i=1

   
   wj

=

(cid:0)gj(  xi,   yi)     e

y   p(y|  xi)gj(  xi, y)(cid:1)

(23)

104 / 169

crfs: implementation

calculating the objective and    rst derivatives requires a particular
kind of id136 that sums over y for a given x:

  (w, x, r) =(cid:88)

(cid:124) (cid:123)(cid:122) (cid:125)

r(x, y)

y   y

some function

exp w(cid:62)g(x, y)

(24)

for some function r. the two cases we need are

      (w, x, 1)     z(w, x)
      (w, x, gj)        z

   wj

if the features factor su   ciently locally, the above are solvable with
id145.

105 / 169

crfs: id173

like most discriminative learners, crfs tend to over   t the training
data.
most common solution is to penalize models with large |w|:

  n(cid:88)

i=1

max
w

log p(  yi |   xi)     c

d(cid:88)

j=1

w2
j

(25)

this can be seen as    l2 id173,    or as a map estimator
with a gaussian prior (zero means and   2i covariance matrix) on
w [chen and rosenfeld, 2000].

there   s also an l1 version, but it   s less widely used.

106 / 169

generalizing crfs to other structures

though the name    conditional random    eld    is not exactly
appropriate, similar models have been used for many problems
apart from sequence labeling:

    context-free parsing [finkel et al., 2008]
    id33 [smith and smith, 2007]
    coreference [mccallum and wellner, 2004]

it   s becoming more common to see approximate id136
methods, like those used in id114, for dealing with
z(w,   xi) and    z
   wj

.

107 / 169

useful extension of crf-like models

sometimes some of the annotation is missing!

perhaps:

    we believe the annotators could have used more    ne-grained

labels [matsuzaki et al., 2005, petrov et al., 2006].

    we believe there are structural patterns that help explain the

phenomenon, but we are agnostic about the details
[wang et al., 2007, das and smith, 2009],

    or we think humans are incapable of    nding or agreeing about

those patterns [blei and mcauli   e, 2008].

108 / 169

crf-like models with latent variables

if the missing structure is not the essential desired output, i.e., x
and y are observed but some connection between them is missing,
we can generalize crfs (and related) to permit latent variables,
here denoted z.

p(y | x) = (cid:88)
p(y, z | x)
(cid:80)
(cid:80)
z   z exp w(cid:62)g(x, y, z)
y(cid:48)   y,z   z exp w(cid:62)g(x, y(cid:48), z)

z   z

=

(26)

(27)

109 / 169

example

110 / 169

cats prefer to avoid the water.cats hate getting wet.?paraphrasecats prefer to avoid the water.cats prefer running water.?not paraphrasecrfs

pros: probabilistic interpretation (builds on id114),
extends to latent variable models,    exible id173

cons: restricted to log-loss approximation to 0-1 loss on y,
requires z(w,   xi)

bottom line: most promising for scenarios where we wish to
preserve uncertainty/ambiguity, but computationally expensive.

111 / 169

discriminative structure models (take 3)

structured id88 [collins, 2002]:

w     w + g(  xi,   yi)     g(  xi, h(  xi))

(28)

usually weight vectors over all iterations are averaged or (more
expensively) a vote is taken, to deal with the problem of oscillation.

112 / 169

structured id88

it   s very easy to implement. what is it doing?
let (cid:54) yi denote {y     y | y (cid:54)=   yi}.

    it   s searching for a hyperplane in rd that puts {g(  xi,   yi)}   n

on one side and {g(  xi, y) | y    (cid:54) yi}   n

i=1 on the other.
    it will    nd such a hyperplane, eventually, if one exists.
    this implies the use of 0-1 loss: every output in (cid:54) yi is equally

i=1

bad.

113 / 169

structured id88

114 / 169

g(x, y*)structured id88

pros: easy to implement, only need a decoder

cons: brittle id168 (0-1), no built-in mechanism to avoid
over   tting

bottom line: try this    rst, once you have a decoder, to see if your
features make any sense at all.

115 / 169

discriminative structure models (take 4)

recall from classi   cation: the id88    nds some hyperplane,
while id166s look for one with a wide margin.

rationale: wide margins are expected to generalize better.

further, we   d like to take into account alternative id168s,
not just 0-1.

(note: id166s often make people think about kernels. kernels in
id170 are a hot topic, but out of scope.)

116 / 169

large margin structured classi   ers

general setting [taskar et al., 2003, tsochantaridis et al., 2005]:

min

w   rd,     0

1
2

(cid:107)w(cid:107)2 + c

  i

(29)

  n(cid:88)

i=1

{  i}i are slack variables that express the extent to which the model
doesn   t do the    right thing    on instance i. the constraints can be
(   y, i):

w(cid:62)(cid:0)g(  xi,   yi)     g(  xi, y)(cid:1)    
                (cid:96)(y,   xi,   yi)       i,y margin rescaling

1       i,y
0

(cid:96)(y,  xi,  yi)

slack rescaling version
reminiscent of id88

(30)

exponentially many constraints! m3ns are a variant with margin
rescaling that exploit factored g and (cid:96).

117 / 169

hyperplane view, again

118 / 169

g(x, y*)objective, again

collapsing the    (margin rescaling version):

2(cid:107)w(cid:107)2 + c(cid:80)   n

1

i=1

min
w   rd

(cid:16)   w(cid:62)g(  xi,   yi)
(cid:16)

+ max
y   y

(31)

(cid:17)(cid:19)

w(cid:62)g(  xi, y + (cid:96)(y,   xi,   yi)

119 / 169

recall standard decoding:

loss-augmented decoding

h(x) = argmax

y   y

w(cid:62)g(x, y)

(32)

loss-augmented decoding (additive, used with margin rescaling):

h(cid:48)(x, y) = argmax
y(cid:48)   y

w(cid:62)g(x, y(cid:48)) + (cid:96)(y(cid:48), x, y)

(33)

manageable if g and (cid:96) factor the same way; exploited by m3ns.

multiplicative version (used in slack rescaling):

h(cid:48)(x, y) = argmax
y(cid:48)   y

w(cid:62)g(x, y(cid:48))    (cid:96)(y(cid:48), x, y)

(34)

usually intractable.

120 / 169

implementation

specialized algorithms have been developed for both the m3n and
id166struct variations:

    subgradient methods [ratli    et al., 2006] and cutting planes

[tsochantaridis et al., 2005], both of which rely on
loss-augmented decoding.

    for m3ns, dual extragradient [taskar et al., 2005],

exponentiated gradient [bartlett et al., 2004].

       passive-aggressive    online algorithms [crammer et al., 2006];
connection to objectives not entirely clear, but one version is
very close to subgradient with c     +   .

121 / 169

   passive aggressive    online learners

rather than starting from the objective function, these start with
the id88 update idea and try to augment it with loss,
margin, and/or id173. see [crammer et al., 2006].

122 / 169

large margin structured classi   ers

pros: id168    exibility, lots of recent advances in making
training feasible,    exible id173 (in principle)

cons: can require loss-augmented decoding, no probabilistic
interpretation

bottom line: best approach in theory, and becoming more and
more usable.

123 / 169

crfs approximate structured id166s.

assume 0-1 loss. we can substitute the    in the structured id166
objective to get:

  n(cid:88)

i=1

(cid:107)w(cid:107)2 + c

1
2

    1
2

(cid:107)w(cid:107)2 + c

w(cid:62)g(  xi, y)

   w(cid:62)g(  xi,   yi) + max
y   y

  n(cid:88)

i=1

   w(cid:62)g(  xi,   yi) + log(cid:88)
(cid:123)(cid:122)

y   y

(cid:124)

exp

(cid:125)

softmax

(35)

w(cid:62)g(  xi, y)

so with the softmax approximation, we get the crf objective,
with quadratic id173.

124 / 169

what   s special about structured nlp?

    good features are worth a lot; they often come from

linguistics.

    id136 is expensive, so scalability and fast convergence are

important.

    robustness to imperfect data is important.
    robustness to inexact id136 (including decoding) is also

important.

    all things equal, crfs and large-margin classi   ers are both
state-of-the-art, though generative models often sneak in as
   features.   

125 / 169

connection to (weighted) grammars

for several decades, much of nlp was about engineering
grammars for analyzing these structures.

the ambiguity problem led us to statistics.

grammars can be used to impose hard constraints (i.e., de   ne y)
or soft ones (weighted features) within a id170
model.

examples of other grammar classes made into statistical models:
[schabes, 1992, abney, 1997, clark and curran, 2007].

126 / 169

current challenges

    adapting across domains (   d and   d come from di   erent

distributions)

    coping with data sparseness: smoothing, id173,

feature selection

    noise or uncertainty in annotations
    dependence on annotation conventions
    handling features and id168s that do not factor

(approximate structured id136)

    training expense makes tuning of id173 constants,

feature selection, and other    meta-training    onerous.

127 / 169

break #2 (10 minutes)

next up: unsupervised structured natural language processing

128 / 169

unsupervised structured nlp

129 / 169

recap

    we   ve seen a bunch of nlp problems presented as structured

prediction problems.

    we   ve discussed the (generic) problem of decoding (i.e.,

making a prediction).

    we saw a bunch of learning methods for learning these

predictors, supervised.

    next: how to learn the prediction model from unlabeled data.

130 / 169

learning setting

training data:   d = (cid:104)  x1,   x2, . . . ,   x   n(cid:105)

testing data:

  d = (cid:104)(cid:104)   xi,   yi(cid:105)(cid:105)   n

i=1

131 / 169

why    un,    not    semi   ?

semisupervised learning: some annotated training data and a
large set of unannotated training data.

there are many approaches to this. in practice, they mostly boil
down to one of:

    combining a supervised objective and an unsupervised

objective

    using unlabeled data to create features or id173 terms

for supervised learners

    id64 algorithms that gradually modify the training

dataset for a supervised learner

    unsupervised learning with constraints obtained from the

annotated data

so we consider    pure    unsupervised learning, both for use on its
own and within semisupervised learning.

132 / 169

how unsupervised is unsupervised?

for any talk on unsupervised nlp, there will be at least one
member of the audience left unconvinced about whether the
approach was    really    unsupervised.

we will use the term broadly   if unconventionally   to mean that
the training data are incomplete. sometimes the    missing parts   
are not intrinsically interesting (often called    latent-variable   
models), but sometimes they are the desired output (what most
people call    unsupervised   ).

133 / 169

notation

x: set of possible inputs, often x =      
x: random variable taking values x     x

y: set of possible outputs, unseen even during training
y: random variable taking values y     y

134 / 169

expectation-maximization

most people   s    rst exposure comes from mixtures of gaussians for
id91 data in rd.
assume a generative model p     p over x    y.

id113:

max
p   p

  n(cid:89)

i=1

p(  xi)     max
p   p

  n(cid:88)

i=1

log p(  xi)     max
p   p

  n(cid:88)

i=1

log(cid:88)

y   y

p(  xi, y)

(36)

135 / 169

em can be seen as a coordinate ascent algorithm that alternates
between choosing p     p and auxiliary distibutions qi     q.

em

log p(  xi)     dkl(qi(cid:107)p(   |   xi))

(37)

  n(cid:88)

i=1

max
p   p,{qi}   n

i=1   q

the e step optimizes with respect to each qi, holding p    xed, by
solving

qi(y)     p(y |   xi)

(38)

the m step optimizes p, holding the qi    xed; the result looks just
like supervised id113 for family p with fractional counts for y
values:

  n(cid:88)

(cid:88)

i=1

y   y

max
p   p

qi(y) log p(  xi, y)

(39)

136 / 169

structured em (e step)

in id170, y is huge, so qi needs to be represented
compactly, usually using su   cient statistics. in nlp, the e step
almost always uses id145.

the su   cient statistics usually look a lot like    z
   wj
algorithms are almost identical to those for crf models.

, and the

137 / 169

structured em (m step)

making the m step e   cient generally requires us to restrict p to
be a relatively simple family, e.g., stochastic grammars.

em for id48s      baum-welch training      is one of the earliest
examples of em.

138 / 169

one interpretation of em

em is just trying to accomplish:

  n(cid:88)

log(cid:88)

i=1

y   y

max
p   p

p(  xi, y)

(40)

the whole algorithm can be derived from the above; typically:

    the e step exploits independence assumptions in p(  xi, y).
    the m step manages the constraint that p     p.

alternative optimization methods are possible!

139 / 169

examples of em for nlp

    context-free grammars [carroll and charniak, 1992]
    word alignment [brown et al., 1993]
    part-of-speech tagging [merialdo, 1994]
    coreference resolution [charniak, 2001]
    bracketing structure [klein and manning, 2002]
    dependency structure [klein and manning, 2004]

140 / 169

em for nlp

    generative models that work in supervised mode often fail

unsupervised [carroll and charniak, 1992].

    success is most probable when the problem is highly

constrained, the initialization is carefully designed, and/or the
model comes with strong bias.

    rich features pose a problem, because id113 for generative

id148 over x    y have complicated, possibly
divergent z.

141 / 169

discriminative, unsupervised learning?

this is sort of a contradiction.
unsupervised learning assumes that (cid:104)  yi(cid:105)   n

i are unknown.

discriminative learning aims to minimize loss on training data,
training toward (cid:104)  yi(cid:105)   n
i , the missing information. so loss of any
hypothesis y is also unknown.

one way of mixing discriminative methods with unsupervised
learning: contrastive estimation, which maximizes
p(x = x |   (x) =   (x)) [smith and eisner, 2005]

142 / 169

contrastive estimation

(cid:80)
id113 for generative id148 with hidden variable y:
(cid:80)
y   y exp w(cid:62)g(  xi, y)

log p(  xi)     max

  n(cid:88)

  n(cid:88)

(cid:80)

log

max
w

w

y   y exp w(cid:62)g(x, y)

x   x

i=1

i=1

(41)

numerator is like z for crfs (expensive). denominator is even
worse!

idea: make this a conditional model by conditioning against a
variable n that constrains x:

  n(cid:88)

i=1

(cid:80)
(cid:80)
y   y exp w(cid:62)g(  xi,   ni, y)

y   y exp w(cid:62)g(x,   ni, y)

x     ni

(cid:80)

max
w

log p(  xi |   ni)     max

w

log

  n(cid:88)

i=1

(42)

143 / 169

contrastive neighborhoods for sentences

144 / 169

this sentence is grammatical.sentence is grammatical.this is grammatical.this sentence grammatical.this sentence is.sentence this is grammatical.this is sentence grammatical.this sentence grammatical is.the linguistic intuition behind
contrastive estimation

  xi is a positive example (e.g., a sentence).

any grammatical sentence suggests implicitly that a certain set of
sentences is ungrammatical (or less grammatical), namely, those
that are perturbations of it. we call these the neighborhood; for
  xi, they are   ni     x.

on average,   ni should be less good than   xi. instead of simply
making   xi more likely, we make it likely at the expense of   ni.

depending on how we represent   ni, we can make learning e   cient
(id145).

145 / 169

bayesian methods in nlp

146 / 169

bayesian methods in nlp

the general idea is to manage more of our uncertainty; not just
about y, but also about w. there are two    avors for models:

    parametric approaches: dimensionality of w, features, and y

are all assumed known.

    nonparametric approaches: richness of the model depends

on the data.

there are two methodologies:

    full bayesian: write down prior, do id136 on the data.
    empirical bayesian: estimate parameters of prior from data.

147 / 169

bayesian unsupervised structured
prediction

(i use the linear model notation.)

(cid:90) (cid:88)

y   y

p(x | v) =

p(x, y | w)p(w | v)dw

(43)

    full bayesian: write down v,    gure out distribution over y

(and maybe w).

    empirical bayesian: estimate v from data.

148 / 169

bayesian unsupervised structured
prediction

p(x | v) =

p(x, y | w)p(w | v)dw

(44)

(cid:90) (cid:88)

y   y

common choices for v:

    dirichlet distribution(s) [blei et al., 2003,

goldwater and gri   ths, 2007, johnson et al., 2007]

    (hierarchical) pitman-yor process
[teh, 2006, goldwater et al., 2006]

    logistic normal distribution(s)

[blei and la   erty, 2006, cohen et al., 2008]

149 / 169

example: id44
model [blei et al., 2003]

x: (     )    (document collection)
y: ({1, . . . , k}   )    (   topic    for each word token) and ((cid:52)k)    (topic
distribution per document)
w:    = {  x|y}x     ,y   {1,...,k} are word distributions given topics,   
are document-speci   c topic distributions

v: dirichlet distributions over   

p(x, y, w | v) = dirichlet(   | v)

n(cid:89)

i=1

  yi  xi|yi

(45)

empirical bayes: learn v and   

150 / 169

examples of bayesian nlp

    topic models [blei et al., 2003]
    id40 [goldwater et al., 2006]
    part-of-speech tagging [goldwater and gri   ths, 2007]
    syntactic category re   nement

[liang et al., 2007, finkel et al., 2007]

    id33 [cohen et al., 2008]
    coreference resolution [haghighi and klein, 2007]

151 / 169

approximate id136

in principle, the challenge is the same as approximate id136 for
supervised crfs with non-local features.

here, there are usually integrals thrown into the mix, to deal with
w (which is continuous).

variational id136 is widely used and often exploits dynamic
programming with structured y [cohen et al., 2008].

152 / 169

uncertainty about bayesian nlp

while mathematically elegant, bayesian methods haven   t yet
revolutionized nlp.

pros: a    language    for talking about hidden structure and
modeling it, as well as breaking independence assumptions; generic
techniques for (approximate) id136 and learning, priors could
be a way to encode knowledge

cons: many approximations for e   ciency, not always clear how to
encode linguistic knowledge in priors

153 / 169

a new twist?

we   ve discussed how lots of nlp problems can be transformed
into the language of machine learning, using id170.

if you consider hierarchical generative models (bayesian or not),
they begin to look like grammars.

do probabilistic grammars o   er a platform for describing graphical
models? see, e.g., [johnson et al., 2006].

credit for suggesting this idea goes to mark johnson.

154 / 169

155 / 169

important points about unsupervised
learning

    in unsupervised learning, the objective function is usually not
convex (but see [xu et al., 2006]) ; we mainly hope for local
optima. initialization can therefore be very important.

    local optima seem to explode combinatorially because of

symmetries among models.

    bias (priors, constraints, contrast neighborhoods, etc.): very

important for avoiding degenerate solutions.

    unsupervised nlp is basically like unsupervised everything

else, only harder.

156 / 169

tutorial summary

    discussed linguistic analysis problems that are examples of

id170.

    presented algorithmic tools used for making structured

predictions (decoding).

    presented the dominant techniques used in nlp for learning

to make id170s from
    complete data (supervisedly) and
    incomplete data (unsupervisedly).

157 / 169

themes

nlp is a great proving ground for ml ideas.

nlp    tasks    only partially    t ml expectations: loss, assumptions,
and even the output space are always up for debate.

separating models from learning algorithms and both from
id136 methods and id168s has some advantages, and
nlp is headed in that direction.

further abstraction: availability of annotated/unannotated training
data, and how much you trust it.

158 / 169

acknowledgments

    students in    language and stats ii    at cmu in 2006   8
    ph.d. students: shay cohen, dipanjan das, kevin gimpel,
mike heilman, andr  e martins, nathan schneider, tae yano
    colleagues: william cohen, mike collins, hal daum  e, jason
eisner, sharon goldwater, mark johnson, dan klein, john
la   erty, chris manning, fernando pereira, david smith

159 / 169

references i

abney, s. p. (1997).
stochastic attribute-value grammars.
computational linguistics, 23(4):597   617.

bartlett, p. l., collins, m., taskar, b., and mcallester, d. (2004).
exponentiated gradient algorithms for large-margin structured classi   cation.
in nips.

blei, d. and mcauli   e, j. (2008).
supervised topic models.
in advances in nips 20.

blei, d., ng, a., and jordan, m. (2003).
id44.
journal of machine learning research, 3:993   1022.

blei, d. m. and la   erty, j. d. (2006).
correlated topic models.
in advances in nips.

brown, p. f., pietra, s. a. d., pietra, v. j. d., and mercer, r. l. (1993).
the mathematics of id151: parameter estimation.
computational linguistics, 19(2):263   311.

carroll, g. and charniak, e. (1992).
two experiments on learning probabilistic dependency grammars from corpora.
technical report, brown university.

160 / 169

references ii

charniak, e. (2001).
unsupervised learning of name structure from coreference data.
in proc. of naacl.

charniak, e. and johnson, m. (2005).
coarse-to-   ne n-best parsing and maxent discriminative reranking.
in proc. of acl.

chen, s. and rosenfeld, r. (2000).
a survey of smoothing techniques for me models.
ieee transactions on speech and audio processing, 8(1):37   50.

chiang, d. (2007).
hierarchical phrase-based translation.
computational linguistics, 33(2):201   228.

chu, y. j. and liu, t. h. (1965).
on the shortest arborescence of a directed graph.
science sinica, 14:1396   1400.

clark, s. and curran, j. r. (2007).
wide-coverage e   cient statistical parsing with id35 and id148.
computational linguistics, 33(4).

cohen, s. b., gimpel, k., and smith, n. a. (2008).
logistic normal priors for unsupervised probabilistic grammar induction.
in nips 21.

cohen, s. b. and smith, n. a. (2007).
joint morphological and syntactic disambiguation.
in proc. of emnlp-conll.

161 / 169

references iii

collins, m. (2000).
discriminative reranking for natural language parsing.
in proc. of icml.

collins, m. (2002).
discriminative training methods for id48: theory and experiments with id88
algorithms.
in proc. of emnlp.

crammer, k., dekel, o., keshet, j., shalev-schwartz, s., and singer, y. (2006).
online passive aggressive algorithms.
journal of machine learning research, 7.

das, d. and smith, n. a. (2009).
paraphrase identi   cation as probabilistic quasi-synchronous recognition.
in proc. of acl-ijcnlp.

daum  e, h. (2006).
practical structured learning techniques for natural language processing.
phd thesis, university of southern california, los angeles, ca.

edmonds, j. (1967).
optimum branchings.
journal of research of the national bureau of standards, 71b:233   240.

eisner, j. (1996).
three new probabilistic models for id33: an exploration.
in proc. of coling.

162 / 169

eisner, j., goldlust, e., and smith, n. a. (2005).
compiling comp ling: practical weighted id145 and the dyna language.
in proc. of hlt-emnlp.

references iv

finkel, j. r., grenager, t., and manning, c. d. (2007).
the in   nite tree.
in proc. of acl.

finkel, j. r., kleeman, a., and manning, c. d. (2008).
e   cient, feature-based, conditional random    eld parsing.
in proc. of acl-hlt.

finkel, j. r., manning, c. d., and ng, a. y. (2006).
solving the problem of cascading errors: approximate bayesian id136 for linguistic annotation pipelines.
in proc. of emnlp.

gaifman, h. (1965).
dependency systems and phrase-structure systems.
information and control, 8.

germann, u., jahr, m., knight, k., marcu, d., and yamada, k. (2001).
fast decoding and optimal decoding for machine translation.
in proc. of acl.

gimpel, k. and smith, n. a. (2009).
cube summing, approximate id136 with non-local features, and id145 without
semirings.
in proc. of eacl.

goldwater, s. and gri   ths, t. l. (2007).
a fully bayesian approach to unsupervised part-of-speech tagging.
in proc. of acl.

163 / 169

references v

goldwater, s., gri   ths, t. l., and johnson, m. (2006).
contextual dependencies in unsupervised id40.
in proc. of coling-acl.

goodman, j. (1999).
semiring parsing.
computational linguistics, 25(4):573   605.

haghighi, a. and klein, d. (2007).
unsupervised coreference resolution in a nonparametric bayesian model.
in proc. of acl.

johnson, m., gri   ths, t., and goldwater, s. (2007).
bayesian id136 for pid18s via id115.
in proc. of naacl.

johnson, m., gri   ths, t. l., and goldwater, s. (2006).
adaptor grammars: a framework for specifying compositional nonparametric bayesian models.
in proc. of nips.

klein, d. and manning, c. (2001).
parsing and hypergraphs.
in proc. of iwpt.

klein, d. and manning, c. d. (2002).
a generative constituent-context model for improved grammar induction.
in proc. of acl.

klein, d. and manning, c. d. (2003).
a    parsing: fast exact viterbi parse selection.
in proc. of hlt-naacl, pages 119   126.

164 / 169

references vi

klein, d. and manning, c. d. (2004).
corpus-based induction of syntactic structure: models of dependency and constituency.
in proc. of acl.

kou, z. and cohen, w. w. (2007).
stacked id114 for e   cient id136 in markov random    elds.
in proceedings of sdm.

kuhn, h. w. (1955).
the hungarian method for the assignment problem.
naval research logistics quarterly, 2:83   87.

la   erty, j., mccallum, a., and pereira, f. (2001).
conditional random    elds: probabilistic models for segmenting and labeling sequence data.
in proc. of icml.

levenshtein, v. (1965).
binary codes capable of correcting spurious insertions and deletions of ones.
problems of information transmission, 1:8   17.

liang, p., petrov, s., jordan, m., and klein, d. (2007).
the in   nite pid18 using hierarchical dirichlet processes.
in proc. of emnlp-conll.

magerman, d. m. (1995).
statistical decision-tree models for parsing.
in proc. of acl.

martins, a. f. t., smith, n. a., and xing, e. p. (2009).
concise integer id135 formulations for id33.
in proc. of acl-ijcnlp.

165 / 169

references vii

matsuzaki, t., miyao, y., and tsujii, j. (2005).
probabilistic id18 with latent annotations.
in proc. of acl.

mccallum, a., freitag, d., and pereira, f. (2000).
maximum id178 markov models for information extraction and segmentation.
in proc. of icml.

mccallum, a. and wellner, b. (2004).
conditional models of identity uncertainty with application to noun coreference.
in nips.

mcdonald, r., pereira, f., ribarov, k., and haji  c, j. (2005).
non-projective id33 using spanning tree algorithms.
in proc. of hlt-emnlp.

melamed, i. d. (2001).
empirical methods for exploiting parallel texts.
mit press.

merialdo, b. (1994).
tagging english text with a probabilistic model.
computational linguistics, 20(2):155   72.

nivre, j. and scholz, m. (2004).
deterministic id33 of english text.
in proc. of coling.

palmer, m., gildea, d., and kingsbury, p. (2005).
the proposition bank: a corpus annotated with semantic roles.
computational linguistics, 31(1).

166 / 169

references viii

petrov, s., barrett, l., thibaux, r., and klein, d. (2006).
learning accurate, compact, and interpretable tree annotation.
in proc. of coling-acl.

porter, m. f. (1980).
an algorithm for su   x stripping.
program, 14(3):130   137.

ratli   , n., bagnell, j., and zinkevich, m. (2006).
subgradient methods for maximum margin structured learning.
in icml workshop on learning in structured outputs spaces.

ratnaparkhi, a. (1996).
a maximum id178 part-of-speech tagger.
in proc. of emnlp.

ratnaparkhi, a., roukos, s., and ward, r. t. (1994).
a maximum id178 model for parsing.
in proc. of icslp.

rosenfeld, r. (1997).
a whole sentence maximum id178 language model.
in proc. of asru.

roth, d. and yih, w.-t. (2004).
a id135 formulation for global id136 in natural language tasks.
in proc. of conll.

schabes, y. (1992).
stochastic lexicalized tree-adjoining grammars.
in proc. of coling.

167 / 169

references ix

shen, l., satta, g., and joshi, a. k. (2007).
guided learning for bidirectional sequence classi   cation.
in proc. of acl.

smith, d. a. and eisner, j. (2008).
id33 by belief propagation.
in proc. of emnlp.

smith, d. a. and smith, n. a. (2007).
probabilistic models of nonprojective dependency trees.
in proc. of emnlp-conll.

smith, n. a. and eisner, j. (2005).
contrastive estimation: training id148 on unlabeled data.
in proc. of acl.

smith, n. a. and johnson, m. (2007).
weighted and id140 are equally expressive.
computational linguistics, 33(4):477   491.

smith, n. a., vail, d. l., and la   erty, j. d. (2007).
computationally e   cient m-estimation of log-linear structure models.
in proc. of acl.

tarjan, r. e. (1977).
finding optimum branchings.
networks, 7(1):25   36.

taskar, b., guestrin, c., and koller, d. (2003).
max-margin markov networks.
in advances in nips 16.

168 / 169

references x

taskar, b., lacoste-julien, s., and jordan, m. (2005).
id170 via the extragradient method.
in nips.

teh, y. w. (2006).
a hierarchical bayesian language model based on pitman-yor processes.
in proc. of acl.

thompson, c. a., mooney, r. j., and tang, l. r. (1997).
learning to parse natural language database queries into logical form.
in proc. of workshop on automata induction, grammatical id136, and id146.

toutanova, k., klein, d., manning, c. d., and singer, y. (2003).
feature-rich part-of-speech tagging with a cyclic dependency network.
in proc. of hlt-naacl.

tsochantaridis, i., joachims, t., hofmann, t., and altun, y. (2005).
large margin methods for structured and interdependent output variables.
journal of machine learning research, 6:1453   1484.

wang, m., smith, n. a., and mitamura, t. (2007).
what is the jeopardy model? a quasi-synchronous grammar for qa.
in proc. of emnlp-conll.

xu, l., wilkinson, d., southey, f., and schuurmans, d. (2006).
discriminative unsupervised learning of structured predictors.
in proc. of icml.

zettlemoyer, l. s. and collins, m. (2005).
learning to map sentences to logical form: structured classi   cation with probabilistic categorial grammars.
in proc. of uai.

169 / 169

