   #[1]magenta

   [2]magenta logo

   (button)
   [3]get started [4]demos [5]blog [6]research [7]community

   [8]get started [9]demos [10]blog [11]research [12]community

performance id56: generating music with expressive timing and dynamics

   jun 29, 2017
   [13]ian simon [14]iansimon [15]iansimon
   sageev oore [16]osageev [17]osageev

   we present performance id56, an lstm-based recurrent neural network
   designed to model polyphonic music with expressive timing and dynamics.
   here   s an example generated by the model:

   note that this isn   t a performance of an existing piece; the model is
   also choosing the notes to play,    composing    a performance directly.
   the performances generated by the model lack the overall coherence that
   one might expect from a piano composition; in musical jargon, it might
   sound like the model is    noodling       playing without a long-term
   structure. however, to our ears, the local characteristics of the
   performance (i.e. the phrasing within a one or two second time window)
   are quite expressive.

   in the remainder of this post, we describe some of the ingredients that
   make the model work; we believe it is the training dataset and musical
   representation that are most interesting, rather than the neural
   network architecture.

overview

   expressive timing and dynamics are an essential part of music. listen
   to the following two clips of the same chopin piece, the first of which
   has been stripped of these qualities:
   chopin (quantized)
   chopin (performed by sageev oore)

   the first clip is just a direct rendering of the score, but with all
   notes at the same volume and quantized to 16th notes. the second clip
   is a midi-recorded human performance with phrasing. notice how the same
   notes lead to an entirely different musical experience. that difference
   motivates this work.

   performance id56 generates expressive timing and dynamics via a stream
   of midi events. at a basic level, midi consists of precisely-timed
   note-on and note-off events, each of which specifies the pitch of the
   note. note-on events also include velocity, or how hard to strike the
   note.

   these events are then imported into a standard synthesizer to create
   the    sound    of the piano. in other words, the model only determines
   which notes to play, when to play them, and how hard to strike each
   note. it doesn   t create the audio directly.

dataset

   the model is trained on the [18]yamaha e-piano competition dataset,
   which contains midi captures of ~1400 performances by skilled pianists.
   [19]a prior blog post by iman malik also found this dataset useful for
   learning dynamics (velocities) conditioned on notes, while in our case
   we model entire musical sequences with notes and dynamics.

   the yamaha dataset possesses several characteristics which we believe
   make it effective in this context:
    1. note timings are based on human performance rather than a score.
    2. note velocities are based on human performance, i.e. with how much
       force did the performer strike each note?
    3. all of the pieces were composed for and performed on one single
       instrument: piano.
    4. all of the pieces were repertoire selections from a classical piano
       competition. this implies certain statistical constraints and
       coherence in the data set.

   we have also trained on a less carefully dataset having the first three
   of the above characteristics, with some success. thus far, however,
   samples generated by models trained on the yamaha dataset have been
   superior.

representation

   [pianoroll.png]

   our performance representation is a midi-like stream of musical events.
   specifically, we use the following set of events:
     * 128 note-on events, one for each of the 128 midi pitches. these
       events start a new note.
     * 128 note-off events, one for each of the 128 midi pitches. these
       events release a note.
     * 100 time-shift events in increments of 10 ms up to 1 second. these
       events move forward in time to the next note event.
     * 32 velocity events, corresponding to midi velocities quantized into
       32 bins. these events change the velocity applied to subsequent
       notes.

   the neural network operates on a one-hot encoding over these 388
   different events. a typical 30-second clip might contain ~1200 such
   one-hot vectors.

   it   s worth going into some more detail on the timing representation.
   [20]previous [21]magenta [22]models used a fixed metrical grid where a)
   output was generated for every time step, and b) the step size was tied
   to a fixed meter e.g. a 16th note at a particular tempo. here, we
   discard both of those conventions: a time    step    is now a fixed
   absolute size (10 ms), and the model can skip forward in time to the
   next note event. this fine quantization is able to capture more
   expressiveness in note timings. and the sequence representation uses
   many more events in sections with high note density, which matches our
   intuition.

   one way to think about this performance representation is as a
   compressed version of a fixed step size representation, where we skip
   over all steps that consist of    just hold whatever notes you were
   already playing and don   t play any new ones   . [23]as observed by bob
   sturm, this frees the model from having to learn to repeat those steps
   the desired number of times.

preprocessing

   to create additional training examples, we apply time-stretching
   (making each performance up to 5% faster or slower) and transposition
   (raising or lowering the pitch of each performance by up to a major
   third).

   we also split each performance into 30-second segments to keep each
   example of manageable size. we find that the model is still capable of
   generating longer performances without falling over, though of course
   these performances have little-to-no long-term structure. here   s a
   5-minute performance:

more examples

   the performance at the top of the page is one of the better ones from
   performance id56, but almost all samples from the model tend to have
   interesting moments. here are some additional performances generated by
   the model:

temperature

   can we control the output of the model at all? generally, this is an
   open research question; however, one typical knob available in such
   models is a parameter referred to as temperature that affects the
   randomness of the samples. a temperature of 1.0 uses the model   s
   predicted event distribution as is. this is the setting used for all
   previous examples in this post.

   decreasing temperature reduces the randomness of the event
   distribution, which can make performances sound repetitive. for small
   decreases this may be an improvement, as some repetition is natural in
   music. at 0.8, however, the performance seems to overly fixate on a few
   patterns:
   temperature = 0.9
   temperature = 0.8

   increasing the temperature increases the randomness of the event
   distribution:
   temperature = 1.1
   temperature = 1.2

   here   s what happens if we increase the temperature even further:
   temperature = 1.5
   temperature = 2.0

try it out!

   we have released the code for performance id56 in our [24]open-source
   magenta repository, along with two pretrained models: [25]one with
   dynamics, and [26]one without. magenta installation instructions are
   [27]here.

   let us know what you think in our [28]discussion group, especially if
   you create any interesting samples of your own.

   an arxiv paper with more details is forthcoming. in the meantime, if
   you   d like to cite this work, please cite this blog post as
ian simon and sageev oore. "performance id56: generating music with expressive
timing and dynamics." magenta blog, 2017.
https://magenta.tensorflow.org/performance-id56

   to differentiate it from the other music generation models released by
   magenta. you can also use the following bibtex entry:
@misc{performance-id56-2017,
    author = {ian simon and sageev oore},
    title = {
        performance id56: generating music with expressive timing and dynamics
    },
    journal = {magenta blog},
    type = {blog},
    year = {2017},
    howpublished = {\url{https://magenta.tensorflow.org/performance-id56}}
}

acknowledgements

   we thank sander dieleman for discussion and pointing us to the yamaha
   dataset, and kory mathewson, david ha, and doug eck for many helpful
   suggestions when writing this post. we thank adam roberts for
   discussion and much technical assistance.

     * [29]twitter
     * [30]blog
     * [31]github
     * [32]privacy
     * [33]terms

references

   visible links
   1. https://magenta.tensorflow.org/feed.xml
   2. https://magenta.tensorflow.org/
   3. https://magenta.tensorflow.org/get-started
   4. https://magenta.tensorflow.org/demos
   5. https://magenta.tensorflow.org/blog
   6. https://magenta.tensorflow.org/research
   7. https://magenta.tensorflow.org/community
   8. https://magenta.tensorflow.org/get-started
   9. https://magenta.tensorflow.org/demos
  10. https://magenta.tensorflow.org/blog
  11. https://magenta.tensorflow.org/research
  12. https://magenta.tensorflow.org/community
  13. https://g.co/magenta/ian_simon
  14. https://github.com/iansimon
  15. https://twitter.com/iansimon
  16. https://github.com/osageev
  17. https://twitter.com/osageev
  18. http://www.piano-e-competition.com/
  19. http://imanmalik.com/cs/2017/06/05/neural-style.html
  20. https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_id56
  21. https://github.com/tensorflow/magenta/tree/master/magenta/models/polyphony_id56
  22. https://github.com/tensorflow/magenta/tree/master/magenta/models/pianoroll_id56_nade
  23. https://highnoongmt.wordpress.com/2017/06/14/even-more-endless-music-sessions/
  24. https://github.com/tensorflow/magenta/tree/master/magenta/models/performance_id56
  25. http://download.magenta.tensorflow.org/models/performance_with_dynamics.mag
  26. http://download.magenta.tensorflow.org/models/performance.mag
  27. https://github.com/tensorflow/magenta#installation
  28. http://groups.google.com/a/tensorflow.org/forum/#!forum/magenta-discuss
  29. https://twitter.com/search?q=#madewithmagenta
  30. https://magenta.tensorflow.org/blog
  31. https://github.com/tensorflow/magenta
  32. https://www.google.com/policies/privacy/
  33. https://www.google.com/policies/terms/

   hidden links:
  35. https://ai.google/
