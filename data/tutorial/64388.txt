institut 
qu  b  cois 
d   intelligence 
artificielle

institut
 des algorithmes
 d   apprentissage
 de montr  al

entra  nement: 

graphe computationnel & 

id26

ga  tan marceau caron

gaetan.marceau.caron@rd.mila.quebec

rappel 
calcul diff  rentiel

rappel de calcul 1

d  riv  e d   une fonction

bonne approx. 

locale

5/74

exemples de d  riv  es

lin  aire

sigmoid

relu

6/74

rappel de calcul 1

fonction diff  rentiable au point x

approx. lin  aire

7/74

rappel de calcul 1
composition de fonctions

c   est ma r  gle
de la cha  ne!

gottfried wilhelm leibniz 

(1676)

pfff.

8/74

graphe 
computationnel

graphe computationnel: ex. 1

z

y

x

g

f

def g(y):

... let   s do something

return z

def f(x):

... let   s do something else

return y

float x = 3.1415
z = g(f(x))

10/74

graphe computationnel: ex. 1

z

y

x

g

f

11/74

graphe computationnel: ex. 1

z

y

x

g

f

12/74

graphe computationnel: ex. 2

z

y1

y2

...

yn

x

13/74

graphe computationnel: ex. 2

z

y1

y2

...

yn

x

14/74

graphe computationnel: ex. 2

z

y1

y2

...

yn

x

15/74

graphe computationnel: ex. 3

z

y1

x1

y2

...

yn

x2

...

xm

16/74

graphe computationnel: ex. 3

z

y1

x1

y2

...

yn

x2

...

xm

17/74

graphe computationnel: ex. 3

z

y1

x1

y2

...

yn

x2

...

xm

18/74

graphe computationnel: ex. 3

19/74

id26

approx. 
lin  aire

forward

20/74

id26

approx. 
lin  aire

forward

stocke le r  sultat interm  diaire

21/74

id26

approx. 
lin  aire

forward

22/74

id26

approx. 
lin  aire

23/74

id26

stocke le r  sultat interm  diaire

approx. 
lin  aire

backward

24/74

id26

approx. 
lin  aire

backward

stocke le r  sultat interm  diaire

25/74

id26

approx. 
lin  aire

backward

on inverse l   ordre de la propagation!

26/74

id26

probl  me de 
parcours de graphe

27/74

id26

le produit sur ce chemin 
partiel a d  j     t   calcul  !

28/74

id26

programmation 
dynamique

29/74

id26

programmation 
dynamique

30/74

id26

programmation 
dynamique

seppo linnainmaa (1970): automatic differentiation
rumelhart et al., (1986): neural networks

31/74

retour en 2018...

33/74

programmation 
diff  rentiable

34/74

d  finition d   un module

param  tres

transformations

35/74

exemple

fonction 
de co  t

repr  sentations

36/74

impact d   un param  tre

perturbation 

d   un 

param  tre

37/74

impact d   un param  tre

perturbation 

d   un 

param  tre

impact positif 
ou n  gatif sur 

le co  t?

38/74

application en dl

n : nombre d   exemples
n : nombre de param  tres

39/74

impact d   un param  tre

40/74

impact d   un param  tre

algorithme des diff  rences finies

41/74

impact de tous les param  tres

complexit   de l   algorithme des 

diff  rences finies:

2*n*n propagations

(s  quentiel)

n : nombre d   exemples
n : nombre de param  tres

42/74

r  duire le nombre d   exemples
- stochastic id119

approximation

k<<n : nombre d   exemples
n : nombre de param  tres

43/74

impl  mentation en 
programmation diff  rentiable
- forward(x): transformation de la donn  e

- backward(g): transformation du gradient 

(sortie->entr  e)

- update(): calcul du gradient des param  tres

44/74

exemple

forward

binary cross-id178

perturbation 
de l   entr  e

impact positif ou n  gatif 

sur le co  t?

45/74

exemple

forward

backward

binary cross-id178

perturbation 
de l   entr  e

impact positif ou n  gatif 

sur le co  t?

46/74

exemple

sigmoid

forward

backward

perturbation 
de l   entr  e

impact sur 
la sortie?

47/74

exemple

lin  aire

forward

backward

update

perturbation 
de l   entr  e

impact sur 
la sortie?

48/74

exemple
- forward: calcul du point actuel de la 

diff  rentielle

49/74

exemple
- backward: calcul efficace de la r  gle de la cha  ne 

(version matricielle)

- multiplication matricielle: jacobienne*gradient

50/74

exemple: backward

51/74

exemple: backward

52/74

exemple: backward

53/74

exemple: backward

54/74

exemple: update

55/74

exemple: update

56/74

exemple: backward

57/74

exemple: backward

58/74

exemple: backward

59/74

exemple: backward

60/74

exemple: backward

61/74

exemple: backward

62/74

exemple: update

63/74

exemple: update

bug! on ne peut pas 
garder cette notation dans 
le cas de diff  rentielle 
d   une fonction de matrice!

64/74

exemple: update

bug! on ne peut pas 
garder cette notation dans 
le cas de diff  rentielle 
d   une fonction de matrice!

65/74

exemple: backward

66/74

exemple: backward

67/74

exemple: backward

68/74

calcul efficace: id26
- backward: calcul efficace de la r  gle de la cha  ne 

(version matricielle)

- optimisation: jacobienne*gradients
- on peut empiler plusieurs exemples

69/74

impl  mentation
- forward(x): transformation de la donn  e

- backward(g): transformation du gradient 

(sortie->entr  e)

- update(): calcul du gradient des param  tres

diff  renciation automatique

70/74

exemple adversarial

71/74

deepdream

https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html

72/74

saliency map

simonyan, karen, andrea vedaldi, and 
andrew zisserman. "deep inside 
convolutional networks: visualising image 
classification models and saliency maps." 
arxiv:1312.6034 (2013).

erhan, dumitru, et al. "visualizing 
higher-layer features of a deep network." 
university of montreal 1341.3 (2009): 1.

73/74

initialisation des param  tres

sigmoid

-6

glorot, xavier, and yoshua bengio. 
"understanding the difficulty of training 
deep feedforward neural networks." 
proceedings of the thirteenth international 
conference on artificial intelligence and 
statistics. 2010.

75/74

initialisation des param  tres
- s   assurer que le gradient ne sature pas    

l   initialisation avec la distribution des donn  es

- l   initialisation devrait   tre invariantes au nombre 

d   entr  es (variance de j = variance de i)

j

i

76/74

normalisation batchnorm

ioffe, sergey, and christian szegedy. "batch 
id172: accelerating deep network 
training by reducing internal covariate 
shift." international conference on machine 
learning. 2015.

77/74

r  gularisation dropout

srivastava, nitish, et al. "dropout: a simple 
way to prevent neural networks from 
overfitting." the journal of machine 
learning research 15.1 (2014): 1929-1958.

78/74

r  gularisation dropout

srivastava, nitish, et al. "dropout: a simple 
way to prevent neural networks from 
overfitting." the journal of machine 
learning research 15.1 (2014): 1929-1958.

79/74

r  f  rences

- cours de hugo larochelle

https://goo.gl/xfekk1

- colah   s blog on calculus on computational graph: 

https://goo.gl/tw1s9w

- michael nielsen   s chapter on id26:

https://goo.gl/umvaeg

- feature visualization in distill journal:

https://goo.gl/zeaku7

80/74

merci pour votre 

attention!

contacts

https://mila.quebec/

ga  tan marceau caron, membre mila r&d,  
gaetan.marceau.caron@rd.mila.quebec

myriam c  t  , directrice   quipe mila r&d, 
myriam.cote@rd.mila.quebec

