advanced machine learning

lecture 4: deep learning essentials

pierre geurts, gilles louppe, louis wehenkel

1 / 52outline

goal: explain and motivate the basic constructs of neural networks.

from id156 to logistic regresion

stochastic id119

from id28 to the multi-layer id88

vanishing gradients and recti ed networks

universal approximation theorem

2 / 52deep learning 101

3 / 52threshold logic unit

the threshold logic unit (mcculloch and pitts, 1943) was the  rst mathematical
model for a neuron. assuming boolean inputs and outputs, it is de ned as:

f(x) = 1
{
   i

this unit can implement:

  w  x  +b   0}

 

i

i

or(a, b) = 1
and(a, b) = 1
not(a) = 1

{a+b   0.5   0}

 

{a+b   1.5   0}

 

{   a+0.5   0}

 

therefore, any boolean function can be built witch such units.

4 / 52id88

the id88 (rosenblatt, 1957) is very similar, except that the inputs are real:

f(x) =

{1

0

 

  w  x   + b     0

i

   i

if  
otherwise

i

 

this model was originally motivated by biology, with 
and 

  ring rates.

 and 

f

x  i

w  i

 being synaptic weights

this is a cartoonesque biological model.

5 / 52let us de ne the activation function:

  (x) =

{1

0

 

if  x     0
otherwise

 

therefore, the id88 classi cation rule can be rewritten as

f(x) =   (w x + b).

t

6 / 52id156

consider training data 

(x, y)     p

 x,y

, with

,

x     rp
y     {0, 1}

.

assume class populations are gaussian, with same covariance matrix 
(homoscedasticity):

  

p (x   y) =

1

(2  )         

p

1

y )
  exp      (x         )    (x         )

( 2

t    1

y

 

7 / 52using the bayes' rule, we have:

p (y = 1   x) =

p (x   y = 1)p (y = 1)

p (x)

 

=

 

=

p (x   y = 1)p (y = 1)

p (x   y = 0)p (y = 0) + p (x   y = 1)p (y = 1)

 

 

1

1 +

p (x   y =0)p (y =0)
p (x   y =1)p (y =1)

 

 .

8 / 52using the bayes' rule, we have:

p (y = 1   x) =

p (x   y = 1)p (y = 1)

p (x)

 

p (x   y = 1)p (y = 1)

p (x   y = 0)p (y = 0) + p (x   y = 1)p (y = 1)

 

 

=

 

=

1

1 +

p (x   y =0)p (y =0)
p (x   y =1)p (y =1)

 

 .

1

it follows that with

we get

  (x) =

1 + exp(   x)

 ,

p (y = 1   x) =    log

(

p (x   y = 1)
p (x   y = 0)

  + log

p (y = 1))

p (y = 0)

 

.

9 / 52therefore,

p (y = 1   x)

=     log

p (x   y = 1)
p (x   y = 0)

  +

log

p (y = 1)
p (y = 0)

 

a

=    log p (x   y = 1)     log p (x   y = 0) + a
)

(

            

 
 

 

1

 

)
=         (x         )    (x         ) +  (x         )    (x         ) + a

( 2

t    1

t    1

1

1

0

0

 

1
2

 x +

 

1
2

 (                           ) + a

t    1
0

t    1
1

0

1

b

         

 
 

 

=     

(             )   

1

0

t    1

wt
=    w x + b
)

( t

            

         

10 / 5211 / 5211 / 5211 / 52note that the sigmoid function

  (x) =

1

1 + exp(   x)

 

looks like a soft heavyside:

therefore, the overall model 
id88.

f(x; w, b) =   (w x + b)

t

 is very similar to the

12 / 52in terms of tensor operations, the computational graph of 

f

 can be represented as:

where

white nodes correspond to inputs and outputs;

red nodes correspond to model parameters;

blue nodes correspond to intermediate operations, which themselves produce
intermediate output values (not represented).

this unit is the core component all neural networks!

13 / 52id28

same model

p (y = 1   x) =    w x + b
)

( t

as for id156.

but,

ignore model assumptions (gaussian class populations, homoscedasticity);

instead,  nd 

w, b

 that maximizes the likelihood of the data.

14 / 52we have,

arg

max
w,b
= arg

max
w,b

= arg

max
w,b

= arg

min
w,b

 

  p (d   w, b)

 

 

i

x  ,y     d
i

   
   
    i

x  ,y     d
i

i

x  ,y     d
i

i

  p (y = y     x  , w, b)

i

i

    (w x   + b) (1       (w x   + b))

y  i

t

t

i

i

1   y  i

    y   log   (w x   + b)     (1     y  ) log(1       (w x   + b))

t

t

 

 

i

i

i

l(w,b)=      (y  ,

i y^

   i

 (x  ;w,b))

i

this loss is an instance of the cross-id178

h(p, q) = e  [    log q]

p

for 

p = y    x  i

 and 

q =    x  
.
i

y^

15 / 52when 

y

 takes values in 

{   1, 1}

, a similar derivation yields the logistic loss

l(w, b) =    

    ( i

 log    y  (w x   + b)) .
)

t

i

x  ,y     d
i

i

in general, the cross-id178 and the logistic losses do not admit a minimizer
that can be expressed analytically in closed form.

however, a minimizer can be found numerically, using a general minimization
technique such as id119.

16 / 52id119

let 

l(  )

 denote a id168 de ned over model parameters 

  

 (e.g., 

w

 and  ).
b

to minimize 
towards a (local) minimum.

l(  )

, id119 uses local linear information to iteratively move

for 

         r0

d

, a  rst-order approximation around 

 can be de ned as

    0

l^ 0
2
(   +   ) = l(    ) +         l(    ) +                 .

0

0

t

  

1
2  

17 / 52a minimizer of the approximation 
  l^ 0

      (     +   )

l^ 0
(     +   )

 is given for

= 0

 

=      l(    ) +    ,

 

  

0

1
  

which results in the best improvement for the step 

   =           l(    )
0

  

.

therefore, model parameters can be updated iteratively using the update rule:

  

t+1

notes:

  =                 l(    )
t

  

t

    0

 are the initial parameters of the model;

  

 is the learning rate;

both are critical for the convergence of the update rule.

18 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 1: convergence to a local minima

19 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 2: convergence to the global minima

20 / 52example 3: divergence due to a too large learning rate

21 / 52example 3: divergence due to a too large learning rate

21 / 52example 3: divergence due to a too large learning rate

21 / 52example 3: divergence due to a too large learning rate

21 / 52example 3: divergence due to a too large learning rate

21 / 52example 3: divergence due to a too large learning rate

21 / 52stochastic id119

in the empirical risk minimization setup, 

 and its gradient decomposes as

l(  )

 

   l(  )

1
=  
n
1
=  
n

l(  )
    i
    i

i

x  ,y     d
i

x  ,y     d
i

i

    (y  , f(x  ;   ))

i

       (y  , f(x  ;   )).

i

 

therefore, in batch id119 the complexity of an update grows linearly
with the size 

 of the dataset.

n

more importantly, since the empirical risk is already an approximation of the
expected risk, it should not be necessary to carry out the minimization with great
accuracy.

22 / 52instead, stochastic id119 uses as update rule:

  

t+1

  =                  (y

t

i(t+1)

 , f(x

i(t+1)

 ;     ))

t

iteration complexity is independent of 

n

.

the stochastic process 
t
randomly at each iteration.

{       t = 1, ...}

 depends on the examples 

i(t)

 picked

23 / 52instead, stochastic id119 uses as update rule:

  

t+1

  =                  (y

t

i(t+1)

 , f(x

i(t+1)

 ;     ))

t

iteration complexity is independent of 

n

.

the stochastic process 
t
randomly at each iteration.

{       t = 1, ...}

 depends on the examples 

i(t)

 picked

batch id119

stochastic id119

24 / 52why is stochastic id119 still a good idea?

informally, averaging the update

  

t+1

  =                  (y

t

i(t+1)

 , f(x

i(t+1)

 ;     ))

t

over all choices 

i(t + 1)

 restores batch id119.

formally, if the gradient estimate is unbiased, e.g., if

e
i(t+1)

 [      (y

i(t+1)

 , f(x

i(t+1)

 ;     ))]

1
t =  
n

 

    i

       (y  , f(x  ;     ))

t

i

x  ,y     d
i

i

 

=    l(    )t

then the formal convergence of sgd can be proved, under appropriate
assumptions (see references).

when decomposing the excess error in terms of approximation, estimation and
optimization errors, stochastic algorithms yield the best generalization
performance (in terms of expected risk) despite being the worst optimization
algorithms (in terms of empirical risk) (bottou, 2011).

interestingly, if training examples 
online fashion, then sgd directly minimizes the expected risk.

x,y

x  , y       p
i

i

 

 are received and used in an

25 / 52layers

so far we considered the logistic unit 
w     rp

b     r

 and 

.

h =    w x + b
)

( t

, where 

h     r x     rp

, 

, 

these units can be composed in parallel to form a layer with 
q

 outputs:

h =   (w x + b)

t

h     rq x     rp w     rp  q b     rq

where 
element-wise sigmoid function.

, 

, 

, 

 and where 

  (   )

 is upgraded to the

26 / 52multi-layer id88

similarly, layers can be composed in series, such that:

h  0
h  1
...
h  l
f(x;   )

= x
=   (w  h   + b  )
1

0

t
1

 

 

t
l

=   (w  h
= h  l

l   1

  + b  )
l

where 

  

 denotes the model parameters 

{w  , b  , ...   k = 1, ..., l}

.

k

k

this model is the multi-layer id88, also known as the fully connected
feedforward network.

optionally, the last activation 
values 

      ry^

.

  

 can be skipped to produce unbounded output

27 / 5228 / 52to minimize 

l(  )

 with stochastic id119, we need the gradient 

        (    )
t

  

.

therefore, we require the evaluation of the (total) derivatives

d   

dw  k

 ,

d   
db  k

 

   
of the loss 

 with respect to all model parameters 

w  k b  k

, 

, for 

k = 1, ..., l

.

   
these derivatives can be evaluated automatically from the computational graph of 
using automatic differentiation.

29 / 52automatic differentiation

consider a 1-dimensional output composition 

f     g

, such that

y
u

 

= f(u)
= g(x) = (g  (x), ..., g  (x)).

m

1

 

the chain rule of total derivatives states that

dy
dx

  =

m

   

k=1

   y
   u  k

 

 

du  k
 dx

 

 

recursive  case

since a neural network is a composition of differentiable functions, the total
derivatives of the loss can be evaluated by applying the chain rule recursively
over its computational graph.

the implementation of this procedure is called (reverse) automatic
differentiation (ad).

30 / 52as a guiding example, let us consider a simpli ed 2-layer mlp and the following loss
function:

f(x; w  , w  )
2
 ; w  , w  )
2

y^

1

1

   (y,

t
2

(

=    w     w  x
t ))
1
 ) +          w           +       w        
= cross_ent(y,
y^

(

(

1 2

2 2)

for 

x     rp y     r w       r1

, 

, 

p  q w       r2

 and 

q

.

31 / 52as a guiding example, let us consider a simpli ed 2-layer mlp and the following loss
function:

f(x; w  , w  )
2
 ; w  , w  )
2

y^

1

1

   (y,

t
2

(

=    w     w  x
t ))
1
 ) +          w           +       w        
= cross_ent(y,
y^

(

(

1 2

2 2)

for 

x     rp y     r w       r1

, 

, 

p  q w       r2

 and 

q

.

32 / 52the total derivative 

from 

    w  1
 to 

d   
dw  1

 can be computed backward, by walking through all paths

 in the computational graph and accumulating the terms:

d   
dw  1
du  8
dw  1

 

 

=

      
   u  8

 

du  8
dw  1

+

      
   u  4

 

du  4
dw  1

 

= ...

33 / 52this algorithm is known as reverse-mode automatic differentiation, also called
id26.

an equivalent procedure can be de ned to evaluate the derivatives in forward
mode, from inputs to outputs.

automatic differentiation generalizes to 

n

 inputs and 

m

 outputs.

if 

n     m

, reverse-mode automatic differentiation is computationally more ef cient.

otherwise, if 

m     n

, forward automatic differentiation is better.

since differentiation is a linear operator, ad can be implemented ef ciently in
terms of matrix operations.

34 / 52vanishing gradients

training deep mlps with many layers has for long (pre-2011) been very dif cult due
to the vanishing gradient problem.

small gradients slow down, and eventually block, stochastic id119.

this results in a limited capacity of learning.

backpropagated gradients normalized histograms (glorot and bengio, 2010). 

gradients for layers far from the output vanish to zero.

35 / 52consider a simpli ed 3-layer mlp, with 

x, w  , w  , w       r
f(x; w  , w  , w  ) =    w     w     w  x

( 3

( 2

1

2

3

1

2

3

( 1 )))

.

, such that

under the hood, this would be evaluated as

u  1
u  2
u  3
u  4
u  5
 y^

 

= w  x1
=   (u  )1
= w  u  
2 2
=   (u  )3
= w  u  
3 4
=   (u  )5

 

and its derivative 

d  y^
dw  1

 as

 

d  y^
dw  1

 

=

 

=

 

     y^
   u  5
   u  5
   u  4
     (u  )5

 

 

 

   u  4
   u  3

   u  3
   u  2
     (u  )3
3    u  3

   u  2
   u  1

 

 

   u  1
   w  1
     (u  )1
2    u  1

 w  

 w  

   u  5

 

 x

36 / 52the derivative of the sigmoid activation function 

  

 is:

d  
dx
1
0      (x)      
4

d  
dx

notice that 

 for all 

.
x

 (x) =   (x)(1       (x))

37 / 52assume that weights 
zero-mean and small variance, such that with high id203 

 are initialized randomly from a gaussian with

   1     w       1

w  , w  , w  
3

1

2

.

i

then,

d  y^
dw  1

  =

     (u  )5

   u  5

1
     4

 

 

 

 

 w  3
   1

     (u  )3

   u  3

1
     4

 

 

 

 

 w  2
   1

  (u  )1
   u  1

 

 x

 

1
     4

this implies that the gradient 
layers in the network increases.

d  y^
dw  1

 exponentially shrinks to zero as the number of

 

hence the vanishing gradient problem.

in general, bounded id180 (sigmoid, tanh, etc) are prone to the
vanishing gradient problem.

note the importance of a proper initialization scheme.

38 / 52recti ed linear units

instead of the sigmoid activation function, modern neural networks are for most
based on recti ed linear units (relu) (glorot et al, 2011):

relu(x) = max(0, x)

39 / 52note that the derivative of the relu function is

d
dx

 relu(x) =

{0

1

 

if  x     0
otherwise

 

for 

x = 0

, the derivative is unde ned. in practice, it is set to zero.

40 / 52therefore,

d  y^
dw  1

  =

 w  
3

 

 

     (u  )5

   u  5
=1

 w  
2

 

 

     (u  )3

   u  3
=1

     (u  )1

   u  1
=1

 x

 

 

this solves the vanishing gradient problem, even for deep networks! (provided
proper initialization)

note that:

the relu unit dies when its input is negative, which might block gradient
descent.

this is actually a useful property to induce sparsity.

this issue can also be solved using leaky relus, de ned as

leakyrelu(x) = max(  x, x)

for a small 

       r+

 (e.g., 

   = 0.1

).

41 / 52universal approximation

theorem. (cybenko 1989; hornik et al, 1991) let 
continuous function. let 
denote the space of continuous functions on 
there exists 

i  p
v  , w  , b  , i = 1, ..., q
i

q > 0

 and 

i  p

 denote the  -dimensional hypercube, and 
f     c(i  )p

p

. given any 
 such that

i

i

  (   )

c(i  )p
 and 

   > 0

,

 be a bounded, non-constant

f (x) =

satis es

  v    (w  x + b  )
i

t
i

    i

i   q

    f(x)     f (x)    <   .

sup
x   i  p

it guarantees that even a single hidden-layer network can represent any
classi cation problem in which the boundary is locally linear (smooth);

it does not inform about good/bad architectures, nor how they relate to the
optimization procedure.

the universal approximation theorem generalizes to any non-polynomial
(possibly unbounded) activation function, including the relu (leshno, 1993).

42 / 52theorem (barron, 1992) the mean integrated square error between the estimated
network 

 and the target function 

 is bounded by

f^

f

o

2
c  f

( q

  +   log n

qp
n

)

n

where 
dimension, and 

 is the number of training points, 
q

 is the number of neurons, 

p

 is the input

c  f

 measures the global smoothness of 

f

.

combines approximation and estimation errors.

provided enough data, it guarantees that adding more neurons will result in a
better approximation.

43 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52consider the 1-layer mlp

f(x) =

    i

w  relu(x + b  ).

i

this model can approximate any smooth 1d function, provided enough hidden
units.

44 / 52(bayesian) in nite networks

consider the 1-layer mlp with a hidden layer of size 
q
function 

:

  

 and a bounded activation

= b +

  v  h  (x)

j

j

f(x)

 

h (x)
j

q

j=1

   
( j

=    a   +

p

   

i=1

 

  u  x  

i,j

i)

assume gaussian priors 
2
a       n (0,     )
j
a

.

v       n (0,     )
j
v

2 b     n (0,     )b

2 u       n (0,     )
2
u

i,j

, 

, 

 and 

45 / 52for a  xed value 
prior distributions for the weights and biases.

, let us consider the prior distribution of 

x(1)

f(x )(1)

 implied by the

we have

e[v  h  (x )] = e[v  ]e[h  (x )] = 0,

(1)

(1)

j

j

j

j

 and 

v  j

since 
hypothesis.

(1)
h  (x )
j

 are statistically independent and 

v  j

 has zero mean by

the variance of the contribution of each hidden unit 

h  j

 is

v[v  h  (x )]

2
(1)
(1) = e[(v  h  (x )) ]     e[v  h  (x )]

(1)

2

j

j

j

j

j

j

2
j

(1) 2
= e[v  ]e[h  (x ) ]
j
(1) 2
=     e[h  (x ) ],

2
v

j

which must be  nite since 

 is bounded by its activation function.

h  j

we de ne 

(1)

v (x ) = e[h  (x ) ]
(1) 2

j

, and is the same for all 
j

.

46 / 52by the central limit theorem, as 
   j=1
units, 
(1)
q    v (x )

  v  h  (x)
.

q

j

j

, to the value of 

2
v

q        

, the total contribution of the hidden

f(x )(1)

 becomes a gaussian with variance 

the bias 
 is also gaussian, of variance 
b
f(x )(1)
 is a gaussian of variance 

, so for large 
q
(1)
2
2
     + q    v (x )
.
v
b

2
    b

, the prior distribution 

accordingly, for 
gaussian of mean zero and variance 

     =     q
v
v

, for some  xed 
2
v

    v
2
v

(1)
2
     +         v (x )
b

, the prior 
 as 

f(x )(1)
q        

.

1
     2

 converges to a

for two or more  xed values 
q        
with means of zero and covariances of

(1)
x , x , ...

(2)

, the joint distribution of the outputs converges to a multivariate gaussian

, a similar argument shows that, as 

e[f(x )f(x )]

(2) =      +

(1)

2
b

2
      e[h  (x )h  (x )]
v

(1)

(2)

j

j

q

   

j=1
2
v

(2)
=      +     c(x , x )

(1)

2
b

where 

(1)

c(x , x ) = e[h  (x )h  (x )]

(2)

(1)

(2)

j

j

 and is the same for all 
j

.

47 / 52this result states that for any set of  xed points 
of 

f(x ), f(x ), ...

 is a multivariate gaussian.

(1)

(2)

(1)
x , x , ...

(2)

, the joint distribution

in other words, the in nitely wide 1-layer mlp converges towards a gaussian
process.

(neal, 1995)

48 / 52effect of depth

theorem (mont  far et al, 2014) a recti er neural network with 
hidden layers of width 
linear regions.

 can compute functions that have 

q     p

 input units and 
p
q (l   1)p p
  ((  )
q )
p

l

that is, the number of linear regions of deep models grows exponentially in 
and polynomially in 
q

.

l

even for small values of 
substantially more linear regions than shallow recti er models.

, deep recti er models are able to produce

 and 
q

l

49 / 52cooking recipe

get data (loads of them).

get good hardware.

de ne the neural network architecture as a composition of differentiable
functions.

stick to non-saturating activation function to avoid vanishing gradients.

prefer deep over shallow architectures.

optimize with (variants of) stochastic id119.

evaluate gradients with automatic differentiation.

50 / 52references

materials from the  rst part of the lecture are inspired from the excellent deep
learning course by francois fleuret (epfl, 2018).

lecture 3a: linear classi ers, id88

lecture 3b: multi-layer id88

further references:

introduction to ml and stochastic optimization

why are deep neural networks hard to train?

automatic differentiation in machine learning: a survey

51 / 52quiz

explain the model assumptions behind the sigmoid activation function.

why is stochastic id119 a good algorithm for learning? (despite
being a poor optimization procedure in general)

what is automatic differentiation?

why is training deep network with saturating id180 dif cult?

study the importance of weight initialization.

what is the universal approximation theorem?

52 / 52