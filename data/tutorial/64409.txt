a brief introduction to 

deep learning 

--yangyan li 

how would you crack it? 

how to avoid being cracked? 

seam carving! 

labradoodle or fried chicken 

puppy or bagel 

sheepdog or mop 

chihuahua or muffin 

barn owl or apple 

parrot or guacamole 

raw chicken or donald trump 

but, we human actually lose! 

    a demo that shows we, human, lose, on the 
classification task, we are proud of, we have been 
trained for millions of years! 
    if we want to make it hard for bots, it has to be 

hard for human as well. 

how would you crack it? 

we human lose on go! 

we (will) lose on many specific tasks! 

    id103 
    translation 
    self-driving 
        

 

    but, they are not ai yet    
    don   t worry until it dates with your girl/boy friend    

deep learning is so cool for so many problems    

a brief introduction to deep learning 

    id158 

 

    back-propagation 

 

    fully connected layer 

 

    convolutional layer 

 

    overfitting 

id158 

1. activation function 
2. weights 
3. cost function 
4. learning algorithm 

live demo 

neurons are functions 

neurons are functions 

back-propagation 

now, serious stuff, a bit    

fully connected layers 

   when in doubt, use brute force.    
--ken thompson 

   if brute force is possible...    
--yangyan li 

convolutional layers 

convolutional layers 

convolution filters 

feature engineering vs. learning  

    feature engineering is the process of using domain 
knowledge of the data to create features that make 
machine learning algorithms work. 

       when working on a machine learning problem, 

feature engineering is manually designing what the 
input x's should be.     

-- shayne miel 

       coming up with features is difficult, time-

consuming, requires expert knowledge.    

--andrew ng 

how to detect it in training process? 

dropout 

sigmod     relu 

sigmod     relu 

compute, connect, evaluate, correct, train madly    

non-linearity, distributed representation, parallel 
computation, adaptive, self-organizing    

a brief history 

    mcculloch, warren s., and walter pitts. "a logical calculus of the ideas immanent in nervous 

activity." the bulletin of mathematical biophysics 5.4 (1943): 115-133. 

    rosenblatt, frank. "the id88: a probabilistic model for information storage and 

organization in the brain." psychological review 65.6 (1958): 386. 

    rumelhart, david e., geoffrey e. hinton, and ronald j. williams. "learning representations by 

back-propagating errors." cognitive modeling 5.3 (1988): 1. 

   

lecun, yann, et al. "id26 applied to handwritten zip code recognition." neural 
computation 1.4 (1989): 541-551. 

    1993: nvidia started    

    hinton, geoffrey e., simon osindero, and yee-whye teh. "a fast learning algorithm for deep 

belief nets." neural computation 18.7 (2006): 1527-1554. 

    raina, rajat, anand madhavan, and andrew y. ng. "large-scale deep unsupervised learning using 

graphics processors." proceedings of the 26th annual international conference on machine 
learning. acm, 2009. 

    deng, jia, et al. "id163: a large-scale hierarchical image database."id161 and 

pattern recognition, 2009. cvpr 2009. ieee conference on. ieee, 2009. 

    2010:    gpus are only up to 14 times faster than cpus    says intel    nvidia 

    glorot, xavier, antoine bordes, and yoshua bengio. "deep sparse rectifier neural 
networks." international conference on artificial intelligence and statistics. 2011. 

    hinton, geoffrey e., et al. "improving neural networks by preventing co-adaptation of feature 

detectors." arxiv preprint arxiv:1207.0580 (2012). 

    krizhevsky, alex, ilya sutskever, and geoffrey e. hinton. "id163 classification with deep 
convolutional neural networks." advances in neural information processing systems. 2012. 

   now this is not the end. it is not even the beginning of the 
end. but it is, perhaps, the end of the beginning.    
 

--winston churchill 

is deep learning taking over the world? 

    what applications are likely/unlikely to benefit 

from dl? why? 

deep learning, yay or nay? 

a piece of cake, 

elementary math    

 it eats, a lot! 

