   #[1]gist [2]atom

   [3]skip to content
   ____________________

     * [4]all gists
     * [5]back to github

   [6]sign up for a github account [7]sign in

   instantly share code, notes, and snippets.

[8]@karpathy [9]karpathy/[10]min-char-id56.py

   last active apr 5, 2019
     * [11]star [12]2,750
     * [13]fork [14]1,004

   [15]code [16]revisions 7 [17]stars 2750 [18]forks 1004
   embed
   what would you like to do?
   (<script src="https://gist.github.com/karpathy/d4dee566867f8291f086.js"
   ></script>)
   embed embed this gist in your website.
   (https://gist.github.com/karpathy/d4dee566867f8291f086)
   share copy sharable link for this gist.
   (https://gist.github.com/d4dee566867f8291f086.git)
   clone via https clone with git or checkout with svn using the
   repository   s web address.
   learn more about clone urls
   <script src="https:/
   [19]download zip
   minimal character-level language model with a vanilla recurrent neural
   network, in python/numpy
   [20]raw
   [21]min-char-id56.py
   """
   minimal character-level vanilla id56 model. written by andrej karpathy
   (@karpathy)
   bsd license
   """
   import numpy as np
   # data i/o
   data = open('input.txt', 'r').read() # should be simple plain text file
   chars = list(set(data))
   data_size, vocab_size = len(data), len(chars)
   print 'data has %d characters, %d unique.' % (data_size, vocab_size)
   char_to_ix = { ch:i for i,ch in enumerate(chars) }
   ix_to_char = { i:ch for i,ch in enumerate(chars) }
   # hyperparameters
   hidden_size = 100 # size of hidden layer of neurons
   seq_length = 25 # number of steps to unroll the id56 for
   learning_rate = 1e-1
   # model parameters
   wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
   whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
   why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output
   bh = np.zeros((hidden_size, 1)) # hidden bias
   by = np.zeros((vocab_size, 1)) # output bias
   def lossfun(inputs, targets, hprev):
   """
   inputs,targets are both list of integers.
   hprev is hx1 array of initial hidden state
   returns the loss, gradients on model parameters, and last hidden state
   """
   xs, hs, ys, ps = {}, {}, {}, {}
   hs[-1] = np.copy(hprev)
   loss = 0
   # forward pass
   for t in xrange(len(inputs)):
   xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
   xs[t][inputs[t]] = 1
   hs[t] = np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t-1]) + bh) #
   hidden state
   ys[t] = np.dot(why, hs[t]) + by # unnormalized log probabilities for
   next chars
   ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next
   chars
   loss += -np.log(ps[t][targets[t],0]) # softmax (cross-id178 loss)
   # backward pass: compute gradients going backwards
   dwxh, dwhh, dwhy = np.zeros_like(wxh), np.zeros_like(whh),
   np.zeros_like(why)
   dbh, dby = np.zeros_like(bh), np.zeros_like(by)
   dhnext = np.zeros_like(hs[0])
   for t in reversed(xrange(len(inputs))):
   dy = np.copy(ps[t])
   dy[targets[t]] -= 1 # backprop into y. see
   http://cs231n.github.io/neural-networks-case-study/#grad if confused
   here
   dwhy += np.dot(dy, hs[t].t)
   dby += dy
   dh = np.dot(why.t, dy) + dhnext # backprop into h
   dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity
   dbh += dhraw
   dwxh += np.dot(dhraw, xs[t].t)
   dwhh += np.dot(dhraw, hs[t-1].t)
   dhnext = np.dot(whh.t, dhraw)
   for dparam in [dwxh, dwhh, dwhy, dbh, dby]:
   np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding
   gradients
   return loss, dwxh, dwhh, dwhy, dbh, dby, hs[len(inputs)-1]
   def sample(h, seed_ix, n):
   """
   sample a sequence of integers from the model
   h is memory state, seed_ix is seed letter for first time step
   """
   x = np.zeros((vocab_size, 1))
   x[seed_ix] = 1
   ixes = []
   for t in xrange(n):
   h = np.tanh(np.dot(wxh, x) + np.dot(whh, h) + bh)
   y = np.dot(why, h) + by
   p = np.exp(y) / np.sum(np.exp(y))
   ix = np.random.choice(range(vocab_size), p=p.ravel())
   x = np.zeros((vocab_size, 1))
   x[ix] = 1
   ixes.append(ix)
   return ixes
   n, p = 0, 0
   mwxh, mwhh, mwhy = np.zeros_like(wxh), np.zeros_like(whh),
   np.zeros_like(why)
   mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for
   adagrad
   smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0
   while true:
   # prepare inputs (we're sweeping from left to right in steps seq_length
   long)
   if p+seq_length+1 >= len(data) or n == 0:
   hprev = np.zeros((hidden_size,1)) # reset id56 memory
   p = 0 # go from start of data
   inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
   targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]
   # sample from the model now and then
   if n % 100 == 0:
   sample_ix = sample(hprev, inputs[0], 200)
   txt = ''.join(ix_to_char[ix] for ix in sample_ix)
   print '----\n %s \n----' % (txt, )
   # forward seq_length characters through the net and fetch gradient
   loss, dwxh, dwhh, dwhy, dbh, dby, hprev = lossfun(inputs, targets,
   hprev)
   smooth_loss = smooth_loss * 0.999 + loss * 0.001
   if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print
   progress
   # perform parameter update with adagrad
   for param, dparam, mem in zip([wxh, whh, why, bh, by],
   [dwxh, dwhh, dwhy, dbh, dby],
   [mwxh, mwhh, mwhy, mbh, mby]):
   mem += dparam * dparam
   param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update
   p += seq_length # move data pointer
   n += 1 # iteration counter
   [22]@karpathy

this comment has been minimized.

   [23]sign in to view
   copy link (button) quote reply
   owner author

[24]karpathy commented [25]jul 27, 2015

   also here is the gradient check code as well. it's ugly but works:
# gradient checking
from random import uniform
def gradcheck(inputs, target, hprev):
  global wxh, whh, why, bh, by
  num_checks, delta = 10, 1e-5
  _, dwxh, dwhh, dwhy, dbh, dby, _ = lossfun(inputs, targets, hprev)
  for param,dparam,name in zip([wxh, whh, why, bh, by], [dwxh, dwhh, dwhy, dbh,
dby], ['wxh', 'whh', 'why', 'bh', 'by']):
    s0 = dparam.shape
    s1 = param.shape
    assert s0 == s1, 'error dims dont match: %s and %s.' % (`s0`, `s1`)
    print name
    for i in xrange(num_checks):
      ri = int(uniform(0,param.size))
      # evaluate cost at [x + delta] and [x - delta]
      old_val = param.flat[ri]
      param.flat[ri] = old_val + delta
      cg0, _, _, _, _, _, _ = lossfun(inputs, targets, hprev)
      param.flat[ri] = old_val - delta
      cg1, _, _, _, _, _, _ = lossfun(inputs, targets, hprev)
      param.flat[ri] = old_val # reset old value for this parameter
      # fetch both numerical and analytic gradient
      grad_analytic = dparam.flat[ri]
      grad_numerical = (cg0 - cg1) / ( 2 * delta )
      rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical + gra
d_analytic)
      print '%f, %f => %e ' % (grad_numerical, grad_analytic, rel_error)
      # rel_error should be on order of 1e-7 or less

   [26]@denis-bz

this comment has been minimized.

   [27]sign in to view
   copy link (button) quote reply

[28]denis-bz commented [29]aug 1, 2015

   nice. could you add a few words describing the problem being solved, or
   links ?
   is there a straw man e.g. naive bayes, for which id56 is much better ?
   bytheway, the clip line should be
np.clip( dparam, -1, 1, out=dparam )  # clip to mitigate exploding gradients

   (any ideas on ways to plot gradients before / after smoothing ?)
   [30]@voho

this comment has been minimized.

   [31]sign in to view
   copy link (button) quote reply

[32]voho commented [33]aug 12, 2015

   wonderful. for a beginner, could you please add usage description with
   some example? would be very grateful!
   [34]@farizrahman4u

this comment has been minimized.

   [35]sign in to view
   copy link (button) quote reply

[36]farizrahman4u commented [37]aug 15, 2015

   why is the loss going up sometimes during training?
   [38]@r03ert0

this comment has been minimized.

   [39]sign in to view
   copy link (button) quote reply

[40]r03ert0 commented [41]aug 15, 2015

   yes! more comments please (it will not count for the number of lines
   ;d)
   [42]@suhaspillai

this comment has been minimized.

   [43]sign in to view
   copy link (button) quote reply

[44]suhaspillai commented [45]aug 15, 2015

   i think it goes up for first 100 iterations but reduces for all other
   iterations. i think the reason it goes up is because initially some
   letters might have different output letters.
   like for example:
   winter is harsh in rochester,usa
   summer is harsh in india
   now for one sentence n-> r and for another sentence you have n->i. so,
   for first few iterations the weights are trying to learn this features,
   i think they might be capturing some information about weather (summer
   and winter, in this eg). thus, after few hundred iterations your
   weights have learned that information and then predicts the correct
   letter based on some conditional information of the past(like weather
   in this case), thereby increasing the class score for that letter and
   -log(score) decreases, thus reducing the loss.
   [46]@daquang

this comment has been minimized.

   [47]sign in to view
   copy link (button) quote reply

[48]daquang commented [49]aug 29, 2015

   does this code implement mini-batch truncated bptt?
   [50]@ozancaglayan

this comment has been minimized.

   [51]sign in to view
   copy link (button) quote reply

[52]ozancaglayan commented [53]sep 17, 2015

   the original blog post referring to this code is:
   [54]http://karpathy.github.io/2015/05/21/id56-effectiveness/
   [55]@popwin

this comment has been minimized.

   [56]sign in to view
   copy link (button) quote reply

[57]popwin commented [58]nov 9, 2015

   thank you~i learned a lot from your code
   [59]@griffinliang

this comment has been minimized.

   [60]sign in to view
   copy link (button) quote reply

[61]griffinliang commented [62]nov 20, 2015

   thanks for sharing~
   [63]@kkunte

this comment has been minimized.

   [64]sign in to view
   copy link (button) quote reply

[65]kkunte commented [66]dec 4, 2015

   thanks for sharing an excellent article and the code.
   i am bit confused about the [targets[t],0] array reference in following
   line:
   loss += -np.log(ps[t][targets[t],0]) # softmax (cross-id178 loss)

   i tried searching the documentation for numpy but with no luck.
   [67]@bshillingford

this comment has been minimized.

   [68]sign in to view
   copy link (button) quote reply

[69]bshillingford commented [70]dec 13, 2015

   [71]@kkunte numpy lets you do: array_of_floats[array_of_indices] to
   select out the elements of an array, so that syntax computes result[i]
   = array_of_floats[array_of_indices[i]] for
   i=0,...,len(array_of_indices)-1.(more quickly, conveniently, and
   without a loop)
   [72]@ijkilchenko

this comment has been minimized.

   [73]sign in to view
   copy link (button) quote reply

[74]ijkilchenko commented [75]jan 9, 2016

   if you want a word-level language model, insert data = data.split()
   after reading the input file (after line 8 at the time of writing this
   comment). leave everything else as is.
   [76]@jayanthkoushik

this comment has been minimized.

   [77]sign in to view
   copy link (button) quote reply

[78]jayanthkoushik commented [79]jan 9, 2016

   what's the purpose of smooth_loss here?
   [80]@to0ms

this comment has been minimized.

   [81]sign in to view
   copy link (button) quote reply

[82]to0ms commented [83]jan 17, 2016

   [84]@bshillingford imo not the good answer.
   [85]@kkunte targets is a list of integers (so targets[t] is an integer
   which plays index) and ps[t] a column matrix, so ps[t][targets[t], 0]
   -> ps[t][targets[t]][0]

   more generally with x, a numpy matrix with (2,4) shape, x[1, 3] ==
   x[1][3]

   "unlike lists and tuples, numpy arrays support multidimensional
   indexing for multidimensional arrays. that means that it is not
   necessary to separate each dimension   s index into its own set of square
   brackets."
   [86]@rajarsheem

this comment has been minimized.

   [87]sign in to view
   copy link (button) quote reply

[88]rajarsheem commented [89]jan 30, 2016

   while performing word level modelling, isn't it better to use id97
   representation for each word instead of onehot encoding ?
   [90]@shuaiw

this comment has been minimized.

   [91]sign in to view
   copy link (button) quote reply

[92]shuaiw commented [93]feb 10, 2016

   thanks for this mini id56 (which i also find easier to read than text).

   there is one thing i don't quite understand: what's the intuition of
   dhnext (defined on line 47) and then adding it to the gradient dh (line
   53)? i turned '+ dhnext' (line 53) off and found that without it the
   model enjoys a faster convergence rate and a lower loss. here are my
   experiment results.

   without '+ dhnext' on line 53: iter 10000, loss: 4.696478; iter 40000,
   loss: 0.763520

   with '+ dhnext' on line 53: iter 10000, loss: 5.893804; iter 40000,
   loss: 1.647147
   [94]@karpathy

this comment has been minimized.

   [95]sign in to view
   copy link (button) quote reply
   owner author

[96]karpathy commented [97]feb 10, 2016

   [98]@shuaiw the hidden state variable h is used twice: one going
   vertically up to the prediction (y), and one going horizontally to the
   next hidden state h at the next time step. during id26 these
   two "branches" of computation both contribute gradients to h, and these
   gradients have to add up. the variable dhnext is the gradient
   contributed by the horizontal branch. it's strange to see you get
   better performance without it, but my guess is that if you ran it
   longer the proper way would eventually win out. it's computing the
   correct gradient.
   [99]@xiaoyu32123

this comment has been minimized.

   [100]sign in to view
   copy link (button) quote reply

[101]xiaoyu32123 commented [102]feb 15, 2016

   i think the line 51 should be: dwhy += np.dot(dy, (1/hs[t]).t), also
   line 53, 56, 57. am i wrong?
   [103]@hanumanuom

this comment has been minimized.

   [104]sign in to view
   copy link (button) quote reply

[105]hanumanuom commented [106]mar 10, 2016

   when i am running this python code, min_char_id56.py with a text file
   called input.txt having content as "hello world. best wishes."
   then it is un-ending. its taking more than 24 hours to run. iterations
   and loss are going on but, never ending. please help me out.
   [107]@pmichel31415

this comment has been minimized.

   [108]sign in to view
   copy link (button) quote reply

[109]pmichel31415 commented [110]mar 18, 2016

   [111]@hanumanuom as you can see the last part of the code is a while
   true: loop so it is supposed not to end. it's just a toy script, you
   should check out his char-nn on github for a more complete version.
   this is just to see how it works. run it on fancy text, look at
   the random babbling it produces every second and, when you're bored,
   just ctrl+c your way out of it
   [112]@camjohnson26

this comment has been minimized.

   [113]sign in to view
   copy link (button) quote reply

[114]camjohnson26 commented [115]apr 2, 2016

   so i can't get results as good as the article even after 1.5 million
   iterations. what parameters were you using for the paul graham quotes?
   mine seems to learn structure but can't consistently make actual words
   [116]@0708andreas

this comment has been minimized.

   [117]sign in to view
   copy link (button) quote reply

[118]0708andreas commented [119]apr 13, 2016

   @mostlyharid113ss26 in the article, he links to this github repo:
   [120]https://github.com/karpathy/char-id56. that code is implemented
   using torch and defaults to slightly larger models. you should probably
   use that if you're trying to replicate his results
   [121]@laie

this comment has been minimized.

   [122]sign in to view
   copy link (button) quote reply

[123]laie commented [124]apr 19, 2016

   [125]@shuaiw, actually, as like [126]@karpathy's calculation, it's
   correct to add two terms to calculate exact derivative. in that case
   you are treating previous hidden state as input like they are not
   influenced by the network's weights. but i think that exact
   derivative's harming the first-order optimizer more than your wrong
   assumption. nowadays most researchers don't fully trust gd's weight
   update proposition. so they preprocess gradients by clipping, or using
   element-wise methods like rmsprop, adadelta, ...
   [127]@chizhangrit

this comment has been minimized.

   [128]sign in to view
   copy link (button) quote reply

[129]chizhangrit commented [130]apr 21, 2016

   [131]@karpathy thanks very much for providing the gradient check. when
   i run the gradient checking, i found that all the relative errors are
   very small except for some of them in wxh. they are shown as nan:

   wxh
   0.000000, 0.000000 => nan
   0.000000, 0.000000 => nan
   -0.025170, -0.025170 => 1.155768e-08
   0.000000, 0.000000 => nan
   0.000000, 0.000000 => nan
   0.000000, 0.000000 => nan
   0.000000, 0.000000 => nan
   0.010142, 0.010142 => 3.613506e-09
   -0.002687, -0.002687 => 2.578197e-08
   0.000000, 0.000000 => nan

   i tried to change the dtype to np.float64 but it did not go away. do
   you have any idea what is going on here?

   i appreciate if you could provide help of any kind.
   [132]@benmackenzie

this comment has been minimized.

   [133]sign in to view
   copy link (button) quote reply

[134]benmackenzie commented [135]may 21, 2016

   [136]@shuaiw [137]@karpathy does adding in dhnext on line 53 really
   give you the exact gradient? wouldn't the full gradient for a
   particular output include gradients from all outputs the occur later in
   the letter sequence? it looks like this is an approximation that limits
   influence of an output to the next letter in sequence.
   [138]@rongjiecomputer

this comment has been minimized.

   [139]sign in to view
   copy link (button) quote reply

[140]rongjiecomputer commented [141]jun 4, 2016

   [142]@karpathy thank you so much for the code, it is really helpful for
   learning!

   for those who don't fancy shakespeare much, [143]complete sherlock
   holmes in raw text might be more interesting to play with!
   [144]@alihassan1

this comment has been minimized.

   [145]sign in to view
   copy link (button) quote reply

[146]alihassan1 commented [147]jun 6, 2016    

   edited

   [148]@karpathy thank you so much for this awesome code.

   i'm new to python and i was wondering if you can explain the following
   line (75)

   ix = np.random.choice(range(vocab_size), p=p.ravel())

   shouldn't we be taking the index of max value of 'p' here instead?
   [149]@rohitsaluja22

this comment has been minimized.

   [150]sign in to view
   copy link (button) quote reply

[151]rohitsaluja22 commented [152]jun 9, 2016    

   edited

   hi karpathy, thanks a lot for sharing this code and article on this. it
   helped me a lot growing my understanding about id56.

   @allhassan1, line 75 is doing the same thing you said, i.e. it is
   giving maximum value index of p. i do not know exactly how, but if i
   check on python with some random vocab_size and p, its giving the
   maximum value index of item in p.
   range(vocab_size) will give a normal python list - [ 0 1 2 3 .....
   (vocab_size-1)]
   p.ravel() just readjust m by n matrix to mn by 1 array.

   check these references and let me know if you figure it out why the
   line 75 gives max value index in p:-
   [153]http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.r
   avel.html
   [154]http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.rand
   om.choice.html
   [155]@alihassan1

this comment has been minimized.

   [156]sign in to view
   copy link (button) quote reply

[157]alihassan1 commented [158]jun 12, 2016

   [159]@rohitsaluja22 thanks a lot for your response above. i still think
   it doesn't give maximum value index of p because by definition the
   function np.random.choice generates a non-uniform random sample when
   called with p. i wonder what would be the equivalent function in
   matlab?
   [160]@sunshineatnoon

this comment has been minimized.

   [161]sign in to view
   copy link (button) quote reply

[162]sunshineatnoon commented [163]jun 12, 2016

   [164]@jayanthkoushik, did you figure out the purpose of smooth_loss? i
   have the same question.
   [165]@dvhuang

this comment has been minimized.

   [166]sign in to view
   copy link (button) quote reply

[167]dvhuang commented [168]jun 22, 2016    

   edited

   [169]@alihassan1 [170]@rohitsaluja22
   the code in line (75),it doesn't return the index of max value of 'p'.
   as the code down here,when you try some times it return different value
   p=np.zeros((4,1))
   p[:,0]=[0.3,0.2,0.4,0.1]
   print p,p.ravel(),p.shape,p.ravel().shape
   ix = np.random.choice(4, p=p.ravel())
   print ix,"ix"

   the index of max value of    p'    p.argmax()
   [171]@profplum

this comment has been minimized.

   [172]sign in to view
   copy link (button) quote reply

[173]profplum commented [174]jul 2, 2016    

   edited

   [175]@karpathy
   i'm trying to understand the math behind this... on line 41 you say
   that ys[] contains the "unnormalized log probabilities for next chars"
   and then you use the exp() function to get the real probabilities. at
   what point did those become "log probabilities"? is it an effect of the
   tanh() activation function? any insight would be appreciated.

   edit: ok i figured out that you're computing the softmax by doing that,
   but now i'm curious why you use a different activation function in the
   hidden layers than in the output layer?
   [176]@liuzhi136

this comment has been minimized.

   [177]sign in to view
   copy link (button) quote reply

[178]liuzhi136 commented [179]jul 6, 2016

   thanks very much for your code. it is the code that i can understand
   the id56 more deeply. i wander that what dose the code of "# prepare
   inputs (we're sweeping from left to right in steps seq_length long)"
   mean. i have read your blog
   [180]http://karpathy.github.io/2015/05/21/id56-effectiveness/. and test
   the very simple example "hello". i would be very appreiciate if i could
   receive your anwser.
   [181]@eliben

this comment has been minimized.

   [182]sign in to view
   copy link (button) quote reply

[183]eliben commented [184]jul 22, 2016

   [185]@alihassan1 -- correct, this doesn't use argmax to select the one
   char with highest id203, but rather uses sampling to select from
   all chars weighted by their probabilities (so the maximal prob char
   still has the highest chance of being selected, but now it's a
   id203 distribution). read the documentation of numpy's
   random.choice for the full insight.

   imho in [186]@karpathy's [187]https://github.com/karpathy/char-id56/
   repository this is configurable with the sample option which you set to
   1 if you want sampling and 0 if you want argmax. in case of sampling
   you can also use temperature to scale down all probabilities a bit.

   i hope this makes sense :)
   [188]@modanesh

this comment has been minimized.

   [189]sign in to view
   copy link (button) quote reply

[190]modanesh commented [191]jul 26, 2016    

   edited

   hi, could you explain or give a link describing about the usage of
   dhraw and what's it for in line 54 and 56? i got a little confused
   about it.
   thanks
   [192]@rincerwind

this comment has been minimized.

   [193]sign in to view
   copy link (button) quote reply

[194]rincerwind commented [195]jul 31, 2016    

   edited

   hi, thanks for the code.

   [196]@karpathy, is it correct to say that this network is fully
   recurrent and that the relationship between the neurons in one layer is
   a soft-winner-takes-all? it seems like that from the hidden-weight
   matrix.

   thanks
   [197]@uvarovann

this comment has been minimized.

   [198]sign in to view
   copy link (button) quote reply

[199]uvarovann commented [200]aug 6, 2016

   something error...
   inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
   targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length]]

   length inputs one less targets, for example:
   inputs = [5, 6, 2, 1, 0, 3, 4]
   targets = [6, 2, 1, 0, 3, 4]
   [201]@guotong1988

this comment has been minimized.

   [202]sign in to view
   copy link (button) quote reply

[203]guotong1988 commented [204]aug 17, 2016    

   edited

   [205]@uvarovann same question. have you fixed it ?
   [206]@rukshn

this comment has been minimized.

   [207]sign in to view
   copy link (button) quote reply

[208]rukshn commented [209]aug 19, 2016

   [210]@profplum it's better to use a softmax function as the output's
   activation function over tanh or sigmoid function. softmax function
   also doesn't have the problem of values going to extreme levels, and
   the vanishing gradient problem. usually what i heard was that even for
   the hidden nodes it's better to use the relu function over the sigmoid
   of tanh functions because they also then doesn't suffer the vanishing
   gradient problem, however it's far more difficult to train the network
   oppsed to tanh or sigmoid.
   [211]@rukshn

this comment has been minimized.

   [212]sign in to view
   copy link (button) quote reply

[213]rukshn commented [214]aug 19, 2016

   [215]@uvarovann usually this happens at the end of the corpus because
   there is no character after the last character of the corpus meaning
   that the target is always one short from the input, what i did was
   append a space to the end of it     
   [216]@cyrilfurtado

this comment has been minimized.

   [217]sign in to view
   copy link (button) quote reply

[218]cyrilfurtado commented [219]aug 31, 2016

   when does the training complete?
   also after training how does it output any learnt data like 'code' or
   'shakespeare'?
   [220]@eduos

this comment has been minimized.

   [221]sign in to view
   copy link (button) quote reply

[222]eduos commented [223]sep 5, 2016    

   edited

   [224]@sunshineatnoon [225]@jayanthkoushik i thought the smooth_loss has
   nothing to do with the algorithm, since it is only used as a
   friendly(smooth) dashboard to check the decreasing value of the true
   loss.
   [226]@pavelkomarov

this comment has been minimized.

   [227]sign in to view
   copy link (button) quote reply

[228]pavelkomarov commented [229]sep 16, 2016    

   edited

   i think i've managed to reimplement the above in a slightly more
   sensible way. i couldn't understand it very well before this exercise.
   maybe this will help some others, give you a different jumping-off
   point.
#implemented as i read andrej karpathy's post on id56s.
import numpy as np
import matplotlib.pyplot as plt

class id56(object):

    def __init__(self, insize, outsize, hidsize, learning_rate):
        self.insize = insize

        self.h = np.zeros((hidsize , 1))#a [h x 1] hidden state stored from last
 batch of inputs

        #parameters
        self.w_hh = np.random.randn(hidsize, hidsize)*0.01#[h x h]
        self.w_xh = np.random.randn(hidsize, insize)*0.01#[h x x]
        self.w_hy = np.random.randn(outsize, hidsize)*0.01#[y x h]
        self.b_h = np.zeros((hidsize, 1))#biases
        self.b_y = np.zeros((outsize, 1))

        #the adagrad gradient update relies upon having a memory of the sum of s
quares of dparams
        self.adaw_hh = np.zeros((hidsize, hidsize))
        self.adaw_xh = np.zeros((hidsize, insize))
        self.adaw_hy = np.zeros((outsize, hidsize))
        self.adab_h = np.zeros((hidsize, 1))
        self.adab_y = np.zeros((outsize, 1))

        self.learning_rate = learning_rate

    #give the id56 a sequence of inputs and outputs (seq_length long), and use
    #them to adjust the internal state
    def train(self, x, y):
        #=====initialize=====
        xhat = {}#holds 1-of-k representations of x
        yhat = {}#holds 1-of-k representations of predicted y (unnormalized log
probs)
        p = {}#the normalized probabilities of each output through time
        h = {}#holds state vectors through time
        h[-1] = np.copy(self.h)#we will need to access the previous state to cal
culate the current state

        dw_xh = np.zeros_like(self.w_xh)
        dw_hh = np.zeros_like(self.w_hh)
        dw_hy = np.zeros_like(self.w_hy)
        db_h = np.zeros_like(self.b_h)
        db_y = np.zeros_like(self.b_y)
        dh_next = np.zeros_like(self.h)

        #=====forward pass=====
        loss = 0
        for t in range(len(x)):
            xhat[t] = np.zeros((self.insize, 1))
            xhat[t][x[t]] = 1#xhat[t] = 1-of-k representation of x[t]

            h[t] = np.tanh(np.dot(self.w_xh, xhat[t]) + np.dot(self.w_hh, h[t-1]
) + self.b_h)#find new hidden state
            yhat[t] = np.dot(self.w_hy, h[t]) + self.b_y#find unnormalized log p
robabilities for next chars

            p[t] = np.exp(yhat[t]) / np.sum(np.exp(yhat[t]))#find probabilities
for next chars

            loss += -np.log(p[t][y[t],0])#softmax (cross-id178 loss)

        #=====backward pass: compute gradients going backwards=====
        for t in reversed(range(len(x))):
            #backprop into y. see http://cs231n.github.io/neural-networks-case-s
tudy/#grad if confused here
            dy = np.copy(p[t])
            dy[y[t]] -= 1

            #find updates for y
            dw_hy += np.dot(dy, h[t].t)
            db_y += dy

            #backprop into h and through tanh nonlinearity
            dh = np.dot(self.w_hy.t, dy) + dh_next
            dh_raw = (1 - h[t]**2) * dh

            #find updates for h
            dw_xh += np.dot(dh_raw, xhat[t].t)
            dw_hh += np.dot(dh_raw, h[t-1].t)
            db_h += dh_raw

            #save dh_next for subsequent iteration
            dh_next = np.dot(self.w_hh.t, dh_raw)

        for dparam in [dw_xh, dw_hh, dw_hy, db_h, db_y]:
            np.clip(dparam, -5, 5, out=dparam)#clip to mitigate exploding gradie
nts

        #update id56 parameters according to adagrad
        for param, dparam, adaparam in zip([self.w_hh, self.w_xh, self.w_hy, sel
f.b_h, self.b_y], \
                                [dw_hh, dw_xh, dw_hy, db_h, db_y], \
                                [self.adaw_hh, self.adaw_xh, self.adaw_hy, self.
adab_h, self.adab_y]):
            adaparam += dparam*dparam
            param += -self.learning_rate*dparam/np.sqrt(adaparam+1e-8)

        self.h = h[len(x)-1]

        return loss

    #let the id56 generate text
    def sample(self, seed, n):
        ndxs = []
        h = self.h

        xhat = np.zeros((self.insize, 1))
        xhat[seed] = 1#transform to 1-of-k

        for t in range(n):
            h = np.tanh(np.dot(self.w_xh, xhat) + np.dot(self.w_hh, h) + self.b_
h)#update the state
            y = np.dot(self.w_hy, h) + self.b_y
            p = np.exp(y) / np.sum(np.exp(y))
            ndx = np.random.choice(range(self.insize), p=p.ravel())

            xhat = np.zeros((self.insize, 1))
            xhat[ndx] = 1

            ndxs.append(ndx)

        return ndxs


def test():
    #open a text file
    data = open('shakespeare.txt', 'r').read() # should be simple plain text fil
e
    chars = list(set(data))
    data_size, vocab_size = len(data), len(chars)
    print 'data has %d characters, %d unique.' % (data_size, vocab_size)

    #make some dictionaries for encoding and decoding from 1-of-k
    char_to_ix = { ch:i for i,ch in enumerate(chars) }
    ix_to_char = { i:ch for i,ch in enumerate(chars) }

    #insize and outsize are len(chars). hidsize is 100. seq_length is 25. learni
ng_rate is 0.1.
    id56 = id56(len(chars), len(chars), 100, 0.1)

    #iterate over batches of input and target output
    seq_length = 25
    losses = []
    smooth_loss = -np.log(1.0/len(chars))*seq_length#loss at iteration 0
    losses.append(smooth_loss)

    for i in range(len(data)/seq_length):
        x = [char_to_ix[c] for c in data[i*seq_length:(i+1)*seq_length]]#inputs
to the id56
        y = [char_to_ix[c] for c in data[i*seq_length+1:(i+1)*seq_length+1]]#the
 targets it should be outputting

        if i%1000==0:
            sample_ix = id56.sample(x[0], 200)
            txt = ''.join([ix_to_char[n] for n in sample_ix])
            print txt

        loss = id56.train(x, y)
        smooth_loss = smooth_loss*0.999 + loss*0.001

        if i%1000==0:
            print 'iteration %d, smooth_loss = %f' % (i, smooth_loss)
            losses.append(smooth_loss)

    plt.plot(range(len(losses)), losses, 'b', label='smooth loss')
    plt.xlabel('time in thousands of iterations')
    plt.ylabel('loss')
    plt.legend()
    plt.show()

if __name__ == "__main__":
    test()

   [230]@pavelkomarov

this comment has been minimized.

   [231]sign in to view
   copy link (button) quote reply

[232]pavelkomarov commented [233]sep 21, 2016    

   edited

   [234]@karpathy how can we extend this to multiple layers? it's
   irritating to me that all the implementations i can easily google use
   libraries like tensorflow. i want to know how to do this at a rawer
   level.
   [235]@pavelkomarov

this comment has been minimized.

   [236]sign in to view
   copy link (button) quote reply

[237]pavelkomarov commented [238]sep 22, 2016    

   edited

   [239]@karpathy i also would like a more granular explanation of how to
   backprop through id127s like this. these are great
   [240]http://cs231n.github.io/optimization-2/, but it is unclear how
   that scales up to more dimensions.
#                  [b_h]                                              [b_y]
#                    v                                                  v
#   x -> [w_xh] -> [sum] -> h_raw -> [nonlinearity] -> h -> [w_hy] -> [sum] -> y
 -> [exp(y[k])/sum(exp(y))] -> p
#                    ^                                 |
#                    '----h_next------[w_hh]-----------'
#

   i can follow the notes and understand how to get from p to dy, and i
   can see your expressions for propagating through the rest of this, but
   i do not understand how they are derived analytically. if i want to
   understand what gradient i should be passing from one layer of a
   network to the previous one in id26, i need to be able to
   get through all of this.
   [241]@pavelkomarov

this comment has been minimized.

   [242]sign in to view
   copy link (button) quote reply

[243]pavelkomarov commented [244]sep 22, 2016    

   edited

   i managed to find something that sort of works, but i am still having
   issues.

   if i calculate dy for the output layer as before, let dx =
   np.dot(self.w_xh.t, dh_raw) in backprop steps, and use dx as dy for the
   next layers, i see my id168 decrease. but it only does so to
   some point, and i know that my 3-layer id56 should have more
   characterizing power than this.

   i implemented a smooth_error like smooth_loss and ran it over the first
   50th of my shakespeare.txt training set 10 times. i should see that the
   network is getting more and more overfit to these inputs, but the error
   rate remains at about 0.77 through the 10 iterations. why should it get
   stuck? i am using a small update and adagrad.

   here is a plot of the loss during that process:

   [245]loss

   here is the complete but not-cleaned-up code in case you want to run or
   comb through it:
#an attempt at a batched id56s
#
#i don't think this is an lstm. what is the difference, exactly? i want to
#know the more complicated functional forms, how to backprop them, and what
#the advantage is.
import numpy as np
import matplotlib.pyplot as plt

class id56layer(object):

    def __init__(self, x_size, h_size, y_size, learning_rate):
        self.h_size = h_size
        self.learning_rate = learning_rate#ugh, nightmares

        #inputs and internal states for each layer, used during id26
        self.x = {}
        self.h = {}
        self.h_last = np.zeros((h_size, 1))

        #x is the input. h is the internal hidden stuff. y is the output.
        self.w_xh = np.random.randn(h_size, x_size)*0.01#x -> h
        self.w_hh = np.random.randn(h_size, h_size)*0.01#h -> h
        self.w_hy = np.random.randn(y_size, h_size)*0.01#h -> y
        self.b_h = np.zeros((h_size, 1))#biases
        self.b_y = np.zeros((y_size, 1))

        #the adagrad gradient update relies upon having a memory of the sum of s
quares of dparams
        self.adaw_xh = np.zeros((h_size, x_size))#start sums at 0
        self.adaw_hh = np.zeros((h_size, h_size))
        self.adaw_hy = np.zeros((y_size, h_size))
        self.adab_h = np.zeros((h_size, 1))
        self.adab_y = np.zeros((y_size, 1))

    #given an input, step the internal state and return the output of the networ
k
    #because the whole network is together in one object, i can make it easy and
 just
    #take a list of input ints, transform them to 1-of-k once, and prop everywhe
re.
    #
    #   here is a diagram of what's happening. useful to understand backprop too
.
    #
    #                  [b_h]                                              [b_y]
    #                    v                                                  v
    #   x -> [w_xh] -> [sum] -> h_raw -> [nonlinearity] -> h -> [w_hy] -> [sum]
-> y ... -> [e] -> p
    #                    ^                                 |
    #                    '----h_next------[w_hh]-----------'
    #
    def step(self, x):
        #load the last state from the last batch in to the beginning of h
        #it is necessary to save it outside of h because h is used in backprop
        self.h[-1] = self.h_last
        self.x = x

        y = {}
        p = {}#p[t] = the probabilities of next chars given chars passed in at t
imes <=t
        for t in range(len(self.x)):#for each moment in time

            #self.h[t] = np.maximum(0, np.dot(self.w_xh, self.xhat[t]) + \
            #   np.dot(self.w_hh, self.h[t-1]) + self.b_h)#relu

            #find new hidden state in this layer at this time
            self.h[t] = np.tanh(np.dot(self.w_xh, self.x[t]) + \
                np.dot(self.w_hh, self.h[t-1]) + self.b_h)#tanh

            #find unnormalized log probabilities for next chars
            y[t] = np.dot(self.w_hy, self.h[t]) + self.b_y#output from this laye
r is input to the next
            p[t] = np.exp(y[t]) / np.sum(np.exp(y[t]))#find probabilities for ne
xt chars

        #save the last state from this batch for next batch
        self.h_last = self.h[len(x)-1]

        return y, p

    #given the id56 a sequence of correct outputs (seq_length long), use
    #them and the internal state to adjust weights
    def backprop(self, dy):

        #we will need some place to store gradients
        dw_xh = np.zeros_like(self.w_xh)
        dw_hh = np.zeros_like(self.w_hh)
        dw_hy = np.zeros_like(self.w_hy)
        db_h = np.zeros_like(self.b_h)
        db_y = np.zeros_like(self.b_y)

        dh_next = np.zeros((self.h_size, 1))#i think this is the right dimension
        dx = {}

        for t in reversed(range(len(dy))):
            #find updates for y stuff
            dw_hy += np.dot(dy[t], self.h[t].t)
            db_y += dy[t]

            #backprop into h and through nonlinearity
            dh = np.dot(self.w_hy.t, dy[t]) + dh_next
            dh_raw = (1 - self.h[t]**2)*dh#tanh
            #dh_raw = self.h[t][self.h[t] <= 0] = 0#relu

            #find updates for h stuff
            dw_xh += np.dot(dh_raw, self.x[t].t)
            dw_hh += np.dot(dh_raw, self.h[t-1].t)
            db_h += dh_raw

            #save dh_next for subsequent iteration
            dh_next = np.dot(self.w_hh.t, dh_raw)

            #save the error to propagate to the next layer. am i doing this corr
ectly?
            dx[t] = np.dot(self.w_xh.t, dh_raw)

        #clip to mitigate exploding gradients
        for dparam in [dw_xh, dw_hh, dw_hy, db_h, db_y]:
            dparam = np.clip(dparam, -5, 5)
        for t in range(len(dx)):
            dx[t] = np.clip(dx[t], -5, 5)

        #update id56 parameters according to adagrad
        #yes, it calls by reference, so the actual things do get updated
        for param, dparam, adaparam in zip([self.w_hh, self.w_xh, self.w_hy, sel
f.b_h, self.b_y], \
                    [dw_hh, dw_xh, dw_hy, db_h, db_y], \
                    [self.adaw_hh, self.adaw_xh, self.adaw_hy, self.adab_h, self
.adab_y]):
            adaparam += dparam*dparam
            param += -self.learning_rate*dparam/np.sqrt(adaparam+1e-8)

        return dx

def test():
    #open a text file
    data = open('shakespeare.txt', 'r').read() # should be simple plain text fil
e
    chars = list(set(data))
    data_size, vocab_size = len(data), len(chars)
    print 'data has %d characters, %d unique.' % (data_size, vocab_size)

    #make some dictionaries for encoding and decoding from 1-of-k
    char_to_ix = { ch:i for i,ch in enumerate(chars) }
    ix_to_char = { i:ch for i,ch in enumerate(chars) }

    #num_hid_layers = 3, insize and outsize are len(chars). hidsize is 512 for a
ll layers. learning_rate is 0.1.
    id561 = id56layer(len(chars), 50, 50, 0.001)
    id562 = id56layer(50, 50, 50, 0.001)
    id563 = id56layer(50, 50, len(chars), 0.001)

    #iterate over batches of input and target output
    seq_length = 25
    losses = []
    smooth_loss = -np.log(1.0/len(chars))*seq_length#loss at iteration 0
    losses.append(smooth_loss)
    smooth_error = seq_length

    for j in range(10):
        print "============== j = ",j," =================="
        for i in range(len(data)/(seq_length*50)):
            inputs = [char_to_ix[c] for c in data[i*seq_length:(i+1)*seq_length]
]#inputs to the id56
            targets = [char_to_ix[c] for c in data[i*seq_length+1:(i+1)*seq_leng
th+1]]#the targets it should be outputting

            if i%1000==0:
                sample_ix = sample([id561, id562, id563], inputs[0], 200, len(chars
))
                txt = ''.join([ix_to_char[n] for n in sample_ix])
                print txt
                losses.append(smooth_loss)

            #forward pass
            x = oneofk(inputs, len(chars))
            y1, p1 = id561.step(x)
            y2, p2 = id562.step(y1)
            y3, p3 = id563.step(y2)

            #calculate loss and error rate
            loss = 0
            error = 0
            for t in range(len(targets)):
                loss += -np.log(p3[t][targets[t],0])
                if np.argmax(p3[t]) != targets[t]:
                    error += 1
            smooth_loss = smooth_loss*0.999 + loss*0.001
            smooth_error = smooth_error*0.999 + error*0.001

            if i%10==0:
                print i,"\tsmooth loss =",smooth_loss,"\tsmooth error rate =",fl
oat(smooth_error)/len(targets)

            #backward pass
            dy = logprobs(p3, targets)
            dx3 = id563.backprop(dy)
            dx2 = id562.backprop(dx3)
            dx1 = id561.backprop(dx2)

    plt.plot(range(len(losses)), losses, 'b', label='smooth loss')
    plt.xlabel('time in thousands of iterations')
    plt.ylabel('loss')
    plt.legend()
    plt.show()


#let the id56 generate text
def sample(id56s, seed, n, k):

    ndxs = []
    ndx = seed

    for t in range(n):
        x = oneofk([ndx], k)
        for i in range(len(id56s)):
            x, p = id56s[i].step(x)

        ndx = np.random.choice(range(len(p[0])), p=p[0].ravel())
        ndxs.append(ndx)

    return ndxs

#i have these out here because it's not really the id56's concern how you transfo
rm
#things to a form it can understand

#get the initial dy to pass back through the first layer
def logprobs(p, targets):
    dy = {}
    for t in range(len(targets)):
        #see http://cs231n.github.io/neural-networks-case-study/#grad if confuse
d here
        dy[t] = np.copy(p[t])
        dy[t][targets[t]] -= 1
    return dy

#encode inputs in 1-of-k so they match inputs between layers
def oneofk(inputs, k):
    x = {}
    for t in range(len(inputs)):
        x[t] = np.zeros((k, 1))#initialize x input to 1st hidden layer
        x[t][inputs[t]] = 1#it's encoded in 1-of-k representation
    return x

if __name__ == "__main__":
    test()

   [246]@mfagerlund

this comment has been minimized.

   [247]sign in to view
   copy link (button) quote reply

[248]mfagerlund commented [249]sep 24, 2016

   [250]@pavelkomarov, for an analytical treatment of this very code, have
   a look here: [251]http://www.existor.com/en/ml-id56.html
   [252]@caverac

this comment has been minimized.

   [253]sign in to view
   copy link (button) quote reply

[254]caverac commented [255]oct 10, 2016

   [256]@karpathy thanks a lot for this posting! i tried to run it with
   your [257]hello example ... and this is what i get

   traceback (most recent call last):
   file "min-char-id56.py", line 100, in
   loss, dwxh, dwhh, dwhy, dbh, dby, hprev = lossfun(inputs, targets,
   hprev)
   file "min-char-id56.py", line 43, in lossfun
   loss += -np.log(ps[t][targets[t],0]) # softmax (cross-id178 loss)
   indexerror: list index out of range

   any ideas?
   thanks!
   [258]@pavelkomarov

this comment has been minimized.

   [259]sign in to view
   copy link (button) quote reply

[260]pavelkomarov commented [261]oct 12, 2016    

   edited

   [262]@mfagerlund damn, that's some crazy calculus, but thanks. karpathy
   should have referenced something like this in his post for those of us
   who want to do it ourselves. how to backpropagate through to get dx
   (the thing you would want to pass back to the next layer in the
   network) is still unclear, so i have no idea whether i did it
   correctly. i also still have no idea why my loss stops decreasing as it
   does.

   also, is my diagram/flowchart correct?
   [263]@delijati

this comment has been minimized.

   [264]sign in to view
   copy link (button) quote reply

[265]delijati commented [266]nov 1, 2016    

   edited

   explanation of the id56 code:
   [267]https://youtu.be/co0a0qymfm8?list=plljy-ebtnft6eumxfyrinrs07mcwn5u
   ia&t=836
   [268]@georgeblck

this comment has been minimized.

   [269]sign in to view
   copy link (button) quote reply

[270]georgeblck commented [271]nov 29, 2016

   [272]@pavelkomarov and all others
   if you want to check the math more thoroughly, you can do so with the
   recently published deep learning book by goodfellow/bengio/courville.
   [273]check chapter 10.1 & 10.2 or more specifically look on page
   385-386 for the backprop equations. the architecture used in that
   example is exactly the same as the one used here.

   it does take some time to connect the notation in the book with the
   notation in the code, but it is worth it.
   [274]@taosiqin1991

this comment has been minimized.

   [275]sign in to view
   copy link (button) quote reply

[276]taosiqin1991 commented [277]dec 18, 2016

   wonderful
   [278]@zoson

this comment has been minimized.

   [279]sign in to view
   copy link (button) quote reply

[280]zoson commented [281]dec 24, 2016

   the loss divided by the length of inputs before backprop will be
   better.
   [282]@somah1411

this comment has been minimized.

   [283]sign in to view
   copy link (button) quote reply

[284]somah1411 commented [285]jan 12, 2017

   how can i use this code for translation where shall i put the input and
   target langauge
   [286]@ppaquette

this comment has been minimized.

   [287]sign in to view
   copy link (button) quote reply

[288]ppaquette commented [289]jan 16, 2017

   [290]@caverac the variable seq_length must be smaller than the size of
   the data in your input.txt, otherwise there are not enough targets to
   calculate the id168. decreasing seq_length or adding more text
   in your input.txt should fix the issue.
   [291]@georgeblck

this comment has been minimized.

   [292]sign in to view
   copy link (button) quote reply

[293]georgeblck commented [294]feb 7, 2017

   for anyone interested, i rewrote the entire code in r.
   you can find it [295]here.
   [296]@bhomass

this comment has been minimized.

   [297]sign in to view
   copy link (button) quote reply

[298]bhomass commented [299]feb 9, 2017

   is seq_length the same as the the number of hidden nodes? i think it
   is, just want to be sure.
   [300]@georgeblck

this comment has been minimized.

   [301]sign in to view
   copy link (button) quote reply

[302]georgeblck commented [303]feb 9, 2017

   [304]@bhomass seq_length is not the number of hidden nodes. seq_length
   determines for how many time steps you want to unravel your id56. in
   this case one time step is a letter, so you train your network based on
   the 25 previous time steps/letters.
   [305]@bhomass

this comment has been minimized.

   [306]sign in to view
   copy link (button) quote reply

[307]bhomass commented [308]feb 10, 2017    

   edited

   [309]@georgeblck that agrees with my understanding what seq_length is.
   i see now there is another variable for hidden_size of 100. i
   understand the difference now. thanks!
   [310]@inexxt

this comment has been minimized.

   [311]sign in to view
   copy link (button) quote reply

[312]inexxt commented [313]feb 22, 2017    

   edited

   [314]@chizhangrit the reason is because if two of the derivatives are
   exactly zero, you're dividing by zero - it's a special case not handled
   by the code.
   [315]@karpathy worse situation is when exactly one of them is equal to
   zero - then dividing by the abs(sum) yields just that value, which can
   be correctly greater than the treshold.
   [316]@shaktisd

this comment has been minimized.

   [317]sign in to view
   copy link (button) quote reply

[318]shaktisd commented [319]mar 5, 2017

   [320]@karpathy can you share similar implementation using keras ? it is
   much easier to understand the code using keras.
   [321]@hkxiron

this comment has been minimized.

   [322]sign in to view
   copy link (button) quote reply

[323]hkxiron commented [324]mar 7, 2017

   according to    supervised sequence labelling with recurrent neural
   networks   ,alex graves,2012,we have no gradient for dh and dhnext,can
   you explain it ?
   [325]@coolboygym

this comment has been minimized.

   [326]sign in to view
   copy link (button) quote reply

[327]coolboygym commented [328]mar 11, 2017

   a nice material. thanks a lot !
   [329]@wilderfield

this comment has been minimized.

   [330]sign in to view
   copy link (button) quote reply

[331]wilderfield commented [332]mar 14, 2017

   i want to apply this concept to a different problem without using
   one-hot encoding. my input vector has 2^48 possibilities. i am afraid
   to one-hot encode that. if i strip out the one-hot encoding, can i use
   a different cost function such as 1/2 l2norm^2 ?? i feel like i can't
   use softmax since i am not expecting the log probabilities to add up to
   1... my input could be 0,0,1,1... and my output could be 1,0,1,0
   [333]@wilderfield

this comment has been minimized.

   [334]sign in to view
   copy link (button) quote reply

[335]wilderfield commented [336]mar 15, 2017

   @karapathy why the need for dictionaries to hold various y[t] vectors
   when you could just create a matrix y, whose columns are time steps,
   and rows are dimensions?
   [337]@fmthoker

this comment has been minimized.

   [338]sign in to view
   copy link (button) quote reply

[339]fmthoker commented [340]apr 3, 2017

   @karapathy in the forward pass, you hs[t] is calculated as hs[t] =
   np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t-1]) + bh) # hidden state.
   however in the backward pass shouldn,t we back propagate through tanh
   using this formula 1-tanh^2(x). where x= np.dot(wxh, xs[t]) +
   np.dot(whh, hs[t-1]) + bh. in place of hs[t] which is the ouput after
   applying tanh.
   [341]@guntabutya

this comment has been minimized.

   [342]sign in to view
   copy link (button) quote reply

[343]guntabutya commented [344]apr 10, 2017    

   edited

   can you please give detail on how the line: ix =
   np.random.choice(range(vocab_size), p=p.ravel()) accurately returns a
   sequence that is human readable:

   sample_ix = sample(hprev, inputs[0], 200) txt = ''.join(ix_to_char[ix]
   for ix in sample_ix) print '----\n %s \n----' % (txt, )

   thank you very much!!!
   [345]@cgoliver

this comment has been minimized.

   [346]sign in to view
   copy link (button) quote reply

[347]cgoliver commented [348]apr 18, 2017

   hello,

   thanks for this! just a quick question.
   i'm trying to learn a model that generates sequences in a character
   based manner exactly like this one. except the training data is not one
   long corpus. it is actually a set of fixed length sequences. and i want
   to generate new sequences of the same length. so i don't want the model
   to read the end of one sequence and the beginning of another as would
   happen with the way the scanning is implemented here. i was wondering
   if there is a workaround here like padding by the window size or maybe
   preparing the data differently that could make this work. thanks!
   [349]@gongxijun

this comment has been minimized.

   [350]sign in to view
   copy link (button) quote reply

[351]gongxijun commented [352]apr 27, 2017

   thanks a lot !
   [353]@bishwa420

this comment has been minimized.

   [354]sign in to view
   copy link (button) quote reply

[355]bishwa420 commented [356]may 11, 2017

   it's a wonderful post to begin with. thanks karpathy.
   [357]@zklgame

this comment has been minimized.

   [358]sign in to view
   copy link (button) quote reply

[359]zklgame commented [360]may 15, 2017

   wonderful code!
   [361]@chiragyadav

this comment has been minimized.

   [362]sign in to view
   copy link (button) quote reply

[363]chiragyadav commented [364]may 26, 2017

   thanks, it's a wonderful code to begin with on id56s
   [365]@cristinabattaglino

this comment has been minimized.

   [366]sign in to view
   copy link (button) quote reply

[367]cristinabattaglino commented [368]may 31, 2017    

   edited

   hi,
   i'm getting into deep learning for the task of text id172 in
   tts. your post was very useful. i run the python code with python 3.5
   and found some minor syntax problems:
     * xrange --> range
     * print needs () --> e.g. print ('iter %d, loss: %f' % (n,
       smooth_loss))

   thanks for your work :)
   [369]@skylord-a52

this comment has been minimized.

   [370]sign in to view
   copy link (button) quote reply

[371]skylord-a52 commented [372]jun 11, 2017

   [373]@cristinabattaglino i believe that is because this was written in
   python 2.x, while you're writing in 3.5. the syntax is correct when run
   in python 2, which has slightly different names and syntax for certain
   simple functions. (in python 2, range() produced an array, while
   xrange() produced a one-time generator, which is a lot faster and uses
   less memory. in python 3, the array version was removed, and python 3's
   range() acts like python 2's xrange())

   the one part i don't really understand is the id168. i'm not
   sure i get why you would use -log(prob) instead of (target - prob)**2.

   i only sort of get why (dloss/dy)[t] = p[t] - [0, .. 1, .. 0], but i'm
   willing to trust that, because i'd really rather not take the
   derivative of that myself. if anyone has a simple, intuitive
   explanation of that that isn't just a bunch of chainruling and
   arithmetic, i'd appreciate that.

   oh, and is there any reason why he uses -np.log(ps[t][targets[t],0])
   (aka -np.log(ps[t][targets[t]][0])) instead of just
   -np.log(ps[t][targets[t]])? since ps is an dict of vectors with time as
   the key, wouldn't ps[t] be 1d (a single vector of probabilities at time
   t), and therefore ps[t][targets[t]] already be a scalar? what's with
   the extra zero?
   [374]@samrat1997

this comment has been minimized.

   [375]sign in to view
   copy link (button) quote reply

[376]samrat1997 commented [377]jun 12, 2017    

   edited

   why initially dhnext is taken as zero vector ? it can be computed at
   the first iteration during id26.
   ... its cleared now !
   [378]@chuchienshu

this comment has been minimized.

   [379]sign in to view
   copy link (button) quote reply

[380]chuchienshu commented [381]jun 17, 2017

   wonderful code   but there are somecodes confused me.
# perform parameter update with adagrad
    for param, dparam, mem in zip([wxh, whh, why, bh, by],
                                  [dwxh, dwhh, dwhy, dbh, dby],
                                [mwxh, mwhh, mwhy, mbh, mby]):
        mem += dparam * dparam#                
        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update

   wxh,..mby were defined as global variables   while param   dparam   mem are
   just local variables.how could adagrad update change the value of
   global variables?i tried to test my thought by code like below.
import numpy as np
wxh, whh, why, bh, by=1,2,3,4,5
dwxh, dwhh, dwhy, dbh, dby=1,2,3,4,5
mwxh, mwhh, mwhy, mbh, mby=1,2,3,4,5
while true:
    for param, dparam, mem in zip([wxh, whh, why, bh, by],
                                  [dwxh, dwhh, dwhy, dbh, dby],
                                [mwxh, mwhh, mwhy, mbh, mby]):
        mem += dparam * dparam#                
        param += dparam / np.sqrt(mem + 1e-8)
    print(wxh)#output never change!

   it's might be a very simple problem.just puzzled me!someone
   helps?thanks a lot.
   [382]@chuchienshu

this comment has been minimized.

   [383]sign in to view
   copy link (button) quote reply

[384]chuchienshu commented [385]jun 17, 2017

   [386]@karpathy
   [387]@chuchienshu

this comment has been minimized.

   [388]sign in to view
   copy link (button) quote reply

[389]chuchienshu commented [390]jun 17, 2017

import numpy as np
hidden_size = 2 # size of hidden layer of neurons
vocab_size = 1
wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output
bh = np.zeros((hidden_size, 1)) # hidden bias
by = np.zeros((vocab_size, 1)) # output bias
learning_rate = 1e-1
dwxh, dwhh, dwhy, dbh, dby=1,2,3,4,5
mwxh, mwhh, mwhy, mbh, mby=1,2,3,4,5
while true:
    for param, dparam, mem in zip([wxh, whh, why, bh, by],
                                  [dwxh, dwhh, dwhy, dbh, dby],
                                [mwxh, mwhh, mwhy, mbh, mby]):
        mem += dparam * dparam#                
        param += dparam / np.sqrt(mem + 1e-8)
    print(wxh)

   run the code above   it works as expect.but i don't know why.
   [391]@dluman

this comment has been minimized.

   [392]sign in to view
   copy link (button) quote reply

[393]dluman commented [394]jun 22, 2017    

   edited

   [395]@chuchienshu -- i'm going to try my hand at answering your
   question. disclaimer: i'm new to all of this, but have spent some time
   rewriting parts of [396]@karpathy's awesome example in order to
   understand it for myself. i have a newly-working implementation in
   julia that i'm pretty proud of.

   i have one question: i assume you are referring to the fact that wxh
   increments over time (as do all of the others) without explicitly
   calling it?
   [397]@jinwu07

this comment has been minimized.

   [398]sign in to view
   copy link (button) quote reply

[399]jinwu07 commented [400]jul 11, 2017    

   edited

   i am getting the following run time error:

   loss += -np.log(ps[t][targets[t],0]) # softmax (cross-id178 loss)
   indexerror: list index out of range

   this is because the length of inputs is longer than that of targets
   near the end according to the following two lines (90, 91):
   inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
   targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

   it is a mystery to me that this code works for most of the people - i
   am lost.

   edited: it looks the code would hit the exception if input data is
   short len(data) < seq_legth. the corner case will be "cut" in the
   forever loop with large data set. so, it might be a minor issue. it
   needs large data set to make sense anyway.
   [401]@xdxuefei

this comment has been minimized.

   [402]sign in to view
   copy link (button) quote reply

[403]xdxuefei commented [404]jul 11, 2017

   thanks for sharing!
   [405]@sleebapaul

this comment has been minimized.

   [406]sign in to view
   copy link (button) quote reply

[407]sleebapaul commented [408]jul 18, 2017

   [409]@alihassan1 [410]@guntabutya [411]@dvhuang [412]@rohitsaluja22

   for the query you've asked, do we've a clarification? i'm adding the
   iteration results for both arguments.

   ix = np.random.choice(range(vocab_size), p=p.ravel())

iter 9700, loss: 23.750007

     d god said, webe the it efrus der the fab is whetcr creit the the
     fis the the vers igetheren ling, aulis, and sreat lit ollyo hry, let
     soud the foceh, d,.
     and be the fidermo ning: and create. letreat

   ix = p.argmax()

iter 9700, loss: 22.398720

     d the evening was good the firmament of the evening and the earth
     breate of the ficre for heaven and the morning was it was whe day
     upon the firmament of the evening and the earth breate of the ficre

   second one is more readable.
   [413]@shibonskaria

this comment has been minimized.

   [414]sign in to view
   copy link (button) quote reply

[415]shibonskaria commented [416]aug 11, 2017

   why are the random numbers multiplied by 0.01 in lines 21-23?
   eg: wxh = np.random.randn(hidden_size, vocab_size)*0.01
   [417]@piotrbazan

this comment has been minimized.

   [418]sign in to view
   copy link (button) quote reply

[419]piotrbazan commented [420]aug 19, 2017

   [421]@shibonskaria - big weights can push tanh values near plateau and
   the gradient will be very small or just will vanish. small weights can
   prevent network from learn anything. initialization is a tricky
   subject. please take a look at
   [http://cs231n.github.io/neural-networks-2/#init]
   [422]@redinton

this comment has been minimized.

   [423]sign in to view
   copy link (button) quote reply

[424]redinton commented [425]aug 26, 2017

   [426]@sleebapaul [427]@alihassan1 [428]@guntabutya [429]@dvhuang
   [430]@rohitsaluja22

   recently, i found that the question was about two method to choose the
   next word. these two methods are called max and sample.
   "max" is about choosing the next word with the largest id203. and
   "sample" is about choosing the next word like karpathy's code that
   sampling according to the word's id203. this will bring more
   randomness to the result. for more specific content, please take a look
   at this [431]paper hope that will help you guys :)
   [432]@angryman14

this comment has been minimized.

   [433]sign in to view
   copy link (button) quote reply

[434]angryman14 commented [435]aug 29, 2017

   how long does it generally takes for complete run of this code?
   [436]@bailubenben

this comment has been minimized.

   [437]sign in to view
   copy link (button) quote reply

[438]bailubenben commented [439]aug 30, 2017

   [440]@karpathy i have some confusion about the intermediate calculation
   dhraw = (1 - hs[t] * hs[t]) * dh .
   according to the math ,
   (sinx)' = cosx,
   (cosx)' = - sinx ,
   (tanx)' = (sinx / cosx)' =((sinx)' cosx - (cosx)'sinx) / cosx ** 2 =
   (cosx ** 2 + sinx ** 2 ) / cosx ** 2 = 1 + tanx ** 2.
   why dhraw = (1 + hs[t] * hs[t]) * dh is wrong? would you please explain
   to me ?
   thank you very much!
   ps: when i try to use (tanx)' = 1 / cosx ** 2 instead in cs231's
   id56_layers.py, i have runtime warning as following:
   assignment3/cs231n/id56_layers.py:70: runtimewarning: overflow
   encountered in square
   dtanh = 1 / (np.cosh(prev_h.dot(wh) + x.dot(wx) + b) ** 2)
   i am wondering whether such output depends on the condition that
   denominator may be possible to be close to zeros
   thank you karpathy
   [441]@reachlin

this comment has been minimized.

   [442]sign in to view
   copy link (button) quote reply

[443]reachlin commented [444]sep 7, 2017    

   edited

   changed [445]this code to support unicode and trained with some chinese
   poems from [446]li bai, this is some interesting outputs:
                                    
                                          
   [447]@klory

this comment has been minimized.

   [448]sign in to view
   copy link (button) quote reply

[449]klory commented [450]sep 18, 2017    

   edited

   [451]@bailubenben, it's tanh(x) not tan(x), tanh(x) is a hyperbolic
   function.
   [452]@klory

this comment has been minimized.

   [453]sign in to view
   copy link (button) quote reply

[454]klory commented [455]sep 18, 2017

   [456]@sleebapaul, since we are treating this as a probabilistic
   problem, we want to generate(sample) the next character from a
   distribution, not just from the maximum value.
   [457]@klory

this comment has been minimized.

   [458]sign in to view
   copy link (button) quote reply

[459]klory commented [460]sep 18, 2017

   [461]@jinwu07, in the main loop (l90 - l91), you could say that the
   data size has to be at least seq_length
   [462]@yanwenx

this comment has been minimized.

   [463]sign in to view
   copy link (button) quote reply

[464]yanwenx commented [465]nov 6, 2017

   in line 43, isn't the id168 a log-likelihood one, instead of a
   cross-id178 one?
   [466]@photonzhao

this comment has been minimized.

   [467]sign in to view
   copy link (button) quote reply

[468]photonzhao commented [469]nov 7, 2017

   [470]@kkunte i think ps[t] is a shape of (vocab_size, 1) 2d array,
   ps[t][targets[t],0] just pick one specific cell element which is a
   single float number scalar.
   [471]@yanwenx

this comment has been minimized.

   [472]sign in to view
   copy link (button) quote reply

[473]yanwenx commented [474]nov 7, 2017

   [475]@shuaiw i am down with your point on line 53, the derivative of
   current id168 (say c_t) w.r.t. the current hidden state (h_t).
   by formula,
   c_t = -log(p_t)
   p_t = softmax(y_t)
   y_t = dot(w_hy, h_t) + b_y
   h_t = tanh(dot(w_hx, x_t) + dot(w_hh, h_t-1) + b_h)
   hence dc_t/dh_t = (dc_t/dp_t) * (dp_t/dy_t) * (dy_t/dh_t), and y_t has
   a explicit expression w.r.t. h_t, without h_t-1 involved. so i think we
   don't need to add the term "dhnext" to "dh".
   [476]@yanwenx

this comment has been minimized.

   [477]sign in to view
   copy link (button) quote reply

[478]yanwenx commented [479]nov 7, 2017

   [480]@shuaiw meanwhile i also got a lower loss without adding dhnext
   [481]@tmatha

this comment has been minimized.

   [482]sign in to view
   copy link (button) quote reply

[483]tmatha commented [484]nov 19, 2017

   [485]@yanwenx - please read [486]@karpathy comment from feb 10, 2016.
   you need to add "dhnext" because there is gradient flow from the next
   hidden state as well. in other words, a hidden state influences the
   next hidden state. the derivative of the loss with respect to the next
   state, multiplied by the derivative of the next state with respect to
   the current state, is given by "dhnext".
   [487]@huyu2jason

this comment has been minimized.

   [488]sign in to view
   copy link (button) quote reply

[489]huyu2jason commented [490]jan 16, 2018

   i'm very grateful for what you taught us. i'm your big fan.
   [491]@muradmath

this comment has been minimized.

   [492]sign in to view
   copy link (button) quote reply

[493]muradmath commented [494]jan 16, 2018

   [495]@karpathy thanks for sharing the code, really helpful to
   understand id56s. one question though, is their a reason why there is no
   bias for the input in the model? i.e. we have wxh but no bxh.
   [496]@trungkak

this comment has been minimized.

   [497]sign in to view
   copy link (button) quote reply

[498]trungkak commented [499]jan 26, 2018

   [500]@karpathy hi, i have the same question with [501]@muradmath above,
   why there is no bias term for input to hidden state?
   [502]@satyajitvg

this comment has been minimized.

   [503]sign in to view
   copy link (button) quote reply

[504]satyajitvg commented [505]jan 27, 2018

   while learning from this awesome blog post , i ended up re-writing this
   code a bit.
   [506]https://gist.github.com/satyajitvg/9a5f782ccef5ff81f7f9863b62218b0
   6
   cheers ! and thanks to [507]@karpathy for this awesome blog and code !
   [508]@furkhanshaikh

this comment has been minimized.

   [509]sign in to view
   copy link (button) quote reply

[510]furkhanshaikh commented [511]feb 22, 2018

   [512]@hanumanuom use while(n<=no_of_iterations) if you want to run it
   for finite time
   [513]@hkhojasteh

this comment has been minimized.

   [514]sign in to view
   copy link (button) quote reply

[515]hkhojasteh commented [516]mar 5, 2018

   thank you for sharing your code with us.
   [517]i rewrote the code in c++ with opencv, if anyone is interested.
   [518]@rakeshmallick

this comment has been minimized.

   [519]sign in to view
   copy link (button) quote reply

[520]rakeshmallick commented [521]mar 13, 2018

   andrej thank you so much for providing this code of a mini id56, it uses
   no deep learning framework, quite useful in getting started.
   [522]@315567599

this comment has been minimized.

   [523]sign in to view
   copy link (button) quote reply

[524]315567599 commented [525]apr 14, 2018

   [526]uploading 21690225_424408761287767_3315548193680261120_n.jpg   

dddd

   [527]@lukaszog

this comment has been minimized.

   [528]sign in to view
   copy link (button) quote reply

[529]lukaszog commented [530]jun 5, 2018

   anyone have this version with lstm?
   [531]@tararengan

this comment has been minimized.

   [532]sign in to view
   copy link (button) quote reply

[533]tararengan commented [534]jul 6, 2018

   andrej and others,

   i took a shot at recoding this with one difference: get the gradients
   using the forward pass itself. so, andrej's code figures out in the
   backward pass, the contribution to the gradient w.r.t. a given hidden
   state vector from terms involving it in id168 expression for t
   and t+1 (at t+1 we get dhnext which is part of the chain rule and
   needed for t), and adds them. i reversed that idea. during forward pass
   in iteration t, i get the gradient contributed by the hidden vector for
   t in so much as it figures in the id168 for the t-th character,
   and the gradient contributed by the hidden vector for t-1 which also
   figures in the same id168 expression for t (i save the
   derivatives of hidden from t-1 w.r.t the w's and b's). exactly the same
   thing mathematically. but here's my surmise of the difference in
   performance (mine converged slower)) that eluded me for a bit
   annoyingly: as t increases, the hidden vector gets richer and gradient
   components get larger in size i think. so, let's think of the gradients
   for the parameters as composed of len(inputs) summands, indexed from
   t:0 to len(inputs)-1 where absolute value increases as t increases. to
   give a simple analogy, its like adding a sorted set of numbers. cs101
   teaches us to add from smallest to largest to avoid precision issues. i
   tried to abide by this and ended up with larger gradient components
   than andrej. everything changes! when the derivatives are different,
   the step is different, the next parameter values are different which
   makes everything downstream different. but here's the deal: andrej's
   code makes id168 go down steadily while mine doesn't. aargh!!!
   controlling for bugs in my code, is it because the richest hidden layer
   should come first in computing summands for gradients? is that a golden
   rule in id56's? lesson learned but a bit flummoxed still.

   i have a fork and happy to share.
   [535]@tararengan

this comment has been minimized.

   [536]sign in to view
   copy link (button) quote reply

[537]tararengan commented [538]jul 6, 2018

   finally, it works. it wasn't because of forward or backward. my
   learning rate was too conservative.
   [539]@morenoh149

this comment has been minimized.

   [540]sign in to view
   copy link (button) quote reply

[541]morenoh149 commented [542]jul 7, 2018

   [543]@lukaszog [544]https://github.com/nicodjimenez/lstm
   [545]@tararengan

this comment has been minimized.

   [546]sign in to view
   copy link (button) quote reply

[547]tararengan commented [548]jul 8, 2018

   my version referenced above that takes fewer iterations to spit out
   meaningful text with each iteration taking longer.

   [549]https://github.com/tararengan/nlp.git
   [550]@tararengan

this comment has been minimized.

   [551]sign in to view
   copy link (button) quote reply

[552]tararengan commented [553]jul 8, 2018

   andrej - it seems that your code not incorporate derivates of the last
   hidden layer from previous batch into gradient calculations for current
   batch? this seems strange given that the said hidden layer itself is
   used in the forward pass for the current batch.
   [554]@anushanemilidinne

this comment has been minimized.

   [555]sign in to view
   copy link (button) quote reply

[556]anushanemilidinne commented [557]jul 9, 2018

   [558]@karpathy thanks a lot for such a nice code. but i have been
   trying to figure out how to arrive at the following gradients:
    1. dy[targets[
       t]] -= 1
    2. dwhy += np.dot(dy, hs[t].t)
    3. dh = np.dot(why.t, dy) + dhnext # backprop into h
       dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh
       nonlinearity

   i followed the references given in the code but couldn't get any
   clarity.
   can you/ anyone here please help me in getting these equations right.

   thanks a lot in advance!
   [559]@rubyjohn

this comment has been minimized.

   [560]sign in to view
   copy link (button) quote reply

[561]rubyjohn commented [562]jul 24, 2018

   [563]@anushanemilidinne
    1. ps : [0.1 0.8 0.2]
       ans : [0.0 0.0 1.0]
       dy : [0.1 0.8 -0.8]
    2. calculate dwhy by 1.
    3. derivate tanh -> y' = 1 - y^2 = 1 - y * y

   [564]@sleebapaul

this comment has been minimized.

   [565]sign in to view
   copy link (button) quote reply

[566]sleebapaul commented [567]jul 26, 2018    

   edited

   hi, can someone explain to me why [568]@karpathy is smoothing the loss?
   because otherwise, the loss behaves erratically. the smoothened loss is
   decaying gradually.

   normal loss

   [569]one

   smoothened loss

   [570]two
   [571]@pxng

this comment has been minimized.

   [572]sign in to view
   copy link (button) quote reply

[573]pxng commented [574]aug 2, 2018

   i'm getting an error when trying to run this code with python 3.7.0 and
   numpy:
   line 11:
   print 'data has %d characters, %d unique.' % (data_size, vocab_size)
   [575]@yzheng97

this comment has been minimized.

   [576]sign in to view
   copy link (button) quote reply

[577]yzheng97 commented [578]aug 7, 2018

   [579]@pxng plz refer to the docs on differences between py2 & py3
   [580]@corruptedmonkey

this comment has been minimized.

   [581]sign in to view
   copy link (button) quote reply

[582]corruptedmonkey commented [583]aug 10, 2018

   data = open("c:\users\amir\desktop\serious programming\neural
   networks\training data.txt", 'r')
   ^
   syntaxerror: (unicode error) 'unicodeescape' codec can't decode bytes
   in position 2-3: truncated \uxxxxxxxx escape
   [584]@icdi0906

this comment has been minimized.

   [585]sign in to view
   copy link (button) quote reply

[586]icdi0906 commented [587]aug 12, 2018

   can anyone tell me why the loss is so large after iter 232000? and loss
   is : 42.046858
   [588]@emerycarr

this comment has been minimized.

   [589]sign in to view
   copy link (button) quote reply

[590]emerycarr commented [591]sep 3, 2018

   [592]@sleebapaul smoothing the loss just reduces the noise to make it
   easier to see the average trend.
   [593]@kovaacs

this comment has been minimized.

   [594]sign in to view
   copy link (button) quote reply

[595]kovaacs commented [596]sep 4, 2018    

   edited

"""
minimal character-level vanilla id56 model. written by andrej karpathy (@karpathy
)
bsd license
"""
import numpy as np

# data i/o
data = open('names.txt', 'r').read()  # should be simple plain text file
chars = list(set(data))
data_size, vocab_size = len(data), len(chars)
print('data has %d characters, %d unique.' % (data_size, vocab_size))
char_to_ix = {ch: i for i, ch in enumerate(chars)}
ix_to_char = {i: ch for i, ch in enumerate(chars)}

# hyperparameters
hidden_size = 100  # size of hidden layer of neurons
seq_length = 25  # number of steps to unroll the id56 for
learning_rate = 1e-1

# model parameters
wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden
whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden
why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output
bh = np.zeros((hidden_size, 1))  # hidden bias
by = np.zeros((vocab_size, 1))  # output bias


def lossfun(inputs, targets, hprev):
    """
    inputs,targets are both list of integers.
    hprev is hx1 array of initial hidden state
    returns the loss, gradients on model parameters, and last hidden state
    """
    xs, hs, ys, ps = {}, {}, {}, {}
    hs[-1] = np.copy(hprev)
    loss = 0
    # forward pass
    for t in range(len(inputs)):
        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation
        xs[t][inputs[t]] = 1
        hs[t] = np.tanh(np.dot(wxh, xs[t]) + np.dot(whh, hs[t - 1]) + bh)  # hid
den state
        ys[t] = np.dot(why, hs[t]) + by  # unnormalized log probabilities for ne
xt chars
        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next
chars
        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-id178 loss)
    # backward pass: compute gradients going backwards
    dwxh, dwhh, dwhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why
)
    dbh, dby = np.zeros_like(bh), np.zeros_like(by)
    dhnext = np.zeros_like(hs[0])
    for t in reversed(range(len(inputs))):
        dy = np.copy(ps[t])
        dy[targets[
            t]] -= 1  # backprop into y. see http://cs231n.github.io/neural-netw
orks-case-study/#grad if confused here
        dwhy += np.dot(dy, hs[t].t)
        dby += dy
        dh = np.dot(why.t, dy) + dhnext  # backprop into h
        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity
        dbh += dhraw
        dwxh += np.dot(dhraw, xs[t].t)
        dwhh += np.dot(dhraw, hs[t - 1].t)
        dhnext = np.dot(whh.t, dhraw)
    for dparam in [dwxh, dwhh, dwhy, dbh, dby]:
        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradien
ts
    return loss, dwxh, dwhh, dwhy, dbh, dby, hs[len(inputs) - 1]


def sample(h, seed_ix, n):
    """
    sample a sequence of integers from the model
    h is memory state, seed_ix is seed letter for first time step
    """
    x = np.zeros((vocab_size, 1))
    x[seed_ix] = 1
    ixes = []
    for t in range(n):
        h = np.tanh(np.dot(wxh, x) + np.dot(whh, h) + bh)
        y = np.dot(why, h) + by
        p = np.exp(y) / np.sum(np.exp(y))
        ix = np.random.choice(range(vocab_size), p=p.ravel())
        x = np.zeros((vocab_size, 1))
        x[ix] = 1
        ixes.append(ix)
    return ixes


n, p = 0, 0
mwxh, mwhh, mwhy = np.zeros_like(wxh), np.zeros_like(whh), np.zeros_like(why)
mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for adagrad
smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0
while true:
    # prepare inputs (we're sweeping from left to right in steps seq_length long
)
    if p + seq_length + 1 >= len(data) or n == 0:
        hprev = np.zeros((hidden_size, 1))  # reset id56 memory
        p = 0  # go from start of data
    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]
    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]

    # sample from the model now and then
    if n % 100 == 0:
        sample_ix = sample(hprev, inputs[0], 200)
        txt = ''.join(ix_to_char[ix] for ix in sample_ix)
        print('----\n %s \n----' % (txt,))

    # forward seq_length characters through the net and fetch gradient
    loss, dwxh, dwhh, dwhy, dbh, dby, hprev = lossfun(inputs, targets, hprev)
    smooth_loss = smooth_loss * 0.999 + loss * 0.001
    if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss))  # print prog
ress

    # perform parameter update with adagrad
    for param, dparam, mem in zip([wxh, whh, why, bh, by],
                                  [dwxh, dwhh, dwhy, dbh, dby],
                                  [mwxh, mwhh, mwhy, mbh, mby]):
        mem += dparam * dparam
        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update

    p += seq_length  # move data pointer
    n += 1  # iteration counter

   the same code upgraded to python 3
   [597]@moon412

this comment has been minimized.

   [598]sign in to view
   copy link (button) quote reply

[599]moon412 commented [600]sep 18, 2018

   come here after watching the cs231 id56 lecture. this code is really
   helpful for me to understand the nuts and bolts of id56. if i understand
   correctly, the seq_length variable in this code is the number of time
   steps or the truncated backdrop length, right? we can consider there is
   only one batch? for example, the dynamic_id56(run_cell, inputs) function
   in tensorflow requires the dimension of inputs is [batch_size,
   time_steps, input_size]. so, for this code, batch_size = len(inputs),
   time_steps = seq_length, input_size = vocab_size.
   [601]@jianchao-li

this comment has been minimized.

   [602]sign in to view
   copy link (button) quote reply

[603]jianchao-li commented [604]dec 10, 2018    

   edited

   two minor problems in gradcheck.
    1. the argument target should be targets;

def gradcheck(inputs, targets, hprev):

    2. while computing rel_error, it is safer to add a small positive
       number (e.g. np.spacing(1)) to the denominator to avoid division by
       zero.

rel_error = abs(grad_analytic - grad_numerical) / (abs(grad_numerical + grad_ana
lytic) + np.spacing(1))

   then the gradient checking code can be added to the training loop.
...
targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]

# gradient checking
gradcheck(inputs, targets, hprev)
break

# sample from the model now and then
...

   i performed gradient checking for once using [605]this graham essay and
   got the following result.
wxh
0.000000, 0.000000 => 0.000000e+00
0.000000, 0.000000 => 0.000000e+00
-0.014157, -0.014157 => 8.656690e-09
-0.019454, -0.019454 => 1.602097e-09
0.000000, 0.000000 => 0.000000e+00
-0.001941, -0.001941 => 3.264141e-08
0.000000, 0.000000 => 0.000000e+00
0.000000, 0.000000 => 0.000000e+00
0.000000, 0.000000 => 0.000000e+00
0.000000, 0.000000 => 0.000000e+00
whh
0.000440, 0.000440 => 1.384989e-06
0.000028, 0.000028 => 6.500306e-06
-0.000209, -0.000209 => 3.029682e-06
-0.000065, -0.000065 => 3.384390e-06
-0.000059, -0.000059 => 4.728270e-06
-0.000368, -0.000368 => 2.784317e-06
-0.000389, -0.000389 => 1.886930e-06
-0.000415, -0.000415 => 8.665300e-07
0.000843, 0.000843 => 2.889790e-07
0.000201, 0.000201 => 3.186529e-06
why
-0.001653, -0.001653 => 1.778729e-07
-0.000226, -0.000226 => 4.249548e-06
0.000452, 0.000452 => 1.155304e-06
0.000935, 0.000935 => 7.176607e-09
0.000730, 0.000730 => 3.622312e-07
0.001707, 0.001707 => 2.076062e-07
0.000554, 0.000554 => 2.735726e-07
-0.000993, -0.000993 => 5.649166e-07
-0.001289, -0.001289 => 4.574623e-08
0.001008, 0.001008 => 7.008325e-07
bh
0.016470, 0.016470 => 2.742773e-08
0.026154, 0.026154 => 7.704196e-09
0.011059, 0.011059 => 3.937953e-08
-0.071711, -0.071711 => 4.566180e-09
0.085213, 0.085213 => 9.833822e-09
-0.012542, -0.012542 => 2.043637e-08
-0.074818, -0.074818 => 4.422074e-09
0.039099, 0.039099 => 2.601257e-09
0.039106, 0.039106 => 2.275082e-10
0.029532, 0.029532 => 2.463476e-09
by
-0.479289, -0.479289 => 6.507750e-10
0.520856, 0.520856 => 3.960052e-10
0.520917, 0.520917 => 9.232581e-10
0.520963, 0.520963 => 2.742692e-10
0.520963, 0.520963 => 2.742692e-10
0.520857, 0.520857 => 2.421469e-10
0.520924, 0.520924 => 4.027698e-10
0.520783, 0.520783 => 4.971356e-10
-0.479004, -0.479004 => 2.579928e-09
-2.479215, -2.479215 => 2.026870e-11

   [606]@jy-yoon

this comment has been minimized.

   [607]sign in to view
   copy link (button) quote reply

[608]jy-yoon commented [609]dec 18, 2018

   thank you for sharing!!

   here is my python 3 ver.

   [610]https://github.com/jy-yoon/id56-implementation-using-numpy/blob/mas
   ter/id56%20implementation%20using%20numpy.ipynb
   [611]@cyndakwil

this comment has been minimized.

   [612]sign in to view
   copy link (button) quote reply

[613]cyndakwil commented [614]dec 22, 2018

   how would you use the new biases (wxh, whh, why, bh, by) in the program
   in order to have the finished product? i save the arrays to a .npz and
   load them again, but i receive the same amount of loss and similar
   samples of random characters as i did from starting the first time.
   [615]@datatalking

this comment has been minimized.

   [616]sign in to view
   copy link (button) quote reply

[617]datatalking commented [618]jan 25, 2019

   i'm running the python version of this and its not saving the training,
   when i stop the program there is no memory of all the millions of
   cycles we completed. is there a "save the results" function i am
   missing?
   [619]@datatalking

this comment has been minimized.

   [620]sign in to view
   copy link (button) quote reply

[621]datatalking commented [622]jan 25, 2019

     thank you for sharing!!

     here is my python 3 ver.

     [623]https://github.com/jy-yoon/id56-implementation-using-numpy/blob/
     master/id56%20implementation%20using%20numpy.ipynb

   does your version save the training results?
   [624]@aditya9898

this comment has been minimized.

   [625]sign in to view
   copy link (button) quote reply

[626]aditya9898 commented [627]jan 29, 2019

   i need help with ----- loss += -np.log(ps[t][targets[t],0]) ----.
   dont understand the syntax. looked it up on google but no solution. is
   there any other way around.
   does sum(-np.log(ys[t])*<one hot array of targets[t]>) mean the same
   thing?
   [628]@aditya9898

this comment has been minimized.

   [629]sign in to view
   copy link (button) quote reply

[630]aditya9898 commented [631]jan 29, 2019

   also is the same loss_func applicable to python 3 too? (apart from
   changing the xrange to range)
   [632]sign up for free to join this conversation on github. already have
   an account? [633]sign in to comment

     *    2019 github, inc.
     * [634]terms
     * [635]privacy
     * [636]security
     * [637]status
     * [638]help

     * [639]contact github
     * [640]pricing
     * [641]api
     * [642]training
     * [643]blog
     * [644]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [645]reload to refresh your
   session. you signed out in another tab or window. [646]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://gist.github.com/opensearch-gist.xml
   2. https://gist.github.com/karpathy.atom
   3. https://gist.github.com/karpathy/d4dee566867f8291f086#start-of-content
   4. https://gist.github.com/discover
   5. https://github.com/
   6. https://gist.github.com/join?source=header-gist
   7. https://gist.github.com/auth/github?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
   8. https://gist.github.com/karpathy
   9. https://gist.github.com/karpathy
  10. https://gist.github.com/karpathy/d4dee566867f8291f086
  11. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  12. https://gist.github.com/karpathy/d4dee566867f8291f086/stargazers
  13. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  14. https://gist.github.com/karpathy/d4dee566867f8291f086/forks
  15. https://gist.github.com/karpathy/d4dee566867f8291f086
  16. https://gist.github.com/karpathy/d4dee566867f8291f086/revisions
  17. https://gist.github.com/karpathy/d4dee566867f8291f086/stargazers
  18. https://gist.github.com/karpathy/d4dee566867f8291f086/forks
  19. https://gist.github.com/karpathy/d4dee566867f8291f086/archive/119a6930b670bced5800b6b03ec4b8cb6b8ff4ec.zip
  20. https://gist.github.com/karpathy/d4dee566867f8291f086/raw/119a6930b670bced5800b6b03ec4b8cb6b8ff4ec/min-char-id56.py
  21. https://gist.github.com/karpathy/d4dee566867f8291f086#file-min-char-id56-py
  22. https://gist.github.com/karpathy
  23. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  24. https://gist.github.com/karpathy
  25. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1508982
  26. https://gist.github.com/denis-bz
  27. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  28. https://gist.github.com/denis-bz
  29. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1540415
  30. https://gist.github.com/voho
  31. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  32. https://gist.github.com/voho
  33. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1549001
  34. https://gist.github.com/farizrahman4u
  35. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  36. https://gist.github.com/farizrahman4u
  37. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1551670
  38. https://gist.github.com/r03ert0
  39. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  40. https://gist.github.com/r03ert0
  41. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1551784
  42. https://gist.github.com/suhaspillai
  43. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  44. https://gist.github.com/suhaspillai
  45. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1551827
  46. https://gist.github.com/daquang
  47. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  48. https://gist.github.com/daquang
  49. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1562502
  50. https://gist.github.com/ozancaglayan
  51. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  52. https://gist.github.com/ozancaglayan
  53. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1576186
  54. http://karpathy.github.io/2015/05/21/id56-effectiveness/
  55. https://gist.github.com/popwin
  56. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  57. https://gist.github.com/popwin
  58. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1616275
  59. https://gist.github.com/griffinliang
  60. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  61. https://gist.github.com/griffinliang
  62. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1626046
  63. https://gist.github.com/kkunte
  64. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  65. https://gist.github.com/kkunte
  66. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1639137
  67. https://gist.github.com/bshillingford
  68. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  69. https://gist.github.com/bshillingford
  70. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1648502
  71. https://github.com/kkunte
  72. https://gist.github.com/ijkilchenko
  73. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  74. https://gist.github.com/ijkilchenko
  75. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1666162
  76. https://gist.github.com/jayanthkoushik
  77. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  78. https://gist.github.com/jayanthkoushik
  79. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1666373
  80. https://gist.github.com/to0ms
  81. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  82. https://gist.github.com/to0ms
  83. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1672069
  84. https://github.com/bshillingford
  85. https://github.com/kkunte
  86. https://gist.github.com/rajarsheem
  87. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  88. https://gist.github.com/rajarsheem
  89. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1684620
  90. https://gist.github.com/shuaiw
  91. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  92. https://gist.github.com/shuaiw
  93. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1693553
  94. https://gist.github.com/karpathy
  95. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
  96. https://gist.github.com/karpathy
  97. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1694245
  98. https://github.com/shuaiw
  99. https://gist.github.com/xiaoyu32123
 100. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 101. https://gist.github.com/xiaoyu32123
 102. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1697826
 103. https://gist.github.com/hanumanuom
 104. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 105. https://gist.github.com/hanumanuom
 106. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1720038
 107. https://gist.github.com/pmichel31415
 108. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 109. https://gist.github.com/pmichel31415
 110. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1727913
 111. https://github.com/hanumanuom
 112. https://gist.github.com/camjohnson26
 113. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 114. https://gist.github.com/camjohnson26
 115. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1740425
 116. https://gist.github.com/0708andreas
 117. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 118. https://gist.github.com/0708andreas
 119. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1750658
 120. https://github.com/karpathy/char-id56
 121. https://gist.github.com/laie
 122. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 123. https://gist.github.com/laie
 124. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1755867
 125. https://github.com/shuaiw
 126. https://github.com/karpathy
 127. https://gist.github.com/chizhangrit
 128. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 129. https://gist.github.com/chizhangrit
 130. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1758075
 131. https://github.com/karpathy
 132. https://gist.github.com/benmackenzie
 133. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 134. https://gist.github.com/benmackenzie
 135. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1783255
 136. https://github.com/shuaiw
 137. https://github.com/karpathy
 138. https://gist.github.com/rongjiecomputer
 139. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 140. https://gist.github.com/rongjiecomputer
 141. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1793985
 142. https://github.com/karpathy
 143. https://gist.github.com/rongjiecomputer/94154e0bf01ef19a4999fef70264c48a
 144. https://gist.github.com/alihassan1
 145. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 146. https://gist.github.com/alihassan1
 147. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1794872
 148. https://github.com/karpathy
 149. https://gist.github.com/rohitsaluja22
 150. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 151. https://gist.github.com/rohitsaluja22
 152. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1798120
 153. http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ravel.html
 154. http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.choice.html
 155. https://gist.github.com/alihassan1
 156. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 157. https://gist.github.com/alihassan1
 158. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1799523
 159. https://github.com/rohitsaluja22
 160. https://gist.github.com/sunshineatnoon
 161. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 162. https://gist.github.com/sunshineatnoon
 163. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1799661
 164. https://github.com/jayanthkoushik
 165. https://gist.github.com/dvhuang
 166. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 167. https://gist.github.com/dvhuang
 168. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1808573
 169. https://github.com/alihassan1
 170. https://github.com/rohitsaluja22
 171. https://gist.github.com/profplum
 172. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 173. https://gist.github.com/profplum
 174. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1817224
 175. https://github.com/karpathy
 176. https://gist.github.com/liuzhi136
 177. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 178. https://gist.github.com/liuzhi136
 179. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1819541
 180. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 181. https://gist.github.com/eliben
 182. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 183. https://gist.github.com/eliben
 184. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1832171
 185. https://github.com/alihassan1
 186. https://github.com/karpathy
 187. https://github.com/karpathy/char-id56/
 188. https://gist.github.com/modanesh
 189. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 190. https://gist.github.com/modanesh
 191. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1834660
 192. https://gist.github.com/rincerwind
 193. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 194. https://gist.github.com/rincerwind
 195. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1838403
 196. https://github.com/karpathy
 197. https://gist.github.com/uvarovann
 198. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 199. https://gist.github.com/uvarovann
 200. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1843625
 201. https://gist.github.com/guotong1988
 202. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 203. https://gist.github.com/guotong1988
 204. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1851297
 205. https://github.com/uvarovann
 206. https://gist.github.com/rukshn
 207. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 208. https://gist.github.com/rukshn
 209. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1853070
 210. https://github.com/profplum
 211. https://gist.github.com/rukshn
 212. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 213. https://gist.github.com/rukshn
 214. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1853658
 215. https://github.com/uvarovann
 216. https://gist.github.com/cyrilfurtado
 217. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 218. https://gist.github.com/cyrilfurtado
 219. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1863137
 220. https://gist.github.com/eduos
 221. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 222. https://gist.github.com/eduos
 223. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1866553
 224. https://github.com/sunshineatnoon
 225. https://github.com/jayanthkoushik
 226. https://gist.github.com/pavelkomarov
 227. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 228. https://gist.github.com/pavelkomarov
 229. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1876224
 230. https://gist.github.com/pavelkomarov
 231. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 232. https://gist.github.com/pavelkomarov
 233. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1879897
 234. https://github.com/karpathy
 235. https://gist.github.com/pavelkomarov
 236. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 237. https://gist.github.com/pavelkomarov
 238. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1879969
 239. https://github.com/karpathy
 240. http://cs231n.github.io/optimization-2/
 241. https://gist.github.com/pavelkomarov
 242. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 243. https://gist.github.com/pavelkomarov
 244. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1880739
 245. https://cloud.githubusercontent.com/assets/5639551/18764868/42d4edd8-80e1-11e6-9c09-cc66b12db875.png
 246. https://gist.github.com/mfagerlund
 247. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 248. https://gist.github.com/mfagerlund
 249. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1882108
 250. https://github.com/pavelkomarov
 251. http://www.existor.com/en/ml-id56.html
 252. https://gist.github.com/caverac
 253. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 254. https://gist.github.com/caverac
 255. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1893996
 256. https://github.com/karpathy
 257. http://karpathy.github.io/2015/05/21/id56-effectiveness/
 258. https://gist.github.com/pavelkomarov
 259. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 260. https://gist.github.com/pavelkomarov
 261. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1896294
 262. https://github.com/mfagerlund
 263. https://gist.github.com/delijati
 264. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 265. https://gist.github.com/delijati
 266. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1911140
 267. https://youtu.be/co0a0qymfm8?list=plljy-ebtnft6eumxfyrinrs07mcwn5uia&t=836
 268. https://gist.github.com/georgeblck
 269. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 270. https://gist.github.com/georgeblck
 271. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1933403
 272. https://github.com/pavelkomarov
 273. http://www.deeplearningbook.org/contents/id56.html
 274. https://gist.github.com/taosiqin1991
 275. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 276. https://gist.github.com/taosiqin1991
 277. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1950913
 278. https://gist.github.com/zoson
 279. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 280. https://gist.github.com/zoson
 281. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1956314
 282. https://gist.github.com/somah1411
 283. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 284. https://gist.github.com/somah1411
 285. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1968973
 286. https://gist.github.com/ppaquette
 287. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 288. https://gist.github.com/ppaquette
 289. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1972513
 290. https://github.com/caverac
 291. https://gist.github.com/georgeblck
 292. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 293. https://gist.github.com/georgeblck
 294. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1990491
 295. https://gist.github.com/georgeblck/4d806e56693420ad22af37a3c29affde
 296. https://gist.github.com/bhomass
 297. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 298. https://gist.github.com/bhomass
 299. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1992314
 300. https://gist.github.com/georgeblck
 301. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 302. https://gist.github.com/georgeblck
 303. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1992494
 304. https://github.com/bhomass
 305. https://gist.github.com/bhomass
 306. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 307. https://gist.github.com/bhomass
 308. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-1993176
 309. https://github.com/georgeblck
 310. https://gist.github.com/inexxt
 311. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 312. https://gist.github.com/inexxt
 313. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2007641
 314. https://github.com/chizhangrit
 315. https://github.com/karpathy
 316. https://gist.github.com/shaktisd
 317. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 318. https://gist.github.com/shaktisd
 319. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2018019
 320. https://github.com/karpathy
 321. https://gist.github.com/hkxiron
 322. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 323. https://gist.github.com/hkxiron
 324. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2019711
 325. https://gist.github.com/coolboygym
 326. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 327. https://gist.github.com/coolboygym
 328. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2024375
 329. https://gist.github.com/wilderfield
 330. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 331. https://gist.github.com/wilderfield
 332. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2026426
 333. https://gist.github.com/wilderfield
 334. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 335. https://gist.github.com/wilderfield
 336. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2027426
 337. https://gist.github.com/fmthoker
 338. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 339. https://gist.github.com/fmthoker
 340. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2045398
 341. https://gist.github.com/guntabutya
 342. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 343. https://gist.github.com/guntabutya
 344. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2054013
 345. https://gist.github.com/cgoliver
 346. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 347. https://gist.github.com/cgoliver
 348. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2064756
 349. https://gist.github.com/gongxijun
 350. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 351. https://gist.github.com/gongxijun
 352. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2075834
 353. https://gist.github.com/bishwa420
 354. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 355. https://gist.github.com/bishwa420
 356. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2090606
 357. https://gist.github.com/zklgame
 358. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 359. https://gist.github.com/zklgame
 360. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2095744
 361. https://gist.github.com/chiragyadav
 362. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 363. https://gist.github.com/chiragyadav
 364. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2106943
 365. https://gist.github.com/cristinabattaglino
 366. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 367. https://gist.github.com/cristinabattaglino
 368. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2110524
 369. https://gist.github.com/skylord-a52
 370. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 371. https://gist.github.com/skylord-a52
 372. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2120798
 373. https://github.com/cristinabattaglino
 374. https://gist.github.com/samrat1997
 375. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 376. https://gist.github.com/samrat1997
 377. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2121139
 378. https://gist.github.com/chuchienshu
 379. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 380. https://gist.github.com/chuchienshu
 381. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2125735
 382. https://gist.github.com/chuchienshu
 383. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 384. https://gist.github.com/chuchienshu
 385. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2125766
 386. https://github.com/karpathy
 387. https://gist.github.com/chuchienshu
 388. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 389. https://gist.github.com/chuchienshu
 390. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2125832
 391. https://gist.github.com/dluman
 392. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 393. https://gist.github.com/dluman
 394. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2130384
 395. https://github.com/chuchienshu
 396. https://github.com/karpathy
 397. https://gist.github.com/jinwu07
 398. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 399. https://gist.github.com/jinwu07
 400. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2144526
 401. https://gist.github.com/xdxuefei
 402. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 403. https://gist.github.com/xdxuefei
 404. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2144759
 405. https://gist.github.com/sleebapaul
 406. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 407. https://gist.github.com/sleebapaul
 408. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2150845
 409. https://github.com/alihassan1
 410. https://github.com/guntabutya
 411. https://github.com/dvhuang
 412. https://github.com/rohitsaluja22
 413. https://gist.github.com/shibonskaria
 414. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 415. https://gist.github.com/shibonskaria
 416. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2174016
 417. https://gist.github.com/piotrbazan
 418. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 419. https://gist.github.com/piotrbazan
 420. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2180482
 421. https://github.com/shibonskaria
 422. https://gist.github.com/redinton
 423. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 424. https://gist.github.com/redinton
 425. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2185838
 426. https://github.com/sleebapaul
 427. https://github.com/alihassan1
 428. https://github.com/guntabutya
 429. https://github.com/dvhuang
 430. https://github.com/rohitsaluja22
 431. https://papers.nips.cc/paper/5956-scheduled-sampling-for-sequence-prediction-with-recurrent-neural-networks.pdf
 432. https://gist.github.com/angryman14
 433. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 434. https://gist.github.com/angryman14
 435. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2187701
 436. https://gist.github.com/bailubenben
 437. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 438. https://gist.github.com/bailubenben
 439. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2189137
 440. https://github.com/karpathy
 441. https://gist.github.com/reachlin
 442. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 443. https://gist.github.com/reachlin
 444. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2195887
 445. https://github.com/reachlin/machinelearning
 446. https://en.wikiquote.org/wiki/li_bai
 447. https://gist.github.com/klory
 448. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 449. https://gist.github.com/klory
 450. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2206665
 451. https://github.com/bailubenben
 452. https://gist.github.com/klory
 453. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 454. https://gist.github.com/klory
 455. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2206680
 456. https://github.com/sleebapaul
 457. https://gist.github.com/klory
 458. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 459. https://gist.github.com/klory
 460. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2206683
 461. https://github.com/jinwu07
 462. https://gist.github.com/yanwenx
 463. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 464. https://gist.github.com/yanwenx
 465. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2249638
 466. https://gist.github.com/photonzhao
 467. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 468. https://gist.github.com/photonzhao
 469. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2249746
 470. https://github.com/kkunte
 471. https://gist.github.com/yanwenx
 472. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 473. https://gist.github.com/yanwenx
 474. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2250784
 475. https://github.com/shuaiw
 476. https://gist.github.com/yanwenx
 477. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 478. https://gist.github.com/yanwenx
 479. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2250785
 480. https://github.com/shuaiw
 481. https://gist.github.com/tmatha
 482. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 483. https://gist.github.com/tmatha
 484. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2262660
 485. https://github.com/yanwenx
 486. https://github.com/karpathy
 487. https://gist.github.com/huyu2jason
 488. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 489. https://gist.github.com/huyu2jason
 490. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2322319
 491. https://gist.github.com/muradmath
 492. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 493. https://gist.github.com/muradmath
 494. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2322385
 495. https://github.com/karpathy
 496. https://gist.github.com/trungkak
 497. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 498. https://gist.github.com/trungkak
 499. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2332694
 500. https://github.com/karpathy
 501. https://github.com/muradmath
 502. https://gist.github.com/satyajitvg
 503. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 504. https://gist.github.com/satyajitvg
 505. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2333691
 506. https://gist.github.com/satyajitvg/9a5f782ccef5ff81f7f9863b62218b06
 507. https://github.com/karpathy
 508. https://gist.github.com/furkhanshaikh
 509. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 510. https://gist.github.com/furkhanshaikh
 511. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2359026
 512. https://github.com/hanumanuom
 513. https://gist.github.com/hkhojasteh
 514. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 515. https://gist.github.com/hkhojasteh
 516. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2370497
 517. https://github.com/hkhojasteh/char-id56-opencv
 518. https://gist.github.com/rakeshmallick
 519. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 520. https://gist.github.com/rakeshmallick
 521. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2379299
 522. https://gist.github.com/315567599
 523. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 524. https://gist.github.com/315567599
 525. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2555152
 526. https://gist.github.com/karpathy/d4dee566867f8291f086
 527. https://gist.github.com/lukaszog
 528. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 529. https://gist.github.com/lukaszog
 530. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2610539
 531. https://gist.github.com/tararengan
 532. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 533. https://gist.github.com/tararengan
 534. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2639477
 535. https://gist.github.com/tararengan
 536. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 537. https://gist.github.com/tararengan
 538. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2640130
 539. https://gist.github.com/morenoh149
 540. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 541. https://gist.github.com/morenoh149
 542. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2640824
 543. https://github.com/lukaszog
 544. https://github.com/nicodjimenez/lstm
 545. https://gist.github.com/tararengan
 546. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 547. https://gist.github.com/tararengan
 548. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2640942
 549. https://github.com/tararengan/nlp.git
 550. https://gist.github.com/tararengan
 551. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 552. https://gist.github.com/tararengan
 553. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2640946
 554. https://gist.github.com/anushanemilidinne
 555. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 556. https://gist.github.com/anushanemilidinne
 557. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2641781
 558. https://github.com/karpathy
 559. https://gist.github.com/rubyjohn
 560. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 561. https://gist.github.com/rubyjohn
 562. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2656906
 563. https://github.com/anushanemilidinne
 564. https://gist.github.com/sleebapaul
 565. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 566. https://gist.github.com/sleebapaul
 567. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2659777
 568. https://github.com/karpathy
 569. https://user-images.githubusercontent.com/11848469/43309615-eab46bc2-91a2-11e8-97b2-4cdd9acfbd5c.png
 570. https://user-images.githubusercontent.com/11848469/43309630-f7b40bde-91a2-11e8-87b8-03ea4ad3b33c.png
 571. https://gist.github.com/pxng
 572. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 573. https://gist.github.com/pxng
 574. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2667460
 575. https://gist.github.com/yzheng97
 576. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 577. https://gist.github.com/yzheng97
 578. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2671728
 579. https://github.com/pxng
 580. https://gist.github.com/corruptedmonkey
 581. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 582. https://gist.github.com/corruptedmonkey
 583. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2675094
 584. https://gist.github.com/icdi0906
 585. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 586. https://gist.github.com/icdi0906
 587. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2676526
 588. https://gist.github.com/emerycarr
 589. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 590. https://gist.github.com/emerycarr
 591. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2695788
 592. https://github.com/sleebapaul
 593. https://gist.github.com/kovaacs
 594. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 595. https://gist.github.com/kovaacs
 596. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2697308
 597. https://gist.github.com/moon412
 598. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 599. https://gist.github.com/moon412
 600. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2710185
 601. https://gist.github.com/jianchao-li
 602. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 603. https://gist.github.com/jianchao-li
 604. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2781810
 605. http://www.paulgraham.com/sun.html
 606. https://gist.github.com/jy-yoon
 607. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 608. https://gist.github.com/jy-yoon
 609. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2788204
 610. https://github.com/jy-yoon/id56-implementation-using-numpy/blob/master/id56 implementation using numpy.ipynb
 611. https://gist.github.com/cyndakwil
 612. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 613. https://gist.github.com/cyndakwil
 614. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2793221
 615. https://gist.github.com/datatalking
 616. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 617. https://gist.github.com/datatalking
 618. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2819563
 619. https://gist.github.com/datatalking
 620. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 621. https://gist.github.com/datatalking
 622. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2819564
 623. https://github.com/jy-yoon/id56-implementation-using-numpy/blob/master/id56 implementation using numpy.ipynb
 624. https://gist.github.com/aditya9898
 625. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 626. https://gist.github.com/aditya9898
 627. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2823466
 628. https://gist.github.com/aditya9898
 629. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 630. https://gist.github.com/aditya9898
 631. https://gist.github.com/karpathy/d4dee566867f8291f086#gistcomment-2823469
 632. https://gist.github.com/join?source=comment-gist
 633. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/d4dee566867f8291f086
 634. https://github.com/site/terms
 635. https://github.com/site/privacy
 636. https://github.com/security
 637. https://githubstatus.com/
 638. https://help.github.com/
 639. https://github.com/contact
 640. https://github.com/pricing
 641. https://developer.github.com/
 642. https://training.github.com/
 643. https://github.blog/
 644. https://github.com/about
 645. https://gist.github.com/karpathy/d4dee566867f8291f086
 646. https://gist.github.com/karpathy/d4dee566867f8291f086

   hidden links:
 648. https://gist.github.com/
 649. https://help.github.com/articles/which-remote-url-should-i-use
 650. https://github.com/
