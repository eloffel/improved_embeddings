   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

[16]understanding feature engineering (part 4)

a hands-on intuitive approach to deep learning methods for text
data         id97, glove and fasttext

newer, advanced strategies for taming unstructured, textual data

   go to the profile of dipanjan (dj) sarkar
   [17]dipanjan (dj) sarkar (button) blockedunblock (button)
   followfollowing
   mar 14, 2018
   [1*wvjr1rgd85div2wufltgha.jpeg]

introduction

   working with unstructured text data is hard especially when you are
   trying to build an intelligent system which interprets and understands
   free flowing natural language just like humans. you need to be able to
   process and transform noisy, unstructured textual data into some
   structured, vectorized formats which can be understood by any machine
   learning algorithm. principles from natural language processing,
   machine learning or deep learning all of which fall under the broad
   umbrella of artificial intelligence are effective tools of the trade.
   based on my previous posts, an important point to remember here is that
   any machine learning algorithm is based on principles of statistics,
   math and optimization. hence they are not intelligent enough to start
   processing text in their raw, native form. we covered some traditional
   strategies for extracting meaningful features from text data in
   [18]part-3: traditional methods for text data. i encourage you to check
   out the same for a brief refresher. in this article, we will be looking
   at more advanced feature engineering strategies which often leverage
   deep learning models. more specifically we will be covering the
   [19]id97, [20]glove and [21]fasttext models.

motivation

   we have discussed time and again including in [22]our previous article
   that feature engineering is the secret sauce to creating superior and
   better performing machine learning models. always remember that even
   with the advent of automated feature engineering capabilities, you
   would still need to understand the core concepts behind applying the
   techniques. otherwise they would just be black box models which you
   wouldn   t know how to tweak and tune for the problem you are trying to
   solve.

shortcomings of traditional models

   traditional (count-based) feature engineering strategies for textual
   data involve models belonging to a family of models popularly known as
   the id159. this includes term frequencies, tf-idf (term
   frequency-inverse document frequency), id165s and so on. while they
   are effective methods for extracting features from text, due to the
   inherent nature of the model being just a bag of unstructured words, we
   lose additional information like the semantics, structure, sequence and
   context around nearby words in each text document. this forms as enough
   motivation for us to explore more sophisticated models which can
   capture this information and give us features which are vector
   representation of words, popularly known as embeddings.

the need for id27s

   while this does make some sense, why should we be motivated enough to
   learn and build these id27s? with regard to speech or image
   recognition systems, all the information is already present in the form
   of rich dense feature vectors embedded in high-dimensional datasets
   like audio spectrograms and image pixel intensities. however when it
   comes to raw text data, especially count based models like bag of
   words, we are dealing with individual words which may have their own
   identifiers and do not capture the semantic relationship amongst words.
   this leads to huge sparse word vectors for textual data and thus if we
   do not have enough data, we may end up getting poor models or even
   overfitting the data due to the curse of dimensionality.
   [1*jwjiz0l6vmygxgt5ytrsng.png]
   comparing feature representations for audio, image and text

   to overcome the shortcomings of losing out semantics and feature
   sparsity in id159 based features, we need to make use of
   [23]vector space models (vsms) in such a way that we can embed word
   vectors in this continuous vector space based on semantic and
   contextual similarity. in fact the [24]distributional hypothesis in the
   field of [25]id65 tells us that words which occur
   and are used in the same context, are semantically similar to one
   another and have similar meanings. in simple terms,    a word is
   characterized by the company it keeps   . one of the famous papers
   talking about these semantic word vectors and various types in detail
   is [26]   don   t count, predict! a systematic comparison of
   context-counting vs. context-predicting semantic vectors    by baroni et
   al. we won   t go into extensive depth but in short, there are two main
   types of methods for contextual word vectors. count-based methods like
   [27]latent semantic analysis (lsa) which can be used to compute some
   statistical measures of how often words occur with their neighboring
   words in a corpus and then building out dense word vectors for each
   word from these measures. predictive methods like [28]neural network
   based language models try to predict words from its neighboring words
   looking at word sequences in the corpus and in the process it learns
   distributed representations giving us dense id27s. we will be
   focusing on these predictive methods in this article.

feature engineering strategies

   let   s look at some of these advanced strategies for handling text data
   and extracting meaningful features from the same, which can be used in
   downstream machine learning systems. do note that you can access all
   the code used in this article in [29]my github repository also for
   future reference. we   ll start by loading up some basic dependencies and
   settings.
import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt
pd.options.display.max_colwidth = 200
%matplotlib inline

   we will now take a few corpora of documents on which we will perform
   all our analyses. for one of the corpora, we will reuse our corpus from
   our previous article, [30]part-3: traditional methods for text data. we
   mention the code as follows for ease of understanding.

   iframe: [31]/media/415b5ca94e6f79be5e8fdb192802e5d2?postid=96c44370bbfa

   [1*ngajqkmla8_n6c4tkmxkga.png]
   our sample text corpus

   our toy corpus consists of documents belonging to several categories.
   another corpus we will use in this article is the [32]the king james
   version of the bible available freely from [33]project gutenberg
   through the corpus module in nltk. we will load this up shortly, in the
   next section. before we talk about feature engineering, we need to
   pre-process and normalize this text.

text pre-processing

   there can be multiple ways of cleaning and pre-processing textual data.
   the most important techniques which are used heavily in natural
   language processing (nlp) pipelines have been highlighted in detail in
   the    text pre-processing    section in [34]part 3 of this series. since
   the focus of this article is on feature engineering, just like our
   previous article, we will re-use our simple text pre-processor which
   focuses on removing special characters, extra whitespaces, digits,
   stopwords and lower casing the text corpus.

   iframe: [35]/media/5bbcde487e9dcdc4c5b24815bc9a90e7?postid=96c44370bbfa

   once we have our basic pre-processing pipeline ready, let   s first apply
   the same to our toy corpus.
norm_corpus = normalize_corpus(corpus)
norm_corpus
output
------
array(['sky blue beautiful', 'love blue beautiful sky',
       'quick brown fox jumps lazy dog',
       'kings breakfast sausages ham bacon eggs toast beans',
       'love green eggs ham sausages bacon',
       'brown fox quick blue dog lazy',
       'sky blue sky beautiful today',
       'dog lazy brown fox quick'],
      dtype='<u51')

   let   s now load up our other corpus based on [36]the king james version
   of the bible using nltk and pre-process the text.

   iframe: [37]/media/ce875748c8d1c3edf76518b2621c24e6?postid=96c44370bbfa

   the following output shows the total number of lines in our corpus and
   how the pre-processing works on the textual content.
output
------
total lines: 30103
sample line: ['1', ':', '6', 'and', 'god', 'said', ',', 'let', 'there', 'be', 'a
', 'firmament', 'in', 'the', 'midst', 'of', 'the', 'waters', ',', 'and', 'let',
'it', 'divide', 'the', 'waters', 'from', 'the', 'waters', '.']
processed line: god said let firmament midst waters let divide waters waters

   let   s look at some of the popular id27 models now and
   engineering features from our corpora!

the id97 model

   this model was created by google in 2013 and is a predictive deep
   learning based model to compute and generate high quality, distributed
   and continuous dense vector representations of words, which capture
   contextual and semantic similarity. essentially these are unsupervised
   models which can take in massive textual corpora, create a vocabulary
   of possible words and generate dense id27s for each word in
   the vector space representing that vocabulary. usually you can specify
   the size of the id27 vectors and the total number of vectors
   are essentially the size of the vocabulary. this makes the
   dimensionality of this dense vector space much lower than the
   high-dimensional sparse vector space built using traditional bag of
   words models.

   there are two different model architectures which can be leveraged by
   id97 to create these id27 representations. these include,
     * the continuous bag of words (cbow) model
     * the skip-gram model

   there were originally introduced by mikolov et al. and i recommend
   interested readers to read up on the original papers around these
   models which include, [38]   distributed representations of words and
   phrases and their compositionality    by mikolov et al. and
   [39]   efficient estimation of word representations in vector space    by
   mikolov et al. to gain some good in-depth perspective.

the continuous bag of words (cbow) model

   the cbow model architecture tries to predict the current target word
   (the center word) based on the source context words (surrounding
   words). considering a simple sentence,    the quick brown fox jumps over
   the lazy dog   , this can be pairs of (context_window, target_word) where
   if we consider a context window of size 2, we have examples like
   ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so
   on. thus the model tries to predict the target_word based on the
   context_window words.
   [1*uve8b6cwyykcxbbor6uid18.png]
   the cbow model architecture (source:
   [40]https://arxiv.org/pdf/1301.3781.pdf mikolov el al.)

   while the id97 family of models are unsupervised, what this means
   is that you can just give it a corpus without additional labels or
   information and it can construct dense id27s from the corpus.
   but you will still need to leverage a supervised, classification
   methodology once you have this corpus to get to these embeddings. but
   we will do that from within the corpus itself, without any auxiliary
   information. we can model this cbow architecture now as a deep learning
   classification model such that we take in the context words as our
   input, x and try to predict the target word, y. in fact building this
   architecture is simpler than the skip-gram model where we try to
   predict a whole bunch of context words from a source target word.

implementing the continuous bag of words (cbow) model

   while it   s excellent to use robust frameworks which have the id97
   model like gensim, let   s try and implement this from scratch to gain
   some perspective on how things really work behind the scenes. we will
   leverage our bible corpus contained in the norm_bible variable for
   training our model. the implementation will focus on four parts
     * build the corpus vocabulary
     * build a cbow (context, target) generator
     * build the cbow model architecture
     * train the model
     * get id27s

   without further delay, let   s get started!

build the corpus vocabulary

   to start off, we will first build our corpus vocabulary where we
   extract out each unique word from our vocabulary and map a unique
   numeric identifier to it.

   iframe: [41]/media/bd9bbb8b81c0e5954ddde15b9198658e?postid=96c44370bbfa

output
------
vocabulary size: 12425
vocabulary sample: [('perceived', 1460), ('flagon', 7287), ('gardener', 11641),
('named', 973), ('remain', 732), ('sticketh', 10622), ('abstinence', 11848), ('r
ufus', 8190), ('adversary', 2018), ('jehoiachin', 3189)]

   thus you can see that we have created a vocabulary of unique words in
   our corpus and also ways to map a word to its unique identifier and
   vice versa. the pad term is typically used to pad context words to a
   fixed length if needed.

build a cbow (context, target) generator

   we need pairs which consist of a target centre word and surround
   context words. in our implementation, a target word is of length 1 and
   surrounding context is of length 2 x window_size where we take
   window_size words before and after the target word in our corpus. this
   will become clearer with the following example.

   iframe: [42]/media/d78b2f29365578f0c2b7573a141d4176?postid=96c44370bbfa

context (x): ['old','testament','james','bible'] -> target (y): king
context (x): ['first','book','called','genesis'] -> target(y): moses
context(x):['beginning','god','heaven','earth'] -> target(y):created
context (x):['earth','without','void','darkness'] -> target(y): form
context (x): ['without','form','darkness','upon'] -> target(y): void
context (x): ['form', 'void', 'upon', 'face'] -> target(y): darkness
context (x): ['void', 'darkness', 'face', 'deep'] -> target(y): upon
context (x): ['spirit', 'god', 'upon', 'face'] -> target (y): moved
context (x): ['god', 'moved', 'face', 'waters'] -> target (y): upon
context (x): ['god', 'said', 'light', 'light'] -> target (y): let
context (x): ['god', 'saw', 'good', 'god'] -> target (y): light

   the preceding output should give you some more perspective of how x
   forms our context words and we are trying to predict the target center
   word y based on this context. for example, if the original text was    in
   the beginning god created heaven and earth    which after pre-processing
   and removal of stopwords became    beginning god created heaven earth   
   and for us, what we are trying to achieve is that. given [beginning,
   god, heaven, earth] as the context, what the target center word is,
   which is    created    in this case.

build the cbow model architecture

   we now leverage keras on top of tensorflow to build our deep learning
   architecture for the cbow model. for this our inputs will be our
   context words which are passed to an embedding layer (initialized with
   random weights). the id27s are propagated to a lambda layer
   where we average out the id27s (hence called cbow because we
   don   t really consider the order or sequence in the context words when
   averaged) and then we pass this averaged context embedding to a dense
   softmax layer which predicts our target word. we match this with the
   actual target word, compute the loss by leveraging the
   categorical_crossid178 loss and perform id26 with each
   epoch to update the embedding layer in the process. following code
   shows us our model architecture.

   iframe: [43]/media/8318bc06a7102916a95bc08066d0ab95?postid=96c44370bbfa

   [1*x_ri3nz-gfddbj1qghorhq.png]
   cbow model summary and architecture

   in case you still have difficulty in visualizing the above deep
   learning model, i would recommend you to read through the papers i
   mentioned earlier. i will try to summarize the core concepts of this
   model in simple terms. we have input context words of dimensions (2 x
   window_size), we will pass them to an embedding layer of size
   (vocab_size x embed_size) which will give us dense id27s for
   each of these context words (1 x embed_size for each word). next up we
   use a lambda layer to average out these embeddings and get an average
   dense embedding (1 x embed_size) which is sent to the dense softmax
   layer which outputs the most likely target word. we compare this with
   the actual target word, compute the loss, backpropagate the errors to
   adjust the weights (in the embedding layer) and repeat this process for
   all (context, target) pairs for multiple epochs. the following figure
   tries to explain the same.
   [1*d66fyqimwtdctouj_gcqag.png]
   visual depiction of the cbow deep learning model

   we are now ready to train this model on our corpus using our data
   generator to feed in (context, target_word) pairs.

train the model

   running the model on our complete corpus takes a fair bit of time, so i
   just ran it for 5 epochs. you can leverage the following code and
   increase it for more epochs if necessary.

   iframe: [44]/media/be6a9d59d0c47602c160fd00c2f3f1e7?postid=96c44370bbfa

epoch: 1        loss: 4257900.60084
epoch: 2        loss: 4256209.59646
epoch: 3        loss: 4247990.90456
epoch: 4        loss: 4225663.18927
epoch: 5        loss: 4104501.48929

     note: running this model is computationally expensive and works
     better if trained using a gpu. i trained this on an aws p2.x
     instance with a tesla k80 gpu and it took me close to 1.5 hours for
     just 5 epochs!

   once this model is trained, similar words should have similar weights
   based off the embedding layer and we can test out the same.

get id27s

   to get id27s for our entire vocabulary, we can extract out
   the same from our embedding layer by leveraging the following code. we
   don   t take the embedding at position 0 since it belongs to the padding
   (pad) term which is not really a word of interest.

   iframe: [45]/media/4e2be32513e0b6f4feb7a9502f7a4fbf?postid=96c44370bbfa

   [1*webdyfsnma-hdbmptiwi5g.png]
   id27s for our vocabulary based on the cbow model

   thus you can clearly see that each word has a dense embedding of size
   (1x100) as depicted in the preceding output. let   s try and find out
   some contextually similar words for specific words of interest based on
   these embeddings. for this, we build out a pairwise distance matrix
   amongst all the words in our vocabulary based on the dense embedding
   vectors and then find out the n-nearest neighbors of each word of
   interest based on the shortest (euclidean) distance.

   iframe: [46]/media/31ce0d73016c1ad96263d9ec2385c83c?postid=96c44370bbfa

(12424, 12424)
{'egypt': ['destroy', 'none', 'whole', 'jacob', 'sea'],
 'famine': ['wickedness', 'sore', 'countries', 'cease', 'portion'],
 'god': ['therefore', 'heard', 'may', 'behold', 'heaven'],
 'gospel': ['church', 'fowls', 'churches', 'preached', 'doctrine'],
 'jesus': ['law', 'heard', 'world', 'many', 'dead'],
 'john': ['dream', 'bones', 'held', 'present', 'alive'],
 'moses': ['pharaoh', 'gate', 'jews', 'departed', 'lifted'],
 'noah': ['abram', 'plagues', 'hananiah', 'korah', 'sarah']}

   you can clearly see that some of these make sense contextually (god,
   heaven), (gospel, church) and so on and some may not. training for more
   epochs usually ends up giving better results. we will now explore the
   skip-gram architecture which often gives better results as compared to
   cbow.

the skip-gram model

   the skip-gram model architecture usually tries to achieve the reverse
   of what the cbow model does. it tries to predict the source context
   words (surrounding words) given a target word (the center word).
   considering our simple sentence from earlier,    the quick brown fox
   jumps over the lazy dog   . if we used the cbow model, we get pairs of
   (context_window, target_word) where if we consider a context window of
   size 2, we have examples like ([quick, fox], brown), ([the, brown],
   quick), ([the, dog], lazy) and so on. now considering that the
   skip-gram model   s aim is to predict the context from the target word,
   the model typically inverts the contexts and targets, and tries to
   predict each context word from its target word. hence the task becomes
   to predict the context [quick, fox] given target word    brown    or [the,
   brown] given target word    quick    and so on. thus the model tries to
   predict the context_window words based on the target_word.
   [1*sr6l59udy05_buicajb6-w.png]
   the skip-gram model architecture (source:
   [47]https://arxiv.org/pdf/1301.3781.pdf mikolov el al.)

   just like we discussed in the cbow model, we need to model this
   skip-gram architecture now as a deep learning classification model such
   that we take in the target word as our input and try to predict the
   context words.this becomes slightly complex since we have multiple
   words in our context. we simplify this further by breaking down each
   (target, context_words) pair into (target, context) pairs such that
   each context consists of only one word. hence our dataset from earlier
   gets transformed into pairs like (brown, quick), (brown, fox), (quick,
   the), (quick, brown) and so on. but how to supervise or train the model
   to know what is contextual and what is not?

   for this, we feed our skip-gram model pairs of (x, y) where x is our
   input and y is our label. we do this by using [(target, context), 1]
   pairs as positive input samples where target is our word of interest
   and context is a context word occurring near the target word and the
   positive label 1 indicates this is a contextually relevant pair. we
   also feed in [(target, random), 0] pairs as negative input samples
   where target is again our word of interest but random is just a
   randomly selected word from our vocabulary which has no context or
   association with our target word. hence the negative label 0 indicates
   this is a contextually irrelevant pair. we do this so that the model
   can then learn which pairs of words are contextually relevant and which
   are not and generate similar embeddings for semantically similar words.

implementing the skip-gram model

   let   s now try and implement this model from scratch to gain some
   perspective on how things work behind the scenes and also so that we
   can compare it with our implementation of the cbow model. we will
   leverage our bible corpus as usual which is contained in the norm_bible
   variable for training our model. the implementation will focus on five
   parts
     * build the corpus vocabulary
     * build a skip-gram [(target, context), relevancy] generator
     * build the skip-gram model architecture
     * train the model
     * get id27s

   let   s get cracking and build our skip-gram id97 model!

build the corpus vocabulary

   to start off, we will follow the standard process of building our
   corpus vocabulary where we extract out each unique word from our
   vocabulary and assign a unique identifier, similar to what we did in
   the cbow model. we also maintain mappings to transform words to their
   unique identifiers and vice-versa.

   iframe: [48]/media/f47bf5fcb06c55ee82a8b080e62c92d2?postid=96c44370bbfa

vocabulary size: 12425
vocabulary sample: [('perceived', 1460), ('flagon', 7287), ('gardener', 11641),
('named', 973), ('remain', 732), ('sticketh', 10622), ('abstinence', 11848), ('r
ufus', 8190), ('adversary', 2018), ('jehoiachin', 3189)]

   just like we wanted, each unique word from the corpus is a part of our
   vocabulary now with a unique numeric identifier.

build a skip-gram [(target, context), relevancy] generator

   it   s now time to build out our skip-gram generator which will give us
   pair of words and their relevance like we discussed earlier. luckily,
   keras has a nifty skipgrams utility which can be used and we don   t have
   to manually implement this generator like we did in cbow.

     note: the function [49]skipgrams(   ) is present in
     [50]keras.preprocessing.sequence

     this function transforms a sequence of word indexes (list of
     integers) into tuples of words of the form:

     - (word, word in the same window), with label 1 (positive samples).

     - (word, random word from the vocabulary), with label 0 (negative
     samples).

   iframe: [51]/media/8ce56b75591d313a69c914a2b1425949?postid=96c44370bbfa

(james (1154), king (13)) -> 1
(king (13), james (1154)) -> 1
(james (1154), perform (1249)) -> 0
(bible (5766), dismissed (6274)) -> 0
(king (13), alter (5275)) -> 0
(james (1154), bible (5766)) -> 1
(king (13), bible (5766)) -> 1
(bible (5766), king (13)) -> 1
(king (13), compassion (1279)) -> 0
(james (1154), foreskins (4844)) -> 0

   thus you can see we have successfully generated our required skip-grams
   and based on the sample skip-grams in the preceding output, you can
   clearly see what is relevant and what is irrelevant based on the label
   (0 or 1).

build the skip-gram model architecture

   we now leverage keras on top of tensorflow to build our deep learning
   architecture for the skip-gram model. for this our inputs will be our
   target word and context or random word pair. each of which are passed
   to an embedding layer (initialized with random weights) of it   s own.
   once we obtain the id27s for the target and the context word,
   we pass it to a merge layer where we compute the dot product of these
   two vectors. then we pass on this dot product value to a dense sigmoid
   layer which predicts either a 1 or a 0 depending on if the pair of
   words are contextually relevant or just random words (y   ). we match
   this with the actual relevance label (y), compute the loss by
   leveraging the mean_squared_error loss and perform id26 with
   each epoch to update the embedding layer in the process. following code
   shows us our model architecture.

   iframe: [52]/media/6454e0eee726e3ca198026fc0a8ed428?postid=96c44370bbfa

   [1*tximfta5r2jswnjvt80q5q.png]
   skip-gram model summary and architecture

   understanding the above deep learning model is pretty straightforward.
   however, i will try to summarize the core concepts of this model in
   simple terms for ease of understanding. we have a pair of input words
   for each training example consisting of one input target word having a
   unique numeric identifier and one context word having a unique numeric
   identifier. if it is a positive sample the word has contextual meaning,
   is a context word and our label y=1, else if it is a negative sample,
   the word has no contextual meaning, is just a random word and our label
   y=0. we will pass each of them to an embedding layer of their own,
   having size (vocab_size x embed_size) which will give us dense word
   embeddings for each of these two words (1 x embed_size for each word).
   next up we use a merge layer to compute the dot product of these two
   embeddings and get the dot product value. this is then sent to the
   dense sigmoid layer which outputs either a 1 or 0. we compare this with
   the actual label y (1 or 0), compute the loss, backpropagate the errors
   to adjust the weights (in the embedding layer) and repeat this process
   for all (target, context) pairs for multiple epochs. the following
   figure tries to explain the same.
   [1*4uil1zwwf5-jlt-fnrjgaq.png]
   visual depiction of the skip-gram deep learning model

   let   s now start training our model with our skip-grams.

train the model

   running the model on our complete corpus takes a fair bit of time but
   lesser than the cbow model. hence i just ran it for 5 epochs. you can
   leverage the following code and increase it for more epochs if
   necessary.

   iframe: [53]/media/e471024a945674b6d3664fdc901a7a91?postid=96c44370bbfa

epoch: 1 loss: 4529.63803683
epoch: 2 loss: 3750.71884749
epoch: 3 loss: 3752.47489296
epoch: 4 loss: 3793.9177565
epoch: 5 loss: 3716.07605051

   once this model is trained, similar words should have similar weights
   based off the embedding layer and we can test out the same.

get id27s

   to get id27s for our entire vocabulary, we can extract out
   the same from our embedding layer by leveraging the following code. do
   note that we are only interested in the target id27 layer,
   hence we will extract the embeddings from our word_model embedding
   layer. we don   t take the embedding at position 0 since none of our
   words in the vocabulary have a numeric identifier of 0 and we ignore
   it.

   iframe: [54]/media/2f211b35e43c26a3cb69827700c516a6?postid=96c44370bbfa

   [1*guud_ckjhyowniit_6ycha.png]
   id27s for our vocabulary based on the skip-gram model

   thus you can clearly see that each word has a dense embedding of size
   (1x100) as depicted in the preceding output similar to what we had
   obtained from the cbow model. let   s now apply the euclidean distance
   metric on these dense embedding vectors to generate a pairwise distance
   metric for each word in our vocabulary. we can then find out the
   n-nearest neighbors of each word of interest based on the shortest
   (euclidean) distance similar to what we did on the embeddings from our
   cbow model.

   iframe: [55]/media/deb35755aae77661488e6834a10eb42a?postid=96c44370bbfa

(12424, 12424)
{'egypt': ['pharaoh', 'mighty', 'houses', 'kept', 'possess'],
 'famine': ['rivers', 'foot', 'pestilence', 'wash', 'sabbaths'],
 'god': ['evil', 'iniquity', 'none', 'mighty', 'mercy'],
 'gospel': ['grace', 'shame', 'believed', 'verily', 'everlasting'],
 'jesus': ['christ', 'faith', 'disciples', 'dead', 'say'],
 'john': ['ghost', 'knew', 'peter', 'alone', 'master'],
 'moses': ['commanded', 'offerings', 'kept', 'presence', 'lamb'],
 'noah': ['flood', 'shem', 'peleg', 'abram', 'chose']}

   you can clearly see from the results that a lot of the similar words
   for each of the words of interest are making sense and we have obtained
   better results as compared to our cbow model. let   s visualize these
   words embeddings now using [56]id167 which stands for [57]t-distributed
   stochastic neighbor embedding a popular [58]id84
   technique to visualize higher dimension spaces in lower dimensions
   (e.g. 2-d).

   iframe: [59]/media/363f427f5938c7f3f119018070966e82?postid=96c44370bbfa

   [1*5n5p0cqcitiigpfbqufc-a.png]
   visualizing skip-gram id97 id27s using id167

   i have marked some circles in red which seemed to show different words
   of contextual similarity positioned near each other in the vector
   space. if you find any other interesting patterns feel free to let me
   know!

robust id97 models with gensim

   while our implementations are decent enough, they are not optimized
   enough to work well on large corpora. the [60]gensim framework, created
   by radim   eh    ek consists of a robust, efficient and scalable
   implementation of the id97 model. we will leverage the same on our
   bible corpus. in our workflow, we will tokenize our normalized corpus
   and then focus on the following four parameters in the id97 model
   to build it.
     * size: the id27 dimensionality
     * window: the context window size
     * min_count: the minimum word count
     * sample: the downsample setting for frequent words

   after building our model, we will use our words of interest to see the
   top similar words for each of them.

   iframe: [61]/media/4228a77291e1a2bdc64eb4d0a742c084?postid=96c44370bbfa

   [1*37qdyf5blmnumlyhhsobaa.png]

   the similar words here definitely are more related to our words of
   interest and this is expected given that we ran this model for more
   number of iterations which must have yield better and more contextual
   embeddings. do you notice any interesting associations?
   [1*faxo2ucuoggww6ryrxcewa.png]
   noah   s sons come up as the most contextually similar entities from
   our model!

   let   s also visualize the words of interest and their similar words
   using their embedding vectors after reducing their dimensions to a 2-d
   space with id167.

   iframe: [62]/media/e2e5db88b62304244ef8b02440227fa0?postid=96c44370bbfa

   [1*ktp0x8q65c5zrg2z2lvufq.png]
   visualizing our id97 id27s using id167

   the red circles have been drawn by me to point out some interesting
   associations which i found out. we can clearly see based on what i
   depicted earlier that noah and his sons are quite close to each other
   based on the id27s from our model!

applying id97 features for machine learning tasks

   if you remember reading the previous article [63]part-3: traditional
   methods for text data you might have seen me using features for some
   actual machine learning tasks like id91. let   s leverage our other
   top corpus and try to achieve the same. to start with, we will build a
   simple id97 model on the corpus and visualize the embeddings.

   iframe: [64]/media/c8c38f67581baea9a87eff17d48c0110?postid=96c44370bbfa

   [1*uqphn8yrbijexpl9nmfrqg.png]
   visualizing id97 id27s on our toy corpus

   remember that our corpus is extremely small so to get meaninful word
   embeddings and for the model to get more context and semantics, more
   data helps. now what is a id27 in this scenario? it   s
   typically a dense vector for each word as depicted in the following
   example for the word sky.
w2v_model.wv['sky']
output
------
array([ 0.04576328,  0.02328374, -0.04483001,  0.0086611 ,  0.05173225, 0.009533
58, -0.04087641, -0.00427487, -0.0456274 ,  0.02155695], dtype=float32)

   now suppose we wanted to cluster the eight documents from our toy
   corpus, we would need to get the document level embeddings from each of
   the words present in each document. one strategy would be to average
   out the id27s for each word in a document. this is an
   extremely useful strategy and you can adopt the same for your own
   problems. let   s apply this now on our corpus to get features for each
   document.

   iframe: [65]/media/d6f950cb56e857d991d4916e75b259b6?postid=96c44370bbfa

   [1*4hddrzf4myweyuwwv-qa9w.png]
   document level embeddings

   now that we have our features for each document, let   s cluster these
   documents using the [66]affinity propagation algorithm, which is a
   id91 algorithm based on the concept of    message passing    between
   data points and does not need the number of clusters as an explicit
   input which is often required by partition-based id91 algorithms.

   iframe: [67]/media/4a2dbb6aabe44bdde0694b488dc7d47b?postid=96c44370bbfa

   [1*dz8k4wuqdclefj-hplcsoq.png]
   clusters assigned based on our document features from id97

   we can see that our algorithm has clustered each document into the
   right group based on our id97 features. pretty neat! we can also
   visualize how each document in positioned in each cluster by using
   [68]principal component analysis (pca) to reduce the feature dimensions
   to 2-d and then visualizing the same (by color coding each cluster).

   iframe: [69]/media/957b0abf60d65fcfaf74f6eb25bc7a42?postid=96c44370bbfa

   [1*padhl9llqbc0v-dl_4kqbg.png]
   visualizing our document clusters

   everything looks to be in order as documents in each cluster are closer
   to each other and far apart from other clusters.

the glove model

   the glove model stands for global vectors which is an unsupervised
   learning model which can be used to obtain dense word vectors similar
   to id97. however the technique is different and training is
   performed on an aggregated global word-word co-occurrence matrix,
   giving us a vector space with meaningful sub-structures. this method
   was invented in stanford by pennington et al. and i recommend you to
   read the original paper on glove, [70]   glove: global vectors for word
   representation    by pennington et al. which is an excellent read to get
   some perspective on how this model works.

   we won   t cover the implementation of the model from scratch in too much
   detail here but if you are interested in the actual code, you can check
   out the [71]official glove page. we will keep things simple here and
   try to understand the basic concepts behind the glove model. we have
   talked about count based id105 methods like lsa and
   predictive methods like id97. the paper claims that currently, both
   families suffer significant drawbacks. methods like lsa efficiently
   leverage statistical information but they do relatively poorly on the
   word analogy task like how we found out semantically similar words.
   methods like skip-gram may do better on the analogy task, but they
   poorly utilize the statistics of the corpus on a global level.

   the basic methodology of the glove model is to first create a huge
   word-context co-occurence matrix consisting of (word, context) pairs
   such that each element in this matrix represents how often a word
   occurs with the context (which can be a sequence of words). the idea
   then is to apply id105 to approximate this matrix as
   depicted in the following figure.
   [1*untssilztkxjlg99vxxsqw.png]
   conceptual model for the glove model   s implementation

   considering the word-context (wc) matrix, word-feature (wf) matrix and
   feature-context (fc) matrix, we try to factorize wc = wf x fc, such
   that we we aim to reconstruct wc from wf and fc by multiplying them.
   for this, we typically initialize wf and fc with some random weights
   and attempt to multiply them to get wc    (an approximation of wc) and
   measure how close it is to wc. we do this multiple times using
   [72]stochastic id119 (sgd) to minimize the error. finally,
   the word-feature matrix (wf) gives us the id27s for each word
   where f can be preset to a specific number of dimensions. a very
   important point to remember is that both id97 and glove models are
   very similar in how they work. both of them aim to build a vector space
   where the position of each word is influenced by its neighboring words
   based on their context and semantics. id97 starts with local
   individual examples of word co-occurrence pairs and glove starts with
   global aggregated co-occurrence statistics across all words in the
   corpus.

applying glove features for machine learning tasks

   let   s try and leverage glove based embeddings for our document
   id91 task. the very popular [73]spacy framework comes with
   capabilities to leverage glove embeddings based on different language
   models. you can also [74]get pre-trained word vectors and load them up
   as needed using gensim or spacy. we will first install spacy and use
   the [75]en_vectors_web_lg model which consists of 300-dimensional word
   vectors trained on [76]common crawl with glove.
# use the following command to install spacy
> pip install -u spacy
or
> conda install -c conda-forge spacy
# download the following language model and store it in disk
[77]https://github.com/explosion/spacy-models/releases/tag/en_vectors_web_lg-2.0
.0
# link the same to spacy
> python -m spacy link ./spacymodels/en_vectors_web_lg-2.0.0/en_vectors_web_lg e
n_vecs
linking successful
    ./spacymodels/en_vectors_web_lg-2.0.0/en_vectors_web_lg --> ./anaconda3/lib/
site-packages/spacy/data/en_vecs
you can now load the model via spacy.load('en_vecs')

   there are automated ways to install models in spacy too, you can check
   their [78]models & languages page for more information if needed. i had
   some issues with the same so i had to manually load them up. we will
   now load up our language model using spacy.

   iframe: [79]/media/6ba2e1d4af0cfd77adaf70b1dc64fca9?postid=96c44370bbfa

total word vectors: 1070971

   this validates that everything is working and in order. let   s get the
   glove embeddings for each of our words now in our toy corpus.

   iframe: [80]/media/dc94d854bf1aef113ea611a8abeca8fc?postid=96c44370bbfa

   [1*g5esnpyyeu_pnwzwmycbnw.png]
   glove embeddings for words in our toy corpus

   we can now use id167 to visualize these embeddings similar to what we
   did using our id97 embeddings.

   iframe: [81]/media/418817267fd1d42d29c18f2e79d58ad0?postid=96c44370bbfa

   [1*ik7wvohok7mdava6tqpoia.png]
   visualizing glove id27s on our toy corpus

   the beauty of spacy is that it will automatically provide you the
   averaged embeddings for words in each document without having to
   implement a function like we did in id97. we will leverage the same
   to get document features for our corpus and use [82]id116 id91
   to cluster our documents.

   iframe: [83]/media/32488eefff2c2e17fefce9348510c3f4?postid=96c44370bbfa

   [1*euubxnjykkjtnqmcz4wyxa.png]
   clusters assigned based on our document features from glove

   we see consistent clusters similar to what we obtained from our
   id97 model which is good! the glove model claims to perform better
   than the id97 model in many scenarios as illustrated in the
   following graph from the [84]original paper by pennington el al.
   [1*m84ros1ymc7azutuyjnonw.png]
   glove vs id97 performance (source:
   [85]https://nlp.stanford.edu/pubs/glove.pdf by pennington et al.)

   the above experiments were done by training 300-dimensional vectors on
   the same 6b token corpus (wikipedia 2014 + gigaword 5) with the same
   400,000 word vocabulary and a symmetric context window of size 10 in
   case anyone is interested in the details.

the fasttext model

   the [86]fasttext model was first introduced by facebook in 2016 as an
   extension and supposedly improvement of the vanilla id97 model.
   based on the original paper titled [87]   enriching word vectors with
   subword information    by mikolov et al. which is an excellent read to
   gain an in-depth understanding of how this model works. overall,
   fasttext is a framework for learning word representations and also
   performing robust, fast and accurate text classification. the framework
   is open-sourced by [88]facebook on [89]github and claims to have the
   following.
     * recent state-of-the-art [90]english word vectors.
     * word vectors for [91]157 languages trained on wikipedia and crawl.
     * models for [92]id46 and [93]various supervised
       tasks.

   though i haven   t implemented this model from scratch, based on the
   research paper, following is what i learnt about how the model works.
   in general, predictive models like the id97 model typically
   considers each word as a distinct entity (e.g. where) and generates a
   dense embedding for the word. however this poses to be a serious
   limitation with languages having massive vocabularies and many rare
   words which may not occur a lot in different corpora. the id97
   model typically ignores the morphological structure of each word and
   considers a word as a single entity. the fasttext model considers each
   word as a bag of character id165s. this is also called as a subword
   model in the paper.

   we add special boundary symbols < and > at the beginning and end of
   words. this enables us to distinguish prefixes and suffixes from other
   character sequences. we also include the word w itself in the set of
   its id165s, to learn a representation for each word (in addition to
   its character id165s). taking the word where and n=3 (tri-grams) as an
   example, it will be represented by the character id165s: <wh, whe,
   her, ere, re> and the special sequence <where> representing the whole
   word. note that the sequence , corresponding to the word <her> is
   different from the tri-gram her from the word where.

   in practice, the paper recommends in extracting all the id165s for n    
   3 and n     6. this is a very simple approach, and different sets of
   id165s could be considered, for example taking all prefixes and
   suffixes. we typically associate a vector representation (embedding) to
   each id165 for a word. thus, we can represent a word by the sum of the
   vector representations of its id165s or the average of the embedding
   of these id165s. thus, due to this effect of leveraging id165s from
   individual words based on their characters, there is a higher chance
   for rare words to get a good representation since their character based
   id165s should occur across other words of the corpus.

applying fasttext features for machine learning tasks

   the gensim package has nice wrappers providing us interfaces to
   leverage the fasttext model available under the gensim.models.fasttext
   module. let   s apply this once again on our bible corpus and look at our
   words of interest and their most similar words.

   iframe: [94]/media/8d0852c4225d1fa1cc024cbcc62df05a?postid=96c44370bbfa

   [1*yjuxkhlhkwa59bh1bzcwha.png]

   you can see a lot of similarity in the results with our id97 model
   with relevant similar words for each of our words of interest. do you
   notice any interesting associations and similarities?
   [1*-9s6tow0lyspmqsmcdwpna.png]
   moses, his brother aaron and the tabernacle of moses

     note: running this model is computationally expensive and usually
     takes more time as compared to the skip-gram model since it
     considers id165s for each word. this works better if trained using
     a gpu or a good cpu. i trained this on an aws p2.x instance and it
     took me around 10 minutes as compared to over 2   3 hours on a regular
     system.

   let   s now use [95]principal component analysis (pca) to reduce the word
   embedding dimensions to 2-d and then visualize the same.

   iframe: [96]/media/149135fa965843c90d44437cd779bb2b?postid=96c44370bbfa

   [1*s1v1abviw1a6pwlwifd2wq.png]
   visualizing fasttest id27s on our bible corpus

   we can see a lot of interesting patterns! noah, his son shem and
   grandfather methuselah are close to each other. we also see god
   associated with moses and egypt where it endured the biblical plagues
   including famine and pestilence. also jesus and some of his disciples
   are associated close to each other.

   to access any of the id27s you can just index the model with
   the word as follows.
ft_model.wv['jesus']
array([-0.23493268,  0.14237943,  0.35635167,  0.34680951,
        0.09342121,..., -0.15021783, -0.08518736, -0.28278247,
       -0.19060139], dtype=float32)

   having these embeddings, we can perform some interesting natural
   language tasks. one of these would be to find out similarity between
   different words (entities).
print(ft_model.wv.similarity(w1='god', w2='satan'))
print(ft_model.wv.similarity(w1='god', w2='jesus'))
output
------
0.333260876685
0.698824900473

   we can see that    god    is more closely associated with    jesus    rather
   than    satan    based on the text in our bible corpus. quite relevant!

   considering id27s being present, we can even find out odd
   words from a bunch of words as follows.
st1 = "god jesus satan john"
print('odd one out for [',st1, ']:',
      ft_model.wv.doesnt_match(st1.split()))
st2 = "john peter james judas"
print('odd one out for [',st2, ']:',
      ft_model.wv.doesnt_match(st2.split()))
output
------
odd one out for [ god jesus satan john ]: satan
odd one out for [ john peter james judas ]: judas

   interesting and relevant results in both cases for the odd entity
   amongst the other words!

conclusion

   these examples should give you a good idea about newer and efficient
   strategies around leveraging deep learning language models to extract
   features from text data and also address problems like word semantics,
   context and data sparsity. next up will be detailed strategies on
   leveraging deep learning models for feature engineering on image data.
   stay tuned!
     __________________________________________________________________

   to read about feature engineering strategies for continuous numeric
   data, check out [97]part 1 of this series!

   to read about feature engineering strategies for discrete categoricial
   data, check out [98]part 2 of this series!

   to read about traditional feature engineering strategies for
   unstructured text data, check out [99]part 3 of this series!

   all the code and datasets used in this article can be accessed from my
   [100]github

   the code is also available as a [101]jupyter notebook

   architecture diagrams unless explicitly cited are my copyright. feel
   free to use them, but please do remember to cite the source if you want
   to use them in your own work.

   if you have any feedback, comments or interesting insights to share
   about my article or data science in general, feel free to reach out to
   me on my linkedin social media channel.
   [102]dipanjan sarkar - data scientist - intel corporation | linkedin
   view dipanjan sarkar's profile on linkedin, the world's largest
   professional community. dipanjan has 5 jobs listed
   on   www.linkedin.com

     * [103]machine learning
     * [104]data science
     * [105]python
     * [106]deep learning
     * [107]tds feature engineering

   (button)
   (button)
   (button) 2k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of dipanjan (dj) sarkar

[108]dipanjan (dj) sarkar

   medium member since dec 2018

   data scientist [109]@redhat, author, consultant, mentor
   [110]@springboard, editor [111]@tdatascience. feel free to connect with
   me at [112]https://www.linkedin.com/in/dipanzan

     (button) follow
   [113]towards data science

[114]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2k
     * (button)
     *
     *

   [115]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [116]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/96c44370bbfa
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_iiqbhsofdmp1---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/tagged/tds-feature-engineering
  17. https://towardsdatascience.com/@dipanzan.sarkar
  18. https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41
  19. https://en.wikipedia.org/wiki/id97
  20. https://nlp.stanford.edu/projects/glove/
  21. https://research.fb.com/fasttext/
  22. https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41
  23. https://en.wikipedia.org/wiki/vector_space_model
  24. https://en.wikipedia.org/wiki/distributional_semantics#distributional_hypothesis
  25. https://en.wikipedia.org/wiki/distributional_semantics#distributional_hypothesis
  26. http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf
  27. https://en.wikipedia.org/wiki/latent_semantic_analysis
  28. http://www.scholarpedia.org/article/neural_net_language_models
  29. https://github.com/dipanjans/practical-machine-learning-with-python/tree/master/bonus content/feature engineering text data
  30. https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41
  31. https://towardsdatascience.com/media/415b5ca94e6f79be5e8fdb192802e5d2?postid=96c44370bbfa
  32. https://www.gutenberg.org/files/10/10-h/10-h.htm
  33. https://www.gutenberg.org/
  34. https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41
  35. https://towardsdatascience.com/media/5bbcde487e9dcdc4c5b24815bc9a90e7?postid=96c44370bbfa
  36. https://www.gutenberg.org/files/10/10-h/10-h.htm
  37. https://towardsdatascience.com/media/ce875748c8d1c3edf76518b2621c24e6?postid=96c44370bbfa
  38. https://arxiv.org/pdf/1310.4546.pdf
  39. https://arxiv.org/pdf/1301.3781.pdf
  40. https://arxiv.org/pdf/1301.3781.pdf
  41. https://towardsdatascience.com/media/bd9bbb8b81c0e5954ddde15b9198658e?postid=96c44370bbfa
  42. https://towardsdatascience.com/media/d78b2f29365578f0c2b7573a141d4176?postid=96c44370bbfa
  43. https://towardsdatascience.com/media/8318bc06a7102916a95bc08066d0ab95?postid=96c44370bbfa
  44. https://towardsdatascience.com/media/be6a9d59d0c47602c160fd00c2f3f1e7?postid=96c44370bbfa
  45. https://towardsdatascience.com/media/4e2be32513e0b6f4feb7a9502f7a4fbf?postid=96c44370bbfa
  46. https://towardsdatascience.com/media/31ce0d73016c1ad96263d9ec2385c83c?postid=96c44370bbfa
  47. https://arxiv.org/pdf/1301.3781.pdf
  48. https://towardsdatascience.com/media/f47bf5fcb06c55ee82a8b080e62c92d2?postid=96c44370bbfa
  49. https://keras.io/preprocessing/sequence/#skipgrams
  50. https://keras.io/preprocessing/sequence
  51. https://towardsdatascience.com/media/8ce56b75591d313a69c914a2b1425949?postid=96c44370bbfa
  52. https://towardsdatascience.com/media/6454e0eee726e3ca198026fc0a8ed428?postid=96c44370bbfa
  53. https://towardsdatascience.com/media/e471024a945674b6d3664fdc901a7a91?postid=96c44370bbfa
  54. https://towardsdatascience.com/media/2f211b35e43c26a3cb69827700c516a6?postid=96c44370bbfa
  55. https://towardsdatascience.com/media/deb35755aae77661488e6834a10eb42a?postid=96c44370bbfa
  56. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  57. https://en.wikipedia.org/wiki/t-distributed_stochastic_neighbor_embedding
  58. https://en.wikipedia.org/wiki/dimensionality_reduction
  59. https://towardsdatascience.com/media/363f427f5938c7f3f119018070966e82?postid=96c44370bbfa
  60. https://radimrehurek.com/gensim/
  61. https://towardsdatascience.com/media/4228a77291e1a2bdc64eb4d0a742c084?postid=96c44370bbfa
  62. https://towardsdatascience.com/media/e2e5db88b62304244ef8b02440227fa0?postid=96c44370bbfa
  63. https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41
  64. https://towardsdatascience.com/media/c8c38f67581baea9a87eff17d48c0110?postid=96c44370bbfa
  65. https://towardsdatascience.com/media/d6f950cb56e857d991d4916e75b259b6?postid=96c44370bbfa
  66. https://en.wikipedia.org/wiki/affinity_propagation
  67. https://towardsdatascience.com/media/4a2dbb6aabe44bdde0694b488dc7d47b?postid=96c44370bbfa
  68. https://en.wikipedia.org/wiki/principal_component_analysis
  69. https://towardsdatascience.com/media/957b0abf60d65fcfaf74f6eb25bc7a42?postid=96c44370bbfa
  70. https://nlp.stanford.edu/pubs/glove.pdf
  71. https://nlp.stanford.edu/projects/glove/
  72. https://en.wikipedia.org/wiki/stochastic_gradient_descent
  73. https://spacy.io/
  74. https://nlp.stanford.edu/projects/glove/
  75. https://spacy.io/models/en#en_vectors_web_lg
  76. http://commoncrawl.org/
  77. https://github.com/explosion/spacy-models/releases/tag/en_vectors_web_lg-2.0.0
  78. https://spacy.io/usage/models
  79. https://towardsdatascience.com/media/6ba2e1d4af0cfd77adaf70b1dc64fca9?postid=96c44370bbfa
  80. https://towardsdatascience.com/media/dc94d854bf1aef113ea611a8abeca8fc?postid=96c44370bbfa
  81. https://towardsdatascience.com/media/418817267fd1d42d29c18f2e79d58ad0?postid=96c44370bbfa
  82. https://en.wikipedia.org/wiki/id116_id91
  83. https://towardsdatascience.com/media/32488eefff2c2e17fefce9348510c3f4?postid=96c44370bbfa
  84. https://nlp.stanford.edu/pubs/glove.pdf
  85. https://nlp.stanford.edu/pubs/glove.pdf
  86. https://fasttext.cc/
  87. https://arxiv.org/pdf/1607.04606.pdf
  88. https://www.facebook.com/
  89. https://github.com/facebookresearch/fasttext
  90. https://fasttext.cc/docs/en/english-vectors.html
  91. https://github.com/facebookresearch/fasttext/blob/master/docs/crawl-vectors.md
  92. https://fasttext.cc/docs/en/language-identification.html#content
  93. https://fasttext.cc/docs/en/supervised-models.html#content
  94. https://towardsdatascience.com/media/8d0852c4225d1fa1cc024cbcc62df05a?postid=96c44370bbfa
  95. https://en.wikipedia.org/wiki/principal_component_analysis
  96. https://towardsdatascience.com/media/149135fa965843c90d44437cd779bb2b?postid=96c44370bbfa
  97. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  98. https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63
  99. https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41
 100. https://github.com/dipanjans/practical-machine-learning-with-python/tree/master/bonus content/feature engineering text data
 101. https://github.com/dipanjans/practical-machine-learning-with-python/blob/master/bonus content/feature engineering text data/feature engineering text data - advanced deep learning strategies.ipynb
 102. https://www.linkedin.com/in/dipanzan/
 103. https://towardsdatascience.com/tagged/machine-learning?source=post
 104. https://towardsdatascience.com/tagged/data-science?source=post
 105. https://towardsdatascience.com/tagged/python?source=post
 106. https://towardsdatascience.com/tagged/deep-learning?source=post
 107. https://towardsdatascience.com/tagged/tds-feature-engineering?source=post
 108. https://towardsdatascience.com/@dipanzan.sarkar
 109. http://twitter.com/redhat
 110. http://twitter.com/springboard
 111. http://twitter.com/tdatascience
 112. https://www.linkedin.com/in/dipanzan
 113. https://towardsdatascience.com/?source=footer_card
 114. https://towardsdatascience.com/?source=footer_card
 115. https://towardsdatascience.com/
 116. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 118. https://towardsdatascience.com/@dipanzan.sarkar?source=post_header_lockup
 119. https://www.linkedin.com/in/dipanzan/
 120. https://medium.com/p/96c44370bbfa/share/twitter
 121. https://medium.com/p/96c44370bbfa/share/facebook
 122. https://towardsdatascience.com/@dipanzan.sarkar?source=footer_card
 123. https://medium.com/p/96c44370bbfa/share/twitter
 124. https://medium.com/p/96c44370bbfa/share/facebook
