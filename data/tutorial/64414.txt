   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

[16]understanding feature engineering (part 3)

traditional methods for text data

traditional strategies for taming unstructured, textual data

   go to the profile of dipanjan (dj) sarkar
   [17]dipanjan (dj) sarkar (button) blockedunblock (button)
   followfollowing
   jan 29, 2018
   [1*vxkke3j-lfi1yq7hc6onxq.jpeg]

introduction

   we have covered various feature engineering strategies for dealing with
   structured data in the first two parts of this series. check out
   [18]part-i: continuous, numeric data and [19]part-ii: discrete,
   categorical data for a refresher. in this article, we will look at how
   to work with text data, which is definitely one of the most abundant
   sources of unstructured data. text data usually consists of documents
   which can represent words, sentences or even paragraphs of free flowing
   text. the inherent unstructured (no neatly formatted data columns!) and
   noisy nature of textual data makes it harder for machine learning
   methods to directly work on raw text data. hence, in this article, we
   will follow a hands-on approach to explore some of the most popular and
   effective strategies for extracting meaningful features from text data.
   these features can then be used in building machine learning or deep
   learning models easily.

motivation

   feature engineering is often known as the secret sauce to creating
   superior and better performing machine learning models. just one
   excellent feature could be your ticket to winning a [20]kaggle
   challenge! the importance of feature engineering is even more important
   for unstructured, textual data because we need to convert free flowing
   text into some numeric representations which can then be understood by
   machine learning algorithms. even with the advent of automated feature
   engineering capabilities, you would still need to understand the core
   concepts behind different feature engineering strategies before
   applying them as black box models. always remember,    if you are given a
   box of tools to repair a house, you should know when to use a power
   drill and when to use a hammer!   .

understanding text data

   i   m sure all of you must be having a fair idea of what textual data
   comprises of in this scenario. do remember you can always have text
   data in the form of structured data attributes, but usually those fall
   under the umbrella of structured, categorical data.
   [1*1ihhqrbr0m6c-zgher7i_q.png]

   in this scenario, we are talking about free flowing text in the form of
   words, phrases, sentences and entire documents. essentially, we do have
   some syntactic structure like words make phrases, phrases make
   sentences which in turn make paragraphs. however, there is no inherent
   structure to text documents because you can have a wide variety of
   words which can vary across documents and each sentence will also be of
   variable length as compared to a fixed number of data dimensions in
   structured datasets. this article itself is a perfect example of text
   data!

feature engineering strategies

   let   s look at some popular and effective strategies for handling text
   data and extracting meaningful features from the same which can be used
   in downstream machine learning systems. do note that you can access all
   the code used in this article in [21]my github repository also for
   future reference. we   ll start by loading up some basic dependencies and
   settings.
import pandas as pd
import numpy as np
import re
import nltk
import matplotlib.pyplot as plt
pd.options.display.max_colwidth = 200
%matplotlib inline

   let   s now take a sample corpus of documents on which we will run most
   of our analyses in this article. a [22]corpus is typically a collection
   of text documents usually belonging to one or more subjects.

   iframe: [23]/media/415b5ca94e6f79be5e8fdb192802e5d2?postid=f6f7d70acd41

   [1*ngajqkmla8_n6c4tkmxkga.png]
   our sample text corpus

   you can see that we have taken a few sample text documents belonging to
   different categories for our toy corpus. before we talk about feature
   engineering, as always, we need to do some data pre-processing or
   wrangling to remove unnecessary characters, symbols and tokens.

text pre-processing

   there can be multiple ways of cleaning and pre-processing textual data.
   in the following points, we highlight some of the most important ones
   which are used heavily in natural language processing (nlp) pipelines.
     * removing tags: our text often contains unnecessary content like
       html tags, which do not add much value when analyzing text. the
       beautifulsoup library does an excellent job in providing necessary
       functions for this.
     * removing accented characters: in any text corpus, especially if you
       are dealing with the english language, often you might be dealing
       with accented characters\letters. hence we need to make sure that
       these characters are converted and standardized into ascii
       characters. a simple example would be converting    to e.
     * expanding contractions: in the english language, contractions are
       basically shortened versions of words or syllables. these shortened
       versions of existing words or phrases are created by removing
       specific letters and sounds. examples would be, do not to don   t and
       i would to i   d. converting each contraction to its expanded,
       original form often helps with text standardization.
     * removing special characters: special characters and symbols which
       are usually non alphanumeric characters often add to the extra
       noise in unstructured text. more than often, simple regular
       expressions (regexes) can be used to achieve this.
     * id30 and lemmatization: word stems are usually the base form of
       possible words that can be created by attaching affixes like
       prefixes and suffixes to the stem to create new words. this is
       known as inflection. the reverse process of obtaining the base form
       of a word is known as id30. a simple example are the words
       watches, watching, and watched. they have the word root stem watch
       as the base form. lemmatization is very similar to id30, where
       we remove word affixes to get to the base form of a word. however
       the base form in this case is known as the root word but not the
       root stem. the difference being that the root word is always a
       lexicographically correct word (present in the dictionary) but the
       root stem may not be so.
     * removing stopwords: words which have little or no significance
       especially when constructing meaningful features from text are
       known as stopwords or stop words. these are usually words that end
       up having the maximum frequency if you do a simple term or word
       frequency in a corpus. words like a, an, the, and so on are
       considered to be stopwords. there is no universal stopword list but
       we use a standard english language stopwords list from nltk. you
       can also add your own domain specific stopwords as needed.

   [1*htzcb81c8wi5vxmsgazd-w.png]

   besides this you can also do other standard operations like
   id121, removing extra whitespaces, text lower casing and more
   advanced operations like id147s, grammatical error
   corrections, removing repeated characters and so on. if you are
   interested, you can check out [24]a sample notebook on text
   pre-processing from one of [25]my recent books.

   since the focus of this article is on feature engineering, we will
   build a simple text pre-processor which focuses on removing special
   characters, extra whitespaces, digits, stopwords and lower casing the
   text corpus.

   iframe: [26]/media/5bbcde487e9dcdc4c5b24815bc9a90e7?postid=f6f7d70acd41

   once we have our basic pre-processing pipeline ready, let   s apply the
   same to our sample corpus.
norm_corpus = normalize_corpus(corpus)
norm_corpus
output
------
array(['sky blue beautiful', 'love blue beautiful sky',
       'quick brown fox jumps lazy dog',
       'kings breakfast sausages ham bacon eggs toast beans',
       'love green eggs ham sausages bacon',
       'brown fox quick blue dog lazy',
       'sky blue sky beautiful today',
       'dog lazy brown fox quick'],
      dtype='<u51')

   the above output should give you a clear view of how each of our sample
   documents look like after pre-processing. let   s engineer some features
   now!

id159

   this is perhaps the most simple vector space representational model for
   unstructured text. a vector space model is simply a mathematical model
   to represent unstructured text (or any other data) as numeric vectors,
   such that each dimension of the vector is a specific feature\attribute.
   the id159 represents each text document as a numeric
   vector where each dimension is a specific word from the corpus and the
   value could be its frequency in the document, occurrence (denoted by 1
   or 0) or even weighted values. the model   s name is such because each
   document is represented literally as a    bag    of its own words,
   disregarding word orders, sequences and grammar.

   iframe: [27]/media/207b6309e11b3712955503625a6f93af?postid=f6f7d70acd41

output
------
array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],
       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],
       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],
       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],
       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],
       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],
       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],
       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]
      ], dtype=int64)

   thus you can see that our documents have been converted into numeric
   vectors such that each document is represented by one vector (row) in
   the above feature matrix. the following code will help represent this
   in a more easy to understand format.

   iframe: [28]/media/95e694caa117ecb5ca761bcf2024bbb1?postid=f6f7d70acd41

   [1*zmdhvqq7hyv_mmz5ne-2yq.png]
   our id159 based document feature vectors

   this should make things more clearer! you can clearly see that each
   column or dimension in the feature vectors represents a word from the
   corpus and each row represents one of our documents. the value in any
   cell, represents the number of times that word (represented by column)
   occurs in the specific document (represented by row). hence if a corpus
   of documents consists of n unique words across all the documents, we
   would have an n-dimensional vector for each of the documents.

bag of id165s model

   a word is just a single token, often known as a unigram or 1-gram. we
   already know that the id159 doesn   t consider order of
   words. but what if we also wanted to take into account phrases or
   collection of words which occur in a sequence? id165s help us achieve
   that. an id165 is basically a collection of word tokens from a text
   document such that these tokens are contiguous and occur in a sequence.
   bi-grams indicate id165s of order 2 (two words), tri-grams indicate
   id165s of order 3 (three words), and so on. the bag of id165s model
   is hence just an extension of the id159 so we can also
   leverage id165 based features. the following example depicts bi-gram
   based features in each document feature vector.

   iframe: [29]/media/9086178311c628eb472aaad9dbecc20a?postid=f6f7d70acd41

   [1*trmmdjpylfzqiu6ey8inpw.png]
   bi-gram based feature vectors using the bag of id165s model

   this gives us feature vectors for our documents, where each feature
   consists of a bi-gram representing a sequence of two words and values
   represent how many times the bi-gram was present for our documents.

tf-idf model

   there are some potential problems which might arise with the bag of
   words model when it is used on large corpora. since the feature vectors
   are based on absolute term frequencies, there might be some terms which
   occur frequently across all documents and these may tend to overshadow
   other terms in the feature set. the tf-idf model tries to combat this
   issue by using a scaling or normalizing factor in its computation.
   tf-idf stands for term frequency-inverse document frequency, which uses
   a combination of two metrics in
   its computation, namely: term frequency (tf) and inverse document
   frequency (idf). this technique was developed for ranking results for
   queries in search engines and now it is an indispensable model in the
   world of information retrieval and nlp.

   mathematically, we can define tf-idf as tfidf = tf x idf, which can be
   expanded further to be represented as follows.
   [1*putph3jj0spirucghzpxsg.png]

   here, tfidf(w, d) is the tf-idf score for word w in document d. the
   term tf(w, d) represents the term frequency of the word w in document
   d, which can be obtained from the id159. the term idf(w,
   d) is the inverse document frequency for the term w, which can be
   computed as the log transform of the total number of documents in the
   corpus c divided by the document frequency of the word w, which is
   basically the frequency of documents in the corpus where the word w
   occurs. there are multiple variants of this model but they all end up
   giving quite similar results. let   s apply this on our corpus now!

   iframe: [30]/media/c5ec9a072eba6a451d4efb8382ac7bd2?postid=f6f7d70acd41

   [1*vti7eplnqxecmm_48sztww.png]
   our tf-idf model based document feature vectors

   the tf-idf based feature vectors for each of our text documents show
   scaled and normalized values as compared to the raw id159
   values. interested readers who might want to dive into further details
   of how the internals of this model work can refer to page 181 of
   [31]text analytics with python (springer\apress; dipanjan sarkar,
   2016).

document similarity

   document similarity is the process of using a distance or similarity
   based metric that can be used to identify how similar a text document
   is with any other document(s) based on features extracted from the
   documents like bag of words or tf-idf.
   [0*yqsgboqiokyxkmh3.]
   are we similar?

   thus you can see that we can build on top of the tf-idf based features
   we engineered in the previous section and use them to generate new
   features which can be useful in domains like search engines, document
   id91 and information retrieval by leveraging these similarity
   based features.

   pairwise document similarity in a corpus involves computing document
   similarity for each pair of documents in a corpus. thus if you have c
   documents in a corpus, you would end up with a c x c matrix such that
   each row and column represents the similarity score for a pair of
   documents, which represent the indices at the row and column,
   respectively. there are several similarity and distance metrics that
   are used to compute document similarity. these include cosine
   distance/similarity, euclidean distance, manhattan distance, bm25
   similarity, jaccard distance and so on. in our analysis, we will be
   using perhaps the most popular and widely used similarity metric,
   cosine similarity and compare pairwise document similarity based on
   their tf-idf feature vectors.

   iframe: [32]/media/c0948ca7bafce55f9b00103c7df42c15?postid=f6f7d70acd41

   [1*1ft_yxdrc1hc7ns18non2q.png]
   pairwise document similarity matrix (cosine similarity)

   cosine similarity basically gives us a metric representing the cosine
   of the angle between the feature vector representations of two text
   documents. lower the angle between the documents, the closer and more
   similar they are as depicted in the following figure.
   [1*6kolpsrmovtfiuggb0qbjw.png]
   cosine similarity depictions for text document feature vectors

   looking closely at the similarity matrix clearly tells us that
   documents (0, 1 and 6), (2, 5 and 7) are very similar to one another
   and documents 3 and 4 are slightly similar to each other but the
   magnitude is not very strong, however still stronger than the other
   documents. this must indicate these similar documents have some similar
   features. this is a perfect example of grouping or id91 that can
   be solved by unsupervised learning especially when you are dealing with
   huge corpora of millions of text documents.

document id91 with similarity features

   id91 leverages unsupervised learning to group data points
   (documents in this scenario) into groups or clusters. we will be
   leveraging an unsupervised hierarchical id91 algorithm here to
   try and group similar documents from our toy corpus together by
   leveraging the document similarity features we generated earlier. there
   are two types of hierarchical id91 algorithms namely,
   agglomerative and divisive methods. we will be using a agglomerative
   id91 algorithm, which is hierarchical id91 using a bottom
   up approach i.e. each observation or document starts in its own cluster
   and clusters are successively merged together using a distance metric
   which measures distances between data points and a linkage merge
   criterion. a sample depiction is shown in the following figure.
   [0*nkjubhfvynblrxv7.png]
   agglomerative hierarchical id91

   the selection of the linkage criterion governs the merge strategy. some
   examples of linkage criteria are ward, complete linkage, average
   linkage and so on. this criterion is very useful for choosing the pair
   of clusters (individual documents at the lowest step and clusters in
   higher steps) to merge at each step is based on the optimal value of an
   objective function. we choose the ward   s minimum variance method as our
   linkage criterion to minimize total within-cluster variance. hence, at
   each step, we find the pair of clusters that leads to minimum increase
   in total within-cluster variance after merging. since we already have
   our similarity features, let   s build out the linkage matrix on our
   sample documents.

   iframe: [33]/media/98551abbf6ee87df7c4316032ff04f91?postid=f6f7d70acd41

   [1*2dcwnqrb8ws3d8abmu1ira.png]
   linkage matrix for our corpus

   if you closely look at the linkage matrix, you can see that each step
   (row) of the linkage matrix tells us which data points (or clusters)
   were merged together. if you have n data points, the linkage matrix, z
   will be having a shape of (n         1) x 4 where z[i] will tell us which
   clusters were merged at step i. each row has four elements, the first
   two elements are either data point identifiers or cluster labels (in
   the later parts of the matrix once
   multiple data points are merged), the third element is the cluster
   distance between the first two elements (either data points or
   clusters), and the last element is the total number of elements\data
   points in the cluster once the merge is complete. we recommend you
   refer to the [34]scipy documentation, which explains this in detail.

   let   s now visualize this matrix as a dendrogram to understand the
   elements better!

   iframe: [35]/media/9c4a23b84f2ebbd40cba9f9a15599205?postid=f6f7d70acd41

   [1*be-evwh7xbpq0j04ixl4uq.png]
   dendrogram visualizing our hierarchical id91 process

   we can see how each data point starts as an individual cluster and
   slowly starts getting merged with other data points to form clusters.
   on a high level from the colors and the dendrogram, you can see that
   the model has correctly identified three major clusters if you consider
   a distance metric of around 1.0 or above (denoted by the dotted line).
   leveraging this distance, we get our cluster labels.

   iframe: [36]/media/58c1a784a468ea9a0e503d78cc956612?postid=f6f7d70acd41

   [1*p-f02ggicwiundqxwrgqsg.png]
   id91 our documents into groups with hierarchical id91

   thus you can clearly see our algorithm has correctly identified the
   three distinct categories in our documents based on the cluster labels
   assigned to them. this should give you a good idea of how our tf-idf
   features were leveraged to build our similarity features which in turn
   helped in id91 our documents. you can actually use this pipeline
   in the future for id91 your own documents!

topic models

   we can also use some summarization techniques to extract topic or
   concept based features from text documents. the idea of topic models
   revolves around the process of extracting key themes or concepts from a
   corpus of documents which are represented as topics. each topic can be
   represented as a bag or collection of words/terms from the document
   corpus. together, these terms signify a specific topic, theme or a
   concept and each topic can be easily distinguished from other topics by
   virtue of the semantic meaning conveyed by these terms. however often
   you do end up with overlapping topics based on the data. these concepts
   can range from simple facts and statements to opinions and outlook.
   topic models are extremely useful in summarizing large corpus of text
   documents to extract and depict key concepts. they are also useful in
   extracting features from text data that capture latent patterns in the
   data.
   [0*3yigp_yuwcwpnfm8.png]
   an example of topic models

   there are various techniques for id96 and most of them
   involve some form of matrix decomposition. some techniques like latent
   semantic indexing (lsi) use matrix decomposition operations, more
   specifically singular valued decomposition. we will be using another
   technique is id44 (lda), which uses a generative
   probabilistic model where each document consists of a combination of
   several topics and each term or word can be assigned to a specific
   topic. this is similar to plsi based model (probabilistic lsi). each
   latent topic contains a dirichlet prior over them in the case of lda.

   the math behind in this technique is pretty involved, so i will try to
   summarize it without boring you with a lot of details. i recommend
   readers to go through [37]this excellent talk by christine doig.
   [0*qzampfl2xlhf2gct.png]
   end-to-end lda framework (courtesy of c. doig, introduction to topic
   modeling in python)

   the black box in the above figure represents the core algorithm that
   makes use of the previously mentioned parameters to extract k topics
   from m documents. the following steps give a simplistic explanation of
   what happens in the algorithm behind the scenes.
   [1*wa5tvzipafcjj3bv6rw7xg.png]

   once this runs for several iterations, we should have topic mixtures
   for each document and then generate the constituents of each topic from
   the terms that point to that topic. frameworks like gensim or
   scikit-learn enable us to leverage the lda model for generating topics.

   for the purpose of feature engineering which is the intent of this
   article, you need to remember that when lda is applied on a
   document-term matrix (tf-idf or bag of words feature matrix), it gets
   decomposed into two main components.
     * a document-topic matrix, which would be the feature matrix we
       are looking for.
     * a topic-term matrix, which helps us in looking at potential topics
       in the corpus.

   let   s leverage scikit-learn to get the document-topic matrix as
   follows.

   iframe: [38]/media/61cfaa9f6f4e84d3fdc4324ff64174f8?postid=f6f7d70acd41

   [1*jfwmr3ye0wvxushwop7dpq.png]
   document-topic matrix from our lda model

   you can clearly see which documents contribute the most to which of the
   three topics in the above output. you can view the topics and their
   main constituents as follows.

   iframe: [39]/media/0aeba89a1232a57d54567f1d6231af7b?postid=f6f7d70acd41

topic 1
-------
[('sky', 4.3324395825632624), ('blue', 3.3737531748317711), ('beautiful', 3.3323
652405224857), ('today', 1.3325579841038182), ('love', 1.3304224288080069)]
topic 2
-------
[('bacon', 2.3326959484799978), ('eggs', 2.3326959484799978), ('ham', 2.33269594
84799978), ('sausages', 2.3326959484799978), ('love', 1.335454457601996), ('bean
s', 1.3327735253784641), ('breakfast', 1.3327735253784641), ('kings', 1.33277352
53784641), ('toast', 1.3327735253784641), ('green', 1.3325433207547732)]
topic 3
-------
[('brown', 3.3323474595768783), ('dog', 3.3323474595768783), ('fox', 3.332347459
5768783), ('lazy', 3.3323474595768783), ('quick', 3.3323474595768783), ('jumps',
 1.3324193736202712), ('blue', 1.2919635624485213)]

   thus you can clearly see the three topics are quite distinguishable
   from each other based on their constituent terms, first one talking
   about weather, second one about food and the last one about animals.
   choosing the number of topics for id96 is an entire topic on
   its own (pun not intended!) and is an art as well as a science. there
   are various methods and heuristics to get the optimal number of topics
   but due to the detailed nature of these techniques, we don   t discuss
   them here.

document id91 with topic model features

   we used our id159 based features to build out topic model
   based features using lda. we can now actually leverage the document
   term matrix we obtained and use an unsupervised id91 algorithm to
   try and group our documents similar to what we did earlier with our
   similarity features.

   we will use a very popular partition based id91 method this time,
   id116 id91 to cluster or group these documents based on their
   topic model feature representations. in id116 id91, we have an
   input parameter k, which specifies the number of clusters it will
   output using the document features. this id91 method is a
   centroid based id91 method, where it tries to cluster these
   documents into clusters of equal variance. it tries to create these
   clusters by minimizing the within-cluster sum of squares measure, also
   known as inertia. there are multiple ways to select the optimal value
   of k like using the sum of squared errors metric, silhouette
   coefficients and the elbow method.

   iframe: [40]/media/bc4eb0f7da8812422f01b75a0609eed0?postid=f6f7d70acd41

   [1*wxrbrltr92luqg5ozs08ua.png]
   id91 our documents into groups with id116 id91

   we can see from the above output that our documents were correctly
   assigned to the right clusters!

future scope for advanced strategies

   what we didn   t cover in this article are several advanced strategies
   around feature engineering for text data which have recently come into
   prominence. this includes leveraging deep learning based models to
   obtain id27s. we will take a deep dive into such models in
   the next part of this series and cover popular id27 models
   like [41]id97 and [42]glove with detailed hands-on examples so stay
   tuned!

conclusion

   these examples should give you a good idea about popular strategies for
   feature engineering on text data. do remember that these are
   traditional strategies based on concepts from mathematics, information
   retrieval and natural language processing. hence these tried and tested
   methods over time have proven to be successful in a wide variety of
   datasets and problems. next up will be detailed strategies on
   leveraging deep learning models for feature engineering on text data!
     __________________________________________________________________

   to read about feature engineering strategies for continuous numeric
   data, check out [43]part 1 of this series!

   to read about feature engineering strategies for discrete categoricial
   data, check out [44]part 2 of this series!

   all the code and datasets used in this article can be accessed from my
   [45]github

   the code is also available as a [46]jupyter notebook

   if you have any feedback, comments or interesting insights to share
   about my article or data science in general, feel free to reach out to
   me on my linkedin social media channel.
   [47]dipanjan sarkar | linkedin
   view dipanjan sarkar's profile on linkedin, the world's largest
   professional community. dipanjan has 5 jobs listed
   on   www.linkedin.com

     * [48]data science
     * [49]machine learning
     * [50]python
     * [51]programming
     * [52]tds feature engineering

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of dipanjan (dj) sarkar

[53]dipanjan (dj) sarkar

   medium member since dec 2018

   data scientist [54]@redhat, author, consultant, mentor
   [55]@springboard, editor [56]@tdatascience. feel free to connect with
   me at [57]https://www.linkedin.com/in/dipanzan

     (button) follow
   [58]towards data science

[59]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [60]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [61]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f6f7d70acd41
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_drveunkenosy---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/tagged/tds-feature-engineering
  17. https://towardsdatascience.com/@dipanzan.sarkar
  18. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  19. https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63
  20. https://www.kaggle.com/
  21. https://github.com/dipanjans/practical-machine-learning-with-python/tree/master/bonus content/feature engineering text data
  22. https://en.wikipedia.org/wiki/text_corpus
  23. https://towardsdatascience.com/media/415b5ca94e6f79be5e8fdb192802e5d2?postid=f6f7d70acd41
  24. https://github.com/dipanjans/practical-machine-learning-with-python/blob/master/notebooks/ch07_analyzing_movie_reviews_sentiment/text id172 demo.ipynb
  25. https://github.com/dipanjans/practical-machine-learning-with-python
  26. https://towardsdatascience.com/media/5bbcde487e9dcdc4c5b24815bc9a90e7?postid=f6f7d70acd41
  27. https://towardsdatascience.com/media/207b6309e11b3712955503625a6f93af?postid=f6f7d70acd41
  28. https://towardsdatascience.com/media/95e694caa117ecb5ca761bcf2024bbb1?postid=f6f7d70acd41
  29. https://towardsdatascience.com/media/9086178311c628eb472aaad9dbecc20a?postid=f6f7d70acd41
  30. https://towardsdatascience.com/media/c5ec9a072eba6a451d4efb8382ac7bd2?postid=f6f7d70acd41
  31. https://github.com/dipanjans/text-analytics-with-python
  32. https://towardsdatascience.com/media/c0948ca7bafce55f9b00103c7df42c15?postid=f6f7d70acd41
  33. https://towardsdatascience.com/media/98551abbf6ee87df7c4316032ff04f91?postid=f6f7d70acd41
  34. https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html
  35. https://towardsdatascience.com/media/9c4a23b84f2ebbd40cba9f9a15599205?postid=f6f7d70acd41
  36. https://towardsdatascience.com/media/58c1a784a468ea9a0e503d78cc956612?postid=f6f7d70acd41
  37. http://chdoig.github.io/pygotham-topic-modeling/#/
  38. https://towardsdatascience.com/media/61cfaa9f6f4e84d3fdc4324ff64174f8?postid=f6f7d70acd41
  39. https://towardsdatascience.com/media/0aeba89a1232a57d54567f1d6231af7b?postid=f6f7d70acd41
  40. https://towardsdatascience.com/media/bc4eb0f7da8812422f01b75a0609eed0?postid=f6f7d70acd41
  41. https://en.wikipedia.org/wiki/id97
  42. https://nlp.stanford.edu/projects/glove/
  43. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  44. https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63
  45. https://github.com/dipanjans/practical-machine-learning-with-python/tree/master/bonus content/feature engineering text data
  46. https://github.com/dipanjans/practical-machine-learning-with-python/blob/master/bonus content/feature engineering text data/feature engineering text data - traditional strategies.ipynb
  47. https://www.linkedin.com/in/dipanzan/
  48. https://towardsdatascience.com/tagged/data-science?source=post
  49. https://towardsdatascience.com/tagged/machine-learning?source=post
  50. https://towardsdatascience.com/tagged/python?source=post
  51. https://towardsdatascience.com/tagged/programming?source=post
  52. https://towardsdatascience.com/tagged/tds-feature-engineering?source=post
  53. https://towardsdatascience.com/@dipanzan.sarkar
  54. http://twitter.com/redhat
  55. http://twitter.com/springboard
  56. http://twitter.com/tdatascience
  57. https://www.linkedin.com/in/dipanzan
  58. https://towardsdatascience.com/?source=footer_card
  59. https://towardsdatascience.com/?source=footer_card
  60. https://towardsdatascience.com/
  61. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  63. https://towardsdatascience.com/@dipanzan.sarkar?source=post_header_lockup
  64. https://www.linkedin.com/in/dipanzan/
  65. https://medium.com/p/f6f7d70acd41/share/twitter
  66. https://medium.com/p/f6f7d70acd41/share/facebook
  67. https://towardsdatascience.com/@dipanzan.sarkar?source=footer_card
  68. https://medium.com/p/f6f7d70acd41/share/twitter
  69. https://medium.com/p/f6f7d70acd41/share/facebook
