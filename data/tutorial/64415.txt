   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

[16]understanding feature engineering (part 2)

categorical data

strategies for working with discrete, categorical data

   go to the profile of dipanjan (dj) sarkar
   [17]dipanjan (dj) sarkar (button) blockedunblock (button)
   followfollowing
   jan 6, 2018
   [1*fgmehrpzkmgdc1rcrl8jnw.jpeg]
   source: [18]https://pixabay.com

introduction

   we covered various feature engineering strategies for dealing with
   structured continuous numeric data in the [19]previous article in this
   series. in this article, we will look at another type of structured
   data, which is discrete in nature and is popularly termed as
   categorical data. dealing with numeric data is often easier than
   categorical data given that we do not have to deal with additional
   complexities of the semantics pertaining to each category value in any
   data attribute which is of a categorical type. we will use a hands-on
   approach to discuss several encoding schemes for dealing with
   categorical data and also a couple of popular techniques for dealing
   with large scale feature explosion, often known as the [20]   curse of
   dimensionality   .

motivation

   i   m sure by now you must realize the motivation and the importance of
   feature engineering, we do stress on the same in detail in [21]   part 1   
   of this series. do check it out for a quick refresher if necessary. in
   short, machine learning algorithms cannot work directly with
   categorical data and you do need to do some amount of engineering and
   transformations on this data before you can start modeling on your
   data.

understanding categorical data

   let   s get an idea about categorical data representations before diving
   into feature engineering strategies. typically, any data attribute
   which is categorical in nature represents discrete values which belong
   to a specific finite set of categories or classes. these are also often
   known as classes or labels in the context of attributes or variables
   which are to be predicted by a model (popularly known as response
   variables). these discrete values can be text or numeric in nature (or
   even unstructured data like images!). there are two major classes of
   categorical data, nominal and ordinal.

   in any nominal categorical data attribute, there is no concept of
   ordering amongst the values of that attribute. consider a simple
   example of weather categories, as depicted in the following figure. we
   can see that we have six major classes or categories in this particular
   scenario without any concept or notion of order (windy doesn   t always
   occur before sunny nor is it smaller or bigger than sunny).
   [0*iksdex5fubqoytju.png]
   weather as a categorical attribute

   similarly movie, music and video game genres, country names, food and
   cuisine types are other examples of nominal categorical attributes.

   ordinal categorical attributes have some sense or notion of order
   amongst its values. for instance look at the following figure for shirt
   sizes. it is quite evident that order or in this case    size    matters
   when thinking about shirts (s is smaller than m which is smaller than l
   and so on).
   [1*ychlo4dae5cvd1uwuuvjzw.png]
   shirt size as an ordinal categorical attribute

   shoe sizes, education level and employment roles are some other
   examples of ordinal categorical attributes. having a decent idea about
   categorical data, let   s now look at some feature engineering
   strategies.

feature engineering on categorical data

   while a lot of advancements have been made in various machine learning
   frameworks to accept complex categorical data types like text labels.
   typically any standard workflow in feature engineering involves some
   form of transformation of these categorical values into numeric labels
   and then applying some encoding scheme on these values. we load up the
   necessary essentials before getting started.
import pandas as pd
import numpy as np

transforming nominal attributes

   nominal attributes consist of discrete categorical values with no
   notion or sense of order amongst them. the idea here is to transform
   these attributes into a more representative numerical format which can
   be easily understood by downstream code and pipelines. let   s look at a
   new dataset pertaining to video game sales. this dataset is also
   available on [22]kaggle as well as in my [23]github repository.
vg_df = pd.read_csv('datasets/vgsales.csv', encoding='utf-8')
vg_df[['name', 'platform', 'year', 'genre', 'publisher']].iloc[1:7]

   [1*xlsfdg01yzh1imphmhw3wg.png]
   dataset for video game sales

   let   s focus on the video game genre attribute as depicted in the above
   data frame. it is quite evident that this is a nominal categorical
   attribute just like publisher and platform. we can easily get the list
   of unique video game genres as follows.
genres = np.unique(vg_df['genre'])
genres
output
------
array(['action', 'adventure', 'fighting', 'misc', 'platform',
       'puzzle', 'racing', 'role-playing', 'shooter', 'simulation',
       'sports', 'strategy'], dtype=object)

   this tells us that we have 12 distinct video game genres. we can now
   generate a label encoding scheme for mapping each category to a numeric
   value by leveraging scikit-learn.
from sklearn.preprocessing import labelencoder
gle = labelencoder()
genre_labels = gle.fit_transform(vg_df['genre'])
genre_mappings = {index: label for index, label in
                  enumerate(gle.classes_)}
genre_mappings

output
------
{0: 'action', 1: 'adventure', 2: 'fighting', 3: 'misc',
 4: 'platform', 5: 'puzzle', 6: 'racing', 7: 'role-playing',
 8: 'shooter', 9: 'simulation', 10: 'sports', 11: 'strategy'}

   thus a mapping scheme has been generated where each genre value is
   mapped to a number with the help of the labelencoder object gle. the
   transformed labels are stored in the genre_labels value which we can
   write back to our data frame.
vg_df['genrelabel'] = genre_labels
vg_df[['name', 'platform', 'year', 'genre', 'genrelabel']].iloc[1:7]

   [1*_dhz4-lspvuzrdhuwbtbnq.png]
   video game genres with their encoded labels

   these labels can be used directly often especially with frameworks like
   scikit-learn if you plan to use them as response variables for
   prediction, however as discussed earlier, we will need an additional
   step of encoding on these before we can use them as features.

transforming ordinal attributes

   ordinal attributes are categorical attributes with a sense of order
   amongst the values. let   s consider our [24]pok  mon dataset which we
   used in [25]part 1 of this series. let   s focus more specifically on the
   generation attribute.
poke_df = pd.read_csv('datasets/pokemon.csv', encoding='utf-8')
poke_df = poke_df.sample(random_state=1,
                         frac=1).reset_index(drop=true)
np.unique(poke_df['generation'])
output
------
array(['gen 1', 'gen 2', 'gen 3', 'gen 4', 'gen 5', 'gen 6'],
         dtype=object)

   based on the above output, we can see there are a total of 6
   generations and each pok  mon typically belongs to a specific generation
   based on the video games (when they were released) and also the
   television series follows a similar timeline. this attribute is
   typically ordinal (domain knowledge is necessary here) because most
   pok  mon belonging to generation 1 were introduced earlier in the video
   games and the television shows than generation 2 as so on. fans can
   check out the following figure to remember some of the popular pok  mon
   of each generation (views may differ among fans!).
   [0*zqcdjmfdx-4uo3ec.png]
   popular pok  mon based on generation and type (source:
   [26]https://www.reddit.com/r/pokemon/comments/2s2upx/heres_my_favorite_
   pokemon_by_type_and_gen_chart)

   hence they have a sense of order amongst them. in general, there is no
   generic module or function to map and transform these features into
   numeric representations based on order automatically. hence we can use
   a custom encoding\mapping scheme.
gen_ord_map = {'gen 1': 1, 'gen 2': 2, 'gen 3': 3,
               'gen 4': 4, 'gen 5': 5, 'gen 6': 6}
poke_df['generationlabel'] = poke_df['generation'].map(gen_ord_map)
poke_df[['name', 'generation', 'generationlabel']].iloc[4:10]

   [1*ikqnf9wj11xdlit5-eq50q.png]
   pok  mon generation encoding

   it is quite evident from the above code that the map(   ) function from
   pandas is quite helpful in transforming this ordinal feature.

encoding categorical attributes

   if you remember what we mentioned earlier, typically feature
   engineering on categorical data involves a transformation process which
   we depicted in the previous section and a compulsory encoding process
   where we apply specific encoding schemes to create dummy variables or
   features for each category\value in a specific categorical attribute.

   you might be wondering, we just converted categories to numerical
   labels in the previous section, why on earth do we need this now? the
   reason is quite simple. considering video game genres, if we directly
   fed the genrelabel attribute as a feature in a machine learning model,
   it would consider it to be a continuous numeric feature thinking value
   10 (sports) is greater than 6 (racing) but that is meaningless because
   the sports genre is certainly not bigger or smaller than racing, these
   are essentially different values or categories which cannot be compared
   directly. hence we need an additional layer of encoding schemes where
   dummy features are created for each unique value or category out of all
   the distinct categories per attribute.

one-hot encoding scheme

   considering we have the numeric representation of any categorical
   attribute with m labels (after transformation), the one-hot encoding
   scheme, encodes or transforms the attribute into m binary features
   which can only contain a value of 1 or 0. each observation in the
   categorical feature is thus converted into a vector of size m with only
   one of the values as 1 (indicating it as active). let   s take a subset
   of our pok  mon dataset depicting two attributes of interest.
poke_df[['name', 'generation', 'legendary']].iloc[4:10]

   [1*keuxov3noszfdf8bmd4iyq.png]
   subset of our pok  mon dataset

   the attributes of interest are pok  mon generation and their legendary
   status. the first step is to transform these attributes into numeric
   representations based on what we learnt earlier.
from sklearn.preprocessing import onehotencoder, labelencoder
# transform and map pokemon generations
gen_le = labelencoder()
gen_labels = gen_le.fit_transform(poke_df['generation'])
poke_df['gen_label'] = gen_labels
# transform and map pokemon legendary status
leg_le = labelencoder()
leg_labels = leg_le.fit_transform(poke_df['legendary'])
poke_df['lgnd_label'] = leg_labels
poke_df_sub = poke_df[['name', 'generation', 'gen_label',
                       'legendary', 'lgnd_label']]
poke_df_sub.iloc[4:10]

   [1*vmzdgwj1fvzj-zofgn21ug.png]
   attributes with transformed (numeric) labels

   the features gen_label and lgnd_label now depict the numeric
   representations of our categorical features. let   s now apply the
   one-hot encoding scheme on these features.
# encode generation labels using one-hot encoding scheme
gen_ohe = onehotencoder()
gen_feature_arr = gen_ohe.fit_transform(
                              poke_df[['gen_label']]).toarray()
gen_feature_labels = list(gen_le.classes_)
gen_features = pd.dataframe(gen_feature_arr,
                            columns=gen_feature_labels)
# encode legendary status labels using one-hot encoding scheme
leg_ohe = onehotencoder()
leg_feature_arr = leg_ohe.fit_transform(
                                poke_df[['lgnd_label']]).toarray()
leg_feature_labels = ['legendary_'+str(cls_label)
                           for cls_label in leg_le.classes_]
leg_features = pd.dataframe(leg_feature_arr,
                            columns=leg_feature_labels)

   in general, you can always encode both the features together using the
   fit_transform(   ) function by passing it a two dimensional array of the
   two features together (check out the [27]documentation!). but we encode
   each feature separately, to make things easier to understand. besides
   this, we can also create separate data frames and label them
   accordingly. let   s now concatenate these feature frames and see the
   final result.
poke_df_ohe = pd.concat([poke_df_sub, gen_features, leg_features], axis=1)
columns = sum([['name', 'generation', 'gen_label'],
               gen_feature_labels, ['legendary', 'lgnd_label'],
               leg_feature_labels], [])
poke_df_ohe[columns].iloc[4:10]

   [1*wibsygby-ggrzzvnubvqfq.png]
   one-hot encoded features for pok  mon generation and legendary status

   thus you can see that 6 dummy variables or binary features have been
   created for generation and 2 for legendary since those are the total
   number of distinct categories in each of these attributes respectively.
   active state of a category is indicated by the 1 value in one of these
   dummy variables which is quite evident from the above data frame.

   consider you built this encoding scheme on your training data and built
   some model and now you have some new data which has to be engineered
   for features before predictions as follows.
new_poke_df = pd.dataframe([['pikazoom', 'gen 3', true],
                           ['charmytoast', 'gen 4', false]],
                       columns=['name', 'generation', 'legendary'])
new_poke_df

   [1*mkqrnwodbfd33tglmwm7lw.png]
   sample new data

   you can leverage scikit-learn   s excellent api here by calling the
   transform(   ) function of the previously build labelencoder and
   onehotencoder objects on the new data. remember our workflow, first we
   do the transformation.
new_gen_labels = gen_le.transform(new_poke_df['generation'])
new_poke_df['gen_label'] = new_gen_labels
new_leg_labels = leg_le.transform(new_poke_df['legendary'])
new_poke_df['lgnd_label'] = new_leg_labels
new_poke_df[['name', 'generation', 'gen_label', 'legendary',
             'lgnd_label']]

   [1*j-rli_vxx-ma7pexi3g5qq.png]
   categorical attributes after transformation

   once we have numerical labels, let   s apply the encoding scheme now!
new_gen_feature_arr = gen_ohe.transform(new_poke_df[['gen_label']]).toarray()
new_gen_features = pd.dataframe(new_gen_feature_arr,
                                columns=gen_feature_labels)
new_leg_feature_arr = leg_ohe.transform(new_poke_df[['lgnd_label']]).toarray()
new_leg_features = pd.dataframe(new_leg_feature_arr,
                                columns=leg_feature_labels)
new_poke_ohe = pd.concat([new_poke_df, new_gen_features, new_leg_features], axis
=1)
columns = sum([['name', 'generation', 'gen_label'],
               gen_feature_labels,
               ['legendary', 'lgnd_label'], leg_feature_labels], [])
new_poke_ohe[columns]

   [1*za1jyh-6ooffcclxdggpkg.png]
   categorical attributes after one-hot encoding

   thus you can see it   s quite easy to apply this scheme on new data
   easily by leveraging scikit-learn   s powerful api.

   you can also apply the one-hot encoding scheme easily by leveraging the
   to_dummies(   ) function from pandas.
gen_onehot_features = pd.get_dummies(poke_df['generation'])
pd.concat([poke_df[['name', 'generation']], gen_onehot_features],
           axis=1).iloc[4:10]

   [1*wrikttjex1udy4pzapszeg.png]
   one-hot encoded features by leveraging pandas

   the above data frame depicts the one-hot encoding scheme applied on the
   generation attribute and the results are same as compared to the
   earlier results as expected.

dummy coding scheme

   the dummy coding scheme is similar to the one-hot encoding scheme,
   except in the case of dummy coding scheme, when applied on a
   categorical feature with m distinct labels, we get m - 1 binary
   features. thus each value of the categorical variable gets converted
   into a vector of size m - 1. the extra feature is completely
   disregarded and thus if the category values range from {0, 1,    , m-1}
   the 0th or the m - 1th feature column is dropped and corresponding
   category values are usually represented by a vector of all zeros (0).
   let   s try applying dummy coding scheme on pok  mon generation by
   dropping the first level binary encoded feature (gen 1).
gen_dummy_features = pd.get_dummies(poke_df['generation'],
                                    drop_first=true)
pd.concat([poke_df[['name', 'generation']], gen_dummy_features],
          axis=1).iloc[4:10]

   [1*rrtvtvnqayti__cvrsqb8q.png]
   dummy coded features for pok  mon generation

   if you want, you can also choose to drop the last level binary encoded
   feature (gen 6) as follows.
gen_onehot_features = pd.get_dummies(poke_df['generation'])
gen_dummy_features = gen_onehot_features.iloc[:,:-1]
pd.concat([poke_df[['name', 'generation']], gen_dummy_features],
          axis=1).iloc[4:10]

   [1*pcnqikj-hrdn7fajo0sh1q.png]
   dummy coded features for pok  mon generation

   based on the above depictions, it is quite clear that categories
   belonging to the dropped feature are represented as a vector of zeros
   (0) like we discussed earlier.

effect coding scheme

   the effect coding scheme is actually very similar to the dummy coding
   scheme, except during the encoding process, the encoded features or
   feature vector, for the category values which represent all 0 in the
   dummy coding scheme, is replaced by -1 in the effect coding scheme.
   this will become clearer with the following example.
gen_onehot_features = pd.get_dummies(poke_df['generation'])
gen_effect_features = gen_onehot_features.iloc[:,:-1]
gen_effect_features.loc[np.all(gen_effect_features == 0,
                               axis=1)] = -1.
pd.concat([poke_df[['name', 'generation']], gen_effect_features],
          axis=1).iloc[4:10]

   [1*clxvy4hgiwzwl3zxdqohqg.png]
   effect coded features for pok  mon generation

   the above output clearly shows that the pok  mon belonging to generation
   6 are now represented by a vector of -1 values as compared to zeros in
   dummy coding.

bin-counting scheme

   the encoding schemes we discussed so far, work quite well on
   categorical data in general, but they start causing problems when the
   number of distinct categories in any feature becomes very large.
   essential for any categorical feature of m distinct labels, you get m
   separate features. this can easily increase the size of the feature set
   causing problems like storage issues, model training problems with
   regard to time, space and memory. besides this, we also have to deal
   with what is popularly known as the [28]   curse of dimensionality    where
   basically with an enormous number of features and not enough
   representative samples, model performance starts getting affected often
   leading to overfitting.
   [0*fwubnnonlt6coo9j.png]

   hence we need to look towards other categorical data feature
   engineering schemes for features having a large number of possible
   categories (like ip addresses). the bin-counting scheme is a useful
   scheme for dealing with categorical variables having many categories.
   in this scheme, instead of using the actual label values for encoding,
   we use id203 based statistical information about the value and
   the actual target or response value which we aim to predict in our
   modeling efforts. a simple example would be based on past historical
   data for ip addresses and the ones which were used in ddos attacks; we
   can build id203 values for a ddos attack being caused by any of
   the ip addresses. using this information, we can encode an input
   feature which depicts that if the same ip address comes in the future,
   what is the id203 value of a ddos attack being caused. this
   scheme needs historical data as a pre-requisite and is an elaborate
   one. depicting this with a complete example would be currently
   difficult here but there are several resources online which you can
   refer to for the same.

feature hashing scheme

   the feature hashing scheme is another useful feature engineering scheme
   for dealing with large scale categorical features. in this scheme, a
   hash function is typically used with the number of encoded features
   pre-set (as a vector of pre-defined length) such that the hashed values
   of the features are used as indices in this pre-defined vector and
   values are updated accordingly. since a hash function maps a large
   number of values into a small finite set of values, multiple different
   values might create the same hash which is termed as collisions.
   typically, a signed hash function is used so that the sign of the value
   obtained from the hash is used as the sign of the value which is stored
   in the final feature vector at the appropriate index. this should
   ensure lesser collisions and lesser accumulation of error due to
   collisions.

   hashing schemes work on strings, numbers and other structures like
   vectors. you can think of hashed outputs as a finite set of b bins such
   that when hash function is applied on the same values\categories, they
   get assigned to the same bin (or subset of bins) out of the b bins
   based on the hash value. we can pre-define the value of b which becomes
   the final size of the encoded feature vector for each categorical
   attribute that we encode using the feature hashing scheme.

   thus even if we have over 1000 distinct categories in a feature and we
   set b=10 as the final feature vector size, the output feature set will
   still have only 10 features as compared to 1000 binary features if we
   used a one-hot encoding scheme. let   s consider the genre attribute in
   our video game dataset.
unique_genres = np.unique(vg_df[['genre']])
print("total game genres:", len(unique_genres))
print(unique_genres)
output
------
total game genres: 12
['action' 'adventure' 'fighting' 'misc' 'platform' 'puzzle' 'racing'
 'role-playing' 'shooter' 'simulation' 'sports' 'strategy']

   we can see that there are a total of 12 genres of video games. if we
   used a one-hot encoding scheme on the genre feature, we would end up
   having 12 binary features. instead, we will now use a feature hashing
   scheme by leveraging scikit-learn   s featurehasher class, which uses a
   signed 32-bit version of the murmurhash3 hash function. we will
   pre-define the final feature vector size to be 6 in this case.
from sklearn.feature_extraction import featurehasher
fh = featurehasher(n_features=6, input_type='string')
hashed_features = fh.fit_transform(vg_df['genre'])
hashed_features = hashed_features.toarray()
pd.concat([vg_df[['name', 'genre']], pd.dataframe(hashed_features)],
          axis=1).iloc[1:7]

   [1*-ebaid925l7pnuljzx39yq.png]
   feature hashing on the genre attribute

   based on the above output, the genre categorical attribute has been
   encoded using the hashing scheme into 6 features instead of 12. we can
   also see that rows 1 and 6 denote the same genre of games, platform
   which have been rightly encoded into the same feature vector.

conclusion

   these examples should give you a good idea about popular strategies for
   feature engineering on discrete, categorical data. if you read [29]part
   1 of this series, you would have seen that it is slightly challenging
   to work with categorical data as compared to continuous, numeric data
   but definitely interesting! we also talked about some ways to handle
   large feature spaces using feature engineering but you should also
   remember that there are other techniques including [30]feature
   selection and [31]id84 methods to handle large
   feature spaces. we will cover some of these methods in a later article.

   next up will be feature engineering strategies for unstructured text
   data. stay tuned!
     __________________________________________________________________

   to read about feature engineering strategies for continuous numeric
   data, check out [32]part 1 of this series!

   all the code and datasets used in this article can be accessed from my
   [33]github

   the code is also available as a [34]jupyter notebook

   thanks to [35]ludovic benistant.
     * [36]machine learning
     * [37]data science
     * [38]feature engineering
     * [39]programming
     * [40]tds feature engineering

   (button)
   (button)
   (button) 2.1k claps
   (button) (button) (button) 14 (button) (button)

     (button) blockedunblock (button) followfollowing
   go to the profile of dipanjan (dj) sarkar

[41]dipanjan (dj) sarkar

   medium member since dec 2018

   data scientist [42]@redhat, author, consultant, mentor
   [43]@springboard, editor [44]@tdatascience. feel free to connect with
   me at [45]https://www.linkedin.com/in/dipanzan

     (button) follow
   [46]towards data science

[47]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 2.1k
     * (button)
     *
     *

   [48]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [49]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f54324193e63
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_068oq0h1ezm2---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/tagged/tds-feature-engineering
  17. https://towardsdatascience.com/@dipanzan.sarkar
  18. https://pixabay.com/
  19. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  20. https://en.wikipedia.org/wiki/curse_of_dimensionality
  21. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  22. https://www.kaggle.com/gregorut/videogamesales
  23. https://github.com/dipanjans/practical-machine-learning-with-python/tree/master/notebooks/ch04_feature_engineering_and_selection
  24. https://www.kaggle.com/abcsds/pokemon/data
  25. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  26. https://www.reddit.com/r/pokemon/comments/2s2upx/heres_my_favorite_pokemon_by_type_and_gen_chart
  27. http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.onehotencoder.html
  28. https://en.wikipedia.org/wiki/curse_of_dimensionality
  29. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  30. https://en.wikipedia.org/wiki/feature_selection
  31. https://en.wikipedia.org/wiki/dimensionality_reduction
  32. https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b
  33. https://github.com/dipanjans/practical-machine-learning-with-python/tree/master/notebooks/ch04_feature_engineering_and_selection
  34. https://github.com/dipanjans/practical-machine-learning-with-python/blob/master/notebooks/ch04_feature_engineering_and_selection/feature engineering on categorical data.ipynb
  35. https://medium.com/@ludobenistant?source=post_page
  36. https://towardsdatascience.com/tagged/machine-learning?source=post
  37. https://towardsdatascience.com/tagged/data-science?source=post
  38. https://towardsdatascience.com/tagged/feature-engineering?source=post
  39. https://towardsdatascience.com/tagged/programming?source=post
  40. https://towardsdatascience.com/tagged/tds-feature-engineering?source=post
  41. https://towardsdatascience.com/@dipanzan.sarkar
  42. http://twitter.com/redhat
  43. http://twitter.com/springboard
  44. http://twitter.com/tdatascience
  45. https://www.linkedin.com/in/dipanzan
  46. https://towardsdatascience.com/?source=footer_card
  47. https://towardsdatascience.com/?source=footer_card
  48. https://towardsdatascience.com/
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. https://towardsdatascience.com/@dipanzan.sarkar?source=post_header_lockup
  52. https://medium.com/p/f54324193e63/share/twitter
  53. https://medium.com/p/f54324193e63/share/facebook
  54. https://towardsdatascience.com/@dipanzan.sarkar?source=footer_card
  55. https://medium.com/p/f54324193e63/share/twitter
  56. https://medium.com/p/f54324193e63/share/facebook
