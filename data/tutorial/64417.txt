   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]the startup
     * [9]top story
     * [10]write for us
     * [11]get smarter at building startups
     __________________________________________________________________

deep learning for text made easy with allennlp

   [12]go to the profile of d  borah mesquita
   [13]d  borah mesquita (button) blockedunblock (button) followfollowing
   mar 19, 2018
   [1*zigfmm8rmmqb-qcjhde6ma.jpeg]

   one of the key principles of a good learning process is to keep
   learning things just a little above your current understanding. if the
   subject is too similar to what you already know you end up not making
   much progress. on the other hand, and if the subject is too hard you   ll
   stall and make little or no progress.

   deep learning involves a lot of different topics and things we need to
   learn, so a good strategy is to start working on something that people
   already built for us. that   s why frameworks are great. they enable us
   to not care much about the details of how to build a model so we can
   focus more on what we want to accomplish (instead of focusing on how to
   do it).

   [14]allennlp is a framework that makes the task of building deep
   learning models for natural language processing something really
   enjoyable. that was a surprise for me, because all my prior experiences
   with deep learning for nlp were some kind of painful.

   working with nlp tasks requires a different type of neural network
   cells, so before getting into how to use the allennlp framework let   s
   take a quick overview of the theory behind theses cells.
     __________________________________________________________________

     when simple neural networks are not enough

   in simple works the task of reading a text consists of building on
   things we read previously. for example, this sentence would probably
   make no sense if you didn   t read the sentence before. so the thinking
   behind the creation of these neural network cells was:

        if humans take what they read before in order to understand what
     comes next, maybe if we use this mechanism in our models they could
     understand the text better, right?   

     recurrent neural networks

   in order to use a network that takes time into account we need a way to
   represent time. but how do we do that?

     one obvious way of dealing with patterns that have a temporal extent
     is to represent time explicitly by associating the serial order of
     the pattern with the dimensionality of the pattern vector. the first
     temporal event is represented by the first element in the pattern
     vector, the second temporal event is represented by the second
     position in the pattern vector, and so on.         [15]jeffrey l. elman

   the thing is that there are several drawbacks to this approach. for
   example:

     [   ] the shift register imposes a rigid limit on the duration of
     patterns (since the input layer must provide for the longest
     possible pattern), and furthermore, suggests that all input vectors
     be the same length. these problems are particularly troublesome in
     domains such as language, where one would like comparable
     representations for patterns that are of variable length. this is as
     true of the basic units of speech (phonetic segments) as it is of
     sentences.

   [16]jeffrey l. elman talks about other drawbacks in the paper
   [17]finding structure in time. the paper introduces the elman network,
   a three-layer network with the addition of a set of    context unities   .

   if you   re totally new to neural networks maybe it   s a good idea to
   read[18] this other article i wrote. but to put simply, a neural
   network is something that has unities that are activated or not by the
   inputs.

   elman starts his work based on the approach suggested by jordan (1986).
   jordan introduces recurrent connections.

     the recurrent connections allow the network   s hidden units to see
     its own previous output, so that the subsequent behavior can be
     shaped by previous responses. these recurrent connections are what
     give the network memory.

   elman then adds context units. these context units work as a clock to
   say when we should    let go    the previous inputs. but how? the context
   units also have a mechanism to adjust the weights, like the other
   neural network units.

   both the context units and the inputs activate the neural network
   hidden units. when the neural network    learns    it means that it has a
   representation of the patterns of all inputs the network processed. the
   context units remember the previous internal state.

   if none of these make much sense, don   t worry. just think that now we
   have a neural network cell that takes the previous state into account
   to produce the next state.

        now we have a neural network cell that takes the previous state
     into account to produce the next state.   

     when id56s are not enough: lstm

   as christopher ola explains [19]here (this article is awesome if you
   want to learn more about lstms) sometimes we need more context, i.e.
   sometimes we need to store information that was seen long ago.

     consider trying to predict the last word in the text    i grew up in
     france    i speak fluent french.    recent information suggests that the
     next word is probably the name of a language, but if we want to
     narrow down which language, we need the context of france, from
     further back. it   s entirely possible for the gap between the
     relevant information and the point where it is needed to become very
     large         [20]christopher ola

   the lstm cells solve this problem. they are a special kind of id56,
   capable of learning long-term dependencies. we will just use the lstm
   cells, not build them, so for our purposes here you can just think of
   the lstm cells as cells that have a different architecture and that are
   capable of learning long-term dependencies.
     __________________________________________________________________

    building a fancy model for text classification

   ok, enough of theory, let   s go to the fun part and build the model.
   [1*ne7s40fqch7hti8lgj5gjq.png]
   the training process

   the picture above shows us how we   ll set everything. first we get the
   data, then we encode it to a format that the model will understand
   (   tokens    and    internal_text_encoder   ) and then we feed the network
   with this data, compare the labels and adjust the weights. at the end
   of this process the model is ready to make a prediction.

   and now we will finally feel the allennlp magic! we will specify
   everything in the picture above with a simple json file.
{
  "dataset_reader": {
    "type": "20newsgroups"
  },
  "train_data_path": "train",
  "test_data_path": "test",
  "evaluate_on_test": true,
  "model": {
    "type": "20newsgroups_classifier",
    "model_text_field_embedder": {
      "tokens": {
        "type": "embedding",
        "pretrained_file": "https://s3-us-west-2.amazonaws.com/allennlp/datasets
/glove/glove.6b.100d.txt.gz",
        "embedding_dim": 100,
        "trainable": false
      }
    },
    "internal_text_encoder": {
      "type": "lstm",
      "bidirectional": true,
      "input_size": 100,
      "hidden_size": 100,
      "num_layers": 1,
      "dropout": 0.2
    },
    "classifier_feedforward": {
      "input_dim": 200,
      "num_layers": 2,
      "hidden_dims": [200, 100],
      "activations": ["relu", "linear"],
      "dropout": [0.2, 0.0]
    }
  },
  "iterator": {
    "type": "bucket",
    "sorting_keys": [["text", "num_tokens"]],
    "batch_size": 64
  },
  "trainer": {
    "num_epochs": 40,
    "patience": 3,
    "cuda_device": 0,
    "grad_clipping": 5.0,
    "validation_metric": "+accuracy",
    "optimizer": {
      "type": "adagrad"
    }
  }
}

   let   s take a look on what   s going on there.

1    data inputs

   to tell allennlp the input dataset and how to read from it we set the
   'dataset_reader' key in the json file.

     a datasetreader reads data from some location and constructs a
     dataset. all parameters necessary to read the data apart from the
     filepath should be passed to the constructor of the
     datasetreader         [21]allennlp documentation

   our dataset will be the 20 newgroups and we will define how to read
   from it later (in a python class). first let   s define the rest of our
   model.

2         the model

   to specify the model we   ll set the 'model' key. there are three more
   parameters inside the 'model' key:

   'model_text_field_embedder', 'internal_text_encoder' and
   'classifier_feedforward'

   let   s deal with the first one and save the other two for latter.

   with the key 'model_text_field_embedder' we tell allennlp how the data
   should be encoded before passing it to the model. in simple words we
   want to make our data more    meaningful   . the idea behind it is this
   one: what if we could compare words like we are able to compare
   numbers?

   if 5 - 3 + 2 = 4 why not king - man + woman = queen?

   with id27s we can do that. this is also good for the model
   because now we don   t need to use a lot of sparse arrays (arrays with a
   lot of zeros) as inputs.

     id27 is the collective name for a set of id38
     and id171 techniques in natural language processing (nlp)
     where words or phrases from the vocabulary are mapped to vectors of
     real numbers. conceptually it involves a mathematical [22]embedding
     from a space with one dimension per word to a continuous [23]vector
     space with much lower dimension.         [24]wikipedia

   in our model we   ll use [25]glove: global vectors for word
   representation

     glove is an unsupervised learning algorithm for obtaining vector
     representations for words. training is performed on aggregated
     global word-word co-occurrence statistics from a corpus, and the
     resulting representations showcase interesting linear substructures
     of the word vector space.         [26]glove

   if any of this makes sense, just think of glove as a model where we
   pass the words and get them encoded into vectors. we   ll set the size of
   each embedding vector to 100.

     glove takes the words and encode them into vectors

   and that   s what the 'model_text_field_embedder' does.

3         the data iterator

   as usual we   ll separate the training data in batches. allennlp provides
   an iterator called [27]bucketiterator that makes the computations
   (padding) more efficient by padding batches with respect to the maximum
   input lengths per batch. to do that it sorts the instances by the
   number of tokens in each text. we set these parameters in the
   'iterator' key.

4         the trainer

   and the last step is to set the configuration for the training phase.
   the trainer uses the adagrad optimizer for 10 epochs, stopping if
   validation accuracy has not increased for the last 3 epochs.

   to train the model we just need to run:

   python run.py our_classifier.json -s /tmp/your_output_dir_here

   another cool thing is that with the framework we can stop and restore
   the training later. but before that we need to specify the
   dataset_reader and the model python classes.
     __________________________________________________________________

                writing the allennlp python classes

dataset_reader.py

   we   ll use the 20 newsgroups provided by [28]scikit-learn. to reference
   the datasetreader in the json file we need to register it:
@datasetreader.register("20newsgroups")
class newsgroupsdatasetreader(datasetreader):

   you   ll implement three methods: read() and text_to_instance().
     * read()

   the read() method gets data from scikit-learn. with allennlp you can
   set the path for the data files (the path for a json file for example),
   but in our case we   ll just import the data like a python module. we   ll
   read every text and every label from the dataset and wrap it with
   text_to_instance().
     * text_to_instance()

   this method    does whatever id121 or processing is necessary to
   go from textual input to an instance    [29](allennlp documentation).
   which in our case means doing this:
    @overrides
    def text_to_instance(self, newsgroups_post: str, label: str = none) -> insta
nce:
        tokenized_text = self._tokenizer.tokenize(newsgroups_post)
        post_field = textfield(tokenized_text, self._token_indexers)
        fields = {'post': post_field}
        if label is not none:
            fields['label'] = labelfield(int(label),skip_indexing=true)
        return instance(fields)

   we wrap the text from the 20 newsgroups and the label into
   [30]textfield and [31]labelfield.

model.py

   we will use a bi-directional lstm network, which is a cell where the
   first recurrent layer is duplicated. one layer receives the input as is
   and the other receives a reversed copy of the input sequence. thus the
   blstm network is designed to capture information of sequential dataset
   and maintain contextual features from past and future. (source:
   [32]bi-directional lstm recurrent neural network for chinese
   wordsegmentation)

   first let   s define the model class parameters
     * vocab

     because there are often several different mappings you want to have
     in your model, the vocabulary keeps track of separate namespaces. in
     this case, we have a "tokens" vocabulary for the text (that's not
     shown in the code, but is the default value used behind the scenes),
     and a "labels" vocabulary for the labels that we're trying to
     predict      [33]   using allennlp in your project

     * model_text_field_embedder

   used to embed the tokens textfield we get as input. returns glove
   vector representations for words.
     * internal_text_encoder

   the encoder we   ll use to convert the input text to a single vector
   (id56s, remember?). a seq2vecencoder is a module that takes as input a
   sequence of vectors and returns a single vector.
     * num_classes         the number of labels to predict

   now let's implement the model class methods
     * forward()

   [1*ux1es-qyujxnllel7crviq.png]
   what the forward() method does

     the first thing that the model does is embed the text, then encode
     it as a single vector. in order to encode the text we need to get
     masks representing which elements of the token sequences are merely
     there for padding.

     then we pass it through a feed-forward network to get class logits.
     we pass the logits through a softmax to get prediction
     probabilities. lastly, if we were given a label, we can compute a
     loss and evaluate our metrics.         [34]using allennlp in your project

   the forward method basically does the model training task. if you want
   to understand a little bit more of what   s going on you can go [35]here.
   now let   s talk about an important piece of the model class parameters:
   classifier_feedforward.

     we need model.forward to compute its own loss. the training code
     will look for the loss value in the dictionary returned by forward,
     and compute the gradients of that loss to update the model's
     parameters.         [36]using allennlp in your project

     * decode()

     decode has two functions: it takes the output of forward and does
     any necessary id136 or decoding on it, and it converts integers
     into strings to make things human-readable (e.g., for the
     demo).         [37]using allennlp in your
     __________________________________________________________________

     running the code

   as i said before to train the model though the command line we can use
   this:

   python run.py our_classifier.json -s /tmp/your_output_dir_here

   i also created a notebook so we can run the experiment on google
   colaboratory and use the gpu for free, here is the link:
   [38]https://colab.research.google.com/drive/1q3b5hakcjysvd6yhrwnxl2byqg
   k08jhq

   you can also check the code on [39]this repository.

   we built a simple classification model but the possibilities are
   endless. the framework is a great tool to create fancy deep learning
   models to our products.    
     __________________________________________________________________

   [40][1*yqdjlkfwscoqyq62dwedig.png]

this story is published in [41]the startup, medium   s largest entrepreneurship
publication followed by 307,871+ people.

subscribe to receive [42]our top stories here.

   [43][1*ouk9xr4xunwtces-tiunaw.png]

     * [44]machine learning
     * [45]deep learning
     * [46]neural networks

   (button)
   (button)
   (button) 331 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [47]go to the profile of d  borah mesquita

[48]d  borah mesquita

   award-winning data scientist                 loves to write and explain things in
   different ways    - [49]http://deborahmesquita.com/

     (button) follow
   [50]the startup

[51]the startup

   medium's largest publication for makers. subscribe to receive our top
   stories here     [52]https://goo.gl/zhclji

     * (button)
       (button) 331
     * (button)
     *
     *

   [53]the startup
   never miss a story from the startup, when you sign up for medium.
   [54]learn more
   never miss a story from the startup
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/62bc79d41f31
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/swlh/deep-learning-for-text-made-easy-with-allennlp-62bc79d41f31&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/swlh/deep-learning-for-text-made-easy-with-allennlp-62bc79d41f31&source=--------------------------nav_reg&operation=register
   8. https://medium.com/swlh?source=logo-lo_s19ldemxbbhr---f5af2b715248
   9. https://medium.com/swlh/how-we-got-11-3-million-pageviews-without-the-growth-hacking-bullshit-5e0456dcbe3
  10. https://medium.com/swlh/step-1-please-fill-out-the-form-below-c354ca4bad31
  11. https://growthsupply.com/the-startup-newsletter/?utm_source=tn&utm_medium=medium
  12. https://medium.com/@dehhmesquita?source=post_header_lockup
  13. https://medium.com/@dehhmesquita
  14. http://allennlp.org/
  15. http://psych.colorado.edu/~kimlab/elman1990.pdf
  16. https://en.wikipedia.org/wiki/jeffrey_elman
  17. http://psych.colorado.edu/~kimlab/elman1990.pdf
  18. https://medium.freecodecamp.org/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274
  19. http://colah.github.io/posts/2015-08-understanding-lstms/
  20. http://colah.github.io/posts/2015-08-understanding-lstms/
  21. https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html?highlight=dataset_reader#module-allennlp.data.dataset_readers.dataset_reader
  22. https://en.wikipedia.org/wiki/embedding
  23. https://en.wikipedia.org/wiki/vector_space
  24. https://en.wikipedia.org/wiki/word_embedding
  25. https://nlp.stanford.edu/projects/glove/
  26. https://nlp.stanford.edu/projects/glove/
  27. https://allenai.github.io/allennlp-docs/api/allennlp.data.iterators.html#bucket-iterator
  28. http://scikit-learn.org/stable/datasets/twenty_newsgroups.html
  29. https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.dataset_reader.html?highlight=datasetreader
  30. https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html?highlight=textfield#text-field
  31. https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html?highlight=textfield#allennlp.data.fields.label_field.labelfield
  32. https://arxiv.org/pdf/1602.04874.pdf
  33. https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md
  34. https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md
  35. https://medium.freecodecamp.org/big-picture-machine-learning-classifying-text-with-neural-networks-and-tensorflow-d94036ac2274#20cc
  36. https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md
  37. https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/using_in_your_repo.md
  38. https://colab.research.google.com/drive/1q3b5hakcjysvd6yhrwnxl2byqgk08jhq
  39. https://github.com/dmesquita/easy-deep-learning-with-allennlp
  40. https://medium.com/swlh
  41. https://medium.com/swlh
  42. http://growthsupply.com/the-startup-newsletter/
  43. https://medium.com/swlh
  44. https://medium.com/tag/machine-learning?source=post
  45. https://medium.com/tag/deep-learning?source=post
  46. https://medium.com/tag/neural-networks?source=post
  47. https://medium.com/@dehhmesquita?source=footer_card
  48. https://medium.com/@dehhmesquita
  49. http://deborahmesquita.com/
  50. https://medium.com/swlh?source=footer_card
  51. https://medium.com/swlh?source=footer_card
  52. https://goo.gl/zhclji
  53. https://medium.com/swlh
  54. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  56. https://medium.com/p/62bc79d41f31/share/twitter
  57. https://medium.com/p/62bc79d41f31/share/facebook
  58. https://medium.com/p/62bc79d41f31/share/twitter
  59. https://medium.com/p/62bc79d41f31/share/facebook
