   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

a history of machine translation from the cold war to deep learning

   [11]go to the profile of ilya pestov
   [12]ilya pestov (button) blockedunblock (button) followfollowing
   mar 12, 2018
   [1*ba4rvf1phr3cq5vkvymxow.jpeg]
   photo by [13]ant rozetsky on [14]unsplash

   i open google translate twice as often as facebook, and the instant
   translation of the price tags is not a cyberpunk for me anymore. that   s
   what we call reality. it   s hard to imagine that this is the result of a
   centennial fight to build the algorithms of machine translation and
   that there has been no visible success during half of that period.

   the precise developments i   ll discuss in this article set the basis of
   all modern language processing systems         from search engines to
   voice-controlled microwaves. i   m talking about the evolution and
   structure of online translation today.
   [1*svp9qnt2zekxzfkiiniwba.png]
   the translating machine of p. p. troyanskii (illustration made from
   descriptions. no photos left, unfortunately.)

in the beginning

   the story begins in 1933. soviet scientist peter troyanskii presented
      the machine for the selection and printing of words when translating
   from one language to another    to the academy of sciences of the ussr.
   the invention was super simple         it had cards in four different
   languages, a typewriter, and an old-school film camera.

   the operator took the first word from the text, found a corresponding
   card, took a photo, and typed its morphological characteristics (noun,
   plural, genitive) on the typewriter. the typewriter   s keys encoded one
   of the features. the tape and the camera   s film were used
   simultaneously, making a set of frames with words and their morphology.
   [1*blaqduih7skxbp4u_cjwca.png]

   despite all this, as often happened in the ussr, the invention was
   considered    useless   . troyanskii died of stenocardia after trying to
   finish his invention for 20 years. no one in the world knew about the
   machine until two soviet scientists found his patents in 1956.

   it was at the beginning of the cold war. on january 7th 1954, at ibm
   headquarters in new york, the [15]georgetown   ibm experiment started.
   the ibm 701 computer automatically translated 60 russian sentences into
   english for the first time in history.

        a girl who didn   t understand a word of the language of the soviets
     punched out the russian messages on ibm cards. the    brain    dashed
     off its english translations on an automatic printer at the
     breakneck speed of two and a half lines per second,            reported the
     ibm press release.

   [1*djrnvaxxrunxqh-tglpm6w.jpeg]
   ibm 701

   however, the triumphant headlines hid one little detail. no one
   mentioned the translated examples were carefully selected and tested to
   exclude any ambiguity. for everyday use, that system was no better than
   a pocket phrasebook. nevertheless, this sort of arms race launched:
   canada, germany, france, and especially japan, all joined the race for
   machine translation.

the race for machine translation

   the vain struggles to improve machine translation lasted for forty
   years. in 1966, the us alpac committee, in its famous report, called
   machine translation expensive, inaccurate, and unpromising. they
   instead recommended focusing on dictionary development, which
   eliminated us researchers from the race for almost a decade.

   even so, a basis for modern natural language processing was created
   only by the scientists and their attempts, research, and developments.
   all of today   s search engines, spam filters, and personal assistants
   appeared thanks to a bunch of countries spying on each other.
   [1*d-if6wcvycwfdlkghpjvkw.png]

rule-based machine translation (rbmt)

   the first ideas surrounding rule-based machine translation appeared in
   the 70s. the scientists peered over the interpreters    work, trying to
   compel the tremendously sluggish computers to repeat those actions.
   these systems consisted of:
     * bilingual dictionary (ru -> en)
     * a set of linguistic rules for each language (for example, nouns
       ending in certain suffixes such as -heit, -keit, -ung are feminine)

   that   s it. if needed, systems could be supplemented with hacks, such as
   lists of names, spelling correctors, and transliterators.
   [1*_xwoe70tzyystf4p8dwbjg.png]

   prompt and systran are the most famous examples of rbmt systems. just
   take a look at the aliexpress to feel the soft breath of this golden
   age.

   but even they had some nuances and subspecies.

direct machine translation

   this is the most straightforward type of machine translation. it
   divides the text into words, translates them, slightly corrects the
   morphology, and harmonizes syntax to make the whole thing sound right,
   more or less. when the sun goes down, trained linguists write the rules
   for each word.

   the output returns some kind of translation. usually, it   s quite
   crappy. it seems that the linguists wasted their time for nothing.

   modern systems do not use this approach at all, and modern linguists
   are grateful.
   [1*5ma5py_ycxd9n9gw1qyxhw.png]

transfer-based machine translation

   in contrast to direct translation, we prepare first by determining the
   grammatical structure of the sentence, as we were taught at school.
   then we manipulate whole constructions, not words, afterwards. this
   helps to get quite decent conversion of the word order in translation.
   in theory.

   in practice, it still resulted in verbatim translation and exhausted
   linguists. on the one hand, it brought simplified general grammar
   rules. but on the other, it became more complicated because of the
   increased number of word constructions in comparison with single words.
   [0*bdun2vdxvzquwbxi.png]

interlingual machine translation

   in this method, the source text is transformed to the intermediate
   representation, and is unified for all the world   s languages
   (interlingua). it   s the same interlingua descartes dreamed of: a
   meta-language, which follows the universal rules and transforms the
   translation into a simple    back and forth    task. next, interlingua
   would convert to any target language, and here was the singularity!

   because of the conversion, interlingua is often confused with
   transfer-based systems. the difference is the linguistic rules specific
   to every single language and interlingua, and not the language pairs.
   this means, we can add a third language to the interlingua system and
   translate between all three. we can   t do this in transfer-based
   systems.
   [0*4tmna3bnqugt1d3o.png]

   it looks perfect, but in real life it   s not. it was extremely hard to
   create such universal interlingua         a lot of scientists have worked on
   it their whole lives. they   ve not succeeded, but thanks to them we now
   have morphological, syntactic, and even semantic levels of
   representation. but the only [16]meaning-text theory costs a fortune!

   the idea of intermediate language will be back. let   s wait awhile.
   [0*xeultdzzjf9ajrh9.png]

   as you can see, all rbmt are dumb and terrifying, and that   s the reason
   they are rarely used unless for specific cases (like the weather report
   translation, and so on). among the advantages of rbmt, often mentioned
   are its morphological accuracy (it doesn   t confuse the words),
   reproducibility of results (all translators get the same result), and
   the ability to tune it to the subject area (to teach economists or
   terms specific to programmers, for example).

   even if anyone were to succeed in creating an ideal rbmt, and linguists
   enhanced it with all the spelling rules, there would always be some
   exceptions: all the irregular verbs in english, separable prefixes in
   german, suffixes in russian, and situations when people just say it
   differently. any attempt to take into account all the nuances would
   waste millions of man hours.

   and don   t forget about homonyms. the same word can have a different
   meaning in a different context, which leads to a variety of
   translations. how many meanings can you catch here: i saw a man on a
   hill with a telescope?

   languages did not develop based on a fixed set of rules         a fact which
   linguists love. they were much more influenced by the history of
   invasions in past three hundred years. how could you explain that to a
   machine?

   forty years of the cold war didn   t help in finding any distinct
   solution. rbmt was dead.

example-based machine translation (ebmt)

   japan was especially interested in fighting for machine translation.
   there was no cold war, but there were reasons: very few people in the
   country knew english. it promised to be quite an issue at the upcoming
   globalization party. so the japanese were extremely motivated to find a
   working method of machine translation.

   rule-based english-japanese translation is extremely complicated. the
   language structure is completely different, and almost all words have
   to be rearranged and new ones added. in 1984, makoto nagao from kyoto
   university came up with the idea of using ready-made phrases instead of
   repeated translation.

   let   s imagine that we have to translate a simple sentence            i   m going
   to the cinema.    and let   s say we   ve already translated another similar
   sentence            i   m going to the theater            and we can find the word
      cinema    in the dictionary.

   all we need is to figure out the difference between the two sentences,
   translate the missing word, and then not screw it up. the more examples
   we have, the better the translation.

   i build phrases in unfamiliar languages exactly the same way!
   [0*gc2lyl8ig_tbrqjp.png]

   ebmt showed the light of day to scientists from all over the world: it
   turns out, you can just feed the machine with existing translations and
   not spend years forming rules and exceptions. not a revolution yet, but
   clearly the first step towards it. the revolutionary invention of
   statistical translation would happen in just five years.

id151 (smt)

   in early 1990, at the ibm research center, a machine translation system
   was first shown which knew nothing about rules and linguistics as a
   whole. it analyzed similar texts in two languages and tried to
   understand the patterns.
   [0*fvorkj59wnwimggl.png]

   the idea was simple yet beautiful. an identical sentence in two
   languages split into words, which were matched afterwards. this
   operation repeated about 500 million times to count, for example, how
   many times the word    das haus    translated as    house    vs    building    vs
      construction   , and so on.

   if most of the time the source word was translated as    house   , the
   machine used this. note that we did not set any rules nor use any
   dictionaries         all conclusions were done by machine, guided by stats
   and the logic that    if people translate that way, so will i.    and so
   statistical translation was born.
   [0*2ycwdts_fru1fiwu.png]

   the method was much more efficient and accurate than all the previous
   ones. and no linguists were needed. the more texts we used, the better
   translation we got.
   [0*kh8ntmuiylngau9w.png]
   google   s statistical translation from the inside. it shows not only the
   probabilities but also counts the reverse statistics.

   there was still one question left: how would the machine correlate the
   word    das haus,    and the word    building            and how would we know these
   were the right translations?

   the answer was that we wouldn   t know. at the start, the machine assumed
   that the word    das haus    equally correlated with any word from the
   translated sentence. next, when    das haus    appeared in other sentences,
   the number of correlations with the    house    would increase. that   s the
      word alignment algorithm,    a typical task for university-level machine
   learning.

   the machine needed millions and millions of sentences in two languages
   to collect the relevant statistics for each word. how did we get them?
   well, we decided to take the abstracts of the european parliament and
   the united nations security council meetings         they were available in
   the languages of all member countries and were now available for
   download at [17]un corpora and [18]europarl corpora.

word-based smt

   in the beginning, the first statistical translation systems worked by
   splitting the sentence into words, since this approach was
   straightforward and logical. ibm   s first statistical translation model
   was called model one. quite elegant, right? guess what they called the
   second one?

   model 1:    the bag of words   
   [0*wx7m2xjzwlkg8kmc.png]

   model one used a classical approach         to split into words and count
   stats. the word order wasn   t taken into account. the only trick was
   translating one word into multiple words. for example,    der
   staubsauger    could turn into    vacuum cleaner,    but that didn   t mean it
   would turn out vice versa.

   here   re some simple implementations in python: [19]shawa/ibm-model-1.

   model 2: considering the word order in sentences
   [0*x-xpacu2oiwk1ib3.png]

   the lack of knowledge about languages    word order became a problem for
   model 1, and it   s very important in some cases.

   model 2 dealt with that: it memorized the usual place the word takes at
   the output sentence and shuffled the words for the more natural sound
   at the intermediate step. things got better, but they were still kind
   of crappy.

   model 3: extra fertility
   [0*csptjuq-hkkpveyg.png]

   new words appeared in the translation quite often, such as articles in
   german or using    do    when negating in english.    ich will keine
   persimonen           i do not want persimmons.    to deal with it, two more
   steps were added to model 3.
     * the null token insertion, if the machine considers the necessity of
       a new word
     * choosing the right grammatical particle or word for each token-word
       alignment

   model 4: word alignment

   model 2 considered the word alignment, but knew nothing about the
   reordering. for example, adjectives would often switch places with the
   noun, and no matter how good the order was memorized, it wouldn   t make
   the output better. therefore, model 4 took into account the so-called
      relative order            the model learned if two words always switched
   places.

   model 5: bugfixes

   nothing new here. model 5 got some more parameters for the learning and
   fixed the issue with conflicting word positions.

   despite their revolutionary nature, word-based systems still failed to
   deal with cases, gender, and homonymy. every single word was translated
   in a single-true way, according to the machine. such systems are not
   used anymore, as they   ve been replaced by the more advanced
   phrase-based methods.

phrase-based smt

   this method is based on all the word-based translation principles:
   statistics, reordering, and lexical hacks. although, for the learning,
   it split the text not only into words but also phrases. these were the
   id165s, to be precise, which were a contiguous sequence of n words in
   a row.

   thus, the machine learned to translate steady combinations of words,
   which noticeably improved accuracy.
   [0*sk18cdwmzm8oyv4r.png]

   the trick was, the phrases were not always simple syntax constructions,
   and the quality of the translation dropped significantly if anyone who
   was aware of linguistics and the sentences    structure interfered.
   frederick jelinek, the pioneer of the computer linguistics, joked about
   it once:    every time i fire a linguist, the performance of the speech
   recognizer goes up.   

   besides improving accuracy, the phrase-based translation provided more
   options in choosing the bilingual texts for learning. for the
   word-based translation, the exact match of the sources was critical,
   which excluded any literary or free translation. the phrase-based
   translation had no problem learning from them. to improve the
   translation, researchers even started to parse the news websites in
   different languages for that purpose.
   [0*wldbsrqs9s1kk630.png]

   starting in 2006, everyone began to use this approach. google
   translate, yandex, bing, and other high-profile online translators
   worked as phrase-based right up until 2016. each of you can probably
   recall the moments when google either translated the sentence
   flawlessly or resulted in complete nonsense, right? the nonsense came
   from phrase-based features.

   the good old rule-based approach consistently provided a predictable
   though terrible result. the statistical methods were surprising and
   puzzling. google translate turns    three hundred    into    300    without any
   hesitation. that   s called a statistical anomaly.

   phrase-based translation has become so popular, that when you hear
      id151    that is what is actually meant. up
   until 2016, all studies [20]lauded phrase-based translation as the
   state-of-the-art. back then, no one even thought that google was
   already stoking its fires, getting ready to change our whole image of
   machine translation.

syntax-based smt

   this method should also be mentioned, briefly. many years before the
   emergence of neural networks, syntax-based translation was considered
      the future or translation,    but the idea did not take off.

   the proponents of syntax-based translation believed it was possible to
   merge it with the rule-based method. it   s necessary to do quite a
   precise syntax analysis of the sentence         to determine the subject, the
   predicate, and other parts of the sentence, and then to build a
   sentence tree. using it, the machine learns to convert syntactic units
   between languages and translates the rest by words or phrases. that
   would have solved the word alignment issue once and for all.
   [0*m65befobrhhm6ioz.png]
   example taken from the [21]yamada and knight [2001] and this [22]great
   slide show.

   the problem is, the syntactic parsing works terribly, despite the fact
   that we consider it solved a while ago (as we have the ready-made
   libraries for many languages). i tried to use syntactic trees for tasks
   a bit more complicated than to parse the subject and the predicate. and
   every single time i gave up and used another method.

   let me know in the comments if you succeed using it at least once.

id4 (id4)

   a quite amusing [23]paper on using neural networks in machine
   translation was published in 2014. the internet didn   t notice it at
   all, except google         they took out their shovels and started to dig.
   two years later, in november 2016, google made a game-changing
   [24]announcement.

   the idea was close to transferring the style between photos. remember
   apps like [25]prisma, which enhanced pictures in some famous artist   s
   style? there was no magic. the neural network [26]was taught to
   recognize the artist   s paintings. next, the last layers containing the
   network   s decision were removed. the resulting stylized picture was
   just the intermediate image that network got. that   s the network   s
   fantasy, and we consider it beautiful.
   [0*sj5jkim-jzqtczmc.jpg]

   if we can transfer the style to the photo, what if we try to impose
   another language to a source text? the text would be that precise
      artist   s style,    and we would try to transfer it while keeping the
   essence of the image (in other words, the essence of the text).

   imagine i   m trying to describe my dog         average size, sharp nose, short
   tail, always barks. if i gave you this set of the dog   s features, and
   if the description was precise, you could draw it, even though you have
   never seen it.
   [0*nbri8zzksuoyyl0d.png]

   now, imagine the source text is the set of specific features.
   basically, it means that you encode it, and let the other neural
   network decode it back to the text, but, in another language. the
   decoder only knows its language. it has no idea about of the features   
   origin, but it can express them in, for example, spanish. continuing
   the analogy, it doesn   t matter how you draw the dog         with crayons,
   watercolor or your finger. you paint it as you can.

   once again         one neural network can only encode the sentence to the
   specific set of features, and another one can only decode them back to
   the text. both have no idea about the each other, and each of them
   knows only its own language. recall something? interlingua is back.
   ta-da.
   [0*ik-sdu3fqhnv6y5e.png]

   the question is, how do we find those features? it   s obvious when we   re
   talking about the dog, but how to deal with the text? thirty years ago
   scientists already tried to create the universal language code, and it
   ended in a total failure.

   nevertheless, we have deep learning now. and that   s its essential task!
   the primary distinction between the deep learning and classic neural
   networks lays precisely in the ability to search for those specific
   features, without any idea of their nature. if the neural network is
   big enough, and there are a couple of thousand video cards at hand,
   it   s possible to find those features in the text as well.

   theoretically, we can pass the features gotten from the neural networks
   to the linguists, so that they can open brave new horizons for
   themselves.

   the question is, what type of neural network should be used for
   encoding and decoding? convolutional neural networks (id98) fit
   perfectly for pictures since they operate with independent blocks of
   pixels.

   but there are no independent blocks in the text         every word depends on
   its surroundings. text, speech, and music are always consistent. so
   recurrent neural networks (id56) would be the best choice to handle
   them, since they remember the previous result         the prior word, in our
   case.

   now id56s are used everywhere         siri   s id103 (it   s parsing
   the sequence of sounds, where the next depends on the previous),
   keyboard   s tips (memorize the prior, guess the next), music generation,
   and even chatbots.
   [0*uoawqr_t-7hds9if.png]

     for the nerds like me: in fact, the neural translators    architecture
     varies widely. the regular id56 was used at the beginning, then
     upgraded to bi-directional, where the translator considered not only
     words before the source word, but also the next word. that was much
     more effective. then it followed with the hardcore multilayer id56
     with lstm-units for long-term storing of the translation context.

   in two years, neural networks surpassed everything that had appeared in
   the past 20 years of translation. neural translation contains 50% fewer
   word order mistakes, 17% fewer lexical mistakes, and 19% fewer grammar
   mistakes. the neural networks even learned to harmonize gender and case
   in different languages. and no one taught them to do so.

   the most noticeable improvements occurred in fields where direct
   translation was never used. id151 methods
   always worked using english as the key source. thus, if you translated
   from russian to german, the machine first translated the text to
   english and then from english to german, which leads to a double loss.

   neural translation doesn   t need that         only a decoder is required so it
   can work. that was the first time that direct translation between
   languages with no   ommon dictionary became possible.
   [0*ta7lurpkkgmvrkur.jpg]

google translate (since 2016)

   in 2016, google turned on neural translation for [27]nine languages.
   they developed their system named google id4
   (gid4). it consists of 8 encoder and 8 decoder layers of id56s, as well
   as attention connections from the decoder network.
   [0*j6xn1ygmi6jamagy.png]

   they not only divided sentences, but also words. that was how they
   dealt with one of the major id4 issues         rare words. id4s are helpless
   when the word is not in their lexicon. let   s say,    vas3k   . i doubt
   anyone taught the neural network to translate my nickname. in that
   case, gmnt tries to break words into word pieces and recover the
   translation of them. smart.

     hint: google translate used for website translation in the browser
     still uses the old phrase-based algorithm. somehow, google hasn   t
     upgraded it, and the differences are quite noticeable compared to
     the online version.

   google uses a id104 mechanism in the online version. people can
   choose the version they consider the most correct, and if lots of users
   like it, google will always translate this phrase that way and mark it
   with a special badge. this works fantastically for short everyday
   phrases such as,    let   s go to the cinema,    or,    i   m waiting for you.   
   google knows conversational english better than i do :(

   microsoft   s bing works exactly like google translate. but yandex is
   different.

yandex translate (since 2017)

   yandex launched its neural translation system in 2017. its main
   feature, as declared, was hybridity. yandex combines neural and
   statistical approaches to translate the sentence, and then it choose
   the best one with its favorite catboost algorithm.

   the thing is, neural translation often fails when translating short
   phrases, since it uses context to choose the right word. it would be
   hard if the word appeared very few times in a training data. in such
   cases, a simple statistical translation finds the right word quickly
   and simply.
   [0*jah8ebhc6sk9nivu.png]

   yandex doesn   t share the details. it fends us off with marketing
   [28]press-releases. okay.

     it looks like google uses smt for the translation of words and short
     phrases. they don   t mention that in any articles, but it   s quite
     noticeable if you look at the difference between the translation of
     short and long expressions. besides, smt is used for displaying the
     word   s stats.

the conclusion and the future

   everyone   s still excited about the idea of    babel fish            instant
   speech translation. google has made steps towards it with its pixel
   buds, but in fact, it   s still not what we were dreaming of. the instant
   speech translation is different from the usual translation. you need to
   know when to start translating and when to shut up and listen. i
   haven   t seen suitable approaches to solve this yet. unless, maybe,
   skype   

   and here   s one more empty area: all the learning is limited to the set
   of parallel text blocks. the deepest neural networks still learn at
   parallel texts. we can   t teach the neural network without providing it
   with a source. people, instead, can complement their lexicon with
   reading books or articles, even if not translating them to their native
   language.

   if people can do it, the neural network can do it too, in theory. i
   found [29]only one prototype attempting to incite the network, which
   knows one language, to read the texts in another language in order to
   gain experience. i   d try it myself, but i   m silly. ok, that   s it.

     this story originally was written in russian and then translated
     into english on [30]vas3k.com by vasily zubarev. he is my pen-friend
     and i   m pretty sure that his blog should be spread.

useful links

     * [31]philipp koehn: id151. most complete
       collection of the methods i   ve found.
     * [32]moses         popular library for creating own statistical
       translations
     * [33]openid4         one more library, but for the neural translators
     * the article from one of my favorite bloggers [34]explaining id56 and
       lstm
     * a video [35]   how to make a language translator   , funny guy, neat
       explanation. still not enough.
     * text guide from tensorflow about [36]creation of your own neural
       translator, for those who want more examples and to try the code.
     __________________________________________________________________

others articles from vas3k.com

   [37]how ethereum and smart contracts work
   distributed turing machine with block  hain protectionvas3k.com
   [38]blockchain inside out: how bitcoin works
   once and for all in simple wordsvas3k.com

one last thing   

   if you liked this article, click the     below, and share it with other
   people so they can enjoy it as well.

     * [39]machine learning
     * [40]tech
     * [41]technology
     * [42]programming
     * [43]artificial intelligence

   (button)
   (button)
   (button) 2k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [44]go to the profile of ilya pestov

[45]ilya pestov

   startup hunter, analyst, bot evangelist. ex-cmo at statsbot.

     (button) follow
   [46]freecodecamp.org

[47]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 2k
     * (button)
     *
     *

   [48]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [49]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/f1d335ce8b5
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/a-history-of-machine-translation-from-the-cold-war-to-deep-learning-f1d335ce8b5&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_7l4ywvtw85np---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@ipestov?source=post_header_lockup
  12. https://medium.freecodecamp.org/@ipestov
  13. https://unsplash.com/photos/h9m6mfeeaku?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  14. https://unsplash.com/search/photos/russia?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  15. https://en.wikipedia.org/wiki/georgetown   ibm_experiment
  16. https://en.wikipedia.org/wiki/meaning-text_theory
  17. https://catalog.ldc.upenn.edu/ldc2013t06
  18. http://www.statmt.org/europarl/
  19. https://github.com/shawa/ibm-model-1
  20. http://www.aclweb.org/anthology/d16-1161
  21. http://www.aclweb.org/anthology/p01-1067
  22. http://homepages.inf.ed.ac.uk/pkoehn/publications/esslli-slides-day5.pdf
  23. https://arxiv.org/abs/1406.1078
  24. https://research.googleblog.com/2016/09/a-neural-network-for-machine.html
  25. https://prisma-ai.com/
  26. https://harishnarayanan.org/writing/artistic-style-transfer/
  27. https://en.wikipedia.org/wiki/google_neural_machine_translation
  28. https://yandex.com/company/blog/one-model-is-better-than-two-yu-yandex-translate-launches-a-hybrid-machine-translation-system/
  29. https://arxiv.org/abs/1710.04087
  30. http://vas3k.com/blog/machine_translation/
  31. https://www.amazon.com/dp/0521874157/
  32. http://www.statmt.org/moses/
  33. http://openid4.net/
  34. http://colah.github.io/posts/2015-08-understanding-lstms/
  35. https://www.youtube.com/watch?v=nrbnh4qbphi
  36. https://www.tensorflow.org/tutorials/id195
  37. http://vas3k.com/blog/ethereum/
  38. http://vas3k.com/blog/blockchain/
  39. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  40. https://medium.freecodecamp.org/tagged/tech?source=post
  41. https://medium.freecodecamp.org/tagged/technology?source=post
  42. https://medium.freecodecamp.org/tagged/programming?source=post
  43. https://medium.freecodecamp.org/tagged/artificial-intelligence?source=post
  44. https://medium.freecodecamp.org/@ipestov?source=footer_card
  45. https://medium.freecodecamp.org/@ipestov
  46. https://medium.freecodecamp.org/?source=footer_card
  47. https://medium.freecodecamp.org/?source=footer_card
  48. https://medium.freecodecamp.org/
  49. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  51. http://vas3k.com/blog/ethereum/
  52. http://vas3k.com/blog/blockchain/
  53. https://medium.com/p/f1d335ce8b5/share/twitter
  54. https://medium.com/p/f1d335ce8b5/share/facebook
  55. https://medium.com/p/f1d335ce8b5/share/twitter
  56. https://medium.com/p/f1d335ce8b5/share/facebook
