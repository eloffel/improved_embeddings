   #[1]gist [2]atom

   [3]skip to content
   ____________________

     * [4]all gists
     * [5]back to github

   [6]sign up for a github account [7]sign in

   instantly share code, notes, and snippets.

[8]@karpathy [9]karpathy/[10]gist:587454dc0146a6ae21fc

   last active mar 20, 2019
     * [11]star [12]251
     * [13]fork [14]109

   [15]code [16]revisions 5 [17]stars 251 [18]forks 109
   embed
   what would you like to do?
   (<script src="https://gist.github.com/karpathy/587454dc0146a6ae21fc.js"
   ></script>)
   embed embed this gist in your website.
   (https://gist.github.com/karpathy/587454dc0146a6ae21fc)
   share copy sharable link for this gist.
   (https://gist.github.com/587454dc0146a6ae21fc.git)
   clone via https clone with git or checkout with svn using the
   repository   s web address.
   learn more about clone urls
   <script src="https:/
   [19]download zip
   an efficient, batched lstm.
   [20]raw
   [21]gistfile1.py
   """
   this is a batched lstm forward and backward pass
   """
   import numpy as np
   import code
   class lstm:
   @staticmethod
   def init(input_size, hidden_size, fancy_forget_bias_init = 3):
   """
   initialize parameters of the lstm (both weights and biases in one
   matrix)
   one might way to have a positive fancy_forget_bias_init number (e.g.
   maybe even up to 5, in some papers)
   """
   # +1 for the biases, which will be the first row of wlstm
   wlstm = np.random.randn(input_size + hidden_size + 1, 4 * hidden_size)
   / np.sqrt(input_size + hidden_size)
   wlstm[0,:] = 0 # initialize biases to zero
   if fancy_forget_bias_init != 0:
   # forget gates get little bit negative bias initially to encourage them
   to be turned off
   # remember that due to xavier initialization above, the raw output
   activations from gates before
   # nonlinearity are zero mean and on order of standard deviation ~1
   wlstm[0,hidden_size:2*hidden_size] = fancy_forget_bias_init
   return wlstm
   @staticmethod
   def forward(x, wlstm, c0 = none, h0 = none):
   """
   x should be of shape (n,b,input_size), where n = length of sequence, b
   = batch size
   """
   n,b,input_size = x.shape
   d = wlstm.shape[1]/4 # hidden size
   if c0 is none: c0 = np.zeros((b,d))
   if h0 is none: h0 = np.zeros((b,d))
   # perform the lstm forward pass with x as the input
   xphpb = wlstm.shape[0] # x plus h plus bias, lol
   hin = np.zeros((n, b, xphpb)) # input [1, xt, ht-1] to each tick of the
   lstm
   hout = np.zeros((n, b, d)) # hidden representation of the lstm (gated
   cell content)
   ifog = np.zeros((n, b, d * 4)) # input, forget, output, gate (ifog)
   ifogf = np.zeros((n, b, d * 4)) # after nonlinearity
   c = np.zeros((n, b, d)) # cell content
   ct = np.zeros((n, b, d)) # tanh of cell content
   for t in xrange(n):
   # concat [x,h] as input to the lstm
   prevh = hout[t-1] if t > 0 else h0
   hin[t,:,0] = 1 # bias
   hin[t,:,1:input_size+1] = x[t]
   hin[t,:,input_size+1:] = prevh
   # compute all gate activations. dots: (most work is this line)
   ifog[t] = hin[t].dot(wlstm)
   # non-linearities
   ifogf[t,:,:3*d] = 1.0/(1.0+np.exp(-ifog[t,:,:3*d])) # sigmoids; these
   are the gates
   ifogf[t,:,3*d:] = np.tanh(ifog[t,:,3*d:]) # tanh
   # compute the cell activation
   prevc = c[t-1] if t > 0 else c0
   c[t] = ifogf[t,:,:d] * ifogf[t,:,3*d:] + ifogf[t,:,d:2*d] * prevc
   ct[t] = np.tanh(c[t])
   hout[t] = ifogf[t,:,2*d:3*d] * ct[t]
   cache = {}
   cache['wlstm'] = wlstm
   cache['hout'] = hout
   cache['ifogf'] = ifogf
   cache['ifog'] = ifog
   cache['c'] = c
   cache['ct'] = ct
   cache['hin'] = hin
   cache['c0'] = c0
   cache['h0'] = h0
   # return c[t], as well so we can continue lstm with prev state init if
   needed
   return hout, c[t], hout[t], cache
   @staticmethod
   def backward(dhout_in, cache, dcn = none, dhn = none):
   wlstm = cache['wlstm']
   hout = cache['hout']
   ifogf = cache['ifogf']
   ifog = cache['ifog']
   c = cache['c']
   ct = cache['ct']
   hin = cache['hin']
   c0 = cache['c0']
   h0 = cache['h0']
   n,b,d = hout.shape
   input_size = wlstm.shape[0] - d - 1 # -1 due to bias
   # backprop the lstm
   difog = np.zeros(ifog.shape)
   difogf = np.zeros(ifogf.shape)
   dwlstm = np.zeros(wlstm.shape)
   dhin = np.zeros(hin.shape)
   dc = np.zeros(c.shape)
   dx = np.zeros((n,b,input_size))
   dh0 = np.zeros((b, d))
   dc0 = np.zeros((b, d))
   dhout = dhout_in.copy() # make a copy so we don't have any funny side
   effects
   if dcn is not none: dc[n-1] += dcn.copy() # carry over gradients from
   later
   if dhn is not none: dhout[n-1] += dhn.copy()
   for t in reversed(xrange(n)):
   tanhct = ct[t]
   difogf[t,:,2*d:3*d] = tanhct * dhout[t]
   # backprop tanh non-linearity first then continue backprop
   dc[t] += (1-tanhct**2) * (ifogf[t,:,2*d:3*d] * dhout[t])
   if t > 0:
   difogf[t,:,d:2*d] = c[t-1] * dc[t]
   dc[t-1] += ifogf[t,:,d:2*d] * dc[t]
   else:
   difogf[t,:,d:2*d] = c0 * dc[t]
   dc0 = ifogf[t,:,d:2*d] * dc[t]
   difogf[t,:,:d] = ifogf[t,:,3*d:] * dc[t]
   difogf[t,:,3*d:] = ifogf[t,:,:d] * dc[t]
   # backprop id180
   difog[t,:,3*d:] = (1 - ifogf[t,:,3*d:] ** 2) * difogf[t,:,3*d:]
   y = ifogf[t,:,:3*d]
   difog[t,:,:3*d] = (y*(1.0-y)) * difogf[t,:,:3*d]
   # backprop matrix multiply
   dwlstm += np.dot(hin[t].transpose(), difog[t])
   dhin[t] = difog[t].dot(wlstm.transpose())
   # backprop the identity transforms into hin
   dx[t] = dhin[t,:,1:input_size+1]
   if t > 0:
   dhout[t-1,:] += dhin[t,:,input_size+1:]
   else:
   dh0 += dhin[t,:,input_size+1:]
   return dx, dwlstm, dc0, dh0
   # -------------------
   # test cases
   # -------------------
   def checksequentialmatchesbatch():
   """ check lstm i/o forward/backward interactions """
   n,b,d = (5, 3, 4) # sequence length, batch size, hidden size
   input_size = 10
   wlstm = lstm.init(input_size, d) # input size, hidden size
   x = np.random.randn(n,b,input_size)
   h0 = np.random.randn(b,d)
   c0 = np.random.randn(b,d)
   # sequential forward
   cprev = c0
   hprev = h0
   caches = [{} for t in xrange(n)]
   hcat = np.zeros((n,b,d))
   for t in xrange(n):
   xt = x[t:t+1]
   _, cprev, hprev, cache = lstm.forward(xt, wlstm, cprev, hprev)
   caches[t] = cache
   hcat[t] = hprev
   # sanity check: perform batch forward to check that we get the same
   thing
   h, _, _, batch_cache = lstm.forward(x, wlstm, c0, h0)
   assert np.allclose(h, hcat), 'sequential and batch forward don''t
   match!'
   # eval loss
   wrand = np.random.randn(*hcat.shape)
   loss = np.sum(hcat * wrand)
   dh = wrand
   # get the batched version gradients
   bdx, bdwlstm, bdc0, bdh0 = lstm.backward(dh, batch_cache)
   # now perform sequential backward
   dx = np.zeros_like(x)
   dwlstm = np.zeros_like(wlstm)
   dc0 = np.zeros_like(c0)
   dh0 = np.zeros_like(h0)
   dcnext = none
   dhnext = none
   for t in reversed(xrange(n)):
   dht = dh[t].reshape(1, b, d)
   dx, dwlstmt, dcprev, dhprev = lstm.backward(dht, caches[t], dcnext,
   dhnext)
   dhnext = dhprev
   dcnext = dcprev
   dwlstm += dwlstmt # accumulate lstm gradient
   dx[t] = dx[0]
   if t == 0:
   dc0 = dcprev
   dh0 = dhprev
   # and make sure the gradients match
   print 'making sure batched version agrees with sequential version:
   (should all be true)'
   print np.allclose(bdx, dx)
   print np.allclose(bdwlstm, dwlstm)
   print np.allclose(bdc0, dc0)
   print np.allclose(bdh0, dh0)
   def checkbatchgradient():
   """ check that the batch gradient is correct """
   # lets gradient check this beast
   n,b,d = (5, 3, 4) # sequence length, batch size, hidden size
   input_size = 10
   wlstm = lstm.init(input_size, d) # input size, hidden size
   x = np.random.randn(n,b,input_size)
   h0 = np.random.randn(b,d)
   c0 = np.random.randn(b,d)
   # batch forward backward
   h, ct, ht, cache = lstm.forward(x, wlstm, c0, h0)
   wrand = np.random.randn(*h.shape)
   loss = np.sum(h * wrand) # weighted sum is a nice hash to use i think
   dh = wrand
   dx, dwlstm, dc0, dh0 = lstm.backward(dh, cache)
   def fwd():
   h,_,_,_ = lstm.forward(x, wlstm, c0, h0)
   return np.sum(h * wrand)
   # now gradient check all
   delta = 1e-5
   rel_error_thr_warning = 1e-2
   rel_error_thr_error = 1
   tocheck = [x, wlstm, c0, h0]
   grads_analytic = [dx, dwlstm, dc0, dh0]
   names = ['x', 'wlstm', 'c0', 'h0']
   for j in xrange(len(tocheck)):
   mat = tocheck[j]
   dmat = grads_analytic[j]
   name = names[j]
   # gradcheck
   for i in xrange(mat.size):
   old_val = mat.flat[i]
   mat.flat[i] = old_val + delta
   loss0 = fwd()
   mat.flat[i] = old_val - delta
   loss1 = fwd()
   mat.flat[i] = old_val
   grad_analytic = dmat.flat[i]
   grad_numerical = (loss0 - loss1) / (2 * delta)
   if grad_numerical == 0 and grad_analytic == 0:
   rel_error = 0 # both are zero, ok.
   status = 'ok'
   elif abs(grad_numerical) < 1e-7 and abs(grad_analytic) < 1e-7:
   rel_error = 0 # not enough precision to check this
   status = 'val small warning'
   else:
   rel_error = abs(grad_analytic - grad_numerical) / abs(grad_numerical +
   grad_analytic)
   status = 'ok'
   if rel_error > rel_error_thr_warning: status = 'warning'
   if rel_error > rel_error_thr_error: status = '!!!!! notok'
   # print stats
   print '%s checking param %s index %s (val = %+8f), analytic = %+8f,
   numerical = %+8f, relative error = %+8f' \
   % (status, name, `np.unravel_index(i, mat.shape)`, old_val,
   grad_analytic, grad_numerical, rel_error)
   if __name__ == "__main__":
   checksequentialmatchesbatch()
   raw_input('check ok, press key to continue to gradient check')
   checkbatchgradient()
   print 'every line should start with ok. have a nice day!'
   [22]@pranv

this comment has been minimized.

   [23]sign in to view
   copy link (button) quote reply

[24]pranv commented [25]sep 11, 2015

   this is indeed very efficient. i sat down hoping to rewrite this
   faster. early on i changed a lot of things. but later, i reverted most
   of it.

   have you used numba. it gave me quite a bit of speedup.
   [26]@upul

this comment has been minimized.

   [27]sign in to view
   copy link (button) quote reply

[28]upul commented [29]nov 24, 2015

   thanks a lot, karpathy!!!!

   could you please suggest me a good reference to get familiar with sltm
   equations? presently, i'm using

   [30]http://arxiv.org/abs/1503.04069 and
   [31]https://apaszke.github.io/lstm-explained.html

   thanks
   [32]@ghost

this comment has been minimized.

   [33]sign in to view
   copy link (button) quote reply

[34]ghost commented [35]feb 4, 2016

   this is fantastic and clearly written. thanks for this
   [36]@arita37

this comment has been minimized.

   [37]sign in to view
   copy link (button) quote reply

[38]arita37 commented [39]may 18, 2016

   hello,
   any try to convert it slightly in cython ? (it should give a boost of
   x3)
   [40]@georgeblck

this comment has been minimized.

   [41]sign in to view
   copy link (button) quote reply

[42]georgeblck commented [43]feb 14, 2017

   thanks for this!
   [44]i rewrote the code in r, if anyone is interested.
   [45]@amin07

this comment has been minimized.

   [46]sign in to view
   copy link (button) quote reply

[47]amin07 commented [48]nov 2, 2018

   what should be the shape of 'dhout_in' in backward pass if i want to
   consider only nth time's state in my classification/softmax layer??
   [49]@alessiobrini

this comment has been minimized.

   [50]sign in to view
   copy link (button) quote reply

[51]alessiobrini commented [52]jan 3, 2019

   can someone post some example of using this batched lstm for training
   over a dataset? i'm new to the domain and i have learned a lot by
   reading this well written code, but when it is time to use it in a real
   learning situation i find problems.
   [53]sign up for free to join this conversation on github. already have
   an account? [54]sign in to comment

     *    2019 github, inc.
     * [55]terms
     * [56]privacy
     * [57]security
     * [58]status
     * [59]help

     * [60]contact github
     * [61]pricing
     * [62]api
     * [63]training
     * [64]blog
     * [65]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [66]reload to refresh your
   session. you signed out in another tab or window. [67]reload to refresh
   your session.

   (button)

references

   visible links
   1. https://gist.github.com/opensearch-gist.xml
   2. https://gist.github.com/karpathy.atom
   3. https://gist.github.com/karpathy/587454dc0146a6ae21fc#start-of-content
   4. https://gist.github.com/discover
   5. https://github.com/
   6. https://gist.github.com/join?source=header-gist
   7. https://gist.github.com/auth/github?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
   8. https://gist.github.com/karpathy
   9. https://gist.github.com/karpathy
  10. https://gist.github.com/karpathy/587454dc0146a6ae21fc
  11. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  12. https://gist.github.com/karpathy/587454dc0146a6ae21fc/stargazers
  13. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  14. https://gist.github.com/karpathy/587454dc0146a6ae21fc/forks
  15. https://gist.github.com/karpathy/587454dc0146a6ae21fc
  16. https://gist.github.com/karpathy/587454dc0146a6ae21fc/revisions
  17. https://gist.github.com/karpathy/587454dc0146a6ae21fc/stargazers
  18. https://gist.github.com/karpathy/587454dc0146a6ae21fc/forks
  19. https://gist.github.com/karpathy/587454dc0146a6ae21fc/archive/029a6e85048960fec169411f34eea304b42846a5.zip
  20. https://gist.github.com/karpathy/587454dc0146a6ae21fc/raw/029a6e85048960fec169411f34eea304b42846a5/gistfile1.py
  21. https://gist.github.com/karpathy/587454dc0146a6ae21fc#file-gistfile1-py
  22. https://gist.github.com/pranv
  23. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  24. https://gist.github.com/pranv
  25. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-1571525
  26. https://gist.github.com/upul
  27. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  28. https://gist.github.com/upul
  29. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-1628962
  30. http://arxiv.org/abs/1503.04069
  31. https://apaszke.github.io/lstm-explained.html
  32. https://gist.github.com/ghost
  33. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  34. https://gist.github.com/ghost
  35. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-1689707
  36. https://gist.github.com/arita37
  37. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  38. https://gist.github.com/arita37
  39. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-1780886
  40. https://gist.github.com/georgeblck
  41. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  42. https://gist.github.com/georgeblck
  43. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-1997259
  44. https://gist.github.com/georgeblck/4179a5f9bbb1244d829f1fc904068c31
  45. https://gist.github.com/amin07
  46. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  47. https://gist.github.com/amin07
  48. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-2749279
  49. https://gist.github.com/alessiobrini
  50. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  51. https://gist.github.com/alessiobrini
  52. https://gist.github.com/karpathy/587454dc0146a6ae21fc#gistcomment-2800462
  53. https://gist.github.com/join?source=comment-gist
  54. https://gist.github.com/login?return_to=https://gist.github.com/karpathy/587454dc0146a6ae21fc
  55. https://github.com/site/terms
  56. https://github.com/site/privacy
  57. https://github.com/security
  58. https://githubstatus.com/
  59. https://help.github.com/
  60. https://github.com/contact
  61. https://github.com/pricing
  62. https://developer.github.com/
  63. https://training.github.com/
  64. https://github.blog/
  65. https://github.com/about
  66. https://gist.github.com/karpathy/587454dc0146a6ae21fc
  67. https://gist.github.com/karpathy/587454dc0146a6ae21fc

   hidden links:
  69. https://gist.github.com/
  70. https://help.github.com/articles/which-remote-url-should-i-use
  71. https://github.com/
