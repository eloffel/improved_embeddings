   this app works best with javascript enabled.

   [1]

spacy

   [usage___]
     * [2]usage
     * [3]models
     * [4]api
     * [5]universe
     *

   ____________________
    guides
       [guides     vectors & similarity___..]
          + get started
          + [6]installation
          + [7]models & languages
          + [8]facts & figures
          + [9]spacy 101
          + [10]new in v2.1
          + [11]new in v2.0
          + guides
          + [12]linguistic features
          + [13]rule-based matching
          + [14]processing pipelines
          + [15]vectors & similarity
               o [16]basics
               o [17]custom vectors
               o [18]gpu usage
          + [19]training models
          + [20]saving & loading
          + [21]adding languages
          + [22]visualizers
          + in-depth
          + [23]code examples

[24]word vectors and semantic similarity

training word vectors

   dense, real valued vectors representing distributional similarity
   information are now a cornerstone of practical nlp. the most common way
   to train these vectors is the [25]id97 family of algorithms. if you
   need to train a id97 model, we recommend the implementation in the
   python library [26]gensim.

   similarity is determined by comparing word vectors or    word
   embeddings   , multi-dimensional meaning representations of a word. word
   vectors can be generated using an algorithm like [27]id97 and
   usually look like this:

banana.vector
array([2.02280000e-01,  -7.66180009e-02,   3.70319992e-01,
       3.28450017e-02,  -4.19569999e-01,   7.20689967e-02,
      -3.74760002e-01,   5.74599989e-02,  -1.24009997e-02,
       5.29489994e-01,  -5.23800015e-01,  -1.97710007e-01,
      -3.41470003e-01,   5.33169985e-01,  -2.53309999e-02,
       1.73800007e-01,   1.67720005e-01,   8.39839995e-01,
       5.51070012e-02,   1.05470002e-01,   3.78719985e-01,
       2.42750004e-01,   1.47449998e-02,   5.59509993e-01,
       1.25210002e-01,  -6.75960004e-01,   3.58420014e-01,
       # ... and so on ...
       3.66849989e-01,   2.52470002e-03,  -6.40089989e-01,
      -2.97650009e-01,   7.89430022e-01,   3.31680000e-01,
      -1.19659996e+00,  -4.71559986e-02,   5.31750023e-01], dtype=float32)

important note

   to make them compact and fast, spacy   s small [28]models (all packages
   that end in sm) don   t ship with word vectors, and only include
   context-sensitive tensors. this means you can still use the
   similarity() methods to compare documents, spans and tokens     but the
   result won   t be as good, and individual tokens won   t have any vectors
   assigned. so in order to use real word vectors, you need to download a
   larger model:
- python -m spacy download en_core_web_sm
+ python -m spacy download en_core_web_lg

   models that come with built-in word vectors make them available as the
   [29]token.vector attribute. [30]doc.vector and [31]span.vector will
   default to an average of their token vectors. you can also check if a
   token has a vector assigned, and get the l2 norm, which can be used to
   normalize vectors.
import spacy

nlp = spacy.load('en_core_web_md')
tokens = nlp(u'dog cat banana afskfsd')

for token in tokens:
    print(token.text, token.has_vector, token.vector_norm, token.is_oov)

     * text: the original token text.
     * has vector: does the token have a vector representation?
     * vector norm: the l2 norm of the token   s vector (the square root of
       the sum of the values squared)
     * oov: out-of-vocabulary

   the words    dog   ,    cat    and    banana    are all pretty common in english,
   so they   re part of the model   s vocabulary, and come with a vector. the
   word    afskfsd    on the other hand is a lot less common and
   out-of-vocabulary     so its vector representation consists of 300
   dimensions of 0, which means it   s practically nonexistent. if your
   application will benefit from a large vocabulary with more vectors, you
   should consider using one of the larger models or loading in a full
   vector package, for example, [32]en_vectors_web_lg, which includes over
   1 million unique vectors.

   spacy is able to compare two objects, and make a prediction of how
   similar they are. predicting similarity is useful for building
   id126s or flagging duplicates. for example, you can
   suggest a user content that   s similar to what they   re currently looking
   at, or label a support ticket as a duplicate if it   s very similar to an
   already existing one.

   each doc, span and token comes with a [33].similarity() method that
   lets you compare it with another object, and determine the similarity.
   of course similarity is always subjective     whether    dog    and    cat    are
   similar really depends on how you   re looking at it. spacy   s similarity
   model usually assumes a pretty general-purpose definition of
   similarity.
import spacy

nlp = spacy.load('en_core_web_md')  # make sure to use larger model!
tokens = nlp(u'dog cat banana')

for token1 in tokens:
    for token2 in tokens:
        print(token1.text, token2.text, token1.similarity(token2))

   in this case, the model   s predictions are pretty on point. a dog is
   very similar to a cat, whereas a banana is not very similar to either
   of them. identical tokens are obviously 100% similar to each other
   (just not always exactly 1.0, because of vector math and floating point
   imprecisions).

[34]customizing word vectors

   word vectors let you import knowledge from raw text into your model.
   the knowledge is represented as a table of numbers, with one row per
   term in your vocabulary. if two terms are used in similar contexts, the
   algorithm that learns the vectors should assign them rows that are
   quite similar, while words that are used in different contexts will
   have quite different values. this lets you use the row-values assigned
   to the words as a kind of dictionary, to tell you some things about
   what the words in your text mean.

   word vectors are particularly useful for terms which aren   t well
   represented in your labelled training data. for instance, if you   re
   doing id39, there will always be lots of names that
   you don   t have examples of. for instance, imagine your training data
   happens to contain some examples of the term    microsoft   , but it
   doesn   t contain any examples of the term    symantec   . in your raw text
   sample, there are plenty of examples of both terms, and they   re used in
   similar contexts. the word vectors make that fact available to the
   entity recognition model. it still won   t see examples of    symantec   
   labelled as a company. however, it   ll see that    symantec    has a word
   vector that usually corresponds to company terms, so it can make the
   id136.

   in order to make best use of the word vectors, you want the word
   vectors table to cover a very large vocabulary. however, most words are
   rare, so most of the rows in a large word vectors table will be
   accessed very rarely, or never at all. you can usually cover more than
   95% of the tokens in your corpus with just a few thousand rows in the
   vector table. however, it   s those 5% of rare terms where the word
   vectors are most useful. the problem is that increasing the size of the
   vector table produces rapidly diminishing returns in coverage over
   these rare terms.

[35]converting word vectors for use in spacy v2.0.10

   custom word vectors can be trained using a number of open-source
   libraries, such as [36]gensim, [37]fast text, or tomas mikolov   s
   original [38]id97 implementation. most word vector libraries output
   an easy-to-read text-based format, where each line consists of the word
   followed by its vector. for everyday use, we want to convert the
   vectors model into a binary format that loads faster and takes up less
   space on disk. the easiest way to do this is the [39]init-model
   command-line utility:
wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/word-vectors-v2/cc.la.3
00.vec.gz
python -m spacy init-model en /tmp/la_vectors_wiki_lg --vectors-loc cc.la.300.ve
c.gz

   this will output a spacy model in the directory
   /tmp/la_vectors_wiki_lg, giving you access to some nice latin vectors     
   you can then pass the directory path to [40]spacy.load().
nlp_latin = spacy.load("/tmp/la_vectors_wiki_lg")
doc1 = nlp_latin(u"caecilius est in horto")
doc2 = nlp_latin(u"servus est in atrio")
doc1.similarity(doc2)

   the model directory will have a /vocab directory with the strings,
   lexical entries and word vectors from the input vectors model. the
   [41]init-model command supports a number of archive formats for the
   word vectors: the vectors can be in plain text (.txt), zipped (.zip),
   or tarred and zipped (.tgz).

[42]optimizing vector coverage v2.0

   to help you strike a good balance between coverage and memory usage,
   spacy   s [43]vectors class lets you map multiple keys to the same row of
   the table. if you   re using the [44]spacy init-model command to create a
   vocabulary, pruning the vectors will be taken care of automatically if
   you set the --prune-vectors flag. you can also do it manually in the
   following steps:
    1. start with a word vectors model that covers a huge vocabulary. for
       instance, the [45]en_vectors_web_lg model provides 300-dimensional
       glove vectors for over 1 million terms of english.
    2. if your vocabulary has values set for the lexeme.prob attribute,
       the lexemes will be sorted by descending id203 to determine
       which vectors to prune. otherwise, lexemes will be sorted by their
       order in the vocab.
    3. call [46]vocab.prune_vectors with the number of vectors you want to
       keep.

nlp = spacy.load('en_vectors_web_lg')
n_vectors = 105000  # number of vectors to keep
removed_words = nlp.vocab.prune_vectors(n_vectors)

assert len(nlp.vocab.vectors) <= n_vectors  # unique vectors have been pruned
assert nlp.vocab.vectors.n_keys > n_vectors  # but not the total entries

   [47]vocab.prune_vectors reduces the current vector table to a given
   number of unique entries, and returns a dictionary containing the
   removed words, mapped to (string, score) tuples, where string is the
   entry the removed word was mapped to, and score the similarity score
   between the two words.

removed words
{
    "shore": ("coast", 0.732257),
    "precautionary": ("caution", 0.490973),
    "hopelessness": ("sadness", 0.742366),
    "continous": ("continuous", 0.732549),
    "disemboweled": ("corpse", 0.499432),
    "biostatistician": ("scientist", 0.339724),
    "somewheres": ("somewheres", 0.402736),
    "observing": ("observe", 0.823096),
    "leaving": ("leaving", 1.0),
}

   in the example above, the vector for    shore    was removed and remapped
   to the vector of    coast   , which is deemed about 73% similar.    leaving   
   was remapped to the vector of    leaving   , which is identical.

   if you   re using the [48]init-model command, you can set the
   --prune-vectors option to easily reduce the size of the vectors as you
   add them to a spacy model:
python -m spacy init-model /tmp/la_vectors_web_md --vectors-loc la.300d.vec.tgz
--prune-vectors 10000

   this will create a spacy model with vectors for the first 10,000 words
   in the vectors model. all other words in the vectors model are mapped
   to the closest vector among those retained.

[49]adding vectors v2.0

   spacy   s new [50]vectors class greatly improves the way word vectors are
   stored, accessed and used. the data is stored in two structures:
     * an array, which can be either on cpu or [51]gpu.
     * a dictionary mapping string-hashes to rows in the table.

   keep in mind that the vectors class itself has no [52]stringstore, so
   you have to store the hash-to-string mapping separately. if you need to
   manage the strings, you should use the vectors via the [53]vocab class,
   e.g. vocab.vectors. to add vectors to the vocabulary, you can use the
   [54]vocab.set_vector method.

adding vectors
from spacy.vocab import vocab

vector_data = {u"dog": numpy.random.uniform(-1, 1, (300,)),
               u"cat": numpy.random.uniform(-1, 1, (300,)),
               u"orange": numpy.random.uniform(-1, 1, (300,))}

vocab = vocab()
for word, vector in vector_data.items():
    vocab.set_vector(word, vector)

[55]loading glove vectors v2.0

   spacy comes with built-in support for loading [56]glove vectors from a
   directory. the [57]vectors.from_glove method assumes a binary format,
   the vocab provided in a vocab.txt, and the naming scheme of
   vectors.{size}.[fd.bin]. for example:

directory structure
          vectors
              vectors.128.f.bin  # vectors file
              vocab.txt          # vocabulary

       file name     dimensions    data type
   vectors.128.f.bin 128        float32
   vectors.300.d.bin 300        float64 (double)
nlp = spacy.load("en_core_web_sm")
nlp.vocab.vectors.from_glove("/path/to/vectors")

   if your instance of language already contains vectors, they will be
   overwritten. to create your own glove vectors model package like
   spacy   s [58]en_vectors_web_lg, you can call [59]nlp.to_disk, and then
   package the model using the [60]package command.

[61]using custom similarity methods

   by default, [62]token.vector returns the vector for its underlying
   [63]lexeme, while [64]doc.vector and [65]span.vector return an average
   of the vectors of their tokens. you can customize these behaviors by
   modifying the doc.user_hooks, doc.user_span_hooks and
   doc.user_token_hooks dictionaries.

     custom user hooks

   for more details on adding hooks and overwriting the built-in doc, span
   and token methods, see the usage guide on [66]user hooks.

[67]storing vectors on a gpu

   if you   re using a gpu, it   s much more efficient to keep the word
   vectors on the device. you can do that by setting the [68]vectors.data
   attribute to a cupy.ndarray object if you   re using spacy or
   [69]chainer, or a torch.tensor object if you   re using [70]pytorch. the
   data object just needs to support __iter__ and __getitem__, so if
   you   re using another library such as [71]tensorflow, you could also
   create a wrapper for your vectors data.

spacy, thinc or chainer
import cupy.cuda
from spacy.vectors import vectors

vector_table = numpy.zeros((3, 300), dtype="f")
vectors = vectors([u"dog", u"cat", u"orange"], vector_table)
with cupy.cuda.device(0):
    vectors.data = cupy.asarray(vectors.data)

pytorch
import torch
from spacy.vectors import vectors

vector_table = numpy.zeros((3, 300), dtype="f")
vectors = vectors([u"dog", u"cat", u"orange"], vector_table)
vectors.data = torch.tensor(vectors.data).cuda(0)

   [72]suggest edits
     * spacy
     * [73]usage
     * [74]models
     * [75]api
     * [76]universe

     * support
     * [77]issue tracker
     * [78]stack overflow
     * [79]reddit user group
     * [80]gitter chat

     * connect
     * [81]twitter
     * [82]github
     * [83]blog

     * stay in the loop!
     * receive updates about new releases, tutorials and more.
     * ____________________
       ____________________ (button) sign up

      2016-2019 [84]explosion ai[85]legal / imprint

references

   visible links
   1. https://spacy.io/
   2. https://spacy.io/usage
   3. https://spacy.io/models
   4. https://spacy.io/api
   5. https://spacy.io/universe
   6. https://spacy.io/usage
   7. https://spacy.io/usage/models
   8. https://spacy.io/usage/facts-figures
   9. https://spacy.io/usage/spacy-101
  10. https://spacy.io/usage/v2-1
  11. https://spacy.io/usage/v2
  12. https://spacy.io/usage/linguistic-features
  13. https://spacy.io/usage/rule-based-matching
  14. https://spacy.io/usage/processing-pipelines
  15. https://spacy.io/usage/vectors-similarity
  16. https://spacy.io/usage/vectors-similarity/#basics
  17. https://spacy.io/usage/vectors-similarity/#custom
  18. https://spacy.io/usage/vectors-similarity/#gpu
  19. https://spacy.io/usage/training
  20. https://spacy.io/usage/saving-loading
  21. https://spacy.io/usage/adding-languages
  22. https://spacy.io/usage/visualizers
  23. https://spacy.io/usage/examples
  24. https://spacy.io/usage/vectors-similarity/#_title
  25. https://en.wikipedia.org/wiki/id97
  26. https://radimrehurek.com/gensim/
  27. https://en.wikipedia.org/wiki/id97
  28. https://spacy.io/models
  29. https://spacy.io/api/token#vector
  30. https://spacy.io/api/doc#vector
  31. https://spacy.io/api/span#vector
  32. https://spacy.io/models/en#en_vectors_web_lg
  33. https://spacy.io/api/token#similarity
  34. https://spacy.io/usage/vectors-similarity/#custom
  35. https://spacy.io/usage/vectors-similarity/#converting
  36. https://radimrehurek.com/gensim
  37. https://fasttext.cc/
  38. https://code.google.com/archive/p/id97/
  39. https://spacy.io/api/cli#init-model
  40. https://spacy.io/api/top-level#spacy.load
  41. https://spacy.io/api/cli#init-model
  42. https://spacy.io/usage/vectors-similarity/#custom-vectors-coverage
  43. https://spacy.io/api/vectors
  44. https://spacy.io/api/cli#init-model
  45. https://spacy.io/models/en#en_vectors_web_lg
  46. https://spacy.io/api/vocab#prune_vectors
  47. https://spacy.io/api/vocab#prune_vectors
  48. https://spacy.io/api/cli#init-model
  49. https://spacy.io/usage/vectors-similarity/#custom-vectors-add
  50. https://spacy.io/api/vectors
  51. https://spacy.io/usage/vectors-similarity/#gpu
  52. https://spacy.io/api/stringstore
  53. https://spacy.io/api/vocab
  54. https://spacy.io/api/vocab#set_vector
  55. https://spacy.io/usage/vectors-similarity/#custom-loading-glove
  56. https://nlp.stanford.edu/projects/glove/
  57. https://spacy.io/api/vectors#from_glove
  58. https://spacy.io/models/en#en_vectors_web_lg
  59. https://spacy.io/api/language#to_disk
  60. https://spacy.io/api/cli#package
  61. https://spacy.io/usage/vectors-similarity/#custom-similarity
  62. https://spacy.io/api/token#vector
  63. https://spacy.io/api/lexeme
  64. https://spacy.io/api/doc#vector
  65. https://spacy.io/api/span#vector
  66. https://spacy.io/usage/processing-pipelines#user-hooks
  67. https://spacy.io/usage/vectors-similarity/#gpu
  68. https://spacy.io/api/vectors#attributes
  69. "https://chainer.org"/
  70. "http://pytorch.org"/
  71. "https://www.tensorflow.org"/
  72. https://github.com/explosion/spacy/tree/master/website/docs/usage/vectors-similarity.md
  73. https://spacy.io/usage
  74. https://spacy.io/models
  75. https://spacy.io/api
  76. https://spacy.io/universe
  77. https://github.com/explosion/spacy/issues
  78. http://stackoverflow.com/questions/tagged/spacy
  79. https://www.reddit.com/r/spacynlp/
  80. https://gitter.im/explosion/spacy
  81. https://twitter.com/spacy_io
  82. https://github.com/explosion/spacy
  83. https://explosion.ai/blog
  84. https://explosion.ai/
  85. https://explosion.ai/legal

   hidden links:
  87. https://github.com/explosion/spacy
  88. https://explosion.ai/
