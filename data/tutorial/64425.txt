   #[1]machinelearning-blog.com    feed [2]machinelearning-blog.com   
   comments feed [3]machinelearning-blog.com    id202 for deep
   learning comments feed [4]data types in statistics [5]how to build a
   neural network with keras [6]alternate [7]alternate
   [8]machinelearning-blog.com [9]wordpress.com

   [10]skip to content

   [11]search

   search for: ____________________ search

   [12]machinelearning-blog.com

   tutorials and explanations about applied machine learning.

   (button) menu
     * [13]home
     * [14]about
     * [15]contact

   [16]open search

[17]id202 for deep learning

   the concepts of id202 are crucial for understanding the theory
   behind machine learning, especially for deep learning. it gives you a
   better intuition for how algorithms really work under the hood, which
   enables you to make better decisions. so if you really want to be a
   professional in this field, you will not come around mastering some of
   its concepts. this post will give you an introduction to the most
   important concepts of id202, that are used in machine
   learning.


table of contents:

     * introduction
     * mathematical objects
          + scalar
          + vector
          + matrix
          + tensor
     * computational rules
          + 1. matrix-scalar operations
          + 2. matrix-vector multiplication
          + 3. matrix-matrix addition and subtraction
          + 4. matrix-id127
     * id127 properties
          + 1. commutative
          + 2. associative
          + 3. distributive
          + 4. identity matrix
     * inverse and transpose
          + 1. inverse
          + 2. transpose
     * summary
     * resources


introduction

   id202 is a continuous form of mathematics and it is applied
   throughout science and engineering because it allows you to model
   natural phenomena and to compute them efficiently. because it is a form
   of continuous and not discrete mathematics, a lot of computer
   scientists don   t have a lot of experience with it. id202 is
   also central to almost all areas of mathematics like geometry and
   functional analysis. its concepts are a crucial prerequisite for
   understanding the theory behind machine learning, especially if you are
   working with deep learning algorithms. you don   t need to understand
   id202 before you get started with machine learning but at some
   point, you want to gain a better intuition for how the different
   machine learning algorithms really work under the hood. this will help
   you to make better decisions during a machine learning system   s
   development. so if you really want to be a professional in this field,
   you will not come around mastering the parts id202 that are
   important for machine learning. in id202, data is represented
   by linear equations, which are presented in the form of matrices and
   vectors. because of that, you are mostly dealing with matrices and
   vectors rather than with scalars (we will cover these terms in the
   following section). when you have the right libraries, like numpy, in
   your proposal, you can compute complex id127 very
   easily with just a few lines of code. note that this blogpost ignores
   concepts of id202 that are not important for machine learning.

mathematical objects

   bildschirmfoto 2018-03-17 um 12.27.10.png

scalar

   a scalar is simply just a single number. for example 24.

vector

   a vector is an ordered array of numbers and can be in a row or a
   column. a vector has just a single index, that can point to a specific
   value within the vector. for example, v2 refers to the second value
   within the vector, which is    -8    in the yellow picture above.

   1

matrix

   a matrix is an ordered 2d array of numbers and it has two indices. the
   first one points to the row and the second one to the column. for
   example, m23 refers to the value in the second row and the third
   column, which is    8    in the yellow picture above. a matrix can have
   multiple numbers of rows and columns. note that a vector is also a
   matrix, but with only one row or one column.

   the matrix in the example in the yellow picture is also a 2 by
   3-dimensional matrix (rows*columns). below you can see another example
   of a matrix along with it   s notation:

   2

tensor

   a tensor is an array of numbers, arranged on a regular grid, with a
   variable number of axis. a tensor has three indices, where the first
   one points to the row, the second to the column and the third one to
   the axis. for example, v232 points to the second row, the third column,
   and the second axis. this refers to the value 0 in right tensor at the
   picture below:

   bildschirmfoto 2018-03-17 um 12.50.26.png

   it is the most general term for all of these concepts above because a
   tensor is a multidimensional array and it can be a vector and a matrix,
   which depends on the number of indices it has. for example, a
   first-order tensor would be a vector (1 index). a second order tensor
   is a matrix (2 indices) and third-order tensors (3 indices) and higher
   are called higher-order tensors (more than 3 indices).

computational rules

1. matrix-scalar operations

   if you multiply, divide, subtract or add a scalar with a matrix, you
   just do this mathematical operation with every element of the matrix.
   the image below shows that perfectly for the example of multiplication:

   3

2. matrix-vector multiplication

   multiplying a matrix with a vector can be thought of as multiplying
   each row of the matrix with the column of the vector. the output will
   be a vector that has the same number of rows as the matrix. the imagee
   below show how this works:

   4

   5

   to better understand the concept, we will go through the calculation of
   the second image. to get the first value of the resulting vector (16),
   we take the numbers of the vector we want to multiply with the matrix
   (1 and 5), and multiply them with the numbers of the first row of the
   matrix (1 and 3). this looks like this:

   1*1 + 3*5 = 16

   we do the same for the values within the second row of the matrix:

   4*1 + 0*5 = 4

   and again for the third row of the matrix:

   2*1 + 1*5 = 7

   here is another example:

   6

   and here is some kind of cheat-sheet:

   7

3. matrix-matrix addition and subtraction

   matrix-matrix addition and subtraction is fairly easy and
   straightforward.  the requirement is that the matrices have the same
   dimensions and the result will be a matrix that has also the same
   dimensions. you just add or subtract each value of the first matrix
   with its corresponding value in the second matrix. the picture below
   shows what i mean:

   img_0738


4. matrix-id127

   multiplying two matrices together isn   t the hard either if you know how
   to multiply a matrix by a vector. note that you can only multiply
   matrices together if the number of the first matrixes columns matches
   the number of the second matrixes rows. the result will be a matrix
   that has the same number of rows as the first matrix has and the same
   number of columns as the second matrix.  it works as follows:

   you simply split the second matrix into column-vectors and multiply the
   first matrix separately with each of these vectors. then you put the
   results in a new matrix (without adding them up!). the image below
   explains this step by step:

   img_0740 2

   and here is again some kind of cheat sheet:

   img_0741

id127 properties

   id127 has several properties that allow us to bundle a
   lot of computation into one id127. we will discuss them
   one by one below. we will start by explaining these concepts with
   scalars and then with matrices because this gives you a better
   understanding.


1. not commutative

   scalar multiplication is commutative but not id127.
   this means that when we are multiplying scalars, 7*3 is the same as 3 *
   7. but when we multiply matrices with each other, a*b isn   t the same as
   b*a.


2. associative

   scalar and id127 are both associative. this means that
   the scalar multiplication 3 (5*3) is the same as (3*5) 3 and that the
   id127 a (b*c) is the same as (a*b) c.


3. distributive

   scalar and id127 are also both distributive. this means
   that 3 (5 + 3) is the same as 3*5 + 3*3 and that a (b+c) is the same as
   a*b + a*c.


4. identity matrix

   the identity matrix is a special kind of matrix but first, we need to
   define what an identity is. the number 1 is an identity because
   everything you multiply with 1 is equal to itself. therefore every
   matrix that is multiplied by an identity matrix is equal to itself. for
   example, matrix a times its identity-matrix is equal to a.

   you can spot an identity matrix by the fact that it has ones along its
   diagonals and that every other value is zero. it is also a    squared
   matrix   , meaning that its number of rows matches its number of columns.

   img_0742

   we previously discussed that id127 is not commutative
   but there is one exception, namely if we multiply a matrix by an
   identity matrix. because of that, the following equation is true: a * i
   = i * a = a


inverse and transpose

   the matrix inverse and the matrix transpose are two special kinds of
   matrix properties. again, we will start by discussing how these
   properties relate to real numbers and then how they relate to matrices.

1. inverse

   first of all, what is an inverse? a number that is multiplied by its
   inverse is equal to 1. note that every number except 0 has an inverse.
   if you would multiply a matrix by its inverse, the result would be its
   identity matrix. the example below shows how the inverse of scalars
   looks like:

   dfg.jpg

   but not every matrix has an inverse. you can compute the inverse of a
   matrix if it is a    squared matrix    and if it can have an inverse.
   discussing which matrices have an inverse would be unfortunately out of
   the scope of this post.

   why do we need an inverse? because we can   t divide matrices. there is
   no concept of dividing by a matrix but we can multiply a matrix by an
   inverse, which results in the same thing.

   the image below shows a matrix that gets multiplied by its own inverse,
   which results in a 2 by 2 identity matrix.

   xo.jpg

   you can easily compute the inverse of a matrix (if it can have one)
   using numpy. heres the link to the documentation:
   [18]https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.l
   inalg.inv.html.

2. transpose

   and lastly, we will discuss the matrix transpose property. this is
   basically the mirror image of a matrix, along a 45 degree axis. it is
   fairly simple to get the transpose of a matrix. its first column is
   just the first row of the transpose-matrix and the second column is
   turned into the second row of the matrix-transpose. an m*n matrix is
   simply transformed into an n*m matrix. also, the aij element of a is
   equal to the aji(transpose) element. the image below illustrates that:

   img_0745

summary

   in this post, you learned about the mathematical objects of linear
   algebra that are used in machine learning. you also learned how to
   multiply, divide, add and subtract these mathematical
   objects. furthermore, you have learned about the most important
   properties of matrices and why they enable us to make more efficient
   computations. on top of that, you have learned what inverse- and
   transpose matrices are and what you can do with it. although there are
   also other parts of id202 used in machine learning, this
   post gave you a proper introduction to the most important concepts.


resources

   [19]deep learning (book)     ian goodfellow, joshua bengio, aaron
   courville

   [20]https://machinelearningmastery.com/linear-algebra-machine-learning/

   [21]andrew ng   s machine learning course on coursera

   [22]https://en.wikipedia.org/wiki/linear_algebra

   [23]https://www.mathsisfun.com/algebra/scalar-vector-matrix.html

   [24]https://www.quantstart.com/articles/scalars-vectors-matrices-and-te
   nsors-linear-algebra-for-deep-learning-part-1

   [25]https://www.aplustopper.com/understanding-scalar-vector-quantities/

   [26]https://machinelearning-blog.com/2017/11/04/calculus-derivatives/

teilen mit:

     * [27]twitter
     * [28]facebook
     *

like this:

   like loading...

related

published by niklas donges

   [29]view all posts by niklas donges
   18. march 2018

   [30]general

post navigation

   [31]data types in statistics
   [32]how to build a neural network with keras

one thought on    id202 for deep learning   

[33]add yours

    1.
   luka loncar says:
       [34]24. march 2018 at 23:17
       great summary, it most definitely refreshed and helped me recollect
       some concepts of id202. i hope you   ll make more on this
       topic. cheers.
       [35]likeliked by [36]1 person
       [37]reply

leave a reply [38]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [39]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [40]log out /
   [41]change )
   google photo

   you are commenting using your google account. ( [42]log out /
   [43]change )
   twitter picture

   you are commenting using your twitter account. ( [44]log out /
   [45]change )
   facebook photo

   you are commenting using your facebook account. ( [46]log out /
   [47]change )
   [48]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   post comment

follow blog via email

   enter your email address to follow this blog and receive notifications
   of new posts by email.

   ____________________

   (button) follow

archives

     * [49]february 2019 (1)
     * [50]september 2018 (2)
     * [51]august 2018 (1)
     * [52]july 2018 (1)
     * [53]april 2018 (4)
     * [54]march 2018 (3)
     * [55]february 2018 (4)
     * [56]january 2018 (4)
     * [57]december 2017 (3)
     * [58]november 2017 (5)

recent posts

     * [59]6 concepts of andrew ng   s book:    machine learning yearning   
     * [60]a brief history of asr: automatic id103
     * [61]connectionist temporal classification
     * [62]agile and non-agile project management
     * [63]introduction to nlp

     * [64]impressum
     * [65]privacy policy

   [66]wordpress.com.

   [67]up    


   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   post to
   [68]cancel reblog post

   close and accept privacy & cookies: this site uses cookies. by
   continuing to use this website, you agree to their use.
   to find out more, including how to control cookies, see here:
   [69]cookie policy

   iframe: [70]likes-master

   %d bloggers like this:

references

   visible links
   1. https://machinelearning-blog.com/feed/
   2. https://machinelearning-blog.com/comments/feed/
   3. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/feed/
   4. https://machinelearning-blog.com/2018/03/07/data-types-in-statistics/
   5. https://machinelearning-blog.com/2018/03/25/how-to-build-a-neural-network-with-keras/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/&for=wpcom-auto-discovery
   8. https://machinelearning-blog.com/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#content
  11. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#search-container
  12. https://machinelearning-blog.com/
  13. https://machinelearning-blog.com/
  14. https://machinelearning-blog.com/about/
  15. https://machinelearning-blog.com/kontakt/
  16. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  17. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  18. https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.inv.html
  19. https://www.amazon.de/deep-learning-adaptive-computation-machine/dp/0262035618/ref=sr_1_1?s=books-intl-de&ie=utf8&qid=1521294134&sr=1-1&keywords=deep+learning
  20. https://machinelearningmastery.com/linear-algebra-machine-learning/
  21. https://www.coursera.org/learn/machine-learning
  22. https://en.wikipedia.org/wiki/linear_algebra
  23. https://www.mathsisfun.com/algebra/scalar-vector-matrix.html
  24. https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1
  25. https://www.aplustopper.com/understanding-scalar-vector-quantities/
  26. https://machinelearning-blog.com/2017/11/04/calculus-derivatives/
  27. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/?share=twitter
  28. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/?share=facebook
  29. https://machinelearning-blog.com/author/niklasd96/
  30. https://machinelearning-blog.com/category/general/
  31. https://machinelearning-blog.com/2018/03/07/data-types-in-statistics/
  32. https://machinelearning-blog.com/2018/03/25/how-to-build-a-neural-network-with-keras/
  33. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#respond
  34. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#comment-52
  35. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/?like_comment=52&_wpnonce=7c735ed770
  36. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  37. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/?replytocom=52#respond
  38. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#respond
  39. https://gravatar.com/site/signup/
  40. javascript:highlandercomments.doexternallogout( 'wordpress' );
  41. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  42. javascript:highlandercomments.doexternallogout( 'googleplus' );
  43. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  44. javascript:highlandercomments.doexternallogout( 'twitter' );
  45. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  46. javascript:highlandercomments.doexternallogout( 'facebook' );
  47. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  48. javascript:highlandercomments.cancelexternalwindow();
  49. https://machinelearning-blog.com/2019/02/
  50. https://machinelearning-blog.com/2018/09/
  51. https://machinelearning-blog.com/2018/08/
  52. https://machinelearning-blog.com/2018/07/
  53. https://machinelearning-blog.com/2018/04/
  54. https://machinelearning-blog.com/2018/03/
  55. https://machinelearning-blog.com/2018/02/
  56. https://machinelearning-blog.com/2018/01/
  57. https://machinelearning-blog.com/2017/12/
  58. https://machinelearning-blog.com/2017/11/
  59. https://machinelearning-blog.com/2019/02/18/6-concepts-of-andrew-ngs-book-machine-learning-yearning/
  60. https://machinelearning-blog.com/2018/09/07/a-brief-history-of-asr-automatic-speech-recognition/
  61. https://machinelearning-blog.com/2018/09/05/753/
  62. https://machinelearning-blog.com/2018/08/24/process-management-agile-and-non-agile-practices/
  63. https://machinelearning-blog.com/2018/07/25/natural-language-processing/
  64. https://machinelearning-blog.com/impressum/
  65. https://machinelearning-blog.com/privacy-policy/
  66. https://wordpress.com/?ref=footer_custom_com
  67. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  68. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/
  69. https://automattic.com/cookies
  70. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  72. https://machinelearning-blog.com/
  73. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#comment-form-guest
  74. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#comment-form-load-service:wordpress.com
  75. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#comment-form-load-service:twitter
  76. https://machinelearning-blog.com/2018/03/18/linear-algebra-for-deep-learning/#comment-form-load-service:facebook
