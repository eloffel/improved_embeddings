   #[1]github [2]recent commits to convai:master

   [3]skip to content

   (button)

     * why github?
       [4]features    
          + [5]code review
          + [6]project management
          + [7]integrations
          + [8]actions
          + [9]team management
          + [10]social coding
          + [11]documentation
          + [12]code hosting
          + [13]customer stories    
          + [14]security    
     * [15]enterprise
     * explore
          + [16]explore github    

learn & contribute
          + [17]topics
          + [18]collections
          + [19]trending
          + [20]learning lab
          + [21]open source guides

connect with others
          + [22]events
          + [23]community forum
          + [24]github education
     * [25]marketplace
     * pricing
       [26]plans    
          + [27]compare plans
          + [28]contact sales
          + [29]nonprofit    
          + [30]education    

   ____________________
     * in this repository all github    
       jump to    

     * no suggested jump to results

     * in this repository all github    
       jump to    
     * in this repository all github    
       jump to    

   [31]sign in [32]sign up

     * [33]watch [34]34
     * [35]star [36]253
     * [37]fork [38]62

[39]deeppavlov/[40]convai

   [41]code [42]issues 2 [43]pull requests 0 [44]projects 0 [45]insights
   (button) dismiss

join github today

   github is home to over 31 million developers working together to host
   and review code, manage projects, and build software together.
   [46]sign up
   no description, website, or topics provided.
     * [47]448 commits
     * [48]2 branches
     * [49]0 releases
     * [50]fetching contributors

    1. [51]python 82.5%
    2. [52]jupyter notebook 13.1%
    3. [53]html 1.7%
    4. [54]shell 0.9%
    5. [55]javascript 0.7%
    6. [56]lua 0.7%
    7. other 0.4%

   (button) python jupyter notebook html shell javascript lua other
   branch: master (button) new pull request
   [57]find file
   clone or download

clone with https

   use git or checkout with svn using the web url.
   https://github.com/d
   [58]download zip

downloading...

   want to be notified of new releases in deeppavlov/convai?
   [59]sign in [60]sign up

launching github desktop...

   if nothing happens, [61]download github desktop and try again.

   (button) go back

launching github desktop...

   if nothing happens, [62]download github desktop and try again.

   (button) go back

launching xcode...

   if nothing happens, [63]download xcode and try again.

   (button) go back

launching visual studio...

   if nothing happens, [64]download the github extension for visual studio
   and try again.

   (button) go back
   [65]@jaseweston
   [66]jaseweston [67]update readme.md
   latest commit [68]cde1e67 feb 8, 2019
   [69]permalink
   type name latest commit message commit time
   failed to load latest commit information.
   [70]2017 [71]update index.md mar 18, 2018
   [72]_layouts [73]change pic aug 16, 2018
   [74]data [75]added links to data jan 25, 2019
   [76]wild [77]chng: desc compliant with latest code version oct 12, 2018
   [78].gitignore [79]delete trash nov 28, 2017
   [80]cname
   [81]neuripsconvai2-intropres.pptx [82]slides from neurips dec 9, 2018
   [83]neuripsconvai2futurework.pptx
   [84]neuripsconvai2resultspres.pptx
   [85]neuripsparticipantslides.pptx [86]add participant slides dec 12,
   2018
   [87]readme.md
   [88]_config.yml
   [89]aws.png [90]aws logo nov 15, 2018
   [91]aws_small.png
   [92]index.md.template
   [93]ipavlov_logo.png
   [94]leaderboards.md
   [95]personachat-example.png [96]personachat example figure mar 16, 2018

readme.md

convai2: overview of the competition

   there are currently few datasets appropriate for training and
   evaluating models for non-goal-oriented dialogue systems (chatbots);
   and equally problematic, there is currently no standard procedure for
   evaluating such models beyond the classic turing test.

   the aim of our competition is therefore to establish a concrete
   scenario for testing chatbots that aim to engage humans, and become a
   standard evaluation tool in order to make such systems directly
   comparable.

   this is the second conversational intelligence (convai) challenge. the
   previous one was conducted under the scope of nips 2017 competitions
   track. this year we aim to improve over last year:
     * providing a dataset from the beginning, [97]persona-chat
     * making the conversations more engaging for humans
     * simpler evaluation process (automatic evaluation, followed then by
       human evaluation)

prize

   the winning entry in human evaluations will receive $20,000 in
   mechanical turk funding -- in order to encourage further data
   collection for dialogue research. the winner in the automatic metrics
   also receives $5000 in aws compute.

news

     * jan 31: [98]paper online summarizing the results of the
       competition. thanks to everyone for taking part!
     * december 9: and the winner has been announced: [99]"lost in
       conversation"! see the presentation slides for all details:
       [100]intro, [101]results, [102]top team presentations, and
       [103]future work.
     * november 20: schedule for the presentations and awards for the
       competition at neurips are up [104]here.
     * november 12:      call for chat volunteers. convai2 competition is now
       at the final stage and we kindly invite community to volunteer in
       evaluating submitted solutions. even 2-3 evaluated dialogues from
       you will contribute significantly to the success of the
       competition. importantly, all the dialogues collected during
       competition will be published open source to promote future
       dialogue system research. final results will be presented at
       neurips 2018 competition track. evaluation system is available at
       fb messenger via link [105]https://m.me/convai.io
     * september 30: the submissions are locked, automatic evals are final
       on the leaderboard, and the final stage of human evaluation is
       almost here. we invite successful teams from the first round (    
       teams) to prepare and test their solutions for mturk and the 'wild'
       evaluation part of the challenge. for wild evaluation more details
       can be found at [106]here. for mturk evaluation, teams should
       provide an interactive script, e.g. like this [107]one.
     * july 20: additional dataset published: this [108]dataset was
       collected during [109]deephack.chat hackathon as a part of
       [110]convai2 competition. the dataset contains more than 2000
       dialogues for [111]personachat task where human evaluators
       recruited via the crowd sourcing platform [112]yandex.toloka
       chatted with bots submitted by teams. this human-bot data has a
       different distribution from the human-human [113]personachat data,
       but may be useful!
     * july 10: 'wild' evaluation started: evaluation by human volunteers
       is open. chat with competing bots! bots are available in messenger
       [114]m.me/convai.io and telegram [115]t.me/convai_chat_bot.
     * may 9: hackathon: we will be organizing a non-compulsory hackathon
       around the competition: [116]deephack.chat hackathon. the most
       promising team attending will receive a travel grant to attend
       neurips 2018!!
     * april 21: leaderboard and baselines: leaderboard, baseline numbers
       and code for training and evaluating them are up!

dataset

   dialogues collected during deephack.chat hackathon and `wild'
   evaluation round are [117]available online.

human evaluation leaderboard

            rank                      creator              rating persona detect
   1                            lost in conversation [118][code] 3.11      0.9
   2                                       (hugging face)                 2.68   0.98
   3                         little baby(ai         )               2.44   0.79
   4                         mohd shadab alam                 2.33   0.93
   5                         happy minions                    1.92   0.46
   6                         adapt centre                     1.6    0.93
   [119]kv profile memory parlai team                      2.44   0.76
   human                  mturk                            3.48   0.96

automatic evaluation leaderboard (hidden test set)

             rank                 creator        ppl    hits@1 f1
   1                                (hugging face)     16.28     80.7      19.5    
   2                           adapt centre         31.4   -      18.39
   3                           happy minions        29.01  -      16.01
   4                           high five            -      65.9   -
   5                           mohd shadab alam     29.94  13.8   16.91
   6                           lost in conversation -      17.1   17.77
   7                           little baby(ai         )   -      64.8   -
   8                        sweet fish           -      45.7   -
   9                        1st-contact          31.98  13.2   16.42
   10                       neurobotics          35.47  -      16.68
   11                       cats'team            -      35.9   -
   12                       sonic                33.46  -      16.67
   13                       pinta                32.49  -      16.39
   14                       khai mai alt         -      34.6   13.03
   15                       loopai               -      25.6   -
   16                       salty fish           34.32  -      -
   17                       team pat             -      -      16.11
   18                       tensorborne          38.24  12.0   15.94
   19                       team dialog 6        40.35  10.9   7.27
   20                       roboy                -      -      15.83
   21                       iamnotadele          66.47  -      13.09
   22                       flooders             -      -      15.47
   23                       clova xiaodong gu    -      -      14.37
   [120]id195 + attention parlai team          29.8   12.6   16.18
   [121]language model      parlai team          46.0   -      15.02
   [122]kv profile memory   parlai team          -      55.2   11.9

        denotes the best performing model for each metric on the hidden test
   set. the rank is determined by sorting by the minimum rank of the score
   in any of the three metrics, where ties are broken by considering the
   second (and then third) smallest ranks. the      teams have made it to the
   next round (top 3 in each metric). for these teams: please make sure
   your github repo contains an interactive script (see [123]here for
   example) that allows your model to function in interactive mode. (to be
   clear, this must be the same model that we evaluated for the
   leaderboard above.)

   models by parlai team are baselines, and not entries into the
   competition; code is included for those models.

   note that the [124]scripts that you can run locally will give metrics
   on the validation set, not the hidden test set which is reported here
   (for that, you need to submit your code, see below).

   validation leaderboard we also provide an additional validation set
   leaderboard [125]here.

personachat convai2 dataset

   [126][personachat-example.png]

   persona-chat training set consists of conversations between
   crowdworkers who were randomly paired and asked to act the part of a
   given provided persona (randomly assigned, and created by another set
   of crowdworkers). the paired workers were asked to chat naturally and
   to get to know each other during the conversation. this produces
   interesting and engaging conversations that learning agents can try to
   mimic.

   the persona-chat task aims to model normal conversation when two
   interlocutors first meet, and get to know each other. their aim is to
   be engaging, to learn about the other's interests, discuss their own
   interests and find common ground. the task is technically challenging
   as it involves both asking and answering questions, and maintaining a
   persistent persona, which is provided.

   conversing with current chit-chat models for even a short amount of
   time quickly exposes their weaknesses. common issues with chit-chat
   models include:
     * (i) the lack of a consistent personality [127](li et al., 2016) as
       they are typically trained over many dialogs each with different
       speakers,
     * (ii) the lack of an explicit long-term memory as they are typically
       trained to produce an utterance given only the recent dialogue
       history [128](vinyals et al., 2015); and
     * (iii) a tendency to produce non-specific answers like ``i don't
       know'' [129](li et al., 2015).

   this competition aims to find models that address those specific
   issues. the baseline systems we have already run indicate that there is
   hope we can make steps in that direction.

   the dataset consists of 164,356 utterances in over 10,981 dialogs, some
   of which are set aside for validation. the speaker pairs each have
   assigned profiles coming from a set of 1155 possible personas, each
   consisting of at least 4 profile sentences, setting aside 200 never
   seen before personas for validation. to avoid modeling that takes
   advantage of trivial word overlap, we crowdsourced additional rewritten
   sets of the same personas, with related sentences that are rephrases,
   generalizations or specializations, rendering the task much more
   challenging.

   more details can be found in the [130]paper describing the dataset.

   the competition dataset is available in our open source system
   [131]parlai, more specifically [132]here. that is, install parlai and
   then do:
python examples/display_data.py --task convai2 --datatype train

   to look at the data.

   source code for baseline methods for the competition are also already
   provided in parlai [133]here, including training loop and evaluation
   code for the automatic id74. baseline results are
   provided in the [134]paper, although the dataset is now larger. (we
   have hence run new baselines that appear on the leaderboard above.)

   as the original test set was released, we have crowdsourced further
   data for a hidden test set unseen by the competitors for automatic
   evaluation.

evaluation

   competitors' models will then be compared in three ways:
     * (i) automated id74 on a new test set hidden from the
       competitors;
     * (ii) evaluation on amazon mechanical turk; and
     * (iii) `wild' live evaluation by volunteers having conversations
       with the bots.

   the winning dialogue systems will be chosen based on these scores.

metrics

   there are three types of metrics we will evaluate:
     * automated metrics - perplexity, f1 and hits@k. these will be
       computed on the hidden test set. competitors will provide their
       code, and we will run the final evaluation (a validation set will
       be provided for their own local tests). perplexity is only scored
       for probabilistic generative models, f1 can be computed for any
       model that produces a response, and hits@k is computed for any
       model that can rank a set of candidate responses that we provide
       (either retrieval based models, or generative models capable of
       assigning a id203 to a candidate response). as some methods
       are not applicable to some metrics, we will have a separate
       leaderboard for each. the top performing methods for each metric
       will be evaluated in the live experiments.
     * amazon mechanical turk - given the entrant's model code, we will
       run live experiments where turkers chat to their model given
       instructions identical to the creation of the original dataset, but
       with new profiles, and then score its performance. turkers will
       score the models between 1-5 with three metrics: fluency,
       consistency and engagingness. finally the turker will try to guess
       the persona being used by the bot (which was provided) as another
       measure of the ability of the bot to stick to its given persona.
       see [135](zhang et al., 2018) for more details of these metrics and
       collected scores of baseline systems.
     * `wild' live chat with volunteers - finally, we will solicit
       volunteers to also chat to the models in a similar way to the
       mechanical turk setup. as volunteers, unlike turkers, are not paid
       and will likely not follow the instructions as closely, the
       distribution will likely be different, hence serving as a test of
       the robustness of the models. this setup will be hosted through the
       messenger and telegram apis.

protocol

   we will run live volunteer chat throughout the competition so that
   competitors can try out their bots talking to humans and to collect
   live data, if they so wish (however, they are also free to only use the
   fixed train/test format at this stage).

   the automated metrics will be used to obtain a shortlist of best
   performing systems, likely the top 3 scoring systems from each of the
   three metrics (perplexity, f1 and hits@k). if those three leaderboards
   feature the same models at the top we will take systems further down
   the leaderboards, up to a maximum of 10. these systems will be
   evaluated in the final live experiments on mechanical turk and via
   volunteers using the same scoring protocols, already described.

   during neurips the `wild' live conversation can continue, and the best
   performing systems will be showcased and conversed with.

   we will declare winners in both the automated metrics tracks, and in
   the live evaluations (which will be considered the grand prize, being
   more important). the latter will consist of the weighted average of the
   turk and wild (volunteer) scores. finally, the solutions and any data
   collected will be made open source to the community.

rules

     * competitors should indicate which training sources are used to
       build their models, and whether (and how) ensembling is used (we
       may place these in separate tracks in an attempt to deemphasize the
       use of ensembles).
     * competitors must provide their source code so that the hidden test
       set evaluation and live experiments can be computed without the
       team's influence, and so that the competition has further impact as
       those models can be released for future research to build off them.
       code can be in any language but a thin python wrapper must be
       provided in order to work with our evaluation and live experiment
       code via [136]parlai's interface.
     * we will require that the winning systems also release their
       training code so that their work is reproducible (although we also
       encourage that for all systems).
     * competitors are free to augment training with other datasets as
       long as they are publicly released (and hence, reproducible).
       hence, all entrants are expected to work on publicly available data
       or release the data they use to train.

model submission

   to submit an entry, create a private repo with your model that works
   with our evaluation code, and share it with the following github
   accounts: [137]emilydinan, [138]klshuster, [139]jaseweston,
   [140]jackurb, [141]varvara-l, [142]madrugado.

   see [143]this directory for example baseline submissions.

   you are free to use any system (e.g. pytorch, tensorflow, c++,..) as
   long as you can wrap your model with parlai for the evaluation. if you
   use pytorch your models should work with pytorch 0.4. the top level
   readme should tell us your team name, model name, and where the
   eval_ppl.py, eval_hits.py etc. files are so we can run them. those
   should give the numbers on the validation set. please also include
   those numbers in the readme so we can check we get the same. we will
   then run the automatic evaluations on the hidden test set and update
   the leaderboard. you can submit a maximum of once per month. we will
   use the same submitted code for the top performing models for computing
   human evaluations when the submission system is locked on september
   30th.

schedule

   up until september 30th competitors will be able to submit models
   (source code) to be evaluated on the hidden test set using automated
   metrics (which we will run on our servers).

   ability to submit a model for evaluation by automatic metrics to be
   displayed on the leaderboard will be available by april 6th. the
   current leaderboards will be visible to all competitors.

   `wild' live evaluation can also be performed at this time to obtain
   id74 and data, although those metrics will not be used
   for final judgement of the systems, but more for tuning systems if the
   competitors so wish.

   on september 30th the source code submission system will be locked, and
   the best performing systems will be evaluated over the next month using
   mechanical turk and the `wild' live evaluation.

   winners will be announced at neurips 2018.

faq

    1. why does the eval_ppl.py script report different perplexity than my
       model training/testing logs? the eval_ppl script will give
       different performance than the perplexity reported by our baseline
       model training scripts. for instance, the model reported perplexity
       for the id195 model includes    easier    tokens such as predicting
       the    end    token which the model appends to each target. the
       separate eval_ppl script does a more careful job of evaluating
       (comparable across models) and doesn't include these extra special
       tokens from the model.
    2. which personas (self:original, self:revised, etc.) will my model be
       evaluated on? all submissions for the leaderboard will be evaluated
       using the    self:original    personas.
    3. how do i view the data? the competition dataset is available in our
       open source system [144]parlai, more specifically [145]here. that
       is, install parlai and then run python examples/display_data.py
       --task convai2 --datatype train to look at the data.
    4. what is the best way to store my model file for submission to the
       competition? we recommend storing your model files and any other
       large files you are using with git-lfs. please see more information
       [146]here.
    5. can i submit different models for different metrics? no. this is
       supposed to measure the performance of a single model in varying
       ways.

deephack.chat hackathon

   this is not a compulsory part of the competition, but you may also be
   interested in the following hackathon:

   if you submit your solution before the 15th of june, you can
   participate in the qualification round of [147]deephack.chat hackathon
   which will take place in moscow on july 2-8. we will select 10 to 12
   teams whose systems score best in terms of automatic metrics, and
   invite them to participate in the final round of the hackathon. at the
   hackathon teams will further improve their systems and listen to
   lectures from the top researchers in the field. participants will also
   take part in live human evaluation of dialogue systems of other teams.
   the winning team will get a travel grant to [148]neurips to convai
   finals.

   if you want to participate in the hackathon, please include a file
   ``team'' in your repository when you submit your models. the file
   should contain names and emails of the members of your team. the team
   should consist of no more than 5 people.

organizing team

   the organizing team comes from multiple groups --- moscow institute of
   physics and technology, facebook ai research, university of montreal,
   mcgill and carnegie mellon university.

   the team consists of: mikhail burtsev, varvara logacheva, valentin
   malykh, ryan lowe, iulian serban, shrimai prabhumoye, emily dinan,
   douwe kiela, alexander miller, kurt shuster, arthur szlam, jack urbanek
   and jason weston.

   advisory board: yoshua bengio, alan w. black, joelle pineau, alexander
   rudnicky, jason williams.

partners

platinum partner

   [149][facebook.png]

gold partner

   [150][aws_small.png]

convai 2017

   webpage of the 1st nips conversational intelligence challenge is
   available at [151]convai.io/2017

     *    2019 github, inc.
     * [152]terms
     * [153]privacy
     * [154]security
     * [155]status
     * [156]help

     * [157]contact github
     * [158]pricing
     * [159]api
     * [160]training
     * [161]blog
     * [162]about

   (button) you can   t perform that action at this time.

   you signed in with another tab or window. [163]reload to refresh your
   session. you signed out in another tab or window. [164]reload to
   refresh your session.

   (button)

references

   visible links
   1. https://github.com/opensearch.xml
   2. https://github.com/deeppavlov/convai/commits/master.atom
   3. https://github.com/deeppavlov/convai#start-of-content
   4. https://github.com/features
   5. https://github.com/features/code-review/
   6. https://github.com/features/project-management/
   7. https://github.com/features/integrations
   8. https://github.com/features/actions
   9. https://github.com/features#team-management
  10. https://github.com/features#social-coding
  11. https://github.com/features#documentation
  12. https://github.com/features#code-hosting
  13. https://github.com/customer-stories
  14. https://github.com/security
  15. https://github.com/enterprise
  16. https://github.com/explore
  17. https://github.com/topics
  18. https://github.com/collections
  19. https://github.com/trending
  20. https://lab.github.com/
  21. https://opensource.guide/
  22. https://github.com/events
  23. https://github.community/
  24. https://education.github.com/
  25. https://github.com/marketplace
  26. https://github.com/pricing
  27. https://github.com/pricing#feature-comparison
  28. https://enterprise.github.com/contact
  29. https://github.com/nonprofit
  30. https://education.github.com/
  31. https://github.com/login?return_to=/deeppavlov/convai
  32. https://github.com/join
  33. https://github.com/login?return_to=/deeppavlov/convai
  34. https://github.com/deeppavlov/convai/watchers
  35. https://github.com/login?return_to=/deeppavlov/convai
  36. https://github.com/deeppavlov/convai/stargazers
  37. https://github.com/login?return_to=/deeppavlov/convai
  38. https://github.com/deeppavlov/convai/network/members
  39. https://github.com/deeppavlov
  40. https://github.com/deeppavlov/convai
  41. https://github.com/deeppavlov/convai
  42. https://github.com/deeppavlov/convai/issues
  43. https://github.com/deeppavlov/convai/pulls
  44. https://github.com/deeppavlov/convai/projects
  45. https://github.com/deeppavlov/convai/pulse
  46. https://github.com/join?source=prompt-code
  47. https://github.com/deeppavlov/convai/commits/master
  48. https://github.com/deeppavlov/convai/branches
  49. https://github.com/deeppavlov/convai/releases
  50. https://github.com/deeppavlov/convai/graphs/contributors
  51. https://github.com/deeppavlov/convai/search?l=python
  52. https://github.com/deeppavlov/convai/search?l=jupyter-notebook
  53. https://github.com/deeppavlov/convai/search?l=html
  54. https://github.com/deeppavlov/convai/search?l=shell
  55. https://github.com/deeppavlov/convai/search?l=javascript
  56. https://github.com/deeppavlov/convai/search?l=lua
  57. https://github.com/deeppavlov/convai/find/master
  58. https://github.com/deeppavlov/convai/archive/master.zip
  59. https://github.com/login?return_to=https://github.com/deeppavlov/convai
  60. https://github.com/join?return_to=/deeppavlov/convai
  61. https://desktop.github.com/
  62. https://desktop.github.com/
  63. https://developer.apple.com/xcode/
  64. https://visualstudio.github.com/
  65. https://github.com/jaseweston
  66. https://github.com/deeppavlov/convai/commits?author=jaseweston
  67. https://github.com/deeppavlov/convai/commit/cde1e67b47cb73d2bb41ef303a257bb9d76f140e
  68. https://github.com/deeppavlov/convai/commit/cde1e67b47cb73d2bb41ef303a257bb9d76f140e
  69. https://github.com/deeppavlov/convai/tree/cde1e67b47cb73d2bb41ef303a257bb9d76f140e
  70. https://github.com/deeppavlov/convai/tree/master/2017
  71. https://github.com/deeppavlov/convai/commit/550746a05979cf9d0813d8a3778344ac49f5d752
  72. https://github.com/deeppavlov/convai/tree/master/_layouts
  73. https://github.com/deeppavlov/convai/commit/965400ca8230cc00866ac2b756af95455cc0f000
  74. https://github.com/deeppavlov/convai/tree/master/data
  75. https://github.com/deeppavlov/convai/commit/6adf1bc8748e7597d3a094b3d4f8edd9996192b3
  76. https://github.com/deeppavlov/convai/tree/master/wild
  77. https://github.com/deeppavlov/convai/commit/26bcee161f03644bcf813a8e9e3dbeab6a3a8bd4
  78. https://github.com/deeppavlov/convai/blob/master/.gitignore
  79. https://github.com/deeppavlov/convai/commit/c59025d7c0044bc1ec3515bed16d81020bfe31a2
  80. https://github.com/deeppavlov/convai/blob/master/cname
  81. https://github.com/deeppavlov/convai/blob/master/neuripsconvai2-intropres.pptx
  82. https://github.com/deeppavlov/convai/commit/503e242f011e95127b727084beb9d292660683a4
  83. https://github.com/deeppavlov/convai/blob/master/neuripsconvai2futurework.pptx
  84. https://github.com/deeppavlov/convai/blob/master/neuripsconvai2resultspres.pptx
  85. https://github.com/deeppavlov/convai/blob/master/neuripsparticipantslides.pptx
  86. https://github.com/deeppavlov/convai/commit/ef443b2f78e6ae4aa4c37e0f01b504ec21392ad5
  87. https://github.com/deeppavlov/convai/blob/master/readme.md
  88. https://github.com/deeppavlov/convai/blob/master/_config.yml
  89. https://github.com/deeppavlov/convai/blob/master/aws.png
  90. https://github.com/deeppavlov/convai/commit/ed47e204a8f2d67894dd169e28790572b54250d0
  91. https://github.com/deeppavlov/convai/blob/master/aws_small.png
  92. https://github.com/deeppavlov/convai/blob/master/index.md.template
  93. https://github.com/deeppavlov/convai/blob/master/ipavlov_logo.png
  94. https://github.com/deeppavlov/convai/blob/master/leaderboards.md
  95. https://github.com/deeppavlov/convai/blob/master/personachat-example.png
  96. https://github.com/deeppavlov/convai/commit/772e73825dfebe6ec089cc51aa51ff81a34c06dd
  97. http://convai.io/#personachat-convai2-dataset
  98. https://arxiv.org/abs/1902.00098
  99. https://github.com/atselousov/transformer_chatbot
 100. https://github.com/deeppavlov/convai/blob/master/neuripsconvai2-intropres.pptx
 101. https://github.com/deeppavlov/convai/blob/master/neuripsconvai2resultspres.pptx
 102. https://github.com/deeppavlov/convai/blob/master/neuripsparticipantslides.pptx
 103. https://github.com/deeppavlov/convai/blob/master/neuripsconvai2futurework.pptx
 104. https://neurips.cc/conferences/2018/schedule?showevent=10945
 105. https://m.me/convai.io
 106. http://convai.io/wild/
 107. https://github.com/facebookresearch/parlai/blob/master/projects/convai2/baselines/kvmemnn/interactive.py
 108. http://convai.io/data/
 109. http://deephack.me/chat
 110. http://convai.io/
 111. http://convai.io/#personachat-convai2-dataset
 112. https://toloka.yandex.com/
 113. http://convai.io/#personachat-convai2-dataset
 114. https://m.me/convai.io
 115. https://t.me/convai_chat_bot
 116. http://convai.io/#deephackchat-hackathon
 117. http://convai.io/data
 118. https://github.com/atselousov/transformer_chatbot
 119. https://github.com/facebookresearch/parlai/tree/master/projects/convai2/baselines/kvmemnn
 120. https://github.com/facebookresearch/parlai/tree/master/projects/convai2/baselines/id195
 121. https://github.com/facebookresearch/parlai/tree/master/projects/convai2/baselines/language_model
 122. https://github.com/facebookresearch/parlai/tree/master/projects/convai2/baselines/kvmemnn
 123. https://github.com/facebookresearch/parlai/blob/master/projects/convai2/baselines/kvmemnn/interactive.py
 124. https://github.com/facebookresearch/parlai/tree/master/projects/convai2/baselines
 125. https://github.com/deeppavlov/convai/blob/master/leaderboards.md
 126. https://github.com/deeppavlov/convai/blob/master/personachat-example.png
 127. https://arxiv.org/abs/1603.06155
 128. https://arxiv.org/abs/1506.05869
 129. https://arxiv.org/abs/1510.03055
 130. https://arxiv.org/abs/1801.07243
 131. http://parl.ai/
 132. https://github.com/facebookresearch/parlai/tree/master/parlai/tasks/convai2
 133. https://github.com/facebookresearch/parlai/tree/master/projects/convai2
 134. https://arxiv.org/abs/1801.07243
 135. https://arxiv.org/abs/1801.07243
 136. http://parl.ai/
 137. https://github.com/emilydinan
 138. https://github.com/klshuster
 139. https://github.com/jaseweston
 140. https://github.com/jackurb
 141. https://github.com/varvara-l
 142. https://github.com/madrugado
 143. https://github.com/facebookresearch/parlai/tree/master/projects/convai2
 144. http://parl.ai/
 145. https://github.com/facebookresearch/parlai/tree/master/parlai/tasks/convai2
 146. https://git-lfs.github.com/
 147. http://deephack.me/chat
 148. https://neurips.cc/
 149. https://research.fb.com/category/facebook-ai-research-fair/
 150. https://aws.amazon.com/
 151. http://convai.io/2017/
 152. https://github.com/site/terms
 153. https://github.com/site/privacy
 154. https://github.com/security
 155. https://githubstatus.com/
 156. https://help.github.com/
 157. https://github.com/contact
 158. https://github.com/pricing
 159. https://developer.github.com/
 160. https://training.github.com/
 161. https://github.blog/
 162. https://github.com/about
 163. https://github.com/deeppavlov/convai
 164. https://github.com/deeppavlov/convai

   hidden links:
 166. https://github.com/
 167. https://github.com/deeppavlov/convai
 168. https://github.com/deeppavlov/convai
 169. https://github.com/deeppavlov/convai
 170. https://help.github.com/articles/which-remote-url-should-i-use
 171. https://github.com/deeppavlov/convai#convai2-overview-of-the-competition
 172. https://github.com/deeppavlov/convai#prize
 173. https://github.com/deeppavlov/convai#news
 174. https://github.com/deeppavlov/convai#dataset
 175. https://github.com/deeppavlov/convai#human-evaluation-leaderboard
 176. https://github.com/deeppavlov/convai#automatic-evaluation-leaderboard-hidden-test-set
 177. https://github.com/deeppavlov/convai#personachat-convai2-dataset
 178. https://github.com/deeppavlov/convai#evaluation
 179. https://github.com/deeppavlov/convai#metrics
 180. https://github.com/deeppavlov/convai#protocol
 181. https://github.com/deeppavlov/convai#rules
 182. https://github.com/deeppavlov/convai#model-submission
 183. https://github.com/deeppavlov/convai#schedule
 184. https://github.com/deeppavlov/convai#faq
 185. https://github.com/deeppavlov/convai#deephackchat-hackathon
 186. https://github.com/deeppavlov/convai#organizing-team
 187. https://github.com/deeppavlov/convai#partners
 188. https://github.com/deeppavlov/convai#platinum-partner
 189. https://github.com/deeppavlov/convai#gold-partner
 190. https://github.com/deeppavlov/convai#convai-2017
 191. https://github.com/
