   #[1]jonathan hui blog posts

   [2][rssicon.svg]
   [3]jonathan hui blog

   [4]about

   convolutional neural networks (id98) tutorial   

   mar 16, 2017

overview

   in a fully connected network, all nodes in a layer are fully connected
   to all the nodes in the previous layer. this produces a complex model
   to explore all possible connections among nodes. but the complexity
   pays a high price in training the network and how deep the network can
   be. for spatial data like image, this complexity provides no additional
   benefits since most features are localized.
   [ppl.jpg]

   for face detection, the areas of interested are all localized.
   convolution neural networks apply small size filter to explore the
   images. the number of trainable parameters is significantly smaller and
   therefore allow id98 to use many filters to extract interesting
   features.

filters

   filters are frequently applied to images for different purposes. the
   human visual system applies edge detection filters to recognize an
   object.
   [edge.png]

   for example, to blur an image, we can apply a filter with patch size
   3x3 over every pixel in the image:
   [filter_b.png]

   to apply the filter to an image, we move the filter 1 pixel at a time
   from left to right and top to bottom until we process every pixel.
   [stride.png]

stride and padding

   however, we may encounter some problem on the edge. for example, on the
   top left corner, a filter may cover beyond the edge of an image. for a
   filter with patch size 3x3, we may ignore the edge and generate an
   output with width and height reduce by 2 pixels. otherwise, we can pack
   extra 0 or replicate the edge of the original image. all these settings
   are possible and configurable as    padding    in a id98.
   [padding.png]

     padding with extra 0 is more popular because it maintains spatial
     dimensions and better preserve information on the edge.

   for a id98, sometimes we do not move the filter only by 1 pixel. if we
   move the filter 2 pixels to the right, we say the    x stride    is equal
   to 2.
   [stride2.png]

   notice that both padding and stride may change the spatial dimension of
   the output. a stride of 2 in x direction will reduce x-dimension by 2.
   without padding and x stride equals 2, the output shrink n pixels:

convolutional neural network (id98)

   a convolutional neural network composes of convolution layers, polling
   layers and fully connected layers(fc).
   [conv_layer.png]

   when we process the image, we apply filters which each generates an
   output that we call feature map. if k-features map is created, we have
   feature maps with depth k.
   [filter_m.png]

visualization

   id98 uses filters to extract features of an image. it would be
   interesting to see what kind of filters that a id98 eventually trained.
   this gives us some insight understanding what the id98 trying to learn.

   here are the 96 filters learned in the first convolution layer in
   alexnet. many filters turn out to be edge detection filters common to
   human visual systems. (source from krizhevsky et al.)
   [id98filter.png]

   the right side shows images with the highest activation in some feature
   maps at layer 4. then we reconstruct the images based on the
   activations in the feature maps. this gives up some understanding of
   what the our model is looking for. (source from matthew d zeiler et
   al.)
   [id98layer_4.png]

     if the visualization of the filters seems lossy, it indicates we
     need more training iterations or we are overfitting.

batch id172 & relu

   after applying filters on the input, we apply a batch id172
   followed by a relu for non-linearity. the batch id172
   renormalizes data to make learning faster with the id119.

   batch id172 applies this equation to the input:

   for a feature map with the spatial dimension 10x10, we compute 100
   means and 100 variance from the batch samples. for example, if the
   batch size is 16, the mean for the feature at location (i, j) is
   computed by:

   we feed to a linear equation with the trainable scalar values and (1
   pair for each normalized layer).

   the id172 can be undone if and . we initialize and , so the
   input is normalized and therefore learns faster, and the parameters
   will be learned during the training.

   this is the code to implement batch id172 in tensorflow:
w_bn = tf.variable(w_initial)
z_bn = tf.matmul(x, w_bn)

bn_mean, bn_var = tf.nn.moments(z_bn, [0])
scale = tf.variable(tf.ones([100]))
beta = tf.variable(tf.zeros([100]))

bn_layer = tf.nn.batch_id172(z_bn, bn_mean, bn_var, beta, scale, 1e-3)
l_bn = tf.nn.relu(bn_layer)

pooling

   to reduce the spatial dimension of a feature map, we apply maximum
   pool. a 2x2 maximum pool replaces a 2x2 area by its maximum. after
   applying a 2x2 pool, we reduce the spatial dimension for the example
   below from 4x4 to 2x2. (filter size=2, stride = 2)
   [pooling.png]

   here, we construct a id98 using convolution and pooling:
   [conv_layer2.png]

   pooling is often used with a convolution layer. therefore, we often
   consider it as part of the convolution layer rather than a separate
   layer. the most common configuration is the maximum pool with filter
   size 2 and stride size 2. a filter size of 3 and stride size 2 is less
   common. other pooling like average pooling has been used but fall out
   of favor lately. as a side note, some researcher may prefer using
   striding in a convolution filter to reduce dimension rather than
   pooling.

multiple convolution layers

   like deep learning, the depth of the network increases the complexity
   of a model. a id98 network usually composes of many convolution layers.
   [convolution_b1.png]

   the id98 above composes of 3 convolution layer. we start with a 32x32
   pixel image with 3 channels (rgb). we apply a 3x4 filter and a 2x2 max
   pooling which convert the image to 16x16x4 feature maps. the following
   table walks through the filter and layer shape at each layer:
   [id98_chanl.png]

fully connected (fc) layers

   after using convolution layers to extract the spatial features of an
   image, we apply fully connected layers for the final classification.
   first, we flatten the output of the convolution layers. for example, if
   the final features maps have a dimension of 4x4x512, we will flatten it
   to an array of 8192 elements. we apply 2 more hidden layers here before
   we perform the final classification. the techniques needed are no
   difference from a fc network in deep learning.
   [convolution_b2.jpg]

tips

   here are some of the tips to construct a id98:
     * use smaller filters like 3x3 or 5x5 with more convolution layer.
     * convolution filter with small stride works better.
     * if gpu memory is not large enough, sacrifice the first layer with a
       larger filter like 7x7 with stride 2.
     * use padding fill with 0.
     * use filter size 2, stride size 2 for the maximum pooling if needed.

   for the network design:
    1. start with 2-3 convolution layers with small filters 3x3 or 5x5 and
       no pooling.
    2. add a 2x2 maximum pool to reduce the spatial dimension.
    3. repeat 1-2 until a desired spatial dimension is reached for the
       fully connected layer. this can be a try and error process.
    4. use 2-3 hidden layers for the fully-connection layers.

convolutional pyramid

   for each convolution layer, we reduce the spatial dimension while
   increasing the depth of the feature maps. because of the shape, we call
   this a convolutional pyramid.
   [id983d.png]

   here, we reduce the spatial dimension of each convolution layer through
   pooling or sometimes apply a filter with stride size > 1.
   [id983d4.png]

   the depth of the feature map can be increased by applying more filters.
   [id983d2.png]

   the core thinking of id98 is to apply small filters to explore spatial
   feature. the spatial dimension will gradually decrease as we go deep
   into the network. on the other hand, the depth of the feature maps will
   increase. it will eventually reach a stage that spatial locality is
   less important and we can apply a fc network for final analysis.

google inceptions with 1x1 convolution

   in our previous discussion, the convolution filter in each layer is of
   the same patch size say 3x3. to increase the depth of the feature maps,
   we can apply more filters using the same patch size. however, in
   googlenet, it applies a different approach to increase the depth.
   googlenet uses different filter patch size for the same layer. here we
   can have filters with patch size 3x3 and 1x1. don   t mistake that a 1x1
   filter is doing nothing. it does not explore the spatial dimension but
   it explores the depth of the feature maps. for example, in the 1x1
   filter below, we convert the rgb channels (depth 3) into two feature
   maps output. the first set of filters generates 8 features map while
   the second one generates two. we can concatenate them to form maps of
   depth 10. the inception idea is to increase the depth of the feature
   map by concatenating feature maps using different patch size of
   convolution filters and pooling.
   [inception.png]

   inceptions can be considered as one way to introduce non-linearity into
   the system.

fully connected network

   after exploring the spatial relationship, we flatten the convolution
   layer output and connect it to a fully connected network:
   [id983d5.png]
   [id983d6.png]

tensoflowr code

   we will implement coding for a id98 to classify handwriting for digits
   (0 to 9).

     we will use tensorflow to implement a id98. nevertheless, the full
     understanding of the code is not needed or suggested even the code
     is pretty self-explainable.

construct a id98

   in the code below, we construct a id98 with 2 convolution layer followed
   by 2 fc layer and then 1 classifier. here is where we construct our id98
   network.
# model.
def model(data):
    # first convolution layer with stride = 1 and pad the edge to make the outpu
t size the same.
    # apply relu and a maximum 2x2 pool
    conv1 = tf.nn.conv2d(data, id981_w, [1, 1, 1, 1], padding='same')
    hidden1 = tf.nn.relu(conv1 + id981_b)
    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='same')

    # second convolution layer
    conv2 = tf.nn.conv2d(pool1, id982_w, [1, 1, 1, 1], padding='same')
    hidden2 = tf.nn.relu(conv2 + id982_b)
    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='same')

    # flattern the convolution output
    shape = pool2.get_shape().as_list()
    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])

    # 2 fc hidden layers
    fc1 = tf.nn.relu(tf.matmul(reshape, fc1_w) + fc1_b)
    fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w) + fc2_b)

    # return the result of the classifier
    return tf.matmul(fc2, classifier_w) + classifier_b

   for the convolution filter, we want the output spatial dimension to be
   the same as the input and therefore we assign padding =    same   . we also
   have stride = 1. after the filter, we apply the standard relu and a 2x2
   maximum pool.
conv1 = tf.nn.conv2d(data, id981_w, [1, 1, 1, 1], padding='same')
hidden1 = tf.nn.relu(conv1 + id981_b)
pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='same')

   after the second convolution layer, we flatten the layer for the fc
   layer.
shape = pool2.get_shape().as_list()
reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])

   we applied 2 fc layers before return the result from the classifier.
fc1 = tf.nn.relu(tf.matmul(reshape, fc1_w) + fc1_b)
fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w) + fc2_b)

# return the result of the classifier
return tf.matmul(fc2, classifier_w) + classifier_b

id98 configuration:

   here is our model configuration. we have 2 convolution layers both
   using a filter with patch size 5x5 and generate feature maps with depth
   16. the first fc output 256 values while the second output 64.
batch_size = 16
patch_size = 5
depth = 16
num_hidden1 = 256
num_hidden2 = 64

   here is where we define the trainable parameters for id98 layer 1 and 2.
   for example, the shape of the weight in id981 is 5x5x3x16. it applies
   5x5 filter patch for rgb channels which output feature maps with depth
   16.
# id98 layer 1 with filter (num_channels, depth) (3, 16)
id981_w = tf.variable(tf.truncated_normal([patch_size, patch_size, num_channels,
depth], stddev=0.1))
id981_b = tf.variable(tf.zeros([depth]))

# id98 layer 2 with filter (depth, depth) (16, 16)
id982_w = tf.variable(tf.truncated_normal([patch_size, patch_size, depth, depth],
 stddev=0.1))
id982_b = tf.variable(tf.constant(1.0, shape=[depth]))

   here are the fc trainable parameters that is not much difference from
   other deep learning network using fc.
 # compute the output size of the id982 as a 1d array.
size = image_size // 4 * image_size // 4 * depth

# fc1 (size, num_hidden1) (size, 256)
fc1_w = tf.variable(tf.truncated_normal([size, num_hidden1], stddev=np.sqrt(2.0
/ size)))
fc1_b = tf.variable(tf.constant(1.0, shape=[num_hidden1]))

# fc2 (num_hidden1, num_hidden2) (size, 64)
fc2_w = tf.variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=np.sq
rt(2.0 / (num_hidden1))))
fc2_b = tf.variable(tf.constant(1.0, shape=[num_hidden2]))

# classifier (num_hidden2, num_labels) (64, 10)
classifier_w = tf.variable(tf.truncated_normal([num_hidden2, num_labels], stddev
=np.sqrt(2.0 / (num_hidden2))))
classifier_b = tf.variable(tf.constant(1.0, shape=[num_labels]))

   here is the full code for completeness. nevertheless, the code requires
   the datafile    notmnist.pickle    to run which is not provided here.
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import pickle
import os


def set_working_dir():
    tmp_dir = 'tmp'
    if not os.path.exists(tmp_dir):
        os.makedirs(tmp_dir)
    os.chdir(tmp_dir)
    print("change working directory to", os.getcwd())


set_working_dir()

pickle_file = 'notmnist.pickle'

with open(pickle_file, 'rb') as f:
    save = pickle.load(f)
    train_dataset = save['train_dataset']
    train_labels = save['train_labels']
    valid_dataset = save['valid_dataset']
    valid_labels = save['valid_labels']
    test_dataset = save['test_dataset']
    test_labels = save['test_labels']
    del save  # hint to help gc free up memory
    print('training set', train_dataset.shape, train_labels.shape)
    print('validation set', valid_dataset.shape, valid_labels.shape)
    print('test set', test_dataset.shape, test_labels.shape)

image_size = 28
num_labels = 10
num_channels = 1 # grayscale


def reformat(dataset, labels):
  dataset = dataset.reshape(
    (-1, image_size, image_size, num_channels)).astype(np.float32)
  labels = (np.arange(num_labels) == labels[:,none]).astype(np.float32)
  return dataset, labels
train_dataset, train_labels = reformat(train_dataset, train_labels)
valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)
test_dataset, test_labels = reformat(test_dataset, test_labels)
print('training set', train_dataset.shape, train_labels.shape)
print('validation set', valid_dataset.shape, valid_labels.shape)
print('test set', test_dataset.shape, test_labels.shape)


def accuracy(predictions, labels):
  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))
          / predictions.shape[0])

batch_size = 16
patch_size = 5
depth = 16
num_hidden1 = 256
num_hidden2 = 64

graph = tf.graph()

with graph.as_default():
    # define the training dataset and lables
    tf_train_dataset = tf.placeholder(
        tf.float32, shape=(batch_size, image_size, image_size, num_channels))
    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))

    # validation/test dataset
    tf_valid_dataset = tf.constant(valid_dataset)
    tf_test_dataset = tf.constant(test_dataset)

    # id98 layer 1 with filter (num_channels, depth) (3, 16)
    id981_w = tf.variable(tf.truncated_normal(
        [patch_size, patch_size, num_channels, depth], stddev=0.1))
    id981_b = tf.variable(tf.zeros([depth]))

    # id98 layer 2 with filter (depth, depth) (16, 16)
    id982_w = tf.variable(tf.truncated_normal(
        [patch_size, patch_size, depth, depth], stddev=0.1))
    id982_b = tf.variable(tf.constant(1.0, shape=[depth]))

    # compute the output size of the id982 as a 1d array.
    size = image_size // 4 * image_size // 4 * depth

    # fc1 (size, num_hidden1) (size, 256)
    fc1_w = tf.variable(tf.truncated_normal(
        [size, num_hidden1], stddev=np.sqrt(2.0 / size)))
    fc1_b = tf.variable(tf.constant(1.0, shape=[num_hidden1]))

    # fc2 (num_hidden1, num_hidden2) (size, 64)
    fc2_w = tf.variable(tf.truncated_normal(
        [num_hidden1, num_hidden2], stddev=np.sqrt(2.0 / (num_hidden1))))
    fc2_b = tf.variable(tf.constant(1.0, shape=[num_hidden2]))

    # classifier (num_hidden2, num_labels) (64, 10)
    classifier_w = tf.variable(tf.truncated_normal(
        [num_hidden2, num_labels], stddev=np.sqrt(2.0 / (num_hidden2))))
    classifier_b = tf.variable(tf.constant(1.0, shape=[num_labels]))

    # model.
    def model(data):
        # first convolution layer with stride = 1 and pad the edge to make the o
utput size the same.
        # apply relu and a maximum 2x2 pool
        conv1 = tf.nn.conv2d(data, id981_w, [1, 1, 1, 1], padding='same')
        hidden1 = tf.nn.relu(conv1 + id981_b)
        pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='sam
e')

        # second convolution layer
        conv2 = tf.nn.conv2d(pool1, id982_w, [1, 1, 1, 1], padding='same')
        hidden2 = tf.nn.relu(conv2 + id982_b)
        pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='sam
e')

        # flattern the convolution output
        shape = pool2.get_shape().as_list()
        reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])

        # 2 fc hidden layers
        fc1 = tf.nn.relu(tf.matmul(reshape, fc1_w) + fc1_b)
        fc2 = tf.nn.relu(tf.matmul(fc1, fc2_w) + fc2_b)

        # return the result of the classifier
        return tf.matmul(fc2, classifier_w) + classifier_b

    # training computation.
    logits = model(tf_train_dataset, true)
    loss = tf.reduce_mean(
        tf.nn.softmax_cross_id178_with_logits(labels=tf_train_labels, logits=l
ogits))

    # optimizer.
    optimizer = tf.train.adamoptimizer(0.0005).minimize(loss)

    # predictions for the training, validation, and test data.
    train_prediction = tf.nn.softmax(logits)
    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))
    test_prediction = tf.nn.softmax(model(tf_test_dataset))

num_steps = 20001

with tf.session(graph=graph) as session:
  tf.global_variables_initializer().run()
  print('initialized')
  for step in range(num_steps):
    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]
    batch_labels = train_labels[offset:(offset + batch_size), :]
    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}
    _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
    if (step % 500 == 0):
      print('minibatch loss at step %d: %f' % (step, l))
      print('minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))
      print('validation accuracy: %.1f%%' % accuracy(
        valid_prediction.eval(), valid_labels))
  print('test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))

  # test accuracy: 95.2%

id21

   training a network can take a long time and a large dataset. transfer
   learning is about using other people models to solve your problems. for
   example, can we use a pre-built natural language processing network in
   english for spanish? can we use a id98 network to predict different
   kinds of classes? in practice, there are more commons than we think.
   the features extracted at earlier layers are similar in many problem
   domains. for example, we can reuse a mature id98 model pre-trained with
   a huge dataset, and replace a few right most fc layers. in the network
   below, we replace the red layer and the ones on its right. we can add
   or remove nodes and layers. we initialize these new layers and train
   with our smaller dataset. there are 2 options in the training. allow
   the whole system to be trained or just perform id119 on the
   changed layers.
   [id98.png]

   in addition, we can feed the activation output at certain layer to a
   different network to solve a different problem. for example, we want to
   create a caption for images automatically. first, we can process images
   by a id98 and use the features in the fc layer as input to a recurrent
   network to generate caption.

credits

   for the tensorflow coding, we start with the id98 class assignment 4
   from the google deep learning class on udacity. we implement a id98
   design with additional code to complete the assignment.
   please enable javascript to view the [5]comments powered by disqus.
   [6]comments powered by disqus

     * jonathan hui blog

     * [7]jhui

   deep learning

references

   visible links
   1. https://jhui.github.io/feed.xml
   2. https://jhui.github.io/feed.xml
   3. https://jhui.github.io/
   4. https://jhui.github.io/about/
   5. http://disqus.com/?ref_noscript
   6. http://disqus.com/
   7. https://github.com/jhui

   hidden links:
   9. https://jhui.github.io/2017/03/16/id98-convolutional-neural-network/
