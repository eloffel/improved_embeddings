   #[1]rss

[2]deepgrid

   organic deep learning.

   latest article:
   [3]vanishing and exploding gradient problems

   21 may 2018

   [4]home [5]about [6]archive

   [7]github

   [8]twitter @jefkine

      2018. all rights reserved.

id26 in convolutional neural networks

   jefkine, 5 september 2016

introduction

   convolutional neural networks (id98s) are a biologically-inspired
   variation of the multilayer id88s (mlps). neurons in id98s share
   weights unlike in mlps where each neuron has a separate weight vector.
   this sharing of weights ends up reducing the overall number of
   trainable weights hence introducing sparsity.

   id98

   utilizing the weights sharing strategy, neurons are able to perform
   convolutions on the data with the convolution filter being formed by
   the weights. this is then followed by a pooling operation which as a
   form of non-linear down-sampling, progressively reduces the spatial
   size of the representation thus reducing the amount of computation and
   parameters in the network.

   existing between the convolution and the pooling layer is an activation
   function such as the relu layer; a [9]non-saturating activation is
   applied element-wise, i.e. thresholding at zero. after several
   convolutional and pooling layers, the image size (feature map size) is
   reduced and more complex features are extracted.

   eventually with a small enough feature map, the contents are squashed
   into a one dimension vector and fed into a fully-connected mlp for
   processing. the last layer of this fully-connected mlp seen as the
   output, is a loss layer which is used to specify how the network
   training penalizes the deviation between the predicted and true labels.

   before we begin lets take look at the mathematical definitions of
   convolution and cross-correlation:

cross-correlation

   given an input image and a filter (kernel) of dimensions , the
   cross-correlation operation is given by:

convolution

   given an input image and a filter (kernel) of dimensions , the
   convolution operation is given by:

   from eq. it is easy to see that convolution is the same as
   cross-correlation with a flipped kernel i.e: for a kernel where .

convolution neural networks - id98s

   id98s consists of convolutional layers which are characterized by an
   input map , a bank of filters and biases .

   in the case of images, we could have as input an image with height ,
   width and channels (red, blue and green) such that . subsequently for a
   bank of filters we have and biases , one for each filter.

   the output from this convolution procedure is as follows:

   the convolution operation carried out here is the same as
   cross-correlation, except that the kernel is    flipped    (horizontally
   and vertically).

   for the purposes of simplicity we shall use the case where the input
   image is grayscale i.e single channel . the eq. will be transformed to:

notation

   to help us explore the forward and id26, we shall make use
   of the following notation:
    1. is the layer where is the first layer and is the last layer.
    2. input is of dimension and has by as the iterators
    3. filter or kernel is of dimension has by as the iterators
    4. is the weight matrix connecting neurons of layer with neurons of
       layer .
    5. is the bias unit at layer .
    6. is the convolved input vector at layer plus the bias represented as
       $$ \begin{align} x_{i,j}^l = \sum_{m}\sum_{n} w_{m,n}^l o_{i + m,j
       + n}^{l-1} + b^l \end{align} $$
    7. is the output vector at layer given by $$ \begin{align} o_{i,j}^l =
       f(x_{i,j}^{l}) \end{align} $$
    8. is the activation function. application of the activation layer to
       the convolved input vector at layer is given by

foward propagation

   to perform a convolution operation, the kernel is flipped and slid
   across the input feature map in equal and finite strides. at each
   location, the product between each element of the kernel and the input
   input feature map element it overlaps is computed and the results
   summed up to obtain the output at that current location.

   this procedure is repeated using different kernels to form as many
   output feature maps as desired. the concept of weight sharing is used
   as demonstrated in the diagram below:

   forward id98

   units in convolutional layer illustrated above have receptive fields of
   size 4 in the input feature map and are thus only connected to 4
   adjacent neurons in the input layer. this is the idea of sparse
   connectivity in id98s where there exists local connectivity pattern
   between neurons in adjacent layers.

   the color codes of the weights joining the input layer to the
   convolutional layer show how the kernel weights are distributed
   (shared) amongst neurons in the adjacent layers. weights of the same
   color are constrained to be identical.

   the convolution process here is usually expressed as a
   cross-correlation but with a flipped kernel. in the diagram below we
   illustrate a kernel that has been flipped both horizontally and
   vertically:

   forward flipped

   the convolution equation of the input at layer is given by:

   this is illustrated below:

   forward convolution

error

   for a total of predictions, the predicted network outputs and their
   corresponding targeted values the the mean squared error is given by:
   $$ \begin{align} e &= \frac{1}{2}\sum_{p} \left(t_p - y_p \right)^2
   \tag {9} \end{align} $$

   learning will be achieved by adjusting the weights such that is as
   close as possible or equals to corresponding . in the classical
   id26 algorithm, the weights are changed according to the
   id119 direction of an error surface .

id26

   for id26 there are two updates performed, for the weights
   and the deltas. lets begin with the weight update.

   we are looking to compute which can be interpreted as the measurement
   of how the change in a single pixel in the weight kernel affects the
   id168 .

   kernel pixel affecting backprop

   during forward propagation, the convolution operation ensures that the
   yellow pixel in the weight kernel makes a contribution in all the
   products (between each element of the weight kernel and the input
   feature map element it overlaps). this means that pixel will eventually
   affect all the elements in the output feature map.

   convolution between the input feature map of dimension and the weight
   kernel of dimension produces an output feature map of size by . the
   gradient component for the individual weights can be obtained by
   applying the chain rule in the following way:

   in eq. is equivalent to and expanding this part of the equation gives
   us:

   further expanding the summations in eq. and taking the partial
   derivatives for all the components results in zero values for all
   except the components where and in as follows:

   substituting eq. in eq. gives us the following results:

   the dual summation in eq. is as a result of weight sharing in the
   network (same weight kernel is slid over all of the input feature map
   during convolution). the summations represents a collection of all the
   gradients coming from all the outputs in layer .

   obtaining gradients w.r.t to the filter maps, we have a
   cross-correlation which is transformed to a convolution by    flipping   
   the delta matrix (horizontally and vertically) the same way we flipped
   the filters during the forward propagation.

   an illustration of the flipped delta matrix is shown below:

   flipped error matrix

   the diagram below shows gradients generated during id26:

   backward id98

   the convolution operation used to obtain the new set of weights as is
   shown below:

   backward convolution

   during the reconstruction process, the deltas are used. these deltas
   are provided by an equation of the form:

   at this point we are looking to compute which can be interpreted as the
   measurement of how the change in a single pixel in the input feature
   map affects the id168 .

   input pixel affecting backprop

   from the diagram above, we can see that region in the output affected
   by pixel from the input is the region in the output bounded by the
   dashed lines where the top left corner pixel is given by and the bottom
   right corner pixel is given by .

   using chain rule and introducing sums give us the following equation:

   in the summation above represents the output region bounded by dashed
   lines and is composed of pixels in the output that are affected by the
   single pixel in the input feature map. a more formal way of
   representing eq. is:

   in the region , the height ranges from to and width to . these two can
   simply be represented by and in the summation since the iterators and
   exists in the following similar ranges from and .

   in eq. is equivalent to and expanding this part of the equation gives
   us:

   further expanding the summation in eq. and taking the partial
   derivatives for all the components results in zero values for all
   except the components where and so that becomes and becomes in the
   relevant part of the expanded summation as follows:

   substituting eq. in eq. gives us the following results:

   for id26, we make use of the flipped kernel and as a result
   we will now have a convolution that is expressed as a cross-correlation
   with a flipped kernel:

pooling layer

   the function of the pooling layer is to progressively reduce the
   spatial size of the representation to reduce the amount of parameters
   and computation in the network, and hence to also control overfitting.
   no learning takes place on the pooling layers [2].

   pooling units are obtained using functions like max-pooling, average
   pooling and even l2-norm pooling. at the pooling layer, forward
   propagation results in an pooling block being reduced to a single value
   - value of the    winning unit   . id26 of the pooling layer
   then computes the error which is acquired by this single value    winning
   unit   .

   to keep track of the    winning unit    its index noted during the forward
   pass and used for gradient routing during id26. gradient
   routing is done in the following ways:
     * max-pooling - the error is just assigned to where it comes from -
       the    winning unit    because other units in the previous layer   s
       pooling blocks did not contribute to it hence all the other
       assigned values of zero
     * average pooling - the error is multiplied by and assigned to the
       whole pooling block (all units get this same value).

conclusion

   convolutional neural networks employ a weight sharing strategy that
   leads to a significant reduction in the number of parameters that have
   to be learned. the presence of larger receptive field sizes of neurons
   in successive convolutional layers coupled with the presence of pooling
   layers also lead to translation invariance. as we have observed the
   derivations of forward and backward propagations will differ depending
   on what layer we are propagating through.

references

    1. dumoulin, vincent, and francesco visin.    a guide to convolution
       arithmetic for deep learning.    stat 1050 (2016): 23. [10][pdf]
    2. lecun, y., boser, b., denker, j.s., henderson, d., howard, r.e.,
       hubbard, w.,jackel, l.d.: id26 applied to handwritten
       zip code recognition. neural computation 1(4), 541   551 (1989)
    3. [11]wikipedia page on convolutional neural network
    4. convolutional neural networks (lenet) [12]deeplearning.net
    5. convolutional neural networks [13]ufldl tutorial

related posts

     * [14]formulating the relu 24 aug 2016
     * [15]vanishing and exploding gradient problems 21 may 2018

   please enable javascript to view the [16]comments powered by disqus.

references

   1. https://www.jefkine.com/atom.xml
   2. https://www.jefkine.com/
   3. https://www.jefkine.com/general/2018/05/21/2018-05-21-vanishing-and-exploding-gradient-problems/
   4. https://www.jefkine.com/
   5. https://www.jefkine.com/about/
   6. https://www.jefkine.com/archive/
   7. https://github.com/jefkine
   8. https://twitter.com/jefkine
   9. http://www.jefkine.com/general/2016/08/24/formulating-the-relu/
  10. https://arxiv.org/pdf/1603.07285.pdf
  11. https://en.wikipedia.org/wiki/convolutional_neural_network
  12. http://deeplearning.net/tutorial/lenet.html
  13. http://ufldl.stanford.edu/tutorial/supervised/convolutionalneuralnetwork/
  14. https://www.jefkine.com/general/2016/08/24/formulating-the-relu/
  15. https://www.jefkine.com/general/2018/05/21/2018-05-21-vanishing-and-exploding-gradient-problems/
  16. https://disqus.com/?ref_noscript
