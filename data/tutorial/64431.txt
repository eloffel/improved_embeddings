   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

   [1*qa8krruuxyqagaeulo75dg.png]

10 applications of id158s in natural language processing

   [9]go to the profile of data monsters
   [10]data monsters (button) blockedunblock (button) followfollowing
   aug 17, 2017

   by olga davydova

   since id158s allow modeling of nonlinear processes,
   they have turned into a very popular and useful tool for solving many
   problems such as classification, id91, regression, pattern
   recognition, dimension reduction, id170, machine
   translation, anomaly detection, decision making, visualization,
   id161, and others. this wide range of abilities makes it
   possible to use id158s in many areas. in this
   article, we discuss applications of id158s in
   natural language processing tasks (nlp).

   nlp includes a wide set of syntax, semantics, discourse, and speech
   tasks. we will describe prime tasks in which neural networks
   demonstrated state-of-the-art performance.

1. text classification and categorization

   text classification is an essential part in many applications, such as
   web searching, information filtering, id46,
   readability assessment, and id31. neural networks are
   actively used for these tasks.

   in [11]convolutional neural networks for sentence classification by
   yoon kim, a series of experiments with [12]convolutional neural
   networks (id98) built on top of [13]id97 was presented. the
   suggested model was tested against several benchmarks. in [14]movie
   reviews (mr) and [15]customer reviews (cr), the task was to detect
   positive/negative sentiment. in [16]stanford sentiment treebank
   (sst-1), there were already more classes to predict: very positive,
   positive, neutral, negative, very negative. in [17]subjectivity data
   set (subj), sentences were classified into two types, subjective or
   objective. in [18]trec the goal was to classify a question into six
   question types (whether the question is about person, location, numeric
   information, etc.) the results of numerous tests described in the paper
   show that after little tuning of hyperparameters the model performs
   excellent suggesting that the pre-trained vectors are universal feature
   extractors and can be utilized for various classification tasks
   [19][1].

   the article [20]text understanding from scratch by xiang zhang and yann
   lecun shows that it   s possible to apply deep learning to text
   understanding from character-level inputs all the way up to abstract
   text concepts with help of temporal [21]convolutional networks
   (convnets) (id98). here, the authors assert that convnets can achieve
   excellent performance without the knowledge of words, phrases,
   sentences and any other syntactic or semantic structures with regards
   to a human language [22][2]. to prove their assertion several
   experiments were conducted. the model was tested on the [23]dbpedia
   ontology classification data set with 14 classes (company, educational
   institution, artist, athlete, office holder, mean of transportation,
   building, natural place, village, animal, plant, album, film, written
   work). the results indicate both good training (99.96%) and testing
   (98.40 %) accuracy, with some improvement from thesaurus augmentation.
   in addition, the [24]id31 test was performed on the
   [25]amazon review data set. in this study, the researchers constructed
   a sentiment polarity data set with two negative and two positive
   labels. the result is 97.57% training accuracy and 95.07% testing
   accuracy. the model was also tested on [26]yahoo! answers comprehensive
   questions and answers data set with 10 classes (society & culture,
   science & mathematics, health, education & reference, computers &
   internet, sports, business & finance, entertainment & music, family &
   relationships, politics & government) and on [27]ag   s corpus where the
   task was a news categorization into four categories (world, sports,
   business, sci/tech.). obtained results confirm that to achieve good
   text understanding convnets require a large corpus in order to learn
   from scratch.

   siwei lai, liheng xu, kang liu, and jun zhao introduced [28]recurrent
   convolutional neural networks for text classification without
   human-designed features in their document [29]recurrent convolutional
   neural networks for text classification [30][3]. the team tested their
   model using four data sets: [31]20newsgroup (with four categories such
   as computers, politics, recreation, and religion), fudan set (a chinese
   document classification set that consists of 20 classes, including art,
   education, and energy), [32]acl anthology network (with five languages:
   english, japanese, german, chinese, and french), and [33]sentiment
   treebank (with very negative, negative, neutral, positive, and very
   positive labels). after testing, the model was compared to existing
   text classification methods like [34]bag of words, [35]bigrams + lr,
   [36]id166, [37]lda, [38]tree kernels, [39]recursivenn, and [40]id98. it
   turned out that neural network approaches outperform traditional
   methods for all four data sets, and the proposed model outperforms id98
   and recursivenn.

2. id39 (ner)

   the main task of id39 ([41]ner) is to classify
   named entities, such as [42]guido van rossum, microsoft, london, etc.,
   into predefined categories like persons, organizations, locations,
   time, dates, and so on. many ner systems were already created, and the
   best of them use neural networks.

   in the paper, [43]neural architectures for id39,
   two models for ner were proposed. the models require character-based
   word representations learned from the supervised corpus and
   unsupervised word representations learned from unannotated corpora
   [44][4]. numerous tests were carried on using different data sets like
   [45]conll-2002 and [46]conll-2003 in english, dutch, german, and
   spanish languages. the team concluded that without a requirement of any
   language-specific knowledge or resources, such as gazetteers, their
   models show state-of-the-art performance in ner.

3. part-of-speech tagging

   part-of-speech ([47]pos) tagging has many applications including
   parsing, text-to-speech conversion, information extraction, and so on.
   in the work, [48]part-of-speech tagging with bidirectional long
   short-term memory recurrent neural network a [49]recurrent neural
   network with id27 for part-of-speech (pos) tagging task is
   presented [50][5]. the model was tested on the [51]wall street journal
   data from id32 iii data set and achieved a performance of
   97.40% tagging accuracy.

4. id29 and id53

   [52]id53 systems automatically answer different types of
   questions asked in natural languages including definition questions,
   biographical questions, multilingual questions, and so on. neural
   networks usage makes it possible to develop high performing question
   answering systems.

   in [53]id29 via staged query graph generation question
   answering with knowledge base wen-tau yih, ming-wei chang, xiaodong he,
   and jianfeng gao described the developed id29 framework for
   id53 using a knowledge base. authors say their method
   uses the knowledge base at an early stage to prune the search space and
   thus simplifies the semantic matching problem[54] [6]. it also applies
   an advanced entity linking system and a deep [55]convolutional neural
   network model that matches questions and predicate sequences. the model
   was tested on [56]webquestions data set, and it outperforms previous
   methods substantially.

5. paraphrase detection

   paraphrase detection determines whether two sentences have the same
   meaning. this task is especially important for id53
   systems since there are many ways to ask the same question.

   [57]detecting semantically equivalent questions in online user forums
   suggests a method for identifying semantically equivalent questions
   based on a [58]convolutional neural network. the experiments are
   performed using the [59]ask ubuntu community questions and answers
   (q&a) site and [60]meta stack exchange data. it was shown that the
   proposed [61]id98 model achieves high accuracy especially when the words
   embedded are pre-trained on in-domain data. the authors compared their
   model   s performance with [62]support vector machines and a
   [63]duplicate detection approach. they demonstrated that their [64]id98
   model outperforms the baselines by a large margin [65][7].

   in the study, [66]paraphrase detection using recursive autoencoder, a
   novel recursive [67]autoencoder architecture is presented. it learns
   phrasal representations using [68]id56s. these
   representations are vectors in an n-dimensional semantic space where
   phrases with similar meanings are close to each other [69][8]. for
   evaluating the system, the [70]microsoft research paraphrase corpus and
   [71]english gigaword corpus were used. the model was compared to three
   baselines, and it outperforms them all.

6. language generation and id57

   id86 has many applications such as automated
   writing of reports, generating texts based on analysis of retail sales
   data, summarizing electronic medical records, producing textual weather
   forecasts from weather data, and even producing jokes.

   in a recent paper, [72]id86, id141 and
   summarization of user reviews with recurrent neural networks,
   researchers describe a [73]recurrent neural network (id56) model capable
   of generating novel sentences and document summaries. the paper
   described and evaluated a database of 820,000 consumer reviews in the
   russian language. the design of the network permits users control of
   the meaning of generated sentences. by choosing sentence-level features
   vector, it is possible to instruct the network; for example,    say
   something good about a screen and sound quality in about ten words   
   [74][9]. the ability of language generation allows production of
   abstractive summaries of multiple user reviews that often have
   reasonable quality. usually, the summary report makes it possible for
   users to quickly obtain the information contained in a large cluster of
   documents.

7. machine translation

   machine translation software is used around the world despite its
   limitations. in some domains, the quality of translation is not good.
   to improve the results researchers try different techniques and models,
   including the neural network approach. the purpose of [75]neural-based
   machine translation for medical text domain study is to inspect the
   effects of different training methods on a polish-english machine
   translation system used for medical data. to train neural and
   statistical network-based translation systems [76]the european
   medicines agency parallel text corpus was used. it was demonstrated
   that a neural network requires fewer resources for training and
   maintenance. in addition, a neural network often substituted words with
   other words occurring in a similar context [77][10].

8. id103

   id103 has many applications, such as home automation,
   mobile telephony, virtual assistance, hands-free computing, video
   games, and so on. neutral networks are widely used in this area.

   in [78]convolutional neural networks for id103, scientists
   explain how to apply [79]id98s to id103 in a novel way,
   such that the [80]id98   s structure directly accommodates some types of
   speech variability like varying speaking rate [81][11]. [82]timit phone
   recognition and a large-vocabulary voice search tasks were used.

9. character recognition

   character recognition systems also have numerous applications like
   receipt character recognition, invoice character recognition, check
   character recognition, legal billing document character recognition,
   and so on. the article [83]character recognition using neural network
   presents a method for the recognition of handwritten characters with
   85% accuracy [84][12].

10. spell checking

   most text editors let users check if their text contains spelling
   mistakes. neural networks are now incorporated into many spell-checking
   tools.

   in [85]personalized spell checking using neural networks a new system
   for detecting misspelled words was proposed. this system is trained on
   observations of the specific corrections that a typist makes [86][13].
   it outwits many of the shortcomings that traditional spell-checking
   methods have.

summary

   in this article, we described natural language processing problems that
   can be solved using neural networks. as we showed, neural networks have
   many applications such as text classification, information extraction,
   id29, id53, paraphrase detection, language
   generation, id57, machine translation, and
   speech and character recognition. in many cases, neural networks
   methods outperform other methods.

resources

   1. [87]http://www.aclweb.org/anthology/d14-1181

   2. [88]https://arxiv.org/pdf/1502.01710.pdf

   3.
   [89]https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552

   4. [90]http://www.aclweb.org/anthology/n16-1030

   5. [91]https://arxiv.org/pdf/1510.06168.pdf

   6. [92]http://www.aclweb.org/anthology/p15-1128

   7. [93]https://www.aclweb.org/anthology/k15-1013

   8. [94]https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf

   9. [95]http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf

   10.[96]
   http://www.sciencedirect.com/science/article/pii/s1877050915025910

   11.
   [97]https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02
   /id98_aslptrans2-14.pdf

   12. [98]http://www.ijettjournal.org/volume-4/issue-4/ijett-v4i4p230.pdf

   13.
   [99]http://www.cs.umb.edu/~marc/pubs/garaas_xiao_pomplun_hcii2007.pdf

     * [100]machine learning
     * [101]neural networks
     * [102]nlp
     * [103]id103
     * [104]machine translation

   (button)
   (button)
   (button) 537 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [105]go to the profile of data monsters

[106]data monsters

   [107]https://datamonsters.com

     * (button)
       (button) 537
     * (button)
     *
     *

   [108]go to the profile of data monsters
   never miss a story from data monsters, when you sign up for medium.
   [109]learn more
   never miss a story from data monsters
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bcf62aa9151a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@datamonsters/artificial-neural-networks-in-natural-language-processing-bcf62aa9151a&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@datamonsters/artificial-neural-networks-in-natural-language-processing-bcf62aa9151a&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@datamonsters?source=post_header_lockup
  10. https://medium.com/@datamonsters
  11. http://www.aclweb.org/anthology/d14-1181
  12. https://en.wikipedia.org/wiki/convolutional_neural_network
  13. https://en.wikipedia.org/wiki/id97
  14. http://www.cs.cornell.edu/people/pabo/movie-review-data/
  15. http://mpqa.cs.pitt.edu/
  16. https://nlp.stanford.edu/sentiment/treebank.html
  17. http://www.cs.cornell.edu/people/pabo/movie-review-data/
  18. http://cogcomp.cs.illinois.edu/data/qa/qc/
  19. http://www.aclweb.org/anthology/d14-1181
  20. https://arxiv.org/pdf/1502.01710.pdf
  21. https://en.wikipedia.org/wiki/convolutional_neural_network
  22. https://arxiv.org/pdf/1502.01710.pdf
  23. http://wiki.dbpedia.org/services-resources/dbpedia-data-set-2014
  24. https://en.wikipedia.org/wiki/sentiment_analysis
  25. https://snap.stanford.edu/data/web-amazon.html
  26. https://webscope.sandbox.yahoo.com/catalog.php?datatype=l
  27. https://www.di.unipi.it/~gulli/ag_corpus_of_news_articles.html
  28. https://en.wikipedia.org/wiki/recurrent_neural_network
  29. https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552
  30. https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552
  31. http://qwone.com/~jason/20newsgroups/
  32. http://clair.eecs.umich.edu/aan/index.php
  33. https://nlp.stanford.edu/sentiment/treebank.html
  34. https://en.wikipedia.org/wiki/bag-of-words_model
  35. http://www.aclweb.org/anthology/o97-1006
  36. https://en.wikipedia.org/wiki/support_vector_machine
  37. https://en.wikipedia.org/wiki/latent_dirichlet_allocation
  38. https://en.wikipedia.org/wiki/tree_kernel
  39. https://en.wikipedia.org/wiki/recursive_neural_network
  40. https://en.wikipedia.org/wiki/convolutional_neural_network
  41. https://en.wikipedia.org/wiki/named-entity_recognition
  42. https://en.wikipedia.org/wiki/guido_van_rossum
  43. http://www.aclweb.org/anthology/n16-1030
  44. http://www.aclweb.org/anthology/n16-1030
  45. http://www.cnts.ua.ac.be/conll2002/
  46. http://www.cnts.ua.ac.be/conll2003/
  47. https://en.wikipedia.org/wiki/part-of-speech_tagging
  48. https://arxiv.org/pdf/1510.06168.pdf
  49. https://en.wikipedia.org/wiki/recurrent_neural_network
  50. https://arxiv.org/pdf/1510.06168.pdf
  51. https://catalog.ldc.upenn.edu/ldc99t42
  52. https://en.wikipedia.org/wiki/question_answering
  53. http://www.aclweb.org/anthology/p15-1128
  54. http://www.aclweb.org/anthology/p15-1128
  55. https://en.wikipedia.org/wiki/convolutional_neural_network
  56. https://nlp.stanford.edu/software/sempre/
  57. https://www.aclweb.org/anthology/k15-1013
  58. https://en.wikipedia.org/wiki/convolutional_neural_network
  59. https://askubuntu.com/
  60. https://meta.stackexchange.com/
  61. https://en.wikipedia.org/wiki/convolutional_neural_network
  62. https://en.wikipedia.org/wiki/support_vector_machine
  63. https://en.wikiversity.org/wiki/duplicate_record_detection
  64. https://en.wikipedia.org/wiki/convolutional_neural_network
  65. https://www.aclweb.org/anthology/k15-1013
  66. https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf
  67. https://en.wikipedia.org/wiki/autoencoder
  68. https://en.wikipedia.org/wiki/recursive_neural_network
  69. https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf
  70. https://www.microsoft.com/en-us/download/details.aspx?id=52398
  71. https://catalog.ldc.upenn.edu/ldc2003t05
  72. http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf
  73. https://en.wikipedia.org/wiki/recurrent_neural_network
  74. http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf
  75. http://www.sciencedirect.com/science/article/pii/s1877050915025910
  76. http://opus.lingfil.uu.se/emea.php
  77. http://www.sciencedirect.com/science/article/pii/s1877050915025910
  78. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/id98_aslptrans2-14.pdf
  79. https://en.wikipedia.org/wiki/convolutional_neural_network
  80. https://en.wikipedia.org/wiki/convolutional_neural_network
  81. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/id98_aslptrans2-14.pdf
  82. https://catalog.ldc.upenn.edu/ldc93s1
  83. http://www.ijettjournal.org/volume-4/issue-4/ijett-v4i4p230.pdf
  84. http://www.ijettjournal.org/volume-4/issue-4/ijett-v4i4p230.pdf
  85. http://www.cs.umb.edu/~marc/pubs/garaas_xiao_pomplun_hcii2007.pdf
  86. http://www.cs.umb.edu/~marc/pubs/garaas_xiao_pomplun_hcii2007.pdf
  87. http://www.aclweb.org/anthology/d14-1181
  88. https://arxiv.org/pdf/1502.01710.pdf
  89. https://www.aaai.org/ocs/index.php/aaai/aaai15/paper/view/9745/9552
  90. http://www.aclweb.org/anthology/n16-1030
  91. https://arxiv.org/pdf/1510.06168.pdf
  92. http://www.aclweb.org/anthology/p15-1128
  93. https://www.aclweb.org/anthology/k15-1013
  94. https://nlp.stanford.edu/courses/cs224n/2011/reports/ehhuang.pdf
  95. http://www.meanotek.ru/files/tarasovds(2)2015-dialogue.pdf
  96. http://www.sciencedirect.com/science/article/pii/s1877050915025910
  97. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/id98_aslptrans2-14.pdf
  98. http://www.ijettjournal.org/volume-4/issue-4/ijett-v4i4p230.pdf
  99. http://www.cs.umb.edu/~marc/pubs/garaas_xiao_pomplun_hcii2007.pdf
 100. https://medium.com/tag/machine-learning?source=post
 101. https://medium.com/tag/neural-networks?source=post
 102. https://medium.com/tag/nlp?source=post
 103. https://medium.com/tag/speech-recognition?source=post
 104. https://medium.com/tag/machine-translation?source=post
 105. https://medium.com/@datamonsters?source=footer_card
 106. https://medium.com/@datamonsters
 107. https://datamonsters.com/
 108. https://medium.com/@datamonsters
 109. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
 111. https://medium.com/p/bcf62aa9151a/share/twitter
 112. https://medium.com/p/bcf62aa9151a/share/facebook
 113. https://medium.com/p/bcf62aa9151a/share/twitter
 114. https://medium.com/p/bcf62aa9151a/share/facebook
