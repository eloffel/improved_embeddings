   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]becoming human: artificial intelligence magazine
     * [9]     consulting
     * [10]     tutorials
     * [11]       submit an article
     * [12]     communities
     * [13]     our bot
     __________________________________________________________________

only numpy: implementing convolutional neural network using numpy ( deriving
forward feed and back propagation ) with interactive code

   [14]go to the profile of jae duk seo
   [15]jae duk seo (button) blockedunblock (button) followfollowing
   jan 20, 2018

   convolutional neural network (id98) many have heard it   s name, well i
   wanted to know it   s forward feed process as well as back propagation
   process. since i am only going focus on the neural network part, i
   won   t explain what convolution operation is, if you aren   t aware of
   this operation please read this    [16]example of 2d convolution    from
   songho it is amazing.
     __________________________________________________________________

   initialize weight, declare hyper parameter, and training data
   [1*-q3rud5ts0air8fmwpjotw.png]

   lets keep things very simple, we have four (3*3) images. (lol well too
   small to call them images but, it   ll do the job). and as you can see in
   the ground truth label data (y), if the image have more 1, the resulted
   output increases. the max is set to 1.1 since we are using logistic
   sigmoid function as final output.

trending ai articles:

     [17]1. making a simple neural network

     [18]2. keras cheat sheet: neural networks in python

     [19]3. a noob   s guide to implementing id56-lstm using tensorflow

   network architecture
   [1*n9vrm-mfdwwvmzjcvhl7zg.jpeg]

   so as seen above, we have a very simple network structure.

   x     3*3 image
   k     convolution operation (right is matrix form, left is vectorization
   form)
   green start     resulted image (right matrix form, left is vectorization
   form)

   if above image is confusing for you please see the image below.
   [0*vuwxvgjznz7fndui.]
   image from [20]grzegorzgwardys

   basically, the 3*3 pixel convolution operation can be thought of
   multiplying certain pixels located in different images with given
   weight.
     __________________________________________________________________

   forward feed
   [1*ywckrbvlryahquswvrxyrq.jpeg]

   when we write the convolution operation in linear line, we can express
   each node as like above. but please take note the orange box, where it
   represents the l1 as a [1*4] vector. also, please look below for what
   each variable in red box represents.
   [1*mj1p4mpxq2t1lzhpzcc3_a.png]

   as seen above, each of the node represent the resulted image of
   convolution operation.
     __________________________________________________________________

   back propagation with respect to w2
   [1*tkmyn45glchvdzssvefjnw.jpeg]

   standard back propagation with sgd, nothing special with above
   operation.
     __________________________________________________________________

   back propagation with respect to w(1,1)
   [1*emfxqhi8udms1eod-tvzgw.jpeg]

   there is a lot going on so, i   ll start from the easiest one.

   orange box / orange star     i did not have enough space to write all of
   the derivative of tanh(), so every    dl    symbol stands for derivative
   respect to tanh().

   blue box     again did not have enough space to write the equation down,
   however simple dot product between vectors.

   green box star 1     the first part of derivative respect to w(1,1) in
   python code implementation it looks like below.
   [1*_ykznvsag53p-r8hgru8-w.png]

   as seen above, we transpose w2, so the dimension change from (1,4) to
   (4,1). and we will use the symbol    g    to represent result of the
   operation.

   green box star 2     dot product between variable g and array of
   derivative dl, so the dimension stays as (1,4).
   [1*02yrn-w1m-gojoaxddhjga.jpeg]
   close up look
   [1*gd9m8zn7fnbxwh_llrodzq.jpeg]
   close up look
     __________________________________________________________________

   back propagation respect to all weight
   [1*swp0bewxxhv2fd-wojenba.jpeg]

   i skipped the derivative notations, but wrote down the actual variable
   that are needed for the derivative. also, please take note the variable
      g    represent the variables in the teal (light green) box.

   take a closer look at all of the equations for the derivative, notice
   something? (especially take a closer look a variable x). it   s a
   convolution operation. what do i mean? see below.
     __________________________________________________________________

   [1*mkc_j4rh40rwgccyojwooa.jpeg]

   the whole derivative can be written like above, convolution operation
   between the input image and derivative respect to all of the nodes in
   layer 1. in python code we can implement it like below.
   [1*hcjfa7zbanmsuhifginhfg.png]

   two things to note here.

   1     grad_1_part_1_reshape: reshaping the vector into (2*2) image
   2     highlighted part is rotating the    grad_1_temp_1    variable   .. why   ?

   lets take a closer look at the kernel as show below.
   [1*oxd4oej0xqas4fgdxmaoia.png]

   as seen above, the matrix is in the below form:

                                                               
   |gdl1(2,2) |gdl1(2,1)|
                                                               
   |gdl1(1,2) |gdl1(1,1)|
                                                               

   however, our vector have variables in this order

   [ gdl1(1,1), gdl1(1,2), gdl1(2,1), gdl1(2,2) ]

   so when we convert the above vector into matrix it will look like
   below.
                                                               
   |gdl1(1,1) |gdl1(1,2)|
                                                               
   |gdl1(2,1) |gdl1(2,2)|
                                                               

   that   s why we rotate the matrix before convolution operation.
     __________________________________________________________________

   update         jan 28 2018

   so the theory is correct however there is one important aspect that i
   have not mentioned yet.
   [1*5unpodzrz8ssbgv-9eavkw.jpeg]

   as seen in the red box, the calculated gradient is in the order of

                                                   
   |w(2,2) | w(2,1)|
                                                   
   |w(1,2) | w(1,1) |
                                                   

   so when updating the weights, we need to transpose the calculated
   gradients once more, and i did not do that in the code.
   [1*g2m46pm_qccynwwdhp9xxq.png]
   how it was
   [1*pwpspandawmq8r0hqrhc_w.png]
   how it should have been

   i updated now, so don   t worry! but if anything is wrong please let me
   know at jae.duk.seo@gmail.com :d
     __________________________________________________________________

   interactive code
   [1*kvhacdbuno8mpn7enptfbw.png]

   please the [21]link here to access the code.
     __________________________________________________________________

final words

   i wanted to cover this for quite a while now, can   t wait to see the
   result if i add id173 or drop out layers.

   if any errors are found, please email me at jae.duk.seo@gmail.com.

   meanwhile follow me on my twitter [22]here, or visit [23]my website, or
   my [24]youtube channel for more content. i also did deriving back
   propagation on simple id56 [25]here if you are interested.
     __________________________________________________________________

   references
    1. ahn, s. h. (n.d.). example of 2d convolution. retrieved january 20,
       2018, from
       [26]http://www.songho.ca/dsp/convolution/convolution2d_example.html
    2. g. (2017, january 14). convolutional neural networks
       id26: from intuition to derivation. retrieved january
       20, 2018, from
       [27]https://grzegorzgwardys.wordpress.com/2016/04/22/8/?blogsub=con
       firming#subscribe-blog
    3. id26 in convolutional neural networks. (2016, september
       05). retrieved january 20, 2018, from
       [28]http://www.jefkine.com/general/2016/09/05/id26-in-co
       nvolutional-neural-networks/
    4. hello, when computing the gradients id98, the weights need to be
       rotated, why ? . (n.d.). retrieved january 20, 2018, from
       [29]https://plus.google.com/111541909707081118542/posts/p8bzbnpg84z
    5. [2]2018. [online]. available:
       [30]https://www.slideshare.net/kuwajima/id98bpa. [accessed: 21- jan-
       2018].

   iframe: [31]/media/c43026df6fee7cdb1aab8aaf916125ea?postid=458a5250d6e4

   [32][1*2f7oqe2ajk1ksrhkmd9zmw.png]
   [33][1*v-ppfkswhbvlwwamsvhhwg.png]
   [34][1*wt2auqisieaozxj-i7brdq.png]

     * [35]machine learning
     * [36]convolutional network
     * [37]neural networks
     * [38]artificial intelligence
     * [39]id26

   (button)
   (button)
   (button) 464 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [40]go to the profile of jae duk seo

[41]jae duk seo

   [42]https://jaedukseo.me | | | | |your everyday seo, who likes kimchi

     (button) follow
   [43]becoming human: artificial intelligence magazine

[44]becoming human: artificial intelligence magazine

   latest news, info and tutorials on artificial intelligence, machine
   learning, deep learning, big data and what it means for humanity.

     * (button)
       (button) 464
     * (button)
     *
     *

   [45]becoming human: artificial intelligence magazine
   never miss a story from becoming human: artificial intelligence
   magazine, when you sign up for medium. [46]learn more
   never miss a story from becoming human: artificial intelligence
   magazine
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://becominghuman.ai/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/458a5250d6e4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://becominghuman.ai/only-numpy-implementing-convolutional-neural-network-using-numpy-deriving-forward-feed-and-back-458a5250d6e4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://becominghuman.ai/only-numpy-implementing-convolutional-neural-network-using-numpy-deriving-forward-feed-and-back-458a5250d6e4&source=--------------------------nav_reg&operation=register
   8. https://becominghuman.ai/?source=logo-lo_iixlm91qr09n---5e5bef33608a
   9. https://becominghuman.ai/artificial-intelligence-software-developers-af09386dc05d
  10. https://becominghuman.ai/tagged/tutorial
  11. https://becominghuman.ai/write-for-us-48270209de63
  12. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  13. http://m.me/becominghumanai
  14. https://becominghuman.ai/@seojaeduk?source=post_header_lockup
  15. https://becominghuman.ai/@seojaeduk
  16. http://www.songho.ca/dsp/convolution/convolution2d_example.html
  17. https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20
  18. https://becominghuman.ai/keras-cheat-sheet-neural-networks-in-python-738c0d170c2e
  19. https://becominghuman.ai/a-noobs-guide-to-implementing-id56-lstm-using-tensorflow-1907a5bbb1fa
  20. https://grzegorzgwardys.wordpress.com/2016/04/22/8/?blogsub=confirming#subscribe-blog
  21. https://trinket.io/python3/b195e28e24
  22. https://twitter.com/jaedukseo
  23. https://jaedukseo.me/
  24. https://www.youtube.com/c/jaedukseo
  25. https://medium.com/@seojaeduk/only-numpy-vanilla-recurrent-neural-network-with-activation-deriving-back-propagation-through-time-4110964a9316
  26. http://www.songho.ca/dsp/convolution/convolution2d_example.html
  27. https://grzegorzgwardys.wordpress.com/2016/04/22/8/?blogsub=confirming#subscribe-blog
  28. http://www.jefkine.com/general/2016/09/05/id26-in-convolutional-neural-networks/
  29. https://plus.google.com/111541909707081118542/posts/p8bzbnpg84z
  30. https://www.slideshare.net/kuwajima/id98bpa.
  31. https://becominghuman.ai/media/c43026df6fee7cdb1aab8aaf916125ea?postid=458a5250d6e4
  32. https://becominghuman.ai/artificial-intelligence-communities-c305f28e674c
  33. https://upscri.be/8f5f8b
  34. https://becominghuman.ai/write-for-us-48270209de63
  35. https://becominghuman.ai/tagged/machine-learning?source=post
  36. https://becominghuman.ai/tagged/convolutional-network?source=post
  37. https://becominghuman.ai/tagged/neural-networks?source=post
  38. https://becominghuman.ai/tagged/artificial-intelligence?source=post
  39. https://becominghuman.ai/tagged/id26?source=post
  40. https://becominghuman.ai/@seojaeduk?source=footer_card
  41. https://becominghuman.ai/@seojaeduk
  42. https://jaedukseo.me/
  43. https://becominghuman.ai/?source=footer_card
  44. https://becominghuman.ai/?source=footer_card
  45. https://becominghuman.ai/
  46. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  48. https://medium.com/p/458a5250d6e4/share/twitter
  49. https://medium.com/p/458a5250d6e4/share/facebook
  50. https://medium.com/p/458a5250d6e4/share/twitter
  51. https://medium.com/p/458a5250d6e4/share/facebook
