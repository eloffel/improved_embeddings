   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 1.2: id119 with math.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   sep 26, 2017

   this story i wanna talk about a famous machine learning algorithm
   called id119 which is used for optimizing the machine
   leaning algorithms and how it works including the math.

   from [12]chapter 1 we know that we need to update m and b values, we
   call them weights in machine learning. lets alias b and m as       0 and
     1 (theta 0 and theta 1 ) respectively.

   first time we take random values for   0 and   1, and we calculate y

   y =   0+  1*x
   in machine learning we say hypothesis so h(x) =   0+  1*x

   h(x)=y but this y is not actual value in our data-set, this is
   predicted y from our hypothesis.

   for example lets say our data-set is something like below and we take
   random values which are 1 and 0.5 for   0 and   1 respectively.
   [1*amvs3zfftpmgpa-kmdnyoq.jpeg]

   from this we calculate the error which is
error = (h(x)-y)   --> (predicted - actual)  
error = (6-5)   = 1
   is to get rid of negative values (what if actual y=6 and py=5)

   we just calculated the error for one data point in our data-set , we
   need to repeat this for all data points in our data set and sum up the
   all errors to one error which is called cost function    j(  )    in machine
   learning.
   [1*xmdblknn9nzmkxgbu3cyhq.png]
   cost function.
   [1*ie6lfet_zgumsxk9n51sta.png]

   our goal is to minimize the cost function (error) we want our error
   close to zero period.

   we have the error 1 for first data-point so lets treat that as whole
   error and reduce to zero for sake of understanding.

   for (h(x)-y)   function we get always positive values and graph will
   look like this(left) and lets plot the error graph.
   [1*ggdwokqoymt32kzugcmgdg.jpeg]

   here is the id119 work comes into the picture.
   [1*sqpjfjbgozk5k6lq9kd-lw.jpeg]
   +   values (left), -   values(right)

   by taking the little steps down to reach the minimum value (bottom of
   the curve) and changing the    values in the process.

     how does it know how much value it should go down???

   the answer is in math.
    1. it draws the line(tangent) from the point.
    2. it finds the slope of that line.
    3. it identifies how much change is required by taking the partial
       derivative of the function with respective to   
    4. the change value will be multiplied with a variable called
       alpha(learning rate) we provide the value for alpha usually 0.01
    5. it subtracts this change value from the earlier    value to get new
          value .

   [1*mpptwbt9er5ka82s7t5juq.jpeg]

   from above picture we can define our   0 and   1.

   and alpha here is a learning rate usually we give 0.01 but it depends,
   it tells how big the step-size is towards reaching the minimum value.
   [1*9amnui7yqmwtwdu0cdx-wq.jpeg]
     0 and   1 values(left),more than two      s (right)

   again we know our j(  0,  1) so if we apply this to above equations for
     0 and   1, we get our new   0 and   1 values.

     how to calculate the derivatives???

   for example f(x) =x       df/dx=2x how ???
   [1*fl31xiqny60s9kustdfmra.jpeg]

     how to calculate the partial derivatives???

   its same as calculating derivatives but here we calculate the
   derivative with respective to that value , others are constants (so
   d/dx(constant)=0)
   [1*0-wrp9fechw2tgb6hqlvwg.jpeg]

   the same thing we can apply for calculating partial derivative with
   respective to   0 and   1.
   [1*mhhppmlbktaacxuwckploa.jpeg]

   how come that box drawn disappeared in the next step above? just wait
   and see.

   for calculating partial derivative with respective to   1 is also same
   as above except one little part is added
   [1*vhuwkngwzyx7ctt-lqpcqq.jpeg]
     0 box disappeared because value is 1 (top)

   so final picture is
   [1*nvh0hlqkgzoh5a9s1mkkng.jpeg]
   final   0 and   1 values

   hope its not confusing , and i know its little bit hard to grasp in the
   beginning but i am sure that this will make sense as you go through
   again and again.

   so that   s it for this story , in the next story i will cover another
   interesting topic in machine learning so see ya!

   update : [13]code for id119 and id75

     * [14]machine learning
     * [15]id119
     * [16]supervised learning
     * [17]regression
     * [18]classification

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 8 (button) (button)

     (button) blockedunblock (button) followfollowing
   [19]go to the profile of madhu sanjeevi ( mady )

[20]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [21]deep math machine learning.ai

[22]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 1k
     * (button)
     *
     *

   [23]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [24]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d4f2871af402
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_hro4xhunpzcc-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_hro4xhunpzcc---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-1-2-gradient-descent-with-math-d4f2871af402&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-1-2-gradient-descent-with-math-d4f2871af402&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/deep-math-machine-learning-ai/chapter-1-complete-linear-regression-with-math-25b2639dde23
  13. https://medium.com/@madhusanjeevi.ai/chapter-1-3-code-for-linear-regression-with-gradient-descent-from-scratch-and-using-libraries-46d07f654cdc
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/gradient-descent?source=post
  16. https://medium.com/tag/supervised-learning?source=post
  17. https://medium.com/tag/regression?source=post
  18. https://medium.com/tag/classification?source=post
  19. https://medium.com/@madhusanjeevi.ai?source=footer_card
  20. https://medium.com/@madhusanjeevi.ai
  21. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  22. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  23. https://medium.com/deep-math-machine-learning-ai
  24. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  26. https://medium.com/p/d4f2871af402/share/twitter
  27. https://medium.com/p/d4f2871af402/share/facebook
  28. https://medium.com/p/d4f2871af402/share/twitter
  29. https://medium.com/p/d4f2871af402/share/facebook
