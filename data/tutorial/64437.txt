   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 3: support vector machine with math.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   sep 29, 2017

   last story we talked about [12]id28 for classification
   problems, this story i wanna talk about one of the main algorithms in
   machine learning which is support vector machine. lets get started!

   id166 can be used to solve both classification and regression problems,
   in this story we talk about classification only(binary classification
   and linear data).

   before we dive into the story i wanna point this

     why id166 ???

   we have couple of other classifiers there, so why do we have to choose
   id166 over any other ??

   well! it does a pretty good job at classification than others. for
   example observe the below image.
   [1*gwiftueyofxgpfl97e9sdg.jpeg]

   we have a data-set which is linearly separable from +   s and            s , we
   can separate the data using id28 (or other) we may get
   something like above (which is reasonable).

   this is how id166 does
   [1*tudh6yvvh7-h5zyf2djv2w.jpeg]

   this is a high level view of what id166 does, the yellow dashed line is
   the line which separates the data (we call this line    decision
   boundary    (hyperplane) in id166), the other two lines (also hyperplanes)
   help us make the right decision boundary.

     so what the heck is hyperplane?

   the answer is    a line in more that 3 dimensions    ( in 1-d it   s called a
   point, in 2-d it   s called a line, in 3-d it   s called a plane, more than
   3 - hyperplane).

     how is id166   s hyperplane different from linear classifiers?

   motivation: maximize margin: we want to find the classifier whose
   decision boundary is furthest away from any data point.we can express
   the separating hyper-plane in terms of the data points that are closest
   to the boundary. and these points are called support vectors.

   we would like to learn the weights that maximize the margin. so we have
   the hyperplane!

   margin is the distance between the left hyperplane and right
   hyperplane.

     peroid.

   these are couple of examples that i ran id166 (written from scratch) over
   different data sets.
   [1*hh1ptoac30u6oseif5rigq.jpeg]

   okay, you get the idea what id166 does , now lets talk about how it
   does???
   [1*eyy0-zmchfvln1prhyznyg.png]
   math

   first i want you to be familiar with the above words so lets complete
   that

vector , its a n dimensional object which has magnitude(length) and
direction, it starts from origin(0,0).

   [1*gbbpqosldi2mc0e2mvbgha.jpeg]
   u and v are vectors

   the magnitude or length of a vector u(o-a distance) is written     u    and
   is called its norm.

   the direction of the vector u is defined by the angle    with respect to
   the horizontal axis, and with the angle    with respect to the vertical
   axis.

   addition and subtraction of vectors
   [1*aviw6czjhs4yrnrhbwmy-w.jpeg]
   addition(left) sub(center and right)

   dot product of vectors
   [1*etqdcsperspibast1_gk7w.jpeg]
   dot product of x and y(finding the    )
   [1*rpurm3wu8xazlr0coxvltg.jpeg]

   if we work on a bit we get this    x      y   cos(  )=x1y1+x2y2

      x      y   cos(  )=x   y

   the orthogonal projection of a vector.
   [1*61xcp9o_t13doutstthrhq.jpeg]
   vector x onto vector y (left) ,new vector z(right)

   why are we interested by the orthogonal projection ? well , it allows
   us to compute the distance between x and the line which goes through y
   (x-z).

the id166 hyperplane

   [1*h2qewsp9-w4rbdiaxfvexg.jpeg]

   the line equation and hyperplane equation         same, its a different way
   to express the same thing,

   it is easier to work on more than two dimensions with the hyperplane
   notation.

   so now we know how to draw a hyperplane with the given dataset,

     so what   s next??

   [1*afbhbyuombwlqktwt7x3rq.jpeg]

   we have a data-set , we want to draw a hyper plane something like above
   (which separates the data well).

     how can we find the optimal hyperplane(yellow line) ?

   if we maximize the margin(distance) between two hyperplanes then divide
   by 2 we get the decision boundary.

     how do we maximize the margin??

   lets take only 2 dimensions, we get the equation for hyper line is

   w.x+b=0 which is same as w.x =0 (which has more dimensions)
   [1*ivpshjgommr0enndoft_jq.jpeg]
   rules for separating dataset
   [1*k6ohxzx2fs5ycs-mckuhzw.jpeg]
   first two are following our rules while others ain   t

     if w.x+b=0 then we get the decision boundary

      the yellow dashed line

     if w.x+b=1 then we get (+)class hyperplane

   for all positive(x) points satisfy this rule (w.x+b    1)

     if w.x+b=-1 then we get (-)class hyperplane

   for all negative(x) points satisfy this rule (w.x+b   -1)

   observe this picture.
   [1*ndzjcki-l7bywc35okpaog.png]

     above both are following our rules so how do i pick the max margin
     one???

   answer : pick one which has minimum magnitude of w
   [1*colcmutp_ln4zvjfrszcqa.jpeg]
   [1*wbd77tmv4dowfa3q05smha.png]
   ||w|| should be minimum.

   this is the final picture designed by me
   [1*6mcff3dedutft21ivll1_q.jpeg]

   so either we save the w and b values and keep going or we adjust the
   parameter (w and b) and keep going. another optimization problem, id166.

   adjusting parameters? sounds like id119 right? yes!!!

   it is a id76 problem which surely gives us global minmum
   value.

   once its optimized we are done!

     so what   s next??

   predicting!
   [1*ts40tecedo8b_fr7t_6ziq.jpeg]

   now we give a unknown point and we want to predict whether it belongs
   to positive class or negative class.
   [1*kuzo37hcxnziq9wfv74evg.jpeg]

   that   s it for this story. hope you enjoyed and learned something.

   we just talked about half of id166 only , the other half is about kernals
   ( we will talk about it later)

   in the next story we will code this algorithm from scratch (without
   using any ml libraries).

   until then

   see ya!

     * [13]machine learning
     * [14]support vector machine
     * [15]classification
     * [16]supervised learning
     * [17]id166

   (button)
   (button)
   (button) 1.1k claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [18]go to the profile of madhu sanjeevi ( mady )

[19]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [20]deep math machine learning.ai

[21]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 1.1k
     * (button)
     *
     *

   [22]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [23]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/47d6193c82be
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_0s4a5rbiqjja-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_0s4a5rbiqjja---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-3-support-vector-machine-with-math-47d6193c82be&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-3-support-vector-machine-with-math-47d6193c82be&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/deep-math-machine-learning-ai/chapter-2-0-logistic-regression-with-math-e9cbb3ec6077
  13. https://medium.com/tag/machine-learning?source=post
  14. https://medium.com/tag/support-vector-machine?source=post
  15. https://medium.com/tag/classification?source=post
  16. https://medium.com/tag/supervised-learning?source=post
  17. https://medium.com/tag/id166?source=post
  18. https://medium.com/@madhusanjeevi.ai?source=footer_card
  19. https://medium.com/@madhusanjeevi.ai
  20. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  21. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  22. https://medium.com/deep-math-machine-learning-ai
  23. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  25. https://medium.com/p/47d6193c82be/share/twitter
  26. https://medium.com/p/47d6193c82be/share/facebook
  27. https://medium.com/p/47d6193c82be/share/twitter
  28. https://medium.com/p/47d6193c82be/share/facebook
