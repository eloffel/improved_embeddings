   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 10.1: deepnlp         lstm (long short term memory) networks with math.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   jan 21, 2018

   note: i am writing this article with the assumption that you know the
   deep learning a bit. in case if you don   t know much, please read my
   earlier stories to understand the entire series on deep learning.

   in the last story we talked about [12]recurrent neural networks, so we
   now know what id56   s are, how they work and what kind of problems it can
   solve and also we talked about a limitation in id56   s which is

     vanishing /exploding gradient problem

   we all know that a neural network uses an algorithm called
   id26 to update the weights of the network. so what bp does
   is

   it first calculates the gradients from the error using the chain rule
   in calculus, then it updates the weights(id119).

   since the bp starts from the output layer to all the way back to input
   layer , in a simple neural network we may not face problems with
   updating weights but in a deep neural network we might face some
   issues.
   [1*-idk9rrxuk_rf3wuv0i7lw.png]
   deep nn

   as we go back with the gradients , it is possible that the values get
   either smaller exponentially which causes vanishing gradient problem or
   larger exponentially which causes exploding gradient problem.

   due to this we get the problems of training the network.

   in id56   s, we have time steps and current time step value depends on the
   previous time step so we need to go all the way back to make an update.
   [1*0jmewsrjx4gzmn00n-s6rq.jpeg]

   there are couple of remedies there to avoid this problem.

   we can use relu unit as an activation function, rms prop as an
   optimization algorithm and lstm   s or gru   s.

   so lets focus on lstm
   [1*tgvpf2964jopwxu9q36a_a.jpeg]

   lstm ( long short term memory ) networks are called fancy recurrent
   neural networks with some additional features.
   [1*fpmg411ck1usfgs0j6te_g.jpeg]
   rolled network

   just like id56, we have time steps in lstm but we have extra piece of
   information which is called    memory    in lstm cell for every time step.

   so the lstm cell contains the following components
    1. forget gate    f    ( a neural network with sigmoid)
    2. candidate layer    c`"(a nn with tanh)
    3. input gate    i    ( a nn with sigmoid )
    4. output gate    o   ( a nn with sigmoid)
    5. hidden state    h    ( a vector )
    6. memory state    c    ( a vector)

   here is the diagram for lstm cell at the time step t
   [1*ghagjzbm7lzz7batvrdx4w.png]

   don   t panic i will explain every single hecking detail of it. just get
   the overall picture stored in your brain.

   lemme take only one time step (t) and explain it.

     what are the inputs and outputs of the lstm cell at any step ??

   inputs to the lstm cell at any step are x (current input) , h (
   previous hidden state ) and c ( previous memory state)

   outputs from the lstm cell are h ( current hidden state ) and c (
   current memory state)

   here is the diagram for a lstm cell at t time step.
   [1*qn_quuusyzozyh3cheoqsa.png]

     how does the lstm flow work??

   if you observe carefully,the above diagram explains it all.

   anyway, lemme also try with words

     forget gate(f) , cndate(c`), input gate(i), output gate(o)

   are single layered neural networks with the sigmoid activation function
   except candidate layer( it takes tanh as activation function)

   these gates first take input vector.dot(u) and previous hidden
   state.dot(w) then concatenate them and apply activation function

   finally these gate produce vectors ( between 0 and 1 for sigmoid, -1 to
   1 for tanh) so we get four vectors f, c`, i, o for every time step.
   [1*r9m-hco4b8dr3r4sj1aktq.jpeg]

   now let me tell you an important piece called memory state c

   this is the state where the memory (context) of input is stored

   ex : mady walks in to the room, monica also walks in to the room. mady
   said    hi    to ____??

   inorder to predict correctly here it stores    monica    into memory c.

   this state can be modified. i mean lstm cell can add /remove the
   information.

   ex : mady and monica walk in to the room together , later richard walks
   in to the room. mady said    hi    to ____??

   the assumption i am making is memory might change from monica to
   richard.

   i hope you get the idea.

   so lstm cell takes the previous memory state ct-1 and does element wise
   multiplication with forget gate (f)

     ct = ct-1*ft

   if forget gate value is 0 then previous memory state is completely
   forgotten

   if forget gate value is 1 then previous memory state is completely
   passed to the cell ( remember f gate gives values between 0 and 1 )

   now with current memory state ct we calculate new memory state from
   input state and c layer.

   ct= ct + (it*c`t)

   ct = current memory state at time step t. and it gets passed to next
   time step.

   here is flow diagram for ct
   [1*tzqti88lgysfucdqccasrw.jpeg]

   finally, we need to calculate what we   re going to output. this output
   will be based on our cell state ct but will be a filtered version. so
   we apply tanh to ct then we do element wise multiplication with the
   output gate o, that will be our current hidden state ht

     ht = tanh(ct)

   we pass these two ct and ht to the next time step and repeat the same
   process.

   here is the full diagram for lstm for different time steps.
   [1*tfzq9uyv33clh9p0x-trwg.jpeg]

   well i hope you get the idea of lstm.

conclusion

   id56    s have been an active research area and many people have been
   achieving amazing results lately using id56   s (most of all are using
   lstms) they really work a lot better for most tasks!

   lstm   s are really good but still face some issues for some problems so
   many people developed other methods also after lstm   s ( hope i can
   cover later stories)
   [1*kky1ioouoh3jsrzgrlfa5a.jpeg]

   so that   s it for this story , in the next story i will be writing about
   more advanced topics. have a great day   .!

   suggestions /questions are welcome.

   photos are designed using paint in windows inspired by christopher olah
   [13]understanding lstms;

   see ya!

     * [14]machine learning
     * [15]deep learning
     * [16]artificial intelligence
     * [17]lstm
     * [18]recurrent neural network

   (button)
   (button)
   (button) 615 claps
   (button) (button) (button) 4 (button) (button)

     (button) blockedunblock (button) followfollowing
   [19]go to the profile of madhu sanjeevi ( mady )

[20]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [21]deep math machine learning.ai

[22]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 615
     * (button)
     *
     *

   [23]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [24]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/21477f8e4235
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_3m2uszuy6cm7-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_3m2uszuy6cm7---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/deep-math-machine-learning-ai/chapter-10-deepnlp-recurrent-neural-networks-with-math-c4a6846a50a2
  13. https://colah.github.io/posts/2015-08-understanding-lstms/
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/deep-learning?source=post
  16. https://medium.com/tag/artificial-intelligence?source=post
  17. https://medium.com/tag/lstm?source=post
  18. https://medium.com/tag/recurrent-neural-network?source=post
  19. https://medium.com/@madhusanjeevi.ai?source=footer_card
  20. https://medium.com/@madhusanjeevi.ai
  21. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  22. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  23. https://medium.com/deep-math-machine-learning-ai
  24. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  26. https://medium.com/p/21477f8e4235/share/twitter
  27. https://medium.com/p/21477f8e4235/share/facebook
  28. https://medium.com/p/21477f8e4235/share/twitter
  29. https://medium.com/p/21477f8e4235/share/facebook
