   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 10: deepnlp - recurrent neural networks with math.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   jan 10, 2018

   we talked about normal neural networks quite a bit, let   s talk about
   fancy neural networks called recurrent neural networks.

   before we talk about what exactly id56   s are, let me first put this    why
   id56   s ???    (i am a big fan of [12]simon sinek, so i start with why.)

   a neural network usually takes an independent variable x (or a set of
   independent variables ) and a dependent variable y then it learns the
   mapping between x and y (we call this training), once training is
   done , we give a new independent variable to predict the dependent
   variable.

   infact that   s all most of machine learning(supervised).

   but what if the order of data matters????? just imagine what if the
   order of all independent variables matter???

   let me explain visually (i call this the recurant theory).
   [1*ziqn6mci2e3d1tdawiukgq.png]

   just assume every ant is an independent variable if one ant goes in a
   different direction , it does not matter for other ants right?

   but what if the order of the ants matters ?
   [1*xi65q53pq8acqiahdfuaqq.jpeg]

   if one ant misses or turns away from the group, it affects the
   following ants.
   [1*ymulxmr9jfo2hajknbu9aq.jpeg]

   so a normal neural network does not follow the order, so when we tackle
   the real world problems where the order matters , we need recurrent
   neural networks. period.
   [1*5xdpnoz2evhysefbcswqww.jpeg]

   so which data where the order matter in our ml space ????
    1. natural language data where the order of words matter
    2. speech data
    3. time series data
    4. video/music sequences data
    5. stock markets data

   etc   .

   so how id56   s solve    the whole order matters thing    data??????

     note: i take natural text data as an example to explain id56   s.

   let   s say i am doing id31 on user reviews on a movie

      this movie is good        positive    this movie is bad        negative

   we can classify these by using simple model    bag of words    and we can
   predict (positive or negative) but wait   

   what if the review is    this movie is not good   

   the bow model may say it   s a positive sign but actually it   s not.

   the id56 understands it and predicts that it   s negative.

how??????

   first let   s admit that here the order of the text matters. cool? okay

   id56 has the following models
    1. one to many

   id56 takes one input lets say an image and generates a sequence of
   words.
   [1*guhbxl-0nkqof9heeo6j0g.jpeg]

   2.many to one

   id56 takes sequence of words as input and generates one output.
   [1*eczr_-elntredwia0js08a.jpeg]

   3.many to many

   id56 takes sequence of words as input and generates sequence of words as
   output. (lets say language translations).
   [1*uuilfslg1i-noshs1vluja.jpeg]

   currently we are focusing on 2nd model    many to one   .

   in id56   s input is considered as time steps.

   ex : input(x) = [   this   ,    movie   ,    is   ,    not   ,    good   ]

   time stamp for    this    is x(0),    movie    is x(1),    is    is x(2) ,   not    is
   x(3) and    good    is x(4).

   first let   s understand what id56 cell contains!

   i hope and assume you know feed forward nn or you can read my earlier
   story here nn. summary of ffnn is
   [1*iir7xfhd9gkamtjo2h0k8a.jpeg]
   feed forward nn.

   in feed forward neural network we have x(input) and h(hidden) and
   y(output)

   you can have as many hidden layers as you want but weights (w)for every
   hidden layers are different.

   above wh1 and wh2 are different.

   the id56 cell contains a set of feed forward neural networks cause we
   have time steps.

   the id56 has: sequential input, sequential output, multiple timesteps,
   and multiple hidden layers.

   unlike ffnn , here we calculate hidden layer values not only from input
   values but also previous time step values and weights ( w ) at hidden
   layers are same for time steps.

   here is the complete picture for id56 and it   s math.
   [1*szgydyddpfvdldlas1zzvw.jpeg]

   in the picture we are calculating the hidden layer time step (t) values
   so

   ht = activatefunction(input * hweights + w * ht-1)

   yt = softmax(hweight* ht)

   ht-1 is the previous time step and as i said w   s are same for all
   timesteps.

   the activation function can be tanh, relu, sigmoid, etc..
   [1*trk5q_btqrsb_lymvk8gnq.jpeg]

   above we calculated only for ht similarly we can calculate for all
   other timesteps.

   steps:
    1. calculate ht-1 from u and x
    2. calculate yt-1 from ht-1 and v
    3. calculate ht from u,x,w and ht-1
    4. calculate yt from v and ht and so on   

   note :

   1.u and v are weight vectors, different for every time step.

   2.we can even calculate hidden layer( all time steps ) first then
   calculate y values.

   3. weight vectors are random initially.

   once feed forwarding is done then we need to calculate the error and
   backpropagate the error using back propagation.

   we use cross id178 as cost function ( assume you know so not going
   into details)

     bptt ( back propagation through time )

   if you know how normal neural network works , the rest is pretty easy ,
   if you don   t know, here is my article that talks about [13]artificial
   neural networks.

   we need to calculate the below terms
    1. how much does the total error change with respect to the output
       (hidden and output units) ? (or how much is a change in output)
    2. how much does the output change with respect to weights (u,v,w)?
       (or how much is a change in weights)

   since w   s are same for all time steps we need to go all the way back to
   make an update.
   [1*jn9y70ojwkevripnl0zleq.jpeg]

   remember the bp for id56 is as same as neural networks bp

   but here current time step is calculated based on the previous time
   step so we have to traverse all the way back.

   if we apply chain rule which looks like this
   [1*acwcgpzh9xk8kbtqrmbrra.jpeg]

   w   s are same for all the time steps so the chain rule expands more and
   more

   a similar but a different way of working out the equations can be seen
   in richard sochers   s [14]recurrent neural network lecture slide.
   [1*zkc1eploylylub-sug3w6a.jpeg]

   so here et is same as our j(   )

   u, v and w should get updated using any optimization algorithms like
   id119 ( take a look at my story here[15] gd).

   now if we go back and talk about our sentiment problem here is the id56
   for that
   [1*tzf9tvcsnt3lduvdmh7ngq.jpeg]

   we give word vectors or one hot encoding vectors for every word as
   input and we do feed forward and bptt ,once the training is done, we
   can give new text for prediction.

   it learns something like whereever    not    + positive word = negative.

   i hope you get that.
   [1*dytyazq2pumdiyz-kate3q.jpeg]

   problems with id56     vanishing/exploding gradient problem

   since w   s are same for all timesteps, during back propagation as we go
   back adjusting the weights, the signal gets either too weak or too
   strong which cause either vanishing or exploding problem.

   to avoid this we use either gru or lstm which i will cover in the next
   stories

   so that   s it for this story , in the next story i will build the
   recurrent neural network from scratch and using tensorflow using the
   above steps and same math.

   suggestions /questions are welcome.

   photos are designed using paint in windows.

   see ya!

     * [16]machine learning
     * [17]deep learning
     * [18]recurrent neural network
     * [19]nlp
     * [20]artificial intelligence

   (button)
   (button)
   (button) 593 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [21]go to the profile of madhu sanjeevi ( mady )

[22]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [23]deep math machine learning.ai

[24]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 593
     * (button)
     *
     *

   [25]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [26]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/c4a6846a50a2
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_nujik0vs2nus-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_nujik0vs2nus---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-10-deepnlp-recurrent-neural-networks-with-math-c4a6846a50a2&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-10-deepnlp-recurrent-neural-networks-with-math-c4a6846a50a2&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://www.ted.com/talks/simon_sinek_how_great_leaders_inspire_action
  13. https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b
  14. http://cs224d.stanford.edu/lectures/cs224d-lecture7.pdf
  15. https://medium.com/deep-math-machine-learning-ai/chapter-1-2-gradient-descent-with-math-d4f2871af402
  16. https://medium.com/tag/machine-learning?source=post
  17. https://medium.com/tag/deep-learning?source=post
  18. https://medium.com/tag/recurrent-neural-network?source=post
  19. https://medium.com/tag/nlp?source=post
  20. https://medium.com/tag/artificial-intelligence?source=post
  21. https://medium.com/@madhusanjeevi.ai?source=footer_card
  22. https://medium.com/@madhusanjeevi.ai
  23. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  24. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  25. https://medium.com/deep-math-machine-learning-ai
  26. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  28. https://medium.com/p/c4a6846a50a2/share/twitter
  29. https://medium.com/p/c4a6846a50a2/share/facebook
  30. https://medium.com/p/c4a6846a50a2/share/twitter
  31. https://medium.com/p/c4a6846a50a2/share/facebook
