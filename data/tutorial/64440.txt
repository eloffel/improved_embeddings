   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 9.2: nlp- code for id97 neural network(tensorflow).

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   oct 28, 2017

   last story we talked about [12]word vectors , this story we write the
   code to build the id97 model using tensorflow let   s get started!!!
   [1*y6dozikoj1zs_pzbzg6rgg.jpeg]
   overview

   let   s first take a data set ( unstructured data ) , i take here is how
   i met your mother series subtitles data , you can take any data ( it
   does not matter).
   [1*jxiapdma9k25_cdorhcqug.jpeg]

   so the sentence column is the actual raw data , we need to normalize
   the data ( removing symbols, spaces and etc   )
   [1*52p8bt53qu3n6zvh9i4n9w.jpeg]

   so now we have the clean data with us , let   s create a dictionary out
   of these sentences ( here i take unigrams (1 word) ) you can take
   bi-grams also,
   [1*xzpjwpyclyxpzjdlvk9toq.jpeg]

   here we first identified the unique words by taking the count then we
   created the dictionary. ( word order depends based on the count).

   let me take the first sentence in our data set to see how it looks
   [1*nyqhqo3sxnsmsb1nxhj00a.jpeg]

   we just replaced the words with numbers , remember these numbers are
   not the vectors , these are just the indexes in our dictionary.

   now let   s create the id27s ( word2 vec)
   [1*yttw_pbq-amvjstfyqgeqw.jpeg]

   to build the model we can use skip gram ( as we discussed in the last
   story)

     continuous bag-of-words model (cbow)

   it predicts one word based on the surrounding words (it takes an entire
   context as an observation or takes a window sized context as an
   observation)

   ex: text=    mady goes crazy about machine leaning    and window size is 3

   it takes 3 words at a time predicts the center word based on the
   surrounding words     [ [   mady   ,   crazy    ] ,    goes   ]        goes    is the
   target word , and the other two are inputs.

     skip-gram model

   it takes one word as input and try to predict the surrounding
   (neighboring) words,

   [   mady   ,    goes   ],[   goes   ,   crazy   ]        goes    is the input word and    mady   
   and    crazy    are the surrounding words (output probabilities)
   [1*-ezemcljb6-tdgl4auc1fg.jpeg]

   now we have the skip gram pairs as x and y values , lets create a
   functions that gives us the pairs batch wise
   [1*frrolvvkmhipxtfawjxdcg.jpeg]

   now we have the batch inputs to feed to neural network so let   s build
   the neural network using tensorflow
   [1*ncp-mgyddiebjtzu9yixea.jpeg]

   as we dicussed in the last story , id97 model has a 3 layer neural
   network (input , hidden and output).

   here hidden layer is just the dot product of inputs and weights ( no
   activation function here)

   output layer has an activation function ( here it is
   [13]noise-contrastive estimation ) we can use softmax also here but nce
   is good you can read about it in that paper(nce internally uses the
   softmax).

   and we can define the size of hidden layer (here i took [voc_size,
   embedding_size(2)] but you can choose as your wish.

   okay let   s train the model for 10000 iterations
   [1*lmydsch_tmi4oagvbrvxnw.jpeg]

   so the error is decreased from 66 to 5 which is okay , now the model
   learned the weights so we got the final embeddings

   let   s visualize them
   [1*rgdejzaojhtve_79mb5yia.jpeg]

   so here are the first 100 words in vector space

   the main theme of id97 is we get the similar vectors for the words
      india   ,    china   ,    america   ,    germany   , and etc    from a big corpus.

   even if we are not labeling or telling that those are country names.

   so if i give a text like     i live in ____    we can get the predictions
   like    india   ,    china   ,    america   ,    germany   , and etc   

   if we try with different data we get this
   [1*xih92cvxivac8ibaqpfhvw.jpeg]

   based on the data , it understands the similar world (idea is similar
   words appear in similar contexts)
   [1*8ka90dejqpie7z_nuegceg.png]

   the more data, the better results.

   well, that   s it for this story.

   in the next story i will cover another interesting nlp/deep leaning
   concept until then see ya!

   full code is available on my [14]github.

     * [15]machine learning
     * [16]nlp
     * [17]id97
     * [18]artificial intelligence
     * [19]tensorflow

   (button)
   (button)
   (button) 323 claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [20]go to the profile of madhu sanjeevi ( mady )

[21]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [22]deep math machine learning.ai

[23]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 323
     * (button)
     *
     *

   [24]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [25]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/544db99f5334
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_hbiozlffrsuw-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_hbiozlffrsuw---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-9-2-nlp-code-for-id97-neural-network-tensorflow-544db99f5334&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-9-2-nlp-code-for-id97-neural-network-tensorflow-544db99f5334&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1
  13. http://www.jmlr.org/proceedings/papers/v9/gutmann10a/gutmann10a.pdf
  14. https://github.com/madhu009/deep-math-machine-learning.ai/tree/master/nlp
  15. https://medium.com/tag/machine-learning?source=post
  16. https://medium.com/tag/nlp?source=post
  17. https://medium.com/tag/id97?source=post
  18. https://medium.com/tag/artificial-intelligence?source=post
  19. https://medium.com/tag/tensorflow?source=post
  20. https://medium.com/@madhusanjeevi.ai?source=footer_card
  21. https://medium.com/@madhusanjeevi.ai
  22. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  23. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  24. https://medium.com/deep-math-machine-learning-ai
  25. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  27. https://medium.com/p/544db99f5334/share/twitter
  28. https://medium.com/p/544db99f5334/share/facebook
  29. https://medium.com/p/544db99f5334/share/twitter
  30. https://medium.com/p/544db99f5334/share/facebook
