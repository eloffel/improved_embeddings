   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 9.1 : nlp - word vectors.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   oct 21, 2017

   last story we talked about the basic fundamentals of natural language
   processing and id174, this story we talk about how
   documents will be converted to vectors of values.

   let   s get going!!!!

   in the last story only we talked about count vectorization and how the
   documents are converted into vectors with the help of count vectorizer.
   [1*yxy_txtmtttw85vv05jdrq.jpeg]

   this is how the documents are converted into vectors.

   just taking count doesn   t help in real data, because we can have words
   repeated many times in one document while not in other documents.

   then we might get high count vectors for one document and low count
   vectors for others. this can be a problem, so to solve this problem we
   use a technique called tf-idf(term frequency-inverse term frequency).

     tf-idf(term frequency-inverse term frequency)

   tf-idf is a weighting factor which is used to get the important
   features from the documents(corpus).

   it actually tells us how important a word is to a document in a corpus,
   the importance of a word increases proportionally to the number of
   times the word appears in the individual document, this is called term
   frequency(tf).

   ex : document 1:

       mady loves programming. he programs all day, he will be a world class
   programmer one day    

   if we apply id121, steeming and stopwords (we discussed in the
   last story) to this document, we get features with high count like    
   program(3), day(2),love(1) and etc   .

   tf = (no of times the word appear in the doc) / (total no of words in
   the doc)

   here program is the highest frequent term in the document.

   so program is a good feature if we consider tf.

   however, if multiple documents contain the word    program    many times
   then we might say   

   it   s also a frequent word in all other documents in our corpus so it
   does not give much meaning so it probably may not be an important
   feature.

   to adjust this we use idf.

   the inverse document frequency is a measure of how much information the
   word provides, that is, whether the term is common or rare across all
   documents.

   idf         log(total no of documents / no of documents with the term t in
   it).

   so tf-idf = tf * idf.

   you can read more about it in [12]wiki

   so finally by using tf-idf we get the most important features from the
   documents(corpus) with weights.

   here is the comparison between count vectorizer and tfidf vectorizer
   based on the last story discussion and data.
   [1*-azmvsiextbxiphcvniing.jpeg]

   we can use scikit learn to this job
   [1*pvvahh_a69lhyxboywk_cq.jpeg]

   if we take one word as a feature , that   s called unigram. if we take
   two words at a time as a feature , that   s called bigram. three words at
   a time as a feature , that   s called trigram

   if we take n words at a time as a feature , that   s called id165
   [1*mi8dsrjoiuhpaj_fz2hx8q.jpeg]
   with bigrams.

   these values make more sense(adds much meaning) than just count values.

   this table is called a sparse matrix ( a lot of zeros ) , we can now
   pass this table as x_train to any machine learning model to train.
   [1*bd9o3uhxnoxjaea-vod9iq.jpeg]

   let   s come back

   the count vectorizer and tfidf vectorizer focus only on the count, they
   just only take care of how many times a word appeared in the corpus ,
   they just don   t care about the order for example

       this movie is good    

      good is the movie   

   if we use count vectorizer or tfidf vectorizer, we get the same vectors
   for example , let   s say we have good and movie are features and use
   (count vec)
   [1*eikmd_kxtkb4gkqtentqdg.jpeg]

   these don   t understand the meaning so we don   t get useful
   representations.

   to get the meaningful numerical representations, we use one of the
   important model in natural language processing which is id97.

   let   s just forget about tfidf for while and start with fresh mind, as
   we know if we want to feed words into machine learning models, we need
   to convert the words into some set of numeric vectors.

   a good way of doing this would be to use a    one-hot    method of
   converting the word into a sparse representation with only one element
   of the vector set to 1, the rest being zero.

   but it would be extremely costly as the vocabulary gets increased and
   miss the meaning of the words.

   for example if vocabulary size is 10000 then we have a 10000 sized
   vector for every word in the document for all documents in corpus.

   word 2 vec takes care of two things
    1. converts this high dimensional vector (10000 sized) into low
       dimensional vector (let   s say 200 sized)

   the conversion of 10,000 columned matrix into a 200 columned matrix is
   called id27.
   [1*ixfyhcmpwesjpbqjqhfl4g.jpeg]

   2. maintains the word context (meaning)
   [1*sx0l5hom7wbenxqghhnfaw.jpeg]

   the word context / meaning can be created using 2 simple algorithms
   which are
    1. continuous bag-of-words model (cbow)

   it predicts one word based on the surrounding words (it takes an entire
   context as an observation or takes a window sized context as an
   observation)

   ex: text=    mady goes crazy about machine leaning    and window size is 3

   it takes 3 words at a time predicts the center word based on the
   surrounding words     [ [   mady   ,   crazy    ] ,    goes   ]        goes    is the
   target word , and the other two are inputs.

   2. skip-gram model

   it takes one word as input and try to predict the surrounding
   (neighboring) words,

   [   mady   ,    goes   ],[   goes   ,   crazy   ]        goes    is the input word and    mady   
   and    crazy    are the surrounding words (output probabilities)

     word 2 vec works how?

   we can use any of these models to get the word vectors , let   s use skip
   gram model as it performs better, so here is how word2 vec works
    1. take a 3 layer neural network with 1 input layer , 1 hidden layer
       and 1 output layer

   [1*a1mrbydokpb2q_zfz4ue4g.png]

   assume we have 10000 unique words in dictionary so for one word    goes   
   we get a 10000 sized vector as an input then we take 200 neurons and do
   the neural network training for all words using skip gram model ( i
   have already talked about neural network training here)

   once the training is complete , we get the final weights for hidden
   layer and output layer

   2. ignore the last (output layer) and keep the input and hidden layer.

   so we get the 200 sized weights( scores ) for every word

   now, input a word from within the vocabulary. the output given at the
   hidden layer is the    id27    of the input word.
   [1*q8qat4ksifpm2ahg_oge0g.jpeg]

   simply word 2 vec

       it   s a neural network training for all the words in our dictionary to
   get the weights(vectors )

       it has id27s for every word in the dictionary

     what   s the ultimate result for word 2 vec?

   well, we get the similar vectors for similar words for ex:

   we get the similar vectors for the words    india   ,    china   ,    america   ,
      germany   , and etc    from a big corpus.

   even if we are not labeling or telling that those are country names.

   so if i give a text like     i live in ____    we can get the predictions
   like    india   ,    china   ,    america   ,    germany   , and etc   

   that   s the power of word 2 vec.

   that   s it for this story. hope you got an idea about how words can be
   represented numerically.

   in the next story we will discuss how to write the code for word2 vec
   and the neural network training step by step with some more examples

   until then see ya!!!

     * [13]machine learning
     * [14]id97
     * [15]naturallanguageprocessing
     * [16]artificial intelligence
     * [17]skip gram

   (button)
   (button)
   (button) 323 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [18]go to the profile of madhu sanjeevi ( mady )

[19]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [20]deep math machine learning.ai

[21]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 323
     * (button)
     *
     *

   [22]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [23]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d51bff9628c1
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_r52cm3we7hfh-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_r52cm3we7hfh---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-9-1-nlp-word-vectors-d51bff9628c1&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://en.wikipedia.org/wiki/tf   idf
  13. https://medium.com/tag/machine-learning?source=post
  14. https://medium.com/tag/id97?source=post
  15. https://medium.com/tag/naturallanguageprocessing?source=post
  16. https://medium.com/tag/artificial-intelligence?source=post
  17. https://medium.com/tag/skip-gram?source=post
  18. https://medium.com/@madhusanjeevi.ai?source=footer_card
  19. https://medium.com/@madhusanjeevi.ai
  20. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  21. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  22. https://medium.com/deep-math-machine-learning-ai
  23. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  25. https://medium.com/p/d51bff9628c1/share/twitter
  26. https://medium.com/p/d51bff9628c1/share/facebook
  27. https://medium.com/p/d51bff9628c1/share/twitter
  28. https://medium.com/p/d51bff9628c1/share/facebook
