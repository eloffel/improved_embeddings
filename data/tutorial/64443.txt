   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 8 .1: code for convolutional neural networks(tensorflow and
keras-theano).

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   oct 16, 2017

   last story we talked about [12]convolutional neural networks, this
   story we will build the convoultional neural network using both
   tensorflow and keras (backed by theano).

     let   s get started!!!!!!

   [1*snfprf1v-prt8nchii27zg.jpeg]

   first let   s take a problem and the dataset, we will take image
   classification as problem and use cifar10 image dataset. you can
   download [13]here.
   [1*qalsbzt3owjxfqzbtdy_gq.jpeg]

   we have 10 classes and a total of 50000 training and 10000 testing
   images.

   let   s take the dataset into the code. in keras , we can get the data by
   calling this function.
   [1*b9m6747vrdreldtquk-evw.jpeg]

   let   s understand the data and how it   s represented
   [1*dsiwcztnnge8unkjr-24sq.jpeg]

   so every image 3 x 32 x 32 (colors, width and height) and the values
   are in the image are pixel values (0   255). and there are 50000 images.
   [1*durloabx_r0juk86gylohq.jpeg]

   let   s convert the pixel values into between 0 and 1 to make the math
   easy.
   [1*k_kherxo395poxl4hv0fyq.jpeg]

   now let   s see how labels are represented
   [1*nnjaeq0eikeivh2vnyheag.jpeg]

   labels are class numbers (class 0, class 1 ,    class 9) so we need to
   convert into one hot vector (the vector length is 10 as we have 10
   classes) so at the end we get the probabilities score for every class,
   we pick the maximum score.

   here class 6 is the label so 1 is stored at 6th index, remaining all
   0's.

   so the basic data understanding is done. now let   s pick any deep
   learning framework to implement the convolutional neural network.

     tensorflow

   first let   s pick the architecture , i am gonna choose this architecture
   (you can try any other architecture)
   [1*_cjmsdebnl_yl5p2mwkmvg.jpeg]
   [1*1m23hicutpzbcuiaqdiyoq.jpeg]

   placeholders for the input and output of the network. placeholders are
   variables which we need to fill in when we are ready to compute the
   graph.

   and weights for convolution 1 ,2,3 and fully connected are initialized
   with the random numbers.

   let   s write the code to build the model
def model(x, w_c1, w_c2, w_c3, w_fc, w_o,p_keep_conv,p_keep_hidden):

    c1 = tf.nn.relu(tf.nn.conv2d(x,w_c1,
                                strides=[1,1,1,1], padding ="same"))

    p1 = tf.nn.max_pool(c1,ksize=[1,2,2,1],
                         strides=[1,2,2,1], padding = "same" )

    d1 = tf.nn.dropout(p1,p_keep_conv) # 1st dropout at conv

    c2 = tf.nn.relu(tf.nn.conv2d(d1,w_c2,
                                strides=[1,1,1,1], padding ="same"))

    p2 = tf.nn.max_pool(c2,ksize=[1,2,2,1],
                         strides=[1,2,2,1], padding = "same" )

    d2 = tf.nn.dropout(p2,p_keep_conv) # 2nd dropout at conv


    c3 = tf.nn.relu(tf.nn.conv2d(d2,w_c3,
                                strides=[1,1,1,1], padding ="same"))
    p3 = tf.nn.max_pool(c3,ksize=[1,2,2,1],
                         strides=[1,2,2,1], padding = "same" )

    p3 = tf.reshape(p3, [-1, w_fc.get_shape().as_list()[0]])
    d3 = tf.nn.dropout(p3, p_keep_conv) # 3rd dropout at conv

    fc = tf.nn.relu(tf.matmul(d3,w_fc))
    fc = tf.nn.dropout(fc, p_keep_hidden) #dropout at fc

    output = tf.matmul(fc,w_o)

    return output

   we defined the model according to the diagram , here we used dropout ,
   the dropout is used to prevent overfitting of the data, in the
   network , some % of neurons(random) don   t get computed.

   next let   s define the cost function ,optimizer and predicted function.
y_pred = model(x, w_c1, w_c2, w_c3, w_fc, w_o,p_keep_conv,p_keep_hidden)  # call
ing the above model.
cost = tf.reduce_mean(tf.nn.softmax_cross_id178_with_logits(logits = y_pred ,l
abels = y))
optimizer = tf.train.rmspropoptimizer(0.001, 0.9).minimize(cost)
predict_op = tf.argmax(y_pred, 1)

   we used here rmsprop as optimizer, but we can try others also and
   learning rate is 0.001, momentum=0.9.

   one more thing i forgot which is we need to reshape our data according
   to tensorflow,
#reshape as images according to tf
x_train = x_train.reshape(-1,32,32,3)
x_test = x_test.reshape(-1,32,32,3)

   all the variables done.

   now we need to create a session in tensorflow to run the code.
   [1*ah7oxti5a4rddbasdwmflg.jpeg]

   for every 10 epochs let   s try our test data to see the accuracy.
   [1*jr-x_mqtqms7sr0e8pslja.jpeg]

   so after a few hours of laptop heating ( if cpu ) we would get the
   trained model with the accuracy.
   [1*cqzbtdrywck_wxbpznvvpw.jpeg]

   ting!!!
   [1*rdvrd-iaxpowwxsxck-dmg.jpeg]

   we can increase the accuracy by just playing with network(trying out
   different numbers for neurons and filters), it took me 2 hours to train
   the model( as i have other models also running at the same time) so i
   am not gonna try other, if you wish you can try, to increase the
   accuracy.

   cifar 10 can get even more than 90% accuracy, if you wanna improve the
   accuracy , please follow these [14]papers (there are many ways to
   improve).
   [1*_80nc81slq11pxcgsuqwpg.jpeg]

   let   s try out another model using keras(backed by theano).

     keras

   okay let   s try out a simple model of 2 convolution layer , 1 pooling
   layer and a fully connected layer.
   [1*tcjvjl8dolmjfgnscku_-a.jpeg]

   note: i write only required code , full code is on my [15]github.
def model():
    model=sequential()
    model.add(convolution2d(32,3,3,activation='relu',input_shape       (3,32,32)
,border_mode='same',w_constraint=maxnorm(3)))

    model.add(dropout(0.2))
    model.add(convolution2d(32,3,3,activation='relu',input_shape=(3,32,32),borde
r_mode='same',w_constraint=maxnorm(3)))

    model.add(maxpooling2d(pool_size=(2,2),strides=(2,2)))

    model.add(flatten())
    model.add(dense(512,activation='relu',w_constraint=maxnorm(3)))
    model.add(dropout(0.5))
    model.add(dense(10,activation='softmax'))

    return model

   let   s call the model and check the network architecture.
   [1*nfer6q8x6wxj6hjk4e-s2g.jpeg]

   let   s do the training by calling the fit method.
model.fit(x_train, y_train, validation_data=(x_test, y_test), nb_epoch=epochs, b
atch_size=32)
# final evaluation of the model
scores = model.evaluate(x_test, y_test, verbose=0)
print("accuracy: %.2f%%" % (scores[1]*100))

   [1*dkl_kysghnabhyszsat6ta.jpeg]

   woww!!!!!!!!!!

96 % accuracy. that   s hecking awesome.

   [1*cx6gso_a2m6zjidfxkyiwq.jpeg]

   you can still try to improve it don   t give up.

   that   s it for this story. hope you enjoyed and learned something let me
   know if it helps .

   in the next story i will cover another interesting machine leaning
   concept until then see ya!

   full code is available on my [16]github.

     * [17]machine learning
     * [18]deep learning
     * [19]artificial intelligence
     * [20]neural networks
     * [21]tensorflow

   (button)
   (button)
   (button) 107 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [22]go to the profile of madhu sanjeevi ( mady )

[23]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [24]deep math machine learning.ai

[25]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 107
     * (button)
     *
     *

   [26]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [27]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/33bef285dd93
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_xe5mp6znzow5-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_xe5mp6znzow5---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-8-1-code-for-convolutional-neural-networks-tensorflow-and-keras-theano-33bef285dd93&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-8-1-code-for-convolutional-neural-networks-tensorflow-and-keras-theano-33bef285dd93&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/deep-math-machine-learning-ai/chapter-8-0-convolutional-neural-networks-for-deep-learning-364971e34ab2
  13. https://www.cs.toronto.edu/~kriz/cifar.html
  14. http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html
  15. https://github.com/madhu009/deep-math-machine-learning.ai/tree/master/neural_networks
  16. https://github.com/madhu009/deep-math-machine-learning.ai/tree/master/neural_networks
  17. https://medium.com/tag/machine-learning?source=post
  18. https://medium.com/tag/deep-learning?source=post
  19. https://medium.com/tag/artificial-intelligence?source=post
  20. https://medium.com/tag/neural-networks?source=post
  21. https://medium.com/tag/tensorflow?source=post
  22. https://medium.com/@madhusanjeevi.ai?source=footer_card
  23. https://medium.com/@madhusanjeevi.ai
  24. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  25. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  26. https://medium.com/deep-math-machine-learning-ai
  27. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  29. https://medium.com/p/33bef285dd93/share/twitter
  30. https://medium.com/p/33bef285dd93/share/facebook
  31. https://medium.com/p/33bef285dd93/share/twitter
  32. https://medium.com/p/33bef285dd93/share/facebook
