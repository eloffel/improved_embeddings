   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 7 : id158s with math.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   oct 11, 2017

   i have been talking about the machine learning for a while, i wanna
   talk about deep learning as i got bored of ml.

   so this article we will talk about neural networks which are part of
   deep learning which is part of machine learning. let   s get
   started!!!!!!

   note: understanding of math from previous article [12]gd is required a
   bit.

   as usual before i dive into deep learning, i wanna point this.

why deep learning???

we already have a lot of algorithms in machine learning which we still don   t
understand many, so why there is a pain of leaning new called    deep
learning    ?????

   well, there are plenty of reasons for why   s from researchers and other
   scientists, as a machine learning scientist i believe in few which are
   below
    1. deep learning is preferred than shallow level learning when you
       have enormous amount of data (either labeled or not).
    2. awesome state-of-the-art performance in tasks involving text,
       sound, or image. many advances in id161,nlp and speech
       recognition.
    3. feature representation or abstract representation, we don   t need to
       spend time on feature engineering much.

   etc    there are a lot actually

   note: just because dl is cool, does not mean that we don   t need to use
   ml techniques,(based data and problem we (   i) choose models,
   algorithms , frame works and tools)
   [1*hhzopkipsjtcup5dr36yca.jpeg]
   don   t get caught up!

   okay man got it let   s go ahead and tell us about neural networks.

   id48, let   s first understand the neuron

   neuron is a computational unit which takes the input(   s) , does some
   calculations and produces the output. that   s it no big deal.
   [1*vlrcdyu-nvyrwjltyrahew.jpeg]

   above, the 2nd picture is the one we use in neural networks, we have
   the input and we have some weights(parameters) we apply the dot product
   of these two vectors and produce the result (which would be a
   continuous value -infinity to + infinity).

   if we want to restrict the output values we use an activation function.

   the activation function squashes the output value and produce a value
   within a rage (which is based on the type of activation function).
   [1*trk5q_btqrsb_lymvk8gnq.jpeg]

   we use often these three (sigmoid range from 0 to 1, tanh from -1 to 1
   and relu from 0 to +infinity).

   that   s the neuron.

   a neural network is a set of layers(a layer has set of neurons) stacked
   together sequentially.
   [1*7j0q_2dvx6_l6ax9aa4dgg.jpeg]
   cs231n standford.edu

   the output of one layer would be the input of the next layer.

   here we have three layers
    1. input layer: a set of input neurons where each neuron represents
       each feature in our dataset. it takes the inputs and pass them to
       the next layer.
    2. hidden layer: a set of (n) no of neurons where each neuron has a
       weight(parameter) assigned to it. it takes the input from previous
       layer and does the dot product of inputs and weights, applies
       activation function (as we have seen above),produce the result and
       pass the data to next layer.

   note:we can have (n) no of hidden layers in between.(for sake of
   understanding let   s take only one hidden layer).

   3. output layer: it   s same hidden layer except it gives the final
   result(outcome/class/value).

   so how do we define no of neurons in each layer and the whole
   network???

   well, input layer   s neurons are based on no of features in the dataset.

   n_features= n_i/p_neurons+1(bias)

   we can define as many neurons/layers as we wish (it depends on the data
   and problem) but would be good to define more than features and all
   hidden layers have same no of neurons.

   n_h_neurons+1(bias)>n_features

   output layer   s neurons are based the type of problem and outcomes.

   if regression then 1 neuron ,for binary classification you can have 1
   or 2 neurons. and for multi classification more than 2 neurons.

   note: there is no bias here as it is the last layer in the network.

   we got the basic understanding of neural network so let   s get into
   deep.

     let   s understand how neural networks work.

   once you got the dataset and problem identified, you can follow the
   below steps:
1. pick the network architecture(initialize with random weights)
2. do a forward pass (forward propagation)
3. calculate the total error(we need to minimize this error)
4. back propagate the error and update weights(back propagation)
5. repeat the process(2-4)for no of epochs/until error is minimum.

   there are 2 algorithms in neural networks

   1.forward propagation.

   2.back propagation.
1. pick the network architecture

   lets take a toy dataset (xor) and pick the architecture with

   2 inputs , 2 outputs and 1 hidden of 3 neurons.
2.forward propagation

   this is a simple process, we feed forward the inputs through each layer
   in the network , the outputs from the previous layer become the inputs
   to the next layer.(first we feed our data as the inputs)
   [1*5rktg9slp8axqbwxlinisw.jpeg]

   first we provide the inputs(example) from our dataset ,
dataset (xor table)
 x     y
1 1    0 --> x1=1 and x2=1
1 0    1     h1 = sigmoid(x1*w1+x2*w2) = 0.5(assume with random

0 1    1                                                 weights)
0 0    0     similarly h2, h3 and o1, o2
3.calculate the total error.

   [1*tsluwwyiycug_-duqhnuow.png]

   assume random weights and activation(a1,2   ) we get the errors for each
   neuron.

   sum = inputs*weights and a = activation(sum) here sigmoid(sum).

   out cost function from andrew ng is
   [1*rcjc1kx2gqsgbegzmjpoma.jpeg]

   note: we take partial derivative w.r.t result (by using [13]chain rule
   in calculus)
4. back propagation

   [1*or-9v58o9rv2lwlag-82ca.jpeg]

   trust me it   s easy! or i will make it easy.

   the main goal of id26 is to update each of the weights in
   the network so that they cause the predicted output to be closer the
   target output, thereby minimizing the error for each output neuron and
   the network as a whole.

   so far we got the total error which is to be minimized.

   if you know how id119 works , the rest is pretty easy , if
   you don   t know, here is my article that talks about [14]gradient
   descent.

   we need to calculate the below terms
    1. how much does the total error change with respect to the result?
       (or how much is a change in results) already we did in the above
       picture.
    2. next, how much does the result of change with respect to its sum?
       (or how much is a change in sum)
    3. finally, how much does the sum of change with respect to weights?
       (or how much is a change in weights)

   [1*7kbwk1h9np1pmhcjh05kna.jpeg]

   well, that   s it.
5. repeat the process(2-4)for no of epochs/until error is minimum.

   we repeat the process forwarding the weights(fp) and updating
   weights(bp) for no of epochs or we reach the minimum error.

   once the training process is done, we can do the prediction by feed
   forwarding the input to the trained network, that   s it.

   hope its not confusing , and if you are not good at derivatives, you
   can let me know i can help, but i am sure that this will make sense as
   you go through again and again.

   i put lot of efforts in adding the math stuff and diagrams i feel
   pictures are awesome than words so please let me know if it helps.

   suggestions /questions are welcome.

   so that   s it for this story , in the next story i will build the neural
   network from scratch using the above steps and same math.

   until then

   see ya!

     * [15]machine learning
     * [16]deep learning
     * [17]neural networks
     * [18]artificial intelligence
     * [19]deep neural networks

   (button)
   (button)
   (button) 457 claps
   (button) (button) (button) 5 (button) (button)

     (button) blockedunblock (button) followfollowing
   [20]go to the profile of madhu sanjeevi ( mady )

[21]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [22]deep math machine learning.ai

[23]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 457
     * (button)
     *
     *

   [24]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [25]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/bb711169481b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_ozaaywaitp6b-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_ozaaywaitp6b---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-7-artificial-neural-networks-with-math-bb711169481b&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/deep-math-machine-learning-ai/chapter-1-2-gradient-descent-with-math-d4f2871af402
  13. https://en.wikipedia.org/wiki/chain_rule
  14. https://medium.com/deep-math-machine-learning-ai/chapter-1-2-gradient-descent-with-math-d4f2871af402
  15. https://medium.com/tag/machine-learning?source=post
  16. https://medium.com/tag/deep-learning?source=post
  17. https://medium.com/tag/neural-networks?source=post
  18. https://medium.com/tag/artificial-intelligence?source=post
  19. https://medium.com/tag/deep-neural-networks?source=post
  20. https://medium.com/@madhusanjeevi.ai?source=footer_card
  21. https://medium.com/@madhusanjeevi.ai
  22. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  23. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  24. https://medium.com/deep-math-machine-learning-ai
  25. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  27. https://medium.com/p/bb711169481b/share/twitter
  28. https://medium.com/p/bb711169481b/share/facebook
  29. https://medium.com/p/bb711169481b/share/twitter
  30. https://medium.com/p/bb711169481b/share/facebook
