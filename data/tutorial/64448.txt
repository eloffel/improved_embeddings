   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 4: id90 algorithms

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   oct 6, 2017

   decision tree is one of the most popular machine learning algorithms
   used all along, this story i wanna talk about it so let   s get
   started!!!

   id90 are used for both classification and regression
   problems, this story we talk about classification.

   before we dive into it , let me ask you this

     why id90?

   we have couple of other algorithms there, so why do we have to choose
   id90??

   well, there might be many reasons but i believe a few which are
    1. decision tress often mimic the human level thinking so its so
       simple to understand the data and make some good interpretations.
    2. id90 actually make you see the logic for the data to
       interpret(not like black box algorithms like id166,nn,etc..)

   [1*2jnsfce0ymrjb8evvao93w.gif]

   for example : if we are classifying bank loan application for a
   customer, the decision tree may look like this

   here we can see the logic how it is making the decision.

   it   s simple and clear.

     so what is the decision tree??

   a decision tree is a tree where each node represents a
   feature(attribute), each link(branch) represents a decision(rule) and
   each leaf represents an outcome(categorical or continues value).

   the whole idea is to create a tree like this for the entire data and
   process a single outcome at every leaf(or minimize the error in every
   leaf).
   [1*kiacler1syei-0m1ctq9-g.jpeg]

     okay so how to build this??

   there are couple of algorithms there to build a decision tree , we only
   talk about a few which are
    1. cart (classification and regression trees)     uses gini
       index(classification) as metric.
    2.   (iterative dichotomiser 3)     uses id178 function and
       [12]information gain as metrics.

   lets just first build decision tree for classification problem using
   above algorithms,

     classification with using the   algorithm.

   let   s just take a famous dataset in the machine learning world which is
   weather dataset(playing game y or n based on weather condition).
   [1*bn3d4z62sof3k4u1_0pslq.jpeg]

   we have four x values (outlook,temp,humidity and windy) being
   categorical and one y value (play y or n) also being categorical.

   so we need to learn the mapping (what machine learning always does)
   between x and y.

   this is a binary classification problem, lets build the tree using the
     algorithm

   to create a tree, we need to have a root node first and we know that
   nodes are features/attributes(outlook,temp,humidity and windy),

     so which one do we need to pick first??

   answer: determine the attribute that best classifies the training data;
   use this attribute at the root of the tree. repeat this process at for
   each branch.

   this means we are performing top-down, greedy search through the space
   of possible id90.

     okay so how do we choose the best attribute?

   answer: use the attribute with the highest information gain in  

   in order to define information gain precisely, we begin by defining a
   measure commonly used in id205, called id178 that
   characterizes the (im)purity of an arbitrary collection of examples.   
   [1*eowj8bxc-iqbs-df-xxsba.jpeg]
   wikipedia

   for a binary classification problem
     * if all examples are positive or all are negative then id178 will
       be zero i.e, low.
     * if half of the examples are of positive class and half are of
       negative class then id178 is one i.e, high.

   [1*wqjvzx7zcvb87htqk46vua.jpeg]
   wikipedia

   okay lets apply these metrics to our dataset to split the data(getting
   the root node)

   steps:
1.compute the id178 for data-set
2.for every attribute/feature:
       1.calculate id178 for all categorical values
       2.take average information id178 for the current attribute
       3.calculate gain for the current attribute
3. pick the highest gain attribute.
4. repeat until we get the tree we desired.

   what the heck???

   okay i got it , if it does not make sense to you , let me make it sense
   to you.

     compute the id178 for the weather data set:

   [1*bi168nedjivjxfeyd8gokw.jpeg]

     for every feature calculate the id178 and information gain

   [1*a-poohxvgxh-hfcwovzwua.jpeg]

   similarity we can calculate for other two attributes(humidity and
   temp).

     pick the highest gain attribute.

   [1*rf-hfi7z3kdn4fcjzbmiag.jpeg]

   so our root node is outlook.
   [1*8jwnlxy4cyiff6wnbes5oa.jpeg]

     repeat the same thing for sub-trees till we get the tree.

   [1*680un0-onkaei_chbd0iwa.jpeg]

   finally we get the tree something like his.
   [1*tltzgt8i_5dusbmzmrkyqq.jpeg]

     classification with using the cart algorithm.

   in cart we use gini index as a metric,

   we use the gini index as our cost function used to evaluate splits in
   the dataset.

   our target variable is binary variable which means it take two values
   (yes and no). there can be 4 combinations.
actual=1 predicted 1
1 0 , 0,1, 0 0
p(target=1).p(target=1) + p(target=1).p(target=0) + p(target=0).p(target=1) + p(
target=0).p(target=0) = 1
p(target=1).p(target=0) + p(target=0).p(target=1) = 1     p^2(target=0)     p^2(targ
et=1)

   gini index for binary target variable is

   = 1         p^2(target=0)         p^2(target=1)
   [1*-x-0swbciq8-f5vxszgs3w.png]
   gini index

   a gini score gives an idea of how good a split is by how mixed the
   classes are in the two groups created by the split. a perfect
   separation results in a gini score of 0, whereas the worst case split
   that results in 50/50 classes.

   we calculate it for every row and split the data accordingly in our
   binary tree. we repeat this process recursively.

   for binary target variable, max gini index value

   = 1         (1/2)^2         (1/2)^2
   = 1   2*(1/2)^2
   = 1- 2*(1/4)
   = 1   0.5
   = 0.5

   similarly if target variable is categorical variable with multiple
   levels, the gini index will be still similar. if target variable takes
   k different values, the gini index will be
   [1*kie4cjvkn8pxe9 i9hxq.png]

   maximum value of gini index could be when all target values are equally
   distributed.

   similarly for nominal variable with k level, the maximum value gini
   index is

   = 1   1/k

   minimum value of gini index will be 0 when all observations belong to
   one label.

   steps:
1.compute the gini index for data-set
2.for every attribute/feature:
       1.calculate gini index for all categorical values
       2.take average information id178 for the current attribute
       3.calculate the gini gain
3. pick the best gini gain attribute.
4. repeat until we get the tree we desired.

   the calculations are similar to   ,except the formula changes.

   for example :compute gini index for dataset
   [1*fgy28bbelyg9zg4_q4lxta.png]

   similarly we can follow other steps to build the tree
   [1*f_amlaxtzrnp3lrupjx24a.jpeg]

   that   s it for this story. hope you enjoyed and learned something.

   let me know your thoughts/suggestions/questions.

   we just talked the first half of id90 , we can talk about the
   other half later (some statistical notations,theories and algorithms)

   in the next story we will code this algorithm from scratch (without
   using any ml libraries).

   until then

   see ya!

   the images i borrowed from a pdf book which i am not sure and don   t
   have link to add it. let me know if anyone finds the abouve diagrams in
   a pdf book so i can link it.
   [1*z-4jf9apbzr8pgb21zp4yg.jpeg]

     * [13]machine learning
     * [14]supervised learning
     * [15]decision tree
     * [16]id178
     * [17]classification

   (button)
   (button)
   (button) 3.3k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [18]go to the profile of madhu sanjeevi ( mady )

[19]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [20]deep math machine learning.ai

[21]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 3.3k
     * (button)
     *
     *

   [22]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [23]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/b93975f7a1f1
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_khxksbstaosr-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_khxksbstaosr---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://en.wikipedia.org/wiki/information_gain_in_decision_trees
  13. https://medium.com/tag/machine-learning?source=post
  14. https://medium.com/tag/supervised-learning?source=post
  15. https://medium.com/tag/decision-tree?source=post
  16. https://medium.com/tag/id178?source=post
  17. https://medium.com/tag/classification?source=post
  18. https://medium.com/@madhusanjeevi.ai?source=footer_card
  19. https://medium.com/@madhusanjeevi.ai
  20. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  21. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  22. https://medium.com/deep-math-machine-learning-ai
  23. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  25. https://medium.com/p/b93975f7a1f1/share/twitter
  26. https://medium.com/p/b93975f7a1f1/share/facebook
  27. https://medium.com/p/b93975f7a1f1/share/twitter
  28. https://medium.com/p/b93975f7a1f1/share/facebook
