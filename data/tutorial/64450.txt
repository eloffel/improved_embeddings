   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]deep math machine learning.ai
   [7]deep math machine learning.ai
   [8]sign in[9]get started
     __________________________________________________________________

chapter 2.0 : id28 with math.

   [10]go to the profile of madhu sanjeevi ( mady )
   [11]madhu sanjeevi ( mady ) (button) blockedunblock (button)
   followfollowing
   sep 26, 2017
   [1*zlfpo6f_bfi6uvrl6ilx_q.jpeg]

   in the previous story we talked about [12]id75 for solving
   regression problems in machine learning , this story we will talk about
   id28 for classification problems.

   you may be wondering why the name says regression if it is a
   classification algorithm, well,it uses the regression inside to be the
   classification algorithm.

   classification : separates the data from one to another.
   [1*639efjxxfjql9wtwiy7ang.png]
   one vs all method (right)

   this story we talk about binary classification ( 0 or 1) here target
   variable is either 0 or 1

goal is to find that green straight line (which separates the data at best)

   so we use regression for drawing the line , makes sense right?

   lets take a random dataset and see how it works,
   [1*325ngpe6ywlcvwyrqwepta.jpeg]
   (x1,x2) plot(left), (x1,y) plot(right)

   if we observe the right picture we have our independent variable (x)
   and dependent variable(y) so this is the graph we should consider for
   the classification problem

   given x or (set of x values) we need to predict whether it   s 0 or 1
   (yes/no).

   if we apply id75 for above data we get something like
   this,
   [1*kxsp--g1vbfhdhcmfrcu2g.jpeg]

   given x value 6 we can say y is 0.7 (close to 1), that   s cool but wait,
   what if i give negative x value or greater x value??? the output is
   this
   [1*bfvd1ammdg6np8pyr9i17w.jpeg]

   we only accept the values between 0 and 1 (we don   t accept other
   values) to make a decision (yes/no)

     so how do we proceed further?

   there is an awesome function called sigmoid or logistic function , we
   use to get the values between 0 and 1
   [1*be3bhz0ba-ngziyqrvztug.gif]

   this function squashes the value (any value ) and gives the value
   between 0 and 1

   how??? and what is    e    ???

   e here is    exponential function    the value is 2.71828

   this is how the value is always between 0 and 1.
   [1*1yg1r3g9q-eycoywisboiw.png]

   so far we know that we first apply the linear equation and apply
   sigmoid function for the result so we get the value which is between 0
   and 1.

   the hypothesis for id75 is h(x) =   0+  1*x

   the hypothesis for this algorithm is
   [1*bn7b2ar4bxp0fblascz1fa.jpeg]
   logistic function for id28.

     how does it work??

    1. first we calculate the logit function, what the heck is that??

   logit =   0+  1*x (hypothesis of id75)

   2. we apply the above sigmoid function (logistic function) to logit.

   3 we calculate the error , cost function (maximum log-likelihood)

   cost function for id75 is
   [1*xmdblknn9nzmkxgbu3cyhq.png]
   cost function

   here it does not work as h(x) hypothesis gives non convex function for
   j(  0,  1) so we are not guaranteed that we reach best minimum.
   [1*27o7lzedfbeqsfttraimva.jpeg]

   we take log( hypothesis) to calculate the cost function
   [1*hkc6ekqgfgajru_rmf5trq.png]

   if it does not make sense , let me make it sense to you

   usually error is what?? (predicted         actual)**2 right??
so if predicted  = 1 and actual= 1
error = 0
so if predicted  = 1 and actual= 0
error = 1
so if predicted  = 0 and actual= 1
error = 1
so if predicted  = 0 and actual= 0
error = 0
note: predicted can be 0.5 and so on... also
so every time we get the error between 0 and 1 which is not useful.

   just take a look at this picture and observe something..
   [1*dmnfxwjiatugd3e32knwog.jpeg]
   0%-100% = 0   1

     from left picture

   if actual y =1 and predicted =0 the cost goes to infinity and if actual
   y =1 and predicted =1 the cost goes to minimum.

   if actual y =0 and predicted =1 the cost goes to infinity and if actual
   y =0 and predicted =0 the cost goes to minimum.

     from right picture

   if we apply log to hypothesis (predicted) we get some values (cost)
   which is useful to estimate the overall error.

   here is the final picture.
   [1*gbr8vm1kmn1yzgsnmr9jfg.jpeg]

   that   s it. based on the actual y values we calculate different
   functions.

   4. next step is to apply id119 to change the    values in our
   hypothesis ( i already covered check this [13]link).

   that   s it we are done!

   we got the id28 ready, we can now predict new data with
   the model we just built.

   predicting new data, remember?? we give new x values we get the
   predicted y values how does it work ??
   [1*bn7b2ar4bxp0fblascz1fa.jpeg]

     bam!!!!

   we get the id203 score(s).

   so that   s it for this story , in the next story i will code this
   algorithm from scratch and also using tensorflow and scikitlearn.

   see ya!

     * [14]machine learning
     * [15]id28
     * [16]supervised learning
     * [17]classification
     * [18]artificial intelligence

   (button)
   (button)
   (button) 379 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [19]go to the profile of madhu sanjeevi ( mady )

[20]madhu sanjeevi ( mady )

   writes about technology (ai, ml, dl) | writes about human mind and
   computer mind. interested in ||programming || science || psychology ||
   neuroscience || math

     (button) follow
   [21]deep math machine learning.ai

[22]deep math machine learning.ai

   this is all about machine learning and deep learning (topics cover
   math,theory and programming)

     * (button)
       (button) 379
     * (button)
     *
     *

   [23]deep math machine learning.ai
   never miss a story from deep math machine learning.ai, when you sign up
   for medium. [24]learn more
   never miss a story from deep math machine learning.ai
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/e9cbb3ec6077
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/deep-math-machine-learning-ai?source=avatar-lo_rmywxqkfdqxq-dedce56b468f
   7. https://medium.com/deep-math-machine-learning-ai?source=logo-lo_rmywxqkfdqxq---dedce56b468f
   8. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-2-0-logistic-regression-with-math-e9cbb3ec6077&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://medium.com/deep-math-machine-learning-ai/chapter-2-0-logistic-regression-with-math-e9cbb3ec6077&source=--------------------------nav_reg&operation=register
  10. https://medium.com/@madhusanjeevi.ai?source=post_header_lockup
  11. https://medium.com/@madhusanjeevi.ai
  12. https://medium.com/@madhusanjeevi.ai/complete-linear-regression-with-math-edb05500e7ee
  13. https://medium.com/@madhusanjeevi.ai/chapter-1-2-gradient-descent-with-math-ad303eb33be8
  14. https://medium.com/tag/machine-learning?source=post
  15. https://medium.com/tag/logistic-regression?source=post
  16. https://medium.com/tag/supervised-learning?source=post
  17. https://medium.com/tag/classification?source=post
  18. https://medium.com/tag/artificial-intelligence?source=post
  19. https://medium.com/@madhusanjeevi.ai?source=footer_card
  20. https://medium.com/@madhusanjeevi.ai
  21. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  22. https://medium.com/deep-math-machine-learning-ai?source=footer_card
  23. https://medium.com/deep-math-machine-learning-ai
  24. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  26. https://medium.com/p/e9cbb3ec6077/share/twitter
  27. https://medium.com/p/e9cbb3ec6077/share/facebook
  28. https://medium.com/p/e9cbb3ec6077/share/twitter
  29. https://medium.com/p/e9cbb3ec6077/share/facebook
