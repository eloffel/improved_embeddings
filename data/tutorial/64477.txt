debugging the machine learning pipeline

jerry zhu

university of wisconsin-madison

joint work with xuezhou zhang, stephen wright

interpretable ml symposium, nips 2017

debugging provides an opportunity for machine learning
interpretability.

harry potter toy example

hired by the ministry of magic?

+ yes

o no

00.51magical heritage00.51educationdata contain historical biases

learned vs. ideal decision boundary

(rbf kernel id28)

00.51magical heritage00.51educationtrusted items

(cid:73) obtained by expensive vetting
(cid:73) insu   cient to learn from

00.51magical heritage00.51educationdebugging using trusted items

(cid:73) propose training label bugs
(cid:73)    ipping them makes re-trained model agree with trusted items

00.51magical heritage00.51educationproposed bugs

(cid:73) given to experts to interpret

00.51magical heritage00.51educationthe ml pipeline

data (x, y )     learner (cid:96)     parameters        model     

     = argmin

       

(cid:96)(x, y,   ) +   (cid:107)  (cid:107)

postconditions

  (    )

examples:

(cid:73)    the learned model must correctly predict an important item

(  x,   y)   

    (  x) =   y

(cid:73)    the learned model must satisfy individual fairness   

   x, x(cid:48),|p(y = 1 | x,     )     p(y = 1 | x(cid:48),     )|     l(cid:107)x     x(cid:48)(cid:107)

bug assumptions

(cid:73)    satis   ed if we were to train through    clean pipeline   
(cid:73) bugs are changes to the clean pipeline
(cid:73)    violated on the dirty pipeline

this is not our goal

just to learn a better model:

min
       
s.t.

(cid:96)(x, y,   ) +   (cid:107)  (cid:107)

  (  ) = true

this is our goal

to identify bugs and    x them (and learn a better model):

min
y (cid:48),    

s.t.

(cid:107)y     y (cid:48)(cid:107)

  (    ) = true
     = argmin

       

(cid:96)(x, y (cid:48),   ) +   (cid:107)  (cid:107)

special case: bugs in training labels

(cid:73)    satis   ed if we were to train on    clean data    (x, y (cid:48))
(cid:73) bugs are changes to clean labels

(x, y ) = (x, y (cid:48) +    )

(cid:73) not just about outliers
(cid:73) may contain systematic biases

input / output to our debugger

input:

1. dirty training set (x, y )
2. trusted items (   x,   y )
3. the learner

output:
1. y (cid:48)
2. con   dence

formulation equivalent to machine teaching

min
y (cid:48)
s.t.

(cid:107)y (cid:48)     y (cid:107)
    (   x) =   y

     = argmin

       

1
n

n(cid:88)

i=1

(cid:96)(xi, y(cid:48)

i,   ) +   (cid:107)  (cid:107)2

di   cult!

(cid:73) combinatorial
(cid:73) bilevel optimization (stackelberg game)

[dec. 9 workshop on teaching machines, robots, and humans]

combinatorial to continuous relaxation

step 1. label to id203 simplex
i       i        
y(cid:48)
n(cid:88)

step 2. counting to id203 mass

(cid:107)y (cid:48)     y (cid:107)     1
n

step 3. soften postcondition

    (   x) =   y     1
m

i=1

(1       i,yi)
m(cid:88)

(cid:96)(  xi,   yi,   )

i=1

continuous now, but still bilevel

m(cid:88)

i=1

1
m

argmin
        n,    

(cid:96)(  xi,   yi,     ) +   

n(cid:88)

k(cid:88)

i=1

j=1

s.t.

     = argmin

  

1
n

n(cid:88)
(1       i,yi)

i=1

1
n

  ij(cid:96)(xi, j,   ) +   (cid:107)  (cid:107)2

removing the lower level problem

n(cid:88)

k(cid:88)

i=1

j=1

1
n

     = argmin

  

  ij(cid:96)(xi, j,   ) +   (cid:107)  (cid:107)2

step 1. the kkt condition

n(cid:88)

k(cid:88)

  ij     (cid:96)(xi, j,   ) + 2     = 0

1
n

i=1

j=1

step 2. plug implicit function   (  ) into upper level problem

argmin

  

1
m

(cid:96)(  xi,   yi,   (  )) +   

1
n

step 3. compute gradient       with implicit function theorem

m(cid:88)

i=1

n(cid:88)
(1       i,yi)

i=1

software available.

harry potter toy example

data

our debugger

in   uence function

nearest neighbor

label noise detection

average pr

00.51magical heritage00.51education00.51magical heritage00.51education00.51magical heritage00.51education00.51magical heritage00.51education00.51magical heritage00.51education00.51recall00.51precisiondutiinfnnlnd (oracle)another special case: bug in id173 weight

(id28)

020406080100score00.51pass/failpostcondition violated

  (    ): individual fairness (lipschitz condition)

   x, x(cid:48),|p(y = 1 | x,     )     p(y = 1 | x(cid:48),     )|     l(cid:107)x     x(cid:48)(cid:107)

bug assumption

learner   s id173 weight    = 0.001 was inappropriate

     = argmin

       

(cid:96)(x, y,   ) +   (cid:107)  (cid:107)2

debugging formulation

min
  (cid:48),    

s.t.

(  (cid:48)       )2

  (    ) = true
     = argmin

       

(cid:96)(x, y,   ) +   (cid:48)(cid:107)  (cid:107)2

suggested bug

   = 0.001       (cid:48) = 121

020406080100score00.51pass/failcall for ml bug repository

(cid:73) like software bug repositories in software engineering
(cid:73) need data provenance

(cid:73) which training items (or other things) were wrong
(cid:73) what they should be

references

(cid:73) xuezhou zhang, xiaojin zhu, and stephen wright. training

set debugging using trusted items. aaai 2018

(cid:73) gabriel cadamuro, ran gilad-bachrach, and xiaojin zhu.
debugging machine learning models. icml workshop on
reliable machine learning in the wild, 2016.

(cid:73) shalini ghosh, patrick lincoln, ashish tiwari, and xiaojin
zhu. trusted machine learning for probabilistic models.    

(cid:73) http://www.cs.wisc.edu/~jerryzhu/machineteaching/

