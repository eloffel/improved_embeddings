geometric deep learning
on graphs and manifolds

michael bronstein, federico monti

usi

switzerland

tau
israel

intel
israel

harvard

usa

fabula ai

uk

odsc east, 1 may 2018, boston

1958 1959 1982 1987 1995 1997 1998 1999 2006 2012 2014 2015 id88 rosenblatt visual cortex hubel&wiesel backprop werbos neurocognitron fukushima first nips id166 vapnik id56 / lstm schmidhuber id98 lecun first gpu autoencoder lecun, hinton id163  breakthrough krizhevsky ai research 2010 renaissance dark ages ai birth 2016 autonomous  cars alphago speech  recognition 2017 2018 breakthrough in image recognition

id163 ilsvrc challenge

2010 2011 2012 2013 2014 2015 10 20 30 error % deep learning2016 2.2%2017 convolutional neural networks (id98): lenet-5

3 convolutional + 1 fully connected layer

1m parameters

training set: mnist 70k images

trained on cpu

tanh non-linearity

lecun et al. 1998

convolutional neural networks (id98): alexnet

5 convolutional + 3 fully connected layers

60m parameters

trained on id163 1.5m images

trained on gpu

relu non-linearity

dropout id173

krizhevsky, sutskever, hinton 2012

convolutional neural networks (id98): alexnet

5 convolutional + 3 fully connected layers (architecture)

60m parameters (architecture)

trained on id163 1.5m images

trained on gpu

relu non-linearity (architecture)

dropout id173 (architecture)

krizhevsky, sutskever, hinton 2012

convolutional neural networks (id98): alexnet

5 convolutional + 3 fully connected layers (architecture)

60m parameters (architecture)

trained on id163 1.5m images (data)

trained on gpu

relu non-linearity (architecture)

dropout id173 (architecture)

krizhevsky, sutskever, hinton 2012

convolutional neural networks (id98): alexnet

5 convolutional + 3 fully connected layers (architecture)

60m parameters (architecture)

trained on id163 1.5m images (data)

trained on gpu (computation)

relu non-linearity (architecture)

dropout id173 (architecture)

krizhevsky, sutskever, hinton 2012

datasets over algorithms

wissner-gross 2016

text

audio signals

images

text

audio signals

social networks

regulatory networks

images

functional networks

3d shapes

how to design deep nets on

non-euclidean domains

and what to do with them?

prototypical non-euclidean objects

manifolds

graphs

domain structure vs data on a domain

domain structure vs data on a domain

domain structure

domain structure vs data on a domain

domain structure

data on a domain

vertexgenderage...edgefriendshipfrequency...fixed vs di   erent domain

social network
(   xed graph)

fixed vs di   erent domain

social network
(   xed graph)

3d shapes

(di   erent manifolds)

graph-wise classi   cation

molecule graph

duvenaud et al. 2015

graph-wise classi   cation

molecule graph

image recognition

duvenaud et al. 2015

   cat   vertex-wise classi   cation

social network

vertex-wise classi   cation

social network

agegendereducationworkvertex-wise classi   cation

social network

vertex-wise classi   cation

social network

semantic image segmentation

vertex-wise classi   cation

social network

semantic image segmentation

groundtreemotorcyclepersonknown vs unknown domain

given graph

learned graph

di   erent formulations of non-euclidean id98s

spectral domain

spatial domain

1053-5888/17  2017ieee18many scienti(cid:31)c (cid:31)elds study data with an underlying structure that is non-euclidean. some examples include social networks in computational social sci-ences, sensor networks in communications, func-tional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. in many applications, such geometric data are large and com-plex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. in particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from id161, natural-language processing, and audio analysis. however, these tools have been most successful on data with an underlying euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.geometric deep learning is an umbrella term for  emerging techniques attempting to generalize (structured) deep neural mod-els to non-euclidean domains, such as graphs and manifolds. the purpose of this article is to overview di(cid:30)erent examples of geometric deep-learning problems and present available solutions, key di(cid:29)cul-ties, applications, and future research directions in this nascent (cid:31)eld.overview of deep learningdeep learning refers to learning complicated concepts by building them from simpler ones in a hierarchical or multilayer manner. arti(cid:31)cial neural networks are popular realizations of such deep multilayer hierarchies. in the past few years, the growing computational power of modern graphics processing unit (gpu)-based computers and the avail-ability of large training data sets have allowed successfully training neural networks with many layers and degrees of freedom (dof) [1]. this has led to qualitative breakthroughs on a wide variety of tasks, from id103 [2], [3] and machine translation [4] to image analysis and id161 [5]   [11] (see [12] michael m. bronstein, joan bruna, yann lecun,  arthur szlam, and pierre vandergheynstgoing beyond euclidean datageometric deep learning  istockphoto.com/liuzishanieee signal processing magazine   |   july 2017   |digital object identi(cid:31)er 10.1109/msp.2017.2693418date of publication: 11 july 201719ieee signal processing magazine   |   july 2017   |and [13] for many additional examples of successful applications of deep learning). today, deep learning has matured into a technology that is widely used in commercial applications, including siri speech recog-nition in apple iphone, google text translation, and mobileye vision-based technology for autonomously driving cars.one of the key reasons for the success of deep neural networks is their ability to leverage sta-tistical properties of the data, such as stationarity and compositionality through local statistics, which are present in natural images, video, and speech [14], [15]. these statistical properties have been related to physics [16] and formalized in speci(cid:31)c classes of convolutional neural networks (id98s) [17]   [19]. in image analysis applications, one can consider images as functions on the euclidean space (plane), sampled on a grid. in this setting, stationarity is owed to shift invariance, locality is due to the local connectivity, and compositional-ity stems from the multiresolution structure of the grid. these properties are exploited by convolutional architectures [20], which are built of alternating convolutional and downsampling (pooling) layers. the use of convolutions has a twofold e(cid:30)ect. first, it allows extracting local features that are shared across the image domain and great-ly reduces the number of parameters in the network with respect to generic deep  architectures (and thus also the risk of over(cid:31)tting), without sacri(cid:31)cing the expres-sive capacity of the network. second, the convolutional architecture itself imposes some priors about the data, which appear very suitable especially for natural images [17]   [19], [21].while deep-learning models have been particularly successful when dealing with speech, image, and video signals, in which there are an underlying euclide-an structure, recently there has been a growing interest in trying to apply learning on non-euclidean geometric data. such kinds of data arise in numerous applica-tions. for instance, in social networks, the characteristics of users can be modeled as signals on the vertices of the social graph [22]. sensor networks are graph models of distributed interconnected sensors, whose readings are modeled as time-depen-dent signals on the vertices. in genetics, gene expression data are modeled as signals de(cid:31)ned on the regulatory network [23]. in neuroscience, graph models are used to rep-resent anatomical and functional structures of the brain. in computer graphics and vision, three-dimensional (3-d) objects are modeled as riemannian manifolds (surfaces) endowed with properties such as color texture.the non-euclidean nature of such data implies that there are no such familiar properties as global parameterization, common system of coordinates, vector space structure, or shift invariance. consequently, basic operations like convolution that are taken for granted in the euclidean case are even not well de(cid:31)ned on non-euclidean domains. the purpose of this article is to show di(cid:30)erent methods of translating the key ingredients of suc-cessful deep-learning methods, such as id98s, to non-euclidean data.geometric learning problemsbroadly speaking, we can distinguish between two classes of geometric learning problems. in the (cid:31)rst class of problems, the goal is to characterize the structure of the data. the second class of problems deals with analyz-ing functions de(cid:31)ned on a given non-euclidean domain. these two class-es are related, because understanding the properties of functions de(cid:31)ned on a domain conveys certain information about the domain, and vice versa, the structure of the domain imposes certain properties on the func-tions on it.structure of the domainas an example of the (cid:31)rst class of problems, assume to be given a set of data points with some underlying low-dimensional structure embedded into a high-dimensional euclidean space. recovering that low-dimensional structure is often referred to as manifold learning or nonlinear id84 and is an instance of unsupervised learning (note that the notion of manifold in this setting can be considerably more general than a classical smooth manifold; see, e.g., what this tutorial is about?

basics of euclidean convolutional neural networks

basics of id207

fourier analysis on graphs

spectral-domain methods

spatial-domain methods

applications: network analysis, recommender systems, computer
graphics, chemistry, high-energy physics, ...

geometricdeeplearning.com

background

supervised learning

data vectors f     rp
(e.g. for 512  512 images p     105)

unknown classi   cation functional
y : rp     {1, . . . , l} in l classes

training set

s = {(fi     rp, yi = y(fi))}t

i=1

parametric model y   of y

rp

supervised learning

data vectors f     rp
(e.g. for 512  512 images p     105)

unknown classi   cation functional
y : rp     {1, . . . , l} in l classes

training set

s = {(fi     rp, yi = y(fi))}t

i=1

parametric model y   of y

rp

supervised learning:    nd optimal model parameters by minimizing the
loss (cid:96) on the training set

t(cid:88)

      = argmin

  

i=1

(cid:96)(y  (fi), yi)

neural network (nn)

f1

f2

fp

w11

g(1)
1
g1

g2

wqp
w

gq

linear layer

gl =   

single linear layer

(cid:33)

(cid:32) p(cid:88)

l(cid:48)=1

fl(cid:48)wl,l(cid:48)

l = 1, . . . , q
l(cid:48) = 1, . . . , p

activation, e.g.

  (x) = max{x, 0}

recti   ed linear unit (relu)

parameters

layer weights w (including bias)

neural network (nn)

f in
1

f in
2

f in
p

g(1)
1

g(1)
2

g(1)
q1

g(2)
1

g(2)
2

g(2)
q2

g(3)
1

g(3)
2

g(l   1)

1

g(l   1)

2

g(3)
q3

g(l   1)
ql   1

w(1)

w(2)

w(3)

w(l)

gout
1

gout
2

gout
q

deep neural network consisting of l layers

(cid:33)

(cid:32)qk   1(cid:88)

l(cid:48)=1

linear layer

g(k)
l =   

g(k   1)
l(cid:48)

w(k)
l,l(cid:48)

l = 1, . . . , qk
l(cid:48) = 1, . . . , qk   1

activation, e.g.

  (x) = max{x, 0}

recti   ed linear unit (relu)

parameters

weights of all layers w(1), . . . , w(l) (including biases)

...neural network (nn)

f in
1

f in
2

f in
p

g(1)
1

g(1)
2

g(1)
q1

g(2)
1

g(2)
2

g(2)
q2

g(3)
1

g(3)
2

g(l   1)

1

g(l   1)

2

g(3)
q3

g(l   1)
ql   1

w(1)

w(2)

w(3)

w(l)

gout
1

gout
2

gout
q

deep neural network consisting of l layers

g(k) =   (cid:0)w(k)g(k   1)(cid:1)

linear layer

activation, e.g.

  (x) = max{x, 0}

recti   ed linear unit (relu)

parameters

weights of all layers w(1), . . . , w(l) (including biases)

...neural network (nn)

f in
1

f in
2

f in
p

g(1)
1

g(1)
2

g(1)
q1

g(2)
1

g(2)
2

g(2)
q2

g(3)
1

g(3)
2

g(l   1)

1

g(l   1)

2

g(3)
q3

g(l   1)
ql   1

w(1)

w(2)

w(3)

w(l)

gout
1

gout
2

gout
q

deep neural network consisting of l layers

gout =   (cid:0). . . w(2)  (cid:0)w(1)f in(cid:1)(cid:1) = y(w(1),...,w(l))(f in)

net output

activation, e.g.

  (x) = max{x, 0}

recti   ed linear unit (relu)

parameters

weights of all layers w(1), . . . , w(l) (including biases)

...neural nets as universal approximators

universal approximation theorem let    be a non-constant,
bounded, and monotonically-increasing continuous activation function,
y : [0, 1]p     r continuous function, and   > 0. then,    n and parame-
ters a, b     rn, w     rn  p s.t.

ai  (w(cid:62)

i f + bi)     y(f )

   f     [0, 1]p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) <  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

cybenko 1989; hornik 1991

neural nets as universal approximators

universal approximation theorem let    be a non-constant,
bounded, and monotonically-increasing continuous activation function,
y : [0, 1]p     r continuous function, and   > 0. then,    n and parame-
ters a, b     rn, w     rn  p s.t.

ai  (w(cid:62)

i f + bi)     y(f )

   f     [0, 1]p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) <  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

f1

f2

fp

w11

wnp

a1

a2

an

cybenko 1989; hornik 1991

neural nets as universal approximators

universal approximation theorem let    be a non-constant,
bounded, and monotonically-increasing continuous activation function,
y : [0, 1]p     r continuous function, and   > 0. then,    n and parame-
ters a, b     rn, w     rn  p s.t.

ai  (w(cid:62)

i f + bi)     y(f )

   f     [0, 1]p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) <  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

any continuous function can be approximated arbitrarily well by a
neural network with a single hidden layer

cybenko 1989; hornik 1991

neural nets as universal approximators

universal approximation theorem let    be a non-constant,
bounded, and monotonically-increasing continuous activation function,
y : [0, 1]p     r continuous function, and   > 0. then,    n and parame-
ters a, b     rn, w     rn  p s.t.

ai  (w(cid:62)

i f + bi)     y(f )

   f     [0, 1]p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) <  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

any continuous function can be approximated arbitrarily well by a
neural network with a single hidden layer
how many neurons?

cybenko 1989; hornik 1991

neural nets as universal approximators

universal approximation theorem let    be a non-constant,
bounded, and monotonically-increasing continuous activation function,
y : [0, 1]p     r continuous function, and   > 0. then,    n and parame-
ters a, b     rn, w     rn  p s.t.

ai  (w(cid:62)

i f + bi)     y(f )

   f     [0, 1]p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) <  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

any continuous function can be approximated arbitrarily well by a
neural network with a single hidden layer
how many neurons?
how to    nd the parameters?

cybenko 1989; hornik 1991

neural nets as universal approximators

universal approximation theorem let    be a non-constant,
bounded, and monotonically-increasing continuous activation function,
y : [0, 1]p     r continuous function, and   > 0. then,    n and parame-
ters a, b     rn, w     rn  p s.t.

ai  (w(cid:62)

i f + bi)     y(f )

   f     [0, 1]p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) <  

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

any continuous function can be approximated arbitrarily well by a
neural network with a single hidden layer
how many neurons?
how to    nd the parameters?
does it generalize well / over   t?

cybenko 1989; hornik 1991

take advantage of the
structure of the data!

stationarity and self-similarity

data is self-similar across the domain

locality

visual cortex cell structure (local receptive    elds) emulated in

neocognitron neural network model

hubel, wiesel 1962; fukushima 1980

translation invariance (image classi   cation tasks)

y(tvf ) = y(f )

   f, v

where

image is modeled as a function f     l2([0, 1]2)
tvf (x) = f (x     v) is a translation operator
v     [0, 1]2 is a translation vector
y : l2([0, 1]2)     {1, . . . , l} is classi   cation functional

bruna, mallat 2012

deformation invariance (image classi   cation tasks)

|y(l   f )     y(f )|     (cid:107)     (cid:107)

   f,   

where

image is modeled as a function f     l2([0, 1]2)
l   f (x) = f (x        (x)) is a warping operator
   : [0, 1]2     [0, 1]2 is a smooth deformation    eld
y : l2([0, 1]2)     {1, . . . , l} is classi   cation functional

bruna, mallat 2012

hierarchy and compositionality

typical features learned by a id98 becoming increasingly complex

at deeper layers

zeiler, fergus 2013

convolutional neural network (id98)

f1(x)

f2(x)

(cid:63)w11

g(1)
1

g1(x)

g2(x)

fp(x)

(cid:63)wqp

w = (wl,l(cid:48) )

gq(x)

conv. layer

gl(x) =   

(fl(cid:48) (cid:63) wl,l(cid:48))(x)

single convolutional layer

(cid:32) p(cid:88)

l(cid:48)=1

  (x) = max{x, 0}
   lters w

(cid:33)

l = 1, . . . , q
l(cid:48) = 1, . . . , p

recti   ed linear unit (relu)

activation, e.g.

parameters

lecun et al. 1989

convolutional neural network (id98)

g(1)
1 (x)

g(1)
2 (x)

g(2)
1 (x)

g(2)
2 (x)

g(3)
1 (x)

g(3)
2 (x)

g(l   1)

1

(x)

g(l   1)

2

(x)

gout
1

(x)

gout
2

(x)

gout
q

(x)

g(1)
q1 (x)

g(2)
q2 (x)

g(3)
q3 (x)

w (1)

w (2)

w (3)

id98 consisting of l convolutional layers

(cid:32)qk   1(cid:88)

g(l   1)
ql   1 (x)
w (l)

(cid:33)

g(k)
l

(x) =   

(g(k   1)
l(cid:48)

(cid:63) w(k)

l,l(cid:48) )(x)

l(cid:48)=1
  (x) = max{x, 0}
   lters of all layers w (1), . . . , w (l)

recti   ed linear unit (relu)

l = 1, . . . , qk
l(cid:48) = 1, . . . , qk   1

f in
1 (x)

f in
2 (x)

f in
p (x)

conv. layer

activation, e.g.

parameters

lecun et al. 1989

...convolutional neural network (id98)

gout
1

(x)

gout
2

(x)

gout
q

(x)

f in
1 (x)

f in
2 (x)

f in
p (x)

w (1)

conv. layer

activation, e.g.

parameters

pooling

lecun et al. 1989

w (2)

w (3)

w (4)

id98 consisting of l convolutional layers interleaved with pooling

(cid:32)qk   1(cid:88)

(cid:33)

g(k)
l

(x) =   

(g(k   1)
l(cid:48)

(cid:63) w(k)

l,l(cid:48) )(x)

l(cid:48)=1
  (x) = max{x, 0}
   lters of all layers w (1), . . . , w (l)

recti   ed linear unit (relu)

l = 1, . . . , qk
l(cid:48) = 1, . . . , qk   1

g(k)
l

(x) = (cid:107)g(k   1)

l

(x(cid:48)) : x(cid:48)     n (x)(cid:107)p

p = 1, 2, or    

pooling pooling pooling ...pooling

filter

input image

feature map

3 3 2 1 0 0 3 3 2 1 0 0 3 3 2 1 0 0 3 3 3 2 0 0 3 3 2 1 0 0 3 2 1 1 0 0 1 0 1 0 1 0 1 0 1 6 8 6 3 1 0 9 13 10 5 2 0 9 14 11 6 3 0 9 13 11 6 2 0 8 13 10 5 3 0 6 7 5 3 1 0 *=pooling

filter

input image

feature map

max pooling

3 3 2 1 0 0 3 3 2 1 0 0 3 3 2 1 0 0 3 3 3 2 0 0 3 3 2 1 0 0 3 2 1 1 0 0 1 0 1 0 1 0 1 0 1 6 8 6 3 1 0 9 13 10 5 2 0 9 14 11 6 3 0 9 13 11 6 2 0 8 13 10 5 3 0 6 7 5 3 1 0 13 10 2 14 11 3 13 10 3 *=key properties of id98s

convolutional    lters (translation invariance+self-similarity)

lecun et al. 1989

key properties of id98s

convolutional    lters (translation invariance+self-similarity)
multiple layers (compositionality)

lecun et al. 1989

key properties of id98s

convolutional    lters (translation invariance+self-similarity)
multiple layers (compositionality)
filters localized in space (locality)

lecun et al. 1989

key properties of id98s

convolutional    lters (translation invariance+self-similarity)
multiple layers (compositionality)
filters localized in space (locality)
o(1) parameters per    lter (independent of input image size n)

lecun et al. 1989

key properties of id98s

convolutional    lters (translation invariance+self-similarity)
multiple layers (compositionality)
filters localized in space (locality)
o(1) parameters per    lter (independent of input image size n)
o(n) complexity per layer (   ltering done in the spatial domain)

lecun et al. 1989

key properties of id98s

convolutional    lters (translation invariance+self-similarity)
multiple layers (compositionality)
filters localized in space (locality)
o(1) parameters per    lter (independent of input image size n)
o(n) complexity per layer (   ltering done in the spatial domain)
o(log n) layers in classi   cation tasks

lecun et al. 1989

going non-euclidean

prototypical non-euclidean objects

manifolds

graphs

challenges of geometric deep learning

extend neural network techniques to graph- or manifold-structured
data

assumption: non-euclidean data are locally stationary and manifest
hierarchical structures

how to de   ne compositionality? (convolution and pooling on
graphs/manifolds)

how to make them fast? (linear complexity)

id207 in one minute

weighted undirected graph g with
vertices v = {1, . . . , n}, edges e     v   v
and edge weights wij     0 for (i, j)     e

wij

id207 in one minute

weighted undirected graph g with
vertices v = {1, . . . , n}, edges e     v   v
and edge weights wij     0 for (i, j)     e

fi

functions over the vertices
l2(v) = {f : v     r}

id207 in one minute

weighted undirected graph g with
vertices v = {1, . . . , n}, edges e     v   v
and edge weights wij     0 for (i, j)     e

fi

functions over the vertices
l2(v) = {f : v     r} represented as
vectors f = (f1, . . . , fn)

id207 in one minute

weighted undirected graph g with
vertices v = {1, . . . , n}, edges e     v   v
and edge weights wij     0 for (i, j)     e

fi

functions over the vertices
l2(v) = {f : v     r} represented as
vectors f = (f1, . . . , fn)

hilbert space with inner product

(cid:104)f, g(cid:105)l2(v) =

fi gi = f(cid:62)g

(cid:88)

i   v

graph laplacian

unnormalized laplacian     : l2(v)     l2(v)

(   f )i =

wij(fi     fj)

(cid:88)

j:(i,j)   e

fj

fi

graph laplacian

(   f )i =

unnormalized laplacian     : l2(v)     l2(v)

(cid:88)
(cid:88)

j:(i,j)   e

wij(fi     fj)

wij     (cid:88)

= fi

wijfj

j:(i,j)   e

j:(i,j)   e

(up to scale) di   erence between f and its
local average

fj

fi

graph laplacian

unnormalized laplacian     : l2(v)     l2(v)

(   f )i =

wij(fi     fj)

(cid:88)

j:(i,j)   e

fj

fi

(up to scale) di   erence between f and its
local average

represented as a positive semi-de   nite
n    n matrix     = d     w where

w = (wij) and d = diag((cid:80)

j(cid:54)=i wij)

graph laplacian

unnormalized laplacian     : l2(v)     l2(v)

(   f )i =

wij(fi     fj)

(cid:88)

j:(i,j)   e

fj

fi

(up to scale) di   erence between f and its
local average

represented as a positive semi-de   nite
n    n matrix     = d     w where

w = (wij) and d = diag((cid:80)

j(cid:54)=i wij)

dirichlet energy of f
(cid:107)f(cid:107)2g =

n(cid:88)

1
2

ij=1

wij(fi     fj)2 = f(cid:62)   f

measures the smoothness of f (how fast it changes locally)

riemannian manifolds in one minute

manifold x = topological space
tangent plane txx = local
euclidean representation of
manifold x around x

txx

x

x

riemannian manifolds in one minute

manifold x = topological space
tangent plane txx = local
euclidean representation of
manifold x around x
riemannian metric describes the
local intrinsic structure at x

(cid:104)  ,  (cid:105)txx : txx    txx     r

txx

x

x

riemannian manifolds in one minute

manifold x = topological space
tangent plane txx = local
euclidean representation of
manifold x around x
riemannian metric describes the
local intrinsic structure at x

(cid:104)  ,  (cid:105)txx : txx    txx     r

scalar    elds f : x     r and vector
   elds f : x     tx

f

f

riemannian manifolds in one minute

manifold x = topological space
tangent plane txx = local
euclidean representation of
manifold x around x
riemannian metric describes the
local intrinsic structure at x

(cid:104)  ,  (cid:105)txx : txx    txx     r

scalar    elds f : x     r and vector
   elds f : x     tx
hilbert spaces with inner products
(cid:104)f, g(cid:105)l2(x ) =
(cid:104)f, g(cid:105)l2(tx ) =

f (x)g(x)dx
(cid:104)f (x), g(x)(cid:105)txx dx

(cid:90)
(cid:90)

x

x

f

f

manifold laplacian

laplacian     : l2(x )     l2(x )

   f (x) =    div    f (x)

where gradient     : l2(x )    l2(tx )
and divergence div : l2(tx )    l2(x )
are adjoint operators
(cid:104)f,   f(cid:105)l2(tx ) = (cid:104)   divf, f(cid:105)l2(x )

x
div   f (x)

f

manifold laplacian

laplacian     : l2(x )     l2(x )

   f (x) =    div    f (x)

where gradient     : l2(x )    l2(tx )
and divergence div : l2(tx )    l2(x )
are adjoint operators
(cid:104)f,   f(cid:105)l2(tx ) = (cid:104)   divf, f(cid:105)l2(x )

laplacian is self-adjoint

(cid:104)   f, f(cid:105)l2(x ) = (cid:104)f,    f(cid:105)l2(x )

x
div   f (x)

f

manifold laplacian

laplacian     : l2(x )     l2(x )

   f (x) =    div    f (x)

where gradient     : l2(x )    l2(tx )
and divergence div : l2(tx )    l2(x )
are adjoint operators
(cid:104)f,   f(cid:105)l2(tx ) = (cid:104)   divf, f(cid:105)l2(x )

laplacian is self-adjoint

(cid:104)   f, f(cid:105)l2(x ) = (cid:104)f,    f(cid:105)l2(x )

x
div   f (x)

f

continuous limit of graph
laplacian under some conditions

(cid:90)

dirichlet energy of f
(cid:104)   f,   f(cid:105)l2(tx ) =
measures the smoothness of f (how fast it changes locally)

f (x)   f (x)dx

x

orthogonal bases on graphs

find the smoothest orthogonal basis {  1, . . . ,   n}     l2(v)

min
  1
min
  k

edir(  1)

edir(  k)

s.t. (cid:107)  1(cid:107) = 1
s.t. (cid:107)  k(cid:107) = 1, k = 2, 3, . . . n
  k     span{  1, . . . ,   k   1}

orthogonal bases on graphs

find the smoothest orthogonal basis {  1, . . . ,   n}     l2(v)

trace(  (cid:62)     )

s.t.   (cid:62)   = i

min

     rn  n

orthogonal bases on graphs

find the smoothest orthogonal basis {  1, . . . ,   n}     l2(v)

trace(  (cid:62)     )

s.t.   (cid:62)   = i

min

     rn  n

solution:    = laplacian eigenvectors

laplacian eigenvectors and eigenvalues

eigendecomposition of a graph laplacian
    =       (cid:62)

where    = (  1, . . . ,   n) are orthogonal eigenvectors (  (cid:62)   = i) and
   = diag(  1, . . . ,   n) the corresponding non-negative eigenvalues

laplacian eigenvectors and eigenvalues

eigendecomposition of a graph laplacian
    =       (cid:62)

where    = (  1, . . . ,   n) are orthogonal eigenvectors (  (cid:62)   = i) and
   = diag(  1, . . . ,   n) the corresponding non-negative eigenvalues

+1

0

   1

     

first eigenfunctions of 1d euclidean laplacian

0

+  

laplacian eigenvectors and eigenvalues

eigendecomposition of a graph laplacian
    =       (cid:62)

where    = (  1, . . . ,   n) are orthogonal eigenvectors (  (cid:62)   = i) and
   = diag(  1, . . . ,   n) the corresponding non-negative eigenvalues

  1

  2

  3

  4

first eigenfunctions of a graph laplacian

0maxminlaplacian eigenvectors and eigenvalues

eigendecomposition of a graph laplacian
    =       (cid:62)

where    = (  1, . . . ,   n) are orthogonal eigenvectors (  (cid:62)   = i) and
   = diag(  1, . . . ,   n) the corresponding non-negative eigenvalues

  1

  2

  3

  4

first eigenfunctions of a manifold laplacian

0maxminfourier analysis on euclidean spaces

a function f : [     ,   ]     r can be written as a fourier series

(cid:88)

k   0

1
2  

(cid:90)   

     

f (x) =

f (x(cid:48))e   ikx(cid:48)

dx(cid:48)eikx

fourier analysis on euclidean spaces

a function f : [     ,   ]     r can be written as a fourier series

(cid:88)

k   0

f (x) =

(cid:104)f, eikx(cid:105)l2([     ,  ])

eikx

=

  f1

+

  f2

+

  f3

+ . . .

fourier analysis on euclidean spaces

a function f : [     ,   ]     r can be written as a fourier series

(cid:88)

k   0

f (x) =

(cid:124)

(cid:123)(cid:122)

(cid:104)f, eikx(cid:105)l2([     ,  ])
  fk fourier coe   cient

(cid:125)

eikx

=

  f1

+

  f2

+

  f3

+ . . .

fourier analysis on euclidean spaces

a function f : [     ,   ]     r can be written as a fourier series

(cid:88)

k   0

f (x) =

(cid:124)

(cid:123)(cid:122)

(cid:104)f, eikx(cid:105)l2([     ,  ])
  fk fourier coe   cient

(cid:125)

eikx

=

  f1

+

  f2

+

  f3

+ . . .

fourier basis = laplacian eigenfunctions:     d2

dx2 eikx = k2eikx

fourier analysis on graphs

a function f : v     r can be written as fourier series

n(cid:88)

k=1

(cid:124)

f =

(cid:104)f,   k(cid:105)l2(v)

  k

(cid:123)(cid:122)

  fk

(cid:125)

fourier basis = laplacian eigenfunctions:      k =   k  k

fourier analysis on graphs

a function f : v     r can be written as fourier series

n(cid:88)

k=1

(cid:124)

f =

(cid:104)f,   k(cid:105)l2(v)

  k

(cid:123)(cid:122)

  fk

(cid:125)

fourier basis = laplacian eigenfunctions:      k =   k  k

  k = frequency

  1
  4
first fourier basis elements of a manifold.

  2

  3

0maxminconvolution: euclidean space

given two functions f, g : [     ,   ]     r their convolution is a function

(cid:90)   

     

(f (cid:63) g)(x) =

f (x(cid:48))g(x     x(cid:48))dx(cid:48)

convolution: euclidean space

given two functions f, g : [     ,   ]     r their convolution is a function

(cid:90)   

     

(f (cid:63) g)(x) =

f (x(cid:48))g(x     x(cid:48))dx(cid:48)

shift-invariance: f (x     x0) (cid:63) g(x) = (f (cid:63) g)(x     x0)

convolution: euclidean space

given two functions f, g : [     ,   ]     r their convolution is a function

(cid:90)   

     

(f (cid:63) g)(x) =

f (x(cid:48))g(x     x(cid:48))dx(cid:48)

shift-invariance: f (x     x0) (cid:63) g(x) = (f (cid:63) g)(x     x0)

convolution theorem: fourier transform diagonalizes the convolution
operator

convolution: euclidean space

given two functions f, g : [     ,   ]     r their convolution is a function

(cid:90)   

     

(f (cid:63) g)(x) =

f (x(cid:48))g(x     x(cid:48))dx(cid:48)

shift-invariance: f (x     x0) (cid:63) g(x) = (f (cid:63) g)(x     x0)

convolution theorem: fourier transform diagonalizes the convolution
operator     convolution can be computed in the fourier domain as

(cid:92)(f (cid:63) g) =   f      g

convolution: euclidean space

given two functions f, g : [     ,   ]     r their convolution is a function

(cid:90)   

     

(f (cid:63) g)(x) =

f (x(cid:48))g(x     x(cid:48))dx(cid:48)

shift-invariance: f (x     x0) (cid:63) g(x) = (f (cid:63) g)(x     x0)

convolution theorem: fourier transform diagonalizes the convolution
operator     convolution can be computed in the fourier domain as

(cid:92)(f (cid:63) g) =   f      g

e   cient computation using fft (o(n log n) complexity)

convolution theorem

convolution of two vectors f = (f1, . . . , fn)(cid:62) and g = (g1, . . . , gn)(cid:62)

                     

f (cid:63) g =

g1
gn
...
g3
g2

g2
g1
...
g4
g3

. . .
g2
. . .
. . .
. . .

. . .
. . .
. . .
g1
. . .

gn
gn   1
...
g2
g1

                     

         

          f1

...
fn

convolution theorem

convolution of two vectors f = (f1, . . . , fn)(cid:62) and g = (g1, . . . , gn)(cid:62)

f (cid:63) g =

                     

(cid:124)

g1
gn
...
g3
g2

g2
g1
...
g4
g3

. . .
. . .
. . .
g1
. . .

gn
gn   1
...
g2
g1

. . .
g2
. . .
. . .
. . .

(cid:123)(cid:122)

circulant matrix

         

          f1

...
fn

                     

(cid:125)

convolution theorem

convolution of two vectors f = (f1, . . . , fn)(cid:62) and g = (g1, . . . , gn)(cid:62)

f (cid:63) g =

                     

(cid:124)

g1
gn
...
g3
g2

g2
g1
...
g4
g3

. . .
. . .
. . .
g1
. . .

gn
gn   1
...
g2
g1

. . .
g2
. . .
. . .
. . .

(cid:123)(cid:122)

diagonalized by fourier basis

         

          f1

...
fn

                     

(cid:125)

convolution theorem

convolution of two vectors f = (f1, . . . , fn)(cid:62) and g = (g1, . . . , gn)(cid:62)

f (cid:63) g =

                     

         

          f1

...
fn

g1
gn
...
g3
g2

                     
            g1

=   

g2
g1
...
g4
g3

. . .
g2
. . .
. . .
. . .

. . .

  gn

. . .
. . .
. . .
g1
. . .

gn
gn   1
...
g2
g1

            (cid:62)f

convolution theorem

convolution of two vectors f = (f1, . . . , fn)(cid:62) and g = (g1, . . . , gn)(cid:62)

f (cid:63) g =

                     

         

          f1

...
fn

g1
gn
...
g3
g2

                     
            g1

=   

g2
g1
...
g4
g3

. . .
g2
. . .
. . .
. . .

. . .

  gn

. . .
. . .
. . .
g1
. . .

         

gn
gn   1
...
g2
g1

         

            f1

...
  fn

convolution theorem

convolution of two vectors f = (f1, . . . , fn)(cid:62) and g = (g1, . . . , gn)(cid:62)

f (cid:63) g =

                     

         

          f1

...
fn

g2
g1
...
g4
g3

g1
gn
...
g3
g2

                     
            f1      g1

...

  fn      gn

=   

. . .
. . .
. . .
g1
. . .

gn
gn   1
...
g2
g1

. . .
g2
. . .
. . .
. . .

         

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

f (cid:63) g =

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

(cid:88)

k   1

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

(cid:88)

k   1

(cid:124)

f (cid:63) g =

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

(cid:123)(cid:122)

(cid:125)

product in the fourier domain

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

f (cid:63) g =

(cid:88)
(cid:124)

k   1

(cid:124)

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

product in the fourier domain

inverse fourier transform

(cid:123)(cid:122)
(cid:123)(cid:122)

(cid:125)

  k

(cid:125)

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

f (cid:63) g =

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

(cid:88)

k   1

in matrix-vector notation

f (cid:63) g =    (  (cid:62)g)     (  (cid:62)f )

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

f (cid:63) g =

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

(cid:88)

k   1

in matrix-vector notation

f (cid:63) g =    diag(  g1, . . . ,   gn)  (cid:62)

f

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

f (cid:63) g =

in matrix-vector notation

f (cid:63) g =    diag(  g1, . . . ,   gn)  (cid:62)

f

(cid:123)(cid:122)

g

(cid:125)

(cid:88)

k   1

(cid:124)

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

f (cid:63) g =

in matrix-vector notation

f (cid:63) g =    diag(  g1, . . . ,   gn)  (cid:62)

f

(cid:123)(cid:122)

g

(cid:125)

not shift-invariant! (g has no circulant structure)

(cid:88)

k   1

(cid:124)

spectral convolution

spectral convolution of f , g     l2(v) can be de   ned by analogy

(cid:104)f ,   k(cid:105)l2(v)(cid:104)g,   k(cid:105)l2(v)

  k

f (cid:63) g =

in matrix-vector notation

f (cid:63) g =    diag(  g1, . . . ,   gn)  (cid:62)

f

(cid:123)(cid:122)

g

(cid:125)

not shift-invariant! (g has no circulant structure)

filter coe   cients depend on basis   1, . . . ,   n

(cid:88)

k   1

(cid:124)

instability under deformation

function f

instability under deformation

   edge detecting    spectral    lter   g  (cid:62)f

instability under deformation

same spectral    lter, di   erent basis   g  (cid:62)f

instability under deformation

high-frequency laplacian eigenvector   50

spectral domain

geometric deep learning methods

spectral graph id98

convolution expressed in the spectral domain
g =   w  (cid:62)f

where w is n    n diagonal matrix of learnable spectral    lter coe   cients

bruna, zaremba, szlam, lecun 2014

spectral graph id98

convolution expressed in the spectral domain
g =   w  (cid:62)f

where w is n    n diagonal matrix of learnable spectral    lter coe   cients

filters are basis-dependent     does not generalize across graphs!

bruna, zaremba, szlam, lecun 2014

spectral graph id98

convolution expressed in the spectral domain
g =   w  (cid:62)f

where w is n    n diagonal matrix of learnable spectral    lter coe   cients

filters are basis-dependent     does not generalize across graphs!
o(n) parameters per layer

bruna, zaremba, szlam, lecun 2014

spectral graph id98

convolution expressed in the spectral domain
g =   w  (cid:62)f

where w is n    n diagonal matrix of learnable spectral    lter coe   cients

filters are basis-dependent     does not generalize across graphs!
o(n) parameters per layer
o(n2) computation of forward and inverse fourier transforms
  (cid:62),    (no fft on graphs)

bruna, zaremba, szlam, lecun 2014

spectral graph id98

convolution expressed in the spectral domain
g =   w  (cid:62)f

where w is n    n diagonal matrix of learnable spectral    lter coe   cients

filters are basis-dependent     does not generalize across graphs!
o(n) parameters per layer
o(n2) computation of forward and inverse fourier transforms
  (cid:62),    (no fft on graphs)
no guarantee of spatial localization of    lters

bruna, zaremba, szlam, lecun 2014

localization and smoothness

vanishing moments: in the euclidean setting

(cid:90) +   

      

|x|2k|f (x)|2dx =

(cid:90) +   

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    k   f (  )

     k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d  

bruna, zaremba, szlam, lecun 2014; hena   , bruna, lecun 2015

localization and smoothness

vanishing moments: in the euclidean setting

(cid:90) +   

      

|x|2k|f (x)|2dx =

(cid:90) +   

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    k   f (  )

     k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d  

localization in space = smoothness in frequency domain

bruna, zaremba, szlam, lecun 2014; hena   , bruna, lecun 2015

localization and smoothness

vanishing moments: in the euclidean setting

(cid:90) +   

      

|x|2k|f (x)|2dx =

(cid:90) +   

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    k   f (  )

     k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d  

localization in space = smoothness in frequency domain

parametrize the    lter using a smooth spectral transfer function    (  )

bruna, zaremba, szlam, lecun 2014; hena   , bruna, lecun 2015

localization and smoothness

vanishing moments: in the euclidean setting

(cid:90) +   

      

|x|2k|f (x)|2dx =

(cid:90) +   

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    k   f (  )

     k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d  

localization in space = smoothness in frequency domain

parametrize the    lter using a smooth spectral transfer function    (  )

application of the    lter

  (   )f =     (  )  (cid:62)f

bruna, zaremba, szlam, lecun 2014; hena   , bruna, lecun 2015

localization and smoothness

vanishing moments: in the euclidean setting

(cid:90) +   

      

|x|2k|f (x)|2dx =

(cid:90) +   

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    k   f (  )

     k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d  

localization in space = smoothness in frequency domain

parametrize the    lter using a smooth spectral transfer function    (  )

application of the    lter

  (   )f =   

            (  1)

            (cid:62)f

. . .

  (  n)

bruna, zaremba, szlam, lecun 2014; hena   , bruna, lecun 2015

localization and smoothness

vanishing moments: in the euclidean setting

(cid:90) +   

      

|x|2k|f (x)|2dx =

(cid:90) +   

      

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    k   f (  )

     k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

d  

localization in space = smoothness in frequency domain

parametrize the    lter using a smooth spectral transfer function    (  )

application of the parametric    lter with learnable parameters   

              (  1)

            (cid:62)f

    (   )f =   

. . .

    (  n)

bruna, zaremba, szlam, lecun 2014; hena   , bruna, lecun 2015

localization and smoothness

+1

)
  
(
  

   1

0

frequency

  500

non-smooth spectral    lter (delocalized in space)

localization and smoothness

+1

)
  
(
  

   1

0

frequency

  500

smooth spectral    lter (localized in space)

spectral graph id98 with polynomial    lters (chebnet)

represent spectral transfer function as a polynomial or order r

r(cid:88)

    (  ) =

  j  j

where    = (  0, . . . ,   r)(cid:62) is the vector of    lter parameters

j=0

de   errard, bresson, vandergheynst 2016

spectral graph id98 with polynomial    lters (chebnet)

represent spectral transfer function as a polynomial or order r

r(cid:88)

    (  ) =

  j  j

where    = (  0, . . . ,   r)(cid:62) is the vector of    lter parameters

j=0

o(1) parameters per layer

de   errard, bresson, vandergheynst 2016

spectral graph id98 with polynomial    lters (chebnet)

represent spectral transfer function as a polynomial or order r

r(cid:88)

    (  ) =

  j  j

where    = (  0, . . . ,   r)(cid:62) is the vector of    lter parameters

j=0

o(1) parameters per layer
filters have guaranteed r-hops support

de   errard, bresson, vandergheynst 2016

spectral graph id98 with polynomial    lters (chebnet)

represent spectral transfer function as a polynomial or order r

r(cid:88)

    (  ) =

  j  j

where    = (  0, . . . ,   r)(cid:62) is the vector of    lter parameters

j=0

o(1) parameters per layer
filters have guaranteed r-hops support
no explicit computation of   (cid:62),        o(nr) computational
complexity

de   errard, bresson, vandergheynst 2016

graph convolutional net: simpli   ed chebnet

use chebychev polynomials of degree r = 2 and assume   n     2

    (   )f =   0f +   1(        i)f

=   0f       1d   1/2wd   1/2f

kipf, welling 2016

graph convolutional net: simpli   ed chebnet

use chebychev polynomials of degree r = 2 and assume   n     2

    (   )f =   0f +   1(        i)f

=   0f       1d   1/2wd   1/2f

further constrain    =   0 =      1 to obtain a single-parameter    lter

    (   )f =   (i + d   1/2wd   1/2)f

kipf, welling 2016

graph convolutional net: simpli   ed chebnet

use chebychev polynomials of degree r = 2 and assume   n     2

    (   )f =   0f +   1(        i)f

=   0f       1d   1/2wd   1/2f

further constrain    =   0 =      1 to obtain a single-parameter    lter

    (   )f =   (i + d   1/2wd   1/2)f

caveat: the eigenvalues of i + d   1/2wd   1/2 are now in [0, 2]
    repeated application of the    lter can result in numerical instability

kipf, welling 2016

graph convolutional net: simpli   ed chebnet

use chebychev polynomials of degree r = 2 and assume   n     2

    (   )f =   0f +   1(        i)f

=   0f       1d   1/2wd   1/2f

further constrain    =   0 =      1 to obtain a single-parameter    lter

    (   )f =   (i + d   1/2wd   1/2)f

caveat: the eigenvalues of i + d   1/2wd   1/2 are now in [0, 2]
    repeated application of the    lter can result in numerical instability

remedy: apply a reid172

where   w = w + i and   d = diag((cid:80)

    (   )f =      d   1/2   w   d   1/2f

j(cid:54)=i   wij).

kipf, welling 2016

example: id191

figure: monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

example: id191

method
manifold id1733
semide   nite embedding4
label propagation5
deepwalk6
planeto 
id1978

pubmed2

cora1
70.7%
59.5%
71.1%
59.0%
63.0%
68.0%
65.3%
67.2%
75.7%
77.2%
81.6% 78.7%

classi   cation accuracy of di   erent methods on citation network datasets

data: 1,2sen et al. 2008; methods: 3belkin et al. 2006; 4weston et al. 2012; 5zhu et
al. 2003; 6perozzi et al. 2014; 7yang et al. 2016; 8kipf, welling 2016

spectral graph id98 demo

community graph

15

10

5

0

0

200

400

unnorm. laplacian eigenvalues

1

0.5

0

synthetic graph with 15 communities

normalized laplacian eigenvalues

0

200

400

levie et al. 2017

community graph

1

|
)
  
(
  
|

0

  min

example of chebyshev    lters learned on the 15-communities graph

  max

levie et al. 2017

community graph

1

|
)
  
(
  
|

0

  min

example of chebyshev    lters learned on the 15-communities graph

  max

hard to localize polynomials in narrow frequency bands!

levie et al. 2017

cayley polynomials

cayley polynomials of order r are a family of real-valued rational
functions with complex coe   cients

(cid:110) r(cid:88)

j=1

(cid:16)        i

(cid:17)j(cid:111)

cj

   + i

p(  ) = c0 + 2re

(cayley 1846); levie et al. 2017

cayley polynomials

cayley polynomials of order r are a family of real-valued rational
functions with complex coe   cients

p(  ) = c0 + 2re

cj

(cid:110) r(cid:88)

j=1

(cid:17)j(cid:111)

(cid:16)        i
(cid:124)(cid:123)(cid:122)(cid:125)

   + i

c(  )

cayley transform c(  ) =      i

  +i is a smooth bijection from r to eir \ {1}

(cayley 1846); levie et al. 2017

cayley polynomials

cayley polynomials of order r are a family of real-valued rational
functions with complex coe   cients

p(  ) = c0 + 2re

cj

(cid:110) r(cid:88)

j=1

(cid:17)j(cid:111)

(cid:16)        i
(cid:124)(cid:123)(cid:122)(cid:125)

   + i

c(  )

cayley transform c(  ) =      i
since z   1 =   z for z     eir
conjugate-even laurent polynomial w.r.t. c(  )

  +i is a smooth bijection from r to eir \ {1}
and 2re{z} = z +   z, we can rewrite p(  ) as a
r(cid:88)

cj c j(  )

+   cj c   j(  )

p(  ) = c0 +

j=1

(cayley 1846); levie et al. 2017

cayley polynomials

cayley polynomials of order r are a family of real-valued rational
functions with complex coe   cients

p(  ) = c0 + 2re

cj

(cid:110) r(cid:88)

j=1

(cid:17)j(cid:111)

(cid:16)        i
(cid:124)(cid:123)(cid:122)(cid:125)

   + i

c(  )

cayley transform c(  ) =      i
since z   1 =   z for z     eir
conjugate-even laurent polynomial w.r.t. c(  )

  +i is a smooth bijection from r to eir \ {1}
and 2re{z} = z +   z, we can rewrite p(  ) as a
r(cid:88)

p(  ) = c0 +

(cid:124) (cid:123)(cid:122) (cid:125)

cj c j(  )

eij    

(cid:124) (cid:123)(cid:122) (cid:125)

+   cj c   j(  )
e   ij    

j=1

(cayley 1846); levie et al. 2017

cayley polynomials

cayley polynomials of order r are a family of real-valued rational
functions with complex coe   cients

p(  ) = c0 + 2re

cj

(cid:110) r(cid:88)

j=1

(cid:17)j(cid:111)

(cid:16)        i
(cid:124)(cid:123)(cid:122)(cid:125)

   + i

c(  )

cayley transform c(  ) =      i
since z   1 =   z for z     eir
conjugate-even laurent polynomial w.r.t. c(  )

  +i is a smooth bijection from r to eir \ {1}
and 2re{z} = z +   z, we can rewrite p(  ) as a
r(cid:88)
r(cid:88)

+   cj c   j(  )
e   ij    
cjeij     +   cje   ij    

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

cj c j(  )

eij    

j=1

= c0 +

p(  ) = c0 +

j=1

(cayley 1846); levie et al. 2017

cayley polynomials

cayley polynomials of order r are a family of real-valued rational
functions with complex coe   cients

p(  ) = c0 + 2re

cj

(cid:110) r(cid:88)

j=1

(cid:17)j(cid:111)

(cid:16)        i
(cid:124)(cid:123)(cid:122)(cid:125)

   + i

c(  )

cayley transform c(  ) =      i
since z   1 =   z for z     eir
real trigonometric polynomial w.r.t. c(  )

  +i is a smooth bijection from r to eir \ {1}
and 2re{z} = z +   z, we can rewrite p(  ) as a
r(cid:88)
r(cid:88)

+   cj c   j(  )
e   ij    

re{cj} cos(j    )     im{cj} sin(j    )

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

cj c j(  )

eij    

j=1

p(  ) = c0 +

= c0 + 2

j=1

(cayley 1846); levie et al. 2017

spectral zoom

applying cayley transform to the scaled laplacian h   
c(h   ) = (h        ii)(h    + ii)   1

results in a non-linear transformation of the eigenvalues (spectral zoom)

levie et al. 2017

spectral zoom

applying cayley transform to the scaled laplacian h   
c(h   ) = (h        ii)(h    + ii)   1

results in a non-linear transformation of the eigenvalues (spectral zoom)

m

i

0
   0.5

   1

m

i

0
   0.5

   1

   1    0.5 0
re

0.5

1

m

i

0
   0.5

   1

   1    0.5 0
re

0.5

1

   1    0.5 0
re

0.5

1

cayley transform c(h  ) for (left-to-right) h = 0.1, 1, and 10

of the 15-communities graph laplacian spectrum

levie et al. 2017

fast inversion

application of cayley    lter    (h   )f requires the solution or recursive
linear system

y0 = f

(h    + ii)yj = (h        ii)yj   1

j = 1, . . . , r

(jacobi 1834); levie et al. 2017

fast inversion

application of cayley    lter    (h   )f requires the solution or recursive
linear system

y0 = f

(h    + ii)yj = (h        ii)yj   1

j = 1, . . . , r

(stable since c(h   ) is unitary and has condition number = 1)

(jacobi 1834); levie et al. 2017

fast inversion

application of cayley    lter    (h   )f requires the solution or recursive
linear system

y0 = f

(h    + ii)yj = (h        ii)yj   1

j = 1, . . . , r

exact solution: o(n3) complexity

(jacobi 1834); levie et al. 2017

fast inversion

application of cayley    lter    (h   )f requires the solution or recursive
linear system

y0 = f

(h    + ii)yj = (h        ii)yj   1

j = 1, . . . , r

approximate solution   yj     yj using k jacobi iterations

= j  y(k)

j + diag   1(h    + ii)(h        ii)  yj   1

  y(k+1)
j
j = diag   1(h    + ii)(h        ii)  yj   1
  y(0)

with j =    diag   1(h    + ii)o   (h    + ii)

(jacobi 1834); levie et al. 2017

fast inversion

application of cayley    lter    (h   )f requires the solution or recursive
linear system

y0 = f

(h    + ii)yj = (h        ii)yj   1

j = 1, . . . , r

approximate solution   yj     yj using k jacobi iterations

= j  y(k)

j + diag   1(h    + ii)(h        ii)  yj   1

  y(k+1)
j
j = diag   1(h    + ii)(h        ii)  yj   1
  y(0)

with j =    diag   1(h    + ii)o   (h    + ii)

application of the approximate cayley    lter

r(cid:88)

cj   yj        (h   )f

has o(rk|e|) complexity

j=0

(jacobi 1834); levie et al. 2017

fast inversion

application of cayley    lter    (h   )f requires the solution or recursive
linear system

y0 = f

(h    + ii)yj = (h        ii)yj   1

j = 1, . . . , r

approximate solution   yj     yj using k jacobi iterations

= j  y(k)

j + diag   1(h    + ii)(h        ii)  yj   1

  y(k+1)
j
j = diag   1(h    + ii)(h        ii)  yj   1
  y(0)

with j =    diag   1(h    + ii)o   (h    + ii)

application of the approximate cayley    lter

r(cid:88)

cj   yj        (h   )f

has o(rkn) complexity for a sparse graph

j=0

(jacobi 1834); levie et al. 2017

cayleynet: spectral graph id98 with cayley polynomials

represent spectral transfer function as a cayley polynomial or order r

(cid:110) r(cid:88)

cj(h       i)j(h   + i)   j(cid:111)

  c,h(  ) = c0 + 2re

where the    lter parameters are the vector of real/complex coe   cients
c = (c0, . . . , cr)(cid:62) and the spectral zoom h

j=1

levie et al. 2017

cayleynet: spectral graph id98 with cayley polynomials

represent spectral transfer function as a cayley polynomial or order r

(cid:110) r(cid:88)

cj(h       i)j(h   + i)   j(cid:111)

  c,h(  ) = c0 + 2re

where the    lter parameters are the vector of real/complex coe   cients
c = (c0, . . . , cr)(cid:62) and the spectral zoom h

j=1

o(1) parameters per layer
filters have guaranteed exponential spatial decay
o(nr) computational complexity with jacobi approximate
inversion (assuming sparsely-connected graph)

levie et al. 2017

cayleynet: spectral graph id98 with cayley polynomials

represent spectral transfer function as a cayley polynomial or order r

(cid:110) r(cid:88)

cj(h       i)j(h   + i)   j(cid:111)

  c,h(  ) = c0 + 2re

where the    lter parameters are the vector of real/complex coe   cients
c = (c0, . . . , cr)(cid:62) and the spectral zoom h

j=1

o(1) parameters per layer
filters have guaranteed exponential spatial decay
o(nr) computational complexity with jacobi approximate
inversion (assuming sparsely-connected graph)
spectral zoom property allowing to better localize in frequency

levie et al. 2017

cayleynet: spectral graph id98 with cayley polynomials

represent spectral transfer function as a cayley polynomial or order r

(cid:110) r(cid:88)

cj(h       i)j(h   + i)   j(cid:111)

  c,h(  ) = c0 + 2re

where the    lter parameters are the vector of real/complex coe   cients
c = (c0, . . . , cr)(cid:62) and the spectral zoom h

j=1

o(1) parameters per layer
filters have guaranteed exponential spatial decay
o(nr) computational complexity with jacobi approximate
inversion (assuming sparsely-connected graph)
spectral zoom property allowing to better localize in frequency
richer class of    lters than chebyshev for the same order

levie et al. 2017

chebyshev vs cayley

1

|
)
  
(
  
|

0

  min

example of chebyshev    lters (order r = 3) on euclidean grid

  max

chebyshev vs cayley

1

|
)
  
(
  
|

0

  min

example of chebyshev    lters (order r = 7) on euclidean grid

  max

chebyshev vs cayley

1

|
)
  
(
  
|

0

  min

example of cayley    lters (order r = 3) on euclidean grid

  max

chebyshev vs cayley: community graph example

synthetic graph with 15 communities

levie et al. 2017

chebyshev vs cayley: community graph example

1

|
)
  
(
  
|

0

  min

example of chebyshev    lters learned on the 15-communities graph

  max

levie et al. 2017

chebyshev vs cayley: community graph example

1

|
)
  
(
  
|

0

  min

example of cayley    lters learned on the 15-communities graph

  max

levie et al. 2017

chebyshev vs cayley: community graph example

100

80

60

40

20

%
y
c
a
r
u
c
c
a

chebnet
cayleynet

1

3

5

7

order r

9

11

13

community detection accuracy of chebnet and cayleynet

on the synthetic 15-community graph

levie et al. 2017

computational complexity of approximate inversion

training

testing

0.5

)
c
e
s
(

e
m
t

i

0

full

13

9

5

1

cheb

0.1

)
c
e
s
(

e
m
t

i

0

full

13

9

5

1

cheb

1

3

5

7

9

11

13

1

3

5

7

9

11

13

order r

order r

training and testing computational complexity of cayleynet with

approximate jacobi inversion for    lters of di   erent order

levie et al. 2017

computational complexity of approximate inversion

0.5

)
c
e
s
(

e
m
t

i

0

training

testing

full

13

9

5

1

cheb

0.1

)
c
e
s
(

e
m
t

i

0

full

13

9

5

1

cheb

200

400

600

800

1,000

200

600

1,000

#vertices n

#vertices n

training and testing computational complexity of cayleynet with

approximate jacobi inversion for graphs of di   erent size

levie et al. 2017

accuracy of approximate inversion

100

f u l l

80

13

%
y
c
a
r
u
c
c
a

60

40

20

9

5

1

c h e b

1

3

5

7

order r

9

11

13

community detection accuracy of cayleynet using approximate jacobi inversion

on the synthetic 15-community graph

levie et al. 2017

chebyshev vs cayley: cora example

normalized laplacian

unnormalized laplacian

%
88
y
c
a
r
u
86
c
c
a

84

87.1

87.9

86.9

86.9

85.2

87.1

1

2

3

order r

100

80

60

40

20

0

85.1

87.7

27.5

86.0

15.2

86.8

1

2

3

order r

accuracy of chebnet (blue) and cayleynet (orange) on cora dataset

levie et al. 2017; data: set et al. 2008

limitations of id106

poor generalization across non-isometric domains unless kernels are
localized

limitations of id106

poor generalization across non-isometric domains unless kernels are
localized (can be remedied to some extent with spectral transformer
networks1)

1yi, su, guo, guibas 2017

limitations of id106

poor generalization across non-isometric domains unless kernels are
localized (can be remedied to some extent with spectral transformer
networks1)

spectral kernels are isotropic due to rotation invariance of the
laplacian

1yi, su, guo, guibas 2017

only rotationally-symmetric kernels!

example of chebyshev    lters (order r = 7) on euclidean grid

anisotropic kernels on manifolds

scale t

orientation   

elongation   

examples of anisotropic heat kernels on a manifold

boscaini, masci, rodol`a, bronstein, cremers 2016

limitations of id106

poor generalization across non-isometric domains unless kernels are
localized (can be remedied to some extent with spectral transformer
networks1)

spectral kernels are isotropic due to rotation invariance of the
laplacian (on manifolds, anisotropic laplacians can be constructed2)

1yi, su, guo, guibas 2017; 2boscaini, masci, rodol`a, bronstein, cremers 2016

limitations of id106

poor generalization across non-isometric domains unless kernels are
localized (can be remedied to some extent with spectral transformer
networks1)

spectral kernels are isotropic due to rotation invariance of the
laplacian (on manifolds, anisotropic laplacians can be constructed2)

only undirected graphs, as symmetry of the laplacian matrix is
assumed

1yi, su, guo, guibas 2017; 2boscaini, masci, rodol`a, bronstein, cremers 2016

directed graphs

                                                

.
1
.
.
.
.
.
.
.
.

1
.
.
.
.
1
1
1
.
.

1
1
.
.
.
.
.
.
.
.

1
1
.
.
.
.
.
.
.
.

                                                

.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1
1
.
.
.

.
.
.
.
.
1
.
.

.
1 1
.
.
1
.
.
.
.
.
.
.
.
.
.
.
.
.
1
.
.
.
1
1 1 1
.
.
.
.

.

directed graph

asymmetric adjacency matrix

benson et al. 2016

motif-based graph analysis

                                                

.
1
.
.
.
.
.
.
.
.

1
.
.
.
.
1
1
1
.
.

1
1
.
.
.
.
.
.
.
.

1
1
.
.
.
.
.
.
.
.

                                                

.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1
1
.
.
.

.
.
.
.
.
1
.
.

.
1 1
.
.
1
.
.
.
.
.
.
.
.
.
.
.
.
.
1
.
.
.
1
1 1 1
.
.
.
.

.

directed graph

asymmetric adjacency matrix

benson et al. 2016

graph motifs

motif-based graph analysis

                                                

.
3
1
1
1
.
.
.
.
.

3
.
1
1
1
1
1
.
.
.

1
1
.
.
.
.
.
.
.
.

1
1
.
.
.
.
.
.
.
.

                                                

.

.
.
.
.
.

.
.
1
.
.
.
1 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1 1
.
.
.
.
.
.
2 1
.
.
.
1
2
.
.
.
.
.
1 1
.

.
.
.
.
1
1
1
.

directed graph

motif adjacency matrix for m7

benson et al. 2016

graph motifs

motif laplacians

                                                

.
3
1
1
1
.
.
.
.
.

3
.
1
1
1
1
1
.
.
.

1
1
.
.
.
.
.
.
.
.

1
1
.
.
.
.
.
.
.
.

                                                

.

.
.
.
.
.

.
.
1
.
.
.
1 1 1
.
.
.
.
.
.
.
.
.
.
.
.
.
1 1 1
.
.
.
.
.
.
2 1
.
.
.
1
2
.
.
.
.
.
1 1
.

.
.
.
.
1
1
1
.

undirected weighted graph

motif adjacency matrix for m7

motif laplacian for motif k = 1, . . . , k

     k = i       d

   1/2
k

   1/2
  wk   d
k

benson et al. 2016

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures
anisotropic kernels

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures
anisotropic kernels
o(1) parameters per layer

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures
anisotropic kernels
1+kr+1
k   1

parameters per layer, intractable in practice

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures
anisotropic kernels
kr + 1 parameters per layer using recurrent multivariate
polynomials with dependent coe   cients

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures
anisotropic kernels
kr + 1 parameters per layer using recurrent multivariate
polynomials with dependent coe   cients
filters have guaranteed r-hops support

monti, otness, bronstein 2018

motifnet: multivariate polynomial

apply k-variate polynomial or order r to the motif laplacians

    (      1, . . . ,      k) =   0i +

  k1,...,kj

     k1             kj

r(cid:88)

(cid:88)

j=1

k1,...,kj   {1,...,k}

where    = (  0,   1, . . . ,   k,...,k) is the vector of    lter parameters

explicitly accounts for directed graph structures
anisotropic kernels
kr + 1 parameters per layer using recurrent multivariate
polynomials with dependent coe   cients
filters have guaranteed r-hops support
o(n) computational complexity

monti, otness, bronstein 2018

example: directed id191

figure: monti, otness, bronstein 2018; data: bojchevski, g  unnemann 2017

example: directed id191

%
y
c
a
r
u
c
c
a

65

60

55

50

1

2

3

4

0.55

0.60

0.62

0.63

0.54

0.60

0.63

order r

0.63

0.54

0.60

0.63

0.63

0.54

0.59

0.62

0.63

0.53

classi   cation accuracy on directed cora obtained with chebnet applied with
directed adjacency matrix w (blue) and w(cid:62) (green), and motifnet-m (red)

monti, otness, bronstein 2018; data: bojchevski, g  unnemann, 2017

applications in matrix completion

and recommender systems

matrix completion:    net   ix challenge   

4

s
e
i
v
o
m
m

3

5

5

1

3

n users

min

x   rm  n

rank(x)

s.t. xij = aij    ij        

matrix completion:    net   ix challenge   

4

s
e
i
v
o
m
m

3

5

5

1

3

n users

min

x   rm  n

(cid:107)x(cid:107)   

s.t. xij = aij    ij        

cand`es 2008

matrix completion:    net   ix challenge   

4

s
e
i
v
o
m
m

3

5

5

1

3

n users

min

x   rm  n

(cid:107)x(cid:107)    +   (cid:107)        (x     a)(cid:107)2

f

cand`es 2008

geometric matrix completion

s
e
i
v
o
m
m

5

5

...

2

5

5

...

1

user graph

1

2

...

5

min

x   rm  n

  (cid:107)        (x     a)(cid:107)2

f +   c tr(x   cx(cid:62))

kalofolias, bresson, bronstein, vandergheynst 2014

geometric matrix completion

s
e
i
v
o
m
m

5

5

...

2

5

5

...

1

user graph

1

2

...

5

min

x   rm  n

  (cid:107)        (x     a)(cid:107)2

(cid:125)
f +   c tr(x   cx(cid:62))

(cid:123)(cid:122)

(cid:124)

(cid:107)x(cid:107)2gc

kalofolias, bresson, bronstein, vandergheynst 2014

geometric matrix completion

h
p
a
r
g

e
i
v
o
m

5

5

4

4

5

5

5

1

user graph

1

2

4

5

min

x   rm  n

  (cid:107)        (x     a)(cid:107)2

(cid:125)
f +   c tr(x   cx(cid:62))

(cid:123)(cid:122)

(cid:124)

(cid:107)x(cid:107)2gc

+   r tr(x(cid:62)   rx)

(cid:124)

(cid:123)(cid:122)

(cid:107)x(cid:107)2gr

(cid:125)

kalofolias, bresson, bronstein, vandergheynst 2014

factorized matrix completion models

h(cid:62)

m users

s
e
i
v
o
m
n

w

  (cid:107)        (x     a)(cid:107)2

f +   c(cid:107)w(cid:107)2

f +   r(cid:107)h(cid:107)2

f

min

w   rm  s
h   rn  s

srebro, rennie, jaakkola 2004

factorized matrix completion models

h(cid:62)

m users

s
e
i
v
o
m
n

w

  (cid:107)        (x     a)(cid:107)2

f +   c(cid:107)w(cid:107)2

f +   r(cid:107)h(cid:107)2

f

min

w   rm  s
h   rn  s

o(n + m) variables instead of o(nm)

srebro, rennie, jaakkola 2004

factorized matrix completion models

h(cid:62)

m users

s
e
i
v
o
m
n

w

  (cid:107)        (x     a)(cid:107)2

f +   c(cid:107)w(cid:107)2

f +   r(cid:107)h(cid:107)2

f

min

w   rm  s
h   rn  s

o(n + m) variables instead of o(nm)
rank(x) = rank(wh(cid:62))     s by construction

srebro, rennie, jaakkola 2004

factorized geometric matrix completion models

h(cid:62)

user graph

h
p
a
r
g

e
i
v
o
m

w

  (cid:107)        (x     a)(cid:107)2

f +   ctr(h(cid:62)   ch) +   rtr(w(cid:62)   rw)

min

w   rm  s
h   rn  s

o(n + m) variables instead of o(nm)
rank(x) = rank(wh(cid:62))     s by construction

rao et al. 2015

how to learn    lters on

multiple graphs?

2d fourier transform

x

2d fourier transform

=

  

(  x1, . . . ,   xn)

  (cid:62)

x

column-wise trasform

2d fourier transform

=

  

  

  x

  (cid:62)

x

  

column-wise trasform + row-wise transform = 2d transform

multi-graph fourier transform

h
p
a
r
g
w
o
r

column graph

multi-graph fourier transform

  x =   (cid:62)

r x  c

where   c and   r are the eigenvectors of the column- and row-graph
laplacians    c and    r, respectively

monti, bresson, bronstein 2017

multi-graph convolution

h
p
a
r
g
w
o
r

column graph

multi-graph spectral convolution
x (cid:63) g =   r((  (cid:62)

r x  c)     (  (cid:62)

r g  c))  (cid:62)

c

monti, bresson, bronstein 2017

multi-graph convolution

h
p
a
r
g
w
o
r

column graph

multi-graph spectral convolution

x (cid:63) g =   r(   x       g)  (cid:62)

c

monti, bresson, bronstein 2017

multi-graph spectral    lters

h
p
a
r
g
w
o
r

column graph

multi-graph bi-variate polynomial    lter

y =     (x) =

  jj(cid:48)   j

r x   j(cid:48)

c

where    = (  jj(cid:48)) is the r    r matrix of    lter parameters

monti, bresson, bronstein 2017

r(cid:88)

j,j(cid:48)=1

multi-graph spectral    lters

r
  

r
  

r
  

r
  

  c
examples of multi-graph spectral    lters (shown is |   (  c,   r)|)

  c

  c

  c

monti, bresson, bronstein 2017

matrix completion with recurrent multi-graph id98

x(t+1) = x(t) + dx(t)

dx(t)

x

x(t)

mgid98

y(t)

id56

  

  

recurrent multigraph id98 (rmid98) architecture

(cid:107)x(t )

  ,  (cid:107)2gr

min
  ,  

for matrix completion
+ (cid:107)x(t )

  ,  (cid:107)2gc

+

  
2

(cid:107)        (x(t )

  ,       a)(cid:107)2

f

monti, bresson, bronstein 2017

incremental updates with id56

rmse=2.26

matrix completion results on a synthetic dataset (initialization)

monti, bresson, bronstein 2017

incremental updates with id56

rmse=0.52

matrix completion results on a synthetic dataset after t = 5 instances

monti, bresson, bronstein 2017

incremental updates with id56

rmse=0.38

matrix completion results on a synthetic dataset after t = 8 instances

monti, bresson, bronstein 2017

incremental updates with id56

rmse=0.01

matrix completion results on a synthetic dataset after t = 10 instances

monti, bresson, bronstein 2017

matrix completion methods comparison

movielens1

method
imc5
gmc6
mc7
grals8
srgid98 (cheb)9
srgid98 (cayley)10

1.653
0.996
0.973
0.945
0.929
0.922

flixster2 douban3 yahoo4

   
   
   

1.245
0.926

   

   
   
   

   
   
   

0.833
0.801

   

38.042
22.415

   

accuracy (rms error) of matrix completion methods on real data

data: 1miller et al. 2003; 2jamali, ester 2010; 3ma et al. 2011; 4dror et al. 2012
methods: 5jain, dhillon 2013; 6kalofolias, bresson, bronstein, vandergheynst 2014;
7cand`es, recht 2012; 8rao et al. 2015; 9monti, bresson, bronstein 2017; 10levie,
monti, bresson, bronstein 2017

multigraph id98 demo

graph pooling

g0 = g

g1

g2

coarsening structure

produce a sequence of coarsened graphs

(binary tree)

0 1 6 45 2370,1 6,7 2,3 4,5 0 3 2 1 0,1 2,3 4,5 6,7 0 1 0 1 2 3 4 5 6 7 2 3 0 1 0 1 graph pooling

g0 = g

g1

g2

coarsening structure

produce a sequence of coarsened graphs

max or average pooling of collapsed vertices

(binary tree)

0 1 6 45 2370,1 6,7 2,3 4,5 0 3 2 1 0,1 2,3 4,5 6,7 0 1 0 1 2 3 4 5 6 7 2 3 0 1 0 1 graph pooling

g0 = g

g1

g2

coarsening structure

(binary tree)

produce a sequence of coarsened graphs

max or average pooling of collapsed vertices

binary tree arrangement of node indices

0 1 6 45 2370,1 6,7 2,3 4,5 0 3 2 1 0,1 2,3 4,5 6,7 0 1 0 1 2 3 4 5 6 7 2 3 0 1 0 1 example: mnist digits classi   cation

regular grid

superpixels

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

example: mnist digits classi   cation

regular grid

superpixels

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

example: mnist digits classi   cation

regular grid

superpixels

(   xed graph, di   erent data)

(di   erent graph and data)

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

example: mnist digits classi   cation

dataset
   full grid
    1
4 grid

lenet-51 chebnet2
99.33%
98.59%

99.14%
97.51%

classi   cation accuracy of di   erent methods on mnist dataset

   all images have the same graph

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017; 1lecun et al. 1998;
2de   errard, bresson, vandergheynst 2016

example: mnist digits classi   cation

dataset
   full grid
    1
4 grid
300 superpixels
150 superpixels
75 superpixels

lenet-51 chebnet2
99.33%
98.59%

99.14%
97.51%
88.05%
80.94%
75.62%

-
-
-

classi   cation accuracy of di   erent methods on mnist dataset

   all images have the same graph

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017; 1lecun et al. 1998;
2de   errard, bresson, vandergheynst 2016

chebnet / cayleynet demo on

mnist

spatial domain (charting-based)
geometric deep learning methods

convolution

spatial domain

(f (cid:63)g)(x) =

f (x(cid:48))g(x   x(cid:48))dx(cid:48)

euclidean

(cid:90)   

     

non-euclidean

?

spectral domain

(cid:92)(f (cid:63) g)(  ) =   f (  )      g(  )

(cid:92)(f (cid:63) g)k = (cid:104)f,   k(cid:105)l2(x )(cid:104)g,   k(cid:105)l2(x )

   convolution theorem   

patch operators

x

x

image

manifold

boscaini, masci, bronstein, vandergheynst 2015

patch operators

u2

x

u1

u2

x

u1

image

manifold

boscaini, masci, bronstein, vandergheynst 2015

patch operators

u2

x

u1

u2

x(cid:48)

u1

image

manifold

boscaini, masci, bronstein, vandergheynst 2015

spatial convolution on manifolds

geodesic polar coordinates

u(x, y) = (  (x, y),   (x, y))

x

x

u1 =   

u2 =   

monti, boscaini, rodol`a, svoboda, bronstein 2017

spatial convolution on manifolds

geodesic polar coordinates

u(x, y) = (  (x, y),   (x, y))

x

x

set of weighting functions

w1(u), . . . , wj (u)

u1 =   

u2 =   

monti, boscaini, rodol`a, svoboda, bronstein 2017

spatial convolution on manifolds

geodesic polar coordinates

u(x, y) = (  (x, y),   (x, y))

x

x

set of weighting functions

w1(u), . . . , wj (u)

spatial convolution

(f (cid:63) g)(x) =

j(cid:88)

j=1

gj

(cid:90)
(cid:124)

x

u1 =   

u2 =   

wj(u(x, x(cid:48)))f (x(cid:48))dx(cid:48)
patch operator dj (x)f

(cid:123)(cid:122)

(cid:125)

where g1, . . . , gj are the spatial    lter coe   cients

monti, boscaini, rodol`a, svoboda, bronstein 2017

learnable patch operator

geodesic polar coordinates

u(x, y) = (  (x, y),   (x, y))

x

x

gaussian weighting functions

w  ,  (u) = exp(cid:0)    1

2 (u       )(cid:62)     1(u       )(cid:1)

with learnable covariance    and
mean   

spatial convolution

(f (cid:63) g)(x) =

j(cid:88)

j=1

gj

(cid:90)
(cid:124)

x

u1 =   

u2 =   

w  j ,  j (u(x, x(cid:48)))f (x(cid:48))dx(cid:48)
patch operator dj (x)f

(cid:123)(cid:122)

(cid:125)

where g1, . . . , gj are the spatial    lter coe   cients and   1, . . . ,   j and
  1, . . . ,   j are patch operator parameters

monti, boscaini, rodol`a, svoboda, bronstein 2017

learnable patch operator

geodesic polar coordinates

u(x, y) = (  (x, y),   (x, y))

x

x

gaussian weighting functions

w  ,  (u) = exp(cid:0)    1

2 (u       )(cid:62)     1(u       )(cid:1)

with learnable covariance    and
mean   

spatial convolution

(f (cid:63) g)(x) =

(cid:90)

x

u1 =   

u2 =   

gjw  j ,  j (u(x, x(cid:48)))

f (x(cid:48))dx(cid:48)

j(cid:88)

j=1

where g1, . . . , gj are the spatial    lter coe   cients and   1, . . . ,   j and
  1, . . . ,   j are patch operator parameters
monti, boscaini, rodol`a, svoboda, bronstein 2017

mixture model network (monet)

geodesic polar coordinates

u(x, y) = (  (x, y),   (x, y))

x

x

gaussian weighting functions

w  ,  (u) = exp(cid:0)    1

2 (u       )(cid:62)     1(u       )(cid:1)

with learnable covariance    and
mean   

spatial convolution

(f (cid:63) g)(x) =

(cid:90)

x

j(cid:88)
(cid:124)

j=1

u1 =   

u2 =   

gjw  j ,  j (u(x, x(cid:48)))

f (x(cid:48))dx(cid:48)

(cid:123)(cid:122)

(cid:125)

gaussian mixture

where g1, . . . , gj are the spatial    lter coe   cients and   1, . . . ,   j and
  1, . . . ,   j are patch operator parameters
monti, boscaini, rodol`a, svoboda, bronstein 2017

patch operator weighting functions on manifolds

gid981

aid982

monet3

1masci, boscaini, bronstein, vandergheynst 2015; 2boscaini, masci, rodol`a,
bronstein 2016; 3monti, boscaini, rodol`a, svoboda, bronstein 2017

mixture model network on graphs

local coordinates u(i, j), e.g.
vertex degree, geodesic distance,...

gaussian weighting functions

w  ,  (u) = exp(cid:0)    1

2 (u       )(cid:62)     1(u       )(cid:1)

with learnable covariance    and
mean   

i

local coordinates on graph

spatial convolution

(f (cid:63) g)i =

j(cid:88)

j=1

gj

n(cid:88)
(cid:124)

i(cid:48)=1

w  j ,  j (u(i, i(cid:48)))fi(cid:48)

(cid:123)(cid:122)

patch operator (df )ij

(cid:125)

where g1, . . . , gj are the spatial    lter coe   cients and   1, . . . ,   j and
  1, . . . ,   j are patch operator parameters

monti, boscaini, rodol`a, svoboda, bronstein 2017

mixture model network on graphs

local coordinates u(i, j), e.g.
vertex degree, geodesic distance,...

gaussian weighting functions

w  ,  (u) = exp(cid:0)    1

2 (u       )(cid:62)     1(u       )(cid:1)

i

local coordinates on graph

with learnable covariance    and
mean   

spatial convolution

(f (cid:63) g)i =

n(cid:88)

i(cid:48)=1

j(cid:88)
(cid:124)

j=1

gjw  j ,  j (u(i, i(cid:48)))

fi(cid:48)

(cid:123)(cid:122)

(cid:125)

gaussian mixture

where g1, . . . , gj are the spatial    lter coe   cients and   1, . . . ,   j and
  1, . . . ,   j are patch operator parameters

monti, boscaini, rodol`a, svoboda, bronstein 2017

monet as generalization of previous methods

method coordinates u(x, x(cid:48)) weight function w  (u)
id981

u(x(cid:48))     u(x)

id1972

deg(x), deg(x(cid:48))

gid983

  (x, x(cid:48)),   (x, x(cid:48))

aid984

  (x, x(cid:48)),   (x, x(cid:48))

monet5

  (x, x(cid:48)),   (x, x(cid:48))

(cid:17)

u2

u1

(cid:16)

|(cid:17)

1     |1     1   

  (u     v)
   xed parameters    = v
1     |1     1   

|(cid:17)(cid:16)
2 (u     v)(cid:62)(cid:16)   2
(cid:16)    1
exp(cid:0)   tu(cid:62)r  (   
exp(cid:0)     1
2 (u       )(cid:62)     1(u       )(cid:1)

(u     v)
exp
   xed parameters    = (v,     ,     )

(cid:17)   1
   u(cid:1)

   xed parameters    = (  ,   , t)

1 )r(cid:62)

learnable parameters    = (  ,   )

  2
  

  

some id98 models can be considered as particular settings of monet

with weighting functions of di   erent form

methods: 1lecun et al. 1998; 2kipf, welling 2016; 3masci, boscaini, bronstein,
vandergheynst 2015; 4boscaini, masci, rodol`a, bronstein 2016; 5monti, boscaini,
masci, rodol`a, svoboda, bronstein 2017

spectral vs spatial methods

chebnet    lter

h =     (   )f

spatial    lter

h = (df )g

de   errard, bresson, vandergheynst 2016; masci, boscaini, bronstein, vandergheynst
2015; monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

spectral vs spatial methods

r(cid:88)

chebnet    lter

h =

  j   jf

spatial    lter

h = (df )g

j=0

de   errard, bresson, vandergheynst 2016; masci, boscaini, bronstein, vandergheynst
2015; monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

spectral vs spatial methods

chebnet    lter

r(cid:88)

spatial    lter

j(cid:88)

gj(df )ij

hi =

  j(   jf )i

hi =

j=0

j=1

chebnet is a particular setting of spatial convolution with local weighting
functions given by the powers of the laplacian wj(i, i(cid:48)) = (   j  i)i(cid:48)

de   errard, bresson, vandergheynst 2016; masci, boscaini, bronstein, vandergheynst
2015; monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

i(cid:48)

i

edge convolution

operator (cid:3) (e.g. (cid:80) or max) on

permutation-invariant aggregation

the neighborhood of i

wang, sun, liu, sarma, bronstein, solomon 2018

i(cid:48)

i

edge convolution

operator (cid:3) (e.g. (cid:80) or max) on

permutation-invariant aggregation

the neighborhood of i
edge feature function h  (  ,  )
parametrized by   

wang, sun, liu, sarma, bronstein, solomon 2018

edge convolution

operator (cid:3) (e.g. (cid:80) or max) on

permutation-invariant aggregation

the neighborhood of i
edge feature function h  (  ,  )
parametrized by   

i(cid:48)

i

edge convolution (edgeconv)

i = (cid:3)
f(cid:48)

i(cid:48):(i,i(cid:48))   e

h  (fi, fi(cid:48))

wang, sun, liu, sarma, bronstein, solomon 2018

edge convolution

operator (cid:3) (e.g. (cid:80) or max) on

permutation-invariant aggregation

the neighborhood of i
edge feature function h  (  ,  )
parametrized by   

i(cid:48)

i

edge convolution (edgeconv)

i = (cid:3)
f(cid:48)

i(cid:48):(i,i(cid:48))   e

h  (fi, fi(cid:48))

particular cases of edgeconv

method
pointnet1

monet2

aggregation (cid:3)

   

(cid:80)

edge feature h
h(fi, fi(cid:48)) = h(fi)

h(fi, fi(cid:48)) =(cid:80)j

j=1 gjw  j ,  j (uii(cid:48))fi(cid:48)

1qi et al. 2017; 2monti, boscaini, rodol`a, svoboda, bronstein 2017
wang, sun, liu, sarma, bronstein, solomon 2018

dynamic graph id98 (dyngid98)

construct id92 graph in feature space and update it after each layer

layer l

layer l + 1

n     rdl

1 , . . . , x(l)

features x(l)
id92 graph g(l)
h(l) : rdl    rdl     rdl+1

wang, sun, liu, sarma, bronstein, solomon 2018

1

, . . . , x(l+1)

features x(l+1)
id92 graph g(l+1)
h(l+1) : rdl+1    rdl+1     rdl+2

n

    rdl+1

learning semantic features

segmentation

input

layer 1

layer 2

layer 3

left: distance from red point in the feature space of di   erent dyngid98 layers

right: semantic segmentation results

wang, sun, liu, sarma, bronstein, solomon 2018

semantic segmentation

point cloud semantic segmentation using dyngid98

wang, sun, liu, sarma, bronstein, solomon 2018; data: yi et al. 2016 (shapenet)

semantic segmentation

pointnet

dyngid98

groundtruth

scene

results of semantic segmentation of point cloud+rgb data

using di   erent architectures

methods: qi et al. 2017 (pointnet); wang, sun, liu, sarma, bronstein, solomon
2018 (dyngid98); data: armeni et al. 2016 (s3dis)

shape classi   cation

method
3dshapenet1
voxnet2
subvolume3
ecc4
pointnet5
pointnet++6
kd-net7
dungid988

mean

class accuracy

overall
accuracy

77.3%
83.0%
86.0%
83.2%
86.0%

   
   

90.2%

84.7%
85.9%
89.2%
87.4%
89.2%
90.7%
91.8%
92.2%

classi   cation accuracy of di   erent methods on modelnet40

methods: 1wu et al. 2015; 2maturana et al. 2015; qi et al. 2016; 4simonovsky,
komodakis 2017; 5qi et al. 2017; 6qi, su et al. 2017; 7klokov, lempitsky 2017;
8wang, sun, liu, sarma, bronstein, solomon 2018; data: wu et al. 2015 (modelnet)

applications in computer graphics

and vision

deformable 3d correspondence

isometric

non-isometric

partial

correspondence i: local id171

x

y

  

intrinsic deep net

f(x)

(cid:96)s

g(y)

intrinsic deep net

siamese net

poitwise feature cost

(cid:88)

two net instances with shared parameters   

(cid:96)s(  ) =   

(x,y)   c
+ (1       )

(cid:88)

(cid:107)f  (x)     g  (y)(cid:107)2

2

(cid:2)       (cid:107)f  (x)     g  (y)(cid:107)2

2

(cid:3)

+

(x,y) /   c

masci, boscaini, bronstein, vandergheynst 2015; cosmo, rodol`a, masci, torsello,
bronstein 2016

correspondence ii: labeling

groundtruth correspondence
      : x     y from query shape x
to some reference shape y
(discretized with n vertices)

correspondence = label each query
vertex x as reference vertex y

x

     (x)

query x

reference y

rodol`a et al. 2014; masci, boscaini, bronstein, vandergheynst 2015

correspondence ii: labeling

groundtruth correspondence
      : x     y from query shape x
to some reference shape y
(discretized with n vertices)

correspondence = label each query
vertex x as reference vertex y

net output at x after softmax layer

f  (x) = (f  ,1(x), . . . , f  ,n(x))
= id203 distribution on y

x

id98

f  (x)

     (x)

query x

reference y

rodol`a et al. 2014; masci, boscaini, bronstein, vandergheynst 2015

correspondence ii: labeling

groundtruth correspondence
      : x     y from query shape x
to some reference shape y
(discretized with n vertices)

correspondence = label each query
vertex x as reference vertex y

net output at x after softmax layer

f  (x) = (f  ,1(x), . . . , f  ,n(x))
= id203 distribution on y

x

id98

f  (x)

     (x)

query x

reference y

minimize on training set the cross id178 between groundtruth
correspondence and output id203 distribution w.r.t. net parameters   

(cid:88)

x

min
  

h(       (x), f  (x))

rodol`a et al. 2014; masci, boscaini, bronstein, vandergheynst 2015

correspondence evaluation: princeton benchmark

g i v e n   

  (x)

dy (     (x),   (x))

x

groundtruth      

     (x)

query x

reference y

pointwise correspondence error = geodesic distance from the groundtruth

 (x) = dy (     (x),   (x))

kim et al. 2011

correspondence quality comparison

s
e
c
n
e
d
n
o
p
s
e
r
r
o
c
%

100

80

60

40

20

0

cm

4

8

12

16

20

bim
rf
gid98
aid98
monet

2

4

6

8

10

% geodesic diameter

correspondence evaluated using asymmetric princeton benchmark

(training and testing: disjoint subsets of faust)

methods: kim et al. 2011 (bim); rodol`a et al. 2014 (rf); masci, boscaini,
bronstein, vandergheynst 2015 (gid98); boscaini, masci, rodol`a, bronstein, cremers
2016 (aid98); monti, boscaini, masci, rodol`a, svoboda, bronstein 2017 (monet);
data: bogo et al. 2014 (faust); benchmark: kim et al. 2011

shape correspondence error: blended intrinsic map

15cm

0

pointwise correspondence error (geodesic distance from groundtruth)

kim et al. 2011

shape correspondence error: geodesic id98

15cm

0

pointwise correspondence error (geodesic distance from groundtruth)

masci, boscaini, bronstein, vandergheynst 2015

shape correspondence error: anisotropic id98

15cm

0

pointwise correspondence error (geodesic distance from groundtruth)

boscaini, masci, rodol`a, bronstein, cremers 2016

shape correspondence error: monet

15cm

0

pointwise correspondence error (geodesic distance from groundtruth)

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

shape correspondence visualization: monet

reference

texture transferred from reference to query shapes

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

correspondence on range images: monet

15cm

0

pointwise correspondence error (geodesic distance from groundtruth)

monti, boscaini, masci, rodol`a, svoboda, bronstein 2017

correspondence as classi   cation problem, revisited

x

  

groundtruth      

  (cid:48)

query x

reference y

classi   cation cost considers equally correspondences

that deviate from the groundtruth (no matter how far)

kim et al. 2011

soft correspondence error

x

groundtruth      

y

p(x, y)

query x

reference y

soft correspondence error = id203-weighted geodesic distance from
the groundtruth

   (x) =

p(x, y)dy (     (x), y)dy

(cid:90)

y

kovnatsky, bronstein, bresson, vandergheynst 2015

pointwise vs structured learning

x(cid:48)
x

y(cid:48)
y(cid:48)
y

query x

reference y

nearby points x, x(cid:48) on query shape are not guaranteed to map

to nearby points y, y(cid:48) on reference shape at test time

litany, remez, rodol`a, bronstein, bronstein 2017

correspondence i: local id171

x

y

  

intrinsic deep net

f(x)

(cid:96)s

g(y)

intrinsic deep net

siamese net

poitwise feature cost

(cid:88)

two net instances with shared parameters   

(cid:96)s(  ) =   

(x,y)   c
+ (1       )

(cid:88)

(cid:107)f  (x)     g  (y)(cid:107)2

2

(cid:2)       (cid:107)f  (x)     g  (y)(cid:107)2

2

(cid:3)

+

(x,y) /   c

masci, boscaini, bronstein, vandergheynst 2015

correspondence iii: intrinsic id170 (fmnet)

x

y

  

intrinsic deep net

f(x)

  (cid:62)

  f

intrinsic deep net

g(y)

  (cid:62)

  g

fm

c

soft corr

p

(cid:96)f

siamese net

functional map layer soft corr layer

siamese net

two net instances with shared parameters   
c   
   = argmin

(cid:107)   f  c       g  (cid:107)2

f

functional map layer
soft correspondence layer p   = |  c    (cid:62)|(cid:107)  (cid:107)

c

(cid:90)

(cid:90)

soft error cost

x
litany, remez, rodol`a, bronstein, bronstein 2017

y

(cid:96)f(  ) =

p  (x, y)dy (     (x), y) dx dy

correspondence iii: intrinsic id170 (fmnet)

x

y

  

intrinsic deep net

f(x)

  (cid:62)

  f

intrinsic deep net

g(y)

  (cid:62)

  g

fm

c

soft corr

p

(cid:96)f

siamese net

functional map layer soft corr layer

siamese net

two net instances with shared parameters   
c   
   =   f

  g  

   
  

functional map layer
soft correspondence layer p   = |  c    (cid:62)|(cid:107)  (cid:107)
(cid:96)f(  ) = (cid:107)p       dy(cid:107)

soft error cost

litany, remez, rodol`a, bronstein, bronstein 2017

functional maps

id203 p(x, y) of point x mapping to y
p = |  c  (cid:62)|(cid:107)  (cid:107)

ovsjanikov et al. 2012

correspondence quality comparison

s
e
c
n
e
d
n
o
p
s
e
r
r
o
c
%

100

80

60

40

20

0

cm

4

8

12

16

20

bim
rf
gid98
aid98
monet
fmnet

2

4

6

8

10

% geodesic diameter

correspondence evaluated using asymmetric princeton benchmark

(training and testing: disjoint subsets of faust)

methods: kim et al. 2011 (bim); rodol`a et al. 2014 (rf); masci, boscaini,
bronstein, vandergheynst 2015 (gid98); boscaini, masci, rodol`a, bronstein, cremers
2016 (aid98); monti, boscaini, masci, rodol`a, svoboda, bronstein 2017 (monet);
litany, remez, rodol`a, bronstein, bronstein 2017 (fmnet); data: bogo et al. 2014
(faust); benchmark: kim et al. 2011

3d shape analysis and synthesis

input

3d scan

correspondence to
reference shape

deformation of
reference shape

shape generation with intrinsic variational autoencoder

encoder

decoder

x

dec(enc(x))

litany, bronstein, bronstein, makadia 2018

gconv(64)meanpoolgconv(128)no bn......lin(16)gconv(96)...fc(2  128)gconv(32)...gconv(128)gconv(96)gconv(64)gconv(32)gconv(16)no bnlin(3)  latent spacerefshapemron hctabmron hctabnb...    zshape completion examples with intrinsic vae

litany, bronstein, bronstein, makadia 2018; data: bogo et al. 2017 (faust scans)

molecule property prediction

input molecule

density functional theory

    103 sec

predicted
properties
u0, h, g
  1,     
(cid:104)r2(cid:105),   ,   

graph neural network

    10   2 sec

duvenaud et al. 2015; gomez-bombarelli et al. 2016; gilmer et al. 2017

molecule generation

molecules generated with a graph vae

simonovsky, komodakis 2017; you et al. 2018

h2nfohnfnh2nohfnh2nnh2fnhnnh2fonhh2nfnnhnh2fnonh2h2nfonhonhooh2nhoh2nhoohnhooonoonoohnooooooohooohoooh2oh2h2ooh2ohoooh2oh2h2ooh2oh2oh2oohoooohoohoohohoohhoohhoohoohohonh2onoh2h2onoh2honohohnohnohonohnoopolypharmacy and drug repurposing

prediction of side e   ects of drug combinations using

deep learning on multimodal graph

zitnik, agrawal, leskovec 2018

drugproteinr1gastrointestinal bleed side effect  r2bradycardia side effectprotein-protein interactiondrug-protein interactionpolypharmacy side effectsciprofloxacinsimvastatindoxycyclinemupirocinr2r2r1node feature vectordsmcprotein-protein interaction

matched protein

matched seed

target protein

designing protein binder for the pd-l1 protein target

collaboration with b. correia and p. gainza-cirauqui (epfl)

brain imaging

tim e

h
p
a
r
g

i

n
a
r
b

patients graph

collaboration with d. rueckert et al. (imperial college)

atlas detector in the large hadron collider (cern)

henrion et al. 2017

icecube neutrino observatory

collaboration with j. bruna (nyu) and prabhat (lbl)

conclusions

geometric priors exist beyond euclidean domains

they can be exploited by modeling locality and weight sharing

spectral and spatial models obtain locality and sharing through
di   erent routes

resulting neural models related to attention mechanisms

challenge: scaling up models to millions of nodes

large areas of application: physical sciences, machine translation,
graphics, relational learning

bibliography

m. m. bronstein, j. bruna, y. lecun, a. szlam, p. vandergheynst,    geometric
deep learning: going beyond euclidean data   , arxiv:1611.08097, 2016. first
review paper of geometric deep learning

m. gori, g. monfardini, f. scarselli,    a new model for learning in graph
domains   , proc. ijid98 2005. first neural net on graphs (gnn framework)

f. scarselli, m. gori, a. c. tsoi, m. hagenbuchner, g. monfardini,    the graph
neural network model   , ieee trans. neural networks 20(1):61   80, 2009.

j. bruna, w. zaremba, a. szlam, y. lecun,    spectral networks and locally
connected networks on graphs   , proc. icml 2014. first spectral id98 on
graphs

m. hena   , j. bruna, and y. lecun,    deep convolutional networks on
graph-structured data   , arxiv:1506.05163, 2015. spectral id98 with smooth
multipliers

j. masci, d. boscaini, m. m. bronstein, p. vandergheynst,    geodesic
convolutional neural networks on riemannian manifolds   , arxiv:1501.06297v2,
2015. first spatial id98 on manifolds (geodesic id98 framework)

bibliography

m. de   errard, x. bresson, p. vandergheynst,    convolutional neural networks
on graphs with fast localized spectral    ltering   , proc. nips 2016. spectral
id98 with chebychev polynomial    lters (chebnet)

t. n. kipf, m. welling,    semi-supervised classi   cation with graph
convolutional networks   , arxiv:1609.02907, 2016. graph convolutional
network (id197) framework, a simpli   cation of chebnet

r. levie, f. monti, x. bresson, m. m. bronstein,    cayleynets: graph
convolutional neural networks with complex rational spectral    lters   ,
arxiv:1705.07664, 2017. spectral id98 with complex rational    lters
(cayleynet)

j. atwood and d. towsley,    di   usion-convolutional neural networks   ,
arxiv:1511.02136v2, 2016. di   usion id98 framework

f. monti, k. otness, m. m. bronstein,    motifnet: a motif-based graph
convolutional network for directed graphs   , arxiv:1802.01572, 2018. motifnet

bibliography

f. monti, d. boscaini, j. masci, e. rodol`a, j. svoboda, m. m. bronstein,
   geometric deep learning on graphs and manifolds using mixture model
id98s   , proc. cvpr 2017. mixture model network (monet) framework

d. boscaini, j. masci, s. melzi, m. m. bronstein, u. castellani, p.
vandergheynst,    learning class-speci   c descriptors for deformable shapes using
localized spectral convolutional networks   , computer graphics forum
34(5):13-23, 2015. localized spectral id98 framework

d. boscaini, j. masci, e. rodol`a, m. m. bronstein,    learning shape
correspondence with anisotropic convolutional neural networks   , proc. nips
2016. anisotropic id98 framework

f. monti, m. m. bronstein, x. bresson,    geometric matrix completion with
recurrent multi-graph neural networks   , arxiv:1704.06803, 22 april 2017.
id98s on multiple graphs and application to matrix completion

bibliography

a. sinha, j. bai, k. ramani,    deep learning 3d shape surfaces using geometry
images   , proc. eccv 2016. id98 on spherical authalic parametrization

h. maron, m. galun, n. aigerman, m. trope, n. dym, e. yumer, v. g. kim,
y. lipman,    convolutional neural networks on surfaces via seaid113ss toric
covers   , trans. graphics 36(4), 2017. id98 on planar    at-torus
parametrization

l. yi, h. su, x. guo, l. guibas,    syncspecid98: synchronized spectral id98
for 3d shape segmentation   , proc. cvpr 2017. spectral transformer
networks

c. r. qi, h. su, k. mo, l. guibas,    pointnet: deep learning on point sets for
3d classi   cation and segmentation   , proc. cvpr 2017. pointnet

y. wang, y. sun, z. liu, s. e. sarma, m. m. bronstein, j. m. solomon,
   dynamic graph id98 for learning on point clouds   , arxiv:1712.00268, 2018.
edge convolution

bibliography

o. litany, t. remez, e. rodol`a, a. m. bronstein, m. m. bronstein,    deep
functional maps: id170 for dense shape correspondence   , proc.
iccv 2017. intrinsic id170 (fmnet)

o. litany, a. m. bronstein, m. m. bronstein, a. makadia,    deformable shape
completion with graph convolutional autoencoders   , proc. cvpr 2018. shape
completion with intrinsic variational autoencoder

y. lecun, b. boser, j. s. denker, r. e. howard, w. hubbard, l. d. jackel,
   id26 applied to handwritten zip code recognition   , neural
computation 1(4):541   551, 1989. classical euclidean id98

