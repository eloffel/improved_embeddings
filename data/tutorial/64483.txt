tensor methods for large-scale machine learning

anima anandkumar

u.c. irvine

learning with big data

data vs. information

data vs. information

data vs. information

missing observations, gross corruptions, outliers.

data vs. information

missing observations, gross corruptions, outliers.

high dimensional regime: as data grows, more variables !

data vs. information

missing observations, gross corruptions, outliers.

high dimensional regime: as data grows, more variables !

data deluge an information desert!

learning in high dimensional regime

useful information: low-dimensional structures.

learning with big data: ill-posed problem.

learning in high dimensional regime

useful information: low-dimensional structures.

learning with big data: ill-posed problem.

learning is    nding needle in a haystack

learning in high dimensional regime

useful information: low-dimensional structures.

learning with big data: ill-posed problem.

learning is    nding needle in a haystack

learning with big data: computationally challenging!

principled approaches for    nding low dimensional structures?

how to model information structures?

latent variable models

incorporate hidden or latent variables.

information structures: relationships between latent variables and
observed data.

how to model information structures?

latent variable models

incorporate hidden or latent variables.

information structures: relationships between latent variables and
observed data.

basic approach: mixtures/clusters

hidden variable is categorical.

how to model information structures?

latent variable models

incorporate hidden or latent variables.

information structures: relationships between latent variables and
observed data.

basic approach: mixtures/clusters

hidden variable is categorical.

advanced: probabilistic models

hidden variables have more general distributions.

can model mixed membership/hierarchical
groups.

h1

h2

h3

x1 x2 x3 x4 x5

latent variable models (lvms)

document modeling
observed: words.

hidden: topics.

social network modeling

observed: social interactions.

hidden: communities, relationships.

id126s

observed: recommendations (e.g., reviews).

hidden: user and business attributes

unsupervised learning: learn lvm without labeled examples.

lvm for feature engineering

learn good features/representations for classi   cation tasks, e.g.,
id161 and nlp.

sparse coding/dictionary learning

sparse representations, low dimensional hidden structures.

a few dictionary elements make complicated shapes.

challenges in learning lvms

computational challenges

maximum likelihood: non-id76. np-hard.

practice: local search approaches such as id119, em,
id58 have no consistency guarantees.

can get stuck in bad local optima. poor convergence rates.

hard to parallelize.

alternatives? guaranteed and e   cient learning?

classical id106: matrix pca

for centered samples {xi},    nd projection p with
rank(p ) = k s.t.

min

p

1
n x

i   [n]

kxi     p xik2.

result: eigen-decomposition of cov(x).

beyond pca: id106 on tensors?

moment matrices and tensors

multivariate moments

m1 := e[x], m2 := e[x     x], m3 := e[x     x     x].

matrix

e[x     x]     rd  d is a second order tensor.
e[x     x]i1,i2 = e[xi1 xi2].
for matrices: e[x     x] = e[xx   ].

tensor

e[x     x     x]     rd  d  d is a third order tensor.
e[x     x     x]i1,i2,i3 = e[xi1 xi2 xi3].

spectral decomposition of tensors

m2 = p

i

  iui     vi

=

+

....

matrix m2

  1u1     v1

  2u2     v2

spectral decomposition of tensors

m2 = p

i

  iui     vi

=

+

....

matrix m2

  1u1     v1

  2u2     v2

m3 = p

i

  iui     vi     wi

=

+

....

tensor m3

  1u1     v1     w1

  2u2     v2     w2

u     v     w is a rank-1 tensor since its (i1, i2, i3)th entry is ui1 vi2 wi3.

e   cient methods for tensor decomposition

id96

tractable learning for lvms

gmm

id48

h1

h2

h3

x1

x2

x3

ica

h1 h2

hk

x1 x2

xd

multiview and topic models

overall framework

=

+

....

unlabeled
data

probabilistic
admixture
models

tensor
method

id136

conclusion: tensor methods for learning

tensor decomposition

e   cient sample and computational complexities

better performance compared to em, id58 etc.

in practice

scalable and embarrassingly parallel: handle large datasets.

e   cient performance: perplexity or ground truth validation.

related topics

tensor methods for discriminative learning: learning neural
networks, mixtures of classi   ers, etc.

overcomplete tensor decomposition: neural networks, sparse
coding and ica models tend to be overcomplete (more neurons than
input dimensions).

my research group and resources

furong huang

majid janzamin

hanie sedghi

niranjan un

forough arabshahi

ml summer school lectures available at
http://newport.eecs.uci.edu/anandkumar/mlss.html

