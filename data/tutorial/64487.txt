id76

id76

stephen boyd

department of electrical engineering
stanford university

lieven vandenberghe

electrical engineering department
university of california, los angeles

cambridge university press
cambridge, new york, melbourne, madrid, cape town, singapore, s  ao paolo, delhi

cambridge university press
the edinburgh building, cambridge, cb2 8ru, uk

published in the united states of america by cambridge university press, new york

http://www.cambridge.org
information on this title: www.cambridge.org/9780521833783

c(cid:13) cambridge university press 2004
this publication is in copyright. subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without
the written permission of cambridge university press.

first published 2004
seventh printing with corrections 2009

printed in the united kingdom at the university press, cambridge

a catalogue record for this publication is available from the british library

library of congress cataloguing-in-publication data

boyd, stephen p.
id76 / stephen boyd & lieven vandenberghe

p. cm.

includes bibliographical references and index.
isbn 0 521 83378 7
1. mathematical optimization. 2. convex functions. i. vandenberghe, lieven. ii. title.

qa402.5.b69 2004
519.6   dc22

2003063284

isbn 978-0-521-83378-3 hardback

cambridge university press has no responsiblity for the persistency or accuracy of urls
for external or third-party internet websites referred to in this publication, and does not
guarantee that any content on such websites is, or will remain, accurate or appropriate.

for

anna, nicholas, and nora

dani  el and margriet

contents

preface

xi

1 introduction

1
1
1.1 mathematical optimization . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2 least-squares and id135 . . . . . . . . . . . . . . . . . .
7
1.3 id76 . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.4 nonlinear optimization . . . . . . . . . . . . . . . . . . . . . . . . . .
1.5 outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
1.6 notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

i theory

19

2 convex sets

21
2.1 a   ne and convex sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2 some important examples . . . . . . . . . . . . . . . . . . . . . . . . . 27
2.3 operations that preserve convexity . . . . . . . . . . . . . . . . . . . . 35
2.4 generalized inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2.5 separating and supporting hyperplanes . . . . . . . . . . . . . . . . . . 46
2.6 dual cones and generalized inequalities . . . . . . . . . . . . . . . . . . 51
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

3 convex functions

67
. . . . . . . . . . . . . . . . . . . . . . 67
3.1 basic properties and examples
3.2 operations that preserve convexity . . . . . . . . . . . . . . . . . . . . 79
3.3 the conjugate function . . . . . . . . . . . . . . . . . . . . . . . . . . 90
3.4 quasiconvex functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
. . . . . . . . . . . . . . . . . . 104
3.5 log-concave and log-convex functions
. . . . . . . . . . . . 108
3.6 convexity with respect to generalized inequalities
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113

viii

contents

4 id76 problems

127
. . . . . . . . . . . . . . . . . . . . . . . . . . 127
4.1 optimization problems
4.2 id76 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136
4.3 linear optimization problems . . . . . . . . . . . . . . . . . . . . . . . 146
4.4 quadratic optimization problems . . . . . . . . . . . . . . . . . . . . . 152
4.5 geometric programming . . . . . . . . . . . . . . . . . . . . . . . . . . 160
4.6 generalized inequality constraints . . . . . . . . . . . . . . . . . . . . . 167
4.7 vector optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

5 duality

215
5.1 the lagrange dual function . . . . . . . . . . . . . . . . . . . . . . . . 215
5.2 the lagrange dual problem . . . . . . . . . . . . . . . . . . . . . . . . 223
5.3 geometric interpretation . . . . . . . . . . . . . . . . . . . . . . . . . 232
5.4 saddle-point interpretation . . . . . . . . . . . . . . . . . . . . . . . . 237
5.5 optimality conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
. . . . . . . . . . . . . . . . . . . 249
5.6 perturbation and sensitivity analysis
5.7 examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
. . . . . . . . . . . . . . . . . . . . . . . . . 258
5.8 theorems of alternatives
5.9 generalized inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . 264
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273

ii applications

289

6 approximation and    tting

291
6.1 norm approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
. . . . . . . . . . . . . . . . . . . . . . . . . . . 302
6.2 least-norm problems
6.3 regularized approximation . . . . . . . . . . . . . . . . . . . . . . . . 305
6.4 robust approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
6.5 function    tting and interpolation . . . . . . . . . . . . . . . . . . . . . 324
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344

7 statistical estimation

351
7.1 parametric distribution estimation . . . . . . . . . . . . . . . . . . . . 351
7.2 nonparametric distribution estimation . . . . . . . . . . . . . . . . . . 359
7.3 optimal detector design and hypothesis testing . . . . . . . . . . . . . 364
. . . . . . . . . . . . . . . . . . . . . 374
7.4 chebyshev and cherno    bounds
7.5 experiment design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 392
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393

contents

ix

8 geometric problems

397
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
8.1 projection on a set
8.2 distance between sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
8.3 euclidean distance and angle problems . . . . . . . . . . . . . . . . . . 405
. . . . . . . . . . . . . . . . . . . . . . . . 410
8.4 extremal volume ellipsoids
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416
8.5 centering
8.6 classi   cation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 422
8.7 placement and location . . . . . . . . . . . . . . . . . . . . . . . . . . 432
8.8 floor planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 446
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447

iii algorithms

455

9 unconstrained minimization

457
. . . . . . . . . . . . . . . . . . 457
9.1 unconstrained minimization problems
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 463
9.2 descent methods
9.3 id119 method . . . . . . . . . . . . . . . . . . . . . . . . . 466
9.4 steepest descent method . . . . . . . . . . . . . . . . . . . . . . . . . 475
9.5 newton   s method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 484
9.6 self-concordance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 496
implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
9.7
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 513
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514

10 equality constrained minimization

521
. . . . . . . . . . . . . . . 521
10.1 equality constrained minimization problems
10.2 newton   s method with equality constraints . . . . . . . . . . . . . . . . 525
10.3 infeasible start id77 . . . . . . . . . . . . . . . . . . . . . . 531
10.4 implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 542
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557

11 interior-point methods

561
. . . . . . . . . . . . . . 561
11.1 inequality constrained minimization problems
11.2 logarithmic barrier function and central path . . . . . . . . . . . . . . 562
11.3 the barrier method . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
11.4 feasibility and phase i methods . . . . . . . . . . . . . . . . . . . . . . 579
11.5 complexity analysis via self-concordance . . . . . . . . . . . . . . . . . 585
. . . . . . . . . . . . . . . . . . 596
11.6 problems with generalized inequalities
11.7 primal-dual interior-point methods . . . . . . . . . . . . . . . . . . . . 609
11.8 implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 615
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 623

x

appendices

contents

631

a mathematical background

633
a.1 norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 637
a.2 analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639
a.3 functions
a.4 derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640
a.5 id202 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 652

b problems involving two quadratic functions

653
b.1 single constraint quadratic optimization . . . . . . . . . . . . . . . . . 653
b.2 the s-procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655
b.3 the    eld of values of two symmetric matrices . . . . . . . . . . . . . . 656
b.4 proofs of the strong duality results . . . . . . . . . . . . . . . . . . . . 657
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659

c numerical id202 background

661
c.1 matrix structure and algorithm complexity . . . . . . . . . . . . . . . . 661
c.2 solving linear equations with factored matrices . . . . . . . . . . . . . . 664
c.3 lu, cholesky, and ldlt factorization . . . . . . . . . . . . . . . . . . 668
c.4 block elimination and schur complements . . . . . . . . . . . . . . . . 672
c.5 solving underdetermined linear equations . . . . . . . . . . . . . . . . . 681
bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 684

references

notation

index

685

697

701

preface

this book is about id76, a special class of mathematical optimiza-
tion problems, which includes least-squares and id135 problems. it
is well known that least-squares and id135 problems have a fairly
complete theory, arise in a variety of applications, and can be solved numerically
very e   ciently. the basic point of this book is that the same can be said for the
larger class of id76 problems.

while the mathematics of id76 has been studied for about a
century, several related recent developments have stimulated new interest in the
topic. the    rst is the recognition that interior-point methods, developed in the
1980s to solve id135 problems, can be used to solve convex optimiza-
tion problems as well. these new methods allow us to solve certain new classes
of id76 problems, such as semide   nite programs and second-order
cone programs, almost as easily as linear programs.

the second development is the discovery that id76 problems
(beyond least-squares and linear programs) are more prevalent in practice than
was previously thought. since 1990 many applications have been discovered in
areas such as automatic control systems, estimation and signal processing, com-
munications and networks, electronic circuit design, data analysis and modeling,
statistics, and    nance. id76 has also found wide application in com-
binatorial optimization and global optimization, where it is used to    nd bounds on
the optimal value, as well as approximate solutions. we believe that many other
applications of id76 are still waiting to be discovered.

there are great advantages to recognizing or formulating a problem as a convex
optimization problem. the most basic advantage is that the problem can then be
solved, very reliably and e   ciently, using interior-point methods or other special
methods for id76. these solution methods are reliable enough to be
embedded in a computer-aided design or analysis tool, or even a real-time reactive
or automatic control system. there are also theoretical or conceptual advantages
of formulating a problem as a id76 problem. the associated dual
problem, for example, often has an interesting interpretation in terms of the original
problem, and sometimes leads to an e   cient or distributed method for solving it.

we think that id76 is an important enough topic that everyone
who uses computational mathematics should know at least a little bit about it.
in our opinion, id76 is a natural next topic after advanced linear
algebra (topics like least-squares, singular values), and id135.

xii

preface

goal of this book

for many general purpose optimization methods, the typical approach is to just
try out the method on the problem to be solved. the full bene   ts of convex
optimization, in contrast, only come when the problem is known ahead of time to
be convex. of course, many optimization problems are not convex, and it can be
di   cult to recognize the ones that are, or to reformulate a problem so that it is
convex.

our main goal is to help the reader develop a working knowledge of
id76, i.e., to develop the skills and background needed
to recognize, formulate, and solve id76 problems.

developing a working knowledge of id76 can be mathematically
demanding, especially for the reader interested primarily in applications. in our
experience (mostly with graduate students in electrical engineering and computer
science), the investment often pays o    well, and sometimes very well.

there are several books on id135, and general nonlinear pro-
gramming, that focus on problem formulation, modeling, and applications. several
other books cover the theory of id76, or interior-point methods and
their complexity analysis. this book is meant to be something in between, a book
on general id76 that focuses on problem formulation and modeling.
we should also mention what this book is not. it is not a text primarily about
convex analysis, or the mathematics of id76; several existing texts
cover these topics well. nor is the book a survey of algorithms for convex optimiza-
tion. instead we have chosen just a few good algorithms, and describe only simple,
stylized versions of them (which, however, do work well in practice). we make no
attempt to cover the most recent state of the art in interior-point (or other) meth-
ods for solving convex problems. our coverage of numerical implementation issues
is also highly simpli   ed, but we feel that it is adequate for the potential user to
develop working implementations, and we do cover, in some detail, techniques for
exploiting structure to improve the e   ciency of the methods. we also do not cover,
in more than a simpli   ed way, the complexity theory of the algorithms we describe.
we do, however, give an introduction to the important ideas of self-concordance
and complexity analysis for interior-point methods.

audience

this book is meant for the researcher, scientist, or engineer who uses mathemat-
ical optimization, or more generally, computational mathematics. this includes,
naturally, those working directly in optimization and operations research, and also
many others who use optimization, in    elds like computer science, economics,    -
nance, statistics, data mining, and many    elds of science and engineering. our
primary focus is on the latter group, the potential users of id76,
and not the (less numerous) experts in the    eld of id76.

the only background required of the reader is a good knowledge of advanced
calculus and id202. if the reader has seen basic mathematical analysis (e.g.,
norms, convergence, elementary topology), and basic id203 theory, he or she
should be able to follow every argument and discussion in the book. we hope that

preface

xiii

readers who have not seen analysis and id203, however, can still get all of the
essential ideas and important points. prior exposure to numerical computing or
optimization is not needed, since we develop all of the needed material from these
areas in the text or appendices.

using this book in courses

we hope that this book will be useful as the primary or alternate textbook for
several types of courses. since 1995 we have been using drafts of this book for
graduate courses on linear, nonlinear, and id76 (with engineering
applications) at stanford and ucla. we are able to cover most of the material,
though not in detail, in a one quarter graduate course. a one semester course allows
for a more leisurely pace, more applications, more detailed treatment of theory,
and perhaps a short student project. a two quarter sequence allows an expanded
treatment of the more basic topics such as linear and quadratic programming (which
are very useful for the applications oriented student), or a more substantial student
project.

this book can also be used as a reference or alternate text for a more traditional
course on linear and nonlinear optimization, or a course on control systems (or
other applications area), that includes some coverage of id76. as
the secondary text in a more theoretically oriented course on id76,
it can be used as a source of simple practical examples.

acknowledgments

we have been developing the material for this book for almost a decade. over the
years we have bene   ted from feedback and suggestions from many people, including
our own graduate students, students in our courses, and our colleagues at stanford,
ucla, and elsewhere. unfortunately, space limitations and shoddy record keeping
do not allow us to name everyone who has contributed. however, we wish to
particularly thank a. aggarwal, v. balakrishnan, a. bernard, b. bray, r. cottle,
a. d   aspremont, j. dahl, j. dattorro, d. donoho, j. doyle, l. el ghaoui, p. glynn,
m. grant, a. hansson, t. hastie, a. lewis, m. lobo, z.-q. luo, m. mesbahi, w.
naylor, p. parrilo, i. pressman, r. tibshirani, b. van roy, l. xiao, and y. ye.
j. jalden and a. d   aspremont contributed the time-frequency analysis example
in   6.5.4, and the consumer preference bounding example in   6.5.5, respectively.
p. parrilo suggested exercises 4.4 and 4.56. newer printings bene   ted greatly from
igal sason   s meticulous reading of the book.

we want to single out two others for special acknowledgment. arkadi ne-
mirovski incited our original interest in id76, and encouraged us
to write this book. we also want to thank kishan baheti for playing a critical
role in the development of this book. in 1994 he encouraged us to apply for a na-
tional science foundation combined research and curriculum development grant,
on id76 with engineering applications, and this book is a direct (if
delayed) consequence.

stephen boyd
lieven vandenberghe

stanford, california
los angeles, california

chapter 1

introduction

in this introduction we give an overview of mathematical optimization, focusing on
the special role of id76. the concepts introduced informally here
will be covered in later chapters, with more care and technical detail.

1.1 mathematical optimization

a mathematical optimization problem, or just optimization problem, has the form

minimize
subject to

f0(x)
fi(x)     bi,

i = 1, . . . , m.

(1.1)

here the vector x = (x1, . . . , xn) is the optimization variable of the problem, the
function f0 : rn     r is the objective function, the functions fi
: rn     r,
i = 1, . . . , m, are the (inequality) constraint functions, and the constants b1, . . . , bm
are the limits, or bounds, for the constraints. a vector x    is called optimal, or a
solution of the problem (1.1), if it has the smallest objective value among all vectors
that satisfy the constraints: for any z with f1(z)     b1, . . . , fm(z)     bm, we have
f0(z)     f0(x   ).
we generally consider families or classes of optimization problems, characterized
by particular forms of the objective and constraint functions. as an important
example, the optimization problem (1.1) is called a linear program if the objective
and constraint functions f0, . . . , fm are linear, i.e., satisfy

fi(  x +   y) =   fi(x) +   fi(y)

(1.2)
for all x, y     rn and all   ,        r. if the optimization problem is not linear, it is
called a nonlinear program.
this book is about a class of optimization problems called convex optimiza-
tion problems. a id76 problem is one in which the objective and
constraint functions are convex, which means they satisfy the inequality

fi(  x +   y)       fi(x) +   fi(y)

(1.3)

2

1 introduction

for all x, y     rn and all   ,        r with    +    = 1,        0,        0. comparing (1.3)
and (1.2), we see that convexity is more general than linearity: inequality replaces
the more restrictive equality, and the inequality must hold only for certain values
of    and   . since any linear program is therefore a id76 problem,
we can consider id76 to be a generalization of id135.

1.1.1 applications

the optimization problem (1.1) is an abstraction of the problem of making the best
possible choice of a vector in rn from a set of candidate choices. the variable x
represents the choice made; the constraints fi(x)     bi represent    rm requirements
or speci   cations that limit the possible choices, and the objective value f0(x) rep-
resents the cost of choosing x. (we can also think of    f0(x) as representing the
value, or utility, of choosing x.) a solution of the optimization problem (1.1) corre-
sponds to a choice that has minimum cost (or maximum utility), among all choices
that meet the    rm requirements.

in portfolio optimization, for example, we seek the best way to invest some
capital in a set of n assets. the variable xi represents the investment in the ith
asset, so the vector x     rn describes the overall portfolio allocation across the set of
assets. the constraints might represent a limit on the budget (i.e., a limit on the
total amount to be invested), the requirement that investments are nonnegative
(assuming short positions are not allowed), and a minimum acceptable value of
expected return for the whole portfolio. the objective or cost function might be
a measure of the overall risk or variance of the portfolio return.
in this case,
the optimization problem (1.1) corresponds to choosing a portfolio allocation that
minimizes risk, among all possible allocations that meet the    rm requirements.

another example is device sizing in electronic design, which is the task of choos-
ing the width and length of each device in an electronic circuit. here the variables
represent the widths and lengths of the devices. the constraints represent a va-
riety of engineering requirements, such as limits on the device sizes imposed by
the manufacturing process, timing requirements that ensure that the circuit can
operate reliably at a speci   ed speed, and a limit on the total area of the circuit. a
common objective in a device sizing problem is the total power consumed by the
circuit. the optimization problem (1.1) is to    nd the device sizes that satisfy the
design requirements (on manufacturability, timing, and area) and are most power
e   cient.

in data    tting, the task is to    nd a model, from a family of potential models,
that best    ts some observed data and prior information. here the variables are the
parameters in the model, and the constraints can represent prior information or
required limits on the parameters (such as nonnegativity). the objective function
might be a measure of mis   t or prediction error between the observed data and
the values predicted by the model, or a statistical measure of the unlikeliness or
implausibility of the parameter values. the optimization problem (1.1) is to    nd
the model parameter values that are consistent with the prior information, and give
the smallest mis   t or prediction error with the observed data (or, in a statistical

1.1 mathematical optimization

3

framework, are most likely).

an amazing variety of practical problems involving decision making (or system
design, analysis, and operation) can be cast in the form of a mathematical opti-
mization problem, or some variation such as a multicriterion optimization problem.
indeed, mathematical optimization has become an important tool in many areas.
it is widely used in engineering, in electronic design automation, automatic con-
trol systems, and optimal design problems arising in civil, chemical, mechanical,
and aerospace engineering. optimization is used for problems arising in network
design and operation,    nance, supply chain management, scheduling, and many
other areas. the list of applications is still steadily expanding.

for most of these applications, mathematical optimization is used as an aid to
a human decision maker, system designer, or system operator, who supervises the
process, checks the results, and modi   es the problem (or the solution approach)
when necessary. this human decision maker also carries out any actions suggested
by the optimization problem, e.g., buying or selling assets to achieve the optimal
portfolio.

a relatively recent phenomenon opens the possibility of many other applications
for mathematical optimization. with the proliferation of computers embedded in
products, we have seen a rapid growth in embedded optimization.
in these em-
bedded applications, optimization is used to automatically make real-time choices,
and even carry out the associated actions, with no (or little) human intervention or
oversight. in some application areas, this blending of traditional automatic control
systems and embedded optimization is well under way; in others, it is just start-
ing. embedded real-time optimization raises some new challenges:
in particular,
it requires solution methods that are extremely reliable, and solve problems in a
predictable amount of time (and memory).

1.1.2 solving optimization problems

a solution method for a class of optimization problems is an algorithm that com-
putes a solution of the problem (to some given accuracy), given a particular problem
from the class, i.e., an instance of the problem. since the late 1940s, a large e   ort
has gone into developing algorithms for solving various classes of optimization prob-
lems, analyzing their properties, and developing good software implementations.
the e   ectiveness of these algorithms, i.e., our ability to solve the optimization prob-
lem (1.1), varies considerably, and depends on factors such as the particular forms
of the objective and constraint functions, how many variables and constraints there
are, and special structure, such as sparsity. (a problem is sparse if each constraint
function depends on only a small number of the variables).

even when the objective and constraint functions are smooth (for example,
polynomials) the general optimization problem (1.1) is surprisingly di   cult to solve.
approaches to the general problem therefore involve some kind of compromise, such
as very long computation time, or the possibility of not    nding the solution. some
of these methods are discussed in   1.4.
there are, however, some important exceptions to the general rule that most
optimization problems are di   cult to solve. for a few problem classes we have

4

1 introduction

e   ective algorithms that can reliably solve even large problems, with hundreds or
thousands of variables and constraints. two important and well known examples,
described in   1.2 below (and in detail in chapter 4), are least-squares problems and
linear programs. it is less well known that id76 is another exception
to the rule: like least-squares or id135, there are very e   ective
algorithms that can reliably and e   ciently solve even large convex problems.

1.2 least-squares and id135

in this section we describe two very widely known and used special subclasses of
id76: least-squares and id135. (a complete technical
treatment of these problems will be given in chapter 4.)

1.2.1 least-squares problems

a least-squares problem is an optimization problem with no constraints (i.e., m =
0) and an objective which is a sum of squares of terms of the form at

i x     bi:

minimize

f0(x) = kax     bk2

i=1(at

i x     bi)2.

(1.4)

here a     rk  n (with k     n), at
optimization variable.

i are the rows of a, and the vector x     rn is the

2 =pk

solving least-squares problems

the solution of a least-squares problem (1.4) can be reduced to solving a set of
linear equations,

(at a)x = at b,

so we have the analytical solution x = (at a)   1at b. for least-squares problems
we have good algorithms (and software implementations) for solving the problem to
high accuracy, with very high reliability. the least-squares problem can be solved
in a time approximately proportional to n2k, with a known constant. a current
desktop computer can solve a least-squares problem with hundreds of variables, and
thousands of terms, in a few seconds; more powerful computers, of course, can solve
larger problems, or the same size problems, faster. (moreover, these solution times
will decrease exponentially in the future, according to moore   s law.) algorithms
and software for solving least-squares problems are reliable enough for embedded
optimization.

in many cases we can solve even larger least-squares problems, by exploiting
some special structure in the coe   cient matrix a. suppose, for example, that the
matrix a is sparse, which means that it has far fewer than kn nonzero entries. by
exploiting sparsity, we can usually solve the least-squares problem much faster than
order n2k. a current desktop computer can solve a sparse least-squares problem

1.2 least-squares and id135

5

with tens of thousands of variables, and hundreds of thousands of terms, in around
a minute (although this depends on the particular sparsity pattern).

for extremely large problems (say, with millions of variables), or for problems
with exacting real-time computing requirements, solving a least-squares problem
can be a challenge. but in the vast majority of cases, we can say that existing
methods are very e   ective, and extremely reliable. indeed, we can say that solving
least-squares problems (that are not on the boundary of what is currently achiev-
able) is a (mature) technology, that can be reliably used by many people who do
not know, and do not need to know, the details.

using least-squares

the least-squares problem is the basis for regression analysis, optimal control, and
many parameter estimation and data    tting methods. it has a number of statistical
interpretations, e.g., as id113 of a vector x, given linear
measurements corrupted by gaussian measurement errors.

recognizing an optimization problem as a least-squares problem is straightfor-
ward; we only need to verify that the objective is a quadratic function (and then
test whether the associated quadratic form is positive semide   nite). while the
basic least-squares problem has a simple    xed form, several standard techniques
are used to increase its    exibility in applications.

in weighted least-squares, the weighted least-squares cost

kxi=1

wi(at

i x     bi)2,

where w1, . . . , wk are positive, is minimized.
(this problem is readily cast and
solved as a standard least-squares problem.) here the weights wi are chosen to
re   ect di   ering levels of concern about the sizes of the terms at
i x     bi, or simply
to in   uence the solution.
in a statistical setting, weighted least-squares arises
in estimation of a vector x, given linear measurements corrupted by errors with
unequal variances.

another technique in least-squares is id173, in which extra terms are
added to the cost function. in the simplest case, a positive multiple of the sum of
squares of the variables is added to the cost function:

kxi=1

(at
i x     bi)2 +   

x2
i ,

nxi=1

where    > 0. (this problem too can be formulated as a standard least-squares
problem.) the extra terms penalize large values of x, and result in a sensible
solution in cases when minimizing the    rst sum only does not. the parameter    is
chosen by the user to give the right trade-o    between making the original objective
i not too big. id173
comes up in statistical estimation when the vector x to be estimated is given a prior
distribution.

i x    bi)2 small, while keepingpn

functionpk

i=1(at

i=1 x2

weighted least-squares and id173 are covered in chapter 6; their sta-

tistical interpretations are given in chapter 7.

6

1 introduction

1.2.2 id135

another important class of optimization problems is id135, in which
the objective and all constraint functions are linear:

minimize
subject to at

ct x
i x     bi,

i = 1, . . . , m.

(1.5)

here the vectors c, a1, . . . , am     rn and scalars b1, . . . , bm     r are problem pa-
rameters that specify the objective and constraint functions.

solving linear programs

there is no simple analytical formula for the solution of a linear program (as there
is for a least-squares problem), but there are a variety of very e   ective methods for
solving them, including dantzig   s simplex method, and the more recent interior-
point methods described later in this book. while we cannot give the exact number
of arithmetic operations required to solve a linear program (as we can for least-
squares), we can establish rigorous bounds on the number of operations required
to solve a linear program, to a given accuracy, using an interior-point method. the
complexity in practice is order n2m (assuming m     n) but with a constant that is
less well characterized than for least-squares. these algorithms are quite reliable,
although perhaps not quite as reliable as methods for least-squares. we can easily
solve problems with hundreds of variables and thousands of constraints on a small
desktop computer, in a matter of seconds. if the problem is sparse, or has some
other exploitable structure, we can often solve problems with tens or hundreds of
thousands of variables and constraints.

as with least-squares problems, it is still a challenge to solve extremely large
linear programs, or to solve linear programs with exacting real-time computing re-
quirements. but, like least-squares, we can say that solving (most) linear programs
is a mature technology. id135 solvers can be (and are) embedded in
many tools and applications.

using id135

some applications lead directly to linear programs in the form (1.5), or one of
several other standard forms. in many other cases the original optimization prob-
lem does not have a standard linear program form, but can be transformed to an
equivalent linear program (and then, of course, solved) using techniques covered in
detail in chapter 4.

as a simple example, consider the chebyshev approximation problem:

minimize maxi=1,...,k |at

i x     bi|.

(1.6)

here x     rn is the variable, and a1, . . . , ak     rn, b1, . . . , bk     r are parameters
that specify the problem instance. note the resemblance to the least-squares prob-
lem (1.4). for both problems, the objective is a measure of the size of the terms
at
i x     bi. in least-squares, we use the sum of squares of the terms as objective,
whereas in chebyshev approximation, we use the maximum of the absolute values.

1.3 id76

7

one other important distinction is that the objective function in the chebyshev
approximation problem (1.6) is not di   erentiable; the objective in the least-squares
problem (1.4) is quadratic, and therefore di   erentiable.

the chebyshev approximation problem (1.6) can be solved by solving the linear

program

t

minimize
subject to at
i x     t     bi,
   at
i x     t        bi,
with variables x     rn and t     r.
(the details will be given in chapter 6.)
since linear programs are readily solved, the chebyshev approximation problem is
therefore readily solved.

i = 1, . . . , k,

i = 1, . . . , k

(1.7)

anyone with a working knowledge of id135 would recognize the
chebyshev approximation problem (1.6) as one that can be reduced to a linear
program. for those without this background, though, it might not be obvious that
the chebyshev approximation problem (1.6), with its nondi   erentiable objective,
can be formulated and solved as a linear program.

while recognizing problems that can be reduced to linear programs is more
involved than recognizing a least-squares problem, it is a skill that is readily ac-
quired, since only a few standard tricks are used. the task can even be partially
automated; some software systems for specifying and solving optimization prob-
lems can automatically recognize (some) problems that can be reformulated as
linear programs.

1.3 id76

a id76 problem is one of the form

minimize
subject to

f0(x)
fi(x)     bi,

i = 1, . . . , m,

(1.8)

where the functions f0, . . . , fm : rn     r are convex, i.e., satisfy

fi(  x +   y)       fi(x) +   fi(y)

for all x, y     rn and all   ,        r with    +    = 1,        0,        0. the least-squares
problem (1.4) and id135 problem (1.5) are both special cases of the
general id76 problem (1.8).

1.3.1 solving id76 problems

there is in general no analytical formula for the solution of id76
problems, but (as with id135 problems) there are very e   ective meth-
ods for solving them. interior-point methods work very well in practice, and in some
cases can be proved to solve the problem to a speci   ed accuracy with a number of

8

1 introduction

operations that does not exceed a polynomial of the problem dimensions. (this is
covered in chapter 11.)

we will see that interior-point methods can solve the problem (1.8) in a num-
ber of steps or iterations that is almost always in the range between 10 and 100.
ignoring any structure in the problem (such as sparsity), each step requires on the
order of

max{n3, n2m, f}

operations, where f is the cost of evaluating the    rst and second derivatives of the
objective and constraint functions f0, . . . , fm.

like methods for solving linear programs, these interior-point methods are quite
reliable. we can easily solve problems with hundreds of variables and thousands
of constraints on a current desktop computer, in at most a few tens of seconds. by
exploiting problem structure (such as sparsity), we can solve far larger problems,
with many thousands of variables and constraints.

we cannot yet claim that solving general id76 problems is a
mature technology, like solving least-squares or id135 problems. re-
search on interior-point methods for general nonlinear id76 is still
a very active research area, and no consensus has emerged yet as to what the best
method or methods are. but it is reasonable to expect that solving general con-
vex optimization problems will become a technology within a few years. and for
some subclasses of id76 problems, for example second-order cone
programming or geometric programming (studied in detail in chapter 4), it is fair
to say that interior-point methods are approaching a technology.

1.3.2 using id76

using id76 is, at least conceptually, very much like using least-
squares or id135. if we can formulate a problem as a convex opti-
mization problem, then we can solve it e   ciently, just as we can solve a least-squares
problem e   ciently. with only a bit of exaggeration, we can say that, if you formu-
late a practical problem as a id76 problem, then you have solved
the original problem.

there are also some important di   erences. recognizing a least-squares problem
is straightforward, but recognizing a convex function can be di   cult. in addition,
there are many more tricks for transforming convex problems than for transforming
linear programs. recognizing id76 problems, or those that can
be transformed to id76 problems, can therefore be challenging.
the main goal of this book is to give the reader the background needed to do
this. once the skill of recognizing or formulating id76 problems is
developed, you will    nd that surprisingly many problems can be solved via convex
optimization.

the challenge, and art, in using id76 is in recognizing and for-
mulating the problem. once this formulation is done, solving the problem is, like
least-squares or id135, (almost) technology.

1.4 nonlinear optimization

9

1.4 nonlinear optimization

nonlinear optimization (or nonid135) is the term used to describe
an optimization problem when the objective or constraint functions are not linear,
but not known to be convex. sadly, there are no e   ective methods for solving
the general nonid135 problem (1.1). even simple looking problems
with as few as ten variables can be extremely challenging, while problems with a
few hundreds of variables can be intractable. methods for the general nonlinear
programming problem therefore take several di   erent approaches, each of which
involves some compromise.

1.4.1 local optimization

in local optimization, the compromise is to give up seeking the optimal x, which
minimizes the objective over all feasible points. instead we seek a point that is
only locally optimal, which means that it minimizes the objective function among
feasible points that are near it, but is not guaranteed to have a lower objective
value than all other feasible points. a large fraction of the research on general
nonid135 has focused on methods for local optimization, which as a
consequence are well developed.

local optimization methods can be fast, can handle large-scale problems, and
are widely applicable, since they only require di   erentiability of the objective and
constraint functions. as a result, local optimization methods are widely used in
applications where there is value in    nding a good point, if not the very best. in
an engineering design application, for example, local optimization can be used to
improve the performance of a design originally obtained by manual, or other, design
methods.

there are several disadvantages of local optimization methods, beyond (possi-
bly) not    nding the true, globally optimal solution. the methods require an initial
guess for the optimization variable. this initial guess or starting point is critical,
and can greatly a   ect the objective value of the local solution obtained. little
information is provided about how far from (globally) optimal the local solution
is. local optimization methods are often sensitive to algorithm parameter values,
which may need to be adjusted for a particular problem, or family of problems.

using a local optimization method is trickier than solving a least-squares prob-
lem, linear program, or id76 problem. it involves experimenting
with the choice of algorithm, adjusting algorithm parameters, and    nding a good
enough initial guess (when one instance is to be solved) or a method for producing
a good enough initial guess (when a family of problems is to be solved). roughly
speaking, local optimization methods are more art than technology. local opti-
mization is a well developed art, and often very e   ective, but it is nevertheless an
art. in contrast, there is little art involved in solving a least-squares problem or
a linear program (except, of course, those on the boundary of what is currently
possible).

an interesting comparison can be made between local optimization methods for
nonid135, and id76. since di   erentiability of the ob-

10

1 introduction

jective and constraint functions is the only requirement for most local optimization
methods, formulating a practical problem as a nonlinear optimization problem is
relatively straightforward. the art in local optimization is in solving the problem
(in the weakened sense of    nding a locally optimal point), once it is formulated.
in id76 these are reversed: the art and challenge is in problem
formulation; once a problem is formulated as a id76 problem, it is
relatively straightforward to solve it.

1.4.2 global optimization

in global optimization, the true global solution of the optimization problem (1.1)
is found; the compromise is e   ciency. the worst-case complexity of global opti-
mization methods grows exponentially with the problem sizes n and m; the hope
is that in practice, for the particular problem instances encountered, the method is
far faster. while this favorable situation does occur, it is not typical. even small
problems, with a few tens of variables, can take a very long time (e.g., hours or
days) to solve.

global optimization is used for problems with a small number of variables, where
computing time is not critical, and the value of    nding the true global solution is
very high. one example from engineering design is worst-case analysis or veri   ca-
tion of a high value or safety-critical system. here the variables represent uncertain
parameters, that can vary during manufacturing, or with the environment or op-
erating condition. the objective function is a utility function, i.e., one for which
smaller values are worse than larger values, and the constraints represent prior
knowledge about the possible parameter values. the optimization problem (1.1) is
the problem of    nding the worst-case values of the parameters. if the worst-case
value is acceptable, we can certify the system as safe or reliable (with respect to
the parameter variations).

a local optimization method can rapidly    nd a set of parameter values that
is bad, but not guaranteed to be the absolute worst possible. if a local optimiza-
tion method    nds parameter values that yield unacceptable performance, it has
succeeded in determining that the system is not reliable. but a local optimization
method cannot certify the system as reliable; it can only fail to    nd bad parameter
values. a global optimization method, in contrast, will    nd the absolute worst val-
ues of the parameters, and if the associated performance is acceptable, can certify
the system as safe. the cost is computation time, which can be very large, even
for a relatively small number of parameters. but it may be worth it in cases where
the value of certifying the performance is high, or the cost of being wrong about
the reliability or safety is high.

1.4.3 role of id76 in nonconvex problems

in this book we focus primarily on id76 problems, and applications
that can be reduced to id76 problems. but id76
also plays an important role in problems that are not convex.

1.5 outline

11

initialization for local optimization

one obvious use is to combine id76 with a local optimization
method. starting with a nonconvex problem, we    rst    nd an approximate, but
convex, formulation of the problem. by solving this approximate problem, which
can be done easily and without an initial guess, we obtain the exact solution to the
approximate convex problem. this point is then used as the starting point for a
local optimization method, applied to the original nonconvex problem.

convex heuristics for nonid76

id76 is the basis for several heuristics for solving nonconvex prob-
lems. one interesting example we will see is the problem of    nding a sparse vector
x (i.e., one with few nonzero entries) that satis   es some constraints. while this is
a di   cult combinatorial problem, there are some simple heuristics, based on con-
vex optimization, that often    nd fairly sparse solutions. (these are described in
chapter 6.)

another broad example is given by randomized algorithms, in which an ap-
proximate solution to a nonconvex problem is found by drawing some number of
candidates from a id203 distribution, and taking the best one found as the
approximate solution. now suppose the family of distributions from which we will
draw the candidates is parametrized, e.g., by its mean and covariance. we can then
pose the question, which of these distributions gives us the smallest expected value
of the objective? it turns out that this problem is sometimes a convex problem,
and therefore e   ciently solved. (see, e.g., exercise 11.23.)

bounds for global optimization

many methods for global optimization require a cheaply computable lower bound
on the optimal value of the nonconvex problem. two standard methods for doing
this are based on id76. in relaxation, each nonconvex constraint
is replaced with a looser, but convex, constraint.
in lagrangian relaxation, the
lagrangian dual problem (described in chapter 5) is solved. this problem is convex,
and provides a lower bound on the optimal value of the nonconvex problem.

1.5 outline

the book is divided into three main parts, titled theory, applications, and algo-
rithms.

1.5.1 part i: theory

in part i, theory, we cover basic de   nitions, concepts, and results from convex
analysis and id76. we make no attempt to be encyclopedic, and
skew our selection of topics toward those that we think are useful in recognizing

12

1 introduction

and formulating id76 problems. this is classical material, almost
all of which can be found in other texts on convex analysis and optimization. we
make no attempt to give the most general form of the results; for that the reader
can refer to any of the standard texts on convex analysis.

chapters 2 and 3 cover convex sets and convex functions, respectively. we
give some common examples of convex sets and functions, as well as a number of
convex calculus rules, i.e., operations on sets and functions that preserve convexity.
combining the basic examples with the convex calculus rules allows us to form
(or perhaps more importantly, recognize) some fairly complicated convex sets and
functions.

in chapter 4, id76 problems, we give a careful treatment of op-
timization problems, and describe a number of transformations that can be used to
reformulate problems. we also introduce some common subclasses of convex opti-
mization, such as id135 and geometric programming, and the more
recently developed second-order cone programming and semide   nite programming.
chapter 5 covers lagrangian duality, which plays a central role in convex opti-
mization. here we give the classical karush-kuhn-tucker conditions for optimality,
and a local and global sensitivity analysis for id76 problems.

1.5.2 part ii: applications

in part ii, applications, we describe a variety of applications of id76,
in areas like id203 and statistics, computational geometry, and data    tting.
we have described these applications in a way that is accessible, we hope, to a broad
audience. to keep each application short, we consider only simple cases, sometimes
adding comments about possible extensions. we are sure that our treatment of
some of the applications will cause experts to cringe, and we apologize to them
in advance. but our goal is to convey the    avor of the application, quickly and
to a broad audience, and not to give an elegant, theoretically sound, or complete
treatment. our own backgrounds are in electrical engineering, in areas like control
systems, signal processing, and circuit analysis and design. although we include
these topics in the courses we teach (using this book as the main text), only a few
of these applications are broadly enough accessible to be included here.

the aim of part ii is to show the reader, by example, how id76

can be applied in practice.

1.5.3 part iii: algorithms

in part iii, algorithms, we describe numerical methods for solving convex opti-
mization problems, focusing on newton   s algorithm and interior-point methods.
part iii is organized as three chapters, which cover unconstrained optimization,
equality constrained optimization, and inequality constrained optimization, respec-
tively. these chapters follow a natural hierarchy, in which solving a problem is
reduced to solving a sequence of simpler problems. quadratic optimization prob-
lems (including, e.g., least-squares) form the base of the hierarchy; they can be

1.5 outline

13

solved exactly by solving a set of linear equations. newton   s method, developed in
chapters 9 and 10, is the next level in the hierarchy. in newton   s method, solving
an unconstrained or equality constrained problem is reduced to solving a sequence
of quadratic problems. in chapter 11, we describe interior-point methods, which
form the top level of the hierarchy. these methods solve an inequality constrained
problem by solving a sequence of unconstrained, or equality constrained, problems.
overall we cover just a handful of algorithms, and omit entire classes of good
methods, such as quasi-newton, conjugate-gradient, bundle, and cutting-plane al-
gorithms. for the methods we do describe, we give simpli   ed variants, and not the
latest, most sophisticated versions. our choice of algorithms was guided by several
criteria. we chose algorithms that are simple (to describe and implement), but
also reliable and robust, and e   ective and fast enough for most problems.

many users of id76 end up using (but not developing) standard
software, such as a linear or semide   nite programming solver. for these users, the
material in part iii is meant to convey the basic    avor of the methods, and give
some ideas of their basic attributes. for those few who will end up developing new
algorithms, we think that part iii serves as a good introduction.

1.5.4 appendices

there are three appendices. the    rst lists some basic facts from mathematics that
we use, and serves the secondary purpose of setting out our notation. the second
appendix covers a fairly particular topic, optimization problems with quadratic
objective and one quadratic constraint. these are nonconvex problems that never-
theless can be e   ectively solved, and we use the results in several of the applications
described in part ii.

the    nal appendix gives a brief introduction to numerical id202, con-
centrating on methods that can exploit problem structure, such as sparsity, to gain
e   ciency. we do not cover a number of important topics, including roundo    analy-
sis, or give any details of the methods used to carry out the required factorizations.
these topics are covered by a number of excellent texts.

1.5.5 comments on examples

in many places in the text (but particularly in parts ii and iii, which cover ap-
plications and algorithms, respectively) we illustrate ideas using speci   c examples.
in some cases, the examples are chosen (or designed) speci   cally to illustrate our
point; in other cases, the examples are chosen to be    typical   . this means that the
examples were chosen as samples from some obvious or simple id203 distri-
bution. the dangers of drawing conclusions about algorithm performance from a
few tens or hundreds of randomly generated examples are well known, so we will
not repeat them here. these examples are meant only to give a rough idea of al-
gorithm performance, or a rough idea of how the computational e   ort varies with
problem dimensions, and not as accurate predictors of algorithm performance. in
particular, your results may vary from ours.

14

1 introduction

1.5.6 comments on exercises

each chapter concludes with a set of exercises. some involve working out the de-
tails of an argument or claim made in the text. others focus on determining, or
establishing, convexity of some given sets, functions, or problems; or more gener-
ally, id76 problem formulation. some chapters include numerical
exercises, which require some (but not much) programming in an appropriate high
level language. the di   culty level of the exercises is mixed, and varies without
warning from quite straightforward to rather tricky.

1.6 notation

our notation is more or less standard, with a few exceptions. in this section we
describe our basic notation; a more complete list appears on page 697.

we use r to denote the set of real numbers, r+ to denote the set of nonnegative
real numbers, and r++ to denote the set of positive real numbers. the set of real
n-vectors is denoted rn, and the set of real m    n matrices is denoted rm  n. we
delimit vectors and matrices with square brackets, with the components separated
by space. we use parentheses to construct column vectors from comma separated
lists. for example, if a, b, c     r, we have

(a, b, c) =      

a
b

c        = [ a b

c ]t ,

which is an element of r3. the symbol 1 denotes a vector all of whose components
are one (with dimension determined from context). the notation xi can refer to
the ith component of the vector x, or to the ith element of a set or sequence of
vectors x1, x2, . . .. the context, or the text, makes it clear which is meant.

we use sk to denote the set of symmetric k    k matrices, sk

+ to denote the
set of symmetric positive semide   nite k    k matrices, and sk
++ to denote the set
of symmetric positive de   nite k    k matrices. the curled inequality symbol (cid:23)
(and its strict form    ) is used to denote generalized inequality: between vectors,
it represents componentwise inequality; between symmetric matrices, it represents
matrix inequality. with a subscript, the symbol (cid:22)k (or    k) denotes generalized
inequality with respect to the cone k (explained in   2.4.1).
our notation for describing functions deviates a bit from standard notation,
but we hope it will cause no confusion. we use the notation f : rp     rq to mean
that f is an rq-valued function on some subset of rp, speci   cally, its domain,
which we denote dom f . we can think of our use of the notation f : rp     rq as
a declaration of the function type, as in a computer language: f : rp     rq means
that the function f takes as argument a real p-vector, and returns a real q-vector.
the set dom f , the domain of the function f , speci   es the subset of rp of points
x for which f (x) is de   ned. as an example, we describe the logarithm function
as log : r     r, with dom log = r++. the notation log : r     r means that

1.6 notation

15

the logarithm function accepts and returns a real number; dom log = r++ means
that the logarithm is de   ned only for positive numbers.

we use rn as a generic    nite-dimensional vector space. we will encounter
several other    nite-dimensional vector spaces, e.g., the space of polynomials of a
variable with a given maximum degree, or the space sk of symmetric k  k matrices.
by identifying a basis for a vector space, we can always identify it with rn (where
n is its dimension), and therefore the generic results, stated for the vector space
rn, can be applied. we usually leave it to the reader to translate general results
or statements to other vector spaces. for example, any linear function f : rn     r
can be represented in the form f (x) = ct x, where c     rn. the corresponding
statement for the vector space sk can be found by choosing a basis and translating.
this results in the statement: any linear function f : sk     r can be represented
in the form f (x) = tr(cx), where c     sk.

16

1 introduction

bibliography

least-squares is a very old subject; see, for example, the treatise written (in latin) by
gauss in the 1820s, and recently translated by stewart [gau95]. more recent work in-
cludes the books by lawson and hanson [lh95] and bj  orck [bj  o96]. references on linear
programming can be found in chapter 4.

there are many good texts on local methods for nonid135, including gill,
murray, and wright [gmw81], nocedal and wright [nw99], luenberger [lue84], and
bertsekas [ber99].

global optimization is covered in the books by horst and pardalos [hp94], pinter [pin95],
and tuy [tuy98]. using id76 to    nd bounds for nonconvex problems is
an active research topic, and addressed in the books above on global optimization, the
book by ben-tal and nemirovski [btn01,   4.3], and the survey by nesterov, wolkowicz,
and ye [nwy00]. some notable papers on this subject are goemans and williamson
[gw95], nesterov [nes00, nes98], ye [ye99], and parrilo [par03]. randomized methods
are discussed in motwani and raghavan [mr95].

convex analysis, the mathematics of convex sets, functions, and optimization problems, is
a well developed sub   eld of mathematics. basic references include the books by rockafel-
lar [roc70], hiriart-urruty and lemar  echal [hul93, hul01], borwein and lewis [bl00],
and bertsekas, nedi  c, and ozdaglar [ber03]. more references on convex analysis can be
found in chapters 2   5.

nesterov and nemirovski [nn94] were the    rst to point out that interior-point methods
can solve many id76 problems; see also the references in chapter 11. the
book by ben-tal and nemirovski [btn01] covers modern id76, interior-
point methods, and applications.

solution methods for id76 that we do not cover in this book include
subgradient methods [sho85], bundle methods [hul93], cutting-plane methods [kel60,
em75, gly96], and the ellipsoid method [sho91, bgt81].

the idea that id76 problems are tractable is not new. it has long been rec-
ognized that the theory of id76 is far more straightforward (and complete)
than the theory of general nonlinear optimization. in this context rockafellar stated, in
his 1993 siam review survey paper [roc93],

in fact the great watershed in optimization isn   t between linearity and nonlin-
earity, but convexity and nonconvexity.

the    rst formal argument that id76 problems are easier to solve than
general nonlinear optimization problems was made by nemirovski and yudin, in their
1983 book problem complexity and method e   ciency in optimization [ny83]. they
showed that the information-based complexity of id76 problems is far
lower than that of general nonlinear optimization problems. a more recent book on this
topic is vavasis [vav91].

the low (theoretical) complexity of interior-point methods is integral to modern research
in this area. much of the research focuses on proving that an interior-point (or other)
method can solve some class of id76 problems with a number of operations
that grows no faster than a polynomial of the problem dimensions and log(1/  ), where
   > 0 is the required accuracy. (we will see some simple results like these in chapter 11.)
the    rst comprehensive work on this topic is the book by nesterov and nemirovski
[nn94]. other books include ben-tal and nemirovski [btn01, lecture 5] and renegar
[ren01]. the polynomial-time complexity of interior-point methods for various convex
optimization problems is in marked contrast to the situation for a number of nonconvex
optimization problems, for which all known algorithms require, in the worst case, a number
of operations that is exponential in the problem dimensions.

bibliography

17

id76 has been used in many applications areas, too numerous to cite
here. convex analysis is central in economics and    nance, where it is the basis of many
results. for example the separating hyperplane theorem, together with a no-arbitrage
assumption, is used to deduce the existence of prices and risk-neutral probabilities (see,
e.g., luenberger [lue95, lue98] and ross [ros99]). id76, especially our
ability to solve semide   nite programs, has recently received particular attention in au-
tomatic control theory. applications of id76 in control theory can be
found in the books by boyd and barratt [bb91], boyd, el ghaoui, feron, and balakrish-
nan [befb94], dahleh and diaz-bobillo [ddb95], el ghaoui and niculescu [en00], and
dullerud and paganini [dp00]. a good example of embedded (convex) optimization is
model predictive control, an automatic control technique that requires the solution of a
(convex) quadratic program at each step. model predictive control is now widely used in
the chemical process control industry; see morari and za   rou [mz89]. another applica-
tions area where id76 (and especially, geometric programming) has a long
history is electronic circuit design. research papers on this topic include fishburn and
dunlop [fd85], sapatnekar, rao, vaidya, and kang [srvk93], and hershenson, boyd,
and lee [hbl01]. luo [luo03] gives a survey of applications in signal processing and
communications. more references on applications of id76 can be found in
chapters 4 and 6   8.

high quality implementations of recent interior-point methods for id76
problems are available in the loqo [van97] and mosek [mos02] software packages,
and the codes listed in chapter 11. software systems for specifying optimization prob-
lems include ampl [fgk99] and gams [bkmr98]. both provide some support for
recognizing problems that can be transformed to linear programs.

part i

theory

chapter 2

convex sets

2.1 a   ne and convex sets

2.1.1 lines and line segments

suppose x1 6= x2 are two points in rn. points of the form

y =   x1 + (1       )x2,

where        r, form the line passing through x1 and x2. the parameter value    = 0
corresponds to y = x2, and the parameter value    = 1 corresponds to y = x1.
values of the parameter    between 0 and 1 correspond to the (closed) line segment
between x1 and x2.

expressing y in the form

y = x2 +   (x1     x2)

gives another interpretation: y is the sum of the base point x2 (corresponding
to    = 0) and the direction x1     x2 (which points from x2 to x1) scaled by the
parameter   . thus,    gives the fraction of the way from x2 to x1 where y lies. as
   increases from 0 to 1, the point y moves from x2 to x1; for    > 1, the point y lies
on the line beyond x1. this is illustrated in    gure 2.1.

2.1.2 a   ne sets

a set c     rn is a   ne if the line through any two distinct points in c lies in c,
i.e., if for any x1, x2     c and        r, we have   x1 + (1      )x2     c. in other words,
c contains the linear combination of any two points in c, provided the coe   cients
in the linear combination sum to one.

this idea can be generalized to more than two points. we refer to a point
of the form   1x1 +        +   kxk, where   1 +        +   k = 1, as an a   ne combination
of the points x1, . . . , xk. using induction from the de   nition of a   ne set (i.e.,
that it contains every a   ne combination of two points in it), it can be shown that

22

2 convex sets

   = 1.2

x1

   = 1

   = 0.6

x2

   = 0
   =    0.2

figure 2.1 the line passing through x1 and x2 is described parametrically
by   x1 + (1      )x2, where    varies over r. the line segment between x1 and
x2, which corresponds to    between 0 and 1, is shown darker.

an a   ne set contains every a   ne combination of its points: if c is an a   ne set,
x1, . . . , xk     c, and   1 +       +   k = 1, then the point   1x1 +       +   kxk also belongs
to c.

if c is an a   ne set and x0     c, then the set

v = c     x0 = {x     x0 | x     c}

is a subspace, i.e., closed under sums and scalar multiplication. to see this, suppose
v1, v2     v and   ,        r. then we have v1 + x0     c and v2 + x0     c, and so

  v1 +   v2 + x0 =   (v1 + x0) +   (v2 + x0) + (1              )x0     c,

since c is a   ne, and    +    + (1              ) = 1. we conclude that   v1 +   v2     v ,
since   v1 +   v2 + x0     c.

thus, the a   ne set c can be expressed as

c = v + x0 = {v + x0 | v     v },

i.e., as a subspace plus an o   set. the subspace v associated with the a   ne set c
does not depend on the choice of x0, so x0 can be chosen as any point in c. we
de   ne the dimension of an a   ne set c as the dimension of the subspace v = c   x0,
where x0 is any element of c.

example 2.1 solution set of linear equations. the solution set of a system of linear
equations, c = {x | ax = b}, where a     rm  n and b     rm, is an a   ne set. to
show this, suppose x1, x2     c, i.e., ax1 = b, ax2 = b. then for any   , we have

a(  x1 + (1       )x2) =   ax1 + (1       )ax2

=   b + (1       )b
= b,

which shows that the a   ne combination   x1 + (1       )x2 is also in c. the subspace
associated with the a   ne set c is the nullspace of a.

we also have a converse: every a   ne set can be expressed as the solution set of a
system of linear equations.

2.1 a   ne and convex sets

23

the set of all a   ne combinations of points in some set c     rn is called the

a   ne hull of c, and denoted a    c:

a    c = {  1x1 +        +   kxk | x1, . . . , xk     c,   1 +        +   k = 1}.

the a   ne hull is the smallest a   ne set that contains c, in the following sense: if
s is any a   ne set with c     s, then a    c     s.

2.1.3 a   ne dimension and relative interior

we de   ne the a   ne dimension of a set c as the dimension of its a   ne hull. a   ne
dimension is useful in the context of convex analysis and optimization, but is not
always consistent with other de   nitions of dimension. as an example consider the
2 = 1}. its a   ne hull is all of r2, so its
unit circle in r2, i.e., {x     r2 | x2
a   ne dimension is two. by most de   nitions of dimension, however, the unit circle
in r2 has dimension one.

1 + x2

if the a   ne dimension of a set c     rn is less than n, then the set lies in
the a   ne set a    c 6= rn. we de   ne the relative interior of the set c, denoted
relint c, as its interior relative to a    c:

relint c = {x     c | b(x, r)     a    c     c for some r > 0},

where b(x, r) = {y | ky     xk     r}, the ball of radius r and center x in the norm
k    k. (here k    k is any norm; all norms de   ne the same relative interior.) we can
then de   ne the relative boundary of a set c as cl c \ relint c, where cl c is the
closure of c.

example 2.2 consider a square in the (x1, x2)-plane in r3, de   ned as

c = {x     r3 |     1     x1     1,    1     x2     1, x3 = 0}.

its a   ne hull is the (x1, x2)-plane, i.e., a    c = {x     r3 | x3 = 0}. the interior of c
is empty, but the relative interior is

relint c = {x     r3 |     1 < x1 < 1,    1 < x2 < 1, x3 = 0}.

its boundary (in r3) is itself; its relative boundary is the wire-frame outline,

{x     r3 | max{|x1|,|x2|} = 1, x3 = 0}.

2.1.4 convex sets

a set c is convex if the line segment between any two points in c lies in c, i.e.,
if for any x1, x2     c and any    with 0            1, we have

  x1 + (1       )x2     c.

24

2 convex sets

figure 2.2 some simple convex and nonconvex sets. left. the hexagon,
which includes its boundary (shown darker), is convex. middle. the kidney
shaped set is not convex, since the line segment between the two points in
the set shown as dots is not contained in the set. right. the square contains
some boundary points but not others, and is not convex.

figure 2.3 the convex hulls of two sets in r2. left. the convex hull of a
set of    fteen points (shown as dots) is the pentagon (shown shaded). right.
the convex hull of the kidney shaped set in    gure 2.2 is the shaded set.

roughly speaking, a set is convex if every point in the set can be seen by every other
point, along an unobstructed straight path between them, where unobstructed
means lying in the set. every a   ne set is also convex, since it contains the entire
line between any two distinct points in it, and therefore also the line segment
between the points. figure 2.2 illustrates some simple convex and nonconvex sets
in r2.

we call a point of the form   1x1 +        +   kxk, where   1 +        +   k = 1 and
  i     0, i = 1, . . . , k, a convex combination of the points x1, . . . , xk. as with a   ne
sets, it can be shown that a set is convex if and only if it contains every convex
combination of its points. a convex combination of points can be thought of as a
mixture or weighted average of the points, with   i the fraction of xi in the mixture.

the convex hull of a set c, denoted conv c, is the set of all convex combinations

of points in c:

conv c = {  1x1 +        +   kxk | xi     c,   i     0, i = 1, . . . , k,   1 +        +   k = 1}.
as the name suggests, the convex hull conv c is always convex. it is the smallest
convex set that contains c: if b is any convex set that contains c, then conv c    
b. figure 2.3 illustrates the de   nition of convex hull.
the idea of a convex combination can be generalized to include in   nite sums, in-
tegrals, and, in the most general form, id203 distributions. suppose   1,   2, . . .

2.1 a   ne and convex sets

25

satisfy

   xi=1
and x1, x2, . . .     c, where c     rn is convex. then

i = 1, 2, . . . ,

  i     0,

  i = 1,

   xi=1

  ixi     c,

if the series converges. more generally, suppose p : rn     r satis   es p(x)     0 for
all x     c andrc p(x) dx = 1, where c     rn is convex. then

zc

p(x)x dx     c,

if the integral exists.

in the most general form, suppose c     rn is convex and x is a random vector
with x     c with id203 one. then e x     c. indeed, this form includes all
the others as special cases. for example, suppose the random variable x only takes
on the two values x1 and x2, with prob(x = x1) =    and prob(x = x2) = 1       ,
where 0            1. then e x =   x1 + (1       )x2, and we are back to a simple convex
combination of two points.

2.1.5 cones

a set c is called a cone, or nonnegative homogeneous, if for every x     c and        0
we have   x     c. a set c is a convex cone if it is convex and a cone, which means
that for any x1, x2     c and   1,   2     0, we have
  1x1 +   2x2     c.

points of this form can be described geometrically as forming the two-dimensional
pie slice with apex 0 and edges passing through x1 and x2. (see    gure 2.4.)

a point of the form   1x1 +        +   kxk with   1, . . . ,   k     0 is called a conic
if xi are in a
combination (or a nonnegative linear combination) of x1, . . . , xk.
convex cone c, then every conic combination of xi is in c. conversely, a set c is
a convex cone if and only if it contains all conic combinations of its elements. like
convex (or a   ne) combinations, the idea of conic combination can be generalized
to in   nite sums and integrals.

the conic hull of a set c is the set of all conic combinations of points in c, i.e.,

{  1x1 +        +   kxk | xi     c,   i     0, i = 1, . . . , k},

which is also the smallest convex cone that contains c (see    gure 2.5).

26

2 convex sets

x1

x2

0

figure 2.4 the pie slice shows all points of the form   1x1 +   2x2, where
  1,   2     0. the apex of the slice (which corresponds to   1 =   2 = 0) is at
0; its edges (which correspond to   1 = 0 or   2 = 0) pass through the points
x1 and x2.

figure 2.5 the conic hulls (shown shaded) of the two sets of    gure 2.3.

0

0

2.2 some important examples

27

2.2 some important examples

in this section we describe some important examples of convex sets which we will
encounter throughout the rest of the book. we start with some simple examples.

    the empty set    , any single point (i.e., singleton) {x0}, and the whole space

rn are a   ne (hence, convex) subsets of rn.

    any line is a   ne. if it passes through zero, it is a subspace, hence also a

convex cone.

    a line segment is convex, but not a   ne (unless it reduces to a point).
    a ray, which has the form {x0 +   v |        0}, where v 6= 0, is convex, but not

a   ne. it is a convex cone if its base x0 is 0.

    any subspace is a   ne, and a convex cone (hence convex).

2.2.1 hyperplanes and halfspaces

a hyperplane is a set of the form

{x | at x = b},

where a     rn, a 6= 0, and b     r. analytically it is the solution set of a nontrivial
linear equation among the components of x (and hence an a   ne set). geometri-
cally, the hyperplane {x | at x = b} can be interpreted as the set of points with a
constant inner product to a given vector a, or as a hyperplane with normal vector
a; the constant b     r determines the o   set of the hyperplane from the origin. this
geometric interpretation can be understood by expressing the hyperplane in the
form

where x0 is any point in the hyperplane (i.e., any point that satis   es at x0 = b).
this representation can in turn be expressed as

{x | at (x     x0) = 0},

{x | at (x     x0) = 0} = x0 + a   ,

where a    denotes the orthogonal complement of a, i.e., the set of all vectors or-
thogonal to it:

a    = {v | at v = 0}.

this shows that the hyperplane consists of an o   set x0, plus all vectors orthog-
onal to the (normal) vector a. these geometric interpretations are illustrated in
   gure 2.6.

a hyperplane divides rn into two halfspaces. a (closed) halfspace is a set of

the form

(2.1)
where a 6= 0, i.e., the solution set of one (nontrivial) linear inequality. halfspaces
are convex, but not a   ne. this is illustrated in    gure 2.7.

{x | at x     b},

28

2 convex sets

a

x0

x

at x = b

figure 2.6 hyperplane in r2, with normal vector a and a point x0 in the
hyperplane. for any point x in the hyperplane, x     x0 (shown as the darker
arrow) is orthogonal to a.

a

x0

at x     b

at x     b

figure 2.7 a hyperplane de   ned by at x = b in r2 determines two halfspaces.
the halfspace determined by at x     b (not shaded) is the halfspace extending
in the direction a. the halfspace determined by at x     b (which is shown
shaded) extends in the direction    a. the vector a is the outward normal of
this halfspace.

2.2 some important examples

29

x1

x0

a

x2

figure 2.8 the shaded set is the halfspace determined by at (x     x0)     0.
the vector x1   x0 makes an acute angle with a, so x1 is not in the halfspace.
the vector x2     x0 makes an obtuse angle with a, and so is in the halfspace.

the halfspace (2.1) can also be expressed as

{x | at (x     x0)     0},

(2.2)

where x0 is any point on the associated hyperplane, i.e., satis   es at x0 = b. the
representation (2.2) suggests a simple geometric interpretation: the halfspace con-
sists of x0 plus any vector that makes an obtuse (or right) angle with the (outward
normal) vector a. this is illustrated in    gure 2.8.

the boundary of the halfspace (2.1) is the hyperplane {x | at x = b}. the set
{x | at x < b}, which is the interior of the halfspace {x | at x     b}, is called an
open halfspace.

2.2.2 euclidean balls and ellipsoids

a (euclidean) ball (or just ball) in rn has the form

b(xc, r) = {x | kx     xck2     r} = {x | (x     xc)t (x     xc)     r2},

where r > 0, and k    k2 denotes the euclidean norm, i.e., kuk2 = (ut u)1/2. the
vector xc is the center of the ball and the scalar r is its radius; b(xc, r) consists
of all points within a distance r of the center xc. another common representation
for the euclidean ball is

b(xc, r) = {xc + ru | kuk2     1}.

30

2 convex sets

xc

figure 2.9 an ellipsoid in r2, shown shaded. the center xc is shown as a
dot, and the two semi-axes are shown as line segments.

a euclidean ball is a convex set:

0            1, then

if kx1     xck2     r, kx2     xck2     r, and

k  x1 + (1       )x2     xck2 = k  (x1     xc) + (1       )(x2     xc)k2
      kx1     xck2 + (1       )kx2     xck2
    r.

(here we use the homogeneity property and triangle inequality for k  k2; see   a.1.2.)

a related family of convex sets is the ellipsoids, which have the form

e = {x | (x     xc)t p    1(x     xc)     1},

(2.3)
where p = p t     0, i.e., p is symmetric and positive de   nite. the vector xc     rn
is the center of the ellipsoid. the matrix p determines how far the ellipsoid extends
in every direction from xc; the lengths of the semi-axes of e are given by      i, where

  i are the eigenvalues of p . a ball is an ellipsoid with p = r2i. figure 2.9 shows
an ellipsoid in r2.

another common representation of an ellipsoid is
e = {xc + au | kuk2     1},

(2.4)

where a is square and nonsingular. in this representation we can assume without
loss of generality that a is symmetric and positive de   nite. by taking a = p 1/2,
this representation gives the ellipsoid de   ned in (2.3). when the matrix a in (2.4)
is symmetric positive semide   nite but singular, the set in (2.4) is called a degenerate
ellipsoid ; its a   ne dimension is equal to the rank of a. degenerate ellipsoids are
also convex.

2.2.3 norm balls and norm cones

suppose k  k is any norm on rn (see   a.1.2). from the general properties of norms it
can be shown that a norm ball of radius r and center xc, given by {x | kx   xck     r},
is convex. the norm cone associated with the norm k    k is the set

c = {(x, t) | kxk     t}     rn+1.

2.2 some important examples

31

1

0.5

t

0
1

0

x2

0

   1

   1

x1

1

figure 2.10 boundary of second-order cone in r3, {(x1, x2, t) | (x2
t}.

1+x2

2)1/2    

it is (as the name suggests) a convex cone.

example 2.3 the second-order cone is the norm cone for the euclidean norm, i.e.,

c = {(x, t)     rn+1 | kxk2     t}

t (cid:21)t(cid:20) i
(cid:20) x

0    1 (cid:21)(cid:20) x

0

t (cid:21)     0, t     0) .

t (cid:21) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
= ((cid:20) x

the second-order cone is also known by several other names. it is called the quadratic
cone, since it is de   ned by a quadratic inequality. it is also called the lorentz cone
or ice-cream cone. figure 2.10 shows the second-order cone in r3.

2.2.4 polyhedra

a polyhedron is de   ned as the solution set of a    nite number of linear equalities
and inequalities:

p = {x | at

j x     bj, j = 1, . . . , m, ct

j x = dj, j = 1, . . . , p}.

(2.5)

a polyhedron is thus the intersection of a    nite number of halfspaces and hyper-
planes. a   ne sets (e.g., subspaces, hyperplanes, lines), rays, line segments, and
halfspaces are all polyhedra.
it is easily shown that polyhedra are convex sets.
a bounded polyhedron is sometimes called a polytope, but some authors use the
opposite convention (i.e., polytope for any set of the form (2.5), and polyhedron

32

2 convex sets

a1

a2

a5

p

a3

a4

figure 2.11 the polyhedron p (shown shaded) is the intersection of    ve
halfspaces, with outward normal vectors a1, . . . . , a5.

when it is bounded). figure 2.11 shows an example of a polyhedron de   ned as the
intersection of    ve halfspaces.

it will be convenient to use the compact notation

p = {x | ax (cid:22) b, cx = d}

(2.6)

for (2.5), where

a =         

at
1
...
at
m

          ,

c =         

ct
1
...
ct
p

          ,

and the symbol (cid:22) denotes vector inequality or componentwise inequality in rm:
u (cid:22) v means ui     vi for i = 1, . . . , m.

example 2.4 the nonnegative orthant is the set of points with nonnegative compo-
nents, i.e.,

rn
+ = {x     rn | xi     0, i = 1, . . . , n} = {x     rn | x (cid:23) 0}.

(here r+ denotes the set of nonnegative numbers: r+ = {x     r | x     0}.) the
nonnegative orthant is a polyhedron and a cone (and therefore called a polyhedral
cone).

simplexes

simplexes are another important family of polyhedra. suppose the k + 1 points
v0, . . . , vk     rn are a   nely independent, which means v1     v0, . . . , vk     v0 are
linearly independent. the simplex determined by them is given by

c = conv{v0, . . . , vk} = {  0v0 +        +   kvk |    (cid:23) 0, 1t    = 1},

(2.7)

2.2 some important examples

33

where 1 denotes the vector with all entries one. the a   ne dimension of this simplex
is k, so it is sometimes referred to as a k-dimensional simplex in rn.

example 2.5 some common simplexes. a 1-dimensional simplex is a line segment;
a 2-dimensional simplex is a triangle (including its interior); and a 3-dimensional
simplex is a tetrahedron.

the unit simplex is the n-dimensional simplex determined by the zero vector and the
unit vectors, i.e., 0, e1, . . . , en     rn. it can be expressed as the set of vectors that
satisfy

x (cid:23) 0,

1t x     1.

the id203 simplex is the (n     1)-dimensional simplex determined by the unit
vectors e1, . . . , en     rn. it is the set of vectors that satisfy

x (cid:23) 0,

1t x = 1.

vectors in the id203 simplex correspond to id203 distributions on a set
with n elements, with xi interpreted as the id203 of the ith element.

to describe the simplex (2.7) as a polyhedron, i.e., in the form (2.6), we proceed
as follows. by de   nition, x     c if and only if x =   0v0 +   1v1 +       +   kvk for some
   (cid:23) 0 with 1t    = 1. equivalently, if we de   ne y = (  1, . . . ,   k) and

b =(cid:2) v1     v0

      

vk     v0 (cid:3)     rn  k,

we can say that x     c if and only if

x = v0 + by

(2.8)

for some y (cid:23) 0 with 1t y     1. now we note that a   ne independence of the
points v0, . . . , vk implies that the matrix b has rank k. therefore there exists a
nonsingular matrix a = (a1, a2)     rn  n such that
a2 (cid:21) b =(cid:20) i
0 (cid:21) .

ab =(cid:20) a1

multiplying (2.8) on the left with a, we obtain

a1x = a1v0 + y,

a2x = a2v0.

from this we see that x     c if and only if a2x = a2v0, and the vector y =
a1x     a1v0 satis   es y (cid:23) 0 and 1t y     1. in other words we have x     c if and only
if

a2x = a2v0,

a1x (cid:23) a1v0,

1t a1x     1 + 1t a1v0,

which is a set of linear equalities and inequalities in x, and so describes a polyhe-
dron.

34

2 convex sets

convex hull description of polyhedra
the convex hull of the    nite set {v1, . . . , vk} is

conv{v1, . . . , vk} = {  1v1 +        +   kvk |    (cid:23) 0, 1t    = 1}.

this set is a polyhedron, and bounded, but (except in special cases, e.g., a simplex)
it is not simple to express it in the form (2.5), i.e., by a set of linear equalities and
inequalities.

a generalization of this convex hull description is

{  1v1 +        +   kvk |   1 +        +   m = 1,   i     0, i = 1, . . . , k},

(2.9)

where m     k. here we consider nonnegative linear combinations of vi, but only
the    rst m coe   cients are required to sum to one. alternatively, we can inter-
pret (2.9) as the convex hull of the points v1, . . . , vm, plus the conic hull of the
points vm+1, . . . , vk. the set (2.9) de   nes a polyhedron, and conversely, every
polyhedron can be represented in this form (although we will not show this).

the question of how a polyhedron is represented is subtle, and has very im-
portant practical consequences. as a simple example consider the unit ball in the
      -norm in rn,

c = {x | |xi|     1, i = 1, . . . , n}.

the set c can be described in the form (2.5) with 2n linear inequalities   et
i x     1,
where ei is the ith unit vector. to describe it in the convex hull form (2.9) requires
at least 2n points:

c = conv{v1, . . . , v2n},

where v1, . . . , v2n are the 2n vectors all of whose components are 1 or    1. thus
the size of the two descriptions di   ers greatly, for large n.

2.2.5 the positive semide   nite cone

we use the notation sn to denote the set of symmetric n    n matrices,

sn = {x     rn  n | x = x t},

which is a vector space with dimension n(n + 1)/2. we use the notation sn
denote the set of symmetric positive semide   nite matrices:

+ to

and the notation sn

++ to denote the set of symmetric positive de   nite matrices:

sn
+ = {x     sn | x (cid:23) 0},

++ = {x     sn | x     0}.
sn

(this notation is meant to be analogous to r+, which denotes the nonnegative
reals, and r++, which denotes the positive reals.)

2.3 operations that preserve convexity

35

1

0.5

z

0
1

0

y

   1

0

0.5

x

1

figure 2.12 boundary of positive semide   nite cone in s2.

the set sn

+ is a convex cone: if   1,   2     0 and a, b     sn

+, then   1a+  2b     sn
+.
this can be seen directly from the de   nition of positive semide   niteness: for any
x     rn, we have

xt (  1a +   2b)x =   1xt ax +   2xt bx     0,

if a (cid:23) 0, b (cid:23) 0 and   1,   2     0.

example 2.6 positive semide   nite cone in s2. we have

x =(cid:20) x y

z (cid:21)     s2

y

+        x     0,

z     0,

xz     y2.

the boundary of this cone is shown in    gure 2.12, plotted in r3 as (x, y, z).

2.3 operations that preserve convexity

in this section we describe some operations that preserve convexity of sets, or
allow us to construct convex sets from others. these operations, together with the
simple examples described in   2.2, form a calculus of convex sets that is useful for
determining or establishing convexity of sets.

36

2.3.1 intersection

2 convex sets

convexity is preserved under intersection: if s1 and s2 are convex, then s1     s2 is
convex. this property extends to the intersection of an in   nite number of sets: if

s   is convex for every        a, thent     a s   is convex. (subspaces, a   ne sets, and

convex cones are also closed under arbitrary intersections.) as a simple example,
a polyhedron is the intersection of halfspaces and hyperplanes (which are convex),
and therefore is convex.

example 2.7 the positive semide   nite cone sn

+ can be expressed as

\z6=0

{x     sn | zt xz     0}.

for each z 6= 0, zt xz is a (not identically zero) linear function of x, so the sets

{x     sn | zt xz     0}

are, in fact, halfspaces in sn. thus the positive semide   nite cone is the intersection
of an in   nite number of halfspaces, and so is convex.

example 2.8 we consider the set

s = {x     rm | |p(t)|     1 for |t|       /3},

(2.10)

k=1 xk cos kt. the set s can be expressed as the intersection of an

where p(t) =pm
in   nite number of slabs: s =t|t|     /3 st, where

st = {x |     1     (cos t, . . . , cos mt)t x     1},

and so is convex. the de   nition and the set are illustrated in    gures 2.13 and 2.14,
for m = 2.

in the examples above we establish convexity of a set by expressing it as a
(possibly in   nite) intersection of halfspaces. we will see in   2.5.1 that a converse
holds: every closed convex set s is a (usually in   nite) intersection of halfspaces.
in fact, a closed convex set s is the intersection of all halfspaces that contain it:

s =\ {h | h halfspace, s     h}.

2.3.2 a   ne functions

recall that a function f : rn     rm is a   ne if it is a sum of a linear function and
a constant, i.e., if it has the form f (x) = ax + b, where a     rm  n and b     rm.
suppose s     rn is convex and f : rn     rm is an a   ne function. then the image
of s under f ,

f (s) = {f (x) | x     s},

2.3 operations that preserve convexity

37

1

0
   1

)
t
(
p

0

  /3

t

2  /3

  

figure 2.13 three trigonometric polynomials associated with points in the
set s de   ned in (2.10), for m = 2. the trigonometric polynomial plotted
with dashed line type is the average of the other two.

2

1

2
x

0

   1

   2
   2

s

   1

0
x1

1

2

figure 2.14 the set s de   ned in (2.10), for m = 2, is shown as the white
area in the middle of the plot. the set is the intersection of an in   nite
number of slabs (20 of which are shown), hence convex.

38

2 convex sets

is convex. similarly, if f : rk     rn is an a   ne function, the inverse image of s
under f ,

f    1(s) = {x | f (x)     s},

is convex.

two simple examples are scaling and translation. if s     rn is convex,        r,

and a     rn, then the sets   s and s + a are convex, where

  s = {  x | x     s},

s + a = {x + a | x     s}.

the projection of a convex set onto some of its coordinates is convex:
rm    rn is convex, then

t = {x1     rm | (x1, x2)     s for some x2     rn}

if s    

is convex.

the sum of two sets is de   ned as

s1 + s2 = {x + y | x     s1, y     s2}.

if s1 and s2 are convex, then s1 + s2 is convex. to see this, if s1 and s2 are
convex, then so is the direct or cartesian product

s1    s2 = {(x1, x2) | x1     s1, x2     s2}.

the image of this set under the linear function f (x1, x2) = x1 + x2 is the sum
s1 + s2.

we can also consider the partial sum of s1, s2     rn    rm, de   ned as

s = {(x, y1 + y2) | (x, y1)     s1, (x, y2)     s2},

where x     rn and yi     rm. for m = 0, the partial sum gives the intersection of
s1 and s2; for n = 0, it is set addition. partial sums of convex sets are convex (see
exercise 2.16).

example 2.9 polyhedron. the polyhedron {x | ax (cid:22) b, cx = d} can be expressed as
the inverse image of the cartesian product of the nonnegative orthant and the origin
under the a   ne function f (x) = (b     ax, d     cx):

{x | ax (cid:22) b, cx = d} = {x | f (x)     rm

+    {0}}.

example 2.10 solution set of linear matrix inequality. the condition

(2.11)
where b, ai     sm, is called a linear matrix inequality (lmi) in x. (note the similarity
to an ordinary linear inequality,

a(x) = x1a1 +        + xnan (cid:22) b,

at x = x1a1 +        + xnan     b,

with b, ai     r.)
the solution set of a linear matrix inequality, {x | a(x) (cid:22) b}, is convex. indeed,
it is the inverse image of the positive semide   nite cone under the a   ne function
f : rn     sm given by f (x) = b     a(x).

2.3 operations that preserve convexity

39

example 2.11 hyperbolic cone. the set

{x | xt p x     (ct x)2, ct x     0}

where p     sn
cone,

+ and c     rn, is convex, since it is the inverse image of the second-order

under the a   ne function f (x) = (p 1/2x, ct x).

{(z, t) | zt z     t2, t     0},

example 2.12 ellipsoid. the ellipsoid

e = {x | (x     xc)t p    1(x     xc)     1},

where p     sn
++, is the image of the unit euclidean ball {u | kuk2     1} under the
a   ne mapping f (u) = p 1/2u + xc. (it is also the inverse image of the unit ball under
the a   ne mapping g(x) = p    1/2(x     xc).)

2.3.3 linear-fractional and perspective functions

in this section we explore a class of functions, called linear-fractional, that is more
general than a   ne but still preserves convexity.

the perspective function
we de   ne the perspective function p : rn+1     rn, with domain dom p = rn   
r++, as p (z, t) = z/t. (here r++ denotes the set of positive numbers: r++ =
{x     r | x > 0}.) the perspective function scales or normalizes vectors so the last
component is one, and then drops the last component.

remark 2.1 we can interpret the perspective function as the action of a pin-hole
camera. a pin-hole camera (in r3) consists of an opaque horizontal plane x3 = 0,
with a single pin-hole at the origin, through which light can pass, and a horizontal
image plane x3 =    1. an object at x, above the camera (i.e., with x3 > 0), forms
an image at the point    (x1/x3, x2/x3, 1) on the image plane. dropping the last
component of the image point (since it is always    1), the image of a point at x
appears at y =    (x1/x3, x2/x3) =    p (x) on the image plane. this is illustrated in
   gure 2.15.

if c     dom p is convex, then its image

p (c) = {p (x) | x     c}

is convex. this result is certainly intuitive: a convex object, viewed through a
pin-hole camera, yields a convex image. to establish this fact we show that line
segments are mapped to line segments under the perspective function. (this too

40

2 convex sets

x3 = 0

x3 =    1

figure 2.15 pin-hole camera interpretation of perspective function. the
dark horizontal line represents the plane x3 = 0 in r3, which is opaque,
except for a pin-hole at the origin. objects or light sources above the plane
appear on the image plane x3 =    1, which is shown as the lighter horizontal
line. the mapping of the position of a source to the position of its image is
related to the perspective function.

makes sense: a line segment, viewed through a pin-hole camera, yields a line seg-
ment image.) suppose that x = (  x, xn+1), y = (  y, yn+1)     rn+1 with xn+1 > 0,
yn+1 > 0. then for 0            1,

p (  x + (1       )y) =

    x + (1       )  y

  xn+1 + (1       )yn+1

=   p (x) + (1       )p (y),

where

   =

  xn+1

  xn+1 + (1       )yn+1     [0, 1].

this correspondence between    and    is monotonic: as    varies between 0 and 1
(which sweeps out the line segment [x, y]),    varies between 0 and 1 (which sweeps
out the line segment [p (x), p (y)]). this shows that p ([x, y]) = [p (x), p (y)].

now suppose c is convex with c     dom p (i.e., xn+1 > 0 for all x     c), and
x, y     c. to establish convexity of p (c) we need to show that the line segment
[p (x), p (y)] is in p (c). but this line segment is the image of the line segment
[x, y] under p , and so lies in p (c).

the inverse image of a convex set under the perspective function is also convex:

if c     rn is convex, then

p    1(c) = {(x, t)     rn+1 | x/t     c, t > 0}

is convex. to show this, suppose (x, t)     p    1(c), (y, s)     p    1(c), and 0            1.
we need to show that

i.e., that

  (x, t) + (1       )(y, s)     p    1(c),

  x + (1       )y
  t + (1       )s     c

2.3 operations that preserve convexity

41

(  t + (1       )s > 0 is obvious). this follows from

where

  x + (1       )y
  t + (1       )s

=   (x/t) + (1       )(y/s),

   =

  t

  t + (1       )s     [0, 1].

linear-fractional functions

a linear-fractional function is formed by composing the perspective function with
an a   ne function. suppose g : rn     rm+1 is a   ne, i.e.,
d (cid:21) ,
ct (cid:21) x +(cid:20) b

g(x) =(cid:20) a

(2.12)

where a     rm  n, b     rm, c     rn, and d     r. the function f : rn     rm given
by f = p     g, i.e.,

f (x) = (ax + b)/(ct x + d),

dom f = {x | ct x + d > 0},

(2.13)

is called a linear-fractional (or projective) function. if c = 0 and d > 0, the domain
of f is rn, and f is an a   ne function. so we can think of a   ne and linear functions
as special cases of linear-fractional functions.

remark 2.2 projective interpretation. it is often convenient to represent a linear-
fractional function as a matrix

q =(cid:20) a b

d (cid:21)     r(m+1)  (n+1)

ct

(2.14)

that acts on (multiplies) points of form (x, 1), which yields (ax + b, ct x + d). this
result is then scaled or normalized so that its last component is one, which yields
(f (x), 1).
this representation can be interpreted geometrically by associating rn with a set
of rays in rn+1 as follows. with each point z in rn we associate the (open) ray
p(z) = {t(z, 1) | t > 0} in rn+1. the last component of this ray takes on positive
values. conversely any ray in rn+1, with base at the origin and last component
which takes on positive values, can be written as p(v) = {t(v, 1) | t     0} for some
v     rn. this (projective) correspondence p between rn and the halfspace of rays
with positive last component is one-to-one and onto.

the linear-fractional function (2.13) can be expressed as

f (x) = p    1(qp(x)).

thus, we start with x     dom f , i.e., ct x + d > 0. we then form the ray p(x) in
rn+1. the linear transformation with matrix q acts on this ray to produce another
ray qp(x). since x     dom f , the last component of this ray assumes positive values.
finally we take the inverse projective transformation to recover f (x).

42

2 convex sets

1

2
x

0

   1
   1

c

0
x1

1

2
x

0

f (c)

1

   1
   1

0
x1

1

figure 2.16 left. a set c     r2. the dashed line shows the boundary of
the domain of the linear-fractional function f (x) = x/(x1 + x2 + 1) with
dom f = {(x1, x2) | x1 + x2 + 1 > 0}. right. image of c under f . the
dashed line shows the boundary of the domain of f    1.

like the perspective function, linear-fractional functions preserve convexity. if
c is convex and lies in the domain of f (i.e., ct x + d > 0 for x     c), then its
image f (c) is convex. this follows immediately from results above: the image of c
under the a   ne mapping (2.12) is convex, and the image of the resulting set under
the perspective function p , which yields f (c), is convex. similarly, if c     rm is
convex, then the inverse image f    1(c) is convex.

example 2.13 conditional probabilities. suppose u and v are random variables
that take on values in {1, . . . , n} and {1, . . . , m}, respectively, and let pij denote
prob(u = i, v = j). then the id155 fij = prob(u = i|v = j) is
given by

thus f is obtained by a linear-fractional mapping from p.

fij =

.

k=1 pkj

pijpn

it follows that if c is a convex set of joint probabilities for (u, v), then the associated
set of conditional probabilities of u given v is also convex.

figure 2.16 shows a set c     r2, and its image under the linear-fractional

function

f (x) =

1

x1 + x2 + 1

x,

dom f = {(x1, x2) | x1 + x2 + 1 > 0}.

2.4 generalized inequalities

43

2.4 generalized inequalities

2.4.1 proper cones and generalized inequalities

a cone k     rn is called a proper cone if it satis   es the following:

    k is convex.
    k is closed.
    k is solid, which means it has nonempty interior.
    k is pointed, which means that it contains no line (or equivalently, x    

k,     x     k =    x = 0).

a proper cone k can be used to de   ne a generalized inequality, which is a partial
ordering on rn that has many of the properties of the standard ordering on r.
we associate with the proper cone k the partial ordering on rn de   ned by

x (cid:22)k y        y     x     k.

we also write x (cid:23)k y for y (cid:22)k x. similarly, we de   ne an associated strict partial
ordering by

x    k y        y     x     int k,

and write x    k y for y    k x.
(to distinguish the generalized inequality (cid:22)k
from the strict generalized inequality, we sometimes refer to (cid:22)k as the nonstrict
generalized inequality.)
when k = r+, the partial ordering (cid:22)k is the usual ordering     on r, and
the strict partial ordering    k is the same as the usual strict ordering < on r.
so generalized inequalities include as a special case ordinary (nonstrict and strict)
inequality in r.

example 2.14 nonnegative orthant and componentwise inequality. the nonnegative
orthant k = rn
+ is a proper cone. the associated generalized inequality (cid:22)k corre-
sponds to componentwise inequality between vectors: x (cid:22)k y means that xi     yi,
i = 1, . . . , n. the associated strict inequality corresponds to componentwise strict
inequality: x    k y means that xi < yi, i = 1, . . . , n.
the nonstrict and strict partial orderings associated with the nonnegative orthant
arise so frequently that we drop the subscript rn
+; it is understood when the symbol
(cid:22) or     appears between vectors.

example 2.15 positive semide   nite cone and matrix inequality. the positive semidef-
+ is a proper cone in sn. the associated generalized inequality (cid:22)k is the
inite cone sn
usual matrix inequality: x (cid:22)k y means y     x is positive semide   nite. the inte-
rior of sn
+ (in sn) consists of the positive de   nite matrices, so the strict generalized
inequality also agrees with the usual strict inequality between symmetric matrices:
x    k y means y     x is positive de   nite.
here, too, the partial ordering arises so frequently that we drop the subscript: for
symmetric matrices we write simply x (cid:22) y or x     y . it is understood that the
generalized inequalities are with respect to the positive semide   nite cone.

44

2 convex sets

example 2.16 cone of polynomials nonnegative on [0, 1]. let k be de   ned as

k = {c     rn | c1 + c2t +        + cntn   1     0 for t     [0, 1]},

(2.15)

i.e., k is the cone of (coe   cients of) polynomials of degree n   1 that are nonnegative
on the interval [0, 1]. it can be shown that k is a proper cone; its interior is the set
of coe   cients of polynomials that are positive on the interval [0, 1].
two vectors c, d     rn satisfy c (cid:22)k d if and only if

c1 + c2t +        + cntn   1     d1 + d2t +        + dntn   1

for all t     [0, 1].

properties of generalized inequalities

a generalized inequality (cid:22)k satis   es many properties, such as

    (cid:22)k is preserved under addition: if x (cid:22)k y and u (cid:22)k v, then x + u (cid:22)k y + v.
    (cid:22)k is transitive: if x (cid:22)k y and y (cid:22)k z then x (cid:22)k z.
    (cid:22)k is preserved under nonnegative scaling:

if x (cid:22)k y and        0 then

  x (cid:22)k   y.

    (cid:22)k is re   exive: x (cid:22)k x.
    (cid:22)k is antisymmetric: if x (cid:22)k y and y (cid:22)k x, then x = y.
    (cid:22)k is preserved under limits: if xi (cid:22)k yi for i = 1, 2, . . ., xi     x and yi     y

as i        , then x (cid:22)k y.

the corresponding strict generalized inequality    k satis   es, for example,

    if x    k y then x (cid:22)k y.
    if x    k y and u (cid:22)k v then x + u    k y + v.
    if x    k y and    > 0 then   x    k   y.
    x 6   k x.
    if x    k y, then for u and v small enough, x + u    k y + v.

these properties are inherited from the de   nitions of (cid:22)k and    k, and the prop-
erties of proper cones; see exercise 2.30.

2.4 generalized inequalities

45

2.4.2 minimum and minimal elements

the notation of generalized inequality (i.e., (cid:22)k,    k) is meant to suggest the
analogy to ordinary inequality on r (i.e.,    , <). while many properties of ordinary
inequality do hold for generalized inequalities, some important ones do not. the
most obvious di   erence is that     on r is a linear ordering: any two points are
comparable, meaning either x     y or y     x. this property does not hold for
other generalized inequalities. one implication is that concepts like minimum and
maximum are more complicated in the context of generalized inequalities. we
brie   y discuss this in this section.

we say that x     s is the minimum element of s (with respect to the general-
ized inequality (cid:22)k ) if for every y     s we have x (cid:22)k y. we de   ne the maximum
element of a set s, with respect to a generalized inequality, in a similar way. if a
set has a minimum (maximum) element, then it is unique. a related concept is
minimal element. we say that x     s is a minimal element of s (with respect to
the generalized inequality (cid:22)k ) if y     s, y (cid:22)k x only if y = x. we de   ne maxi-
mal element in a similar way. a set can have many di   erent minimal (maximal)
elements.

we can describe minimum and minimal elements using simple set notation. a

point x     s is the minimum element of s if and only if

s     x + k.

here x + k denotes all the points that are comparable to x and greater than or
equal to x (according to (cid:22)k). a point x     s is a minimal element if and only if

(x     k)     s = {x}.

here x     k denotes all the points that are comparable to x and less than or equal
to x (according to (cid:22)k); the only point in common with s is x.
for k = r+, which induces the usual ordering on r, the concepts of minimal
and minimum are the same, and agree with the usual de   nition of the minimum
element of a set.

example 2.17 consider the cone r2
+, which induces componentwise inequality in
r2. here we can give some simple geometric descriptions of minimal and minimum
elements. the inequality x (cid:22) y means y is above and to the right of x. to say that
x     s is the minimum element of a set s means that all other points of s lie above
and to the right. to say that x is a minimal element of a set s means that no other
point of s lies to the left and below x. this is illustrated in    gure 2.17.

example 2.18 minimum and minimal elements of a set of symmetric matrices. we
associate with each a     sn

++ an ellipsoid centered at the origin, given by

ea = {x | xt a   1x     1}.

we have a (cid:22) b if and only if ea     eb.
let v1, . . . , vk     rn be given and de   ne
++ | vt

s = {p     sn

i p    1vi     1, i = 1, . . . , k},

46

2 convex sets

s1

x1

s2

x2

figure 2.17 left. the set s1 has a minimum element x1 with respect to
componentwise inequality in r2. the set x1 + k is shaded lightly; x1 is
the minimum element of s1 since s1     x1 + k. right. the point x2 is a
minimal point of s2. the set x2     k is shown lightly shaded. the point x2
is minimal because x2     k and s2 intersect only at x2.

which corresponds to the set of ellipsoids that contain the points v1, . . . , vk. the
set s does not have a minimum element: for any ellipsoid that contains the points
v1, . . . , vk we can    nd another one that contains the points, and is not comparable
to it. an ellipsoid is minimal if it contains the points, but no smaller ellipsoid does.
figure 2.18 shows an example in r2 with k = 2.

2.5 separating and supporting hyperplanes

2.5.1 separating hyperplane theorem

in this section we describe an idea that will be important later: the use of hyper-
planes or a   ne functions to separate convex sets that do not intersect. the basic
result is the separating hyperplane theorem: suppose c and d are nonempty dis-
joint convex sets, i.e., c     d =    . then there exist a 6= 0 and b such that at x     b
for all x     c and at x     b for all x     d. in other words, the a   ne function at x    b
is nonpositive on c and nonnegative on d. the hyperplane {x | at x = b} is called
a separating hyperplane for the sets c and d, or is said to separate the sets c and
d. this is illustrated in    gure 2.19.

proof of separating hyperplane theorem

here we consider a special case, and leave the extension of the proof to the gen-
eral case as an exercise (exercise 2.22). we assume that the (euclidean) distance
between c and d, de   ned as

dist(c, d) = inf{ku     vk2 | u     c, v     d},

2.5 separating and supporting hyperplanes

47

e2

e3

e1

figure 2.18 three ellipsoids in r2, centered at the origin (shown as the
lower dot), that contain the points shown as the upper dots. the ellipsoid
e1 is not minimal, since there exist ellipsoids that contain the points, and
are smaller (e.g., e3). e3 is not minimal for the same reason. the ellipsoid
e2 is minimal, since no other ellipsoid (centered at the origin) contains the
points and is contained in e2.

at x     b

at x     b

d

a

c

figure 2.19 the hyperplane {x | at x = b} separates the disjoint convex sets
c and d. the a   ne function at x     b is nonpositive on c and nonnegative
on d.

48

2 convex sets

a

d

d

c

c

figure 2.20 construction of a separating hyperplane between two convex
sets. the points c     c and d     d are the pair of points in the two sets that
are closest to each other. the separating hyperplane is orthogonal to, and
bisects, the line segment between c and d.

is positive, and that there exist points c     c and d     d that achieve the minimum
distance, i.e., kc     dk2 = dist(c, d). (these conditions are satis   ed, for example,
when c and d are closed and one set is bounded.)

de   ne

we will show that the a   ne function

a = d     c,

b = kdk2

2     kck2
2

2

.

f (x) = at x     b = (d     c)t (x     (1/2)(d + c))

is nonpositive on c and nonnegative on d, i.e., that the hyperplane {x | at x = b}
separates c and d. this hyperplane is perpendicular to the line segment between
c and d, and passes through its midpoint, as shown in    gure 2.20.

we    rst show that f is nonnegative on d. the proof that f is nonpositive on
c is similar (or follows by swapping c and d and considering    f ). suppose there
were a point u     d for which

f (u) = (d     c)t (u     (1/2)(d + c)) < 0.

(2.16)

we can express f (u) as

f (u) = (d     c)t (u     d + (1/2)(d     c)) = (d     c)t (u     d) + (1/2)kd     ck2
2.

we see that (2.16) implies (d     c)t (u     d) < 0. now we observe that

d
dtkd + t(u     d)     ck2

= 2(d     c)t (u     d) < 0,

2(cid:12)(cid:12)(cid:12)(cid:12)t=0

so for some small t > 0, with t     1, we have

kd + t(u     d)     ck2 < kd     ck2,

2.5 separating and supporting hyperplanes

49

i.e., the point d + t(u     d) is closer to c than d is. since d is convex and contains
d and u, we have d + t(u    d)     d. but this is impossible, since d is assumed to be
the point in d that is closest to c.

example 2.19 separation of an a   ne and a convex set. suppose c is convex and
d is a   ne, i.e., d = {f u + g | u     rm}, where f     rn  m. suppose c and d are
disjoint, so by the separating hyperplane theorem there are a 6= 0 and b such that
at x     b for all x     c and at x     b for all x     d.
now at x     b for all x     d means at f u     b     at g for all u     rm. but a linear
function is bounded below on rm only when it is zero, so we conclude at f = 0 (and
hence, b     at g).
thus we conclude that there exists a 6= 0 such that f t a = 0 and at x     at g for all
x     c.

strict separation

the separating hyperplane we constructed above satis   es the stronger condition
that at x < b for all x     c and at x > b for all x     d. this is called strict
separation of the sets c and d. simple examples show that in general, disjoint
convex sets need not be strictly separable by a hyperplane (even when the sets are
closed; see exercise 2.23). in many special cases, however, strict separation can be
established.

example 2.20 strict separation of a point and a closed convex set. let c be a closed
convex set and x0 6    c. then there exists a hyperplane that strictly separates x0
from c.

to see this, note that the two sets c and b(x0,   ) do not intersect for some    > 0.
by the separating hyperplane theorem, there exist a 6= 0 and b such that at x     b for
x     c and at x     b for x     b(x0,   ).
using b(x0,   ) = {x0 + u | kuk2       }, the second condition can be expressed as

at (x0 + u)     b for all kuk2       .

the u that minimizes the lefthand side is u =      a/kak2; using this value we have

therefore the a   ne function

at x0       kak2     b.

f (x) = at x     b       kak2/2

is negative on c and positive at x0.

as an immediate consequence we can establish a fact that we already mentioned
above: a closed convex set is the intersection of all halfspaces that contain it. indeed,
let c be closed and convex, and let s be the intersection of all halfspaces containing
c. obviously x     c     x     s. to show the converse, suppose there exists x     s,
x 6    c. by the strict separation result there exists a hyperplane that strictly separates
x from c, i.e., there is a halfspace containing c but not x. in other words, x 6    s.

50

2 convex sets

converse separating hyperplane theorems

the converse of the separating hyperplane theorem (i.e., existence of a separating
hyperplane implies that c and d do not intersect) is not true, unless one imposes
additional constraints on c or d, even beyond convexity. as a simple counterex-
ample, consider c = d = {0}     r. here the hyperplane x = 0 separates c and
d.
by adding conditions on c and d various converse separation theorems can be
derived. as a very simple example, suppose c and d are convex sets, with c open,
and there exists an a   ne function f that is nonpositive on c and nonnegative on
d. then c and d are disjoint. (to see this we    rst note that f must be negative
on c; for if f were zero at a point of c then f would take on positive values near
the point, which is a contradiction. but then c and d must be disjoint since f
is negative on c and nonnegative on d.) putting this converse together with the
separating hyperplane theorem, we have the following result: any two convex sets
c and d, at least one of which is open, are disjoint if and only if there exists a
separating hyperplane.

example 2.21 theorem of alternatives for strict linear inequalities. we derive the
necessary and su   cient conditions for solvability of a system of strict linear inequal-
ities

these inequalities are infeasible if and only if the (convex) sets

ax     b.

(2.17)

c = {b     ax | x     rn},

d = rm

++ = {y     rm | y     0}

do not intersect. the set d is open; c is an a   ne set. hence by the result above, c
and d are disjoint if and only if there exists a separating hyperplane, i.e., a nonzero
       rm and        r such that   t y        on c and   t y        on d.
each of these conditions can be simpli   ed. the    rst means   t (b     ax)        for all x.
this implies (as in example 2.19) that at    = 0 and   t b       . the second inequality
means   t y        for all y     0. this implies        0 and    (cid:23) 0,    6= 0.
putting it all together, we    nd that the set of strict inequalities (2.17) is infeasible if
and only if there exists        rm such that

   6= 0,

(2.18)
this is also a system of linear inequalities and linear equations in the variable        rm.
we say that (2.17) and (2.18) form a pair of alternatives: for any data a and b, exactly
one of them is solvable.

   (cid:23) 0,

  t b     0.

at    = 0,

2.5.2 supporting hyperplanes

suppose c     rn, and x0 is a point in its boundary bd c, i.e.,

x0     bd c = cl c \ int c.

if a 6= 0 satis   es at x     at x0 for all x     c, then the hyperplane {x | at x = at x0}
is called a supporting hyperplane to c at the point x0. this is equivalent to saying

2.6 dual cones and generalized inequalities

51

a

x0

c

figure 2.21 the hyperplane {x | at x = at x0} supports c at x0.

that the point x0 and the set c are separated by the hyperplane {x | at x = at x0}.
the geometric interpretation is that the hyperplane {x | at x = at x0} is tangent
to c at x0, and the halfspace {x | at x     at x0} contains c. this is illustrated in
   gure 2.21.
a basic result, called the supporting hyperplane theorem, states that for any
nonempty convex set c, and any x0     bd c, there exists a supporting hyperplane to
c at x0. the supporting hyperplane theorem is readily proved from the separating
hyperplane theorem. we distinguish two cases. if the interior of c is nonempty,
the result follows immediately by applying the separating hyperplane theorem to
the sets {x0} and int c. if the interior of c is empty, then c must lie in an a   ne
set of dimension less than n, and any hyperplane containing that a   ne set contains
c and x0, and is a (trivial) supporting hyperplane.

there is also a partial converse of the supporting hyperplane theorem: if a set
is closed, has nonempty interior, and has a supporting hyperplane at every point
in its boundary, then it is convex. (see exercise 2.27.)

2.6 dual cones and generalized inequalities

2.6.1 dual cones

let k be a cone. the set

k     = {y | xt y     0 for all x     k}

(2.19)

is called the dual cone of k. as the name suggests, k     is a cone, and is always
convex, even when the original cone k is not (see exercise 2.31).

geometrically, y     k     if and only if    y is the normal of a hyperplane that

supports k at the origin. this is illustrated in    gure 2.22.

example 2.22 subspace. the dual cone of a subspace v     rn (which is a cone) is
its orthogonal complement v     = {y | vt y = 0 for all v     v }.

52

2 convex sets

y

k

z

k

figure 2.22 left. the halfspace with inward normal y contains the cone k,
so y     k    . right. the halfspace with inward normal z does not contain k,
so z 6    k    .

example 2.23 nonnegative orthant. the cone rn

+ is its own dual:

xt y     0 for all x (cid:23) 0        y (cid:23) 0.

we call such a cone self-dual.

example 2.24 positive semide   nite cone. on the set of symmetric n    n matrices
i,j=1 xijyij (see   a.1.1). the
positive semide   nite cone sn

sn, we use the standard inner product tr(xy ) =pn

+ is self-dual, i.e., for x, y     sn,
tr(xy )     0 for all x (cid:23) 0        y (cid:23) 0.

we will establish this fact.
suppose y 6    sn

+. then there exists q     rn with

qt y q = tr(qqt y ) < 0.

+)   .

hence the positive semide   nite matrix x = qqt satis   es tr(xy ) < 0; it follows that
y 6    (sn
now suppose x, y     sn

+. we can express x in terms of its eigenvalue decomposition

i , where (the eigenvalues)   i     0, i = 1, . . . , n. then we have

i=1   iqiqt

as x =pn

tr(y x) = tr y

  iqiqt

i! =

nxi=1

nxi=1

  iqt

i y qi     0.

this shows that y     (sn

+)   .

example 2.25 dual of a norm cone. let k    k be a norm on rn. the dual of the
associated cone k = {(x, t)     rn+1 | kxk     t} is the cone de   ned by the dual norm,
i.e.,

k     = {(u, v)     rn+1 | kuk        v},

2.6 dual cones and generalized inequalities

53

where the dual norm is given by kuk    = sup{ut x | kxk     1} (see (a.1.6)).
to prove the result we have to show that

xt u + tv     0 whenever kxk     t        kuk        v.

(2.20)

let us start by showing that the righthand condition on (u, v) implies the lefthand
condition. suppose kuk        v, and kxk     t for some t > 0. (if t = 0, x must be zero,
so obviously ut x + vt     0.) applying the de   nition of the dual norm, and the fact
that k   x/tk     1, we have

ut (   x/t)     kuk        v,

and therefore ut x + vt     0.
next we show that the lefthand condition in (2.20) implies the righthand condition
in (2.20). suppose kuk    > v, i.e., that the righthand condition does not hold. then
by the de   nition of the dual norm, there exists an x with kxk     1 and xt u > v.
taking t = 1, we have

ut (   x) + v < 0,
which contradicts the lefthand condition in (2.20).

dual cones satisfy several properties, such as:
    k     is closed and convex.
    k1     k2 implies k    
2     k    
1 .
    if k has nonempty interior, then k     is pointed.
    if the closure of k is pointed then k     has nonempty interior.
    k        is the closure of the convex hull of k. (hence if k is convex and closed,

k        = k.)

(see exercise 2.31.) these properties show that if k is a proper cone, then so is its
dual k    , and moreover, that k        = k.

2.6.2 dual generalized inequalities

now suppose that the convex cone k is proper, so it induces a generalized inequality
(cid:22)k . then its dual cone k     is also proper, and therefore induces a generalized
inequality. we refer to the generalized inequality (cid:22)k     as the dual of the generalized
inequality (cid:22)k .
some important properties relating a generalized inequality and its dual are:
    x (cid:22)k y if and only if   t x       t y for all    (cid:23)k     0.
    x    k y if and only if   t x <   t y for all    (cid:23)k     0,    6= 0.
since k = k       , the dual generalized inequality associated with (cid:22)k     is (cid:22)k, so
these properties hold if the generalized inequality and its dual are swapped. as a
speci   c example, we have    (cid:22)k        if and only if   t x       t x for all x (cid:23)k 0.

54

2 convex sets

example 2.26 theorem of alternatives for linear strict generalized inequalities. sup-
pose k     rm is a proper cone. consider the strict generalized inequality

ax    k b,

(2.21)

where x     rn.
we will derive a theorem of alternatives for this inequality. suppose it is infeasible,
i.e., the a   ne set {b     ax | x     rn} does not intersect the open convex set int k.
then there is a separating hyperplane, i.e., a nonzero        rm and        r such that
  t (b     ax)        for all x, and   t y        for all y     int k. the    rst condition implies
at    = 0 and   t b       . the second condition implies   t y        for all y     k, which
can only happen if        k     and        0.
putting it all together we    nd that if (2.21) is infeasible, then there exists    such that

   6= 0,

   (cid:23)k     0,

at    = 0,

  t b     0.

(2.22)

now we show the converse: if (2.22) holds, then the inequality system (2.21) cannot
be feasible. suppose that both inequality systems hold. then we have   t (b     ax) >
0, since    6= 0,    (cid:23)k     0, and b     ax    k 0. but using at    = 0 we    nd that
  t (b     ax) =   t b     0, which is a contradiction.
thus, the inequality systems (2.21) and (2.22) are alternatives: for any data a, b,
exactly one of them is feasible. (this generalizes the alternatives (2.17), (2.18) for
the special case k = rm

+ .)

2.6.3 minimum and minimal elements via dual inequalities

we can use dual generalized inequalities to characterize minimum and minimal
elements of a (possibly nonconvex) set s     rm with respect to the generalized
inequality induced by a proper cone k.

dual characterization of minimum element

we    rst consider a characterization of the minimum element: x is the minimum
element of s, with respect to the generalized inequality (cid:22)k, if and only if for all
      k     0, x is the unique minimizer of   t z over z     s. geometrically, this means
that for any       k     0, the hyperplane

{z |   t (z     x) = 0}

is a strict supporting hyperplane to s at x. (by strict supporting hyperplane, we
mean that the hyperplane intersects s only at the point x.) note that convexity
of the set s is not required. this is illustrated in    gure 2.23.

to show this result, suppose x is the minimum element of s, i.e., x (cid:22)k z for
all z     s, and let       k     0. let z     s, z 6= x. since x is the minimum element of
s, we have z     x (cid:23)k 0. from       k     0 and z     x (cid:23)k 0, z     x 6= 0, we conclude
  t (z     x) > 0. since z is an arbitrary element of s, not equal to x, this shows
that x is the unique minimizer of   t z over z     s. conversely, suppose that for all
      k     0, x is the unique minimizer of   t z over z     s, but x is not the minimum

2.6 dual cones and generalized inequalities

55

s

x

figure 2.23 dual characterization of minimum element. the point x is the
minimum element of the set s with respect to r2
+. this is equivalent to:
for every        0, the hyperplane {z |   t (z     x) = 0} strictly supports s at
x, i.e., contains s on one side, and touches it only at x.

element of s. then there exists z     s with z 6(cid:23)k x. since z     x 6(cid:23)k 0, there exists
     (cid:23)k     0 with     t (z    x) < 0. hence   t (z    x) < 0 for       k     0 in the neighborhood
of     . this contradicts the assumption that x is the unique minimizer of   t z over
s.

dual characterization of minimal elements

we now turn to a similar characterization of minimal elements. here there is a gap
between the necessary and su   cient conditions. if       k     0 and x minimizes   t z
over z     s, then x is minimal. this is illustrated in    gure 2.24.
to show this, suppose that       k     0, and x minimizes   t z over s, but x is not
minimal, i.e., there exists a z     s, z 6= x, and z (cid:22)k x. then   t (x     z) > 0, which
contradicts our assumption that x is the minimizer of   t z over s.
the converse is in general false: a point x can be minimal in s, but not a
minimizer of   t z over z     s, for any   , as shown in    gure 2.25. this    gure
suggests that convexity plays an important role in the converse, which is correct.
provided the set s is convex, we can say that for any minimal element x there
exists a nonzero    (cid:23)k     0 such that x minimizes   t z over z     s.
to show this, suppose x is minimal, which means that ((x     k) \ {x})    s =    .
applying the separating hyperplane theorem to the convex sets (x     k) \ {x} and
s, we conclude that there is a    6= 0 and    such that   t (x     y)        for all y     k,
and   t z        for all z     s. from the    rst inequality we conclude    (cid:23)k     0. since
x     s and x     x     k, we have   t x =   , so the second inequality implies that   
is the minimum value of   t z over s. therefore, x is a minimizer of   t z over s,
where    6= 0,    (cid:23)k     0.
this converse theorem cannot be strengthened to       k     0. examples show
that a point x can be a minimal point of a convex set s, but not a minimizer of

56

2 convex sets

  1

x1

s

x2

  2

figure 2.24 a set s     r2. its set of minimal points, with respect to r2
+, is
shown as the darker section of its (lower, left) boundary. the minimizer of
  t
1 z over s is x1, and is minimal since   1     0. the minimizer of   t
2 z over
s is x2, which is another minimal point of s, since   2     0.

s

x

figure 2.25 the point x is a minimal element of s     r2 with respect to
r2
+. however there exists no    for which x minimizes   t z over z     s.

2.6 dual cones and generalized inequalities

57

x1

s1

s2

x2

figure 2.26 left. the point x1     s1 is minimal, but is not a minimizer of
  t z over s1 for any        0. (it does, however, minimize   t z over z     s1 for
   = (1, 0).) right. the point x2     s2 is not minimal, but it does minimize
  t z over z     s2 for    = (0, 1) (cid:23) 0.

  t z over z     s for any       k     0. (see    gure 2.26, left.) nor is it true that any
minimizer of   t z over z     s, with    (cid:23)k     0, is minimal (see    gure 2.26, right.)

example 2.27 pareto optimal production frontier. we consider a product which
requires n resources (such as labor, electricity, natural gas, water) to manufacture.
the product can be manufactured or produced in many ways. with each production
method, we associate a resource vector x     rn, where xi denotes the amount of
resource i consumed by the method to manufacture the product. we assume that xi    
0 (i.e., resources are consumed by the production methods) and that the resources
are valuable (so using less of any resource is preferred).
the production set p     rn is de   ned as the set of all resource vectors x that
correspond to some production method.

production methods with resource vectors that are minimal elements of p , with
respect to componentwise inequality, are called pareto optimal or e   cient. the set
of minimal elements of p is called the e   cient production frontier.

we can give a simple interpretation of pareto optimality. we say that one production
method, with resource vector x, is better than another, with resource vector y, if
xi     yi for all i, and for some i, xi < yi. in other words, one production method
is better than another if it uses no more of each resource than another method, and
for at least one resource, actually uses less. this corresponds to x (cid:22) y, x 6= y. then
we can say: a production method is pareto optimal or e   cient if there is no better
production method.

we can    nd pareto optimal production methods (i.e., minimal resource vectors) by
minimizing

  t x =   1x1 +        +   nxn

over the set p of production vectors, using any    that satis   es        0.
here the vector    has a simple interpretation:   i is the price of resource i. by
minimizing   t x over p we are    nding the overall cheapest production method (for
the resource prices   i). as long as the prices are positive, the resulting production
method is guaranteed to be e   cient.

these ideas are illustrated in    gure 2.27.

58

2 convex sets

fuel

x1

p

x2

x5

x4

  

x3

labor

figure 2.27 the production set p , for a product that requires labor and
fuel to produce, is shown shaded. the two dark curves show the e   cient
production frontier. the points x1, x2 and x3 are e   cient. the points x4
and x5 are not (since in particular, x2 corresponds to a production method
that uses no more fuel, and less labor). the point x1 is also the minimum
cost production method for the price vector    (which is positive). the point
x2 is e   cient, but cannot be found by minimizing the total cost   t x for any
price vector    (cid:23) 0.

bibliography

bibliography

59

minkowski is generally credited with the    rst systematic study of convex sets, and the
introduction of fundamental concepts such as supporting hyperplanes and the supporting
hyperplane theorem, the minkowski distance function (exercise 3.34), extreme points of
a convex set, and many others.

some well known early surveys are bonnesen and fenchel [bf48], eggleston [egg58], klee
[kle63], and valentine [val64]. more recent books devoted to the geometry of convex sets
include lay [lay82] and webster [web94]. klee [kle71], fenchel [fen83], tikhomorov
[tik90], and berger [ber90] give very readable overviews of the history of convexity and
its applications throughout mathematics.

linear inequalities and polyhedral sets are studied extensively in connection with the lin-
ear programming problem, for which we give references at the end of chapter 4. some
landmark publications in the history of linear inequalities and id135 are
motzkin [mot33], von neumann and morgenstern [vnm53], kantorovich [kan60], koop-
mans [koo51], and dantzig [dan63]. dantzig [dan63, chapter 2] includes an historical
survey of linear inequalities, up to around 1963.

generalized inequalities were introduced in nonlinear optimization during the 1960s (see
luenberger [lue69,   8.2] and isii [isi64]), and are used extensively in cone programming
(see the references in chapter 4). bellman and fan [bf63] is an early paper on sets of
generalized linear inequalities (with respect to the positive semide   nite cone).

for extensions and a proof of the separating hyperplane theorem we refer the reader
to rockafellar [roc70, part iii], and hiriart-urruty and lemar  echal [hul93, volume
1,   iii4]. dantzig [dan63, page 21] attributes the term theorem of the alternative to
von neumann and morgenstern [vnm53, page 138]. for more references on theorems of
alternatives, see chapter 5.

the terminology of example 2.27 (including pareto optimality, e   cient production, and
the price interpretation of   ) is discussed in detail by luenberger [lue95].

convex geometry plays a prominent role in the classical theory of moments (krein and
nudelman [kn77], karlin and studden [ks66]). a famous example is the duality between
the cone of nonnegative polynomials and the cone of power moments; see exercise 2.37.

60

2 convex sets

exercises

de   nition of convexity

2.1 let c     rn be a convex set, with x1, . . . , xk     c, and let   1, . . . ,   k     r satisfy   i     0,
  1 +        +   k = 1. show that   1x1 +        +   kxk     c. (the de   nition of convexity is that
this holds for k = 2; you must show it for arbitrary k.) hint. use induction on k.

2.2 show that a set is convex if and only if its intersection with any line is convex. show that

a set is a   ne if and only if its intersection with any line is a   ne.

2.3 midpoint convexity. a set c is midpoint convex if whenever two points a, b are in c, the
average or midpoint (a + b)/2 is in c. obviously a convex set is midpoint convex. it can
be proved that under mild conditions midpoint convexity implies convexity. as a simple
case, prove that if c is closed and midpoint convex, then c is convex.

2.4 show that the convex hull of a set s is the intersection of all convex sets that contain s.
(the same method can be used to show that the conic, or a   ne, or linear hull of a set s
is the intersection of all conic sets, or a   ne sets, or subspaces that contain s.)

examples

2.5 what is the distance between two parallel hyperplanes {x     rn | at x = b1} and {x    

rn | at x = b2}?

2.6 when does one halfspace contain another? give conditions under which

{x | at x     b}     {x |   at x       b}

(where a 6= 0,   a 6= 0). also    nd the conditions under which the two halfspaces are equal.
2.7 voronoi description of halfspace. let a and b be distinct points in rn. show that the set
of all points that are closer (in euclidean norm) to a than b, i.e., {x | kx   ak2     kx   bk2},
is a halfspace. describe it explicitly as an inequality of the form ct x     d. draw a picture.
2.8 which of the following sets s are polyhedra? if possible, express s in the form s =
{x | ax (cid:22) b, f x = g}.
(a) s = {y1a1 + y2a2 |     1     y1     1,     1     y2     1}, where a1, a2     rn.
i=1 xia2

i = b2}, where

a1, . . . , an     r and b1, b2     r.

(b) s = {x     rn | x (cid:23) 0, 1t x = 1, pn
(d) s = {x     rn | x (cid:23) 0, xt y     1 for all y with pn

(c) s = {x     rn | x (cid:23) 0, xt y     1 for all y with kyk2 = 1}.

i=1 xiai = b1, pn

i=1 |yi| = 1}.

2.9 voronoi sets and polyhedral decomposition. let x0, . . . , xk     rn. consider the set of

points that are closer (in euclidean norm) to x0 than the other xi, i.e.,
v = {x     rn | kx     x0k2     kx     xik2, i = 1, . . . , k}.

v is called the voronoi region around x0 with respect to x1, . . . , xk .

(a) show that v is a polyhedron. express v in the form v = {x | ax (cid:22) b}.
(b) conversely, given a polyhedron p with nonempty interior, show how to    nd x0, . . . , xk

so that the polyhedron is the voronoi region of x0 with respect to x1, . . . , xk .

(c) we can also consider the sets

vk = {x     rn | kx     xkk2     kx     xik2, i 6= k}.

the set vk consists of points in rn for which the closest point in the set {x0, . . . , xk}
is xk.

exercises

61

intersect at most along a boundary.

the sets v0, . . . , vk give a polyhedral decomposition of rn. more precisely, the sets
k=0 vk = rn, and int vi     int vj =     for i 6= j, i.e., vi and vj
i=1 pi = rn, and int pi    
int pj =     for i 6= j. can this polyhedral decomposition of rn be described as
the voronoi regions generated by an appropriate set of points?

vk are polyhedra,sk
suppose that p1, . . . , pm are polyhedra such that sm

2.10 solution set of a quadratic inequality. let c     rn be the solution set of a quadratic

inequality,

c = {x     rn | xt ax + bt x + c     0},

with a     sn, b     rn, and c     r.
(a) show that c is convex if a (cid:23) 0.
(b) show that the intersection of c and the hyperplane de   ned by gt x + h = 0 (where

g 6= 0) is convex if a +   ggt (cid:23) 0 for some        r.

are the converses of these statements true?

2.11 hyperbolic sets. show that the hyperbolic set {x     r2

generalization, show that {x     rn
0            1, then a  b1            a + (1       )b; see   3.1.9.

+ | qn

2.12 which of the following sets are convex?

i=1 xi     1} is convex. hint.

+ | x1x2     1} is convex. as a
if a, b     0 and

(a) a slab, i.e., a set of the form {x     rn |        at x       }.
(b) a rectangle, i.e., a set of the form {x     rn |   i     xi       i, i = 1, . . . , n}. a rectangle
(c) a wedge, i.e., {x     rn | at
(d) the set of points closer to a given point than a given set, i.e.,

is sometimes called a hyperrectangle when n > 2.

1 x     b1, at

2 x     b2}.

{x | kx     x0k2     kx     yk2 for all y     s}

where s     rn.

(e) the set of points closer to one set than another, i.e.,

where s, t     rn, and

{x | dist(x, s)     dist(x, t )},

dist(x, s) = inf{kx     zk2 | z     s}.

convex.

(f) [hul93, volume 1, page 93] the set {x | x + s2     s1}, where s1, s2     rn with s1
(g) the set of points whose distance to a does not exceed a    xed fraction    of the
distance to b, i.e., the set {x | kx     ak2       kx     bk2}. you can assume a 6= b and
0            1.

2.13 conic hull of outer products. consider the set of rank-k outer products, de   ned as

{xx t | x     rn  k, rank x = k}. describe its conic hull in simple terms.
2.14 expanded and restricted sets. let s     rn, and let k    k be a norm on rn.

(a) for a     0 we de   ne sa as {x | dist(x, s)     a}, where dist(x, s) = inf y   s kx     yk.
we refer to sa as s expanded or extended by a. show that if s is convex, then sa
is convex.

(b) for a     0 we de   ne s   a = {x | b(x, a)     s}, where b(x, a) is the ball (in the norm
k    k), centered at x, with radius a. we refer to s   a as s shrunk or restricted by a,
since s   a consists of all points that are at least a distance a from rn\s. show that
if s is convex, then s   a is convex.

62

2 convex sets

2.15 some sets of id203 distributions. let x be a real-valued random variable with
prob(x = ai) = pi, i = 1, . . . , n, where a1 < a2 <        < an. of course p     rn lies
in the standard id203 simplex p = {p | 1t p = 1, p (cid:23) 0}. which of the following
conditions are convex in p? (that is, for which of the following conditions is the set of
p     p that satisfy the condition convex?)
(a)        e f (x)       , where e f (x) is the expected value of f (x), i.e., e f (x) =

i=1 pif (ai). (the function f : r     r is given.)

pn

(b) prob(x >   )       .
(c) e|x3|        e|x|.
(d) e x2       .
(e) e x2       .
(f) var(x)       , where var(x) = e(x     e x)2 is the variance of x.
(g) var(x)       .
(h) quartile(x)       , where quartile(x) = inf{   | prob(x       )     0.25}.
(i) quartile(x)       .

operations that preserve convexity

2.16 show that if s1 and s2 are convex sets in rm+n, then so is their partial sum
s = {(x, y1 + y2) | x     rm, y1, y2     rn, (x, y1)     s1, (x, y2)     s2}.

2.17 image of polyhedral sets under perspective function. in this problem we study the image
of hyperplanes, halfspaces, and polyhedra under the perspective function p (x, t) = x/t,
with dom p = rn    r++. for each of the following sets c, give a simple description of

p (c) = {v/t | (v, t)     c, t > 0}.

(a) the polyhedron c = conv{(v1, t1), . . . , (vk , tk )} where vi     rn and ti > 0.
(b) the hyperplane c = {(v, t) | f t v + gt = h} (with f and g not both zero).
(c) the halfspace c = {(v, t) | f t v + gt     h} (with f and g not both zero).
(d) the polyhedron c = {(v, t) | f v + gt (cid:22) h}.

2.18 invertible linear-fractional functions. let f : rn     rn be the linear-fractional function

f (x) = (ax + b)/(ct x + d),

dom f = {x | ct x + d > 0}.

suppose the matrix

q =(cid:20) a b
d (cid:21)

ct

is nonsingular. show that f is invertible and that f    1 is a linear-fractional mapping.
give an explicit expression for f    1 and its domain in terms of a, b, c, and d. hint. it
may be easier to express f    1 in terms of q.

2.19 linear-fractional functions and convex sets. let f : rm     rn be the linear-fractional

function

dom f = {x | ct x + d > 0}.
in this problem we study the inverse image of a convex set c under f , i.e.,

f (x) = (ax + b)/(ct x + d),

f    1(c) = {x     dom f | f (x)     c}.

for each of the following sets c     rn, give a simple description of f    1(c).

exercises

63

(a) the halfspace c = {y | gt y     h} (with g 6= 0).
(b) the polyhedron c = {y | gy (cid:22) h}.
(c) the ellipsoid {y | yt p    1y     1} (where p     sn
(d) the solution set of a linear matrix inequality, c = {y | y1a1 +        + ynan (cid:22) b},

++).

where a1, . . . , an, b     sp.

separation theorems and supporting hyperplanes

2.20 strictly positive solution of linear equations. suppose a     rm  n, b     rm, with b     r(a).

show that there exists an x satisfying

x     0,

ax = b

if and only if there exists no    with
at    (cid:23) 0,

at    6= 0,

bt        0.

hint. first prove the following fact from id202: ct x = d for all x satisfying
ax = b if and only if there is a vector    such that c = at   , d = bt   .

2.21 the set of separating hyperplanes. suppose that c and d are disjoint subsets of rn.
consider the set of (a, b)     rn+1 for which at x     b for all x     c, and at x     b for all
x     d. show that this set is a convex cone (which is the singleton {0} if there is no
hyperplane that separates c and d).

2.22 finish the proof of the separating hyperplane theorem in   2.5.1: show that a separating
hyperplane exists for two disjoint convex sets c and d. you can use the result proved
in   2.5.1, i.e., that a separating hyperplane exists when there exist points in the two sets
whose distance is equal to the distance between the two sets.
hint. if c and d are disjoint convex sets, then the set {x     y | x     c, y     d} is convex
and does not contain the origin.

2.23 give an example of two closed convex sets that are disjoint but cannot be strictly sepa-

rated.

2.24 supporting hyperplanes.

(a) express the closed convex set {x     r2
+ | x1x2     1} as an intersection of halfspaces.
(b) let c = {x     rn | kxk        1}, the       -norm unit ball in rn, and let   x be a point

in the boundary of c. identify the supporting hyperplanes of c at   x explicitly.

2.25 inner and outer polyhedral approximations. let c     rn be a closed convex set, and
i (x   xi) = 0
i (x     xi)     0}. consider the

suppose that x1, . . . , xk are on the boundary of c. suppose that for each i, at
de   nes a supporting hyperplane for c at xi, i.e., c     {x | at
two polyhedra

pinner = conv{x1, . . . , xk},

pouter = {x | at

i (x     xi)     0, i = 1, . . . , k}.

show that pinner     c     pouter. draw a picture illustrating this.

2.26 support function. the support function of a set c     rn is de   ned as

sc (y) = sup{yt x | x     c}.

(we allow sc (y) to take on the value +   .) suppose that c and d are closed convex sets
in rn. show that c = d if and only if their support functions are equal.

2.27 converse supporting hyperplane theorem. suppose the set c is closed, has nonempty
interior, and has a supporting hyperplane at every point in its boundary. show that c is
convex.

64

2 convex sets

convex cones and generalized inequalities

2.28 positive semide   nite cone for n = 1, 2, 3. give an explicit description of the positive
+, in terms of the matrix coe   cients and ordinary inequalities, for

semide   nite cone sn
n = 1, 2, 3. to describe a general element of sn, for n = 1, 2, 3, use the notation

x1,

(cid:20) x1

x2

x2

x3 (cid:21) ,

" x1

x2
x3

x3
x5

x6 # .

x2
x4
x5

2.29 cones in r2. suppose k     r2 is a closed convex cone.

(a) give a simple description of k in terms of the polar coordinates of its elements

(b) give a simple description of k    , and draw a plot illustrating the relation between

(x = r(cos   , sin   ) with r     0).
k and k    .

(c) when is k pointed?

(d) when is k proper (hence, de   nes a generalized inequality)? draw a plot illustrating

what x (cid:22)k y means when k is proper.

2.30 properties of generalized inequalities. prove the properties of (nonstrict and strict) gen-

eralized inequalities listed in   2.4.1.

2.31 properties of dual cones. let k     be the dual cone of a convex cone k, as de   ned in (2.19).

prove the following.

(a) k     is indeed a convex cone.
2     k    
(b) k1     k2 implies k    
1 .
(c) k     is closed.
(d) the interior of k     is given by int k     = {y | yt x > 0 for all x     cl k}.
(e) if k has nonempty interior then k     is pointed.
(f) k        is the closure of k. (hence if k is closed, k        = k.)
(g) if the closure of k is pointed then k     has nonempty interior.

2.32 find the dual cone of {ax | x (cid:23) 0}, where a     rm  n.
2.33 the monotone nonnegative cone. we de   ne the monotone nonnegative cone as

km+ = {x     rn | x1     x2                xn     0}.

i.e., all nonnegative vectors with components sorted in nonincreasing order.

(a) show that km+ is a proper cone.
(b) find the dual cone k    

m+. hint. use the identity

nxi=1

xiyi = (x1     x2)y1 + (x2     x3)(y1 + y2) + (x3     x4)(y1 + y2 + y3) +       

+ (xn   1     xn)(y1 +        + yn   1) + xn(y1 +        + yn).
2.34 the lexicographic cone and ordering. the lexicographic cone is de   ned as

klex = {0}     {x     rn | x1 =        = xk = 0, xk+1 > 0, for some k, 0     k < n},

i.e., all vectors whose    rst nonzero coe   cient (if any) is positive.

(a) verify that klex is a cone, but not a proper cone.

exercises

65

(b) we de   ne the lexicographic ordering on rn as follows: x    lex y if and only if
y     x     klex. (since klex is not a proper cone, the lexicographic ordering is not a
generalized inequality.) show that the lexicographic ordering is a linear ordering:
for any x, y     rn, either x    lex y or y    lex x. therefore any set of vectors can be
sorted with respect to the lexicographic cone, which yields the familiar sorting used
in dictionaries.

(c) find k    

lex.

verify that the set of copositive matrices is a proper cone. find its dual cone.

2.35 copositive matrices. a matrix x     sn is called copositive if zt xz     0 for all z (cid:23) 0.
2.36 euclidean distance matrices. let x1, . . . , xn     rk. the matrix d     sn de   ned by dij =
kxi     xjk2
2 is called a euclidean distance matrix. it satis   es some obvious properties such
as dij = dji, dii = 0, dij     0, and (from the triangle inequality) d1/2
ij + d1/2
jk .
we now pose the question: when is a matrix d     sn a euclidean distance matrix (for
some points in rk, for some k)? a famous result answers this question: d     sn is a
euclidean distance matrix if and only if dii = 0 and xt dx     0 for all x with 1t x = 0.
(see   8.3.3.)
show that the set of euclidean distance matrices is a convex cone.

ik     d1/2

2.37 nonnegative polynomials and hankel lmis. let kpol be the set of (coe   cients of) non-

negative polynomials of degree 2k on r:

kpol = {x     r2k+1 | x1 + x2t + x3t2 +        + x2k+1t2k     0 for all t     r}.

(a) show that kpol is a proper cone.

(b) a basic result states that a polynomial of degree 2k is nonnegative on r if and only
if it can be expressed as the sum of squares of two polynomials of degree k or less.
in other words, x     kpol if and only if the polynomial

p(t) = x1 + x2t + x3t2 +        + x2k+1t2k

can be expressed as

p(t) = r(t)2 + s(t)2,

where r and s are polynomials of degree k.
use this result to show that

kpol =(x     r2k+1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

xi = xm+n=i+1

ymn for some y     sk+1

+ ) .

in other words, p(t) = x1 + x2t + x3t2 +        + x2k+1t2k is nonnegative if and only if
there exists a matrix y     sk+1

such that

+

x1 = y11
x2 = y12 + y21
x3 = y13 + y22 + y31

...

x2k+1 = yk+1,k+1.

(c) show that k    

pol = khan where

khan = {z     r2k+1 | h(z) (cid:23) 0}

66

and

h(z) =

                        

z1
z2
z3
...
zk
zk+1

z2
z3
z4
...
zk+1
zk+2

z3
z4
z5
...
zk+2
zk+3

      
      
      
. . .
      
      

zk
zk+1
zk+2
...

z2k   1

z2k

zk+1
zk+2
zk+4
...
z2k
z2k+1

2 convex sets

.

                        

(this is the hankel matrix with coe   cients z1, . . . , z2k+1.)

(d) let kmom be the conic hull of the set of all vectors of the form (1, t, t2, . . . , t2k),

where t     r. show that y     kmom if and only if y1     0 and

y = y1(1, e u, e u2, . . . , e u2k)

for some random variable u. in other words, the elements of kmom are nonnegative
multiples of the moment vectors of all possible distributions on r. show that kpol =
k    

mom.

(e) combining the results of (c) and (d), conclude that khan = cl kmom.

as an example illustrating the relation between kmom and khan, take k = 2 and
z = (1, 0, 0, 0, 1). show that z     khan, z 6    kmom. find an explicit sequence of
points in kmom which converge to z.

2.38 [roc70, pages 15, 61] convex cones constructed from sets.

(a) the barrier cone of a set c is de   ned as the set of all vectors y such that yt x is
bounded above over x     c. in other words, a nonzero vector y is in the barrier cone
if and only if it is the normal vector of a halfspace {x | yt x       } that contains c.
verify that the barrier cone is a convex cone (with no assumptions on c).

(b) the recession cone (also called asymptotic cone) of a set c is de   ned as the set of
all vectors y such that for each x     c, x     ty     c for all t     0. show that the
recession cone of a convex set is a convex cone. show that if c is nonempty, closed,
and convex, then the recession cone of c is the dual of the barrier cone.

(c) the normal cone of a set c at a boundary point x0 is the set of all vectors y such
that yt (x     x0)     0 for all x     c (i.e., the set of vectors that de   ne a supporting
hyperplane to c at x0). show that the normal cone is a convex cone (with no
assumptions on c). give a simple description of the normal cone of a polyhedron
{x | ax (cid:22) b} at a point in its boundary.

2.39 separation of cones. let k and   k be two convex cones whose interiors are nonempty and

disjoint. show that there is a nonzero y such that y     k    ,    y       k    .

chapter 3

convex functions

3.1 basic properties and examples

3.1.1 de   nition

a function f : rn     r is convex if dom f is a convex set and if for all x,
y     dom f , and    with 0            1, we have

f (  x + (1       )y)       f (x) + (1       )f (y).

(3.1)

geometrically, this inequality means that the line segment between (x, f (x)) and
(y, f (y)), which is the chord from x to y, lies above the graph of f (   gure 3.1).
a function f is strictly convex if strict inequality holds in (3.1) whenever x 6= y
and 0 <    < 1. we say f is concave if    f is convex, and strictly concave if    f is
strictly convex.
for an a   ne function we always have equality in (3.1), so all a   ne (and therefore
also linear) functions are both convex and concave. conversely, any function that
is convex and concave is a   ne.

a function is convex if and only if it is convex when restricted to any line that
intersects its domain. in other words f is convex if and only if for all x     dom f and

(x, f (x))

(y, f (y))

figure 3.1 graph of a convex function. the chord (i.e., line segment) be-
tween any two points on the graph lies above the graph.

68

3 convex functions

all v, the function g(t) = f (x + tv) is convex (on its domain, {t | x + tv     dom f}).
this property is very useful, since it allows us to check whether a function is convex
by restricting it to a line.

the analysis of convex functions is a well developed    eld, which we will not
pursue in any depth. one simple result, for example, is that a convex function is
continuous on the relative interior of its domain; it can have discontinuities only
on its relative boundary.

3.1.2 extended-value extensions

it is often convenient to extend a convex function to all of rn by de   ning its value
to be     outside its domain. if f is convex we de   ne its extended-value extension
  f : rn     r     {   } by

  f (x) =(cid:26) f (x) x     dom f

    x 6    dom f.

the extension   f is de   ned on all rn, and takes values in r   {   }. we can recover
the domain of the original function f from the extension   f as dom f = {x |   f (x) <
   }.
the extension can simplify notation, since we do not need to explicitly describe
the domain, or add the quali   er    for all x     dom f     every time we refer to f (x).
consider, for example, the basic de   ning inequality (3.1). in terms of the extension
  f , we can express it as: for 0 <    < 1,

  f (  x + (1       )y)          f (x) + (1       )   f (y)

for any x and y. (for    = 0 or    = 1 the inequality always holds.) of course here we
must interpret the inequality using extended arithmetic and ordering. for x and y
both in dom f , this inequality coincides with (3.1); if either is outside dom f , then
the righthand side is    , and the inequality therefore holds. as another example
of this notational device, suppose f1 and f2 are two convex functions on rn. the
pointwise sum f = f1 + f2 is the function with domain dom f = dom f1     dom f2,
with f (x) = f1(x) + f2(x) for any x     dom f . using extended-value extensions we
can simply say that for any x,   f (x) =   f1(x) +   f2(x). in this equation the domain
of f has been automatically de   ned as dom f = dom f1     dom f2, since   f (x) =    
whenever x 6    dom f1 or x 6    dom f2. in this example we are relying on extended
arithmetic to automatically de   ne the domain.
in this book we will use the same symbol to denote a convex function and its
extension, whenever there is no harm from the ambiguity. this is the same as
assuming that all convex functions are implicitly extended, i.e., are de   ned as    
outside their domains.

example 3.1 indicator function of a convex set. let c     rn be a convex set, and
consider the (convex) function ic with domain c and ic (x) = 0 for all x     c. in
other words, the function is identically zero on the set c. its extended-value extension

3.1 basic properties and examples

69

f (y)

f (x) +    f (x)t (y     x)

(x, f (x))

figure 3.2 if f is convex and di   erentiable, then f (x)+   f (x)t (y   x)     f (y)
for all x, y     dom f .

is given by

  ic (x) =(cid:26) 0

x     c
    x 6    c.

the convex function   ic is called the indicator function of the set c.
we can play several notational tricks with the indicator function   ic . for example
the problem of minimizing a function f (de   ned on all of rn, say) on the set c is the
same as minimizing the function f +   ic over all of rn. indeed, the function f +   ic
is (by our convention) f restricted to the set c.

in a similar way we can extend a concave function by de   ning it to be       

outside its domain.

3.1.3 first-order conditions

suppose f is di   erentiable (i.e., its gradient    f exists at each point in dom f ,
which is open). then f is convex if and only if dom f is convex and

f (y)     f (x) +    f (x)t (y     x)

(3.2)

holds for all x, y     dom f . this inequality is illustrated in    gure 3.2.
the a   ne function of y given by f (x)+   f (x)t (y   x) is, of course, the    rst-order
taylor approximation of f near x. the inequality (3.2) states that for a convex
function, the    rst-order taylor approximation is in fact a global underestimator of
the function. conversely, if the    rst-order taylor approximation of a function is
always a global underestimator of the function, then the function is convex.

the inequality (3.2) shows that from local information about a convex function
(i.e., its value and derivative at a point) we can derive global information (i.e., a
global underestimator of it). this is perhaps the most important property of convex
functions, and explains some of the remarkable properties of convex functions and
id76 problems. as one simple example, the inequality (3.2) shows
that if    f (x) = 0, then for all y     dom f , f (y)     f (x), i.e., x is a global minimizer
of the function f .

70

3 convex functions

strict convexity can also be characterized by a    rst-order condition: f is strictly

convex if and only if dom f is convex and for x, y     dom f , x 6= y, we have

f (y) > f (x) +    f (x)t (y     x).

(3.3)

for concave functions we have the corresponding characterization: f is concave

if and only if dom f is convex and

f (y)     f (x) +    f (x)t (y     x)

for all x, y     dom f .
proof of    rst-order convexity condition

to prove (3.2), we    rst consider the case n = 1: we show that a di   erentiable
function f : r     r is convex if and only if

f (y)     f (x) + f    (x)(y     x)

(3.4)

for all x and y in dom f .

assume    rst that f is convex and x, y     dom f . since dom f is convex (i.e.,
an interval), we conclude that for all 0 < t     1, x + t(y     x)     dom f , and by
convexity of f ,

if we divide both sides by t, we obtain

f (x + t(y     x))     (1     t)f (x) + tf (y).

f (y)     f (x) +

f (x + t(y     x))     f (x)

t

,

and taking the limit as t     0 yields (3.4).
to show su   ciency, assume the function satis   es (3.4) for all x and y in dom f
(which is an interval). choose any x 6= y, and 0            1, and let z =   x + (1      )y.
applying (3.4) twice yields

f (x)     f (z) + f    (z)(x     z),

f (y)     f (z) + f    (z)(y     z).

multiplying the    rst inequality by   , the second by 1       , and adding them yields

  f (x) + (1       )f (y)     f (z),

which proves that f is convex.

now we can prove the general case, with f : rn     r. let x, y     rn and
consider f restricted to the line passing through them, i.e., the function de   ned by
g(t) = f (ty + (1     t)x), so g   (t) =    f (ty + (1     t)x)t (y     x).
we have g(1)     g(0) + g   (0), which means

first assume f is convex, which implies g is convex, so by the argument above

f (y)     f (x) +    f (x)t (y     x).

now assume that this inequality holds for any x and y, so if ty + (1     t)x     dom f
and   ty + (1       t)x     dom f , we have

f (ty + (1     t)x)     f (  ty + (1       t)x) +    f (  ty + (1       t)x)t (y     x)(t       t),
i.e., g(t)     g(  t) + g   (  t)(t       t). we have seen that this implies that g is convex.

3.1 basic properties and examples

71

3.1.4 second-order conditions

we now assume that f is twice di   erentiable, that is, its hessian or second deriva-
tive    2f exists at each point in dom f , which is open. then f is convex if and
only if dom f is convex and its hessian is positive semide   nite: for all x     dom f ,

   2f (x) (cid:23) 0.

for a function on r, this reduces to the simple condition f       (x)     0 (and dom f
convex, i.e., an interval), which means that the derivative is nondecreasing. the
condition    2f (x) (cid:23) 0 can be interpreted geometrically as the requirement that the
graph of the function have positive (upward) curvature at x. we leave the proof
of the second-order condition as an exercise (exercise 3.8).

similarly, f is concave if and only if dom f is convex and    2f (x) (cid:22) 0 for
all x     dom f . strict convexity can be partially characterized by second-order
if    2f (x)     0 for all x     dom f , then f is strictly convex. the
conditions.
for example, the function f : r     r given by
converse, however, is not true:
f (x) = x4 is strictly convex but has zero second derivative at x = 0.

example 3.2 quadratic functions. consider the quadratic function f : rn     r, with
dom f = rn, given by

f (x) = (1/2)xt p x + qt x + r,

with p     sn, q     rn, and r     r. since    2f (x) = p for all x, f is convex if and only
if p (cid:23) 0 (and concave if and only if p (cid:22) 0).
for quadratic functions, strict convexity is easily characterized: f is strictly convex
if and only if p     0 (and strictly concave if and only if p     0).

remark 3.1 the separate requirement that dom f be convex cannot be dropped from
the    rst- or second-order characterizations of convexity and concavity. for example,
the function f (x) = 1/x2, with dom f = {x     r | x 6= 0}, satis   es f       (x) > 0 for all
x     dom f , but is not a convex function.

3.1.5 examples

we have already mentioned that all linear and a   ne functions are convex (and
concave), and have described the convex and concave quadratic functions. in this
section we give a few more examples of convex and concave functions. we start
with some functions on r, with variable x.

    exponential. eax is convex on r, for any a     r.
    powers. xa is convex on r++ when a     1 or a     0, and concave for 0     a     1.
    powers of absolute value. |x|p, for p     1, is convex on r.
    logarithm. log x is concave on r++.

72

3 convex functions

)
y
,

x
(
f

2

1

0
2

2

1

y

0

x

0

   2

figure 3.3 graph of f (x, y) = x2/y.

    negative id178. x log x (either on r++, or on r+, de   ned as 0 for x = 0)

is convex.

convexity or concavity of these examples can be shown by verifying the ba-
sic inequality (3.1), or by checking that the second derivative is nonnegative or
nonpositive. for example, with f (x) = x log x we have

f    (x) = log x + 1,

f       (x) = 1/x,

so that f       (x) > 0 for x > 0. this shows that the negative id178 function is
(strictly) convex.

we now give a few interesting examples of functions on rn.
    norms. every norm on rn is convex.
    max function. f (x) = max{x1, . . . , xn} is convex on rn.
    quadratic-over-linear function. the function f (x, y) = x2/y, with

dom f = r    r++ = {(x, y)     r2 | y > 0},

is convex (   gure 3.3).

    log-sum-exp. the function f (x) = log (ex1 +        + exn ) is convex on rn.
this function can be interpreted as a di   erentiable (in fact, analytic) approx-
imation of the max function, since

max{x1, . . . , xn}     f (x)     max{x1, . . . , xn} + log n

for all x. (the second inequality is tight when all components of x are equal.)
figure 3.4 shows f for n = 2.

3.1 basic properties and examples

73

4

2

0

)
y
,

x
(
f

   2

2

0

y

   2

2

0

   2

x

figure 3.4 graph of f (x, y) = log(ex + ey).

    geometric mean. the geometric mean f (x) = (qn

dom f = rn

++.

i=1 xi)1/n is concave on

    log-determinant. the function f (x) = log det x is concave on dom f =

sn

++.

convexity (or concavity) of these examples can be veri   ed in several ways,
such as directly verifying the inequality (3.1), verifying that the hessian is positive
semide   nite, or restricting the function to an arbitrary line and verifying convexity
of the resulting function of one variable.

norms.

if f : rn     r is a norm, and 0            1, then
f (  x + (1       )y)     f (  x) + f ((1       )y) =   f (x) + (1       )f (y).

the inequality follows from the triangle inequality, and the equality follows from
homogeneity of a norm.

max function. the function f (x) = maxi xi satis   es, for 0            1,

f (  x + (1       )y) = max
       max
=   f (x) + (1       )f (y).

(  xi + (1       )yi)
xi + (1       ) max

i

i

i

yi

quadratic-over-linear function. to show that the quadratic-over-linear function
f (x, y) = x2/y is convex, we note that (for y > 0),

   2f (x, y) =

2

y3(cid:20) y2    xy

x2 (cid:21) =

   xy

2

y3(cid:20) y

   x (cid:21)(cid:20) y

   x (cid:21)t

(cid:23) 0.

74

3 convex functions

log-sum-exp. the hessian of the log-sum-exp function is

where z = (ex1 , . . . , exn ). to verify that    2f (x) (cid:23) 0 we must show that for all v,
vt   2f (x)v     0, i.e.,

   2f (x) =

1

(1t z)2(cid:0)(1t z) diag(z)     zzt(cid:1) ,
i zi!      nxi=1

zi!  nxi=1

v2

1

(1t z)2        nxi=1

vizi!2           0.

but this follows from the cauchy-schwarz inequality (at a)(bt b)     (at b)2 applied
to the vectors with components ai = vi   zi, bi =    zi.

vt   2f (x)v =

(qn

geometric mean.

in a similar way we can show that the geometric mean f (x) =

i=1 xi)1/n is concave on dom f = rn

++. its hessian    2f (x) is given by

i=1 xi)1/n
n2x2
k

,

   2f (x)
   xk   xl

=

(qn

i=1 xi)1/n
n2xkxl

for k 6= l,

   2f (x)

   x2
k

=    (n     1)
and can be expressed as

(qn
   2f (x) =    qn
vt   2f (x)v =    qn

1, . . . , 1/x2
where qi = 1/xi. we must show that    2f (x) (cid:22) 0, i.e., that

i

i=1 x1/n
n2

i=1 x1/n
n2

i

(cid:0)n diag(1/x2
      n
nxi=1

v2
i /x2

i      nxi=1

n)     qqt(cid:1)
vi/xi!2           0

for all v. again this follows from the cauchy-schwarz inequality (at a)(bt b)    
(at b)2, applied to the vectors a = 1 and bi = vi/xi.

log-determinant. for the function f (x) = log det x, we can verify concavity by
considering an arbitrary line, given by x = z + tv , where z, v     sn. we de   ne
g(t) = f (z + tv ), and restrict g to the interval of values of t for which z + tv     0.
without loss of generality, we can assume that t = 0 is inside this interval, i.e.,
z     0. we have

g(t) = log det(z + tv )

= log det(z 1/2(i + tz    1/2v z    1/2)z 1/2)

=

log(1 + t  i) + log det z

where   1, . . . ,   n are the eigenvalues of z    1/2v z    1/2. therefore we have

g   (t) =

  i

1 + t  i

,

g      (t) =    

  2
i

(1 + t  i)2 .

nxi=1

since g      (t)     0, we conclude that f is concave.

nxi=1
nxi=1

3.1 basic properties and examples

75

3.1.6 sublevel sets

the   -sublevel set of a function f : rn     r is de   ned as
c   = {x     dom f | f (x)       }.

sublevel sets of a convex function are convex, for any value of   . the proof is
if x, y     c  , then f (x)        and
immediate from the de   nition of convexity:
f (y)       , and so f (  x + (1      )y)        for 0            1, and hence   x + (1      )y     c  .
the converse is not true: a function can have all its sublevel sets convex, but
not be a convex function. for example, f (x) =    ex is not convex on r (indeed, it
is strictly concave) but all its sublevel sets are convex.
if f is concave, then its   -superlevel set, given by {x     dom f | f (x)       }, is a
convex set. the sublevel set property is often a good way to establish convexity of
a set, by expressing it as a sublevel set of a convex function, or as the superlevel
set of a concave function.

example 3.3 the geometric and arithmetic means of x     rn

+ are, respectively,

g(x) =  nyi=1

xi!1/n

,

a(x) =

1
n

xi,

nxi=1

(where we take 01/n = 0 in our de   nition of g). the arithmetic-geometric mean
inequality states that g(x)     a(x).
suppose 0            1, and consider the set

{x     rn

+ | g(x)       a(x)},

i.e., the set of vectors with geometric mean at least as large as a factor    times the
arithmetic mean. this set is convex, since it is the 0-superlevel set of the function
g(x)       a(x), which is concave. in fact, the set is positively homogeneous, so it is a
convex cone.

3.1.7 epigraph

the graph of a function f : rn     r is de   ned as

which is a subset of rn+1. the epigraph of a function f : rn     r is de   ned as

{(x, f (x)) | x     dom f},

epi f = {(x, t) | x     dom f, f (x)     t},

which is a subset of rn+1.
graph   .) the de   nition is illustrated in    gure 3.5.

(   epi    means    above    so epigraph means    above the

the link between convex sets and convex functions is via the epigraph: a
function is convex if and only if its epigraph is a convex set. a function is concave
if and only if its hypograph, de   ned as

is a convex set.

hypo f = {(x, t) | t     f (x)},

76

3 convex functions

epi f

f

figure 3.5 epigraph of a function f , shown shaded. the lower boundary,
shown darker, is the graph of f .

example 3.4 matrix fractional function. the function f : rn    sn     r, de   ned as

f (x, y ) = xt y    1x

++. (this generalizes the quadratic-over-linear function

is convex on dom f = rn  sn
f (x, y) = x2/y, with dom f = r    r++.)
one easy way to establish convexity of f is via its epigraph:
epi f = {(x, y, t) | y     0, xt y    1x     t}

= (cid:26)(x, y, t) (cid:12)(cid:12)(cid:12)(cid:12)(cid:20) y

xt

x

t (cid:21) (cid:23) 0, y     0(cid:27) ,

using the schur complement condition for positive semide   niteness of a block matrix
(see   a.5.5). the last condition is a linear matrix inequality in (x, y, t), and therefore
epi f is convex.

for the special case n = 1, the matrix fractional function reduces to the quadratic-
over-linear function x2/y, and the associated lmi representation is

(cid:20) y

x

x

t (cid:21) (cid:23) 0,

y > 0

(the graph of which is shown in    gure 3.3).

many results for convex functions can be proved (or interpreted) geometrically
using epigraphs, and applying results for convex sets. as an example, consider the
   rst-order condition for convexity:

f (y)     f (x) +    f (x)t (y     x),

where f is convex and x, y     dom f . we can interpret this basic inequality
geometrically in terms of epi f . if (y, t)     epi f , then

t     f (y)     f (x) +    f (x)t (y     x).

3.1 basic properties and examples

77

epi f

(x, f (x))

(   f (x),   1)

figure 3.6 for a di   erentiable convex function f , the vector (   f (x),    1)
de   nes a supporting hyperplane to the epigraph of f at x.

we can express this as:

(y, t)     epi f =    (cid:20)    f (x)

   1

(cid:21)t(cid:18)(cid:20) y

t (cid:21)    (cid:20)

x

f (x) (cid:21)(cid:19)     0.

this means that the hyperplane de   ned by (   f (x),   1) supports epi f at the
boundary point (x, f (x)); see    gure 3.6.

3.1.8 jensen   s inequality and extensions

the basic inequality (3.1), i.e.,

f (  x + (1       )y)       f (x) + (1       )f (y),

is sometimes called jensen   s inequality. it is easily extended to convex combinations
of more than two points: if f is convex, x1, . . . , xk     dom f , and   1, . . . ,   k     0
with   1 +        +   k = 1, then

f (  1x1 +        +   kxk)       1f (x1) +        +   kf (xk).

as in the case of convex sets, the inequality extends to in   nite sums, integrals, and

expected values. for example, if p(x)     0 on s     dom f ,rs p(x) dx = 1, then

f(cid:18)zs

p(x)x dx(cid:19)    zs

f (x)p(x) dx,

provided the integrals exist. in the most general case we can take any id203
measure with support in dom f . if x is a random variable such that x     dom f
with id203 one, and f is convex, then we have

f (e x)     e f (x),

(3.5)

provided the expectations exist. we can recover the basic inequality (3.1) from
this general form, by taking the random variable x to have support {x1, x2}, with

78

3 convex functions

prob(x = x1) =   , prob(x = x2) = 1       . thus the inequality (3.5) characterizes
convexity: if f is not convex, there is a random variable x, with x     dom f with
id203 one, such that f (e x) > e f (x).
all of these inequalities are now called jensen   s inequality, even though the

inequality studied by jensen was the very simple one

f(cid:18) x + y

2 (cid:19)    

f (x) + f (y)

2

.

remark 3.2 we can interpret (3.5) as follows. suppose x     dom f     rn and z is
any zero mean random vector in rn. then we have
e f (x + z)     f (x).

thus, randomization or dithering (i.e., adding a zero mean random vector to the
argument) cannot decrease the value of a convex function on average.

3.1.9 inequalities

many famous inequalities can be derived by applying jensen   s inequality to some
appropriate convex function. (indeed, convexity and jensen   s inequality can be
made the foundation of a theory of inequalities.) as a simple example, consider
the arithmetic-geometric mean inequality:

(3.6)
for a, b     0. the function     log x is convex; jensen   s inequality with    = 1/2 yields

   ab     (a + b)/2
2 (cid:19)         log a     log b

2

.

    log(cid:18) a + b

taking the exponential of both sides yields (3.6).

as a less trivial example we prove h  older   s inequality: for p > 1, 1/p + 1/q = 1,

and x, y     rn,

xiyi      nxi=1

|xi|p!1/p  nxi=1

nxi=1

|yi|q!1/q

.

by convexity of     log x, and jensen   s inequality with general   , we obtain the more
general arithmetic-geometric mean inequality

a =

a  b1            a + (1       )b,
valid for a, b     0 and 0            1. applying this with
|yi|q
|xi|p
pn
pn
j=1 |xj|p ,
j=1 |yj|q ,
  |xi|p
j=1 |xj|p!1/p  |yi|q
j=1 |yj|q!1/q
pn
pn

summing over i then yields h  older   s inequality.

yields

b =

   

   = 1/p,

|xi|p
ppn
j=1 |xj|p +

|yi|q
qpn
j=1 |yj|q .

3.2 operations that preserve convexity

79

3.2 operations that preserve convexity

in this section we describe some operations that preserve convexity or concavity
of functions, or allow us to construct new convex and concave functions. we start
with some simple operations such as addition, scaling, and pointwise supremum,
and then describe some more sophisticated operations (some of which include the
simple operations as special cases).

3.2.1 nonnegative weighted sums

evidently if f is a convex function and        0, then the function   f is convex.
if f1 and f2 are both convex functions, then so is their sum f1 + f2. combining
nonnegative scaling and addition, we see that the set of convex functions is itself a
convex cone: a nonnegative weighted sum of convex functions,

f = w1f1 +        + wmfm,

is convex. similarly, a nonnegative weighted sum of concave functions is concave. a
nonnegative, nonzero weighted sum of strictly convex (concave) functions is strictly
convex (concave).

these properties extend to in   nite sums and integrals. for example if f (x, y)
is convex in x for each y     a, and w(y)     0 for each y     a, then the function g
de   ned as

g(x) =za

w(y)f (x, y) dy

is convex in x (provided the integral exists).

the fact that convexity is preserved under nonnegative scaling and addition is
easily veri   ed directly, or can be seen in terms of the associated epigraphs. for
example, if w     0 and f is convex, we have

epi(wf ) =(cid:20) i

0 w (cid:21) epi f,

0

which is convex because the image of a convex set under a linear mapping is convex.

3.2.2 composition with an a   ne mapping

suppose f : rn     r, a     rn  m, and b     rn. de   ne g : rm     r by

g(x) = f (ax + b),

with dom g = {x | ax + b     dom f}. then if f is convex, so is g; if f is concave,
so is g.

80

3 convex functions

3.2.3 pointwise maximum and supremum

if f1 and f2 are convex functions then their pointwise maximum f , de   ned by

f (x) = max{f1(x), f2(x)},

with dom f = dom f1     dom f2, is also convex. this property is easily veri   ed: if
0            1 and x, y     dom f , then

f (  x + (1       )y) = max{f1(  x + (1       )y), f2(  x + (1       )y)}

    max{  f1(x) + (1       )f1(y),   f2(x) + (1       )f2(y)}
       max{f1(x), f2(x)} + (1       ) max{f1(y), f2(y)}
=   f (x) + (1       )f (y),

which establishes convexity of f . it is easily shown that if f1, . . . , fm are convex,
then their pointwise maximum

is also convex.

f (x) = max{f1(x), . . . , fm(x)}

example 3.5 piecewise-linear functions. the function

f (x) = max{at

1 x + b1, . . . , at

lx + bl}

de   nes a piecewise-linear (or really, a   ne) function (with l or fewer regions). it is
convex since it is the pointwise maximum of a   ne functions.

the converse can also be shown: any piecewise-linear convex function with l or fewer
regions can be expressed in this form. (see exercise 3.29.)

example 3.6 sum of r largest components. for x     rn we denote by x[i] the ith
largest component of x, i.e.,

are the components of x sorted in nonincreasing order. then the function

x[1]     x[2]                x[n]

f (x) =

x[i],

rxi=1

i.e., the sum of the r largest elements of x, is a convex function. this can be seen by
writing it as

f (x) =

rxi=1

x[i] = max{xi1 +        + xir | 1     i1 < i2 <        < ir     n},

i.e., the maximum of all possible sums of r di   erent components of x. since it is the
pointwise maximum of n!/(r!(n     r)!) linear functions, it is convex.

as an extension it can be shown that the functionpr

w1     w2                wr     0. (see exercise 3.19.)

i=1 wix[i] is convex, provided

3.2 operations that preserve convexity

81

the pointwise maximum property extends to the pointwise supremum over an
in   nite set of convex functions. if for each y     a, f (x, y) is convex in x, then the
function g, de   ned as
(3.7)

f (x, y)

g(x) = sup
y   a

is convex in x. here the domain of g is

dom g = {x | (x, y)     dom f for all y     a, sup

y   a

f (x, y) <    }.

similarly, the pointwise in   mum of a set of concave functions is a concave function.
in terms of epigraphs, the pointwise supremum of functions corresponds to the

intersection of epigraphs: with f , g, and a as de   ned in (3.7), we have

epi g = \y   a

epi f (  , y).

thus, the result follows from the fact that the intersection of a family of convex
sets is convex.

example 3.7 support function of a set. let c     rn, with c 6=    . the support
function sc associated with the set c is de   ned as

sc (x) = sup{xt y | y     c}

(and, naturally, dom sc = {x | supy   c xt y <    }).
for each y     c, xt y is a linear function of x, so sc is the pointwise supremum of a
family of linear functions, hence convex.

example 3.8 distance to farthest point of a set. let c     rn. the distance (in any
norm) to the farthest point of c,

f (x) = sup

y   c kx     yk,

is convex. to see this, note that for any y, the function kx     yk is convex in x. since
f is the pointwise supremum of a family of convex functions (indexed by y     c), it
is a convex function of x.

example 3.9 least-squares cost as a function of weights. let a1, . . . , an     rm. in a
i x    
bi)2 over x     rm. we refer to wi as weights, and allow negative wi (which opens the
possibility that the objective function is unbounded below).

weighted least-squares problem we minimize the objective functionpn

i=1 wi(at

we de   ne the (optimal) weighted least-squares cost as

g(w) = inf
x

wi(at

i x     bi)2,

with domain

dom g =(w (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

inf
x

wi(at

i x     bi)2 >       ) .

nxi=1
nxi=1

82

3 convex functions

since g is the in   mum of a family of linear functions of w (indexed by x     rm), it is
a concave function of w.

we can derive an explicit expression for g, at least on part of its domain. let
w = diag(w), the diagonal matrix with elements w1, . . . , wn, and let a     rn  m
have rows at

i , so we have

g(w) = inf
x

(ax     b)t w (ax     b) = inf

x

(xt at w ax     2bt w ax + bt w b).

from this we see that if at w a 6(cid:23) 0, the quadratic function is unbounded below
in x, so g(w) =       , i.e., w 6    dom g. we can give a simple expression for g
when at w a     0 (which de   nes a strict linear matrix inequality), by analytically
minimizing the quadratic function:

g(w) = bt w b     bt w a(at w a)   1at w b

=

wib2

i    

nxi=1

nxi=1

w2

i b2

i at

wjajat

ai.

i   nxj=1

j!   1

concavity of g from this expression is not immediately obvious (but does follow, for
example, from convexity of the matrix fractional function; see example 3.4).

example 3.10 maximum eigenvalue of a symmetric matrix. the function f (x) =
  max(x), with dom f = sm, is convex. to see this, we express f as

f (x) = sup{yt xy | kyk2 = 1},

i.e., as the pointwise supremum of a family of linear functions of x (i.e., yt xy)
indexed by y     rm.

example 3.11 norm of a matrix. consider f (x) = kxk2 with dom f = rp  q,
where k    k2 denotes the spectral norm or maximum singular value. convexity of f
follows from

f (x) = sup{ut xv | kuk2 = 1, kvk2 = 1},

which shows it is the pointwise supremum of a family of linear functions of x.
as a generalization suppose k    ka and k    kb are norms on rp and rq, respectively.
the induced norm of a matrix x     rp  q is de   ned as
.

kxka,b = sup

v6=0

kxvka
kvkb

(this reduces to the spectral norm when both norms are euclidean.) the induced
norm can be expressed as

kxka,b = sup{kxvka | kvkb = 1}

= sup{ut xv | kuka    = 1, kvkb = 1},
where k    ka    is the dual norm of k    ka, and we use the fact that

kzka = sup{ut z | kuka    = 1}.

since we have expressed kxka,b as a supremum of linear functions of x, it is a convex
function.

3.2 operations that preserve convexity

83

representation as pointwise supremum of a   ne functions

the examples above illustrate a good method for establishing convexity of a func-
tion: by expressing it as the pointwise supremum of a family of a   ne functions.
except for a technical condition, a converse holds: almost every convex function
can be expressed as the pointwise supremum of a family of a   ne functions. for
example, if f : rn     r is convex, with dom f = rn, then we have
f (x) = sup{g(x) | g a   ne, g(z)     f (z) for all z}.

in other words, f is the pointwise supremum of the set of all a   ne global under-
estimators of it. we give the proof of this result below, and leave the case where
dom f 6= rn as an exercise (exercise 3.28).

suppose f is convex with dom f = rn. the inequality

f (x)     sup{g(x) | g a   ne, g(z)     f (z) for all z}

is clear, since if g is any a   ne underestimator of f , we have g(x)     f (x). to
establish equality, we will show that for each x     rn, there is an a   ne function g,
which is a global underestimator of f , and satis   es g(x) = f (x).
the epigraph of f is, of course, a convex set. hence we can    nd a supporting

hyperplane to it at (x, f (x)), i.e., a     rn and b     r with (a, b) 6= 0 and

b (cid:21)t(cid:20) x     z
(cid:20) a

f (x)     t (cid:21)     0

for all (z, t)     epi f . this means that

at (x     z) + b(f (x)     f (z)     s)     0

(3.8)

for all z     dom f = rn and all s     0 (since (z, t)     epi f means t = f (z) + s for
some s     0). for the inequality (3.8) to hold for all s     0, we must have b     0.
if b = 0, then the inequality (3.8) reduces to at (x     z)     0 for all z     rn, which
implies a = 0 and contradicts (a, b) 6= 0. we conclude that b > 0, i.e., that the
supporting hyperplane is not vertical.

using the fact that b > 0 we rewrite (3.8) for s = 0 as

g(z) = f (x) + (a/b)t (x     z)     f (z)

for all z. the function g is an a   ne underestimator of f , and satis   es g(x) = f (x).

3.2.4 composition

in this section we examine conditions on h : rk     r and g : rn     rk that
guarantee convexity or concavity of their composition f = h    g : rn     r, de   ned
by

f (x) = h(g(x)),

dom f = {x     dom g | g(x)     dom h}.

84

3 convex functions

scalar composition
we    rst consider the case k = 1, so h : r     r and g : rn     r. we can restrict
ourselves to the case n = 1 (since convexity is determined by the behavior of a
function on arbitrary lines that intersect its domain).

to discover the composition rules, we start by assuming that h and g are twice
di   erentiable, with dom g = dom h = r. in this case, convexity of f reduces to
f            0 (meaning, f       (x)     0 for all x     r).

the second derivative of the composition function f = h     g is given by

f       (x) = h      (g(x))g   (x)2 + h   (g(x))g      (x).

(3.9)

now suppose, for example, that g is convex (so g           0) and h is convex and
nondecreasing (so h           0 and h        0). it follows from (3.9) that f            0, i.e., f is
convex. in a similar way, the expression (3.9) gives the results:

f is convex if h is convex and nondecreasing, and g is convex,

f is convex if h is convex and nonincreasing, and g is concave,
f is concave if h is concave and nondecreasing, and g is concave,

(3.10)

f is concave if h is concave and nonincreasing, and g is convex.

these statements are valid when the functions g and h are twice di   erentiable and
have domains that are all of r. it turns out that very similar composition rules
hold in the general case n > 1, without assuming di   erentiability of h and g, or
that dom g = rn and dom h = r:

f is convex if h is convex,   h is nondecreasing, and g is convex,
f is convex if h is convex,   h is nonincreasing, and g is concave,
f is concave if h is concave,   h is nondecreasing, and g is concave,
f is concave if h is concave,   h is nonincreasing, and g is convex.

(3.11)

here   h denotes the extended-value extension of the function h, which assigns the
value     (      ) to points not in dom h for h convex (concave). the only di   erence
between these results, and the results in (3.10), is that we require that the extended-
value extension function   h be nonincreasing or nondecreasing, on all of r.

to understand what this means, suppose h is convex, so   h takes on the value    
outside dom h. to say that   h is nondecreasing means that for any x, y     r, with
x < y, we have   h(x)       h(y). in particular, this means that if y     dom h, then x    
dom h. in other words, the domain of h extends in   nitely in the negative direction;
it is either r, or an interval of the form (      , a) or (      , a]. in a similar way, to
say that h is convex and   h is nonincreasing means that h is nonincreasing and
dom h extends in   nitely in the positive direction. this is illustrated in    gure 3.7.

example 3.12 some simple examples will illustrate the conditions on h that appear
in the composition theorems.

    the function h(x) = log x, with dom h = r++, is concave and satis   es   h

nondecreasing.

3.2 operations that preserve convexity

85

1

0

epi f

epi f

1

0

x

0

1

0

x

1

figure 3.7 left. the function x2, with domain r+, is convex and nonde-
creasing on its domain, but its extended-value extension is not nondecreas-
ing. right. the function max{x, 0}2, with domain r, is convex, and its
extended-value extension is nondecreasing.

    the function h(x) = x1/2, with dom h = r+, is concave and satis   es the

condition   h nondecreasing.

    the function h(x) = x3/2, with dom h = r+, is convex but does not satisfy the

condition   h nondecreasing. for example, we have   h(   1) =    , but   h(1) = 1.
is convex and does satisfy the condition   h nondecreasing.

    the function h(x) = x3/2 for x     0, and h(x) = 0 for x < 0, with dom h = r,

the composition results (3.11) can be proved directly, without assuming dif-
ferentiability, or using the formula (3.9). as an example, we will prove the fol-
if g is convex, h is convex, and   h is nondecreasing,
lowing composition theorem:
then f = h     g is convex. assume that x, y     dom f , and 0            1. since
x, y     dom f , we have that x, y     dom g and g(x), g(y)     dom h. since dom g
is convex, we conclude that   x + (1       )y     dom g, and from convexity of g, we
have
(3.12)
since g(x), g(y)     dom h, we conclude that   g(x) + (1       )g(y)     dom h, i.e.,
the righthand side of (3.12) is in dom h. now we use the assumption that   h
is nondecreasing, which means that its domain extends in   nitely in the negative
direction. since the righthand side of (3.12) is in dom h, we conclude that the
lefthand side, i.e., g(  x+(1     )y)     dom h. this means that   x+(1     )y     dom f .
at this point, we have shown that dom f is convex.

g(  x + (1       )y)       g(x) + (1       )g(y).

now using the fact that   h is nondecreasing and the inequality (3.12), we get

h(g(  x + (1       )y))     h(  g(x) + (1       )g(y)).

from convexity of h, we have

h(  g(x) + (1       )g(y))       h(g(x)) + (1       )h(g(y)).

(3.13)

(3.14)

86

3 convex functions

putting (3.13) and (3.14) together, we have

h(g(  x + (1       )y))       h(g(x)) + (1       )h(g(y)).

which proves the composition theorem.

example 3.13 simple composition results.

    if g is convex then exp g(x) is convex.
    if g is concave and positive, then log g(x) is concave.
    if g is concave and positive, then 1/g(x) is convex.
    if g is convex and nonnegative and p     1, then g(x)p is convex.
    if g is convex then     log(   g(x)) is convex on {x | g(x) < 0}.

remark 3.3 the requirement that monotonicity hold for the extended-value extension
  h, and not just the function h, cannot be removed. for example, consider the function
g(x) = x2, with dom g = r, and h(x) = 0, with dom h = [1, 2]. here g is convex,
and h is convex and nondecreasing. but the function f = h     g, given by

f (x) = 0,

dom f = [   

   2,   1]     [1,   2],

is not convex, since its domain is not convex. here, of course, the function   h is not
nondecreasing.

vector composition
we now turn to the more complicated case when k     1. suppose

f (x) = h(g(x)) = h(g1(x), . . . , gk(x)),

with h : rk     r, gi : rn     r. again without loss of generality we can assume n =
1. as in the case k = 1, we start by assuming the functions are twice di   erentiable,
with dom g = r and dom h = rk, in order to discover the composition rules. we
have

f       (x) = g   (x)t   2h(g(x))g   (x) +    h(g(x))t g      (x),

(3.15)

which is the vector analog of (3.9). again the issue is to determine conditions under
which f       (x)     0 for all x (or f       (x)     0 for all x for concavity). from (3.15) we
can derive many rules, for example:

f is convex if h is convex, h is nondecreasing in each argument,
and gi are convex,
f is convex if h is convex, h is nonincreasing in each argument,
and gi are concave,
f is concave if h is concave, h is nondecreasing in each argument,
and gi are concave.

3.2 operations that preserve convexity

87

as in the scalar case, similar composition results hold in general, with n > 1, no as-
sumption of di   erentiability of h or g, and general domains. for the general results,
the monotonicity condition on h must hold for the extended-value extension   h.

to understand the meaning of the condition that the extended-value exten-
sion   h be monotonic, we consider the case where h : rk     r is convex, and   h
nondecreasing, i.e., whenever u (cid:22) v, we have   h(u)       h(v). this implies that if
v     dom h, then so is u: the domain of h must extend in   nitely in the    rk
directions. we can express this compactly as dom h     rk

+ = dom h.

+

example 3.14 vector composition examples.

    let h(z) = z[1] +       + z[r], the sum of the r largest components of z     rk. then
h is convex and nondecreasing in each argument. suppose g1, . . . , gk are convex
functions on rn. then the composition function f = h     g, i.e., the pointwise
sum of the r largest gi   s, is convex.

i=1 ezi ) is convex and nondecreasing in each argu-

+ is concave, and
its extension (which has the value        for z 6(cid:23) 0) is nondecreasing in each
component. so if gi are concave and nonnegative, we conclude that f (x) =

i )1/p on rk

i=1 egi ) is convex whenever gi are.
i=1 zp

    the function h(z) = log(pk
ment, so log(pk
    for 0 < p     1, the function h(z) = (pk
(pk
(pk

i=1 gi(x)p)1/p is concave.

i=1 gi(x)p)1/p is convex.

to show this, we consider the function h : rk     r de   ned as

    suppose p     1, and g1, . . . , gk are convex and nonnegative. then the function

h(z) =  kxi=1

max{zi, 0}p!1/p

,

with dom h = rk, so h =   h. this function is convex, and nondecreasing, so
we conclude h(g(x)) is a convex function of x. for z (cid:23) 0, we have h(z) =

+ is concave and its extension
is nondecreasing in each argument. it follows that if g1, . . . , gk are nonnegative

i=1 zp

i )1/p, so our conclusion is that (pk

(pk
    the geometric mean h(z) = (qk
concave functions, then so is their geometric mean, (qk

i=1 zi)1/k on rk

i=1 gi(x)p)1/p is convex.

i=1 gi)1/k.

3.2.5 minimization

we have seen that the maximum or supremum of an arbitrary family of convex
functions is convex. it turns out that some special forms of minimization also yield
convex functions. if f is convex in (x, y), and c is a convex nonempty set, then
the function

g(x) = inf
y   c

f (x, y)

(3.16)

88

3 convex functions

is convex in x, provided g(x) >        for all x. the domain of g is the projection of
dom f on its x-coordinates, i.e.,

dom g = {x | (x, y)     dom f for some y     c}.

we prove this by verifying jensen   s inequality for x1, x2     dom g. let    > 0.
then there are y1, y2     c such that f (xi, yi)     g(xi) +    for i = 1, 2. now let
       [0, 1]. we have

g(  x1 + (1       )x2) = inf

y   c

f (  x1 + (1       )x2, y)

    f (  x1 + (1       )x2,   y1 + (1       )y2)
      f (x1, y1) + (1       )f (x2, y2)
      g(x1) + (1       )g(x2) +   .

since this holds for any    > 0, we have

g(  x1 + (1       )x2)       g(x1) + (1       )g(x2).

the result can also be seen in terms of epigraphs. with f , g, and c de   ned as

in (3.16), and assuming the in   mum over y     c is attained for each x, we have

epi g = {(x, t) | (x, y, t)     epi f for some y     c}.

thus epi g is convex, since it is the projection of a convex set on some of its
components.

example 3.15 schur complement. suppose the quadratic function

f (x, y) = xt ax + 2xt by + yt cy,

(where a and c are symmetric) is convex in (x, y), which means

(cid:20) a b
bt c (cid:21) (cid:23) 0.

we can express g(x) = inf y f (x, y) as

g(x) = xt (a     bc    bt )x,

where c     is the pseudo-inverse of c (see   a.5.4). by the minimization rule, g is
convex, so we conclude that a     bc    bt (cid:23) 0.
if c is invertible, i.e., c     0, then the matrix a     bc    1bt is called the schur
complement of c in the matrix

(see   a.5.5).

bt c (cid:21)
(cid:20) a b

example 3.16 distance to a set. the distance of a point x to a set s     rn, in the
norm k    k, is de   ned as

dist(x, s) = inf

y   s kx     yk.

the function kx   yk is convex in (x, y), so if the set s is convex, the distance function
dist(x, s) is a convex function of x.

3.2 operations that preserve convexity

89

example 3.17 suppose h is convex. then the function g de   ned as

is convex. to see this, we de   ne f by

g(x) = inf{h(y) | ay = x}

f (x, y) =(cid:26) h(y)

if ay = x
    otherwise,

which is convex in (x, y). then g is the minimum of f over y, and hence is convex.
(it is not hard to show directly that g is convex.)

3.2.6 perspective of a function

if f : rn     r, then the perspective of f is the function g : rn+1     r de   ned by

g(x, t) = tf (x/t),

with domain

dom g = {(x, t) | x/t     dom f, t > 0}.

the perspective operation preserves convexity: if f is a convex function, then so
is its perspective function g. similarly, if f is concave, then so is g.

this can be proved several ways, for example, direct veri   cation of the de   ning
inequality (see exercise 3.33). we give a short proof here using epigraphs and the
perspective mapping on rn+1 described in   2.3.3 (which will also explain the name
   perspective   ). for t > 0 we have

(x, t, s)     epi g        tf (x/t)     s
       f (x/t)     s/t
       (x/t, s/t)     epi f.

therefore epi g is the inverse image of epi f under the perspective mapping that
takes (u, v, w) to (u, w)/v. it follows (see   2.3.3) that epi g is convex, so the function
g is convex.

example 3.18 euclidean norm squared. the perspective of the convex function
f (x) = xt x on rn is

g(x, t) = t(x/t)t (x/t) =

xt x

t

,

which is convex in (x, t) for t > 0.

we can deduce convexity of g using several other methods. first, we can express g as
the sum of the quadratic-over-linear functions x2
i /t, which were shown to be convex
in   3.1.5. we can also express g as a special case of the matrix fractional function
xt (ti)   1x (see example 3.4).

90

3 convex functions

example 3.19 negative logarithm. consider the convex function f (x) =     log x on
r++. its perspective is

g(x, t) =    t log(x/t) = t log(t/x) = t log t     t log x,

and is convex on r2
x = 1, g reduces to the negative id178 function.

++. the function g is called the relative id178 of t and x. for

from convexity of g we can establish convexity or concavity of several interesting
related functions. first, the relative id178 of two vectors u, v     rn

++, de   ned as

ui log(ui/vi),

nxi=1

nxi=1

is convex in (u, v), since it is a sum of relative entropies of ui, vi.
a closely related function is the id181 between u, v     rn
given by

++,

dkl(u, v) =

(ui log(ui/vi)     ui + vi) ,

(3.17)

which is convex, since it is the relative id178 plus a linear function of (u, v). the
id181 satis   es dkl(u, v)     0, and dkl(u, v) = 0 if and only if
u = v, and so can be used as a measure of deviation between two positive vectors; see
exercise 3.13. (note that the relative id178 and the id181
are the same when u and v are id203 vectors, i.e., satisfy 1t u = 1t v = 1.)
if we take vi = 1t u in the relative id178 function, we obtain the concave (and
homogeneous) function of u     rn

++ given by

nxi=1

ui log(1t u/ui) = (1t u)

zi log(1/zi),

nxi=1

where z = u/(1t u), which is called the normalized id178 function. the vector
z = u/1t u is a normalized vector or id203 distribution, since its components
sum to one; the normalized id178 of u is 1t u times the id178 of this normalized
distribution.

example 3.20 suppose f : rm     r is convex, and a     rm  n, b     rm, c     rn,
and d     r. we de   ne

g(x) = (ct x + d)f(cid:0)(ax + b)/(ct x + d)(cid:1) ,

dom g = {x | ct x + d > 0, (ax + b)/(ct x + d)     dom f}.

with

then g is convex.

3.3 the conjugate function

in this section we introduce an operation that will play an important role in later
chapters.

3.3 the conjugate function

91

f (x)

xy

x

(0,   f    (y))

figure 3.8 a function f : r     r, and a value y     r. the conjugate
function f    (y) is the maximum gap between the linear function yx and
f (x), as shown by the dashed line in the    gure. if f is di   erentiable, this
occurs at a point x where f    (x) = y.

3.3.1 de   nition and examples

let f : rn     r. the function f     : rn     r, de   ned as
x   dom f(cid:0)yt x     f (x)(cid:1) ,

f    (y) = sup

(3.18)

is called the conjugate of the function f . the domain of the conjugate function
consists of y     rn for which the supremum is    nite, i.e., for which the di   erence
yt x     f (x) is bounded above on dom f . this de   nition is illustrated in    gure 3.8.
we see immediately that f     is a convex function, since it is the pointwise
supremum of a family of convex (indeed, a   ne) functions of y. this is true whether
or not f is convex. (note that when f is convex, the subscript x     dom f is not
necessary since, by convention, yt x     f (x) =        for x 6    dom f .)
we start with some simple examples, and then describe some rules for conjugat-
ing functions. this allows us to derive an analytical expression for the conjugate
of many common convex functions.

example 3.21 we derive the conjugates of some convex functions on r.

    a   ne function. f (x) = ax + b. as a function of x, yx     ax     b is bounded if
and only if y = a, in which case it is constant. therefore the domain of the
conjugate function f     is the singleton {a}, and f    (a) =    b.

    negative logarithm. f (x) =     log x, with dom f = r++. the function xy+log x
is unbounded above if y     0 and reaches its maximum at x =    1/y otherwise.
therefore, dom f     = {y | y < 0} =    r++ and f    (y) =     log(   y)   1 for y < 0.
    exponential. f (x) = ex. xy     ex is unbounded if y < 0. for y > 0, xy     ex
reaches its maximum at x = log y, so we have f    (y) = y log y     y. for y = 0,

92

3 convex functions

f    (y) = supx    ex = 0. in summary, dom f     = r+ and f    (y) = y log y     y
(with the interpretation 0 log 0 = 0).

    negative id178. f (x) = x log x, with dom f = r+ (and f (0) = 0). the
function xy     x log x is bounded above on r+ for all y, hence dom f     = r. it
attains its maximum at x = ey   1, and substituting we    nd f    (y) = ey   1.

    inverse. f (x) = 1/x on r++. for y > 0, yx     1/x is unbounded above. for
y = 0 this function has supremum 0; for y < 0 the supremum is attained at
x = (   y)   1/2. therefore we have f    (y) =    2(   y)1/2, with dom f     =    r+.

example 3.22 strictly convex quadratic function. consider f (x) = 1
q     sn
it attains its maximum at x = q   1y, so

2 xt qx, with
2 xt qx is bounded above as a function of x for all y.

++. the function yt x     1

f    (y) =

yt q   1y.

1
2

example 3.23 log-determinant. we consider f (x) = log det x    1 on sn
conjugate function is de   ned as

++. the

f    (y ) = sup
x   0

(tr(y x) + log det x) ,

since tr(y x) is the standard inner product on sn. we    rst show that tr(y x) +
log det x is unbounded above unless y     0. if y 6    0, then y has an eigenvector v,
with kvk2 = 1, and eigenvalue        0. taking x = i + tvvt we    nd that

tr(y x) + log det x = tr y + t   + log det(i + tvvt ) = tr y + t   + log(1 + t),

which is unbounded above as t        .
now consider the case y     0. we can    nd the maximizing x by setting the gradient
with respect to x equal to zero:

   x (tr(y x) + log det x) = y + x    1 = 0

(see   a.4.1), which yields x =    y    1 (which is, indeed, positive de   nite). therefore
we have

with dom f     =    sn

++.

f    (y ) = log det(   y )   1     n,

example 3.24 indicator function. let is be the indicator function of a (not neces-
sarily convex) set s     rn, i.e., is(x) = 0 on dom is = s. its conjugate is

i    
s(y) = sup
x   s

yt x,

which is the support function of the set s.

3.3 the conjugate function

93

function f (x) = log(pn

example 3.25 log-sum-exp function. to derive the conjugate of the log-sum-exp
i=1 exi ), we    rst determine the values of y for which the
maximum over x of yt x     f (x) is attained. by setting the gradient with respect to
x equal to zero, we obtain the condition

yi =

,

i = 1, . . . , n.

exi
j=1 exj

pn

the expression for yi into yt x   f (x) we obtain f    (y) =pn

these equations are solvable for x if and only if y     0 and 1t y = 1. by substituting
i=1 yi log yi. this expression
for f     is still correct if some components of y are zero, as long as y (cid:23) 0 and 1t y = 1,
and we interpret 0 log 0 as 0.
in fact the domain of f     is exactly given by 1t y = 1, y (cid:23) 0. to show this, suppose
that a component of y is negative, say, yk < 0. then we can show that yt x     f (x) is
unbounded above by choosing xk =    t, and xi = 0, i 6= k, and letting t go to in   nity.
if y (cid:23) 0 but 1t y 6= 1, we choose x = t1, so that

yt x     f (x) = t1t y     t     log n.

if 1t y > 1, this grows unboundedly as t        ; if 1t y < 1, it grows unboundedly as
t           .
in summary,

f    (y) =(cid:26) pn

   

i=1 yi log yi

if y (cid:23) 0 and 1t y = 1
otherwise.

in other words, the conjugate of the log-sum-exp function is the negative id178
function, restricted to the id203 simplex.

example 3.26 norm. let k    k be a norm on rn, with dual norm k    k   . we will
show that the conjugate of f (x) = kxk is

f    (y) =(cid:26) 0

kyk        1
    otherwise,

i.e., the conjugate of a norm is the indicator function of the dual norm unit ball.
if kyk    > 1, then by de   nition of the dual norm, there is a z     rn with kzk     1 and
yt z > 1. taking x = tz and letting t        , we have

yt x     kxk = t(yt z     kzk)        ,

which shows that f    (y) =    . conversely, if kyk        1, then we have yt x     kxkkyk   
for all x, which implies for all x, yt x     kxk     0. therefore x = 0 is the value that
maximizes yt x     kxk, with maximum value 0.

example 3.27 norm squared. now consider the function f (x) = (1/2)kxk2, where k  k
is a norm, with dual norm k  k   . we will show that its conjugate is f    (y) = (1/2)kyk2
   .
from yt x     kyk   kxk, we conclude

yt x     (1/2)kxk2     kyk   kxk     (1/2)kxk2

94

3 convex functions

for all x. the righthand side is a quadratic function of kxk, which has maximum
value (1/2)kyk2

   . therefore for all x, we have

yt x     (1/2)kxk2     (1/2)kyk2
   ,

which shows that f    (y)     (1/2)kyk2
   .
to show the other inequality, let x be any vector with yt x = kyk   kxk, scaled so that
kxk = kyk   . then we have, for this x,

which shows that f    (y)     (1/2)kyk2
   .

yt x     (1/2)kxk2 = (1/2)kyk2
   ,

example 3.28 revenue and pro   t functions. we consider a business or enterprise that
consumes n resources and produces a product that can be sold. we let r = (r1, . . . , rn)
denote the vector of resource quantities consumed, and s(r) denote the sales revenue
derived from the product produced (as a function of the resources consumed). now
let pi denote the price (per unit) of resource i, so the total amount paid for resources
by the enterprise is pt r. the pro   t derived by the    rm is then s(r)    pt r. let us    x
the prices of the resources, and ask what is the maximum pro   t that can be made, by
wisely choosing the quantities of resources consumed. this maximum pro   t is given
by

m (p) = sup

r (cid:0)s(r)     pt r(cid:1) .

the function m (p) gives the maximum pro   t attainable, as a function of the resource
prices. in terms of conjugate functions, we can express m as

thus the maximum pro   t (as a function of resource prices) is closely related to the
conjugate of gross sales (as a function of resources consumed).

m (p) = (   s)   (   p).

3.3.2 basic properties

fenchel   s inequality

from the de   nition of conjugate function, we immediately obtain the inequality

f (x) + f    (y)     xt y

for all x, y. this is called fenchel   s inequality (or young   s inequality when f is
di   erentiable).

for example with f (x) = (1/2)xt qx, where q     sn

++, we obtain the inequality

xt y     (1/2)xt qx + (1/2)yt q   1y.

conjugate of the conjugate

the examples above, and the name    conjugate   , suggest that the conjugate of the
conjugate of a convex function is the original function. this is the case provided a
technical condition holds: if f is convex, and f is closed (i.e., epi f is a closed set;
see   a.3.3), then f        = f . for example, if dom f = rn, then we have f        = f ,
i.e., the conjugate of the conjugate of f is f again (see exercise 3.39).

3.4 quasiconvex functions

95

di   erentiable functions

the conjugate of a di   erentiable function f is also called the legendre transform
of f . (to distinguish the general de   nition from the di   erentiable case, the term
fenchel conjugate is sometimes used instead of conjugate.)

suppose f is convex and di   erentiable, with dom f = rn. any maximizer x   
of yt x    f (x) satis   es y =    f (x   ), and conversely, if x    satis   es y =    f (x   ), then
x    maximizes yt x     f (x). therefore, if y =    f (x   ), we have

f    (y) = x   t   f (x   )     f (x   ).

this allows us to determine f    (y) for any y for which we can solve the gradient
equation y =    f (z) for z.
then we have

we can express this another way. let z     rn be arbitrary and de   ne y =    f (z).

f    (y) = zt   f (z)     f (z).
scaling and composition with a   ne transformation
for a > 0 and b     r, the conjugate of g(x) = af (x) + b is g   (y) = af    (y/a)     b.
suppose a     rn  n is nonsingular and b     rn. then the conjugate of g(x) =
f (ax + b) is

with dom g    = at dom f    .

g   (y) = f    (a   t y)     bt a   t y,

sums of independent functions

if f (u, v) = f1(u) + f2(v), where f1 and f2 are convex functions with conjugates
1 and f    
f    

2 , respectively, then

f    (w, z) = f    

1 (w) + f    

2 (z).

in other words, the conjugate of the sum of independent convex functions is the sum
of the conjugates. (   independent    means they are functions of di   erent variables.)

3.4 quasiconvex functions

3.4.1 de   nition and examples

a function f : rn     r is called quasiconvex (or unimodal ) if its domain and all
its sublevel sets

s   = {x     dom f | f (x)       },

for        r, are convex. a function is quasiconcave if    f is quasiconvex, i.e., every
superlevel set {x | f (x)       } is convex. a function that is both quasiconvex and
quasiconcave is called quasilinear. if a function f is quasilinear, then its domain,
and every level set {x | f (x) =   } is convex.

96

3 convex functions

  

  

a

b

c

figure 3.9 a quasiconvex function on r. for each   , the   -sublevel set s  
is convex, i.e., an interval. the sublevel set s   is the interval [a, b]. the
sublevel set s   is the interval (      , c].

for a function on r, quasiconvexity requires that each sublevel set be an interval
(including, possibly, an in   nite interval). an example of a quasiconvex function on
r is shown in    gure 3.9.

convex functions have convex sublevel sets, and so are quasiconvex. but simple
examples, such as the one shown in    gure 3.9, show that the converse is not true.

example 3.29 some examples on r:

    logarithm. log x on r++ is quasiconvex (and quasiconcave, hence quasilinear).
    ceiling function. ceil(x) = inf{z     z | z     x} is quasiconvex (and quasicon-

cave).

these examples show that quasiconvex functions can be concave, or discontinuous.
we now give some examples on rn.

example 3.30 length of a vector. we de   ne the length of x     rn as the largest
index of a nonzero component, i.e.,

(we de   ne the length of the zero vector to be zero.) this function is quasiconvex on
rn, since its sublevel sets are subspaces:

f (x) = max{i | xi 6= 0}.

f (x)               xi = 0 for i =          + 1, . . . , n.

example 3.31 consider f : r2     r, with dom f = r2
function is neither convex nor concave since its hessian

+ and f (x1, x2) = x1x2. this

   2f (x) =(cid:20) 0

1

1

0 (cid:21)

3.4 quasiconvex functions

97

is inde   nite;
quasiconcave, however, since the superlevel sets

it has one positive and one negative eigenvalue. the function f is

are convex sets for all   . (note, however, that f is not quasiconcave on r2.)

{x     r2

+ | x1x2       }

example 3.32 linear-fractional function. the function

f (x) =

at x + b
ct x + d

,

with dom f = {x | ct x + d > 0}, is quasiconvex, and quasiconcave, i.e., quasilinear.
its   -sublevel set is

s   = {x | ct x + d > 0, (at x + b)/(ct x + d)       }

= {x | ct x + d > 0, at x + b       (ct x + d)},

which is convex, since it is the intersection of an open halfspace and a closed halfspace.
(the same method can be used to show its superlevel sets are convex.)

example 3.33 distance ratio function. suppose a, b     rn, and de   ne

f (x) = kx     ak2
kx     bk2

,

i.e., the ratio of the euclidean distance to a to the distance to b. then f is quasiconvex
on the halfspace {x | kx     ak2     kx     bk2}. to see this, we consider the   -sublevel
set of f , with        1 since f (x)     1 on the halfspace {x | kx     ak2     kx     bk2}. this
sublevel set is the set of points satisfying

kx     ak2       kx     bk2.

squaring both sides, and rearranging terms, we see that this is equivalent to

(1       2)xt x     2(a       2b)t x + at a       2bt b     0.

this describes a convex set (in fact a euclidean ball) if        1.

example 3.34 internal rate of return. let x = (x0, x1, . . . , xn) denote a cash    ow
sequence over n periods, where xi > 0 means a payment to us in period i, and xi < 0
means a payment by us in period i. we de   ne the present value of a cash    ow, with
interest rate r     0, to be

pv(x, r) =

(1 + r)   ixi.

nxi=0

(the factor (1 + r)   i is a discount factor for a payment by or to us in period i.)
now we consider cash    ows for which x0 < 0 and x0 + x1 +        + xn > 0. this
means that we start with an investment of |x0| in period 0, and that the total of the

98

3 convex functions

remaining cash    ow, x1 +        + xn, (not taking any discount factors into account)
exceeds our initial investment.
for such a cash    ow, pv(x, 0) > 0 and pv(x, r)     x0 < 0 as r        , so it follows
that for at least one r     0, we have pv(x, r) = 0. we de   ne the internal rate of
return of the cash    ow as the smallest interest rate r     0 for which the present value
is zero:

irr(x) = inf{r     0 | pv(x, r) = 0}.

internal rate of return is a quasiconcave function of x (restricted to x0 < 0, x1 +       +
xn > 0). to see this, we note that

irr(x)     r        pv(x, r) > 0 for 0     r < r.

the lefthand side de   nes the r-superlevel set of irr. the righthand side is the
intersection of the sets {x | pv(x, r) > 0}, indexed by r, over the range 0     r < r.
for each r, pv(x, r) > 0 de   nes an open halfspace, so the righthand side de   nes a
convex set.

3.4.2 basic properties

the examples above show that quasiconvexity is a considerable generalization of
convexity. still, many of the properties of convex functions hold, or have analogs,
for quasiconvex functions. for example, there is a variation on jensen   s inequality
that characterizes quasiconvexity: a function f is quasiconvex if and only if dom f
is convex and for any x, y     dom f and 0            1,

f (  x + (1       )y)     max{f (x), f (y)},

(3.19)

i.e., the value of the function on a segment does not exceed the maximum of
its values at the endpoints. the inequality (3.19) is sometimes called jensen   s
inequality for quasiconvex functions, and is illustrated in    gure 3.10.

example 3.35 cardinality of a nonnegative vector. the cardinality or size of a
vector x     rn is the number of nonzero components, and denoted card(x). the
function card is quasiconcave on rn
+ (but not rn). this follows immediately from
the modi   ed jensen inequality

card(x + y)     min{card(x), card(y)},

which holds for x, y (cid:23) 0.

example 3.36 rank of positive semide   nite matrix. the function rank x is quasi-
concave on sn

+. this follows from the modi   ed jensen inequality (3.19),

rank(x + y )     min{rank x, rank y }

which holds for x, y     sn
example, since rank(diag(x)) = card(x) for x (cid:23) 0.)

+. (this can be considered an extension of the previous

3.4 quasiconvex functions

99

max{f (x), f (y)}

(y, f (y))

(x, f (x))

figure 3.10 a quasiconvex function on r. the value of f between x and y
is no more than max{f (x), f (y)}.

like convexity, quasiconvexity is characterized by the behavior of a function f
on lines: f is quasiconvex if and only if its restriction to any line intersecting its
domain is quasiconvex. in particular, quasiconvexity of a function can be veri   ed by
restricting it to an arbitrary line, and then checking quasiconvexity of the resulting
function on r.

quasiconvex functions on r

we can give a simple characterization of quasiconvex functions on r. we consider
continuous functions, since stating the conditions in the general case is cumbersome.
a continuous function f : r     r is quasiconvex if and only if at least one of the
following conditions holds:

    f is nondecreasing
    f is nonincreasing
    there is a point c     dom f such that for t     c (and t     dom f ), f is

nonincreasing, and for t     c (and t     dom f ), f is nondecreasing.

the point c can be chosen as any point which is a global minimizer of f . figure 3.11
illustrates this.

3.4.3 di   erentiable quasiconvex functions

first-order conditions
suppose f : rn     r is di   erentiable. then f is quasiconvex if and only if dom f
is convex and for all x, y     dom f

f (y)     f (x) =       f (x)t (y     x)     0.

(3.20)

100

3 convex functions

c

t

figure 3.11 a quasiconvex function on r. the function is nonincreasing for
t     c and nondecreasing for t     c.

   f (x)

x

figure 3.12 three level curves of a quasiconvex function f are shown. the
vector    f (x) de   nes a supporting hyperplane to the sublevel set {z | f (z)    
f (x)} at x.

this is the analog of inequality (3.2), for quasiconvex functions. we leave the proof
as an exercise (exercise 3.43).

the condition (3.20) has a simple geometric interpretation when    f (x) 6= 0. it
states that    f (x) de   nes a supporting hyperplane to the sublevel set {y | f (y)    
f (x)}, at the point x, as illustrated in    gure 3.12.

while the    rst-order condition for convexity (3.2), and the    rst-order condition
for quasiconvexity (3.20) are similar, there are some important di   erences. for
example, if f is convex and    f (x) = 0, then x is a global minimizer of f . but this
statement is false for quasiconvex functions: it is possible that    f (x) = 0, but x
is not a global minimizer of f .

3.4 quasiconvex functions

101

second-order conditions
now suppose f is twice di   erentiable. if f is quasiconvex, then for all x     dom f ,
and all y     rn, we have

yt   f (x) = 0 =    yt   2f (x)y     0.

(3.21)

for a quasiconvex function on r, this reduces to the simple condition

f    (x) = 0 =    f       (x)     0,

i.e., at any point with zero slope, the second derivative is nonnegative. for a
quasiconvex function on rn, the interpretation of the condition (3.21) is a bit
more complicated. as in the case n = 1, we conclude that whenever    f (x) = 0,
we must have    2f (x) (cid:23) 0. when    f (x) 6= 0, the condition (3.21) means that
   2f (x) is positive semide   nite on the (n     1)-dimensional subspace    f (x)   . this
implies that    2f (x) can have at most one negative eigenvalue.

as a (partial) converse, if f satis   es

yt   f (x) = 0 =    yt   2f (x)y > 0

(3.22)
for all x     dom f and all y     rn, y 6= 0, then f is quasiconvex. this condition is
the same as requiring    2f (x) to be positive de   nite for any point with    f (x) = 0,
and for all other points, requiring    2f (x) to be positive de   nite on the (n     1)-
dimensional subspace    f (x)   .
proof of second-order conditions for quasiconvexity

by restricting the function to an arbitrary line, it su   ces to consider the case in
which f : r     r.
we    rst show that if f : r     r is quasiconvex on an interval (a, b), then it
must satisfy (3.21), i.e., if f    (c) = 0 with c     (a, b), then we must have f       (c)     0. if
f    (c) = 0 with c     (a, b), f       (c) < 0, then for small positive    we have f (c     ) < f (c)
it follows that the sublevel set {x | f (x)     f (c)       } is
and f (c +   ) < f (c).
disconnected for small positive   , and therefore not convex, which contradicts our
assumption that f is quasiconvex.

now we show that if the condition (3.22) holds, then f is quasiconvex. assume
that (3.22) holds, i.e., for each c     (a, b) with f    (c) = 0, we have f       (c) > 0. this
means that whenever the function f     crosses the value 0, it is strictly increasing.
if f     does not cross the value
therefore it can cross the value 0 at most once.
0 at all, then f is either nonincreasing or nondecreasing on (a, b), and therefore
quasiconvex. otherwise it must cross the value 0 exactly once, say at c     (a, b).
since f       (c) > 0, it follows that f    (t)     0 for a < t     c, and f    (t)     0 for c     t < b.
this shows that f is quasiconvex.

3.4.4 operations that preserve quasiconvexity

nonnegative weighted maximum

a nonnegative weighted maximum of quasiconvex functions, i.e.,

f = max{w1f1, . . . , wmfm},

102

3 convex functions

with wi     0 and fi quasiconvex, is quasiconvex. the property extends to the
general pointwise supremum

f (x) = sup
y   c

(w(y)g(x, y))

where w(y)     0 and g(x, y) is quasiconvex in x for each y. this fact can be easily
veri   ed: f (x)        if and only if

w(y)g(x, y)        for all y     c,

i.e., the   -sublevel set of f is the intersection of the   -sublevel sets of the functions
w(y)g(x, y) in the variable x.

example 3.37 generalized eigenvalue. the maximum generalized eigenvalue of a
pair of symmetric matrices (x, y ), with y     0, is de   ned as

  max(x, y ) = sup
u6=0

ut xu
ut y u

= sup{   | det(  y     x) = 0}.

(see   a.5.3). this function is quasiconvex on dom f = sn    sn
to see this we consider the expression

++.

  max(x, y ) = sup
u6=0

ut xu
ut y u

.

for each u 6= 0, the function ut xu/ut y u is linear-fractional in (x, y ), hence a
quasiconvex function of (x, y ). we conclude that   max is quasiconvex, since it is the
supremum of a family of quasiconvex functions.

composition
if g : rn     r is quasiconvex and h : r     r is nondecreasing, then f = h     g is
quasiconvex.
the composition of a quasiconvex function with an a   ne or linear-fractional
transformation yields a quasiconvex function.
if f is quasiconvex, then g(x) =
f (ax + b) is quasiconvex, and   g(x) = f ((ax + b)/(ct x + d)) is quasiconvex on the
set

{x | ct x + d > 0, (ax + b)/(ct x + d)     dom f}.

minimization

if f (x, y) is quasiconvex jointly in x and y and c is a convex set, then the function

g(x) = inf
y   c

f (x, y)

is quasiconvex.

to show this, we need to show that {x | g(x)       } is convex, where        r is
arbitrary. from the de   nition of g, g(x)        if and only if for any    > 0 there exists

3.4 quasiconvex functions

103

a y     c with f (x, y)        +   . now let x1 and x2 be two points in the   -sublevel
set of g. then for any    > 0, there exists y1, y2     c with

f (x1, y1)        +   ,

f (x2, y2)        +   ,

and since f is quasiconvex in x and y, we also have

f (  x1 + (1       )x2,   y1 + (1       )y2)        +   ,

for 0            1. hence g(  x1 + (1       )x2)       , which proves that {x | g(x)       } is
convex.

3.4.5 representation via family of convex functions

in the sequel, it will be convenient to represent the sublevel sets of a quasiconvex
function f (which are convex) via inequalities of convex functions. we seek a family
of convex functions   t : rn     r, indexed by t     r, with

f (x)     t          t(x)     0,

(3.23)

i.e., the t-sublevel set of the quasiconvex function f is the 0-sublevel set of the
convex function   t. evidently   t must satisfy the property that for all x     rn,
  t(x)     0 =      s(x)     0 for s     t. this is satis   ed if for each x,   t(x) is a
nonincreasing function of t, i.e.,   s(x)       t(x) whenever s     t.
to see that such a representation always exists, we can take

  t(x) =(cid:26) 0

f (x)     t
    otherwise,

i.e.,   t is the indicator function of the t-sublevel of f . obviously this representation
is not unique; for example if the sublevel sets of f are closed, we can take

  t(x) = dist (x,{z | f (z)     t}) .

we are usually interested in a family   t with nice properties, such as di   erentia-
bility.

example 3.38 convex over concave function. suppose p is a convex function, q is a
concave function, with p(x)     0 and q(x) > 0 on a convex set c. then the function
f de   ned by f (x) = p(x)/q(x), on c, is quasiconvex.

here we have

f (x)     t        p(x)     tq(x)     0,

so we can take   t(x) = p(x)     tq(x) for t     0. for each t,   t is convex and for each
x,   t(x) is decreasing in t.

104

3 convex functions

3.5 log-concave and log-convex functions

3.5.1 de   nition

a function f : rn     r is logarithmically concave or log-concave if f (x) > 0
for all x     dom f and log f is concave.
it is said to be logarithmically convex
or log-convex if log f is convex. thus f is log-convex if and only if 1/f is log-
concave. it is convenient to allow f to take on the value zero, in which case we
take log f (x) =       . in this case we say f is log-concave if the extended-value
function log f is concave.
we can express log-concavity directly, without logarithms: a function f : rn    
r, with convex domain and f (x) > 0 for all x     dom f , is log-concave if and only
if for all x, y     dom f and 0            1, we have

f (  x + (1       )y)     f (x)  f (y)1     .

in particular, the value of a log-concave function at the average of two points is at
least the geometric mean of the values at the two points.

from the composition rules we know that eh is convex if h is convex, so a log-
convex function is convex. similarly, a nonnegative concave function is log-concave.
it is also clear that a log-convex function is quasiconvex and a log-concave function
is quasiconcave, since the logarithm is monotone increasing.

example 3.39 some simple examples of log-concave and log-convex functions.

    a   ne function. f (x) = at x + b is log-concave on {x | at x + b > 0}.
    powers. f (x) = xa, on r++, is log-convex for a     0, and log-concave for a     0.
    exponentials. f (x) = eax is log-convex and log-concave.
    the cumulative distribution function of a gaussian density,

  (x) =

is log-concave (see exercise 3.54).

1

   2  z x

      

e   u2/2 du,

    gamma function. the gamma function,

  (x) =z    

0

ux   1e   u du,

is log-convex for x     1 (see exercise 3.52).
    determinant. det x is log concave on sn
++.
    determinant over trace. det x/ tr x is log concave on sn

++ (see exercise 3.49).

example 3.40 log-concave density functions. many common id203 density
functions are log-concave. two examples are the multivariate normal distribution,

f (x) =

1

p(2  )n det   

e    1

2 (x     x)t      1(x     x)

3.5 log-concave and log-convex functions

105

(where   x     rn and        sn

++), and the exponential distribution on rn
+,

  i! e     t x
f (x) =  nyi=1
f (x) =(cid:26) 1/   x     c

x 6    c

0

(where        0). another example is the uniform distribution over a convex set c,

where    = vol(c) is the volume (lebesgue measure) of c. in this case log f takes
on the value        outside c, and     log    on c, hence is concave.
as a more exotic example consider the wishart distribution, de   ned as follows. let
x1, . . . , xp     rn be independent gaussian random vectors with zero mean and co-
i has the wishart
density

variance        sn, with p > n. the random matrix x =pp

i=1 xixt

f (x) = a (det x)(p   n   1)/2 e    1

tr(     1x),

2

++, and a is a positive constant. the wishart density is log-concave,

with dom f = sn
since

log f (x) = log a +

which is a concave function of x.

p     n     1

2

log det x    

1
2

tr(     1x),

3.5.2 properties

twice di   erentiable log-convex/concave functions

suppose f is twice di   erentiable, with dom f convex, so

1
f (x)2   f (x)   f (x)t .
we conclude that f is log-convex if and only if for all x     dom f ,

1
f (x)   2f (x)    

   2 log f (x) =

f (x)   2f (x) (cid:23)    f (x)   f (x)t ,

and log-concave if and only if for all x     dom f ,

f (x)   2f (x) (cid:22)    f (x)   f (x)t .

multiplication, addition, and integration

log-convexity and log-concavity are closed under multiplication and positive scal-
ing. for example, if f and g are log-concave, then so is the pointwise product
h(x) = f (x)g(x), since log h(x) = log f (x) + log g(x), and log f (x) and log g(x) are
concave functions of x.

simple examples show that the sum of log-concave functions is not, in general,
log-concave. log-convexity, however, is preserved under sums. let f and g be log-
convex functions, i.e., f = log f and g = log g are convex. from the composition
rules for convex functions, it follows that

log (exp f + exp g) = log(f + g)

106

3 convex functions

is convex. therefore the sum of two log-convex functions is log-convex.

more generally, if f (x, y) is log-convex in x for each y     c then

g(x) =zc

f (x, y) dy

is log-convex.

example 3.41 laplace transform of a nonnegative function and the moment and
cumulant generating functions. suppose p : rn     r satis   es p(x)     0 for all x. the
laplace transform of p,

p (z) =z p(x)e   zt x dx,

is log-convex on rn. (here dom p is, naturally, {z | p (z) <    }.)

now suppose p is a density, i.e., satis   esr p(x) dx = 1. the function m (z) = p (   z)

is called the moment generating function of the density. it gets its name from the fact
that the moments of the density can be found from the derivatives of the moment
generating function, evaluated at z = 0, e.g.,

   m (0) = e v,

   2m (0) = e vvt ,

where v is a random variable with density p.

the function log m (z), which is convex, is called the cumulant generating function
for p, since its derivatives give the cumulants of the density. for example, the    rst
and second derivatives of the cumulant generating function, evaluated at zero, are
the mean and covariance of the associated random variable:

    log m (0) = e v,

   2 log m (0) = e(v     e v)(v     e v)t .

integration of log-concave functions
in some special cases log-concavity is preserved by integration. if f : rn  rm     r
is log-concave, then

g(x) =z f (x, y) dy

is a log-concave function of x (on rn). (the integration here is over rm.) a proof
of this result is not simple; see the references.

this result has many important consequences, some of which we describe in
the rest of this section. it implies, for example, that marginal distributions of log-
concave id203 densities are log-concave. it also implies that log-concavity is
closed under convolution, i.e., if f and g are log-concave on rn, then so is the
convolution

(f     g)(x) =z f (x     y)g(y) dy.

(to see this, note that g(y) and f (x   y) are log-concave in (x, y), hence the product
f (x     y)g(y) is; then the integration result applies.)

3.5 log-concave and log-convex functions

107

suppose c     rn is a convex set and w is a random vector in rn with log-

concave id203 density p. then the function

is log-concave in x. to see this, express f as

f (x) = prob(x + w     c)

where g is de   ned as

f (x) =z g(x + w)p(w) dw,
g(u) =(cid:26) 1 u     c

0 u 6    c,

(which is log-concave) and apply the integration result.

example 3.42 the cumulative distribution function of a id203 density function
f : rn     r is de   ned as

f (x) = prob(w (cid:22) x) =z xn

             z x1

      

f (z) dz1        dzn,

if f is log-concave, then f is log-
where w is a random variable with density f .
concave. we have already encountered a special case: the cumulative distribution
function of a gaussian random variable,

f (x) =

1

   2  z x

      

e   t2/2 dt,

is log-concave. (see example 3.39 and exercise 3.54.)

example 3.43 yield function. let x     rn denote the nominal or target value of a
set of parameters of a product that is manufactured. variation in the manufacturing
process causes the parameters of the product, when manufactured, to have the value
x + w, where w     rn is a random vector that represents manufacturing variation,
and is usually assumed to have zero mean. the yield of the manufacturing process,
as a function of the nominal parameter values, is given by

y (x) = prob(x + w     s),

where s     rn denotes the set of acceptable parameter values for the product, i.e.,
the product speci   cations.

if the density of the manufacturing error w is log-concave (for example, gaussian) and
the set s of product speci   cations is convex, then the yield function y is log-concave.
this implies that the   -yield region, de   ned as the set of nominal parameters for
which the yield exceeds   , is convex. for example, the 95% yield region

{x | y (x)     0.95} = {x | log y (x)     log 0.95}
is convex, since it is a superlevel set of the concave function log y .

108

3 convex functions

example 3.44 volume of polyhedron. let a     rm  n. de   ne

pu = {x     rn | ax (cid:22) u}.
then its volume vol pu is a log-concave function of u.

to prove this, note that the function

is log-concave. by the integration result, we conclude that

otherwise,

0

  (x, u) =(cid:26) 1 ax (cid:22) u
z   (x, u) dx = vol pu

is log-concave.

3.6 convexity with respect to generalized inequalities

we now consider generalizations of the notions of monotonicity and convexity, using
generalized inequalities instead of the usual ordering on r.

3.6.1 monotonicity with respect to a generalized inequality

suppose k     rn is a proper cone with associated generalized inequality (cid:22)k. a
function f : rn     r is called k-nondecreasing if

and k-increasing if

x (cid:22)k y =    f (x)     f (y),

x (cid:22)k y, x 6= y =    f (x) < f (y).

we de   ne k-nonincreasing and k-decreasing functions in a similar way.

example 3.45 monotone vector functions. a function f : rn     r is nondecreasing
with respect to rn

+ if and only if

x1     y1, . . . , xn     yn =    f (x)     f (y)

for all x, y. this is the same as saying that f , when restricted to any component xi
(i.e., xi is considered the variable while xj for j 6= i are    xed), is nondecreasing.

example 3.46 matrix monotone functions. a function f : sn     r is called ma-
trix monotone (increasing, decreasing) if it is monotone with respect to the posi-
tive semide   nite cone. some examples of matrix monotone functions of the variable
x     sn:

3.6 convexity with respect to generalized inequalities

109

    tr(w x), where w     sn, is matrix nondecreasing if w (cid:23) 0, and matrix in-
creasing if w     0 (it is matrix nonincreasing if w (cid:22) 0, and matrix decreasing
if w     0).

    tr(x    1) is matrix decreasing on sn
    det x is matrix increasing on sn

++.

++, and matrix nondecreasing on sn
+.

gradient conditions for monotonicity
recall that a di   erentiable function f : r     r, with convex (i.e., interval) domain,
is nondecreasing if and only if f    (x)     0 for all x     dom f , and increasing if
f    (x) > 0 for all x     dom f (but the converse is not true). these conditions
are readily extended to the case of monotonicity with respect to a generalized
inequality. a di   erentiable function f , with convex domain, is k-nondecreasing if
and only if

(3.24)
for all x     dom f . note the di   erence with the simple scalar case: the gradi-
ent must be nonnegative in the dual inequality. for the strict case, we have the
following: if

   f (x) (cid:23)k     0

   f (x)    k     0

(3.25)
for all x     dom f , then f is k-increasing. as in the scalar case, the converse is
not true.
let us prove these    rst-order conditions for monotonicity. first, assume that
f satis   es (3.24) for all x, but is not k-nondecreasing, i.e., there exist x, y with
x (cid:22)k y and f (y) < f (x). by di   erentiability of f there exists a t     [0, 1] with

d
dt

f (x + t(y     x)) =    f (x + t(y     x))t (y     x) < 0.

since y     x     k this means

   f (x + t(y     x)) 6    k    ,

which contradicts our assumption that (3.24) is satis   ed everywhere. in a similar
way it can be shown that (3.25) implies f is k-increasing.

it is also straightforward to see that it is necessary that (3.24) hold everywhere.
assume (3.24) does not hold for x = z. by the de   nition of dual cone this means
there exists a v     k with
now consider h(t) = f (z + tv) as a function of t. we have h   (0) =    f (z)t v < 0,
and therefore there exists t > 0 with h(t) = f (z + tv) < h(0) = f (z), which means
f is not k-nondecreasing.

   f (z)t v < 0.

3.6.2 convexity with respect to a generalized inequality

suppose k     rm is a proper cone with associated generalized inequality (cid:22)k. we
say f : rn     rm is k-convex if for all x, y, and 0            1,

f (  x + (1       )y) (cid:22)k   f (x) + (1       )f (y).

110

3 convex functions

the function is strictly k-convex if

f (  x + (1       )y)    k   f (x) + (1       )f (y)

for all x 6= y and 0 <    < 1. these de   nitions reduce to ordinary convexity and
strict convexity when m = 1 (and k = r+).

example 3.47 convexity with respect to componentwise inequality. a function f :
rn     rm is convex with respect to componentwise inequality (i.e., the generalized
inequality induced by rm

+ ) if and only if for all x, y and 0            1,

f (  x + (1       )y) (cid:22)   f (x) + (1       )f (y),

i.e., each component fi is a convex function. the function f is strictly convex with
respect to componentwise inequality if and only if each component fi is strictly con-
vex.

example 3.48 matrix convexity. suppose f is a symmetric matrix valued function,
i.e., f : rn     sm. the function f is convex with respect to matrix inequality if

f (  x + (1       )y) (cid:22)   f (x) + (1       )f (y)

for any x and y, and for        [0, 1]. this is sometimes called matrix convexity. an
equivalent de   nition is that the scalar function zt f (x)z is convex for all vectors z.
(this is often a good way to prove matrix convexity). a matrix function is strictly
matrix convex if

f (  x + (1       )y)       f (x) + (1       )f (y)

when x 6= y and 0 <    < 1, or, equivalently, if zt f z is strictly convex for every z 6= 0.
some examples:

    the function f (x) = xx t where x     rn  m is matrix convex, since for
   xed z the function zt xx t z = kx t zk2
2 is a convex quadratic function of (the
components of) x. for the same reason, f (x) = x 2 is matrix convex on sn.
++ for 1     p     2 or    1     p     0, and

    the function x p is matrix convex on sn

matrix concave for 0     p     1.

    the function f (x) = ex is not matrix convex on sn, for n     2.

many of the results for convex functions have extensions to k-convex functions.
as a simple example, a function is k-convex if and only if its restriction to any
line in its domain is k-convex. in the rest of this section we list a few results for
k-convexity that we will use later; more results are explored in the exercises.

dual characterization of k-convexity
a function f is k-convex if and only if for every w (cid:23)k     0, the (real-valued) function
wt f is convex (in the ordinary sense); f is strictly k-convex if and only if for every
nonzero w (cid:23)k     0 the function wt f is strictly convex. (these follow directly from
the de   nitions and properties of dual inequality.)

3.6 convexity with respect to generalized inequalities

111

di   erentiable k-convex functions

a di   erentiable function f is k-convex if and only if its domain is convex, and for
all x, y     dom f ,

f (y) (cid:23)k f (x) + df (x)(y     x).

(here df (x)     rm  n is the derivative or jacobian matrix of f at x; see   a.4.1.)
the function f is strictly k-convex if and only if for all x, y     dom f with x 6= y,

f (y)    k f (x) + df (x)(y     x).

composition theorem

many of the results on composition can be generalized to k-convexity. for example,
if g : rn     rp is k-convex, h : rp     r is convex, and   h (the extended-value
extension of h) is k-nondecreasing, then h     g is convex. this generalizes the fact
that a nondecreasing convex function of a convex function is convex. the condition
that   h be k-nondecreasing implies that dom h     k = dom h.

example 3.49 the quadratic matrix function g : rm  n     sn de   ned by

g(x) = x t ax + bt x + x t b + c,

where a     sm, b     rm  n, and c     sn, is convex when a (cid:23) 0.
the function h : sn     r de   ned by h(y ) =     log det(   y ) is convex and increasing
on dom h =    sn
by the composition theorem, we conclude that

++.

f (x) =     log det(   (x t ax + bt x + x t b + c))

is convex on

dom f = {x     rm  n | x t ax + bt x + x t b + c     0}.

this generalizes the fact that

is convex on

provided a     0.

    log(   (ax2 + bx + c))

{x     r | ax2 + bx + c < 0},

112

3 convex functions

bibliography

the standard reference on convex analysis is rockafellar [roc70]. other books on convex
functions are stoer and witzgall [sw70], roberts and varberg [rv73], van tiel [vt84],
hiriart-urruty and lemar  echal [hul93], ekeland and t  emam [et99], borwein and lewis
[bl00], florenzano and le van [fl01], barvinok [bar02], and bertsekas, nedi  c, and
ozdaglar [ber03]. most nonid135 texts also include chapters on convex
functions (see, for example, mangasarian [man94], bazaraa, sherali, and shetty [bss93],
bertsekas [ber99], polyak [pol87], and peressini, sullivan, and uhl [psu88]).

jensen   s inequality appears in [jen06]. a general study of inequalities, in which jensen   s
inequality plays a central role, is presented by hardy, littlewood, and p  olya [hlp52],
and beckenbach and bellman [bb65].

the term perspective function is from hiriart-urruty and lemar  echal [hul93, volume
1, page 100]. for the de   nitions in example 3.19 (relative id178 and kullback-leibler
divergence), and the related exercise 3.13, see cover and thomas [ct91].

some important early references on quasiconvex functions (as well as other extensions of
convexity) are nikaid  o [nik54], mangasarian [man94, chapter 9], arrow and enthoven
[ae61], ponstein [pon67], and luenberger [lue68]. for a more comprehensive reference
list, we refer to bazaraa, sherali, and shetty [bss93, page 126].

pr  ekopa [pr  e80] gives a survey of log-concave functions. log-convexity of the laplace
transform is mentioned in barndor   -nielsen [bn78,   7]. for a proof of the integration
result of log-concave functions, see pr  ekopa [pr  e71, pr  e73].

generalized inequalities are used extensively in the recent literature on cone programming,
starting with nesterov and nemirovski [nn94, page 156]; see also ben-tal and nemirovski
[btn01] and the references at the end of chapter 4. convexity with respect to generalized
inequalities also appears in the work of luenberger [lue69,   8.2] and isii [isi64]. matrix
monotonicity and matrix convexity are attributed to l  owner [l  ow34], and are discussed
in detail by davis [dav63], roberts and varberg [rv73, page 216] and marshall and
olkin [mo79,   16e]. for the result on convexity and concavity of the function x p in
example 3.48, see bondar [bon94, theorem 16.1]. for a simple example that demonstrates
that ex is not matrix convex, see marshall and olkin [mo79, page 474].

113

exercises

exercises

de   nition of convexity

3.1 suppose f : r     r is convex, and a, b     dom f with a < b.

(a) show that

f (x)    

b     x
b     a

f (a) +

x     a
b     a

f (b)

for all x     [a, b].

(b) show that

f (x)     f (a)

x     a

   

f (b)     f (a)

b     a

   

f (b)     f (x)

b     x

for all x     (a, b). draw a sketch that illustrates this inequality.
(c) suppose f is di   erentiable. use the result in (b) to show that

f    (a)    

f (b)     f (a)

b     a

    f    (b).

note that these inequalities also follow from (3.2):

f (b)     f (a) + f    (a)(b     a),

f (a)     f (b) + f    (b)(a     b).

(d) suppose f is twice di   erentiable. use the result in (c) to show that f       (a)     0 and

f       (b)     0.

3.2 level sets of convex, concave, quasiconvex, and quasiconcave functions. some level sets

of a function f are shown below. the curve labeled 1 shows {x | f (x) = 1}, etc.

3
2

1

could f be convex (concave, quasiconvex, quasiconcave)? explain your answer. repeat
for the level curves shown below.

1 2 3

4

5

6

114

3 convex functions

3.3 inverse of an increasing convex function. suppose f : r     r is increasing and convex
on its domain (a, b). let g denote its inverse, i.e., the function with domain (f (a), f (b))
and g(f (x)) = x for a < x < b. what can you say about convexity or concavity of g?

3.4 [rv73, page 15] show that a continuous function f : rn     r is convex if and only if for
every line segment, its average value on the segment is less than or equal to the average
of its values at the endpoints of the segment: for every x, y     rn,

z 1

0

f (x +   (y     x)) d      

f (x) + f (y)

2

.

3.5 [rv73, page 22] running average of a convex function. suppose f : r     r is convex,

with r+     dom f . show that its running average f , de   ned as

f (x) =

f (t) dt,

dom f = r++,

1

xz x

0

is convex. hint. for each s, f (sx) is convex in x, sor 1

0

3.6 functions and epigraphs. when is the epigraph of a function a halfspace? when is the
epigraph of a function a convex cone? when is the epigraph of a function a polyhedron?
3.7 suppose f : rn     r is convex with dom f = rn, and bounded above on rn. show that

f (sx) ds is convex.

f is constant.

3.8 second-order condition for convexity. prove that a twice di   erentiable function f is convex
if and only if its domain is convex and    2f (x) (cid:23) 0 for all x     dom f . hint. first consider
the case f : r     r. you can use the    rst-order condition for convexity (which was proved
on page 70).
3.9 second-order conditions for convexity on an a   ne set. let f     rn  m,   x     rn. the
restriction of f : rn     r to the a   ne set {f z +   x | z     rm} is de   ned as the function
  f : rm     r with

  f (z) = f (f z +   x),

dom   f = {z | f z +   x     dom f}.

suppose f is twice di   erentiable with a convex domain.
(a) show that   f is convex if and only if for all z     dom   f
f t   2f (f z +   x)f (cid:23) 0.

(b) suppose a     rp  n is a matrix whose nullspace is equal to the range of f , i.e.,
af = 0 and rank a = n     rank f . show that   f is convex if and only if for all
z     dom   f there exists a        r such that

   2f (f z +   x) +   at a (cid:23) 0.

hint. use the following result: if b     sn and a     rp  n, then xt bx     0 for all
x     n (a) if and only if there exists a    such that b +   at a (cid:23) 0.

3.10 an extension of jensen   s inequality. one interpretation of jensen   s inequality is that
randomization or dithering hurts, i.e., raises the average value of a convex function: for
f convex and v a zero mean random variable, we have e f (x0 + v)     f (x0). this leads
to the following conjecture. if f is convex, then the larger the variance of v, the larger
e f (x0 + v).

(a) give a counterexample that shows that this conjecture is false. find zero mean
random variables v and w, with var(v) > var(w), a convex function f , and a point
x0, such that e f (x0 + v) < e f (x0 + w).

exercises

115

(b) the conjecture is true when v and w are scaled versions of each other. show that
e f (x0 + tv) is monotone increasing in t     0, when f is convex and v is zero mean.
3.11 monotone mappings. a function    : rn     rn is called monotone if for all x, y     dom   ,

(  (x)       (y))t (x     y)     0.

(note that    monotone    as de   ned here is not the same as the de   nition given in   3.6.1.
both de   nitions are widely used.) suppose f : rn     r is a di   erentiable convex function.
show that its gradient    f is monotone.
is the converse true, i.e., is every monotone
mapping the gradient of a convex function?

3.12 suppose f : rn     r is convex, g : rn     r is concave, dom f = dom g = rn, and
for all x, g(x)     f (x). show that there exists an a   ne function h such that for all x,
g(x)     h(x)     f (x). in other words, if a concave function g is an underestimator of a
convex function f , then we can    t an a   ne function between f and g.

3.13 id181 and the information inequality. let dkl be the kullback-
leibler divergence, as de   ned in (3.17). prove the information inequality: dkl(u, v)     0
for all u, v     rn
hint. the id181 can be expressed as

++. also show that dkl(u, v) = 0 if and only if u = v.

dkl(u, v) = f (u)     f (v)        f (v)t (u     v),

i=1 vi log vi is the negative id178 of v.

where f (v) =pn

3.14 convex-concave functions and saddle-points. we say the function f : rn    rm     r
is convex-concave if f (x, z) is a concave function of z, for each    xed x, and a convex
function of x, for each    xed z. we also require its domain to have the product form
dom f = a    b, where a     rn and b     rm are convex.
(a) give a second-order condition for a twice di   erentiable function f : rn    rm     r
(b) suppose that f : rn  rm     r is convex-concave and di   erentiable, with    f (  x,   z) =

to be convex-concave, in terms of its hessian    2f (x, z).

0. show that the saddle-point property holds: for all x, z, we have

show that this implies that f satis   es the strong max-min property:

f (  x, z)     f (  x,   z)     f (x,   z).

sup

z

inf
x

f (x, z) = inf
x

sup

f (x, z)

z

(and their common value is f (  x,   z)).

(c) now suppose that f : rn    rm     r is di   erentiable, but not necessarily convex-

concave, and the saddle-point property holds at   x,   z:

f (  x, z)     f (  x,   z)     f (x,   z)

for all x, z. show that    f (  x,   z) = 0.

examples

3.15 a family of concave utility functions. for 0 <        1 let

u  (x) =

x       1

  

,

with dom u   = r+. we also de   ne u0(x) = log x (with dom u0 = r++).

(a) show that for x > 0, u0(x) = lim     0 u  (x).

116

3 convex functions

(b) show that u   are concave, monotone increasing, and all satisfy u  (1) = 0.

these functions are often used in economics to model the bene   t or utility of some quantity
of goods or money. concavity of u   means that the marginal utility (i.e., the increase
in utility obtained for a    xed increase in the goods) decreases as the amount of goods
increases. in other words, concavity models the e   ect of satiation.

3.16 for each of the following functions determine whether it is convex, concave, quasiconvex,

or quasiconcave.
(a) f (x) = ex     1 on r.
(b) f (x1, x2) = x1x2 on r2
(c) f (x1, x2) = 1/(x1x2) on r2
(d) f (x1, x2) = x1/x2 on r2
(e) f (x1, x2) = x2
(f) f (x1, x2) = x  

++.

++.

1/x2 on r    r++.
1 x1     

++.

2

, where 0            1, on r2

++.

3.17 suppose p < 1, p 6= 0. show that the function

f (x) =  nxi=1

i!1/p

xp

++ is concave. this includes as special cases f (x) = (pn

)2 and
i=1 1/xi)   1. hint. adapt the proofs for the log-sum-exp

i=1 x1/2

i

3.18 adapt the proof of concavity of the log-determinant function in   3.1.5 to show the follow-

with dom f = rn

ing.

(b) f (x) = (det x)1/n is concave on dom f = sn

function and the geometric mean in   3.1.5.

the harmonic mean f (x) = (pn
(a) f (x) = tr(cid:0)x    1(cid:1) is convex on dom f = sn
(a) show that f (x) = pr
f (x) =pk

i=1 x[i] is convex on rn.)

(b) let t (x,   ) denote the trigonometric polynomial

++.

++.

3.19 nonnegative weighted sums and integrals.

i=1   ix[i] is a convex function of x, where   1       2               
  r     0, and x[i] denotes the ith largest component of x. (you can use the fact that

t (x,   ) = x1 + x2 cos    + x3 cos 2   +        + xn cos(n     1)  .

show that the function

f (x) =    z 2  

0

log t (x,   ) d  

is convex on {x     rn | t (x,   ) > 0, 0            2  }.

3.20 composition with an a   ne function. show that the following functions f : rn     r are

convex.
(a) f (x) = kax     bk, where a     rm  n, b     rm, and k    k is a norm on rm.
(b) f (x) =     (det(a0 + x1a1 +        + xnan))1/m, on {x | a0 + x1a1 +        + xnan     0},
(c) f (x) = tr (a0 + x1a1 +        + xnan)   1, on {x | a0 +x1a1 +      +xnan     0}, where

where ai     sm.
ai     sm. (use the fact that tr(x    1) is convex on sm

++; see exercise 3.18.)

exercises

117

3.21 pointwise maximum and supremum. show that the following functions f : rn     r are

on rm.

convex.
(a) f (x) = maxi=1,...,k ka(i)x     b(i)k, where a(i)     rm  n, b(i)     rm and k    k is a norm
i=1 |x|[i] on rn, where |x| denotes the vector with |x|i = |xi| (i.e., |x| is
the absolute value of x, componentwise), and |x|[i] is the ith largest component of
|x|. in other words, |x|[1], |x|[2], . . . , |x|[n] are the absolute values of the components
of x, sorted in nonincreasing order.

(b) f (x) =pr

3.22 composition rules. show that the following functions are convex.

i=1 eat
i=1 eyi ) is convex.

i x+bi )) on dom f = {x | pm

(a) f (x) =     log(    log(pm
i x+bi < 1}. you can
use the fact that log(pn
(b) f (x, u, v) =       uv     xt x on dom f = {(x, u, v) | uv > xt x, u, v > 0}. use the
fact that xt x/u is convex in (x, u) for u > 0, and that       x1x2 is convex on r2
(c) f (x, u, v) =     log(uv     xt x) on dom f = {(x, u, v) | uv > xt x, u, v > 0}.
(d) f (x, t) =    (tp    kxkp

p)1/p where p > 1 and dom f = {(x, t) | t     kxkp}. you can use
p/up   1 is convex in (x, u) for u > 0 (see exercise 3.23), and that

i=1 eat

++.

+ (see exercise 3.16).

p) where p > 1 and dom f = {(x, t) | t > kxkp}. you can

p/up   1 is convex in (x, u) for u > 0 (see exercise 3.23).

the fact that kxkp
   x1/py1   1/p is convex on r2
use the fact that kxkp
3.23 perspective of a function.

(e) f (x, t) =     log(tp     kxkp

(a) show that for p > 1,

f (x, t) = |x1|p +        + |xn|p

tp   1

= kxkp
p
tp   1

is convex on {(x, t) | t > 0}.

(b) show that

f (x) = kax + bk2
ct x + d

2

is convex on {x | ct x + d > 0}, where a     rm  n, b     rm, c     rn and d     r.

3.24 some functions on the id203 simplex. let x be a real-valued random variable which
takes values in {a1, . . . , an} where a1 < a2 <        < an, with prob(x = ai) = pi,
i = 1, . . . , n. for each of the following functions of p (on the id203 simplex {p    
+ | 1t p = 1}), determine if the function is convex, concave, quasiconvex, or quasicon-
rn
cave.

(a) e x.
(b) prob(x       ).
(c) prob(       x       ).

(d) pn

i=1 pi log pi, the negative id178 of the distribution.

(e) var x = e(x     e x)2.
(f) quartile(x) = inf{   | prob(x       )     0.25}.
(g) the cardinality of the smallest set a     {a1, . . . , an} with id203     90%. (by

cardinality we mean the number of elements in a.)

(h) the minimum width interval that contains 90% of the id203, i.e.,

inf {          | prob(       x       )     0.9} .

118

3 convex functions

3.25 maximum id203 distance between distributions. let p, q     rn represent two proba-
bility distributions on {1, . . . , n} (so p, q (cid:23) 0, 1t p = 1t q = 1). we de   ne the maximum
id203 distance dmp(p, q) between p and q as the maximum di   erence in id203
assigned by p and q, over all events:

dmp(p, q) = max{| prob(p, c)     prob(q, c)| | c     {1, . . . , n}}.

here prob(p, c) is the id203 of c, under the distribution p, i.e., prob(p, c) =

pi   c pi.
find a simple expression for dmp, involving kp     qk1 =pn

i=1 |pi     qi|, and show that dmp
is a convex function on rn    rn. (its domain is {(p, q) | p, q (cid:23) 0, 1t p = 1t q = 1}, but
it has a natural extension to all of rn    rn.)
3.26 more functions of eigenvalues. let   1(x)       2(x)                  n(x) denote the eigenvalues
of a matrix x     sn. we have already seen several functions of the eigenvalues that are
convex or concave functions of x.

  n(x) is concave.

++ (exercise 3.18).

i=1 1/  i(x), is convex on sn

    the maximum eigenvalue   1(x) is convex (example 3.10). the minimum eigenvalue
    the sum of the eigenvalues (or trace), tr x =   1(x) +        +   n(x), is linear.
    the sum of the inverses of the eigenvalues (or trace of the inverse), tr(x    1) =
i=1   i(x))1/n, and the
i=1 log   i(x), are concave

pn
    the geometric mean of the eigenvalues, (det x)1/n = (qn
logarithm of the product of the eigenvalues, log det x =pn
(a) sum of k largest eigenvalues. show thatpk

in this problem we explore some more functions of eigenvalues, by exploiting variational
characterizations.

i=1   i(x) is convex on sn. hint. [hj85,

page 191] use the variational characterization

++ (exercise 3.18 and page 74).

on x     sn

kxi=1

  i(x) = sup{tr(v t xv ) | v     rn  k, v t v = i}.

(b) geometric mean of k smallest eigenvalues. show that (qn

++. hint. [mo79, page 513] for x     0, we have

cave on sn

i=n   k+1   i(x))1/k is con-

  nyi=n   k+1

  i(x)!1/k

=

1
k

inf{tr(v t xv ) | v     rn  k, det v t v = 1}.

(c) log of product of k smallest eigenvalues. show thatpn

++. hint. [mo79, page 513] for x     0,

on sn

i=n   k+1 log   i(x) is concave

nyi=n   k+1

  i(x) = inf( kyi=1

v     rn  k, v t v = i) .

3.27 diagonal elements of cholesky factor. each x     sn

++ has a unique cholesky factorization
x = llt , where l is lower triangular, with lii > 0. show that lii is a concave function
of x (with domain sn
hint. lii can be expressed as lii = (w     zt y    1z)1/2, where

++).

(v t xv )ii(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:20) y
zt w (cid:21)

z

is the leading i    i submatrix of x.

exercises

119

operations that preserve convexity

3.28 expressing a convex function as the pointwise supremum of a family of a   ne functions.
in this problem we extend the result proved on page 83 to the case where dom f 6= rn.
let f : rn     r be a convex function. de   ne   f : rn     r as the pointwise supremum of
all a   ne functions that are global underestimators of f :

  f (x) = sup{g(x) | g a   ne, g(z)     f (z) for all z}.

(a) show that f (x) =   f (x) for x     int dom f .
(b) show that f =   f if f is closed (i.e., epi f is a closed set; see   a.3.3).
dom f = rn, is called piecewise-linear if there exists a partition of rn as

3.29 representation of piecewise-linear convex functions. a function f : rn     r, with

rn = x1     x2                xl,

where int xi 6=     and int xi     int xj =     for i 6= j, and a family of a   ne functions
1 x + b1, . . . , at
lx + bl such that f (x) = at
at
show that this means that f (x) = max{at
f : rn     r is de   ned as

3.30 convex hull or envelope of a function. the convex hull or convex envelope of a function

i x + bi for x     xi.
1 x + b1, . . . , at

lx + bl}.

g(x) = inf{t | (x, t)     conv epi f}.

geometrically, the epigraph of g is the convex hull of the epigraph of f .
show that g is the largest convex underestimator of f . in other words, show that if h is
convex and satis   es h(x)     f (x) for all x, then h(x)     g(x) for all x.

3.31 [roc70, page 35] largest homogeneous underestimator. let f be a convex function. de   ne

the function g as

g(x) = inf
  >0

f (  x)

  

.

(a) show that g is homogeneous (g(tx) = tg(x) for all t     0).
(b) show that g is the largest homogeneous underestimator of f : if h is homogeneous

and h(x)     f (x) for all x, then we have h(x)     g(x) for all x.

(c) show that g is convex.

3.32 products and ratios of convex functions. in general the product or ratio of two convex
functions is not convex. however, there are some results that apply to functions on r.
prove the following.

(a) if f and g are convex, both nondecreasing (or nonincreasing), and positive functions

on an interval, then f g is convex.

(b) if f , g are concave, positive, with one nondecreasing and the other nonincreasing,

then f g is concave.

(c) if f is convex, nondecreasing, and positive, and g is concave, nonincreasing, and

positive, then f /g is convex.

3.33 direct proof of perspective theorem. give a direct proof that the perspective function g,
as de   ned in   3.2.6, of a convex function f is convex: show that dom g is a convex set,
and that for (x, t), (y, s)     dom g, and 0            1, we have

g(  x + (1       )y,   t + (1       )s)       g(x, t) + (1       )g(y, s).

3.34 the minkowski function. the minkowski function of a convex set c is de   ned as

mc (x) = inf{t > 0 | t   1x     c}.

120

3 convex functions

(a) draw a picture giving a geometric interpretation of how to    nd mc (x).
(b) show that mc is homogeneous, i.e., mc (  x) =   mc (x) for        0.
(c) what is dom mc ?

(d) show that mc is a convex function.
(e) suppose c is also closed, bounded, symmetric (if x     c then    x     c), and has
nonempty interior. show that mc is a norm. what is the corresponding unit ball?
3.35 support function calculus. recall that the support function of a set c     rn is de   ned as

sc (y) = sup{yt x | x     c}. on page 81 we showed that sc is a convex function.
(a) show that sb = sconv b.

(b) show that sa+b = sa + sb.
(c) show that sa   b = max{sa, sb}.
(d) let b be closed and convex. show that a     b if and only if sa(y)     sb(y) for all

y.

conjugate functions

3.36 derive the conjugates of the following functions.

(a) max function. f (x) = maxi=1,...,n xi on rn.

(b) sum of largest elements. f (x) =pr

i=1 x[i] on rn.

(c) piecewise-linear function on r. f (x) = maxi=1,...,m(aix + bi) on r. you can
assume that the ai are sorted in increasing order, i.e., a1                am, and that none
of the functions aix + bi is redundant, i.e., for each k there is at least one x with
f (x) = akx + bk.

(d) power function. f (x) = xp on r++, where p > 1. repeat for p < 0.

(e) negative geometric mean. f (x) =    (q xi)1/n on rn

(f) negative generalized logarithm for second-order cone. f (x, t) =     log(t2     xt x) on

++.

{(x, t)     rn    r | kxk2 < t}.

3.37 show that the conjugate of f (x) = tr(x    1) with dom f = sn

++ is given by

f    (y ) =    2 tr(   y )1/2,
hint. the gradient of f is    f (x) =    x    2.

dom f     =    sn
+.

3.38 young   s inequality. let f : r     r be an increasing function, with f (0) = 0, and let g be

its inverse. de   ne f and g as

f (x) =z x

0

f (a) da,

g(y) =z y

0

g(a) da.

show that f and g are conjugates. give a simple graphical interpretation of young   s
inequality,

3.39 properties of conjugate functions.

xy     f (x) + g(y).

(a) conjugate of convex plus a   ne function. de   ne g(x) = f (x) + ct x + d, where f is

convex. express g    in terms of f     (and c, d).

(b) conjugate of perspective. express the conjugate of the perspective of a convex

function f in terms of f    .

exercises

121

(c) conjugate and minimization. let f (x, z) be convex in (x, z) and de   ne g(x) =

inf z f (x, z). express the conjugate g    in terms of f    .
as an application, express the conjugate of g(x) = inf z{h(z) | az + b = x}, where h
is convex, in terms of h   , a, and b.
(d) conjugate of conjugate. show that the conjugate of the conjugate of a closed convex
function is itself: f = f        if f is closed and convex. (a function is closed if its
epigraph is closed; see   a.3.3.) hint. show that f        is the pointwise supremum of
all a   ne global underestimators of f . then apply the result of exercise 3.28.

3.40 gradient and hessian of conjugate function. suppose f : rn     r is convex and twice
continuously di   erentiable. suppose   y and   x are related by   y =    f (  x), and that    2f (  x)    
0.
(a) show that    f    (  y) =   x.
(b) show that    2f    (  y) =    2f (  x)   1.

3.41 conjugate of negative normalized id178. show that the conjugate of the negative nor-

malized id178

with dom f = rn

++, is given by

f (x) =

nxi=1
f    (y) =(cid:26) 0

xi log(xi/1t x),

i=1 eyi     1

+    otherwise.

pn

quasiconvex functions

3.42 approximation width. let f0, . . . , fn : r     r be given continuous functions. we consider
the problem of approximating f0 as a linear combination of f1, . . . , fn. for x     rn, we
say that f = x1f1 +        + xnfn approximates f0 with tolerance    > 0 over the interval
[0, t ] if |f (t)     f0(t)|        for 0     t     t . now we choose a    xed tolerance    > 0 and de   ne
the approximation width as the largest t such that f approximates f0 over the interval
[0, t ]:

w (x) = sup{t | |x1f1(t) +        + xnfn(t)     f0(t)|        for 0     t     t}.

show that w is quasiconcave.

3.43 first-order condition for quasiconvexity. prove the    rst-order condition for quasiconvexity
given in   3.4.3: a di   erentiable function f : rn     r, with dom f convex, is quasiconvex
if and only if for all x, y     dom f ,

f (y)     f (x) =       f (x)t (y     x)     0.

it su   ces to prove the result for a function on r; the general result follows by

hint.
restriction to an arbitrary line.

3.44 second-order conditions for quasiconvexity.

in this problem we derive alternate repre-
sentations of the second-order conditions for quasiconvexity given in   3.4.3. prove the
following.
(a) a point x     dom f satis   es (3.21) if and only if there exists a    such that

it satis   es (3.22) for all y 6= 0 if and only if there exists a    such

   2f (x) +      f (x)   f (x)t (cid:23) 0.

hint. we can assume without loss of generality that    2f (x) is diagonal.

   2f (x) +      f (x)   f (x)t     0.

(3.26)

(3.27)

122

3 convex functions

(b) a point x     dom f satis   es (3.21) if and only if either    f (x) = 0 and    2f (x) (cid:23) 0,

or    f (x) 6= 0 and the matrix

h(x) =(cid:20)    2f (x)    f (x)

   f (x)t

0

(cid:21)

has exactly one negative eigenvalue. it satis   es (3.22) for all y 6= 0 if and only if
h(x) has exactly one nonpositive eigenvalue.
hint. you can use the result of part (a). the following result, which follows from
the eigenvalue interlacing theorem in id202, may also be useful: if b     sn
and a     rn, then

  n(cid:18)(cid:20) b a

0 (cid:21)(cid:19)       n(b).

at

3.45 use the    rst and second-order conditions for quasiconvexity given in   3.4.3 to verify

quasiconvexity of the function f (x) =    x1x2, with dom f = r2

3.46 quasilinear functions with domain rn. a function on r that is quasilinear (i.e., qua-
siconvex and quasiconcave) is monotone, i.e., either nondecreasing or nonincreasing. in
this problem we consider a generalization of this result to functions on rn.
suppose the function f : rn     r is quasilinear and continuous with dom f = rn. show
that it can be expressed as f (x) = g(at x), where g : r     r is monotone and a     rn.
in other words, a quasilinear function with domain rn must be a monotone function of
a linear function. (the converse is also true.)

++.

log-concave and log-convex functions

3.47 suppose f : rn     r is di   erentiable, dom f is convex, and f (x) > 0 for all x     dom f .

show that f is log-concave if and only if for all x, y     dom f ,

f (y)

f (x)     exp(cid:18)   f (x)t (y     x)

f (x)

(cid:19) .

3.48 show that if f : rn     r is log-concave and a     0, then the function g = f     a is

log-concave, where dom g = {x     dom f | f (x) > a}.

3.49 show that the following functions are log-concave.

(a) logistic function: f (x) = ex/(1 + ex) with dom f = r.

(b) harmonic mean:

f (x) =

1

1/x1 +        + 1/xn

,

dom f = rn

++.

(c) product over sum:

(d) determinant over trace:

f (x) = qn
i=1 xipn

i=1 xi

,

dom f = rn

++.

f (x) =

det x
tr x

,

dom f = sn

++.

exercises

123

3.50 coe   cients of a polynomial as a function of the roots. show that the coe   cients of a
polynomial with real negative roots are log-concave functions of the roots. in other words,
the functions ai : rn     r, de   ned by the identity

sn + a1(  )sn   1 +        + an   1(  )s + an(  ) = (s       1)(s       2)       (s       n),

are log-concave on    rn
hint. the function

++.

sk(x) = x1   i1<i2<      <ik   n

xi1 xi2        xik ,

with dom sk     rn
rn. it can be shown that s1/k

k

+ and 1     k     n, is called the kth elementary symmetric function on

is concave (see [ml57]).

3.51 [bl00, page 41] let p be a polynomial on r, with all its roots real. show that it is

log-concave on any interval on which it is positive.

3.52 [mo79,   3.e.2] log-convexity of moment functions. suppose f : r     r is nonnegative

with r+     dom f . for x     0 de   ne

  (x) =z    

0

uxf (u) du.

show that    is a log-convex function. (if x is a positive integer, and f is a id203
density function, then   (x) is the xth moment of the distribution.)
use this to show that the gamma function,

  (x) =z    

0

ux   1e   u du,

is log-convex for x     1.

3.53 suppose x and y are independent random vectors in rn, with log-concave id203
density functions f and g, respectively. show that the id203 density function of the
sum z = x + y is log-concave.

3.54 log-concavity of gaussian cumulative distribution function. the cumulative distribution

function of a gaussian random variable,

f (x) =

1

   2  z x

      

e   t2/2 dt,

is log-concave. this follows from the general result that the convolution of two log-concave
functions is log-concave. in this problem we guide you through a simple self-contained
proof that f is log-concave. recall that f is log-concave if and only if f       (x)f (x)     f    (x)2
for all x.
(a) verify that f       (x)f (x)     f    (x)2 for x     0. that leaves us the hard part, which is to
(b) verify that for any t and x we have t2/2        x2/2 + xt.
(c) using part (b) show that e   t2/2     ex2/2   xt. conclude that, for x < 0,

show the inequality for x < 0.

z x

      

e   t2/2 dt     ex2/2z x

      

e   xt dt.

(d) use part (c) to verify that f       (x)f (x)     f    (x)2 for x     0.

124

3 convex functions

3.55 log-concavity of the cumulative distribution function of a log-concave id203 density.
in this problem we extend the result of exercise 3.54. let g(t) = exp(   h(t)) be a di   er-
entiable log-concave id203 density function, and let

f (x) =z x

      

g(t) dt =z x

      

e   h(t) dt

be its cumulative distribution. we will show that f is log-concave, i.e.,
f       (x)f (x)     (f    (x))2 for all x.
(a) express the derivatives of f in terms of the function h. verify that f       (x)f (x)    
(b) assume that h   (x) < 0. use the inequality

(f    (x))2 if h   (x)     0.

it satis   es

(which follows from convexity of h), to show that

h(t)     h(x) + h   (x)(t     x)

z x

      

e   h(t) dt    

e   h(x)
   h   (x)

.

use this inequality to verify that f       (x)f (x)     (f    (x))2 if h   (x) < 0.

3.56 more log-concave densities. show that the following densities are log-concave.

(a) [mo79, page 493] the gamma density, de   ned by

f (x) =

    
  (  )

x     1e     x,

with dom f = r+. the parameters    and    satisfy        1,    > 0.

(b) [mo79, page 306] the dirichlet density

f (x) =

  (1t   )

  (  1)         (  n+1)

x  1   1
1

       x  n   1

n

 1    

xi!  n+1   1

nxi=1

with dom f = {x     rn

++ | 1t x < 1}. the parameter    satis   es    (cid:23) 1.

convexity with respect to a generalized inequality

3.57 show that the function f (x) = x    1 is matrix convex on sn
3.58 schur complement. suppose x     sn partitioned as

++.

bt c (cid:21) ,
x =(cid:20) a b

where a     sk. the schur complement of x (with respect to a) is s = c     bt a   1b
(see   a.5.5). show that the schur complement, viewed as a function from sn into sn   k,
is matrix concave on sn
3.59 second-order conditions for k-convexity. let k     rm be a proper convex cone, with
associated generalized inequality (cid:22)k . show that a twice di   erentiable function f : rn    
rm, with convex domain, is k-convex if and only if for all x     dom f and all y     rn,

++.

   2f (x)
   xi   xj

yiyj (cid:23)k 0,

nxi,j=1

i.e., the second derivative is a k-nonnegative bilinear form. (here    2f /   xi   xj     rm,
with components    2fk/   xi   xj, for k = 1, . . . , m; see   a.4.1.)

exercises

125

3.60 sublevel sets and epigraph of k-convex functions. let k     rm be a proper convex cone
with associated generalized inequality (cid:22)k , and let f : rn     rm. for        rm, the
  -sublevel set of f (with respect to (cid:22)k ) is de   ned as

c   = {x     rn | f (x) (cid:22)k   }.
the epigraph of f , with respect to (cid:22)k , is de   ned as the set

epik f = {(x, t)     rn+m | f (x) (cid:22)k t}.

show the following:

(a) if f is k-convex, then its sublevel sets c   are convex for all   .
(b) f is k-convex if and only if epik f is a convex set.

chapter 4

id76 problems

4.1 optimization problems

4.1.1 basic terminology

we use the notation

minimize
subject to

f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p

(4.1)

to describe the problem of    nding an x that minimizes f0(x) among all x that satisfy
the conditions fi(x)     0, i = 1, . . . , m, and hi(x) = 0, i = 1, . . . , p. we call x     rn
the optimization variable and the function f0 : rn     r the objective function or
cost function. the inequalities fi(x)     0 are called inequality constraints, and the
corresponding functions fi : rn     r are called the inequality constraint functions.
the equations hi(x) = 0 are called the equality constraints, and the functions
hi : rn     r are the equality constraint functions. if there are no constraints (i.e.,
m = p = 0) we say the problem (4.1) is unconstrained.
the set of points for which the objective and all constraint functions are de   ned,

d =

m\i=0

dom fi    

p\i=1

dom hi,

is called the domain of the optimization problem (4.1). a point x     d is feasible
if it satis   es the constraints fi(x)     0, i = 1, . . . , m, and hi(x) = 0, i = 1, . . . , p.
the problem (4.1) is said to be feasible if there exists at least one feasible point,
and infeasible otherwise. the set of all feasible points is called the feasible set or
the constraint set.

the optimal value p    of the problem (4.1) is de   ned as

p    = inf {f0(x) | fi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p} .

we allow p    to take on the extended values      . if the problem is infeasible, we
have p    =     (following the standard convention that the in   mum of the empty set

128

4 id76 problems

is    ). if there are feasible points xk with f0(xk)            as k        , then p    =       ,
and we say the problem (4.1) is unbounded below.

optimal and locally optimal points

we say x    is an optimal point, or solves the problem (4.1), if x    is feasible and
f0(x   ) = p   . the set of all optimal points is the optimal set, denoted

xopt = {x | fi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p, f0(x) = p   }.

if there exists an optimal point for the problem (4.1), we say the optimal value
is attained or achieved, and the problem is solvable.
if xopt is empty, we say
the optimal value is not attained or not achieved. (this always occurs when the
problem is unbounded below.) a feasible point x with f0(x)     p    +    (where
   > 0) is called   -suboptimal, and the set of all   -suboptimal points is called the
  -suboptimal set for the problem (4.1).

we say a feasible point x is locally optimal if there is an r > 0 such that

f0(x) = inf{f0(z) | fi(z)     0, i = 1, . . . , m,

hi(z) = 0, i = 1, . . . , p, kz     xk2     r},

or, in other words, x solves the optimization problem

minimize
subject to

f0(z)
fi(z)     0,
hi(z) = 0,
kz     xk2     r

i = 1, . . . , m
i = 1, . . . , p

with variable z. roughly speaking, this means x minimizes f0 over nearby points
in the feasible set. the term    globally optimal    is sometimes used for    optimal   
to distinguish between    locally optimal    and    optimal   . throughout this book,
however, optimal will mean globally optimal.

if x is feasible and fi(x) = 0, we say the ith inequality constraint fi(x)     0 is
active at x. if fi(x) < 0, we say the constraint fi(x)     0 is inactive. (the equality
constraints are active at all feasible points.) we say that a constraint is redundant
if deleting it does not change the feasible set.

example 4.1 we illustrate these de   nitions with a few simple unconstrained opti-
mization problems with variable x     r, and dom f0 = r++.

    f0(x) = 1/x: p    = 0, but the optimal value is not achieved.
    f0(x) =     log x: p    =       , so this problem is unbounded below.
    f0(x) = x log x: p    =    1/e, achieved at the (unique) optimal point x    = 1/e.

feasibility problems

if the objective function is identically zero, the optimal value is either zero (if the
feasible set is nonempty) or     (if the feasible set is empty). we call this the

4.1 optimization problems

129

feasibility problem, and will sometimes write it as

   nd
subject to

x
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p.

the feasibility problem is thus to determine whether the constraints are consistent,
and if so,    nd a point that satis   es them.

4.1.2 expressing problems in standard form

we refer to (4.1) as an optimization problem in standard form. in the standard
form problem we adopt the convention that the righthand side of the inequality
and equality constraints are zero. this can always be arranged by subtracting any
nonzero righthand side: we represent the equality constraint gi(x) =   gi(x), for
example, as hi(x) = 0, where hi(x) = gi(x)       gi(x). in a similar way we express
inequalities of the form fi(x)     0 as    fi(x)     0.

example 4.2 box constraints. consider the optimization problem

minimize
subject to

f0(x)
li     xi     ui,

i = 1, . . . , n,

where x     rn is the variable. the constraints are called variable bounds (since they
give lower and upper bounds for each xi) or box constraints (since the feasible set is
a box).

we can express this problem in standard form as

minimize
subject to

f0(x)
li     xi     0,
xi     ui     0,

i = 1, . . . , n
i = 1, . . . , n.

there are 2n inequality constraint functions:

and

fi(x) = li     xi,

i = 1, . . . , n,

fi(x) = xi   n     ui   n,

i = n + 1, . . . , 2n.

maximization problems

we concentrate on the minimization problem by convention. we can solve the
maximization problem

maximize
subject to

f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p

(4.2)

130

4 id76 problems

by minimizing the function    f0 subject to the constraints. by this correspondence
we can de   ne all the terms above for the maximization problem (4.2). for example
the optimal value of (4.2) is de   ned as

p    = sup{f0(x) | fi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p},

and a feasible point x is   -suboptimal if f0(x)     p          . when the maximization
problem is considered, the objective is sometimes called the utility or satisfaction
level instead of the cost.

4.1.3 equivalent problems

in this book we will use the notion of equivalence of optimization problems in an
informal way. we call two problems equivalent if from a solution of one, a solution
of the other is readily found, and vice versa. (it is possible, but complicated, to
give a formal de   nition of equivalence.)

as a simple example, consider the problem
  f (x) =   0f0(x)
  fi(x) =   ifi(x)     0,
  hi(x) =   ihi(x) = 0,

minimize
subject to

i = 1, . . . , m
i = 1, . . . , p,

(4.3)

where   i > 0, i = 0, . . . , m, and   i 6= 0, i = 1, . . . , p. this problem is obtained from
the standard form problem (4.1) by scaling the objective and inequality constraint
functions by positive constants, and scaling the equality constraint functions by
nonzero constants. as a result, the feasible sets of the problem (4.3) and the original
problem (4.1) are identical. a point x is optimal for the original problem (4.1) if
and only if it is optimal for the scaled problem (4.3), so we say the two problems are
equivalent. the two problems (4.1) and (4.3) are not, however, the same (unless
  i and   i are all equal to one), since the objective and constraint functions di   er.
we now describe some general transformations that yield equivalent problems.

change of variables
suppose    : rn     rn is one-to-one, with image covering the problem domain d,
i.e.,   (dom   )     d. we de   ne functions   fi and   hi as

  fi(z) = fi(  (z)),

i = 0, . . . , m,

  hi(z) = hi(  (z)),

i = 1, . . . , p.

now consider the problem

minimize
subject to

  f0(z)
  fi(z)     0,
  hi(z) = 0,

i = 1, . . . , m
i = 1, . . . , p,

(4.4)

with variable z. we say that the standard form problem (4.1) and the problem (4.4)
are related by the change of variable or substitution of variable x =   (z).

the two problems are clearly equivalent:

if x solves the problem (4.1), then
z =      1(x) solves the problem (4.4); if z solves the problem (4.4), then x =   (z)
solves the problem (4.1).

4.1 optimization problems

131

transformation of objective and constraint functions
suppose that   0 : r     r is monotone increasing,   1, . . . ,   m : r     r satisfy
  i(u)     0 if and only if u     0, and   m+1, . . . ,   m+p : r     r satisfy   i(u) = 0 if
and only if u = 0. we de   ne functions   fi and   hi as the compositions

  fi(x) =   i(fi(x)),

i = 0, . . . , m,

  hi(x) =   m+i(hi(x)),

i = 1, . . . , p.

evidently the associated problem

minimize
subject to

  f0(x)
  fi(x)     0,
  hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p

and the standard form problem (4.1) are equivalent; indeed, the feasible sets are
identical, and the optimal points are identical. (the example (4.3) above, in which
the objective and constraint functions are scaled by appropriate constants, is the
special case when all   i are linear.)

example 4.3 least-norm and least-norm-squared problems. as a simple example
consider the unconstrained euclidean norm minimization problem

minimize

kax     bk2,

(4.5)

with variable x     rn. since the norm is always nonnegative, we can just as well solve
the problem
(4.6)

minimize

kax     bk2

2 = (ax     b)t (ax     b),

in which we minimize the square of the euclidean norm. the problems (4.5) and (4.6)
are clearly equivalent; the optimal points are the same. the two problems are not
the same, however. for example, the objective in (4.5) is not di   erentiable at any
x with ax     b = 0, whereas the objective in (4.6) is di   erentiable for all x (in fact,
quadratic).

slack variables
one simple transformation is based on the observation that fi(x)     0 if and only if
there is an si     0 that satis   es fi(x) + si = 0. using this transformation we obtain
the problem

minimize
subject to

f0(x)
si     0,
fi(x) + si = 0,
hi(x) = 0,

i = 1, . . . , m

i = 1, . . . , m

i = 1, . . . , p,

(4.7)

where the variables are x     rn and s     rm. this problem has n + m variables,
m inequality constraints (the nonnegativity constraints on si), and m + p equality
constraints. the new variable si is called the slack variable associated with the
original inequality constraint fi(x)     0. introducing slack variables replaces each
inequality constraint with an equality constraint, and a nonnegativity constraint.
the problem (4.7) is equivalent to the original standard form problem (4.1).
indeed, if (x, s) is feasible for the problem (4.7), then x is feasible for the original

132

4 id76 problems

problem, since si =    fi(x)     0. conversely, if x is feasible for the original problem,
then (x, s) is feasible for the problem (4.7), where we take si =    fi(x). similarly,
x is optimal for the original problem (4.1) if and only if (x, s) is optimal for the
problem (4.7), where si =    fi(x).
eliminating equality constraints

if we can explicitly parametrize all solutions of the equality constraints

hi(x) = 0,

(4.8)
using some parameter z     rk, then we can eliminate the equality constraints
from the problem, as follows. suppose the function    : rk     rn is such that
x satis   es (4.8) if and only if there is some z     rk such that x =   (z). the
optimization problem

i = 1, . . . , p,

minimize
subject to

  f0(z) = f0(  (z))
  fi(z) = fi(  (z))     0,

i = 1, . . . , m

is then equivalent to the original problem (4.1). this transformed problem has
variable z     rk, m inequality constraints, and no equality constraints.
if z is
optimal for the transformed problem, then x =   (z) is optimal for the original
problem. conversely, if x is optimal for the original problem, then (since x is
feasible) there is at least one z such that x =   (z). any such z is optimal for the
transformed problem.

eliminating linear equality constraints

the process of eliminating variables can be described more explicitly, and easily
carried out numerically, when the equality constraints are all linear, i.e., have the
form ax = b. if ax = b is inconsistent, i.e., b 6    r(a), then the original problem is
infeasible. assuming this is not the case, let x0 denote any solution of the equality
constraints. let f     rn  k be any matrix with r(f ) = n (a), so the general
solution of the linear equations ax = b is given by f z + x0, where z     rk. (we
can choose f to be full rank, in which case we have k = n     rank a.)

substituting x = f z + x0 into the original problem yields the problem

minimize
subject to

f0(f z + x0)
fi(f z + x0)     0,

i = 1, . . . , m,

with variable z, which is equivalent to the original problem, has no equality con-
straints, and rank a fewer variables.

introducing equality constraints

we can also introduce equality constraints and new variables into a problem. in-
stead of describing the general case, which is complicated and not very illuminating,
we give a typical example that will be useful later. consider the problem

minimize
subject to

f0(a0x + b0)
fi(aix + bi)     0,
hi(x) = 0,

i = 1, . . . , p,

i = 1, . . . , m

4.1 optimization problems

133

where x     rn, ai     rki  n, and fi : rki     r.
in this problem the objective
and constraint functions are given as compositions of the functions fi with a   ne
transformations de   ned by aix + bi.

we introduce new variables yi     rki, as well as new equality constraints yi =

aix + bi, for i = 0, . . . , m, and form the equivalent problem

minimize
subject to

f0(y0)
fi(yi)     0,
yi = aix + bi,
hi(x) = 0,

i = 1, . . . , m

i = 0, . . . , m

i = 1, . . . , p.

this problem has k0 +        + km new variables,

y0     rk0,

. . . ,

ym     rkm,

and k0 +        + km new equality constraints,
. . . ,

y0 = a0x + b0,

ym = amx + bm.

the objective and inequality constraints in this problem are independent, i.e., in-
volve di   erent optimization variables.

optimizing over some variables

we always have

inf
x,y

f (x, y) = inf
x

  f (x)

where   f (x) = inf y f (x, y). in other words, we can always minimize a function by
   rst minimizing over some of the variables, and then minimizing over the remaining
ones. this simple and general principle can be used to transform problems into
equivalent forms. the general case is cumbersome to describe and not illuminating,
so we describe instead an example.

suppose the variable x     rn is partitioned as x = (x1, x2), with x1     rn1 ,

x2     rn2 , and n1 + n2 = n. we consider the problem

minimize
subject to

f0(x1, x2)
fi(x1)     0,
  fi(x2)     0,

i = 1, . . . , m1
i = 1, . . . , m2,

(4.9)

in which the constraints are independent, in the sense that each constraint function
depends on x1 or x2. we    rst minimize over x2. de   ne the function   f0 of x1 by

  f0(x1) = inf{f0(x1, z) |   fi(z)     0, i = 1, . . . , m2}.

the problem (4.9) is then equivalent to

minimize
subject to

  f0(x1)
fi(x1)     0,

i = 1, . . . , m1.

(4.10)

134

4 id76 problems

example 4.4 minimizing a quadratic function with constraints on some variables.
consider a problem with strictly convex quadratic objective, with some of the vari-
ables unconstrained:

minimize
subject to

xt
1 p11x1 + 2xt
fi(x1)     0,

1 p12x2 + xt
i = 1, . . . , m,

2 p22x2

where p11 and p22 are symmetric. here we can analytically minimize over x2:

1 p11x1 + 2xt

1 p12x2 + xt

inf

x2(cid:0)xt

(see   a.5.5). therefore the original problem is equivalent to

22 p t

12(cid:1) x1

2 p22x2(cid:1) = xt
1 (cid:0)p11     p12p    1

xt
fi(x1)     0,

1 (cid:0)p11     p12p    1
12(cid:1) x1

i = 1, . . . , m.

22 p t

minimize
subject to

epigraph problem form

the epigraph form of the standard problem (4.1) is the problem

minimize
subject to

t
f0(x)     t     0
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p,

(4.11)

with variables x     rn and t     r. we can easily see that it is equivalent to the
original problem: (x, t) is optimal for (4.11) if and only if x is optimal for (4.1)
and t = f0(x). note that the objective function of the epigraph form problem is a
linear function of the variables x, t.

the epigraph form problem (4.11) can be interpreted geometrically as an op-
timization problem in the    graph space    (x, t): we minimize t over the epigraph of
f0, subject to the constraints on x. this is illustrated in    gure 4.1.

implicit and explicit constraints
by a simple trick already mentioned in   3.1.2, we can include any of the constraints
implicitly in the objective function, by rede   ning its domain. as an extreme ex-
ample, the standard form problem can be expressed as the unconstrained problem

minimize f (x),

(4.12)

where we de   ne the function f as f0, but with domain restricted to the feasible
set:

dom f = {x     dom f0 | fi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p},

and f (x) = f0(x) for x     dom f . (equivalently, we can de   ne f (x) to have value
    for x not feasible.) the problems (4.1) and (4.12) are clearly equivalent: they
have the same feasible set, optimal points, and optimal value.
of course this transformation is nothing more than a notational trick. making
the constraints implicit has not made the problem any easier to analyze or solve,

4.1 optimization problems

135

t

epi f0

(x   , t   )

x

figure 4.1 geometric interpretation of epigraph form problem, for a prob-
lem with no constraints. the problem is to    nd the point in the epigraph
(shown shaded) that minimizes t, i.e., the    lowest    point in the epigraph.
the optimal point is (x   , t   ).

even though the problem (4.12) is, at least nominally, unconstrained. in some ways
the transformation makes the problem more di   cult. suppose, for example, that
the objective f0 in the original problem is di   erentiable, so in particular its domain
is open. the restricted objective function f is probably not di   erentiable, since
its domain is likely not to be open.

conversely, we will encounter problems with implicit constraints, which we can

then make explicit. as a simple example, consider the unconstrained problem

minimize

f (x)

(4.13)

where the function f is given by

f (x) =(cid:26) xt x ax = b

    otherwise.

thus, the objective function is equal to the quadratic form xt x on the a   ne set
de   ned by ax = b, and     o    the a   ne set. since we can clearly restrict our
attention to points that satisfy ax = b, we say that the problem (4.13) has an
implicit equality constraint ax = b hidden in the objective. we can make the
implicit equality constraint explicit, by forming the equivalent problem

minimize
subject to ax = b.

xt x

(4.14)

while the problems (4.13) and (4.14) are clearly equivalent, they are not the same.
the problem (4.13) is unconstrained, but its objective function is not di   erentiable.
the problem (4.14), however, has an equality constraint, but its objective and
constraint functions are di   erentiable.

136

4 id76 problems

4.1.4 parameter and oracle problem descriptions

for a problem in the standard form (4.1), there is still the question of how the
objective and constraint functions are speci   ed.
in many cases these functions
have some analytical or closed form, i.e., are given by a formula or expression that
involves the variable x as well as some parameters. suppose, for example, the
objective is quadratic, so it has the form f0(x) = (1/2)xt p x + qt x + r. to specify
the objective function we give the coe   cients (also called problem parameters or
problem data) p     sn, q     rn, and r     r. we call this a parameter problem
description, since the speci   c problem to be solved (i.e., the problem instance) is
speci   ed by giving the values of the parameters that appear in the expressions for
the objective and constraint functions.

in other cases the objective and constraint functions are described by oracle
models (which are also called black box or subroutine models). in an oracle model,
we do not know f explicitly, but can evaluate f (x) (and usually also some deriva-
tives) at any x     dom f . this is referred to as querying the oracle, and is usually
associated with some cost, such as time. we are also given some prior information
about the function, such as convexity and a bound on its values. as a concrete
example of an oracle model, consider an unconstrained problem, in which we are
to minimize the function f . the function value f (x) and its gradient    f (x) are
evaluated in a subroutine. we can call the subroutine at any x     dom f , but do
not have access to its source code. calling the subroutine with argument x yields
(when the subroutine returns) f (x) and    f (x). note that in the oracle model,
we never really know the function; we only know the function value (and some
derivatives) at the points where we have queried the oracle. (we also know some
given prior information about the function, such as di   erentiability and convexity.)
in practice the distinction between a parameter and oracle problem description
is not so sharp. if we are given a parameter problem description, we can construct
an oracle for it, which simply evaluates the required functions and derivatives when
queried. most of the algorithms we study in part iii work with an oracle model, but
can be made more e   cient when they are restricted to solve a speci   c parametrized
family of problems.

4.2 id76

4.2.1 id76 problems in standard form

a id76 problem is one of the form

minimize
subject to

f0(x)
fi(x)     0,
at
i x = bi,

i = 1, . . . , m
i = 1, . . . , p,

(4.15)

where f0, . . . , fm are convex functions. comparing (4.15) with the general standard
form problem (4.1), the convex problem has three additional requirements:

4.2 id76

137

    the objective function must be convex,
    the inequality constraint functions must be convex,
    the equality constraint functions hi(x) = at

i x     bi must be a   ne.

we immediately note an important property: the feasible set of a convex optimiza-
tion problem is convex, since it is the intersection of the domain of the problem

d =

m\i=0

dom fi,

which is a convex set, with m (convex) sublevel sets {x | fi(x)     0} and p hyper-
planes {x | at
i x = bi}. (we can assume without loss of generality that ai 6= 0: if
ai = 0 and bi = 0 for some i, then the ith equality constraint can be deleted; if
ai = 0 and bi 6= 0, the ith equality constraint is inconsistent, and the problem is in-
feasible.) thus, in a id76 problem, we minimize a convex objective
function over a convex set.

if f0 is quasiconvex instead of convex, we say the problem (4.15) is a (standard
form) quasiid76 problem. since the sublevel sets of a convex or
quasiconvex function are convex, we conclude that for a convex or quasiconvex
optimization problem the   -suboptimal sets are convex. in particular, the optimal
set is convex. if the objective is strictly convex, then the optimal set contains at
most one point.

concave maximization problems

with a slight abuse of notation, we will also refer to

maximize
subject to

f0(x)
fi(x)     0,
at
i x = bi,

i = 1, . . . , m
i = 1, . . . , p,

(4.16)

as a id76 problem if the objective function f0 is concave, and the
inequality constraint functions f1, . . . , fm are convex. this concave maximization
problem is readily solved by minimizing the convex objective function    f0. all
of the results, conclusions, and algorithms that we describe for the minimization
problem are easily transposed to the maximization case.
in a similar way the
maximization problem (4.16) is called quasiconvex if f0 is quasiconcave.

abstract form id76 problem

it is important to note a subtlety in our de   nition of id76 problem.
consider the example with x     r2,

minimize
subject to

1 + x2
2

f0(x) = x2
f1(x) = x1/(1 + x2
2)     0
h1(x) = (x1 + x2)2 = 0,

(4.17)

which is in the standard form (4.1). this problem is not a id76
problem in standard form since the equality constraint function h1 is not a   ne, and

138

4 id76 problems

the inequality constraint function f1 is not convex. nevertheless the feasible set,
which is {x | x1     0, x1 + x2 = 0}, is convex. so although in this problem we are
minimizing a convex function f0 over a convex set, it is not a id76
problem by our de   nition.

of course, the problem is readily reformulated as

minimize
subject to

f0(x) = x2
1 + x2
2
  f1(x) = x1     0
  h1(x) = x1 + x2 = 0,

(4.18)

which is in standard id76 form, since f0 and   f1 are convex, and   h1
is a   ne.

some authors use the term abstract id76 problem to describe the
(abstract) problem of minimizing a convex function over a convex set. using this
terminology, the problem (4.17) is an abstract id76 problem. we
will not use this terminology in this book. for us, a id76 problem is
not just one of minimizing a convex function over a convex set; it is also required
that the feasible set be described speci   cally by a set of inequalities involving
convex functions, and a set of linear equality constraints. the problem (4.17) is
not a id76 problem, but the problem (4.18) is a id76
problem. (the two problems are, however, equivalent.)

our adoption of the stricter de   nition of id76 problem does not
matter much in practice. to solve the abstract problem of minimizing a convex
function over a convex set, we need to    nd a description of the set in terms of
convex inequalities and linear equality constraints. as the example above suggests,
this is usually straightforward.

4.2.2 local and global optima

a fundamental property of id76 problems is that any locally optimal
point is also (globally) optimal. to see this, suppose that x is locally optimal for
a id76 problem, i.e., x is feasible and

f0(x) = inf{f0(z) | z feasible, kz     xk2     r},

(4.19)

for some r > 0. now suppose that x is not globally optimal, i.e., there is a feasible
y such that f0(y) < f0(x). evidently ky     xk2 > r, since otherwise f0(x)     f0(y).
consider the point z given by

z = (1       )x +   y,

   =

r

2ky     xk2

.

then we have kz     xk2 = r/2 < r, and by convexity of the feasible set, z is
feasible. by convexity of f0 we have

f0(z)     (1       )f0(x) +   f0(y) < f0(x),

which contradicts (4.19). hence there exists no feasible y with f0(y) < f0(x), i.e.,
x is globally optimal.

4.2 id76

139

x

x

      f0(x)

figure 4.2 geometric interpretation of the optimality condition (4.21). the
feasible set x is shown shaded. some level curves of f0 are shown as dashed
lines. the point x is optimal:       f0(x) de   nes a supporting hyperplane
(shown as a solid line) to x at x.

it is not true that locally optimal points of quasiid76 problems

are globally optimal; see   4.2.5.

4.2.3 an optimality criterion for di   erentiable f0

suppose that the objective f0 in a id76 problem is di   erentiable,
so that for all x, y     dom f0,

f0(y)     f0(x) +    f0(x)t (y     x)

(4.20)

(see   3.1.3). let x denote the feasible set, i.e.,

x = {x | fi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p}.

then x is optimal if and only if x     x and

   f0(x)t (y     x)     0 for all y     x.

(4.21)

this optimality criterion can be understood geometrically: if    f0(x) 6= 0, it means
that       f0(x) de   nes a supporting hyperplane to the feasible set at x (see    g-
ure 4.2).

proof of optimality condition
first suppose x     x and satis   es (4.21). then if y     x we have, by (4.20),
f0(y)     f0(x). this shows x is an optimal point for (4.1).
conversely, suppose x is optimal, but the condition (4.21) does not hold, i.e.,
for some y     x we have

   f0(x)t (y     x) < 0.

140

4 id76 problems

consider the point z(t) = ty + (1    t)x, where t     [0, 1] is a parameter. since z(t) is
on the line segment between x and y, and the feasible set is convex, z(t) is feasible.
we claim that for small positive t we have f0(z(t)) < f0(x), which will prove that
x is not optimal. to show this, note that

d
dt

f0(z(t))(cid:12)(cid:12)(cid:12)(cid:12)t=0

=    f0(x)t (y     x) < 0,

so for small positive t, we have f0(z(t)) < f0(x).

we will pursue the topic of optimality conditions in much more depth in chap-

ter 5, but here we examine a few simple examples.

unconstrained problems

for an unconstrained problem (i.e., m = p = 0), the condition (4.21) reduces to
the well known necessary and su   cient condition

   f0(x) = 0

(4.22)

for x to be optimal. while we have already seen this optimality condition, it is
useful to see how it follows from (4.21). suppose x is optimal, which means here
that x     dom f0, and for all feasible y we have    f0(x)t (y     x)     0. since f0 is
di   erentiable, its domain is (by de   nition) open, so all y su   ciently close to x are
feasible. let us take y = x    t   f0(x), where t     r is a parameter. for t small and
positive, y is feasible, and so

   f0(x)t (y     x) =    tk   f0(x)k2

2     0,

from which we conclude    f0(x) = 0.
there are several possible situations, depending on the number of solutions
of (4.22). if there are no solutions of (4.22), then there are no optimal points; the
optimal value of the problem is not attained. here we can distinguish between
two cases: the problem is unbounded below, or the optimal value is    nite, but not
attained. on the other hand we can have multiple solutions of the equation (4.22),
in which case each such solution is a minimizer of f0.

example 4.5 unconstrained quadratic optimization. consider the problem of mini-
mizing the quadratic function

f0(x) = (1/2)xt p x + qt x + r,

where p     sn
x to be a minimizer of f0 is

+ (which makes f0 convex). the necessary and su   cient condition for

   f0(x) = p x + q = 0.

several cases can occur, depending on whether this (linear) equation has no solutions,
one solution, or many solutions.

    if q 6    r(p ), then there is no solution. in this case f0 is unbounded below.
    if p     0 (which is the condition for f0 to be strictly convex), then there is a

unique minimizer, x    =    p    1q.

4.2 id76

141

    if p is singular, but q     r(p ), then the set of optimal points is the (a   ne) set
xopt =    p    q + n (p ), where p     denotes the pseudo-inverse of p (see   a.5.4).

example 4.6 analytic centering. consider the (unconstrained) problem of minimiz-
ing the (convex) function f0 : rn     r, de   ned as

f0(x) =    

mxi=1

log(bi     at

i x),

dom f0 = {x | ax     b},

where at
and su   cient conditions for x to be optimal are

1 , . . . , at

m are the rows of a. the function f0 is di   erentiable, so the necessary

ax     b,

   f0(x) =

mxi=1

1

bi     at
i x

ai = 0.

(4.23)

(the condition ax     b is just x     dom f0.) if ax     b is infeasible, then the domain
of f0 is empty. assuming ax     b is feasible, there are still several possible cases (see
exercise 4.2):

    there are no solutions of (4.23), and hence no optimal points for the problem.

this occurs if and only if f0 is unbounded below.

    there are many solutions of (4.23).

solutions form an a   ne set.

in this case it can be shown that the

    there is a unique solution of (4.23), i.e., a unique minimizer of f0. this occurs

if and only if the open polyhedron {x | ax     b} is nonempty and bounded.

problems with equality constraints only

consider the case where there are equality constraints but no inequality constraints,
i.e.,

minimize
subject to ax = b.

f0(x)

here the feasible set is a   ne. we assume that it is nonempty; otherwise the
problem is infeasible. the optimality condition for a feasible x is that

   f0(x)t (y     x)     0

must hold for all y satisfying ay = b. since x is feasible, every feasible y has the
form y = x + v for some v     n (a). the optimality condition can therefore be
expressed as:

   f0(x)t v     0 for all v     n (a).

if a linear function is nonnegative on a subspace, then it must be zero on the
subspace, so it follows that    f0(x)t v = 0 for all v     n (a). in other words,

   f0(x)     n (a).

142

4 id76 problems

using the fact that n (a)    = r(at ), this optimality condition can be expressed
as    f0(x)     r(at ), i.e., there exists a        rp such that

   f0(x) + at    = 0.

together with the requirement ax = b (i.e., that x is feasible), this is the classical
lagrange multiplier optimality condition, which we will study in greater detail in
chapter 5.

minimization over the nonnegative orthant

as another example we consider the problem

minimize
f0(x)
subject to x (cid:23) 0,

where the only inequality constraints are nonnegativity constraints on the variables.

the optimality condition (4.21) is then

x (cid:23) 0,

   f0(x)t (y     x)     0 for all y (cid:23) 0.

the term    f0(x)t y, which is a linear function of y, is unbounded below on y (cid:23) 0,
unless we have    f0(x) (cid:23) 0. the condition then reduces to       f0(x)t x     0. but
x (cid:23) 0 and    f0(x) (cid:23) 0, so we must have    f0(x)t x = 0, i.e.,

nxi=1

(   f0(x))ixi = 0.

now each of the terms in this sum is the product of two nonnegative numbers, so
we conclude that each term must be zero, i.e., (   f0(x))i xi = 0 for i = 1, . . . , n.

the optimality condition can therefore be expressed as

x (cid:23) 0,

   f0(x) (cid:23) 0,

xi (   f0(x))i = 0,

i = 1, . . . , n.

the last condition is called complementarity, since it means that the sparsity pat-
terns (i.e., the set of indices corresponding to nonzero components) of the vectors x
and    f0(x) are complementary (i.e., have empty intersection). we will encounter
complementarity conditions again in chapter 5.

4.2.4 equivalent convex problems

it is useful to see which of the transformations described in   4.1.3 preserve convex-
ity.

eliminating equality constraints

for a convex problem the equality constraints must be linear, i.e., of the form
ax = b. in this case they can be eliminated by    nding a particular solution x0 of

4.2 id76

143

ax = b, and a matrix f whose range is the nullspace of a, which results in the
problem

minimize
subject to

f0(f z + x0)
fi(f z + x0)     0,

i = 1, . . . , m,

with variable z. since the composition of a convex function with an a   ne func-
tion is convex, eliminating equality constraints preserves convexity of a problem.
moreover, the process of eliminating equality constraints (and reconstructing the
solution of the original problem from the solution of the transformed problem)
involves standard id202 operations.

at least in principle, this means we can restrict our attention to convex opti-
mization problems which have no equality constraints. in many cases, however, it
is better to retain the equality constraints, since eliminating them can make the
problem harder to understand and analyze, or ruin the e   ciency of an algorithm
that solves it. this is true, for example, when the variable x has very large dimen-
sion, and eliminating the equality constraints would destroy sparsity or some other
useful structure of the problem.

introducing equality constraints

we can introduce new variables and equality constraints into a id76
problem, provided the equality constraints are linear, and the resulting problem
will also be convex. for example, if an objective or constraint function has the form
fi(aix + bi), where ai     rki  n, we can introduce a new variable yi     rki, replace
fi(aix + bi) with fi(yi), and add the linear equality constraint yi = aix + bi.

slack variables

by introducing slack variables we have the new constraints fi(x) + si = 0. since
equality constraint functions must be a   ne in a convex problem, we must have fi
a   ne. in other words: introducing slack variables for linear inequalities preserves
convexity of a problem.

epigraph problem form

the epigraph form of the id76 problem (4.15) is

minimize
subject to

t
f0(x)     t     0
fi(x)     0,
at
i x = bi,

i = 1, . . . , m
i = 1, . . . , p.

the objective is linear (hence convex) and the new constraint function f0(x)     t is
also convex in (x, t), so the epigraph form problem is convex as well.
it is sometimes said that a linear objective is universal for id76,
since any id76 problem is readily transformed to one with linear
objective. the epigraph form of a convex problem has several practical uses. by
assuming the objective of a id76 problem is linear, we can simplify
theoretical analysis.
it can also simplify algorithm development, since an algo-
rithm that solves id76 problems with linear objective can, using

144

4 id76 problems

the transformation above, solve any id76 problem (provided it can
handle the constraint f0(x)     t     0).
minimizing over some variables

minimizing a convex function over some variables preserves convexity. therefore,
if f0 in (4.9) is jointly convex in x1 and x2, and fi, i = 1, . . . , m1, and   fi, i =
1, . . . , m2, are convex, then the equivalent problem (4.10) is convex.

4.2.5 quasiid76

recall that a quasiid76 problem has the standard form

minimize
subject to

f0(x)
fi(x)     0,
ax = b,

i = 1, . . . , m

(4.24)

where the inequality constraint functions f1, . . . , fm are convex, and the objective
f0 is quasiconvex (instead of convex, as in a id76 problem). (qua-
siconvex constraint functions can be replaced with equivalent convex constraint
functions, i.e., constraint functions that are convex and have the same 0-sublevel
set, as in   3.4.5.)
in this section we point out some basic di   erences between convex and quasicon-
vex optimization problems, and also show how solving a quasiid76
problem can be reduced to solving a sequence of id76 problems.

locally optimal solutions and optimality conditions

the most important di   erence between convex and quasiid76 is
that a quasiid76 problem can have locally optimal solutions that
are not (globally) optimal. this phenomenon can be seen even in the simple case
of unconstrained minimization of a quasiconvex function on r, such as the one
shown in    gure 4.3.

nevertheless, a variation of the optimality condition (4.21) given in   4.2.3 does
hold for quasiid76 problems with di   erentiable objective function.
let x denote the feasible set for the quasiid76 problem (4.24). it
follows from the    rst-order condition for quasiconvexity (3.20) that x is optimal if

x     x,

   f0(x)t (y     x) > 0 for all y     x \ {x}.

(4.25)

there are two important di   erences between this criterion and the analogous
one (4.21) for id76:

    the condition (4.25) is only su   cient for optimality; simple examples show
that it need not hold for an optimal point. in contrast, the condition (4.21)
is necessary and su   cient for x to solve the convex problem.

    the condition (4.25) requires the gradient of f0 to be nonzero, whereas the
condition (4.21) does not. indeed, when    f0(x) = 0 in the convex case, the
condition (4.21) is satis   ed, and x is optimal.

4.2 id76

145

(x, f (x))

figure 4.3 a quasiconvex function f on r, with a locally optimal point x
that is not globally optimal. this example shows that the simple optimality
condition f    (x) = 0, valid for convex functions, does not hold for quasiconvex
functions.

quasiid76 via convex feasibility problems

one general approach to quasiid76 relies on the representation of
the sublevel sets of a quasiconvex function via a family of convex inequalities, as
described in   3.4.5. let   t : rn     r, t     r, be a family of convex functions that
satisfy

f0(x)     t          t(x)     0,

and also, for each x,   t(x) is a nonincreasing function of t, i.e.,   s(x)       t(x)
whenever s     t.
let p    denote the optimal value of the quasiid76 problem (4.24).
if the feasibility problem

x

   nd
subject to   t(x)     0
fi(x)     0,
ax = b,

i = 1, . . . , m

(4.26)

is feasible, then we have p        t. conversely, if the problem (4.26) is infeasible, then
we can conclude p        t. the problem (4.26) is a convex feasibility problem, since
the inequality constraint functions are all convex, and the equality constraints
are linear. thus, we can check whether the optimal value p    of a quasiconvex
optimization problem is less than or more than a given value t by solving the
convex feasibility problem (4.26). if the convex feasibility problem is feasible then
we have p        t, and any feasible point x is feasible for the quasiconvex problem
and satis   es f0(x)     t. if the convex feasibility problem is infeasible, then we know
that p        t.
this observation can be used as the basis of a simple algorithm for solving the
quasiid76 problem (4.24) using bisection, solving a convex feasi-
bility problem at each step. we assume that the problem is feasible, and start
with an interval [l, u] known to contain the optimal value p   . we then solve the
convex feasibility problem at its midpoint t = (l + u)/2, to determine whether the

146

4 id76 problems

optimal value is in the lower or upper half of the interval, and update the interval
accordingly. this produces a new interval, which also contains the optimal value,
but has half the width of the initial interval. this is repeated until the width of
the interval is small enough:

algorithm 4.1 bisection method for quasiid76.

given l     p   , u     p   , tolerance    > 0.

repeat

1. t := (l + u)/2.
2. solve the convex feasibility problem (4.26).
3. if (4.26) is feasible, u := t;

else l := t.

until u     l       .

the interval [l, u] is guaranteed to contain p   , i.e., we have l     p        u at
in each iteration the interval is divided in two, i.e., bisected, so the
each step.
length of the interval after k iterations is 2   k(u     l), where u     l is the length of
the initial interval. it follows that exactly    log2((u     l)/  )    iterations are required
before the algorithm terminates. each step involves solving the convex feasibility
problem (4.26).

4.3 linear optimization problems

when the objective and constraint functions are all a   ne, the problem is called a
linear program (lp). a general linear program has the form

ct x + d
minimize
subject to gx (cid:22) h
ax = b,

(4.27)

where g     rm  n and a     rp  n. linear programs are, of course, convex opti-
mization problems.
it is common to omit the constant d in the objective function, since it does not
a   ect the optimal (or feasible) set. since we can maximize an a   ne objective ct x+
d, by minimizing    ct x     d (which is still convex), we also refer to a maximization
problem with a   ne objective and constraint functions as an lp.
the geometric interpretation of an lp is illustrated in    gure 4.4. the feasible
set of the lp (4.27) is a polyhedron p; the problem is to minimize the a   ne
function ct x + d (or, equivalently, the linear function ct x) over p.
standard and inequality form linear programs

two special cases of the lp (4.27) are so widely encountered that they have been
given separate names. in a standard form lp the only inequalities are componen-

4.3 linear optimization problems

147

   c

x   

p

figure 4.4 geometric interpretation of an lp. the feasible set p, which
is a polyhedron, is shaded. the objective ct x is linear, so its level curves
are hyperplanes orthogonal to c (shown as dashed lines). the point x    is
optimal; it is the point in p as far as possible in the direction    c.

twise nonnegativity constraints x (cid:23) 0:

ct x

minimize
subject to ax = b
x (cid:23) 0.

(4.28)

if the lp has no equality constraints, it is called an inequality form lp, usually
written as

ct x

minimize
subject to ax (cid:22) b.

(4.29)

converting lps to standard form

it is sometimes useful to transform a general lp (4.27) to one in standard form (4.28)
(for example in order to use an algorithm for standard form lps). the    rst step
is to introduce slack variables si for the inequalities, which results in

minimize
subject to gx + s = h

ct x + d

ax = b
s (cid:23) 0.

the second step is to express the variable x as the di   erence of two nonnegative
variables x+ and x   , i.e., x = x+     x   , x+, x    (cid:23) 0. this yields the problem

minimize
subject to gx+     gx    + s = h

ct x+     ct x    + d
ax+     ax    = b
x    (cid:23) 0,
x+ (cid:23) 0,

s (cid:23) 0,

148

4 id76 problems

which is an lp in standard form, with variables x+, x   , and s. (for equivalence
of this problem and the original one (4.27), see exercise 4.10.)

these techniques for manipulating problems (along with many others we will
see in the examples and exercises) can be used to formulate many problems as linear
programs. with some abuse of terminology, it is common to refer to a problem
that can be formulated as an lp as an lp, even if it does not have the form (4.27).

4.3.1 examples

lps arise in a vast number of    elds and applications; here we give a few typical
examples.

diet problem

a healthy diet contains m di   erent nutrients in quantities at least equal to b1, . . . ,
bm. we can compose such a diet by choosing nonnegative quantities x1, . . . , xn of
n di   erent foods. one unit quantity of food j contains an amount aij of nutrient
i, and has a cost of cj. we want to determine the cheapest diet that satis   es the
nutritional requirements. this problem can be formulated as the lp

ct x

minimize
subject to ax (cid:23) b
x (cid:23) 0.

several variations on this problem can also be formulated as lps. for example,
we can insist on an exact amount of a nutrient in the diet (which gives a linear
equality constraint), or we can impose an upper bound on the amount of a nutrient,
in addition to the lower bound as above.

chebyshev center of a polyhedron

we consider the problem of    nding the largest euclidean ball that lies in a poly-
hedron described by linear inequalities,
p = {x     rn | at

i x     bi, i = 1, . . . , m}.

(the center of the optimal ball is called the chebyshev center of the polyhedron;
it is the point deepest inside the polyhedron, i.e., farthest from the boundary;
see   8.5.1.) we represent the ball as

b = {xc + u | kuk2     r}.

the variables in the problem are the center xc     rn and the radius r; we wish to
maximize r subject to the constraint b     p.
we start by considering the simpler constraint that b lies in one halfspace
at
i x     bi, i.e.,
(4.30)

kuk2     r =    at

i (xc + u)     bi.

since

sup{at

i u | kuk2     r} = rkaik2

4.3 linear optimization problems

149

we can write (4.30) as

at
i xc + rkaik2     bi,

(4.31)
which is a linear inequality in xc and r. in other words, the constraint that the
ball lies in the halfspace determined by the inequality at
i x     bi can be written as
a linear inequality.
therefore b     p if and only if (4.31) holds for all i = 1, . . . , m. hence the

chebyshev center can be determined by solving the lp

maximize
r
subject to at

i xc + rkaik2     bi,

i = 1, . . . , m,

with variables r and xc. (for more on the chebyshev center, see   8.5.1.)
dynamic activity planning

we consider the problem of choosing, or planning, the activity levels of n activities,
or sectors of an economy, over n time periods. we let xj(t)     0, t = 1, . . . , n ,
denote the activity level of sector j, in period t. the activities both consume and
produce products or goods in proportion to their activity levels. the amount of
good i produced per unit of activity j is given by aij. similarly, the amount of good i
consumed per unit of activity j is bij. the total amount of goods produced in period
t is given by ax(t)     rm, and the amount of goods consumed is bx(t)     rm.
(although we refer to these products as    goods   , they can also include unwanted
products such as pollutants.)

the goods consumed in a period cannot exceed those produced in the previous
period: we must have bx(t + 1) (cid:22) ax(t) for t = 1, . . . , n . a vector g0     rm of
initial goods is given, which constrains the    rst period activity levels: bx(1) (cid:22) g0.
the (vectors of) excess goods not consumed by the activities are given by

s(0) = g0     bx(1)
s(t) = ax(t)     bx(t + 1),
s(n ) = ax(n ).

t = 1, . . . , n     1

the objective is to maximize a discounted total value of excess goods:

ct s(0) +   ct s(1) +        +   n ct s(n ),

where c     rm gives the values of the goods, and    > 0 is a discount factor. (the
value ci is negative if the ith product is unwanted, e.g., a pollutant; |ci| is then the
cost of disposal per unit.)

putting it all together we arrive at the lp

ct s(0) +   ct s(1) +        +   n ct s(n )
maximize
subject to x(t) (cid:23) 0,
s(t) (cid:23) 0,
s(0) = g0     bx(1)
s(t) = ax(t)     bx(t + 1),
s(n ) = ax(n ),

t = 1, . . . , n
t = 0, . . . , n

t = 1, . . . , n     1

with variables x(1), . . . , x(n ), s(0), . . . , s(n ). this problem is a standard form lp;
the variables s(t) are the slack variables associated with the constraints bx(t+1) (cid:22)
ax(t).

150

4 id76 problems

chebyshev inequalities

we consider a id203 distribution for a discrete random variable x on a set
{u1, . . . , un}     r with n elements. we describe the distribution of x by a vector
p     rn, where

pi = prob(x = ui),

so p satis   es p (cid:23) 0 and 1t p = 1. conversely, if p satis   es p (cid:23) 0 and 1t p = 1, then
it de   nes a id203 distribution for x. we assume that ui are known and    xed,
but the distribution p is not known.

if f is any function of x, then

e f =

nxi=1

pif (ui)

is a linear function of p. if s is any subset of r, then

prob(x     s) = xui   s

pi

is a linear function of p.

although we do not know p, we are given prior knowledge of the following form:
we know upper and lower bounds on expected values of some functions of x, and
probabilities of some subsets of r. this prior knowledge can be expressed as linear
inequality constraints on p,

  i     at

i p       i,

i = 1, . . . , m.

the problem is to give lower and upper bounds on e f0(x) = at
function of x.

0 p, where f0 is some

to    nd a lower bound we solve the lp

at
0 p

minimize
subject to p (cid:23) 0,
  i     at

1t p = 1
i p       i,

i = 1, . . . , m,

with variable p. the optimal value of this lp gives the lowest possible value of
e f0(x) for any distribution that is consistent with the prior information. more-
over, the bound is sharp: the optimal solution gives a distribution that is consistent
with the prior information and achieves the lower bound. in a similar way, we can
   nd the best upper bound by maximizing at
0 p subject to the same constraints. (we
will consider chebyshev inequalities in more detail in   7.4.1.)
piecewise-linear minimization

consider the (unconstrained) problem of minimizing the piecewise-linear, convex
function

f (x) = max

i=1,...,m

(at

i x + bi).

this problem can be transformed to an equivalent lp by    rst forming the epigraph
problem,

minimize
subject to maxi=1,...,m(at

t

i x + bi)     t,

4.3 linear optimization problems

151

and then expressing the inequality as a set of m separate inequalities:

minimize
subject to at

t

i x + bi     t,

i = 1, . . . , m.

this is an lp (in inequality form), with variables x and t.

4.3.2 linear-fractional programming

the problem of minimizing a ratio of a   ne functions over a polyhedron is called a
linear-fractional program:

f0(x)

minimize
subject to gx (cid:22) h
ax = b

(4.32)

where the objective function is given by

f0(x) =

ct x + d
et x + f

,

dom f0 = {x | et x + f > 0}.

the objective function is quasiconvex (in fact, quasilinear) so linear-fractional pro-
grams are quasiid76 problems.

transforming to a linear program

if the feasible set

{x | gx (cid:22) h, ax = b, et x + f > 0}

is nonempty, the linear-fractional program (4.32) can be transformed to an equiv-
alent linear program

ct y + dz

minimize
subject to gy     hz (cid:22) 0
ay     bz = 0
et y + f z = 1
z     0

(4.33)

with variables y, z.

to show the equivalence, we    rst note that if x is feasible in (4.32) then the

pair

y =

x

et x + f

,

z =

1

et x + f

is feasible in (4.33), with the same objective value ct y + dz = f0(x). it follows that
the optimal value of (4.32) is greater than or equal to the optimal value of (4.33).
conversely, if (y, z) is feasible in (4.33), with z 6= 0, then x = y/z is feasible
in (4.32), with the same objective value f0(x) = ct y + dz.
if (y, z) is feasible
in (4.33) with z = 0, and x0 is feasible for (4.32), then x = x0 + ty is feasible
in (4.32) for all t     0. moreover, limt       f0(x0 + ty) = ct y + dz, so we can    nd
feasible points in (4.32) with objective values arbitrarily close to the objective value
of (y, z). we conclude that the optimal value of (4.32) is less than or equal to the
optimal value of (4.33).

152

4 id76 problems

generalized linear-fractional programming

a generalization of the linear-fractional program (4.32) is the generalized linear-
fractional program in which

f0(x) = max
i=1,...,r

ct
i x + di
et
i x + fi

,

dom f0 = {x | et

i x + fi > 0, i = 1, . . . , r}.

the objective function is the pointwise maximum of r quasiconvex functions, and
therefore quasiconvex, so this problem is quasiconvex. when r = 1 it reduces to
the standard linear-fractional program.

example 4.7 von neumann growth problem. we consider an economy with n
sectors, and activity levels xi > 0 in the current period, and activity levels x+
i > 0 in
the next period. (in this problem we only consider one period.) there are m goods
which are consumed, and also produced, by the activity: an activity level x consumes
goods bx     rm, and produces goods ax. the goods consumed in the next period
cannot exceed the goods produced in the current period, i.e., bx+ (cid:22) ax. the growth
rate in sector i, over the period, is given by x+

i /xi.

von neumann   s growth problem is to    nd an activity level vector x that maximizes
the minimum growth rate across all sectors of the economy. this problem can be
expressed as a generalized linear-fractional problem

maximize mini=1,...,n x+
subject to x+ (cid:23) 0

i /xi

bx+ (cid:22) ax

with domain {(x, x+) | x     0}. note that this problem is homogeneous in x and x+,
so we can replace the implicit constraint x     0 by the explicit constraint x (cid:23) 1.

4.4 quadratic optimization problems

the id76 problem (4.15) is called a quadratic program (qp) if the
objective function is (convex) quadratic, and the constraint functions are a   ne. a
quadratic program can be expressed in the form

(1/2)xt p x + qt x + r

minimize
subject to gx (cid:22) h
ax = b,

(4.34)

where p     sn
a convex quadratic function over a polyhedron, as illustrated in    gure 4.5.

+, g     rm  n, and a     rp  n. in a quadratic program, we minimize
if the objective in (4.15) as well as the inequality constraint functions are (con-

vex) quadratic, as in

minimize
subject to

(1/2)xt p0x + qt
(1/2)xt pix + qt
ax = b,

0 x + r0
i x + ri     0,

i = 1, . . . , m

(4.35)

4.4 quadratic optimization problems

153

      f0(x   )

x   

p

figure 4.5 geometric illustration of qp. the feasible set p, which is a poly-
hedron, is shown shaded. the contour lines of the objective function, which
is convex quadratic, are shown as dashed curves. the point x    is optimal.

where pi     sn
+, i = 0, 1 . . . , m, the problem is called a quadratically constrained
quadratic program (qcqp). in a qcqp, we minimize a convex quadratic function
over a feasible region that is the intersection of ellipsoids (when pi     0).
quadratic programs include linear programs as a special case, by taking p = 0
in (4.34). quadratically constrained quadratic programs include quadratic pro-
grams (and therefore also linear programs) as a special case, by taking pi = 0
in (4.35), for i = 1, . . . , m.

4.4.1 examples

least-squares and regression

the problem of minimizing the convex quadratic function

kax     bk2

2 = xt at ax     2bt ax + bt b

is an (unconstrained) qp. it arises in many    elds and has many names, e.g., re-
gression analysis or least-squares approximation. this problem is simple enough to
have the well known analytical solution x = a   b, where a    is the pseudo-inverse
of a (see   a.5.4).
when linear inequality constraints are added, the problem is called constrained
regression or constrained least-squares, and there is no longer a simple analytical
solution. as an example we can consider regression with lower and upper bounds
on the variables, i.e.,

minimize
subject to

kax     bk2
li     xi     ui,

2

i = 1, . . . , n,

154

4 id76 problems

which is a qp. (we will study least-squares and regression problems in far more
depth in chapters 6 and 7.)

distance between polyhedra
the (euclidean) distance between the polyhedra p1 = {x | a1x (cid:22) b1} and p2 =
{x | a2x (cid:22) b2} in rn is de   ned as

dist(p1,p2) = inf{kx1     x2k2 | x1     p1, x2     p2}.

if the polyhedra intersect, the distance is zero.

to    nd the distance between p1 and p2, we can solve the qp

minimize
subject to a1x1 (cid:22) b1, a2x2 (cid:22) b2,

kx1     x2k2

2

with variables x1, x2     rn. this problem is infeasible if and only if one of the
polyhedra is empty. the optimal value is zero if and only if the polyhedra intersect,
in which case the optimal x1 and x2 are equal (and is a point in the intersection
p1   p2). otherwise the optimal x1 and x2 are the points in p1 and p2, respectively,
that are closest to each other. (we will study geometric problems involving distance
in more detail in chapter 8.)

bounding variance

we consider again the chebyshev inequalities example (page 150), where the vari-
able is an unknown id203 distribution given by p     rn, about which we have
some prior information. the variance of a random variable f (x) is given by

e f 2     (e f )2 =

f 2

i pi      nxi=1

nxi=1

fipi!2

,

(where fi = f (ui)), which is a concave quadratic function of p.

it follows that we can maximize the variance of f (x), subject to the given prior

information, by solving the qp

i=1 f 2

maximize pn

subject to p (cid:23) 0,
  i     at

i pi     (pn

1t p = 1
i p       i,

i=1 fipi)2

i = 1, . . . , m.

the optimal value gives the maximum possible variance of f (x), over all distribu-
tions that are consistent with the prior information; the optimal p gives a distri-
bution that achieves this maximum variance.

linear program with random cost

we consider an lp,

ct x

minimize
subject to gx (cid:22) h
ax = b,

4.4 quadratic optimization problems

155

with variable x     rn. we suppose that the cost function (vector) c     rn is
random, with mean value c and covariance e(c     c)(c     c)t =   . (we assume
for simplicity that the other problem parameters are deterministic.) for a given
x     rn, the cost ct x is a (scalar) random variable with mean e ct x = ct x and
variance

var(ct x) = e(ct x     e ct x)2 = xt   x.

in general there is a trade-o    between small expected cost and small cost vari-
ance. one way to take variance into account is to minimize a linear combination
of the expected value and the variance of the cost, i.e.,

e ct x +    var(ct x),

which is called the risk-sensitive cost. the parameter        0 is called the risk-
aversion parameter, since it sets the relative values of cost variance and expected
value. (for    > 0, we are willing to trade o    an increase in expected cost for a
su   ciently large decrease in cost variance).

to minimize the risk-sensitive cost we solve the qp

ct x +   xt   x

minimize
subject to gx (cid:22) h
ax = b.

markowitz portfolio optimization

we consider a classical portfolio problem with n assets or stocks held over a period
of time. we let xi denote the amount of asset i held throughout the period, with
xi in dollars, at the price at the beginning of the period. a normal long position
in asset i corresponds to xi > 0; a short position in asset i (i.e., the obligation to
buy the asset at the end of the period) corresponds to xi < 0. we let pi denote
the relative price change of asset i over the period, i.e., its change in price over
the period divided by its price at the beginning of the period. the overall return
on the portfolio is r = pt x (given in dollars). the optimization variable is the
portfolio vector x     rn.
a wide variety of constraints on the portfolio can be considered. the simplest
set of constraints is that xi     0 (i.e., no short positions) and 1t x = b (i.e., the
total budget to be invested is b, which is often taken to be one).
we take a stochastic model for price changes: p     rn is a random vector, with
known mean p and covariance   . therefore with portfolio x     rn, the return r
is a (scalar) random variable with mean pt x and variance xt   x. the choice of
portfolio x involves a trade-o    between the mean of the return, and its variance.

the classical portfolio optimization problem, introduced by markowitz, is the

qp

xt   x

minimize
subject to pt x     rmin
1t x = 1,

x (cid:23) 0,

where x, the portfolio, is the variable. here we    nd the portfolio that minimizes
the return variance (which is associated with the risk of the portfolio) subject to

156

4 id76 problems

achieving a minimum acceptable mean return rmin, and satisfying the portfolio
budget and no-shorting constraints.

many extensions are possible. one standard extension, for example, is to allow
short positions, i.e., xi < 0. to do this we introduce variables xlong and xshort,
with

xlong (cid:23) 0,

xshort (cid:23) 0,

x = xlong     xshort,

1t xshort       1t xlong.

the last constraint limits the total short position at the beginning of the period to
some fraction    of the total long position at the beginning of the period.

as another extension we can include linear transaction costs in the portfolio
optimization problem. starting from a given initial portfolio xinit we buy and sell
assets to achieve the portfolio x, which we then hold over the period as described
above. we are charged a transaction fee for buying and selling assets, which is
proportional to the amount bought or sold. to handle this, we introduce variables
ubuy and usell, which determine the amount of each asset we buy and sell before
the holding period. we have the constraints

x = xinit + ubuy     usell,

ubuy (cid:23) 0,

usell (cid:23) 0.

we replace the simple budget constraint 1t x = 1 with the condition that the initial
buying and selling, including transaction fees, involves zero net cash:

(1     fsell)1t usell = (1 + fbuy)1t ubuy

here the lefthand side is the total proceeds from selling assets, less the selling
transaction fee, and the righthand side is the total cost, including transaction fee,
of buying assets. the constants fbuy     0 and fsell     0 are the transaction fee rates
for buying and selling (assumed the same across assets, for simplicity).
the problem of minimizing return variance, subject to a minimum mean return,

and the budget and trading constraints, is a qp with variables x, ubuy, usell.

4.4.2 second-order cone programming

a problem that is closely related to quadratic programming is the second-order
cone program (socp):

minimize
subject to

f t x
kaix + bik2     ct
f x = g,

i x + di,

i = 1, . . . , m

(4.36)

where x     rn is the optimization variable, ai     rni  n, and f     rp  n. we call a
constraint of the form

kax + bk2     ct x + d,

where a     rk  n, a second-order cone constraint, since it is the same as requiring
the a   ne function (ax + b, ct x + d) to lie in the second-order cone in rk+1.
when ci = 0, i = 1, . . . , m, the socp (4.36) is equivalent to a qcqp (which
is obtained by squaring each of the constraints). similarly, if ai = 0, i = 1, . . . , m,
then the socp (4.36) reduces to a (general) lp. second-order cone programs are,
however, more general than qcqps (and of course, lps).

4.4 quadratic optimization problems

157

robust id135

we consider a linear program in inequality form,

minimize
subject to at

ct x
i x     bi,

i = 1, . . . , m,

in which there is some uncertainty or variation in the parameters c, ai, bi. to
simplify the exposition we assume that c and bi are    xed, and that ai are known
to lie in given ellipsoids:

ai     ei = {ai + piu | kuk2     1},

where pi     rn  n. (if pi is singular we obtain       at    ellipsoids, of dimension rank pi;
pi = 0 means that ai is known perfectly.)
we will require that the constraints be satis   ed for all possible values of the

parameters ai, which leads us to the robust linear program

the robust linear constraint, at

minimize
subject to at

i = 1, . . . , m.

ct x
i x     bi for all ai     ei,
i x     bi for all ai     ei, can be expressed as
sup{at

i x | ai     ei}     bi,

(4.37)

the lefthand side of which can be expressed as

sup{at

i x + sup{ut p t
i x + kp t
i xk2.
thus, the robust linear constraint can be expressed as

i x | ai     ei} = at
= at

i x | kuk2     1}

at
i x + kp t

i xk2     bi,

which is evidently a second-order cone constraint. hence the robust lp (4.37) can
be expressed as the socp

minimize
subject to at

ct x
i x + kp t

i xk2     bi,

i = 1, . . . , m.

note that the additional norm terms act as id173 terms; they prevent x
from being large in directions with considerable uncertainty in the parameters ai.

id135 with random constraints

the robust lp described above can also be considered in a statistical framework.
here we suppose that the parameters ai are independent gaussian random vectors,
with mean ai and covariance   i. we require that each constraint at
i x     bi should
hold with a id203 (or con   dence) exceeding   , where        0.5, i.e.,

prob(at

i x     bi)       .

(4.38)

158

4 id76 problems

we will show that this id203 constraint can be expressed as a second-order
cone constraint.

letting u = at

i x, with   2 denoting its variance, this constraint can be written

as

prob(cid:18) u     u

      

bi     u

   (cid:19)       .

since (u     u)/   is a zero mean unit variance gaussian variable, the id203
above is simply   ((bi     u)/  ), where

  (z) =

e   t2/2 dt

1

   2  z z

      

is the cumulative distribution function of a zero mean unit variance gaussian ran-
dom variable. thus the id203 constraint (4.38) can be expressed as

bi     u

            1(  ),

or, equivalently,

u +      1(  )       bi.

from u = at

i x and    = (xt   ix)1/2 we obtain
i x +      1(  )k  1/2
at

i xk2     bi.

by our assumption that        1/2, we have      1(  )     0, so this constraint is a
second-order cone constraint.
in summary, the problem

minimize
subject to prob(at

ct x

i x     bi)       ,

i = 1, . . . , m

can be expressed as the socp

minimize
subject to at

ct x
i x +      1(  )k  1/2

i xk2     bi,

i = 1, . . . , m.

(we will consider robust id76 problems in more depth in chapter 6.
see also exercises 4.13, 4.28, and 4.59.)

example 4.8 portfolio optimization with loss risk constraints. we consider again the
classical markowitz portfolio problem described above (page 155). we assume here
that the price change vector p     rn is a gaussian random variable, with mean p
and covariance   . therefore the return r is a gaussian random variable with mean
r = pt x and variance   2

r = xt   x.

consider a loss risk constraint of the form

where    is a given unwanted return level (e.g., a large loss) and    is a given maximum
id203.

prob(r       )       ,

(4.39)

4.4 quadratic optimization problems

159

as in the stochastic interpretation of the robust lp given above, we can express this
constraint using the cumulative distribution function    of a unit gaussian random
variable. the inequality (4.39) is equivalent to

pt x +      1(  )k  1/2xk2       .

provided        1/2 (i.e.,      1(  )     0), this loss risk constraint is a second-order cone
constraint. (if    > 1/2, the loss risk constraint becomes nonconvex in x.)

the problem of maximizing the expected return subject to a bound on the loss
risk (with        1/2), can therefore be cast as an socp with one second-order cone
constraint:

maximize
subject to

pt x
pt x +      1(  )k  1/2xk2       
x (cid:23) 0,

1t x = 1.

there are many extensions of this problem. for example, we can impose several loss
risk constraints, i.e.,

prob(r       i)       i,

i = 1, . . . , k,

(where   i     1/2), which expresses the risks (  i) we are willing to accept for various
levels of loss (  i).

minimal surface
consider a di   erentiable function f : r2     r with dom f = c. the surface area
of its graph is given by

a =zcq1 + k   f (x)k2

2 dx =zc k(   f (x), 1)k2 dx,

which is a convex functional of f . the minimal surface problem is to    nd the
function f that minimizes a subject to some constraints, for example, some given
values of f on the boundary of c.

we will approximate this problem by discretizing the function f . let c =
[0, 1]    [0, 1], and let fij denote the value of f at the point (i/k, j/k), for i, j =
0, . . . , k. an approximate expression for the gradient of f at the point x =
(i/k, j/k) can be found using forward di   erences:

substituting this into the expression for the area of the graph, and approximating
the integral as a sum, we obtain an approximation for the area of the graph:

   f (x)     k(cid:20) fi+1,j     fi,j
fi,j+1     fi,j (cid:21) .
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

k   1xi,j=0

      

1
k 2

1

k(fi+1,j     fi,j)
k(fi,j+1     fi,j)

      (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

a     adisc =

the discretized area approximation adisc is a convex function of fij.

we can consider a wide variety of constraints on fij, such as equality or in-
equality constraints on any of its entries (for example, on the boundary values), or

160

4 id76 problems

on its moments. as an example, we consider the problem of    nding the minimal
area surface with    xed boundary values on the left and right edges of the square:

minimize adisc
subject to

f0j = lj,
fkj = rj,

j = 0, . . . , k
j = 0, . . . , k

(4.40)

where fij, i, j = 0, . . . , k, are the variables, and lj, rj are the given boundary
values on the left and right sides of the square.

we can transform the problem (4.40) into an socp by introducing new vari-

ables tij, i, j = 0, . . . , k     1:

i,j=0 tij

(1/k 2)pk   1
      

k(fi+1,j     fi,j)
k(fi,j+1     fi,j)

1

minimize

subject to (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

f0j = lj,
fkj = rj,

j = 0, . . . , k
j = 0, . . . , k.

      (cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

    tij,

i, j = 0, . . . , k     1

4.5 geometric programming

in this section we describe a family of optimization problems that are not convex
in their natural form. these problems can, however, be transformed to convex op-
timization problems, by a change of variables and a transformation of the objective
and constraint functions.

4.5.1 monomials and posynomials

a function f : rn     r with dom f = rn
f (x) = cxa1

++, de   ned as
2        xan
n ,

1 xa2

(4.41)

where c > 0 and ai     r, is called a monomial function, or simply, a monomial.
the exponents ai of a monomial can be any real numbers, including fractional or
negative, but the coe   cient c can only be positive. (the term    monomial    con   icts
with the standard de   nition from algebra, in which the exponents must be non-
negative integers, but this should not cause any confusion.) a sum of monomials,
i.e., a function of the form

f (x) =

kxk=1

ckxa1k

1 xa2k

2

       xank
n ,

(4.42)

where ck > 0, is called a posynomial function (with k terms), or simply, a posyn-
omial.

4.5 geometric programming

161

posynomials are closed under addition, multiplication, and nonnegative scal-
ing. monomials are closed under multiplication and division. if a posynomial is
multiplied by a monomial, the result is a posynomial; similarly, a posynomial can
be divided by a monomial, with the result a posynomial.

4.5.2 geometric programming

an optimization problem of the form

minimize
subject to

f0(x)
fi(x)     1,
hi(x) = 1,

i = 1, . . . , m
i = 1, . . . , p

(4.43)

where f0, . . . , fm are posynomials and h1, . . . , hp are monomials, is called a geomet-
ric program (gp). the domain of this problem is d = rn
++; the constraint x     0
is implicit.

extensions of geometric programming

several extensions are readily handled. if f is a posynomial and h is a monomial,
then the constraint f (x)     h(x) can be handled by expressing it as f (x)/h(x)     1
(since f /h is posynomial). this includes as a special case a constraint of the
form f (x)     a, where f is posynomial and a > 0. in a similar way if h1 and h2
are both nonzero monomial functions, then we can handle the equality constraint
h1(x) = h2(x) by expressing it as h1(x)/h2(x) = 1 (since h1/h2 is monomial). we
can maximize a nonzero monomial objective function, by minimizing its inverse
(which is also a monomial).

for example, consider the problem

maximize
subject to

x/y
2     x     3
x2 + 3y/z        y
x/y = z2,

with variables x, y, z     r (and the implicit constraint x, y, z > 0). using
the simple transformations described above, we obtain the equivalent standard
form gp

minimize
subject to

x   1y
2x   1     1,
(1/3)x     1
x2y   1/2 + 3y1/2z   1     1
xy   1z   2 = 1.

we will refer to a problem like this one, that is easily transformed to an equiva-
lent gp in the standard form (4.43), also as a gp. (in the same way that we refer
to a problem easily transformed to an lp as an lp.)

162

4 id76 problems

4.5.3 geometric program in convex form

geometric programs are not (in general) id76 problems, but they
can be transformed to convex problems by a change of variables and a transforma-
tion of the objective and constraint functions.

we will use the variables de   ned as yi = log xi, so xi = eyi . if f is the monomial

function of x given in (4.41), i.e.,

f (x) = cxa1

1 xa2

2        xan
n ,

then

f (x) = f (ey1 , . . . , eyn )

= c(ey1 )a1        (eyn )an
= eat y+b,

where b = log c. the change of variables yi = log xi turns a monomial function
into the exponential of an a   ne function.

similarly, if f is the posynomial given by (4.42), i.e.,

f (x) =

kxk=1

ckxa1k

1 xa2k

2

       xank
n ,

then

f (x) =

eat

k y+bk ,

kxk=1

where ak = (a1k, . . . , ank) and bk = log ck. after the change of variables, a posyn-
omial becomes a sum of exponentials of a   ne functions.

the geometric program (4.43) can be expressed in terms of the new variable y

as

0ky+b0k

minimize pk0
subject to pki

egt

k=1 eat
k=1 eat
i y+hi = 1,

iky+bik     1,

i = 1, . . . , m

i = 1, . . . , p,

where aik     rn, i = 0, . . . , m, contain the exponents of the posynomial inequality
constraints, and gi     rn, i = 1, . . . , p, contain the exponents of the monomial
equality constraints of the original geometric program.
now we transform the objective and constraint functions, by taking the loga-

rithm. this results in the problem

minimize

subject to

  f0(y) = log(cid:16)pk0
  fi(y) = log(cid:16)pki

k=1 eat
k=1 eat
i y + hi = 0,

  hi(y) = gt

0ky+b0k(cid:17)
iky+bik(cid:17)     0,

i = 1, . . . , p.

i = 1, . . . , m

(4.44)

since the functions   fi are convex, and   hi are a   ne, this problem is a convex
optimization problem. we refer to it as a geometric program in convex form. to

4.5 geometric programming

163

distinguish it from the original geometric program, we refer to (4.43) as a geometric
program in posynomial form.

note that the transformation between the posynomial form geometric pro-
gram (4.43) and the convex form geometric program (4.44) does not involve any
computation; the problem data for the two problems are the same.
it simply
changes the form of the objective and constraint functions.

if the posynomial objective and constraint functions all have only one term,
i.e., are monomials, then the convex form geometric program (4.44) reduces to a
(general) linear program. we can therefore consider geometric programming to be
a generalization, or extension, of id135.

4.5.4 examples

frobenius norm diagonal scaling
consider a matrix m     rn  n, and the associated linear function that maps u
into y = m u. suppose we scale the coordinates, i.e., change variables to   u = du,
  y = dy, where d is diagonal, with dii > 0. in the new coordinates the linear
function is given by   y = dm d   1   u.

now suppose we want to choose the scaling in such a way that the resulting
matrix, dm d   1, is small. we will use the frobenius norm (squared) to measure
the size of the matrix:

kdm d   1k2

f = tr(cid:16)(cid:0)dm d   1(cid:1)t(cid:0)dm d   1(cid:1)(cid:17)

=

=

ij

nxi,j=1(cid:0)dm d   1(cid:1)2
nxi,j=1

i /d2
j ,

ijd2

m 2

where d = diag(d). since this is a posynomial in d, the problem of choosing the
scaling d to minimize the frobenius norm is an unconstrained geometric program,

minimize pn

i,j=1 m 2

ijd2

i /d2
j ,

with variable d. the only exponents in this geometric program are 0, 2, and    2.
design of a cantilever beam

we consider the design of a cantilever beam, which consists of n segments, num-
bered from right to left as 1, . . . , n , as shown in    gure 4.6. each segment has unit
length and a uniform rectangular cross-section with width wi and height hi. a
vertical load (force) f is applied at the right end of the beam. this load causes
the beam to de   ect (downward), and induces stress in each segment of the beam.
we assume that the de   ections are small, and that the material is linearly elastic,
with young   s modulus e.

164

4 id76 problems

segment 4

segment 3

segment 2

segment 1

figure 4.6 segmented cantilever beam with 4 segments. each segment has
unit length and a rectangular pro   le. a vertical force f is applied at the
right end of the beam.

f

the design variables in the problem are the widths wi and heights hi of the n
segments. we seek to minimize the total volume of the beam (which is proportional
to its weight),

subject to some design constraints. we impose upper and lower bounds on width
and height of the segments,

w1h1 +        + wn hn ,

wmin     wi     wmax,

hmin     hi     hmax,

i = 1, . . . , n,

as well as the aspect ratios,

smin     hi/wi     smax.

in addition, we have a limit on the maximum allowable stress in the material, and
on the vertical de   ection at the end of the beam.

we    rst consider the maximum stress constraint. the maximum stress in seg-
i ). we impose the constraints

ment i, which we denote   i, is given by   i = 6if/(wih2

6if
wih2

i       max,

i = 1, . . . , n,

to ensure that the stress does not exceed the maximum allowable value   max any-
where in the beam.

the last constraint is a limit on the vertical de   ection at the end of the beam,

which we will denote y1:

y1     ymax.

the de   ection y1 can be found by a recursion that involves the de   ection and slope
of the beam segments:

vi = 12(i     1/2)

f

ewih3
i

+ vi+1,

yi = 6(i     1/3)

f

ewih3
i

+ vi+1 + yi+1,

(4.45)

for i = n, n     1, . . . , 1, with starting values vn +1 = yn +1 = 0. in this recursion,
yi is the de   ection at the right end of segment i, and vi is the slope at that point.
we can use the recursion (4.45) to show that these de   ection and slope quantities

4.5 geometric programming

165

are in fact posynomial functions of the variables w and h. we    rst note that vn +1
and yn +1 are zero, and therefore posynomials. now assume that vi+1 and yi+1 are
posynomial functions of w and h. the lefthand equation in (4.45) shows that vi is
the sum of a monomial and a posynomial (i.e., vi+1), and therefore is a posynomial.
from the righthand equation in (4.45), we see that the de   ection yi is the sum of
a monomial and two posynomials (vi+1 and yi+1), and so is a posynomial.
in
particular, the de   ection at the end of the beam, y1, is a posynomial.

the problem is then

i=1 wihi

minimize pn

subject to wmin     wi     wmax,
hmin     hi     hmax,
smin     hi/wi     smax,
6if/(wih2
i )       max,
y1     ymax,

i = 1, . . . , n
i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

(4.46)

with variables w and h. this is a gp, since the objective is a posynomial, and
the constraints can all be expressed as posynomial inequalities. (in fact, the con-
straints can be all be expressed as monomial inequalities, with the exception of the
de   ection limit, which is a complicated posynomial inequality.)

when the number of segments n is large, the number of monomial terms ap-
pearing in the posynomial y1 grows approximately as n 2. another formulation of
this problem, explored in exercise 4.31, is obtained by introducing v1, . . . , vn and
y1, . . . , yn as variables, and including a modi   ed version of the recursion as a set
of constraints. this formulation avoids this growth in the number of monomial
terms.

minimizing spectral radius via perron-frobenius theory
suppose the matrix a     rn  n is elementwise nonnegative, i.e., aij     0 for i, j =
1, . . . , n, and irreducible, which means that the matrix (i + a)n   1 is elementwise
positive. the perron-frobenius theorem states that a has a positive real eigenvalue
  pf equal to its spectral radius, i.e., the largest magnitude of its eigenvalues. the
perron-frobenius eigenvalue   pf determines the asymptotic rate of growth or decay
of ak, as k        ; in fact, the matrix ((1/  pf )a)k converges. roughly speaking,
this means that as k        , ak grows like   k
pf , if
  pf < 1.

pf , if   pf > 1, or decays like   k

a basic result in the theory of nonnegative matrices states that the perron-

frobenius eigenvalue is given by

  pf = inf{   | av (cid:22)   v for some v     0}

(and moreover, that the in   mum is achieved). the inequality av (cid:22)   v can be
expressed as

nxj=1

aijvj/(  vi)     1,

i = 1, . . . , n,

(4.47)

which is a set of posynomial inequalities in the variables aij, vi, and   . thus,
the condition that   pf        can be expressed as a set of posynomial inequalities

166

4 id76 problems

in a, v, and   . this allows us to solve some optimization problems involving the
perron-frobenius eigenvalue using geometric programming.

suppose that the entries of the matrix a are posynomial functions of some
underlying variable x     rk.
in this case the inequalities (4.47) are posynomial
inequalities in the variables x     rk, v     rn, and        r. we consider the problem
of choosing x to minimize the perron-frobenius eigenvalue (or spectral radius) of
a, possibly subject to posynomial inequalities on x,

minimize
subject to

  pf (a(x))
fi(x)     1,

i = 1, . . . , p,

where fi are posynomials. using the characterization above, we can express this
problem as the gp

  

minimize

subject to pn

j=1 aijvj/(  vi)     1,
i = 1, . . . , p,

fi(x)     1,

i = 1, . . . , n

where the variables are x, v, and   .

as a speci   c example, we consider a simple model for the population dynamics
for a bacterium, with time or period denoted by t = 0, 1, 2, . . ., in hours. the vector
p(t)     r4
+ characterizes the population age distribution at period t: p1(t) is the
total population between 0 and 1 hours old; p2(t) is the total population between
1 and 2 hours old; and so on. we (arbitrarily) assume that no bacteria live more
than 4 hours. the population propagates in time as p(t + 1) = ap(t), where

a =            

b1
s1
0
0

b2
0
s2
0

b3
0
0
s3

b4
0
0
0

             .

here bi is the birth rate among bacteria in age group i, and si is the survival rate
from age group i into age group i + 1. we assume that bi > 0 and 0 < si < 1,
which implies that the matrix a is irreducible.

the perron-frobenius eigenvalue of a determines the asymptotic growth or
decay rate of the population.
if   pf < 1, the population converges to zero like
  t
pf , and so has a half-life of    1/ log2   pf hours. if   pf > 1 the population grows
geometrically like   t
pf , with a doubling time of 1/ log2   pf hours. minimizing the
spectral radius of a corresponds to    nding the fastest decay rate, or slowest growth
rate, for the population.

as our underlying variables, on which the matrix a depends, we take c1 and c2,
the concentrations of two chemicals in the environment that a   ect the birth and
survival rates of the bacteria. we model the birth and survival rates as monomial
functions of the two concentrations:

bi = bnom
si = snom

i

i

1

(c1/cnom
(c1/cnom

1

)  i (c2/cnom
)  i (c2/cnom

2

)  i ,
)  i ,

2

i = 1, . . . , 4
i = 1, . . . , 3.

here, bnom
is nominal
concentration of chemical i. the constants   i,   i,   i, and   i give the e   ect on the

is nominal survival rate, and cnom

is nominal birth rate, snom

i

i

i

4.6 generalized inequality constraints

167

birth and survival rates due to changes in the concentrations of the chemicals away
from the nominal values. for example   2 =    0.3 and   1 = 0.5 means that an
increase in concentration of chemical 1, over the nominal concentration, causes a
decrease in the birth rate of bacteria that are between 1 and 2 hours old, and an
increase in the survival rate of bacteria from 0 to 1 hours old.

we assume that the concentrations c1 and c2 can be independently increased or
decreased (say, within a factor of 2), by administering drugs, and pose the problem
of    nding the drug mix that maximizes the population decay rate (i.e., minimizes
  pf (a)). using the approach described above, this problem can be posed as the
gp

minimize
subject to

  
b1v1 + b2v2 + b3v3 + b4v4       v1
s1v1       v2
s2v2       v3
s3v3       v4
1/2     ci/cnom
bi = bnom
si = snom

i = 1, 2
)  i (c2/cnom
)  i (c2/cnom

i     2,
(c1/cnom
(c1/cnom

1

2

i

2

1

i

)  i ,
)  i ,

i = 1, . . . , 4
i = 1, . . . , 3,

with variables bi, si, ci, vi, and   .

4.6 generalized inequality constraints

one very useful generalization of the standard form id76 prob-
lem (4.15) is obtained by allowing the inequality constraint functions to be vector
valued, and using generalized inequalities in the constraints:

minimize
subject to

f0(x)
fi(x) (cid:22)ki 0,
ax = b,

i = 1, . . . , m

(4.48)

where f0 : rn     r, ki     rki are proper cones, and fi : rn     rki are ki-convex.
we refer to this problem as a (standard form) id76 problem with
generalized inequality constraints. problem (4.15) is a special case with ki = r+,
i = 1, . . . , m.

many of the results for ordinary id76 problems hold for problems

with generalized inequalities. some examples are:

    the feasible set, any sublevel set, and the optimal set are convex.
    any point that is locally optimal for the problem (4.48) is globally optimal.
    the optimality condition for di   erentiable f0, given in   4.2.3, holds without

any change.

we will also see (in chapter 11) that id76 problems with generalized
inequality constraints can often be solved as easily as ordinary id76
problems.

168

4 id76 problems

4.6.1 conic form problems

among the simplest id76 problems with generalized inequalities are
the conic form problems (or cone programs), which have a linear objective and one
inequality constraint function, which is a   ne (and therefore k-convex):

ct x

minimize
subject to f x + g (cid:22)k 0

ax = b.

(4.49)

when k is the nonnegative orthant, the conic form problem reduces to a linear
program. we can view conic form problems as a generalization of linear programs
in which componentwise inequality is replaced with a generalized linear inequality.
continuing the analogy to id135, we refer to the conic form prob-

lem

ct x

minimize
subject to x (cid:23)k 0
ax = b

as a conic form problem in standard form. similarly, the problem

ct x

minimize
subject to f x + g (cid:22)k 0

is called a conic form problem in inequality form.

4.6.2 semide   nite programming

when k is sk
conic form problem is called a semide   nite program (sdp), and has the form

+, the cone of positive semide   nite k    k matrices, the associated

ct x

minimize
subject to x1f1 +        + xnfn + g (cid:22) 0

ax = b,

(4.50)

where g, f1, . . . , fn     sk, and a     rp  n. the inequality here is a linear matrix
inequality (see example 2.10).
if the matrices g, f1, . . . , fn are all diagonal, then the lmi in (4.50) is equiva-
lent to a set of n linear inequalities, and the sdp (4.50) reduces to a linear program.

standard and inequality form semide   nite programs

following the analogy to lp, a standard form sdp has linear equality constraints,
and a (matrix) nonnegativity constraint on the variable x     sn:

minimize
subject to

tr(cx)
tr(aix) = bi,
x (cid:23) 0,

i = 1, . . . , p

(4.51)

4.6 generalized inequality constraints

169

where c, a1, . . . , ap     sn. (recall that tr(cx) =pn

i,j=1 cijxij is the form of a
general real-valued linear function on sn.) this form should be compared to the
standard form linear program (4.28). in lp and sdp standard forms, we minimize
a linear function of the variable, subject to p linear equality constraints on the
variable, and a nonnegativity constraint on the variable.

an inequality form sdp, analogous to an inequality form lp (4.29), has no

equality constraints, and one lmi:

ct x

minimize
subject to x1a1 +        + xnan (cid:22) b,

with variable x     rn, and parameters b, a1, . . . , an     sk, c     rn.
multiple lmis and linear inequalities

it is common to refer to a problem with linear objective, linear equality and in-
equality constraints, and several lmi constraints, i.e.,

minimize
subject to f (i)(x) = x1f (i)

ct x

gx (cid:22) h,

1 +        + xnf (i)

n + g(i) (cid:22) 0,

ax = b,

i = 1, . . . , k

as an sdp as well. such problems are readily transformed to an sdp, by forming
a large block diagonal lmi from the individual lmis and linear inequalities:

ct x

minimize
subject to diag(gx     h, f (1)(x), . . . , f (k)(x)) (cid:22) 0

ax = b.

4.6.3 examples

second-order cone programming

the socp (4.36) can be expressed as a conic form problem

ct x

minimize
subject to    (aix + bi, ct

f x = g,

i x + di) (cid:22)ki 0,

i = 1, . . . , m

in which

ki = {(y, t)     rni+1 | kyk2     t},

i.e., the second-order cone in rni+1. this explains the name second-order cone
program for the optimization problem (4.36).

matrix norm minimization
let a(x) = a0 + x1a1 +        + xnan, where ai     rp  q. we consider the uncon-
strained problem

minimize

ka(x)k2,

170

4 id76 problems

where k    k2 denotes the spectral norm (maximum singular value), and x     rn is
the variable. this is a convex problem since ka(x)k2 is a convex function of x.
using the fact that kak2     s if and only if at a (cid:22) s2i (and s     0), we can
express the problem in the form

s

minimize
subject to a(x)t a(x) (cid:22) si,

with variables x and s. since the function a(x)t a(x)     si is matrix convex in
(x, s), this is a id76 problem with a single q    q matrix inequality
constraint.
we can also formulate the problem using a single linear matrix inequality of

size (p + q)    (p + q), using the fact that

at a (cid:22) t2i (and t     0)        (cid:20) ti

at

a

ti (cid:21) (cid:23) 0.

(see   a.5.5). this results in the sdp
t

minimize

subject to (cid:20)

ti

a(x)t

a(x)

ti

(cid:21) (cid:23) 0

in the variables x and t.

moment problems

let t be a random variable in r. the expected values e tk (assuming they exist)
are called the (power) moments of the distribution of t. the following classical
results give a characterization of a moment sequence.

if there is a id203 distribution on r such that xk = e tk, k = 0, . . . , 2n,

then x0 = 1 and

h(x0, . . . , x2n) =

(cid:23) 0.

(4.52)

x0
x1
x2
...
xn   1
xn

x2
x1
x3
x2
x4
x3
...
...
xn+1
xn
xn+1 xn+2

. . .
. . .
. . .

xn   1
xn
xn+1
...

xn
xn+1
xn+2
...

. . . x2n   2 x2n   1
. . . x2n   1
x2n

                           

                           

(the matrix h is called the hankel matrix associated with x0, . . . , x2n.) this is
easy to see: let xi = e ti, i = 0, . . . , 2n be the moments of some distribution, and
let y = (y0, y1, . . . yn)     rn+1. then we have

yt h(x0, . . . , x2n)y =

nxi,j=0

yiyj e ti+j = e(y0 + y1t1 +        + yntn)2     0.

the following partial converse is less obvious: if x0 = 1 and h(x)     0, then there
exists a id203 distribution on r such that xi = e ti, i = 0, . . . , 2n. (for a

4.6 generalized inequality constraints

171

proof, see exercise 2.37.) now suppose that x0 = 1, and h(x) (cid:23) 0 (but possibly
h(x) 6    0), i.e., the linear matrix inequality (4.52) holds, but possibly not strictly.
in this case, there is a sequence of distributions on r, whose moments converge to
x. in summary: the condition that x0, . . . , x2n be the moments of some distribution
on r (or the limit of the moments of a sequence of distributions) can be expressed
as the linear matrix inequality (4.52) in the variable x, together with the linear
equality x0 = 1. using this fact, we can cast some interesting problems involving
moments as sdps.

suppose t is a random variable on r. we do not know its distribution, but we

do know some bounds on the moments, i.e.,

  

k     e tk       k,

k = 1, . . . , 2n

(which includes, as a special case, knowing exact values of some of the moments).
let p(t) = c0 + c1t +        + c2nt2n be a given polynomial in t. the expected value
of p(t) is linear in the moments e ti:

e p(t) =

ci e ti =

2nxi=0

cixi.

2nxi=0

we can compute upper and lower bounds for e p(t),

minimize (maximize) e p(t)
subject to

  

k     e tk       k,

k = 1, . . . , 2n,

over all id203 distributions that satisfy the given moment bounds, by solving
the sdp

minimize (maximize)
subject to

c1x1 +        + c2nx2n
k     xk       k,
  
h(1, x1, . . . , x2n) (cid:23) 0

k = 1, . . . , 2n

with variables x1, . . . , x2n. this gives bounds on e p(t), over all id203 dis-
tributions that satisfy the known moment constraints. the bounds are sharp in
the sense that there exists a sequence of distributions, whose moments satisfy the
given moment bounds, for which e p(t) converges to the upper and lower bounds
found by these sdps.

bounding portfolio risk with incomplete covariance information

we consider once again the setup for the classical markowitz portfolio problem (see
page 155). we have a portfolio of n assets or stocks, with xi denoting the amount
of asset i that is held over some investment period, and pi denoting the relative
price change of asset i over the period. the change in total value of the portfolio
is pt x. the price change vector p is modeled as a random vector, with mean and
covariance

p = e p,

   = e(p     p)(p     p)t .

the change in value of the portfolio is therefore a random variable with mean pt x
and standard deviation    = (xt   x)1/2. the risk of a large loss, i.e., a change
in portfolio value that is substantially below its expected value, is directly related

172

4 id76 problems

to the standard deviation   , and increases with it. for this reason the standard
deviation    (or the variance   2) is used as a measure of the risk associated with
the portfolio.

in the classical portfolio optimization problem, the portfolio x is the optimiza-
tion variable, and we minimize the risk subject to a minimum mean return and
other constraints. the price change statistics p and    are known problem param-
eters. in the risk bounding problem considered here, we turn the problem around:
we assume the portfolio x is known, but only partial information is available about
the covariance matrix   . we might have, for example, an upper and lower bound
on each entry:

lij       ij     uij,

i, j = 1, . . . , n,

where l and u are given. we now pose the question: what is the maximum risk
for our portfolio, over all covariance matrices consistent with the given bounds?
we de   ne the worst-case variance of the portfolio as

  2
wc = sup{xt   x | lij       ij     uij, i, j = 1, . . . , n,    (cid:23) 0}.

we have added the condition    (cid:23) 0, which the covariance matrix must, of course,
satisfy.

we can    nd   wc by solving the sdp

xt   x

maximize
subject to lij       ij     uij,

   (cid:23) 0

i, j = 1, . . . , n

with variable        sn (and problem parameters x, l, and u ). the optimal    is
the worst covariance matrix consistent with our given bounds on the entries, where
   worst    means largest risk with the (given) portfolio x. we can easily construct
a distribution for p that is consistent with the given bounds, and achieves the
worst-case variance, from an optimal    for the sdp. for example, we can take
p = p +   1/2v, where v is any random vector with e v = 0 and e vvt = i.

evidently we can use the same method to determine   wc for any prior informa-

tion about    that is convex. we list here some examples.

    known variance of certain portfolios. we might have equality constraints

such as

k   uk =   2
ut
k,

where uk and   k are given. this corresponds to prior knowledge that certain
known portfolios (given by uk) have known (or very accurately estimated)
variance.

    including e   ects of estimation error. if the covariance    is estimated from
empirical data, the estimation method will give an estimate     , and some in-
formation about the reliability of the estimate, such as a con   dence ellipsoid.
this can be expressed as

where c is a positive de   nite quadratic form on sn, and the constant   
determines the con   dence level.

c(           )       ,

4.6 generalized inequality constraints

173

    factor models. the covariance might have the form

   = f   factorf t + d,

where f     rn  k,   factor     sk, and d is diagonal. this corresponds to a
model of the price changes of the form

p = f z + d,

where z is a random variable (the underlying factors that a   ect the price
changes) and di are independent (additional volatility of each asset price).
we assume that the factors are known. since    is linearly related to   factor
and d, we can impose any convex constraint on them (representing prior
information) and still compute   wc using id76.

    information about correlation coe   cients. in the simplest case, the diagonal
entries of    (i.e., the volatilities of each asset price) are known, and bounds
on correlation coe   cients between price changes are known:

lij       ij =

  ij
  1/2
ii   1/2

jj

    uij,

i, j = 1, . . . , n.

since   ii are known, but   ij for i 6= j are not, these are linear inequalities.

fastest mixing markov chain on a graph

we consider an undirected graph, with nodes 1, . . . , n, and a set of edges

e     {1, . . . , n}    {1, . . . , n}.

here (i, j)     e means that nodes i and j are connected by an edge. since the
graph is undirected, e is symmetric: (i, j)     e if and only if (j, i)     e. we allow
the possibility of self-loops, i.e., we can have (i, i)     e.
we de   ne a markov chain, with state x(t)     {1, . . . , n}, for t     z+ (the set
of nonnegative integers), as follows. with each edge (i, j)     e we associate a
id203 pij, which is the id203 that x makes a transition between nodes
i and j. state transitions can only occur across edges; we have pij = 0 for (i, j) 6    e.
the probabilities associated with the edges must be nonnegative, and for each node,
the sum of the probabilities of links connected to the node (including a self-loop,
if there is one) must equal one.

the markov chain has transition id203 matrix

pij = prob(x(t + 1) = i | x(t) = j),

i, j = 1, . . . , n.

this matrix must satisfy

pij     0,

and also

i, j = 1, . . . , n,

1t p = 1t ,

p = p t ,

(4.53)

pij = 0 for (i, j) 6    e.

(4.54)

174

4 id76 problems

since p is symmetric and 1t p = 1t , we conclude p 1 = 1, so the uniform
distribution (1/n)1 is an equilibrium distribution for the markov chain. conver-
gence of the distribution of x(t) to (1/n)1 is determined by the second largest (in
magnitude) eigenvalue of p , i.e., by r = max{  2,     n}, where

1 =   1       2                  n

are the eigenvalues of p . we refer to r as the mixing rate of the markov chain.
if r = 1, then the distribution of x(t) need not converge to (1/n)1 (which means
the markov chain does not mix). when r < 1, the distribution of x(t) approaches
(1/n)1 asymptotically as rt, as t        . thus, the smaller r is, the faster the
markov chain mixes.
the fastest mixing markov chain problem is to    nd p , subject to the con-
straints (4.53) and (4.54), that minimizes r. (the problem data is the graph, i.e.,
e.) we will show that this problem can be formulated as an sdp.
since the eigenvalue   1 = 1 is associated with the eigenvector 1, we can express
the mixing rate as the norm of the matrix p , restricted to the subspace 1   : r =
kqp qk2, where q = i   (1/n)11t is the matrix representing orthogonal projection
on 1   . using the property p 1 = 1, we have

r = kqp qk2

= k(i     (1/n)11t )p (i     (1/n)11t )k2
= kp     (1/n)11tk2.

this shows that the mixing rate r is a convex function of p , so the fastest mixing
markov chain problem can be cast as the id76 problem

kp     (1/n)11tk2
minimize
subject to p 1 = 1
pij     0,
pij = 0 for (i, j) 6    e,

i, j = 1, . . . , n

with variable p     sn. we can express the problem as an sdp by introducing a
scalar variable t to bound the norm of p     (1/n)11t :

t

minimize
subject to    ti (cid:22) p     (1/n)11t (cid:22) ti
i, j = 1, . . . , n

p 1 = 1
pij     0,
pij = 0 for (i, j) 6    e.

(4.55)

4.7 vector optimization

4.7.1 general and convex vector optimization problems

in   4.6 we extended the standard form problem (4.1) to include vector-valued
constraint functions. in this section we investigate the meaning of a vector-valued

4.7 vector optimization

175

objective function. we denote a general vector optimization problem as

minimize (with respect to k)
subject to

f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p.

(4.56)

here x     rn is the optimization variable, k     rq is a proper cone, f0 : rn     rq
is the objective function, fi : rn     r are the inequality constraint functions, and
hi : rn     r are the equality constraint functions. the only di   erence between this
problem and the standard optimization problem (4.1) is that here, the objective
function takes values in rq, and the problem speci   cation includes a proper cone
k, which is used to compare objective values. in the context of vector optimization,
the standard optimization problem (4.1) is sometimes called a scalar optimization
problem.

we say the vector optimization problem (4.56) is a convex vector optimization
problem if the objective function f0 is k-convex, the inequality constraint functions
f1, . . . , fm are convex, and the equality constraint functions h1, . . . , hp are a   ne.
(as in the scalar case, we usually express the equality constraints as ax = b, where
a     rp  n.)
what meaning can we give to the vector optimization problem (4.56)? suppose
x and y are two feasible points (i.e., they satisfy the constraints). their associated
objective values, f0(x) and f0(y), are to be compared using the generalized inequal-
ity (cid:22)k . we interpret f0(x) (cid:22)k f0(y) as meaning that x is    better than or equal    in
value to y (as judged by the objective f0, with respect to k). the confusing aspect
of vector optimization is that the two objective values f0(x) and f0(y) need not be
comparable; we can have neither f0(x) (cid:22)k f0(y) nor f0(y) (cid:22)k f0(x), i.e., neither
is better than the other. this cannot happen in a scalar objective optimization
problem.

4.7.2 optimal points and values

we    rst consider a special case, in which the meaning of the vector optimization
problem is clear. consider the set of objective values of feasible points,

o = {f0(x) |    x     d, fi(x)     0, i = 1, . . . , m, hi(x) = 0, i = 1, . . . , p}     rq,

which is called the set of achievable objective values. if this set has a minimum
element (see   2.4.2), i.e., there is a feasible x such that f0(x) (cid:22)k f0(y) for all
feasible y, then we say x is optimal for the problem (4.56), and refer to f0(x) as
the optimal value of the problem. (when a vector optimization problem has an
optimal value, it is unique.) if x    is an optimal point, then f0(x   ), the objective
at x   , can be compared to the objective at every other feasible point, and is better
than or equal to it. roughly speaking, x    is unambiguously a best choice for x,
among feasible points.

a point x    is optimal if and only if it is feasible and

o     f0(x   ) + k

(4.57)

176

4 id76 problems

o

f0(x   )

figure 4.7 the set o of achievable values for a vector optimization with
objective values in r2, with cone k = r2
+, is shown shaded. in this case,
the point labeled f0(x   ) is the optimal value of the problem, and x    is an
optimal point. the objective value f0(x   ) can be compared to every other
achievable value f0(y), and is better than or equal to f0(y). (here,    better
than or equal to    means    is below and to the left of   .) the lightly shaded
region is f0(x   )+k, which is the set of all z     r2 corresponding to objective
values worse than (or equal to) f0(x   ).

(see   2.4.2). the set f0(x   ) + k can be interpreted as the set of values that are
worse than, or equal to, f0(x   ), so the condition (4.57) states that every achievable
value falls in this set. this is illustrated in    gure 4.7. most vector optimization
problems do not have an optimal point and an optimal value, but this does occur
in some special cases.

example 4.9 best linear unbiased estimator. suppose y = ax + v, where v     rm is
a measurement noise, y     rm is a vector of measurements, and x     rn is a vector to
be estimated, given the measurement y. we assume that a has rank n, and that the
measurement noise satis   es e v = 0, e vvt = i, i.e., its components are zero mean
and uncorrelated.

a linear estimator of x has the formbx = f y. the estimator is called unbiased if for
all x we have ebx = x, i.e., if f a = i. the error covariance of an unbiased estimator

is

our goal is to    nd an unbiased estimator that has a    small    error covariance matrix.
we can compare error covariances using matrix inequality, i.e., with respect to sn
+.

e(bx     x)(bx     x)t = e f vvt f t = f f t .

this has the following interpretation: supposebx1 = f1y,bx2 = f2y are two unbiased

estimators. then the    rst estimator is at least as good as the second, i.e., f1f t
f2f t

2 , if and only if for all c,

1 (cid:22)

in other words, for any linear function of x, the estimator f1 yields at least as good
an estimate as does f2.

e(ctbx1     ct x)2     e(ctbx2     ct x)2.

4.7 vector optimization

177

we can express the problem of    nding an unbiased estimator for x as the vector
optimization problem

minimize (w.r.t. sn
subject to

+) f f t

f a = i,

(4.58)

with variable f     rn  m. the objective f f t is convex with respect to sn
+, so the
problem (4.58) is a convex vector optimization problem. an easy way to see this is
to observe that vt f f t v = kf t vk2
2 is a convex function of f for any    xed v.
it is a famous result that the problem (4.58) has an optimal solution, the least-squares
estimator, or pseudo-inverse,

f     = a    = (at a)   1at .

for any f with f a = i, we have f f t (cid:23) f    f    t . the matrix

f    f    t = a   a   t = (at a)   1

is the optimal value of the problem (4.58).

4.7.3 pareto optimal points and values

we now consider the case (which occurs in most vector optimization problems of
interest) in which the set of achievable objective values does not have a minimum
element, so the problem does not have an optimal point or optimal value. in these
cases minimal elements of the set of achievable values play an important role. we
say that a feasible point x is pareto optimal (or e   cient) if f0(x) is a minimal
element of the set of achievable values o.
in this case we say that f0(x) is a
pareto optimal value for the vector optimization problem (4.56). thus, a point x
is pareto optimal if it is feasible and, for any feasible y, f0(y) (cid:22)k f0(x) implies
f0(y) = f0(x). in other words: any feasible point y that is better than or equal to
x (i.e., f0(y) (cid:22)k f0(x)) has exactly the same objective value as x.

a point x is pareto optimal if and only if it is feasible and

(f0(x)     k)     o = {f0(x)}

(4.59)

(see   2.4.2). the set f0(x)     k can be interpreted as the set of values that are
better than or equal to f0(x), so the condition (4.59) states that the only achievable
value better than or equal to f0(x) is f0(x) itself. this is illustrated in    gure 4.8.
a vector optimization problem can have many pareto optimal values (and

points). the set of pareto optimal values, denoted p, satis   es

p     o     bdo,

i.e., every pareto optimal value is an achievable objective value that lies in the
boundary of the set of achievable objective values (see exercise 4.52).

178

4 id76 problems

o

f0(xpo)

figure 4.8 the set o of achievable values for a vector optimization problem
with objective values in r2, with cone k = r2
+, is shown shaded. this
problem does not have an optimal point or value, but it does have a set of
pareto optimal points, whose corresponding values are shown as the dark-
ened curve on the lower left boundary of o. the point labeled f0(xpo)
is a pareto optimal value, and xpo is a pareto optimal point. the lightly
shaded region is f0(xpo)     k, which is the set of all z     r2 corresponding
to objective values better than (or equal to) f0(xpo).

4.7.4 scalarization

scalarization is a standard technique for    nding pareto optimal (or optimal) points
for a vector optimization problem, based on the characterization of minimum and
minimal points via dual generalized inequalities given in   2.6.3. choose any       k    
0, i.e., any vector that is positive in the dual generalized inequality. now consider
the scalar optimization problem

minimize
subject to

  t f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p,

(4.60)

and let x be an optimal point. then x is pareto optimal for the vector optimization
problem (4.56). this follows from the dual inequality characterization of minimal
points given in   2.6.3, and is also easily shown directly. if x were not pareto optimal,
then there is a y that is feasible, satis   es f0(y) (cid:22)k f0(x), and f0(x) 6= f0(y).
since f0(x)     f0(y) (cid:23)k 0 and is nonzero, we have   t (f0(x)     f0(y)) > 0, i.e.,
  t f0(x) >   t f0(y). this contradicts the assumption that x is optimal for the
scalar problem (4.60).

using scalarization, we can    nd pareto optimal points for any vector opti-
mization problem by solving the ordinary scalar optimization problem (4.60). the
vector   , which is sometimes called the weight vector, must satisfy       k     0. the
weight vector is a free parameter; by varying it we obtain (possibly) di   erent pareto
optimal solutions of the vector optimization problem (4.56). this is illustrated in
   gure 4.9. the    gure also shows an example of a pareto optimal point that cannot

4.7 vector optimization

179

o

f0(x1)

  1

f0(x3)

  2

f0(x2)

figure 4.9 scalarization. the set o of achievable values for a vector opti-
mization problem with cone k = r2
+. three pareto optimal values f0(x1),
f0(x2), f0(x3) are shown. the    rst two values can be obtained by scalar-
ization: f0(x1) minimizes   t
2 u,
where   1,   2     0. the value f0(x3) is pareto optimal, but cannot be found
by scalarization.

1 u over all u     o and f0(x2) minimizes   t

be obtained via scalarization, for any value of the weight vector       k     0.
the method of scalarization can be interpreted geometrically. a point x is
optimal for the scalarized problem, i.e., minimizes   t f0 over the feasible set, if
and only if   t (f0(y)     f0(x))     0 for all feasible y. but this is the same as saying
that {u |       t (u     f0(x)) = 0} is a supporting hyperplane to the set of achievable
objective values o at the point f0(x); in particular

{u |   t (u     f0(x)) < 0}     o =    .

(4.61)

(see    gure 4.9.) thus, when we    nd an optimal point for the scalarized problem, we
not only    nd a pareto optimal point for the original vector optimization problem;
we also    nd an entire halfspace in rq, given by (4.61), of objective values that
cannot be achieved.

scalarization of convex vector optimization problems

now suppose the vector optimization problem (4.56) is convex. then the scalarized
problem (4.60) is also convex, since   t f0 is a (scalar-valued) convex function (by
the results in   3.6). this means that we can    nd pareto optimal points of a convex
vector optimization problem by solving a convex scalar optimization problem. for
each choice of the weight vector       k     0 we get a (usually di   erent) pareto optimal
point.
for convex vector optimization problems we have a partial converse: for every
pareto optimal point xpo, there is some nonzero    (cid:23)k     0 such that xpo is a solution
of the scalarized problem (4.60). so, roughly speaking, for convex problems the
method of scalarization yields all pareto optimal points, as the weight vector   

180

4 id76 problems

varies over the k    -nonnegative, nonzero values. we have to be careful here, because
it is not true that every solution of the scalarized problem, with    (cid:23)k     0 and    6= 0,
is a pareto optimal point for the vector problem. (in contrast, every solution of
the scalarized problem with       k     0 is pareto optimal.)
in some cases we can use this partial converse to    nd all pareto optimal points
of a convex vector optimization problem. scalarization with       k     0 gives a set
of pareto optimal points (as it would in a nonconvex vector optimization problem
as well). to    nd the remaining pareto optimal solutions, we have to consider
nonzero weight vectors    that satisfy    (cid:23)k     0. for each such weight vector, we
   rst identify all solutions of the scalarized problem. then among these solutions we
must check which are, in fact, pareto optimal for the vector optimization problem.
these    extreme    pareto optimal points can also be found as the limits of the pareto
optimal points obtained from positive weight vectors.

to establish this partial converse, we consider the set

a = o + k = {t     rq | f0(x) (cid:22)k t for some feasible x},

(4.62)

which consists of all values that are worse than or equal to (with respect to (cid:22)k)
some achievable objective value. while the set o of achievable objective values
need not be convex, the set a is convex, when the problem is convex. moreover,
the minimal elements of a are exactly the same as the minimal elements of the
set o of achievable values, i.e., they are the same as the pareto optimal values.
(see exercise 4.53.) now we use the results of   2.6.3 to conclude that any minimal
element of a minimizes   t z over a for some nonzero    (cid:23)k     0. this means that
every pareto optimal point for the vector optimization problem is optimal for the
scalarized problem, for some nonzero weight    (cid:23)k     0.

example 4.10 minimal upper bound on a set of matrices. we consider the (convex)
vector optimization problem, with respect to the positive semide   nite cone,

minimize (w.r.t. sn
subject to

+) x

x (cid:23) ai,

i = 1, . . . , m,

(4.63)

where ai     sn, i = 1, . . . , m, are given. the constraints mean that x is an upper
bound on the given matrices a1, . . . , am; a pareto optimal solution of (4.63) is a
minimal upper bound on the matrices.
to    nd a pareto optimal point, we apply scalarization: we choose any w     sn
form the problem

++ and

minimize
tr(w x)
subject to x (cid:23) ai,

i = 1, . . . , m,

(4.64)

which is an sdp. di   erent choices for w will, in general, give di   erent minimal
solutions.

the partial converse tells us that if x is pareto optimal for the vector problem (4.63)
then it is optimal for the sdp (4.64), for some nonzero weight matrix w (cid:23) 0.
(in this case, however, not every solution of (4.64) is pareto optimal for the vector
optimization problem.)

we can give a simple geometric interpretation for this problem. we associate with
each a     sn

++ an ellipsoid centered at the origin, given by

ea = {u | ut a   1u     1},

4.7 vector optimization

181

x2

x1

figure 4.10 geometric interpretation of the problem (4.63). the three
shaded ellipsoids correspond to the data a1, a2, a3     s2
++; the pareto
optimal points correspond to minimal ellipsoids that contain them. the two
ellipsoids, with boundaries labeled x1 and x2, show two minimal ellipsoids
obtained by solving the sdp (4.64) for two di   erent weight matrices w1 and
w2.

so that a (cid:22) b if and only if ea     eb. a pareto optimal point x for the prob-
lem (4.63) corresponds to a minimal ellipsoid that contains the ellipsoids associated
with a1, . . . , am. an example is shown in    gure 4.10.

4.7.5 multicriterion optimization

when a vector optimization problem involves the cone k = rq
+, it is called a
multicriterion or multi-objective optimization problem. the components of f0,
say, f1, . . . , fq, can be interpreted as q di   erent scalar objectives, each of which
we would like to minimize. we refer to fi as the ith objective of the problem. a
multicriterion optimization problem is convex if f1, . . . , fm are convex, h1, . . . , hp
are a   ne, and the objectives f1, . . . , fq are convex.

since multicriterion problems are vector optimization problems, all of the ma-
terial of   4.7.1     4.7.4 applies. for multicriterion problems, though, we can be a
bit more speci   c in the interpretations. if x is feasible, we can think of fi(x) as
its score or value, according to the ith objective.
if x and y are both feasible,
fi(x)     fi(y) means that x is at least as good as y, according to the ith objective;
fi(x) < fi(y) means that x is better than y, or x beats y, according to the ith ob-
jective. if x and y are both feasible, we say that x is better than y, or x dominates
y, if fi(x)     fi(y) for i = 1, . . . , q, and for at least one j, fj(x) < fj(y). roughly
speaking, x is better than y if x meets or beats y on all objectives, and beats it in
at least one objective.

in a multicriterion problem, an optimal point x    satis   es

fi(x   )     fi(y),

i = 1, . . . , q,

182

4 id76 problems

for every feasible y. in other words, x    is simultaneously optimal for each of the
scalar problems

minimize
subject to

fj(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p,

for j = 1, . . . , q. when there is an optimal point, we say that the objectives are
noncompeting, since no compromises have to be made among the objectives; each
objective is as small as it could be made, even if the others were ignored.

a pareto optimal point xpo satis   es the following: if y is feasible and fi(y)    
fi(xpo) for i = 1, . . . , q, then fi(xpo) = fi(y), i = 1, . . . , q. this can be restated
as: a point is pareto optimal if and only if it is feasible and there is no better
feasible point. in particular, if a feasible point is not pareto optimal, there is at
least one other feasible point that is better. in searching for good points, then, we
can clearly limit our search to pareto optimal points.

trade-o    analysis

now suppose that x and y are pareto optimal points with, say,

fi(x) < fi(y),
fi(x) = fi(y),
fi(x) > fi(y),

i     a
i     b
i     c,

where a    b    c = {1, . . . , q}. in other words, a is the set of (indices of) objectives
for which x beats y, b is the set of objectives for which the points x and y are tied,
and c is the set of objectives for which y beats x. if a and c are empty, then
the two points x and y have exactly the same objective values. if this is not the
case, then both a and c must be nonempty. in other words, when comparing two
pareto optimal points, they either obtain the same performance (i.e., all objectives
equal), or, each beats the other in at least one objective.

in comparing the point x to y, we say that we have traded or traded o    better
objective values for i     a for worse objective values for i     c. optimal trade-o   
analysis (or just trade-o    analysis) is the study of how much worse we must do
in one or more objectives in order to do better in some other objectives, or more
generally, the study of what sets of objective values are achievable.

as an example, consider a bi-criterion (i.e., two criterion) problem. suppose
x is a pareto optimal point, with objectives f1(x) and f2(x). we might ask how
much larger f2(z) would have to be, in order to obtain a feasible point z with
f1(z)     f1(x)    a, where a > 0 is some constant. roughly speaking, we are asking
how much we must pay in the second objective to obtain an improvement of a in
the    rst objective. if a large increase in f2 must be accepted to realize a small
decrease in f1, we say that there is a strong trade-o    between the objectives, near
the pareto optimal value (f1(x), f2(x)). if, on the other hand, a large decrease
in f1 can be obtained with only a small increase in f2, we say that the trade-o   
between the objectives is weak (near the pareto optimal value (f1(x), f2(x))).

we can also consider the case in which we trade worse performance in the    rst
objective for an improvement in the second. here we    nd how much smaller f2(z)

4.7 vector optimization

183

can be made, to obtain a feasible point z with f1(z)     f1(x) + a, where a > 0
is some constant. in this case we receive a bene   t in the second objective, i.e., a
reduction in f2 compared to f2(x). if this bene   t is large (i.e., by increasing f1
a small amount we obtain a large reduction in f2), we say the objectives exhibit
a strong trade-o   . if it is small, we say the objectives trade o    weakly (near the
pareto optimal value (f1(x), f2(x))).

optimal trade-o    surface

the set of pareto optimal values for a multicriterion problem is called the optimal
trade-o    surface (in general, when q > 2) or the optimal trade-o    curve (when
q = 2). (since it would be foolish to accept any point that is not pareto optimal,
we can restrict our trade-o    analysis to pareto optimal points.) trade-o    analysis
is also sometimes called exploring the optimal trade-o    surface. (the optimal trade-
o    surface is usually, but not always, a surface in the usual sense. if the problem
has an optimal point, for example, the optimal trade-o    surface consists of a single
point, the optimal value.)

an optimal trade-o    curve is readily interpreted. an example is shown in
   gure 4.11, on page 185, for a (convex) bi-criterion problem. from this curve we
can easily visualize and understand the trade-o   s between the two objectives.

    the endpoint at the right shows the smallest possible value of f2, without

any consideration of f1.

    the endpoint at the left shows the smallest possible value of f1, without any

consideration of f2.

    by    nding the intersection of the curve with a vertical line at f1 =   , we can

see how large f2 must be to achieve f1       .

    by    nding the intersection of the curve with a horizontal line at f2 =   , we

can see how large f1 must be to achieve f2       .

    the slope of the optimal trade-o    curve at a point on the curve (i.e., a pareto
optimal value) shows the local optimal trade-o    between the two objectives.
where the slope is steep, small changes in f1 are accompanied by large
changes in f2.

    a point of large curvature is one where small decreases in one objective can
only be accomplished by a large increase in the other. this is the prover-
bial knee of the trade-o    curve, and in many applications represents a good
compromise solution.

all of these have simple extensions to a trade-o    surface, although visualizing a
surface with more than three objectives is di   cult.

scalarizing multicriterion problems

when we scalarize a multicriterion problem by forming the weighted sum objective

  t f0(x) =

  ifi(x),

qxi=1

184

4 id76 problems

where        0, we can interpret   i as the weight we attach to the ith objective.
the weight   i can be thought of as quantifying our desire to make fi small (or
our objection to having fi large).
in particular, we should take   i large if we
want fi to be small; if we care much less about fi, we can take   i small. we can
interpret the ratio   i/  j as the relative weight or relative importance of the ith
objective compared to the jth objective. alternatively, we can think of   i/  j as
exchange rate between the two objectives, since in the weighted sum objective a
decrease (say) in fi by    is considered the same as an increase in fj in the amount
(  i/  j)  .

these interpretations give us some intuition about how to set or change the
weights while exploring the optimal trade-o    surface. suppose, for example, that
the weight vector        0 yields the pareto optimal point xpo, with objective values
f1(xpo), . . . , fq(xpo). to    nd a (possibly) new pareto optimal point which trades
o    a better kth objective value (say), for (possibly) worse objective values for the
other objectives, we form a new weight vector      with

    k >   k,

    j =   j,

j 6= k,

j = 1, . . . , q,

i.e., we increase the weight on the kth objective. this yields a new pareto optimal
point   xpo with fk(  xpo)     fk(xpo) (and usually, fk(  xpo) < fk(xpo)), i.e., a new
pareto optimal point with an improved kth objective.
we can also see that at any point where the optimal trade-o    surface is smooth,
   gives the inward normal to the surface at the associated pareto optimal point.
in particular, when we choose a weight vector    and apply scalarization, we obtain
a pareto optimal point where    gives the local trade-o   s among objectives.

in practice, optimal trade-o    surfaces are explored by ad hoc adjustment of the
weights, based on the intuitive ideas above. we will see later (in chapter 5) that
the basic idea of scalarization, i.e., minimizing a weighted sum of objectives, and
then adjusting the weights to obtain a suitable solution, is the essence of duality.

4.7.6 examples

regularized least-squares
we are given a     rm  n and b     rm, and want to choose x     rn taking into
account two quadratic objectives:

    f1(x) = kax     bk2
between ax and b,

2 = xt at ax     2bt ax + bt b is a measure of the mis   t

    f2(x) = kxk2

2 = xt x is a measure of the size of x.

our goal is to    nd x that gives a good    t (i.e., small f1) and that is not large (i.e.,
small f2). we can formulate this problem as a vector optimization problem with
respect to the cone r2

+, i.e., a bi-criterion problem (with no constraints):

minimize (w.r.t. r2

+)

f0(x) = (f1(x), f2(x)).

4.7 vector optimization

185

15

10

22
k
x
k
=

)
x
(
2
f

5

0
0

10
5
f1(x) = kax     bk2

2

15

figure 4.11 optimal trade-o    curve for a regularized least-squares problem.
the shaded set is the set of achievable values (kax   bk2
2). the optimal
trade-o    curve, shown darker, is the lower left part of the boundary.

2,kxk2

we can scalarize this problem by taking   1 > 0 and   2 > 0 and minimizing the
scalar weighted sum objective

  t f0(x) =   1f1(x) +   2f2(x)

= xt (  1at a +   2i)x     2  1bt ax +   1bt b,

which yields

x(  ) = (  1at a +   2i)   1  1at b = (at a +   i)   1at b,

where    =   2/  1. for any    > 0, this point is pareto optimal for the bi-criterion
problem. we can interpret    =   2/  1 as the relative weight we assign f2 compared
to f1.

this method produces all pareto optimal points, except two, associated with
the extremes            and        0. in the    rst case we have the pareto optimal
solution x = 0, which would be obtained by scalarization with    = (0, 1). at the
other extreme we have the pareto optimal solution a   b, where a    is the pseudo-
inverse of a. this pareto optimal solution is obtained as the limit of the optimal
solution of the scalarized problem as        0, i.e., as        (1, 0). (we will encounter
the regularized least-squares problem again in   6.3.2.)
figure 4.11 shows the optimal trade-o    curve and the set of achievable values
for a regularized least-squares problem with problem data a     r100  10, b     r100.
(see exercise 4.50 for more discussion.)

risk-return trade-o    in portfolio optimization

the classical markowitz portfolio optimization problem described on page 155 is
naturally expressed as a bi-criterion problem, where the objectives are the negative

186

4 id76 problems

mean return (since we wish to maximize mean return) and the variance of the
return:

minimize (w.r.t. r2
subject to

+)

(f1(x), f2(x)) = (   pt x, xt   x)
1t x = 1,

x (cid:23) 0.

in forming the associated scalarized problem, we can (without loss of generality)
take   1 = 1 and   2 =    > 0:

minimize    pt x +   xt   x
subject to 1t x = 1,

x (cid:23) 0,

which is a qp. in this example too, we get all pareto optimal portfolios except for
the two limiting cases corresponding to        0 and           . roughly speaking, in
the    rst case we get a maximum mean return, without regard for return variance;
in the second case we form a minimum variance return, without regard for mean
return. assuming that pk > pi for i 6= k, i.e., that asset k is the unique asset with
maximum mean return, the portfolio allocation x = ek is the only one correspond-
ing to        0. (in other words, we concentrate the portfolio entirely in the asset
that has maximum mean return.) in many portfolio problems asset n corresponds
to a risk-free investment, with (deterministic) return rrf . assuming that   , with its
last row and column (which are zero) removed, is full rank, then the other extreme
pareto optimal portfolio is x = en, i.e., the portfolio is concentrated entirely in the
risk-free asset.

as a speci   c example, we consider a simple portfolio optimization problem with
4 assets, with price change mean and standard deviations given in the following
table.

asset

1
2
3
4

  1/2
pi
ii
12% 20%
10% 10%
5%
7%
3%
0%

asset 4 is a risk-free asset, with a (certain) 3% return. assets 3, 2, and 1 have
increasing mean returns, ranging from 7% to 12%, as well as increasing standard
deviations, which range from 5% to 20%. the correlation coe   cients between the
assets are   12 = 30%,   13 =    40%, and   23 = 0%.
figure 4.12 shows the optimal trade-o    curve for this portfolio optimization
problem. the plot is given in the conventional way, with the horizontal axis show-
ing standard deviation (i.e., squareroot of variance) and the vertical axis showing
expected return. the lower plot shows the optimal asset allocation vector x for
each pareto optimal point.

the results in this simple example agree with our intuition. for small risk,
the optimal allocation consists mostly of the risk-free asset, with a mixture of the
other assets in smaller quantities. note that a mixture of asset 3 and asset 1, which
are negatively correlated, gives some hedging, i.e., lowers variance for a given level
of mean return. at the other end of the trade-o    curve, we see that aggressive
growth portfolios (i.e., those with large mean returns) concentrate the allocation
in assets 1 and 2, the ones with the largest mean returns (and variances).

4.7 vector optimization

187

15%

10%

5%

n
r
u
t
e
r

n
a
e
m

0%

0%

1

n
o
i
t
a
c
o
l
l
a

0.5

0

0%

10%

20%

x(4)

x(3)

x(2)

x(1)

10%

20%

standard deviation of return

figure 4.12 top. optimal risk-return trade-o    curve for a simple portfolio
optimization problem. the lefthand endpoint corresponds to putting all
resources in the risk-free asset, and so has zero standard deviation. the
righthand endpoint corresponds to putting all resources in asset 1, which
has highest mean return. bottom. corresponding optimal allocations.

188

4 id76 problems

bibliography

id135 has been studied extensively since the 1940s, and is the subject of
many excellent books, including dantzig [dan63], luenberger [lue84], schrijver [sch86],
papadimitriou and steiglitz [ps98], bertsimas and tsitsiklis [bt97], vanderbei [van96],
and roos, terlaky, and vial [rtv97]. dantzig and schrijver also provide detailed ac-
counts of the history of id135. for a recent survey, see todd [tod02].

schaible [sch82, sch83] gives an overview of fractional programming, which includes
linear-fractional problems and extensions such as convex-concave fractional problems (see
exercise 4.7). the model of a growing economy in example 4.7 appears in von neumann
[vn46].

research on quadratic programming began in the 1950s (see, e.g., frank and wolfe
[fw56], markowitz [mar56], hildreth [hil57]), and was in part motivated by the portfo-
lio optimization problem discussed on page 155 (markowitz [mar52]), and the lp with
random cost discussed on page 154 (see freund [fre56]).

interest in second-order cone programming is more recent, and started with nesterov
and nemirovski [nn94,   6.2.3]. the theory and applications of socps are surveyed by
alizadeh and goldfarb [ag03], ben-tal and nemirovski [btn01, lecture 3] (where the
problem is referred to as conic quadratic programming), and lobo, vandenberghe, boyd,
and lebret [lvbl98].

robust id135, and robust id76 in general, originated with
ben-tal and nemirovski [btn98, btn99] and el ghaoui and lebret [el97]. goldfarb
and iyengar [gi03a, gi03b] discuss robust qcqps and applications in portfolio optimiza-
tion. el ghaoui, oustry, and lebret [eol98] focus on robust semide   nite programming.

geometric programming has been known since the 1960s. its use in engineering design
was    rst advocated by du   n, peterson, and zener [dpz67] and zener [zen71]. peterson
[pet76] and ecker [eck80] describe the progress made during the 1970s. these articles
and books also include examples of engineering applications, in particular in chemical
and civil engineering. fishburn and dunlop [fd85], sapatnekar, rao, vaidya, and kang
[srvk93], and hershenson, boyd, and lee [hbl01]) apply geometric programming to
problems in integrated circuit design. the cantilever beam design example (page 163)
is from vanderplaats [van84, page 147]. the variational characterization of the perron-
frobenius eigenvalue (page 165) is proved in berman and plemmons [bp94, page 31].

nesterov and nemirovski [nn94, chapter 4] introduced the conic form problem (4.49)
as a standard problem format in nonlinear id76. the cone programming
approach is further developed in ben-tal and nemirovski [btn01], who also describe
numerous applications.
alizadeh [ali91] and nesterov and nemirovski [nn94,   6.4] were the    rst to make a
systematic study of semide   nite programming, and to point out the wide variety of
applications in id76. subsequent research in semide   nite programming
during the 1990s was driven by applications in combinatorial optimization (goemans
and williamson [gw95]), control (boyd, el ghaoui, feron, and balakrishnan [befb94],
scherer, gahinet, and chilali [sgc97], dullerud and paganini [dp00]), communications
and signal processing (luo [luo03], davidson, luo, wong, and ma [dlw00, mdw+02]),
and other areas of engineering. the book edited by wolkowicz, saigal, and vandenberghe
[wsv00] and the articles by todd [tod01], lewis and overton [lo96], and vandenberghe
and boyd [vb95] provide overviews and extensive bibliographies. connections between
sdp and moment problems, of which we give a simple example on page 170, are explored
in detail by bertsimas and sethuraman [bs00], nesterov [nes00], and lasserre [las02].
the fastest mixing markov chain problem is from boyd, diaconis, and xiao [bdx04].

multicriterion optimization and pareto optimality are fundamental tools in economics;
see pareto [par71], debreu [deb59] and luenberger [lue95]. the result in example 4.9 is
known as the gauss-markov theorem (kailath, sayed, and hassibi [ksh00, page 97]).

189

exercises

exercises

basic terminology and optimality conditions

4.1 consider the optimization problem

minimize
subject to

f0(x1, x2)
2x1 + x2     1
x1 + 3x2     1
x1     0,

x2     0.

make a sketch of the feasible set. for each of the following objective functions, give the
optimal set and the optimal value.

(a) f0(x1, x2) = x1 + x2.
(b) f0(x1, x2) =    x1     x2.
(c) f0(x1, x2) = x1.
(d) f0(x1, x2) = max{x1, x2}.
(e) f0(x1, x2) = x2

1 + 9x2
2.

4.2 consider the optimization problem

minimize

f0(x) =    pm

i=1 log(bi     at

i x)

with domain dom f0 = {x | ax     b}, where a     rm  n (with rows at
dom f0 is nonempty.
prove the following facts (which include the results quoted without proof on page 141).

i ). we assume that

(a) dom f0 is unbounded if and only if there exists a v 6= 0 with av (cid:22) 0.
(b) f0 is unbounded below if and only if there exists a v with av (cid:22) 0, av 6= 0. hint.
there exists a v such that av (cid:22) 0, av 6= 0 if and only if there exists no z     0
such that at z = 0. this follows from the theorem of alternatives in example 2.21,
page 50.

(c) if f0 is bounded below then its minimum is attained, i.e., there exists an x that

satis   es the optimality condition (4.23).

(d) the optimal set is a   ne: xopt = {x    + v | av = 0}, where x    is any optimal point.

4.3 prove that x    = (1, 1/2,   1) is optimal for the optimization problem

(1/2)xt p x + qt x + r

minimize
subject to    1     xi     1,

i = 1, 2, 3,

where

p =" 13

12
   2

12    2
17
6
6

12 # ,

13.0 # ,
q ="    22.0

   14.5

r = 1.

4.4 [p. parrilo] symmetries and id76. suppose g = {q1, . . . , qk}     rn  n is a
group, i.e., closed under products and inverse. we say that the function f : rn     r is g-
invariant, or symmetric with respect to g, if f (qix) = f (x) holds for all x and i = 1, . . . , k.
i=1 qix, which is the average of x over its g-orbit. we de   ne the
   xed subspace of g as

we de   ne x = (1/k)pk

f = {x | qix = x, i = 1, . . . , k}.

(a) show that for any x     rn, we have x     f .

190

4 id76 problems

(b) show that if f : rn     r is convex and g-invariant, then f (x)     f (x).
(c) we say the optimization problem

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m

is g-invariant if the objective f0 is g-invariant, and the feasible set is g-invariant,
which means

f1(x)     0, . . . , fm(x)     0 =    f1(qix)     0, . . . , fm(qix)     0,

for i = 1, . . . , k. show that if the problem is convex and g-invariant, and there exists
an optimal point, then there exists an optimal point in f . in other words, we can
adjoin the equality constraints x     f to the problem, without loss of generality.

(d) as an example, suppose f is convex and symmetric, i.e., f (p x) = f (x) for every
permutation p . show that if f has a minimizer, then it has a minimizer of the form
  1. (this means to minimize f over x     rn, we can just as well minimize f (t1)
over t     r.)

4.5 equivalent convex problems. show that the following three convex problems are equiva-
lent. carefully explain how the solution of each problem is obtained from the solution of
the other problems. the problem data are the matrix a     rm  n (with rows at
i ), the
vector b     rm, and the constant m > 0.
(a) the robust least-squares problem

with variable x     rn, where    : r     r is de   ned as

i=1   (at

i x     bi),

minimize pm
  (u) =(cid:26) u2

m (2|u|     m )

|u|     m
|u| > m.

(this function is known as the huber penalty function; see   6.1.2.)

(b) the least-squares problem with variable weights

minimize pm

subject to w (cid:23) 0,

i=1(at

i x     bi)2/(wi + 1) + m 21t w

with variables x     rn and w     rm, and domain d = {(x, w)     rn  rm | w        1}.
hint. optimize over w assuming x is    xed, to establish a relation with the problem
in part (a).
(this problem can be interpreted as a weighted least-squares problem in which we
are allowed to adjust the weight of the ith residual. the weight is one if wi = 0, and
decreases if we increase wi. the second term in the objective penalizes large values
of w, i.e., large adjustments of the weights.)

(c) the quadratic program

minimize pm

subject to    u     v (cid:22) ax     b (cid:22) u + v

i=1(u2

i + 2m vi)

0 (cid:22) u (cid:22) m 1
v (cid:23) 0.

exercises

191

4.6 handling convex equality constraints. a id76 problem can have only linear
equality constraint functions.
in some special cases, however, it is possible to handle
convex equality constraint functions, i.e., constraints of the form h(x) = 0, where h is
convex. we explore this idea in this problem.
consider the optimization problem

minimize
subject to

f0(x)
fi(x)     0,
h(x) = 0,

i = 1, . . . , m

(4.65)

where fi and h are convex functions with domain rn. unless h is a   ne, this is not a
id76 problem. consider the related problem

minimize
subject to

f0(x)
fi(x)     0,
h(x)     0,

i = 1, . . . , m,

(4.66)

where the convex equality constraint has been relaxed to a convex inequality. this prob-
lem is, of course, convex.
now suppose we can guarantee that at any optimal solution x    of the convex prob-
lem (4.66), we have h(x   ) = 0, i.e., the inequality h(x)     0 is always active at the solution.
then we can solve the (nonconvex) problem (4.65) by solving the convex problem (4.66).
show that this is the case if there is an index r such that

    f0 is monotonically increasing in xr
    f1, . . . , fm are nondecreasing in xr
    h is monotonically decreasing in xr.

we will see speci   c examples in exercises 4.31 and 4.58.

4.7 convex-concave fractional problems. consider a problem of the form

minimize
subject to

f0(x)/(ct x + d)
fi(x)     0,
ax = b

i = 1, . . . , m

where f0, f1, . . . , fm are convex, and the domain of the objective function is de   ned as
{x     dom f0 | ct x + d > 0}.
(a) show that this is a quasiid76 problem.

(b) show that the problem is equivalent to

minimize
subject to

g0(y, t)
gi(y, t)     0,
ay = bt
ct y + dt = 1,

i = 1, . . . , m

where gi is the perspective of fi (see   3.2.6). the variables are y     rn and t     r.
show that this problem is convex.

(c) following a similar argument, derive a convex formulation for the convex-concave

fractional problem

minimize
subject to

f0(x)/h(x)
fi(x)     0,
ax = b

i = 1, . . . , m

192

4 id76 problems

where f0, f1, . . . , fm are convex, h is concave, the domain of the objective function
is de   ned as {x     dom f0     dom h | h(x) > 0} and f0(x)     0 everywhere.
as an example, apply your technique to the (unconstrained) problem with

f0(x) = (tr f (x))/m,

h(x) = (det(f (x))1/m,

with dom(f0/h) = {x | f (x)     0}, where f (x) = f0 + x1f1 +        + xnfn for given
fi     sm. in this problem, we minimize the ratio of the arithmetic mean over the
geometric mean of the eigenvalues of an a   ne matrix function f (x).

linear optimization problems

4.8 some simple lps. give an explicit solution of each of the following lps.

(a) minimizing a linear function over an a   ne set.

minimize
subject to ax = b.

ct x

(b) minimizing a linear function over a halfspace.

minimize
subject to

ct x
at x     b,

where a 6= 0.

(c) minimizing a linear function over a rectangle.

minimize
subject to

ct x
l (cid:22) x (cid:22) u,

where l and u satisfy l (cid:22) u.

(d) minimizing a linear function over the id203 simplex.

minimize
subject to 1t x = 1,

ct x

x (cid:23) 0.

what happens if the equality constraint is replaced by an inequality 1t x     1?
we can interpret this lp as a simple portfolio optimization problem. the vector
x represents the allocation of our total budget over di   erent assets, with xi the
fraction invested in asset i. the return of each investment is    xed and given by    ci,
so our total return (which we want to maximize) is    ct x. if we replace the budget
constraint 1t x = 1 with an inequality 1t x     1, we have the option of not investing
a portion of the total budget.

(e) minimizing a linear function over a unit box with a total budget constraint.

minimize
subject to 1t x =   ,

ct x

0 (cid:22) x (cid:22) 1,

where    is an integer between 0 and n. what happens if    is not an integer (but
satis   es 0            n)? what if we change the equality to an inequality 1t x       ?
(f) minimizing a linear function over a unit box with a weighted budget constraint.

minimize
subject to

ct x
dt x =   ,

0 (cid:22) x (cid:22) 1,

with d     0, and 0            1t d.

exercises

193

4.9 square lp. consider the lp

with a square and nonsingular. show that the optimal value is given by

ct x

minimize
subject to ax (cid:22) b

p    =(cid:26) ct a   1b a   t c (cid:22) 0

otherwise.

      

4.10 converting general lp to standard form. work out the details on page 147 of   4.3.
explain in detail the relation between the feasible sets, the optimal solutions, and the
optimal values of the standard form lp and the original lp.

4.11 problems involving    1- and       -norms. formulate the following problems as lps. explain
in detail the relation between the optimal solution of each problem and the solution of its
equivalent lp.
(a) minimize kax     bk    (      -norm approximation).
(b) minimize kax     bk1 (   1-norm approximation).
(c) minimize kax     bk1 subject to kxk        1.
(d) minimize kxk1 subject to kax     bk        1.
(e) minimize kax     bk1 + kxk   .
in each problem, a     rm  n and b     rm are given. (see   6.1 for more problems involving
approximation and constrained approximation.)
4.12 network    ow problem. consider a network of n nodes, with directed links connecting each
pair of nodes. the variables in the problem are the    ows on each link: xij will denote the
   ow from node i to node j. the cost of the    ow along the link from node i to node j is
given by cijxij, where cij are given constants. the total cost across the network is

c =

nxi,j=1

cijxij.

each link    ow xij is also subject to a given lower bound lij (usually assumed to be
nonnegative) and an upper bound uij.
the external supply at node i is given by bi, where bi > 0 means an external    ow enters
the network at node i, and bi < 0 means that at node i, an amount |bi|    ows out of the
network. we assume that 1t b = 0, i.e., the total external supply equals total external
demand. at each node we have conservation of    ow: the total    ow into node i along links
and the external supply, minus the total    ow out along the links, equals zero.
the problem is to minimize the total cost of    ow through the network, subject to the
constraints described above. formulate this problem as an lp.

4.13 robust lp with interval coe   cients. consider the problem, with variable x     rn,

ct x

minimize
subject to ax (cid:22) b for all a     a,

where a     rm  n is the set

a = {a     rm  n |   aij     vij     aij       aij + vij, i = 1, . . . , m, j = 1, . . . , n}.

(the matrices   a and v are given.) this problem can be interpreted as an lp where each
coe   cient of a is only known to lie in an interval, and we require that x must satisfy the
constraints for all possible values of the coe   cients.
express this problem as an lp. the lp you construct should be e   cient, i.e., it should
not have dimensions that grow exponentially with n or m.

194

4 id76 problems

4.14 approximating a matrix in in   nity norm. the       -norm induced norm of a matrix a    

rm  n, denoted kak   , is given by

kak    = sup

x6=0

kaxk   
kxk   

= max

i=1,...,m

nxj=1

|aij|.

this norm is sometimes called the max-row-sum norm, for obvious reasons (see   a.1.5).
consider the problem of approximating a matrix, in the max-row-sum norm, by a linear
combination of other matrices. that is, we are given k + 1 matrices a0, . . . , ak     rm  n,
and need to    nd x     rk that minimizes

ka0 + x1a1 +        + xkakk   .

express this problem as a linear program. explain the signi   cance of any extra variables
in your lp. carefully explain how your lp formulation solves this problem, e.g., what is
the relation between the feasible set for your lp and this problem?

4.15 relaxation of boolean lp. in a boolean linear program, the variable x is constrained to

have components equal to zero or one:

ct x

minimize
subject to ax (cid:22) b

xi     {0, 1},

i = 1, . . . , n.

(4.67)

in general, such problems are very di   cult to solve, even though the feasible set is    nite
(containing at most 2n points).
in a general method called relaxation, the constraint that xi be zero or one is replaced
with the linear inequalities 0     xi     1:

ct x

minimize
subject to ax (cid:22) b

0     xi     1,

i = 1, . . . , n.

(4.68)

we refer to this problem as the lp relaxation of the boolean lp (4.67). the lp relaxation
is far easier to solve than the original boolean lp.

(a) show that the optimal value of the lp relaxation (4.68) is a lower bound on the
optimal value of the boolean lp (4.67). what can you say about the boolean lp
if the lp relaxation is infeasible?

(b) it sometimes happens that the lp relaxation has a solution with xi     {0, 1}. what

can you say in this case?

4.16 minimum fuel optimal control. we consider a linear dynamical system with state x(t)    
rn, t = 0, . . . , n , and actuator or input signal u(t)     r, for t = 0, . . . , n     1. the
dynamics of the system is given by the linear recurrence

x(t + 1) = ax(t) + bu(t),

t = 0, . . . , n     1,

where a     rn  n and b     rn are given. we assume that the initial state is zero, i.e.,
x(0) = 0.
the minimum fuel optimal control problem is to choose the inputs u(0), . . . , u(n     1) so
as to minimize the total fuel consumed, which is given by

f =

n    1xt=0

f (u(t)),

exercises

195

subject to the constraint that x(n ) = xdes, where n is the (given) time horizon, and
xdes     rn is the (given) desired    nal or target state. the function f : r     r is the fuel
use map for the actuator, and gives the amount of fuel used as a function of the actuator
signal amplitude. in this problem we use

f (a) =(cid:26) |a|

2|a|     1

|a|     1
|a| > 1.

this means that fuel use is proportional to the absolute value of the actuator signal, for
actuator signals between    1 and 1; for larger actuator signals the marginal fuel e   ciency
is half.
formulate the minimum fuel optimal control problem as an lp.

4.17 optimal activity levels. we consider the selection of n nonnegative activity levels, denoted
x1, . . . , xn. these activities consume m resources, which are limited. activity j consumes
aijxj of resource i, where aij are given. the total resource consumption is additive, so
j=1 aijxj. (ordinarily we have aij     0, i.e.,
activity j consumes resource i. but we allow the possibility that aij < 0, which means
that activity j actually generates resource i as a by-product.) each resource consumption
is limited: we must have ci     cmax
are given. each activity generates revenue,
which is a piecewise-linear concave function of the activity level:

the total of resource i consumed is ci =pn

, where cmax

i

i

rj(xj) =(cid:26) pjxj

pjqj + pdisc

j

0     xj     qj

(xj     qj) xj     qj.

here pj > 0 is the basic price, qj > 0 is the quantity discount level, and pdisc
quantity discount price, for (the product of) activity j. (we have 0 < pdisc

is the
j < pj.) the
j=1 rj(xj).
the goal is to choose activity levels that maximize the total revenue while respecting the
resource limits. show how to formulate this problem as an lp.

total revenue is the sum of the revenues associated with each activity, i.e.,pn

j

4.18 separating hyperplanes and spheres. suppose you are given two sets of points in rn,
{v1, v2, . . . , vk} and {w1, w2, . . . , wl}. formulate the following two problems as lp fea-
sibility problems.
(a) determine a hyperplane that separates the two sets, i.e.,    nd a     rn and b     r

with a 6= 0 such that

at vi     b,

i = 1, . . . , k,

at wi     b,

i = 1, . . . , l.

note that we require a 6= 0, so you have to make sure that your formulation excludes
the trivial solution a = 0, b = 0. you can assume that

rank(cid:20) v1

1

v2
1

      
      

vk w1 w2
1
1

1

       wl
      

1 (cid:21) = n + 1

(i.e., the a   ne hull of the k + l points has dimension n).

(b) determine a sphere separating the two sets of points, i.e.,    nd xc     rn and r     0

such that

kvi     xck2     r,

i = 1, . . . , k,

kwi     xck2     r,

i = 1, . . . , l.

(here xc is the center of the sphere; r is its radius.)

(see chapter 8 for more on separating hyperplanes, separating spheres, and related topics.)

196

4 id76 problems

4.19 consider the problem

minimize
subject to

kax     bk1/(ct x + d)
kxk        1,

where a     rm  n, b     rm, c     rn, and d     r. we assume that d > kck1, which implies
that ct x + d > 0 for all feasible x.

(a) show that this is a quasiid76 problem.
(b) show that it is equivalent to the id76 problem

minimize
subject to

kay     btk1
kyk        t
ct y + dt = 1,

with variables y     rn, t     r.

4.20 power assignment in a wireless communication system. we consider n transmitters with
powers p1, . . . , pn     0, transmitting to n receivers. these powers are the optimization
variables in the problem. we let g     rn  n denote the matrix of path gains from the
transmitters to the receivers; gij     0 is the path gain from transmitter j to receiver i.
the signal power at receiver i is then si = giipi, and the interference power at receiver i

is ii =pk6=i gikpk. the signal to interference plus noise ratio, denoted sinr, at receiver

i, is given by si/(ii +   i), where   i > 0 is the (self-) noise power in receiver i. the
objective in the problem is to maximize the minimum sinr ratio, over all receivers, i.e.,
to maximize

si

min

i=1,...,n

.

ii +   i

i

, where p max

there are a number of constraints on the powers that must be satis   ed, in addition to the
obvious one pi     0. the    rst is a maximum allowable power for each transmitter, i.e.,
pi     p max
> 0 is given. in addition, the transmitters are partitioned into
groups, with each group sharing the same power supply, so there is a total power constraint
for each group of transmitter powers. more precisely, we have subsets k1, . . . , km of
{1, . . . , n} with k1                km = {1, . . . , n}, and kj     kl = 0 if j 6= l. for each group kl,
the total associated transmitter power cannot exceed p gp

i

l > 0:

finally, we have a limit p rc

k > 0 on the total received power at each receiver:

xk   kl
nxk=1

pk     p gp

l

,

l = 1, . . . , m.

gikpk     p rc
i ,

i = 1, . . . , n.

(this constraint re   ects the fact that the receivers will saturate if the total received power
is too large.)
formulate the sinr maximization problem as a generalized linear-fractional program.

quadratic optimization problems

4.21 some simple qcqps. give an explicit solution of each of the following qcqps.

(a) minimizing a linear function over an ellipsoid centered at the origin.

ct x

minimize
subject to xt ax     1,

where a     sn
(a 6    sn

+)?

++ and c 6= 0. what is the solution if the problem is not convex

exercises

197

(b) minimizing a linear function over an ellipsoid.

minimize
subject to

ct x
(x     xc)t a(x     xc)     1,

where a     sn

++ and c 6= 0.

(c) minimizing a quadratic form over an ellipsoid centered at the origin.

xt bx

minimize
subject to xt ax     1,

where a     sn
(see   b.1.)

++ and b     sn

+. also consider the nonconvex extension with b 6    sn
+.

4.22 consider the qcqp

(1/2)xt p x + qt x + r

minimize
subject to xt x     1,

with p     sn
solution of the nonlinear equation

++. show that x    =    (p +   i)   1q where    = max{0,     } and      is the largest

4.23    4-norm approximation via qcqp. formulate the    4-norm approximation problem

qt (p +   i)   2q = 1.

as a qcqp. the matrix a     rm  n (with rows at

4.24 complex    1-,    2- and       -norm approximation. consider the problem

minimize

kax     bk4 = (pm

i x     bi)4)1/4

i=1(at
i ) and the vector b     rm are given.

minimize

kax     bkp,

where a     cm  n, b     cm, and the variable is x     cn. the complex    p-norm is de   ned
by

kykp =  mxi=1

|yi|p!1/p

for p     1, and kyk    = maxi=1,...,m |yi|. for p = 1, 2, and    , express the complex    p-norm
approximation problem as a qcqp or socp with real variables and data.

4.25 linear separation of two sets of ellipsoids. suppose we are given k + l ellipsoids

ei = {piu + qi | kuk2     1},

i = 1, . . . , k + l,

where pi     sn. we are interested in    nding a hyperplane that strictly separates e1, . . . ,
ek from ek+1, . . . , ek+l, i.e., we want to compute a     rn, b     r such that

at x + b > 0 for x     e1                ek ,

at x + b < 0 for x     ek+1                ek+l,

or prove that no such hyperplane exists. express this problem as an socp feasibility
problem.

4.26 hyperbolic constraints as soc constraints. verify that x     rn, y, z     r satisfy

xt x     yz,

y     0,

z     0

if and only if

use this observation to cast the following problems as socps.

    y + z,

y     0,

z     0.

(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) 2x
y     z (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2

198

4 id76 problems

(a) maximizing harmonic mean.

maximize

with domain {x | ax     b}, where at

i

(b) maximizing geometric mean.

i=1 1/(at

i x     bi)(cid:1)   1

,

is the ith row of a.

maximize

i=1(at

i x     bi)(cid:1)1/m

,

is the ith row of a.

with domain {x | ax (cid:23) b}, where at

i

(cid:0)pm
(cid:0)qm

4.27 matrix fractional minimization via socp. express the following problem as an socp:

(ax + b)t (i + b diag(x)bt )   1(ax + b)

minimize
subject to x (cid:23) 0,

with a     rm  n, b     rm, b     rm  n. the variable is x     rn.
hint. first show that the problem is equivalent to

minimize
subject to

vt v + wt diag(x)   1w
v + bw = ax + b
x (cid:23) 0,

with variables v     rm, w, x     rn. (if xi = 0 we interpret w2
    otherwise.) then use the results of exercise 4.26.

i /xi as zero if wi = 0 and as

4.28 robust quadratic programming. in   4.4.2 we discussed robust id135 as an
in this problem we consider a similar

application of second-order cone programming.
robust variation of the (convex) quadratic program

(1/2)xt p x + qt x + r

minimize
subject to ax (cid:22) b.

for simplicity we assume that only the matrix p is subject to errors, and the other
parameters (q, r, a, b) are exactly known. the robust quadratic program is de   ned as

supp    e ((1/2)xt p x + qt x + r)

minimize
subject to ax (cid:22) b
where e is the set of possible matrices p .
for each of the following sets e, express the robust qp as a convex problem. be as speci   c
as you can. if the problem can be expressed in a standard form (e.g., qp, qcqp, socp,
sdp), say so.
(a) a    nite set of matrices: e = {p1, . . . , pk}, where pi     sn
(b) a set speci   ed by a nominal value p0     sn

+ plus a bound on the eigenvalues of the

+, i = 1, . . . , k.

deviation p     p0:
e = {p     sn |      i (cid:22) p     p0 (cid:22)   i}
where        r and p0     sn
+,
(c) an ellipsoid of matrices:

e =( p0 +

kxi=1

piui(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) kuk2     1) .

you can assume pi     sn

+, i = 0, . . . , k.

exercises

199

4.29 maximizing id203 of satisfying a linear inequality. let c be a random variable in rn,

normally distributed with mean   c and covariance matrix r. consider the problem

maximize
subject to f x (cid:22) g, ax = b.

prob(ct x       )

assuming there exists a feasible point   x for which   ct   x       , show that this problem is
equivalent to a convex or quasiid76 problem. formulate the problem as a
qp, qcqp, or socp (if the problem is convex), or explain how you can solve it by solving
a sequence of qp, qcqp, or socp feasibility problems (if the problem is quasiconvex).

geometric programming

4.30 a heated    uid at temperature t (degrees above ambient temperature)    ows in a pipe
with    xed length and circular cross section with radius r. a layer of insulation, with
thickness w     r, surrounds the pipe to reduce heat loss through the pipe walls. the
design variables in this problem are t , r, and w.
the heat loss is (approximately) proportional to t r/w, so over a    xed lifetime, the energy
cost due to heat loss is given by   1t r/w. the cost of the pipe, which has a    xed wall
thickness, is approximately proportional to the total material, i.e., it is given by   2r. the
cost of the insulation is also approximately proportional to the total insulation material,
i.e.,   3rw (using w     r). the total cost is the sum of these three costs.
the heat    ow down the pipe is entirely due to the    ow of the    uid, which has a    xed
velocity, i.e., it is given by   4t r2. the constants   i are all positive, as are the variables
t , r, and w.
now the problem: maximize the total heat    ow down the pipe, subject to an upper limit
cmax on total cost, and the constraints

tmin     t     tmax,

rmin     r     rmax,
express this problem as a geometric program.

wmin     w     wmax, w     0.1r.

4.31 recursive formulation of optimal beam design problem. show that the gp (4.46) is equiv-

alent to the gp

minimize pn

i=1 wihi

subject to wi/wmax     1, wmin/wi     1,
hmin/hi     1,
i )     1,

hi/hmax     1,
hi/(wismax)     1, sminwi/hi     1,
6if/(  maxwih2
i = 1, . . . , n
(2i     1)di/vi + vi+1/vi     1,
(i     1/3)di/yi + vi+1/yi + yi+1/yi     1,
y1/ymax     1
ewih3

i = 1, . . . , n.

i di/(6f ) = 1,

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

i = 1, . . . , n

the variables are wi, hi, vi, di, yi for i = 1, . . . , n .

4.32 approximating a function as a monomial. suppose the function f : rn     r is di   er-
entiable at a point x0     0, with f (x0) > 0. how would you    nd a monomial function
  f : rn     r such that f (x0) =   f (x0) and for x near x0,   f (x) is very near f (x)?

4.33 express the following problems as id76 problems.
(a) minimize max{p(x), q(x)}, where p and q are posynomials.
(b) minimize exp(p(x)) + exp(q(x)), where p and q are posynomials.
(c) minimize p(x)/(r(x)     q(x)), subject to r(x) > q(x), where p, q are posynomials,

and r is a monomial.

200

4 id76 problems

4.34 log-convexity of perron-frobenius eigenvalue. let a     rn  n be an elementwise positive
matrix, i.e., aij > 0.
(the results of this problem hold for irreducible nonnegative
matrices as well.) let   pf (a) denotes its perron-frobenius eigenvalue, i.e., its eigenvalue
of largest magnitude.
(see the de   nition and the example on page 165.) show that
log   pf (a) is a convex function of log aij. this means, for example, that we have the
inequality

  pf (c)     (  pf (a)  pf (b))1/2 ,

where cij = (aijbij)1/2, and a and b are elementwise positive matrices.
hint. use the characterization of the perron-frobenius eigenvalue given in (4.47), or,
alternatively, use the characterization

log   pf (a) = lim
k      

(1/k) log(1t ak1).

4.35 signomial and geometric programs. a signomial is a linear combination of monomials of
some positive variables x1, . . . , xn. signomials are more general than posynomials, which
are signomials with all positive coe   cients. a signomial program is an optimization
problem of the form

minimize
subject to

f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p,

where f0, . . . , fm and h1, . . . , hp are signomials. in general, signomial programs are very
di   cult to solve.
some signomial programs can be transformed to gps, and therefore solved e   ciently.
show how to do this for a signomial program of the following form:

cients.

    the objective signomial f0 is a posynomial, i.e., its terms have only positive coe   -
    each inequality constraint signomial f1, . . . , fm has exactly one term with a negative
coe   cient: fi = pi     qi where pi is posynomial, and qi is monomial.
    each equality constraint signomial h1, . . . , hp has exactly one term with a positive
coe   cient and one term with a negative coe   cient: hi = ri     si where ri and si are
monomials.

4.36 explain how to reformulate a general gp as an equivalent gp in which every posynomial
(in the objective and constraints) has at most two monomial terms. hint. express each
sum (of monomials) as a sum of sums, each with two terms.

4.37 generalized posynomials and geometric programming. let x1, . . . , xn be positive variables,
and suppose the functions fi : rn     r, i = 1, . . . , k, are posynomials of x1, . . . , xn. if
   : rk     r is a polynomial with nonnegative coe   cients, then the composition

h(x) =   (f1(x), . . . , fk(x))

(4.69)

1 f2 + 2f1 + f 3

1z2 + 2z1 + 3z3

2 is a posynomial.

is a posynomial, since posynomials are closed under products, sums, and multiplication
by nonnegative scalars. for example, suppose f1 and f2 are posynomials, and consider
the polynomial   (z1, z2) = 3z2
2 (which has nonnegative coe   cients). then
h = 3f 2
in this problem we consider a generalization of this idea, in which    is allowed to be
a posynomial, i.e., can have fractional exponents. speci   cally, assume that    : rk    
r is a posynomial, with all its exponents nonnegative.
in this case we will call the
function h de   ned in (4.69) a generalized posynomial. as an example, suppose f1 and f2
are posynomials, and consider the posynomial (with nonnegative exponents)   (z1, z2) =
2z0.3

2 + 2. then the function

2 + z1z0.5

1 z1.2

h(x) = 2f1(x)0.3f2(x)1.2 + f1(x)f2(x)0.5 + 2

exercises

201

is a generalized posynomial. note that it is not a posynomial, however (unless f1 and f2
are monomials or constants).
a generalized geometric program (ggp) is an optimization problem of the form

h0(x)

minimize
subject to hi(x)     1,
gi(x) = 1,

i = 1, . . . , m
i = 1, . . . , p,

(4.70)

where g1, . . . , gp are monomials, and h0, . . . , hm are generalized posynomials.
show how to express this generalized geometric program as an equivalent geometric pro-
gram. explain any new variables you introduce, and explain how your gp is equivalent
to the ggp (4.70).

semide   nite programming and conic form problems

4.38 lmis and sdps with one variable. the generalized eigenvalues of a matrix pair (a, b),

where a, b     sn, are de   ned as the roots of the polynomial det(  b     a) (see   a.5.3).
suppose b is nonsingular, and that a and b can be simultaneously diagonalized by a
congruence, i.e., there exists a nonsingular r     rn  n such that
rt br = diag(b),

rt ar = diag(a),

where a, b     rn. (a su   cient condition for this to hold is that there exists t1, t2 such
that t1a + t2b     0.)
(a) show that the generalized eigenvalues of (a, b) are real, and given by   i = ai/bi,

i = 1, . . . , n.

(b) express the solution of the sdp

minimize
subject to

ct
tb (cid:22) a,

with variable t     r, in terms of a and b.

4.39 sdps and congruence transformations. consider the sdp

ct x

minimize
subject to x1f1 + x2f2 +        + xnfn + g (cid:22) 0,

with fi, g     sk, c     rn.
(a) suppose r     rk  k is nonsingular. show that the sdp is equivalent to the sdp

ct x

minimize
subject to x1   f1 + x2   f2 +        + xn   fn +   g (cid:22) 0,

where   fi = rt fir,   g = rt gr.

(b) suppose there exists a nonsingular r such that   fi and   g are diagonal. show that

the sdp is equivalent to an lp.

(c) suppose there exists a nonsingular r such that   fi and   g have the form

  fi =(cid:20)   ii

at
i

ai

  i (cid:21) ,

i = 1, . . . , n,

  g =(cid:20)   i

bt

b

   (cid:21) ,

where   i,        r, ai, b     rk   1. show that the sdp is equivalent to an socp with
a single second-order cone constraint.

202

4 id76 problems

4.40 lps, qps, qcqps, and socps as sdps. express the following problems as sdps.

(a) the lp (4.27).
(b) the qp (4.34), the qcqp (4.35) and the socp (4.36). hint. suppose a     sr

++,

c     ss, and b     rr  s. then

bt c (cid:21) (cid:23) 0        c     bt a   1b (cid:23) 0.
(cid:20) a b

for a more complete statement, which applies also to singular a, and a proof,
see   a.5.5.

(c) the matrix fractional optimization problem

minimize

(ax + b)t f (x)   1(ax + b)

where a     rm  n, b     rm,

f (x) = f0 + x1f1 +        + xnfn,

with fi     sm, and we take the domain of the objective to be {x | f (x)     0}. you
can assume the problem is feasible (there exists at least one x with f (x)     0).

4.41 lmi tests for copositive matrices and p0-matrices. a matrix a     sn is said to be copositive
if xt ax     0 for all x (cid:23) 0 (see exercise 2.35). a matrix a     rn  n is said to be a p0-
matrix if maxi=1,...,n xi(ax)i     0 for all x. checking whether a matrix is copositive or
a p0-matrix is very di   cult in general. however, there exist useful su   cient conditions
that can be veri   ed using semide   nite programming.

(a) show that a is copositive if it can be decomposed as a sum of a positive semide   nite

and an elementwise nonnegative matrix:

a = b + c,

b (cid:23) 0,

cij     0,

i, j = 1, . . . , n.

(4.71)

express the problem of    nding b and c that satisfy (4.71) as an sdp feasibility
problem.

(b) show that a is a p0-matrix if there exists a positive diagonal matrix d such that

express the problem of    nding a d that satis   es (4.72) as an sdp feasibility problem.

da + at d (cid:23) 0.

(4.72)

4.42 complex lmis and sdps. a complex lmi has the form

x1f1 +        + xnfn + g (cid:22) 0

where f1, . . . , fn, g are complex n    n hermitian matrices, i.e., f h
i = fi, gh = g, and
x     rn is a real variable. a complex sdp is the problem of minimizing a (real) linear
function of x subject to a complex lmi constraint.
complex lmis and sdps can be transformed to real lmis and sdps, using the fact that

x (cid:23) 0        (cid:20)    x       x

   x    x (cid:21) (cid:23) 0,

where    x     rn  n is the real part of the complex hermitian matrix x, and    x     rn  n
is the imaginary part of x.
verify this result, and show how to pose a complex sdp as a real sdp.

exercises

203

4.43 eigenvalue optimization via sdp. suppose a : rn     sm is a   ne, i.e.,

a(x) = a0 + x1a1 +        + xnan

where ai     sm. let   1(x)       2(x)                  m(x) denote the eigenvalues of a(x). show
how to pose the following problems as sdps.

(a) minimize the maximum eigenvalue   1(x).
(b) minimize the spread of the eigenvalues,   1(x)       m(x).
(c) minimize the condition number of a(x), subject to a(x)     0. the condition number
is de   ned as   (a(x)) =   1(x)/  m(x), with domain {x | a(x)     0}. you may assume
that a(x)     0 for at least one x.
hint. you need to minimize   /  , subject to

change variables to y = x/  , t =   /  , s = 1/  .

0       i (cid:22) a(x) (cid:22)   i.

(d) minimize the sum of the absolute values of the eigenvalues, |  1(x)| +        + |  m(x)|.

hint. express a(x) as a(x) = a+     a   , where a+ (cid:23) 0, a    (cid:23) 0.

4.44 optimization over polynomials. pose the following problem as an sdp. find the polyno-

mial p : r     r,

p(t) = x1 + x2t +        + x2k+1t2k,

that satis   es given bounds li     p(ti)     ui, at m speci   ed points ti, and, of all the
polynomials that satisfy these bounds, has the greatest minimum value:

maximize
subject to

inf t p(t)
li     p(ti)     ui,

i = 1, . . . , m.

the variables are x     r2k+1.
hint. use the lmi characterization of nonnegative polynomials derived in exercise 2.37,
part (b).

4.45 [nes00, par00] sum-of-squares representation via lmis. consider a polynomial p : rn    
r of degree 2k. the polynomial is said to be positive semide   nite (psd) if p(x)     0
for all x     rn. except for special cases (e.g., n = 1 or k = 1), it is extremely di   cult
to determine whether or not a given polynomial is psd, let alone solve an optimization
problem, with the coe   cients of p as variables, with the constraint that p be psd.
a famous su   cient condition for a polynomial to be psd is that it have the form

p(x) =

qi(x)2,

rxi=1

for some polynomials qi, with degree no more than k. a polynomial p that has this
sum-of-squares form is called sos.
the condition that a polynomial p be sos (viewed as a constraint on its coe   cients)
turns out to be equivalent to an lmi, and therefore a variety of optimization problems,
with sos constraints, can be posed as sdps. you will explore these ideas in this problem.

(a) let f1, . . . , fs be all monomials of degree k or less. (here we mean monomial in
the standard sense, i.e., xm1
n , where mi     z+, and not in the sense used in
geometric programming.) show that if p can be expressed as a positive semide   nite
quadratic form p = f t v f , with v     ss
+, then p is sos. conversely, show that if
p is sos, then it can be expressed as a positive semide   nite quadratic form in the
monomials, i.e., p = f t v f , for some v     ss
+.

       xmn

1

204

4 id76 problems

(b) show that the condition p = f t v f is a set of linear equality constraints relating the
coe   cients of p and the matrix v . combined with part (a) above, this shows that
the condition that p be sos is equivalent to a set of linear equalities relating v and
the coe   cients of p, and the matrix inequality v (cid:23) 0.

(c) work out the lmi conditions for sos explicitly for the case where p is polynomial

of degree four in two variables.

1tj

2, where i, j are nonnegative integers.

4.46 multidimensional moments. the moments of a random variable t on r2 are de   ned as
  ij = e ti
in this problem we derive necessary
conditions for a set of numbers   ij, 0     i, j     2k, i + j     2k, to be the moments of a
distribution on r2.
let p : r2     r be a polynomial of degree k with coe   cients cij,

p(t) =

kxi=0

k   ixj=0

cijti

1tj
2,

and let t be a random variable with moments   ij. suppose c     r(k+1)(k+2)/2 contains
the coe   cients cij in some speci   c order, and        r(k+1)(2k+1) contains the moments   ij
in the same order. show that e p(t)2 can be expressed as a quadratic form in c:

e p(t)2 = ct h(  )c,

where h : r(k+1)(2k+1)     s(k+1)(k+2)/2 is a linear function of   . from this, conclude
that    must satisfy the lmi h(  ) (cid:23) 0.
remark: for random variables on r, the matrix h can be taken as the hankel matrix
de   ned in (4.52). in this case, h(  ) (cid:23) 0 is a necessary and su   cient condition for    to be
the moments of a distribution, or the limit of a sequence of moments. on r2, however,
the lmi is only a necessary condition.

4.47 maximum determinant positive semide   nite matrix completion. we consider a matrix
a     sn, with some entries speci   ed, and the others not speci   ed. the positive semide   nite
matrix completion problem is to determine values of the unspeci   ed entries of the matrix
so that a (cid:23) 0 (or to determine that such a completion does not exist).
(a) explain why we can assume without loss of generality that the diagonal entries of

a are speci   ed.

(b) show how to formulate the positive semide   nite completion problem as an sdp

feasibility problem.

(c) assume that a has at least one completion that is positive de   nite, and the diag-
onal entries of a are speci   ed (i.e.,    xed). the positive de   nite completion with
largest determinant is called the maximum determinant completion. show that the
maximum determinant completion is unique. show that if a    is the maximum de-
terminant completion, then (a   )   1 has zeros in all the entries of the original matrix
that were not speci   ed. hint. the gradient of the function f (x) = log det x is
   f (x) = x    1 (see   a.4.1).
(d) suppose a is speci   ed on its tridiagonal part, i.e., we are given a11, . . . , ann and
a12, . . . , an   1,n. show that if there exists a positive de   nite completion of a, then
there is a positive de   nite completion whose inverse is tridiagonal.

4.48 generalized eigenvalue minimization. recall (from example 3.37, or   a.5.3) that the

largest generalized eigenvalue of a pair of matrices (a, b)     sk    sk

++ is given by

  max(a, b) = sup
u6=0

ut au
ut bu

= max{   | det(  b     a) = 0}.

as we have seen, this function is quasiconvex (if we take sk    sk

++ as its domain).

exercises

we consider the problem

205

minimize   max(a(x), b(x))

(4.73)

where a, b : rn     sk are a   ne functions, de   ned as

a(x) = a0 + x1a1 +        + xnan,

b(x) = b0 + x1b1 +        + xnbn.

with ai, bi     sk.
(a) give a family of convex functions   t : sk    sk     r, that satisfy

for all (a, b)     sk    sk
sequence of convex feasibility problems.

  max(a, b)     t          t(a, b)     0
++. show that this allows us to solve (4.73) by solving a

(b) give a family of matrix-convex functions   t : sk    sk     sk that satisfy

for all (a, b)     sk    sk
sequence of convex feasibility problems with lmi constraints.

  max(a, b)     t          t(a, b) (cid:22) 0
++. show that this allows us to solve (4.73) by solving a

(c) suppose b(x) = (at x+b)i, with a 6= 0. show that (4.73) is equivalent to the convex

problem

minimize
subject to

with variables y     rn, s     r.

  max(sa0 + y1a1 +        + ynan)
at y + bs = 1
s     0,

4.49 generalized fractional programming. let k     rm be a proper cone. show that the

function f0 : rn     rm, de   ned by

f0(x) = inf{t | cx + d (cid:22)k t(f x + g)},
with c, f     rm  n, d, g     rm, is quasiconvex.
a quasiid76 problem with objective function of this form is called a gen-
eralized fractional program. express the generalized linear-fractional program of page 152
and the generalized eigenvalue minimization problem (4.73) as generalized fractional pro-
grams.

dom f0 = {x | f x + g    k 0},

vector and multicriterion optimization

4.50 bi-criterion optimization. figure 4.11 shows the optimal trade-o    curve and the set of

achievable values for the bi-criterion optimization problem

minimize (w.r.t. r2

+)

(kax     bk2,kxk2
2),

for some a     r100  10, b     r100. answer the following questions using information from
the plot. we denote by xls the solution of the least-squares problem

minimize

kax     bk2
2.

(a) what is kxlsk2?
(b) what is kaxls     bk2?
(c) what is kbk2?

206

4 id76 problems

(d) give the optimal value of the problem

minimize
subject to

kax     bk2
2
kxk2
2 = 1.

(e) give the optimal value of the problem

minimize
subject to

kax     bk2
kxk2
2     1.

2

(f) give the optimal value of the problem

(g) what is the rank of a?

minimize kax     bk2

2 + kxk2
2.

4.51 monotone transformation of objective in vector optimization. consider the vector opti-
mization problem (4.56). suppose we form a new vector optimization problem by replacing
the objective f0 with        f0, where    : rq     rq satis   es

u (cid:22)k v, u 6= v =      (u) (cid:22)k   (v),   (u) 6=   (v).

show that a point x is pareto optimal (or optimal) for one problem if and only if it is
pareto optimal (optimal) for the other, so the two problems are equivalent. in particular,
composing each objective in a multicriterion problem with an increasing function does
not a   ect the pareto optimal points.

4.52 pareto optimal points and the boundary of the set of achievable values. consider a vector
optimization problem with cone k. let p denote the set of pareto optimal values, and
let o denote the set of achievable objective values. show that p     o     bdo, i.e., every
pareto optimal value is an achievable objective value that lies in the boundary of the set
of achievable objective values.

4.53 suppose the vector optimization problem (4.56) is convex. show that the set

a = o + k = {t     rq | f0(x) (cid:22)k t for some feasible x},

is convex. also show that the minimal elements of a are the same as the minimal points
of o.

4.54 scalarization and optimal points. suppose a (not necessarily convex) vector optimization
problem has an optimal point x   . show that x    is a solution of the associated scalarized
problem for any choice of       k     0. also show the converse: if a point x is a solution of
the scalarized problem for any choice of       k     0, then it is an optimal point for the (not
necessarily convex) vector optimization problem.

4.55 generalization of weighted-sum scalarization. in   4.7.4 we showed how to obtain pareto
optimal solutions of a vector optimization problem by replacing the vector objective f0 :
rn     rq with the scalar objective   t f0, where       k     0. let    : rq     r be a
k-increasing function, i.e., satisfying

show that any solution of the problem

u (cid:22)k v, u 6= v =      (u) <   (v).

minimize
subject to

  (f0(x))
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p

exercises

207

is pareto optimal for the vector optimization problem

minimize (w.r.t. k)
subject to

f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p.

note that   (u) =   t u, where       k     0, is a special case.
as a related example, show that in a multicriterion optimization problem (i.e., a vector
optimization problem with f0 = f : rn     rq, and k = rq
+), a unique solution of the
scalar optimization problem

minimize maxi=1,...,q fi(x)
subject to

fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p,

is pareto optimal.

miscellaneous problems

4.56 [p. parrilo] we consider the problem of minimizing the convex function f0 : rn     r

over the convex hull of the union of some convex sets, conv(cid:0)sq

described via convex inequalities,

i=1 ci(cid:1). these sets are

ci = {x | fij(x)     0, j = 1, . . . , ki},

where fij : rn     r are convex. our goal is to formulate this problem as a convex
optimization problem.
the obvious approach is to introduce variables x1, . . . , xq     rn, with xi     ci,        rq
with    (cid:23) 0, 1t    = 1, and a variable x     rn, with x =   1x1 +        +   qxq. this equality
constraint is not a   ne in the variables, so this approach does not yield a convex problem.
a more sophisticated formulation is given by

minimize
subject to

f0(x)
sifij(zi/si)     0,
1t s = 1,
s (cid:23) 0
x = z1 +        + zq,

i = 1, . . . , q,

j = 1, . . . , ki

with variables z1, . . . , zq     rn, x     rn, and s1, . . . , sq     r. (when si = 0, we take
sifij(zi/si) to be 0 if zi = 0 and     if zi 6= 0.) explain why this problem is convex, and
equivalent to the original problem.

4.57 capacity of a communication channel. we consider a communication channel, with input
x(t)     {1, . . . , n}, and output y (t)     {1, . . . , m}, for t = 1, 2, . . . (in seconds, say). the
relation between the input and the output is given statistically:

pij = prob(y (t) = i|x(t) = j),

i = 1, . . . , m,

j = 1, . . . , n.

the matrix p     rm  n is called the channel transition matrix, and the channel is called
a discrete memoryless channel.
a famous result of shannon states that information can be sent over the communication
channel, with arbitrarily small id203 of error, at any rate less than a number c,
called the channel capacity, in bits per second. shannon also showed that the capacity of
a discrete memoryless channel can be found by solving an optimization problem. assume
that x has a id203 distribution denoted x     rn, i.e.,

xj = prob(x = j),

j = 1, . . . , n.

208

4 id76 problems

the mutual information between x and y is given by

i(x; y ) =

mxi=1

nxj=1

xjpij log2

then the channel capacity c is given by

.

k=1 xkpik

pijpn

c = sup

i(x; y ),

x

where the supremum is over all possible id203 distributions for the input x, i.e.,
over x (cid:23) 0, 1t x = 1.
show how the channel capacity can be computed using id76.
hint.
output y , and show that the mutual information can be expressed as

introduce the variable y = p x, which gives the id203 distribution of the

i(x; y ) = ct x    

yi log2 yi,

mxi=1

where cj =pm

i=1 pij log2 pij, j = 1, . . . , n.

4.58 optimal consumption. in this problem we consider the optimal way to consume (or spend)
an initial amount of money (or other asset) k0 over time. the variables are c0, . . . , ct ,
where ct     0 denotes the consumption in period t. the utility derived from a consumption
level c is given by u(c), where u : r     r is an increasing concave function. the present
value of the utility derived from the consumption is given by

u =

txt=0

  tu(ct),

where 0 <    < 1 is a discount factor.
let kt denote the amount of money available for investment in period t. we assume
that it earns an investment return given by f (kt), where f : r     r is an increasing,
concave investment return function, which satis   es f (0) = 0. for example if the funds
earn simple interest at rate r percent per period, we have f (a) = (r/100)a. the amount
to be consumed, i.e., ct, is withdrawn at the end of the period, so we have the recursion

kt+1 = kt + f (kt)     ct,

t = 0, . . . , t.

the initial sum k0 > 0 is given. we require kt     0, t = 1, . . . , t +1 (but more sophisticated
models, which allow kt < 0, can be considered).
show how to formulate the problem of maximizing u as a id76 problem.
explain how the problem you formulate is equivalent to this one, and exactly how the
two are related.
hint. show that we can replace the recursion for kt given above with the inequalities

kt+1     kt + f (kt)     ct,

t = 0, . . . , t.

(interpretation: the inequalities give you the option of throwing money away in each
period.) for a more general version of this trick, see exercise 4.6.

4.59 robust optimization.

in some optimization problems there is uncertainty or variation
in the objective and constraint functions, due to parameters or factors that are either
beyond our control or unknown. we can model this situation by making the objective
and constraint functions f0, . . . , fm functions of the optimization variable x     rn and
a parameter vector u     rk that is unknown, or varies. in the stochastic optimization

exercises

209

approach, the parameter vector u is modeled as a random variable with a known dis-
tribution, and we work with the expected values eu fi(x, u). in the worst-case analysis
approach, we are given a set u that u is known to lie in, and we work with the maximum
or worst-case values supu   u fi(x, u). to simplify the discussion, we assume there are no
equality constraints.

(a) stochastic optimization. we consider the problem

minimize e f0(x, u)
subject to e fi(x, u)     0,

i = 1, . . . , m,

where the expectation is with respect to u. show that if fi are convex in x for each
u, then this stochastic optimization problem is convex.

(b) worst-case optimization. we consider the problem

minimize
subject to

supu   u f0(x, u)
supu   u fi(x, u)     0,

i = 1, . . . , m.

show that if fi are convex in x for each u, then this worst-case optimization problem
is convex.

(c) finite set of possible parameter values. the observations made in parts (a) and (b)
are most useful when we have analytical or easily evaluated expressions for the
expected values e fi(x, u) or the worst-case values supu   u fi(x, u).
suppose we are given the set of possible values of the parameter is    nite, i.e., we
have u     {u1, . . . , un}. for the stochastic case, we are also given the probabilities
of each value: prob(u = ui) = pi, where p     rn , p (cid:23) 0, 1t p = 1. in the worst-case
formulation, we simply take u     {u1, . . . , un}.
show how to set up the worst-case and stochastic optimization problems explicitly
(i.e., give explicit expressions for supu   u fi and eu fi).

4.60 log-optimal investment strategy. we consider a portfolio problem with n assets held over
n periods. at the beginning of each period, we re-invest our total wealth, redistributing
it over the n assets using a    xed, constant, allocation strategy x     rn, where x (cid:23) 0,
1t x = 1. in other words, if w (t     1) is our wealth at the beginning of period t, then
during period t we invest xiw (t    1) in asset i. we denote by   (t) the total return during
period t, i.e.,   (t) = w (t)/w (t     1). at the end of the n periods our wealth has been

multiplied by the factorqn

t=1   (t). we call

1
n

nxt=1

log   (t)

the growth rate of the investment over the n periods. we are interested in determining
an allocation strategy x that maximizes growth of our total wealth for large n .
we use a discrete stochastic model to account for the uncertainty in the returns. we
assume that during each period there are m possible scenarios, with probabilities   j,
j = 1, . . . , m.
in scenario j, the return for asset i over one period is given by pij.
therefore, the return   (t) of our portfolio during period t is a random variable, with
m possible values pt

mx, and distribution

1 x, . . . , pt

  j = prob(  (t) = pt

j x),

j = 1, . . . , m.

we assume the same scenarios for each period, with (identical) independent distributions.
using the law of large numbers, we have

lim
n      

1
n

log(cid:18) w (n )

w (0)(cid:19) = lim

n      

1
n

nxt=1

log   (t) = e log   (t) =

  j log(pt

j x).

mxj=1

210

4 id76 problems

in other words, with investment strategy x, the long term growth rate is given by

rlt =

mxj=1

  j log(pt

j x).

the investment strategy x that maximizes this quantity is called the log-optimal invest-
ment strategy, and can be found by solving the optimization problem

maximize pm

subject to x (cid:23) 0,

j=1   j log(pt

j x)
1t x = 1,

with variable x     rn.
show that this is a id76 problem.

4.61 optimization with logistic model. a random variable x     {0, 1} satis   es

prob(x = 1) = p =

exp(at x + b)

1 + exp(at x + b)

,

where x     rn is a vector of variables that a   ect the id203, and a and b are known
parameters. we can think of x = 1 as the event that a consumer buys a product, and
x as a vector of variables that a   ect the id203, e.g., advertising e   ort, retail price,
discounted price, packaging expense, and other factors. the variable x, which we are to
optimize over, is subject to a set of linear constraints, f x (cid:22) g.
formulate the following problems as id76 problems.

(a) maximizing buying id203. the goal is to choose x to maximize p.
(b) maximizing expected pro   t. let ct x+d be the pro   t derived from selling the product,
which we assume is positive for all feasible x. the goal is to maximize the expected
pro   t, which is p(ct x + d).

4.62 optimal power and bandwidth allocation in a gaussian broadcast channel. we consider a
communication system in which a central node transmits messages to n receivers. (   gaus-
sian    refers to the type of noise that corrupts the transmissions.) each receiver channel
is characterized by its (transmit) power level pi     0 and its bandwidth wi     0. the
power and bandwidth of a receiver channel determine its bit rate ri (the rate at which
information can be sent) via

ri =   iwi log(1 +   ipi/wi),

where   i and   i are known positive constants. for wi = 0, we take ri = 0 (which is
what you get if you take the limit as wi     0).
the powers must satisfy a total power constraint, which has the form

p1 +        + pn = ptot,

where ptot > 0 is a given total power available to allocate among the channels. similarly,
the bandwidths must satisfy

w1 +        + wn = wtot,

where wtot > 0 is the (given) total available bandwidth. the optimization variables in
this problem are the powers and bandwidths, i.e., p1, . . . , pn, w1, . . . , wn.
the objective is to maximize the total utility,

ui(ri),

nxi=1

exercises

211

where ui : r     r is the utility function associated with the ith receiver.
(you can
think of ui(ri) as the revenue obtained for providing a bit rate ri to receiver i, so the
objective is to maximize the total revenue.) you can assume that the utility functions ui
are nondecreasing and concave.
pose this problem as a id76 problem.

4.63 optimally balancing manufacturing cost and yield. the vector x     rn denotes the nomi-
nal parameters in a manufacturing process. the yield of the process, i.e., the fraction of
manufactured goods that is acceptable, is given by y (x). we assume that y is log-concave
(which is often the case; see example 3.43). the cost per unit to manufacture the product
is given by ct x, where c     rn. the cost per acceptable unit is ct x/y (x). we want to
minimize ct x/y (x), subject to some convex constraints on x such as a linear inequalities
ax (cid:22) b. (you can assume that over the feasible set we have ct x > 0 and y (x) > 0.)
this problem is not a convex or quasiid76 problem, but it can be solved
using id76 and a one-dimensional search. the basic ideas are given below;
you must supply all details and justi   cation.
(a) show that the function f : r     r given by

f (a) = sup{y (x) | ax (cid:22) b, ct x = a},

which gives the maximum yield versus cost, is log-concave. this means that by
solving a id76 problem (in x) we can evaluate the function f .

(b) suppose that we evaluate the function f for enough values of a to give a good approx-
imation over the range of interest. explain how to use these data to (approximately)
solve the problem of minimizing cost per good product.

4.64 optimization with recourse. in an optimization problem with recourse, also called two-
stage optimization, the cost function and constraints depend not only on our choice of
variables, but also on a discrete random variable s     {1, . . . , s}, which is interpreted as
specifying which of s scenarios occurred. the scenario random variable s has known
id203 distribution   , with   i = prob(s = i), i = 1, . . . , s.
in two-stage optimization, we are to choose the values of two variables, x     rn and
z     rq. the variable x must be chosen before the particular scenario s is known; the
variable z, however, is chosen after the value of the scenario random variable is known.
in other words, z is a function of the scenario random variable s. to describe our choice
z, we list the values we would choose under the di   erent scenarios, i.e., we list the vectors

here z3 is our choice of z when s = 3 occurs, and so on. the set of values

z1, . . . , zs     rq.

x     rn,

z1, . . . , zs     rq

is called the policy, since it tells us what choice to make for x (independent of which
scenario occurs), and also, what choice to make for z in each possible scenario.
the variable z is called the recourse variable (or second-stage variable), since it allows
us to take some action or make a choice after we know which scenario occurred.
in
contrast, our choice of x (which is called the    rst-stage variable) must be made without
any knowledge of the scenario.
for simplicity we will consider the case with no constraints. the cost function is given by

f : rn    rq    {1, . . . , s}     r,

where f (x, z, i) gives the cost when the    rst-stage choice x is made, second-stage choice
z is made, and scenario i occurs. we will take as the overall objective, to be minimized
over all policies, the expected cost

e f (x, zs, s) =

  if (x, zi, i).

sxi=1

212

4 id76 problems

suppose that f is a convex function of (x, z), for each scenario i = 1, . . . , s. explain
how to    nd an optimal policy, i.e., one that minimizes the expected cost over all possible
policies, using id76.

4.65 optimal operation of a hybrid vehicle. a hybrid vehicle has an internal combustion engine,
a motor/generator connected to a storage battery, and a conventional (friction) brake. in
this exercise we consider a (highly simpli   ed) model of a parallel hybrid vehicle, in which
both the motor/generator and the engine are directly connected to the drive wheels. the
engine can provide power to the wheels, and the brake can take power from the wheels,
turning it into heat. the motor/generator can act as a motor, when it uses energy stored
in the battery to deliver power to the wheels, or as a generator, when it takes power from
the wheels or engine, and uses the power to charge the battery. when the generator takes
power from the wheels and charges the battery, it is called regenerative braking; unlike
ordinary friction braking, the energy taken from the wheels is stored, and can be used
later. the vehicle is judged by driving it over a known,    xed test track to evaluate its
fuel e   ciency.
a diagram illustrating the power    ow in the hybrid vehicle is shown below. the arrows
indicate the direction in which the power    ow is considered positive. the engine power
peng, for example, is positive when it is delivering power; the brake power pbr is positive
when it is taking power from the wheels. the power preq is the required power at the
wheels. it is positive when the wheels require power (e.g., when the vehicle accelerates,
climbs a hill, or cruises on level terrain). the required wheel power is negative when the
vehicle must decelerate rapidly, or descend a hill.

engine

peng

brake

pbr

preq

wheels

battery

pmg

motor/
generator

all of these powers are functions of time, which we discretize in one second intervals, with
t = 1, 2, . . . , t . the required wheel power preq(1), . . . , preq(t ) is given. (the speed of
the vehicle on the track is speci   ed, so together with known road slope information, and
known aerodynamic and other losses, the power required at the wheels can be calculated.)
power is conserved, which means we have

preq(t) = peng(t) + pmg(t)     pbr(t),

t = 1, . . . , t.

the brake can only dissipate power, so we have pbr(t)     0 for each t. the engine can only
provide power, and only up to a given limit p max

eng , i.e., we have

the motor/generator power is also limited: pmg must satisfy

0     peng(t)     p max
eng ,

t = 1, . . . , t.

p min
mg     pmg(t)     p max
mg ,

t = 1, . . . , t.

mg > 0 is the maximum motor power, and    p min

here p max
power.
the battery charge or energy at time t is denoted e(t), t = 1, . . . , t + 1. the battery
energy satis   es

mg > 0 is the maximum generator

e(t + 1) = e(t)     pmg(t)       |pmg(t)|,

t = 1, . . . , t,

exercises

213

where    > 0 is a known parameter. (the term    pmg(t) represents the energy removed
or added the battery by the motor/generator, ignoring any losses. the term      |pmg(t)|
represents energy lost through ine   ciencies in the battery or motor/generator.)
the battery charge must be between 0 (empty) and its limit emax
batt (full), at all times. (if
e(t) = 0, the battery is fully discharged, and no more energy can be extracted from it;
when e(t) = emax
batt , the battery is full and cannot be charged.) to make the comparison
with non-hybrid vehicles fair, we    x the initial battery charge to equal the    nal battery
charge, so the net energy change is zero over the track: e(1) = e(t + 1). we do not
specify the value of the initial (and    nal) energy.
the objective in the problem (to be minimized) is the total fuel consumed by the engine,
which is

ftotal =

f (peng(t)),

txt=1

where f : r     r is the fuel use characteristic of the engine. we assume that f is
positive, increasing, and convex.
formulate this problem as a id76 problem, with variables peng(t), pmg(t),
and pbr(t) for t = 1, . . . , t , and e(t) for t = 1, . . . , t + 1. explain why your formulation
is equivalent to the problem described above.

chapter 5

duality

5.1 the lagrange dual function

5.1.1 the lagrangian

we consider an optimization problem in the standard form (4.1):

minimize
subject to

f0(x)
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p,

(5.1)

with variable x     rn. we assume its domain d =tm

i=1 dom hi
is nonempty, and denote the optimal value of (5.1) by p   . we do not assume the
problem (5.1) is convex.

i=0 dom fi     tp

the basic idea in lagrangian duality is to take the constraints in (5.1) into
account by augmenting the objective function with a weighted sum of the constraint
functions. we de   ne the lagrangian l : rn    rm    rp     r associated with the
problem (5.1) as

l(x,   ,   ) = f0(x) +

  ifi(x) +

mxi=1

  ihi(x),

pxi=1

with dom l = d    rm    rp. we refer to   i as the lagrange multiplier associated
with the ith inequality constraint fi(x)     0; similarly we refer to   i as the lagrange
multiplier associated with the ith equality constraint hi(x) = 0. the vectors    and
   are called the dual variables or lagrange multiplier vectors associated with the
problem (5.1).

216

5 duality

5.1.2 the lagrange dual function

we de   ne the lagrange dual function (or just dual function) g : rm    rp     r as
the minimum value of the lagrangian over x: for        rm,        rp,
pxi=1

x   d f0(x) +

  ihi(x)! .

g(  ,   ) = inf
x   d

l(x,   ,   ) = inf

mxi=1

  ifi(x) +

when the lagrangian is unbounded below in x, the dual function takes on the
value       . since the dual function is the pointwise in   mum of a family of a   ne
functions of (  ,   ), it is concave, even when the problem (5.1) is not convex.

5.1.3 lower bounds on optimal value

the dual function yields lower bounds on the optimal value p    of the problem (5.1):
for any    (cid:23) 0 and any    we have

g(  ,   )     p   .

(5.2)

this important property is easily veri   ed. suppose   x is a feasible point for the
problem (5.1), i.e., fi(  x)     0 and hi(  x) = 0, and    (cid:23) 0. then we have

  ifi(  x) +

mxi=1

pxi=1

  ihi(  x)     0,

since each term in the    rst sum is nonpositive, and each term in the second sum is
zero, and therefore

l(  x,   ,   ) = f0(  x) +

  ifi(  x) +

mxi=1

pxi=1

  ihi(  x)     f0(  x).

g(  ,   ) = inf
x   d

l(x,   ,   )     l(  x,   ,   )     f0(  x).

hence

since g(  ,   )     f0(  x) holds for every feasible point   x, the inequality (5.2) follows.
the lower bound (5.2) is illustrated in    gure 5.1, for a simple problem with x     r
and one inequality constraint.
the inequality (5.2) holds, but is vacuous, when g(  ,   ) =       . the dual
function gives a nontrivial lower bound on p    only when    (cid:23) 0 and (  ,   )     dom g,
i.e., g(  ,   ) >       . we refer to a pair (  ,   ) with    (cid:23) 0 and (  ,   )     dom g as dual
feasible, for reasons that will become clear later.

5.1.4 linear approximation interpretation

the lagrangian and lower bound property can be given a simple interpretation,
based on a linear approximation of the indicator functions of the sets {0} and    r+.

5.1 the lagrange dual function

217

5

4

3

2

1

0

   1
   2
   1

   0.5

0
x

0.5

1

figure 5.1 lower bound from a dual feasible point. the solid curve shows the
objective function f0, and the dashed curve shows the constraint function f1.
the feasible set is the interval [   0.46, 0.46], which is indicated by the two
dotted vertical lines. the optimal point and value are x    =    0.46, p    = 1.54
(shown as a circle). the dotted curves show l(x,   ) for    = 0.1, 0.2, . . . , 1.0.
each of these has a minimum value smaller than p   , since on the feasible set
(and for        0) we have l(x,   )     f0(x).

)
  
(
g

1.6

1.5

1.4

1.3

1.2

1.1

1
0

0.2

0.4

  

0.6

0.8

1

figure 5.2 the dual function g for the problem in    gure 5.1. neither f0 nor
f1 is convex, but the dual function is concave. the horizontal dashed line
shows p   , the optimal value of the problem.

218

5 duality

we    rst rewrite the original problem (5.1) as an unconstrained problem,

i=1 i0(hi(x)),
where i    : r     r is the indicator function for the nonpositive reals,

i=1 i   (fi(x)) +pp

minimize

f0(x) +pm
i   (u) =(cid:26) 0

u     0
    u > 0,

(5.3)

and similarly, i0 is the indicator function of {0}. in the formulation (5.3), the func-
tion i   (u) can be interpreted as expressing our irritation or displeasure associated
with a constraint function value u = fi(x): it is zero if fi(x)     0, and in   nite if
fi(x) > 0. in a similar way, i0(u) gives our displeasure for an equality constraint
value u = hi(x). we can think of i    as a    brick wall    or    in   nitely hard    displea-
sure function; our displeasure rises from zero to in   nite as fi(x) transitions from
nonpositive to positive.

now suppose in the formulation (5.3) we replace the function i   (u) with the
linear function   iu, where   i     0, and the function i0(u) with   iu. the objective
becomes the lagrangian function l(x,   ,   ), and the dual function value g(  ,   ) is
the optimal value of the problem

minimize l(x,   ,   ) = f0(x) +pm

i=1   ifi(x) +pp

in this formulation, we use a linear or    soft    displeasure function in place of i   
and i0. for an inequality constraint, our displeasure is zero when fi(x) = 0, and is
positive when fi(x) > 0 (assuming   i > 0); our displeasure grows as the constraint
becomes    more violated   . unlike the original formulation, in which any nonpositive
value of fi(x) is acceptable, in the soft formulation we actually derive pleasure from
constraints that have margin, i.e., from fi(x) < 0.

i=1   ihi(x).

(5.4)

clearly the approximation of the indicator function i   (u) with a linear function
  iu is rather poor. but the linear function is at least an underestimator of the
indicator function. since   iu     i   (u) and   iu     i0(u) for all u, we see immediately
that the dual function yields a lower bound on the optimal value of the original
problem.

the idea of replacing the    hard    constraints with    soft    versions will come up

again when we consider interior-point methods (  11.2.1).

5.1.5 examples

in this section we give some examples for which we can derive an analytical ex-
pression for the lagrange dual function.

least-squares solution of linear equations

we consider the problem

minimize
subject to ax = b,

xt x

(5.5)

where a     rp  n. this problem has no inequality constraints and p (linear) equality
constraints. the lagrangian is l(x,   ) = xt x +   t (ax     b), with domain rn   

5.1 the lagrange dual function

219

rp. the dual function is given by g(  ) = inf x l(x,   ). since l(x,   ) is a convex
quadratic function of x, we can    nd the minimizing x from the optimality condition

   xl(x,   ) = 2x + at    = 0,

which yields x =    (1/2)at   . therefore the dual function is

g(  ) = l(   (1/2)at   ,   ) =    (1/4)  t aat        bt   ,

which is a concave quadratic function, with domain rp. the lower bound prop-
erty (5.2) states that for any        rp, we have

   (1/4)  t aat        bt        inf{xt x | ax = b}.

standard form lp

consider an lp in standard form,

ct x

minimize
subject to ax = b
x (cid:23) 0,

(5.6)

which has inequality constraint functions fi(x) =    xi, i = 1, . . . , n. to form
the lagrangian we introduce multipliers   i for the n inequality constraints and
multipliers   i for the equality constraints, and obtain

l(x,   ,   ) = ct x    

nxi=1

the dual function is

  ixi +   t (ax     b) =    bt    + (c + at          )t x.

g(  ,   ) = inf
x

l(x,   ,   ) =    bt    + inf

x

(c + at          )t x,

which is easily determined analytically, since a linear function is bounded below
only when it is identically zero. thus, g(  ,   ) =        except when c + at           = 0,
in which case it is    bt   :

g(  ,   ) =(cid:26)    bt    at           + c = 0

       otherwise.

note that the dual function g is    nite only on a proper a   ne subset of rm    rp.
we will see that this is a common occurrence.
the lower bound property (5.2) is nontrivial only when    and    satisfy    (cid:23) 0
and at           + c = 0. when this occurs,    bt    is a lower bound on the optimal
value of the lp (5.6).

two-way partitioning problem

we consider the (nonconvex) problem

minimize
subject to x2

xt w x
i = 1,

i = 1, . . . , n,

(5.7)

220

5 duality

where w     sn. the constraints restrict the values of xi to 1 or    1, so the problem
is equivalent to    nding the vector with components   1 that minimizes xt w x. the
feasible set here is    nite (it contains 2n points) so this problem can in principle
be solved by simply checking the objective value of each feasible point. since the
number of feasible points grows exponentially, however, this is possible only for
small problems (say, with n     30). in general (and for n larger than, say, 50) the
problem (5.7) is very di   cult to solve.
we can interpret the problem (5.7) as a two-way partitioning problem on a set

of n elements, say, {1, . . . , n}: a feasible x corresponds to the partition

{1, . . . , n} = {i | xi =    1}     {i | xi = 1}.

the matrix coe   cient wij can be interpreted as the cost of having the elements i
and j in the same partition, and    wij is the cost of having i and j in di   erent
partitions. the objective in (5.7) is the total cost, over all pairs of elements, and
the problem (5.7) is to    nd the partition with least total cost.

we now derive the dual function for this problem. the lagrangian is

l(x,   ) = xt w x +

nxi=1

  i(x2

i     1)

= xt (w + diag(  ))x     1t   .
we obtain the lagrange dual function by minimizing over x:

g(  ) = inf
x

xt (w + diag(  ))x     1t   
= (cid:26)    1t    w + diag(  ) (cid:23) 0

       otherwise,

where we use the fact that the in   mum of a quadratic form is either zero (if the
form is positive semide   nite) or        (if the form is not positive semide   nite).
problem (5.7). for example, we can take the speci   c value of the dual variable

this dual function provides lower bounds on the optimal value of the di   cult

   =      min(w )1,

which is dual feasible, since

w + diag(  ) = w       min(w )i (cid:23) 0.

this yields the bound on the optimal value p   

p           1t    = n  min(w ).

(5.8)

remark 5.1 this lower bound on p    can also be obtained without using the lagrange
dual function. first, we replace the constraints x2
i = n,
to obtain the modi   ed problem

1 = 1, . . . , x2

i=1 x2

n = 1 withpn

xt w x

minimize

subject to pn

i=1 x2

i = n.

(5.9)

5.1 the lagrange dual function

221

the constraints of the original problem (5.7) imply the constraint here, so the optimal
value of the problem (5.9) is a lower bound on p   , the optimal value of (5.7). but the
modi   ed problem (5.9) is easily solved as an eigenvalue problem, with optimal value
n  min(w ).

5.1.6 the lagrange dual function and conjugate functions

recall from   3.3 that the conjugate f     of a function f : rn     r is given by

f    (y) = sup

x   dom f(cid:0)yt x     f (x)(cid:1) .

the conjugate function and lagrange dual function are closely related. to see one
simple connection, consider the problem

minimize
f (x)
subject to x = 0

(which is not very interesting, and solvable by inspection). this problem has
lagrangian l(x,   ) = f (x) +   t x, and dual function

g(  ) = inf

x (cid:0)f (x) +   t x(cid:1) =     sup

x (cid:0)(     )t x     f (x)(cid:1) =    f    (     ).

more generally (and more usefully), consider an optimization problem with

linear inequality and equality constraints,

minimize
f0(x)
subject to ax (cid:22) b
cx = d.

(5.10)

using the conjugate of f0 we can write the dual function for the problem (5.10) as

g(  ,   ) = inf

x (cid:0)f0(x) +   t (ax     b) +   t (cx     d)(cid:1)

=    bt        dt    + inf
=    bt        dt        f    

x (cid:0)f0(x) + (at    + c t   )t x(cid:1)
0 (   at        c t   ).

(5.11)

the domain of g follows from the domain of f    
0 :

dom g = {(  ,   ) |     at        c t        dom f    
0}.

let us illustrate this with a few examples.

equality constrained norm minimization

consider the problem

minimize
subject to ax = b,

kxk

(5.12)

222

5 duality

where k    k is any norm. recall (from example 3.26 on page 93) that the conjugate
of f0 = k    k is given by

f    

0 (y) =(cid:26) 0

kyk        1
    otherwise,

the indicator function of the dual norm unit ball.

using the result (5.11) above, the dual function for the problem (5.12) is given

by

g(  ) =    bt        f    

0 (   at   ) =(cid:26)    bt   

kat   k        1
       otherwise.

id178 maximization

consider the id178 maximization problem

minimize
subject to ax (cid:22) b
1t x = 1

f0(x) =pn

i=1 xi log xi

(5.13)

where dom f0 = rn
++. the conjugate of the negative id178 function u log u,
with scalar variable u, is ev   1 (see example 3.21 on page 91). since f0 is a sum of
negative id178 functions of di   erent variables, we conclude that its conjugate is

f    
0 (y) =

eyi   1,

nxi=1

with dom f    
given by

0 = rn. using the result (5.11) above, the dual function of (5.13) is

g(  ,   ) =    bt              

nxi=1

where ai is the ith column of a.

e   at

i           1 =    bt               e        1

e   at
i   

nxi=1

minimum volume covering ellipsoid
consider the problem with variable x     sn,

minimize
subject to at

f0(x) = log det x    1
i xai     1,

i = 1, . . . , m,

(5.14)

where dom f0 = sn
with each x     sn

++. the problem (5.14) has a simple geometric interpretation.

++ we associate the ellipsoid, centered at the origin,

ex = {z | zt xz     1}.

the volume of this ellipsoid is proportional to (cid:0)det x    1(cid:1)1/2

, so the objective
of (5.14) is, except for a constant and a factor of two, the logarithm of the volume

5.2 the lagrange dual problem

223

of ex . the constraints of the problem (5.14) are that ai     ex . thus the prob-
lem (5.14) is to determine the minimum volume ellipsoid, centered at the origin,
that includes the points a1, . . . , am.

the inequality constraints in problem (5.14) are a   ne; they can be expressed

as

in example 3.23 (page 92) we found that the conjugate of f0 is

i )x(cid:1)     1.

tr(cid:0)(aiat
f    
0 (y ) = log det(   y )   1     n,

with dom f    
problem (5.14) is given by

0 =    sn

++. applying the result (5.11) above, the dual function for the

i(cid:1)     1t    + n pm

i=1   iaiat

i     0

otherwise.

(5.15)

      

i=1   iaiat

g(  ) =(cid:26) log det(cid:0)pm
thus, for any    (cid:23) 0 withpm
log det  mxi=1

i=1   iaiat

i     0, the number

  iaiat

i!     1t    + n

is a lower bound on the optimal value of the problem (5.14).

5.2 the lagrange dual problem

for each pair (  ,   ) with    (cid:23) 0, the lagrange dual function gives us a lower bound
on the optimal value p    of the optimization problem (5.1). thus we have a lower
bound that depends on some parameters   ,   . a natural question is: what is the
best lower bound that can be obtained from the lagrange dual function?

this leads to the optimization problem

maximize
g(  ,   )
subject to    (cid:23) 0.

(5.16)

this problem is called the lagrange dual problem associated with the problem (5.1).
in this context the original problem (5.1) is sometimes called the primal problem.
the term dual feasible, to describe a pair (  ,   ) with    (cid:23) 0 and g(  ,   ) >       ,
now makes sense. it means, as the name implies, that (  ,   ) is feasible for the dual
problem (5.16). we refer to (     ,      ) as dual optimal or optimal lagrange multipliers
if they are optimal for the problem (5.16).

the lagrange dual problem (5.16) is a id76 problem, since the
objective to be maximized is concave and the constraint is convex. this is the case
whether or not the primal problem (5.1) is convex.

224

5 duality

5.2.1 making dual constraints explicit

the examples above show that it is not uncommon for the domain of the dual
function,

dom g = {(  ,   ) | g(  ,   ) >       },

to have dimension smaller than m + p. in many cases we can identify the a   ne
hull of dom g, and describe it as a set of linear equality constraints. roughly
speaking, this means we can identify the equality constraints that are    hidden    or
   implicit    in the objective g of the dual problem (5.16). in this case we can form
an equivalent problem, in which these equality constraints are given explicitly as
constraints. the following examples demonstrate this idea.

lagrange dual of standard form lp

on page 219 we found that the lagrange dual function for the standard form lp

ct x

minimize
subject to ax = b
x (cid:23) 0

(5.17)

is given by

g(  ,   ) =(cid:26)    bt    at           + c = 0

       otherwise.

strictly speaking, the lagrange dual problem of the standard form lp is to maxi-
mize this dual function g subject to    (cid:23) 0, i.e.,

g(  ,   ) =(cid:26)    bt    at           + c = 0

       otherwise

maximize
subject to    (cid:23) 0.

(5.18)

here g is    nite only when at           + c = 0. we can form an equivalent problem
by making these equality constraints explicit:

maximize    bt   
subject to at           + c = 0
   (cid:23) 0.

this problem, in turn, can be expressed as

maximize    bt   
subject to at    + c (cid:23) 0,

(5.19)

(5.20)

which is an lp in inequality form.

note the subtle distinctions between these three problems. the lagrange dual
of the standard form lp (5.17) is the problem (5.18), which is equivalent to (but
not the same as) the problems (5.19) and (5.20). with some abuse of terminology,
we refer to the problem (5.19) or the problem (5.20) as the lagrange dual of the
standard form lp (5.17).

5.2 the lagrange dual problem

225

lagrange dual of inequality form lp

in a similar way we can    nd the lagrange dual problem of a linear program in
inequality form

ct x

minimize
subject to ax (cid:22) b.

(5.21)

the lagrangian is

l(x,   ) = ct x +   t (ax     b) =    bt    + (at    + c)t x,

so the dual function is

g(  ) = inf
x

l(x,   ) =    bt    + inf

x

(at    + c)t x.

the in   mum of a linear function is       , except in the special case when it is
identically zero, so the dual function is

g(  ) =(cid:26)    bt    at    + c = 0

       otherwise.

the dual variable    is dual feasible if    (cid:23) 0 and at    + c = 0.
the lagrange dual of the lp (5.21) is to maximize g over all    (cid:23) 0. again
we can reformulate this by explicitly including the dual feasibility conditions as
constraints, as in

maximize    bt   
subject to at    + c = 0
   (cid:23) 0,

(5.22)

which is an lp in standard form.

note the interesting symmetry between the standard and inequality form lps
and their duals: the dual of a standard form lp is an lp with only inequality
constraints, and vice versa. one can also verify that the lagrange dual of (5.22) is
(equivalent to) the primal problem (5.21).

5.2.2 weak duality

the optimal value of the lagrange dual problem, which we denote d   , is, by def-
inition, the best lower bound on p    that can be obtained from the lagrange dual
function. in particular, we have the simple but important inequality

d        p   ,

(5.23)

which holds even if the original problem is not convex. this property is called weak
duality.

the weak duality inequality (5.23) holds when d    and p    are in   nite. for
example, if the primal problem is unbounded below, so that p    =       , we must
have d    =       , i.e., the lagrange dual problem is infeasible. conversely, if the
dual problem is unbounded above, so that d    =    , we must have p    =    , i.e., the
primal problem is infeasible.

226

5 duality

we refer to the di   erence p        d    as the optimal duality gap of the original
problem, since it gives the gap between the optimal value of the primal problem
and the best (i.e., greatest) lower bound on it that can be obtained from the
lagrange dual function. the optimal duality gap is always nonnegative.

the bound (5.23) can sometimes be used to    nd a lower bound on the optimal
value of a problem that is di   cult to solve, since the dual problem is always convex,
and in many cases can be solved e   ciently, to    nd d   . as an example, consider
the two-way partitioning problem (5.7) described on page 219. the dual problem
is an sdp,

maximize    1t   
subject to w + diag(  ) (cid:23) 0,

with variable        rn. this problem can be solved e   ciently, even for relatively
its optimal value is a lower bound on the
large values of n, such as n = 1000.
optimal value of the two-way partitioning problem, and is always at least as good
as the lower bound (5.8) based on   min(w ).

5.2.3 strong duality and slater   s constraint quali   cation

if the equality

d    = p   

(5.24)

holds, i.e., the optimal duality gap is zero, then we say that strong duality holds.
this means that the best bound that can be obtained from the lagrange dual
function is tight.

strong duality does not, in general, hold. but if the primal problem (5.1) is

convex, i.e., of the form

minimize
subject to

f0(x)
fi(x)     0,
ax = b,

i = 1, . . . , m,

(5.25)

with f0, . . . , fm convex, we usually (but not always) have strong duality. there are
many results that establish conditions on the problem, beyond convexity, under
which strong duality holds. these conditions are called constraint quali   cations.
one simple constraint quali   cation is slater   s condition: there exists an x    

relintd such that

fi(x) < 0,

i = 1, . . . , m,

ax = b.

(5.26)

such a point is sometimes called strictly feasible, since the inequality constraints
hold with strict inequalities. slater   s theorem states that strong duality holds, if
slater   s condition holds (and the problem is convex).

slater   s condition can be re   ned when some of the inequality constraint func-
tions fi are a   ne.
if the    rst k constraint functions f1, . . . , fk are a   ne, then
strong duality holds provided the following weaker condition holds: there exists
an x     relintd with

fi(x)     0,

i = 1, . . . , k,

fi(x) < 0,

i = k + 1, . . . , m,

ax = b.

(5.27)

5.2 the lagrange dual problem

227

in other words, the a   ne inequalities do not need to hold with strict inequal-
ity. note that the re   ned slater condition (5.27) reduces to feasibility when the
constraints are all linear equalities and inequalities, and dom f0 is open.

slater   s condition (and the re   nement (5.27)) not only implies strong duality
for convex problems. it also implies that the dual optimal value is attained when
d    >       , i.e., there exists a dual feasible (     ,      ) with g(     ,      ) = d    = p   . we
will prove that strong duality obtains, when the primal problem is convex and
slater   s condition holds, in   5.3.2.

5.2.4 examples

least-squares solution of linear equations

recall the problem (5.5):

minimize
subject to ax = b.

xt x

the associated dual problem is

maximize    (1/4)  t aat        bt   ,

which is an unconstrained concave quadratic maximization problem.

slater   s condition is simply that the primal problem is feasible, so p    = d   
provided b     r(a), i.e., p    <    . in fact for this problem we always have strong
duality, even when p    =    . this is the case when b 6    r(a), so there is a z with
at z = 0, bt z 6= 0. it follows that the dual function is unbounded above along the
line {tz | t     r}, so d    =     as well.
lagrange dual of lp

by the weaker form of slater   s condition, we    nd that strong duality holds for
any lp (in standard or inequality form) provided the primal problem is feasible.
applying this result to the duals, we conclude that strong duality holds for lps
if the dual is feasible. this leaves only one possible situation in which strong
duality for lps can fail: both the primal and id78 are infeasible. this
pathological case can, in fact, occur; see exercise 5.23.

lagrange dual of qcqp

we consider the qcqp

with p0     sn

(1/2)xt p0x + qt
minimize
subject to (1/2)xt pix + qt
++, and pi     sn

0 x + r0
i x + ri     0,

+, i = 1, . . . , m. the lagrangian is
l(x,   ) = (1/2)xt p (  )x + q(  )t x + r(  ),

i = 1, . . . , m,

(5.28)

where

p (  ) = p0 +

mxi=1

  ipi,

q(  ) = q0 +

  iqi,

r(  ) = r0 +

mxi=1

  iri.

mxi=1

228

5 duality

it is possible to derive an expression for g(  ) for general   , but it is quite compli-
cated. if    (cid:23) 0, however, we have p (  )     0 and

g(  ) = inf
x

l(x,   ) =    (1/2)q(  )t p (  )   1q(  ) + r(  ).

we can therefore express the dual problem as

maximize    (1/2)q(  )t p (  )   1q(  ) + r(  )
subject to    (cid:23) 0.

(5.29)

the slater condition says that strong duality between (5.29) and (5.28) holds if the
quadratic inequality constraints are strictly feasible, i.e., there exists an x with

(1/2)xt pix + qt

i x + ri < 0,

i = 1, . . . , m.

id178 maximization

our next example is the id178 maximization problem (5.13):

minimize pn

subject to ax (cid:22) b
1t x = 1,

i=1 xi log xi

with domain d = rn
dual problem is

+. the lagrange dual function was derived on page 222; the

maximize    bt               e        1pn

subject to    (cid:23) 0,

i=1 e   at
i   

(5.30)

with variables        rm,        r. the (weaker) slater condition for (5.13) tells us
that the optimal duality gap is zero if there exists an x     0 with ax (cid:22) b and
1t x = 1.
we can simplify the dual problem (5.30) by maximizing over the dual variable
   analytically. for    xed   , the objective function is maximized when the derivative
with respect to    is zero, i.e.,

   = log

nxi=1

e   at

i        1.

substituting this optimal value of    into the dual problem gives

maximize    bt        log(cid:16)pn

subject to    (cid:23) 0,

i=1 e   at

i   (cid:17)

which is a geometric program (in convex form) with nonnegativity constraints.

minimum volume covering ellipsoid

we consider the problem (5.14):

minimize
subject to at

log det x    1
i xai     1,

i = 1, . . . , m,

5.2 the lagrange dual problem

229

with domain d = sn
problem can be expressed as

++. the lagrange dual function is given by (5.15), so the dual

maximize
subject to    (cid:23) 0

log det(cid:0)pm

i=1   iaiat

i(cid:1)     1t    + n

(5.31)

where we take log det x =        if x 6    0.
x     sn
duality always obtains between (5.14) and the dual problem (5.31).

the (weaker) slater condition for the problem (5.14) is that there exists an
i xai     1, for i = 1, . . . , m. this is always satis   ed, so strong

++ with at

a nonconvex quadratic problem with strong duality

on rare occasions strong duality obtains for a nonconvex problem. as an important
example, we consider the problem of minimizing a nonconvex quadratic function
over the unit ball,

xt ax + 2bt x

minimize
subject to xt x     1,

(5.32)

where a     sn, a 6(cid:23) 0, and b     rn. since a 6(cid:23) 0, this is not a convex problem. this
problem is sometimes called the trust region problem, and arises in minimizing a
second-order approximation of a function over the unit ball, which is the region in
which the approximation is assumed to be approximately valid.

the lagrangian is

l(x,   ) = xt ax + 2bt x +   (xt x     1) = xt (a +   i)x + 2bt x       ,

so the dual function is given by

g(  ) =(cid:26)    bt (a +   i)   b        a +   i (cid:23) 0,

otherwise,

      

b     r(a +   i)

where (a +   i)    is the pseudo-inverse of a +   i. the lagrange dual problem is
thus

maximize    bt (a +   i)   b       
subject to a +   i (cid:23) 0,

b     r(a +   i),

(5.33)

with variable        r. although it is not obvious from this expression, this is a
id76 problem. in fact, it is readily solved since it can be expressed
as

maximize    pn

subject to             min(a),

i=1(qt

i b)2/(  i +   )       

where   i and qi are the eigenvalues and corresponding (orthonormal) eigenvectors
of a, and we interpret (qt

i b)2/0 as 0 if qt

i b = 0 and as     otherwise.

despite the fact that the original problem (5.32) is not convex, we always have
zero optimal duality gap for this problem: the optimal values of (5.32) and (5.33)
are always the same. in fact, a more general result holds: strong duality holds for
any optimization problem with quadratic objective and one quadratic inequality
constraint, provided slater   s condition holds; see   b.1.

230

5 duality

5.2.5 mixed strategies for matrix games

in this section we use strong duality to derive a basic result for zero-sum matrix
games. we consider a game with two players. player 1 makes a choice (or move)
k     {1, . . . , n}, and player 2 makes a choice l     {1, . . . , m}. player 1 then makes a
payment of pkl to player 2, where p     rn  m is the payo    matrix for the game.
the goal of player 1 is to make the payment as small as possible, while the goal of
player 2 is to maximize it.

the players use randomized or mixed strategies, which means that each player
makes his or her choice randomly and independently of the other player   s choice,
according to a id203 distribution:

prob(k = i) = ui,

i = 1, . . . , n,

prob(l = i) = vi,

i = 1, . . . , m.

here u and v give the id203 distributions of the choices of the two players,
i.e., their associated strategies. the expected payo    from player 1 to player 2 is
then

nxk=1

mxl=1

ukvlpkl = ut p v.

player 1 wishes to choose u to minimize ut p v, while player 2 wishes to choose v
to maximize ut p v.

let us    rst analyze the game from the point of view of player 1, assuming her
strategy u is known to player 2 (which clearly gives an advantage to player 2).
player 2 will choose v to maximize ut p v, which results in the expected payo   

sup{ut p v | v (cid:23) 0, 1t v = 1} = max

i=1,...,m

(p t u)i.

the best thing player 1 can do is to choose u to minimize this worst-case payo    to
player 2, i.e., to choose a strategy u that solves the problem

minimize maxi=1,...,m(p t u)i
1t u = 1,
subject to u (cid:23) 0,

(5.34)

which is a piecewise-linear id76 problem. we will denote the opti-
mal value of this problem as p   
1. this is the smallest expected payo    player 1 can
arrange to have, assuming that player 2 knows the strategy of player 1, and plays
to his own maximum advantage.

in a similar way we can consider the situation in which v, the strategy of
player 2, is known to player 1 (which gives an advantage to player 1). in this case
player 1 chooses u to minimize ut p v, which results in an expected payo    of

inf{ut p v | u (cid:23) 0, 1t u = 1} = min

i=1,...,n

(p v)i.

player 2 chooses v to maximize this, i.e., chooses a strategy v that solves the
problem

maximize mini=1,...,n(p v)i
subject to

1t v = 1,

v (cid:23) 0,

(5.35)

5.2 the lagrange dual problem

231

which is another id76 problem, with piecewise-linear (concave) ob-
jective. we will denote the optimal value of this problem as p   
2. this is the largest
expected payo    player 2 can guarantee getting, assuming that player 1 knows the
strategy of player 2.

it is intuitively obvious that knowing your opponent   s strategy gives an advan-
tage (or at least, cannot hurt), and indeed, it is easily shown that we always have
1     p   
p   
2, which is nonnegative, as the
advantage conferred on a player by knowing the opponent   s strategy.

2. we can interpret the di   erence, p   

1     p   

using duality, we can establish a result that is at    rst surprising: p   

1 = p   
2.
in other words, in a matrix game with mixed strategies, there is no advantage to
knowing your opponent   s strategy. we will establish this result by showing that
the two problems (5.34) and (5.35) are lagrange id78, for which strong
duality obtains.

we start by formulating (5.34) as an lp,

t

minimize
subject to u (cid:23) 0,

p t u (cid:22) t1,

1t u = 1

with extra variable t     r. introducing the multiplier    for p t u (cid:22) t1,    for u (cid:23) 0,
and    for 1t u = 1, the lagrangian is

t +   t (p t u     t1)       t u +   (1     1t u) =    + (1     1t   )t + (p          1       )t u,

so the dual function is

g(  ,   ,   ) =(cid:26)   

1t    = 1, p          1 =   

       otherwise.

the dual problem is then

  

maximize
subject to    (cid:23) 0,

1t    = 1,    (cid:23) 0

p          1 =   .

eliminating    we obtain the following lagrange dual of (5.34):

  

maximize
subject to    (cid:23) 0,

p    (cid:23)   1,

1t    = 1

with variables   ,   . but this is clearly equivalent to (5.35). since the lps are
feasible, we have strong duality; the optimal values of (5.34) and (5.35) are equal.

232

5 duality

5.3 geometric interpretation

5.3.1 weak and strong duality via set of values

we can give a simple geometric interpretation of the dual function in terms of the
set
g = {(f1(x), . . . , fm(x), h1(x), . . . , hp(x), f0(x))     rm    rp    r | x     d}, (5.36)
which is the set of values taken on by the constraint and objective functions. the
optimal value p    of (5.1) is easily expressed in terms of g as
p    = inf{t | (u, v, t)     g, u (cid:22) 0, v = 0}.

to evaluate the dual function at (  ,   ), we minimize the a   ne function

(  ,   , 1)t (u, v, t) =

  iui +

mxi=1

pxi=1

  ivi + t

over (u, v, t)     g, i.e., we have

g(  ,   ) = inf{(  ,   , 1)t (u, v, t) | (u, v, t)     g}.

in particular, we see that if the in   mum is    nite, then the inequality

(  ,   , 1)t (u, v, t)     g(  ,   )

de   nes a supporting hyperplane to g. this is sometimes referred to as a nonvertical
supporting hyperplane, because the last component of the normal vector is nonzero.
now suppose    (cid:23) 0. then, obviously, t     (  ,   , 1)t (u, v, t) if u (cid:22) 0 and v = 0.

therefore

p    = inf{t | (u, v, t)     g, u (cid:22) 0, v = 0}

    inf{(  ,   , 1)t (u, v, t) | (u, v, t)     g, u (cid:22) 0, v = 0}
    inf{(  ,   , 1)t (u, v, t) | (u, v, t)     g}
= g(  ,   ),

i.e., we have weak duality. this interpretation is illustrated in    gures 5.3 and 5.4,
for a simple problem with one inequality constraint.

epigraph variation

in this section we describe a variation on the geometric interpretation of duality in
terms of g, which explains why strong duality obtains for (most) convex problems.
we de   ne the set a     rm    rp    r as
a = g +(cid:0)rm

+    {0}    r+(cid:1) ,

or, more explicitly,

(5.37)

a = {(u, v, t) |    x     d, fi(x)     ui, i = 1, . . . , m,

hi(x) = vi, i = 1, . . . , p, f0(x)     t},

5.3 geometric interpretation

233

  u + t = g(  )

t

g

p   

g(  )

u

figure 5.3 geometric interpretation of dual function and lower bound g(  )    
p   , for a problem with one (inequality) constraint. given   , we minimize
(  , 1)t (u, t) over g = {(f1(x), f0(x)) | x     d}. this yields a supporting
hyperplane with slope      . the intersection of this hyperplane with the
u = 0 axis gives g(  ).

  2u + t = g(  2)

     u + t = g(     )

  1u + t = g(  1)

t

g

p   
d   

u

figure 5.4 supporting hyperplanes corresponding to three dual feasible val-
ues of   , including the optimum      . strong duality does not hold; the
optimal duality gap p        d    is positive.

234

5 duality

t

(0, p   )

(0, g(  ))

a

u

  u + t = g(  )

figure 5.5 geometric interpretation of dual function and lower bound g(  )    
p   , for a problem with one (inequality) constraint. given   , we minimize
(  , 1)t (u, t) over a = {(u, t) |    x     d, f0(x)     t, f1(x)     u}. this yields
a supporting hyperplane with slope      . the intersection of this hyperplane
with the u = 0 axis gives g(  ).

we can think of a as a sort of epigraph form of g, since a includes all the points in
g, as well as points that are    worse   , i.e., those with larger objective or inequality
constraint function values.

we can express the optimal value in terms of a as
p    = inf{t | (0, 0, t)     a}.

to evaluate the dual function at a point (  ,   ) with    (cid:23) 0, we can minimize the
a   ne function (  ,   , 1)t (u, v, t) over a: if    (cid:23) 0, then

g(  ,   ) = inf{(  ,   , 1)t (u, v, t) | (u, v, t)     a}.

if the in   mum is    nite, then

(  ,   , 1)t (u, v, t)     g(  ,   )

de   nes a nonvertical supporting hyperplane to a.
in particular, since (0, 0, p   )     bda, we have

p    = (  ,   , 1)t (0, 0, p   )     g(  ,   ),

(5.38)

the weak duality lower bound. strong duality holds if and only if we have equality
in (5.38) for some dual feasible (  ,   ), i.e., there exists a nonvertical supporting
hyperplane to a at its boundary point (0, 0, p   ).

this second interpretation is illustrated in    gure 5.5.

5.3.2 proof of strong duality under constraint quali   cation

in this section we prove that slater   s constraint quali   cation guarantees strong
duality (and that the dual optimum is attained) for a convex problem. we consider

5.3 geometric interpretation

235

the primal problem (5.25), with f0, . . . , fm convex, and assume slater   s condition
holds: there exists   x     relintd with fi(  x) < 0, i = 1, . . . , m, and a  x = b. in
order to simplify the proof, we make two additional assumptions:    rst that d has
nonempty interior (hence, relintd = intd) and second, that rank a = p. we
assume that p    is    nite. (since there is a feasible point, we can only have p    =       
or p       nite; if p    =       , then d    =        by weak duality.)
the set a de   ned in (5.37) is readily shown to be convex if the underlying
problem is convex. we de   ne a second convex set b as

b = {(0, 0, s)     rm    rp    r | s < p   }.

the sets a and b do not intersect. to see this, suppose (u, v, t)     a     b. since
(u, v, t)     b we have u = 0, v = 0, and t < p   . since (u, v, t)     a, there exists an x
with fi(x)     0, i = 1, . . . , m, ax     b = 0, and f0(x)     t < p   , which is impossible
since p    is the optimal value of the primal problem.
by the separating hyperplane theorem of   2.5.1 there exists (    ,     ,   ) 6= 0 and   
(5.39)

(u, v, t)     a =        t u +     t v +   t       ,

such that

and

(u, v, t)     b =        t u +     t v +   t       .

(5.40)
from (5.39) we conclude that      (cid:23) 0 and        0. (otherwise     t u +   t is unbounded
below over a, contradicting (5.39).) the condition (5.40) simply means that   t       
for all t < p   , and hence,   p          . together with (5.39) we conclude that for any
x     d,

    ifi(x) +     t (ax     b) +   f0(x)              p   .

(5.41)

mxi=1

assume that    > 0. in that case we can divide (5.41) by    to obtain

l(x,     /  ,     /  )     p   

for all x     d, from which it follows, by minimizing over x, that g(  ,   )     p   , where
we de   ne

   =     /  ,

   =     /  .

by weak duality we have g(  ,   )     p   , so in fact g(  ,   ) = p   . this shows that
strong duality holds, and that the dual optimum is attained, at least in the case
when    > 0.

now consider the case    = 0. from (5.41), we conclude that for all x     d,

mxi=1

    ifi(x) +     t (ax     b)     0.

(5.42)

applying this to the point   x that satis   es the slater condition, we have

mxi=1

    ifi(  x)     0.

236

5 duality

t

(  u,   t)

b

a

u

figure 5.6 illustration of strong duality proof, for a convex problem that sat-
is   es slater   s constraint quali   cation. the set a is shown shaded, and the
set b is the thick vertical line segment, not including the point (0, p   ), shown
as a small open circle. the two sets are convex and do not intersect, so they
can be separated by a hyperplane. slater   s constraint quali   cation guaran-
tees that any separating hyperplane must be nonvertical, since it must pass
to the left of the point (  u,   t) = (f1(  x), f0(  x)), where   x is strictly feasible.

since fi(  x) < 0 and     i     0, we conclude that      = 0. from (    ,     ,   ) 6= 0 and
     = 0,    = 0, we conclude that      6= 0. then (5.42) implies that for all x     d,
    t (ax     b)     0. but   x satis   es     t (a  x     b) = 0, and since   x     intd, there are
points in d with     t (ax     b) < 0 unless at      = 0. this, of course, contradicts our
assumption that rank a = p.

the geometric idea behind the proof is illustrated in    gure 5.6, for a simple
problem with one inequality constraint. the hyperplane separating a and b de   nes
a supporting hyperplane to a at (0, p   ). slater   s constraint quali   cation is used
to establish that the hyperplane must be nonvertical (i.e., has a normal vector of
the form (     , 1)). (for a simple example of a convex problem with one inequality
constraint for which strong duality fails, see exercise 5.21.)

5.3.3 multicriterion interpretation

there is a natural connection between lagrange duality for a problem without
equality constraints,

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m,

(5.43)

5.4 saddle-point interpretation

237

and the scalarization method for the (unconstrained) multicriterion problem

minimize (w.r.t. rm+1

+ ) f (x) = (f1(x), . . . , fm(x), f0(x))

(5.44)

(see   4.7.4). in scalarization, we choose a positive vector     , and minimize the scalar
function     t f (x); any minimizer is guaranteed to be pareto optimal. since we can
scale      by a positive constant, without a   ecting the minimizers, we can, without
loss of generality, take      = (  , 1). thus, in scalarization we minimize the function

    t f (x) = f0(x) +

  ifi(x),

mxi=1

which is exactly the lagrangian for the problem (5.43).

to establish that every pareto optimal point of a convex multicriterion problem
minimizes the function     t f (x) for some nonnegative weight vector     , we considered
the set a, de   ned in (4.62),

a = {t     rm+1 |    x     d, fi(x)     ti, i = 0, . . . , m},

which is exactly the same as the set a de   ned in (5.37), that arises in lagrange dual-
ity. here too we constructed the required weight vector as a supporting hyperplane
to the set, at an arbitrary pareto optimal point. in multicriterion optimization,
we interpret the components of the weight vector as giving the relative weights
between the objective functions. when we    x the last component of the weight
vector (associated with f0) to be one, the other weights have the interpretation of
the cost relative to f0, i.e., the cost relative to the objective.

5.4 saddle-point interpretation

in this section we give several interpretations of lagrange duality. the material of
this section will not be used in the sequel.

5.4.1 max-min characterization of weak and strong duality

it is possible to express the primal and the dual optimization problems in a form
that is more symmetric. to simplify the discussion we assume there are no equality
constraints; the results are easily extended to cover them.

first note that

l(x,   ) = sup

sup
  (cid:23)0

  ifi(x)!

mxi=1

  (cid:23)0 f0(x) +
= (cid:26) f0(x)

   

fi(x)     0,
otherwise.

i = 1, . . . , m

238

5 duality

indeed, suppose x is not feasible, and fi(x) > 0 for some i. then sup  (cid:23)0 l(x,   ) =
   , as can be seen by choosing   j = 0, j 6= i, and   i        . on the other
hand, if fi(x)     0, i = 1, . . . , m, then the optimal choice of    is    = 0 and
sup  (cid:23)0 l(x,   ) = f0(x). this means that we can express the optimal value of the
primal problem as

p    = inf
x

sup
  (cid:23)0

l(x,   ).

by the de   nition of the dual function, we also have

d    = sup
  (cid:23)0

inf
x

l(x,   ).

thus, weak duality can be expressed as the inequality

sup
  (cid:23)0

inf
x

l(x,   )     inf

x

sup
  (cid:23)0

l(x,   ),

(5.45)

and strong duality as the equality

sup
  (cid:23)0

inf
x

l(x,   ) = inf
x

sup
  (cid:23)0

l(x,   ).

strong duality means that the order of the minimization over x and the maximiza-
tion over    (cid:23) 0 can be switched without a   ecting the result.

in fact, the inequality (5.45) does not depend on any properties of l: we have

sup
z   z

inf
w   w

f (w, z)     inf

w   w

sup
z   z

f (w, z)

(5.46)

for any f : rn  rm     r (and any w     rn and z     rm). this general inequality
is called the max-min inequality. when equality holds, i.e.,

sup
z   z

inf
w   w

f (w, z) = inf
w   w

sup
z   z

f (w, z)

(5.47)

we say that f (and w and z) satisfy the strong max-min property or the saddle-
point property. of course the strong max-min property holds only in special cases,
for example, when f : rn    rm     r is the lagrangian of a problem for which
strong duality obtains, w = rn, and z = rm
+ .

5.4.2 saddle-point interpretation

we refer to a pair   w     w ,   z     z as a saddle-point for f (and w and z) if

f (   w, z)     f (   w,   z)     f (w,   z)

for all w     w and z     z. in other words,   w minimizes f (w,   z) (over w     w ) and
  z maximizes f (   w, z) (over z     z):

f (   w,   z) = inf
w   w

f (w,   z),

f (   w,   z) = sup
z   z

f (   w, z).

5.4 saddle-point interpretation

239

this implies that the strong max-min property (5.47) holds, and that the common
value is f (   w,   z).

returning to our discussion of lagrange duality, we see that if x    and       are
primal and dual optimal points for a problem in which strong duality obtains, they
form a saddle-point for the lagrangian. the converse is also true: if (x,   ) is a
saddle-point of the lagrangian, then x is primal optimal,    is dual optimal, and
the optimal duality gap is zero.

5.4.3 game interpretation

we can interpret the max-min inequality (5.46), the max-min equality (5.47), and
the saddle-point property, in terms of a continuous zero-sum game.
if the    rst
player chooses w     w , and the second player selects z     z, then player 1 pays an
amount f (w, z) to player 2. player 1 therefore wants to minimize f , while player 2
wants to maximize f . (the game is called continuous since the choices are vectors,
and not discrete.)

suppose that player 1 makes his choice    rst, and then player 2, after learning
the choice of player 1, makes her selection. player 2 wants to maximize the payo   
f (w, z), and so will choose z     z to maximize f (w, z). the resulting payo    will
be supz   z f (w, z), which depends on w, the choice of the    rst player. (we assume
here that the supremum is achieved; if not the optimal payo    can be arbitrarily
close to supz   z f (w, z).) player 1 knows (or assumes) that player 2 will follow this
strategy, and so will choose w     w to make this worst-case payo    to player 2 as
small as possible. thus player 1 chooses

which results in the payo   

from player 1 to player 2.

argmin

w   w

sup
z   z

f (w, z),

inf
w   w

sup
z   z

f (w, z)

now suppose the order of play is reversed: player 2 must choose z     z    rst, and
then player 1 chooses w     w (with knowledge of z). following a similar argument,
if the players follow the optimal strategy, player 2 should choose z     z to maximize
inf w   w f (w, z), which results in the payo    of

sup
z   z

inf
w   w

f (w, z)

from player 1 to player 2.

the max-min inequality (5.46) states the (intuitively obvious) fact that it is
better for a player to go second, or more precisely, for a player to know his or her
opponent   s choice before choosing. in other words, the payo    to player 2 will be
larger if player 1 must choose    rst. when the saddle-point property (5.47) holds,
there is no advantage to playing second.

if (   w,   z) is a saddle-point for f (and w and z), then it is called a solution of
the game;   w is called the optimal choice or strategy for player 1, and   z is called

240

5 duality

the optimal choice or strategy for player 2. in this case there is no advantage to
playing second.

now consider the special case where the payo    function is the lagrangian,
w = rn and z = rm
+ . here player 1 chooses the primal variable x, while player 2
chooses the dual variable    (cid:23) 0. by the argument above, the optimal choice for
player 2, if she must choose    rst, is any       which is dual optimal, which results
in a payo    to player 2 of d   . conversely, if player 1 must choose    rst, his optimal
choice is any primal optimal x   , which results in a payo    of p   .

the optimal duality gap for the problem is exactly equal to the advantage
a   orded the player who goes second, i.e., the player who has the advantage of
knowing his or her opponent   s choice before choosing. if strong duality holds, then
there is no advantage to the players of knowing their opponent   s choice.

5.4.4 price or tax interpretation

lagrange duality has an interesting economic interpretation. suppose the variable
x denotes how an enterprise operates and f0(x) denotes the cost of operating at
x, i.e.,    f0(x) is the pro   t (say, in dollars) made at the operating condition x.
each constraint fi(x)     0 represents some limit, such as a limit on resources (e.g.,
warehouse space, labor) or a regulatory limit (e.g., environmental). the operating
condition that maximizes pro   t while respecting the limits can be found by solving
the problem

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m.

the resulting optimal pro   t is    p   .
now imagine a second scenario in which the limits can be violated, by paying an
additional cost which is linear in the amount of violation, measured by fi. thus the
payment made by the enterprise for the ith limit or constraint is   ifi(x). payments
are also made to the    rm for constraints that are not tight; if fi(x) < 0, then   ifi(x)
represents a payment to the    rm. the coe   cient   i has the interpretation of the
price for violating fi(x)     0; its units are dollars per unit violation (as measured
by fi). for the same price the enterprise can sell any    unused    portion of the ith
constraint. we assume   i     0, i.e., the    rm must pay for violations (and receives
income if a constraint is not tight).
as an example, suppose the    rst constraint in the original problem, f1(x)    
in this new
0, represents a limit on warehouse space (say, in square meters).
arrangement, we open the possibility that the    rm can rent extra warehouse space
at a cost of   1 dollars per square meter and also rent out unused space, at the same
rate.

  i, is l(x,   ) = f0(x) +pm

the total cost to the    rm, for operating condition x, and constraint prices
i=1   ifi(x). the    rm will obviously operate so as to
minimize its total cost l(x,   ), which yields a cost g(  ). the dual function therefore
represents the optimal cost to the    rm, as a function of the constraint price vector
  . the optimal dual value, d   , is the optimal cost to the enterprise under the least
favorable set of prices.

5.5 optimality conditions

241

using this interpretation we can paraphrase weak duality as follows: the opti-
mal cost to the    rm in the second scenario (in which constraint violations can be
bought and sold) is less than or equal to the cost in the original situation (which
has constraints that cannot be violated), even with the most unfavorable prices.
this is obvious: if x    is optimal in the    rst scenario, then the operating cost of x   
in the second scenario will be lower than f0(x   ), since some income can be derived
from the constraints that are not tight. the optimal duality gap is then the min-
imum possible advantage to the enterprise of being allowed to pay for constraint
violations (and receive payments for nontight constraints).

now suppose strong duality holds, and the dual optimum is attained. we can
interpret a dual optimal       as a set of prices for which there is no advantage to
the    rm in being allowed to pay for constraint violations (or receive payments for
nontight constraints). for this reason a dual optimal       is sometimes called a set
of shadow prices for the original problem.

5.5 optimality conditions

we remind the reader that we do not assume the problem (5.1) is convex, unless
explicitly stated.

5.5.1 certi   cate of suboptimality and stopping criteria

if we can    nd a dual feasible (  ,   ), we establish a lower bound on the optimal value
of the primal problem: p        g(  ,   ). thus a dual feasible point (  ,   ) provides a
proof or certi   cate that p        g(  ,   ). strong duality means there exist arbitrarily
good certi   cates.
dual feasible points allow us to bound how suboptimal a given feasible point
is, without knowing the exact value of p   . indeed, if x is primal feasible and (  ,   )
is dual feasible, then

f0(x)     p        f0(x)     g(  ,   ).

in particular, this establishes that x is   -suboptimal, with    = f0(x)     g(  ,   ). (it
also establishes that (  ,   ) is   -suboptimal for the dual problem.)

we refer to the gap between primal and dual objectives,

f0(x)     g(  ,   ),

as the duality gap associated with the primal feasible point x and dual feasible
point (  ,   ). a primal dual feasible pair x, (  ,   ) localizes the optimal value of the
primal (and dual) problems to an interval:

p        [g(  ,   ), f0(x)],
the width of which is the duality gap.

d        [g(  ,   ), f0(x)],

if the duality gap of the primal dual feasible pair x, (  ,   ) is zero, i.e., f0(x) =
g(  ,   ), then x is primal optimal and (  ,   ) is dual optimal. we can think of (  ,   )

242

5 duality

as a certi   cate that proves x is optimal (and, similarly, we can think of x as a
certi   cate that proves (  ,   ) is dual optimal).

these observations can be used in optimization algorithms to provide nonheuris-
tic stopping criteria. suppose an algorithm produces a sequence of primal feasible
x(k) and dual feasible (  (k),   (k)), for k = 1, 2, . . ., and   abs > 0 is a given required
absolute accuracy. then the stopping criterion (i.e., the condition for terminating
the algorithm)

f0(x(k))     g(  (k),   (k))       abs

guarantees that when the algorithm terminates, x(k) is   abs-suboptimal. indeed,
(  (k),   (k)) is a certi   cate that proves it. (of course strong duality must hold if
this method is to work for arbitrarily small tolerances   abs.)

a similar condition can be used to guarantee a given relative accuracy   rel > 0.

if

holds, or

g(  (k),   (k)) > 0,

f0(x(k))     g(  (k),   (k))

g(  (k),   (k))

      rel

f0(x(k)) < 0,

f0(x(k))     g(  (k),   (k))

   f0(x(k))

      rel

holds, then p    6= 0 and the relative error

f0(x(k))     p   

|p   |

is guaranteed to be less than or equal to   rel.

5.5.2 complementary slackness

suppose that the primal and dual optimal values are attained and equal (so, in
particular, strong duality holds). let x    be a primal optimal and (     ,      ) be a dual
optimal point. this means that

f0(x   ) = g(     ,      )

= inf

x  f0(x) +
mxi=1
    f0(x   ) +
    f0(x   ).

i hi(x)!

     

     
i fi(x) +

mxi=1

pxi=1
pxi=1

     
i fi(x   ) +

     
i hi(x   )

the    rst line states that the optimal duality gap is zero, and the second line is
the de   nition of the dual function. the third line follows since the in   mum of the
lagrangian over x is less than or equal to its value at x = x   . the last inequality
follows from      
i     0, fi(x   )     0, i = 1, . . . , m, and hi(x   ) = 0, i = 1, . . . , p. we
conclude that the two inequalities in this chain hold with equality.

5.5 optimality conditions

243

we can draw several interesting conclusions from this. for example, since the
inequality in the third line is an equality, we conclude that x    minimizes l(x,      ,      )
over x. (the lagrangian l(x,      ,      ) can have other minimizers; x    is simply a
minimizer.)

another important conclusion is that

     
i fi(x   ) = 0.

mxi=1

since each term in this sum is nonpositive, we conclude that

     
i fi(x   ) = 0,

i = 1, . . . , m.

(5.48)

this condition is known as complementary slackness; it holds for any primal opti-
mal x    and any dual optimal (     ,      ) (when strong duality holds). we can express
the complementary slackness condition as

or, equivalently,

     
i > 0 =    fi(x   ) = 0,

roughly speaking, this means the ith optimal lagrange multiplier is zero unless
the ith constraint is active at the optimum.

fi(x   ) < 0 =         

i = 0.

5.5.3 kkt optimality conditions

we now assume that the functions f0, . . . , fm, h1, . . . , hp are di   erentiable (and
therefore have open domains), but we make no assumptions yet about convexity.

kkt conditions for nonconvex problems

as above, let x    and (     ,      ) be any primal and dual optimal points with zero
duality gap. since x    minimizes l(x,      ,      ) over x, it follows that its gradient
must vanish at x   , i.e.,

   f0(x   ) +

mxi=1

     
i    fi(x   ) +

pxi=1

     
i    hi(x   ) = 0.

thus we have

   f0(x   ) +pm

i=1      

i    fi(x   ) +pp

i=1      

which are called the karush-kuhn-tucker (kkt) conditions.

fi(x   )     0,
hi(x   ) = 0,
     
i     0,
i fi(x   ) = 0,
     
i    hi(x   ) = 0,

i = 1, . . . , m
i = 1, . . . , p
i = 1, . . . , m
i = 1, . . . , m

(5.49)

to summarize, for any optimization problem with di   erentiable objective and
constraint functions for which strong duality obtains, any pair of primal and dual
optimal points must satisfy the kkt conditions (5.49).

244

5 duality

kkt conditions for convex problems

when the primal problem is convex, the kkt conditions are also su   cient for the
points to be primal and dual optimal. in other words, if fi are convex and hi are
a   ne, and   x,     ,      are any points that satisfy the kkt conditions

fi(  x)     0,
hi(  x) = 0,
    i     0,
    ifi(  x) = 0,
i=1     i   hi(  x) = 0,

i = 1, . . . , m
i = 1, . . . , p
i = 1, . . . , m
i = 1, . . . , m

   f0(  x) +pm

i=1

    i   fi(  x) +pp

then   x and (    ,     ) are primal and dual optimal, with zero duality gap.

to see this, note that the    rst two conditions state that   x is primal feasible.
since     i     0, l(x,     ,     ) is convex in x; the last kkt condition states that its
gradient with respect to x vanishes at x =   x, so it follows that   x minimizes l(x,     ,     )
over x. from this we conclude that

g(    ,     ) = l(  x,     ,     )

mxi=1

= f0(  x) +

= f0(  x),

    ifi(  x) +

    ihi(  x)

pxi=1

where in the last line we use hi(  x) = 0 and     ifi(  x) = 0. this shows that   x
and (    ,     ) have zero duality gap, and therefore are primal and dual optimal. in
summary, for any id76 problem with di   erentiable objective and
constraint functions, any points that satisfy the kkt conditions are primal and
dual optimal, and have zero duality gap.

if a id76 problem with di   erentiable objective and constraint
functions satis   es slater   s condition, then the kkt conditions provide necessary
and su   cient conditions for optimality: slater   s condition implies that the optimal
duality gap is zero and the dual optimum is attained, so x is optimal if and only if
there are (  ,   ) that, together with x, satisfy the kkt conditions.

the kkt conditions play an important role in optimization. in a few special
cases it is possible to solve the kkt conditions (and therefore, the optimization
problem) analytically. more generally, many algorithms for id76 are
conceived as, or can be interpreted as, methods for solving the kkt conditions.

example 5.1 equality constrained convex quadratic minimization. we consider the
problem

minimize
subject to ax = b,

(1/2)xt p x + qt x + r

(5.50)

where p     sn

+. the kkt conditions for this problem are

ax    = b,

p x    + q + at       = 0,

which we can write as

(cid:20) p at

0 (cid:21)(cid:20) x   

b (cid:21) .
      (cid:21) =(cid:20)    q

a

5.5 optimality conditions

245

solving this set of m + n equations in the m + n variables x   ,       gives the optimal
primal and dual variables for (5.50).

example 5.2 water-   lling. we consider the id76 problem

minimize    pn

subject to x (cid:23) 0,

i=1 log(  i + xi)

1t x = 1,

where   i > 0. this problem arises in id205, in allocating power to a
set of n communication channels. the variable xi represents the transmitter power
allocated to the ith channel, and log(  i + xi) gives the capacity or communication
rate of the channel, so the problem is to allocate a total power of one to the channels,
in order to maximize the total communication rate.
introducing lagrange multipliers           rn for the inequality constraints x    (cid:23) 0,
and a multiplier           r for the equality constraint 1t x = 1, we obtain the kkt
conditions

x    (cid:23) 0,

1t x    = 1,

   1/(  i + x   

i )          

      (cid:23) 0,
i +       = 0,

     
i x   

i = 0,

i = 1, . . . , n,

i = 1, . . . , n.

we can directly solve these equations to    nd x   ,      , and      . we start by noting that
      acts as a slack variable in the last equation, so it can be eliminated, leaving

x    (cid:23) 0,

1t x    = 1,

x   
i (          1/(  i + x   

i )) = 0,

i = 1, . . . , n,

          1/(  i + x   
i ),

i = 1, . . . , n.

if       < 1/  i, this last condition can only hold if x   
implies that       = 1/(  i + x   
if       < 1/  i.
          1/  i > 1/(  i + x   
therefore, x   

i > 0, which by the third condition
i = 1/            i
i > 0 is impossible, because it would imply
i ), which violates the complementary slackness condition.

if           1/  i, then x   

i , we conclude that x   

i ). solving for x   

i = 0 if           1/  i. thus we have

x   

i =(cid:26) 1/            i

0

      < 1/  i
          1/  i,

or, put more simply, x   
into the condition 1t x    = 1 we obtain

i = max{0, 1/            i}. substituting this expression for x   

i

nxi=1

max{0, 1/            i} = 1.

the lefthand side is a piecewise-linear increasing function of 1/     , with breakpoints
at   i, so the equation has a unique solution which is readily determined.

this solution method is called water-   lling for the following reason. we think of
  i as the ground level above patch i, and then    ood the region with water to a
depth 1/  , as illustrated in    gure 5.7. the total amount of water used is then
i=1 max{0, 1/            i}. we then increase the    ood level until we have used a total
amount of water equal to one. the depth of water above patch i is then the optimal
value x   
i .

pn

246

5 duality

1/     

xi

  i

i

figure 5.7 illustration of water-   lling algorithm. the height of each patch is
given by   i. the region is    ooded to a level 1/      which uses a total quantity
of water equal to one. the height of the water (shown shaded) above each
patch is the optimal value of x   
i .

w

w

x1

x2

l

figure 5.8 two blocks connected by springs to each other, and the left and
right walls. the blocks have width w > 0, and cannot penetrate each other
or the walls.

5.5.4 mechanics interpretation of kkt conditions

the kkt conditions can be given a nice interpretation in mechanics (which indeed,
was one of lagrange   s primary motivations). we illustrate the idea with a simple
example. the system shown in    gure 5.8 consists of two blocks attached to each
other, and to walls at the left and right, by three springs. the position of the
blocks are given by x     r2, where x1 is the displacement of the (middle of the) left
block, and x2 is the displacement of the right block. the left wall is at position 0,
and the right wall is at position l.

the potential energy in the springs, as a function of the block positions, is given

by

f0(x1, x2) =

1
2

k1x2

1 +

1
2

k2(x2     x1)2 +

1
2

k3(l     x2)2,

where ki > 0 are the sti   ness constants of the three springs. the equilibrium
position x    is the position that minimizes the potential energy subject to the in-
equalities

w/2     x1     0,

w + x1     x2     0,

w/2     l + x2     0.

(5.51)

5.5 optimality conditions

247

  1
k1x1

  2
k2(x2     x1)

  2
k2(x2     x1)

  3
k3(l     x2)

figure 5.9 force analysis of the block-spring system. the total force on
each block, due to the springs and also to contact forces, must be zero. the
lagrange multipliers, shown on top, are the contact forces between the walls
and blocks. the spring forces are shown at bottom.

these constraints are called kinematic constraints, and express the fact that the
blocks have width w > 0, and cannot penetrate each other or the walls. the
equilibrium position is therefore given by the solution of the optimization problem

minimize
subject to w/2     x1     0

(1/2)(cid:0)k1x2

1 + k2(x2     x1)2 + k3(l     x2)2(cid:1)

w + x1     x2     0
w/2     l + x2     0,

(5.52)

which is a qp.

with   1,   2,   3 as lagrange multipliers, the kkt conditions for this problem
consist of the kinematic constraints (5.51), the nonnegativity constraints   i     0,
the complementary slackness conditions

  1(w/2     x1) = 0,

  2(w     x2 + x1) = 0,

  3(w/2     l + x2) = 0,

(5.53)

and the zero gradient condition

(cid:20)

k1x1     k2(x2     x1)

k2(x2     x1)     k3(l     x2) (cid:21) +   1(cid:20)    1

0 (cid:21) +   2(cid:20) 1

   1 (cid:21) +   3(cid:20) 0

1 (cid:21) = 0.

(5.54)

the equation (5.54) can be interpreted as the force balance equations for the two
blocks, provided we interpret the lagrange multipliers as contact forces that act
between the walls and blocks, as illustrated in    gure 5.9. the    rst equation states
that the sum of the forces on the    rst block is zero: the term    k1x1 is the force
exerted on the left block by the left spring, the term k2(x2    x1) is the force exerted
by the middle spring,   1 is the force exerted by the left wall, and      2 is the force
exerted by the right block. the contact forces must point away from the contact
surface (as expressed by the constraints   1     0 and      2     0), and are nonzero
only when there is contact (as expressed by the    rst two complementary slackness
conditions (5.53)).
in a similar way, the second equation in (5.54) is the force
balance for the second block, and the last condition in (5.53) states that   3 is zero
unless the right block touches the wall.

in this example, the potential energy and kinematic constraint functions are
convex, and (the re   ned form of) slater   s constraint quali   cation holds provided
2w     l, i.e., there is enough room between the walls to    t the two blocks, so we
can conclude that the energy formulation of the equilibrium given by (5.52), gives
the same result as the force balance formulation, given by the kkt conditions.

248

5 duality

5.5.5 solving the primal problem via the dual

we mentioned at the beginning of   5.5.3 that if strong duality holds and a dual
optimal solution (     ,      ) exists, then any primal optimal point is also a minimizer
of l(x,      ,      ). this fact sometimes allows us to compute a primal optimal solution
from a dual optimal solution.

more precisely, suppose we have strong duality and an optimal (     ,      ) is known.

suppose that the minimizer of l(x,      ,      ), i.e., the solution of

minimize

f0(x) +pm

i=1      

i fi(x) +pp

i=1      

i hi(x),

(5.55)

is unique. (for a convex problem this occurs, for example, if l(x,      ,      ) is a strictly
convex function of x.) then if the solution of (5.55) is primal feasible, it must be
primal optimal; if it is not primal feasible, then no primal optimal point can exist,
i.e., we can conclude that the primal optimum is not attained. this observation is
interesting when the dual problem is easier to solve than the primal problem, for
example, because it can be solved analytically, or has some special structure that
can be exploited.

example 5.3 id178 maximization. we consider the id178 maximization problem

minimize
subject to ax (cid:22) b
1t x = 1

f0(x) =pn

i=1 xi log xi

with domain rn

++, and its dual problem

maximize    bt               e        1pn

subject to    (cid:23) 0

i=1 e   at
i   

where ai are the columns of a (see pages 222 and 228). we assume that the weak
form of slater   s condition holds, i.e., there exists an x     0 with ax (cid:22) b and 1t x = 1,
so strong duality holds and an optimal solution (     ,      ) exists.
suppose we have solved the dual problem. the lagrangian at (     ,      ) is

l(x,      ,      ) =

xi log xi +      t (ax     b) +      (1t x     1)

nxi=1

which is strictly convex on d and bounded below, so it has a unique solution x   ,
given by

x   
i = 1/ exp(at

i       +       + 1),

i = 1, . . . , n.

if x    is primal feasible, it must be the optimal solution of the primal problem (5.13).
if x    is not primal feasible, then we can conclude that the primal optimum is not
attained.

example 5.4 minimizing a separable function subject to an equality constraint. we
consider the problem

minimize
subject to

f0(x) =pn

at x = b,

i=1 fi(xi)

5.6 perturbation and sensitivity analysis

249

where a     rn, b     r, and fi : r     r are di   erentiable and strictly convex. the
objective function is called separable since it is a sum of functions of the individual
variables x1, . . . , xn. we assume that the domain of f0 intersects the constraint set,
i.e., there exists a point x0     dom f0 with at x0 = b. this implies the problem has
a unique optimal point x   .

the lagrangian is

l(x,   ) =

nxi=1

fi(xi) +   (at x     b) =    b   +

(fi(xi) +   aixi),

nxi=1

which is also separable, so the dual function is

g(  ) =    b   + inf

=    b   +

=    b      

the dual problem is thus

(fi(xi) +   aixi)!

(fi(xi) +   aixi)

x   nxi=1
nxi=1
nxi=1

inf
xi

f    
i (     ai).

maximize    b      pn

i=1 f    

i (     ai),

with (scalar) variable        r.
now suppose we have found an optimal dual variable      . (there are several simple
methods for solving a convex problem with one scalar variable, such as the bisection
method.) since each fi is strictly convex, the function l(x,      ) is strictly convex in
x, and so has a unique minimizer   x. but we also know that x    minimizes l(x,      ),
so we must have   x = x   . we can recover x    from    xl(x,      ) = 0, i.e., by solving the
equations f    

i (x   

i ) =         ai.

5.6 perturbation and sensitivity analysis

when strong duality obtains, the optimal dual variables give very useful informa-
tion about the sensitivity of the optimal value with respect to perturbations of the
constraints.

5.6.1 the perturbed problem

we consider the following perturbed version of the original optimization prob-
lem (5.1):

minimize
subject to

f0(x)
fi(x)     ui,
hi(x) = vi,

i = 1, . . . , m
i = 1, . . . , p,

(5.56)

250

5 duality

with variable x     rn. this problem coincides with the original problem (5.1) when
u = 0, v = 0. when ui is positive it means that we have relaxed the ith inequality
constraint; when ui is negative, it means that we have tightened the constraint.
thus the perturbed problem (5.56) results from the original problem (5.1) by tight-
ening or relaxing each inequality constraint by ui, and changing the righthand side
of the equality constraints by vi.

we de   ne p   (u, v) as the optimal value of the perturbed problem (5.56):

p   (u, v) = inf{f0(x) |    x     d, fi(x)     ui, i = 1, . . . , m,

hi(x) = vi, i = 1, . . . , p}.

we can have p   (u, v) =    , which corresponds to perturbations of the constraints
that result in infeasibility. note that p   (0, 0) = p   , the optimal value of the un-
perturbed problem (5.1).
(we hope this slight abuse of notation will cause no
confusion.) roughly speaking, the function p    : rm    rp     r gives the optimal
value of the problem as a function of perturbations to the righthand sides of the
constraints.

when the original problem is convex, the function p    is a convex function of u
and v; indeed, its epigraph is precisely the closure of the set a de   ned in (5.37)
(see exercise 5.32).

5.6.2 a global inequality

now we assume that strong duality holds, and that the dual optimum is attained.
(this is the case if the original problem is convex, and slater   s condition is satis   ed).
let (     ,      ) be optimal for the dual (5.16) of the unperturbed problem. then for
all u and v we have

p   (u, v)     p   (0, 0)          t u          t v.

(5.57)

to establish this inequality, suppose that x is any feasible point for the per-
turbed problem, i.e., fi(x)     ui for i = 1, . . . , m, and hi(x) = vi for i = 1, . . . , p.
then we have, by strong duality,

p   (0, 0) = g(     ,      )     f0(x) +

     
i fi(x) +

mxi=1

     
i hi(x)

pxi=1

    f0(x) +      t u +      t v.

(the    rst inequality follows from the de   nition of g(     ,      ); the second follows
since       (cid:23) 0.) we conclude that for any x feasible for the perturbed problem, we
have

from which (5.57) follows.

f0(x)     p   (0, 0)          t u          t v,

sensitivity interpretations

when strong duality holds, various sensitivity interpretations of the optimal la-
grange variables follow directly from the inequality (5.57). some of the conclusions
are:

5.6 perturbation and sensitivity analysis

251

u = 0

u
p   (u)

p   (0)          u

figure 5.10 optimal value p   (u) of a convex problem with one constraint
f1(x)     u, as a function of u. for u = 0, we have the original unperturbed
problem; for u < 0 the constraint is tightened, and for u > 0 the constraint
is loosened. the a   ne function p   (0)          u is a lower bound on p   .

    if      

i is large and we tighten the ith constraint (i.e., choose ui < 0), then the

optimal value p   (u, v) is guaranteed to increase greatly.

i

    if      

is large and positive and we take vi < 0, or if      
i

is large and negative
and we take vi > 0, then the optimal value p   (u, v) is guaranteed to increase
greatly.

    if      

i

    if      

i

is small, and we loosen the ith constraint (ui > 0), then the optimal

value p   (u, v) will not decrease too much.

is small and positive, and vi > 0, or if      
i

is small and negative and

vi < 0, then the optimal value p   (u, v) will not decrease too much.

the inequality (5.57), and the conclusions listed above, give a lower bound on
the perturbed optimal value, but no upper bound. for this reason the results are
not symmetric with respect to loosening or tightening a constraint. for example,
suppose that      
i is large, and we loosen the ith constraint a bit (i.e., take ui small
and positive).
in this case the inequality (5.57) is not useful; it does not, for
example, imply that the optimal value will decrease considerably.

the inequality (5.57) is illustrated in    gure 5.10 for a convex problem with one
inequality constraint. the inequality states that the a   ne function p   (0)          u is
a lower bound on the convex function p   .

5.6.3 local sensitivity analysis

suppose now that p   (u, v) is di   erentiable at u = 0, v = 0. then, provided strong
duality holds, the optimal dual variables      ,       are related to the gradient of p    at

252

5 duality

u = 0, v = 0:

     
i =    

   p   (0, 0)

   ui

,

     
i =    

   p   (0, 0)

   vi

.

(5.58)

this property can be seen in the example shown in    gure 5.10, where          is the
slope of p    near u = 0.
thus, when p   (u, v) is di   erentiable at u = 0, v = 0, and strong duality holds,
the optimal lagrange multipliers are exactly the local sensitivities of the optimal
value with respect to constraint perturbations. in contrast to the nondi   erentiable
case, this interpretation is symmetric: tightening the ith inequality constraint
a small amount (i.e., taking ui small and negative) yields an increase in p    of
approximately         
i ui; loosening the ith constraint a small amount (i.e., taking ui
small and positive) yields a decrease in p    of approximately      

i ui.

to show (5.58), suppose p   (u, v) is di   erentiable and strong duality holds. for

the perturbation u = tei, v = 0, where ei is the ith unit vector, we have

p   (tei, 0)     p   

t

lim
t   0

=

   p   (0, 0)

   ui

.

the inequality (5.57) states that for t > 0,

p   (tei, 0)     p   

t

            
i ,

while for t < 0 we have the opposite inequality. taking the limit t     0, with t > 0,
yields

   p   (0, 0)

   ui

            
i ,

while taking the limit with t < 0 yields the opposite inequality, so we conclude that

   p   (0, 0)

   ui

=         
i .

the same method can be used to establish

   p   (0, 0)

   vi

=         
i .

the local sensitivity result (5.58) gives us a quantitative measure of how active
a constraint is at the optimum x   . if fi(x   ) < 0, then the constraint is inactive,
and it follows that the constraint can be tightened or loosened a small amount
without a   ecting the optimal value. by complementary slackness, the associated
optimal lagrange multiplier must be zero. but now suppose that fi(x   ) = 0, i.e.,
the ith constraint is active at the optimum. the ith optimal lagrange multiplier
tells us how active the constraint is: if      
is small, it means that the constraint
i
can be loosened or tightened a bit without much e   ect on the optimal value; if      
i
is large, it means that if the constraint is loosened or tightened a bit, the e   ect on
the optimal value will be great.

5.7 examples

253

shadow price interpretation

we can also give a simple geometric interpretation of the result (5.58) in terms
of economics. we consider (for simplicity) a convex problem with no equality
constraints, which satis   es slater   s condition. the variable x     rm determines
how a    rm operates, and the objective f0 is the cost, i.e.,    f0 is the pro   t. each
constraint fi(x)     0 represents a limit on some resource such as labor, steel, or
warehouse space. the (negative) perturbed optimal cost function    p   (u) tells us
how much more or less pro   t could be made if more, or less, of each resource were
made available to the    rm. if it is di   erentiable near u = 0, then we have

     
i =    

   p   (0)

   ui

.

in other words,      
make, for a small increase in availability of resource i.

i tells us approximately how much more pro   t the    rm could

it follows that      

i would be the natural or equilibrium price for resource i, if
it were possible for the    rm to buy or sell it. suppose, for example, that the    rm
can buy or sell resource i, at a price that is less than      
i . in this case it would
certainly buy some of the resource, which would allow it to operate in a way that
increases its pro   t more than the cost of buying the resource. conversely, if the
price exceeds      
i , the    rm would sell some of its allocation of resource i, and obtain
a net gain since its income from selling some of the resource would be larger than
its drop in pro   t due to the reduction in availability of the resource.

5.7 examples

in this section we show by example that simple equivalent reformulations of a
problem can lead to very di   erent id78. we consider the following types
of reformulations:

    introducing new variables and associated equality constraints.
    replacing the objective with an increasing function of the original objective.
    making explicit constraints implicit, i.e., incorporating them into the domain

of the objective.

5.7.1 introducing new variables and equality constraints

consider an unconstrained problem of the form

minimize

f0(ax + b).

(5.59)

its lagrange dual function is the constant p   . so while we do have strong duality,
i.e., p    = d   , the lagrangian dual is neither useful nor interesting.

254

5 duality

now let us reformulate the problem (5.59) as

minimize
subject to ax + b = y.

f0(y)

(5.60)

here we have introduced new variables y, as well as new equality constraints ax +
b = y. the problems (5.59) and (5.60) are clearly equivalent.

the lagrangian of the reformulated problem is

l(x, y,   ) = f0(y) +   t (ax + b     y).

to    nd the dual function we minimize l over x and y. minimizing over x we    nd
that g(  ) =        unless at    = 0, in which case we are left with
0 (  ),

(f0(y)       t y) = bt        f    

g(  ) = bt    + inf
y

where f    
expressed as

0 is the conjugate of f0. the dual problem of (5.60) can therefore be

bt        f    
maximize
subject to at    = 0.

0 (  )

(5.61)

thus, the dual of the reformulated problem (5.60) is considerably more useful than
the dual of the original problem (5.59).

example 5.5 unconstrained geometric program. consider the unconstrained geomet-
ric program

we    rst reformulate it by introducing new variables and equality constraints:

minimize

i=1 exp(at

log(cid:0)pm
f0(y) = log(cid:0)pm

i x + bi)(cid:1) .
i=1 exp yi(cid:1)

minimize
subject to ax + b = y,

where at

i are the rows of a. the conjugate of the log-sum-exp function is

f    

0 (  ) =(cid:26) pm

   

i=1   i log   i

   (cid:23) 0, 1t    = 1
otherwise

(example 3.25, page 93), so the dual of the reformulated problem can be expressed
as

maximize
subject to 1t    = 1
at    = 0
   (cid:23) 0,
which is an id178 maximization problem.

bt       pm

i=1   i log   i

(5.62)

example 5.6 norm approximation problem. we consider the unconstrained norm
approximation problem

(5.63)
where k    k is any norm. here too the lagrange dual function is constant, equal to
the optimal value of (5.63), and therefore not useful.

kax     bk,

minimize

5.7 examples

255

once again we reformulate the problem as

minimize
subject to ax     b = y.

kyk

the lagrange dual problem is, following (5.61),

maximize
subject to

bt   
k  k        1
at    = 0,

(5.64)

where we use the fact that the conjugate of a norm is the indicator function of the
dual norm unit ball (example 3.26, page 93).

the idea of introducing new equality constraints can be applied to the constraint

functions as well. consider, for example, the problem

minimize
subject to

f0(a0x + b0)
fi(aix + bi)     0,

i = 1, . . . , m,

(5.65)

where ai     rki  n and fi : rki     r are convex. (for simplicity we do not include
equality constraints here.) we introduce a new variable yi     rki , for i = 0, . . . , m,
and reformulate the problem as

minimize
subject to

f0(y0)
fi(yi)     0,
aix + bi = yi,

i = 1, . . . , m

i = 0, . . . , m.

(5.66)

the lagrangian for this problem is

l(x, y0, . . . , ym,   ,   0, . . . ,   m) = f0(y0) +

  ifi(yi) +

mxi=1

mxi=0

  t
i (aix + bi     yi).

to    nd the dual function we minimize over x and yi. the minimum over x is       
unless

at

i   i = 0,

mxi=0

in which case we have, for        0,

g(  ,   0, . . . ,   m)

=

=

=

mxi=0
mxi=0
mxi=0

  t
i bi + inf

y0,...,ym  f0(y0) +
mxi=1
mxi=1
0 y0(cid:1) +
y0 (cid:0)f0(y0)       t
mxi=1

0 (  0)    

i (  i/  i).

  if    

  t
i bi + inf

i bi     f    
  t

  ifi(yi)    

i yi!

  t

mxi=0

  i inf

yi (cid:0)fi(yi)     (  i/  i)t yi(cid:1)

256

5 duality

the last expression involves the perspective of the conjugate function, and is there-
fore concave in the dual variables. finally, we address the question of what happens
when    (cid:23) 0, but some   i are zero. if   i = 0 and   i 6= 0, then the dual function is
      . if   i = 0 and   i = 0, however, the terms involving yi,   i, and   i are all zero.
thus, the expression above for g is valid for all    (cid:23) 0, if we take   if    
i (  i/  i) = 0
when   i = 0 and   i = 0, and   if    
i (  i/  i) =     when   i = 0 and   i 6= 0.

therefore we can express the dual of the problem (5.66) as

maximize pm
subject to    (cid:23) 0pm

i=0   t

i=0 at

0 (  0)    pm

i bi     f    
i   i = 0.

i=1   if    

i (  i/  i)

(5.67)

example 5.7 inequality constrained geometric program. the inequality constrained
geometric program

k=1 eat
k=1 eat

0kx+b0k(cid:17)
ikx+bik(cid:17)     0,

i = 1, . . . , m

is of the form (5.65) with fi : rki     r given by fi(y) = log(cid:0)pki

conjugate of this function is

k=1 eyk(cid:1). the

k=1   k log   k

1t    = 1

   (cid:23) 0,
otherwise.

using (5.67) we can immediately write down the dual problem as

maximize
subject to

k=1   0k log   0k +pm

i=1(cid:0)bt

i   i    pki

k=1   ik log(  ik/  i)(cid:1)

i = 1, . . . , m

which further simpli   es to

maximize
subject to

k=1   0k log   0k +pm

i=1(cid:0)bt

i   i    pki

k=1   ik log(  ik/1t   i)(cid:1)

minimize

subject to

f    

   

log(cid:16)pk0
log(cid:16)pki
i (  ) =(cid:26) pki
0   0    pk0
pm
0   0    pk0
pm

bt
  0 (cid:23) 0,
  i (cid:23) 0,
  i     0,
i=0 at

bt
  i (cid:23) 0,
1t   0 = 1
i=0 at

i = 0, . . . , m

1t   0 = 1
1t   i =   i,
i = 1, . . . , m

i   i = 0,

i   i = 0.

5.7.2 transforming the objective

if we replace the objective f0 by an increasing function of f0, the resulting problem
is clearly equivalent (see   4.1.3). the dual of this equivalent problem, however, can
be very di   erent from the dual of the original problem.

example 5.8 we consider again the minimum norm problem

minimize

kax     bk,

5.7 examples

257

where k    k is some norm. we reformulate this problem as

(1/2)kyk2
minimize
subject to ax     b = y.

here we have introduced new variables, and replaced the objective by half its square.
evidently it is equivalent to the original problem.

the dual of the reformulated problem is

maximize    (1/2)k  k2
subject to at    = 0,

    + bt   

where we use the fact that the conjugate of (1/2)k  k2 is (1/2)k  k2
page 93).

    (see example 3.27,

note that this dual problem is not the same as the dual problem (5.64) derived earlier.

5.7.3 implicit constraints

the next simple reformulation we study is to include some of the constraints in
the objective function, by modifying the objective function to be in   nite when the
constraint is violated.

example 5.9 linear program with box constraints. we consider the linear program

minimize
subject to ax = b

ct x

l (cid:22) x (cid:22) u

(5.68)

where a     rp  n and l     u. the constraints l (cid:22) x (cid:22) u are sometimes called box
constraints or variable bounds.

we can, of course, derive the dual of this linear program. the dual will have a
lagrange multiplier    associated with the equality constraint,   1 associated with the
inequality constraint x (cid:22) u, and   2 associated with the inequality constraint l (cid:22) x.
the dual is

maximize    bt          t
subject to at    +   1       2 + c = 0

1 u +   t
2 l

  2 (cid:23) 0.
instead, let us    rst reformulate the problem (5.68) as

  1 (cid:23) 0,

(5.69)

(5.70)

where we de   ne

minimize
subject to ax = b,

f0(x)

f0(x) =(cid:26) ct x l (cid:22) x (cid:22) u

    otherwise.

the problem (5.70) is clearly equivalent to (5.68); we have merely made the explicit
box constraints implicit.

258

5 duality

the dual function for the problem (5.70) is

g(  ) =

inf

l(cid:22)x(cid:22)u(cid:0)ct x +   t (ax     b)(cid:1)

where y+
ical formula for g, which is a concave piecewise-linear function.

i = max{yi, 0}, y   

=    bt        ut (at    + c)    + lt (at    + c)+
i = max{   yi, 0}. so here we are able to derive an analyt-

the dual problem is the unconstrained problem

maximize    bt        ut (at    + c)    + lt (at    + c)+,

(5.71)

which has a quite di   erent form from the dual of the original problem.

(the problems (5.69) and (5.71) are closely related, in fact, equivalent; see exer-
cise 5.8.)

5.8 theorems of alternatives

5.8.1 weak alternatives via the dual function

in this section we apply lagrange duality theory to the problem of determining
feasibility of a system of inequalities and equalities

fi(x)     0,

i = 1, . . . , m,

hi(x) = 0,

i = 1, . . . , p.

(5.72)

i=1 dom fi    
i=1 dom hi, is nonempty. we can think of (5.72) as the standard problem (5.1),

we assume the domain of the inequality system (5.72), d = tm
tp

with objective f0 = 0, i.e.,

minimize
subject to

0
fi(x)     0,
hi(x) = 0,

i = 1, . . . , m
i = 1, . . . , p.

this problem has optimal value

p    =(cid:26) 0

(5.72) is feasible

    (5.72) is infeasible,

(5.73)

(5.74)

so solving the optimization problem (5.73) is the same as solving the inequality
system (5.72).

the dual function

we associate with the inequality system (5.72) the dual function

g(  ,   ) = inf

x   d  mxi=1

  ifi(x) +

  ihi(x)! ,

pxi=1

5.8 theorems of alternatives

259

which is the same as the dual function for the optimization problem (5.73). since
f0 = 0, the dual function is positive homogeneous in (  ,   ): for    > 0, g(    ,     ) =
  g(  ,   ). the dual problem associated with (5.73) is to maximize g(  ,   ) subject
to    (cid:23) 0. since g is homogeneous, the optimal value of this dual problem is given
by

d    =(cid:26)        (cid:23) 0, g(  ,   ) > 0 is feasible

   (cid:23) 0, g(  ,   ) > 0 is infeasible.

0

(5.75)

weak duality tells us that d        p   . combining this fact with (5.74) and (5.75)

yields the following: if the inequality system

   (cid:23) 0,

g(  ,   ) > 0

(5.76)

is feasible (which means d    =    ), then the inequality system (5.72) is infeasible
(since we then have p    =    ). indeed, we can interpret any solution (  ,   ) of the
inequalities (5.76) as a proof or certi   cate of infeasibility of the system (5.72).
we can restate this implication in terms of feasibility of the original system: if
the original inequality system (5.72) is feasible, then the inequality system (5.76)
must be infeasible. we can interpret an x which satis   es (5.72) as a certi   cate
establishing infeasibility of the inequality system (5.76).

two systems of inequalities (and equalities) are called weak alternatives if at
most one of the two is feasible. thus, the systems (5.72) and (5.76) are weak
alternatives. this is true whether or not the inequalities (5.72) are convex (i.e.,
fi convex, hi a   ne); moreover, the alternative inequality system (5.76) is always
convex (i.e., g is concave and the constraints   i     0 are convex).
strict inequalities

we can also study feasibility of the strict inequality system

fi(x) < 0,

i = 1, . . . , m,

hi(x) = 0,

i = 1, . . . , p.

(5.77)

with g de   ned as for the nonstrict inequality system, we have the alternative
inequality system

   (cid:23) 0,

   6= 0,

g(  ,   )     0.

(5.78)

we can show directly that (5.77) and (5.78) are weak alternatives. suppose there
exists an   x with fi(  x) < 0, hi(  x) = 0. then for any    (cid:23) 0,    6= 0, and   ,

  1f1(  x) +        +   mfm(  x) +   1h1(  x) +        +   php(  x) < 0.

it follows that

g(  ,   ) = inf

  ifi(x) +

  ihi(x)!

pxi=1

x   d  mxi=1
mxi=1

   
< 0.

  ifi(  x) +

  ihi(  x)

pxi=1

260

5 duality

therefore, feasibility of (5.77) implies that there does not exist (  ,   ) satisfy-
ing (5.78).

thus, we can prove infeasibility of (5.77) by producing a solution of the sys-
tem (5.78); we can prove infeasibility of (5.78) by producing a solution of the
system (5.77).

5.8.2 strong alternatives

when the original inequality system is convex, i.e., fi are convex and hi are a   ne,
and some type of constraint quali   cation holds, then the pairs of weak alternatives
described above are strong alternatives, which means that exactly one of the two
alternatives holds. in other words, each of the inequality systems is feasible if and
only if the other is infeasible.

in this section we assume that fi are convex and hi are a   ne, so the inequality

system (5.72) can be expressed as

fi(x)     0,

i = 1, . . . , m,

ax = b,

where a     rp  n.
strict inequalities

we    rst study the strict inequality system

fi(x) < 0,

i = 1, . . . , m,

ax = b,

and its alternative

   (cid:23) 0,

   6= 0,

g(  ,   )     0.

(5.79)

(5.80)

we need one technical condition: there exists an x     relintd with ax = b. in
other words we not only assume that the linear equality constraints are consistent,
but also that they have a solution in relintd. (very often d = rn, so the condition
is satis   ed if the equality constraints are consistent.) under this condition, exactly
one of the inequality systems (5.79) and (5.80) is feasible.
in other words, the
inequality systems (5.79) and (5.80) are strong alternatives.

we will establish this result by considering the related optimization problem

minimize
subject to

s
fi(x)     s     0,
ax = b

i = 1, . . . , m

(5.81)

with variables x, s, and domain d    r. the optimal value p    of this problem is
negative if and only if there exists a solution to the strict inequality system (5.79).

the lagrange dual function for the problem (5.81) is

x   d, s s +

inf

mxi=1

  i(fi(x)     s) +   t (ax     b)! =(cid:26) g(  ,   ) 1t    = 1

      

otherwise.

5.8 theorems of alternatives

261

therefore we can express the dual problem of (5.81) as

maximize
g(  ,   )
subject to    (cid:23) 0,

1t    = 1.

now we observe that slater   s condition holds for the problem (5.81). by the
hypothesis there exists an   x     relintd with a  x = b. choosing any   s > maxi fi(  x)
yields a point (  x,   s) which is strictly feasible for (5.81). therefore we have d    = p   ,
and the dual optimum d    is attained. in other words, there exist (     ,      ) such that

g(     ,      ) = p   ,

      (cid:23) 0,

1t       = 1.

(5.82)

now suppose that the strict inequality system (5.79) is infeasible, which means that
p        0. then (     ,      ) from (5.82) satisfy the alternate inequality system (5.80).
similarly, if the alternate inequality system (5.80) is feasible, then d    = p       
0, which shows that the strict inequality system (5.79) is infeasible. thus, the
inequality systems (5.79) and (5.80) are strong alternatives; each is feasible if and
only if the other is not.

nonstrict inequalities

we now consider the nonstrict inequality system

and its alternative

fi(x)     0,

i = 1, . . . , m,

ax = b,

   (cid:23) 0,

g(  ,   ) > 0.

(5.83)

(5.84)

we will show these are strong alternatives, provided the following conditions hold:
there exists an x     relintd with ax = b, and the optimal value p    of (5.81) is
attained. this holds, for example, if d = rn and maxi fi(x)         as x        .
with these assumptions we have, as in the strict case, that p    = d   , and that both
the primal and dual optimal values are attained. now suppose that the nonstrict
inequality system (5.83) is infeasible, which means that p    > 0. (here we use the
assumption that the primal optimal value is attained.) then (     ,      ) from (5.82)
satisfy the alternate inequality system (5.84). thus, the inequality systems (5.83)
and (5.84) are strong alternatives; each is feasible if and only if the other is not.

5.8.3 examples

linear inequalities
consider the system of linear inequalities ax (cid:22) b. the dual function is

g(  ) = inf
x

  t (ax     b) =(cid:26)    bt    at    = 0

       otherwise.

the alternative inequality system is therefore

   (cid:23) 0,

at    = 0,

bt    < 0.

262

5 duality

these are, in fact, strong alternatives. this follows since the optimum in the related
problem (5.81) is achieved, unless it is unbounded below.

we now consider the system of strict linear inequalities ax     b, which has the

strong alternative system
   (cid:23) 0,

   6= 0,

at    = 0,

bt        0.

in fact we have encountered (and proved) this result before, in   2.5.1; see (2.17)
and (2.18) (on page 50).

intersection of ellipsoids

we consider m ellipsoids, described as

ei = {x | fi(x)     0},

with fi(x) = xt aix + 2bt
++. we ask when
the intersection of these ellipsoids has nonempty interior. this is equivalent to
feasibility of the set of strict quadratic inequalities

i x + ci, i = 1, . . . , m, where ai     sn

fi(x) = xt aix + 2bt

i x + ci < 0,

i = 1, . . . , m.

(5.85)

the dual function g is

g(  ) = inf

x (cid:0)xt a(  )x + 2b(  )t x + c(  )(cid:1)

= (cid:26)    b(  )t a(  )   b(  ) + c(  ) a(  ) (cid:23) 0,

otherwise,

      

where

b(  )     r(a(  ))

a(  ) =

  iai,

b(  ) =

  ibi,

c(  ) =

  ici.

mxi=1

mxi=1

mxi=1

note that for    (cid:23) 0,    6= 0, we have a(  )     0, so we can simplify the expression
for the dual function as

the strong alternative of the system (5.85) is therefore

g(  ) =    b(  )t a(  )   1b(  ) + c(  ).

   (cid:23) 0,

   6= 0,

   b(  )t a(  )   1b(  ) + c(  )     0.

(5.86)

we can give a simple geometric interpretation of this pair of strong alternatives.

for any nonzero    (cid:23) 0, the (possibly empty) ellipsoid

e   = {x | xt a(  )x + 2b(  )t x + c(  )     0}

contains e1                em, since fi(x)     0 implies pm

empty interior if and only if

inf

x (cid:0)xt a(  )x + 2b(  )t x + c(  )(cid:1) =    b(  )t a(  )   1b(  ) + c(  )     0.

therefore the alternative system (5.86) means that e   has empty interior.
weak duality is obvious: if (5.86) holds, then e   contains the intersection e1    
           em, and has empty interior, so naturally the intersection has empty interior.
the fact that these are strong alternatives states the (not obvious) fact that if the
intersection e1              em has empty interior, then we can construct an ellipsoid e  
that contains the intersection and has empty interior.

i=1   ifi(x)     0. now, e   has

5.8 theorems of alternatives

263

farkas    lemma

in this section we describe a pair of strong alternatives for a mixture of strict and
nonstrict linear inequalities, known as farkas    lemma: the system of inequalities

ax (cid:22) 0,

ct x < 0,

where a     rm  n and c     rn, and the system of equalities and inequalities

at y + c = 0,

y (cid:23) 0,

(5.87)

(5.88)

are strong alternatives.

we can prove farkas    lemma directly, using lp duality. consider the lp

and its dual

ct x

minimize
subject to ax (cid:22) 0,

maximize
subject to at y + c = 0

0

y (cid:23) 0.

(5.89)

(5.90)

the primal lp (5.89) is homogeneous, and so has optimal value 0, if (5.87) is
not feasible, and optimal value       , if (5.87) is feasible. the dual lp (5.90) has
optimal value 0, if (5.88) is feasible, and optimal value       , if (5.88) is infeasible.
since x = 0 is feasible in (5.89), we can rule out the one case in which strong
duality can fail for lps, so we must have p    = d   . combined with the remarks
above, this shows that (5.87) and (5.88) are strong alternatives.

example 5.10 arbitrage-free bounds on price. we consider a set of n assets, with
prices at the beginning of an investment period p1, . . . , pn, respectively. at the end
of the investment period, the value of the assets is v1, . . . , vn. if x1, . . . , xn represents
the initial investment in each asset (with xj < 0 meaning a short position in asset j),
the cost of the initial investment is pt x, and the    nal value of the investment is vt x.

the value of the assets at the end of the investment period, v, is uncertain. we will
assume that only m possible scenarios, or outcomes, are possible. if outcome i occurs,
the    nal value of the assets is v(i), and therefore, the overall value of the investments
is v(i)t x.
if there is an investment vector x with pt x < 0, and in all possible scenarios, the
   nal value is nonnegative, i.e., v(i)t x     0 for i = 1, . . . , m, then an arbitrage is said
to exist. the condition pt x < 0 means you are paid to accept the investment mix,
and the condition v(i)t x     0 for i = 1, . . . , m means that no matter what outcome
occurs, the    nal value is nonnegative, so an arbitrage corresponds to a guaranteed
money-making investment strategy. it is generally assumed that the prices and values
are such that no arbitrage exists. this means that the inequality system

v x (cid:23) 0,

pt x < 0

is infeasible, where vij = v(i)
j
using farkas    lemma, we have no arbitrage if and only if there exists y such that

.

   v t y + p = 0,

y (cid:23) 0.

264

5 duality

we can use this characterization of arbitrage-free prices and values to solve several
interesting problems.

suppose, for example, that the values v are known, and all prices except the last
one, pn, are known. the set of prices pn that are consistent with the no-arbitrage
assumption is an interval, which can be found by solving a pair of lps. the optimal
value of the lp

minimize
subject to v t y = p,

pn

y (cid:23) 0,

with variables pn and y, gives the smallest possible arbitrage-free price for asset n.
solving the same lp with maximization instead of minimization yields the largest
possible price for asset n. if the two values are equal, i.e., the no-arbitrage assumption
leads us to a unique price for asset n, we say the market is complete. for an example,
see exercise 5.38.

this method can be used to    nd bounds on the price of a derivative or option that
is based on the    nal value of other underlying assets, i.e., when the value or payo   
of asset n is a function of the values of the other assets.

5.9 generalized inequalities

in this section we examine how lagrange duality extends to a problem with gen-
eralized inequality constraints

minimize
subject to

f0(x)
fi(x) (cid:22)ki 0,
hi(x) = 0,

i = 1, . . . , m

i = 1, . . . , p,

(5.91)

where ki     rki are proper cones. for now, we do not assume convexity of the prob-
lem (5.91). we assume the domain of (5.91), d =tm
i=1 dom hi, is
nonempty.

i=0 dom fi     tp

5.9.1 the lagrange dual

with each generalized inequality fi(x) (cid:22)ki 0 in (5.91) we associate a lagrange
multiplier vector   i     rki and de   ne the associated lagrangian as

l(x,   ,   ) = f0(x) +   t

1 f1(x) +        +   t

mfm(x) +   1h1(x) +        +   php(x),

where    = (  1, . . . ,   m) and    = (  1, . . . ,   p). the dual function is de   ned exactly
as in a problem with scalar inequalities:

g(  ,   ) = inf
x   d

l(x,   ,   ) = inf

x   d f0(x) +

  t
i fi(x) +

mxi=1

  ihi(x)! .

pxi=1

since the lagrangian is a   ne in the dual variables (  ,   ), and the dual function is
a pointwise in   mum of the lagrangian, the dual function is concave.

5.9 generalized inequalities

265

as in a problem with scalar inequalities, the dual function gives lower bounds
on p   , the optimal value of the primal problem (5.91). for a problem with scalar
inequalities, we require   i     0. here the nonnegativity requirement on the dual
variables is replaced by the condition

  i (cid:23)k    

i

0,

i = 1, . . . , m,

where k    
associated with inequalities must be dual nonnegative.

i denotes the dual cone of ki. in other words, the lagrange multipliers

weak duality follows immediately from the de   nition of dual cone. if   i (cid:23)k    

0
i fi(  x)     0. therefore for any primal feasible point   x and

i

and fi(  x) (cid:22)ki 0, then   t
any   i (cid:23)k    

0, we have

i

f0(  x) +

  t
i fi(  x) +

  ihi(  x)     f0(  x).

mxi=1

pxi=1

taking the in   mum over   x yields g(  ,   )     p   .
the lagrange dual optimization problem is

maximize
g(  ,   )
subject to   i (cid:23)k    

i

0,

i = 1, . . . , m.

(5.92)

we always have weak duality, i.e., d        p   , where d    denotes the optimal value of
the dual problem (5.92), whether or not the primal problem (5.91) is convex.

slater   s condition and strong duality

as might be expected, strong duality (d    = p   ) holds when the primal problem
is convex and satis   es an appropriate constraint quali   cation. for example, a
generalized version of slater   s condition for the problem

minimize
subject to

f0(x)
fi(x) (cid:22)ki 0,
ax = b,

i = 1, . . . , m

where f0 is convex and fi is ki-convex, is that there exists an x     relintd with
ax = b and fi(x)    ki 0, i = 1, . . . , m. this condition implies strong duality (and
also, that the dual optimum is attained).

example 5.11 lagrange dual of semide   nite program. we consider a semide   nite
program in inequality form,

ct x

minimize
subject to x1f1 +        + xnfn + g (cid:22) 0

(5.93)

where f1, . . . , fn, g     sk. (here f1 is a   ne, and k1 is sk
cone.)
we associate with the constraint a dual variable or multiplier z     sk, so the la-
grangian is

+, the positive semide   nite

l(x, z) = ct x + tr ((x1f1 +        + xnfn + g) z)

= x1(c1 + tr(f1z)) +        + xn(cn + tr(fnz)) + tr(gz),

266

5 duality

which is a   ne in x. the dual function is given by

g(z) = inf
x

l(x, z) =(cid:26) tr(gz)

      

tr(fiz) + ci = 0,
otherwise.

i = 1, . . . , n

the dual problem can therefore be expressed as

maximize
subject to

tr(gz)
tr(fiz) + ci = 0,
z (cid:23) 0.

i = 1, . . . , n

(we use the fact that sk

+ is self-dual, i.e., (sk

+)    = sk

+; see   2.6.)

strong duality obtains if the semide   nite program (5.93) is strictly feasible, i.e., there
exists an x with

x1f1 +        + xnfn + g     0.

example 5.12 lagrange dual of cone program in standard form. we consider the
cone program

ct x

minimize
subject to ax = b
x (cid:23)k 0,

where a     rm  n, b     rm, and k     rn is a proper cone. we associate with the
equality constraint a multiplier        rm, and with the nonnegativity constraint a
multiplier        rn. the lagrangian is

l(x,   ,   ) = ct x       t x +   t (ax     b),

so the dual function is

g(  ,   ) = inf
x

l(x,   ,   ) =(cid:26)    bt    at           + c = 0

       otherwise.

the dual problem can be expressed as

maximize    bt   
subject to at    + c =   

   (cid:23)k     0.

by eliminating    and de   ning y =      , this problem can be simpli   ed to

bt y

maximize
subject to at y (cid:22)k     c,

which is a cone program in inequality form, involving the dual generalized inequality.
strong duality obtains if the slater condition holds, i.e., there is an x    k 0 with
ax = b.

5.9.2 optimality conditions

the optimality conditions of   5.5 are readily extended to problems with generalized
inequalities. we    rst derive the complementary slackness conditions.

5.9 generalized inequalities

267

complementary slackness

assume that the primal and dual optimal values are equal, and attained at the
optimal points x   ,      ,      . as in   5.5.2, the complementary slackness conditions
follow directly from the equality f0(x   ) = g(     ,      ), along with the de   nition of g.
we have

f0(x   ) = g(     ,      )

mxi=1

    f0(x   ) +
    f0(x   ),

     
i

t fi(x   ) +

     
i hi(x   )

pxi=1

and therefore we conclude that x    minimizes l(x,      ,      ), and also that the two
sums in the second line are zero. since the second sum is zero (since x    satis   es
t fi(x   ) = 0. since each term in this

i=1      
i

the equality constraints), we have pm

sum is nonpositive, we conclude that

     
i

t fi(x   ) = 0,

i = 1, . . . , m,

(5.94)

which generalizes the complementary slackness condition (5.48). from (5.94) we
can conclude that

     
i    k    

i

0 =    fi(x   ) = 0,

fi(x   )    ki 0, =         

i = 0.

however, in contrast to problems with scalar inequalities, it is possible to sat-
isfy (5.94) with      

i 6= 0 and fi(x   ) 6= 0.

kkt conditions

now we add the assumption that the functions fi, hi are di   erentiable, and gener-
alize the kkt conditions of   5.5.3 to problems with generalized inequalities. since
x    minimizes l(x,      ,      ), its gradient with respect to x vanishes at x   :

   f0(x   ) +

mxi=1

dfi(x   )t      

i +

pxi=1

     
i    hi(x   ) = 0,

where dfi(x   )     rki  n is the derivative of fi evaluated at x    (see   a.4.1). thus,
if strong duality holds, any primal optimal x    and any dual optimal (     ,      ) must
satisfy the optimality conditions (or kkt conditions)

fi(x   ) (cid:22)ki
hi(x   )
=
     
i (cid:23)k    
t fi(x   )
     
=
i
i=1      
i    hi(x   )
=

i

i = 1, . . . , m
i = 1, . . . , p
i = 1, . . . , m
i = 1, . . . , m

0,
0,
0,
0,
0.

   f0(x   ) +pm

i=1 dfi(x   )t      

i +pp

(5.95)
if the primal problem is convex, the converse also holds, i.e., the conditions (5.95)
are su   cient conditions for optimality of x   , (     ,      ).

268

5 duality

5.9.3 perturbation and sensitivity analysis

the results of   5.6 can be extended to problems involving generalized inequalities.
we consider the associated perturbed version of the problem,

minimize
subject to

f0(x)
fi(x) (cid:22)ki ui,
hi(x) = vi,

i = 1, . . . , m

i = 1, . . . , p,

where ui     rki, and v     rp. we de   ne p   (u, v) as the optimal value of the
perturbed problem. as in the case with scalar inequalities, p    is a convex function
when the original problem is convex.

now let (     ,      ) be optimal for the dual of the original (unperturbed) problem,

which we assume has zero duality gap. then for all u and v we have

p   (u, v)     p       

mxi=1

     
i

t ui          t v,

the analog of the global sensitivity inequality (5.57). the local sensitivity result
holds as well: if p   (u, v) is di   erentiable at u = 0, v = 0, then the optimal dual
variables      

i satis   es

i =       ui p   (0, 0),
     

the analog of (5.58).

example 5.13 semide   nite program in inequality form. we consider a semide   nite
program in inequality form, as in example 5.11. the primal problem is

ct x

minimize
subject to f (x) = x1f1 +        + xnfn + g (cid:22) 0,

with variable x     rn (and f1, . . . , fn, g     sk), and the dual problem is

maximize
subject to

tr(gz)
tr(fiz) + ci = 0,
z (cid:23) 0,

i = 1, . . . , n

with variable z     sk.
suppose that x    and z     are primal and dual optimal, respectively, with zero duality
gap. the complementary slackness condition is tr(f (x   )z    ) = 0. since f (x   ) (cid:22) 0
and z     (cid:23) 0, we can conclude that f (x   )z     = 0. thus, the complementary slackness
condition can be expressed as

r(f (x   ))     r(z    ),

i.e., the ranges of the primal and dual matrices are orthogonal.
let p   (u ) denote the optimal value of the perturbed sdp

ct x

minimize
subject to f (x) = x1f1 +        + xnfn + g (cid:22) u.

5.9 generalized inequalities

269

then we have, for all u , p   (u )     p        tr(z    u ). if p   (u ) is di   erentiable at u = 0,
then we have

this means that for u small, the optimal value of the perturbed sdp is very close
to (the lower bound) p        tr(z    u ).

   p   (0) =    z    .

5.9.4 theorems of alternatives

we can derive theorems of alternatives for systems of generalized inequalities and
equalities

fi(x) (cid:22)ki 0,

(5.96)
where ki     rki are proper cones. we will also consider systems with strict in-
equalities,

i = 1, . . . , m,

i = 1, . . . , p,

hi(x) = 0,

fi(x)    ki 0,

we assume that d =tm

weak alternatives

i=0 dom fi     tp

i = 1, . . . , m,

hi(x) = 0,

i = 1, . . . , p.

(5.97)

i=1 dom hi is nonempty.

we associate with the systems (5.96) and (5.97) the dual function

g(  ,   ) = inf

x   d  mxi=1

  t
i fi(x) +

  ihi(x)!

pxi=1

where    = (  1, . . . ,   m) with   i     rki and        rp. in analogy with (5.76), we
claim that
(5.98)

i = 1, . . . , m,

g(  ,   ) > 0

0,

  i (cid:23)k    

i

is a weak alternative to the system (5.96). to verify this, suppose there exists an
x satisfying (5.96) and (  ,   ) satisfying (5.98). then we have a contradiction:

0 < g(  ,   )       t

1 f1(x) +        +   t

mfm(x) +   1h1(x) +        +   php(x)     0.

therefore at least one of the two systems (5.96) and (5.98) must be infeasible, i.e.,
the two systems are weak alternatives.

in a similar way, we can prove that (5.97) and the system

  i (cid:23)k    

i

0,

i = 1, . . . , m,

   6= 0,

g(  ,   )     0.

form a pair of weak alternatives.

strong alternatives

we now assume that the functions fi are ki-convex, and the functions hi are a   ne.
we    rst consider a system with strict inequalities

fi(x)    ki 0,

i = 1, . . . , m,

ax = b,

(5.99)

270

5 duality

and its alternative

  i (cid:23)k    

i

0,

i = 1, . . . , m,

   6= 0,

g(  ,   )     0.

(5.100)

we have already seen that (5.99) and (5.100) are weak alternatives. they are also
strong alternatives provided the following constraint quali   cation holds: there
exists an   x     relintd with a  x = b. to prove this, we select a set of vectors
ei    ki 0, and consider the problem
s
fi(x) (cid:22)ki sei,
ax = b

minimize
subject to

i = 1, . . . , m

(5.101)

with variables x and s     r. slater   s condition holds since (  x,   s) satis   es the strict
inequalities fi(  x)    ki   sei provided   s is large enough.

the dual of (5.101) is

maximize
g(  ,   )
subject to   i (cid:23)k    
i=1 et

i

0,
i   i = 1

pm

i = 1, . . . , m

(5.102)

with variables    = (  1, . . . ,   m) and   .

now suppose the system (5.99) is infeasible. then the optimal value of (5.101)
is nonnegative. since slater   s condition is satis   ed, we have strong duality and the
dual optimum is attained. therefore there exist (    ,     ) that satisfy the constraints
of (5.102) and g(    ,     )     0, i.e., the system (5.100) has a solution.
ax = b is not su   cient for the system of nonstrict inequalities

as we noted in the case of scalar inequalities, existence of an x     relintd with

fi(x) (cid:22)ki 0,

i = 1, . . . , m,

ax = b

and its alternative

  i (cid:23)k    

i

0,

i = 1, . . . , m,

g(  ,   ) > 0

to be strong alternatives. an additional condition is required, e.g., that the optimal
value of (5.101) is attained.

example 5.14 feasibility of a linear matrix inequality. the following systems are
strong alternatives:

where fi, g     sk, and

f (x) = x1f1 +        + xnfn + g     0,

z (cid:23) 0,

z 6= 0,

tr(gz)     0,

tr(fiz) = 0,

i = 1, . . . , n,

where z     sk. this follows from the general result, if we take for k the positive
semide   nite cone sk

+, and

g(z) = inf
x

(tr(f (x)z)) =(cid:26) tr(gz)

      

tr(fiz) = 0,
otherwise.

i = 1, . . . , n

5.9 generalized inequalities

271

the nonstrict inequality case is slightly more involved, and we need an extra assump-
tion on the matrices fi to have strong alternatives. one such condition is

nxi=1

vifi (cid:23) 0 =   

vifi = 0.

nxi=1

if this condition holds, the following systems are strong alternatives:

f (x) = x1f1 +        + xnfn + g (cid:22) 0

tr(gz) > 0,

tr(fiz) = 0,

i = 1, . . . , n

and

z (cid:23) 0,

(see exercise 5.44).

272

5 duality

bibliography

lagrange duality is covered in detail by luenberger [lue69, chapter 8], rockafellar [roc70,
part vi], whittle [whi71], hiriart-urruty and lemar  echal [hul93], and bertsekas, nedi  c,
and ozdaglar [ber03]. the name is derived from lagrange   s method of multipliers for
optimization problems with equality constraints; see courant and hilbert [ch53, chapter
iv].
the max-min result for matrix games in   5.2.5 predates id135 duality.
it is proved via a theorem of alternatives by von neuman and morgenstern [vnm53,
page 153]. the strong duality result for id135 on page 227 is due to von
neumann [vn63] and gale, kuhn, and tucker [gkt51]. strong duality for the nonconvex
quadratic problem (5.32) is a fundamental result in the literature on trust region methods
for nonlinear optimization (nocedal and wright [nw99, page 78]). it is also related to the
s-procedure in control theory, discussed in appendix   b.1. for an extension of the proof
of strong duality of   5.3.2 to the re   ned slater condition (5.27), see rockafellar [roc70,
page 277].

conditions that guarantee the saddle-point property (5.47) can be found in rockafel-
lar [roc70, part vii] and bertsekas, nedi  c, and ozdaglar [ber03, chapter 2]; see also
exercise 5.25.

the kkt conditions are named after karush (whose unpublished 1939 master   s thesis
is summarized in kuhn [kuh76]), kuhn, and tucker [kt51]. related optimality condi-
tions were also derived by john [joh85]. the water-   lling algorithm in example 5.2 has
applications in id205 and communications (cover and thomas [ct91, page
252]).

farkas    lemma was published by farkas [far02].
it is the best known theorem of al-
ternatives for systems of linear inequalities and equalities, but many variants exist; see
mangasarian [man94,   2.4]. the application of farkas    lemma to asset pricing (exam-
ple 5.10) is discussed by bertsimas and tsitsiklis [bt97, page 167] and ross [ros99].

the extension of lagrange duality to problems with generalized inequalities appears in
isii [isi64], luenberger [lue69, chapter 8], berman [ber73], and rockafellar [roc89, page
47].
it is discussed in the context of cone programming in nesterov and nemirovski
[nn94,   4.2] and ben-tal and nemirovski [btn01, lecture 2]. theorems of alternatives
for generalized inequalities were studied by ben-israel [bi69], berman and ben-israel
[bbi71], and craven and kohila [ck77]. bellman and fan [bf63], wolkowicz [wol81],
and lasserre [las95] give extensions of farkas    lemma to linear matrix inequalities.

exercises

exercises

basic de   nitions

273

5.1 a simple example. consider the optimization problem

minimize
subject to

x2 + 1
(x     2)(x     4)     0,

with variable x     r.
(a) analysis of primal problem. give the feasible set, the optimal value, and the optimal

solution.

(b) lagrangian and dual function. plot the objective x2 + 1 versus x. on the same plot,
show the feasible set, optimal point and value, and plot the lagrangian l(x,   ) versus
x for a few positive values of   . verify the lower bound property (p        inf x l(x,   )
for        0). derive and sketch the lagrange dual function g.
(c) lagrange dual problem. state the dual problem, and verify that it is a concave
maximization problem. find the dual optimal value and dual optimal solution      .
does strong duality hold?

(d) sensitivity analysis. let p   (u) denote the optimal value of the problem

minimize
subject to

x2 + 1
(x     2)(x     4)     u,

as a function of the parameter u. plot p   (u). verify that dp   (0)/du =         .

5.2 weak duality for unbounded and infeasible problems. the weak duality inequality, d        p   ,
clearly holds when d    =        or p    =    . show that it holds in the other two cases as
well: if p    =       , then we must have d    =       , and also, if d    =    , then we must have
p    =    .

5.3 problems with one inequality constraint. express the dual problem of

minimize
subject to

ct x
f (x)     0,

with c 6= 0, in terms of the conjugate f    . explain why the problem you give is convex.
we do not assume f is convex.

examples and applications

5.4 interpretation of lp dual via relaxed problems. consider the inequality form lp

ct x

minimize
subject to ax (cid:22) b,

with a     rm  n, b     rm. in this exercise we develop a simple geometric interpretation
of the dual lp (5.22).
let w     rm
+ . if x is feasible for the lp, i.e., satis   es ax (cid:22) b, then it also satis   es the
inequality

geometrically, for any w (cid:23) 0, the halfspace hw = {x | wt ax     wt b} contains the feasible
set for the lp. therefore if we minimize the objective ct x over the halfspace hw we get
a lower bound on p   .

wt ax     wt b.

274

5 duality

(a) derive an expression for the minimum value of ct x over the halfspace hw (which

will depend on the choice of w (cid:23) 0).

(b) formulate the problem of    nding the best such bound, by maximizing the lower

bound over w (cid:23) 0.

(c) relate the results of (a) and (b) to the lagrange dual of the lp, given by (5.22).

5.5 dual of general lp. find the dual function of the lp

ct x

minimize
subject to gx (cid:22) h
ax = b.

give the dual problem, and make the implicit equality constraints explicit.

5.6 lower bounds in chebyshev approximation from least-squares. consider the chebyshev

or       -norm approximation problem

minimize

kax     bk   ,

(5.103)

where a     rm  n and rank a = n. let xch denote an optimal solution (there may be
multiple optimal solutions; xch denotes one of them).
the chebyshev problem has no closed-form solution, but the corresponding least-squares
problem does. de   ne

xls = argminkax     bk2 = (at a)   1at b.

we address the following question. suppose that for a particular a and b we have com-
puted the least-squares solution xls (but not xch). how suboptimal is xls for the chebyshev
problem? in other words, how much larger is kaxls     bk    than kaxch     bk   ?
(a) prove the lower bound

kaxls     bk           mkaxch     bk   ,

using the fact that for all z     rm,

1
   mkzk2     kzk        kzk2.

(b) in example 5.6 (page 254) we derived a dual for the general norm approximation
problem. applying the results to the       -norm (and its dual norm, the    1-norm), we
can state the following dual for the chebyshev approximation problem:

maximize
subject to

bt   
k  k1     1
at    = 0.

(5.104)

any feasible    corresponds to a lower bound bt    on kaxch     bk   .
denote the least-squares residual as rls = b     axls. assuming rls 6= 0, show that

     =    rls/krlsk1,

     = rls/krlsk1,

are both feasible in (5.104). by duality bt      and bt      are lower bounds on kaxch    
bk   . which is the better bound? how do these bounds compare with the bound
derived in part (a)?

exercises

275

5.7 piecewise-linear minimization. we consider the convex piecewise-linear minimization

problem

minimize maxi=1,...,m(at

i x + bi)

(5.105)

with variable x     rn.
(a) derive a dual problem, based on the lagrange dual of the equivalent problem

minimize maxi=1,...,m yi
at
i x + bi = yi,
subject to

i = 1, . . . , m,

with variables x     rn, y     rm.

(b) formulate the piecewise-linear minimization problem (5.105) as an lp, and form the

dual of the lp. relate the lp dual to the dual obtained in part (a).

(c) suppose we approximate the objective function in (5.105) by the smooth function

f0(x) = log  mxi=1
log(cid:0)pm

minimize

exp(at

i x + bi)! ,

i=1 exp(at

i x + bi)(cid:1) .

pwl and p   

and solve the unconstrained geometric program

(5.106)

a dual of this problem is given by (5.62). let p   
of (5.105) and (5.106), respectively. show that

gp be the optimal values

(d) derive similar bounds for the di   erence between p   

pwl and the optimal value of

0     p   

gp     p   

pwl     log m.

minimize

i=1 exp(  (at

(1/  ) log(cid:0)pm

i x + bi))(cid:1) ,

where    > 0 is a parameter. what happens as we increase   ?

5.8 relate the two id78 derived in example 5.9 on page 257.

5.9 suboptimality of a simple covering ellipsoid. recall the problem of determining the min-
imum volume ellipsoid, centered at the origin, that contains the points a1, . . . , am     rn
(problem (5.14), page 222):

minimize
subject to

f0(x) = log det(x    1)
at
i xai     1,

i = 1, . . . , m,

with dom f0 = sn
the problem is bounded below).

++. we assume that the vectors a1, . . . , am span rn (which implies that

(a) show that the matrix

is feasible. hint. show that

xsim =  mxk=1
(cid:20) pm

k=1 akat

at
i

k

akat

,

k!   1
1 (cid:21) (cid:23) 0,

ai

and use schur complements (  a.5.5) to prove that at

i xai     1 for i = 1, . . . , m.

276

5 duality

(b) now we establish a bound on how suboptimal the feasible point xsim is, via the dual

problem,

maximize
subject to    (cid:23) 0,

log det(cid:0)pm

i=1   iaiat

i(cid:1)     1t    + n

i=1   iaiat

with the implicit constraintpm

i     0. (this dual is derived on page 222.)
to derive a bound, we restrict our attention to dual variables of the form    = t1,
where t > 0. find (analytically) the optimal value of t, and evaluate the dual
objective at this   . use this to prove that the volume of the ellipsoid {u | ut xsimu    
1} is no more than a factor (m/n)n/2 more than the volume of the minimum volume
ellipsoid.

5.10 optimal experiment design. the following problems arise in experiment design (see   7.5).

(a) d-optimal design.

minimize
subject to x (cid:23) 0,

i=1 xivivt

1t x = 1.

log det(cid:0)pp
tr(cid:0)pp

i(cid:1)   1
i(cid:1)   1

(b) a-optimal design.

minimize
subject to x (cid:23) 0,

i=1 xivivt

1t x = 1.
i     0}. the variable is x     rp; the
vectors v1, . . . , vp     rn are given.
derive id78 by    rst introducing a new variable x     sn and an equality con-
i=1 xivivt
i , and then applying lagrange duality. simplify the dual prob-
lems as much as you can.
5.11 derive a dual problem for

the domain of both problems is {x | pp
straint x =pp

i=1 xivivt

minimize pn

i=1 kaix + bik2 + (1/2)kx     x0k2
2.

the problem data are ai     rmi  n, bi     rmi , and x0     rn. first introduce new variables
yi     rmi and equality constraints yi = aix + bi.

5.12 analytic centering. derive a dual problem for

minimize    pm

i=1 log(bi     at

i x)

with domain {x | at
i x < bi, i = 1, . . . , m}. first introduce new variables yi and equality
constraints yi = bi     at
(the solution of this problem is called the analytic center of the linear inequalities at
i x    
bi, i = 1, . . . , m. analytic centers have geometric applications (see   8.5.3), and play an
important role in barrier methods (see chapter 11).)
5.13 lagrangian relaxation of boolean lp. a boolean linear program is an optimization prob-

i x.

lem of the form

ct x

minimize
subject to ax (cid:22) b

xi     {0, 1},

i = 1, . . . , n,

and is, in general, very di   cult to solve. in exercise 4.15 we studied the lp relaxation of
this problem,

ct x

minimize
subject to ax (cid:22) b

0     xi     1,

i = 1, . . . , n,

(5.107)

which is far easier to solve, and gives a lower bound on the optimal value of the boolean
lp. in this problem we derive another lower bound for the boolean lp, and work out the
relation between the two lower bounds.

exercises

277

(a) lagrangian relaxation. the boolean lp can be reformulated as the problem

ct x

minimize
subject to ax (cid:22) b

xi(1     xi) = 0,

i = 1, . . . , n,

which has quadratic equality constraints. find the lagrange dual of this problem.
the optimal value of the dual problem (which is convex) gives a lower bound on
the optimal value of the boolean lp. this method of    nding a lower bound on the
optimal value is called lagrangian relaxation.

(b) show that the lower bound obtained via lagrangian relaxation, and via the lp
relaxation (5.107), are the same. hint. derive the dual of the lp relaxation (5.107).

5.14 a penalty method for equality constraints. we consider the problem

minimize
subject to ax = b,

f0(x)

(5.108)

where f0 : rn     r is convex and di   erentiable, and a     rm  n with rank a = m.
in a quadratic penalty method, we form an auxiliary function

  (x) = f0(x) +   kax     bk2
2,

where    > 0 is a parameter. this auxiliary function consists of the objective plus the
penalty term   kax    bk2
2. the idea is that a minimizer of the auxiliary function,   x, should
be an approximate solution of the original problem. intuition suggests that the larger the
penalty weight   , the better the approximation   x to a solution of the original problem.
suppose   x is a minimizer of   . show how to    nd, from   x, a dual feasible point for (5.108).
find the corresponding lower bound on the optimal value of (5.108).

5.15 consider the problem

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m,

(5.109)

where the functions fi : rn     r are di   erentiable and convex. let h1, . . . , hm : r     r
be increasing di   erentiable convex functions. show that

  (x) = f0(x) +

hi(fi(x))

mxi=1

is convex. suppose   x minimizes   . show how to    nd from   x a feasible point for the dual
of (5.109). find the corresponding lower bound on the optimal value of (5.109).

5.16 an exact penalty method for inequality constraints. consider the problem

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m,

(5.110)

where the functions fi : rn     r are di   erentiable and convex.
method, we solve the auxiliary problem

in an exact penalty

minimize   (x) = f0(x) +    maxi=1,...,m max{0, fi(x)},

(5.111)

where    > 0 is a parameter. the second term in    penalizes deviations of x from feasibility.
the method is called an exact penalty method if for su   ciently large   , solutions of the
auxiliary problem (5.111) also solve the original problem (5.110).

(a) show that    is convex.

278

5 duality

(b) the auxiliary problem can be expressed as

minimize
subject to

f0(x) +   y
fi(x)     y,
0     y

i = 1, . . . , m

where the variables are x and y     r. find the lagrange dual of this problem, and
express it in terms of the lagrange dual function g of (5.110).
(c) use the result in (b) to prove the following property. suppose       is an optimal
solution of the lagrange dual of (5.110), and that strong duality holds.
if    >
1t      , then any solution of the auxiliary problem (5.111) is also an optimal solution
of (5.110).

5.17 robust id135 with polyhedral uncertainty. consider the robust lp

minimize
subject to

ct x
supa   pi

at x     bi,

i = 1, . . . , m,

with variable x     rn, where pi = {a | cia (cid:22) di}. the problem data are c     rn,
ci     rmi  n, di     rmi , and b     rm. we assume the polyhedra pi are nonempty.
show that this problem is equivalent to the lp

minimize
subject to

ct x
dt
i zi     bi,
c t
i zi = x,
zi (cid:23) 0,

i = 1, . . . , m
i = 1, . . . , m

i = 1, . . . , m

with variables x     rn and zi     rmi , i = 1, . . . , m. hint. find the dual of the problem
of maximizing at
5.18 separating hyperplane between two polyhedra. formulate the following problem as an lp
or an lp feasibility problem. find a separating hyperplane that strictly separates two
polyhedra

i x over ai     pi (with variable ai).

i.e.,    nd a vector a     rn and a scalar    such that

p1 = {x | ax (cid:22) b},

p2 = {x | cx (cid:22) d},

at x >    for x     p1,

at x <    for x     p2.

you can assume that p1 and p2 do not intersect.
hint. the vector a and scalar    must satisfy

use lp duality to simplify the in   mum and supremum in these conditions.

inf
x   p1

at x >    > sup
x   p2

at x.

5.19 the sum of the largest elements of a vector. de   ne f : rn     r as

f (x) =

x[i],

rxi=1

where r is an integer between 1 and n, and x[1]     x[2]                x[r] are the components of
x sorted in decreasing order. in other words, f (x) is the sum of the r largest elements of
x. in this problem we study the constraint

f (x)       .

as we have seen in chapter 3, page 80, this is a convex constraint, and equivalent to a set
of n!/(r!(n     r)!) linear inequalities

xi1 +        + xir       ,

1     i1 < i2 <        < ir     n.

the purpose of this problem is to derive a more compact representation.

exercises

279

(a) given a vector x     rn, show that f (x) is equal to the optimal value of the lp

maximize
subject to

xt y
0 (cid:22) y (cid:22) 1
1t y = r

with y     rn as variable.

(b) derive the dual of the lp in part (a). show that it can be written as

minimize
subject to

rt + 1t u
t1 + u (cid:23) x
u (cid:23) 0,

where the variables are t     r, u     rn. by duality this lp has the same optimal
value as the lp in (a), i.e., f (x). we therefore have the following result: x satis   es
f (x)        if and only if there exist t     r, u     rn such that

rt + 1t u       ,

t1 + u (cid:23) x,

u (cid:23) 0.

these conditions form a set of 2n + 1 linear inequalities in the 2n + 1 variables x, u, t.

(c) as an application, we consider an extension of the classical markowitz portfolio

optimization problem

minimize
subject to

xt   x
pt x     rmin
1t x = 1,

x (cid:23) 0

discussed in chapter 4, page 155. the variable is the portfolio x     rn; p and    are
the mean and covariance matrix of the price change vector p.
suppose we add a diversi   cation constraint, requiring that no more than 80% of
the total budget can be invested in any 10% of the assets. this constraint can be
expressed as

   0.1n   xi=1

x[i]     0.8.

formulate the portfolio optimization problem with diversi   cation constraint as a
qp.

5.20 dual of channel capacity problem. derive a dual for the problem

minimize    ct x +pm

subject to p x = y
x (cid:23) 0,

1t x = 1,

i=1 yi log yi

1). the variables are x     rn, y     rm. (for cj =pm

where p     rm  n has nonnegative elements, and its columns add up to one (i.e., p t 1 =
i=1 pij log pij, the optimal value is,
up to a factor log 2, the negative of the capacity of a discrete memoryless channel with
channel transition id203 matrix p ; see exercise 4.57.)
simplify the dual problem as much as possible.

280

5 duality

strong duality and slater   s condition

5.21 a convex problem in which strong duality fails. consider the optimization problem

minimize
subject to x2/y     0
with variables x and y, and domain d = {(x, y) | y > 0}.
(a) verify that this is a id76 problem. find the optimal value.
(b) give the lagrange dual problem, and    nd the optimal solution       and optimal value

e   x

d    of the dual problem. what is the optimal duality gap?

(c) does slater   s condition hold for this problem?
(d) what is the optimal value p   (u) of the perturbed problem

e   x

minimize
subject to x2/y     u

as a function of u? verify that the global sensitivity inequality

does not hold.

p   (u)     p   (0)          u

5.22 geometric interpretation of duality. for each of the following optimization problems,

draw a sketch of the sets

g = {(u, t) |    x     d, f0(x) = t, f1(x) = u},
a = {(u, t) |    x     d, f0(x)     t, f1(x)     u},

give the dual problem, and solve the primal and id78. is the problem convex?
is slater   s condition satis   ed? does strong duality hold?
the domain of the problem is r unless otherwise stated.
(a) minimize x subject to x2     1.
(b) minimize x subject to x2     0.
(c) minimize x subject to |x|     0.
(d) minimize x subject to f1(x)     0 where

f1(x) =(    x + 2 x     1

x
   x     2 x        1.

   1     x     1

(e) minimize x3 subject to    x + 1     0.
(f) minimize x3 subject to    x + 1     0 with domain d = r+.

5.23 strong duality in id135. we prove that strong duality holds for the lp

and its dual

ct x

minimize
subject to ax (cid:22) b

maximize    bt z
subject to at z + c = 0,

z (cid:23) 0,

provided at least one of the problems is feasible. in other words, the only possible excep-
tion to strong duality occurs when p    =     and d    =       .

exercises

281

(a) suppose p    is    nite and x    is an optimal solution. (if    nite, the optimal value of an

lp is attained.) let i     {1, 2, . . . , m} be the set of active constraints at x   :

at
i x    < bi,
show that there exists a z     rm that satis   es

at
i x    = bi,

i     i,

zi     0,

i     i,

zi = 0,

i 6    i.

ziai + c = 0.

i 6    i, xi   i

show that z is dual optimal with objective value ct x   .

hint. assume there exists no such z, i.e.,    c 6    {pi   i ziai | zi     0}. reduce

this to a contradiction by applying the strict separating hyperplane theorem of
example 2.20, page 49. alternatively, you can use farkas    lemma (see   5.8.3).
(b) suppose p    =     and the dual problem is feasible. show that d    =    . hint. show
that there exists a nonzero v     rm such that at v = 0, v (cid:23) 0, bt v < 0. if the dual
is feasible, it is unbounded in the direction v.

(c) consider the example

x

minimize

subject to (cid:20) 0

1 (cid:21) x (cid:22)(cid:20)    1
1 (cid:21) .

formulate the dual lp, and solve the primal and id78. show that p    =    
and d    =       .

5.24 weak max-min inequality. show that the weak max-min inequality

sup
z   z

inf
w   w

f (w, z)     inf

w   w

sup
z   z

f (w, z)

always holds, with no assumptions on f : rn    rm     r, w     rn, or z     rm.

5.25 [bl00, page 95] convex-concave functions and the saddle-point property. we derive con-

ditions under which the saddle-point property

sup
z   z

inf
w   w

f (w, z) = inf
w   w

sup
z   z

f (w, z)

(5.112)

holds, where f : rn    rm     r, w    z     dom f , and w and z are nonempty. we will
assume that the function

is closed and convex for all z     z, and the function

otherwise

   

gz(w) =(cid:26) f (w, z) w     w
hw(z) =(cid:26)    f (w, z)

   

z     z
otherwise

is closed and convex for all w     w .
(a) the righthand side of (5.112) can be expressed as p(0), where

p(u) = inf
w   w

sup
z   z

(f (w, z) + ut z).

show that p is a convex function.

282

5 duality

(b) show that the conjugate of p is given by

p   (v) =(cid:26)     inf w   w f (w, v)

   

v     z
otherwise.

(c) show that the conjugate of p    is given by

p      (u) = sup
z   z

inf
w   w

(f (w, z) + ut z).

combining this with (a), we can express the max-min equality (5.112) as p      (0) =
p(0).

conclude that this is the case if w and z are bounded.

(d) from exercises 3.28 and 3.39 (d), we know that p      (0) = p(0) if 0     int dom p.
(e) as another consequence of exercises 3.28 and 3.39, we have p      (0) = p(0) if 0    
dom p and p is closed. show that p is closed if the sublevel sets of gz are bounded.

optimality conditions

5.26 consider the qcqp

minimize
subject to

1 + x2
x2
2
(x1     1)2 + (x2     1)2     1
(x1     1)2 + (x2 + 1)2     1

with variable x     r2.
(a) sketch the feasible set and level sets of the objective. find the optimal point x    and
optimal value p   .

(b) give the kkt conditions. do there exist lagrange multipliers      

1 and      

2 that prove

that x    is optimal?

(c) derive and solve the lagrange dual problem. does strong duality hold?

5.27 equality constrained least-squares. consider the equality constrained least-squares prob-

lem

minimize
subject to gx = h

kax     bk2

2

where a     rm  n with rank a = n, and g     rp  n with rank g = p.
give the kkt conditions, and derive expressions for the primal solution x    and the dual
solution      .

5.28 prove (without using any id135 code) that the optimal solution of the lp

minimize

subject to                

47x1 + 93x2 + 17x3     93x4

1
7

   6
   1
3
   2
   1
1
3    10    1
0
   2
   6    11
12
   1    3
6
1

               

x1
x2
x3
x4

         

          (cid:22)

               

   3
5
   8
   7
4

               

is unique, and given by x    = (1, 1, 1, 1).

5.29 the problem

minimize    3x2
subject to x2

1 + x2

1 + x2

2 + x2

2 + 2x2
3 = 1,

3 + 2(x1 + x2 + x3)

is a special case of (5.32), so strong duality holds even though the problem is not convex.
derive the kkt conditions. find all solutions x,    that satisfy the kkt conditions.
which pair corresponds to the optimum?

exercises

283

5.30 derive the kkt conditions for the problem

minimize
subject to xs = y,

tr x     log det x

with variable x     sn and domain sn
verify that the optimal solution is given by

++. y     rn and s     rn are given, with st y = 1.

x     = i + yyt    

1

st s

sst .

5.31 supporting hyperplane interpretation of kkt conditions. consider a convex problem with

no equality constraints,

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m.

assume that x        rn and           rm satisfy the kkt conditions

   f0(x   ) +pm

i=1      

show that

fi(x   )     0,
     
i     0,
     
i fi(x   ) = 0,
i    fi(x   ) = 0.

i = 1, . . . , m
i = 1, . . . , m
i = 1, . . . , m

   f0(x   )t (x     x   )     0

for all feasible x. in other words the kkt conditions imply the simple optimality criterion
of   4.2.3.

perturbation and sensitivity analysis

5.32 optimal value of perturbed problem. let f0, f1, . . . , fm : rn     r be convex. show that

the function

p   (u, v) = inf{f0(x) |    x     d, fi(x)     ui, i = 1, . . . , m, ax     b = v}

is convex. this function is the optimal cost of the perturbed problem, as a function of
the perturbations u and v (see   5.6.1).

5.33 parametrized    1-norm approximation. consider the    1-norm minimization problem

minimize

kax + b +   dk1

with variable x     r3, and

a =

,

b =

,

d =

                     

   2
1
7
   5    1
3
3    5
   7
4    4
   1
5
5
1
2    5    1

                     

   4
3
9
0
   11
5

                     

                     

   10
   13
   27
   10
   7
14

                     

.

                     

we denote by p   (  ) the optimal value as a function of   .

(a) suppose    = 0. prove that x    = 1 is optimal. are there any other optimal points?
(b) show that p   (  ) is a   ne on an interval that includes    = 0.

284

5 duality

5.34 consider the pair of primal and dual lps

(c +   d)t x
minimize
subject to ax (cid:22) b +   f

maximize    (b +   f )t z
subject to at z + c +   d = 0

z (cid:23) 0

and

where

a =               

   4
   17
1
3
   11

12    2
1
7
12
11
0    6
1
22    1
3
2    1    8

,

               

b =               

8
13
   4
27
   18

,

               

f =               

6
15
   13
48
8

,

               

c = (49,   34,   50,   5), d = (3, 8, 21, 25), and    is a parameter.
(a) prove that x    = (1, 1, 1, 1) is optimal when    = 0, by constructing a dual optimal
point z    that has the same objective value as x   . are there any other primal or dual
optimal solutions?

(b) give an explicit expression for the optimal value p   (  ) as a function of    on an
interval that contains    = 0. specify the interval on which your expression is valid.
also give explicit expressions for the primal solution x   (  ) and the dual solution
z   (  ) as a function of   , on the same interval.
hint. first calculate x   (  ) and z   (  ), assuming that the primal and dual constraints
that are active at the optimum for    = 0, remain active at the optimum for values
of    around 0. then verify that this assumption is correct.

5.35 sensitivity analysis for gps. consider a gp

minimize
subject to

f0(x)
fi(x)     1,
hi(x) = 1,

i = 1, . . . , m
i = 1, . . . , p,

where f0, . . . , fm are posynomials, h1, . . . , hp are monomials, and the domain of the prob-
lem is rn

++. we de   ne the perturbed gp as

minimize
subject to

f0(x)
fi(x)     eui ,
hi(x) = evi ,

i = 1, . . . , m
i = 1, . . . , p,

and we denote the optimal value of the perturbed gp as p   (u, v). we can think of ui and
vi as relative, or fractional, perturbations of the constraints. for example, u1 =    0.01
corresponds to tightening the    rst inequality constraint by (approximately) 1%.
let       and       be optimal dual variables for the convex form gp

minimize
subject to

log f0(y)
log fi(y)     0,
log hi(y) = 0,

i = 1, . . . , m
i = 1, . . . , p,

with variables yi = log xi. assuming that p   (u, v) is di   erentiable at u = 0, v = 0, relate
      and       to the derivatives of p   (u, v) at u = 0, v = 0. justify the statement    relaxing
the ith constraint by    percent will give an improvement in the objective of around        
i
percent, for    small.   

exercises

theorems of alternatives

285

5.36 alternatives for linear equalities. consider the linear equations ax = b, where a     rm  n.
from id202 we know that this equation has a solution if and only b     r(a), which
occurs if and only if b     n (at ). in other words, ax = b has a solution if and only if
there exists no y     rm such that at y = 0 and bt y 6= 0.
derive this result from the theorems of alternatives in   5.8.2.

5.37 [bt97] existence of equilibrium distribution in    nite state markov chain. let p     rn  n

be a matrix that satis   es

pij     0,

i, j = 1, . . . , n,

p t 1 = 1,

i.e., the coe   cients are nonnegative and the columns sum to one. use farkas    lemma to
prove there exists a y     rn such that
p y = y,

1t y = 1.

y (cid:23) 0,

(we can interpret y as an equilibrium distribution of the markov chain with n states and
transition id203 matrix p .)

5.38 [bt97] option pricing. we apply the results of example 5.10, page 263, to a simple
problem with three assets: a riskless asset with    xed return r > 1 over the investment
period of interest (for example, a bond), a stock, and an option on the stock. the option
gives us the right to purchase the stock at the end of the period, for a predetermined
price k.
we consider two scenarios.
in the    rst scenario, the price of the stock goes up from
s at the beginning of the period, to su at the end of the period, where u > r. in this
scenario, we exercise the option only if su > k, in which case we make a pro   t of su    k.
otherwise, we do not exercise the option, and make zero pro   t. the value of the option
at the end of the period, in the    rst scenario, is therefore max{0, su     k}.
in the second scenario, the price of the stock goes down from s to sd, where d < 1. the
value at the end of the period is max{0, sd     k}.
in the notation of example 5.10,

v =(cid:20) r us max{0, su     k}
ds max{0, sd     k} (cid:21) ,

r

p1 = 1,

p2 = s,

p3 = c,

where c is the price of the option.
show that for given r, s, k, u, d, the option price c is uniquely determined by the
no-arbitrage condition. in other words, the market for the option is complete.

generalized inequalities

5.39 sdp relaxations of two-way partitioning problem. we consider the two-way partitioning

problem (5.7), described on page 219,

minimize
subject to x2

xt w x
i = 1,

i = 1, . . . , n,

(5.113)

with variable x     rn. the lagrange dual of this (nonconvex) problem is given by the
sdp

maximize    1t   
subject to w + diag(  ) (cid:23) 0

(5.114)

with variable        rn. the optimal value of this sdp gives a lower bound on the optimal
value of the partitioning problem (5.113). in this exercise we derive another sdp that
gives a lower bound on the optimal value of the two-way partitioning problem, and explore
the connection between the two sdps.

286

5 duality

(a) two-way partitioning problem in matrix form. show that the two-way partitioning

problem can be cast as

minimize
tr(w x)
subject to x (cid:23) 0,
xii = 1,

rank x = 1

i = 1, . . . , n,

with variable x     sn. hint. show that if x is feasible, then it has the form
x = xxt , where x     rn satis   es xi     {   1, 1} (and vice versa).
(b) sdp relaxation of two-way partitioning problem. using the formulation in part (a),
we can form the relaxation

tr(w x)

minimize
subject to x (cid:23) 0
xii = 1,

i = 1, . . . , n,

(5.115)

with variable x     sn. this problem is an sdp, and therefore can be solved e   -
ciently. explain why its optimal value gives a lower bound on the optimal value of
the two-way partitioning problem (5.113). what can you say if an optimal point
x     for this sdp has rank one?

(c) we now have two sdps that give a lower bound on the optimal value of the two-way
partitioning problem (5.113): the sdp relaxation (5.115) found in part (b), and the
lagrange dual of the two-way partitioning problem, given in (5.114). what is the
relation between the two sdps? what can you say about the lower bounds found
by them? hint: relate the two sdps via duality.

5.40 e-optimal experiment design. a variation on the two optimal experiment design problems

of exercise 5.10 is the e-optimal design problem

minimize
subject to x (cid:23) 0,

  max(cid:0)pp

i=1 xivivt
1t x = 1.

i(cid:1)   1

(see also   7.5.) derive a dual for this problem, by    rst reformulating it as

1/t

minimize

subject to pp

x (cid:23) 0,

i=1 xivivt

i (cid:23) ti
1t x = 1,

with variables t     r, x     rp and domain r++    rp, and applying lagrange duality.
simplify the dual problem as much as you can.

5.41 dual of fastest mixing markov chain problem. on page 174, we encountered the sdp

t

minimize
subject to    ti (cid:22) p     (1/n)11t (cid:22) ti
i, j = 1, . . . , n

p 1 = 1
pij     0,
pij = 0 for (i, j) 6    e,

with variables t     r, p     sn.
show that the dual of this problem can be expressed as
1t z     (1/n)1t y 1
ky k2        1
(zi + zj)     yij for (i, j)     e

maximize
subject to

with variables z     rn and y     sn. the norm k    k2    is the dual of the spectral norm
i=1 |  i(y )|, the sum of the absolute values of the eigenvalues of y .
(see   a.1.6, page 637.)

on sn: ky k2    =pn

exercises

287

5.42 lagrange dual of conic form problem in inequality form. find the lagrange dual problem

of the conic form problem in inequality form

ct x

minimize
subject to ax (cid:22)k b

where a     rm  n, b     rm, and k is a proper cone in rm. make any implicit equality
constraints explicit.

5.43 dual of socp. show that the dual of the socp

minimize
subject to

f t x
kaix + bik2     ct

i x + di,

i = 1, . . . , m,

with variables x     rn, can be expressed as

maximize pm
subject to pm

i=1(bt
i=1(at
kuik2     vi,

i ui     divi)
i ui     civi) + f = 0
i = 1, . . . , m,

with variables ui     rni , vi     r, i = 1, . . . , m. the problem data are f     rn, ai     rni  n,
bi     rni , ci     r and di     r, i = 1, . . . , m.
derive the dual in the following two ways.
(a) introduce new variables yi     rni and ti     r and equalities yi = aix + bi, ti =

ct
i x + di, and derive the lagrange dual.

(b) start from the conic formulation of the socp and use the conic dual. use the fact

that the second-order cone is self-dual.

5.44 strong alternatives for nonstrict lmis.

in example 5.14, page 270, we mentioned that

the system

tr(gz) > 0,

tr(fiz) = 0,

i = 1, . . . , n,

(5.116)

z (cid:23) 0,

is a strong alternative for the nonstrict lmi

if the matrices fi satisfy

f (x) = x1f1 +        + xnfn + g (cid:22) 0,

(5.117)

nxi=1

vifi (cid:23) 0 =   

nxi=1

vifi = 0.

(5.118)

in this exercise we prove this result, and give an example to illustrate that the systems
are not always strong alternatives.

(a) suppose (5.118) holds, and that the optimal value of the auxiliary sdp

s

minimize
subject to f (x) (cid:22) si

is positive. show that the optimal value is attained. if follows from the discussion
in   5.9.4 that the systems (5.117) and (5.116) are strong alternatives.
hint. the proof simpli   es if you assume, without loss of generality, that the matrices
i=1 vifi (cid:23) 0     v = 0.

f1, . . . , fn are independent, so (5.118) may be replaced bypn
1 (cid:21) .

f1 =(cid:20) 0

g =(cid:20) 0

0 (cid:21) ,

1

0

1

0

show that (5.117) and (5.116) are both infeasible.

(b) take n = 1, and

part ii

applications

chapter 6

approximation and    tting

6.1 norm approximation

6.1.1 basic norm approximation problem

the simplest norm approximation problem is an unconstrained problem of the form

minimize

kax     bk

(6.1)

where a     rm  n and b     rm are problem data, x     rn is the variable, and k  k is
a norm on rm. a solution of the norm approximation problem is sometimes called
an approximate solution of ax     b, in the norm k    k. the vector

r = ax     b

is called the residual for the problem; its components are sometimes called the
individual residuals associated with x.

the norm approximation problem (6.1) is a convex problem, and is solvable,
i.e., there is always at least one optimal solution.
its optimal value is zero if
and only if b     r(a); the problem is more interesting and useful, however, when
b 6    r(a). we can assume without loss of generality that the columns of a are
independent; in particular, that m     n. when m = n the optimal point is simply
a   1b, so we can assume that m > n.

approximation interpretation

by expressing ax as

ax = x1a1 +        + xnan,

where a1, . . . , an     rm are the columns of a, we see that the goal of the norm
approximation problem is to    t or approximate the vector b by a linear combination
of the columns of a, as closely as possible, with deviation measured in the norm
k    k.
the approximation problem is also called the regression problem. in this context
the vectors a1, . . . , an are called the regressors, and the vector x1a1 +        + xnan,

292

6 approximation and    tting

where x is an optimal solution of the problem, is called the regression of b (onto
the regressors).

estimation interpretation

a closely related interpretation of the norm approximation problem arises in the
problem of estimating a parameter vector on the basis of an imperfect linear vector
measurement. we consider a linear measurement model

y = ax + v,

where y     rm is a vector measurement, x     rn is a vector of parameters to be
estimated, and v     rm is some measurement error that is unknown, but presumed
to be small (in the norm k  k). the estimation problem is to make a sensible guess
as to what x is, given y.
if we guess that x has the value   x, then we are implicitly making the guess that
v has the value y     a  x. assuming that smaller values of v (measured by k    k) are
more plausible than larger values, the most plausible guess for x is

  x = argminzkaz     yk.

(these ideas can be expressed more formally in a statistical framework; see chap-
ter 7.)

geometric interpretation
we consider the subspace a = r(a)     rm, and a point b     rm. a projection of
the point b onto the subspace a, in the norm k    k, is any point in a that is closest
to b, i.e., any optimal point for the problem

ku     bk
minimize
subject to u     a.

parametrizing an arbitrary element of r(a) as u = ax, we see that solving the
norm approximation problem (6.1) is equivalent to computing a projection of b
onto a.
design interpretation

we can interpret the norm approximation problem (6.1) as a problem of optimal
design. the n variables x1, . . . , xn are design variables whose values are to be
determined. the vector y = ax gives a vector of m results, which we assume to
be linear functions of the design variables x. the vector b is a vector of target or
desired results. the goal is to choose a vector of design variables that achieves, as
closely as possible, the desired results, i.e., ax     b. we can interpret the residual
vector r as the deviation between the actual results (i.e., ax) and the desired
or target results (i.e., b). if we measure the quality of a design by the norm of
the deviation between the actual results and the desired results, then the norm
approximation problem (6.1) is the problem of    nding the best design.

6.1 norm approximation

293

weighted norm approximation problems

an extension of the norm approximation problem is the weighted norm approxima-
tion problem

minimize

kw (ax     b)k

where the problem data w     rm  m is called the weighting matrix. the weight-
ing matrix is often diagonal, in which case it gives di   erent relative emphasis to
di   erent components of the residual vector r = ax     b.
the weighted norm problem can be considered as a norm approximation prob-
lem with norm k  k, and data   a = w a,   b = w b, and therefore treated as a standard
norm approximation problem (6.1). alternatively, the weighted norm approxima-
tion problem can be considered a norm approximation problem with data a and
b, and the w -weighted norm de   ned by

kzkw = kw zk

(assuming here that w is nonsingular).

least-squares approximation

the most common norm approximation problem involves the euclidean or    2-
norm. by squaring the objective, we obtain an equivalent problem which is called
the least-squares approximation problem,
kax     bk2

2 +        + r2
m,

minimize

2 = r2

1 + r2

where the objective is the sum of squares of the residuals. this problem can be
solved analytically by expressing the objective as the convex quadratic function

f (x) = xt at ax     2bt ax + bt b.

a point x minimizes f if and only if

i.e., if and only if x satis   es the so-called normal equations

   f (x) = 2at ax     2at b = 0,

at ax = at b,

which always have a solution. since we assume the columns of a are independent,
the least-squares approximation problem has the unique solution x = (at a)   1at b.

chebyshev or minimax approximation

when the       -norm is used, the norm approximation problem
kax     bk    = max{|r1|, . . . ,|rm|}

minimize

is called the chebyshev approximation problem, or minimax approximation problem,
since we are to minimize the maximum (absolute value) residual. the chebyshev
approximation problem can be cast as an lp

t

minimize
subject to    t1 (cid:22) ax     b (cid:22) t1,

with variables x     rn and t     r.

294

6 approximation and    tting

sum of absolute residuals approximation

when the    1-norm is used, the norm approximation problem

minimize

kax     bk1 = |r1| +        + |rm|

is called the sum of (absolute) residuals approximation problem, or, in the context
of estimation, a robust estimator (for reasons that will be clear soon). like the
chebyshev approximation problem, the    1-norm approximation problem can be
cast as an lp

1t t

minimize
subject to    t (cid:22) ax     b (cid:22) t,

with variables x     rn and t     rm.

6.1.2 penalty function approximation

in    p-norm approximation, for 1     p <    , the objective is

(|r1|p +        + |rm|p)1/p .

as in least-squares problems, we can consider the equivalent problem with objective

|r1|p +        + |rm|p,

which is a separable and symmetric function of the residuals. in particular, the
objective depends only on the amplitude distribution of the residuals, i.e., the
residuals in sorted order.

we will consider a useful generalization of the    p-norm approximation problem,
in which the objective depends only on the amplitude distribution of the residuals.
the penalty function approximation problem has the form

minimize
subject to r = ax     b,

  (r1) +        +   (rm)

(6.2)

where    : r     r is called the (residual) penalty function. we assume that    is
convex, so the penalty function approximation problem is a id76
problem. in many cases, the penalty function    is symmetric, nonnegative, and
satis   es   (0) = 0, but we will not use these properties in our analysis.

interpretation

we can interpret the penalty function approximation problem (6.2) as follows. for
the choice x, we obtain the approximation ax of b, which has the associated resid-
ual vector r. a penalty function assesses a cost or penalty for each component
of residual, given by   (ri); the total penalty is the sum of the penalties for each
residual, i.e.,   (r1) +        +   (rm). di   erent choices of x lead to di   erent resulting
residuals, and therefore, di   erent total penalties. in the penalty function approxi-
mation problem, we minimize the total penalty incurred by the residuals.

6.1 norm approximation

295

2

1.5

)
u
(
  

1

0.5

log barrier

quadratic

deadzone-linear

0
   1.5

   1

   0.5

0
u

0.5

1

1.5

figure 6.1 some common penalty functions: the quadratic penalty function
  (u) = u2, the deadzone-linear penalty function with deadzone width a =
1/4, and the log barrier penalty function with limit a = 1.

example 6.1 some common penalty functions and associated approximation problems.

    by taking   (u) = |u|p, where p     1, the penalty function approximation prob-
in particular, the
lem is equivalent to the    p-norm approximation problem.
quadratic penalty function   (u) = u2 yields least-squares or euclidean norm
approximation, and the absolute value penalty function   (u) = |u| yields    1-
norm approximation.

    the deadzone-linear penalty function (with deadzone width a > 0) is given by

  (u) =(cid:26) 0

|u|     a

|u|     a
|u| > a.

the deadzone-linear function assesses no penalty for residuals smaller than a.

    the log barrier penalty function (with limit a > 0) has the form

  (u) =(cid:26)    a2 log(1     (u/a)2)

   

|u| < a
|u|     a.

the log barrier penalty function assesses an in   nite penalty for residuals larger
than a.

a deadzone-linear, log barrier, and quadratic penalty function are plotted in    g-
ure 6.1. note that the log barrier function is very close to the quadratic penalty for
|u/a|     0.25 (see exercise 6.1).

scaling the penalty function by a positive number does not a   ect the solution of
the penalty function approximation problem, since this merely scales the objective

296

6 approximation and    tting

function. but the shape of the penalty function has a large e   ect on the solution of
the penalty function approximation problem. roughly speaking,   (u) is a measure
of our dislike of a residual of value u. if    is very small (or even zero) for small
values of u, it means we care very little (or not at all) if residuals have these values.
if   (u) grows rapidly as u becomes large, it means we have a strong dislike for
large residuals; if    becomes in   nite outside some interval, it means that residuals
outside the interval are unacceptable. this simple interpretation gives insight into
the solution of a penalty function approximation problem, as well as guidelines for
choosing a penalty function.

as an example, let us compare    1-norm and    2-norm approximation, associ-
ated with the penalty functions   1(u) = |u| and   2(u) = u2, respectively. for
|u| = 1, the two penalty functions assign the same penalty. for small u we have
  1(u)       2(u), so    1-norm approximation puts relatively larger emphasis on small
residuals compared to    2-norm approximation. for large u we have   2(u)       1(u),
so    1-norm approximation puts less weight on large residuals, compared to    2-norm
approximation. this di   erence in relative weightings for small and large residuals
is re   ected in the solutions of the associated approximation problems. the ampli-
tude distribution of the optimal residual for the    1-norm approximation problem
will tend to have more zero and very small residuals, compared to the    2-norm ap-
proximation solution. in contrast, the    2-norm solution will tend to have relatively
fewer large residuals (since large residuals incur a much larger penalty in    2-norm
approximation than in    1-norm approximation).

example
an example will illustrate these ideas. we take a matrix a     r100  30 and vector
b     r100 (chosen at random, but the results are typical), and compute the    1-norm
and    2-norm approximate solutions of ax     b, as well as the penalty function
approximations with a deadzone-linear penalty (with a = 0.5) and log barrier
penalty (with a = 1). figure 6.2 shows the four associated penalty functions,
and the amplitude distributions of the optimal residuals for these four penalty
approximations. from the plots of the penalty functions we note that

    the    1-norm penalty puts the most weight on small residuals and the least

weight on large residuals.

    the    2-norm penalty puts very small weight on small residuals, but strong

weight on large residuals.

    the deadzone-linear penalty function puts no weight on residuals smaller

than 0.5, and relatively little weight on large residuals.

    the log barrier penalty puts weight very much like the    2-norm penalty for
small residuals, but puts very strong weight on residuals larger than around
0.8, and in   nite weight on residuals larger than 1.

several features are clear from the amplitude distributions:

    for the    1-optimal solution, many residuals are either zero or very small. the

   1-optimal solution also has relatively more large residuals.

6.1 norm approximation

297

1
=
p

2
=
p

e
n
o
z
d
a
e
d

r
e
i
r
r
a
b

g
o
l

40

0
   2
10

0
   2
20

0
   2
10

0
   2

   1

   1

   1

   1

0

0

0

0
r

1

1

1

1

2

2

2

2

figure 6.2 histogram of residual amplitudes for four penalty functions, with
the (scaled) penalty functions also shown for reference. for the log barrier
plot, the quadratic penalty is also shown, in dashed curve.

298

6 approximation and    tting

1.5

1

0.5

)
u
(
  

0
   1.5    1    0.5

0
u

0.5

1

1.5

figure 6.3 a (nonconvex) penalty function that assesses a    xed penalty to
residuals larger than a threshold (which in this example is one):   (u) = u2
if |u|     1 and   (u) = 1 if |u| > 1. as a result, penalty approximation with
this function would be relatively insensitive to outliers.

    the    2-norm approximation has many modest residuals, and relatively few

larger ones.

    for the deadzone-linear penalty, we see that many residuals have the value

  0.5, right at the edge of the    free    zone, for which no penalty is assessed.

    for the log barrier penalty, we see that no residuals have a magnitude larger
than 1, but otherwise the residual distribution is similar to the residual dis-
tribution for    2-norm approximation.

sensitivity to outliers or large errors

in the estimation or regression context, an outlier is a measurement yi = at
i x + vi
for which the noise vi is relatively large. this is often associated with faulty data
or a    awed measurement. when outliers occur, any estimate of x will be associated
with a residual vector with some large components. ideally we would like to guess
which measurements are outliers, and either remove them from the estimation
process or greatly lower their weight in forming the estimate. (we cannot, however,
assign zero penalty for very large residuals, because then the optimal point would
likely make all residuals large, which yields a total penalty of zero.) this could be
accomplished using penalty function approximation, with a penalty function such
as

  (u) =(cid:26) u2

m 2

|u|     m
|u| > m,

(6.3)

shown in    gure 6.3. this penalty function agrees with least-squares for any residual
smaller than m , but puts a    xed weight on any residual larger than m , no matter
how much larger it is. in other words, residuals larger than m are ignored; they
are assumed to be associated with outliers or bad data. unfortunately, the penalty

6.1 norm approximation

299

2

1.5

1

0.5

)
u
(
b
u
h
  

0
   1.5    1    0.5

0
u

0.5

1

1.5

figure 6.4 the solid line is the robust least-squares or huber penalty func-
tion   hub, with m = 1. for |u|     m it is quadratic, and for |u| > m it
grows linearly.

function (6.3) is not convex, and the associated penalty function approximation
problem becomes a hard combinatorial optimization problem.

the sensitivity of a penalty function based estimation method to outliers de-
pends on the (relative) value of the penalty function for large residuals.
if we
restrict ourselves to convex penalty functions (which result in id76
problems), the ones that are least sensitive are those for which   (u) grows linearly,
i.e., like |u|, for large u. penalty functions with this property are sometimes called
robust, since the associated penalty function approximation methods are much less
sensitive to outliers or large errors than, for example, least-squares.

one obvious example of a robust penalty function is   (u) = |u|, corresponding
to    1-norm approximation. another example is the robust least-squares or huber
penalty function, given by

  hub(u) =(cid:26) u2

m (2|u|     m )

|u|     m
|u| > m,

(6.4)

shown in    gure 6.4. this penalty function agrees with the least-squares penalty
function for residuals smaller than m , and then reverts to    1-like linear growth for
larger residuals. the huber penalty function can be considered a convex approx-
imation of the outlier penalty function (6.3), in the following sense: they agree
for |u|     m , and for |u| > m , the huber penalty function is the convex function
closest to the outlier penalty function (6.3).

example 6.2 robust regression. figure 6.5 shows 42 points (ti, yi) in a plane, with
two obvious outliers (one at the upper left, and one at lower right). the dashed line
shows the least-squares approximation of the points by a straight line f (t) =    +   t.
the coe   cients    and    are obtained by solving the least-squares problem

minimize p42

i=1(yi              ti)2,

300

6 approximation and    tting

20

10

0

)
t
(
f

   10

   20

   10

   5

0
t

5

10

figure 6.5 the 42 circles show points that can be well approximated by
an a   ne function, except for the two outliers at upper left and lower right.
the dashed line is the least-squares    t of a straight line f (t) =    +   t
to the points, and is rotated away from the main locus of points, toward
the outliers. the solid line shows the robust least-squares    t, obtained by
minimizing huber   s penalty function with m = 1. this gives a far better    t
to the non-outlier data.

with variables    and   . the least-squares approximation is clearly rotated away from
the main locus of the points, toward the two outliers.

the solid line shows the robust least-squares approximation, obtained by minimizing
the huber penalty function

with m = 1. this approximation is far less a   ected by the outliers.

minimize p42

i=1   hub(yi              ti),

since    1-norm approximation is among the (convex) penalty function approxi-
mation methods that are most robust to outliers,    1-norm approximation is some-
times called robust estimation or robust regression. the robustness property of
   1-norm estimation can also be understood in a statistical framework; see page 353.

small residuals and    1-norm approximation

we can also focus on small residuals. least-squares approximation puts very small
weight on small residuals, since   (u) = u2 is very small when u is small. penalty
functions such as the deadzone-linear penalty function put zero weight on small
residuals. for penalty functions that are very small for small residuals, we expect
the optimal residuals to be small, but not very small. roughly speaking, there is
little or no incentive to drive small residuals smaller.

in contrast, penalty functions that put relatively large weight on small residuals,
such as   (u) = |u|, corresponding to    1-norm approximation, tend to produce

6.1 norm approximation

301

optimal residuals many of which are very small, or even exactly zero. this means
that in    1-norm approximation, we typically    nd that many of the equations are
satis   ed exactly, i.e., we have at
i x = bi for many i. this phenomenon can be seen
in    gure 6.2.

6.1.3 approximation with constraints

it is possible to add constraints to the basic norm approximation problem (6.1).
when these constraints are convex, the resulting problem is convex. constraints
arise for a variety of reasons.

    in an approximation problem, constraints can be used to rule out certain un-
acceptable approximations of the vector b, or to ensure that the approximator
ax satis   es certain properties.

    in an estimation problem, the constraints arise as prior knowledge of the
vector x to be estimated, or from prior knowledge of the estimation error v.

    constraints arise in a geometric setting in determining the projection of a
point b on a set more complicated than a subspace, for example, a cone or
polyhedron.

some examples will make these clear.

nonnegativity constraints on variables
we can add the constraint x (cid:23) 0 to the basic norm approximation problem:

minimize
subject to x (cid:23) 0.

kax     bk

in an estimation setting, nonnegativity constraints arise when we estimate a vector
x of parameters known to be nonnegative, e.g., powers, intensities, or rates. the
geometric interpretation is that we are determining the projection of a vector b onto
the cone generated by the columns of a. we can also interpret this problem as
approximating b using a nonnegative linear (i.e., conic) combination of the columns
of a.

variable bounds
here we add the constraint l (cid:22) x (cid:22) u, where l, u     rn are problem parameters:

minimize
subject to

kax     bk
l (cid:22) x (cid:22) u.

in an estimation setting, variable bounds arise as prior knowledge of intervals in
which each variable lies. the geometric interpretation is that we are determining
the projection of a vector b onto the image of a box under the linear mapping
induced by a.

302

6 approximation and    tting

id203 distribution
we can impose the constraint that x satisfy x (cid:23) 0, 1t x = 1:

minimize
subject to x (cid:23) 0,

kax     bk

1t x = 1.

this would arise in the estimation of proportions or relative frequencies, which are
nonnegative and sum to one. it can also be interpreted as approximating b by a
convex combination of the columns of a. (we will have much more to say about
estimating probabilities in   7.2.)
norm ball constraint

we can add to the basic norm approximation problem the constraint that x lie in
a norm ball:

minimize
subject to

kax     bk
kx     x0k     d,

where x0 and d are problem parameters. such a constraint can be added for several
reasons.

    in an estimation setting, x0 is a prior guess of what the parameter x is, and d
is the maximum plausible deviation of our estimate from our prior guess. our
estimate of the parameter x is the value   x which best matches the measured
data (i.e., minimizes kaz     bk) among all plausible candidates (i.e., z that
satisfy kz     x0k     d).

    the constraint kx   x0k     d can denote a trust region. here the linear relation
y = ax is only an approximation of some nonlinear relation y = f (x) that is
valid when x is near some point x0, speci   cally kx     x0k     d. the problem
is to minimize kax     bk but only over those x for which the model y = ax is
trusted.

these ideas also come up in the context of id173; see   6.3.2.

6.2 least-norm problems

the basic least-norm problem has the form

minimize
subject to ax = b

kxk

(6.5)

where the data are a     rm  n and b     rm, the variable is x     rn, and k    k is a
norm on rn. a solution of the problem, which always exists if the linear equations
ax = b have a solution, is called a least-norm solution of ax = b. the least-norm
problem is, of course, a id76 problem.

we can assume without loss of generality that the rows of a are independent, so
m     n. when m = n, the only feasible point is x = a   1b; the least-norm problem
is interesting only when m < n, i.e., when the equation ax = b is underdetermined.

6.2 least-norm problems

303

reformulation as norm approximation problem

the least-norm problem (6.5) can be formulated as a norm approximation problem
by eliminating the equality constraint. let x0 be any solution of ax = b, and let
z     rn  k be a matrix whose columns are a basis for the nullspace of a. the
general solution of ax = b can then be expressed as x0 + zu where u     rk. the
least-norm problem (6.5) can be expressed as

minimize

kx0 + zuk,

with variable u     rk, which is a norm approximation problem.
in particular,
our analysis and discussion of norm approximation problems applies to least-norm
problems as well (when interpreted correctly).

control or design interpretation

we can interpret the least-norm problem (6.5) as a problem of optimal design or
optimal control. the n variables x1, . . . , xn are design variables whose values are
to be determined. in a control setting, the variables x1, . . . , xn represent inputs,
whose values we are to choose. the vector y = ax gives m attributes or results of
the design x, which we assume to be linear functions of the design variables x. the
m < n equations ax = b represent m speci   cations or requirements on the design.
since m < n, the design is underspeci   ed; there are n     m degrees of freedom in
the design (assuming a is rank m).
among all the designs that satisfy the speci   cations, the least-norm problem
chooses the smallest design, as measured by the norm k  k. this can be thought of
as the most e   cient design, in the sense that it achieves the speci   cations ax = b,
with the smallest possible x.

estimation interpretation

we assume that x is a vector of parameters to be estimated. we have m < n
perfect (noise free) linear measurements, given by ax = b. since we have fewer
measurements than parameters to estimate, our measurements do not completely
determine x. any parameter vector x that satis   es ax = b is consistent with our
measurements.

to make a good guess about what x is, without taking further measurements,
we must use prior information. suppose our prior information, or assumption, is
that x is more likely to be small (as measured by k    k) than large. the least-norm
problem chooses as our estimate of the parameter vector x the one that is smallest
(hence, most plausible) among all parameter vectors that are consistent with the
measurements ax = b. (for a statistical interpretation of the least-norm problem,
see page 359.)

geometric interpretation

we can also give a simple geometric interpretation of the least-norm problem (6.5).
the feasible set {x | ax = b} is a   ne, and the objective is the distance (measured
by the norm k    k) between x and the point 0. the least-norm problem    nds the

304

6 approximation and    tting

point in the a   ne set with minimum distance to 0, i.e., it determines the projection
of the point 0 on the a   ne set {x | ax = b}.
least-squares solution of linear equations

the most common least-norm problem involves the euclidean or    2-norm. by
squaring the objective we obtain the equivalent problem

minimize
subject to ax = b,

kxk2

2

the unique solution of which is called the least-squares solution of the equations
ax = b. like the least-squares approximation problem, this problem can be solved
analytically. introducing the dual variable        rm, the optimality conditions are

2x    + at       = 0,

ax    = b,

which is a pair of linear equations, and readily solved. from the    rst equation
we obtain x    =    (1/2)at      ; substituting this into the second equation we obtain
   (1/2)aat       = b, and conclude

      =    2(aat )   1b,

x    = at (aat )   1b.

(since rank a = m < n, the matrix aat is invertible.)

least-penalty problems

a useful variation on the least-norm problem (6.5) is the least-penalty problem

minimize
subject to ax = b,

  (x1) +        +   (xn)

(6.6)

where    : r     r is convex, nonnegative, and satis   es   (0) = 0. the penalty
function value   (u) quanti   es our dislike of a component of x having value u;
the least-penalty problem then    nds x that has least total penalty, subject to the
constraint ax = b.

all of the discussion and interpretation of penalty functions in penalty function
approximation can be transposed to the least-penalty problem, by substituting
the amplitude distribution of x (in the least-penalty problem) for the amplitude
distribution of the residual r (in the penalty approximation problem).

sparse solutions via least    1-norm

recall from the discussion on page 300 that    1-norm approximation gives relatively
large weight to small residuals, and therefore results in many optimal residuals
small, or even zero. a similar e   ect occurs in the least-norm context. the least
   1-norm problem,

minimize
subject to ax = b,

kxk1

tends to produce a solution x with a large number of components equal to zero.
in other words, the least    1-norm problem tends to produce sparse solutions of
ax = b, often with m nonzero components.

6.3 regularized approximation

305

it is easy to    nd solutions of ax = b that have only m nonzero components.
choose any set of m indices (out of 1, . . . , n) which are to be the nonzero com-
ponents of x. the equation ax = b reduces to   a  x = b, where   a is the m    m
submatrix of a obtained by selecting only the chosen columns, and   x     rm is the
subvector of x containing the m selected components. if   a is nonsingular, then
we can take   x =   a   1b, which gives a feasible solution x with m or less nonzero
components. if   a is singular and b 6    r(   a), the equation   a  x = b is unsolvable,
which means there is no feasible x with the chosen set of nonzero components. if
  a is singular and b     r(   a), there is a feasible solution with fewer than m nonzero
components.

this approach can be used to    nd the smallest x with m (or fewer) nonzero
entries, but in general requires examining and comparing all n!/(m!(n   m)!) choices
of m nonzero coe   cients of the n coe   cients in x. solving the least    1-norm
problem, on the other hand, gives a good heuristic for    nding a sparse, and small,
solution of ax = b.

6.3 regularized approximation

6.3.1 bi-criterion formulation

in the basic form of regularized approximation, the goal is to    nd a vector x that
is small (if possible), and also makes the residual ax     b small. this is naturally
described as a (convex) vector optimization problem with two objectives, kax    bk
and kxk:

minimize (w.r.t. r2

+)

(kax     bk,kxk) .

(6.7)

the two norms can be di   erent: the    rst, used to measure the size of the residual,
is on rm; the second, used to measure the size of x, is on rn.

the optimal trade-o    between the two objectives can be found using several
methods. the optimal trade-o    curve of kax     bk versus kxk, which shows how
large one of the objectives must be made to have the other one small, can then be
plotted. one endpoint of the optimal trade-o    curve between kax     bk and kxk
is easy to describe. the minimum value of kxk is zero, and is achieved only when
x = 0. for this value of x, the residual norm has the value kbk.

the other endpoint of the trade-o    curve is more complicated to describe. let
c denote the set of minimizers of kax     bk (with no constraint on kxk). then any
minimum norm point in c is pareto optimal, corresponding to the other endpoint
of the trade-o    curve. in other words, pareto optimal points at this endpoint are
given by minimum norm minimizers of kax    bk. if both norms are euclidean, this
pareto optimal point is unique, and given by x = a   b, where a    is the pseudo-
inverse of a. (see   4.7.6, page 184, and   a.5.4.)

306

6 approximation and    tting

6.3.2 id173

id173 is a common scalarization method used to solve the bi-criterion
problem (6.7). one form of id173 is to minimize the weighted sum of the
objectives:

minimize

(6.8)
where    > 0 is a problem parameter. as    varies over (0,   ), the solution of (6.8)
traces out the optimal trade-o    curve.
another common method of id173, especially when the euclidean norm

kax     bk +   kxk,

is used, is to minimize the weighted sum of squared norms, i.e.,

minimize

kax     bk2 +   kxk2,

(6.9)

for a variety of values of    > 0.

these regularized approximation problems each solve the bi-criterion problem
of making both kax     bk and kxk small, by adding an extra term or penalty
associated with the norm of x.

interpretations

id173 is used in several contexts. in an estimation setting, the extra term
penalizing large kxk can be interpreted as our prior knowledge that kxk is not too
large. in an optimal design setting, the extra term adds the cost of using large
values of the design variables to the cost of missing the target speci   cations.

the constraint that kxk be small can also re   ect a modeling issue. it might be,
for example, that y = ax is only a good approximation of the true relationship
y = f (x) between x and y. in order to have f (x)     b, we want ax     b, and also
need x small in order to ensure that f (x)     ax.
we will see in   6.4.1 and   6.4.2 that id173 can be used to take into
account variation in the matrix a. roughly speaking, a large x is one for which
variation in a causes large variation in ax, and hence should be avoided.

id173 is also used when the matrix a is square, and the goal is to
solve the linear equations ax = b. in cases where a is poorly conditioned, or even
singular, id173 gives a compromise between solving the equations (i.e.,
making kax     bk zero) and keeping x of reasonable size.

id173 comes up in a statistical setting; see   7.1.2.

tikhonov id173

the most common form of id173 is based on (6.9), with euclidean norms,
which results in a (convex) quadratic optimization problem:

minimize

kax     bk2

2 +   kxk2

2 = xt (at a +   i)x     2bt ax + bt b.

(6.10)

this tikhonov id173 problem has the analytical solution

x = (at a +   i)   1at b.

since at a +   i     0 for any    > 0, the tikhonov regularized least-squares solution
requires no rank (or dimension) assumptions on the matrix a.

6.3 regularized approximation

307

smoothing id173

the idea of id173, i.e., adding to the objective a term that penalizes large
x, can be extended in several ways. in one useful extension we add a id173
term of the form kdxk, in place of kxk.
in many applications, the matrix d
represents an approximate di   erentiation or second-order di   erentiation operator,
so kdxk represents a measure of the variation or smoothness of x.
for example, suppose that the vector x     rn represents the value of some
continuous physical parameter, say, temperature, along the interval [0, 1]: xi is
the temperature at the point i/n. a simple approximation of the gradient or
   rst derivative of the parameter near i/n is given by n(xi+1     xi), and a simple
approximation of its second derivative is given by the second di   erence
n (n(xi+1     xi)     n(xi     xi   1)) = n2(xi+1     2xi + xi   1).

if     is the (tridiagonal, toeplitz) matrix

    = n2

                                 

0       
1    2
0
1
0
1       
1    2
0
0
0
1    2       
0
0
0
0
...
...
...
...
...
...
0           2
0
0
0
1
0       
1    2
0
0
0
0       
0
0
0
0

0
0
0
...
0
1
1    2

0
0
0
...
0
0
1

                                 

    r(n   2)  n,

then    x represents an approximation of the second derivative of the parameter, so
k   xk2
2 represents a measure of the mean-square curvature of the parameter over
the interval [0, 1].

the tikhonov regularized problem

minimize

kax     bk2

2 +   k   xk2

2

can be used to trade o    the objective kax     bk2, which might represent a measure
of    t, or consistency with experimental data, and the objective k   xk2, which is
(approximately) the mean-square curvature of the underlying physical parameter.
the parameter    is used to control the amount of id173 required, or to
plot the optimal trade-o    curve of    t versus smoothness.

we can also add several id173 terms. for example, we can add terms

associated with smoothness and size, as in
kax     bk2

minimize

2 +   k   xk2

2 +   kxk2
2.

here, the parameter        0 is used to control the smoothness of the approximate
solution, and the parameter        0 is used to control its size.

example 6.3 optimal input design. we consider a dynamical system with scalar
input sequence u(0), u(1), . . . , u(n ), and scalar output sequence y(0), y(1), . . . , y(n ),
related by convolution:

y(t) =

tx   =0

h(   )u(t        ),

t = 0, 1, . . . , n.

308

6 approximation and    tting

the sequence h(0), h(1), . . . , h(n ) is called the convolution kernel or impulse response
of the system.

our goal is to choose the input sequence u to achieve several goals.

    output tracking. the primary goal is that the output y should track, or follow,
a desired target or reference signal ydes. we measure output tracking error by
the quadratic function

jtrack =

1

n + 1

nxt=0

(y(t)     ydes(t))2.

    small input. the input should not be large. we measure the magnitude of the

input by the quadratic function

jmag =

1

n + 1

u(t)2.

nxt=0

    small input variations. the input should not vary rapidly. we measure the

magnitude of the input variations by the quadratic function

jder =

1
n

n    1xt=0

(u(t + 1)     u(t))2.

by minimizing a weighted sum

jtrack +   jder +   jmag,

where    > 0 and    > 0, we can trade o    the three objectives.

now we consider a speci   c example, with n = 200, and impulse response

h(t) =

1
9

(0.9)t(1     0.4 cos(2t)).

figure 6.6 shows the optimal input, and corresponding output (along with the desired
trajectory ydes), for three values of the id173 parameters    and   . the top
row shows the optimal input and corresponding output for    = 0,    = 0.005. in this
case we have some id173 for the magnitude of the input, but no id173
for its variation. while the tracking is good (i.e., we have jtrack is small), the input
required is large, and rapidly varying. the second row corresponds to    = 0,    = 0.05.
in this case we have more magnitude id173, but still no id173 for
variation in u. the corresponding input is indeed smaller, at the cost of a larger
tracking error. the bottom row shows the results for    = 0.3,    = 0.05.
in this
case we have added some id173 for the variation. the input variation is
substantially reduced, with not much increase in output tracking error.

   1-norm id173

id173 with an    1-norm can be used as a heuristic for    nding a sparse
solution. for example, consider the problem

minimize

kax     bk2 +   kxk1,

(6.11)

6.3 regularized approximation

309

5

0

)
t
(
u

   5

   10
0

4

2

0

)
t
(
u

   2

   4
0

4

2

0

)
t
(
u

   2

   4
0

1

0.5

)
t
(
y

0

   0.5

50

100

t

150

200

   1
0

1

0.5

)
t
(
y

0

   0.5

50

100

t

150

200

   1
0

1

0.5

)
t
(
y

0

   0.5

50

100

t

150

200

50

100

t

150

200

50

100

t

150

200

   1
0

50

100

t

150

200

figure 6.6 optimal inputs (left) and resulting outputs (right) for three values
of the id173 parameters    (which corresponds to input variation) and
   (which corresponds to input magnitude). the dashed line in the righthand
plots shows the desired output ydes. top row:    = 0,    = 0.005; middle row:
   = 0,    = 0.05; bottom row:    = 0.3,    = 0.05.

310

6 approximation and    tting

in which the residual is measured with the euclidean norm and the id173 is
done with an    1-norm. by varying the parameter    we can sweep out the optimal
trade-o    curve between kax     bk2 and kxk1, which serves as an approximation
of the optimal trade-o    curve between kax     bk2 and the sparsity or cardinality
card(x) of the vector x, i.e., the number of nonzero elements. the problem (6.11)
can be recast and solved as an socp.

example 6.4 regressor selection problem. we are given a matrix a     rm  n,
whose columns are potential regressors, and a vector b     rm that is to be    t by a
linear combination of k < n columns of a. the problem is to choose the subset of k
regressors to be used, and the associated coe   cients. we can express this problem
as

in general, this is a hard combinatorial problem.

minimize
subject to

kax     bk2
card(x)     k.

one straightforward approach is to check every possible sparsity pattern in x with k
nonzero entries. for a    xed sparsity pattern, we can    nd the optimal x by solving
a least-squares problem, i.e., minimizing k   a  x     bk2, where   a denotes the submatrix
of a obtained by keeping the columns corresponding to the sparsity pattern, and
  x is the subvector with the nonzero components of x. this is done for each of the
n!/(k!(n     k)!) sparsity patterns with k nonzeros.
a good heuristic approach is to solve the problem (6.11) for di   erent values of   ,
   nding the smallest value of    that results in a solution with card(x) = k. we then
   x this sparsity pattern and    nd the value of x that minimizes kax     bk2.
figure 6.7 illustrates a numerical example with a     r10  20, x     r20, b     r10. the
circles on the dashed curve are the (globally) pareto optimal values for the trade-o   
between card(x) (vertical axis) and the residual kax     bk2 (horizontal axis). for
each k, the pareto optimal point was obtained by enumerating all possible sparsity
patterns with k nonzero entries, as described above. the circles on the solid curve
were obtained with the heuristic approach, by using the sparsity patterns of the
solutions of problem (6.11) for di   erent values of   . note that for card(x) = 1, the
heuristic method actually    nds the global optimum.
this idea will come up again in basis pursuit (  6.5.4).

6.3.3 reconstruction, smoothing, and de-noising

in this section we describe an important special case of the bi-criterion approxi-
mation problem described above, and give some examples showing how di   erent
id173 methods perform. in reconstruction problems, we start with a signal
represented by a vector x     rn. the coe   cients xi correspond to the value of
some function of time, evaluated (or sampled, in the language of signal processing)
at evenly spaced points. it is usually assumed that the signal does not vary too
rapidly, which means that usually, we have xi     xi+1. (in this section we consider
signals in one dimension, e.g., audio signals, but the same ideas can be applied to
signals in two or more dimensions, e.g., images or video.)

6.3 regularized approximation

311

)
x
(
d
r
a
c

10

8

6

4

2

0
0

1

2
kax     bk2

3

4

figure 6.7 sparse regressor selection with a matrix a     r10  20. the circles
on the dashed line are the pareto optimal values for the trade-o    between
the residual kax     bk2 and the number of nonzero elements card(x). the
points indicated by circles on the solid line are obtained via the    1-norm
regularized heuristic.

the signal x is corrupted by an additive noise v:

xcor = x + v.

the noise can be modeled in many di   erent ways, but here we simply assume that
it is unknown, small, and, unlike the signal, rapidly varying. the goal is to form an
estimate   x of the original signal x, given the corrupted signal xcor. this process is
called signal reconstruction (since we are trying to reconstruct the original signal
from the corrupted version) or de-noising (since we are trying to remove the noise
from the corrupted signal). most reconstruction methods end up performing some
sort of smoothing operation on xcor to produce   x, so the process is also called
smoothing.

one simple formulation of the reconstruction problem is the bi-criterion problem

minimize (w.r.t. r2

+)

(k  x     xcork2,   (  x)) ,

(6.12)

where   x is the variable and xcor is a problem parameter. the function    : rn     r
it is
is convex, and is called the id173 function or smoothing objective.
meant to measure the roughness, or lack of smoothness, of the estimate   x. the
reconstruction problem (6.12) seeks signals that are close (in    2-norm) to the cor-
rupted signal, and that are smooth, i.e., for which   (  x) is small. the reconstruction
problem (6.12) is a convex bi-criterion problem. we can    nd the pareto optimal
points by scalarization, and solving a (scalar) id76 problem.

312

6 approximation and    tting

quadratic smoothing

the simplest reconstruction method uses the quadratic smoothing function

  quad(x) =

n   1xi=1

(xi+1     xi)2 = kdxk2
2,

where d     r(n   1)  n is the bidiagonal matrix
0       
0
1       
0
...
...
0           1
0       

   1
1
0    1
...
...
0
0
0
0

d =

0
0
...
1
0    1

                     

0
0
...
0
1

.

                     

we can obtain the optimal trade-o    between k  x    xcork2 and kd  xk2 by minimizing

k  x     xcork2

2 +   kd  xk2
2,

where    > 0 parametrizes the optimal trade-o    curve. the solution of this quadratic
problem,

  x = (i +   dt d)   1xcor,

can be computed very e   ciently since i +   dt d is tridiagonal; see appendix c.

quadratic smoothing example
figure 6.8 shows a signal x     r4000 (top) and the corrupted signal xcor (bottom).
the optimal trade-o    curve between the objectives k  x    xcork2 and kd  xk2 is shown
in    gure 6.9. the extreme point on the left of the trade-o    curve corresponds to
  x = xcor, and has objective value kdxcork2 = 4.4. the extreme point on the right
corresponds to   x = 0, for which k  x     xcork2 = kxcork2 = 16.2. note the clear knee
in the trade-o    curve near k  x     xcork2     3.
figure 6.10 shows three smoothed signals on the optimal trade-o    curve, cor-
responding to k  x     xcork2 = 8 (top), 3 (middle), and 1 (bottom). comparing the
reconstructed signals with the original signal x, we see that the best reconstruction
is obtained for k  x     xcork2 = 3, which corresponds to the knee of the trade-o   
curve. for higher values of k  x     xcork2, there is too much smoothing; for smaller
values there is too little smoothing.

total variation reconstruction

simple quadratic smoothing works well as a reconstruction method when the orig-
inal signal is very smooth, and the noise is rapidly varying. but any rapid varia-
tions in the original signal will, obviously, be attenuated or removed by quadratic
smoothing. in this section we describe a reconstruction method that can remove
much of the noise, while still preserving occasional rapid variations in the original
signal. the method is based on the smoothing function

  tv(  x) =

n   1xi=1

|  xi+1       xi| = kd  xk1,

6.3 regularized approximation

313

0.5

x

0

   0.5
0

0.5

r
o
c
x

0

   0.5
0

1000

2000

3000

4000

1000

2000

i

3000

4000

figure 6.8 top: the original signal x     r4000. bottom: the corrupted signal
xcor.

4

3

2

k
  x
d
k

2

1

0
0

5

10

15

20

k  x     xcork2
figure 6.9 optimal trade-o    curve between kd  xk2 and k  x     xcork2. the
curve has a clear knee near k  x     xcork     3.

314

6 approximation and    tting

0.5

  x

0

   0.5
0

0.5

  x

0

   0.5
0

0.5

  x

0

   0.5
0

1000

2000

3000

4000

1000

2000

3000

4000

1000

2000

i

3000

4000

figure 6.10 three smoothed or reconstructed signals   x. the top one cor-
responds to k  x     xcork2 = 8, the middle one to k  x     xcork2 = 3, and the
bottom one to k  x     xcork2 = 1.

which is called the total variation of x     rn. like the quadratic smoothness
measure   quad, the total variation function assigns large values to rapidly varying
  x. the total variation measure, however, assigns relatively less penalty to large
values of |xi+1     xi|.

total variation reconstruction example

figure 6.11 shows a signal x     r2000 (in the top plot), and the signal corrupted
with noise xcor. the signal is mostly smooth, but has several rapid variations or
jumps in value; the noise is rapidly varying.

we    rst use quadratic smoothing. figure 6.12 shows three smoothed signals on
the optimal trade-o    curve between kd  xk2 and k  x   xcork2. in the    rst two signals,
the rapid variations in the original signal are also smoothed. in the third signal
the steep edges in the signal are better preserved, but there is still a signi   cant
amount of noise left.

now we demonstrate total variation reconstruction. figure 6.13 shows the
optimal trade-o    curve between kd  xk1 and k  x    xcorrk2. figure 6.14 shows the re-
constructed signals on the optimal trade-o    curve, for kd  xk1 = 5 (top), kd  xk1 = 8
(middle), and kd  xk1 = 10 (bottom). we observe that, unlike quadratic smoothing,
total variation reconstruction preserves the sharp transitions in the signal.

6.3 regularized approximation

315

2

1

0

x

   1
   2
0

2

1

0

r
o
c
x

   1
   2
0

500

1000

1500

2000

500

1000

i

1500

2000

figure 6.11 a signal x     r2000, and the corrupted signal xcor     r2000. the
noise is rapidly varying, and the signal is mostly smooth, with a few rapid
variations.

316

6 approximation and    tting

2

0

  x

   2
0
2

  x

0

   2
0
2

  x

0

   2
0

500

1000

1500

2000

500

1000

1500

2000

500

1000

i

1500

2000

figure 6.12 three quadratically smoothed signals   x. the top one corre-
sponds to k  x     xcork2 = 10, the middle one to k  x     xcork2 = 7, and the
bottom one to k  x     xcork2 = 4. the top one greatly reduces the noise, but
also excessively smooths out the rapid variations in the signal. the bottom
smoothed signal does not give enough noise reduction, and still smooths out
the rapid variations in the original signal. the middle smoothed signal gives
the best compromise, but still smooths out the rapid variations.

250

200

150

1

k
  x
d
k

100

50

0
0

10

figure 6.13 optimal trade-o    curve between kd  xk1 and k  x     xcork2.

30
20
k  x     xcork2

40

50

6.3 regularized approximation

317

2

0

  x

   2
0
2

  x

0

   2
0
2

  x

0

   2
0

500

1000

i

1500

2000

500

1000

1500

2000

500

1000

1500

2000

figure 6.14 three reconstructed signals   x, using total variation reconstruc-
tion. the top one corresponds to kd  xk1 = 5, the middle one to kd  xk1 = 8,
and the bottom one to kd  xk1 = 10. the bottom one does not give quite
enough noise reduction, while the top one eliminates some of the slowly vary-
ing parts of the signal. note that in total variation reconstruction, unlike
quadratic smoothing, the sharp changes in the signal are preserved.

318

6 approximation and    tting

6.4 robust approximation

6.4.1 stochastic robust approximation

we consider an approximation problem with basic objective kax   bk, but also wish
to take into account some uncertainty or possible variation in the data matrix a.
(the same ideas can be extended to handle the case where there is uncertainty in
both a and b.) in this section we consider some statistical models for the variation
in a.

we assume that a is a random variable taking values in rm  n, with mean   a,

so we can describe a as

a =   a + u,

where u is a random matrix with zero mean. here, the constant matrix   a gives
the average value of a, and u describes its statistical variation.

it is natural to use the expected value of kax     bk as the objective:

minimize ekax     bk.

(6.13)

we refer to this problem as the stochastic robust approximation problem.
it is
always a id76 problem, but usually not tractable since in most
cases it is very di   cult to evaluate the objective or its derivatives.

one simple case in which the stochastic robust approximation problem (6.13)

can be solved occurs when a assumes only a    nite number of values, i.e.,

prob(a = ai) = pi,

i = 1, . . . , k,

where ai     rm  n, 1t p = 1, p (cid:23) 0. in this case the problem (6.13) has the form

minimize p1ka1x     bk +        + pkkakx     bk,

which is often called a sum-of-norms problem. it can be expressed as

minimize
subject to

pt t
kaix     bk     ti,

i = 1, . . . , k,

where the variables are x     rn and t     rk. if the norm is the euclidean norm,
this sum-of-norms problem is an socp. if the norm is the    1- or       -norm, the
sum-of-norms problem can be expressed as an lp; see exercise 6.8.

some variations on the statistical robust approximation problem (6.13) are

tractable. as an example, consider the statistical robust least-squares problem

where the norm is the euclidean norm. we can express the objective as

minimize ekax     bk2
2,

ekax     bk2

2 = e(   ax     b + u x)t (   ax     b + u x)
= (   ax     b)t (   ax     b) + e xt u t u x
= k   ax     bk2

2 + xt p x,

6.4 robust approximation

319

where p = e u t u . therefore the statistical robust approximation problem has
the form of a regularized least-squares problem
k   ax     bk2

2 + kp 1/2xk2
2,

minimize

with solution

x = (   at   a + p )   1   at b.

this makes perfect sense: when the matrix a is subject to variation, the vector
ax will have more variation the larger x is, and jensen   s inequality tells us that
variation in ax will increase the average value of kax    bk2. so we need to balance
making   ax     b small with the desire for a small x (to keep the variation in ax
small), which is the essential idea of id173.
this observation gives us another interpretation of the tikhonov regularized
least-squares problem (6.10), as a robust least-squares problem, taking into account
possible variation in the matrix a. the solution of the tikhonov regularized least-
squares problem (6.10) minimizes ek(a + u )x     bk2, where uij are zero mean,
uncorrelated random variables, with variance   /m (and here, a is deterministic).

6.4.2 worst-case robust approximation

it is also possible to model the variation in the matrix a using a set-based, worst-
case approach. we describe the uncertainty by a set of possible values for a:

a     a     rm  n,

which we assume is nonempty and bounded. we de   ne the associated worst-case
error of a candidate approximate solution x     rn as

ewc(x) = sup{kax     bk | a     a},

which is always a convex function of x. the (worst-case) robust approximation
problem is to minimize the worst-case error:

minimize

ewc(x) = sup{kax     bk | a     a},

(6.14)

where the variable is x, and the problem data are b and the set a. when a is the
singleton a = {a}, the robust approximation problem (6.14) reduces to the basic
norm approximation problem (6.1). the robust approximation problem is always
a id76 problem, but its tractability depends on the norm used and
the description of the uncertainty set a.

example 6.5 comparison of stochastic and worst-case robust approximation. to
illustrate the di   erence between the stochastic and worst-case formulations of the
robust approximation problem, we consider the least-squares problem

minimize

ka(u)x     bk2
2,

where u     r is an uncertain parameter and a(u) = a0 + ua1. we consider a
speci   c instance of the problem, with a(u)     r20  10, ka0k = 10, ka1k = 1, and u

320

6 approximation and    tting

12

10

8

6

4

2

)
u
(
r

xnom

xstoch

xwc

0
   2

   1

0
u

1

2

figure 6.15 the residual r(u) = ka(u)x     bk2 as a function of the un-
certain parameter u for three approximate solutions x: (1) the nominal
least-squares solution xnom; (2) the solution of the stochastic robust approx-
imation problem xstoch (assuming u is uniformly distributed on [   1, 1]); and
(3) the solution of the worst-case robust approximation problem xwc, as-
suming the parameter u lies in the interval [   1, 1]. the nominal solution
achieves the smallest residual when u = 0, but gives much larger residuals
as u approaches    1 or 1. the worst-case solution has a larger residual when
u = 0, but its residuals do not rise much as the parameter u varies over the
interval [   1, 1].

in the interval [   1, 1]. (so, roughly speaking, the variation in the matrix a is around
  10%.)
we    nd three approximate solutions:

    nominal optimal. the optimal solution xnom is found, assuming a(u) has its

nominal value a0.

bk2

    stochastic robust approximation. we    nd xstoch, which minimizes eka(u)x    

2, assuming the parameter u is uniformly distributed on [   1, 1].
    worst-case robust approximation. we    nd xwc, which minimizes

   1   u   1ka(u)x     bk2 = max{k(a0     a1)x     bk2,k(a0 + a1)x     bk2}.
sup

for each of these three values of x, we plot the residual r(u) = ka(u)x     bk2 as a
function of the uncertain parameter u, in    gure 6.15. these plots show how sensitive
an approximate solution can be to variation in the parameter u. the nominal solu-
tion achieves the smallest residual when u = 0, but is quite sensitive to parameter
variation: it gives much larger residuals as u deviates from 0, and approaches    1 or
1. the worst-case solution has a larger residual when u = 0, but its residuals do not
rise much as u varies over the interval [   1, 1]. the stochastic robust approximate
solution is in between.

6.4 robust approximation

321

the robust approximation problem (6.14) arises in many contexts and applica-
tions. in an estimation setting, the set a gives our uncertainty in the linear relation
between the vector to be estimated and our measurement vector. sometimes the
noise term v in the model y = ax + v is called additive noise or additive error,
since it is added to the    ideal    measurement ax. in contrast, the variation in a is
called multiplicative error, since it multiplies the variable x.

in an optimal design setting, the variation can represent uncertainty (arising in
manufacture, say) of the linear equations that relate the design variables x to the
results vector ax. the robust approximation problem (6.14) is then interpreted as
the robust design problem:    nd design variables x that minimize the worst possible
mismatch between ax and b, over all possible values of a.

finite set
here we have a = {a1, . . . , ak}, and the robust approximation problem is

minimize maxi=1,...,k kaix     bk.

this problem is equivalent to the robust approximation problem with the polyhe-
dral set a = conv{a1, . . . , ak}:

minimize

sup{kax     bk | a     conv{a1, . . . , ak}} .

we can cast the problem in epigraph form as

minimize
subject to

t
kaix     bk     t,

i = 1, . . . , k,

which can be solved in a variety of ways, depending on the norm used. if the norm
is the euclidean norm, this is an socp. if the norm is the    1- or       -norm, we can
express it as an lp.

norm bound error
here the uncertainty set a is a norm ball, a = {   a + u | kuk     a}, where k    k is a
norm on rm  n. in this case we have

ewc(x) = sup{k   ax     b + u xk | kuk     a},

which must be carefully interpreted since the    rst norm appearing is on rm (and
is used to measure the size of the residual) and the second one appearing is on
rm  n (used to de   ne the norm ball a).
this expression for ewc(x) can be simpli   ed in several cases. as an example,
let us take the euclidean norm on rn and the associated induced norm on rm  n,
i.e., the maximum singular value. if   ax     b 6= 0 and x 6= 0, the supremum in the
expression for ewc(x) is attained for u = auvt , with

u =

  ax     b
k   ax     bk2

,

v =

,

x
kxk2

and the resulting worst-case error is

ewc(x) = k   ax     bk2 + akxk2.

322

6 approximation and    tting

(it is easily veri   ed that this expression is also valid if x or   ax     b is zero.) the
robust approximation problem (6.14) then becomes

which is a regularized norm problem, solvable as the socp

minimize

k   ax     bk2 + akxk2,

minimize
subject to

t1 + at2
k   ax     bk2     t1,

kxk2     t2.

since the solution of this problem is the same as the solution of the regularized

least-squares problem

minimize

k   ax     bk2

2 +   kxk2

2

for some value of the id173 parameter   , we have another interpretation of
the regularized least-squares problem as a worst-case robust approximation prob-
lem.

uncertainty ellipsoids

we can also describe the variation in a by giving an ellipsoid of possible values for
each row:

a = {[a1        am]t | ai     ei, i = 1, . . . , m},

where

ei = {  ai + piu | kuk2     1}.

the matrix pi     rn  n describes the variation in ai. we allow pi to have a nontriv-
ial nullspace, in order to model the situation when the variation in ai is restricted
to a subspace. as an extreme case, we take pi = 0 if there is no uncertainty in ai.
with this ellipsoidal uncertainty description, we can give an explicit expression

for the worst-case magnitude of each residual:

ai   ei |at
sup

i x     bi| = sup{|  at

i x     bi + (piu)t x| | kuk2     1}

= |  at

i x     bi| + kp t

i xk2.

using this result we can solve several robust approximation problems. for

example, the robust    2-norm approximation problem

minimize

ewc(x) = sup{kax     bk2 | ai     ei, i = 1, . . . , m}

can be reduced to an socp, as follows. an explicit expression for the worst-case
error is given by

ewc(x) =  mxi=1(cid:18) sup

ai   ei |at

i x     bi|(cid:19)2!1/2

=  mxi=1

(|  at

i x     bi| + kp t

i xk2)2!1/2

.

to minimize ewc(x) we can solve

minimize
subject to

ktk2
|  at
i x     bi| + kp t

i xk2     ti,

i = 1, . . . , m,

6.4 robust approximation

323

where we introduced new variables t1, . . . , tm. this problem can be formulated as

minimize
subject to

i xk2     ti,
i xk2     ti,
which becomes an socp when put in epigraph form.

ktk2
i x     bi + kp t
  at
     at
i x + bi + kp t

i = 1, . . . , m

i = 1, . . . , m,

norm bounded error with linear structure
as a generalization of the norm bound description a = {   a + u | kuk     a}, we can
de   ne a as the image of a norm ball under an a   ne transformation:

a = {   a + u1a1 + u2a2 +        + upap | kuk     1},

where k    k is a norm on rp, and the p + 1 matrices   a, a1, . . . , ap     rm  n are
given. the worst-case error can be expressed as

ewc(x) = sup

kuk   1k(   a + u1a1 +        + upap)x     bk
kuk   1kp (x)u + q(x)k,

= sup

where p and q are de   ned as

p (x) =(cid:2) a1x a2x        apx (cid:3)     rm  p,

q(x) =   ax     b     rm.

as a    rst example, we consider the robust chebyshev approximation problem

minimize

ewc(x) = supkuk      1 k(   a + u1a1 +        + upap)x     bk   .

in this case we can derive an explicit expression for the worst-case error. let pi(x)t
denote the ith row of p (x). we have

ewc(x) =

kuk      1kp (x)u + q(x)k   
sup

= max

i=1,...,m

= max

i=1,...,m

kuk      1|pi(x)t u + qi(x)|
sup
(kpi(x)k1 + |qi(x)|).

the robust chebyshev approximation problem can therefore be cast as an lp

t

minimize
subject to    y0 (cid:22)   ax     b (cid:22) y0
   yk (cid:22) akx (cid:22) yk,
k=1 yk (cid:22) t1,

y0 +pp
with variables x     rn, yk     rm, t     r.

k = 1, . . . , p

as another example, we consider the robust least-squares problem

minimize

ewc(x) = supkuk2   1 k(   a + u1a1 +        + upap)x     bk2.

324

6 approximation and    tting

here we use lagrange duality to evaluate ewc. the worst-case error ewc(x) is the
squareroot of the optimal value of the (nonconvex) quadratic optimization problem

maximize
subject to ut u     1,

kp (x)u + q(x)k2

2

with u as variable. the lagrange dual of this problem can be expressed as the
sdp

t +   

minimize

subject to       

i

p (x)

p (x)t
q(x)t

  i
0

q(x)

0

t        (cid:23) 0

(6.15)

with variables t,        r. moreover, as mentioned in   5.2 and   b.1 (and proved
in   b.4), strong duality holds for this pair of primal and id78. in other
words, for    xed x, we can compute ewc(x)2 by solving the sdp (6.15) with variables
t and   . optimizing jointly over t,   , and x is equivalent to minimizing ewc(x)2.
we conclude that the robust least-squares problem is equivalent to the sdp (6.15)
with x,   , t as variables.

example 6.6 comparison of worst-case robust, tikhonov regularized, and nominal
least-squares solutions. we consider an instance of the robust approximation problem

minimize

supkuk2   1 k(   a + u1a1 + u2a2)x     bk2,

(6.16)
with dimensions m = 50, n = 20. the matrix   a has norm 10, and the two matrices
a1 and a2 have norm 1, so the variation in the matrix a is, roughly speaking, around
10%. the uncertainty parameters u1 and u2 lie in the unit disk in r2.
we compute the optimal solution of the robust least-squares problem (6.16) xrls, as
well as the solution of the nominal least-squares problem xls (i.e., assuming u = 0),
and also the tikhonov regularized solution xtik, with    = 1.

to illustrate the sensitivity of each of these approximate solutions to the parameter
u, we generate 105 parameter vectors, uniformly distributed on the unit disk, and
evaluate the residual

for each parameter value. the distributions of the residuals are shown in    gure 6.16.

k(a0 + u1a1 + u2a2)x     bk2

we can make several observations. first, the residuals of the nominal least-squares
solution are widely spread, from a smallest value around 0.52 to a largest value
around 4.9. in particular, the least-squares solution is very sensitive to parameter
variation. in contrast, both the robust least-squares and tikhonov regularized so-
lutions exhibit far smaller variation in residual as the uncertainty parameter varies
over the unit disk. the robust least-squares solution, for example, achieves a residual
between 2.0 and 2.6 for all parameters in the unit disk.

6.5 function    tting and interpolation

in function    tting problems, we select a member of a    nite-dimensional subspace
of functions that best    ts some given data or requirements. for simplicity we

6.5 function    tting and interpolation

325

y
c
n
e
u
q
e
r
f

0.25

0.2

0.15

0.1

0.05

0
0

xrls

xtik

xls

1

2

3

4

5

k(a0 + u1a1 + u2a2)x     bk2

figure 6.16 distribution of the residuals for the three solutions of a least-
squares problem (6.16): xls, the least-squares solution assuming u = 0; xtik,
the tikhonov regularized solution with    = 1; and xrls, the robust least-
squares solution. the histograms were obtained by generating 105 values of
the uncertain parameter vector u from a uniform distribution on the unit
disk in r2. the bins have width 0.1.

326

6 approximation and    tting

consider real-valued functions; the ideas are readily extended to handle vector-
valued functions as well.

6.5.1 function families

f (u) = x1f1(u) +        + xnfn(u)

we consider a family of functions f1, . . . , fn : rk     r, with common domain
dom fi = d. with each x     rn we associate the function f : rk     r given by
(6.17)
with dom f = d. the family {f1, . . . , fn} is sometimes called the set of basis
functions (for the    tting problem) even when the functions are not independent.
the vector x     rn, which parametrizes the subspace of functions, is our optimiza-
tion variable, and is sometimes called the coe   cient vector. the basis functions
generate a subspace f of functions on d.
in many applications the basis functions are specially chosen, using prior knowl-
edge or experience, in order to reasonably model functions of interest with the
   nite-dimensional subspace of functions.
in other cases, more generic function
families are used. we describe a few of these below.

polynomials

one common subspace of functions on r consists of polynomials of degree less
than n. the simplest basis consists of the powers, i.e., fi(t) = ti   1, i = 1, . . . , n.
in many applications, the same subspace is described using a di   erent basis, for
example, a set of polynomials f1, . . . , fn, of degree less than n, that are orthonormal
with respect to some positive function (or measure)    : rn     r+, i.e.,

z fi(t)fj(t)  (t) dt =(cid:26) 1

0

i = j
i 6= j.

another common basis for polynomials is the lagrange basis f1, . . . , fn associated
with distinct points t1, . . . , tn, which satisfy

fi(tj) =(cid:26) 1

0

i = j
i 6= j.

we can also consider polynomials on rk, with a maximum total degree, or a
maximum degree for each variable.

as a related example, we have trigonometric polynomials of degree less than n,

with basis

sin kt,

k = 1, . . . , n     1,

cos kt,

k = 0, . . . , n     1.

piecewise-linear functions

we start with a triangularization of the domain d, which means the following. we
have a set of mesh or grid points g1, . . . , gn     rk, and a partition of d into a set
of simplexes:

d = s1                sm,

int(si     sj) =     for i 6= j.

6.5 function    tting and interpolation

327

)
2
u

,
1
u
(
f

1

0
0

0

u1

1

1

u2

figure 6.17 a piecewise-linear function of two variables, on the unit square.
the triangulation consists of 98 simplexes, and a uniform grid of 64 points
in the unit square.

each simplex is the convex hull of k + 1 grid points, and we require that each grid
point is a vertex of any simplex it lies in.

given a triangularization, we can construct a piecewise-linear (or more precisely,
piecewise-a   ne) function f by assigning function values f (gi) = xi to the grid
points, and then extending the function a   nely on each simplex. the function f
can be expressed as (6.17) where the basis functions fi are a   ne on each simplex
and are de   ned by the conditions

fi(gj) =(cid:26) 1 i = j

0 i 6= j.

by construction, such a function is continuous.

figure 6.17 shows an example for k = 2.

piecewise polynomials and splines

the idea of piecewise-a   ne functions on a triangulated domain is readily extended
to piecewise polynomials and other functions.

piecewise polynomials are de   ned as polynomials (of some maximum degree)
on each simplex of the triangulation, which are continuous, i.e., the polynomials
agree at the boundaries between simplexes. by further restricting the piecewise
polynomials to have continuous derivatives up to a certain order, we can de   ne
various classes of spline functions. figure 6.18 shows an example of a cubic spline,
i.e., a piecewise polynomial of degree 3 on r, with continuous    rst and second
derivatives.

328

6 approximation and    tting

p2(u)

p3(u)

)
u
(
f

p1(u)

u0

u1

u2

u

u3

figure 6.18 cubic spline. a cubic spline is a piecewise polynomial, with
continuous    rst and second derivatives. in this example, the cubic spline f
is formed from the three cubic polynomials p1 (on [u0, u1]), p2 (on [u1, u2]),
and p3 (on [u2, u3]). adjacent polynomials have the same function value,
and equal    rst and second derivatives, at the boundary points u1 and u2.
in this example, the dimension of the family of functions is n = 6, since
we have 12 polynomial coe   cients (4 per cubic polynomial), and 6 equality
constraints (3 each at u1 and u2).

6.5 function    tting and interpolation

329

6.5.2 constraints

in this section we describe some constraints that can be imposed on the function
f , and therefore, on the variable x     rn.
function value interpolation and inequalities

let v be a point in d. the value of f at v,

f (v) =

nxi=1

xifi(v),

is a linear function of x. therefore interpolation conditions

f (vj) = zj,

j = 1, . . . , m,

which require the function f to have the values zj     r at speci   ed points vj     d,
form a set of linear equalities in x. more generally, inequalities on the function
value at a given point, as in l     f (v)     u, are linear inequalities on the variable x.
there are many other interesting convex constraints on f (hence, x) that involve
the function values at a    nite set of points v1, . . . , vn . for example, the lipschitz
constraint

|f (vj)     f (vk)|     lkvj     vkk,

j, k = 1, . . . , m,

forms a set of linear inequalities in x.

we can also impose inequalities on the function values at an in   nite number of

points. as an example, consider the nonnegativity constraint

f (u)     0 for all u     d.

this is a convex constraint on x (since it is the intersection of an in   nite number
of halfspaces), but may not lead to a tractable problem except in special cases
that exploit the particular structure of the functions. one simple example occurs
when the functions are piecewise-linear.
in this case, if the function values are
nonnegative at the grid points, the function is nonnegative everywhere, so we obtain
a simple (   nite) set of linear inequalities.

as a less trivial example, consider the case when the functions are polynomials
on r, with even maximum degree 2k (i.e., n = 2k + 1), and d = r. as shown in
exercise 2.37, page 65, the nonnegativity constraint

p(u) = x1 + x2u +        + x2k+1u2k     0 for all u     r,

is equivalent to

xi = xm+n=i+1

ymn,

i = 1, . . . , 2k + 1,

y (cid:23) 0,

where y     sk+1 is an auxiliary variable.

330

6 approximation and    tting

derivative constraints
suppose the basis functions fi are di   erentiable at a point v     d. the gradient

   f (v) =

nxi=1

xi   fi(v),

is a linear function of x, so interpolation conditions on the derivative of f at v
reduce to linear equality constraints on x. requiring that the norm of the gradient
at v not exceed a given limit,

k   f (v)k =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

nxi=1

xi   fi(v)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)     m,

is a convex constraint on x. the same idea extends to higher derivatives. for
example, if f is twice di   erentiable at v, the requirement that

li (cid:22)    2f (v) (cid:22) ui
is a linear matrix inequality in x, hence convex.

we can also impose constraints on the derivatives at an in   nite number of

points. for example, we can require that f is monotone:

f (u)     f (v) for all u, v     d, u (cid:23) v.

this is a convex constraint in x, but may not lead to a tractable problem except in
special cases. when f is piecewise a   ne, for example, the monotonicity constraint
is equivalent to the condition    f (v) (cid:23) 0 inside each of the simplexes. since the
gradient is a linear function of the grid point values, this leads to a simple (   nite)
set of linear inequalities.

as another example, we can require that the function be convex, i.e., satisfy

f ((u + v)/2)     (f (u) + f (v))/2 for all u, v     d

(which is enough to ensure convexity when f is continuous). this is a convex con-
straint, which has a tractable representation in some cases. one obvious example
is when f is quadratic, in which case the convexity constraint reduces to the re-
quirement that the quadratic part of f be nonnegative, which is an lmi. another
example in which a convexity constraint leads to a tractable problem is described
in more detail in   6.5.5.
integral constraints
any linear functional l on the subspace of functions can be expressed as a linear
function of x, i.e., we have l(f ) = ct x. evaluation of f (or a derivative) at a point
is just a special case. as another example, the linear functional

l(f ) =zd

  (u)f (u) du,

6.5 function    tting and interpolation

331

where    : rk     r, can be expressed as l(f ) = ct x, where

ci =zd

  (u)fi(u) du.

thus, a constraint of the form l(f ) = a is a linear equality constraint on x. one
example of such a constraint is the moment constraint

zd

tmf (t) dt = a

(where f : r     r).

6.5.3 fitting and interpolation problems

minimum norm function    tting

in a    tting problem, we are given data

(u1, y1),

. . . ,

(um, ym)

with ui     d and yi     r, and seek a function f     f that matches this data as
closely as possible. for example in least-squares    tting we consider the problem

minimize pm

i=1(f (ui)     yi)2,

which is a simple least-squares problem in the variable x. we can add a variety of
constraints, for example linear inequalities that must be satis   ed by f at various
points, constraints on the derivatives of f , monotonicity constraints, or moment
constraints.

example 6.7 polynomial    tting. we are given data u1, . . . , um     r and v1, . . . , vm    
r, and hope to approximately    t a polynomial of the form

p(u) = x1 + x2u +        + xnun   1

to the data. for each x we form the vector of errors,

e = (p(u1)     v1, . . . , p(um)     vm) .

to    nd the polynomial that minimizes the norm of the error, we solve the norm
approximation problem

minimize

kek = kax     vk
, i = 1, . . . , m, j = 1, . . . , n.

with variable x     rn, where aij = uj   1
figure 6.19 shows an example with m = 40 data points and n = 6 (i.e., polynomials
of maximum degree 5), for the    2- and       -norms.

i

332

6 approximation and    tting

0.2

0.1

)
u
(
p

0

   0.1

   1

   0.5

0
u

0.5

1

figure 6.19 two polynomials of degree 5 that approximate the 40 data
points shown as circles. the polynomial shown as a solid line minimizes the
   2-norm of the error; the polynomial shown as a dashed line minimizes the
      -norm.

0.2

0.1

)
u
(
f

0

   0.1

   1

   0.5

0
u

0.5

1

figure 6.20 two cubic splines that approximate the 40 data points shown as
circles (which are the same as the data in    gure 6.19). the spline shown as
a solid line minimizes the    2-norm of the error; the spline shown as a dashed
line minimizes the       -norm. as in the polynomial approximation shown in
   gure 6.19, the dimension of the subspace of    tting functions is 6.

6.5 function    tting and interpolation

333

example 6.8 spline    tting. figure 6.20 shows the same data as in example 6.7,
and two optimal    ts with cubic splines. the interval [   1, 1] is divided into three
equal intervals, and we consider piecewise polynomials, with maximum degree 3, with
continuous    rst and second derivatives. the dimension of this subspace of functions
is 6, the same as the dimension of polynomials with maximum degree 5, considered
in example 6.7.

in the simplest forms of function    tting, we have m     n, i.e., the number
of data points is much larger than the dimension of the subspace of functions.
smoothing is accomplished automatically, since all members of the subspace are
smooth.

least-norm interpolation

in another variation of function    tting, we have fewer data points than the dimen-
sion of the subspace of functions. in the simplest case, we require that the function
we choose must satisfy the interpolation conditions

f (ui) = yi,

i = 1, . . . , m,

which are linear equality constraints on x. among the functions that satisfy these
interpolation conditions, we might seek one that is smoothest, or smallest. these
lead to least-norm problems.

in the most general function    tting problem, we can optimize an objective
(such as some measure of the error e), subject to a variety of convex constraints
that represent our prior knowledge of the underlying function.

interpolation, extrapolation, and bounding
by evaluating the optimal function    t   f at a point v not in the original data set,
we obtain a guess of what the value of the underlying function is, at the point v.
this is called interpolation when v is between or near the given data points (e.g.,
v     conv{v1, . . . , vm}), and extrapolation otherwise.
we can also produce an interval in which the value f (v) can lie, by maximizing
and minimizing (the linear function) f (v), subject to the constraints. we can use
the function    t to help identify faulty data or outliers. here we might use, for
example, an    1-norm    t, and look for data points with large errors.

6.5.4 sparse descriptions and basis pursuit

in basis pursuit, there is a very large number of basis functions, and the goal is to
   nd a good    t of the given data as a linear combination of a small number of the
basis functions. (in this context the function family is linearly dependent, and is
sometimes referred to as an over-complete basis or dictionary.) this is called basis
pursuit since we are selecting a much smaller basis, from the given over-complete
basis, to model the data.

334

6 approximation and    tting

thus we seek a function f     f that    ts the data well,

f (ui)     yi,

i = 1, . . . , m,

with a sparse coe   cient vector x, i.e., card(x) small. in this case we refer to

f = x1f1 +        + xnfn =xi   b

xifi,

where b = {i | xi 6= 0} is the set of indices of the chosen basis elements, as a sparse
description of the data. mathematically, basis pursuit is the same as the regressor
selection problem (see   6.4), but the interpretation (and scale) of the optimization
problem are di   erent.
sparse descriptions and basis pursuit have many uses. they can be used for
de-noising or smoothing, or data compression for e   cient transmission or storage
of a signal. in data compression, the sender and receiver both know the dictionary,
or basis elements. to send a signal to the receiver, the sender    rst    nds a sparse
representation of the signal, and then sends to the receiver only the nonzero coef-
   cients (to some precision). using these coe   cients, the receiver can reconstruct
(an approximation of) the original signal.

minimize pm

one common approach to basis pursuit is the same as the method for regressor
selection described in   6.4, and based on    1-norm id173 as a heuristic for
   nding sparse descriptions. we    rst solve the convex problem
i=1(f (ui)     yi)2 +   kxk1,

(6.18)

where    > 0 is a parameter used to trade o    the quality of the    t to the data,
and the sparsity of the coe   cient vector. the solution of this problem can be used
directly, or followed by a re   nement step, in which the best    t is found, using the
sparsity pattern of the solution of (6.18). in other words, we    rst solve (6.18), to
obtain   x. we then set b = {i |   xi 6= 0}, i.e., the set of indices corresponding to
nonzero coe   cients. then we solve the least-squares problem

minimize pm

i=1(f (ui)     yi)2

with variables xi, i     b, and xi = 0 for i 6    b.
in basis pursuit and sparse description applications it is not uncommon to have
a very large dictionary, with n on the order of 104 or much more. to be e   ective,
algorithms for solving (6.18) must exploit problem structure, which derives from
the structure of the dictionary signals.

time-frequency analysis via basis pursuit

in this section we illustrate basis pursuit and sparse representation with a simple
example. we consider functions (or signals) on r, with the range of interest [0, 1].
we think of the independent variable as time, so we use t (instead of u) to denote
it.

we    rst describe the basis functions in the dictionary. each basis function is a

gaussian sinusoidal pulse, or gabor function, with form

e   (t      )2/  2

cos(  t +   ),

6.5 function    tting and interpolation

335

1

0

c
,
0
,
5
.
0
f

   1
0
1

c
,
5
7
,
5
.
0
f

0

   1
0
1

0

   1
0

c
,
0
5
1
,
5
.
0
f

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

0.2

0.4

0.6

0.8

t

1

1

1

figure 6.21 three of the basis elements in the dictionary, all with center time
   = 0.5 and cosine phase. the top signal has frequency    = 0, the middle
one has frequency    = 75, and the bottom one has frequency    = 150.

where    > 0 gives the width of the pulse,    is the time of (the center of) the pulse,
       0 is the frequency, and    is the phase angle. all of the basis functions have
width    = 0.05. the pulse times and frequencies are

   = 0.002k,

k = 0, . . . , 500,

   = 5k,

k = 0, . . . , 30.

for each time    , there is one basis element with frequency zero (and phase    = 0),
and 2 basis elements (cosine and sine, i.e., phase    = 0 and    =   /2) for each of 30
remaining frequencies, so all together there are 501    61 = 30561 basis elements.
the basis elements are naturally indexed by time, frequency, and phase (cosine or
sine), so we denote them as

f  ,  ,c,
f  ,  ,s,

   = 0, 0.002, . . . , 1,
   = 0, 0.002, . . . , 1,

   = 0, 5, . . . , 150,
   = 5, . . . , 150.

three of these basis functions (all with time    = 0.5) are shown in    gure 6.21.

basis pursuit with this dictionary can be thought of as a time-frequency analysis
of the data. if a basis element f  ,  ,c or f  ,  ,s appears in the sparse representation
of a signal (i.e., with a nonzero coe   cient), we can interpret this as meaning that
the data contains the frequency    at time    .

we will use basis pursuit to    nd a sparse approximation of the signal

y(t) = a(t) sin   (t)

336

6 approximation and    tting

)
t
(
y

,
)
t
(
  y

1.5

0.5

   0.5

   1.5
0

0.05

)
t
(
  y
   

)
t
(
y

0

   0.05
0

0.2

0.4

0.2

0.4

t

t

0.6

0.8

1

0.6

0.8

1

figure 6.22 top. the original signal (solid line) and approximation   y ob-
tained by basis pursuit (dashed line) are almost indistinguishable. bottom.
the approximation error y(t)       y(t), with di   erent vertical scale.

where

a(t) = 1 + 0.5 sin(11t),

  (t) = 30 sin(5t).

(this signal is chosen only because it is simple to describe, and exhibits noticeable
changes in its spectral content over time.) we can interpret a(t) as the signal
amplitude, and   (t) as its total phase. we can also interpret

  (t) =(cid:12)(cid:12)(cid:12)(cid:12)

d  

dt(cid:12)(cid:12)(cid:12)(cid:12) = 150| cos(5t)|

as the instantaneous frequency of the signal at time t. the data are given as 501
uniformly spaced samples over the interval [0, 1], i.e., we are given 501 pairs (tk, yk)
with

tk = 0.005k,

yk = y(tk),

k = 0, . . . , 500.

we    rst solve the    1-norm regularized least-squares problem (6.18), with    =
1. the resulting optimal coe   cient vector is very sparse, with only 42 nonzero
coe   cients out of 30561. we then    nd the least-squares    t of the original signal
using these 42 basis vectors. the result   y is compared with the original signal
y in    gure 6.22. the top    gure shows the approximated signal (in dashed line)
and, almost indistinguishable, the original signal y(t) (in solid line). the bottom
   gure shows the error y(t)       y(t). as is clear from the    gure, we have obtained an

6.5 function    tting and interpolation

337

)
t
(
y

)
t
(
  

1.5

0.5

   0.5

   1.5
0

150

100

50

0
0

0.2

0.4

0.2

0.4

t

  

0.6

0.8

1

0.6

0.8

1

figure 6.23 top: original signal. bottom: time-frequency plot. the dashed
curve shows the instantaneous frequency   (t) = 150| cos(5t)| of the original
signal. each circle corresponds to a chosen basis element in the approxima-
tion obtained by basis pursuit. the horizontal axis shows the time index    ,
and the vertical axis shows the frequency index    of the basis element.

approximation   y with a very good relative    t. the relative error is

(1/501)p501

(1/501)p501

i=1(y(ti)       y(ti))2

i=1 y(ti)2

= 2.6    10   4.

by plotting the pattern of nonzero coe   cients versus time and frequency, we
obtain a time-frequency analysis of the original data. such a plot is shown in    g-
ure 6.23, along with the instantaneous frequency. the plot shows that the nonzero
components closely track the instantaneous frequency.

6.5.5 interpolation with convex functions

in some special cases we can solve interpolation problems involving an in   nite-
dimensional set of functions, using    nite-dimensional id76. in this
section we describe an example.

we start with the following question: when does there exist a convex function

f : rk     r, with dom f = rk, that satis   es the interpolation conditions

f (ui) = yi,

i = 1, . . . , m,

338

6 approximation and    tting

at given points ui     rk? (here we do not restrict f to lie in any    nite-dimensional
subspace of functions.) the answer is:
if and only if there exist g1, . . . , gm such
that

(6.19)
to see this,    rst suppose that f is convex, dom f = rk, and f (ui) = yi,

i (uj     ui),

i, j = 1, . . . , m.

yj     yi + gt

i = 1, . . . , m. at each ui we can    nd a vector gi such that

f (z)     f (ui) + gt

i (z     ui)

(6.20)

for all z. if f is di   erentiable, we can take gi =    f (ui); in the more general case,
we can construct gi by    nding a supporting hyperplane to epi f at (ui, yi). (the
vectors gi are called subgradients.) by applying (6.20) to z = uj, we obtain (6.19).

conversely, suppose g1, . . . , gm satisfy (6.19). de   ne f as

f (z) = max

i=1,...,m

(yi + gt

i (z     ui))

for all z     rk. clearly, f is a (piecewise-linear) convex function. the inequali-
ties (6.19) imply that f (ui) = yi, for i = 1, . . . , m.
we can use this result to solve several problems involving interpolation, approx-

imation, or bounding, with convex functions.

fitting a convex function to given data

perhaps the simplest application is to compute the least-squares    t of a convex
function to given data (ui, yi), i = 1, . . . , m:

minimize pm

subject to

i=1(yi     f (ui))2

f : rk     r is convex, dom f = rk.

this is an in   nite-dimensional problem, since the variable is f , which is in the
space of continuous real-valued functions on rk. using the result above, we can
formulate this problem as

minimize pm

subject to

i=1(yi       yi)2

  yj       yi + gt

i (uj     ui),

i, j = 1, . . . , m,

which is a qp with variables   y     rm and g1, . . . , gm     rk. the optimal value of
this problem is zero if and only if the given data can be interpolated by a convex
function, i.e., if there is a convex function that satis   es f (ui) = yi. an example is
shown in    gure 6.24.

bounding values of an interpolating convex function

as another simple example, suppose that we are given data (ui, yi), i = 1, . . . , m,
which can be interpolated by a convex function. we would like to determine the
range of possible values of f (u0), where u0 is another point in rk, and f is any
convex function that interpolates the given data. to    nd the smallest possible
value of f (u0) we solve the lp

minimize
subject to

y0
yj     yi + gt

i (uj     ui),

i, j = 0, . . . , m,

6.5 function    tting and interpolation

339

figure 6.24 least-squares    t of a convex function to data, shown as circles.
the (piecewise-linear) function shown minimizes the sum of squared    tting
error, over all convex functions.

which is an lp with variables y0     r, g0, . . . , gm     rk. by maximizing y0 (which
is also an lp) we    nd the largest possible value of f (u0) for a convex function that
interpolates the given data.

interpolation with monotone convex functions

as an extension of convex interpolation, we can consider interpolation with a convex
and monotone nondecreasing function. it can be shown that there exists a convex
function f : rk     r, with dom f = rk, that satis   es the interpolation conditions

f (ui) = yi,

i = 1, . . . , m,

and is monotone nondecreasing (i.e., f (u)     f (v) whenever u (cid:23) v), if and only if
there exist g1, . . . , gm     rk, such that

gi (cid:23) 0,

i = 1, . . . , m,

yj     yi + gt

i (uj     ui),

i, j = 1, . . . , m.

(6.21)

in other words, we add to the convex interpolation conditions (6.19), the condition
that the subgradients gi are all nonnegative. (see exercise 6.12.)

bounding consumer preference

as an application, we consider a problem of predicting consumer preferences. we
consider di   erent baskets of goods, consisting of di   erent amounts of n consumer
goods. a goods basket is speci   ed by a vector x     [0, 1]n where xi denotes the
amount of consumer good i. we assume the amounts are normalized so that
0     xi     1, i.e., xi = 0 is the minimum and xi = 1 is the maximum possible
amount of good i. given two baskets of goods x and   x, a consumer can either
prefer x to   x, or prefer   x to x, or consider x and   x equally attractive. we consider
one model consumer, whose choices are repeatable.

340

6 approximation and    tting

we model consumer preference in the following way. we assume there is an
underlying utility function u : rn     r, with domain [0, 1]n; u(x) gives a measure
of the utility derived by the consumer from the goods basket x. given a choice
between two baskets of goods, the consumer chooses the one that has larger utility,
and will be ambivalent when the two baskets have equal utility. it is reasonable to
assume that u is monotone nondecreasing. this means that the consumer always
prefers to have more of any good, with the amounts of all other goods the same. it
is also reasonable to assume that u is concave. this models satiation, or decreasing
marginal utility as we increase the amount of goods.

now suppose we are given some consumer preference data, but we do not know
the underlying utility function u. speci   cally, we have a set of goods baskets
a1, . . . , am     [0, 1]n, and some information about preferences among them:

u(ai) > u(aj) for (i, j)     p,

u(ai)     u(aj) for (i, j)     pweak,

(6.22)

where p, pweak     {1, . . . , m}  {1, . . . , m} are given. here p gives the set of known
preferences: (i, j)     p means that basket ai is known to be preferred to basket aj.
the set pweak gives the set of known weak preferences: (i, j)     pweaid116 that
basket ai is preferred to basket aj, or that the two baskets are equally attractive.
we    rst consider the following question: how can we determine if the given data
are consistent, i.e., whether or not there exists a concave nondecreasing utility
function u for which (6.22) holds? this is equivalent to solving the feasibility
problem

u

   nd
subject to u : rn     r concave and nondecreasing

u(ai) > u(aj),
u(ai)     u(aj),

(i, j)     p
(i, j)     pweak,

(6.23)

with the function u as the (in   nite-dimensional) optimization variable. since the
constraints in (6.23) are all homogeneous, we can express the problem in the equiv-
alent form

u

   nd
subject to u : rn     r concave and nondecreasing

u(ai)     u(aj) + 1,
u(ai)     u(aj),

(i, j)     p
(i, j)     pweak,

(6.24)

which uses only nonstrict inequalities. (it is clear that if u satis   es (6.24), then
it must satisfy (6.23); conversely, if u satis   es (6.23), then it can be scaled to
satisfy (6.24).) this problem, in turn, can be cast as a (   nite-dimensional) linear
programming feasibility problem, using the interpolation result on page 339:

   nd
subject to

u1, . . . , um, g1, . . . , gm
gi (cid:23) 0,
i = 1, . . . , m
uj     ui + gt
i (aj     ai),
(i, j)     p
ui     uj + 1,
(i, j)     pweak.
ui     uj,

i, j = 1, . . . , m

(6.25)

by solving this id135 feasibility problem, we can determine whether
there exists a concave, nondecreasing utility function that is consistent with the

6.5 function    tting and interpolation

341

given sets of strict and nonstrict preferences. if (6.25) is feasible, there is at least
one such utility function (and indeed, we can construct one that is piecewise-linear,
from a feasible u1, . . . , um, g1, . . . , gm). if (6.25) is not feasible, we can conclude
that there is no concave increasing utility function that is consistent with the given
sets of strict and nonstrict preferences.

as an example, suppose that p and pweak are consumer preferences that are
known to be consistent with at least one concave increasing utility function. con-
sider a pair (k, l) that is not in p or pweak, i.e., consumer preference between
baskets k and l is not known. in some cases we can conclude that a preference
holds between basket k and l, even without knowing the underlying preference
function. to do this we augment the known preferences (6.22) with the inequality
u(ak)     u(al), which means that basket l is preferred to basket k, or they are
equally attractive. we then solve the feasibility linear program (6.25), including
the extra weak preference u(ak)     u(al). if the augmented set of preferences is in-
feasible, it means that any concave nondecreasing utility function that is consistent
with the original given consumer preference data must also satisfy u(ak) > u(al).
in other words, we can conclude that basket k is preferred to basket l, without
knowing the underlying utility function.

example 6.9 here we give a simple numerical example that illustrates the discussion
above. we consider baskets of two goods (so we can easily plot the goods baskets).
to generate the consumer preference data p, we compute 40 random points in [0, 1]2,
and then compare them using the utility function

u(x1, x2) = (1.1x1/2

1 + 0.8x1/2

2

)/1.9.

these goods baskets, and a few level curves of the utility function u, are shown in
   gure 6.25.

we now use the consumer preference data (but not, of course, the true utility function
u) to compare each of these 40 goods baskets to the basket a0 = (0.5, 0.5). for each
original basket ai, we solve the id135 feasibility problem described
above, to see if we can conclude that basket a0 is preferred to basket ai. similarly,
we check whether we can conclude that basket ai is preferred to basket a0. for each
basket ai, there are three possible outcomes: we can conclude that a0 is de   nitely
preferred to ai, that ai is de   nitely preferred to a0, or (if both lp feasibility problems
are feasible) that no conclusion is possible. (here, de   nitely preferred means that the
preference holds for any concave nondecreasing utility function that is consistent with
the original given data.)

we    nd that 21 of the baskets are de   nitely rejected in favor of (0.5, 0.5), and 14
of the baskets are de   nitely preferred. we cannot make any conclusion, from the
consumer preference data, about the remaining 5 baskets. these results are shown in
   gure 6.26. note that goods baskets below and to the left of (0.5, 0.5) will de   nitely
be rejected in favor of (0.5, 0.5), using only the monotonicity property of the utility
function, and similarly, those points that are above and to the right of (0.5, 0.5) must
be preferred. so for these 17 points, there is no need to solve the feasibility lp (6.25).
classifying the 23 points in the other two quadrants, however, requires the concavity
assumption, and solving the feasibility lp (6.25).

342

6 approximation and    tting

1

2
x

0.5

0
0

0.5
x1

1

figure 6.25 forty goods baskets a1, . . . , a40, shown as circles.
the
0.1, 0.2, . . . , 0.9 level curves of the true utility function u are shown as dashed
lines. this utility function is used to    nd the consumer preference data p
among the 40 baskets.

1

2
x

0.5

0
0

0.5
x1

1

figure 6.26 results of consumer preference analysis using the lp (6.25), for a
new goods basket a0 = (0.5, 0.5). the original baskets are displayed as open
circles if they are de   nitely rejected (u(ak) < u(a0)), as solid black circles
if they are de   nitely preferred (u(ak) > u(a0)), and as squares when no
conclusion can be made. the level curve of the underlying utility function,
that passes through (0.5, 0.5), is shown as a dashed curve. the vertical and
horizontal lines passing through (0.5, 0.5) divide [0, 1]2 into four quadrants.
points in the upper right quadrant must be preferred to (0.5, 0.5), by the
monotonicity assumption on u. similarly, (0.5, 0.5) must be preferred to the
points in the lower left quadrant. for the points in the other two quadrants,
the results are not obvious.

bibliography

bibliography

343

the robustness properties of approximations with di   erent penalty functions were an-
alyzed by huber [hub64, hub81], who also proposed the penalty function (6.4). the
log-barrier penalty function arises in control theory, where it is applied to the system
closed-loop frequency response, and has several names, e.g., central h   , or risk-averse
control; see boyd and barratt [bb91] and the references therein.

regularized approximation is covered in many books, including tikhonov and arsenin
[ta77] and hansen [han98]. tikhonov id173 is sometimes called ridge regression
(golub and van loan [gl89, page 564]). least-squares approximation with    1-norm
id173 is also known under the name lasso (tibshirani [tib96]). other least-
squares id173 and regressor selection techniques are discussed and compared in
hastie, tibshirani, and friedman [htf01,   3.4].
total variation denoising was introduced for image reconstruction by rudin, osher, and
fatemi [rof92].

the robust least-squares problem with norm bounded uncertainty (page 321) was in-
troduced by el ghaoui and lebret [el97], and chandrasekaran, golub, gu, and sayed
[cggs98]. el ghaoui and lebret also give the sdp formulation of the robust least-squares
problem with structured uncertainty (page 323).

chen, donoho, and saunders [cds01] discuss basis pursuit via id135. they
refer to the    1-norm regularized problem (6.18) as basis pursuit denoising. meyer and
pratt [mp68] is an early paper on the problem of bounding utility functions.

344

6 approximation and    tting

exercises

norm approximation and least-norm problems

6.1 quadratic bounds for log barrier penalty. let    : r     r be the log barrier penalty

function with limit a > 0:

  (u) =(cid:26)    a2 log(1     (u/a)2)

   

|u| < a
otherwise.

show that if u     rm satis   es kuk    < a, then

this means thatpm

a. for example, if kuk   /a = 0.25, then

i=1   (ui) is well approximated by kuk2

2 if kuk    is small compared to

kuk2

2    

mxi=1

  (ui)    

  (kuk   )
kuk2

    kuk2
2.

kuk2

2    

mxi=1

  (ui)     1.033    kuk2
2.

6.2    1-,    2-, and       -norm approximation by a constant vector. what is the solution of the

norm approximation problem with one scalar variable x     r,

for the    1-,    2-, and       -norms?

minimize

kx1     bk,

6.3 formulate the following approximation problems as lps, qps, socps, or sdps. the

problem data are a     rm  n and b     rm. the rows of a are denoted at
i .

(a) deadzone-linear penalty approximation: minimizepm

i=1   (at

i x     bi), where

  (u) =(cid:26) 0

|u|     a

|u|     a
|u| > a,

where a > 0.

(b) log-barrier penalty approximation: minimizepm
  (u) =(cid:26)    a2 log(1     (u/a)2)

   

i=1   (at

i x     bi), where
|u| < a
|u|     a,

with a > 0.

(c) huber penalty approximation: minimizepm

i=1   (at

  (u) =(cid:26) u2

m (2|u|     m )

i x     bi), where
|u|     m
|u| > m,

with m > 0.

(d) log-chebyshev approximation: minimize maxi=1,...,m | log(at

i x)    log bi|. we assume

b     0. an equivalent convex form is

minimize
subject to

t
1/t     at

i x/bi     t,

i = 1, . . . , m,

with variables x     rn and t     r, and domain rn    r++.

exercises

345

(e) minimizing the sum of the largest k residuals:

minimize pk

subject to

i=1 |r|[i]
r = ax     b,

where |r|[1]     |r|[2]                |r|[m] are the numbers |r1|, |r2|, . . . , |rm| sorted in
decreasing order. (for k = 1, this reduces to       -norm approximation; for k = m, it
reduces to    1-norm approximation.) hint. see exercise 5.19.

6.4 a di   erentiable approximation of    1-norm approximation. the function   (u) = (u2+  )1/2,
with parameter    > 0, is sometimes used as a di   erentiable approximation of the absolute
value function |u|. to approximately solve the    1-norm approximation problem

(6.26)

(6.27)

where a     rm  n, we solve instead the problem

minimize

kax     bk1,

minimize pm

i=1   (at

i x     bi),

is the ith row of a. we assume rank a = n.

where at
i
let p    denote the optimal value of the    1-norm approximation problem (6.26). let   x
denote the optimal solution of the approximate problem (6.27), and let   r denote the
associated residual,   r = a  x     b.
i=1   r2

i +   )1/2.

i /(  r2

(a) show that p       pm

(b) show that

ka  x     bk1     p    +

|  ri|(cid:18)1    

|  ri|

i +   )1/2(cid:19) .

(  r2

mxi=1

(by evaluating the righthand side after computing   x, we obtain a bound on how subop-
timal   x is for the    1-norm approximation problem.)

6.5 minimum length approximation. consider the problem

minimize
subject to

length(x)
kax     bk       ,

where length(x) = min{k | xi = 0 for i > k}. the problem variable is x     rn; the
problem parameters are a     rm  n, b     rm, and    > 0. in a regression context, we are
asked to    nd the minimum number of columns of a, taken in order, that can approximate
the vector b within   .
show that this is a quasiid76 problem.

6.6 duals of some penalty function approximation problems. derive a lagrange dual for the

problem

minimize pm

subject to

i=1   (ri)
r = ax     b,

for the following penalty functions    : r     r. the variables are x     rn, r     rm.
(a) deadzone-linear penalty (with deadzone width a = 1),

(b) huber penalty (with m = 1),

  (u) =(cid:26) 0
  (u) =(cid:26) u2

|u|     1

2|u|     1

|u|     1
|u| > 1.

|u|     1
|u| > 1.

346

6 approximation and    tting

(c) log-barrier (with limit a = 1),

  (u) =     log(1     u2),

dom    = (   1, 1).

(d) relative deviation from one,

  (u) = max{u, 1/u} =(cid:26) u

u     1
1/u u     1,

with dom    = r++.

id173 and robust approximation

6.7 bi-criterion optimization with euclidean norms. we consider the bi-criterion optimization

problem

minimize (w.r.t. r2

+)

(kax     bk2

2,kxk2
2),

where a     rm  n has rank r, and b     rm. show how to    nd the solution of each of the
following problems from the singular value decomposition of a,

a = u diag(  )v t =

  iuivt
i

rxi=1

(see   a.5.4).
(a) tikhonov id173: minimize kax     bk2
(b) minimize kax     bk2
(c) maximize kax     bk2
here    and    are positive parameters.
your results provide e   cient methods for computing the optimal trade-o    curve and the
set of achievable values of the bi-criterion problem.

2 subject to kxk2
2 subject to kxk2

2 +   kxk2
2.

2 =   .
2 =   .

6.8 formulate the following robust approximation problems as lps, qps, socps, or sdps.

for each subproblem, consider the    1-,    2-, and the       -norms.

(a) stochastic robust approximation with a    nite set of parameter values, i.e., the sum-

of-norms problem

where p (cid:23) 0 and 1t p = 1. (see   6.4.1.)

(b) worst-case robust approximation with coe   cient bounds:

minimize pk

i=1 pikaix     bk

minimize

supa   a kax     bk

where

a = {a     rm  n | lij     aij     uij, i = 1, . . . , m, j = 1, . . . , n}.

here the uncertainty set is described by giving upper and lower bounds for the
components of a. we assume lij < uij.

(c) worst-case robust approximation with polyhedral uncertainty:

minimize

supa   a kax     bk

where

a = {[a1        am]t | ciai (cid:22) di, i = 1, . . . , m}.

the uncertainty is described by giving a polyhedron pi = {ai | ciai (cid:22) di} of possible
values for each row. the parameters ci     rpi  n, di     rpi , i = 1, . . . , m, are given.
we assume that the polyhedra pi are nonempty and bounded.

exercises

347

function    tting and interpolation

6.9 minimax rational function    tting. show that the following problem is quasiconvex:

where

minimize

max

i=1,...,k(cid:12)(cid:12)(cid:12)(cid:12)

p(ti)

q(ti)     yi(cid:12)(cid:12)(cid:12)(cid:12)

p(t) = a0 + a1t + a2t2 +        + amtm,

q(t) = 1 + b1t +        + bntn,

and the domain of the objective function is de   ned as

d = {(a, b)     rm+1    rn | q(t) > 0,        t       }.

in this problem we    t a rational function p(t)/q(t) to given data, while constraining the
denominator polynomial to be positive on the interval [  ,   ]. the optimization variables
are the numerator and denominator coe   cients ai, bi. the interpolation points ti     [  ,   ],
and desired function values yi, i = 1, . . . , k, are given.

6.10 fitting data with a concave nonnegative nondecreasing quadratic function. we are given

the data

and wish to    t a quadratic function of the form

x1, . . . , xn     rn,

y1, . . . , yn     r,

f (x) = (1/2)xt p x + qt x + r,

where p     sn, q     rn, and r     r are the parameters in the model (and, therefore, the
variables in the    tting problem).
our model will be used only on the box b = {x     rn | l (cid:22) x (cid:22) u}. you can assume that
l     u, and that the given data points xi are in this box.
we will use the simple sum of squared errors objective,

nxi=1

(f (xi)     yi)2,

as the criterion for the    t. we also impose several constraints on the function f . first,
it must be concave. second, it must be nonnegative on b, i.e., f (z)     0 for all z     b.
third, f must be nondecreasing on b, i.e., whenever z,   z     b satisfy z (cid:22)   z, we have
f (z)     f (  z).
show how to formulate this    tting problem as a convex problem. simplify your formula-
tion as much as you can.

6.11 least-squares direction interpolation. suppose f1, . . . , fn : rk     rp, and we form the

linear combination f : rk     rp,

f (u) = x1f1(u) +        + xnfn(u),

where x is the variable in the interpolation problem.
in this problem we require that 6 (f (vj), qj) = 0, j = 1, . . . , m, where qj are given vectors
in rp, which we assume satisfy kqjk2 = 1. in other words, we require the direction of
f to take on speci   ed values at the points vj. to ensure that f (vj) is not zero (which
makes the angle unde   ned), we impose the minimum length constraints kf (vj)k2       ,
j = 1, . . . , m, where    > 0 is given.
show how to    nd x that minimizes kxk2, and satis   es the direction (and minimum length)
conditions above, using id76.
6.12 interpolation with monotone functions. a function f : rk     r is monotone nondecreas-

ing (with respect to rk

+) if f (u)     f (v) whenever u (cid:23) v.

348

6 approximation and    tting

(a) show that there exists a monotone nondecreasing function f : rk     r, that satis   es

f (ui) = yi for i = 1, . . . , m, if and only if

yi     yj whenever ui (cid:23) uj,

i, j = 1, . . . , m.

(b) show that there exists a convex monotone nondecreasing function f : rk     r, with
dom f = rk, that satis   es f (ui) = yi for i = 1, . . . , m, if and only if there exist
gi     rk, i = 1, . . . , m, such that
i = 1, . . . , m,

i, j = 1, . . . , m.

yj     yi + gt

i (uj     ui),

gi (cid:23) 0,

6.13 interpolation with quasiconvex functions. show that there exists a quasiconvex function
f : rk     r, that satis   es f (ui) = yi for i = 1, . . . , m, if and only if there exist gi     rk,
i = 1, . . . , m, such that

gt
i (uj     ui)        1 whenever yj < yi,

i, j = 1, . . . , m.

6.14 [nes00] interpolation with positive-real functions. suppose z1, . . . , zn     c are n distinct
points with |zi| > 1. we de   ne knp as the set of vectors y     cn for which there exists a
function f : c     c that satis   es the following conditions.

    f is positive-real, which means it is analytic outside the unit circle (i.e., for |z| > 1),

and its real part is nonnegative outside the unit circle (   f (z)     0 for |z| > 1).

    f satis   es the interpolation conditions

f (z1) = y1,

f (z2) = y2,

. . . ,

f (zn) = yn.

if we denote the set of positive-real functions as f , then we can express knp as

knp = {y     cn |    f     f , yk = f (zk), k = 1, . . . , n}.

(a) it can be shown that f is positive-real if and only if there exists a nondecreasing

function    such that for all z with |z| > 1,

f (z) = i   f (   ) +z 2  

0

ei   + z   1

ei       z   1 d  (  ),

is a closed convex cone.

where i =       1 (see [kn77, page 389]). use this representation to show that knp
(b) we will use the inner product    (xh y) between vectors x, y     cn, where xh denotes
the complex conjugate transpose of x. show that the dual cone of knp is given by

l !     0           [0, 2  ]) .

l

e   i   +   z   1
e   i         z   1

qkl
1     z   1
k   z   1

l

, l = 1, . . . , n)

(c) show that

k    

xl

k    

np =(x     cn (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    (1t x) = 0,      nxl=1
np =(x     cn (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    q     hn
nxk=1
nxk=0

+, xl =

(yke   ik   +   ykeik  )

where hn
+ denotes the set of positive semide   nite hermitian matrices of size n    n.
use the following result (known as riesz-fej  er theorem; see [kn77, page 60]). a
function of the form

exercises

349

is nonnegative for all    if and only if there exist a0, . . . , an     c such that

nxk=0

(yke   ik   +   ykeik  ) =(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

nxk=0

2

.

akeik  (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(d) show that knp = {y     cn | p (y) (cid:23) 0} where p (y)     hn is de   ned as

p (y)kl =

yk + yl
1     z   1

k   z   1

l

,

l, k = 1, . . . , n.

the matrix p (y) is called the nevanlinna-pick matrix associated with the points
zk, yk.
hint. as we noted in part (a), knp is a closed convex cone, so knp = k       
np.

(e) as an application, pose the following problem as a id76 problem:

minimize pn

subject to

f     f .

k=1 |f (zk)     wk|2

the problem data are n points zk with |zk| > 1 and n complex numbers w1, . . . ,
wn. we optimize over all positive-real functions f .

chapter 7

statistical estimation

7.1 parametric distribution estimation

7.1.1 id113

we consider a family of id203 distributions on rm, indexed by a vector
x     rn, with densities px(  ). when considered as a function of x, for    xed y     rm,
the function px(y) is called the likelihood function. it is more convenient to work
with its logarithm, which is called the log-likelihood function, and denoted l:

l(x) = log px(y).

there are often constraints on the values of the parameter x, which can repre-
sent prior knowledge about x, or the domain of the likelihood function. these
constraints can be explicitly given, or incorporated into the likelihood function by
assigning px(y) = 0 (for all y) whenever x does not satisfy the prior information
constraints. (thus, the log-likelihood function can be assigned the value        for
parameters x that violate the prior information constraints.)
now consider the problem of estimating the value of the parameter x, based
on observing one sample y from the distribution. a widely used method, called
maximum likelihood (ml) estimation, is to estimate x as

  xml = argmaxxpx(y) = argmaxxl(x),

i.e., to choose as our estimate a value of the parameter that maximizes the like-
lihood (or log-likelihood) function for the observed value of y.
if we have prior
information about x, such as x     c     rn, we can add the constraint x     c
explicitly, or impose it implicitly, by rede   ning px(y) to be zero for x 6    c.
the problem of    nding a maximum likelihood estimate of the parameter vector
x can be expressed as

l(x) = log px(y)

maximize
subject to x     c,

(7.1)

where x     c gives the prior information or other constraints on the parameter
vector x. in this optimization problem, the vector x     rn (which is the parameter

352

7 statistical estimation

in the id203 density) is the variable, and the vector y     rm (which is the
observed sample) is a problem parameter.
the id113 problem (7.1) is a id76
problem if the log-likelihood function l is concave for each value of y, and the set
c can be described by a set of linear equality and convex inequality constraints, a
situation which occurs in many estimation problems. for these problems we can
compute an ml estimate using id76.

linear measurements with iid noise

we consider a linear measurement model,

yi = at

i x + vi,

i = 1, . . . , m,

where x     rn is a vector of parameters to be estimated, yi     r are the measured
or observed quantities, and vi are the measurement errors or noise. we assume
that vi are independent, identically distributed (iid), with density p on r. the
likelihood function is then

px(y) =

myi=1

p(yi     at

i x),

so the log-likelihood function is

l(x) = log px(y) =

mxi=1

log p(yi     at

i x).

the ml estimate is any optimal point for the problem
i=1 log p(yi     at

i x),

maximize pm

with variable x. if the density p is log-concave, this problem is convex, and has the
form of a penalty approximation problem ((6.2), page 294), with penalty function
    log p.

(7.2)

example 7.1 ml estimation for some common noise densities.

    gaussian noise. when vi are gaussian with zero mean and variance   2, the

density is p(z) = (2    2)   1/2e   z2/2  2

, and the log-likelihood function is

l(x) =    (m/2) log(2    2)    
1 , . . . , at

1
2  2 kax     yk2
2,
m. therefore the ml estimate of
2, the solution of a least-squares approximation

where a is the matrix with rows at
x is xml = argminx kax     yk2
problem.

    laplacian noise. when vi are laplacian, i.e., have density p(z) = (1/2a)e   |z|/a
(where a > 0), the ml estimate is   x = argminx kax     yk1, the solution of the
   1-norm approximation problem.

    uniform noise. when vi are uniformly distributed on [   a, a], we have p(z) =

1/(2a) on [   a, a], and an ml estimate is any x satisfying kax     yk        a.

7.1 parametric distribution estimation

353

ml interpretation of penalty function approximation

conversely, we can interpret any penalty function approximation problem

as a id113 problem, with noise density

minimize pm

i=1   (bi     at

i x)

p(z) =

,

e     (z)

r e     (u) du

and measurements b. this observation gives a statistical interpretation of the
penalty function approximation problem. suppose, for example, that the penalty
function    grows very rapidly for large values, which means that we attach a very
large cost or penalty to large residuals. the corresponding noise density function
p will have very small tails, and the ml estimator will avoid (if possible) estimates
with any large residuals because these correspond to very unlikely events.

we can also understand the robustness of    1-norm approximation to large errors
in terms of id113. we interpret    1-norm approximation
as id113 with a noise density that is laplacian;    2-norm
approximation is id113 with a gaussian noise density.
the laplacian density has larger tails than the gaussian, i.e., the id203 of a
very large vi is far larger with a laplacian than a gaussian density. as a result,
the associated maximum likelihood method expects to see greater numbers of large
residuals.

counting problems with poisson distribution

in a wide variety of problems the random variable y is nonnegative integer valued,
with a poisson distribution with mean    > 0:

prob(y = k) =

e       k

k!

.

often y represents the count or number of events (such as photon arrivals, tra   c
accidents, etc.) of a poisson process over some period of time.

in a simple statistical model, the mean    is modeled as an a   ne function of a

vector u     rn:

   = at u + b.

here u is called the vector of explanatory variables, and the vector a     rn and
number b     r are called the model parameters. for example, if y is the number
of tra   c accidents in some region over some period, u1 might be the total tra   c
   ow through the region during the period, u2 the rainfall in the region during the
period, and so on.

we are given a number of observations which consist of pairs (ui, yi), i =
1, . . . , m, where yi is the observed value of y for which the value of the explanatory
variable is ui     rn. our job is to    nd a maximum likelihood estimate of the model
parameters a     rn and b     r from these data.

354

7 statistical estimation

the likelihood function has the form

(at ui + b)yi exp(   (at ui + b))

yi!

,

myi=1

so the log-likelihood function is

l(a, b) =

mxi=1

(yi log(at ui + b)     (at ui + b)     log(yi!)).

we can    nd an ml estimate of a and b by solving the id76 problem

maximize pm

where the variables are a and b.

i=1(yi log(at ui + b)     (at ui + b)),

id28
we consider a random variable y     {0, 1}, with

prob(y = 1) = p,

prob(y = 0) = 1     p,

where p     [0, 1], and is assumed to depend on a vector of explanatory variables
u     rn. for example, y = 1 might mean that an individual in a population acquires
a certain disease. the id203 of acquiring the disease is p, which is modeled
as a function of some explanatory variables u, which might represent weight, age,
height, blood pressure, and other medically relevant variables.

the logistic model has the form

p =

exp(at u + b)

1 + exp(at u + b)

,

(7.3)

where a     rn and b     r are the model parameters that determine how the
id203 p varies as a function of the explanatory variable u.
now suppose we are given some data consisting of a set of values of the explana-
tory variables u1, . . . , um     rn along with the corresponding outcomes y1, . . . , ym    
{0, 1}. our job is to    nd a maximum likelihood estimate of the model parameters
a     rn and b     r. finding an ml estimate of a and b is sometimes called logistic
regression.
we can re-order the data so for u1, . . . , uq, the outcome is y = 1, and for

uq+1, . . . , um the outcome is y = 0. the likelihood function then has the form

pi

qyi=1

myi=q+1

(1     pi),

where pi is given by the logistic model with explanatory variable ui. the log-
likelihood function has the form

l(a, b) =

log pi +

qxi=1

mxi=q+1

log(1     pi)

7.1 parametric distribution estimation

355

)
1
=
y
(
b
o
r
p

1

0.8

0.6

0.4

0.2

0

0

2

4

6

u

8

10

figure 7.1 id28. the circles show 50 points (ui, yi), where
ui     r is the explanatory variable, and yi     {0, 1} is the outcome. the
data suggest that for u < 5 or so, the outcome is more likely to be y = 0,
while for u > 5 or so, the outcome is more likely to be y = 1. the data
also suggest that for u < 2 or so, the outcome is very likely to be y = 0,
and for u > 8 or so, the outcome is very likely to be y = 1. the solid
curve shows prob(y = 1) = exp(au + b)/(1 + exp(au + b)) for the maximum
likelihood parameters a, b. this maximum likelihood model is consistent
with our informal observations about the data set.

log

exp(at ui + b)

1 + exp(at ui + b)

+

mxi=q+1

log

1

1 + exp(at ui + b)

log(1 + exp(at ui + b)).

=

=

qxi=1
qxi=1

(at ui + b)    

mxi=1

since l is a concave function of a and b, the id28 problem can be solved
as a id76 problem. figure 7.1 shows an example with u     r.
covariance estimation for gaussian variables
suppose y     rn is a gaussian random variable with zero mean and covariance
matrix r = e yyt , so its density is

pr(y) = (2  )   n/2 det(r)   1/2 exp(   yt r   1y/2),

where r     sn
++. we want to estimate the covariance matrix r based on n in-
dependent samples y1, . . . , yn     rn drawn from the distribution, and using prior
knowledge about r.

the log-likelihood function has the form

l(r) = log pr(y1, . . . , yn )

356

7 statistical estimation

=    (n n/2) log(2  )     (n/2) log det r     (1/2)
=    (n n/2) log(2  )     (n/2) log det r     (n/2) tr(r   1y ),

yt
k r   1yk

nxk=1

where

y =

1
n

nxk=1

ykyt
k

is the sample covariance of y1, . . . , yn . this log-likelihood function is not a concave
function of r (although it is concave on a subset of its domain sn
++; see exercise 7.4),
but a change of variable yields a concave log-likelihood function. let s denote the
inverse of the covariance matrix, s = r   1 (which is called the information matrix ).
using s in place of r as a new parameter, the log-likelihood function has the form

l(s) =    (n n/2) log(2  ) + (n/2) log det s     (n/2) tr(sy ),

which is a concave function of s.

therefore the ml estimate of s (hence, r) is found by solving the problem

maximize
subject to s     s

log det s     tr(sy )

(7.4)

where s is our prior knowledge of s = r   1. (we also have the implicit constraint
that s     sn
++.) since the objective function is concave, this is a convex problem
if the set s can be described by a set of linear equality and convex inequality
constraints.
first we examine the case in which no prior assumptions are made on r (hence,
s), other than r     0. in this case the problem (7.4) can be solved analytically. the
gradient of the objective is s   1   y , so the optimal s satis   es s   1 = y if y     sn
++.
(if y 6    sn
++, the log-likelihood function is unbounded above.) therefore, when
we have no prior assumptions about r, the maximum likelihood estimate of the
covariance is, simply, the sample covariance:   rml = y .

now we consider some examples of constraints on r that can be expressed as
convex constraints on the information matrix s. we can handle lower and upper
(matrix) bounds on r, of the form

l (cid:22) r (cid:22) u,

where l and u are symmetric and positive de   nite, as

a condition number constraint on r,

u    1 (cid:22) r   1 (cid:22) l   1.

can be expressed as

  max(r)       max  min(r),

  max(s)       max  min(s).

7.1 parametric distribution estimation

357

this is equivalent to the existence of u > 0 such that ui (cid:22) s (cid:22)   maxui. we can
therefore solve the ml problem, with the condition number constraint on r, by
solving the convex problem

log det s     tr(sy )
maximize
subject to ui (cid:22) s (cid:22)   maxui

(7.5)

where the variables are s     sn and u     r.
functions of the underlying random vector y,

as another example, suppose we are given bounds on the variance of some linear

these prior assumptions can be expressed as

e(ct

i y)2       i,

i = 1, . . . , k.

e(ct

i y)2 = ct

i rci = ct

i s   1ci       i,

i = 1, . . . , k.

since ct
bounds can be imposed in the ml problem.

i s   1ci is a convex function of s (provided s     0, which holds here), these

7.1.2 maximum a posteriori id203 estimation

maximum a posteriori id203 (map) estimation can be considered a bayesian
version of id113, with a prior id203 density on the
underlying parameter x. we assume that x (the vector to be estimated) and y (the
observation) are random variables with a joint id203 density p(x, y). this
is in contrast to the statistical estimation setup, where x is a parameter, not a
random variable.

the prior density of x is given by

px(x) =z p(x, y) dy.

this density represents our prior information about what the values of the vector x
might be, before we observe the vector y. similarly, the prior density of y is given
by

py(y) =z p(x, y) dx.

this density represents the prior information about what the measurement or ob-
servation vector y will be.

the conditional density of y, given x, is given by

py|x(x, y) =

p(x, y)
px(x)

.

in the map estimation method, py|x plays the role of the parameter dependent
density px in the id113 setup. the conditional density
of x, given y, is given by

px|y(x, y) =

p(x, y)
py(y)

= py|x(x, y)

px(x)
py(y)

.

358

7 statistical estimation

when we substitute the observed value y into px|y, we obtain the posterior density
of x. it represents our knowledge of x after the observation.

in the map estimation method, our estimate of x, given the observation y, is

given by

  xmap = argmaxxpx|y(x, y)

= argmaxxpy|x(x, y)px(x)
= argmaxxp(x, y).

in other words, we take as estimate of x the value that maximizes the conditional
density of x, given the observed value of y. the only di   erence between this
estimate and the maximum likelihood estimate is the second term, px(x), appearing
here. this term can be interpreted as taking our prior knowledge of x into account.
note that if the prior density of x is uniform over a set c, then    nding the map
estimate is the same as maximizing the likelihood function subject to x     c, which
is the ml estimation problem (7.1).

taking logarithms, we can express the map estimate as

  xmap = argmaxx(log py|x(x, y) + log px(x)).

(7.6)

the    rst term is essentially the same as the log-likelihood function; the second
term penalizes choices of x that are unlikely, according to the prior density (i.e., x
with px(x) small).

brushing aside the philosophical di   erences in setup, the only di   erence between
   nding the map estimate (via (7.6)) and the ml estimate (via (7.1)) is the presence
of an extra term in the optimization problem, associated with the prior density of
x. therefore, for any id113 problem with concave log-
likelihood function, we can add a prior density for x that is log-concave, and the
resulting map estimation problem will be convex.

linear measurements with iid noise
suppose that x     rn and y     rm are related by

yi = at

i x + vi,

i = 1, . . . , m,

where vi are iid with density pv on r, and x has prior density px on rn. the
joint density of x and y is then

p(x, y) = px(x)

pv(yi     at

i x),

myi=1
log px(x) +pm

and the map estimate can be found by solving the optimization problem

maximize

i=1 log pv(yi     at

i x).

(7.7)

if px and pv are log-concave, this problem is convex. the only di   erence between
the map estimation problem (7.7) and the associated ml estimation problem (7.2)
is the extra term log px(x).

7.2 nonparametric distribution estimation

359

for example, if vi are uniform on [   a, a], and the prior distribution of x is
gaussian with mean   x and covariance   , the map estimate is found by solving
the qp

minimize
subject to

(x       x)t      1(x       x)
kax     yk        a,

with variable x.

map with perfect linear measurements
suppose x     rn is a vector of parameters to be estimated, with prior density
px. we have m perfect (noise free, deterministic) linear measurements, given by
y = ax. in other words, the conditional distribution of y, given x, is a point mass
with value one at the point ax. the map estimate can be found by solving the
problem

maximize
log px(x)
subject to ax = y.

if px is log-concave, this is a convex problem.

if under the prior distribution, the parameters xi are iid with density p on r,

then the map estimation problem has the form

maximize pn

subject to ax = y,

i=1 log p(xi)

which is a least-penalty problem ((6.6), page 304), with penalty function   (u) =
    log p(u).

conversely, we can interpret any least-penalty problem,

minimize
subject to ax = b

  (x1) +        +   (xn)

as a map estimation problem, with m perfect linear measurements (i.e., ax = b)
and xi iid with density

p(z) =

.

e     (z)

r e     (u) du

7.2 nonparametric distribution estimation

we consider a random variable x with values in the    nite set {  1, . . . ,   n}     r.
(we take the values to be in r for simplicity; the same ideas can be applied when
the values are in rk, for example.) the distribution of x is characterized by
p     rn, with prob(x =   k) = pk. clearly, p satis   es p (cid:23) 0, 1t p = 1. conversely,
if p     rn satis   es p (cid:23) 0, 1t p = 1, then it de   nes a id203 distribution for a
random variable x, de   ned as prob(x =   k) = pk. thus, the id203 simplex

{p     rn | p (cid:23) 0, 1t p = 1}

360

7 statistical estimation

is in one-to-one correspondence with all possible id203 distributions for a
random variable x taking values in {  1, . . . ,   n}.
in this section we discuss methods used to estimate the distribution p based on
a combination of prior information and, possibly, observations and measurements.

prior information

many types of prior information about p can be expressed in terms of linear equality
constraints or inequalities. if f : r     r is any function, then

e f (x) =

pif (  i)

nxi=1

is a linear function of p. as a special case, if c     r, then prob(x     c) is a linear
function of p:

prob(x     c) = ct p,

ci =(cid:26) 1   i     c

0   i 6    c.

it follows that known expected values of certain functions (e.g., moments) or known
probabilities of certain sets can be incorporated as linear equality constraints on
p     rn. inequalities on expected values or probabilities can be expressed as linear
inequalities on p     rn.
for example, suppose we know that x has mean e x =   , second moment
e x 2 =   , and prob(x     0)     0.3. this prior information can be expressed as

e x =

nxi=1

  ipi =   ,

e x 2 =

nxi=1

  2

i pi =   , x  i   0

pi     0.3,

which are two linear equalities and one linear inequality in p.

we can also include some prior constraints that involve nonlinear functions of

p. as an example, the variance of x is given by

var(x) = e x 2     (e x)2 =

  2

i pi      nxi=1

nxi=1

  ipi!2

.

the    rst term is a linear function of p and the second term is concave quadratic
in p, so the variance of x is a concave function of p. it follows that a lower bound
on the variance of x can be expressed as a convex quadratic inequality on p.

as another example, suppose a and b are subsets of r, and consider the

id155 of a given b:

prob(x     a|x     b) =

prob(x     a     b)

prob(x     b)

.

this function is linear-fractional in p     rn: it can be expressed as

where

prob(x     a|x     b) = ct p/dt p,

ci =(cid:26) 1   i     a     b

0   i 6    a     b

,

di =(cid:26) 1   i     b

0   i 6    b.

7.2 nonparametric distribution estimation

361

therefore we can express the prior constraints

l     prob(x     a|x     b)     u

as the linear inequality constraints on p

ldt p     ct p     udt p.

several other types of prior information can be expressed in terms of nonlinear

convex inequalities. for example, the id178 of x, given by

   

nxi=1

pi log pi,

is a concave function of p, so we can impose a minimum value of id178 as a convex
inequality on p. if q represents another distribution, i.e., q (cid:23) 0, 1t q = 1, then
the id181 between the distribution q and the distribution p
is given by

pi log(pi/qi),

nxi=1

which is convex in p (and q as well; see example 3.19, page 90). it follows that
we can impose a maximum id181 between p and a given
distribution q, as a convex inequality on p.

in the next few paragraphs we express the prior information about the distribu-
tion p as p     p. we assume that p can be described by a set of linear equalities and
convex inequalities. we include in the prior information p the basic constraints
p (cid:23) 0, 1t p = 1.
bounding probabilities and expected values
given prior information about the distribution, say p     p, we can compute upper
or lower bounds on the expected value of a function, or id203 of a set. for
example to determine a lower bound on e f (x) over all distributions that satisfy
the prior information p     p, we solve the convex problem

minimize pn

subject to p     p.

i=1 f (  i)pi

id113

we can use id113 to estimate p based on observations
from the distribution. suppose we observe n independent samples x1, . . . , xn from
the distribution. let ki denote the number of these samples with value   i, so that
k1 +        + kn = n , the total number of observed samples. the log-likelihood
function is then

l(p) =

ki log pi,

nxi=1

362

7 statistical estimation

which is a concave function of p. the maximum likelihood estimate of p can be
found by solving the convex problem

maximize
subject to p     p,

l(p) =pn

i=1 ki log pi

with variable p.

maximum id178

the maximum id178 distribution consistent with the prior assumptions can be
found by solving the convex problem

minimize pn

subject to p     p.

i=1 pi log pi

enthusiasts describe the maximum id178 distribution as the most equivocal or
most random, among those consistent with the prior information.

minimum id181

we can    nd the distribution p that has minimum id181 from
a given prior distribution q, among those consistent with prior information, by
solving the convex problem

minimize pn

subject to p     p,

i=1 pi log(pi/qi)

note that when the prior distribution is the uniform distribution, i.e., q = (1/n)1,
this problem reduces to the maximum id178 problem.

example 7.2 we consider a id203 distribution on 100 equidistant points   i in
the interval [   1, 1]. we impose the following prior assumptions:

e x     [   0.1, 0.1]
e x 2     [0.5, 0.6]
e(3x 3     2x)     [   0.3,    0.2]
prob(x < 0)     [0.3, 0.4].

(7.8)

along with the constraints 1t p = 1, p (cid:23) 0, these constraints describe a polyhedron
of id203 distributions.

figure 7.2 shows the maximum id178 distribution that satis   es these constraints.
the maximum id178 distribution satis   es

e x = 0.056
e x 2 = 0.5
e(3x 3     2x) =    0.2
prob(x < 0) = 0.4.

to illustrate bounding probabilities, we compute upper and lower bounds on the
cumulative distribution prob(x       i), for i = 1, . . . , 100. for each value of i,

7.2 nonparametric distribution estimation

363

0.04

0.03

0.02

0.01

)
i

  
=
x
(
b
o
r
p
=

i
p

0
   1

   0.5

0
  i

0.5

1

figure 7.2 maximum id178 distribution that satis   es the constraints (7.8).

we solve two lps: one that maximizes prob(x       i), and one that minimizes
prob(x       i), over all distributions consistent with the prior assumptions (7.8).
the results are shown in    gure 7.3. the upper and lower curves show the upper and
lower bounds, respectively; the middle curve shows the cumulative distribution of the
maximum id178 distribution.

example 7.3 bounding risk id203 with known marginal distributions. suppose x
and y are two random variables that give the return on two investments. we assume
that x takes values in {  1, . . . ,   n}     r and y takes values in {  1, . . . ,   m}     r,
with pij = prob(x =   i, y =   j). the marginal distributions of the two returns x
and y are known, i.e.,

pij = ri,

i = 1, . . . , n,

mxj=1

nxi=1

pij = qj,

j = 1, . . . , m,

(7.9)

but otherwise nothing is known about the joint distribution p. this de   nes a poly-
hedron of joint distributions consistent with the given marginals.

now suppose we make both investments, so our total return is the random variable
x + y . we are interested in computing an upper bound on the id203 of some
level of loss, or low return, i.e., prob(x + y <   ). we can compute a tight upper
bound on this id203 by solving the lp

maximize p{pij |   i +   j <   }

pij     0,

subject to

(7.9),

i = 1, . . . n,

j = 1, . . . , m.

the optimal value of this lp is the maximum id203 of loss. the optimal
solution p    is the joint distribution, consistent with the given marginal distributions,
that maximizes the id203 of the loss.

the same method can be applied to a derivative of the two investments. let r(x, y )
be the return of the derivative, where r : r2     r. we can compute sharp lower

364

7 statistical estimation

)
i

  
   
x
(
b
o
r
p

1

0.8

0.6

0.4

0.2

0

   1

   0.5

0
  i

0.5

1

figure 7.3 the top and bottom curves show the maximum and minimum
possible values of the cumulative distribution function, prob(x       i), over
all distributions that satisfy (7.8). the middle curve is the cumulative dis-
tribution of the maximum id178 distribution that satis   es (7.8).

and upper bounds on prob(r <   ) by solving a similar lp, with objective function

which we can minimize and maximize.

x{pij | r(  i,   j) <   } ,

7.3 optimal detector design and hypothesis testing

suppose x is a random variable with values in {1, . . . , n}, with a distribution that
depends on a parameter        {1, . . . , m}. the distributions of x, for the m possible
values of   , can be represented by a matrix p     rn  m, with elements

pkj = prob(x = k |    = j).

the jth column of p gives the id203 distribution associated with the param-
eter value    = j.

we consider the problem of estimating   , based on an observed sample of x. in
other words, the sample x is generated from one of the m possible distributions,
and we are to guess which one. the m values of    are called hypotheses, and guessing
which hypothesis is correct (i.e., which distribution generated the observed sample
x) is called hypothesis testing. in many cases one of the hypotheses corresponds
to some normal situation, and each of the other hypotheses corresponds to some
abnormal event. in this case hypothesis testing can be interpreted as observing a

7.3 optimal detector design and hypothesis testing

365

value of x, and then guessing whether or not an abnormal event has occurred, and
if so, which one. for this reason hypothesis testing is also called detection.

in most cases there is no signi   cance to the ordering of the hypotheses; they are
simply m di   erent hypotheses, arbitrarily labeled    = 1, . . . , m. if      =   , where     
denotes the estimate of   , then we have correctly guessed the parameter value   . if
     6=   , then we have (incorrectly) guessed the parameter value   ; we have mistaken
     for   . in other cases, there is signi   cance in the ordering of the hypotheses. in this
case, an event such as      >   , i.e., the event that we overestimate   , is meaningful.
it is also possible to parametrize    by values other than {1, . . . , m}, say as       
{  1, . . . ,   m}, where   i are (distinct) values. these values could be real numbers, or
vectors, for example, specifying the mean and variance of the kth distribution. in
this case, a quantity such as k          k, which is the norm of the parameter estimation
error, is meaningful.

7.3.1 deterministic and randomized detectors

a (deterministic) estimator or detector is a function    from {1, . . . , n} (the set of
possible observed values) into {1, . . . , m} (the set of hypotheses). if x is observed
to have value k, then our guess for the value of    is      =   (k). one obvious
deterministic detector is the maximum likelihood detector, given by

     =   ml(k) = argmax

j

pkj.

(7.10)

when we observe the value x = k, the maximum likelihood estimate of    is a
value that maximizes the id203 of observing x = k, over the set of possible
distributions.

we will consider a generalization of the deterministic detector, in which the
estimate of   , given an observed value of x, is random. a randomized detector
of    is a random variable          {1, . . . , m}, with a distribution that depends on the
observed value of x. a randomized detector can be de   ned in terms of a matrix
t     rm  n with elements

tik = prob(     = i | x = k).

the interpretation is as follows: if we observe x = k, then the detector gives      = i
with id203 tik. the kth column of t , which we will denote tk, gives the
id203 distribution of     , when we observe x = k. if each column of t is a
unit vector, then the randomized detector is a deterministic detector, i.e.,      is a
(deterministic) function of the observed value of x.

at    rst glance, it seems that intentionally introducing additional randomiza-
tion into the estimation or detection process can only make the estimator worse.
but we will see below examples in which a randomized detector outperforms all
deterministic estimators.

we are interested in designing the matrix t that de   nes the randomized detec-
tor. obviously the columns tk of t must satisfy the (linear equality and inequality)
constraints

tk (cid:23) 0,

1t tk = 1.

(7.11)

366

7 statistical estimation

7.3.2 detection id203 matrix

for the randomized detector de   ned by the matrix t , we de   ne the detection
id203 matrix as d = t p . we have

dij = (t p )ij = prob(     = i |    = j),

so dij is the id203 of guessing      = i, when in fact    = j. the m    m
detection id203 matrix d characterizes the performance of the randomized
detector de   ned by t . the diagonal entry dii is the id203 of guessing      = i
when    = i, i.e., the id203 of correctly detecting that    = i. the o   -diagonal
entry dij (with i 6= j) is the id203 of mistaking    = i for    = j, i.e., the
id203 that our guess is      = i, when in fact    = j. if d = i, the detector is
perfect: no matter what the parameter    is, we correctly guess      =   .

the diagonal entries of d, arranged in a vector, are called the detection proba-

bilities, and denoted p d:

i = dii = prob(     = i |    = i).
p d

the error probabilities are the complements, and are denoted p e:

i = 1     dii = prob(     6= i |    = i).
p e

since the columns of the detection id203 matrix d add up to one, we can
express the error probabilities as

p e

i =xj6=i

dji.

7.3.3 optimal detector design

in this section we show that a wide variety of objectives for detector design are
linear, a   ne, or convex piecewise-linear functions of d, and therefore also of t
(which is the optimization variable). similarly, a variety of constraints for detector
design can be expressed in terms of linear inequalities in d. it follows that a wide
variety of optimal detector design problems can be expressed as lps. we will see
in   7.3.4 that some of these lps have simple solutions; in this section we simply
formulate the problem.

limits on errors and detection probabilities

we can impose a lower bound on the id203 of correctly detecting the jth
hypothesis,

p d
j = djj     lj,

which is a linear inequality in d (hence, t ). similarly, we can impose a maximum
allowable id203 for mistaking    = i for    = j:

dij     uij,

7.3 optimal detector design and hypothesis testing

367

which are also linear constraints on t . we can take any of the detection prob-
abilities as an objective to be maximized, or any of the error probabilities as an
objective to be minimized.

minimax detector design

we can take as objective (to be minimized) the minimax error id203, maxj p e
j ,
which is a piecewise-linear convex function of d (hence, also of t ). with this as
the only objective, we have the problem of minimizing the maximum id203
of detection error,

minimize maxj p e
j
tk (cid:23) 0,
subject to

1t tk = 1,

k = 1, . . . , n,

where the variables are t1, . . . , tn     rm. this can be reformulated as an lp. the
minimax detector minimizes the worst-case (largest) id203 of error over all m
hypotheses.

we can, of course, add further constraints to the minimax detector design prob-

lem.

bayes detector design

in bayes detector design, we have a prior distribution for the hypotheses, given by
q     rm, where

qi = prob(   = i).

in this case, the probabilities pij are interpreted as conditional probabilities of x,
given   . the id203 of error for the detector is then given by qt p e, which is
an a   ne function of t . the bayes optimal detector is the solution of the lp

minimize
subject to

qt p e
tk (cid:23) 0,

1t tk = 1,

k = 1, . . . , n.

we will see in   7.3.4 that this problem has a simple analytical solution.
one special case is when q = (1/m)1. in this case the bayes optimal detector
minimizes the average id203 of error, where the (unweighted) average is over
the hypotheses. in   7.3.4 we will see that the maximum likelihood detector (7.10)
is optimal for this problem.

bias, mean-square error, and other quantities

in this section we assume that the ordering of the values of    have some signi   cance,
i.e., that the value    = i can be interpreted as a larger value of the parameter than
   = j, when i > j. this might be the case, for example, when    = i corresponds to
the hypothesis that i events have occurred. here we may be interested in quantities
such as

which is the id203 that we overestimate    when    = i. this is an a   ne
function of d:

prob(     >    |    = i),
prob(     >    |    = i) =xj>i

dji,

368

7 statistical estimation

so a maximum allowable value for this id203 can be expressed as a linear
inequality on d (hence, t ). as another example, the id203 of misclassifying
   by more than one, when    = i,

prob(|           | > 1 |    = i) = x|j   i|>1

dji,

is also a linear function of d.

we now suppose that the parameters have values {  1, . . . ,   m}     r. the es-
timation or detection (parameter) error is then given by            , and a number of
quantities of interest are given by linear functions of d. examples include:

    bias. the bias of the detector, when    =   i, is given by the linear function

e
i

(           ) =

mxj=1

(  j       i)dji,

where the subscript on e means the expectation is with respect to the dis-
tribution of the hypothesis    =   i.

    mean square error. the mean square error of the detector, when    =   i, is

given by the linear function

e
i

(           )2 =

mxj=1

(  j       i)2dji.

    average absolute error. the average absolute error of the detector, when

   =   i, is given by the linear function

e

i |           | =

mxj=1

|  j       i|dji.

7.3.4 multicriterion formulation and scalarization

the optimal detector design problem can be considered a multicriterion problem,
with the constraints (7.11), and the m(m     1) objectives given by the o   -diagonal
entries of d, which are the probabilities of the di   erent types of detection error:

minimize (w.r.t. rm(m   1)
subject to

+

) dij,

tk (cid:23) 0,

i, j = 1, . . . , m,

i 6= j

1t tk = 1,

k = 1, . . . , n,

(7.12)

with variables t1, . . . , tn     rm. since each objective dij is a linear function of the
variables, this is a multicriterion linear program.
we can scalarize this multicriterion problem by forming the weighted sum ob-

jective

wijdij = tr(w t d)

mxi,j=1

7.3 optimal detector design and hypothesis testing

369

where the weight matrix w     rm  m satis   es
wij > 0,

i = 1, . . . , m,

wii = 0,

i, j = 1, . . . , m,

i 6= j.

this objective is a weighted sum of the m(m     1) error probabilities, with weight
wij associated with the error of guessing      = i when in fact    = j. the weight
matrix is sometimes called the loss matrix.

to    nd a pareto optimal point for the multicriterion problem (7.12), we form

the scalar optimization problem

minimize
subject to

tr(w t d)
tk (cid:23) 0,

1t tk = 1,

k = 1, . . . , n,

(7.13)

which is an lp. this lp is separable in the variables t1, . . . , tn. the objective can
be expressed as a sum of (linear) functions of tk:

tr(w t d) = tr(w t t p ) = tr(p w t t ) =

ct
k tk,

nxk=1

where ck is the kth column of w p t . the constraints are separable (i.e., we have
separate constraints on each ti). therefore we can solve the lp (7.13) by separately
solving

minimize
subject to

ct
k tk
tk (cid:23) 0,

1t tk = 1,

for k = 1, . . . , n. each of these lps has a simple analytical solution (see exer-
cise 4.8). we    rst    nd an index q such that ckq = minj ckj. then we take t   
k = eq.
this optimal point corresponds to a deterministic detector: when x = k is ob-
served, our estimate is

     = argmin

j

(w p t )jk.

(7.14)

thus, for every weight matrix w with positive o   -diagonal elements we can    nd
a deterministic detector that minimizes the weighted sum objective. this seems
to suggest that randomized detectors are not needed, but we will see this is not
the case. the pareto optimal trade-o    surface for the multicriterion lp (7.12) is
piecewise-linear; the deterministic detectors of the form (7.14) correspond to the
vertices on the pareto optimal surface.

map and ml detectors

consider a bayes detector design with prior distribution q. the mean id203
of error is

qt p e =

dij =

wijdij,

mxj=1

qjxi6=j

mxi,j=1

if we de   ne the weight matrix w as

wij = qj,

i, j = 1, . . . , m,

i 6= j,

wii = 0,

i = 1, . . . , m.

370

7 statistical estimation

thus, a bayes optimal detector is given by the deterministic detector (7.14), with

(w p t )jk =xi6=j

qipki =

mxi=1

qipki     qjpkj.

the    rst term is independent of j, so the optimal detector is simply

     = argmax

j

(pkjqj),

when x = k is observed. the solution has a simple interpretation: since pkjqj
gives the id203 that    = j and x = k, this detector is a maximum a posteriori
id203 (map) detector.

for the special case q = (1/m)1, i.e., a uniform prior distribution on   , this

map detector reduces to a maximum likelihood (ml) detector:

     = argmax

j

pkj.

thus, a maximum likelihood detector minimizes the (unweighted) average or mean
id203 of error.

7.3.5 binary hypothesis testing

as an illustration, we consider the special case m = 2, which is called binary
hypothesis testing. the random variable x is generated from one of two distribu-
tions, which we denote p     rn and q     rn, to simplify the notation. often the
hypothesis    = 1 corresponds to some normal situation, and the hypothesis    = 2
corresponds to some abnormal event that we are trying to detect. if      = 1, we say
the test is negative (i.e., we guess that the event did not occur); if      = 2, we say
the test is positive (i.e., we guess that the event did occur).

the detection id203 matrix d     r2  2 is traditionally expressed as

d =(cid:20) 1     pfp

pfp

pfn

1     pfn (cid:21) .

here pfn is the id203 of a false negative (i.e., the test is negative when in fact
the event has occurred) and pfp is the id203 of a false positive (i.e., the test
is positive when in fact the event has not occurred), which is also called the false
alarm id203. the optimal detector design problem is a bi-criterion problem,
with objectives pfn and pfp.

the optimal trade-o    curve between pfn and pfp is called the receiver operating
characteristic (roc), and is determined by the distributions p and q. the roc
can be found by scalarizing the bi-criterion problem, as described in   7.3.4. for
the weight matrix w , an optimal detector (7.14) is

     =(cid:26) 1 w21pk > w12qk

2 w21pk     w12qk

7.3 optimal detector design and hypothesis testing

371

n
f
p

1

0.8

0.6

0.4

0.2

0
0

1

2

4

3

0.2

0.4

pfp

0.6

0.8

1

figure 7.4 optimal trade-o    curve between id203 of a false negative,
and id203 of a false positive test result, for the matrix p given in (7.15).
the vertices of the trade-o    curve, labeled 1   3, correspond to deterministic
detectors; the point labeled 4, which is a randomized detector, is the mini-
max detector. the dashed line shows pfn = pfp, the points where the error
probabilities are equal.

when x = k is observed. this is called a likelihood ratio threshold test:
if the
ratio pk/qk is more than the threshold w12/w21, the test is negative (i.e.,      =
1); otherwise the test is positive. by choosing di   erent values of the threshold,
we obtain (deterministic) pareto optimal detectors that give di   erent levels of
false positive versus false negative error probabilities. this result is known as
the neyman-pearson lemma.

the likelihood ratio detectors do not give all the pareto optimal detectors; they

are the vertices of the optimal trade-o    curve, which is piecewise-linear.

example 7.4 we consider a binary hypothesis testing example with n = 4, and

(7.15)

the optimal trade-o    curve between pfn and pfp, i.e., the receiver operating curve,
is shown in    gure 7.4. the left endpoint corresponds to the detector which is always
negative, independent of the observed value of x; the right endpoint corresponds to
the detector that is always positive. the vertices labeled 1, 2, and 3 correspond to
the deterministic detectors

p =         

0.70
0.20
0.05
0.05

0.10
0.10
0.70
0.10

          .

t (1) = (cid:20) 1
t (2) = (cid:20) 1

0

0

1
0

1
0

0
1

0
1

1

0 (cid:21) ,
1 (cid:21) ,

0

372

7 statistical estimation

respectively. the point labeled 4 corresponds to the nondeterministic detector

0

t (3) = (cid:20) 1
t (4) =(cid:20) 1

0

0
1

0
1

2/3
1/3

0
1

0

1 (cid:21) ,
1 (cid:21) ,

0

which is the minimax detector. this minimax detector yields equal id203 of
a false positive and false negative, which in this case is 1/6. every deterministic
detector has either a false positive or false negative id203 that exceeds 1/6,
so this is an example where a randomized detector outperforms every deterministic
detector.

7.3.6 robust detectors

so far we have assumed that p , which gives the distribution of the observed variable
x, for each value of the parameter   , is known. in this section we consider the case
where these distributions are not known, but certain prior information about them
is given. we assume that p     p, where p is the set of possible distributions. with
a randomized detector characterized by t , the detection id203 matrix d now
depends on the particular value of p . we will judge the error probabilities by
their worst-case values, over p     p. we de   ne the worst-case detection id203
matrix dwc as

and

dwc

ij = sup
p    p

dij,

i, j = 1, . . . , m,

i 6= j

dwc

ii = inf
p    p

dii,

i = 1, . . . , m.

the o   -diagonal entries give the largest possible id203 of errors, and the
diagonal entries give the smallest possible id203 of detection, over p     p.
6= 1 in general, i.e., the columns of a worst-case detection
id203 matrix do not necessarily add up to one.

i=1 dwc
ij

note that pn

we de   ne the worst-case id203 of error as

p wce
i = 1     dwc
ii .

i

is the largest id203 of error, when    = i, over all possible distri-

thus, p wce
butions in p.
using the worst-case detection id203 matrix, or the worst-case id203
of error vector, we can develop various robust versions of detector design problems.
in the rest of this section we concentrate on the robust minimax detector design
problem, as a generic example that illustrates the ideas.

we de   ne the robust minimax detector as the detector that minimizes the worst-

case id203 of error, over all hypotheses, i.e., minimizes the objective

max

i

p wce

i = max

i=1,...,m

sup
p    p

(1     (t p )ii) = 1     min

i=1,...,m

inf
p    p

(t p )ii.

the robust minimax detector minimizes the worst possible id203 of error,
over all m hypotheses, and over all p     p.

7.3 optimal detector design and hypothesis testing

373

robust minimax detector for    nite p
when the set of possible distributions is    nite, the robust minimax detector design
problem is readily formulated as an lp. with p = {p1, . . . , pk}, we can    nd the
robust minimax detector by solving

maximize mini=1,...,m inf p    p (t p )ii = mini=1,...,m minj=1,...,k(t pj)ii
subject to

i = 1, . . . , n,

1t ti = 1,

ti (cid:23) 0,

the objective is piecewise-linear and concave, so this problem can be expressed as
an lp. note that we can just as well consider p to be the polyhedron conv p;
the associated worst-case detection matrix, and robust minimax detector, are the
same.

robust minimax detector for polyhedral p
it is also possible to e   ciently formulate the robust minimax detector problem as an
lp when p is a polyhedron described by linear equality and inequality constraints.
this formulation is less obvious, and relies on a dual representation of p.

to simplify the discussion, we assume that p has the form

p =(cid:8)p = [p1        pm] (cid:12)(cid:12) akpk = bk, 1t pk = 1, pk (cid:23) 0(cid:9) .

in other words, for each distribution pk, we are given some expected values akpk =
bk. (these might represent known moments, probabilities, etc.) the extension to
the case where we are given inequalities on expected values is straightforward.

(7.16)

the robust minimax design problem is

maximize
subject to

  
inf{  tt
ti (cid:23) 0,

i p | aip = bi, 1t p = 1, p (cid:23) 0}       ,

i = 1, . . . , n,

1t ti = 1,

i = 1, . . . , m

where   tt

i denotes the ith row of t (so that (t p )ii =   tt
inf{  tt

i p | aip = bi, 1t p = 1, p (cid:23) 0} = sup{  t bi +    | at

i pi). by lp duality,
i    +   1 (cid:22)   ti}.

using this, the robust minimax detector design problem can be expressed as the
lp

maximize
subject to

  
  t
i bi +   i       ,
i   i +   i1 (cid:22)   ti,
at
1t ti = 1,
ti (cid:23) 0,

i = 1, . . . , m

i = 1, . . . , m

i = 1, . . . , n,

with variables   1, . . . ,   m,   1, . . . ,   n, and t (which has columns ti and rows   tt

i ).

example 7.5 robust binary hypothesis testing. suppose m = 2 and the set p in (7.16)
is de   ned by

a1 = a2 = a =(cid:20) a1

a2
1

a2
a2
2

      
      

an
a2

n (cid:21) ,

b1 =(cid:20)   1
  2 (cid:21) ,

b2 =(cid:20)   1
  2 (cid:21) .

designing a robust minimax detector for this set p can be interpreted as a binary
hypothesis testing problem: based on an observation of a random variable x    
{a1, . . . , an}, choose between the following two hypotheses:

374

7 statistical estimation

1. e x =   1, e x 2 =   2
2. e x =   1, e x 2 =   2.

let   tt denote the    rst row of t (and so, (1      t)t is the second row). for given   t, the
worst-case probabilities of correct detection are

aipi =   1,

dwc

dwc

11 = inf(  tt p (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
nxi=1
22 = inf((1       t)t p (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

nxi=1

a2

nxi=1

i pi =   2, 1t p = 1, p (cid:23) 0)
nxi=1

i pi =   2, 1t p = 1, p (cid:23) 0) .

a2

aipi =   1,

using lp duality we can express dwc

11 as the optimal value of the lp

maximize
subject to

z0 + z1  1 + z2  2
z0 + aiz1 + a2
with variables z0, z1, z2     r. similarly dwc
maximize w0 + w1  1 + w2  2
subject to w0 + aiw1 + a2

i = 1, . . . , n,

i z2       ti,
22 is the optimal value of the lp

i w2     1       ti,

i = 1, . . . , n,

with variables w0, w1, w2     r. to obtain the minimax detector, we have to maximize
the minimum of dwc

22 , i.e., solve the lp

11 and dwc

maximize
subject to

  
z0 + z1  2 + z2  2       
w0 +   1w1 +   2w2       
i       ti,
z0 + z1ai + z2a2
i     1       ti,
w0 + w1ai + w2a2
0 (cid:22)   t (cid:22) 1.

i = 1, . . . , n

i = 1, . . . , n

the variables are z0, z1, z2, w0, w1, w2 and   t.

7.4 chebyshev and cherno    bounds

in this section we consider two types of classical bounds on the id203 of a set,
and show that generalizations of each can be cast as id76 problems.
the original classical bounds correspond to simple id76 problems
with analytical solutions; the id76 formulation of the general cases
allow us to compute better bounds, or bounds for more complex situations.

7.4.1 chebyshev bounds

chebyshev bounds give an upper bound on the id203 of a set based on known
expected values of certain functions (e.g., mean and variance). the simplest ex-
ample is markov   s inequality: if x is a random variable on r+ with e x =   ,

7.4 chebyshev and cherno    bounds

375

then we have prob(x     1)       , no matter what the distribution of x is. an-
other simple example is chebyshev   s bound: if x is a random variable on r with
e x =    and e(x       )2 =   2, then we have prob(|x       |     1)       2, again no
matter what the distribution of x is. the idea behind these simple bounds can be
generalized to a setting in which id76 is used to compute a bound
on the id203.

let x be a random variable on s     rm, and c     s be the set for which we
want to bound prob(x     c). let 1c denote the 0-1 indicator function of the set
c, i.e., 1c(z) = 1 if z     c and 1c(z) = 0 if z 6    c.
our prior knowledge of the distribution consists of known expected values of
some functions:

e fi(x) = ai,

i = 1, . . . , n,

where fi : rm     r. we take f0 to be the constant function with value one, for
which we always have e f0(x) = a0 = 1. consider a linear combination of the
functions fi, given by

f (z) =

xifi(z),

nxi=0

where xi     r, i = 0, . . . , n. from our knowledge of e fi(x), we have e f (x) =
at x.
now suppose that f satis   es the condition f (z)     1c(z) for all z     s, i.e., f
is pointwise greater than or equal to the indicator function of c (on s). then we
have

e f (x) = at x     e 1c(x) = prob(x     c).

in other words, at x is an upper bound on prob(x     c), valid for all distributions
supported on s, with e fi(x) = ai.
we can search for the best such upper bound on prob(x     c), by solving the

problem

minimize
subject to

x0 + a1x1 +        + anxn

f (z) =pn
f (z) =pn

i=0 xifi(z)     1 for z     c
i=0 xifi(z)     0 for z     s, z /    c,

(7.17)

with variable x     rn+1. this problem is always convex, since the constraints can
be expressed as

g1(x) = 1     inf

z   c

f (z)     0,

g2(x) =     inf

z   s\c

f (z)     0

(g1 and g2 are convex). the problem (7.17) can also be thought of as a semi-in   nite
linear program, i.e., an optimization problem with a linear objective and an in   nite
number of linear inequalities, one for each z     s.
in simple cases we can solve the problem (7.17) analytically. as an example, we
take s = r+, c = [1,   ), f0(z) = 1, and f1(z) = z, with e f1(x) = e x =        1
as our prior information. the constraint f (z)     0 for z     s reduces to x0     0,
x1     0. the constraint f (z)     1 for z     c, i.e., x0 + x1z     1 for all z     1, reduces
to x0 + x1     1. the problem (7.17) is then

minimize
x0 +   x1
subject to x0     0,

x1     0

x0 + x1     1.

376

7 statistical estimation

since 0            1, the optimal point for this simple lp is x0 = 0, x1 = 1. this gives
the classical markov bound prob(x     1)       .

in other cases we can solve the problem (7.17) using id76.

remark 7.1 duality and the chebyshev bound problem. the chebyshev bound prob-
lem (7.17) determines a bound on prob(x     c) for all id203 measures that
satisfy the given expected value constraints. thus we can think of the chebyshev
bound problem (7.17) as producing a bound on the optimal value of the in   nite-
dimensional problem

maximize
subject to

  (dz)
fi(z)  (dz) = ai,
  (dz) = 1

i = 1, . . . , n

(7.18)

rc
rs
rs

       0,

where the variable is the measure   , and        0 means that the measure is nonnegative.
since the chebyshev problem (7.17) produces a bound on the problem (7.18), it
should not be a surprise that they are related by duality. while semi-in   nite and
in   nite-dimensional problems are beyond the scope of this book, we can still formally
construct a dual of the problem (7.17), introducing a lagrange multiplier function
p : s     r, with p(z) the lagrange multiplier associated with the inequality f (z)     1
(for z     c) or f (z)     0 (for z     s\c). using an integral over z where we would have
a sum in the    nite-dimensional case, we arrive at the formal dual

maximize
subject to

p(z) dz
fi(z)p(z) dz = ai,
p(z) dz = 1

i = 1, . . . , n

p(z)     0 for all z     s,

rc
rs
rs

where the optimization variable is the function p. this is, essentially, the same
as (7.18).

id203 bounds with known    rst and second moments
as an example, suppose that s = rm, and that we are given the    rst and second
moments of the random variable x:
e x = a     rm,

e xx t =        sm.

in other words, we are given the expected value of the m functions zi, i = 1, . . . , m,
and the m(m + 1)/2 functions zizj, i, j = 1, . . . , m, but no other information about
the distribution.

in this case we can express f as the general quadratic function

f (z) = zt p z + 2qt z + r,

where the variables (i.e., the vector x in the discussion above) are p     sm, q     rm,
and r     r. from our knowledge of the    rst and second moments, we    nd that

e f (x) = e(x t p x + 2qt x + r)

= e tr(p xx t ) + 2 e qt x + r
= tr(  p ) + 2qt a + r.

7.4 chebyshev and cherno    bounds

377

the constraint that f (z)     0 for all z can be expressed as the linear matrix in-
equality

(cid:20) p

qt

q

r (cid:21) (cid:23) 0.

in particular, we have p (cid:23) 0.

now suppose that the set c is the complement of an open polyhedron,

c = rm \ p,

p = {z | at

i z < bi, i = 1, . . . , k}.

the condition that f (z)     1 for all z     c is the same as requiring that

at
i z     bi =    zt p z + 2qt z + r     1

for i = 1, . . . , k. this, in turn, can be expressed as: there exist   1, . . . ,   k     0 such
that

(cid:20) p

qt

q

r     1 (cid:21) (cid:23)   i(cid:20)

0
at

ai/2

i /2    bi (cid:21) ,

i = 1, . . . , k.

(see   b.2.)
as

putting it all together, the chebyshev bound problem (7.17) can be expressed

tr(  p ) + 2qt a + r

minimize

subject to (cid:20) p
(cid:20) p

qt

qt

  i     0,

q

r     1 (cid:21) (cid:23)   i(cid:20)
r (cid:21) (cid:23) 0,

i = 1, . . . , k

q

0
at

ai/2

i /2    bi (cid:21) ,

i = 1, . . . , k

(7.19)

which is a semide   nite program in the variables p , q, r, and   1, . . . ,   k. the
optimal value, say   , is an upper bound on prob(x     c) over all distributions
with mean a and second moment   . or, turning it around, 1        is a lower bound
on prob(x     p).

remark 7.2 duality and the chebyshev bound problem. the dual sdp associated
with (7.19) can be expressed as

subject to

maximize pk
i=1(cid:20) zi
pk
(cid:20) zi

i=1   i
at
i zi     b  i,
zt
i
zi

zt
i

  i (cid:21) (cid:23) 0,

i = 1, . . . , k
zi

1 (cid:21)
  i (cid:21) (cid:22)(cid:20)    a

at

i = 1, . . . , k.

the variables are zi     sm, zi     rm, and   i     r, for i = 1, . . . , k. since the
sdp (7.19) is strictly feasible, strong duality holds and the dual optimum is attained.

we can give an interesting id203 interpretation to the dual problem. suppose
zi, zi,   i are dual feasible and that the    rst r components of    are positive, and the

378

7 statistical estimation

i=1   i. with these de   nitions the dual feasibility constraints can be

i=1   i < 1. we de   ne

xi = (1/  i)zi,

rest are zero. for simplicity we also assume thatpk
  ixi! ,
rxi=1
i! ,
rxi=1

   a    
         

  ixixt

w0 =

i = 1, . . . , r,

w =

1

1

where    = 1   pk

expressed as

and

moreover, from dual feasibility,

at
i xi     bi,

i = 1, . . . , r

xi

wt
0

i
xt
i

  i(cid:20) xixt
rxi=1
  (cid:20) w w0
1 (cid:21) = (cid:20)    a
= (cid:20)    a
(cid:23) (cid:20)    a

1 (cid:21) +   (cid:20) w w0
1 (cid:21)    
1 (cid:21)    
1 (cid:21)    

wt
0

at

at

at

(cid:23) 0.

xi

at

i
xt
i

1 (cid:21) =(cid:20)    a
1 (cid:21) .
1 (cid:21)
  i(cid:20) xixt
rxi=1
rxi=1(cid:20) (1/  i)zizt
  i (cid:21)
rxi=1(cid:20) zi
  i (cid:21)
0 = ps

zt
i

zt
i

zi

zi

i

therefore, w (cid:23) w0wt
i . now
consider a discrete random variable x with the following distribution. if s     1, we
take

0 , so it can be factored as w     w0wt

i=1 wiwt

with id203   i,

x = xi
x = w0 +    s wi with id203   /(2s),
x = w0        s wi with id203   /(2s),

i = 1, . . . , r

i = 1, . . . , s
i = 1, . . . , s.

if s = 0, we take

x = xi with id203   i,
x = w0 with id203   .

i = 1, . . . , r

it is easily veri   ed that e x = a and e xx t =   , i.e., the distribution matches the
given moments. furthermore, since xi     c,

prob(x     c)    

  i.

rxi=1

in particular, by applying this interpretation to the dual optimal solution, we can
construct a distribution that satis   es the chebyshev bound from (7.19) with equality,
which shows that the chebyshev bound is sharp for this case.

7.4 chebyshev and cherno    bounds

379

7.4.2 cherno    bounds

let x be a random variable on r. the cherno    bound states that

prob(x     u)     inf

     0

e e  (x   u),

which can be expressed as

log prob(x     u)     inf

     0{     u + log e e  x}.

(7.20)

recall (from example 3.41, page 106) that the righthand term, log e e  x , is called
the cumulant generating function of the distribution, and is always convex, so the
function to be minimized is convex. the bound (7.20) is most useful in cases when
the cumulant generating function has an analytical expression, and the minimiza-
tion over    can be carried out analytically.

for example, if x is gaussian with zero mean and unit variance, the cumulant

generating function is

log e e  x =   2/2,

and the in   mum over        0 of      u +   2/2 occurs with    = u (if u     0), so the
cherno    bound is (for u     0)

prob(x     u)     e   u2/2.

the idea behind the cherno    bound can be extended to a more general setting,
in which id76 is used to compute a bound on the id203 of a
set in rm. let c     rm, and as in the description of chebyshev bounds above,
let 1c denote the 0-1 indicator function of c. we will derive an upper bound on
prob(x     c). (in principle we can compute prob(x     c), for example by monte
carlo simulation, or numerical integration, but either of these can be a daunting
computational task, and neither method produces guaranteed bounds.)

let        rm and        r, and consider the function f : rm     r given by

f (z) = e  t z+  .

as in the development of chebyshev bounds, if f satis   es f (z)     1c(z) for all z,
then we can conclude that

prob(x     c) = e 1c(x)     e f (x).

clearly we have f (z)     0 for all z; to have f (z)     1 for z     c is the same as
  t z +        0 for all z     c, i.e.,      t z        for all z     c. thus, if      t z        for all
z     c, we have the bound

prob(x     c)     e exp(  t x +   ),

or, taking logarithms,

log prob(x     c)        + log e exp(  t x).

380

7 statistical estimation

from this we obtain a general form of cherno      s bound:

log prob(x     c)     inf{   + log e exp(  t x) |       t z        for all z     c}

= inf

   (cid:18)sup

(     t z) + log e exp(  t x)(cid:19)
= inf(cid:0)sc(     ) + log e exp(  t x)(cid:1) ,

z   c

where sc is the support function of c. note that the second term, log e exp(  t x),
is the cumulant generating function of the distribution, and is always convex (see
example 3.41, page 106). evaluating this bound is, in general, a convex optimiza-
tion problem.

cherno    bound for a gaussian variable on a polyhedron
as a speci   c example, suppose that x is a gaussian random vector on rm with
zero mean and covariance i, so its cumulant generating function is

log e exp(  t x) =   t   /2.

we take c to be a polyhedron described by inequalities:

c = {x | ax (cid:22) b},

which we assume is nonempty.

for use in the cherno    bound, we use a dual characterization of the support

function sc:

sc(y) = sup{yt x | ax (cid:22) b}

=     inf{   yt x | ax (cid:22) b}
=     sup{   bt u | at u = y, u (cid:23) 0}
= inf{bt u | at u = y, u (cid:23) 0}

where in the third line we use lp duality:

inf{ct x | ax (cid:22) b} = sup{   bt u | at u + c = 0, u (cid:23) 0}

with c =    y. using this expression for sc in the cherno    bound we obtain

log prob(x     c)     inf
= inf
  

   (cid:0)sc(     ) + log e exp(  t x)(cid:1)
u {bt u +   t   /2 (cid:12)(cid:12) u (cid:23) 0, at u +    = 0}.

inf

thus, the cherno    bound on prob(x     c) is the exponential of the optimal value
of the qp

bt u +   t   /2

minimize
subject to u (cid:23) 0, at u +    = 0,

(7.21)

where the variables are u and   .

7.4 chebyshev and cherno    bounds

381

this problem has an interesting geometric interpretation. it is equivalent to

minimize
subject to u (cid:23) 0,

bt u + (1/2)kat uk2

2

which is the dual of

in other words, the cherno    bound is

maximize    (1/2)kxk2
subject to ax (cid:22) b.

2

prob(x     c)     exp(    dist(0, c)2/2),
where dist(0, c) is the euclidean distance of the origin to c.

(7.22)

remark 7.3 the bound (7.22) can also be derived without using cherno      s inequality.
if the distance between 0 and c is d, then there is a halfspace h = {z | at z     d},
with kak2 = 1, that contains c. the random variable at x is n (0, 1), so

prob(x     c)     prob(x     h) =   (   d),

where    is the cumulative distribution function of a zero mean, unit variance gaus-

sian. since   (   d)     e   d2/2 for d     0, this bound is at least as sharp as the cherno   

bound (7.22).

7.4.3 example

in this section we illustrate the chebyshev and cherno    id203 bounding
methods with a detection example. we have a set of m possible symbols or signals
s     {s1, s2, . . . , sm}     rn, which is called the signal constellation. one of these
signals is transmitted over a noisy channel. the received signal is x = s + v,
where v is a noise, modeled as a random variable. we assume that e v = 0 and
e vvt =   2i, i.e., the noise components v1, . . . , vn are zero mean, uncorrelated,
and have variance   2. the receiver must estimate which signal was sent on the
basis of the received signal x = s + v. the minimum distance detector chooses as
estimate the symbol sk closest (in euclidean norm) to x. (if the noise v is gaussian,
then minimum distance decoding is the same as maximum likelihood decoding.)

if the signal sk is transmitted, correct detection occurs if sk is the estimate,
given x. this occurs when the signal sk is closer to x than the other signals, i.e.,

kx     skk2 < kx     sjk2,

j 6= k.

thus, correct detection of symbol sk occurs if the random variable v satis   es the
linear inequalities

2(sj     sk)t (sk + v) < ksjk2

2     kskk2
2,

j 6= k.

these inequalities de   ne the voronoi region vk of sk in the signal constellation,
i.e., the set of points closer to sk than any other signal in the constellation. the
id203 of correct detection of sk is prob(sk + v     vk).

figure 7.5 shows a simple example with m = 7 signals, with dimension n = 2.

382

7 statistical estimation

s3

s2

s4

s1

s5

s7

s6

figure 7.5 a constellation of 7 signals s1, . . . , s7     r2, shown as small circles.
the line segments show the boundaries of the corresponding voronoi regions.
the minimum distance detector selects symbol sk when the received signal
lies closer to sk than to any of the other points, i.e., if the received signal is
in the interior of the voronoi region around symbol sk. the circles around
each point have radius one, to show the scale.

chebyshev bounds

the sdp bound (7.19) provides a lower bound on the id203 of correct detec-
tion, and is plotted in    gure 7.6, as a function of the noise standard deviation   ,
for the three symbols s1, s2, and s3. these bounds hold for any noise distribution
with zero mean and covariance   2i. they are tight in the sense that there exists
a noise distribution with zero mean and covariance    =   2i, for which the proba-
bility of error is equal to the lower bound. this is illustrated in    gure 7.7, for the
   rst voronoi set, and    = 1.

cherno    bounds

we use the same example to illustrate the cherno    bound. here we assume that the
noise is gaussian, i.e., v     n (0,   2i). if symbol sk is transmitted, the id203
of correct detection is the id203 that sk + v     vk. to    nd a lower bound for
this id203, we use the qp (7.21) to compute upper bounds on the id203
that the ml detector selects symbol i, i = 1, . . . , m, i 6= k. (each of these upper
bounds is related to the distance of sk to the voronoi set vi.) adding these upper
bounds on the probabilities of mistaking sk for si, we obtain an upper bound on
the id203 of error, and therefore, a lower bound on the id203 of correct
detection of symbol sk. the resulting lower bound, for s1, is shown in    gure 7.8,
along with an estimate of the id203 of correct detection obtained using monte
carlo analysis.

7.4 chebyshev and cherno    bounds

383

1

0.8

0.6

0.4

0.2

n
o
i
t
c
e
t
e
d

t
c
e
r
r
o
c

f
o

y
t
i
l
i

b
a
b
o
r
p

0
0

0.5

1

3

2

1

  

1.5

2

2.5

figure 7.6 chebyshev lower bounds on the id203 of correct detection
for symbols s1, s2, and s3. these bounds are valid for any noise distribution
that has zero mean and covariance   2i.

s3

s2

s4

s1

s5

s7

s6

figure 7.7 the chebyshev lower bound on the id203 of correct detec-
tion of symbol 1 is equal to 0.2048 when    = 1. this bound is achieved by
the discrete distribution illustrated in the    gure. the solid circles are the
possible values of the received signal s1 + v. the point in the center of the
ellipse has id203 0.2048. the    ve points on the boundary have a total
id203 0.7952. the ellipse is de   ned by xt p x + 2qt x + r = 1, where
p , q, and r are the optimal solution of the sdp (7.19).

384

7 statistical estimation

1

0.95

n
o
i
t
c
e
t
e
d

t
c
e
r
r
o
c

f
o

y
t
i
l
i

b
a
b
o
r
p

0.9

0.2

0.3

  

0.4

0.5

figure 7.8 the cherno    lower bound (solid line) and a monte carlo esti-
mate (dashed line) of the id203 of correct detection of symbol s1, as
a function of   . in this example the noise is gaussian with zero mean and
covariance   2i.

7.5 experiment design

we consider the problem of estimating a vector x     rn from measurements or
experiments

yi = at

i x + wi,

i = 1, . . . , m,

where wi is measurement noise. we assume that wi are independent gaussian
random variables with zero mean and unit variance, and that the measurement
vectors a1, . . . , am span rn. the maximum likelihood estimate of x, which is the
same as the minimum variance estimate, is given by the least-squares solution

the associated estimation error e =   x     x has zero mean and covariance matrix

yiai.

aiat

  x =  mxi=1
i!   1 mxi=1
e = e eet =  mxi=1
i!   1

aiat

.

the matrix e characterizes the accuracy of the estimation, or the informativeness
of the experiments. for example the   -con   dence level ellipsoid for x is given by

e = {z | (z       x)t e   1(z       x)       },

where    is a constant that depends on n and   .

we suppose that the vectors a1, . . . , am, which characterize the measurements,
can be chosen among p possible test vectors v1, . . . , vp     rn, i.e., each ai is one of

7.5 experiment design

385

the vj. the goal of experiment design is to choose the vectors ai, from among the
possible choices, so that the error covariance e is small (in some sense). in other
words, each of m experiments or measurements can be chosen from a    xed menu
of p possible experiments; our job is to    nd a set of measurements that (together)
are maximally informative.

let mj denote the number of experiments for which ai is chosen to have the

value vj, so we have

we can express the error covariance matrix as

m1 +        + mp = m.

e =  mxi=1

i!   1

aiat

=      
pxj=1

mjvjvt

   1

.

j      

this shows that the error covariance depends only on the numbers of each type of
experiment chosen (i.e., m1, . . . , mp).

the basic experiment design problem is as follows. given the menu of possible
choices for experiments, i.e., v1, . . . , vp, and the total number m of experiments to
be carried out, choose the numbers of each type of experiment, i.e., m1, . . . , mp,
to make the error covariance e small (in some sense). the variables m1, . . . , mp
must, of course, be integers and sum to m, the given total number of experiments.
this leads to the optimization problem

minimize (w.r.t. sn
subject to

+) e =(cid:16)pp

j=1 mjvjvt

j(cid:17)   1

mi     0, m1 +        + mp = m
mi     z,

(7.23)

where the variables are the integers m1, . . . , mp.

the basic experiment design problem (7.23) is a vector optimization problem
over the positive semide   nite cone.
if one experiment design results in e, and
another in   e, with e (cid:22)   e, then certainly the    rst experiment design is as good
as or better than the second. for example, the con   dence ellipsoid for the    rst
experiment design (translated to the origin for comparison) is contained in the
con   dence ellipsoid of the second. we can also say that the    rst experiment design
allows us to estimate qt x better (i.e., with lower variance) than the second experi-
ment design, for any vector q, since the variance of our estimate of qt x is given by
qt eq for the    rst experiment design and qt   eq for the second. we will see below
several common scalarizations for the problem.

7.5.1 the relaxed experiment design problem

the basic experiment design problem (7.23) can be a hard combinatorial problem
when m, the total number of experiments, is comparable to n, since in this case
the mi are all small integers. in the case when m is large compared to n, however,
a good approximate solution of (7.23) can be found by ignoring, or relaxing, the
constraint that the mi are integers. let   i = mi/m, which is the fraction of

386

7 statistical estimation

the total number of experiments for which aj = vi, or the relative frequency of
experiment i. we can express the error covariance in terms of   i as

e =

1

m  pxi=1

  ivivt

i !   1

.

(7.24)

the vector        rp satis   es    (cid:23) 0, 1t    = 1, and also, each   i is an integer multiple
of 1/m. by ignoring this last constraint, we arrive at the problem

minimize (w.r.t. sn
subject to

+) e = (1/m)(cid:0)pp

   (cid:23) 0,

1t    = 1,

i=1   ivivt

i (cid:1)   1

(7.25)

with variable        rp. to distinguish this from the original combinatorial experi-
ment design problem (7.23), we refer to it as the relaxed experiment design problem.
the relaxed experiment design problem (7.25) is a id76 problem,
since the objective e is an sn

+-convex function of   .

several statements can be made about the relation between the (combinato-
rial) experiment design problem (7.23) and the relaxed problem (7.25). clearly
the optimal value of the relaxed problem provides a lower bound on the optimal
value of the combinatorial one, since the combinatorial problem has an additional
constraint. from a solution of the relaxed problem (7.25) we can construct a sub-
optimal solution of the combinatorial problem (7.23) as follows. first, we apply
simple rounding to get

mi = round(m  i),

i = 1, . . . , p.

corresponding to this choice of m1, . . . , mp is the vector     ,

    i = (1/m)round(m  i),

i = 1, . . . , p.

the vector      satis   es the constraint that each entry is an integer multiple of 1/m.
clearly we have |  i         i|     1/(2m), so for m large, we have            . this implies
that the constraint 1t      = 1 is nearly satis   ed, for large m, and also that the error
covariance matrices associated with      and    are close.

we can also give an alternative interpretation of the relaxed experiment design
problem (7.25). we can interpret the vector        rp as de   ning a id203
distribution on the experiments v1, . . . , vp. our choice of    corresponds to a random
experiment: each experiment ai takes the form vj with id203   j.

in the rest of this section, we consider only the relaxed experiment design

problem, so we drop the quali   er    relaxed    in our discussion.

7.5.2 scalarizations

several scalarizations have been proposed for the experiment design problem (7.25),
which is a vector optimization problem over the positive semide   nite cone.

7.5 experiment design

d-optimal design

387

the most widely used scalarization is called d-optimal design, in which we minimize
the determinant of the error covariance matrix e. this corresponds to designing
the experiment to minimize the volume of the resulting con   dence ellipsoid (for
a    xed con   dence level). ignoring the constant factor 1/m in e, and taking the
logarithm of the objective, we can pose this problem as

minimize
subject to    (cid:23) 0,
which is a id76 problem.

log det(cid:0)pp

i=1   ivivt
1t    = 1,

i (cid:1)   1

(7.26)

e-optimal design

of the con   dence ellipsoid e is proportional to kek1/2

in e-optimal design, we minimize the norm of the error covariance matrix, i.e.,
the maximum eigenvalue of e. since the diameter (twice the longest semi-axis)
, minimizing kek2 can be
interpreted geometrically as minimizing the diameter of the con   dence ellipsoid.
e-optimal design can also be interpreted as minimizing the maximum variance of
qt e, over all q with kqk2 = 1.

the e-optimal experiment design problem is

2

minimize
subject to    (cid:23) 0,

(cid:13)(cid:13)(cid:13)(cid:0)pp

i=1   ivivt

1t    = 1.

i (cid:1)   1(cid:13)(cid:13)(cid:13)2

the objective is a convex function of   , so this is a convex problem.

the e-optimal experiment design problem can be cast as an sdp

t

maximize

subject to pp

   (cid:23) 0,

i=1   ivivt

i (cid:23) ti
1t    = 1,

with variables        rp and t     r.
a-optimal design

(7.27)

in a-optimal experiment design, we minimize tr e, the trace of the covariance
matrix. this objective is simply the mean of the norm of the error squared:

the a-optimal experiment design problem is

ekek2

2 = e tr(eet ) = tr e.

minimize
subject to    (cid:23) 0,

tr(cid:0)pp

i=1   ivivt

1t    = 1.

i (cid:1)   1

(7.28)

this, too, is a convex problem. like the e-optimal experiment design problem, it
can be cast as an sdp:

1t u

minimize

subject to (cid:20) pp

   (cid:23) 0,

i=1   ivivt
i

et
k
1t    = 1,

ek

uk (cid:21) (cid:23) 0,

k = 1, . . . , n

388

7 statistical estimation

where the variables are u     rn and        rp, and here, ek is the kth unit vector.
optimal experiment design and duality

the lagrange duals of the three scalarizations have an interesting geometric mean-
ing.

the dual of the d-optimal experiment design problem (7.26) can be expressed

as

maximize
subject to

log det w + n log n
vt
i w vi     1,

i = 1, . . . , p,

with variable w     sn and domain sn
++ (see exercise 5.10). this dual problem
has a simple interpretation: the optimal solution w     determines the minimum
volume ellipsoid, centered at the origin, given by {x | xt w    x     1}, that contains
the points v1, . . . , vp. (see also the discussion of problem (5.14) on page 222.) by
complementary slackness,

     
i (1     vt

i w    vi) = 0,

i = 1, . . . , p,

(7.29)

i.e., the optimal experiment design only uses the experiments vi which lie on the
surface of the minimum volume ellipsoid.

the duals of the e-optimal and a-optimal design problems can be given a
similar interpretation. the duals of problems (7.27) and (7.28) can be expressed
as

and

maximize
subject to

tr w
vt
i w vi     1,
w (cid:23) 0,

maximize
subject to

(tr w 1/2)2
vt
i w vi     1,

i = 1, . . . , p

(7.30)

i = 1, . . . , p,

(7.31)

respectively. the variable in both problems is w     sn. in the second problem
there is an implicit constraint w     sn
as for the d-optimal design, the optimal solution w     determines a minimal
ellipsoid {x | xt w    x     1} that contains the points v1, . . . , vp. moreover w     and
      satisfy the complementary slackness conditions (7.29), i.e., the optimal design
only uses experiments vi that lie on the surface of the ellipsoid de   ned by w    .

+. (see exercises 5.40 and 5.10.)

experiment design example
we consider a problem with x     r2, and p = 20. the 20 candidate measurement
vectors ai are shown as circles in    gure 7.9. the origin is indicated with a cross.
the d-optimal experiment has only two nonzero   i, indicated as solid circles in
   gure 7.9. the e-optimal experiment has two nonzero   i, indicated as solid circles
in    gure 7.10. the a-optimal experiment has three nonzero   i, indicated as solid
circles in    gure 7.11. we also show the three ellipsoids {x | xt w    x     1} associated
with the dual optimal solutions w    . the resulting 90% con   dence ellipsoids are
shown in    gure 7.12, along with the con   dence ellipsoid for the    uniform    design,
with equal weight   i = 1/p on all experiments.

7.5 experiment design

389

  1 = 0.5

  2 = 0.5

figure 7.9 experiment design example. the 20 candidate measurement vec-
tors are indicated with circles. the d-optimal design uses the two measure-
ment vectors indicated with solid circles, and puts an equal weight   i = 0.5
on each of them. the ellipsoid is the minimum volume ellipsoid centered at
the origin, that contains the points vi.

  2 = 0.2

  3 = 0.8

figure 7.10 the e-optimal design uses two measurement vectors. the
dashed lines are (part of) the boundary of the ellipsoid {x | xt w    x     1}
where w     is the solution of the dual problem (7.30).

  1 = 0.30

  2 = 0.38

  3 = 0.32

figure 7.11 the a-optimal design uses three measurement vectors. the
dashed line shows the ellipsoid {x | xt w    x     1} associated with the solution
of the dual problem (7.31).

390

7 statistical estimation

d

a

uniform

e

figure 7.12 shape of the 90% con   dence ellipsoids for d-optimal, a-optimal,
e-optimal, and uniform designs.

7.5.3 extensions

resource limits

suppose that associated with each experiment is a cost ci, which could represent
the economic cost, or time required, to carry out an experiment with vi. the total
cost, or time required (if the experiments are carried out sequentially) is then

m1c1 +        + mpcp = mct   .

we can add a limit on total cost by adding the linear inequality mct        b, where
b is a budget, to the basic experiment design problem. we can add multiple linear
inequalities, representing limits on multiple resources.

multiple measurements per experiment

we can also consider a generalization in which each experiment yields multiple
measurements. in other words, when we carry out an experiment using one of the
possible choices, we obtain several measurements. to model this situation we can
use the same notation as before, with vi as matrices in rn  ki:

vi =(cid:2) ui1

       uiki (cid:3) ,

where ki is the number of (scalar) measurements obtained when the experiment vi
is carried out. the error covariance matrix, in this more complicated setup, has
the exact same form.

in conjunction with additional linear inequalities representing limits on cost or
time, we can model discounts or time savings associated with performing groups
of measurements simultaneously. suppose, for example, that the cost of simulta-
neously making (scalar) measurements v1 and v2 is less than the sum of the costs

7.5 experiment design

391

of making them separately. we can take v3 to be the matrix

and assign costs c1, c2, and c3 associated with making the    rst measurement alone,
the second measurement alone, and the two simultaneously, respectively.

v3 =(cid:2) v1

v2 (cid:3)

when we solve the experiment design problem,   1 will give us the fraction of
times we should carry out the    rst experiment alone,   2 will give us the fraction
of times we should carry out the second experiment alone, and   3 will give us
the fraction of times we should carry out the two experiments simultaneously.
(normally we would expect a choice to be made here; we would not expect to have
  1 > 0,   2 > 0, and   3 > 0.)

392

7 statistical estimation

bibliography

ml and map estimation, hypothesis testing, and detection are covered in books on
statistics, pattern recognition, statistical signal processing, or communications; see, for
example, bickel and doksum [bd77], duda, hart, and stork [dhs99], scharf [sch91], or
proakis [pro01].
id28 is discussed in hastie, tibshirani, and friedman [htf01,   4.4]. for
the covariance estimation problem of page 355, see anderson [and70].

generalizations of chebyshev   s inequality were studied extensively in the sixties, by isii
[isi64], marshall and olkin [mo60], karlin and studden [ks66, chapter 12], and others.
the connection with semide   nite programming was made more recently by bertsimas and
sethuraman [bs00] and lasserre [las02].
the terminology in   7.5 (a-, d-, and e-optimality) is standard in the literature on optimal
experiment design (see, for example, pukelsheim [puk93]). the geometric interpretation
of the dual d-optimal design problem is discussed by titterington [tit75].

exercises

exercises

estimation

393

7.1 linear measurements with exponentially distributed noise. show how to solve the ml

estimation problem (7.2) when the noise is exponentially distributed, with density

p(z) =(cid:26) (1/a)e   z/a

0

z     0
z < 0,

where a > 0.

7.2 ml estimation and       -norm approximation. we consider the linear measurement model

y = ax + v of page 352, with a uniform noise distribution of the form

p(z) =(cid:26) 1/(2  )

0

|z|       
|z| >   .

as mentioned in example 7.1, page 352, any x that satis   es kax     yk           is a ml
estimate.
now assume that the parameter    is not known, and we wish to estimate   , along with
the parameters x. show that the ml estimates of x and    are found by solving the
      -norm approximation problem

where at

i are the rows of a.

minimize

kax     yk   ,

7.3 probit model. suppose y     {0, 1} is random variable given by

y =(cid:26) 1

0

at u + b + v     0
at u + b + v > 0,

where the vector u     rn is a vector of explanatory variables (as in the logistic model
described on page 354), and v is a zero mean unit variance gaussian variable.
formulate the ml estimation problem of estimating a and b, given data consisting of
pairs (ui, yi), i = 1, . . . , n , as a id76 problem.

7.4 estimation of covariance and mean of a multivariate normal distribution. we consider the
problem of estimating the covariance matrix r and the mean a of a gaussian id203
density function

pr,a(y) = (2  )   n/2 det(r)   1/2 exp(   (y     a)t r   1(y     a)/2),

based on n independent samples y1, y2, . . . , yn     rn.
(a) we    rst consider the estimation problem when there are no additional constraints

on r and a. let    and y be the sample mean and covariance, de   ned as

   =

1
n

nxk=1

yk,

y =

1
n

nxk=1

(yk       )(yk       )t .

show that the log-likelihood function

l(r, a) =    (n n/2) log(2  )     (n/2) log det r     (1/2)

nxk=1

(yk     a)t r   1(yk     a)

394

7 statistical estimation

can be expressed as

l(r, a) =

n

2 (cid:0)   n log(2  )     log det r     tr(r   1y )     (a       )t r   1(a       )(cid:1) .

use this expression to show that if y     0, the ml estimates of r and a are unique,
and given by

aml =   ,

rml = y.

(b) the log-likelihood function includes a convex term (    log det r), so it is not obvi-
ously concave. show that l is concave, jointly in r and a, in the region de   ned
by

this means we can use id76 to compute simultaneous ml estimates
of r and a, subject to convex constraints, as long as the constraints include r (cid:22) 2y ,
i.e., the estimate r must not exceed twice the unconstrained ml estimate.

r (cid:22) 2y.

7.5 markov chain estimation. consider a markov chain with n states, and transition proba-

bility matrix p     rn  n de   ned as

pij = prob(y(t + 1) = i | y(t) = j).

the transition probabilities must satisfy pij     0 and pn

i=1 pij = 1, j = 1, . . . , n. we
consider the problem of estimating the transition probabilities, given an observed sample
sequence y(1) = k1, y(2) = k2, . . . , y(n ) = kn.

(a) show that if there are no other prior constraints on pij, then the ml estimates are
the empirical transition frequencies:   pij is the ratio of the number of times the state
transitioned from j into i, divided by the number of times it was j, in the observed
sample.

(b) suppose that an equilibrium distribution p of the markov chain is known, i.e., a
+ satisfying 1t q = 1 and p q = q. show that the problem of computing
vector q     rn
the ml estimate of p , given the observed sequence and knowledge of q, can be
expressed as a id76 problem.

7.6 estimation of mean and variance. consider a random variable x     r with density p,
which is normalized, i.e., has zero mean and unit variance. consider a random variable
y = (x+b)/a obtained by an a   ne transformation of x, where a > 0. the random variable
y has mean b and variance 1/a2. as a and b vary over r+ and r, respectively, we generate
a family of densities obtained from p by scaling and shifting, uniquely parametrized by
mean and variance.
show that if p is log-concave, then    nding the ml estimate of a and b, given samples
y1, . . . , yn of y, is a convex problem.
as an example, work out an analytical solution for the ml estimates of a and b, assuming
p is a normalized laplacian density, p(x) = e   2|x|.

7.7 ml estimation of poisson distributions. suppose xi, i = 1, . . . , n, are independent random

variables with poisson distributions

prob(xi = k) =

e     i   k
i

k!

,

with unknown means   i. the variables xi represent the number of times that one of n
possible independent events occurs during a certain period. in emission tomography, for
example, they might represent the number of photons emitted by n sources.
we consider an experiment designed to determine the means   i. the experiment involves
m detectors. if event i occurs, it is detected by detector j with id203 pji. we assume

exercises

395

the probabilities pji are given (with pji     0,pm

recorded by detector j is denoted yj,

j=1 pji     1). the total number of events

yj =

nxi=1

yji,

j = 1, . . . , m.

formulate the ml estimation problem of estimating the means   i, based on observed
values of yj, j = 1, . . . , m, as a id76 problem.
hint. the variables yji have poisson distributions with means pji  i, i.e.,

prob(yji = k) =

e   pji  i (pji  i)k

k!

.

the sum of n independent poisson variables with means   1, . . . ,   n has a poisson distri-
bution with mean   1 +        +   n.

7.8 estimation using sign measurements. we consider the measurement setup

yi = sign(at

i x + bi + vi),

i = 1, . . . , m,

where x     rn is the vector to be estimated, and yi     {   1, 1} are the measurements. the
vectors ai     rn and scalars bi     r are known, and vi are iid noises with a log-concave
id203 density. (you can assume that at
i x + bi + vi = 0 does not occur.) show that
id113 of x is a id76 problem.

7.9 estimation with unknown sensor nonlinearity. we consider the measurement setup

yi = f (at

i x + bi + vi),

i = 1, . . . , m,

where x     rn is the vector to be estimated, yi     r are the measurements, ai     rn,
bi     r are known, and vi are iid noises with log-concave id203 density. the function
f : r     r, which represents a measurement nonlinearity, is not known. however, it is
known that f    (t)     [l, u] for all t, where 0 < l < u are given.
explain how to use id76 to    nd a maximum likelihood estimate of x, as
well as the function f . (this is an in   nite-dimensional ml estimation problem, but you
can be informal in your approach and explanation.)

7.10 nonparametric distributions on rk. we consider a random variable x     rk with values

in a    nite set {  1, . . . ,   n}, and with distribution

pi = prob(x =   i),

i = 1, . . . , n.

show that a lower bound on the covariance of x,

s (cid:22) e(x     e x)(x     e x)t ,

is a convex constraint in p.

optimal detector design

7.11 randomized detectors. show that every randomized detector can be expressed as a convex

combination of a set of deterministic detectors: if

t =(cid:2) t1

t2

      

tn (cid:3)     rm  n

satis   es tk (cid:23) 0 and 1t tk = 1, then t can be expressed as
t =   1t1 +        +   n tn ,

396

7 statistical estimation

  i     0,pn

where ti is a zero-one matrix with exactly one element equal to one per column, and
i=1   i = 1. what is the maximum number of deterministic detectors n we may

need?
we can interpret this convex decomposition as follows. the randomized detector can be
realized as a bank of n deterministic detectors. when we observe x = k, the estimator
chooses a random index from the set {1, . . . , n}, with id203 prob(j = i) =   i, and
then uses deterministic detector tj.
7.12 optimal action. in detector design, we are given a matrix p     rn  m (whose columns
are id203 distributions), and then design a matrix t     rm  n (whose columns are
id203 distributions), so that d = t p has large diagonal elements (and small o   -
diagonal elements). in this problem we study the dual problem: given p ,    nd a matrix
s     rm  n (whose columns are id203 distributions), so that   d = p s     rn  n has
large diagonal elements (and small o   -diagonal elements). to make the problem speci   c,
we take the objective to be maximizing the minimum element of   d on the diagonal.
we can interpret this problem as follows. there are n outcomes, which depend (stochas-
tically) on which of m inputs or actions we take: pij is the id203 that outcome i
occurs, given action j. our goal is    nd a (randomized) strategy that, to the extent pos-
sible, causes any speci   ed outcome to occur. the strategy is given by the matrix s: sji
is the id203 that we take action j, when we want outcome i to occur. the matrix
  d gives the action error id203 matrix:   dij is the id203 that outcome i occurs,
when we want outcome j to occur. in particular,   dii is the id203 that outcome i
occurs, when we want it to occur.
show that this problem has a simple analytical solution. show that (unlike the corre-
sponding detector problem) there is always an optimal solution that is deterministic.
hint. show that the problem is separable in the columns of s.

chebyshev and cherno    bounds

7.13 chebyshev-type inequalities on a    nite set. assume x is a random variable taking values
in the set {  1,   2, . . . ,   m}, and let s be a subset of {  1, . . . ,   m}. the distribution of x
is unknown, but we are given the expected values of n functions fi:

e fi(x) = bi,

i = 1, . . . , n.

(7.32)

show that the optimal value of the lp

minimize

x0 +pn
subject to x0 +pn
x0 +pn

i=1 bixi
i=1 fi(  )xi     1,        s
i=1 fi(  )xi     0,    6    s,

with variables x0, . . . , xn, is an upper bound on prob(x     s), valid for all distributions
that satisfy (7.32). show that there always exists a distribution that achieves the upper
bound.

chapter 8

geometric problems

8.1 projection on a set

the distance of a point x0     rn to a closed set c     rn, in the norm k    k, is
de   ned as

dist(x0, c) = inf{kx0     xk | x     c}.

the in   mum here is always achieved. we refer to any point z     c which is closest
to x0, i.e., satis   es kz     x0k = dist(x0, c), as a projection of x0 on c. in general
there can be more than one projection of x0 on c, i.e., several points in c closest
to x0.

in some special cases we can establish that the projection of a point on a set
is unique. for example, if c is closed and convex, and the norm is strictly convex
(e.g., the euclidean norm), then for any x0 there is always exactly one z     c which
is closest to x0. as an interesting converse, we have the following result: if for every
x0 there is a unique euclidean projection of x0 on c, then c is closed and convex
(see exercise 8.2).

we use the notation pc : rn     rn to denote any function for which pc(x0)

is a projection of x0 on c, i.e., for all x0,

pc(x0)     c,

kx0     pc(x0)k = dist(x0, c).

in other words, we have

pc(x0) = argmin{kx     x0k | x     c}.

we refer to pc as projection on c.

example 8.1 projection on the unit square in r2. consider the (boundary of the)
unit square in r2, i.e., c = {x     r2 | kxk    = 1}. we take x0 = 0.
in the    1-norm, the four points (1, 0), (0,   1), (   1, 0), and (0, 1) are closest to x0 = 0,
with distance 1, so we have dist(x0, c) = 1 in the    1-norm. the same statement holds
for the    2-norm.

in the       -norm, all points in c lie at a distance 1 from x0, and dist(x0, c) = 1.

398

8 geometric problems

example 8.2 projection onto rank-k matrices. consider the set of m    n matrices
with rank less than or equal to k,

c = {x     rm  n | rank x     k},

with k     min{m, n}, and let x0     rm  n. we can    nd a projection of x0 on
c, in the (spectral or maximum singular value) norm k    k2, via the singular value
decomposition. let

x0 =

  iuivt
i

rxi=1

be the singular value decomposition of x0, where r = rank x0. then the matrix

  iuivt
i

is a projection of x0 on c.

y =pmin{k,r}

i=1

8.1.1 projecting a point on a convex set

if c is convex, then we can compute the projection pc(x0) and the distance
dist(x0, c) by solving a id76 problem. we represent the set c
by a set of linear equalities and convex inequalities

ax = b,

fi(x)     0,

i = 1, . . . , m,

(8.1)

and    nd the projection of x0 on c by solving the problem

minimize
subject to

kx     x0k
fi(x)     0,
ax = b,

i = 1, . . . , m

(8.2)

with variable x. this problem is feasible if and only if c is nonempty; when it is
feasible, its optimal value is dist(x0, c), and any optimal point is a projection of
x0 on c.

euclidean projection on a polyhedron
the projection of x0 on a polyhedron described by linear inequalities ax (cid:22) b can
be computed by solving the qp

kx     x0k2
minimize
subject to ax (cid:22) b.
some special cases have simple analytical solutions.

2

    the euclidean projection of x0 on a hyperplane c = {x | at x = b} is given

by

pc(x0) = x0 + (b     at x0)a/kak2
2.

    the euclidean projection of x0 on a halfspace c = {x | at x     b} is given by

pc(x0) =(cid:26) x0 + (b     at x0)a/kak2

x0

2 at x0 > b
at x0     b.

8.1 projection on a set

399

    the euclidean projection of x0 on a rectangle c = {x | l (cid:22) x (cid:22) u} (where

l     u) is given by

pc(x0)k =         

lk
x0k
uk

x0k     lk
lk     x0k     uk
x0k     uk.

euclidean projection on a proper cone

let x = pk(x0) denote the euclidean projection of a point x0 on a proper cone k.
the kkt conditions of

minimize
subject to x (cid:23)k 0

kx     x0k2

2

are given by

x (cid:23)k 0,

x     x0 = z,

z (cid:23)k     0,

zt x = 0.

introducing the notation x+ = x and x    = z, we can express these conditions as

x0 = x+     x   ,

x+ (cid:23)k 0,

x    (cid:23)k     0,

xt
+x    = 0.

in other words, by projecting x0 on the cone k, we decompose it into the di   erence
of two orthogonal elements: one nonnegative with respect to k (and which is the
projection of x0 on k), and the other nonnegative with respect to k    .

some speci   c examples:
    for k = rn

+, we have pk(x0)k = max{x0k, 0}. the euclidean projection
of a vector onto the nonnegative orthant is found by replacing each negative
component with 0.

    for k = sn
pn

i=1 max{0,   i}vivt

+, and the euclidean (or frobenius) norm k  kf , we have pk(x0) =
is the eigenvalue decomposi-
tion of x0. to project a symmetric matrix onto the positive semide   nite cone,
we form its eigenvalue expansion and drop terms associated with negative
eigenvalues. this matrix is also the projection onto the positive semide   nite
cone in the    2-, or spectral norm.

i , where x0 =pn

i=1   ivivt
i

8.1.2 separating a point and a convex set

suppose c is a closed convex set described by the equalities and inequalities (8.1).
if x0     c, then dist(x0, c) = 0, and the optimal point for the problem (8.2) is
x0. if x0 6    c then dist(x0, c) > 0, and the optimal value of the problem (8.2) is
positive. in this case we will see that any dual optimal point provides a separating
hyperplane between the point x0 and the set c.

the link between projecting a point on a convex set and    nding a hyperplane
that separates them (when the point is not in the set) should not be surprising.
indeed, our proof of the separating hyperplane theorem, given in   2.5.1, relies on

400

8 geometric problems

x0

pc(x0)

c

figure 8.1 a point x0 and its euclidean projection pc (x0) on a convex set c.
the hyperplane midway between the two, with normal vector pc (x0)     x0,
strictly separates the point and the set. this property does not hold for
general norms; see exercise 8.4.

   nding the euclidean distance between the sets. if pc(x0) denotes the euclidean
projection of x0 on c, where x0 6    c, then the hyperplane

(pc(x0)     x0)t (x     (1/2)(x0 + pc(x0))) = 0

(strictly) separates x0 from c, as illustrated in    gure 8.1. in other norms, however,
the clearest link between the projection problem and the separating hyperplane
problem is via lagrange duality.

we    rst express (8.2) as

minimize
subject to

kyk
fi(x)     0,
ax = b
x0     x = y

i = 1, . . . , m

with variables x and y. the lagrangian of this problem is

l(x, y,   ,   ,   ) = kyk +

mxi=1

  ifi(x) +   t (ax     b) +   t (x0     x     y)

and the dual function is

g(  ,   ,   ) =(cid:26) inf x(cid:0)pm

      

so we obtain the dual problem

maximize
subject to    (cid:23) 0

k  k        1,

i=1   ifi(x) +   t (ax     b) +   t (x0     x)(cid:1)
  t x0 + inf x(cid:0)pm

i=1   ifi(x) +   t (ax     b)       t x(cid:1)

k  k        1
otherwise,

with variables   ,   ,   . we can interpret the dual problem as follows. suppose   ,
  ,    are dual feasible with a positive dual objective value, i.e.,    (cid:23) 0, k  k        1,

8.1 projection on a set

401

and

  t x0       t x +

mxi=1

  ifi(x) +   t (ax     b) > 0

for all x. this implies that   t x0 >   t x for x     c, and therefore    de   nes a
strictly separating hyperplane. in particular, suppose (8.2) is strictly feasible, so
strong duality holds. if x0 6    c, the optimal value is positive, and any dual optimal
solution de   nes a strictly separating hyperplane.
note that this construction of a separating hyperplane, via duality, works for
any norm. in contrast, the simple construction described above only works for the
euclidean norm.

separating a point from a polyhedron

the dual problem of

is

minimize
subject to ax (cid:22) b

kyk
x0     x = y
  t x0     bt   
maximize
subject to at    =   
k  k        1
   (cid:23) 0

which can be further simpli   ed as

(ax0     b)t   
maximize
subject to kat   k        1
   (cid:23) 0.

it is easily veri   ed that if the dual objective is positive, then at    is the normal
vector to a separating hyperplane: if ax (cid:22) b, then

(at   )t x =   t (ax)       t b <   t ax0,

so    = at    de   nes a separating hyperplane.

8.1.3 projection and separation via indicator and support functions

the ideas described above in   8.1.1 and   8.1.2 can be expressed in a compact form
in terms of the indicator function ic and the support function sc of the set c,
de   ned as

sc(x) = sup
y   c

xt y,

ic(x) =(cid:26) 0

x     c
+    x 6    c.

the problem of projecting x0 on a closed convex set c can be expressed compactly
as

minimize
subject to

kx     x0k
ic(x)     0,

402

8 geometric problems

or, equivalently, as

minimize
subject to

kyk
ic(x)     0
x0     x = y

where the variables are x and y. the dual function of this problem is

g(z,   ) = inf

x,y(cid:0)kyk +   ic(x) + zt (x0     x     y)(cid:1)
= (cid:26) zt x0 + inf x(cid:0)   zt x + ic(x)(cid:1)
= (cid:26) zt x0     sc(z)

kzk        1,
otherwise

       0

      

      

kzk        1,
otherwise

       0

so we obtain the dual problem

maximize
subject to

zt x0     sc(z)
kzk        1.

if z is dual optimal with a positive objective value, then zt x0 > zt x for all x     c,
i.e., z de   nes a separating hyperplane.

8.2 distance between sets

the distance between two sets c and d, in a norm k    k, is de   ned as

dist(c, d) = inf{kx     yk | x     c, y     d}.

the two sets c and d do not intersect if dist(c, d) > 0. they intersect if
dist(c, d) = 0 and the in   mum in the de   nition is attained (which is the case, for
example, if the sets are closed and one of the sets is bounded).

the distance between sets can be expressed in terms of the distance between a

point and a set,

dist(c, d) = dist(0, d     c),

so the results of the previous section can be applied. in this section, however, we
derive results speci   cally for problems involving distance between sets. this allows
us to exploit the structure of the set c     d, and makes the interpretation easier.

8.2.1 computing the distance between convex sets

suppose c and d are described by two sets of convex inequalities

c = {x | fi(x)     0, i = 1, . . . , m},

d = {x | gi(x)     0, i = 1, . . . , p}.

8.2 distance between sets

403

d

c

figure 8.2 euclidean distance between polyhedra c and d. the dashed line
connects the two points in c and d, respectively, that are closest to each
other in euclidean norm. these points can be found by solving a qp.

(we can include linear equalities, but exclude them here for simplicity.) we can
   nd dist(c, d) by solving the id76 problem

minimize
subject to

kx     yk
fi(x)     0,
gi(y)     0,

i = 1, . . . , m
i = 1, . . . , p.

(8.3)

euclidean distance between polyhedra
let c and d be two polyhedra described by the sets of linear inequalities a1x (cid:22) b1
and a2x (cid:22) b2, respectively. the distance between c and d is the distance between
the closest pair of points, one in c and the other in d, as illustrated in    gure 8.2.
the distance between them is the optimal value of the problem

kx     yk2
minimize
subject to a1x (cid:22) b1
a2y (cid:22) b2.

(8.4)

we can square the objective to obtain an equivalent qp.

8.2.2 separating convex sets

the dual of the problem (8.3) of    nding the distance between two convex sets has
an interesting geometric interpretation in terms of separating hyperplanes between
the sets. we    rst express the problem in the following equivalent form:

minimize
subject to

kwk
fi(x)     0,
gi(y)     0,
x     y = w.

i = 1, . . . , m
i = 1, . . . , p

(8.5)

the dual function is

g(  , z,   ) = inf

x,y,w kwk +

  ifi(x) +

mxi=1

  igi(y) + zt (x     y     w)!

pxi=1

404

8 geometric problems

which results in the dual problem

= (cid:26) inf x(cid:0)pm

      

maximize
subject to

i=1   ifi(x) + zt x(cid:1) + inf y(cid:0)pp
inf x(cid:0)pm

i=1   ifi(x) + zt x(cid:1) + inf y(cid:0)pp

kzk        1
   (cid:23) 0,    (cid:23) 0.

i=1   igi(y)     zt y(cid:1)

kzk        1
otherwise,

i=1   igi(y)     zt y(cid:1)

(8.6)

we can interpret this geometrically as follows.
positive objective value, then

if   ,    are dual feasible with a

  ifi(x) + zt x +

mxi=1

pxi=1

  igi(y)     zt y > 0

for all x and y. in particular, for x     c and y     d, we have zt x     zt y > 0, so we
see that z de   nes a hyperplane that strictly separates c and d.
therefore, if strong duality holds between the two problems (8.5) and (8.6)
(which is the case when (8.5) is strictly feasible), we can make the following con-
clusion. if the distance between the two sets is positive, then they can be strictly
separated by a hyperplane.

separating polyhedra
applying these duality results to sets de   ned by linear inequalities a1x (cid:22) b1 and
a2x (cid:22) b2, we    nd the dual problem

1        bt
maximize    bt
2   
subject to at
1    + z = 0
at
2        z = 0
kzk        1
   (cid:23) 0,    (cid:23) 0.

if   ,   , and z are dual feasible, then for all x     c, y     d,

zt x =      t a1x          t b1,
and, if the dual objective value is positive,

zt y =   t a2x       t b2,

i.e., z de   nes a separating hyperplane.

zt x     zt y          t b1       t b2 > 0,

8.2.3 distance and separation via indicator and support functions

the ideas described above in   8.2.1 and   8.2.2 can be expressed in a compact form
using indicator and support functions. the problem of    nding the distance between
two convex sets can be posed as the convex problem

minimize
subject to

kx     yk
ic(x)     0
id(y)     0,

8.3 euclidean distance and angle problems

405

which is equivalent to

the dual of this problem is

minimize
subject to

kwk
ic(x)     0
id(y)     0
x     y = w.

maximize
subject to

   sc(   z)     sd(z)
kzk        1.

if z is dual feasible with a positive objective value, then sd(z) <    sc(   z), i.e.,

sup
x   d

zt x < inf
x   c

zt x.

in other words, z de   nes a hyperplane that strictly separates c and d.

8.3 euclidean distance and angle problems

suppose a1, . . . , an is a set of vectors in rn, which we assume (for now) have known
euclidean lengths

l1 = ka1k2,

. . . ,

ln = kank2.

we will refer to the set of vectors as a con   guration, or, when they are indepen-
dent, a basis. in this section we consider optimization problems involving various
geometric properties of the con   guration, such as the euclidean distances between
pairs of the vectors, the angles between pairs of the vectors, and various geometric
measures of the conditioning of the basis.

8.3.1 gram matrix and realizability

the lengths, distances, and angles can be expressed in terms of the gram matrix
associated with the vectors a1, . . . , an, given by

so that gij = at

i aj. the diagonal entries of g are given by

g = at a,

a =(cid:2) a1

       an (cid:3) ,

gii = l2
i ,

i = 1, . . . , n,

which (for now) we assume are known and    xed. the distance dij between ai and
aj is

dij = kai     ajk2

= (l2
= (l2

i + l2
i + l2

i aj)1/2
j     2at
j     2gij)1/2.

406

8 geometric problems

conversely, we can express gij in terms of dij as

gij =

i + l2
j     d2
l2
2

ij

,

which we note, for future reference, is an a   ne function of d2
ij.

the correlation coe   cient   ij between (nonzero) ai and aj is given by

  ij =

at
i aj

kaik2kajk2

=

gij
lilj

,

so that gij = lilj  ij is a linear function of   ij. the angle   ij between (nonzero) ai
and aj is given by

  ij = cos   1   ij = cos   1(gij/(lilj)),

where we take cos   1        [0,   ]. thus, we have gij = lilj cos   ij.
the lengths, distances, and angles are invariant under orthogonal transforma-
tions: if q     rn  n is orthogonal, then the set of vectors qai, . . . , qan has the
same gram matrix, and therefore the same lengths, distances, and angles.

realizability

the gram matrix g = at a is, of course, symmetric and positive semide   nite. the
converse is a basic result of id202: a matrix g     sn is the gram matrix
of a set of vectors a1, . . . , an if and only if g (cid:23) 0. when g (cid:23) 0, we can construct
a con   guration with gram matrix g by    nding a matrix a with at a = g. one
solution of this equation is the symmetric squareroot a = g1/2. when g     0, we
can    nd a solution via the cholesky factorization of g: if llt = g, then we can
take a = lt . moreover, we can construct all con   gurations with the given gram
matrix g, given any one solution a, by orthogonal transformation: if   at   a = g is
any solution, then   a = qa for some orthogonal matrix q.

thus, a set of lengths, distances, and angles (or correlation coe   cients) is real-
izable, i.e., those of some con   guration, if and only if the associated gram matrix
g is positive semide   nite, and has diagonal elements l2

1, . . . , l2
n.

we can use this fact to express several geometric problems as convex optimiza-
tion problems, with g     sn as the optimization variable. realizability imposes
the constraint g (cid:23) 0 and gii = l2
i , i = 1, . . . , n; we list below several other convex
constraints and objectives.

angle and distance constraints

we can    x an angle to have a certain value,   ij =   , via the linear equality
constraint gij = lilj cos   . more generally, we can impose a lower and upper
bound on an angle,          ij       , by the constraint

lilj cos        gij     lilj cos   ,

which is a pair of linear inequalities on g. (here we use the fact that cos   1 is
monotone decreasing.) we can maximize or minimize a particular angle   ij, by
minimizing or maximizing gij (again using monotonicity of cos   1).

8.3 euclidean distance and angle problems

407

in a similar way we can impose constraints on the distances. to require that

dij lies in an interval, we use

dmin     dij     dmax        d2
       d2

min     d2
min     l2

ij     d2
i + l2

max

j     2gij     d2

max,

which is a pair of linear inequalities on g. we can minimize or maximize a distance,
by minimizing or maximizing its square, which is an a   ne function of g.

as a simple example, suppose we are given ranges (i.e., an interval of possible
values) for some of the angles and some of the distances. we can then    nd the
minimum and maximum possible value of some other angle, or some other distance,
over all con   gurations, by solving two sdps. we can reconstruct the two extreme
con   gurations by factoring the resulting optimal gram matrices.

singular value and condition number constraints
the singular values of a,   1                  n, are the squareroots of the eigenvalues
  1                  n of g. therefore   2
n is a concave
function of g. thus we can impose an upper bound on the maximum singular value
of a, or minimize it; we can impose a lower bound on the minimum singular value,
or maximize it. the condition number of a,   1/  n, is a quasiconvex function of g,
so we can impose a maximum allowable value, or minimize it over all con   gurations
that satisfy the other geometric constraints, by quasiid76.

1 is a convex function of g, and   2

roughly speaking, the constraints we can impose as convex constraints on g

are those that require a1, . . . , an to be a well conditioned basis.

dual basis
when g     0, a1, . . . , an form a basis for rn. the associated dual basis is b1, . . . , bn,
where

bt

i aj =(cid:26) 1 i = j

0 i 6= j.

the dual basis vectors b1, . . . , bn are simply the rows of the matrix a   1. as a
result, the gram matrix associated with the dual basis is g   1.

we can express several geometric conditions on the dual basis as convex con-

straints on g. the (squared) lengths of the dual basis vectors,

kbik2

2 = et

i g   1ei,

are convex functions of g, and so can be minimized. the trace of g   1, another
convex function of g, gives the sum of the squares of the lengths of the dual basis
vectors (and is another measure of a well conditioned basis).

ellipsoid and simplex volume
the volume of the ellipsoid {au | kuk2     1}, which gives another measure of how
well conditioned the basis is, is given by

  (det(at a))1/2 =   (det g)1/2,

408

8 geometric problems

where    is the volume of the unit ball in rn. the log volume is therefore log    +
(1/2) log det g, which is a concave function of g. we can therefore maximize the
volume of the image ellipsoid, over a convex set of con   gurations, by maximizing
log det g.

the same holds for any set in rn. the volume of the image under a is its
volume, multiplied by the factor (det g)1/2. for example, consider the image under
a of the unit simplex conv{0, e1, . . . , en}, i.e., the simplex conv{0, a1, . . . , an}.
the volume of this simplex is given by   (det g)1/2, where    is the volume of the
unit simplex in rn. we can maximize the volume of this simplex by maximizing
log det g.

8.3.2 problems involving angles only

suppose we only care about the angles (or correlation coe   cients) between the
vectors, and do not specify the lengths or distances between them. in this case it is
intuitively clear that we can simply assume the vectors ai have length li = 1. this
is easily veri   ed: the gram matrix has the form g = diag(l)c diag(l), where l
is the vector of lengths, and c is the correlation matrix, i.e., cij = cos   ij.
it
follows that if g (cid:23) 0 for any set of positive lengths, then g (cid:23) 0 for all sets of
positive lengths, and in particular, this occurs if and only if c (cid:23) 0 (which is the
same as assuming that all lengths are one). thus, a set of angles   ij     [0,   ],
i, j = 1, . . . , n is realizable if and only if c (cid:23) 0, which is a linear matrix inequality
in the correlation coe   cients.
as an example, suppose we are given lower and upper bounds on some of the
angles (which is equivalent to imposing lower and upper bounds on the correlation
coe   cients). we can then    nd the minimum and maximum possible value of some
other angle, over all con   gurations, by solving two sdps.

example 8.3 bounding correlation coe   cients. we consider an example in r4, where
we are given

0.6       12     0.9,
0.5       24     0.7,    0.8       34        0.4.

0.8       13     0.9,

(8.7)

to    nd the minimum and maximum possible values of   14, we solve the two sdps

minimize/maximize
subject to

  12
1
  23
  24

  13
  23
1
  34

  14
  24
  34
1

          (cid:23) 0,

1
  12
  13
  14

  14

(8.7)         
          ,

         

with variables   12,   13,   14,   23,   24,   34. the minimum and maximum values (to two
signi   cant digits) are    0.39 and 0.23, with corresponding correlation matrices

         

1.00
0.60
0.87
   0.39

0.60
1.00
0.33
0.50    0.55

0.87    0.39
0.50
0.33
1.00    0.55
1.00

1.00
0.71
0.80
0.23

0.71
1.00
0.31
0.59    0.40

0.80
0.23
0.31
0.59
1.00    0.40
1.00

          .

8.3 euclidean distance and angle problems

409

8.3.3 euclidean distance problems

in a euclidean distance problem, we are concerned only with the distances between
the vectors, dij, and do not care about the lengths of the vectors, or about the angles
between them. these distances, of course, are invariant not only under orthogonal
transformations, but also translation: the con   guration   a1 = a1+b, . . . ,   an = an+b
has the same distances as the original con   guration, for any b     rn. in particular,
for the choice

b =    (1/n)

ai =    (1/n)a1,

nxi=1

i=1   ai = 0.

we see that   ai have the same distances as the original con   guration, and also satisfy
it follows that in a euclidean distance problem, we can assume,
without any loss of generality, that the average of the vectors a1, . . . , an is zero,
i.e., a1 = 0.

pn

we can solve euclidean distance problems by considering the lengths (which
cannot occur in the objective or constraints of a euclidean distance problem) as
free variables in the optimization problem. here we rely on the fact that there is
a con   guration with distances dij     0 if and only if there are lengths l1, . . . , ln for
which g (cid:23) 0, where gij = (l2
ij (with, of course,
dii = 0). the condition that g (cid:23) 0 for some choice of lengths can be expressed as
(8.8)

i + l2
we de   ne z     rn as zi = l2

j     d2
i , and d     sn by dij = d2

g = (z1t + 1zt     d)/2 (cid:23) 0 for some z (cid:23) 0,

ij)/2.

which is an lmi in d and z. a matrix d     sn, with nonnegative elements,
zero diagonal, and which satis   es (8.8), is called a euclidean distance matrix. a
matrix is a euclidean distance matrix if and only if its entries are the squares
of the euclidean distances between the vectors of some con   guration. (given a
euclidean distance matrix d and the associated length squared vector z, we can
reconstruct one, or all, con   gurations with the given pairwise distances using the
method described above.)

the condition (8.8) turns out to be equivalent to the simpler condition that d

is negative semide   nite on 1   , i.e.,

(8.8)        ut du     0 for all u with 1t u = 0

       (i     (1/n)11t )d(i     (1/n)11t ) (cid:22) 0.

this simple matrix inequality, along with dij     0, dii = 0, is the classical char-
acterization of a euclidean distance matrix. to see the equivalence, recall that we
can assume a1 = 0, which implies that 1t g1 = 1t at a1 = 0. it follows that
g (cid:23) 0 if and only if g is positive semide   nite on 1   , i.e.,

0 (cid:22) (i     (1/n)11t )g(i     (1/n)11t )

= (1/2)(i     (1/n)11t )(z1t + 1zt     d)(i     (1/n)11t )
=    (1/2)(i     (1/n)11t )d(i     (1/n)11t ),

which is the simpli   ed condition.

410

8 geometric problems

in summary, a matrix d     sn is a euclidean distance matrix, i.e., gives the

squared distances between a set of n vectors in rn, if and only if

dii = 0,

i = 1, . . . , n,

dij     0,

i, j = 1, . . . , n,

(i     (1/n)11t )d(i     (1/n)11t ) (cid:22) 0,

which is a set of linear equalities, linear inequalities, and a matrix inequality in
d. therefore we can express any euclidean distance problem that is convex in the
squared distances as a convex problem with variable d     sn.

8.4 extremal volume ellipsoids

suppose c     rn is bounded and has nonempty interior. in this section we consider
the problems of    nding the maximum volume ellipsoid that lies inside c, and the
minimum volume ellipsoid that covers c. both problems can be formulated as
convex programming problems, but are tractable only in special cases.

8.4.1 the l  owner-john ellipsoid

the minimum volume ellipsoid that contains a set c is called the l  owner-john
ellipsoid of the set c, and is denoted elj. to characterize elj, it will be convenient
to parametrize a general ellipsoid as

e = {v | kav + bk2     1} ,

(8.9)

i.e., the inverse image of the euclidean unit ball under an a   ne mapping. we can
assume without loss of generality that a     sn
++, in which case the volume of e is
proportional to det a   1. the problem of computing the minimum volume ellipsoid
containing c can be expressed as

minimize
subject to

log det a   1
supv   c kav + bk2     1,

(8.10)

where the variables are a     sn and b     rn, and there is an implicit constraint
a     0. the objective and constraint functions are both convex in a and b, so the
problem (8.10) is convex. evaluating the constraint function in (8.10), however,
involves solving a convex maximization problem, and is tractable only in certain
special cases.

minimum volume ellipsoid covering a    nite set

we consider the problem of    nding the minimum volume ellipsoid that contains
the    nite set c = {x1, . . . , xm}     rn. an ellipsoid covers c if and only if it
covers its convex hull, so    nding the minimum volume ellipsoid that covers c

8.4 extremal volume ellipsoids

411

is the same as    nding the minimum volume ellipsoid containing the polyhedron
conv{x1, . . . , xm}. applying (8.10), we can write this problem as

minimize
subject to

log det a   1
kaxi + bk2     1,

i = 1, . . . , m

(8.11)

where the variables are a     sn and b     rn, and we have the implicit constraint a    
0. the norm constraints kaxi +bk2     1, i = 1, . . . , m, are convex inequalities in the
variables a and b. they can be replaced with the squared versions, kaxi + bk2
2     1,
which are convex quadratic inequalities in a and b.

minimum volume ellipsoid covering union of ellipsoids

minimum volume covering ellipsoids can also be computed e   ciently for certain
sets c that are de   ned by quadratic inequalities. in particular, it is possible to
compute the l  owner-john ellipsoid for a union or sum of ellipsoids.

as an example, consider the problem of    nding the minimum volume ellip-
soid elj, that contains the ellipsoids e1, . . . ,em (and therefore, the convex hull of
their union). the ellipsoids e1, . . . , em will be described by (convex) quadratic
inequalities:

ei = {x | xt aix + 2bt

i x + ci     0},

i = 1, . . . , m,

where ai     sn

++. we parametrize the ellipsoid elj as

elj = {x | kax + bk2     1}

= {x | xt at ax + 2(at b)t x + bt b     1     0}

where a     sn and b     rn. now we use a result from   b.2, that ei     elj if and
only if there exists a        0 such that

(cid:20) a2        ai

(ab        bi)t

ab        bi

bt b     1        ci (cid:21) (cid:22) 0.

the volume of elj is proportional to det a   1, so we can    nd the minimum volume
ellipsoid that contains e1, . . . ,em by solving

minimize
subject to

log det a   1
  1     0, . . . ,   m     0

(cid:20) a2       iai

(ab       ibi)t

ab       ibi

bt b     1       ici (cid:21) (cid:22) 0,

or, replacing the variable b by   b = ab,

minimize
subject to

log det a   1
  1     0, . . . ,   m     0

  b       ibi
a2       iai
(  b       ibi)t    1       ici

  b

0

      

0
  bt

   a2        (cid:22) 0,

which is convex in the variables a2     sn,   b,   1, . . . ,   m.

i = 1, . . . , m,

i = 1, . . . , m,

412

8 geometric problems

figure 8.3 the outer ellipse is the boundary of the l  owner-john ellipsoid,
i.e., the minimum volume ellipsoid that encloses the points x1, . . . , x6 (shown
as dots), and therefore the polyhedron p = conv{x1, . . . , x6}. the smaller
ellipse is the boundary of the l  owner-john ellipsoid, shrunk by a factor of
n = 2 about its center. this ellipsoid is guaranteed to lie inside p.

e   ciency of l  owner-john ellipsoidal approximation
let elj be the l  owner-john ellipsoid of the convex set c     rn, which is bounded
and has nonempty interior, and let x0 be its center. if we shrink the l  owner-john
ellipsoid by a factor of n, about its center, we obtain an ellipsoid that lies inside
the set c:

x0 + (1/n)(elj     x0)     c     elj.

in other words, the l  owner-john ellipsoid approximates an arbitrary convex set,
within a factor that depends only on the dimension n. figure 8.3 shows a simple
example.

the factor 1/n cannot be improved without additional assumptions on c. any
simplex in rn, for example, has the property that its l  owner-john ellipsoid must
be shrunk by a factor n to    t inside it (see exercise 8.13).

we will prove this e   ciency result for the special case c = conv{x1, . . . , xm}.
we square the norm constraints in (8.11) and introduce variables   a = a2 and
  b = ab, to obtain the problem

the kkt conditions for this problem are

minimize
subject to xi

i = 1, . . . , m.

log det   a   1
t   axi     2  bt xi +   bt   a   1  b     1,
pm
i=1   i(xi       a   1  b) = 0,
t   axi     2  bt xi +   bt   a   1  b     1,
xi
t   axi + 2  bt xi       bt   a   1  b) = 0,
  i(1     xi

t       a   1  b  bt   a   1) =   a   1,

i = 1, . . . , m.

i = 1, . . . , m,

pm

i=1   i(xixi
  i     0,

(8.12)

by a suitable a   ne change of coordinates, we can assume that   a = i and   b = 0,
i.e., the minimum volume ellipsoid is the unit ball centered at the origin. the kkt

8.4 extremal volume ellipsoids

413

conditions then simplify to

  ixixi

t = i,

mxi=1

mxi=1

  ixi = 0,

  i(1     xi

t xi) = 0,

i = 1, . . . , m,

plus the feasibility conditions kxik2     1 and   i     0. by taking the trace of
both sides of the    rst equation, and using complementary slackness, we also have

i=1   i = n.
in the new coordinates the shrunk ellipsoid is a ball with radius 1/n, centered

pm

at the origin. we need to show that

kxk2     1/n =    x     c = conv{x1, . . . , xm}.

suppose kxk2     1/n. from the kkt conditions, we see that

x =

mxi=1

  i(xt xi)xi =

mxi=1

  i(xt xi + 1/n)xi =

mxi=1

  ixi,

(8.13)

where   i =   i(xt xi + 1/n). from the cauchy-schwartz inequality, we note that

  i =   i(xt xi + 1/n)       i(   kxk2kxik2 + 1/n)       i(   1/n + 1/n) = 0.

furthermore

  i =

  i(xt xi + 1/n) =

  i/n = 1.

mxi=1

mxi=1

mxi=1

this, along with (8.13), shows that x is a convex combination of x1, . . . , xm, hence
x     c.
e   ciency of l  owner-john ellipsoidal approximation for symmetric sets

if the set c is symmetric about a point x0, then the factor 1/n can be tightened
to 1/   n:

again, the factor 1/   n is tight. the l  owner-john ellipsoid of the cube

x0 + (1/   n)(elj     x0)     c     elj.

c = {x     rn |     1 (cid:22) x (cid:22) 1}

is the ball with radius    n. scaling down by 1/   n yields a ball enclosed in c, and
touching the boundary at x =   ei.
approximating a norm by a quadratic norm
let k    k be any norm on rn, and let c = {x | kxk     1} be its unit ball. let
elj = {x | xt ax     1}, with a     sn
++, be the l  owner-john ellipsoid of c. since c
is symmetric about the origin, the result above tells us that (1/   n)elj     c     elj.
let k    klj denote the quadratic norm

kzklj = (zt az)1/2,

414

8 geometric problems

whose unit ball is elj. the inclusions (1/   n)elj     c     elj are equivalent to the

inequalities

kzklj     kzk        nkzklj

for all z     rn. in other words, the quadratic norm k    klj approximates the norm
k    k within a factor of    n. in particular, we see that any norm on rn can be
approximated within a factor of    n by a quadratic norm.

8.4.2 maximum volume inscribed ellipsoid

we now consider the problem of    nding the ellipsoid of maximum volume that lies
inside a convex set c, which we assume is bounded and has nonempty interior. to
formulate this problem, we parametrize the ellipsoid as the image of the unit ball
under an a   ne transformation, i.e., as

e = {bu + d | kuk2     1} .

again it can be assumed that b     sn
++, so the volume is proportional to det b. we
can    nd the maximum volume ellipsoid inside c by solving the id76
problem

maximize
subject to

log det b
supkuk2   1 ic(bu + d)     0

(8.14)

in the variables b     sn and d     rn, with implicit constraint b     0.
maximum volume ellipsoid in a polyhedron

we consider the case where c is a polyhedron described by a set of linear inequal-
ities:

to apply (8.14) we    rst express the constraint in a more convenient form:

c = {x | at

i x     bi, i = 1, . . . , m}.

sup

kuk2   1

ic(bu + d)     0        sup

kuk2   1

at
i (bu + d)     bi,

i = 1, . . . , m

       kbaik2 + at

i d     bi,

i = 1, . . . , m.

we can therefore formulate (8.14) as a id76 problem in the variables
b and d:

minimize
subject to

log det b   1
kbaik2 + at

i d     bi,

i = 1, . . . , m.

(8.15)

maximum volume ellipsoid in an intersection of ellipsoids
we can also    nd the maximum volume ellipsoid e that lies in the intersection of
m ellipsoids e1, . . . ,em. we will describe e as e = {bu + d | kuk2     1} with
b     sn

++, and the other ellipsoids via convex quadratic inequalities,

ei = {x | xt aix + 2bt

i x + ci     0},

i = 1, . . . , m,

8.4 extremal volume ellipsoids

415

where ai     sn
if and only if

++. we    rst work out the condition under which e     ei. this occurs

sup

kuk2   1(cid:0)(d + bu)t ai(d + bu) + 2bt

i (d + bu) + ci(cid:1)

= dt aid + 2bt

i d + ci + sup

kuk2   1(cid:0)ut baibu + 2(aid + bi)t bu(cid:1)

    0.

from   b.1,

sup

kuk2   1(cid:0)ut baibu + 2(aid + bi)t bu(cid:1)        (dt aid + 2bt
  ii     baib (cid:21) (cid:23) 0.

if and only if there exists a   i     0 such that
i d     ci

(cid:20)      i     dt aid     2bt

(aid + bi)t b

b(aid + bi)

i d + ci)

the maximum volume ellipsoid contained in e1, . . . ,em can therefore be found by
solving the problem

minimize

log det b   1

subject to (cid:20)      i     dt aid     2bt
  ii     baib (cid:21) (cid:23) 0,
with variables b     sn, d     rn, and        rm, or, equivalently,
minimize

(aid + bi)t b

b(aid + bi)

log det b   1

i d     ci

subject to       

i a   1

i bi

     i     ci + bt
d + a   1
i bi

0

0
  ii
b

(d + a   1

i bi)t
b
a   1

i

       (cid:23) 0,

i = 1, . . . , m,

i = 1, . . . , m.

e   ciency of ellipsoidal inner approximations

approximation e   ciency results, similar to the ones for the l  owner-john ellipsoid,
hold for the maximum volume inscribed ellipsoid. if c     rn is convex, bounded,
with nonempty interior, then the maximum volume inscribed ellipsoid, expanded
by a factor of n about its center, covers the set c. the factor n can be tightened
to    n if the set c is symmetric about a point. an example is shown in    gure 8.4.

8.4.3 a   ne invariance of extremal volume ellipsoids

the l  owner-john ellipsoid and the maximum volume inscribed ellipsoid are both
if elj is the l  owner-john ellipsoid of c, and t     rn  n is
a   nely invariant.
nonsingular, then the l  owner-john ellipsoid of t c is telj. a similar result holds
for the maximum volume inscribed ellipsoid.
to establish this result, let e be any ellipsoid that covers c. then the ellipsoid
te covers t c. the converse is also true: every ellipsoid that covers t c has

416

8 geometric problems

figure 8.4 the maximum volume ellipsoid (shown shaded) inscribed in a
polyhedron p. the outer ellipse is the boundary of the inner ellipsoid,
expanded by a factor n = 2 about its center. the expanded ellipsoid is
guaranteed to cover p.

the form te, where e is an ellipsoid that covers c. in other words, the relation
  e = te gives a one-to-one correspondence between the ellipsoids covering t c and
the ellipsoids covering c. moreover, the volumes of the corresponding ellipsoids are
all related by the ratio | det t|, so in particular, if e has minimum volume among
ellipsoids covering c, then te has minimum volume among ellipsoids covering t c.

8.5 centering

8.5.1 chebyshev center

let c     rn be bounded and have nonempty interior, and x     c. the depth of a
point x     c is de   ned as

depth(x, c) = dist(x, rn \ c),

i.e., the distance to the closest point in the exterior of c. the depth gives the
radius of the largest ball, centered at x, that lies in c. a chebyshev center of the
set c is de   ned as any point of maximum depth in c:

xcheb(c) = argmax depth(x, c) = argmax dist(x, rn \ c).

a chebyshev center is a point inside c that is farthest from the exterior of c; it is
also the center of the largest ball that lies inside c. figure 8.5 shows an example,
in which c is a polyhedron, and the norm is euclidean.

8.5 centering

417

xchebxcheb

figure 8.5 chebyshev center of a polyhedron c, in the euclidean norm. the
center xcheb is the deepest point inside c, in the sense that it is farthest from
the exterior, or complement, of c. the center xcheb is also the center of the
largest euclidean ball (shown lightly shaded) that lies inside c.

chebyshev center of a convex set

when the set c is convex, the depth is a concave function for x     c, so computing
the chebyshev center is a id76 problem (see exercise 8.5). more
speci   cally, suppose c     rn is de   ned by a set of convex inequalities:

c = {x | f1(x)     0, . . . , fm(x)     0}.

we can    nd a chebyshev center by solving the problem

maximize r
subject to

gi(x, r)     0,

i = 1, . . . , m,

(8.16)

where gi is de   ned as

gi(x, r) = sup
kuk   1

fi(x + ru).

problem (8.16) is a id76 problem, since each function gi is the
pointwise maximum of a family of convex functions of x and r, hence convex.
however, evaluating gi involves solving a convex maximization problem (either
numerically or analytically), which may be very hard. in practice, we can    nd the
chebyshev center only in cases where the functions gi are easy to evaluate.

chebyshev center of a polyhedron

suppose c is de   ned by a set of linear inequalities at
have

i x     bi, i = 1, . . . , m. we

gi(x, r) = sup
kuk   1

i (x + ru)     bi = at
at

i x + rkaik        bi

418

8 geometric problems

if r     0, so the chebyshev center can be found by solving the lp

maximize r
subject to at

i x + rkaik        bi,
r     0

i = 1, . . . , m

with variables x and r.

euclidean chebyshev center of intersection of ellipsoids

let c be an intersection of m ellipsoids, de   ned by quadratic inequalities,

c = {x | xt aix + 2bt

i x + ci     0, i = 1, . . . , m},

++. we have

where ai     sn
gi(x, r) =

sup

kuk2   1(cid:0)(x + ru)t ai(x + ru) + 2bt

i (x + ru) + ci(cid:1)

= xt aix + 2bt

i x + ci + sup

kuk2   1(cid:0)r2ut aiu + 2r(aix + bi)t u(cid:1) .

from   b.1, gi(x, r)     0 if and only if there exists a   i such that the matrix
inequality

(cid:20)    xt aixi     2bt

r(aix + bi)

i x     ci       i r(aix + bi)t

  ii     r2ai (cid:21) (cid:23) 0

(8.17)

holds. using this result, we can express the chebyshev centering problem as

maximize r

subject to       

i a   1

i bi

     i     ci + bt
x + a   1
i bi

0

0
  ii
ri

i bi)t

(x + a   1
ri
a   1

i

       (cid:23) 0,

i = 1, . . . , m,

which is an sdp with variables r,   , and x. note that the schur complement of
a   1

in the lmi constraint is equal to the lefthand side of (8.17).

i

8.5.2 maximum volume ellipsoid center

the chebyshev center xcheb of a set c     rn is the center of the largest ball that
lies in c. as an extension of this idea, we de   ne the maximum volume ellipsoid
center of c, denoted xmve, as the center of the maximum volume ellipsoid that lies
in c. figure 8.6 shows an example, where c is a polyhedron.

the maximum volume ellipsoid center is readily computed when c is de   ned
by a set of linear inequalities, by solving the problem (8.15). (the optimal value
of the variable d     rn is xmve.) since the maximum volume ellipsoid inside c is
a   ne invariant, so is the maximum volume ellipsoid center.

8.5 centering

419

xmve

figure 8.6 the lightly shaded ellipsoid shows the maximum volume ellipsoid
contained in the set c, which is the same polyhedron as in    gure 8.5. its
center xmve is the maximum volume ellipsoid center of c.

8.5.3 analytic center of a set of inequalities

the analytic center xac of a set of convex inequalities and linear equalities,

fi(x)     0,

i = 1, . . . , m,

f x = g

is de   ned as an optimal point for the (convex) problem

minimize    pm

subject to f x = g,

i=1 log(   fi(x))

(8.18)

with variable x     rn and implicit constraints fi(x) < 0, i = 1, . . . , m. the objec-
tive in (8.18) is called the logarithmic barrier associated with the set of inequalities.
we assume here that the domain of the logarithmic barrier intersects the a   ne set
de   ned by the equalities, i.e., the strict inequality system

fi(x) < 0,

i = 1, . . . , m,

f x = g

is feasible. the logarithmic barrier is bounded below on the feasible set

c = {x | fi(x) < 0, i = 1, . . . , m, f x = g},

if c is bounded.

when x is strictly feasible, i.e., f x = g and fi(x) < 0 for i = 1, . . . , m, we can
interpret    fi(x) as the margin or slack in the ith inequality. the analytic center
xac is the point that maximizes the product (or geometric mean) of these slacks or
margins, subject to the equality constraints f x = g, and the implicit constraints
fi(x) < 0.

the analytic center is not a function of the set c described by the inequalities
and equalities; two sets of inequalities and equalities can de   ne the same set, but
have di   erent analytic centers. still, it is not uncommon to informally use the

420

8 geometric problems

term    analytic center of a set c    to mean the analytic center of a particular set of
equalities and inequalities that de   ne it.

the analytic center is, however, independent of a   ne changes of coordinates.
it is also invariant under (positive) scalings of the inequality functions, and any
reparametrization of the equality constraints. in other words, if   f and   g are such
that   f x =   g if and only if f x = g, and   1, . . . ,   m > 0, then the analytic center of

  ifi(x)     0,

i = 1, . . . , m,

  f x =   g,

is the same as the analytic center of

(see exercise 8.17).

fi(x)     0,

i = 1, . . . , m,

f x = g

analytic center of a set of linear inequalities

the analytic center of a set of linear inequalities

is the solution of the unconstrained minimization problem

at
i x     bi,

i = 1, . . . , m,

(8.19)

minimize    pm

i=1 log(bi     at

i x),

with implicit constraint bi     at
i x > 0, i = 1, . . . , m. if the polyhedron de   ned by
the linear inequalities is bounded, then the logarithmic barrier is bounded below
and strictly convex, so the analytic center is unique. (see exercise 4.2.)

we can give a geometric interpretation of the analytic center of a set of linear
inequalities. since the analytic center is independent of positive scaling of the
constraint functions, we can assume without loss of generality that kaik2 = 1. in
this case, the slack bi     at
i x =
bi}. therefore the analytic center xac is the point that maximizes the product of
distances to the de   ning hyperplanes.

i x is the distance to the hyperplane hi = {x | at

inner and outer ellipsoids from analytic center of linear inequalities

the analytic center of a set of linear inequalities implicitly de   nes an inscribed and
a covering ellipsoid, de   ned by the hessian of the logarithmic barrier function

   

mxi=1

evaluated at the analytic center, i.e.,

log(bi     at

i x),

h =

i aiat
d2
i ,

mxi=1
we have einner     p     eouter, where
p = {x | at

di =

1
bi     at

i xac

,

i = 1, . . . , m.

i x     bi, i = 1, . . . , m},

einner = {x | (x     xac)t h(x     xac)     1},
eouter = {x | x     xac)t h(x     xac)     m(m     1)}.

8.5 centering

421

xac

figure 8.7 the dashed lines show    ve level curves of the logarithmic barrier
function for the inequalities de   ning the polyhedron c in    gure 8.5. the
minimizer of the logarithmic barrier function, labeled xac, is the analytic
center of the inequalities. the inner ellipsoid einner = {x | (x     xac)h(x    
xac)     1}, where h is the hessian of the logarithmic barrier function at xac,
is shaded.

this is a weaker result than the one for the maximum volume inscribed ellipsoid,
which when scaled up by a factor of n covers the polyhedron. the inner and outer
ellipsoids de   ned by the hessian of the logarithmic barrier, in contrast, are related
by the scale factor (m(m     1))1/2, which is always at least n.

to show that einner     p, suppose x     einner, i.e.,

(x     xac)t h(x     xac) =

mxi=1

(diat

i (x     xac))2     1.

this implies that

at
i (x     xac)     1/di = bi     at

i xac,

i = 1, . . . , m,

and therefore at
i x     bi for i = 1, . . . , m. (we have not used the fact that xac is
the analytic center, so this result is valid if we replace xac with any strictly feasible
point.)

to establish that p     eouter, we will need the fact that xac is the analytic

center, and therefore the gradient of the logarithmic barrier vanishes:

diai = 0.

mxi=1

now assume x     p. then

(x     xac)t h(x     xac)

=

(diat

i (x     xac))2

mxi=1

422

8 geometric problems

i (1/di     at
d2

i (x     xac))2     m

=

=

mxi=1
mxi=1
      mxi=1
=   mxi=1

= m2     m,

d2
i (bi     at

i x)2     m
i x)!2

di(bi     at

    m

di(bi     at

i xac) +

i (xac     x)!2

diat

    m

mxi=1

(the second equality follows from the fact that

which shows that x     eouter.

pm
i=1 diai = 0. the inequality follows frompm
last equality follows frompm

analytic center of a linear matrix inequality

i=1 y2

i     (pm

i=1 diai = 0, and the de   nition of di.)

i=1 yi)2 for y (cid:23) 0. the

the de   nition of analytic center can be extended to sets described by generalized
inequalities with respect to a cone k, if we de   ne a logarithm on k. for example,
the analytic center of a linear matrix inequality

x1a1 + x2a2 +        + xnan (cid:22) b

is de   ned as the solution of

minimize     log det(b     x1a1                xnan).

8.6 classi   cation

in pattern recognition and classi   cation problems we are given two sets of points
in rn, {x1, . . . , xn} and {y1, . . . , ym}, and wish to    nd a function f : rn     r
(within a given family of functions) that is positive on the    rst set and negative on
the second, i.e.,

f (xi) > 0,

i = 1, . . . , n,

f (yi) < 0,

i = 1, . . . , m.

if these inequalities hold, we say that f , or its 0-level set {x | f (x) = 0}, separates,
classi   es, or discriminates the two sets of points. we sometimes also consider weak
separation, in which the weak versions of the inequalities hold.

8.6 classi   cation

423

figure 8.8 the points x1, . . . , xn are shown as open circles, and the points
y1, . . . , ym are shown as    lled circles. these two sets are classi   ed by an
a   ne function f , whose 0-level set (a line) separates them.

8.6.1 linear discrimination

in linear discrimination, we seek an a   ne function f (x) = at x     b that classi   es
the points, i.e.,

at xi     b > 0,

i = 1, . . . , n,

at yi     b < 0,

i = 1, . . . , m.

(8.20)

geometrically, we seek a hyperplane that separates the two sets of points. since
the strict inequalities (8.20) are homogeneous in a and b, they are feasible if and
only if the set of nonstrict linear inequalities

at xi     b     1,

i = 1, . . . , n,

at yi     b        1,

i = 1, . . . , m

(8.21)

(in the variables a, b) is feasible. figure 8.8 shows a simple example of two sets of
points and a linear discriminating function.

linear discrimination alternative

the strong alternative of the set of strict inequalities (8.20) is the existence of   ,
     such that

   (cid:23) 0,

     (cid:23) 0,

(  ,     ) 6= 0,

  ixi =

nxi=1

mxi=1

    iyi,

1t    = 1t      (8.22)

(see   5.8.3). using the third and last conditions, we can express these alternative
conditions as

   (cid:23) 0,

1t    = 1,

     (cid:23) 0,

1t      = 1,

  ixi =

nxi=1

    iyi

mxi=1

424

8 geometric problems

(by dividing by 1t   , which is positive, and using the same symbols for the normal-
ized    and     ). these conditions have a simple geometric interpretation: they state
that there is a point in the convex hull of both {x1, . . . , xn} and {y1, . . . , ym}. in
other words: the two sets of points can be linearly discriminated (i.e., discrimi-
nated by an a   ne function) if and only if their convex hulls do not intersect. we
have seen this result several times before.

robust linear discrimination
the existence of an a   ne classifying function f (x) = at x     b is equivalent to a
set of linear inequalities in the variables a and b that de   ne f .
if the two sets
can be linearly discriminated, then there is a polyhedron of a   ne functions that
discriminate them, and we can choose one that optimizes some measure of robust-
ness. we might, for example, seek the function that gives the maximum possible
   gap    between the (positive) values at the points xi and the (negative) values at the
points yi. to do this we have to normalize a and b, since otherwise we can scale a
and b by a positive constant and make the gap in the values arbitrarily large. this
leads to the problem

t

maximize
subject to at xi     b     t,
at yi     b        t,
kak2     1,

i = 1, . . . , n

i = 1, . . . , m

(8.23)

with variables a, b, and t. the optimal value t    of this convex problem (with
linear objective, linear inequalities, and one quadratic inequality) is positive if
and only if the two sets of points can be linearly discriminated. in this case the
inequality kak2     1 is always tight at the optimum, i.e., we have ka   k2 = 1. (see
exercise 8.23.)
we can give a simple geometric interpretation of the robust linear discrimination
problem (8.23). if kak2 = 1 (as is the case at any optimal point), at xi     b is the
euclidean distance from the point xi to the separating hyperplane h = {z | at z =
b}. similarly, b   at yi is the distance from the point yi to the hyperplane. therefore
the problem (8.23)    nds the hyperplane that separates the two sets of points, and
has maximal distance to the sets. in other words, it    nds the thickest slab that
separates the two sets.

as suggested by the example shown in    gure 8.9, the optimal value t    (which is
half the slab thickness) turns out to be half the distance between the convex hulls
of the two sets of points. this can be seen clearly from the dual of the robust linear
discrimination problem (8.23). the lagrangian (for the problem of minimizing    t)
is

   t +

ui(t + b     at xi) +

vi(t     b + at yi) +   (kak2     1).

nxi=1

mxi=1

minimizing over b and t yields the conditions 1t u = 1/2, 1t v = 1/2. when these
hold, we have

g(u, v,   ) = inf

a  at (

mxi=1

viyi    

nxi=1

uixi) +   kak2       !

8.6 classi   cation

425

figure 8.9 by solving the robust linear discrimination problem (8.23) we
   nd an a   ne function that gives the largest gap in values between the two
sets (with a id172 bound on the linear part of the function). ge-
ometrically, we are    nding the thickest slab that separates the two sets of
points.

the dual problem can then be written as

       otherwise.

(cid:13)(cid:13)(cid:13)pm
= (      
i=1 viyi    pn
maximize    (cid:13)(cid:13)(cid:13)pm
i=1 viyi    pn

subject to u (cid:23) 0,
v (cid:23) 0,

1t u = 1/2
1t v = 1/2.

i=1 uixi(cid:13)(cid:13)(cid:13)2       
i=1 uixi(cid:13)(cid:13)(cid:13)2

we can interpret 2pn
2pm

i=1 uixi as a point in the convex hull of {x1, . . . , xn} and
i=1 viyi as a point in the convex hull of {y1, . . . , ym}. the dual objective is to
minimize (half) the distance between these two points, i.e.,    nd (half) the distance
between the convex hulls of the two sets.

support vector classi   er

when the two sets of points cannot be linearly separated, we might seek an a   ne
function that approximately classi   es the points, for example, one that minimizes
the number of points misclassi   ed. unfortunately, this is in general a di   cult
combinatorial optimization problem. one heuristic for approximate linear discrim-
ination is based on support vector classi   ers, which we describe in this section.

we start with the feasibility problem (8.21). we    rst relax the constraints
by introducing nonnegative variables u1, . . . , un and v1, . . . , um , and forming the
inequalities
at xi     b     1    ui,

at yi     b        (1    vi),

i = 1, . . . , m. (8.24)

i = 1, . . . , n,

426

8 geometric problems

figure 8.10 approximate linear discrimination via id135. the
points x1, . . . , x50, shown as open circles, cannot be linearly separated from
the points y1, . . . , y50, shown as    lled circles. the classi   er shown as a solid
line was obtained by solving the lp (8.25). this classi   er misclassi   es one
point. the dashed lines are the hyperplanes at z     b =   1. four points are
correctly classi   ed, but lie in the slab de   ned by the dashed lines.

when u = v = 0, we recover the original constraints; by making u and v large
enough, these inequalities can always be made feasible. we can think of ui as
a measure of how much the constraint at xi     b     1 is violated, and similarly
for vi. our goal is to    nd a, b, and sparse nonnegative u and v that satisfy the
inequalities (8.24). as a heuristic for this, we can minimize the sum of the variables
ui and vi, by solving the lp

1t u + 1t v

minimize
subject to at xi     b     1     ui,

at yi     b        (1     vi),
u (cid:23) 0,

v (cid:23) 0.

i = 1, . . . , n

i = 1, . . . , m

(8.25)

figure 8.10 shows an example. in this example, the a   ne function at z     b mis-
classi   es 1 out of 100 points. note however that when 0 < ui < 1, the point xi
is correctly classi   ed by the a   ne function at z     b, but violates the inequality
at xi     b     1, and similarly for yi. the objective function in the lp (8.25) can be
interpreted as a relaxation of the number of points xi that violate at xi    b     1 plus
the number of points yi that violate at yi   b        1. in other words, it is a relaxation
of the number of points misclassi   ed by the function at z     b, plus the number of
points that are correctly classi   ed but lie in the slab de   ned by    1 < at z     b < 1.
more generally, we can consider the trade-o    between the number of misclas-
si   ed points, and the width of the slab {z |     1     at z     b     1}, which is
given by 2/kak2. the standard support vector classi   er for the sets {x1, . . . , xn},

8.6 classi   cation

427

figure 8.11 approximate linear discrimination via support vector classi   er,
with    = 0.1. the support vector classi   er, shown as the solid line, misclas-
si   es three points. fifteen points are correctly classi   ed but lie in the slab
de   ned by    1 < at z     b < 1, bounded by the dashed lines.

{y1, . . . , ym} is de   ned as the solution of

minimize
subject to at xi     b     1     ui,

kak2 +   (1t u + 1t v)
at yi     b        (1     vi),
u (cid:23) 0,

v (cid:23) 0,

i = 1, . . . , n

i = 1, . . . , m

the    rst term is proportional to the inverse of the width of the slab de   ned by
   1     at z     b     1. the second term has the same interpretation as above, i.e., it
is a convex relaxation for the number of misclassi   ed points (including the points
in the slab). the parameter   , which is positive, gives the relative weight of the
number of misclassi   ed points (which we want to minimize), compared to the width
of the slab (which we want to maximize). figure 8.11 shows an example.

approximate linear discrimination via logistic modeling

another approach to    nding an a   ne function that approximately classi   es two
sets of points that cannot be linearly separated is based on the logistic model
described in   7.1.1. we start by    tting the two sets of points with a logistic model.
suppose z is a random variable with values 0 or 1, with a distribution that depends
on some (deterministic) explanatory variable u     rn, via a logistic model of the
form

prob(z = 1) = (exp(at u     b))/(1 + exp(at u     b))
prob(z = 0) = 1/(1 + exp(at u     b)).

(8.26)

now we assume that the given sets of points, {x1, . . . , xn} and {y1, . . . , ym},
arise as samples from the logistic model. speci   cally, {x1, . . . , xn} are the values

428

8 geometric problems

of u for the n samples for which z = 1, and {y1, . . . , ym} are the values of u for
the m samples for which z = 0. (this allows us to have xi = yj, which would rule
out discrimination between the two sets. in a logistic model, it simply means that
we have two samples, with the same value of explanatory variable but di   erent
outcomes.)

we can determine a and b by id113 from the observed

samples, by solving the id76 problem

with variables a, b, where l is the log-likelihood function

minimize    l(a, b)

(8.27)

l(a, b) =pn
   pn

i=1(at xi     b)

i=1 log(1 + exp(at xi     b))    pm

i=1 log(1 + exp(at yi     b))

(see   7.1.1). if the two sets of points can be linearly separated, i.e., if there exist a,
b with at xi > b and at yi < b, then the optimization problem (8.27) is unbounded
below.

once we    nd the maximum likelihood values of a and b, we can form a linear
classi   er f (x) = at x    b for the two sets of points. this classi   er has the following
property: assuming the data points are in fact generated from a logistic model
with parameters a and b, it has the smallest id203 of misclassi   cation, over
all linear classi   ers. the hyperplane at u = b corresponds to the points where
prob(z = 1) = 1/2, i.e., the two outcomes are equally likely. an example is shown
in    gure 8.12.

remark 8.1 bayesian interpretation. let x and z be two random variables, taking
values in rn and in {0, 1}, respectively. we assume that

prob(z = 1) = prob(z = 0) = 1/2,

and we denote by p0(x) and p1(x) the id155 densities of x, given
z = 0 and given z = 1, respectively. we assume that p0 and p1 satisfy

p1(x)
p0(x)

= eat x   b

for some a and b. many common distributions satisfy this property. for example,
p0 and p1 could be two normal densities on rn with equal covariance matrices and
di   erent means, or they could be two exponential densities on rn
+.

it follows from bayes    rule that

prob(z = 1 | x = u) =

prob(z = 0 | x = u) =

p1(u)

p1(u) + p0(u)

p0(u)

p1(u) + p0(u)

,

from which we obtain

prob(z = 1 | x = u) =

prob(z = 0 | x = u) =

exp(at u     b)
1 + exp(at u     b)

1

1 + exp(at u     b)

.

8.6 classi   cation

429

figure 8.12 approximate linear discrimination via logistic modeling. the
points x1, . . . , x50, shown as open circles, cannot be linearly separated from
the points y1, . . . , y50, shown as    lled circles. the maximum likelihood lo-
gistic model yields the hyperplane shown as a dark line, which misclassi   es
only two points. the two dashed lines show at u    b =   1, where the proba-
bility of each outcome, according to the logistic model, is 73%. three points
are correctly classi   ed, but lie in between the dashed lines.

the logistic model (8.26) can therefore be interpreted as the posterior distribution of
z, given that x = u.

8.6.2 nonlinear discrimination

we can just as well seek a nonlinear function f , from a given subspace of functions,
that is positive on one set and negative on another:

f (xi) > 0,

i = 1, . . . , n,

f (yi) < 0,

i = 1, . . . , m.

provided f is linear (or a   ne) in the parameters that de   ne it, these inequalities
can be solved in exactly the same way as in linear discrimination. in this section
we examine some interesting special cases.

quadratic discrimination

suppose we take f to be quadratic: f (x) = xt p x + qt x + r. the parameters
p     sn, q     rn, r     r must satisfy the inequalities

xt
i p xi + qt xi + r > 0,
yt
i p yi + qt yi + r < 0,

i = 1, . . . , n
i = 1, . . . , m,

430

8 geometric problems

which is a set of strict linear inequalities in the variables p , q, r. as in linear
discrimination, we note that f is homogeneous in p , q, and r, so we can    nd a
solution to the strict inequalities by solving the nonstrict feasibility problem

xt
i p xi + qt xi + r     1,
yt
i p yi + qt yi + r        1,

i = 1, . . . , n

i = 1, . . . , m.

the separating surface {z | zt p z + qt z + r = 0} is a quadratic surface, and

the two classi   cation regions

{z | zt p z + qt z + r     0},

{z | zt p z + qt z + r     0},

are de   ned by quadratic inequalities. solving the quadratic discrimination problem,
then, is the same as determining whether the two sets of points can be separated
by a quadratic surface.

we can impose conditions on the shape of the separating surface or classi   cation
regions by adding constraints on p , q, and r. for example, we can require that
p     0, which means the separating surface is ellipsoidal. more speci   cally, it means
that we seek an ellipsoid that contains all the points x1, . . . , xn , but none of the
points y1, . . . , ym . this quadratic discrimination problem can be solved as an sdp
feasibility problem

p, q, r
   nd
subject to xt
i p xi + qt xi + r     1,
i p yi + qt yi + r        1,
yt
p (cid:22)    i,

i = 1, . . . , n

i = 1, . . . , m

with variables p     sn, q     rn, and r     r. (here we use homogeneity in p , q, r
to express the constraint p     0 as p (cid:22)    i.) figure 8.13 shows an example.
polynomial discrimination

we consider the set of polynomials on rn with degree less than or equal to d:

f (x) = xi1+      +in   d

ai1      id xi1

1        xin
n .

we can determine whether or not two sets {x1, . . . , xn} and {y1, . . . , ym} can be
separated by such a polynomial by solving a set of linear inequalities in the variables
ai1      id . geometrically, we are checking whether the two sets can be separated by
an algebraic surface (de   ned by a polynomial of degree less than or equal to d).

as an extension, the problem of determining the minimum degree polynomial on
rn that separates two sets of points can be solved via quasiconvex programming,
since the degree of a polynomial is a quasiconvex function of the coe   cients. this
can be carried out by bisection on d, solving a feasibility linear program at each
step. an example is shown in    gure 8.14.

8.6 classi   cation

431

figure 8.13 quadratic discrimination, with the condition that p     0. this
means that we seek an ellipsoid containing all of xi (shown as open circles)
and none of the yi (shown as    lled circles). this can be solved as an sdp
feasibility problem.

figure 8.14 minimum degree polynomial discrimination in r2. in this ex-
ample, there exists no cubic polynomial that separates the points x1, . . . , xn
(shown as open circles) from the points y1, . . . , ym (shown as    lled circles),
but they can be separated by fourth-degree polynomial, the zero level set of
which is shown.

432

8 geometric problems

8.7 placement and location

in this section we discuss a few variations on the following problem. we have
n points in r2 or r3, and a list of pairs of points that must be connected by
links. the positions of some of the n points are    xed; our task is to determine the
positions of the remaining points, i.e., to place the remaining points. the objective
is to place the points so that some measure of the total interconnection length of
the links is minimized, subject to some additional constraints on the positions.
as an example application, we can think of the points as locations of plants or
warehouses of a company, and the links as the routes over which goods must be
shipped. the goal is to    nd locations that minimize the total transportation cost.
in another application, the points represent the position of modules or cells on an
integrated circuit, and the links represent wires that connect pairs of cells. here
the goal might be to place the cells in such a way that the total length of wire used
to interconnect the cells is minimized.

the problem can be described in terms of an undirected graph with n nodes,
representing the n points. with each node we associate a variable xi     rk, where
k = 2 or k = 3, which represents its location or position. the problem is to
minimize

x(i,j)   a

fij(xi, xj)

where a is the set of all links in the graph, and fij : rk    rk     r is a cost
function associated with arc (i, j). (alternatively, we can sum over all i and j, or
over i < j, and simply set fij = 0 when links i and j are not connected.) some of
the coordinate vectors xi are given. the optimization variables are the remaining
coordinates. provided the functions fij are convex, this is a id76
problem.

8.7.1 linear facility location problems

in the simplest version of the problem the cost associated with arc (i, j) is the
distance between nodes i and j: fij(xi, xj) = kxi     xjk, i.e., we minimize

x(i,j)   a

kxi     xjk.

we can use any norm, but the most common applications involve the euclidean
norm or the    1-norm. for example, in circuit design it is common to route the wires
between cells along piecewise-linear paths, with each segment either horizontal or
vertical. (this is called manhattan routing, since paths along the streets in a city
with a rectangular grid are also piecewise-linear, with each street aligned with one
of two orthogonal axes.) in this case, the length of wire required to connect cell i
and cell j is given by kxi     xjk1.

we can include nonnegative weights that re   ect di   erences in the cost per unit

8.7 placement and location

433

distance along di   erent arcs:

x(i,j)   a

wijkxi     xjk.

by assigning a weight wij = 0 to pairs of nodes that are not connected, we can
express this problem more simply using the objective

xi<j

wijkxi     xjk.

(8.28)

this placement problem is convex.

example 8.4 one free point. consider the case where only one point (u, v)     r2 is
free, and we minimize the sum of the distances to    xed points (u1, v1), . . . , (uk , vk ).

       1-norm. we can    nd a point that minimizes

kxi=1

(|u     ui| + |v     vi|)

analytically. an optimal point is any median of the    xed points. in other words,
u can be taken to be any median of the points {u1, . . . , uk}, and v can be taken
to be any median of the points {v1, . . . , vk}. (if k is odd, the minimizer is
unique; if k is even, there can be a rectangle of optimal points.)

    euclidean norm. the point (u, v) that minimizes the sum of the euclidean

distances,

kxi=1(cid:0)(u     ui)2 + (v     vi)2(cid:1)1/2

is called the weber point of the given    xed points.

,

8.7.2 placement constraints

we now list some interesting constraints that can be added to the basic placement
problem, preserving convexity. we can require some positions xi to lie in a speci   ed
convex set, e.g., a particular line, interval, square, or ellipsoid. we can constrain
the relative position of one point with respect to one or more other points, for
example, by limiting the distance between a pair of points. we can impose relative
position constraints, e.g., that one point must lie to the left of another point.

the bounding box of a group of points is the smallest rectangle that contains
the points. we can impose a constraint that limits the points x1, . . . , xp (say) to lie
in a bounding box with perimeter not exceeding pmax, by adding the constraints

u (cid:22) xi (cid:22) v,

i = 1, . . . , p,

21t (v     u)     pmax,

where u, v are additional variables.

434

8 geometric problems

8.7.3 nonlinear facility location problems

more generally, we can associate a cost with each arc that is a nonlinear increasing
function of the length, i.e.,

minimize pi<j wijh(kxi     xjk)

where h is an increasing (on r+) and convex function, and wij     0. we call this
a nonlinear placement or nonlinear facility location problem.
one common example uses the euclidean norm, and the function h(z) = z2,

i.e., we minimize

xi<j

wijkxi     xjk2
2.

this is called a quadratic placement problem. the quadratic placement problem
can be solved analytically when the only constraints are linear equalities; it can be
solved as a qp if the constraints are linear equalities and inequalities.

example 8.5 one free point. consider the case where only one point x is free, and we
minimize the sum of the squares of the euclidean distances to    xed points x1, . . . , xk ,

kx     x1k2

2 + kx     x2k2

2 +        + kx     xkk2
2.

taking derivatives, we see that the optimal x is given by

1
k

(x1 + x2 +        + xk ),

i.e., the average of the    xed points.

some other interesting possibilities are the    deadzone    function h with deadzone

width 2  , de   ned as

and the    quadratic-linear    function h, de   ned as

h(z) =(cid:26) 0
h(z) =(cid:26) z2

|z       |

|z|       
|z|       ,

2  |z|       2

|z|       
|z|       .

example 8.6 we consider a placement problem in r2 with 6 free points, 8    xed
points, and 27 links. figures 8.15   8.17 show the optimal solutions for the criteria

x(i,j)   a

kxi     xjk2, x(i,j)   a

kxi     xjk2

2, x(i,j)   a

kxi     xjk4
2,

i.e., using the penalty functions h(z) = z, h(z) = z2, and h(z) = z4. the    gures also
show the resulting distributions of the link lengths.

comparing the results, we see that the linear placement concentrates the free points in
a small area, while the quadratic and fourth-order placements spread the points over
larger areas. the linear placement includes many very short links, and a few very long
ones (3 lengths under 0.2 and 2 lengths above 1.5.). the quadratic penalty function

8.7 placement and location

435

1

0

   1

1

0

   1

4

3

2

1

0
0

1

0

   1
figure 8.15 linear placement. placement problem with 6 free points (shown
as dots), 8    xed points (shown as squares), and 27 links. the coordinates of
the free points minimize the sum of the euclidean lengths of the links. the
right plot is the distribution of the 27 link lengths. the dashed curve is the
(scaled) penalty function h(z) = z.

0.5

1.5

1

4

3

2

1

0

   1
figure 8.16 quadratic placement. placement that minimizes the sum of
squares of the euclidean lengths of the links, for the same data as in    g-
ure 8.15. the dashed curve is the (scaled) penalty function h(z) = z2.

0.5

1

1

0
0

2

1.5

436

8 geometric problems

1

0

   1

6

5

4

3

2

1

0

   1
figure 8.17 fourth-order placement. placement that minimizes the sum of
the fourth powers of the euclidean lengths of the links. the dashed curve
is the (scaled) penalty function h(z) = z4.

0.5

1

1

0
0

1.5

puts a higher penalty on long lengths relative to short lengths, and for lengths under
0.1, the penalty is almost negligible. as a result, the maximum length is shorter (less
than 1.4), but we also have fewer short links. the fourth-order function puts an even
higher penalty on long lengths, and has a wider interval (between zero and about
0.4) where it is negligible. as a result, the maximum length is shorter than for the
quadratic placement, but we also have more lengths close to the maximum.

8.7.4 location problems with path constraints

path constraints

a p-link path along the points x1, . . . , xn is described by a sequence of nodes,
i0, . . . , ip     {1, . . . , n}. the length of the path is given by

kxi1     xi0k + kxi2     xi1k +        + kxip     xip   1k,

which is a convex function of x1, . . . , xn , so imposing an upper bound on the length
of a path is a convex constraint. several interesting placement problems involve
path constraints, or have an objective based on path lengths. we describe one
typical example, in which the objective is based on a maximum path length over a
set of paths.

minimax delay placement

we consider a directed acyclic graph with nodes 1, . . . , n , and arcs or links repre-
sented by a set a of ordered pairs: (i, j)     a if and only if an arc points from i
to j. we say node i is a source node if no arc a points to it; it is a sink node or
destination node if no arc in a leaves from it. we will be interested in the maximal
paths in the graph, which begin at a source node and end at a sink node.
the arcs of the graph are meant to model some kind of    ow, say of goods or
information, in a network with nodes at positions x1, . . . , xn . the    ow starts at

8.7 placement and location

437

a source node, then moves along a path from node to node, ending at a sink or
destination node. we use the distance between successive nodes to model prop-
agation time, or shipment time, of the goods between nodes; the total delay or
propagation time of a path is (proportional to) the sum of the distances between
successive nodes.

now we can describe the minimax delay placement problem. some of the node
locations are    xed, and the others are free, i.e., optimization variables. the goal
is to choose the free node locations in order to minimize the maximum total delay,
for any path from a source node to a sink node. evidently this is a convex problem,
since the objective

tmax = max{kxi1     xi0k +        + kxip     xip   1k | i0, . . . , ip is a source-sink path}(8.29)

is a convex function of the locations x1, . . . , xn .

while the problem of minimizing (8.29) is convex, the number of source-sink
paths can be very large, exponential in the number of nodes or arcs. there is
a useful reformulation of the problem, which avoids enumerating all sink-source
paths.

we    rst explain how we can evaluate the maximum delay tmax far more ef-
   ciently than by evaluating the delay for every source-sink path, and taking the
maximum. let   k be the maximum total delay of any path from node k to a sink
node. clearly we have   k = 0 when k is a sink node. consider a node k, which has
outgoing arcs to nodes j1, . . . , jp. for a path starting at node k and ending at a
sink node, its    rst arc must lead to one of the nodes j1, . . . , jp. if such a path    rst
takes the arc leading to ji, and then takes the longest path from there to a sink
node, the total length is

kxji     xkk +   ji ,

i.e., the length of the arc to ji, plus the total length of the longest path from ji to
a sink node. it follows that the maximum delay of a path starting at node k and
leading to a sink node satis   es

  k = max{kxj1     xkk +   j1 , . . . ,kxjp     xkk +   jp}.

(8.30)

(this is a simple id145 argument.)

the equations (8.30) give a recursion for    nding the maximum delay from any
node: we start at the sink nodes (which have maximum delay zero), and then
work backward using the equations (8.30), until we reach all source nodes. the
maximum delay over any such path is then the maximum of all the   k, which will
occur at one of the source nodes. this id145 recursion shows
how the maximum delay along any source-sink path can be computed recursively,
without enumerating all the paths. the number of arithmetic operations required
for this recursion is approximately the number of links.

now we show how the recursion based on (8.30) can be used to formulate the

minimax delay placement problem. we can express the problem as

minimize max{  k | k a source node}
subject to

k a sink node

  k = 0,
  k = max{kxj     xkk +   j | there is an arc from k to j},

438

8 geometric problems

with variables   1, . . . ,   n and the free positions. this problem is not convex, but
we can express it in an equivalent form that is convex, by replacing the equality
constraints with inequalities. we introduce new variables t1, . . . , tn , which will be
upper bounds on   1, . . . ,   n , respectively. we will take tk = 0 for all sink nodes,
and in place of (8.30) we take the inequalities

tk     max{kxj1     xkk + tj1 , . . . ,kxjp     xkk + tjp}.

if these inequalities are satis   ed, then tk       k. now we form the problem

minimize max{tk | k a source node}
subject to tk = 0,

k a sink node

tk     max{kxj     xkk + tj | there is an arc from k to j}.

this problem, with variables t1, . . . , tn and the free locations, is convex, and solves
the minimax delay location problem.

8.8 floor planning

in placement problems, the variables represent the coordinates of a number of
points that are to be optimally placed. a    oor planning problem can be considered
an extension of a placement problem in two ways:

    the objects to be placed are rectangles or boxes aligned with the axes (as

opposed to points), and must not overlap.

    each rectangle or box to be placed can be recon   gured, within some limits.
for example we might    x the area of each rectangle, but not the length and
height separately.

the objective is usually to minimize the size (e.g., area, volume, perimeter) of the
bounding box, which is the smallest box that contains the boxes to be con   gured
and placed.

the non-overlap constraints make the general    oor planning problem a compli-
cated combinatorial optimization problem or rectangle packing problem. however,
if the relative positioning of the boxes is speci   ed, several types of    oor planning
problems can be formulated as id76 problems. we explore some
of these in this section. we consider the two-dimensional case, and make a few
comments on extensions to higher dimensions (when they are not obvious).

we have n cells or modules c1, . . . , cn that are to be con   gured and placed
in a rectangle with width w and height h, and lower left corner at the position
(0, 0). the geometry and position of the ith cell is speci   ed by its width wi and
height hi, and the coordinates (xi, yi) of its lower left corner. this is illustrated in
   gure 8.18.

the variables in the problem are xi, yi, wi, hi for i = 1, . . . , n , and the width
in all    oor planning problems, we

w and height h of the bounding rectangle.
require that the cells lie inside the bounding rectangle, i.e.,

xi     0,

yi     0,

xi + wi     w,

yi + hi     h,

i = 1, . . . , n.

(8.31)

8.8 floor planning

439

wi

ci

(xi, yi)

hi

h

w

figure 8.18 floor planning problem. non-overlapping rectangular cells are
placed in a rectangle with width w , height h, and lower left corner at (0, 0).
the ith cell is speci   ed by its width wi, height hi, and the coordinates of its
lower left corner, (xi, yi).

we also require that the cells do not overlap, except possibly on their boundaries:

int (ci     cj) =     for i 6= j.

(it is also possible to require a positive minimum clearance between the cells.) the
non-overlap constraint int(ci     cj) =     holds if and only if for i 6= j,

ci is left of cj, or ci is right of cj, or ci is below cj, or ci is above cj.

these four geometric conditions correspond to the inequalities

xi + wi     xj, or xj + wj     xi, or yi + hj     yj, or yj + hi     yi,

(8.32)

at least one of which must hold for each i 6= j. note the combinatorial nature of
these constraints: for each pair i 6= j, at least one of the four inequalities above
must hold.

8.8.1 relative positioning constraints

the idea of relative positioning constraints is to specify, for each pair of cells,
one of the four possible relative positioning conditions, i.e., left, right, above, or
below. one simple method to specify these constraints is to give two relations on
{1, . . . , n}: l (meaning    left of   ) and b (meaning    below   ). we then impose the
constraint that ci is to the left of cj if (i, j)     l, and ci is below cj if (i, j)     b.
this yields the constraints

xi + wi     xj for (i, j)     l,

yi + hi     yj for (i, j)     b,

(8.33)

440

8 geometric problems

for i, j = 1, . . . , n . to ensure that the relations l and b specify the relative
positioning of each pair of cells, we require that for each (i, j) with i 6= j, one of
the following holds:

(i, j)     l,

(j, i)     l,

(i, j)     b,

(j, i)     b,

and that (i, i) 6    l, (i, i) 6    b. the inequalities (8.33) are a set of n (n     1)/2 linear
inequalities in the variables. these inequalities imply the non-overlap inequali-
ties (8.32), which are a set of n (n     1)/2 disjunctions of four linear inequalities.
we can assume that the relations l and b are anti-symmetric (i.e., (i, j)    
l     (j, i) 6    l) and transitive (i.e., (i, j)     l, (j, k)     l     (i, k)     l). (if this
were not the case, the relative positioning constraints would clearly be infeasible.)
transitivity corresponds to the obvious condition that if cell ci is to the left of cell
cj, which is to the left of cell ck, then cell ci must be to the left of cell ck. in
this case the inequality corresponding to (i, k)     l is redundant; it is implied by
the other two. by exploiting transitivity of the relations l and b we can remove
redundant constraints, and obtain a compact set of relative positioning inequalities.
a minimal set of relative positioning constraints is conveniently described using
two directed acyclic graphs h and v (for horizontal and vertical). both graphs have
n nodes, corresponding to the n cells in the    oor planning problem. the graph
h generates the relation l as follows: we have (i, j)     l if and only if there is
a (directed) path in h from i to j. similarly, the graph v generates the relation
b: (i, j)     b if and only if there is a (directed) path in v from i to j. to ensure
that a relative positioning constraint is given for every pair of cells, we require that
for every pair of cells, there is a directed path from one to the other in one of the
graphs.

evidently, we only need to impose the inequalities that correspond to the edges
of the graphs h and v; the others follow from transitivity. we arrive at the set of
inequalities

yi + hi     yj for (i, j)     v,

xi + wi     xj for (i, j)     h,

(8.34)
which is a set of linear inequalities, one for each edge in h and v. the set of
inequalities (8.34) is a subset of the set of inequalities (8.33), and equivalent.
in a similar way, the 4n inequalities (8.31) can be reduced to a minimal, equiv-
alent set. the constraint xi     0 only needs to be imposed on the left-most cells,
i.e., for i that are minimal in the relation l. these correspond to the sources in
the graph h, i.e., those nodes that have no edges pointing to them. similarly, the
inequalities xi + wi     w only need to be imposed for the right-most cells. in the
same way the vertical bounding box inequalities can be pruned to a minimal set.
this yields the minimal equivalent set of bounding box inequalities
xi + wi     w for i l maximal,
yi + hi     h for i b maximal.

xi     0 for i l minimal,
yi     0 for i b minimal,

(8.35)

a simple example is shown in    gure 8.19. in this example, the l minimal or
left-most cells are c1, c2, and c4, and the only right-most cell is c5. the minimal
set of inequalities specifying the horizontal relative positioning is given by
x1 + w1     x3,

x5 + w5     w,

x2     0,

x4     0,

x1     0,

x2 + w2     x3,

x3 + w3     x5,

x4 + w4     x5.

8.8 floor planning

441

1

2

2

h

v

3

4

1

3

5

4

5

4

1

2

5

3

figure 8.19 example illustrating the horizontal and vertical graphs h and
v that specify the relative positioning of the cells. if there is a path from
node i to node j in h, then cell i must be placed to the left of cell j. if there
is a path from node i to node j in v, then cell i must be placed below cell
j. the    oorplan shown at right satis   es the relative positioning speci   ed by
the two graphs.

the minimal set of inequalities specifying the vertical relative positioning is given
by

y2     0,

y3     0,

y2 + h2     y1,

y5     0,

y4 + h4     h,

y1 + h1     y4,

y3 + h3     y4.

y5 + h5     h,

8.8.2 floor planning via id76

in this formulation, the variables are the bounding box width and height w and
h, and the cell widths, heights, and positions: wi, hi, xi, and wi, for i = 1, . . . , n .
we impose the bounding box constraints (8.35) and the relative positioning con-
straints (8.34), which are linear inequalities. as objective, we take the perimeter
of the bounding box, i.e., 2(w + h), which is a linear function of the variables.
we now list some of the constraints that can be expressed as convex inequalities
or linear equalities in the variables.

minimum spacing

we can impose a minimum spacing    > 0 between cells by changing the relative
position constraints from xi + wi     xj for (i, j)     h, to xi + wi +        xj for
(i, j)     h, and similarly for the vertical graph. we can have a di   erent minimum
spacing associated with each edge in h and v. another possibility is to    x w and
h, and maximize the minimum spacing    as objective.

442

8 geometric problems

minimum cell area

for each cell we specify a minimum area, i.e., we require that wihi     ai, where
ai > 0. these minimum cell area constraints can be expressed as convex inequali-
ties in several ways, e.g., wi     ai/hi, (wihi)1/2     a1/2
, or log wi + log hi     log ai.

i

aspect ratio constraints

we can impose upper and lower bounds on the aspect ratio of each cell, i.e.,

li     hi/wi     ui.

multiplying through by wi transforms these constraints into linear inequalities. we
can also    x the aspect ratio of a cell, which results in a linear equality constraint.

alignment constraints

we can impose the constraint that two edges, or a center line, of two cells are
aligned. for example, the horizontal center line of cell i aligns with the top of cell
j when

yi + wi/2 = yj + wj.

these are linear equality constraints. in a similar way we can require that a cell is
   ushed against the bounding box boundary.

symmetry constraints

we can require pairs of cells to be symmetric about a vertical or horizontal axis,
that can be    xed or    oating (i.e., whose position is    xed or not). for example, to
specify that the pair of cells i and j are symmetric about the vertical axis x = xaxis,
we impose the linear equality constraint

xaxis     (xi + wi/2) = xj + wj/2     xaxis.

we can require that several pairs of cells be symmetric about an unspeci   ed vertical
axis by imposing these equality constraints, and introducing xaxis as a new variable.

similarity constraints

we can require that cell i be an a-scaled translate of cell j by the equality con-
straints wi = awj, hi = ahj. here the scaling factor a must be    xed. by imposing
only one of these constraints, we require that the width (or height) of one cell be
a given factor times the width (or height) of the other cell.

containment constraints

we can require that a particular cell contains a given point, which imposes two lin-
ear inequalities. we can require that a particular cell lie inside a given polyhedron,
again by imposing linear inequalities.

8.8 floor planning

distance constraints

443

we can impose a variety of constraints that limit the distance between pairs of
cells.
in the simplest case, we can limit the distance between the center points
of cell i and j (or any other    xed points on the cells, such as lower left corners).
for example, to limit the distance between the centers of cells i and j, we use the
(convex) inequality

k(xi + wi/2, yi + hi/2)     (xj + wj/2, yj + hj/2)k     dij.

as in placement problems, we can limit sums of distances, or use sums of distances
as the objective.

we can also limit the distance dist(ci, cj) between cell i and cell j, i.e., the
minimum distance between a point in cell i and a point in cell j. in the general
case this can be done as follows. to limit the distance between cells i and j in the
norm k    k, we can introduce four new variables ui, vi, uj, vj. the pair (ui, vi)
will represent a point in ci, and the pair (uj, vj) will represent a point in cj. to
ensure this we impose the linear inequalities

xi     ui     xi + wi,

yi     vi     yi + hi,

and similarly for cell j. finally, to limit dist(ci, cj), we add the convex inequality

k(ui, vi)     (uj, vj)k     dij.

in many speci   c cases we can express these distance constraints more e   ciently,
by exploiting the relative positioning constraints or deriving a more explicit formu-
lation. as an example consider the       -norm, and suppose cell i lies to the left of
cell j (by a relative positioning constraint). the horizontal displacement between
the two cells is xj     (xi + wi) then we have dist(ci, cj)     dij if and only if
yi     (yj + hj)     dij.

xj     (xi + wi)     dij,

yj     (yi + hi)     dij,

the    rst inequality states that the horizontal displacement between the right edge
of cell i and the left edge of cell j does not exceed dij. the second inequality
requires that the bottom of cell j is no more than dij above the top of cell i, and
the third inequality requires that the bottom of cell i is no more than dij above the
top of cell j. these three inequalities together are equivalent to dist(ci, cj)     dij.
in this case, we do not need to introduce any new variables.
we can limit the    1- (or    2-) distance between two cells in a similar way. here
we introduce one new variable dv, which will serve as a bound on the vertical
displacement between the cells. to limit the    1-distance, we add the constraints

yj     (yi + hi)     dv,

yi     (yj + hj)     dv,

dv     0

and the constraints

xj     (xi + wi) + dv     dij.

(the    rst term is the horizontal displacement and the second is an upper bound
on the vertical displacement.) to limit the euclidean distance between the cells,
we replace this last constraint with

(xj     (xi + wi))2 + d2

v     d2
ij.

444

8 geometric problems

4

4

1

2

1

2

5

3

5

3

4

1

2

3

5

4

1

2

5

3

figure 8.20 four instances of an optimal    oor plan, using the relative po-
sitioning constraints shown in    gure 8.19. in each case the objective is to
minimize the perimeter, and the same minimum spacing constraint between
cells is imposed. we also require the aspect ratios to lie between 1/5 and 5.
the four cases di   er in the minimum areas required for each cell. the sum
of the minimum areas is the same for each case.

example 8.7 figure 8.20 shows an example with 5 cells, using the ordering constraints
of    gure 8.19, and four di   erent sets of constraints.
in each case we impose the
same minimum required spacing constraint, and the same aspect ratio constraint
1/5     wi/hi     5. the four cases di   er in the minimum required cell areas ai. the
i=1 ai is the same
for each case.

values of ai are chosen so that the total minimum required areap5

8.8.3 floor planning via geometric programming

the    oor planning problem can also be formulated as a geometric program in the
variables xi, yi, wi, hi, w, h. the objectives and constraints that can be handled
in this formulation are a bit di   erent from those that can be expressed in the convex
formulation.

first we note that the bounding box constraints (8.35) and the relative po-

8.8 floor planning

445

sitioning constraints (8.34) are posynomial inequalities, since the lefthand sides
are sums of variables, and the righthand sides are single variables, hence monomi-
als. dividing these inequalities by the righthand side yields standard posynomial
inequalities.

in the geometric programming formulation we can minimize the bounding box
area, since w h is a monomial, hence posynomial. we can also exactly specify
the area of each cell, since wihi = ai is a monomial equality constraint. on the
other hand alignment, symmetry, and distance constraints cannot be handled in
the geometric programming formulation. similarity, however, can be; indeed it
is possible to require that one cell be similar to another, without specifying the
scaling ratio (which can be treated as just another variable).

446

8 geometric problems

bibliography

the characterization of euclidean distance matrices in   8.3.3 appears in schoenberg
[sch35]; see also gower [gow85].

our use of the term l  owner-john ellipsoid follows gr  otschel, lov  asz, and schrijver
[gls88, page 69]. the e   ciency results for ellipsoidal approximations in   8.4 were proved
by john [joh85]. boyd, el ghaoui, feron, and balakrishnan [befb94,   3.7] give con-
vex formulations of several ellipsoidal approximation problems involving sets de   ned as
unions, intersections or sums of ellipsoids.
the di   erent centers de   ned in   8.5 have applications in design centering (see, for exam-
ple, sei   , ponnambalan, and vlach [spv99]), and cutting-plane methods (elzinga and
moore [em75], tarasov, khachiyan, and `erlikh [tke88], and ye [ye97, chapter 8]). the
inner ellipsoid de   ned by the hessian of the logarithmic barrier function (page 420) is
sometimes called the dikin ellipsoid, and is the basis of dikin   s algorithm for linear and
quadratic programming [dik67]. the expression for the outer ellipsoid at the analytic
center was given by sonnevend [son86]. for extensions to nonpolyhedral convex sets, see
boyd and el ghaoui [be93], jarre [jar94], and nesterov and nemirovski [nn94, page
34].

id76 has been applied to linear and nonlinear discrimination problems
since the 1960s; see mangasarian [man65] and rosen [ros65]. standard texts that dis-
cuss pattern classi   cation include duda, hart, and stork [dhs99] and hastie, tibshirani,
and friedman [htf01]. for a detailed discussion of support vector classi   ers, see vap-
nik [vap00] or sch  olkopf and smola [ss01].

the weber point de   ned in example 8.4 is named after weber [web71]. linear and
quadratic placement is used in circuit design (kleinhaus, sigl, johannes, and antre-
ich [ksja91, sdj91]). sherwani [she99] is a recent overview of algorithms for placement,
layout,    oor planning, and other geometric optimization problems in vlsi circuit design.

exercises

exercises

projection on a set

447

8.1 uniqueness of projection. show that if c     rn is nonempty, closed and convex, and the
norm k    k is strictly convex, then for every x0 there is exactly one x     c closest to x0. in
other words the projection of x0 on c is unique.
8.2 [web94, val64] chebyshev characterization of convexity. a set c     rn is called a cheby-
shev set if for every x0     rn, there is a unique point in c closest (in euclidean norm)
to x0. from the result in exercise 8.1, every nonempty, closed, convex set is a chebyshev
set. in this problem we show the converse, which is known as motzkin   s theorem.
let c     rn be a chebyshev set.
(a) show that c is nonempty and closed.
(b) show that pc , the euclidean projection on c, is continuous.
(c) suppose x0 6    c. show that pc (x) = pc (x0) for all x =   x0 + (1       )pc (x0) with
0            1.
(d) suppose x0 6    c. show that pc (x) = pc (x0) for all x =   x0 + (1       )pc (x0) with
       1.
(e) combining parts (c) and (d), we can conclude that all points on the ray with base
pc (x0) and direction x0     pc (x0) have projection pc (x0). show that this implies
that c is convex.

8.3 euclidean projection on proper cones.

(a) nonnegative orthant. show that euclidean projection onto the nonnegative orthant

is given by the expression on page 399.

(b) positive semide   nite cone. show that euclidean projection onto the positive semidef-

inite cone is given by the expression on page 399.

(c) second-order cone. show that the euclidean projection of (x0, t0) on the second-

order cone

is given by

k = {(x, t)     rn+1 | kxk2     t}

pk (x0, t0) =( 0

(x0, t0)
(1/2)(1 + t0/kx0k2)(x0,kx0k2)

kx0k2        t0
kx0k2     t0
kx0k2     |t0|.

8.4 the euclidean projection of a point on a convex set yields a simple separating hyperplane

(pc (x0)     x0)t (x     (1/2)(x0 + pc (x0))) = 0.

find a counterexample that shows that this construction does not work for general norms.
8.5 [hul93, volume 1, page 154] depth function and signed distance to boundary. let c     rn
be a nonempty convex set, and let dist(x, c) be the distance of x to c in some norm.
we already know that dist(x, c) is a convex function of x.

(a) show that the depth function,

is concave for x     c.

(b) the signed distance to the boundary of c is de   ned as

depth(x, c) = dist(x, rn \ c),

s(x) =(cid:26) dist(x, c)

x 6    c
    depth(x, c) x     c.

thus, s(x) is positive outside c, zero on its boundary, and negative on its interior.
show that s is a convex function.

448

8 geometric problems

distance between sets

8.6 let c, d be convex sets.

(a) show that dist(c, x + d) is a convex function of x.
(b) show that dist(tc, x + td) is a convex function of (x, t) for t > 0.

8.7 separation of ellipsoids. let e1 and e2 be two ellipsoids de   ned as

e1 = {x | (x     x1)t p    1

1

(x     x1)     1},

e2 = {x | (x     x2)t p    1

2

(x     x2)     1},

where p1, p2     sn

++. show that e1     e2 =     if and only if there exists an a     rn with

kp 1/2

2 ak2 + kp 1/2

1 ak2 < at (x1     x2).

8.8 intersection and containment of polyhedra. let p1 and p2 be two polyhedra de   ned as

p1 = {x | ax (cid:22) b},

p2 = {x | f x (cid:22) g},

with a     rm  n, b     rm, f     rp  n, g     rp. formulate each of the following problems
as an lp feasibility problem, or a set of lp feasibility problems.
(a) find a point in the intersection p1     p2.
(b) determine whether p1     p2.
for each problem, derive a set of linear inequalities and equalities that forms a strong
alternative, and give a geometric interpretation of the alternative.
repeat the question for two polyhedra de   ned as

p1 = conv{v1, . . . , vk},

p2 = conv{w1, . . . , wl}.

euclidean distance and angle problems

8.9 closest euclidean distance matrix to given data. we are given data   dij, for i, j = 1, . . . , n,

which are corrupted measurements of the euclidean distances between vectors in rk:

  dij = kxi     xjk2 + vij,

i, j = 1, . . . , n,

where vij is some noise or error. these data satisfy   dij     0 and   dij =   dji, for all i, j. the
dimension k is not speci   ed.
show how to solve the following problem using id76. find a dimension
i,j=1(dij       dij)2 is minimized, where dij = kxi     xjk2,
i, j = 1, . . . , n. in other words, given some data that are approximate euclidean distances,
you are to    nd the closest set of actual euclidean distances, in the least-squares sense.
8.10 minimax angle    tting. suppose that y1, . . . , ym     rk are a   ne functions of a variable

k and x1, . . . , xn     rk so thatpn

x     rn:
and z1, . . . , zm     rk are given nonzero vectors. we want to choose the variable x, subject
to some convex constraints, (e.g., linear inequalities) to minimize the maximum angle
between yi and zi,

yi = aix + bi,

i = 1, . . . , m,

the angle between nonzero vectors is de   ned as usual:

max{6 (y1, z1), . . . , 6 (ym, zm)}.

6 (u, v) = cos   1(cid:18) ut v

kuk2kvk2(cid:19) ,

where we take cos   1(a)     [0,   ]. we are only interested in the case when the optimal
objective value does not exceed   /2.
formulate this problem as a convex or quasiid76 problem. when the
constraints on x are linear inequalities, what kind of problem (or problems) do you have
to solve?

exercises

449

8.11 smallest euclidean cone containing given points. in rn, we de   ne a euclidean cone, with

center direction c 6= 0, and angular radius   , with 0              /2, as the set

{x     rn | 6 (c, x)       }.

(a euclidean cone is a second-order cone, i.e., it can be represented as the image of the
second-order cone under a nonsingular linear mapping.)
let a1, . . . , am     rn. how would you    nd the euclidean cone, of smallest angular radius,
that contains a1, . . . , am? (in particular, you should explain how to solve the feasibility
problem, i.e., how to determine whether there is a euclidean cone which contains the
points.)

extremal volume ellipsoids

8.12 show that the maximum volume ellipsoid enclosed in a set is unique. show that the

l  owner-john ellipsoid of a set is unique.

8.13 l  owner-john ellipsoid of a simplex. in this exercise we show that the l  owner-john el-
lipsoid of a simplex in rn must be shrunk by a factor n to    t inside the simplex. since
the l  owner-john ellipsoid is a   nely invariant, it is su   cient to show the result for one
particular simplex.
derive the l  owner-john ellipsoid elj for the simplex c = conv{0, e1, . . . , en}. show that
elj must be shrunk by a factor 1/n to    t inside the simplex.
8.14 e   ciency of ellipsoidal inner approximation. let c be a polyhedron in rn described as

c = {x | ax (cid:22) b}, and suppose that {x | ax     b} is nonempty.
(a) show that the maximum volume ellipsoid enclosed in c, expanded by a factor n

about its center, is an ellipsoid that contains c.

(b) show that if c is symmetric about the origin, i.e., of the form c = {x |    1 (cid:22) ax (cid:22)
1}, then expanding the maximum volume inscribed ellipsoid by a factor    n gives

an ellipsoid that contains c.

8.15 minimum volume ellipsoid covering union of ellipsoids. formulate the following problem
as a id76 problem. find the minimum volume ellipsoid e = {x | (x    
x0)t a   1(x     x0)     1} that contains k given ellipsoids
i x + ci     0},

ei = {x | xt aix + 2bt

i = 1, . . . , k.

hint. see appendix b.

8.16 maximum volume rectangle inside a polyhedron. formulate the following problem as a

id76 problem. find the rectangle

r = {x     rn | l (cid:22) x (cid:22) u}

of maximum volume, enclosed in a polyhedron p = {x | ax (cid:22) b}. the variables are
l, u     rn. your formulation should not involve an exponential number of constraints.
centering

8.17 a   ne invariance of analytic center. show that the analytic center of a set of inequalities is
a   ne invariant. show that it is invariant with respect to positive scaling of the inequalities.

8.18 analytic center and redundant inequalities. two sets of linear inequalities that describe
the same polyhedron can have di   erent analytic centers. show that by adding redundant
inequalities, we can make any interior point x0 of a polyhedron

p = {x     rn | ax (cid:22) b}

450

8 geometric problems

the analytic center. more speci   cally, suppose a     rm  n and ax0     b. show that there
exist c     rn,        r, and a positive integer q, such that p is the solution set of the m + q
inequalities
(8.36)
(where the inequality ct x        is added q times), and x0 is the analytic center of (8.36).

ct x       ,

ct x       ,

ct x       

ax (cid:22) b,

. . . ,

8.19 let xac be the analytic center of a set of linear inequalities

and de   ne h as the hessian of the logarithmic barrier function at xac:

at
i x     bi,

i = 1, . . . , m,

h =

mxi=1

1
(bi     at
i xac)2

aiat
i .

show that the kth inequality is redundant (i.e., it can be deleted without changing the
feasible set) if

bk     at

k xac     m(at

k h    1ak)1/2.

8.20 ellipsoidal approximation from analytic center of linear matrix inequality. let c be the

solution set of the lmi

where ai, b     sm, and let xac be its analytic center. show that

x1a1 + x2a2 +        + xnan (cid:22) b,

einner     c     eouter,

where

einner = {x | (x     xac)t h(x     xac)     1},
eouter = {x | (x     xac)t h(x     xac)     m(m     1)},

and h is the hessian of the logarithmic barrier function

    log det(b     x1a1     x2a2                xnan)

evaluated at xac.

8.21 [byt99] maximum likelihood interpretation of analytic center. we use the linear mea-

surement model of page 352,

y = ax + v,

where a     rm  n. we assume the noise components vi are iid with support [   1, 1]. the
set of parameters x consistent with the measurements y     rm is the polyhedron de   ned
by the linear inequalities
(8.37)

   1 + y (cid:22) ax (cid:22) 1 + y.

suppose the id203 density function of vi has the form

p(v) =(cid:26)   r(1     v2)r    1     v     1

otherwise,

0

where r     1 and   r > 0. show that the maximum likelihood estimate of x is the analytic
center of (8.37).
8.22 center of gravity. the center of gravity of a set c     rn with nonempty interior is de   ned

as

xcg = rc
rc

u du

1 du

.

exercises

451

the center of gravity is a   ne invariant, and (clearly) a function of the set c, and not
its particular description. unlike the centers described in the chapter, however, it is very
di   cult to compute the center of gravity, except in simple cases (e.g., ellipsoids, balls,
simplexes).
show that the center of gravity xcg is the minimizer of the convex function

f (x) =zc ku     xk2

2 du.

classi   cation

8.23 robust linear discrimination. consider the robust linear discrimination problem given

in (8.23).

(a) show that the optimal value t    is positive if and only if the two sets of points can
be linearly separated. when the two sets of points can be linearly separated, show
that the inequality kak2     1 is tight, i.e., we have ka   k2 = 1, for the optimal a   .

(b) using the change of variables   a = a/t,   b = b/t, prove that the problem (8.23) is

equivalent to the qp

minimize
subject to

k  ak2
  at xi       b     1,
  at yi       b        1,

i = 1, . . . , n

i = 1, . . . , m.

8.24 linear discrimination maximally robust to weight errors. suppose we are given two sets of
points {x1, . . . , xn} and and {y1, . . . , ym} in rn that can be linearly separated. in   8.6.1
we showed how to    nd the a   ne function that discriminates the sets, and gives the largest
gap in function values. we can also consider robustness with respect to changes in the
vector a, which is sometimes called the weight vector. for a given a and b for which
f (x) = at x     b separates the two sets, we de   ne the weight error margin as the norm of
the smallest u     rn such that the a   ne function (a + u)t x     b no longer separates the
two sets of points. in other words, the weight error margin is the maximum    such that

(a + u)t xi     b,

i = 1, . . . , n,

(a + u)t yj     b,

i = 1, . . . , m,

holds for all u with kuk2       .
show how to    nd a and b that maximize the weight error margin, subject to the normal-
ization constraint kak2     1.
8.25 most spherical separating ellipsoid. we are given two sets of vectors x1, . . . , xn     rn, and
y1, . . . , ym     rn, and wish to    nd the ellipsoid with minimum eccentricity (i.e., minimum
condition number of the de   ning matrix) that contains the points x1, . . . , xn , but not the
points y1, . . . , ym . formulate this as a id76 problem.

placement and    oor planning

8.26 quadratic placement. we consider a placement problem in r2, de   ned by an undirected

graph a with n nodes, and with quadratic costs:

minimize p(i,j)   a kxi     xjk2

2.

the variables are the positions xi     r2, i = 1, . . . , m . the positions xi, i = m + 1, . . . , n
are given. we de   ne two vectors u, v     rm by

u = (x11, x21, . . . , xm 1),

v = (x12, x22, . . . , xm 2),

containing the    rst and second components, respectively, of the free nodes.

452

8 geometric problems

show that u and v can be found by solving two sets of linear equations,

cu = d1,

cv = d2,

where c     sm . give a simple expression for the coe   cients of c in terms of the graph a.
8.27 problems with minimum distance constraints. we consider a problem with variables
x1, . . . , xn     rk. the objective, f0(x1, . . . , xn ), is convex, and the constraints

fi(x1, . . . , xn )     0,

i = 1, . . . , m,

are convex (i.e., the functions fi : rn k     r are convex).
minimum distance constraints

in addition, we have the

kxi     xjk2     dmin,

i 6= j,

i, j = 1, . . . , n.

in general, this is a hard nonconvex problem.
following the approach taken in    oorplanning, we can form a convex restriction of the
problem, i.e., a problem which is convex, but has a smaller feasible set. (solving the
restricted problem is therefore easy, and any solution is guaranteed to be feasible for the
nonconvex problem.) let aij     rk, for i < j, i, j = 1, . . . , n , satisfy kaijk2 = 1.
show that the restricted problem

minimize
subject to

f0(x1, . . . , xn )
fi(x1, . . . , xn )     0,
at
ij(xi     xj)     dmin,

i = 1, . . . , m

i < j, i, j = 1, . . . , n,

is convex, and that every feasible point satis   es the minimum distance constraint.
remark. there are many good heuristics for choosing the directions aij. one simple
one starts with an approximate solution   x1, . . . ,   xn (that need not satisfy the minimum
distance constraints). we then set aij = (  xi       xj)/k  xi       xjk2.
miscellaneous problems

8.28 let p1 and p2 be two polyhedra described as

p1 = {x | ax (cid:22) b} ,

p2 = {x |    1 (cid:22) cx (cid:22) 1} ,

where a     rm  n, c     rp  n, and b     rm. the polyhedron p2 is symmetric about the
origin. for t     0 and xc     rn, we use the notation tp2 + xc to denote the polyhedron

tp2 + xc = {tx + xc | x     p2},

which is obtained by    rst scaling p2 by a factor t about the origin, and then translating
its center to xc.
show how to solve the following two problems, via an lp, or a set of lps.

(a) find the largest polyhedron tp2 + xc enclosed in p1, i.e.,

maximize
subject to

t
tp2 + xc     p1
t     0.

(b) find the smallest polyhedron tp2 + xc containing p1, i.e.,

t

minimize
subject to p1     tp2 + xc

t     0.

exercises

453

in both problems the variables are t     r and xc     rn.
8.29 outer polyhedral approximations. let p = {x     rn | ax (cid:22) b} be a polyhedron, and
c     rn a given set (not necessarily convex). use the support function sc to formulate
the following problem as an lp:

t

minimize
subject to c     tp + x

t     0.

here tp + x = {tu + x | u     p}, the polyhedron p scaled by a factor of t about the origin,
and translated by x. the variables are t     r and x     rn.
8.30 interpolation with piecewise-arc curve. a sequence of points a1, . . . , an     r2 is given. we
construct a curve that passes through these points, in order, and is an arc (i.e., part of a
circle) or line segment (which we think of as an arc of in   nite radius) between consecutive
points. many arcs connect ai and ai+1; we parameterize these arcs by giving the angle
  i     (     ,   ) between its tangent at ai and the line segment [ai, ai+1]. thus,   i = 0 means
the arc between ai and ai+1 is in fact the line segment [ai, ai+1];   i =   /2 means the arc
between ai and ai+1 is a half-circle (above the linear segment [a1, a2]);   i =      /2 means
the arc between ai and ai+1 is a half-circle (below the linear segment [a1, a2]). this is
illustrated below.

  i = 3  /4

  i =   /2

  i =   /4

  i = 0

ai

ai+1

our curve is completely speci   ed by the angles   1, . . . ,   n, which can be chosen in the
interval (     ,   ). the choice of   i a   ects several properties of the curve, for example, its
total arc length l, or the joint angle discontinuities, which can be described as follows.
at each point ai, i = 2, . . . , n    1, two arcs meet, one coming from the previous point and
one going to the next point. if the tangents to these arcs exactly oppose each other, so the
curve is di   erentiable at ai, we say there is no joint angle discontinuity at ai. in general,
we de   ne the joint angle discontinuity at ai as |  i   1+  i+  i|, where   i is the angle between
the line segment [ai, ai+1] and the line segment [ai   1, ai], i.e.,   i = 6 (ai    ai+1, ai   1    ai).
this is shown below. note that the angles   i are known (since the ai are known).

  i   1

ai

  i

  i

ai+1

ai   1

we de   ne the total joint angle discontinuity as

d =

|  i   1 +   i +   i|.

nxi=2

formulate the problem of minimizing total arc length length l, and total joint angle
discontinuity d, as a bi-criterion id76 problem. explain how you would
   nd the extreme points on the optimal trade-o    curve.

part iii

algorithms

chapter 9

unconstrained minimization

9.1 unconstrained minimization problems

in this chapter we discuss methods for solving the unconstrained optimization
problem

f (x)

minimize

(9.1)
where f : rn     r is convex and twice continuously di   erentiable (which implies
that dom f is open). we will assume that the problem is solvable, i.e., there exists
an optimal point x   . (more precisely, the assumptions later in the chapter will
imply that x    exists and is unique.) we denote the optimal value, inf x f (x) =
f (x   ), as p   .

since f is di   erentiable and convex, a necessary and su   cient condition for a

point x    to be optimal is

   f (x   ) = 0

(9.2)
(see   4.2.3). thus, solving the unconstrained minimization problem (9.1) is the
same as    nding a solution of (9.2), which is a set of n equations in the n variables
x1, . . . , xn. in a few special cases, we can    nd a solution to the problem (9.1) by
analytically solving the optimality equation (9.2), but usually the problem must
be solved by an iterative algorithm. by this we mean an algorithm that computes
a sequence of points x(0), x(1), . . .     dom f with f (x(k))     p    as k        . such
a sequence of points is called a minimizing sequence for the problem (9.1). the
algorithm is terminated when f (x(k))     p          , where    > 0 is some speci   ed
tolerance.

initial point and sublevel set

the methods described in this chapter require a suitable starting point x(0). the
starting point must lie in dom f , and in addition the sublevel set

s = {x     dom f | f (x)     f (x(0))}

(9.3)

must be closed. this condition is satis   ed for all x(0)     dom f if the function f is
closed, i.e., all its sublevel sets are closed (see   a.3.3). continuous functions with

458

9 unconstrained minimization

dom f = rn are closed, so if dom f = rn, the initial sublevel set condition is
satis   ed by any x(0). another important class of closed functions are continuous
functions with open domains, for which f (x) tends to in   nity as x approaches
bd dom f .

9.1.1 examples

quadratic minimization and least-squares

the general convex quadratic minimization problem has the form

minimize

(1/2)xt p x + qt x + r,

(9.4)
+, q     rn, and r     r. this problem can be solved via the optimality
where p     sn
conditions, p x    + q = 0, which is a set of linear equations. when p     0, there is
a unique solution, x    =    p    1q. in the more general case when p is not positive
de   nite, any solution of p x    =    q is optimal for (9.4); if p x    =    q does not
have a solution, then the problem (9.4) is unbounded below (see exercise 9.1). our
ability to analytically solve the quadratic minimization problem (9.4) is the basis
for newton   s method, a powerful method for unconstrained minimization described
in   9.5.
quently is the least-squares problem
kax     bk2

one special case of the quadratic minimization problem that arises very fre-

2 = xt (at a)x     2(at b)t x + bt b.

minimize

the optimality conditions

at ax    = at b

are called the normal equations of the least-squares problem.

unconstrained geometric programming

as a second example, we consider an unconstrained geometric program in convex
form,

minimize

the optimality condition is

i=1 exp(at

i x + bi)(cid:1) .

f (x) = log(cid:0)pm
mxi=1

j x    + bj)

1

exp(at

i x    + bi)ai = 0,

   f (x   ) =

j=1 exp(at

pm

which in general has no analytical solution, so here we must resort to an iterative
algorithm. for this problem, dom f = rn, so any point can be chosen as the
initial point x(0).

analytic center of linear inequalities

we consider the optimization problem

minimize

f (x) =    pm

i=1 log(bi     at

i x),

(9.5)

9.1 unconstrained minimization problems

459

where the domain of f is the open set

dom f = {x | at

i x < bi, i = 1, . . . , m}.

the objective function f in this problem is called the logarithmic barrier for the
inequalities at
i x     bi. the solution of (9.5), if it exists, is called the analytic
center of the inequalities. the initial point x(0) must satisfy the strict inequalities
at
i x(0) < bi, i = 1, . . . , m. since f is closed, the sublevel set s for any such point
is closed.

analytic center of a linear matrix inequality

a closely related problem is

minimize
where f : rn     sp is a   ne, i.e.,

f (x) = log det f (x)   1

(9.6)

f (x) = f0 + x1f1 +        + xnfn,

with fi     sp. here the domain of f is

dom f = {x | f (x)     0}.

the objective function f is called the logarithmic barrier for the linear matrix
inequality f (x) (cid:23) 0, and the solution (if it exists) is called the analytic center of
the linear matrix inequality. the initial point x(0) must satisfy the strict linear
matrix inequality f (x(0))     0. as in the previous example, the sublevel set of any
such point will be closed, since f is closed.

9.1.2 strong convexity and implications

in much of this chapter (with the exception of   9.6) we assume that the objective
function is strongly convex on s, which means that there exists an m > 0 such that

   2f (x) (cid:23) mi

(9.7)

for all x     s. strong convexity has several interesting consequences. for x, y     s
we have

f (y) = f (x) +    f (x)t (y     x) +

(y     x)t   2f (z)(y     x)

1
2

for some z on the line segment [x, y]. by the strong convexity assumption (9.7), the
last term on the righthand side is at least (m/2)ky    xk2
2, so we have the inequality
m
2 ky     xk2

f (y)     f (x) +    f (x)t (y     x) +

(9.8)

2

for all x and y in s. when m = 0, we recover the basic inequality characterizing
convexity; for m > 0 we obtain a better lower bound on f (y) than follows from
convexity alone.

460

9 unconstrained minimization

we will    rst show that the inequality (9.8) can be used to bound f (x)     p   ,
which is the suboptimality of the point x, in terms of k   f (x)k2. the righthand
side of (9.8) is a convex quadratic function of y (for    xed x). setting the gradient
with respect to y equal to zero, we    nd that   y = x     (1/m)   f (x) minimizes the
righthand side. therefore we have

f (y)     f (x) +    f (x)t (y     x) +
    f (x) +    f (x)t (  y     x) +
= f (x)    

1
2mk   f (x)k2
2.

2

m
2 ky     xk2
m
2 k  y     xk2

2

since this holds for any y     s, we have
p        f (x)    

1
2mk   f (x)k2
2.

(9.9)

this inequality shows that if the gradient is small at a point, then the point is
nearly optimal. the inequality (9.9) can also be interpreted as a condition for
suboptimality which generalizes the optimality condition (9.2):
k   f (x)k2     (2m  )1/2 =    f (x)     p          .

(9.10)

we can also derive a bound on kx     x   k2, the distance between x and any

optimal point x   , in terms of k   f (x)k2:
kx     x   k2    

2
mk   f (x)k2.

(9.11)

to see this, we apply (9.8) with y = x    to obtain

p    = f (x   )     f (x) +    f (x)t (x        x) +

    f (x)     k   f (x)k2kx        xk2 +

m
2 kx        xk2

2

m
2 kx        xk2
2,

where we use the cauchy-schwarz inequality in the second inequality. since p       
f (x), we must have

   k   f (x)k2 kx        xk2 +

m
2 kx        xk2

2     0,

from which (9.11) follows. one consequence of (9.11) is that the optimal point x   
is unique.

upper bound on    2f (x)
the inequality (9.8) implies that the sublevel sets contained in s are bounded, so in
particular, s is bounded. therefore the maximum eigenvalue of    2f (x), which is a
continuous function of x on s, is bounded above on s, i.e., there exists a constant
m such that

   2f (x) (cid:22) m i

(9.12)

9.1 unconstrained minimization problems

461

for all x     s. this upper bound on the hessian implies for any x, y     s,

m
2 ky     xk2
2,
which is analogous to (9.8). minimizing each side over y yields

f (y)     f (x) +    f (x)t (y     x) +

p        f (x)    

1
2m k   f (x)k2
2,

the counterpart of (9.9).

condition number of sublevel sets

(9.13)

(9.14)

from the strong convexity inequality (9.7) and the inequality (9.12), we have

mi (cid:22)    2f (x) (cid:22) m i

(9.15)

for all x     s. the ratio    = m/m is thus an upper bound on the condition
number of the matrix    2f (x), i.e., the ratio of its largest eigenvalue to its smallest
eigenvalue. we can also give a geometric interpretation of (9.15) in terms of the
sublevel sets of f .

we de   ne the width of a convex set c     rn, in the direction q, where kqk2 = 1,

as

w (c, q) = sup
z   c

qt z     inf

z   c

qt z.

the minimum width and maximum width of c are given by

wmin = inf

kqk2=1

w (c, q),

wmax = sup

w (c, q).

kqk2=1

the condition number of the convex set c is de   ned as

cond(c) =

w 2
w 2

max

,

min

i.e., the square of the ratio of its maximum width to its minimum width. the
condition number of c gives a measure of its anisotropy or eccentricity.
if the
condition number of a set c is small (say, near one) it means that the set has
approximately the same width in all directions, i.e., it is nearly spherical. if the
condition number is large, it means that the set is far wider in some directions than
in others.

example 9.1 condition number of an ellipsoid. let e be the ellipsoid

e = {x | (x     x0)t a   1(x     x0)     1},

where a     sn
sup
z   e

++. the width of e in the direction q is
qt z     inf

qt z = (ka1/2qk2 + qt x0)     (   ka1/2qk2 + qt x0)

z   e

= 2ka1/2qk2.

462

9 unconstrained minimization

it follows that its minimum and maximum width are

wmin = 2  min(a)1/2,

wmax = 2  max(a)1/2,

and its condition number is

cond(e) =

  max(a)
  min(a)

=   (a),

where   (a) denotes the condition number of the matrix a, i.e., the ratio of its
maximum singular value to its minimum singular value. thus the condition number
of the ellipsoid e is the same as the condition number of the matrix a that de   nes
it.

now suppose f satis   es mi (cid:22)    2f (x) (cid:22) m i for all x     s. we will derive
a bound on the condition number of the   -sublevel c   = {x | f (x)       }, where
p    <        f (x(0)). applying (9.13) and (9.8) with x = x   , we have
2     f (y)     p    + (m/2)ky     x   k2
2.

p    + (m/2)ky     x   k2

this implies that binner     c       bouter where

binner = {y | ky     x   k2     (2(       p   )/m )1/2},
bouter = {y | ky     x   k2     (2(       p   )/m)1/2}.

in other words, the   -sublevel set contains binner, and is contained in bouter, which
are balls with radii

(2(       p   )/m )1/2,

(2(       p   )/m)1/2,

respectively. the ratio of the radii squared gives an upper bound on the condition
number of c  :

cond(c  )    

m
m

.

we can also give a geometric interpretation of the condition number   (   2f (x   ))
of the hessian at the optimum. from the taylor series expansion of f around x   ,

f (y)     p    +

1
2

(y     x   )t   2f (x   )(y     x   ),

we see that, for    close to p   ,

c       {y | (y     x   )t   2f (x   )(y     x   )     2(       p   )},

i.e., the sublevel set is well approximated by an ellipsoid with center x   . therefore

lim
     p   

cond(c  ) =   (   2f (x   )).

we will see that the condition number of the sublevel sets of f (which is bounded
by m/m) has a strong e   ect on the e   ciency of some common methods for uncon-
strained minimization.

9.2 descent methods

463

the strong convexity constants

it must be kept in mind that the constants m and m are known only in rare cases,
so the inequality (9.10) cannot be used as a practical stopping criterion. it can be
considered a conceptual stopping criterion; it shows that if the gradient of f at x
is small enough, then the di   erence between f (x) and p    is small. if we terminate
an algorithm when k   f (x(k))k2       , where    is chosen small enough to be (very
likely) smaller than (m  )1/2, then we have f (x(k))     p           (very likely).
in the following sections we give convergence proofs for algorithms, which in-
clude bounds on the number of iterations required before f (x(k))     p          , where
   is some positive tolerance. many of these bounds involve the (usually unknown)
constants m and m , so the same comments apply. these results are at least con-
ceptually useful; they establish that the algorithm converges, even if the bound on
the number of iterations required to reach a given accuracy depends on constants
that are unknown.

we will encounter one important exception to this situation. in   9.6 we will
study a special class of convex functions, called self-concordant, for which we can
provide a complete convergence analysis (for newton   s method) that does not de-
pend on any unknown constants.

9.2 descent methods

the algorithms described in this chapter produce a minimizing sequence x(k), k =
1, . . . , where

x(k+1) = x(k) + t(k)   x(k)

and t(k) > 0 (except when x(k) is optimal). here the concatenated symbols     and
x that form    x are to be read as a single entity, a vector in rn called the step or
search direction (even though it need not have unit norm), and k = 0, 1, . . . denotes
the iteration number. the scalar t(k)     0 is called the step size or step length at
iteration k (even though it is not equal to kx(k+1)     x(k)k unless k   x(k)k = 1).
the terms    search step    and    scale factor    are more accurate, but    search direction   
and    step length    are the ones widely used. when we focus on one iteration of
an algorithm, we sometimes drop the superscripts and use the lighter notation
x+ = x + t   x, or x := x + t   x, in place of x(k+1) = x(k) + t(k)   x(k).
all the methods we study are descent methods, which means that

f (x(k+1)) < f (x(k)),

except when x(k) is optimal. this implies that for all k we have x(k)     s, the initial
sublevel set, and in particular we have x(k)     dom f . from convexity we know
that    f (x(k))t (y     x(k))     0 implies f (y)     f (x(k)), so the search direction in a
descent method must satisfy

i.e., it must make an acute angle with the negative gradient. we call such a
direction a descent direction (for f , at x(k)).

   f (x(k))t    x(k) < 0,

464

9 unconstrained minimization

the outline of a general descent method is as follows. it alternates between two

steps: determining a descent direction    x, and the selection of a step size t.

algorithm 9.1 general descent method.

given a starting point x     dom f .
repeat

1. determine a descent direction    x.
2. line search. choose a step size t > 0.
3. update. x := x + t   x.

until stopping criterion is satis   ed.

the second step is called the line search since selection of the step size t deter-
mines where along the line {x + t   x | t     r+} the next iterate will be. (a more
accurate term might be ray search.)
a practical descent method has the same general structure, but might be or-
ganized di   erently. for example, the stopping criterion is often checked while, or
immediately after, the descent direction    x is computed. the stopping criterion
is often of the form k   f (x)k2       , where    is small and positive, as suggested by
the suboptimality condition (9.9).

exact line search

one line search method sometimes used in practice is exact line search, in which t
is chosen to minimize f along the ray {x + t   x | t     0}:
t = argmins   0 f (x + s   x).

(9.16)

an exact line search is used when the cost of the minimization problem with one
variable, required in (9.16), is low compared to the cost of computing the search
direction itself. in some special cases the minimizer along the ray can be found an-
alytically, and in others it can be computed e   ciently. (this is discussed in   9.7.1.)
backtracking line search

most line searches used in practice are inexact: the step length is chosen to ap-
proximately minimize f along the ray {x + t   x | t     0}, or even to just reduce
f    enough   . many inexact line search methods have been proposed. one inexact
line search method that is very simple and quite e   ective is called backtracking line
search. it depends on two constants   ,    with 0 <    < 0.5, 0 <    < 1.

algorithm 9.2 backtracking line search.

given a descent direction    x for f at x     dom f ,        (0, 0.5),        (0, 1).
t := 1.
while f (x + t   x) > f (x) +   t   f (x)t    x,

t :=   t.

9.2 descent methods

465

f (x + t   x)

f (x) + t   f (x)t    x

f (x) +   t   f (x)t    x
t

t = 0

t0

figure 9.1 backtracking line search. the curve shows f , restricted to the line
over which we search. the lower dashed line shows the linear extrapolation
of f , and the upper dashed line has a slope a factor of    smaller. the
backtracking condition is that f lies below the upper dashed line, i.e., 0    
t     t0.

the line search is called backtracking because it starts with unit step size and
then reduces it by the factor    until the stopping condition f (x + t   x)     f (x) +
  t   f (x)t    x holds. since    x is a descent direction, we have    f (x)t    x < 0, so
for small enough t we have

f (x + t   x)     f (x) + t   f (x)t    x < f (x) +   t   f (x)t    x,

which shows that the backtracking line search eventually terminates. the constant
   can be interpreted as the fraction of the decrease in f predicted by linear extrap-
olation that we will accept. (the reason for requiring    to be smaller than 0.5 will
become clear later.)

the backtracking condition is illustrated in    gure 9.1. this    gure suggests,
and it can be shown, that the backtracking exit inequality f (x + t   x)     f (x) +
  t   f (x)t    x holds for t     0 in an interval (0, t0]. it follows that the backtracking
line search stops with a step length t that satis   es

t = 1,

or

t     (  t0, t0].

the    rst case occurs when the step length t = 1 satis   es the backtracking condition,
i.e., 1     t0. in particular, we can say that the step length obtained by backtracking
line search satis   es

t     min{1,   t0}.

when dom f is not all of rn, the condition f (x + t   x)     f (x) +   t   f (x)t    x
in the backtracking line search must be interpreted carefully. by our convention
that f is in   nite outside its domain, the inequality implies that x + t   x     dom f .
in a practical implementation, we    rst multiply t by    until x + t   x     dom f ;

466

9 unconstrained minimization

then we start to check whether the inequality f (x + t   x)     f (x) +   t   f (x)t    x
holds.
the parameter    is typically chosen between 0.01 and 0.3, meaning that we
accept a decrease in f between 1% and 30% of the prediction based on the linear
extrapolation. the parameter    is often chosen to be between 0.1 (which corre-
sponds to a very crude search) and 0.8 (which corresponds to a less crude search).

9.3 id119 method

a natural choice for the search direction is the negative gradient    x =       f (x).
the resulting algorithm is called the gradient algorithm or id119 method.

algorithm 9.3 id119 method.

given a starting point x     dom f .
repeat

1.    x :=       f (x).
2. line search. choose step size t via exact or backtracking line search.
3. update. x := x + t   x.

until stopping criterion is satis   ed.

the stopping criterion is usually of the form k   f (x)k2       , where    is small and
positive. in most implementations, this condition is checked after step 1, rather
than after the update.

9.3.1 convergence analysis

in this section we present a simple convergence analysis for the gradient method,
using the lighter notation x+ = x + t   x for x(k+1) = x(k) + t(k)   x(k), where    x =
      f (x). we assume f is strongly convex on s, so there are positive constants m
and m such that mi (cid:22)    2f (x) (cid:22) m i for all x     s. de   ne the function   f : r     r
by   f (t) = f (x     t   f (x)), i.e., f as a function of the step length t in the negative
in the following discussion we will only consider t for which
gradient direction.
x     t   f (x)     s. from the inequality (9.13), with y = x     t   f (x), we obtain a
quadratic upper bound on   f :

  f (t)     f (x)     tk   f (x)k2

2 +

m t2
2 k   f (x)k2
2.

(9.17)

analysis for exact line search

we now assume that an exact line search is used, and minimize over t both sides
of the inequality (9.17). on the lefthand side we get   f (texact), where texact is the
step length that minimizes   f . the righthand side is a simple quadratic, which

9.3 id119 method

467

is minimized by t = 1/m , and has minimum value f (x)     (1/(2m ))k   f (x)k2
2.
therefore we have

f (x+) =   f (texact)     f (x)    

1
2m k   (f (x))k2
2.

subtracting p    from both sides, we get

f (x+)     p        f (x)     p       

1
2m k   f (x)k2
2.

we combine this with k   f (x)k2
conclude

2     2m(f (x)     p   ) (which follows from (9.9)) to

applying this inequality recursively, we    nd that

f (x+)     p        (1     m/m )(f (x)     p   ).

(9.18)
where c = 1     m/m < 1, which shows that f (x(k)) converges to p    as k        . in
particular, we must have f (x(k))     p           after at most

f (x(k))     p        ck(f (x(0))     p   )

log((f (x(0))     p   )/  )

log(1/c)

(9.19)

iterations of the gradient method with exact line search.

this bound on the number of iterations required, even though crude, can give

some insight into the gradient method. the numerator,

log((f (x(0))     p   )/  )

can be interpreted as the log of the ratio of the initial suboptimality (i.e., gap
between f (x(0)) and p   ), to the    nal suboptimality (i.e., less than   ). this term
suggests that the number of iterations depends on how good the initial point is,
and what the    nal required accuracy is.

the denominator appearing in the bound (9.19), log(1/c), is a function of m/m,
which we have seen is a bound on the condition number of    2f (x) over s, or the
condition number of the sublevel sets {z | f (z)       }. for large condition number
bound m/m, we have

log(1/c) =     log(1     m/m )     m/m,

so our bound on the number of iterations required increases approximately linearly
with increasing m/m.

we will see that the gradient method does in fact require a large number of
iterations when the hessian of f , near x   , has a large condition number. conversely,
when the sublevel sets of f are relatively isotropic, so that the condition number
bound m/m can be chosen to be relatively small, the bound (9.18) shows that
convergence is rapid, since c is small, or at least not too close to one.

the bound (9.18) shows that the error f (x(k))     p    converges to zero at least
as fast as a geometric series. in the context of iterative numerical methods, this
is called linear convergence, since the error lies below a line on a log-linear plot of
error versus iteration number.

468

9 unconstrained minimization

analysis for backtracking line search

now we consider the case where a backtracking line search is used in the gradient
descent method. we will show that the backtracking exit condition,

  f (t)     f (x)       tk   f (x)k2
2,

is satis   ed whenever 0     t     1/m . first note that
m t2
2        t/2

0     t     1/m =        t +

(which follows from convexity of    t+m t2/2). using this result and the bound (9.17),
we have, for 0     t     1/m ,

  f (t)     f (x)     tk   f (x)k2

2 +
    f (x)     (t/2)k   f (x)k2
    f (x)       tk   f (x)k2
2,

2

m t2
2 k   (f (x))k2

2

since    < 1/2. therefore the backtracking line search terminates either with t = 1
or with a value t       /m . this provides a lower bound on the decrease in the
objective function. in the    rst case we have

f (x+)     f (x)       k   f (x)k2
2,

and in the second case we have

f (x+)     f (x)     (    /m )k   f (x)k2
2.

putting these together, we always have

f (x+)     f (x)     min{  ,     /m}k   f (x)k2
2.

now we can proceed exactly as in the case of exact line search. we subtract p   
from both sides to get

f (x+)     p        f (x)     p        min{  ,     /m}k   f (x)k2
2,

and combine this with k   f (x)k2

2     2m(f (x)     p   ) to obtain

f (x+)     p        (1     min{2m  , 2    m/m})(f (x)     p   ).

from this we conclude

f (x(k))     p        ck(f (x(0))     p   )

where

c = 1     min{2m  , 2    m/m} < 1.

in particular, f (x(k)) converges to p    at least as fast as a geometric series with an
exponent that depends (at least in part) on the condition number bound m/m. in
the terminology of iterative methods, the convergence is at least linear.

9.3 id119 method

469

4

2
x

0

   4

   10

0
x1

x(0)

x(1)

10

figure 9.2 some contour lines of the function f (x) = (1/2)(x2
2). the
condition number of the sublevel sets, which are ellipsoids, is exactly 10.
the    gure shows the iterates of the gradient method with exact line search,
started at x(0) = (10, 1).

1 + 10x2

9.3.2 examples

a quadratic problem in r2

our    rst example is very simple. we consider the quadratic objective function on
r2

f (x) =

(x2

1 +   x2

2),

1
2

where    > 0. clearly, the optimal point is x    = 0, and the optimal value is 0. the
hessian of f is constant, and has eigenvalues 1 and   , so the condition numbers of
the sublevel sets of f are all exactly

max{1,   }
min{1,   }

= max{  , 1/  }.

the tightest choices for the strong convexity constants m and m are

m = min{1,   },

m = max{1,   }.

we apply the id119 method with exact line search, starting at the
point x(0) = (  , 1). in this case we can derive the following closed-form expressions
for the iterates x(k) and their function values (exercise 9.6):

and

x(k)

,

x(k)

1 =   (cid:18)        1
   + 1(cid:19)k
(cid:18)        1
   + 1(cid:19)2k

  (   + 1)

       1

   + 1(cid:19)k
2 =(cid:18)   
   + 1(cid:19)2k
=(cid:18)        1

2

,

f (x(k)) =

f (x(0)).

this is illustrated in    gure 9.2, for    = 10.

for this simple example, convergence is exactly linear, i.e., the error is exactly
a geometric series, reduced by the factor |(       1)/(   + 1)|2 at each iteration. for

470

9 unconstrained minimization

   = 1, the exact solution is found in one iteration; for    not far from one (say,
between 1/3 and 3) convergence is rapid. the convergence is very slow for        1
or        1.
we can compare the convergence with the bound derived above in   9.3.1. using
the least conservative values m = min{1,   } and m = max{1,   }, the bound (9.18)
guarantees that the error in each iteration is reduced at least by the factor c =
(1     m/m ). we have seen that the error is in fact reduced exactly by the factor

1 + m/m(cid:19)2
(cid:18) 1     m/m

in each iteration. for small m/m , which corresponds to large condition number,
the upper bound (9.19) implies that the number of iterations required to obtain
a given level of accuracy grows at most like m/m. for this example, the exact
number of iterations required grows approximately like (m/m)/4, i.e., one quarter
of the value of the bound. this shows that for this simple example, the bound on
the number of iterations derived in our simple analysis is only about a factor of four
conservative (using the least conservative values for m and m ). in particular, the
convergence rate (as well as its upper bound) is very dependent on the condition
number of the sublevel sets.

a nonquadratic problem in r2

we now consider a nonquadratic example in r2, with

f (x1, x2) = ex1+3x2   0.1 + ex1   3x2   0.1 + e   x1   0.1.

(9.20)

we apply the gradient method with a backtracking line search, with    = 0.1,
   = 0.7. figure 9.3 shows some level curves of f , and the iterates x(k) generated
by the gradient method (shown as small circles). the lines connecting successive
iterates show the scaled steps,

x(k+1)     x(k) =    t(k)   f (x(k)).

figure 9.4 shows the error f (x(k))    p    versus iteration k. the plot reveals that
the error converges to zero approximately as a geometric series, i.e., the convergence
is approximately linear. in this example, the error is reduced from about 10 to
about 10   7 in 20 iterations, so the error is reduced by a factor of approximately
10   8/20     0.4 each iteration. this reasonably rapid convergence is predicted by
our convergence analysis, since the sublevel sets of f are not too badly conditioned,
which in turn means that m/m can be chosen as not too large.

to compare backtracking line search with an exact line search, we use the
gradient method with an exact line search, on the same problem, and with the
same starting point. the results are given in    gures 9.5 and 9.4. here too the
convergence is approximately linear, about twice as fast as the gradient method
with backtracking line search. with exact line search, the error is reduced by
about 10   11 in 15 iterations, i.e., a reduction by a factor of about 10   11/15     0.2
per iteration.

9.3 id119 method

471

x(0)

x(2)

x(1)

figure 9.3 iterates of the gradient method with backtracking line search,
for the problem in r2 with objective f given in (9.20). the dashed curves
are level curves of f , and the small circles are the iterates of the gradient
method. the solid lines, which connect successive iterates, show the scaled
steps t(k)   x(k).

105

100

   
p
   

10   5

)
)
k
(
x
(
f

10   10

backtracking l.s.

exact l.s.

10   15
0

5

10

15
k

20

25

figure 9.4 error f (x(k))    p    versus iteration k of the gradient method with
backtracking and exact line search, for the problem in r2 with objective f
given in (9.20). the plot shows nearly linear convergence, with the error
reduced approximately by the factor 0.4 in each iteration of the gradient
method with backtracking line search, and by the factor 0.2 in each iteration
of the gradient method with exact line search.

472

9 unconstrained minimization

x(0)

x(1)

figure 9.5 iterates of the gradient method with exact line search for the
problem in r2 with objective f given in (9.20).

a problem in r100

we next consider a larger example, of the form

f (x) = ct x    

mxi=1

log(bi     at

i x),

(9.21)

with m = 500 terms and n = 100 variables.

the progress of the gradient method with backtracking line search, with pa-
rameters    = 0.1,    = 0.5, is shown in    gure 9.6. in this example we see an initial
approximately linear and fairly rapid convergence for about 20 iterations, followed
by a slower linear convergence. overall, the error is reduced by a factor of around
106 in around 175 iterations, which gives an average error reduction by a factor of
around 10   6/175     0.92 per iteration. the initial convergence rate, for the    rst 20
iterations, is around a factor of 0.8 per iteration; the slower    nal convergence rate,
after the    rst 20 iterations, is around a factor of 0.94 per iteration.

figure 9.6 shows the convergence of the gradient method with exact line search.
the convergence is again approximately linear, with an overall error reduction by
approximately a factor 10   6/140     0.91 per iteration. this is only a bit faster than
the gradient method with backtracking line search.
finally, we examine the in   uence of the backtracking line search parameters   
and    on the convergence rate, by determining the number of iterations required
to obtain f (x(k))     p        10   5. in the    rst experiment, we    x    = 0.5, and vary
   from 0.05 to 0.5. the number of iterations required varies from about 80, for
larger values of   , in the range 0.2   0.5, to about 170 for smaller values of   . this,
and other experiments, suggest that the gradient method works better with fairly
large   , in the range 0.2   0.5.

similarly, we can study the e   ect of the choice of    by    xing    = 0.1 and
varying    from 0.05 to 0.95. again the variation in the total number of iterations
is not large, ranging from around 80 (when        0.5) to around 200 (for    small,
or near 1). this experiment, and others, suggest that        0.5 is a good choice.

9.3 id119 method

473

104

102

   
p
   

100

)
)
k
(
x
(
f

10   2

10   4
0

exact l.s.

backtracking l.s.

50

100
k

150

200

figure 9.6 error f (x(k))    p    versus iteration k for the gradient method with
backtracking and exact line search, for a problem in r100.

these experiments suggest that the e   ect of the backtracking parameters on the
convergence is not large, no more than a factor of two or so.

gradient method and condition number

our last experiment will illustrate the importance of the condition number of
   2f (x) (or the sublevel sets) on the rate of convergence of the gradient method.
we start with the function given by (9.21), but replace the variable x by x = t   x,
where

t = diag((1,   1/n,   2/n, . . . ,   (n   1)/n)),

i.e., we minimize

  f (  x) = ct t   x    

mxi=1

log(bi     at

i t   x).

(9.22)

this gives us a family of optimization problems, indexed by   , which a   ects the
problem condition number.

figure 9.7 shows the number of iterations required to achieve   f (  x(k))     p    < 10   5
as a function of   , using a backtracking line search with    = 0.3 and    = 0.7. this
plot shows that for diagonal scaling as small as 10 : 1 (i.e.,    = 10), the number of
iterations grows to more than a thousand; for a diagonal scaling of 20 or more, the
gradient method slows to essentially useless.

the condition number of the hessian    2   f (  x   ) at the optimum is shown in
   gure 9.8. for large and small   , the condition number increases roughly as
max{  2, 1/  2}, in a very similar way as the number of iterations depends on   .
this shows again that the relation between conditioning and convergence speed is
a real phenomenon, and not just an artifact of our analysis.

474

9 unconstrained minimization

103

s
n
o
i
t
a
r
e
t
i

102

10   1

100
  

101

figure 9.7 number of iterations of the gradient method applied to prob-
lem (9.22). the vertical axis shows the number of iterations required to
obtain   f (  x(k))       p    < 10   5. the horizontal axis shows   , which is a param-
eter that controls the amount of diagonal scaling. we use a backtracking
line search with    = 0.3,    = 0.7.

104

103

102

)
)
   
  x
(
  f
2

   
(
  

101

10   1

100
  

101

figure 9.8 condition number of the hessian of the function at its minimum,
as a function of   . by comparing this plot with the one in    gure 9.7, we see
that the condition number has a very strong in   uence on convergence rate.

9.4 steepest descent method

475

conclusions

from the numerical examples shown, and others, we can make the conclusions
summarized below.

    the gradient method often exhibits approximately linear convergence, i.e.,
the error f (x(k))     p    converges to zero approximately as a geometric series.
    the choice of backtracking parameters   ,    has a noticeable but not dramatic
e   ect on the convergence. an exact line search sometimes improves the con-
vergence of the gradient method, but the e   ect is not large (and probably
not worth the trouble of implementing the exact line search).

    the convergence rate depends greatly on the condition number of the hessian,
or the sublevel sets. convergence can be very slow, even for problems that are
moderately well conditioned (say, with condition number in the 100s). when
the condition number is larger (say, 1000 or more) the gradient method is so
slow that it is useless in practice.

the main advantage of the gradient method is its simplicity. its main disadvantage
is that its convergence rate depends so critically on the condition number of the
hessian or sublevel sets.

9.4 steepest descent method

the    rst-order taylor approximation of f (x + v) around x is

f (x + v)     bf (x + v) = f (x) +    f (x)t v.

the second term on the righthand side,    f (x)t v, is the directional derivative of
f at x in the direction v. it gives the approximate change in f for a small step v.
the step v is a descent direction if the directional derivative is negative.

we now address the question of how to choose v to make the directional deriva-
tive as negative as possible. since the directional derivative    f (x)t v is linear in
v, it can be made as negative as we like by taking v large (provided v is a descent
direction, i.e.,    f (x)t v < 0). to make the question sensible we have to limit the
size of v, or normalize by the length of v.
let k    k be any norm on rn. we de   ne a normalized steepest descent direction

(with respect to the norm k    k) as

   xnsd = argmin{   f (x)t v | kvk = 1}.

(9.23)

(we say    a    steepest descent direction because there can be multiple minimizers.)
a normalized steepest descent direction    xnsd is a step of unit norm that gives the
largest decrease in the linear approximation of f .

a normalized steepest descent direction can be interpreted geometrically as

follows. we can just as well de   ne    xnsd as

   xnsd = argmin{   f (x)t v | kvk     1},

476

9 unconstrained minimization

i.e., as the direction in the unit ball of k    k that extends farthest in the direction
      f (x).
it is also convenient to consider a steepest descent step    xsd that is unnormal-
ized, by scaling the normalized steepest descent direction in a particular way:

(9.24)
where k    k    denotes the dual norm. note that for the steepest descent step, we
have

   xsd = k   f (x)k      xnsd,

   f (x)t    xsd = k   f (x)k      f (x)t    xnsd =    k   f (x)k2

   

(see exercise 9.7).

the steepest descent method uses the steepest descent direction as search direc-

tion.

algorithm 9.4 steepest descent method.

given a starting point x     dom f .
repeat

1. compute steepest descent direction    xsd.
2. line search. choose t via backtracking or exact line search.
3. update. x := x + t   xsd.

until stopping criterion is satis   ed.

when exact line search is used, scale factors in the descent direction have no e   ect,
so the normalized or unnormalized direction can be used.

9.4.1 steepest descent for euclidean and quadratic norms

steepest descent for euclidean norm
if we take the norm k  k to be the euclidean norm we    nd that the steepest descent
direction is simply the negative gradient, i.e.,    xsd =       f (x). the steepest
descent method for the euclidean norm coincides with the id119 method.

steepest descent for quadratic norm

we consider the quadratic norm

where p     sn

++. the normalized steepest descent direction is given by

kzkp = (zt p z)1/2 = kp 1/2zk2,
   xnsd =    (cid:0)   f (x)t p    1   f (x)(cid:1)   1/2
   xsd =    p    1   f (x).

p    1   f (x).

the dual norm is given by kzk    = kp    1/2zk2, so the steepest descent step with
respect to k    kp is given by

(9.25)

the normalized steepest descent direction for a quadratic norm is illustrated in
   gure 9.9.

9.4 steepest descent method

477

      f (x)

   xnsd

figure 9.9 normalized steepest descent direction for a quadratic norm. the
ellipsoid shown is the unit ball of the norm, translated to the point x. the
normalized steepest descent direction    xnsd at x extends as far as possible
in the direction       f (x) while staying in the ellipsoid. the gradient and
normalized steepest descent directions are shown.

interpretation via change of coordinates

we can give an interesting alternative interpretation of the steepest descent direc-
tion    xsd as the gradient search direction after a change of coordinates is applied
to the problem. de   ne   u = p 1/2u, so we have kukp = k  uk2. using this change
of coordinates, we can solve the original problem of minimizing f by solving the
equivalent problem of minimizing the function   f : rn     r, given by

  f (  u) = f (p    1/2   u) = f (u).

if we apply the gradient method to   f , the search direction at a point   x (which
corresponds to the point x = p    1/2   x for the original problem) is

     x =          f (  x) =    p    1/2   f (p    1/2   x) =    p    1/2   f (x).

this gradient search direction corresponds to the direction

   x = p    1/2(cid:16)   p    1/2   f (x)(cid:17) =    p    1   f (x)

for the original variable x.
in other words, the steepest descent method in the
quadratic norm k    kp can be thought of as the gradient method applied to the
problem after the change of coordinates   x = p 1/2x.

9.4.2 steepest descent for    1-norm

as another example, we consider the steepest descent method for the    1-norm. a
normalized steepest descent direction,

   xnsd = argmin{   f (x)t v | kvk1     1},

478

9 unconstrained minimization

      f (x)

   xnsd

figure 9.10 normalized steepest descent direction for the    1-norm. the
diamond is the unit ball of the    1-norm, translated to the point x. the
normalized steepest descent direction can always be chosen in the direction
of a standard basis vector; in this example we have    xnsd = e1.

is easily characterized. let i be any index for which k   f (x)k    = |(   f (x))i|. then
a normalized steepest descent direction    xnsd for the    1-norm is given by

   xnsd =    sign(cid:18)    f (x)

   xi (cid:19) ei,

where ei is the ith standard basis vector. an unnormalized steepest descent step
is then

   xsd =    xnsdk   f (x)k    =    

   f (x)
   xi

ei.

thus, the normalized steepest descent step in    1-norm can always be chosen to be a
standard basis vector (or a negative standard basis vector). it is the coordinate axis
direction along which the approximate decrease in f is greatest. this is illustrated
in    gure 9.10.

the steepest descent algorithm in the    1-norm has a very natural interpretation:
at each iteration we select a component of    f (x) with maximum absolute value,
and then decrease or increase the corresponding component of x, according to the
sign of (   f (x))i. the algorithm is sometimes called a coordinate-descent algorithm,
since only one component of the variable x is updated at each iteration. this can
greatly simplify, or even trivialize, the line search.

example 9.2 frobenius norm scaling. in   4.5.4 we encountered the unconstrained
geometric program

where m     rn  n is given, and the variable is d     rn. using the change of variables
xi = 2 log di we can express this geometric program in convex form as

i,j=1 m 2

ijd2

i /d2
j ,

minimize pn
f (x) = log(cid:16)pn

minimize

i,j=1 m 2

ijexi   xj(cid:17) .

9.4 steepest descent method

479

it is easy to minimize f one component at a time. keeping all components except
the kth    xed, we can write f (x) = log(  k +   ke   xk +   kexk ), where

  k = m 2

kk + xi,j6=k

m 2

ijexi   xj ,

  k =xi6=k

m 2

ikexi ,

  k =xj6=k

m 2

kje   xj .

the minimum of f (x), as a function of xk, is obtained for xk = log(  k/  k)/2. so
for this problem an exact line search can be carried out using a simple analytical
formula.

the    1-steepest descent algorithm with exact line search consists of repeating the
following steps.

1. compute the gradient

(   f (x))i =      ie   xi +   iexi

  i +   ie   xi +   iexi

,

i = 1, . . . , n.

2. select a largest (in absolute value) component of    f (x): |   f (x)|k = k   f (x)k   .
3. minimize f over the scalar variable xk, by setting xk = log(  k/  k)/2.

9.4.3 convergence analysis

in this section we extend the convergence analysis for the gradient method with
backtracking line search to the steepest descent method for an arbitrary norm. we
will use the fact that any norm can be bounded in terms of the euclidean norm,
so there exists constants   ,          (0, 1] such that

kxk       kxk2,

kxk            kxk2

(see   a.1.4).
again we assume f is strongly convex on the initial sublevel set s. the upper
bound    2f (x) (cid:22) m i implies an upper bound on the function f (x + t   xsd) as a
function of t:

f (x + t   xsd)     f (x) + t   f (x)t    xsd +
    f (x) + t   f (x)t    xsd +
m
2  2 t2k   f (x)k2
= f (x)     tk   f (x)k2
   .

mk   xsdk2
mk   xsdk2

    +

2  2

t2

t2

2

2

(9.26)

the step size   t =   2/m (which minimizes the quadratic upper bound (9.26))
satis   es the exit condition for the backtracking line search:

f (x +   t   xsd)     f (x)    

  2
2m k   f (x)k2

        f (x) +

    2
m    f (x)t    xsd

(9.27)

480

9 unconstrained minimization

since    < 1/2 and    f (x)t    xsd =    k   f (x)k2
step size t     min{1,     2/m}, and we have

   . the line search therefore returns a

f (x+) = f (x + t   xsd)     f (x)        min{1,     2/m}k   f (x)k2

   

    f (x)           2 min{1,     2/m}k   f (x)k2
2.

subtracting p    from both sides and using (9.9), we obtain

f (x+)     p        c(f (x)     p   ),

where

c = 1     2m      2 min{1,     2/m} < 1.

therefore we have

f (x(k))     p        ck(f (x(0))     p   ),
i.e., linear convergence exactly as in the gradient method.

9.4.4 discussion and examples

choice of norm for steepest descent

the choice of norm used to de   ne the steepest descent direction can have a dra-
matic e   ect on the convergence rate. for simplicity, we consider the case of steep-
est descent with quadratic p -norm. in   9.4.1, we showed that the steepest descent
method with quadratic p -norm is the same as the gradient method applied to the
problem after the change of coordinates   x = p 1/2x. we know that the gradient
method works well when the condition numbers of the sublevel sets (or the hes-
sian near the optimal point) are moderate, and works poorly when the condition
numbers are large. it follows that when the sublevel sets, after the change of coor-
dinates   x = p 1/2x, are moderately conditioned, the steepest descent method will
work well.

this observation provides a prescription for choosing p : it should be chosen
so that the sublevel sets of f , transformed by p    1/2, are well conditioned. for
example if an approximation   h of the hessian at the optimal point h(x   ) were
known, a very good choice of p would be p =   h, since the hessian of   f at the
optimum is then

  h    1/2   2f (x   )   h    1/2     i,

and so is likely to have a low condition number.

this same idea can be described without a change of coordinates. saying that
a sublevel set has low condition number after the change of coordinates   x = p 1/2x
is the same as saying that the ellipsoid

e = {x | xt p x     1}

approximates the shape of the sublevel set. (in other words, it gives a good ap-
proximation after appropriate scaling and translation.)

this dependence of the convergence rate on the choice of p can be viewed from
two sides. the optimist   s viewpoint is that for any problem, there is always a

9.4 steepest descent method

481

x(0)

x(1)

x(2)

figure 9.11 steepest descent method with a quadratic norm k    kp1 . the
ellipses are the boundaries of the norm balls {x | kx     x(k)kp1     1} at x(0)
and x(1).

choice of p for which the steepest descent method works very well. the challenge,
of course, is to    nd such a p . the pessimist   s viewpoint is that for any problem,
there are a huge number of choices of p for which steepest descent works very
poorly. in summary, we can say that the steepest descent method works well in
cases where we can identify a matrix p for which the transformed problem has
moderate condition number.

examples

in this section we illustrate some of these ideas using the nonquadratic problem in
r2 with objective function (9.20). we apply the steepest descent method to the
problem, using the two quadratic norms de   ned by

p1 =(cid:20) 2

0

0

8 (cid:21) ,

p2 =(cid:20) 8 0
0 2 (cid:21) .

in both cases we use a backtracking line search with    = 0.1 and    = 0.7.

figures 9.11 and 9.12 show the iterates for steepest descent with norm k  kp1 and
norm k  kp2 . figure 9.13 shows the error versus iteration number for both norms.
figure 9.13 shows that the choice of norm strongly in   uences the convergence.
with the norm k    kp1, convergence is a bit more rapid than the gradient method,
whereas with the norm k    kp2, convergence is far slower.
this can be explained by examining the problems after the changes of coor-
dinates   x = p 1/2
2 x, respectively. figures 9.14 and 9.15 show the
problems in the transformed coordinates. the change of variables associated with
p1 yields sublevel sets with modest condition number, so convergence is fast. the
change of variables associated with p2 yields sublevel sets that are more poorly
conditioned, which explains the slower convergence.

1 x and   x = p 1/2

482

9 unconstrained minimization

x(0)

x(2)

x(1)

figure 9.12 steepest descent method, with quadratic norm k    kp2 .

105

100

   
p
   

10   5

)
)
k
(
x
(
f

10   10

p2

p1

10   15
0

10

20
k

30

40

figure 9.13 error f (x(k))     p    versus iteration k, for the steepest descent
method with the quadratic norm k    kp1 and the quadratic norm k    kp2 .
convergence is rapid for the norm k    kp1 and very slow for k    kp2 .

9.4 steepest descent method

483

  x(0)

  x(1)

figure 9.14 the iterates of steepest descent with norm k    kp1 , after the
change of coordinates. this change of coordinates reduces the condition
number of the sublevel sets, and so speeds up convergence.

  x(0)

  x(1)

figure 9.15 the iterates of steepest descent with norm k    kp2 , after the
change of coordinates. this change of coordinates increases the condition
number of the sublevel sets, and so slows down convergence.

484

9 unconstrained minimization

(x, f (x))

(x +    xnt, f (x +    xnt))

bf

f

figure 9.16 the function f (shown solid) and its second-order approximation

bf at x (dashed). the newton step    xnt is what must be added to x to give
the minimizer of bf .

9.5 newton   s method

9.5.1 the newton step

for x     dom f , the vector

   xnt =       2f (x)   1   f (x)

is called the newton step (for f , at x). positive de   niteness of    2f (x) implies that

   f (x)t    xnt =       f (x)t   2f (x)   1   f (x) < 0

unless    f (x) = 0, so the newton step is a descent direction (unless x is optimal).
the newton step can be interpreted and motivated in several ways.

minimizer of second-order approximation

the second-order taylor approximation (or model) bf of f at x is
vt   2f (x)v,

1
2

bf (x + v) = f (x) +    f (x)t v +

which is a convex quadratic function of v, and is minimized when v =    xnt. thus,
the newton step    xnt is what should be added to the point x to minimize the
second-order approximation of f at x. this is illustrated in    gure 9.16.

this interpretation gives us some insight into the newton step. if the function
if the function f is
f is quadratic, then x +    xnt is the exact minimizer of f .
nearly quadratic, intuition suggests that x +    xnt should be a very good estimate
of the minimizer of f , i.e., x   . since f is twice di   erentiable, the quadratic model
of f will be very accurate when x is near x   . it follows that when x is near x   ,
the point x +    xnt should be a very good estimate of x   . we will see that this
intuition is correct.

(9.28)

9.5 newton   s method

485

x

x +    xnsd
x +    xnt

figure 9.17 the dashed lines are level curves of a convex function. the
ellipsoid shown (with solid line) is {x + v | vt   2f (x)v     1}. the arrow
shows       f (x), the id119 direction. the newton step    xnt is
the steepest descent direction in the norm k    k   2f (x). the    gure also shows
   xnsd, the normalized steepest descent direction for the same norm.

steepest descent direction in hessian norm

the newton step is also the steepest descent direction at x, for the quadratic norm
de   ned by the hessian    2f (x), i.e.,

kuk   2f (x) = (ut   2f (x)u)1/2.

this gives another insight into why the newton step should be a good search
direction, and a very good search direction when x is near x   .

recall from our discussion above that steepest descent, with quadratic norm
k    kp , converges very rapidly when the hessian, after the associated change of
coordinates, has small condition number. in particular, near x   , a very good choice
is p =    2f (x   ). when x is near x   , we have    2f (x)        2f (x   ), which explains
why the newton step is a very good choice of search direction. this is illustrated
in    gure 9.17.

solution of linearized optimality condition
if we linearize the optimality condition    f (x   ) = 0 near x we obtain

   f (x + v)        f (x) +    2f (x)v = 0,

which is a linear equation in v, with solution v =    xnt. so the newton step    xnt is
what must be added to x so that the linearized optimality condition holds. again,
this suggests that when x is near x    (so the optimality conditions almost hold),
the update x +    xnt should be a very good approximation of x   .

when n = 1, i.e., f : r     r, this interpretation is particularly simple. the
solution x    of the minimization problem is characterized by f    (x   ) = 0, i.e., it is

486

9 unconstrained minimization

bf    

f    

(x +    xnt, f    (x +    xnt))

(x, f    (x))

figure 9.18 the solid curve is the derivative f     of the function f shown in

   gure 9.16. bf     is the linear approximation of f     at x. the newton step    xnt
is the di   erence between the root of bf     and the point x.

the zero-crossing of the derivative f    , which is monotonically increasing since f is
convex. given our current approximation x of the solution, we form a    rst-order
taylor approximation of f     at x. the zero-crossing of this a   ne approximation is
then x +    xnt. this interpretation is illustrated in    gure 9.18.

a   ne invariance of the newton step

an important feature of the newton step is that it is independent of linear (or
a   ne) changes of coordinates. suppose t     rn  n is nonsingular, and de   ne
  f (y) = f (t y). then we have

      f (y) = t t   f (x),

   2   f (y) = t t   2f (x)t,

where x = t y. the newton step for   f at y is therefore

   ynt =    (cid:0)t t   2f (x)t(cid:1)   1(cid:0)t t   f (x)(cid:1)

=    t    1   2f (x)   1   f (x)
= t    1   xnt,

where    xnt is the newton step for f at x. hence the newton steps of f and   f are
related by the same linear transformation, and

x +    xnt = t (y +    ynt).

the newton decrement

the quantity

  (x) =(cid:0)   f (x)t   2f (x)   1   f (x)(cid:1)1/2

is called the newton decrement at x. we will see that the newton decrement
plays an important role in the analysis of newton   s method, and is also useful

9.5 newton   s method

487

as a stopping criterion. we can relate the newton decrement to the quantity

f (x)     inf y bf (y), where bf is the second-order approximation of f at x:

  (x)2.

f (x)     inf

y bf (y) = f (x)     bf (x +    xnt) =

thus,   2/2 is an estimate of f (x)     p   , based on the quadratic approximation of f
at x.

1
2

we can also express the newton decrement as

this shows that    is the norm of the newton step, in the quadratic norm de   ned
by the hessian, i.e., the norm

.

(9.29)

nt   2f (x)   xnt(cid:1)1/2
  (x) =(cid:0)   xt
kuk   2f (x) =(cid:0)ut   2f (x)u(cid:1)1/2
   f (x)t    xnt =      (x)2.

.

the newton decrement comes up in backtracking line search as well, since we have

(9.30)

this is the constant used in a backtracking line search, and can be interpreted as
the directional derivative of f at x in the direction of the newton step:

     (x)2 =    f (x)t    xnt =

d
dt

f (x +    xntt)(cid:12)(cid:12)(cid:12)(cid:12)t=0

.

finally, we note that the newton decrement is, like the newton step, a   ne in-
variant. in other words, the newton decrement of   f (y) = f (t y) at y, where t is
nonsingular, is the same as the newton decrement of f at x = t y.

9.5.2 newton   s method

newton   s method, as outlined below,
is sometimes called the damped newton
method or guarded id77, to distinguish it from the pure id77,
which uses a    xed step size t = 1.

algorithm 9.5 newton   s method.

given a starting point x     dom f , tolerance    > 0.
repeat

1. compute the newton step and decrement.

   xnt :=       2f (x)   1   f (x);

  2 :=    f (x)t   2f (x)   1   f (x).

2. stopping criterion. quit if   2/2       .
3. line search. choose step size t by backtracking line search.
4. update. x := x + t   xnt.

this is essentially the general descent method described in   9.2, using the new-
ton step as search direction. the only di   erence (which is very minor) is that the
stopping criterion is checked after computing the search direction, rather than after
the update.

488

9 unconstrained minimization

9.5.3 convergence analysis

we assume, as before, that f is twice continuously di   erentiable, and strongly
convex with constant m, i.e.,    2f (x) (cid:23) mi for x     s. we have seen that this also
implies that there exists an m > 0 such that    2f (x) (cid:22) m i for all x     s.
in addition, we assume that the hessian of f is lipschitz continuous on s with
constant l, i.e.,

k   2f (x)        2f (y)k2     lkx     yk2

(9.31)
for all x, y     s. the coe   cient l, which can be interpreted as a bound on the
third derivative of f , can be taken to be zero for a quadratic function. more
generally l measures how well f can be approximated by a quadratic model, so
we can expect the lipschitz constant l to play a critical role in the performance
of newton   s method. intuition suggests that newton   s method will work very well
for a function whose quadratic model varies slowly (i.e., has small l).

idea and outline of convergence proof

we    rst give the idea and outline of the convergence proof, and the main conclusion,
and then the details of the proof. we will show there are numbers    and    with
0 <        m2/l and    > 0 such that the following hold.

    if k   f (x(k))k2       , then

f (x(k+1))     f (x(k))          .

(9.32)

    if k   f (x(k))k2 <   , then the backtracking line search selects t(k) = 1 and

l

2m2k   f (x(k+1))k2    (cid:18) l

2m2k   f (x(k))k2(cid:19)2

.

(9.33)

let us analyze the implications of the second condition. suppose that it
is satis   ed for iteration k, i.e., k   f (x(k))k2 <   . since        m2/l, we have
k   f (x(k+1))k2 <   , i.e., the second condition is also satis   ed at iteration k + 1.
continuing recursively, we conclude that once the second condition holds, it will
hold for all future iterates, i.e., for all l     k, we have k   f (x(l))k2 <   . therefore
for all l     k, the algorithm takes a full newton step t = 1, and

l

2m2k   f (x(l+1))k2    (cid:18) l

2m2k   f (x(l))k2(cid:19)2

.

(9.34)

applying this inequality recursively, we    nd that for l     k,

l

2m2k   f (x(l))k2    (cid:18) l

2m2k   f (x(k))k2(cid:19)2l   k
l2 (cid:18) 1

2(cid:19)2l   k
   (cid:18) 1
2(cid:19)2l   k+1

1
2mk   f (x(l))k2

2    

2m3

.

,

(9.35)

and hence

f (x(l))     p       

9.5 newton   s method

489

this last inequality shows that convergence is extremely rapid once the second
condition is satis   ed. this phenomenon is called quadratic convergence. roughly
speaking, the inequality (9.35) means that, after a su   ciently large number of
iterations, the number of correct digits doubles at each iteration.

the iterations in newton   s method naturally fall into two stages. the second
stage, which occurs once the condition k   f (x)k2        holds, is called the quadrat-
ically convergent stage. we refer to the    rst stage as the damped newton phase,
because the algorithm can choose a step size t < 1. the quadratically convergent
stage is also called the pure newton phase, since in these iterations a step size t = 1
is always chosen.

now we can estimate the total complexity. first we derive an upper bound on
the number of iterations in the damped newton phase. since f decreases by at
least    at each iteration, the number of damped newton steps cannot exceed

f (x(0))     p   

  

,

since if it did, f would be less than p   , which is impossible.

we can bound the number of iterations in the quadratically convergent phase
using the inequality (9.35). it implies that we must have f (x)     p           after no
more than

log2 log2(  0/  )

iterations in the quadratically convergent phase, where   0 = 2m3/l2.

overall, then, the number of iterations until f (x)    p           is bounded above by
(9.36)

f (x(0))     p   

+ log2 log2(  0/  ).

  

the term log2 log2(  0/  ), which bounds the number of iterations in the quadrati-
cally convergent phase, grows extremely slowly with required accuracy   , and can
be considered a constant for practical purposes, say    ve or six. (six iterations of
the quadratically convergent stage gives an accuracy of about        5    10   20  0.)
required to minimize f is bounded above by
f (x(0))     p   

not quite accurately, then, we can say that the number of newton iterations

(9.37)

+ 6.

  

a more precise statement is that (9.37) is a bound on the number of iterations to
compute an extremely good approximation of the solution.

damped newton phase
we now establish the inequality (9.32). assume k   f (x)k2       . we    rst derive a
lower bound on the step size selected by the line search. strong convexity implies
that    2f (x) (cid:22) m i on s, and therefore

f (x + t   xnt)     f (x) + t   f (x)t    xnt +

mk   xntk2

2

2

t2

    f (x)     t  (x)2 +

m
2m

t2  (x)2,

490

9 unconstrained minimization

where we use (9.30) and

  (x)2 =    xt

nt   2f (x)   xnt     mk   xntk2
2.

the step size   t = m/m satis   es the exit condition of the line search, since

f (x +   t   xnt)     f (x)    

m
2m

  (x)2     f (x)         t  (x)2.

therefore the line search returns a step size t       m/m , resulting in a decrease of
the objective function

f (x+)     f (x)          t  (x)2

  (x)2

m
m
m
m 2k   f (x)k2

           
           
             2 m
m 2 ,

2

where we use

  (x)2 =    f (x)t   2f (x)   1   f (x)     (1/m )k   f (x)k2
2.

therefore, (9.32) is satis   ed with

   =       2 m
m 2 .

(9.38)

quadratically convergent phase
we now establish the inequality (9.33). assume k   f (x)k2 <   . we    rst show that
the backtracking line search selects unit steps, provided

       3(1     2  )

m2
l

.

by the lipschitz condition (9.31), we have, for t     0,

k   2f (x + t   xnt)        2f (x)k2     tlk   xntk2,

and therefore

nt(cid:0)   2f (x + t   xnt)        2f (x)(cid:1)    xnt(cid:12)(cid:12)     tlk   xntk3
(cid:12)(cid:12)   xt

2.

with   f (t) = f (x + t   xnt), we have   f       (t) =    xt
inequality above is

nt   2f (x + t   xnt)   xnt, so the

we will use this inequality to determine an upper bound on   f (t). we start with

|   f       (t)       f       (0)|     tlk   xntk3
2.

  f       (t)       f       (0) + tlk   xntk3

2       (x)2 + t

l
m3/2   (x)3,

9.5 newton   s method

491

2. we integrate the inequality

where we use   f       (0) =   (x)2 and   (x)2     mk   xntk2
to get
  f    (t)       f    (0) + t  (x)2 + t2 l
=      (x)2 + t  (x)2 + t2 l
using   f    (0) =      (x)2. we integrate once more to get

2m3/2   (x)3

2m3/2   (x)3,

  f (t)       f (0)     t  (x)2 + t2 1

2

  (x)2 + t3 l

6m3/2   (x)3.

finally, we take t = 1 to obtain

f (x +    xnt)     f (x)    

1
2

  (x)2 +

l

6m3/2   (x)3.

(9.39)

now suppose k   f (x)k2            3(1     2  )m2/l. by strong convexity, we have

and by (9.39) we have

  (x)     3(1     2  )m3/2/l,
f (x +    xnt)     f (x)       (x)2(cid:18) 1
    f (x)         (x)2
= f (x) +      f (x)t    xnt,

2    

l  (x)

6m3/2(cid:19)

which shows that the unit step t = 1 is accepted by the backtracking line search.
let us now examine the rate of convergence. applying the lipschitz condition,

we have

k   f (x+)k2 = k   f (x +    xnt)        f (x)        2f (x)   xntk2

= (cid:13)(cid:13)(cid:13)(cid:13)z 1

0 (cid:0)   2f (x + t   xnt)        2f (x)(cid:1)    xnt dt(cid:13)(cid:13)(cid:13)(cid:13)2

   
=

2

l
2 k   xntk2
l
2 k   2f (x)   1   f (x)k2
l
2m2k   f (x)k2
2,

2

   
i.e., the inequality (9.33).

in conclusion, the algorithm selects unit steps and satis   es the condition (9.33)

if k   f (x(k))k2 <   , where

   = min{1, 3(1     2  )}

m2
l

.

substituting this bound and (9.38) into (9.37), we    nd that the number of iterations
is bounded above by

6 +

m 2l2/m5

     min{1, 9(1     2  )2}

(f (x(0))     p   ).

(9.40)

492

9 unconstrained minimization

x(0)

x(1)

figure 9.19 newton   s method for the problem in r2, with objective f given
in (9.20), and backtracking line search parameters    = 0.1,    = 0.7. also
shown are the ellipsoids {x | kx   x(k)k   2f (x(k))     1} at the    rst two iterates.

9.5.4 examples

example in r2

we    rst apply newton   s method with backtracking line search on the test func-
tion (9.20), with line search parameters    = 0.1,    = 0.7. figure 9.19 shows the
newton iterates, and also the ellipsoids

{x | kx     x(k)k   2f (x(k))     1}

for the    rst two iterates k = 0, 1. the method works well because these ellipsoids
give good approximations of the shape of the sublevel sets.

figure 9.20 shows the error versus iteration number for the same example.
this plot shows that convergence to a very high accuracy is achieved in only    ve
iterations. quadratic convergence is clearly apparent: the last step reduces the
error from about 10   5 to 10   10.

example in r100

figure 9.21 shows the convergence of newton   s method with backtracking and exact
line search for a problem in r100. the objective function has the form (9.21), with
the same problem data and the same starting point as was used in    gure 9.6. the
plot for the backtracking line search shows that a very high accuracy is attained in
eight iterations. like the example in r2, quadratic convergence is clearly evident
after about the third iteration. the number of iterations in newton   s method
with exact line search is only one smaller than with a backtracking line search.
this is also typical. an exact line search usually gives a very small improvement in
convergence of newton   s method. figure 9.22 shows the step sizes for this example.
after two damped steps, the steps taken by the backtracking line search are all full,
i.e., t = 1.

experiments with the values of the backtracking parameters    and    reveal that
they have little e   ect on the performance of newton   s method, for this example

9.5 newton   s method

493

105

100

   
p
   

10   5

)
)
k
(
x
(
f

10   10

10   15
0

1

2

k

3

4

5

figure 9.20 error versus iteration k of newton   s method for the problem
in r2. convergence to a very high accuracy is achieved in    ve iterations.

105

100

   
p
   

10   5

)
)
k
(
x
(
f

10   10

10   15
0

backtracking l.s.

exact l.s.

2

4

k

6

8

10

figure 9.21 error versus iteration for newton   s method for the problem in
r100. the backtracking line search parameters are    = 0.01,    = 0.5. here
too convergence is extremely rapid: a very high accuracy is attained in only
seven or eight iterations. the convergence of newton   s method with exact
line search is only one iteration faster than with backtracking line search.

494

9 unconstrained minimization

2

1.5

1

0.5

)
k
(
t

e
z
i
s

p
e
t
s

exact l.s.

backtracking l.s.

0
0

2

4
k

6

8

figure 9.22 the step size t versus iteration for newton   s method with back-
tracking and exact line search, applied to the problem in r100. the back-
tracking line search takes one backtracking step in the    rst two iterations.
after the    rst two iterations it always selects t = 1.

(and others). with       xed at 0.01, and values of    varying between 0.2 and 1,
the number of iterations required varies between 8 and 12. with       xed at 0.5,
the number of iterations is 8, for all values of    between 0.005 and 0.5. for these
reasons, most practical implementations use a backtracking line search with a small
value of   , such as 0.01, and a larger value of   , such as 0.5.

example in r10000

in this last example we consider a larger problem, of the form

minimize    

nxi=1

log(1     x2

i )    

mxi=1

log(bi     at

i x)

with m = 100000 and n = 10000. the problem data ai are randomly generated
sparse vectors. figure 9.23 shows the convergence of newton   s method with back-
tracking line search, with parameters    = 0.01,    = 0.5. the performance is very
similar to the previous convergence plots. a linearly convergent initial phase of
about 13 iterations is followed by a quadratically convergent phase, that achieves
a very high accuracy in 4 or 5 more iterations.

a   ne invariance of newton   s method

a very important feature of newton   s method is that it is independent of linear
(or a   ne) changes of coordinates. let x(k) be the kth iterate of newton   s method,
applied to f : rn     r. suppose t     rn  n is nonsingular, and de   ne   f (y) =
f (t y). if we use newton   s method (with the same backtracking parameters) to

9.5 newton   s method

495

105

   
p
   

100

)
)
k
(
x
(
f

10   5

0

5

10
k

15

20

figure 9.23 error versus iteration of newton   s method,
for a problem
in r10000. a backtracking line search with parameters    = 0.01,    = 0.5 is
used. even for this large scale problem, newton   s method requires only 18
iterations to achieve very high accuracy.

minimize   f , starting from y(0) = t    1x(0), then we have

t y(k) = x(k)

for all k. in other words, newton   s method is the same: the iterates are related
by the same change of coordinates. even the stopping criterion is the same, since
the newton decrement for   f at y(k) is the same as the newton decrement for f at
x(k). this is in stark contrast to the gradient (or steepest descent) method, which
is strongly a   ected by changes of coordinates.

as an example, consider the family of problems given in (9.22), indexed by the
parameter   , which a   ects the condition number of the sublevel sets. we observed
(in    gures 9.7 and 9.8) that the gradient method slows to useless for values of   
smaller than 0.05 or larger than 20. in contrast, newton   s method (with    = 0.01,
   = 0.5) solves this problem (in fact, to a far higher accuracy) in nine iterations,
for all values of    between 10   10 and 1010.

in a real implementation, with    nite precision arithmetic, newton   s method is
not exactly independent of a   ne changes of coordinates, or the condition number
of the sublevel sets. but we can say that condition numbers ranging up to very
large values such as 1010 do not adversely a   ect a real implementation of newton   s
method. for the gradient method, a far smaller range of condition numbers can
be tolerated. while choice of coordinates (or condition number of sublevel sets) is
a    rst-order issue for gradient and steepest descent methods, it is a second-order
issue for newton   s method; its only e   ect is in the numerical id202 required
to compute the newton step.

496

9 unconstrained minimization

summary

newton   s method has several very strong advantages over gradient and steepest
descent methods:

    convergence of newton   s method is rapid in general, and quadratic near x   .
once the quadratic convergence phase is reached, at most six or so iterations
are required to produce a solution of very high accuracy.

    newton   s method is a   ne invariant. it is insensitive to the choice of coordi-

nates, or the condition number of the sublevel sets of the objective.

    newton   s method scales well with problem size. its performance on problems
in r10000 is similar to its performance on problems in r10, with only a modest
increase in the number of steps required.

    the good performance of newton   s method is not dependent on the choice
of algorithm parameters. in contrast, the choice of norm for steepest descent
plays a critical role in its performance.

the main disadvantage of newton   s method is the cost of forming and storing
the hessian, and the cost of computing the newton step, which requires solving
a set of linear equations. we will see in   9.7 that in many cases it is possible to
exploit problem structure to substantially reduce the cost of computing the newton
step.

another alternative is provided by a family of algorithms for unconstrained op-
timization called quasi-id77s. these methods require less computational
e   ort to form the search direction, but they share some of the strong advantages
of id77s, such as rapid convergence near x   . since quasi-newton meth-
ods are described in many books, and tangential to our main theme, we will not
consider them in this book.

9.6 self-concordance

there are two major shortcomings of the classical convergence analysis of newton   s
method given in   9.5.3. the    rst is a practical one: the resulting complexity
estimates involve the three constants m, m , and l, which are almost never known
in practice. as a result, the bound (9.40) on the number of newton steps required
is almost never known speci   cally, since it depends on three constants that are, in
general, not known. of course the convergence analysis and complexity estimate
are still conceptually useful.

the second shortcoming is that while newton   s method is a   nely invariant, the
classical analysis of newton   s method is very much dependent on the coordinate
system used. if we change coordinates the constants m, m , and l all change. if
for no reason other than aesthetic, we should seek an analysis of newton   s method
in
that is, like the method itself, independent of a   ne changes of coordinates.

9.6 self-concordance

497

other words, we seek an alternative to the assumptions

mi (cid:22)    2f (x) (cid:22) m i,

k   2f (x)        2f (y)k2     lkx     yk2,

that is independent of a   ne changes of coordinates, and also allows us to analyze
newton   s method.

a simple and elegant assumption that achieves this goal was discovered by
nesterov and nemirovski, who gave the name self-concordance to their condition.
self-concordant functions are important for several reasons.

    they include many of the logarithmic barrier functions that play an impor-
tant role in interior-point methods for solving id76 problems.

    the analysis of newton   s method for self-concordant functions does not de-

pend on any unknown constants.

    self-concordance is an a   ne-invariant property, i.e., if we apply a linear
transformation of variables to a self-concordant function, we obtain a self-
concordant function. therefore the complexity estimate that we obtain for
newton   s method applied to a self-concordant function is independent of
a   ne changes of coordinates.

9.6.1 de   nition and examples

self-concordant functions on r
we start by considering functions on r. a convex function f : r     r is self-
concordant if
(9.41)
for all x     dom f . since linear and (convex) quadratic functions have zero third
derivative, they are evidently self-concordant. some more interesting examples are
given below.

|f          (x)|     2f       (x)3/2

example 9.3 logarithm and id178.

    negative logarithm. the function f (x) =     log x is self-concordant. using

f       (x) = 1/x2, f          (x) =    2/x3, we    nd that
2/x3

|f          (x)|
2f       (x)3/2 =

2(1/x2)3/2 = 1,

so the de   ning inequality (9.41) holds with equality.

    negative id178 plus negative logarithm. the function f (x) = x log x     log x is

self-concordant. to verify this, we use

to obtain

f       (x) =

x + 1

x2

,

f          (x) =    

x + 2

x3

|f          (x)|
2f       (x)3/2 =

x + 2

2(x + 1)3/2 .

498

9 unconstrained minimization

the function on the righthand side is maximized on r+ by x = 0, where its
value is 1.

the negative id178 function by itself is not self-concordant; see exercise 11.13.

we should make two important remarks about the self-concordance de   ni-
tion (9.41). the    rst concerns the mysterious constant 2 that appears in the
de   nition. in fact, this constant is chosen for convenience, in order to simplify the
formulas later on; any other positive constant could be used instead. suppose, for
example, that the convex function f : r     r satis   es

where k is some positive constant. then the function   f (x) = (k2/4)f (x) satis   es

|f          (x)|     kf       (x)3/2

(9.42)

|   f          (x)| = (k2/4)|f          (x)|
    (k3/4)f       (x)3/2
= (k3/4)(cid:16)(4/k2)   f       (x)(cid:17)3/2

= 2   f       (x)3/2

and therefore is self-concordant. this shows that a function that satis   es (9.42)
for some positive k can be scaled to satisfy the standard self-concordance inequal-
ity (9.41). so what is important is that the third derivative of the function is
bounded by some multiple of the 3/2-power of its second derivative. by appropri-
ately scaling the function, we can change the multiple to the constant 2.

the second comment is a simple calculation that shows why self-concordance
is so important: it is a   ne invariant. suppose we de   ne the function   f by   f (y) =
f (ay + b), where a 6= 0. then   f is self-concordant if and only if f is. to see this,
we substitute

  f       (y) = a2f       (x),

  f          (y) = a3f          (x),

where x = ay + b,
2   f       (y)3/2, to obtain

into the self-concordance inequality for   f , i.e.,

|   f          (y)|    

|a3f          (x)|     2(a2f       (x))3/2,

which (after dividing by a3) is the self-concordance inequality for f . roughly
speaking, the self-concordance condition (9.41) is a way to limit the third derivative
of a function, in a way that is independent of a   ne coordinate changes.

self-concordant functions on rn
we now consider functions on rn with n > 1. we say a function f : rn     r
is self-concordant if it is self-concordant along every line in its domain, i.e., if the
function   f (t) = f (x + tv) is a self-concordant function of t for all x     dom f and
for all v.

9.6 self-concordance

499

9.6.2 self-concordant calculus

scaling and sum

self-concordance is preserved by scaling by a factor exceeding one: if f is self-
concordant and a     1, then af is self-concordant. self-concordance is also preserved
by addition: if f1, f2 are self-concordant, then f1 + f2 is self-concordant. to show
this, it is su   cient to consider functions f1, f2 : r     r. we have

1 (x) + f          
|f          

2 (x)|     |f          
    2(f       
    2(f       

1 (x)| + |f          
2 (x)|
1 (x)3/2 + f       
1 (x) + f       

2 (x))3/2.

2 (x)3/2)

in the last step we use the inequality

(u3/2 + v3/2)2/3     u + v,

which holds for u, v     0.
composition with a   ne function
if f : rn     r is self-concordant, and a     rn  m, b     rn, then f (ax + b) is
self-concordant.

example 9.4 log barrier for linear inequalities. the function

f (x) =    

mxi=1

log(bi     at

i x),

with dom f = {x | at
at
i x) is the composition of     log y with the a   ne transformation y = bi     at
hence self-concordant. therefore the sum is also self-concordant.

i x < bi, i = 1, . . . , m}, is self-concordant. each term     log(bi    
i x, and

example 9.5 log-determinant. the function f (x) =     log det x is self-concordant
++. to show this, we consider the function   f (t) = f (x + tv ), where
on dom f = sn
x     0 and v     sn. it can be expressed as

  f (t) =     log det(x 1/2(i + tx    1/2v x    1/2)x 1/2)
=     log det x     log det(i + tx    1/2v x    1/2)
=     log det x    

log(1 + t  i)

nxi=1

where   i are the eigenvalues of x    1/2v x    1/2. each term     log(1 + t  i) is a self-
concordant function of t, so the sum,   f , is self-concordant.
it follows that f is
self-concordant.

example 9.6 log of concave quadratic. the function

f (x) =     log(xt p x + qt x + r),

500

9 unconstrained minimization

where p        sn

+, is self-concordant on

dom f = {x | xt p x + qt x + r > 0}.

to show this, it su   ces to consider the case n = 1 (since by restricting f to a line,
the general case reduces to the n = 1 case). we can then express f as
f (x) =     log(px2 + qx + r) =     log (   p(x     a)(b     x))

where dom f = (a, b) (i.e., a and b are the roots of px2 +qx+r). using this expression
we have

which establishes self-concordance.

f (x) =     log(   p)     log(x     a)     log(b     x),

composition with logarithm
let g : r     r be a convex function with dom g = r++, and

g      (x)

|g         (x)|     3

x

(9.43)

for all x. then

f (x) =     log(   g(x))     log x

is self-concordant on {x | x > 0, g(x) < 0}. (for a proof, see exercise 9.14.)
the condition (9.43) is homogeneous and preserved under addition. it is sat-
is   ed by all (convex) quadratic functions, i.e., functions of the form ax2 + bx + c,
where a     0. therefore if (9.43) holds for a function g, then it holds for the function
g(x) + ax2 + bx + c, where a     0.

example 9.7 the following functions g satisfy the condition (9.43).

    g(x) =    xp for 0 < p     1.
    g(x) =     log x.
    g(x) = x log x.
    g(x) = xp for    1     p     0.
    g(x) = (ax + b)2/x.

it follows that in each case, the function f (x) =     log(   g(x))   log x is self-concordant.
more generally, the function f (x) =     log(   g(x)     ax2     bx     c)     log x is self-
concordant on its domain,

{x | x > 0, g(x) + ax2 + bx + c < 0},

provided a     0.

example 9.8 the composition with logarithm rule allows us to show self-concordance
of the following functions.

    f (x, y) =     log(y2     xt x) on {(x, y) | kxk2 < y}.
    f (x, y) =    2 log y     log(y2/p     x2), with p     1, on {(x, y)     r2 | |x|p < y}.
    f (x, y) =     log y     log(log y     x) on {(x, y) | ex < y}.

we leave the details as an exercise (exercise 9.15).

9.6 self-concordance

501

9.6.3 properties of self-concordant functions

in   9.1.2 we used strong convexity to derive bounds on the suboptimality of a point
x in terms of the norm of the gradient at x. for strictly convex self-concordant
functions, we can obtain similar bounds in terms of the newton decrement

  (x) =(cid:0)   f (x)t   2f (x)   1   f (x)(cid:1)1/2

.

(it can be shown that the hessian of a strictly convex self-concordant function is
positive de   nite everywhere; see exercise 9.17.) unlike the bounds based on the
norm of the gradient, the bounds based on the newton decrement are not a   ected
by an a   ne change of coordinates.

for future reference we note that the newton decrement can also be expressed

as

  (x) = sup
v6=0

   vt   f (x)

(vt   2f (x)v)1/2

(see exercise 9.9). in other words, we have

   vt   f (x)
(vt   2f (x)v)1/2       (x)

(9.44)

for any nonzero v, with equality for v =    xnt.

upper and lower bounds on second derivatives
suppose f : r     r is a strictly convex self-concordant function. we can write the
self-concordance inequality (9.41) as

for all t     dom f (see exercise 9.16). assuming t     0 and the interval between 0
and t is in dom f , we can integrate (9.45) between 0 and t to obtain

d

(cid:12)(cid:12)(cid:12)(cid:12)
   t    z t

dt(cid:16)f       (t)   1/2(cid:17)(cid:12)(cid:12)(cid:12)(cid:12)     1
d   (cid:16)f       (   )   1/2(cid:17) d       t,

d

0

i.e.,    t     f       (t)   1/2     f       (0)   1/2     t. from this we obtain lower and upper bounds
on f       (t):

f       (0)

(cid:0)1 + tf       (0)1/2(cid:1)2     f       (t)    

(cid:0)1     tf       (0)1/2(cid:1)2 .

f       (0)

the lower bound is valid for all nonnegative t     dom f ; the upper bound is valid
if t     dom f and 0     t < f       (0)   1/2.
bound on suboptimality
let f : rn     r be a strictly convex self-concordant function, and let v be a
descent direction (i.e., any direction satisfying vt   f (x) < 0, not necessarily the

(9.45)

(9.46)

502

9 unconstrained minimization

newton direction). de   ne   f : r     r as   f (t) = f (x + tv). by de   nition, the
function   f is self-concordant.

integrating the lower bound in (9.46) yields a lower bound on   f    (t):

  f    (t)       f    (0) +   f       (0)1/2    
integrating again yields a lower bound on   f (t):

  f       (0)1/2

1 + t   f       (0)1/2

.

  f (t)       f (0) + t   f    (0) + t   f       (0)1/2     log(1 + t   f       (0)1/2).

the righthand side reaches its minimum at

(9.47)

(9.48)

  t =

      f    (0)

  f       (0) +   f       (0)1/2   f    (0)

,

and evaluating at   t provides a lower bound on   f :

inf
t   0

  f (t)       f (0) +   t   f    (0) +   t   f       (0)1/2     log(1 +   t   f       (0)1/2)

=   f (0)       f    (0)   f       (0)   1/2 + log(1 +   f    (0)   f       (0)   1/2).

the inequality (9.44) can be expressed as

(with equality when v =    xnt), since we have

  (x)           f    (0)   f       (0)   1/2

  f    (0) = vt   f (x),

  f       (0) = vt   2f (x)v.

now using the fact that u + log(1     u) is a monotonically decreasing function of u,
and the inequality above, we get

  f (t)       f (0) +   (x) + log(1       (x)).
this inequality holds for any descent direction v. therefore

inf
t   0

(9.49)
provided   (x) < 1. the function     (   + log(1       )) is plotted in    gure 9.24. it
satis   es

p        f (x) +   (x) + log(1       (x))

for small   , and the bound

    (   + log(1       ))       2/2,

for        0.68. thus, we have the bound on suboptimality

    (   + log(1       ))       2

p        f (x)       (x)2,

(9.50)

valid for   (x)     0.68.
recall that   (x)2/2 is the estimate of f (x)     p   , based on the quadratic model
at x; the inequality (9.50) shows that for self-concordant functions, doubling this
estimate gives us a provable bound. in particular, it shows that for self-concordant
functions, we can use the stopping criterion

(where    < 0.682), and guarantee that on exit f (x)     p          .

  (x)2       ,

9.6 self-concordance

503

0.8

0.6

0.4

0.2

0
0

0.2

0.4

0.6

0.8

1

figure 9.24 the solid line is the function    (  +log(1     )), which for small   
is approximately   2/2. the dashed line shows   2, which is an upper bound
in the interval 0            0.68.

9.6.4 analysis of newton   s method for self-concordant functions

we now analyze newton   s method with backtracking line search, when applied to
a strictly convex self-concordant function f . as before, we assume that a starting
point x(0) is known, and that the sublevel set s = {x | f (x)     f (x(0))} is closed.
we also assume that f is bounded below. (this implies that f has a minimizer x   ;
see exercise 9.19.)

the analysis is very similar to the classical analysis given in   9.5.2, except that
we use self-concordance as the basic assumption instead of strong convexity and
the lipschitz condition on the hessian, and the newton decrement will play the
role of the norm of the gradient. we will show that there are numbers    and    > 0,
with 0 <        1/4, that depend only on the line search parameters    and   , such
that the following hold:
    if   (x(k)) >   , then

    if   (x(k))       , then the backtracking line search selects t = 1 and

f (x(k+1))     f (x(k))          .
2  (x(k+1))    (cid:16)2  (x(k))(cid:17)2

.

(9.51)

(9.52)

these are the analogs of (9.32) and (9.33). as in   9.5.3, the second condition can
be applied recursively, so we can conclude that for all l     k, we have   (x(l))       ,
and

as a consequence, for all l     k,

2  (x(l))    (cid:16)2  (x(k))(cid:17)2l   k
    (2  )2l   k
4(cid:18) 1
2(cid:19)2l   k+1

f (x(l))     p          (x(l))2    

1

.

   (cid:18) 1
   (cid:18) 1

2(cid:19)2l   k
2(cid:19)2l   k+1

,

504

9 unconstrained minimization

and hence f (x(l))     p           if l     k     log2 log2(1/  ).
the    rst inequality implies that the damped phase cannot require more than
(f (x(0))     p   )/   steps. thus the total number of iterations required to obtain an
accuracy f (x)     p          , starting at a point x(0), is bounded by

f (x(0))     p   

  

+ log2 log2(1/  ).

(9.53)

this is the analog of the bound (9.36) in the classical analysis of newton   s method.

damped newton phase
let   f (t) = f (x + t   xnt), so we have

  f    (0) =      (x)2,

  f       (0) =   (x)2.

if we integrate the upper bound in (9.46) twice, we obtain an upper bound for   f (t):

  f (t)       f (0) + t   f    (0)     t   f       (0)1/2     log(cid:16)1     t   f       (0)1/2(cid:17)

=   f (0)     t  (x)2     t  (x)     log(1     t  (x)),

(9.54)

valid for 0     t < 1/  (x).
we can use this bound to show the backtracking line search always results in a
step size t       /(1 +   (x)). to prove this we note that the point   t = 1/(1 +   (x))
satis   es the exit condition of the line search:

  f (  t)       f (0)       t  (x)2       t  (x)     log(1       t  (x))

=   f (0)       (x) + log(1 +   (x))
      f (0)       
1 +   (x)
=   f (0)         (x)2  t.

  (x)2

the second inequality follows from the fact that

   x + log(1 + x) +

x2

2(1 + x)     0

for x     0. since t       /(1 +   (x)), we have

so (9.51) holds with

  f (t)       f (0)            

  (x)2

1 +   (x)

,

   =     

  2

1 +   

.

9.6 self-concordance

505

quadratically convergent phase

we will show that we can take

   = (1     2  )/4,

(which satis   es 0 <    < 1/4, since 0 <    < 1/2), i.e., if   (x(k))     (1     2  )/4, then
the backtracking line search accepts the unit step and (9.52) holds.
we    rst note that the upper bound (9.54) implies that a unit step t = 1 yields a
point in dom f if   (x) < 1. moreover, if   (x)     (1     2  )/2, we have, using (9.54),

  f (1)       f (0)       (x)2       (x)     log(1       (x))

  (x)2 +   (x)3

1
2

      f (0)    
      f (0)         (x)2,

the inequality (9.52) follows from the following fact, proved in exercise 9.18. if

so the unit step satis   es the condition of su   cient decrease.
follows from the fact that    x     log(1     x)     1
  (x) < 1, and x+ = x        2f (x)   1   f (x), then
  (x)2

2 x2 + x3 for 0     x     0.81.)

(the second line

(9.55)

  (x+)    

(1       (x))2 .

in particular, if   (x)     1/4,

  (x+)     2  (x)2,
which proves that (9.52) holds when   (x(k))       .
the    nal complexity bound

putting it all together, the bound (9.53) on the number of newton iterations be-
comes

f (x(0))     p   

  

+ log2 log2(1/  ) =

20     8  
    (1     2  )2 (f (x(0))    p   ) + log2 log2(1/  ). (9.56)

this expression depends only on the line search parameters    and   , and the    nal
accuracy   . moreover the term involving    can be safely replaced by the constant
six, so the bound really depends only on    and   . for typical values of    and   , the
constant that scales f (x(0))     p    is on the order of several hundred. for example,
with    = 0.1,    = 0.8, the scaling factor is 375. with tolerance    = 10   10, we
obtain the bound

375(f (x(0))     p   ) + 6.

(9.57)

we will see that this bound is fairly conservative, but does capture what appears
to be the general form of the worst-case number of newton steps required. a more
re   ned analysis, such as the one originally given by nesterov and nemirovski, gives
a similar bound, with a substantially smaller constant scaling f (x(0))     p   .

506

9 unconstrained minimization

s
n
o
i
t
a
r
e
t
i

25

20

15

10

5

0
0

5

10

20
15
f (x(0))     p   

25

30

35

figure 9.25 number of newton iterations required to minimize self-
concordant functions versus f (x(0))     p   . the function f has the form
i x), where the problem data ai and b are ran-
domly generated. the circles show problems with m = 100, n = 50; the
squares show problems with m = 1000, n = 500; and the diamonds show
problems with m = 1000, n = 50. fifty instances of each are shown.

f (x) =    pm

i=1 log(bi     at

9.6.5 discussion and numerical examples

a family of self-concordant functions

it is interesting to compare the upper bound (9.57) with the actual number of
iterations required to minimize a self-concordant function. we consider a family of
problems of the form

f (x) =    

mxi=1

log(bi     at

i x).

the problem data ai and b were generated as follows. for each problem instance,
the coe   cients of ai were generated from independent normal distributions with
mean zero and unit variance, and the coe   cients b were generated from a uniform
distribution on [0, 1]. problem instances which were unbounded below were dis-
carded. for each problem we    rst compute x   . we then generate a starting point
by choosing a random direction v, and taking x(0) = x    + sv, where s is chosen so
that f (x(0))     p    has a prescribed value between 0 and 35. (we should point out
that starting points with values f (x(0))     p    = 10 or higher are actually very close
to the boundary of the polyhedron.) we then minimize the function using new-
ton   s method with a backtracking line search with parameters    = 0.1,    = 0.8,
and tolerance    = 10   10.

figure 9.25 shows the number of newton iterations required versus f (x(0))    p   
for 150 problem instances. the circles show 50 problems with m = 100, n = 50;
the squares show 50 problems with m = 1000, n = 500; and the diamonds show 50
problems with m = 1000, n = 50.

9.6 self-concordance

507

for the values of the backtracking parameters used, the complexity bound found

above is

375(f (x(0))     p   ) + 6,

(9.58)

clearly a much larger value than the number of iterations required (for these 150
instances). the plot suggests that there is a valid bound of the same form, but
with a much smaller constant (say, around 1.5) scaling f (x(0))     p   . indeed, the
expression

f (x(0))     p    + 6

is not a bad gross predictor of the number of newton steps required, although it is
clearly not the only factor. first, there are plenty of problems instances where the
number of newton steps is somewhat smaller, which correspond, we can guess, to
   lucky    starting points. note also that for the larger problems, with 500 variables
(represented by the squares), there seem to be even more cases where the number
of newton steps is unusually small.

we should mention here that the problem family we study is not just self-
concordant, but in fact minimally self-concordant, by which we mean that   f
is not self-concordant for    < 1. hence, the bound (9.58) cannot be improved
(the function f (x) =    20 log x is an example of a self-
by simply scaling f .
concordant function which is not minimally self-concordant, since (1/20)f is also
self-concordant.)

practical importance of self-concordance

we have already observed that newton   s method works in general very well for
strongly convex objective functions. we can justify this vague statement empir-
ically, and also using the classical analysis of newton   s method, which yields a
complexity bound, but one that depends on several constants that are almost al-
ways unknown.

for self-concordant functions we can say somewhat more. we have a complexity
bound that is completely explicit, and does not depend on any unknown constants.
empirical studies suggest that this bound can be tightened considerably, but its
general form, a small constant plus a multiple of f (x(0))     p   , seems to predict, at
least crudely, the number of newton steps required to minimize an approximately
minimally self-concordant function.

it is not yet clear whether self-concordant functions are in practice more easily
minimized by newton   s method than non-self-concordant functions.
(it is not
even clear how one would make this statement precise.) at the moment, we can
say that self-concordant functions are a class of functions for which we can say
considerably more about the complexity of newton   s method than is the case for
non-self-concordant functions.

508

9 unconstrained minimization

9.7 implementation

in this section we discuss some of the issues that arise in implementing an un-
constrained minimization algorithm. we refer the reader to appendix c for more
details on numerical id202.

9.7.1 pre-computation for line searches

in the simplest implementation of a line search, f (x + t   x) is evaluated for each
value of t in the same way that f (z) is evaluated for any z     dom f . but in some
cases we can exploit the fact that f (and its derivatives, in an exact line search) are
to be evaluated at many points along the ray {x + t   x | t     0} to reduce the total
computational e   ort. this usually requires some pre-computation, which is often
on the same order as computing f at any point, after which f (and its derivatives)
can be computed more e   ciently along the ray.

suppose that x     dom f and    x     rn, and de   ne   f as f restricted to the line
or ray determined by x and    x, i.e.,   f (t) = f (x + t   x). in a backtracking line
search we must evaluate   f for several, and possibly many, values of t; in an exact
line search method we must evaluate   f and one or more derivatives at a number of
values of t. in the simple method described above, we evaluate   f (t) by    rst forming
z = x + t   x, and then evaluating f (z). to evaluate   f    (t), we form z = x + t   x,
then evaluate    f (z), and then compute   f    (t) =    f (z)t    x. in some representative
examples below we show how   f can be computed at a number of values of t more
e   ciently.

composition with an a   ne function

a very general case in which pre-computation can speed up the line search process
occurs when the objective has the form f (x) =   (ax + b), where a     rp  n, and   
is easy to evaluate (for example, separable). to evaluate   f (t) = f (x + t   x) for k
values of t using the simple approach, we form a(x + t   x) + b for each value of t
(which costs 2kpn    ops), and then evaluate   (a(x + t   x) + b) for each value of t.
this can be done more e   ciently by    rst computing ax + b and a   x (4pn    ops),
then forming a(x + t   x) + b for each value of t using

a(x + t   x) + b = (ax + b) + t(a   x),

which costs 2kp    ops. the total cost, keeping only the dominant terms, is 4pn+2kp
   ops, compared to 2kpn for the simple method.

analytic center of a linear matrix inequality

here we give an example that is more speci   c, and more complete. we consider
the problem (9.6) of computing the analytic center of a linear matrix inequality,
i.e., minimizing log det f (x)   1, where x     rn and f : rn     sp is a   ne. along
the line through x with direction    x we have

  f (t) = log det(f (x + t   x))   1 =     log det(a + tb)

509

9.7

implementation

where

a = f (x),

b =    x1f1 +        +    xnfn     sp.

since a     0, it has a cholesky factorization a = llt , where l is lower triangular
and nonsingular. therefore we can express   f as

  f (t) =     log det(cid:0)l(i + tl   1bl   t )lt(cid:1) =     log det a    

pxi=1

log(1 + t  i)

(9.59)

where   1, . . . ,   p are the eigenvalues of l   1bl   t . once these eigenvalues are
computed, we can evaluate   f (t), for any t, with 4p simple arithmetic computations,
by using the formula on the right hand side of (9.59). we can evaluate   f    (t) (and
similarly, any higher derivative) in 4p operations, using the formula

  f    (t) =    

  i

1 + t  i

.

pxi=1

let us compare the two methods for carrying out a line search, assuming that
we need to evaluate f (x + t   x) for k values of t. in the simple method, for each
value of t we form f (x+t   x), and then evaluate f (x+t   x) as     log det f (x+t   x).
for example, we can    nd the cholesky factorization of f (x + t   x) = llt , and
then evaluate

    log det f (x + t   x) =    2

log lii.

pxi=1

the cost is np2 to form f (x + t   x), plus (1/3)p3 for the cholesky factorization.
therefore the total cost of the line search is

k(np2 + (1/3)p3) = knp2 + (1/3)kp3.

using the method outlined above, we    rst form a, which costs np2, and factor
it, which costs (1/3)p3. we also form b (which costs np2), and l   1bl   t , which
costs 2p3. the eigenvalues of this matrix are then computed, at a cost of about
(4/3)p3    ops. this pre-computation requires a total of 2np2 + (11/3)p3    ops. after
   nishing this pre-computation, we can now evaluate   f (t) for each value of t at a
cost of 4p    ops. the total cost is then

2np2 + (11/3)p3 + 4kp.

assuming k is small compared to p(2n + (11/3)p), this means the entire line search
can be carried out at an e   ort comparable to simply evaluating f . depending on
the values of k, p, and n, the savings over the simple method can be as large as
order k.

9.7.2 computing the newton step

in this section we brie   y describe some of the issues that arise in implementing
newton   s method. in most cases, the work of computing the newton step    xnt

510

9 unconstrained minimization

dominates the work involved in the line search. to compute the newton step
   xnt, we    rst evaluate and form the hessian matrix h =    2f (x) and the gradient
g =    f (x) at x. then we solve the system of linear equations h   xnt =    g to
   nd the newton step. this set of equations is sometimes called the newton system
(since its solution gives the newton step) or the normal equations, since the same
type of equation arises in solving a least-squares problem (see   9.1.1).
while a general linear equation solver can be used, it is better to use methods
that take advantage of the symmetry and positive de   niteness of h. the most
common approach is to form the cholesky factorization of h, i.e., to compute a
lower triangular matrix l that satis   es llt = h (see   c.3.2). we then solve lw =
   g by forward substitution, to obtain w =    l   1g, and then solve lt    xnt = w by
back substitution, to obtain

   xnt = l   t w =    l   t l   1g =    h    1g.

we can compute the newton decrement as   2 =       xt
2 = kwk2
2.

  2 = gt h    1g = kl   1gk2

ntg, or use the formula

if a dense (unstructured) cholesky factorization is used, the cost of the forward and
back substitution is dominated by the cost of the cholesky factorization, which is
(1/3)n3    ops. the total cost of computing the newton step    xnt is thus f +(1/3)n3
   ops, where f is the cost of forming h and g.

it is often possible to solve the newton system h   xnt =    g more e   ciently,
by exploiting special structure in h, such as band structure or sparsity. in this
context,    structure of h    means structure that is the same for all x. for example,
when we say that    h is tridiagonal    we mean that for every x     dom f ,    2f (x) is
tridiagonal.

band structure
if the hessian h is banded with bandwidth k, i.e., hij = 0 for |i    j| > k, then the
banded cholesky factorization can be used, as well as banded forward and back
substitutions. the cost of computing the newton step    xnt =    h    1g is then
f + nk2    ops (assuming k     n), compared to f + (1/3)n3 for a dense factorization
and substitution method.

the hessian band structure condition

   2f (x)ij =

   2f (x)
   xi   xj

= 0

for

|i     j| > k,

for all x     dom f , has an interesting interpretation in terms of the objective
function f . roughly speaking it means that in the objective function, each variable
xi couples nonlinearly only to the 2k + 1 variables xj, j = i     k, . . . , i + k. this
occurs when f has the partial separability form

f (x) =   1(x1, . . . , xk+1) +   2(x2, . . . , xk+2) +        +   n   k(xn   k, . . . , xn),

where   i : rk+1     r. in other words, f can be expressed as a sum of functions
of k consecutive variables.

9.7

implementation

511

example 9.9 consider the problem of minimizing f : rn     r, which has the form

f (x) =   1(x1, x2) +   2(x2, x3) +        +   n   1(xn   1, xn),

where   i : r2     r are convex and twice di   erentiable. because of this form, the
hessian    2f is tridiagonal, since    2f /   xi   xj = 0 for |i     j| > 1. (and conversely, if
the hessian of a function is tridiagonal for all x, then it has this form.)

using cholesky factorization and forward and back substitution algorithms for tridi-
agonal matrices, we can solve the newton system for this problem in order n    ops.
this should be compared to order n3    ops, if the special form of f were not exploited.

sparse structure

more generally we can exploit sparsity of the hessian h in solving the newton
system. this sparse structure occurs whenever each variable xi is nonlinearly
coupled (in the objective) to only a few other variables, or equivalently, when the
objective function can be expressed as a sum of functions, each depending on only
a few variables, and each variable appearing in only a few of these functions.

to solve h   x =    g when h is sparse, a sparse cholesky factorization is used

to compute a permutation matrix p and lower triangular matrix l for which

h = p llt p t .

the cost of this factorization depends on the particular sparsity pattern, but is
often far smaller than (1/3)n3, and an empirical complexity of order n (for large
n) is not uncommon. the forward and back substitution are very similar to the
basic method without the permutation. we solve lw =    p t g using forward
substitution, and then solve lt v = w by back substitution to obtain

v = l   t w =    l   t l   1p t g.

the newton step is then    x = p v.

since the sparsity pattern of h does not change as x varies (or more precisely,
since we only exploit sparsity that does not change with x) we can use the same
permutation matrix p for each of the newton steps. the step of determining a
good permutation matrix p , which is called the symbolic factorization step, can be
done once, for the whole newton process.

diagonal plus low rank

there are many other types of structure that can be exploited in solving the new-
ton system h   xnt =    g. here we brie   y describe one, and refer the reader to
appendix c for more details. suppose the hessian h can be expressed as a diago-
nal matrix plus one of low rank, say, p. this occurs when the objective function f
has the special form

f (x) =

nxi=1

  i(xi) +   0(ax + b)

(9.60)

512

9 unconstrained minimization

where a     rp  n,   1, . . . ,   n : r     r, and   0 : rp     r.
in other words, f
is a separable function, plus a function that depends on a low dimensional a   ne
function of x.

to    nd the newton step    xnt for (9.60) we must solve the newton system

h   xnt =    g, with

h = d + at h0a.
n(xn)) is diagonal, and h0 =    2  0(ax + b) is the
here d = diag(        
hessian of   0. if we compute the newton step without exploiting the structure,
the cost of solving the newton system is (1/3)n3    ops.

1 (x1), . . . ,         

let h0 = l0lt

rary variable w = lt

0 be the cholesky factorization of h0. we introduce the tempo-
0 a   xnt     rp, and express the newton system as
d   xnt + at l0w =    g,

0 a   xnt.

w = lt

substituting    xnt =    d   1(at l0w + g) (from the    rst equation) into the second
equation, we obtain

(i + lt

0 ad   1at l0)w =    lt

0 ad   1g,

(9.61)

which is a system of p linear equations.

now we proceed as follows to compute the newton step    xnt. first we compute
the cholesky factorization of h0, which costs (1/3)p3. we then form the dense,
positive de   nite symmetric matrix appearing on the lefthand side of (9.61), which
costs 2p2n. we then solve (9.61) for w using a cholesky factorization and a back and
forward substitution, which costs (1/3)p3    ops. finally, we compute    xnt using
   xnt =    d   1(at l0w + g), which costs 2np    ops. the total cost of computing
   xnt is (keeping only the dominant term) 2p2n    ops, which is far smaller than
(1/3)n3 for p     n.

bibliography

bibliography

513

dennis and schnabel [ds96] and ortega and rheinboldt [or00] are two standard refer-
ences on algorithms for unconstrained minimization and nonlinear equations. the result
on quadratic convergence, assuming strong convexity and lipschitz continuity of the hes-
sian, is attributed to kantorovich [kan52]. polyak [pol87,   1.6] gives some insightful
comments on the role of convergence results that involve unknown constants, such as the
results derived in   9.5.3.
self-concordant functions were introduced by nesterov and nemirovski [nn94]. all our
results in   9.6 and exercises 9.14   9.20 can be found in their book, although often in a
more general form or with di   erent notation. renegar [ren01] gives a concise and elegant
presentation of self-concordant functions and their role in the analysis of primal-dual
interior-point algorithms. peng, roos, and terlaky [prt02] study interior-point methods
from the viewpoint of self-regular functions, a class of functions that is similar, but not
identical, to self-concordant functions.
references for the material in   9.7 are given at the end of appendix c.

514

9 unconstrained minimization

exercises

unconstrained minimization

9.1 minimizing a quadratic function.

consider the problem of minimizing a quadratic

function:

minimize

f (x) = (1/2)xt p x + qt x + r,

where p     sn (but we do not assume p (cid:23) 0).
(a) show that if p 6(cid:23) 0, i.e., the objective function f is not convex, then the problem is
(b) now suppose that p (cid:23) 0 (so the objective function is convex), but the optimality
condition p x    =    q does not have a solution. show that the problem is unbounded
below.

unbounded below.

9.2 minimizing a quadratic-over-linear fractional function. consider the problem of minimiz-

ing the function f : rn     r, de   ned as

f (x) = kax     bk2
ct x + d

2

,

dom f = {x | ct x + d > 0}.

we assume rank a = n and b 6    r(a).
(a) show that f is closed.
(b) show that the minimizer x    of f is given by

x    = x1 + tx2

where x1 = (at a)   1at b, x2 = (at a)   1c, and t     r can be calculated by solving
a quadratic equation.

9.3 initial point and sublevel set condition. consider the function f (x) = x2

1 + x2

2 with domain

dom f = {(x1, x2) | x1 > 1}.
(a) what is p   ?
(b) draw the sublevel set s = {x | f (x)     f (x(0))} for x(0) = (2, 2). is the sublevel set
(c) what happens if we apply the gradient method with backtracking line search, start-

s closed? is f strongly convex on s?

ing at x(0)? does f (x(k)) converge to p   ?

9.4 do you agree with the following argument? the    1-norm of a vector x     rm can be

expressed as

kxk1 = (1/2) inf

y   0  mxi=1

i /yi + 1t y! .

x2

therefore the    1-norm approximation problem

minimize

kax     bk1

is equivalent to the minimization problem

f (x, y) =pm

minimize

i=1(at

i x     bi)2/yi + 1t y,

(9.62)

with dom f = {(x, y)     rn    rm | y     0}, where at
is the ith row of a. since f is twice
di   erentiable and convex, we can solve the    1-norm approximation problem by applying
newton   s method to (9.62).

i

9.5 backtracking line search. suppose f is strongly convex with mi (cid:22)    2f (x) (cid:22) m i. let
   x be a descent direction at x. show that the backtracking stopping condition holds for

use this to give an upper bound on the number of backtracking iterations.

0 < t           f (x)t    x
mk   xk2

2

.

exercises

515

gradient and steepest descent methods

9.6 quadratic problem in r2. verify the expressions for the iterates x(k) in the    rst example

of   9.3.2.

9.7 let    xnsd and    xsd be the normalized and unnormalized steepest descent directions at

x, for the norm k    k. prove the following identities.
(a)    f (x)t    xnsd =    k   f (x)k   .
(b)    f (x)t    xsd =    k   f (x)k2
   .
(c)    xsd = argminv(   f (x)t v + (1/2)kvk2).

9.8 steepest descent method in       -norm. explain how to    nd a steepest descent direction in

the       -norm, and give a simple interpretation.

newton   s method

9.9 newton decrement. show that the newton decrement   (x) satis   es

  (x) =

sup

vt    2f (x)v=1

(   vt   f (x)) = sup

v6=0

   vt   f (x)
(vt   2f (x)v)1/2 .

9.10 the pure id77. newton   s method with    xed step size t = 1 can diverge if the

initial point is not close to x   . in this problem we consider two examples.

(a) f (x) = log(ex + e   x) has a unique minimizer x    = 0. run newton   s method with

   xed step size t = 1, starting at x(0) = 1 and at x(0) = 1.1.

(b) f (x) =     log x + x has a unique minimizer x    = 1. run newton   s method with    xed

step size t = 1, starting at x(0) = 3.

plot f and f    , and show the    rst few iterates.

9.11 gradient and id77s for composition functions. suppose    : r     r is increasing
and convex, and f : rn     r is convex, so g(x) =   (f (x)) is convex. (we assume that
f and g are twice di   erentiable.) the problems of minimizing f and minimizing g are
clearly equivalent.
compare the gradient method and newton   s method, applied to f and g. how are the
search directions related? how are the methods related if an exact line search is used?
hint. use the matrix inversion lemma (see   c.4.3).
9.12 trust region id77. if    2f (x) is singular (or very ill-conditioned), the newton
step    xnt =       2f (x)   1   f (x) is not well de   ned. instead we can de   ne a search direction
   xtr as the solution of

minimize
subject to

(1/2)vt hv + gt v
kvk2       ,

where h =    2f (x), g =    f (x), and    is a positive constant. the point x+   xtr minimizes
the second-order approximation of f at x, subject to the constraint that k(x+   xtr)   xk2    
  . the set {v | kvk2       } is called the trust region. the parameter   , the size of the trust
region, re   ects our con   dence in the second-order model.
show that    xtr minimizes

(1/2)vt hv + gt v +     kvk2
2,

for some     . this quadratic function can be interpreted as a regularized quadratic model
for f around x.

516

9 unconstrained minimization

self-concordance

9.13 self-concordance and the inverse barrier.

(a) show that f (x) = 1/x with domain (0, 8/9) is self-concordant.

(b) show that the function

f (x) =   

mxi=1

1

bi     at
i x

with dom f = {x     rn | at
bounded and

i x < bi, i = 1, . . . , m}, is self-concordant if dom f is

   > (9/8) max

i=1,...,m

sup

x   dom f

(bi     at

i x).

9.14 composition with logarithm. let g : r     r be a convex function with dom g = r++,

and

|g         (x)|     3

g      (x)

x

for all x. prove that f (x) =     log(   g(x))     log x is self-concordant on {x | x > 0, g(x) <
0}. hint. use the inequality

rp2 + q3 +

3
2

3
2

p2q + r3     1

9.15 prove that the following functions are self-concordant. in your proof, restrict the function

which holds for p, q, r     r+ with p2 + q2 + r2 = 1.
to a line, and apply the composition with logarithm rule.
(a) f (x, y) =     log(y2     xt x) on {(x, y) | kxk2 < y}.
(b) f (x, y) =    2 log y     log(y2/p     x2), with p     1, on {(x, y)     r2 | |x|p < y}.
(c) f (x, y) =     log y     log(log y     x) on {(x, y) | ex < y}.

9.16 let f : r     r be a self-concordant function.

(a) suppose f       (x) 6= 0. show that the self-concordance condition (9.41) can be ex-

pressed as

(cid:12)(cid:12)(cid:12)

d

dx(cid:0)f       (x)   1/2(cid:1)(cid:12)(cid:12)(cid:12)     1.

find the    extreme    self-concordant functions of one variable, i.e., the functions f
and   f that satisfy

d

dx(cid:0)f       (x)   1/2(cid:1) = 1,

d

dx(cid:0)   f       (x)   1/2(cid:1) =    1,

respectively.

(b) show that either f       (x) = 0 for all x     dom f , or f       (x) > 0 for all x     dom f .

9.17 upper and lower bounds on the hessian of a self-concordant function.

(a) let f : r2     r be a self-concordant function. show that

,

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)

   3f (x)

   x2

   3xi (cid:12)(cid:12)(cid:12)(cid:12)     2(cid:18)    2f (x)
i (cid:19)3/2
i    xj(cid:12)(cid:12)(cid:12)(cid:12)     2
i (cid:18)    2f (x)

   2f (x)

   x2

   x2

   3f (x)
   x2

j (cid:19)1/2

i = 1, 2,

,

i 6= j

for all x     dom f .

exercises

517

hint. if h : r2    r2    r2     r is a symmetric trilinear form, i.e.,

h(u, v, w) = a1u1v1w1 + a2(u1v1w2 + u1v2w1 + u2v1w1)

+ a3(u1v2w2 + u2v1w1 + u2v2w1) + a4u2v2w2,

then

sup

u,v,w6=0

h(u, v, w)

kuk2kvk2kwk2

= sup
u6=0

h(u, u, u)

kuk3

2

.

(b) let f : rn     r be a self-concordant function. show that the nullspace of    2f (x)
is independent of x. show that if f is strictly convex, then    2f (x) is nonsingular
for all x     dom f .
hint. prove that if wt   2f (x)w = 0 for some x     dom f , then wt   2f (y)w = 0 for
all y     dom f . to show this, apply the result in (a) to the self-concordant function
  f (t, s) = f (x + t(y     x) + sw).
(c) let f : rn     r be a self-concordant function. suppose x     dom f , v     rn. show
that

(1     t  )2   2f (x) (cid:22)    2f (x + tv) (cid:22)

1

(1     t  )2    2f (x)

for x + tv     dom f , 0     t <   , where    = (vt   2f (x)v)1/2.

9.18 quadratic convergence. let f : rn     r be a strictly convex self-concordant function.
suppose   (x) < 1, and de   ne x+ = x        2f (x)   1   f (x). prove that   (x+)       (x)2/(1    
  (x))2. hint. use the inequalities in exercise 9.17, part (c).
9.19 bound on the distance from the optimum. let f : rn     r be a strictly convex self-

concordant function.

(a) suppose   (  x) < 1 and the sublevel set {x | f (x)     f (  x)} is closed. show that the

minimum of f is attained and

(cid:0)(  x     x   )t   2f (  x)(  x     x   )(cid:1)1/2

   

  (  x)

1       (  x)

.

(b) show that if f has a closed sublevel set, and is bounded below, then its minimum is

attained.

9.20 conjugate of a self-concordant function. suppose f : rn     r is closed, strictly convex,
and self-concordant. we show that its conjugate (or legendre transform) f     is self-
concordant.
(a) show that for each y     dom f    , there is a unique x     dom f that satis   es y =
(b) suppose   y =    f (  x). de   ne

   f (x). hint. refer to the result of exercise 9.19.

g(t) = f (  x + tv),

h(t) = f    (  y + tw)

where v     rn and w =    2f (  x)v. show that

use these identities to show that f     is self-concordant.

g      (0) = h      (0),

g         (0) =    h         (0).

9.21 optimal line search parameters. consider the upper bound (9.56) on the number of
newton iterations required to minimize a strictly convex self-concordant functions. what
is the minimum value of the upper bound, if we minimize over    and   ?

9.22 suppose that f is strictly convex and satis   es (9.42). give a bound on the number of

newton steps required to compute p    within   , starting at x(0).

518

9 unconstrained minimization

implementation

9.23 pre-computation for line searches. for each of the following functions, explain how the
computational cost of a line search can be reduced by a pre-computation. give the cost
of the pre-computation, and the cost of evaluating g(t) = f (x + t   x) and g   (t) with and
without the pre-computation.

i x).

i=1 log(bi     at
i=1 exp(at

(a) f (x) =    pm
(b) f (x) = log(cid:0)pm
i x + bi)(cid:1).
b     rm and dom f = {x | p0 +pn

(c) f (x) = (ax     b)t (p0 + x1p1 +        + xnpn)   1(ax     b), where pi     sm, a     rm  n,

i=1 xipi     0}.

9.24 exploiting block diagonal structure in the newton system. suppose the hessian    2f (x) of
a convex function f is block diagonal. how do we exploit this structure when computing
the newton step? what does it mean about f ?

9.25 smoothed    t to given data. consider the problem

minimize

f (x) =pn

i=1   (xi     yi) +   pn   1

i=1 (xi+1     xi)2

where    > 0 is smoothing parameter,    is a convex penalty function, and x     rn is the
variable. we can interpret x as a smoothed    t to the vector y.

(a) what is the structure in the hessian of f ?

(b) extend to the problem of making a smooth    t to two-dimensional data, i.e., mini-

mizing the function

  (xij     yij) +    n   1xi=1

nxj=1

nxi,j=1

(xi+1,j     xij)2 +

(xi,j+1     xij)2! ,

nxi=1

n   1xj=1

with variable x     rn  n, where y     rn  n and    > 0 are given.

9.26 newton equations with linear structure. consider the problem of minimizing a function

of the form

f (x) =

nxi=1

  i(aix + bi)

(9.63)

where ai     rmi  n, bi     rmi , and the functions   i : rmi     r are twice di   erentiable
and convex. the hessian h and gradient g of f at x are given by

h =

nxi=1

at

i hiai,

g =

at

i gi.

nxi=1

(9.64)

where hi =    2  i(aix + bi) and gi =      i(aix + bi).
describe how you would implement newton   s method for minimizing f . assume that
n     mi, the matrices ai are very sparse, but the hessian h is dense.

9.27 analytic center of linear inequalities with variable bounds. give the most e   cient method

for computing the newton step of the function

f (x) =    

nxi=1

log(xi + 1)    

nxi=1

log(1     xi)    

mxi=1

log(bi     at

i x),

with dom f = {x     rn |    1     x     1, ax     b}, where at
is dense, and distinguish two cases: m     n and m     n. (see also exercise 9.30.)

is the ith row of a. assume a

i

exercises

519

9.28 analytic center of quadratic inequalities. describe an e   cient method for computing the

newton step of the function

f (x) =    

log(   xt aix     bt

i x     ci),

mxi=1

with dom f = {x | xt aix + bt
ai     sn
hint. the hessian and gradient of f at x are given by

++ are large and sparse, and m     n.

i x + ci < 0, i = 1, . . . , m}. assume that the matrices

h =

(2  iai +   2

i (2aix + bi)(2aix + bi)t ),

g =

  i(2aix + bi),

mxi=1

mxi=1

i x     ci).

where   i = 1/(   xt aix     bt
9.29 exploiting structure in two-stage optimization. this exercise continues exercise 4.64, which
describes optimization with recourse, or two-stage optimization. using the notation and
assumptions in exercise 4.64, we assume in addition that the cost function f is a twice
di   erentiable function of (x, z), for each scenario i = 1, . . . , s.
explain how to e   ciently compute the newton step for the problem of    nding the optimal
policy. how does the approximate    op count for your method compare to that of a generic
method (which exploits no structure), as a function of s, the number of scenarios?

numerical experiments

9.30 gradient and id77s. consider the unconstrained problem

minimize

i=1 log(1     at

i=1 log(1     x2
i ),

f (x) =    pm

i x)    pn

with variable x     rn, and dom f = {x | at
this is the problem of computing the analytic center of the set of linear inequalities

i x < 1, i = 1, . . . , m, |xi| < 1, i = 1, . . . , n}.

at
i x     1,

i = 1, . . . , m,

|xi|     1,

i = 1, . . . , n.

note that we can choose x(0) = 0 as our initial point. you can generate instances of this
problem by choosing ai from some distribution on rn.

(a) use the gradient method to solve the problem, using reasonable choices for the back-
tracking parameters, and a stopping criterion of the form k   f (x)k2       . plot the
objective function and step length versus iteration number. (once you have deter-
mined p    to high accuracy, you can also plot f     p    versus iteration.) experiment
with the backtracking parameters    and    to see their e   ect on the total number of
iterations required. carry these experiments out for several instances of the problem,
of di   erent sizes.

(b) repeat using newton   s method, with stopping criterion based on the newton decre-
ment   2. look for quadratic convergence. you do not have to use an e   cient method
to compute the newton step, as in exercise 9.27; you can use a general purpose dense
solver, although it is better to use one that is based on a cholesky factorization.

hint. use the chain rule to    nd expressions for    f (x) and    2f (x).
9.31 some approximate id77s. the cost of newton   s method is dominated by the
cost of evaluating the hessian    2f (x) and the cost of solving the newton system. for large
problems, it is sometimes useful to replace the hessian by a positive de   nite approximation
that makes it easier to form and solve for the search step. in this problem we explore
some common examples of this idea.
for each of the approximate id77s described below, test the method on some
instances of the analytic centering problem described in exercise 9.30, and compare the
results to those obtained using the id77 and gradient method.

520

9 unconstrained minimization

(a) re-using the hessian. we evaluate and factor the hessian only every n iterations,
where n > 1, and use the search step    x =    h    1   f (x), where h is the last hessian
evaluated. (we need to evaluate and factor the hessian once every n steps; for the
other steps, we compute the search direction using back and forward substitution.)

(b) diagonal approximation. we replace the hessian by its diagonal, so we only have
i , and computing the search step is

to evaluate the n second derivatives    2f (x)/   x2
very easy.

9.32 gauss-id77 for convex nonlinear least-squares problems. we consider a (non-

linear) least-squares problem, in which we minimize a function of the form

f (x) =

fi(x)2,

1
2

mxi=1

where fi are twice di   erentiable functions. the gradient and hessian of f at x are given
by

   f (x) =

mxi=1

fi(x)   fi(x),

   2f (x) =

mxi=1(cid:0)   fi(x)   fi(x)t + fi(x)   2fi(x)(cid:1) .

we consider the case when f is convex. this occurs, for example, if each fi is either
nonnegative and convex, or nonpositive and concave, or a   ne.
the gauss-id77 uses the search direction

   xgn =      mxi=1

   fi(x)   fi(x)t!   1  mxi=1

fi(x)   fi(x)! .

(we assume here that the inverse exists, i.e., the vectors    f1(x), . . . ,   fm(x) span rn.)
this search direction can be considered an approximate newton direction (see exer-
cise 9.31), obtained by dropping the second derivative terms from the hessian of f .
we can give another simple interpretation of the gauss-newton search direction    xgn.
using the    rst-order approximation fi(x + v)     fi(x) +    fi(x)t v we obtain the approxi-
mation

f (x + v)    

1
2

(fi(x) +    fi(x)t v)2.

mxi=1

the gauss-newton search step    xgn is precisely the value of v that minimizes this ap-
proximation of f . (moreover, we conclude that    xgn can be computed by solving a linear
least-squares problem.)
test the gauss-id77 on some problem instances of the form

fi(x) = (1/2)xt aix + bt

i x + 1,

with ai     sn

++ and bt

i a   1

i bi     2 (which ensures that f is convex).

chapter 10

equality constrained
minimization

10.1 equality constrained minimization problems

in this chapter we describe methods for solving a id76 problem
with equality constraints,

minimize
subject to ax = b,

f (x)

(10.1)

where f : rn     r is convex and twice continuously di   erentiable, and a     rp  n
with rank a = p < n. the assumptions on a mean that there are fewer equality
constraints than variables, and that the equality constraints are independent. we
will assume that an optimal solution x    exists, and use p    to denote the optimal
value, p    = inf{f (x) | ax = b} = f (x   ).
and only if there is a           rp such that

recall (from   4.2.3 or   5.5.3) that a point x        dom f is optimal for (10.1) if

ax    = b,

   f (x   ) + at       = 0.

(10.2)

solving the equality constrained optimization problem (10.1) is therefore equivalent
to    nding a solution of the kkt equations (10.2), which is a set of n + p equations
in the n + p variables x   ,      . the    rst set of equations, ax    = b, are called
the primal feasibility equations, which are linear. the second set of equations,
   f (x   ) + at       = 0, are called the dual feasibility equations, and are in general
nonlinear. as with unconstrained optimization, there are a few problems for which
we can solve these optimality conditions analytically. the most important special
case is when f is quadratic, which we examine in   10.1.1.
any equality constrained minimization problem can be reduced to an equiv-
alent unconstrained problem by eliminating the equality constraints, after which
the methods of chapter 9 can be used to solve the problem. another approach
is to solve the dual problem (assuming the dual function is twice di   erentiable)
using an unconstrained minimization method, and then recover the solution of the

522

10 equality constrained minimization

equality constrained problem (10.1) from the dual solution. the elimination and
dual methods are brie   y discussed in   10.1.2 and   10.1.3, respectively.
the bulk of this chapter is devoted to extensions of newton   s method that di-
rectly handle equality constraints. in many cases these methods are preferable to
methods that reduce an equality constrained problem to an unconstrained one. one
reason is that problem structure, such as sparsity, is often destroyed by elimination
(or forming the dual); in contrast, a method that directly handles equality con-
straints can exploit the problem structure. another reason is conceptual: methods
that directly handle equality constraints can be thought of as methods for directly
solving the optimality conditions (10.2).

10.1.1 equality constrained convex quadratic minimization

consider the equality constrained convex quadratic minimization problem

minimize
subject to ax = b,

f (x) = (1/2)xt p x + qt x + r

(10.3)

+ and a     rp  n. this problem is important on its own, and also
where p     sn
because it forms the basis for an extension of newton   s method to equality con-
strained problems.

here the optimality conditions (10.2) are

ax    = b,

p x    + q + at       = 0,

which we can write as

a 0 (cid:21)(cid:20) x   
(cid:20) p at

      (cid:21) =(cid:20)    q
b (cid:21) .

(10.4)

this set of n + p linear equations in the n + p variables x   ,       is called the kkt
system for the equality constrained quadratic optimization problem (10.3). the
coe   cient matrix is called the kkt matrix.

when the kkt matrix is nonsingular, there is a unique optimal primal-dual
pair (x   ,      ). if the kkt matrix is singular, but the kkt system is solvable, any
solution yields an optimal pair (x   ,      ). if the kkt system is not solvable, the
quadratic optimization problem is unbounded below or infeasible. indeed, in this
case there exist v     rn and w     rp such that
av = 0,

p v + at w = 0,

   qt v + bt w > 0.

let   x be any feasible point. the point x =   x + tv is feasible for all t and

f (  x + tv) = f (  x) + t(vt p   x + qt v) + (1/2)t2vt p v

= f (  x) + t(     xt at w + qt v)     (1/2)t2wt av
= f (  x) + t(   bt w + qt v),

which decreases without bound as t        .

10.1 equality constrained minimization problems

523

nonsingularity of the kkt matrix
recall our assumption that p     sn
conditions equivalent to nonsingularity of the kkt matrix:

+ and rank a = p < n. there are several

    n (p )     n (a) = {0}, i.e., p and a have no nontrivial common nullspace.
    ax = 0, x 6= 0 =    xt p x > 0, i.e., p is positive de   nite on the nullspace of

a.

    f t p f     0, where f     rn  (n   p) is a matrix for which r(f ) = n (a).

(see exercise 10.1.) as an important special case, we note that if p     0, the kkt
matrix must be nonsingular.

10.1.2 eliminating equality constraints

one general approach to solving the equality constrained problem (10.1) is to elim-
inate the equality constraints, as described in   4.2.4, and then solve the resulting
unconstrained problem using methods for unconstrained minimization. we    rst
   nd a matrix f     rn  (n   p) and vector   x     rn that parametrize the (a   ne)
feasible set:

{x | ax = b} = {f z +   x | z     rn   p}.

here   x can be chosen as any particular solution of ax = b, and f     rn  (n   p)
is any matrix whose range is the nullspace of a. we then form the reduced or
eliminated optimization problem

minimize

(10.5)
which is an unconstrained problem with variable z     rn   p. from its solution z   ,
we can    nd the solution of the equality constrained problem as x    = f z    +   x.
we can also construct an optimal dual variable       for the equality constrained

  f (z) = f (f z +   x),

problem, as

to show that this expression is correct, we must verify that the dual feasibility
condition

      =    (aat )   1a   f (x   ).

holds. to show this, we note that

   f (x   ) + at (   (aat )   1a   f (x   )) = 0
a (cid:21)(cid:0)   f (x   )     at (aat )   1a   f (x   )(cid:1) = 0,
(cid:20) f t

where in the top block we use f t   f (x   ) =       f (z   ) = 0 and af = 0. since the
matrix on the left is nonsingular, this implies (10.6).

(10.6)

example 10.1 optimal allocation with resource constraint. we consider the problem

minimize pn
subject to pn

i=1 fi(xi)
i=1 xi = b,

524

10 equality constrained minimization

where the functions fi : r     r are convex and twice di   erentiable, and b     r is
a problem parameter. we interpret this as the problem of optimally allocating a
single resource, with a    xed total amount b (the budget) to n otherwise independent
activities.

we can eliminate xn (for example) using the parametrization

which corresponds to the choices

xn = b     x1                xn   1,

  x = ben,

i

f =(cid:20)

   1t (cid:21)     rn  (n   1).
fn(b     x1                xn   1) +pn   1

i=1 fi(xi),

the reduced problem is then

minimize

with variables x1, . . . , xn   1.

choice of elimination matrix

there are, of course, many possible choices for the elimination matrix f , which can
be chosen as any matrix in rn  (n   p) with r(f ) = n (a). if f is one such matrix,
and t     r(n   p)  (n   p) is nonsingular, then   f = f t is also a suitable elimination
matrix, since

r(   f ) = r(f ) = n (a).

conversely, if f and   f are any two suitable elimination matrices, then there is
some nonsingular t such that   f = f t .

if we eliminate the equality constraints using f , we solve the unconstrained

problem

while if   f is used, we solve the unconstrained problem

minimize

f (f z +   x),

minimize

f (   f   z +   x) = f (f (t   z) +   x).

this problem is equivalent to the one above, and is simply obtained by the change
of coordinates z = t   z. in other words, changing the elimination matrix can be
thought of as changing variables in the reduced problem.

10.1.3 solving equality constrained problems via the dual

another approach to solving (10.1) is to solve the dual, and then recover the optimal
primal variable x   , as described in   5.5.5. the dual function of (10.1) is

x

(f (x) +   t ax)

g(  ) =    bt    + inf
=    bt        sup
=    bt        f    (   at   ),

x (cid:0)(   at   )t x     f (x)(cid:1)

10.2 newton   s method with equality constraints

525

where f     is the conjugate of f , so the dual problem is

maximize    bt        f    (   at   ).

since by assumption there is an optimal point, the problem is strictly feasible, so
slater   s condition holds. therefore strong duality holds, and the dual optimum is
attained, i.e., there exists a       with g(     ) = p   .

if the dual function g is twice di   erentiable, then the methods for unconstrained
minimization described in chapter 9 can be used to maximize g. (in general, the
dual function g need not be twice di   erentiable, even if f is.) once we    nd an
optimal dual variable      , we reconstruct an optimal primal solution x    from it.
(this is not always straightforward; see   5.5.5.)

example 10.2 equality constrained analytic center. we consider the problem

minimize
subject to ax = b,

f (x) =    pn

i=1 log xi

where a     rp  n, with implicit constraint x     0. using

f    (y) =

nxi=1

(   1     log(   yi)) =    n    

nxi=1

log(   yi)

(with dom f     =    rn

++), the dual problem is

maximize

g(  ) =    bt    + n +pn

i=1 log(at   )i,

(10.7)

(10.8)

with implicit constraint at        0. here we can easily solve the dual feasibility
equation, i.e.,    nd the x that minimizes l(x,   ):

   f (x) + at    =    (1/x1, . . . , 1/xn) + at    = 0,

and so

xi(  ) = 1/(at   )i.

(10.9)

to solve the equality constrained analytic centering problem (10.7), we solve the
(unconstrained) dual problem (10.8), and then recover the optimal solution of (10.7)
via (10.9).

10.2 newton   s method with equality constraints

in this section we describe an extension of newton   s method to include equality
constraints. the method is almost the same as newton   s method without con-
straints, except for two di   erences: the initial point must be feasible (i.e., satisfy
x     dom f and ax = b), and the de   nition of newton step is modi   ed to take
the equality constraints into account. in particular, we make sure that the newton
step    xnt is a feasible direction, i.e., a   xnt = 0.

526

10 equality constrained minimization

10.2.1 the newton step

de   nition via second-order approximation

to derive the newton step    xnt for the equality constrained problem

minimize
subject to ax = b,

f (x)

at the feasible point x, we replace the objective with its second-order taylor ap-
proximation near x, to form the problem

minimize
subject to a(x + v) = b,

bf (x + v) = f (x) +    f (x)t v + (1/2)vt   2f (x)v

(10.10)

with variable v. this is a (convex) quadratic minimization problem with equality
constraints, and can be solved analytically. we de   ne    xnt, the newton step at x,
as the solution of the convex quadratic problem (10.10), assuming the associated
kkt matrix is nonsingular. in other words, the newton step    xnt is what must
be added to x to solve the problem when the quadratic approximation is used in
place of f .

from our analysis in   10.1.1 of the equality constrained quadratic problem, the

newton step    xnt is characterized by

(cid:20)    2f (x) at

0 (cid:21)(cid:20)    xnt

w (cid:21) =(cid:20)       f (x)

a

0

(cid:21) ,

(10.11)

where w is the associated optimal dual variable for the quadratic problem. the
newton step is de   ned only at points for which the kkt matrix is nonsingular.

as in newton   s method for unconstrained problems, we observe that when the
objective f is exactly quadratic, the newton update x +    xnt exactly solves the
equality constrained minimization problem, and in this case the vector w is the op-
timal dual variable for the original problem. this suggests, as in the unconstrained
case, that when f is nearly quadratic, x +    xnt should be a very good estimate of
the solution x   , and w should be a good estimate of the optimal dual variable      .

solution of linearized optimality conditions

we can interpret the newton step    xnt, and the associated vector w, as the solu-
tions of a linearized approximation of the optimality conditions

ax    = b,

   f (x   ) + at       = 0.

we substitute x +    xnt for x    and w for      , and replace the gradient term in the
second equation by its linearized approximation near x, to obtain the equations
   f (x +    xnt) + at w        f (x) +    2f (x)   xnt + at w = 0.

a(x +    xnt) = b,

using ax = b, these become

a   xnt = 0,

   2f (x)   xnt + at w =       f (x),

which are precisely the equations (10.11) that de   ne the newton step.

10.2 newton   s method with equality constraints

527

the newton decrement

we de   ne the newton decrement for the equality constrained problem as

  (x) = (   xt

nt   2f (x)   xnt)1/2.

(10.12)

this is exactly the same expression as (9.29), used in the unconstrained case, and
the same interpretations hold. for example,   (x) is the norm of the newton step,
in the norm determined by the hessian.

let

(10.13)

be the second-order taylor approximation of f at x. the di   erence between f (x)
and the minimum of the second-order model satis   es

bf (x + v) = f (x) +    f (x)t v + (1/2)vt   2f (x)v
f (x)     inf{bf (x + v) | a(x + v) = b} =   (x)2/2,

exactly as in the unconstrained case (see exercise 10.6). this means that, as in the
unconstrained case,   (x)2/2 gives an estimate of f (x)    p   , based on the quadratic
model at x, and also that   (x) (or a multiple of   (x)2) serves as the basis of a good
stopping criterion.

the newton decrement comes up in the line search as well, since the directional

derivative of f in the direction    xnt is

d
dt

f (x + t   xnt)(cid:12)(cid:12)(cid:12)(cid:12)t=0

as in the unconstrained case.

=    f (x)t    xnt =      (x)2,

(10.14)

feasible descent direction
suppose that ax = b. we say that v     rn is a feasible direction if av = 0. in this
case, every point of the form x + tv is also feasible, i.e., a(x + tv) = b. we say that
v is a descent direction for f at x, if for small t > 0, f (x + tv) < f (x).

the newton step is always a feasible descent direction (except when x is opti-
mal, in which case    xnt = 0). indeed, the second set of equations that de   ne    xnt
are a   xnt = 0, which shows it is a feasible direction; that it is a descent direction
follows from (10.14).

a   ne invariance

like the newton step and decrement for unconstrained optimization, the new-
ton step and decrement for equality constrained optimization are a   ne invariant.
suppose t     rn  n is nonsingular, and de   ne   f (y) = f (t y). we have

      f (y) = t t   f (t y),

   2   f (y) = t t   2f (t y)t,

and the equality constraint ax = b becomes at y = b.

now consider the problem of minimizing   f (y), subject to at y = b. the newton

step    ynt at y is given by the solution of

(cid:20) t t   2f (t y)t t t at

at

0

(cid:21)(cid:20)    ynt

  w (cid:21) =(cid:20)    t t   f (t y)

0

(cid:21) .

528

10 equality constrained minimization

comparing with the newton step    xnt for f at x = t y, given in (10.11), we see
that

t    ynt =    xnt

(and w =   w), i.e., the newton steps at y and x are related by the same change of
coordinates as t y = x.

10.2.2 newton   s method with equality constraints

the outline of newton   s method with equality constraints is exactly the same as
for unconstrained problems.

algorithm 10.1 newton   s method for equality constrained minimization.

given starting point x     dom f with ax = b, tolerance    > 0.
repeat

1. compute the newton step and decrement    xnt,   (x).
2. stopping criterion. quit if   2/2       .
3. line search. choose step size t by backtracking line search.
4. update. x := x + t   xnt.

the method is called a feasible descent method, since all the iterates are feasi-
ble, with f (x(k+1)) < f (x(k)) (unless x(k) is optimal). newton   s method requires
that the kkt matrix be invertible at each x; we will be more precise about the
assumptions required for convergence in   10.2.4.

10.2.3 newton   s method and elimination

we now show that the iterates in newton   s method for the equality constrained
problem (10.1) coincide with the iterates in newton   s method applied to the re-
duced problem (10.5). suppose f satis   es r(f ) = n (a) and rank f = n     p,
and   x satis   es a  x = b. the gradient and hessian of the reduced objective function
  f (z) = f (f z +   x) are

      f (z) = f t   f (f z +   x),

   2   f (z) = f t   2f (f z +   x)f.

from the hessian expression, we see that the newton step for the equality con-
strained problem is de   ned, i.e., the kkt matrix

0 (cid:21)
(cid:20)    2f (x) at

a

is invertible, if and only if the newton step for the reduced problem is de   ned, i.e.,
   2   f (z) is invertible.

the newton step for the reduced problem is

   znt =       2   f (z)   1      f (z) =    (f t   2f (x)f )   1f t   f (x),

(10.15)

10.2 newton   s method with equality constraints

529

where x = f z +   x. this search direction for the reduced problem corresponds to
the direction

f    znt =    f (f t   2f (x)f )   1f t   f (x)

for the original, equality constrained problem. we claim this is precisely the same
as the newton direction    xnt for the original problem, de   ned in (10.11).

to show this, we take    xnt = f    znt, choose

w =    (aat )   1a(   f (x) +    2f (x)   xnt),

and verify that the equations de   ning the newton step,

   2f (x)   xnt + at w +    f (x) = 0,

a   xnt = 0,

(10.16)

hold. the second equation, a   xnt = 0, is satis   ed because af = 0. to verify the
   rst equation, we observe that

a (cid:21)(cid:0)   2f (x)   xnt + at w +    f (x)(cid:1)
(cid:20) f t
= (cid:20) f t   2f (x)   xnt + f t at w + f t   f (x)

a   2f (x)   xnt + aat w + a   f (x)

= 0.

(cid:21)

since the matrix on the left of the    rst line is nonsingular, we conclude that (10.16)
holds.

in a similar way, the newton decrement     (z) of   f at z and the newton decrement

of f at x turn out to be equal:

    (z)2 =    zt
=    zt
=    xt
=   (x)2.

nt   2   f (z)   znt
ntf t   2f (x)f    znt
nt   2f (x)   xnt

10.2.4 convergence analysis

we saw above that applying newton   s method with equality constraints is exactly
the same as applying newton   s method to the reduced problem obtained by elimi-
nating the equality constraints. everything we know about the convergence of new-
ton   s method for unconstrained problems therefore transfers to newton   s method
for equality constrained problems. in particular, the practical performance of new-
ton   s method with equality constraints is exactly like the performance of newton   s
method for unconstrained problems. once x(k) is near x   , convergence is extremely
rapid, with a very high accuracy obtained in only a few iterations.

assumptions

we make the following assumptions.

530

10 equality constrained minimization

    the sublevel set s = {x | x     dom f, f (x)     f (x(0)), ax = b} is closed,
where x(0)     dom f satis   es ax(0) = b. this is the case if f is closed
(see   a.3.3).

    on the set s, we have    2f (x) (cid:22) m i, and

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:20)    2f (x) at

a

0 (cid:21)   1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

    k,

(10.17)

i.e., the inverse of the kkt matrix is bounded on s. (of course the inverse
must exist in order for the newton step to be de   ned at each point in s.)

    for x,   x     s,    2f satis   es the lipschitz condition k   2f (x)        2f (  x)k2    

lkx       xk2.

bounded inverse kkt matrix assumption

the condition (10.17) plays the role of the strong convexity assumption in the
standard id77 (  9.5.3, page 488). when there are no equality con-
straints, (10.17) reduces to the condition k   2f (x)   1k2     k on s, so we can take
k = 1/m, if    2f (x) (cid:23) mi on s, where m > 0. with equality constraints, the
condition is not as simple as a positive lower bound on the minimum eigenvalue.
since the kkt matrix is symmetric, the condition (10.17) is that its eigenvalues,
n of which are positive, and p of which are negative, are bounded away from zero.

analysis via the eliminated problem
the assumptions above imply that the eliminated objective function   f , together
with the associated initial point z(0), where x(0) =   x + f z(0), satisfy the assump-
tions required in the convergence analysis of newton   s method for unconstrained
problems, given in   9.5.3 (with di   erent constants   m,   m , and   l). it follows that
newton   s method with equality constraints converges to x    (and       as well).
to show that the assumptions above imply that the eliminated problem satis   es
the assumptions for the unconstrained id77 is mostly straightforward
(see exercise 10.4). here we show the one implication that is tricky: that the
bounded inverse kkt condition, together with the upper bound    2f (x) (cid:22) m i,
implies that    2   f (z) (cid:23) mi for some positive constant m. more speci   cally we will
show that this inequality holds for

m =

  min(f )2

k 2m

,

(10.18)

which is positive, since f is full rank.

we show this by contradiction. suppose that f t hf 6(cid:23) mi, where h =    2f (x).
then we can    nd u, with kuk2 = 1, such that ut f t hf u < m, i.e., kh 1/2f uk2 <
m1/2. using af = 0, we have

(cid:20) h at

0 (cid:21)(cid:20) f u

0 (cid:21) =(cid:20) hf u

a

0

(cid:21) ,

10.3

infeasible start id77

531

and so

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:20) h at

a

0 (cid:21)   1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

using kf uk2       min(f ) and

    (cid:13)(cid:13)(cid:13)(cid:13)(cid:20) f u
0 (cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:21)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:20) hf u

0

= kf uk2
khf uk2

.

khf uk2     kh 1/2k2kh 1/2f uk2 < m 1/2m1/2,

we conclude

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:20) h at

a

0 (cid:21)   1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

    kf uk2
khf uk2

>

  min(f )
m 1/2m1/2 = k,

using our expression for m given in (10.18).

convergence analysis for self-concordant functions
if f is self-concordant, then so is   f (z) = f (f z +   x). it follows that if f is self-
concordant, we have the exact same complexity estimate as for unconstrained prob-
lems: the number of iterations required to produce a solution within an accuracy
   is no more than

20     8  
    (1     2  )2 (f (x(0))     p   ) + log2 log2(1/  ),

where    and    are the backtracking parameters (see (9.56)).

10.3 infeasible start id77

newton   s method, as described in   10.2, is a feasible descent method.
in this
section we describe a generalization of newton   s method that works with initial
points, and iterates, that are not feasible.

10.3.1 newton step at infeasible points

as in newton   s method, we start with the optimality conditions for the equality
constrained minimization problem:

ax    = b,

   f (x   ) + at       = 0.

let x denote the current point, which we do not assume to be feasible, but we do
assume satis   es x     dom f . our goal is to    nd a step    x so that x +    x satis   es
(at least approximately) the optimality conditions, i.e., x +    x     x   . to do this

532

10 equality constrained minimization

we substitute x +    x for x    and w for       in the optimality conditions, and use the
   rst-order approximation

   f (x +    x)        f (x) +    2f (x)   x

for the gradient to obtain

a(x +    x) = b,

this is a set of linear equations for    x and w,

   f (x) +    2f (x)   x + at w = 0.
0 (cid:21)(cid:20)    x

ax     b (cid:21) .
w (cid:21) =    (cid:20)    f (x)

(cid:20)    2f (x) at

a

(10.19)

the equations are the same as the equations (10.11) that de   ne the newton step
at a feasible point x, with one di   erence: the second block component of the
righthand side contains ax     b, which is the residual vector for the linear equality
constraints. when x is feasible, the residual vanishes, and the equations (10.19)
reduce to the equations (10.11) that de   ne the standard newton step at a feasible
point x. thus, if x is feasible, the step    x de   ned by (10.19) coincides with the
newton step described above (but de   ned only when x is feasible). for this reason
we use the notation    xnt for the step    x de   ned by (10.19), and refer to it as the
newton step at x, with no confusion.

interpretation as primal-dual newton step

we can give an interpretation of the equations (10.19) in terms of a primal-dual
method for the equality constrained problem. by a primal-dual method, we mean
one in which we update both the primal variable x, and the dual variable   , in
order to (approximately) satisfy the optimality conditions.

we express the optimality conditions as r(x   ,      ) = 0, where r : rn    rp    

rn    rp is de   ned as

r(x,   ) = (rdual(x,   ), rpri(x,   )).

here

rdual(x,   ) =    f (x) + at   ,

rpri(x,   ) = ax     b

are the dual residual and primal residual, respectively. the    rst-order taylor ap-
proximation of r, near our current estimate y, is

r(y + z)       r(y + z) = r(y) + dr(y)z,

where dr(y)     r(n+p)  (n+p) is the derivative of r, evaluated at y (see   a.4.1).
we de   ne the primal-dual newton step    ypd as the step z for which the taylor
approximation   r(y + z) vanishes, i.e.,

dr(y)   ypd =    r(y).

(10.20)

note that here we consider both x and    as variables;    ypd = (   xpd,      pd) gives
both a primal and a dual step.

10.3

infeasible start id77

533

evaluating the derivative of r, we can express (10.20) as

writing    +      pd as   +, we can express this as

(cid:20)    2f (x) at

a

0 (cid:21)(cid:20)    xpd
(cid:20)    2f (x) at

     pd (cid:21) =    (cid:20) rdual
0 (cid:21)(cid:20)    xpd

rpri (cid:21) =    (cid:20)    f (x) + at   
  + (cid:21) =    (cid:20)    f (x)
ax     b (cid:21) ,

ax     b

a

(cid:21) .

(10.21)

(10.22)

which is exactly the same set of equations as (10.19). the solutions of (10.19),
(10.21), and (10.22) are therefore related as

   xnt =    xpd,

w =   + =    +      pd.

this shows that the (infeasible) newton step is the same as the primal part of
the primal-dual step, and the associated dual vector w is the updated primal-dual
variable   + =    +      pd.

the two expressions for the newton step and dual variable (or dual step), given
by (10.21) and (10.22), are of course equivalent, but each reveals a di   erent feature
of the newton step. the equation (10.21) shows that the newton step and the
associated dual step are obtained by solving a set of equations, with the primal
and dual residuals as the righthand side. the equation (10.22), which is how we
originally de   ned the newton step, gives the newton step and the updated dual
variable, and shows that the current value of the dual variable is not needed to
compute the primal step, or the updated value of the dual variable.

residual norm reduction property

the newton direction, at an infeasible point, is not necessarily a descent direction
for f . from (10.19), we note that

d
dt

f (x + t   x)(cid:12)(cid:12)(cid:12)(cid:12)t=0

=    f (x)t    x
=       xt(cid:0)   2f (x)   x + at w(cid:1)
=       xt   2f (x)   x + (ax     b)t w,

which is not necessarily negative (unless, of course, x is feasible, i.e., ax = b). the
primal-dual interpretation, however, shows that the norm of the residual decreases
in the newton direction, i.e.,

d
dt kr(y + t   ypd)k2

= 2r(y)t dr(y)   ypd =    2r(y)t r(y).

taking the derivative of the square, we obtain

=    kr(y)k2.

(10.23)

this allows us to use krk2 to measure the progress of the infeasible start newton
method, for example, in the line search. (for the standard id77, we use
the function value f to measure progress of the algorithm, at least until quadratic
convergence is attained.)

2(cid:12)(cid:12)(cid:12)(cid:12)t=0
dt kr(y + t   ypd)k2(cid:12)(cid:12)(cid:12)(cid:12)t=0

d

534

10 equality constrained minimization

full step feasibility property

the newton step    xnt de   ned by (10.19) has the property (by construction) that

a(x +    xnt) = b.

(10.24)

it follows that, if a step length of one is taken using the newton step    xnt, the
following iterate will be feasible. once x is feasible, the newton step becomes a
feasible direction, so all future iterates will be feasible, regardless of the step sizes
taken.

more generally, we can analyze the e   ect of a damped step on the equality
constraint residual rpri. with a step length t     [0, 1], the next iterate is x+ =
x + t   xnt, so the equality constraint residual at the next iterate is

r+
pri = a(x +    xntt)     b = (1     t)(ax     b) = (1     t)rpri,

using (10.24). thus, a damped step, with length t, causes the residual to be scaled
nt , for
nt is the newton step at the point x(i)     dom f , and

down by a factor 1     t. now suppose that we have x(i+1) = x(i) + t(i)   x(i)
i = 0, . . . , k     1, where    x(i)
t(i)     [0, 1]. then we have

r(k) = k   1yi=0

(1     t(i))! r(0),

where r(i) = ax(i)     b is the residual of x(i). this formula shows that the primal
residual at each step is in the direction of the initial primal residual, and is scaled
down at each step. it also shows that once a full step is taken, all future iterates
are primal feasible.

10.3.2 infeasible start id77

we can develop an extension of newton   s method, using the newton step    xnt
de   ned by (10.19), with x(0)     dom f , but not necessarily satisfying ax(0) = b.
we also use the dual part of the newton step:      nt = w        in the notation
of (10.19), or equivalently,      nt =      pd in the notation of (10.21).

algorithm 10.2 infeasible start id77.

given starting point x     dom f ,   , tolerance    > 0,        (0, 1/2),        (0, 1).
repeat

1. compute primal and dual newton steps    xnt,      nt.
2. backtracking line search on krk2.

t := 1.
while kr(x + t   xnt,    + t     nt)k2 > (1       t)kr(x,   )k2,

3. update. x := x + t   xnt,    :=    + t     nt.

t :=   t.

until ax = b and kr(x,   )k2       .

10.3

infeasible start id77

535

this algorithm is very similar to the standard id77 with feasible start-
ing point, with a few exceptions. first, the search directions include the extra
correction terms that depend on the primal residual. second, the line search is
carried out using the norm of the residual, instead of the function value f . finally,
the algorithm terminates when primal feasibility has been achieved, and the norm
of the (dual) residual is small.

the line search in step 2 deserves some comment. using the norm of the residual
in the line search can increase the cost, compared to a line search based on the
function value, but the increase is usually negligible. also, we note that the line
search must terminate in a    nite number of steps, since (10.23) shows that the line
search exit condition is satis   ed for small t.

the equation (10.24) shows that if at some iteration the step length is chosen to
be one, the next iterate will be feasible. thereafter, all iterates will be feasible, and
therefore the search direction for the infeasible start id77 coincides, once
a feasible iterate is obtained, with the search direction for the (feasible) newton
method described in   10.2.
there are many variations on the infeasible start id77. for example,
we can switch to the (feasible) id77 described in   10.2 once feasibility
is achieved. (in other words, we change the line search to one based on f , and
terminate when   (x)2/2       .) once feasibility is achieved, the infeasible start and
the standard (feasible) id77 di   er only in the backtracking and exit
conditions, and have very similar performance.

using infeasible start id77 to simplify initialization

the main advantage of the infeasible start id77 is in the initialization
required. if dom f = rn, then initializing the (feasible) id77 simply
requires computing a solution to ax = b, and there is no particular advantage,
other than convenience, in using the infeasible start id77.

when dom f is not all of rn,    nding a point in dom f that satis   es ax = b
can itself be a challenge. one general approach, probably the best when dom f is
complex and not known to intersect {z | az = b}, is to use a phase i method (de-
scribed in   11.4) to compute such a point (or verify that dom f does not intersect
{z | az = b}). but when dom f is relatively simple, and known to contain a point
satisfying ax = b, the infeasible start id77 gives a simple alternative.
++, as in the equality con-
strained analytic centering problem described in example 10.2. to initialize new-
ton   s method for the problem

one common example occurs when dom f = rn

minimize    pn

subject to ax = b,

i=1 log xi

(10.25)

requires    nding a point x(0)     0 with ax = b, which is equivalent to solving a stan-
dard form lp feasibility problem. this can be carried out using a phase i method,
or alternatively, using the infeasible start id77, with any positive initial
point, e.g., x(0) = 1.

the same trick can be used to initialize unconstrained problems where a starting
point in dom f is not known. as an example, we consider the dual of the equality

536

10 equality constrained minimization

constrained analytic centering problem (10.25),

maximize

g(  ) =    bt    + n +pn

i=1 log(at   )i.

to initialize this problem for the (feasible start) id77, we must    nd a
point   (0) that satis   es at   (0)     0, i.e., we must solve a set of linear inequalities.
this can be done using a phase i method, or using an infeasible start newton
method, after reformulating the problem. we    rst express it as an equality con-
strained problem,

maximize    bt    + n +pn

y = at   ,

subject to

i=1 log yi

with new variable y     rn. we can now use the infeasible start id77,
starting with any positive y(0) (and any   (0)).
the disadvantage of using the infeasible start id77 to initialize prob-
lems for which a strictly feasible starting point is not known is that there is no clear
way to detect that there exists no strictly feasible point; the norm of the residual
will simply converge, slowly, to some positive value. (phase i methods, in contrast,
can determine this fact unambiguously.) in addition, the convergence of the infea-
sible start id77, before feasibility is achieved, can be slow; see   11.4.2.

10.3.3 convergence analysis

in this section we show that the infeasible start id77 converges to the
optimal point, provided certain assumptions hold. the convergence proof is very
similar to those for the standard id77, or the standard id77
with equality constraints. we show that once the norm of the residual is small
enough, the algorithm takes full steps (which implies that feasibility is achieved),
and convergence is subsequently quadratic. we also show that the norm of the
residual is reduced by at least a    xed amount in each iteration before the region
of quadratic convergence is reached. since the norm of the residual cannot be
negative, this shows that within a    nite number of steps, the residual will be small
enough to guarantee full steps, and quadratic convergence.

assumptions

we make the following assumptions.

    the sublevel set

s = {(x,   ) | x     dom f, kr(x,   )k2     kr(x(0),   (0))k2}

(10.26)

is closed. if f is closed, then krk2 is a closed function, and therefore this con-
dition is satis   ed for any x(0)     dom f and any   (0)     rp (see exercise 10.7).

    on the set s, we have

kdr(x,   )   1k2 =(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:20)    2f (x) at

a

0 (cid:21)   1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

    k,

(10.27)

10.3

infeasible start id77

537

for some k.

    for (x,   ), (  x,     )     s, dr satis   es the lipschitz condition

kdr(x,   )     dr(  x,     )k2     lk(x,   )     (  x,     )k2.

(this is equivalent to    2f (x) satisfying a lipschitz condition; see exer-
cise 10.7.)

as we will see below, these assumptions imply that dom f and {z | az = b}
intersect, and that there is an optimal point (x   ,      ).

comparison with standard id77
the assumptions above are very similar to the ones made in   10.2.4 (page 529)
for the analysis of the standard id77. the second and third assump-
tions, the bounded inverse kkt matrix and lipschitz condition, are essentially the
same. the sublevel set condition (10.26) for the infeasible start id77
is, however, more general than the sublevel set condition made in   10.2.4.

as an example, consider the equality constrained maximum id178 problem

minimize
subject to ax = b,

f (x) =pn

i=1 xi log xi

with dom f = rn
++. the objective f is not closed; it has sublevel sets that are not
closed, so the assumptions made in the standard id77 may not hold,
at least for some initial points. the problem here is that the negative id178
function does not converge to     as xi     0. on the other hand the sublevel set
condition (10.26) for the infeasible start id77 does hold for this problem,
since the norm of the gradient of the negative id178 function does converge to
    as xi     0. thus, the infeasible start id77 is guaranteed to solve the
equality constrained maximum id178 problem. (we do not know whether the
standard id77 can fail for this problem; we are only observing here that
our convergence analysis does not hold.) note that if the initial point satis   es the
equality constraints, the only di   erence between the standard and infeasible start
id77s is in the line searches, which di   er only during the damped stage.

a basic inequality
we start by deriving a basic inequality. let y = (x,   )     s with kr(y)k2 6= 0, and
let    ynt = (   xnt,      nt) be the newton step at y. de   ne

tmax = inf{t > 0 | y + t   ynt 6    s}.

if y + t   ynt     s for all t     0, we follow the usual convention and de   ne tmax =    .
otherwise, tmax is the smallest positive value of t such that kr(y + t   ynt)k2 =
kr(y(0))k2. in particular, it follows that y + t   ynt     s for 0     t     tmax.

we will show that

kr(y + t   ynt)k2     (1     t)kr(y)k2 + (k 2l/2)t2kr(y)k2

2

(10.28)

538

10 equality constrained minimization

for 0     t     min{1, tmax}.

we have

r(y + t   ynt) = r(y) +z 1

0

dr(y +    t   ynt)t   ynt d  

= r(y) + tdr(y)   ynt +z 1

= r(y) + tdr(y)   ynt + e
= (1     t)r(y) + e,

0

(dr(y +    t   ynt)     dr(y))t   ynt d  

using dr(y)   ynt =    r(y), and de   ning

e =z 1

0

(dr(y +    t   ynt)     dr(y))t   ynt d  .

now suppose 0     t     tmax, so y +    t   ynt     s for 0            1. we can bound kek2
as follows:

0 kdr(y +    t   ynt)     dr(y)k2 d  

kek2     kt   yntk2z 1
    kt   yntk2z 1
= (l/2)t2k   yntk2
= (l/2)t2kdr(y)   1r(y)k2
    (k 2l/2)t2kr(y)k2
2,

0

2

2

lk   t   yntk2 d  

using the lipschitz condition on the second line, and the bound kdr(y)   1k2     k
on the last. now we can derive the bound (10.28): for 0     t     min{1, tmax},

kr(y + t   ynt)k2 = k(1     t)r(y) + ek2

    (1     t)kr(y)k2 + kek2
    (1     t)kr(y)k2 + (k 2l/2)t2kr(y)k2
2.

damped newton phase
we    rst show that if kr(y)k2 > 1/(k 2l), one iteration of the infeasible start
id77 reduces krk2 by at least a certain minimum amount.
the righthand side of the basic inequality (10.28) is quadratic in t, and mono-
tonically decreasing between t = 0 and its minimizer

  t =

1

k 2lkr(y)k2

< 1.

we must have tmax >   t, because the opposite would imply kr(y + tmax   ynt)k2 <
kr(y)k2, which is false. the basic inequality is therefore valid at t =   t, and therefore

kr(y +   t   ynt)k2     kr(y)k2     1/(2k 2l)
    kr(y)k2       /(k 2l)
= (1         t)kr(y)k2,

10.3

infeasible start id77

539

which shows that the step length   t satis   es the line search exit condition. therefore
we have t         t, where t is the step length chosen by the backtracking algorithm.
from t         t we have (from the exit condition in the backtracking line search)

kr(y + t   ynt)k2     (1       t)kr(y)k2
    (1           t)kr(y)k2
= (cid:18)1    

k 2lkr(y)k2(cid:19)kr(y)k2

    

= kr(y)k2    

    
k 2l

.

thus, as long as we have kr(y)k2 > 1/(k 2l), we obtain a minimum decrease in
krk2, per iteration, of     /(k 2l). it follows that a maximum of

kr(y(0))k2k 2l

    

iterations can be taken before we have kr(y(k))k2     1/(k 2l).
quadratically convergent phase
now suppose kr(y)k2     1/(k 2l). the basic inequality gives
kr(y + t   ynt)k2     (1     t + (1/2)t2)kr(y)k2

(10.29)

for 0     t     min{1, tmax}. we must have tmax > 1, because otherwise it would follow
from (10.29) that kr(y + tmax   ynt)k2 < kr(y)k2, which contradicts the de   nition
of tmax. the inequality (10.29) therefore holds with t = 1, i.e., we have

kr(y +    ynt)k2     (1/2)kr(y)k2     (1       )kr(y)k2.

this shows that the backtracking line search exit criterion is satis   ed for t = 1,
so a full step will be taken. moreover, for all future iterations we have kr(y)k2    
1/(k 2l), so a full step will be taken for all following iterations.

we can write the inequality (10.28) (for t = 1) as

k 2lkr(y+)k2

2

   (cid:18) k 2lkr(y)k2

2

(cid:19)2

,

where y+ = y +    ynt. therefore, if r(y+k) denotes the residual k steps after an
iteration in which kr(y)k2     1/k 2l, we have

k 2lkr(y+k)k2

2

   (cid:18) k 2lkr(y)k2

2

(cid:19)2k

   (cid:18) 1
2(cid:19)2k

,

i.e., we have quadratic convergence of kr(y)k2 to zero.
to show that the sequence of iterates converges, we will show that it is a cauchy
sequence. suppose y is an iterate satisfying kr(y)k2     1/(k 2l), and y+k denotes

540

10 equality constrained minimization

the kth iterate after y. since these iterates are in the region of quadratic conver-
gence, the step size is one, so we have

ky+k     yk2     ky+k     y+(k   1)k2 +        + ky+     yk2
= kdr(y+(k   1))   1r(y+(k   1))k2 +        + kdr(y)   1r(y)k2
    k(cid:16)kr(y+(k   1))k2 +        + kr(y)k2(cid:17)
(cid:19)2i   1

    kkr(y)k2

2

k   1xi=0(cid:18) k 2lkr(y)k2
k   1xi=0(cid:18) 1

2(cid:19)2i   1

    kkr(y)k2
    2kkr(y)k2

where in the third line we use the assumption that kdr   1k2     k for all iterates.
since kr(y(k))k2 converges to zero, we conclude y(k) is a cauchy sequence, and
therefore converges. by continuity of r, the limit point y    satis   es r(y   ) = 0. this
establishes our earlier claim that the assumptions at the beginning of this section
imply that there is an optimal point (x   ,      ).

10.3.4 convex-concave games

the proof of convergence for the infeasible start id77 reveals that the
method can be used for a larger class of problems than equality constrained convex
optimization problems. suppose r : rn     rn is di   erentiable, its derivative
satis   es a lipschitz condition on s, and kdr(x)   1k2 is bounded on s, where

s = {x     dom r | kr(x)k2     kr(x(0))k2}

is a closed set. then the infeasible start id77, started at x(0), converges
to a solution of r(x) = 0 in s. in the infeasible start id77, we apply
this to the speci   c case in which r is the residual for the equality constrained
id76 problem. but it applies in several other interesting cases. one
interesting example is solving a convex-concave game. (see   5.4.3 and exercise 5.25
for discussion of other, related games).
an unconstrained (zero-sum, two-player) game on rp    rq is de   ned by its
payo    function f : rp+q     r. the meaning is that player 1 chooses a value (or
move) u     rp, and player 2 chooses a value (or move) v     rq; based on these
choices, player 1 makes a payment to player 2, in the amount f (u, v). the goal of
player 1 is to minimize this payment, while the goal of player 2 is to maximize it.
if player 1 makes his choice u    rst, and player 2 knows the choice, then player 2
will choose v to maximize f (u, v), which results in a payo    of supv f (u, v) (assuming
the supremum is achieved). if player 1 assumes that player 2 will make this choice,
he should choose u to minimize supv f (u, v). the resulting payo   , from player 1
to player 2, will then be

inf
u

sup

f (u, v)

v

(10.30)

10.3

infeasible start id77

541

(assuming that the supremum is achieved). on the other hand if player 2 makes
the    rst choice, the strategies are reversed, and the resulting payo    from player 1
to player 2 is

sup

v

inf
u

f (u, v).

(10.31)

the payo    (10.30) is always greater than or equal to the payo    (10.31); the dif-
ference between the two payo   s can be interpreted as the advantage a   orded the
player who makes the second move, with knowledge of the other player   s move. we
say that (u   , v   ) is a solution of the game, or a saddle-point for the game, if for all
u, v,

f (u   , v)     f (u   , v   )     f (u, v   ).

when a solution exists, there is no advantage to making the second move; f (u   , v   )
is the common value of both payo   s (10.30) and (10.31). (see exercise 3.14.)

the game is called convex-concave if for each v, f (u, v) is a convex function of
u, and for each u, f (u, v) is a concave function of v. when f is di   erentiable (and
convex-concave), a saddle-point for the game is characterized by    f (u   , v   ) = 0.
solution via infeasible start id77

we can use the infeasible start id77 to compute a solution of a convex-
concave game with twice di   erentiable payo    function. we de   ne the residual as

r(u, v) =    f (u, v) =(cid:20)    uf (u, v)
   vf (u, v) (cid:21) ,

and apply the infeasible start id77. in the context of games, the infea-
sible start id77 is simply called newton   s method (for convex-concave
games).

we can guarantee convergence of the (infeasible start) id77 provided
dr =    2f has bounded inverse, and satis   es a lipschitz condition on the sublevel
set

s = {(u, v)     dom f | kr(u, v)k2     kr(u(0), v(0))k2},

where u(0), v(0) are the starting players    choices.

there is a simple analog of the strong convexity condition in an unconstrained
minimization problem. we say the game with payo    function f is strongly convex-
concave if for some m > 0, we have    2
vvf (u, v) (cid:22)    mi, for
all (u, v)     s. not surprisingly, this strong convex-concave assumption implies the
bounded inverse condition (exercise 10.10).

uuf (u, v) (cid:23) mi and    2

10.3.5 examples

a simple example

we illustrate the infeasible start id77 on the equality constrained an-
alytic center problem (10.25). our    rst example is an instance with dimensions
n = 100 and m = 50, generated randomly, for which the problem is feasible and
bounded below. the infeasible start id77 is used, with initial primal

542

10 equality constrained minimization

and dual points x(0) = 1,   (0) = 0, and backtracking parameters    = 0.01 and
   = 0.5. the plot in    gure 10.1 shows the norms of the primal and dual residu-
als separately, versus iteration number, and the plot in    gure 10.2 shows the step
lengths. a full newton step is taken in iteration 8, so the primal residual becomes
(almost) zero, and remains (almost) zero. after around iteration 9 or so, the (dual)
residual converges quadratically to zero.

an infeasible example

we also consider a problem instance, of the same dimensions as the example above,
for which dom f does not intersect {z | az = b}, i.e., the problem is infeasible.
(this violates the basic assumption in the chapter that problem (10.1) is solvable, as
well as the assumptions made in   10.2.4; the example is meant only to show what
happens to the infeasible start id77 when dom f does not intersect
{z | az = b}.) the norm of the residual for this example is shown in    gure 10.3,
and the step length in    gure 10.4. here, of course, the step lengths are never one,
and the residual does not converge to zero.

a convex-concave game
our    nal example involves a convex-concave game on r100    r100, with payo   
function

f (u, v) = ut av + bt u + ct v     log(1     ut u) + log(1     vt v),

(10.32)

de   ned on

dom f = {(u, v) | ut u < 1, vt v < 1}.

the problem data a, b, and c were randomly generated. the progress of the
(infeasible start) id77, started at u(0) = v(0) = 0, with backtracking
parameters    = 0.01 and    = 0.5, is shown in    gure 10.5.

10.4 implementation

10.4.1 elimination

to implement the elimination method, we have to calculate a full rank matrix f
and an   x such that

{x | ax = b} = {f z +   x | z     rn   p}.

several methods for this are described in   c.5.

10.4.2 solving kkt systems

in this section we describe methods that can be used to compute the newton step
or infeasible newton step, both of which involve solving a set of linear equations

10.4

implementation

543

105

100

2

k

l
a
u
d
r
k

d
n
a

10   5

2

k

i
r
p
r
k

10   10

10   15
0

2

4
8
iteration number

6

10

12

figure 10.1 progress of infeasible start id77 on an equality con-
strained analytic centering problem with 100 variables and 50 constraints.
the    gure shows krprik2 (solid line), and krdualk2 (dashed line). note that
feasibility is achieved (and maintained) after 8 iterations, and convergence
is quadratic, starting from iteration 9 or so.

t

1

0.5

0

2

4
8
iteration number

6

10

12

figure 10.2 step length versus iteration number for the same example prob-
lem. a full step is taken in iteration 8, which results in feasibility from
iteration 8 on.

544

10 equality constrained minimization

102

2

k

l
a
u
d
r
k

d
n
a

2

k

i
r
p
r
k

101
0

5

10

iteration number

15

20

figure 10.3 progress of infeasible start id77 on an equality con-
strained analytic centering problem with 100 variables and 50 constraints,
for which dom f = r100
++ does not intersect {z | az = b}. the    gure shows
krprik2 (solid line), and krdualk2 (dashed line). in this case, the residuals do
not converge to zero.

t

0.3

0.2

0.1

0
0

5

10

iteration number

15

20

figure 10.4 step length versus iteration number for the infeasible example
problem. no full steps are taken, and the step lengths converge to zero.

10.4

implementation

545

105

100

2

k
)
v
,

u
(
f
   
k

10   5

10   10

10   15
0

2

4

iteration number

6

8

figure 10.5 progress of (infeasible start) id77 on a convex-
concave game. quadratic convergence becomes apparent after about 5 iter-
ations.

with kkt form

0 (cid:21)(cid:20) v

(cid:20) h at
here we assume h     sn
+, and a     rp  n with rank a = p < n. similar methods
can be used to compute the newton step for a convex-concave game, in which
the bottom right entry of the coe   cient matrix is negative semide   nite (see exer-
cise 10.13).

h (cid:21) .
w (cid:21) =    (cid:20) g

(10.33)

a

solving full kkt system

one straightforward approach is to simply solve the kkt system (10.33), which is
a set of n + p linear equations in n + p variables. the kkt matrix is symmetric,
but not positive de   nite, so a good way to do this is to use an ldlt factorization
(see   c.3.3). if no structure of the matrix is exploited, the cost is (1/3)(n + p)3
   ops. this can be a reasonable approach when the problem is small (i.e., n and p
are not too large), or when a and h are sparse.

solving kkt system via elimination

a method that is often better than directly solving the full kkt system is based
on eliminating the variable v (see   c.4). we start by describing the simplest case,
in which h     0. starting from the    rst of the kkt equations

we solve for v to obtain

hv + at w =    g,

av =    h,

v =    h    1(g + at w).

546

10 equality constrained minimization

substituting this into the second kkt equation yields ah    1(g + at w) = h, so we
have

w = (ah    1at )   1(h     ah    1g).
these formulas give us a method for computing v and w.

the matrix appearing in the formula for w is the schur complement s of h in

the kkt matrix:

because of the special structure of the kkt matrix, and our assumption that a
has rank p, the matrix s is negative de   nite.

s =    ah    1at .

algorithm 10.3 solving kkt system by block elimination.

given kkt system with h     0.
1. form h    1at and h    1g.
2. form schur complement s =    ah    1at .
3. determine w by solving sw = ah    1g     h.
4. determine v by solving hv =    at w     g.

step 1 can be done by a cholesky factorization of h, followed by p + 1 solves,
which costs f + (p + 1)s, where f is the cost of factoring h and s is the cost of
an associated solve. step 2 requires a p    n by n    p id127. if we
exploit no structure in this calculation, the cost is p2n    ops. (since the result is
symmetric, we only need to compute the upper triangular part of s.) in some cases
special structure in a and h can be exploited to carry out step 2 more e   ciently.
step 3 can be carried out by cholesky factorization of    s, which costs (1/3)p3
   ops if no further structure of s is exploited. step 4 can be carried out using the
factorization of h already calculated in step 1, so the cost is 2np + s    ops. the
total    op count, assuming that no structure is exploited in forming or factoring the
schur complement, is

f + ps + p2n + (1/3)p3

   ops (keeping only dominant terms). if we exploit structure in forming or factoring
s, the last two terms are even smaller.

if h can be factored e   ciently, then block elimination gives us a    op count
advantage over directly solving the kkt system using an ldlt factorization. for
example, if h is diagonal (which corresponds to a separable objective function),
we have f = 0 and s = n, so the total cost is p2n + (1/3)p3    ops, which grows only
linearly with n. if h is banded with bandwidth k     n, then f = nk2, s = 4nk, so
the total cost is around nk2 + 4nkp + p2n + (1/3)p3 which still grows only linearly
with n. other structures of h that can be exploited are block diagonal (which
corresponds to block separable objective function), sparse, or diagonal plus low
rank; see appendix c and   9.7 for more details and examples.

example 10.3 equality constrained analytic center. we consider the problem

minimize    pn

subject to ax = b.

i=1 log xi

10.4

implementation

547

here the objective is separable, so the hessian at x is diagonal:

h = diag(x   2

1 , . . . , x   2

n ).

if we compute the newton direction using a generic method such as an ldlt factor-
ization of the kkt matrix, the cost is (1/3)(n + p)3    ops.
if we compute the newton step using block elimination, the cost is np2 + (1/3)p3
   ops. this is much smaller than the cost of the generic method.

in fact this cost is the same as that of computing the newton step for the dual prob-
lem, described in example 10.2 on page 525. for the (unconstrained) dual problem,
the hessian is

where d is diagonal, with dii = (at   )   2
. forming this matrix costs np2    ops, and
solving for the newton step by a cholesky factorization of    hdual costs (1/3)p3    ops.

i

hdual =    adat ,

example 10.4 minimum length piecewise-linear curve subject to equality constraints.
we consider a piecewise-linear curve in r2 with knot points (0, 0), (1, x1), . . . , (n, xn).
to    nd the minimum length curve that satis   es the equality constraints ax = b, we
form the problem

minimize
subject to ax = b,

(cid:0)1 + x2
1(cid:1)1/2

+pn   1

i=1 (cid:0)1 + (xi+1     xi)2(cid:1)1/2

with variable x     rn, and a     rp  n. in this problem, the objective is a sum of
functions of pairs of adjacent variables, so the hessian h is tridiagonal. using block
elimination, we can compute the newton step in around p2n + (1/3)p3    ops.

elimination with singular h

the block elimination method described above obviously does not work when h
is singular, but a simple variation on the method can be used in this more general
case. the more general method is based on the following result: the kkt matrix
is nonsingular if and only if h + at qa     0 for some q (cid:23) 0, in which case,
h + at qa     0 for all q     0. (see exercise 10.1.) we conclude, for example, that
if the kkt matrix is nonsingular, then h + at a     0.
let q (cid:23) 0 be a matrix for which h +at qa     0. then the kkt system (10.33)
is equivalent to

(cid:20) h + at qa at

0 (cid:21)(cid:20) v

w (cid:21) =    (cid:20) g + at qh

a

h

(cid:21) ,

which can be solved using elimination since h + at qa     0.

10.4.3 examples

in this section we describe some longer examples, showing how structure can be
exploited to e   ciently compute the newton step. we also include some numerical
results.

548

10 equality constrained minimization

equality constrained analytic centering

we consider the equality constrained analytic centering problem

minimize
subject to ax = b.

f (x) =    pn

i=1 log xi

(see examples 10.2 and 10.3.) we compare three methods, for a problem of size
p = 100, n = 500.

the    rst method is newton   s method with equality constraints (  10.2). the

newton step    xnt is de   ned by the kkt system (10.11):

(cid:20) h at

0 (cid:21)(cid:20)    xnt

w (cid:21) =(cid:20)    g
0 (cid:21) ,

a

where h = diag(1/x2
n), and g =    (1/x1, . . . , 1/xn). as explained in
example 10.3, page 546, the kkt system can be e   ciently solved by elimination,
i.e., by solving

1, . . . , 1/x2

ah    1at w =    ah    1g,
and setting    xnt =    h    1(at w + g). in other words,
   xnt =     diag(x)2at w + x,

where w is the solution of

a diag(x)2at w = b.

(10.34)

figure 10.6 shows the error versus iteration. the di   erent curves correspond to
four di   erent starting points. we use a backtracking line search with    = 0.1,
   = 0.5.

the second method is newton   s method applied to the dual

(see example 10.2, page 525). here the newton step is obtained from solving

maximize

i=1 log(at   )i + n

g(  ) =    bt    +pn
a diag(y)2at      nt =    b + ay

(10.35)

where y = (1/(at   )1, . . . , 1/(at   )n). comparing (10.35) and (10.34) we see that
both methods have the same complexity. in    gure 10.7 we show the error for four
di   erent starting points. we use a backtracking line search with    = 0.1,    = 0.5.

the third method is the infeasible start id77 of   10.3, applied to

the optimality conditions

   f (x   ) + at       = 0,

ax    = b.

the newton step is obtained by solving

(cid:20) h at

0 (cid:21)(cid:20)    xnt

ax     b (cid:21) ,
     nt (cid:21) =    (cid:20) g + at   

a

10.4

implementation

549

105

100

   
p
   

)
)
k
(
x
(
f

10   5

10   10
0

5

10
k

15

20

figure 10.6 error f (x(k))     p    in newton   s method, applied to an equality
constrained analytic centering problem of size p = 100, n = 500. the
di   erent curves correspond to four di   erent starting points. final quadratic
convergence is clearly evident.

105

100

)
)
k
(
  
(
g
   
   
p

10   5

10   10
0

2

4

k

6

8

10

figure 10.7 error |g(  (k))     p   | in newton   s method, applied to the dual of
the equality constrained analytic centering problem.

550

10 equality constrained minimization

1010

105

2

k
)
)
k
(
  
,
)
k
(
x
(
r
k

100

10   5

10   10

10   15
0

5

10

k

15

20

25

figure 10.8 residual kr(x(k),   (k))k2 in the infeasible start id77,
applied to the equality constrained analytic centering problem.

where h = diag(1/x2
n), and g =    (1/x1, . . . , 1/xn). this kkt system
can be e   ciently solved by elimination, at the same cost as (10.34) or (10.35). for
example, if we    rst solve

1, . . . , 1/x2

a diag(x)2at w = 2ax     b,

then      nt and    xnt follow from
     nt = w       ,

   xnt = x     diag(x)2at w.

figure 10.8 shows the norm of the residual

r(x,   ) = (   f (x) + at   , ax     b)

versus iteration, for four di   erent starting points. we use a backtracking line search
with    = 0.1,    = 0.5.

the    gures show that for this problem, the dual method appears to be faster,
but only by a factor of two or three.
it takes about six iterations to reach the
region of quadratic convergence, as opposed to 12   15 in the primal method and
10   20 in the infeasible start id77.

the methods also di   er in the initialization they require. the primal method
requires knowledge of a primal feasible point, i.e., satisfying ax(0) = b, x(0)     0.
the dual method requires a dual feasible point, i.e., at   (0)     0. depending on
the problem, one or the other might be more readily available. the infeasible start
id77 requires no initialization; the only requirement is that x(0)     0.
optimal network    ow

we consider a connected directed graph or network with n edges and p + 1 nodes.
we let xj denote the    ow or tra   c on arc j, with xj > 0 meaning    ow in the

10.4

implementation

551

direction of the arc, and xj < 0 meaning    ow in the direction opposite the arc.
there is also a given external source (or sink)    ow si that enters (if si > 0) or
leaves (if si < 0) node i. the    ow must satisfy a conservation equation, which
states that at each node, the total    ow entering the node, including the external
sources and sinks, is zero. this conservation equation can be expressed as   ax = s
where   a     r(p+1)  n is the node incidence matrix of the graph,

  aij =         

1
   1
0

arc j leaves node i
arc j enters node i
otherwise.

the    ow conservation equation   ax = s is inconsistent unless 1t s = 0, which we
assume is the case. (in other words, the total of the source    ows must equal the
total of the sink    ows.) the    ow conservation equations   ax = s are also redundant,
since 1t   a = 0. to obtain an independent set of equations we can delete any one
equation, to obtain ax = b, where a     rp  n is the reduced node incidence matrix
of the graph (i.e., the node incidence matrix with one row removed) and b     rp is
reduced source vector (i.e., s with the associated entry removed).
in summary,    ow conservation is given by ax = b, where a is the reduced node
incidence matrix of the graph and b is the reduced source vector. the matrix a is
very sparse, since each column has at most two nonzero entries (which can only be
+1 or    1).
introduce the objective function

we will take tra   c    ows x as the variables, and the sources as given. we

f (x) =

  i(xi),

nxi=1

where   i : r     r is the    ow cost function for arc i. we assume that the    ow cost
functions are strictly convex and twice di   erentiable.
the problem of choosing the best    ow, that satis   es the    ow conservation re-

quirement, is

minimize pn

subject to ax = b.

i=1   i(xi)

(10.36)

here the hessian h is diagonal, since the objective is separable.

we have several choices for computing the newton step for the optimal network
   ow problem (10.36). the most straightforward is to solve the full kkt system,
using a sparse ldlt factorization.

for this problem it is probably better to compute the newton step using block
elimination. we can characterize the sparsity pattern of the schur complement
s =    ah    1at in terms of the graph: we have sij 6= 0 if and only if node i and
node j are connected by an arc. it follows that if the network is sparse, i.e., if each
node is connected by an arc to only a few other nodes, then the schur complement
s is sparse. in this case, we can exploit sparsity in forming s, and in the associated
factorization and solve steps, as well. we can expect the computational complexity
of computing the newton step to grow approximately linearly with the number of
arcs (which is the number of variables).

552

10 equality constrained minimization

optimal control

we consider the problem

minimize pn

subject to

t=1   t(z(t)) +pn    1

z(t + 1) = atz(t) + btu(t),

t=0   t(u(t))

t = 0, . . . , n     1.

here

    z(t)     rk is the system state at time t
    u(t)     rl is the input or control action at time t
      t : rk     r is the state cost function
      t : rl     r is the input cost function
    n is called the time horizon for the problem.

we assume that the input and state cost functions are strictly convex and twice dif-
ferentiable. the variables in the problem are u(0), . . . , u(n   1), and z(1), . . . , z(n ).
the initial state z(0) is given. the linear equality constraints are called the state
equations or dynamic evolution equations. we de   ne the overall optimization vari-
able x as

x = (u(0), z(1), u(1), . . . , u(n     1), z(n ))     rn (k+l).

since the objective is block separable (i.e., a sum of functions of z(t) and u(t)),

the hessian is block diagonal:

h = diag(r0, q1, . . . , rn    1, qn ),

where

rt =    2  t(u(t)),

t = 0, . . . , n     1,

qt =    2  t(z(t)),

t = 1, . . . , n.

we can collect all the equality constraints (i.e., the state equations) and express
them as ax = b where

      
      
      

0
0
0
...
i

0
0
0
...
0

      
          an    1    bn    1

0
0
0
...
0
i

                           

0
0
i
0
   a2    b2
...
...
0
0
0
0

a =

b =

   b0
0
0
...
0
0

a0z(0)

0
0
...
0
0

                           
                           

i
0
   a1    b1
0
0
...
...
0
0
0
0

.

                           

10.4

implementation

553

the number of rows of a (i.e., equality constraints) is n k.

directly solving the kkt system for the newton step, using a dense ldlt

factorization, would cost

(1/3)(2n k + n l)3 = (1/3)n 3(2k + l)3

   ops. using a sparse ldlt factorization would give a large improvement, since
the method would exploit the many zero entries in a and h.

in fact we can do better by exploiting the special block structure of h and
a, using block elimination to compute the newton step. the schur complement
s =    ah    1at turns out to be block tridiagonal, with k    k blocks:

s =    ah    1at
a1q   1
1

s11

                           

0
...
0
0

=

where

q   1
1 at
1
s22

a2q   1
2

...
0
0

0
q   1
2 at
2
s33
...
0
0

0
0
0
...

      
      
      
. . .
      
       an    1q   1

n    1

sn    1,n    1 q   1

n    1at
sn n

n    1

0
0
0
...

                           

s11 =    b0r   1
0 bt
sii =    ai   1q   1

0     q   1
1 ,
i   1at

i   1     bi   1r   1

i   1bt

i   1     q   1

i

,

i = 2, . . . , n.

in particular, s is banded, with bandwidth 2k     1, so we can factor it in order
k3n    ops. therefore we can compute the newton step in order k3n    ops, assuming
k     n . note that this grows linearly with the time horizon n , whereas for a generic
method, the    op count grows like n 3.
for this problem we could go one step further and exploit the block tridiagonal
structure of s. applying a standard block tridiagonal factorization method would
result in the classic riccati recursion for solving a quadratic optimal control prob-
lem. still, using only the banded nature of s yields an algorithm that is the same
order.

analytic center of a linear matrix inequality

we consider the problem

minimize
subject to

f (x) =     log det x
tr(aix) = bi,

i = 1, . . . , p,

(10.37)

where x     sn is the variable, ai     sn, bi     r, and dom f = sn
conditions for this problem are

++. the kkt

   x       1 +

mxi=1

     
i ai = 0,

tr(aix    ) = bi,

i = 1, . . . , p.

(10.38)

the dimension of the variable x is n(n + 1)/2. we could simply ignore the
special matrix structure of x, and consider it as (vector) variable x     rn(n+1)/2,

554

10 equality constrained minimization

and solve the problem (10.37) using a generic method for a problem with n(n+1)/2
variables and p equality constraints. the cost for computing a newton step would
then be at least

(1/3)(n(n + 1)/2 + p)3

   ops, which is order n6 in n. we will see that there are a number of far more
attractive alternatives.

a    rst option is to solve the dual problem. the conjugate of f is

f    (y ) = log det(   y )   1     n

++ (see example 3.23, page 92), so the dual problem is

with dom f     =    sn
maximize    bt    + log det(pp
with domain {   | pp
i=1   iai     0}. this is an unconstrained problem with variable
       rp. the optimal x     can be recovered from the optimal       by solving the    rst
(dual feasibility) equation in (10.38), i.e., x     = (pp
let us work out the cost of computing the newton step for the dual prob-
lem (10.39). we have to form the gradient and hessian of g, and then solve for the
newton step. the gradient and hessian are given by

i=1   iai) + n,

i ai)   1.

i=1      

(10.39)

i = 1 . . . , p,

i, j = 1, . . . , p,

where a = pp

   2g(  )ij =     tr(a   1aia   1aj),
   g(  )i = tr(a   1ai)     bi,
i=1   iai. to form    2g(  ) and    g(  ) we proceed as follows. we
   rst form a (pn2    ops), and a   1aj for each j (2pn3    ops). then we form the
matrix    2g(  ). each of the p(p + 1)/2 entries of    2g(  ) is the inner product of
two matrices in sn, each of which costs n(n + 1)    ops, so the total is (dropping
dominated terms) (1/2)p2n2    ops. forming    g(  ) is cheap since we already have
the matrices a   1ai. finally, we solve for the newton step       2g(  )   1   g(  ), which
costs (1/3)p3    ops. all together, and keeping only the leading terms, the total cost
of computing the newton step is 2pn3 + (1/2)p2n2 + (1/3)p3. note that this is
order n3 in n, which is far better than the simple primal method described above,
which is order n6.

we can also solve the primal problem more e   ciently, by exploiting its special
matrix structure. to derive the kkt system for the newton step    xnt at a feasible
x, we replace x     in the kkt conditions by x +    xnt and       by w, and linearize
the    rst equation using the    rst-order approximation

(x +    xnt)   1     x    1     x    1   xntx    1.

this gives the kkt system

   x    1 + x    1   xntx    1 +

pxi=1

wiai = 0,

tr(ai   xnt) = 0,

i = 1, . . . , p.

(10.40)
this is a set of n(n + 1)/2 + p linear equations in the variables    xnt     sn and
w     rp. if we solved these equations using a generic method, the cost would be
order n6.

10.4

implementation

555

we can use block elimination to solve the kkt system (10.40) far more e   -

ciently. we eliminate the variable    xnt, by solving the    rst equation to get

   xnt = x     x  pxi=1

wiai! x = x    

pxi=1

wixaix.

(10.41)

substituting this expression for    xnt into the other equation gives

tr(aj   xnt) = tr(ajx)    

pxi=1

wi tr(ajxaix) = 0,

j = 1, . . . , p.

this is a set of p linear equations in w:

cw = d

where cij = tr(aixajx), di = tr(aix). the coe   cient matrix c is symmetric
and positive de   nite, so a cholesky factorization can be used to    nd w. once we
have w, we can compute    xnt from (10.41).

the cost of this method is as follows. we form the products aix (2pn3    ops),
and then form the matrix c. each of the p(p + 1)/2 entries of c is the inner
product of two matrices in rn  n, so forming c costs p2n2    ops. then we solve
for w = c    1d, which costs (1/3)p3. finally we compute    xnt.
if we use the
   rst expression in (10.41), i.e.,    rst compute the sum and then pre- and post-
multiply with x, the cost is approximately pn2 + 3n3. all together, the total cost
is 2pn3 + p2n2 + (1/3)p3    ops to form the newton step for the primal problem,
using block elimination. this is far better than the simple method, which is order
n6. note also that the cost is the same as that of computing the newton step for
the dual problem.

556

10 equality constrained minimization

bibliography

the two key assumptions in our analysis of the infeasible start id77 (the
derivative dr has a bounded inverse and satis   es a lipschitz condition) are central to
most convergence proofs of newton   s method; see ortega and rheinboldt [or00] and
dennis and schnabel [ds96].

the relative merits of solving kkt systems via direct factorization of the full system, or
via elimination, have been extensively studied in the context of interior-point methods
for linear and quadratic programming; see, for example, wright [wri97, chapter 11] and
nocedal and wright [nw99,   16.1-2]. the riccati recursion from optimal control can
be interpreted as a method for exploiting the block tridiagonal structure in the schur
complement s of the example on page 552. this observation was made by rao, wright,
and rawlings [rwr98,   3.3].

exercises

exercises

557

equality constrained minimization

10.1 nonsingularity of the kkt matrix. consider the kkt matrix

(cid:20) p at
0 (cid:21) ,

a

where p     sn
(a) show that each of the following statements is equivalent to nonsingularity of the

+, a     rp  n, and rank a = p < n.

kkt matrix.

    n (p )     n (a) = {0}.
    ax = 0, x 6= 0 =    xt p x > 0.
    f t p f     0, where f     rn  (n   p) is a matrix for which r(f ) = n (a).
    p + at qa     0 for some q (cid:23) 0.

(b) show that if the kkt matrix is nonsingular, then it has exactly n positive and p

negative eigenvalues.

10.2 projected gradient method. in this problem we explore an extension of the gradient method
to equality constrained minimization problems. suppose f is convex and di   erentiable,
and x     dom f satis   es ax = b, where a     rp  n with rank a = p < n. the euclidean
projection of the negative gradient       f (x) on n (a) is given by

   xpg = argmin

au=0 k      f (x)     uk2.

(a) let (v, w) be the unique solution of

(cid:20) i at

0 (cid:21)(cid:20) v

w (cid:21) =(cid:20)       f (x)

a

0

(cid:21) .

show that v =    xpg and w = argminy k   f (x) + at yk2.

(b) what is the relation between the projected negative gradient    xpg and the negative

gradient of the reduced problem (10.5), assuming f t f = i?

(c) the projected gradient method for solving an equality constrained minimization
problem uses the step    xpg, and a backtracking line search on f . use the re-
sults of part (b) to give some conditions under which the projected gradient method
converges to the optimal solution, when started from a point x(0)     dom f with
ax(0) = b.

newton   s method with equality constraints

10.3 dual id77. in this problem we explore newton   s method for solving the dual
of the equality constrained minimization problem (10.1). we assume that f is twice
di   erentiable,    2f (x)     0 for all x     dom f , and that for each        rp, the lagrangian
l(x,   ) = f (x) +   t (ax     b) has a unique minimizer, which we denote x(  ).
(a) show that the dual function g is twice di   erentiable. find an expression for the
newton step for the dual function g, evaluated at   , in terms of f ,    f , and    2f ,
evaluated at x = x(  ). you can use the results of exercise 3.40.

558

10 equality constrained minimization

(b) suppose there exists a k such that

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

(cid:20)    2f (x) at

a

0 (cid:21)   1(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

    k

for all x     dom f . show that g is strongly concave, with    2g(  ) (cid:22)    (1/k)i.

10.4 strong convexity and lipschitz constant of the reduced problem. suppose f satis   es the
assumptions given on page 529. show that the reduced objective function   f (z) = f (f z+  x)
is strongly convex, and that its hessian is lipschitz continuous (on the associated sublevel
set   s). express the strong convexity and lipschitz constants of   f in terms of k, m , l,
and the maximum and minimum singular values of f .

10.5 adding a quadratic term to the objective. suppose q (cid:23) 0. the problem

minimize
subject to ax = b

f (x) + (ax     b)t q(ax     b)

is equivalent to the original equality constrained optimization problem (10.1).
newton step for this problem the same as the newton step for the original problem?

is the

10.6 the newton decrement. show that (10.13) holds, i.e.,

infeasible start id77

f (x)     inf{bf (x + v) | a(x + v) = b} =   (x)2/2.

10.7 assumptions for infeasible start id77. consider the set of assumptions given

on page 536.

(a) suppose that the function f is closed. show that this implies that the norm of the

residual, kr(x,   )k2, is closed.

10.8 infeasible start id77 and initially satis   ed equality constraints. suppose we use

(b) show that dr satis   es a lipschitz condition if and only if    2f does.
the infeasible start id77 to minimize f (x) subject to at

i x = bi, i = 1, . . . , p.

(a) suppose the initial point x(0) satis   es the linear equality at

linear equality will remain satis   ed for future iterates, i.e., if at

i x = bi. show that the
i x(k) = bi for all k.
(b) suppose that one of the equality constraints becomes satis   ed at iteration k, i.e.,
i x(k) = bi. show that at iteration k, all the equality

we have at
constraints are satis   ed.

i x(k   1) 6= bi, at

10.9 equality constrained id178 maximization. consider the equality constrained id178

maximization problem

minimize
subject to ax = b,

f (x) =pn

i=1 xi log xi

(10.42)

with dom f = rn
p < n.

++ and a     rp  n. we assume the problem is feasible and that rank a =

(a) show that the problem has a unique optimal solution x   .
(b) find a, b, and feasible x(0) for which the sublevel set

{x     rn

++ | ax = b, f (x)     f (x(0))}

is not closed. thus, the assumptions listed in   10.2.4, page 529, are not satis   ed for
some feasible initial points.

exercises

559

(c) show that the problem (10.42) satis   es the assumptions for the infeasible start

id77 listed in   10.3.3, page 536, for any feasible starting point.

(d) derive the lagrange dual of (10.42), and explain how to    nd the optimal solution
of (10.42) from the optimal solution of the dual problem. show that the dual problem
satis   es the assumptions listed in   10.2.4, page 529, for any starting point.

the results of part (b), (c), and (d) do not mean the standard id77 will fail,
or that the infeasible start id77 or dual method will work better in practice.
it only means our convergence analysis for the standard id77 does not apply,
while our convergence analysis does apply to the infeasible start and dual methods. (see
exercise 10.15.)

10.10 bounded inverse derivative condition for strongly convex-concave game. consider a convex-
uuf (u, v) (cid:23) mi and

concave game with payo    function f (see page 541). suppose    2
   2
vvf (u, v) (cid:22)    mi, for all (u, v)     dom f . show that

kdr(u, v)   1k2 = k   2f (u, v)   1k2     1/m.

implementation

10.11 consider the resource allocation problem described in example 10.1. you can assume the

fi are strongly convex, i.e., f       

i (z)     m > 0 for all z.

(a) find the computational e   ort required to compute a newton step for the reduced

problem. be sure to exploit the special structure of the newton equations.

(b) explain how to solve the problem via the dual. you can assume that the conjugate
functions f    
i , and their derivatives, are readily computable, and that the equation
f    
i (x) =    is readily solved for x, given   . what is the computational complexity of
   nding a newton step for the dual problem?

(c) what is the computational complexity of computing a newton step for the resource
allocation problem? be sure to exploit the special structure of the kkt equations.

10.12 describe an e   cient way to compute the newton step for the problem

minimize
subject to

tr(x    1)
tr(aix) = bi,

i = 1, . . . , p

with domain sn
++, assuming p and n have the same order of magnitude. also derive the
lagrange dual problem and give the complexity of    nding the newton step for the dual
problem.

10.13 elimination method for computing newton step for convex-concave game. consider a
convex-concave game with payo    function f : rp    rq     r (see page 541). we assume
that f is strongly convex-concave, i.e., for all (u, v)     dom f and some m > 0, we have
   2
uuf (u, v) (cid:23) mi and    2
(a) show how to compute the newton step using cholesky factorizations of    2

uuf (u, v)
and       2fvv(u, v). compare the cost of this method with the cost of using an ldlt
factorization of    f (u, v), assuming    2f (u, v) is dense.
   2
vvf (u, v). how much do you save, if you assume    2

(b) show how you can exploit diagonal or block diagonal structure in    2

vvf (u, v) (cid:22)    mi.

uvf (u, v) is dense?

uuf (u, v) and/or

numerical experiments

10.14 log-optimal investment. consider the log-optimal investment problem described in exer-
cise 4.60, without the constraint x (cid:23) 0. use newton   s method to compute the solution,

560

10 equality constrained minimization

with the following problem data: there are n = 3 assets, and m = 4 scenarios, with
returns

p1 =" 2
1 # ,

1.3

p2 =" 2
1 # ,

0.5

p3 =" 0.5
1 # ,

1.3

p4 =" 0.5
1 # .

0.5

the probabilities of the four scenarios are given by    = (1/3, 1/6, 1/3, 1/6).

10.15 equality constrained id178 maximization. consider the equality constrained id178

maximization problem

minimize
subject to ax = b,

f (x) =pn

i=1 xi log xi

++ and a     rp  n, with p < n. (see exercise 10.9 for some relevant
with dom f = rn
analysis.)
generate a problem instance with n = 100 and p = 30 by choosing a randomly (checking
that it has full rank), choosing   x as a random positive vector (e.g., with entries uniformly
distributed on [0, 1]) and then setting b = a  x. (thus,   x is feasible.)
compute the solution of the problem using the following methods.

(a) standard id77. you can use initial point x(0) =   x.
(b) infeasible start id77. you can use initial point x(0) =   x (to compare with

the standard id77), and also the initial point x(0) = 1.

(c) dual id77, i.e., the standard id77 applied to the dual problem.

verify that the three methods compute the same optimal point (and lagrange multiplier).
compare the computational e   ort per step for the three methods, assuming relevant
structure is exploited. (your implementation, however, does not need to exploit structure
to compute the newton step.)

10.16 convex-concave game. use the infeasible start id77 to solve convex-concave
games of the form (10.32), with randomly generated data. plot the norm of the residual
and step length versus iteration. experiment with the line search parameters and initial
point (which must satisfy kuk2 < 1, kvk2 < 1, however).

chapter 11

interior-point methods

11.1 inequality constrained minimization problems

in this chapter we discuss interior-point methods for solving id76
problems that include inequality constraints,

minimize
subject to

f0(x)
fi(x)     0,
ax = b,

i = 1, . . . , m

(11.1)

where f0, . . . , fm : rn     r are convex and twice continuously di   erentiable, and
a     rp  n with rank a = p < n. we assume that the problem is solvable, i.e., an
optimal x    exists. we denote the optimal value f0(x   ) as p   .
we also assume that the problem is strictly feasible, i.e., there exists x     d that
satis   es ax = b and fi(x) < 0 for i = 1, . . . , m. this means that slater   s constraint
quali   cation holds, so there exist dual optimal           rm,           rp, which together
with x    satisfy the kkt conditions

   f0(x   ) +pm

i=1      

ax    = b,

fi(x   )     0,
      (cid:23) 0
i    fi(x   ) + at       = 0
i fi(x   ) = 0,
     

i = 1, . . . , m

i = 1, . . . , m.

(11.2)

interior-point methods solve the problem (11.1) (or the kkt conditions (11.2))
by applying newton   s method to a sequence of equality constrained problems, or
to a sequence of modi   ed versions of the kkt conditions. we will concentrate on
a particular interior-point algorithm, the barrier method, for which we give a proof
of convergence and a complexity analysis. we also describe a simple primal-dual
interior-point method (in   11.7), but do not give an analysis.
we can view interior-point methods as another level in the hierarchy of convex
optimization algorithms. linear equality constrained quadratic problems are the
simplest. for these problems the kkt conditions are a set of linear equations,
which can be solved analytically. newton   s method is the next level in the hierarchy.
we can think of newton   s method as a technique for solving a linear equality

562

11

interior-point methods

constrained optimization problem, with twice di   erentiable objective, by reducing
it to a sequence of linear equality constrained quadratic problems. interior-point
methods form the next level in the hierarchy: they solve an optimization problem
with linear equality and inequality constraints by reducing it to a sequence of linear
equality constrained problems.

examples

many problems are already in the form (11.1), and satisfy the assumption that the
objective and constraint functions are twice di   erentiable. obvious examples are
lps, qps, qcqps, and gps in convex form; another example is linear inequality
constrained id178 maximization,

minimize pn

subject to f x (cid:22) g
ax = b,

i=1 xi log xi

++.

with domain d = rn
many other problems do not have the required form (11.1), with twice di   eren-
tiable objective and constraint functions, but can be reformulated in the required
form. we have already seen many examples of this, such as the transformation of
an unconstrained convex piecewise-linear minimization problem

minimize maxi=1,...,m(at

i x + bi)

(with nondi   erentiable objective), to the lp

minimize
subject to at

t

i x + bi     t,

i = 1, . . . , m

(which has twice di   erentiable objective and constraint functions).

other id76 problems, such as socps and sdps, are not readily
recast in the required form, but can be handled by extensions of interior-point
methods to problems with generalized inequalities, which we describe in   11.6.

11.2 logarithmic barrier function and central path

our goal is to approximately formulate the inequality constrained problem (11.1)
as an equality constrained problem to which newton   s method can be applied.
our    rst step is to rewrite the problem (11.1), making the inequality constraints
implicit in the objective:

minimize
subject to ax = b,

f0(x) +pm

i=1 i   (fi(x))

(11.3)

where i    : r     r is the indicator function for the nonpositive reals,

i   (u) =(cid:26) 0

u     0
    u > 0.

11.2 logarithmic barrier function and central path

563

10

5

0

   5
   3

   2

   1
u

0

1

figure 11.1 the dashed lines show the function i   (u), and the solid curves

the best approximation.

show bi   (u) =    (1/t) log(   u), for t = 0.5, 1, 2. the curve for t = 2 gives

the problem (11.3) has no inequality constraints, but its objective function is not
(in general) di   erentiable, so newton   s method cannot be applied.

11.2.1 logarithmic barrier

the basic idea of the barrier method is to approximate the indicator function i   
by the function

bi   (u) =    (1/t) log(   u),

dombi    =    r++,

where t > 0 is a parameter that sets the accuracy of the approximation. like

it increases to     as u increases to 0. figure 11.1 shows the function i   , and
becomes more accurate.

i   , the function bi    is convex and nondecreasing, and (by our convention) takes
on the value     for u > 0. unlike i   , however, bi    is di   erentiable and closed:
the approximation bi   , for several values of t. as t increases, the approximation
substituting bi    for i    in (11.3) gives the approximation

i=1    (1/t) log(   fi(x))

(11.4)

minimize
subject to ax = b.

f0(x) +pm

the objective here is convex, since    (1/t) log(   u) is convex and increasing in u,
and di   erentiable. assuming an appropriate closedness condition holds, newton   s
method can be used to solve it.

the function

  (x) =    

mxi=1

log(   fi(x)),

(11.5)

564

11

interior-point methods

with dom    = {x     rn | fi(x) < 0, i = 1, . . . , m}, is called the logarithmic barrier
or log barrier for the problem (11.1). its domain is the set of points that satisfy
the inequality constraints of (11.1) strictly. no matter what value the positive
parameter t has, the logarithmic barrier grows without bound if fi(x)     0, for
any i.
of course, the problem (11.4) is only an approximation of the original prob-
lem (11.3), so one question that arises immediately is how well a solution of (11.4)
approximates a solution of the original problem (11.3). intuition suggests, and we
will soon con   rm, that the quality of the approximation improves as the parameter
t grows.

on the other hand, when the parameter t is large, the function f0 + (1/t)   is
di   cult to minimize by newton   s method, since its hessian varies rapidly near the
boundary of the feasible set. we will see that this problem can be circumvented
by solving a sequence of problems of the form (11.4), increasing the parameter t
(and therefore the accuracy of the approximation) at each step, and starting each
newton minimization at the solution of the problem for the previous value of t.

for future reference, we note that the gradient and hessian of the logarithmic

barrier function    are given by

     (x) =

   2  (x) =

mxi=1
mxi=1

(see   a.4.2 and   a.4.4).

11.2.2 central path

1

   fi(x)   fi(x),
fi(x)2   fi(x)   fi(x)t +

1

mxi=1

1

   fi(x)   2fi(x)

we now consider in more detail the minimization problem (11.4). it will simplify
notation later on if we multiply the objective by t, and consider the equivalent
problem

minimize
subject to ax = b,

tf0(x) +   (x)

(11.6)

which has the same minimizers. we assume for now that the problem (11.6) can
be solved via newton   s method, and, in particular, that it has a unique solution
for each t > 0. (we will discuss this assumption in more detail in   11.3.3.)
for t > 0 we de   ne x   (t) as the solution of (11.6). the central path associated
with problem (11.1) is de   ned as the set of points x   (t), t > 0, which we call
the central points. points on the central path are characterized by the following
necessary and su   cient conditions: x   (t) is strictly feasible, i.e., satis   es

ax   (t) = b,

fi(x   (t)) < 0,

i = 1, . . . , m,

and there exists a          rp such that

0 = t   f0(x   (t)) +      (x   (t)) + at     

11.2 logarithmic barrier function and central path

565

= t   f0(x   (t)) +

mxi=1

1

   fi(x   (t))   fi(x   (t)) + at     

(11.7)

holds.

example 11.1 inequality form id135. the logarithmic barrier function
for an lp in inequality form,

ct x

minimize
subject to ax (cid:22) b,

(11.8)

is given by

  (x) =    

where at
are

1 , . . . , at

     (x) =
or, more compactly,

mxi=1

log(bi     at

i x),

dom    = {x | ax     b},

m are the rows of a. the gradient and hessian of the barrier function

mxi=1

1

bi     at
i x

ai,

   2  (x) =

mxi=1

1
(bi     at

i x)2

aiat
i ,

     (x) = at d,

   2  (x) = at diag(d)2a,

where the elements of d     rm are given by di = 1/(bi     at
i x). since x is strictly
feasible, we have d     0, so the hessian of    is nonsingular if and only if a has rank n.
the centrality condition (11.7) is

tc +

mxi=1

1

bi     at
i x

ai = tc + at d = 0.

(11.9)

we can give a simple geometric interpretation of the centrality condition. at a point
x   (t) on the central path the gradient      (x   (t)), which is normal to the level set of   
through x   (t), must be parallel to    c. in other words, the hyperplane ct x = ct x   (t)
is tangent to the level set of    through x   (t). figure 11.2 shows an example with
m = 6 and n = 2.

dual points from central path

from (11.7) we can derive an important property of the central path: every central
point yields a dual feasible point, and hence a lower bound on the optimal value
p   . more speci   cally, de   ne

     
i (t) =    

1

tfi(x   (t))

,

i = 1, . . . , m,

     (t) =     /t.

(11.10)

we claim that the pair      (t),      (t) is dual feasible.

first, it is clear that      (t)     0 because fi(x   (t)) < 0, i = 1, . . . , m. by

expressing the optimality conditions (11.7) as

   f0(x   (t)) +

mxi=1

i (t)   fi(x   (t)) + at      (t) = 0,
     

566

11

interior-point methods

c

x   

x   (10)

figure 11.2 central path for an lp with n = 2 and m = 6. the dashed
curves show three contour lines of the logarithmic barrier function   . the
central path converges to the optimal point x    as t        . also shown is the
point on the central path with t = 10. the optimality condition (11.9) at
this point can be veri   ed geometrically: the line ct x = ct x   (10) is tangent
to the contour line of    through x   (10).

we see that x   (t) minimizes the lagrangian

l(x,   ,   ) = f0(x) +

  ifi(x) +   t (ax     b),

mxi=1

for    =      (t) and    =      (t), which means that      (t),      (t) is a dual feasible pair.
therefore the dual function g(     (t),      (t)) is    nite, and

g(     (t),      (t)) = f0(x   (t)) +

i (t)fi(x   (t)) +      (t)t (ax   (t)     b)
     

mxi=1
= f0(x   (t))     m/t.

in particular, the duality gap associated with x   (t) and the dual feasible pair      (t),
     (t) is simply m/t. as an important consequence, we have

f0(x   (t))     p        m/t,

i.e., x   (t) is no more than m/t-suboptimal. this con   rms the intuitive idea that
x   (t) converges to an optimal point as t        .

example 11.2 inequality form id135. the dual of the inequality form
lp (11.8) is

maximize    bt   
subject to at    + c = 0
   (cid:23) 0.

from the optimality conditions (11.9), it is clear that

     
i (t) =

1
t(bi     at
i x   (t))

,

i = 1, . . . , m,

11.2 logarithmic barrier function and central path

567

is dual feasible, with dual objective value

   bt      (t) = ct x   (t) + (ax   (t)     b)t      (t) = ct x   (t)     m/t.

interpretation via kkt conditions

we can also interpret the central path conditions (11.7) as a continuous deformation
of the kkt optimality conditions (11.2). a point x is equal to x   (t) if and only if
there exists   ,    such that

   f0(x) +pm

ax = b,

fi(x)     0,
   (cid:23) 0
i=1   i   fi(x) + at    = 0

     ifi(x) = 1/t,

i = 1, . . . , m

i = 1, . . . , m.

(11.11)

the only di   erence between the kkt conditions (11.2) and the centrality condi-
tions (11.11) is that the complementarity condition      ifi(x) = 0 is replaced by
the condition      ifi(x) = 1/t. in particular, for large t, x   (t) and the associated
dual point      (t),      (t)    almost    satisfy the kkt optimality conditions for (11.1).

force    eld interpretation

we can give a simple mechanics interpretation of the central path in terms of
potential forces acting on a particle in the strictly feasible set c. for simplicity we
assume that there are no equality constraints.
we associate with each constraint the force

fi(x) =        (    log(   fi(x))) =

1
fi(x)   fi(x)

acting on the particle when it is at position x. the potential associated with the
total force    eld generated by the constraints is the logarithmic barrier   . as the
particle moves toward the boundary of the feasible set, it is strongly repelled by
the forces generated by the constraints.

now we imagine another force acting on the particle, given by

f0(x) =    t   f0(x),

when the particle is at position x. this objective force    eld acts to pull the particle
in the negative gradient direction, i.e., toward smaller f0. the parameter t scales
the objective force, relative to the constraint forces.

the central point x   (t) is the point where the constraint forces exactly balance
the objective force felt by the particle. as the parameter t increases, the particle is
more strongly pulled toward the optimal point, but it is always trapped in c by the
barrier potential, which becomes in   nite as the particle approaches the boundary.

example 11.3 force    eld interpretation for inequality form lp. the force    eld asso-
ciated with the ith constraint of the lp (11.8) is
fi(x) =    ai
bi     at
i x

.

568

11

interior-point methods

   c

   3c

figure 11.3 force    eld interpretation of central path. the central path is
shown as the dashed curve. the two points x   (1) and x   (3) are shown as
dots in the left and right plots, respectively. the objective force, which is
equal to    c and    3c, respectively, is shown as a heavy arrow. the other
arrows represent the constraint forces, which are given by an inverse-distance
law. as the strength of the objective force varies, the equilibrium position
of the particle traces out the central path.

this force is in the direction of the inward pointing normal to the constraint plane
hi = {x | at
i x = bi}, and has magnitude inversely proportional to the distance to
hi, i.e.,

kfi(x)k2 = kaik2
bi     at
i x

=

1

.

dist(x,hi)

in other words, each constraint hyperplane has an associated repulsive force, given
by the inverse distance to the hyperplane.
the term tct x is the potential associated with a constant force    tc on the particle.
this    objective force    pushes the particle in the direction of low cost. thus, x   (t)
is the equilibrium position of the particle when it is subject to the inverse-distance
constraint forces, and the objective force    tc. when t is very large, the particle is
pushed almost to the optimal point. the strong objective force is balanced by the
opposing constraint forces, which are large because we are near the feasible boundary.

figure 11.3 illustrates this interpretation for a small lp with n = 2 and m = 5. the
lefthand plot shows x   (t) for t = 1, as well as the constraint forces acting on it, which
balance the objective force. the righthand plot shows x   (t) and the associated forces
for t = 3. the larger value of objective force moves the particle closer to the optimal
point.

11.3 the barrier method

we have seen that the point x   (t) is m/t-suboptimal, and that a certi   cate of this
accuracy is provided by the dual feasible pair      (t),      (t). this suggests a very
straightforward method for solving the original problem (11.1) with a guaranteed
speci   ed accuracy   : we simply take t = m/   and solve the equality constrained

11.3 the barrier method

569

problem

minimize
subject to ax = b

(m/  )f0(x) +   (x)

using newton   s method. this method could be called the unconstrained minimiza-
tion method, since it allows us to solve the inequality constrained problem (11.1) to
a guaranteed accuracy by solving an unconstrained, or linearly constrained, prob-
lem. although this method can work well for small problems, good starting points,
and moderate accuracy (i.e.,    not too small), it does not work well in other cases.
as a result it is rarely, if ever, used.

11.3.1 the barrier method

a simple extension of the unconstrained minimization method does work well. it
is based on solving a sequence of unconstrained (or linearly constrained) mini-
mization problems, using the last point found as the starting point for the next
unconstrained minimization problem. in other words, we compute x   (t) for a se-
quence of increasing values of t, until t     m/  , which guarantees that we have an
  -suboptimal solution of the original problem. when the method was    rst proposed
by fiacco and mccormick in the 1960s, it was called the sequential unconstrained
minimization technique (sumt). today the method is usually called the barrier
method or path-following method. a simple version of the method is as follows.

algorithm 11.1 barrier method.

given strictly feasible x, t := t(0) > 0,    > 1, tolerance    > 0.

repeat

1. centering step.

compute x   (t) by minimizing tf0 +   , subject to ax = b, starting at x.

2. update. x := x   (t).
3. stopping criterion. quit if m/t <   .
4. increase t. t :=   t.

at each iteration (except the    rst one) we compute the central point x   (t) starting
from the previously computed central point, and then increase t by a factor    > 1.
the algorithm can also return    =      (t), and    =      (t), a dual   -suboptimal point,
or certi   cate for x.

we refer to each execution of step 1 as a centering step (since a central point
is being computed) or an outer iteration, and to the    rst centering step (the com-
putation of x   (t(0))) as the initial centering step. (thus the simple algorithm with
t(0) = m/   consists of only the initial centering step.) although any method for
linearly constrained minimization can be used in step 1, we will assume that new-
ton   s method is used. we refer to the newton iterations or steps executed during
the centering step as inner iterations. at each inner step, we have a primal fea-
sible point; we have a dual feasible point, however, only at the end of each outer
(centering) step.

570

11

interior-point methods

accuracy of centering

we should make some comments on the accuracy to which we solve the centering
problems. computing x   (t) exactly is not necessary since the central path has no
signi   cance beyond the fact that it leads to a solution of the original problem as
t        ; inexact centering will still yield a sequence of points x(k) that converges to
an optimal point. inexact centering, however, means that the points      (t),      (t),
computed from (11.10), are not exactly dual feasible. this can be corrected by
adding a correction term to the formula (11.10), which yields a dual feasible point
provided the computed x is near the central path, i.e., x   (t) (see exercise 11.9).

on the other hand, the cost of computing an extremely accurate minimizer of
tf0 +   , as compared to the cost of computing a good minimizer of tf0 +   , is
only marginally more, i.e., a few newton steps at most. for this reason it is not
unreasonable to assume exact centering.

choice of   

the choice of the parameter    involves a trade-o    in the number of inner and outer
iterations required. if    is small (i.e., near 1) then at each outer iteration t increases
by a small factor. as a result the initial point for the newton process, i.e., the
previous iterate x, is a very good starting point, and the number of newton steps
needed to compute the next iterate is small. thus for small    we expect a small
number of newton steps per outer iteration, but of course a large number of outer
iterations since each outer iteration reduces the gap by only a small amount. in
this case the iterates (and indeed, the iterates of the inner iterations as well) closely
follow the central path. this explains the alternate name path-following method.

on the other hand if    is large we have the opposite situation. after each
outer iteration t increases a large amount, so the current iterate is probably not
a very good approximation of the next iterate. thus we expect many more inner
iterations. this    aggressive    updating of t results in fewer outer iterations, since the
duality gap is reduced by the large factor    at each outer iteration, but more inner
iterations. with    large, the iterates are widely separated on the central path; the
inner iterates veer way o    the central path.

this trade-o    in the choice of    is con   rmed both in practice and, as we will
see, in theory. in practice, small values of    (i.e., near one) result in many outer
iterations, with just a few newton steps for each outer iteration. for    in a fairly
large range, from around 3 to 100 or so, the two e   ects nearly cancel, so the total
number of newton steps remains approximately constant. this means that the
choice of    is not particularly critical; values from around 10 to 20 or so seem to
work well. when the parameter    is chosen to give the best worst-case bound on
the total number of newton steps required, values of    near one are used.

choice of t(0)

another important issue is the choice of initial value of t. here the trade-o    is
simple: if t(0) is chosen too large, the    rst outer iteration will require too many it-
erations. if t(0) is chosen too small, the algorithm will require extra outer iterations,
and possibly too many inner iterations in the    rst centering step.

since m/t(0) is the duality gap that will result from the    rst centering step, one

11.3 the barrier method

571

reasonable choice is to choose t(0) so that m/t(0) is approximately of the same order
as f0(x(0))     p   , or    times this amount. for example, if a dual feasible point   ,
   is known, with duality gap    = f0(x(0))     g(  ,   ), then we can take t(0) = m/  .
thus, in the    rst outer iteration we simply compute a pair with the same duality
gap as the initial primal and dual feasible points.

another possibility is suggested by the central path condition (11.7). we can

interpret

inf

   (cid:13)(cid:13)(cid:13)t   f0(x(0)) +      (x(0)) + at   (cid:13)(cid:13)(cid:13)2

(11.12)

as a measure for the deviation of x(0) from the point x   (t), and choose for t(0) the
value that minimizes (11.12). (this value of t and    can be found by solving a
least-squares problem.)

a variation on this approach uses an a   ne-invariant measure of deviation be-
tween x and x   (t) in place of the euclidean norm. we choose t and    that minimize

  (t,   ) =(cid:16)t   f0(x(0)) +      (x(0)) + at   (cid:17)t

where

h    1

0 (cid:16)t   f0(x(0)) +      (x(0)) + at   (cid:17) ,

h0 = t   2f0(x(0)) +    2  (x(0)).

(it can be shown that inf      (t,   ) is the square of the newton decrement of tf0 +   
at x(0).) since    is a quadratic-over-linear function of    and t, it is convex.

infeasible start id77

in one variation on the barrier method, an infeasible start id77 (de-
scribed in   10.3) is used for the centering steps. thus, the barrier method is ini-
tialized with a point x(0) that satis   es x(0)     dom f0 and fi(x(0)) < 0, i = 1, . . . , m,
but not necessarily ax(0) = b. assuming the problem is strictly feasible, a full new-
ton step is taken at some point during the    rst centering step, and thereafter, the
iterates are all primal feasible, and the algorithm coincides with the (standard)
barrier method.

11.3.2 examples

id135 in inequality form

our    rst example is a small lp in inequality form,

ct x

minimize
subject to ax (cid:22) b

with a     r100  50. the data were generated randomly, in such a way that the
problem is strictly primal and dual feasible, with optimal value p    = 1.
the initial point x(0) is on the central path, with a duality gap of 100. the
barrier method is used to solve the problem, and terminated when the duality gap
is less than 10   6. the centering problems are solved by newton   s method with
backtracking, using parameters    = 0.01,    = 0.5. the stopping criterion for

572

11

interior-point methods

p
a
g

y
t
i
l
a
u
d

102

100

10   2

10   4

10   6

   = 50    = 150

   = 2

0

20

40

60
newton iterations

80

figure 11.4 progress of barrier method for a small lp, showing duality
gap versus cumulative number of newton steps. three plots are shown,
corresponding to three values of the parameter   : 2, 50, and 150. in each
case, we have approximately linear convergence of duality gap.

newton   s method is   (x)2/2     10   5, where   (x) is the newton decrement of the
function tct x +   (x).
the progress of the barrier method, for three values of the parameter   , is
shown in    gure 11.4. the vertical axis shows the duality gap on a log scale. the
horizontal axis shows the cumulative total number of inner iterations, i.e., newton
steps, which is the natural measure of computational e   ort. each of the plots has
a staircase shape, with each stair associated with one outer iteration. the width of
each stair tread (i.e., horizontal portion) is the number of newton steps required
for that outer iteration. the height of each stair riser (i.e., the vertical portion) is
exactly equal to (a factor of)   , since the duality gap is reduced by the factor    at
the end of each outer iteration.

the plots illustrate several typical features of the barrier method. first of all,
the method works very well, with approximately linear convergence of the duality
gap. this is a consequence of the approximately constant number of newton steps
required to re-center, for each value of   . for    = 50 and    = 150, the barrier
method solves the problem with a total number of newton steps between 35 and 40.
the plots in    gure 11.4 clearly show the trade-o    in the choice of   . for    = 2,
the treads are short; the number of newton steps required to re-center is around 2
or 3. but the risers are also short, since the duality gap reduction per outer iteration
is only a factor of 2. at the other extreme, when    = 150, the treads are longer,
typically around 7 newton steps, but the risers are also much larger, since the
duality gap is reduced by the factor 150 in each outer iteration.

the trade-o    in choice of    is further examined in    gure 11.5. we use the
barrier method to solve the lp, terminating when the duality gap is smaller than
10   3, for 25 values of    between 1.2 and 200. the plot shows the total number
of newton steps required to solve the problem, as a function of the parameter   .

11.3 the barrier method

573

140

120

100

80

60

40

20

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

0
0

40

80

  

120

160

200

figure 11.5 trade-o    in the choice of the parameter   , for a small lp. the
vertical axis shows the total number of newton steps required to reduce the
duality gap from 100 to 10   3, and the horizontal axis shows   . the plot
shows the barrier method works well for values of    larger than around 3,
but is otherwise not sensitive to the value of   .

this plot shows that the barrier method performs very well for a wide range of
values of   , from around 3 to 200. as our intuition suggests, the total number of
newton steps rises when    is too small, due to the larger number of outer iterations
required. one interesting observation is that the total number of newton steps does
not vary much for values of    larger than around 3. thus, as    increases over this
range, the decrease in the number of outer iterations is o   set by an increase in
the number of newton steps per outer iteration. for even larger values of   , the
performance of the barrier method becomes less predictable (i.e., more dependent
on the particular problem instance). since the performance does not improve with
larger values of   , a good choice is in the range 10     100.

geometric programming

we consider a geometric program in convex form,

minimize

0kx + b0k)(cid:17)
ikx + bik)(cid:17)     0,
with variable x     rn, and associated logarithmic barrier

k=1 exp(at
k=1 exp(at

subject to

log(cid:16)pk0
log(cid:16)pki
mxi=1

  (x) =    

log     log

kixk=1

exp(at

ikx + bik)! .

i = 1, . . . , m,

the problem instance we consider has n = 50 variables and m = 100 inequalities
(like the small lp considered above). the objective and constraint functions all

574

11

interior-point methods

p
a
g

y
t
i
l
a
u
d

102

100

10   2

10   4

10   6

   = 150

   = 50

   = 2

0

20

60

40
80
newton iterations

100

120

figure 11.6 progress of barrier method for a small gp, showing duality gap
versus cumulative number of newton steps. again we have approximately
linear convergence of duality gap.

have ki = 5 terms. the problem instance was generated randomly, in such a way
that it is strictly primal and dual feasible, with optimal value one.

we start with a point x(0) on the central path, with a duality gap of 100. the
barrier method is used to solve the problem, with parameters    = 2,    = 50, and
   = 150, and terminated when the duality gap is less than 10   6. the centering
problems are solved using newton   s method, with the same parameter values as in
the lp example, i.e.,    = 0.01,    = 0.5, and stopping criterion   (x)2/2     10   5.
figure 11.6 shows the duality gap versus cumulative number of newton steps.
this plot is very similar to the plot for lp, shown in    gure 11.4. in particular,
we see an approximately constant number of newton steps required per centering
step, and therefore approximately linear convergence of the duality gap.

the variation of the total number of newton steps required to solve the problem,
versus the parameter   , is very similar to that in the lp example. for this gp,
the total number of newton steps required to reduce the duality gap below 10   3
is around 30 (ranging from around 20 to 40 or so) for values of    between 10 and
200. so here, too, a good choice of    is in the range 10     100.

a family of standard form lps

in the examples above we examined the progress of the barrier method, in terms of
duality gap versus cumulative number of newton steps, for a randomly generated
instance of an lp and a gp, with similar dimensions. the results for the two
examples are remarkably similar; each shows approximately linear convergence of
duality gap with the number of newton steps. we also examined the variation in
performance with the parameter   , and found essentially the same results in the
two cases. for    above around 10, the barrier method performs very well, requiring
around 30 newton steps to bring the duality gap down from 102 to 10   6. in both

11.3 the barrier method

575

cases, the choice of    hardly a   ects the total number of newton steps required
(provided    is larger than 10 or so).

in this section we examine the performance of the barrier method as a function

of the problem dimensions. we consider lps in standard form,

minimize
subject to ax = b,

ct x

x (cid:23) 0

with a     rm  n, and explore the total number of newton steps required as a
function of the number of variables n and number of equality constraints m, for a
family of randomly generated problem instances. we take n = 2m, i.e., twice as
many variables as constraints.

the problems were generated as follows. the elements of a are independent and
identically distributed, with zero mean, unit variance normal distribution n (0, 1).
we take b = ax(0) where the elements of x(0) are independent, and uniformly
distributed in [0, 1]. this ensures that the problem is strictly primal feasible, since
x(0)     0 is feasible. to construct the cost vector c, we    rst compute a vector
z     rm with elements distributed according to n (0, 1) and a vector s     rn with
elements from a uniform distribution on [0, 1]. we then take c = at z + s. this
guarantees that the problem is strictly dual feasible, since at z     c.
the algorithm parameters we use are    = 100, and the same parameters for the
centering steps in the examples above: backtracking parameters    = 0.01,    = 0.5,
and stopping criterion   (x)2/2     10   5. the initial point is on the central path
with t(0) = 1 (i.e., gap n). the algorithm is terminated when the initial duality
gap is reduced by a factor 104, i.e., after completing two outer iterations.

figure 11.7 shows the duality gap versus iteration number for three problem
instances, with dimensions m = 50, m = 500, and m = 1000. the plots look very
much like the others, with approximately linear convergence of the duality gap.
the plots show a small increase in the number of newton steps required as the
problem size grows from 50 constraints (100 variables) to 1000 constraints (2000
variables).

to examine the e   ect of problem size on the number of newton steps required,
we generate 100 problem instances for each of 20 values of m, ranging from m = 10
to m = 1000. we solve each of these 2000 problems using the barrier method,
noting the number of newton steps required. the results are summarized in    g-
ure 11.8, which shows the mean and standard deviation in the number of newton
steps, for each value of m. the    rst comment we make is that the standard de-
viation is around 2 iterations, and appears to be approximately independent of
problem size. since the average number of steps required is near 25, this means
that the number of newton steps required varies only around   10%.

the plot shows that the number of newton steps required grows only slightly,
from around 21 to around 27, as the problem dimensions increase by a factor of
100. this behavior is typical for the barrier method in general: the number of
newton steps required grows very slowly with problem dimensions, and is almost
always around a few tens. of course, the computational e   ort to carry out one
newton step grows with the problem dimensions.

576

11

interior-point methods

104

102

100

10   2

p
a
g

y
t
i
l
a
u
d

10   4
0

10

m = 50 m = 500

m = 1000

20

30

newton iterations

40

50

figure 11.7 progress of barrier method for three randomly generated stan-
dard form lps of di   erent dimensions, showing duality gap versus cumula-
tive number of newton steps. the number of variables in each problem is
n = 2m. here too we see approximately linear convergence of the duality
gap, with a slight increase in the number of newton steps required for the
larger problems.

35

30

25

20

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

15

101

102
m

103

figure 11.8 average number of newton steps required to solve 100 randomly
generated lps of di   erent dimensions, with n = 2m. error bars show stan-
dard deviation, around the average value, for each value of m. the growth
in the number of newton steps required, as the problem dimensions range
over a 100 : 1 ratio, is very small.

11.3 the barrier method

577

11.3.3 convergence analysis

convergence analysis for the barrier method is straightforward. assuming that
tf0 +    can be minimized by newton   s method for t = t(0),   t(0),   2t(0), . . ., the
duality gap after the initial centering step, and k additional centering steps, is
m/(  kt(0)). therefore the desired accuracy    is achieved after exactly

(cid:24) log(m/(  t(0)))

log   

(cid:25)

(11.13)

centering steps, plus the initial centering step.

it follows that the barrier method works provided the centering problem (11.6)
is solvable by newton   s method, for t     t(0). for the standard id77, it
su   ces that for t     t(0), the function tf0 +   satis   es the conditions given in   10.2.4,
page 529:
its initial sublevel set is closed, the associated inverse kkt matrix is
bounded, and the hessian satis   es a lipschitz condition. (another set of su   cient
conditions, based on self-concordance, will be discussed in detail in   11.5.) if the
infeasible start id77 is used for centering, then the conditions listed
in   10.3.3, page 536, are su   cient to guarantee convergence.
assuming that f0, . . . , fm are closed, a simple modi   cation of the original
problem ensures that these conditions hold. by adding a constraint of the form
2     r2 to the problem, it follows that tf0 +    is strongly convex, for every
kxk2
t     0; in particular convergence of newton   s method, for the centering steps, is
guaranteed. (see exercise 11.4.)
while this analysis shows that the barrier method does converge, under reason-
able assumptions, it does not address a basic question: as the parameter t increases,
do the centering problems become more di   cult (and therefore take more and more
iterations)? numerical evidence suggests that for a wide variety of problems, this
is not the case; the centering problems appear to require a nearly constant number
of newton steps to solve, even as t increases. we will see (in   11.5) that this issue
can be resolved, for problems that satisfy certain self-concordance conditions.

11.3.4 newton step for modi   ed kkt equations

in the barrier method, the newton step    xnt, and associated dual variable are
given by the linear equations

(cid:20) t   2f0(x) +    2  (x) at

0 (cid:21)(cid:20)    xnt

  nt (cid:21) =    (cid:20) t   f0(x) +      (x)

a

0

(cid:21) .

(11.14)

in this section we show how these newton steps for the centering problem can be
interpreted as newton steps for directly solving the modi   ed kkt equations

   f0(x) +pm

i=1   i   fi(x) + at    = 0
ax = b

     ifi(x) = 1/t,

i = 1, . . . , m

(11.15)

in a particular way.

578

11

interior-point methods

to solve the modi   ed kkt equations (11.15), which is a set of n + p + m
nonlinear equations in the n + p + m variables x,   , and   , we    rst eliminate the
variables   i, using   i =    1/(tfi(x)). this yields

   f0(x) +

mxi=1

1

   tfi(x)   fi(x) + at    = 0,

ax = b,

(11.16)

which is a set of n + p equations in the n + p variables x and   .

to    nd the newton step for solving the set of nonlinear equations (11.16),
we form the taylor approximation for the nonlinear term occurring in the    rst
equation. for v small, we have the taylor approximation

   f0(x + v) +

       f0(x) +

1

1

mxi=1
   tfi(x + v)   fi(x + v)
mxi=1
   tfi(x)   fi(x) +    2f0(x)v
   tfi(x)   2fi(x)v +

1

1

mxi=1

+

mxi=1

tfi(x)2   fi(x)   fi(x)t v.

the newton step is obtained by replacing the nonlinear term in equation (11.16)
by this taylor approximation, which yields the linear equations

where

hv + at    =    g,
mxi=1
h =    2f0(x) +
   tfi(x)   2fi(x) +
mxi=1
   tfi(x)   fi(x).

g =    f0(x) +

1

1

now we observe that

av = 0,

(11.17)

1

tfi(x)2   fi(x)   fi(x)t

mxi=1

h =    2f0(x) + (1/t)   2  (x),

g =    f0(x) + (1/t)     (x),

so, from (11.14), the newton steps    xnt and   nt in the barrier method centering
step satisfy

comparing this with (11.17) shows that

th   xnt + at   nt =    tg,

a   xnt = 0.

v =    xnt,

   = (1/t)  nt.

this shows that the newton step for the centering problem (11.6) can be inter-
preted, after scaling the dual variable, as the newton step for solving the modi   ed
kkt equations (11.16).

in this approach, we    rst eliminated the variable    from the modi   ed kkt
equations, and then applied newton   s method to solve the resulting set of equations.
another variation on this approach is to directly apply newton   s method to the
modi   ed kkt equations, without    rst eliminating   . this method yields the so-
called primal-dual search directions, discussed in   11.7.

11.4 feasibility and phase i methods

579

11.4 feasibility and phase i methods

the barrier method requires a strictly feasible starting point x(0). when such a
point is not known, the barrier method is preceded by a preliminary stage, called
phase i, in which a strictly feasible point is computed (or the constraints are found
to be infeasible). the strictly feasible point found during phase i is then used as
the starting point for the barrier method, which is called the phase ii stage. in
this section we describe several phase i methods.

11.4.1 basic phase i method

we consider a set of inequalities and equalities in the variables x     rn,

i = 1, . . . , m,

fi(x)     0,

(11.18)
where fi : rn     r are convex, with continuous second derivatives. we assume
that we are given a point x(0)     dom f1                dom fm, with ax(0) = b.
our goal is to    nd a strictly feasible solution of these inequalities and equalities,
or determine that none exists. to do this we form the following optimization
problem:

ax = b,

minimize
subject to

s
fi(x)     s,
ax = b

i = 1, . . . , m

(11.19)

in the variables x     rn, s     r. the variable s can be interpreted as a bound on
the maximum infeasibility of the inequalities; the goal is to drive the maximum
infeasibility below zero.

this problem is always strictly feasible, since we can choose x(0) as starting
point for x, and for s, we can choose any number larger than maxi=1,...,m fi(x(0)).
we can therefore apply the barrier method to solve the problem (11.19), which is
called the phase i optimization problem associated with the inequality and equality
system (11.19).

we can distinguish three cases depending on the sign of the optimal value   p   

of (11.19).

1. if   p    < 0, then (11.18) has a strictly feasible solution. moreover if (x, s) is
feasible for (11.19) with s < 0, then x satis   es fi(x) < 0. this means we do
not need to solve the optimization problem (11.19) with high accuracy; we
can terminate when s < 0.

2. if   p    > 0, then (11.18) is infeasible. as in case 1, we do not need to solve
the phase i optimization problem (11.19) to high accuracy; we can terminate
when a dual feasible point is found with positive dual objective (which proves
that   p    > 0). in this case, we can construct the alternative that proves (11.18)
is infeasible from the dual feasible point.

3. if   p    = 0 and the minimum is attained at x    and s    = 0, then the set of
inequalities is feasible, but not strictly feasible. if   p    = 0 and the minimum
is not attained, then the inequalities are infeasible.

580

11

interior-point methods

in practice it is impossible to determine exactly that   p    = 0.
instead, an
optimization algorithm applied to (11.19) will terminate with the conclusion
that |  p   | <    for some small, positive   . this allows us to conclude that the
inequalities fi(x)           are infeasible, while the inequalities fi(x)        are
feasible.

sum of infeasibilities

there are many variations on the basic phase i method just described. one method
is based on minimizing the sum of the infeasibilities, instead of the maximum
infeasibility. we form the problem

minimize
subject to

1t s
fi(x)     si,
ax = b
s (cid:23) 0.

i = 1, . . . , m

(11.20)

for    xed x, the optimal value of si is max{fi(x), 0}, so in this problem we are
minimizing the sum of the infeasibilities. the optimal value of (11.20) is zero and
achieved if and only if the original set of equalities and inequalities is feasible.

this sum of infeasibilities phase i method has a very interesting property when
the system of equalities and inequalities (11.19) is infeasible. in this case, the op-
timal point for the phase i problem (11.20) often violates only a small number,
say r, of the inequalities. therefore, we have computed a point that satis   es many
(m     r) of the inequalities, i.e., we have identi   ed a large subset of inequalities
that is feasible. in this case, the dual variables associated with the strictly satis   ed
inequalities are zero, so we have also proved infeasibility of a subset of the inequal-
ities. this is more informative than    nding that the m inequalities, together, are
mutually infeasible. (this phenomenon is closely related to    1-norm id173,
or basis pursuit, used to    nd sparse approximate solutions; see   6.1.2 and   6.5.4).

example 11.4 comparison of phase i methods. we apply two phase i methods to
an infeasible set of inequalities ax (cid:22) b with dimensions m = 100, n = 50. the    rst
method is the basic phase i method

s

minimize
subject to ax (cid:22) b + 1s,

which minimizes the maximum infeasibility. the second method minimizes the sum
of the infeasibilities, i.e., solves the lp

1t s

minimize
subject to ax (cid:22) b + s

s (cid:23) 0.

figure 11.9 shows the distributions of the infeasibilities bi     at
i x for these two values
of x, denoted xmax and xsum, respectively. the point xmax satis   es 39 of the 100
inequalities, whereas the point xsum satis   es 79 of the inequalities.

replacemen

11.4 feasibility and phase i methods

581

60

50

40

30

20

10

r
e
b
m
u
n

60

50

40

30

20

10

r
e
b
m
u
n

0
   1    0.5

1

1.5

0.5
i xmax

0
bi     at

0
   1    0.5
figure 11.9 distributions of the infeasibilities bi     at
i x for an infeasible set
of 100 inequalities at
i x     bi, with 50 variables. the vector xmax used in
the left plot was obtained by the basic phase i algorithm.
it satis   es 39
of the 100 inequalities. in the right plot the vector xsum was obtained by
minimizing the sum of the infeasibilities. this vector satis   es 79 of the 100
inequalities.

0
bi     at

0.5
i xsum

1

1.5

termination near the phase ii central path

a simple variation on the basic phase i method, using the barrier method, has
the property that (when the equalities and inequalities are strictly feasible) the
central path for the phase i problem intersects the central path for the original
optimization problem (11.1).

we assume a point x(0)     d = dom f0    dom f1             dom fm, with ax(0) = b

is given. we form the phase i optimization problem

minimize
subject to

s
fi(x)     s,
f0(x)     m
ax = b,

i = 1, . . . , m

(11.21)

where m is a constant chosen to be larger than max{f0(x(0)), p   }.
we assume now that the original problem (11.1) is strictly feasible, so the
optimal value   p    of (11.21) is negative. the central path of (11.21) is characterized
by

mxi=1

1

s     fi(x)

=   t,

1

m     f0(x)   f0(x) +

mxi=1

1

s     fi(x)   fi(x) + at    = 0,

where   t is the parameter. if (x, s) is on the central path and s = 0, then x and   
satisfy

t   f0(x) +

mxi=1

1

   fi(x)   fi(x) + at    = 0

for t = 1/(m     f0(x)). this means that x is on the central path for the original

582

11

interior-point methods

optimization problem (11.1), with associated duality gap
m(m     f0(x))     m(m     p   ).

(11.22)

11.4.2 phase i via infeasible start id77

we can also carry out the phase i stage using an infeasible start id77,
applied to a modi   ed version of the original problem

minimize
subject to

f0(x)
fi(x)     0,
ax = b.

i = 1, . . . , m

we    rst express the problem in the (obviously equivalent) form

minimize
subject to

f0(x)
fi(x)     s,
ax = b,

i = 1, . . . , m

s = 0,

with the additional variable s     r. to start the barrier method, we use an infeasible
start id77 to solve

minimize
subject to ax = b,

t(0)f0(x)    pm

s = 0.

i=1 log(s     fi(x))

this can be initialized with any x     d, and any s > maxi fi(x). provided the
problem is strictly feasible, the infeasible start id77 will eventually
take an undamped step, and thereafter we will have s = 0, i.e., x strictly feasible.
the same trick can be applied if a point in d, the common domain of the
functions, is not known. we simply apply the infeasible start id77 to
the problem

minimize
subject to ax = b,

t(0)f0(x + z0)    pm

s = 0,

i=1 log(s     fi(x + zi))
zm = 0
z0 = 0,

. . . ,

with variables x, z0, . . . , zm, and s     r. we initialize zi so that x + zi     dom fi.
the main disadvantage of this approach to the phase i problem is that there is
no good stopping criterion when the problem is infeasible; the residual simply fails
to converge to zero.

11.4.3 examples

we consider a family of linear feasibility problems,

ax (cid:22) b(  )

where a     r50  20 and b(  ) = b +      b. the problem data are chosen so that the
inequalities are strictly feasible for    > 0 and infeasible for    < 0. for    = 0 the
problem is feasible but not strictly feasible.

11.4 feasibility and phase i methods

583

figure 11.10 shows the total number of newton steps required to    nd a strictly
feasible point, or a certi   cate of infeasibility, for 40 values of    in [   1, 1]. we use
the basic phase i method of   11.4.1, i.e., for each value of   , we form the lp

s

minimize
subject to ax (cid:22) b(  ) + s1.

the barrier method is used with    = 10, and starting point x = 0, s =     mini bi(  )+
1. the method terminates when a point (x, s) with s < 0 is found, or a feasible
solution z of the dual problem

maximize    b(  )t z
subject to at z = 0
1t z = 1
z (cid:23) 0

is found with    b(  )t z > 0.
the plot shows that when the inequalities are feasible, with some margin, it
takes around 25 newton steps to produce a strictly feasible point. conversely,
when the inequalities are infeasible, again with some margin, it takes around 35
steps to produce a certi   cate proving infeasibility. the phase i e   ort increases as
the set of inequalities approaches the boundary between feasible and infeasible,
i.e.,    near zero. when    is very near zero, so the inequalities are very near the
boundary between feasible and infeasible, the number of steps grows substantially.
figure 11.11 shows the total number of newton steps required for values of   
near zero. the plots show an approximately logarithmic increase in the number
of steps required to detect feasibility, or prove infeasibility, for problems very near
the boundary between feasible and infeasible.

this example is typical: the cost of solving a set of convex inequalities and
linear equalities using the barrier method is modest, and approximately constant,
as long as the problem is not very close to the boundary between feasibility and
infeasibility. when the problem is very close to the boundary, the number of
newton steps required to    nd a strictly feasible point or produce a certi   cate
of infeasibility grows. when the problem is exactly on the boundary between
strictly feasible and infeasible, for example, feasible but not strictly feasible, the
cost becomes in   nite.

feasibility using infeasible start id77

we also solve the same set of feasibility problems using the infeasible start newton
method, applied to the problem

minimize    pm

subject to ax + s = b(  ).

i=1 log si

we use backtracking parameters    = 0.01,    = 0.9, and initialize with x(0) = 0,
s(0) = 1,   (0) = 0. we consider only feasible problems (i.e.,    > 0) and terminate
once a feasible point is found. (we do not consider infeasible problems, since in
that case the residual simply converges to a positive number.) figure 11.12 shows
the number of newton steps required to    nd a feasible point, as a function of   .

584

11

interior-point methods

infeasible

feasible

100

80

60

40

20

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

0
   1

   0.5

0
  

0.5

1

figure 11.10 number of newton iterations required to detect feasibility or
infeasibility of a set of linear inequalities ax (cid:22) b +      b parametrized by
       r. the inequalities are strictly feasible for    > 0, and infeasible for
   < 0. for    larger than around 0.2, about 30 steps are required to compute
a strictly feasible point; for    less than    0.5 or so, it takes around 35 steps
to produce a certi   cate proving infeasibility. for values of    in between, and
especially near zero, more newton steps are required to determine feasibility.

100

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

80

60

40

20

100

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

80

60

40

20

0
   100

   10   2

   10   4

  

   10   6

0
10   6

10   4

  

10   2

100

figure 11.11 left. number of newton iterations required to    nd a proof of
infeasibility versus   , for    small and negative. right. number of newton
iterations required to    nd a strictly feasible point versus   , for    small and
positive.

11.5 complexity analysis via self-concordance

585

104

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

103

102

101

100
10   2

10   1

  

100

101

figure 11.12 number of iterations required to    nd a feasible point for a set
of linear inequalities ax (cid:22) b +      b parametrized by        r. the infeasible
start id77 is used, and terminated when a feasible point is found.
for    = 10, the starting point x(0) = 0 happened to be feasible (0 iterations).

the plot shows that for    larger than 0.3 or so, it takes fewer than 20 newton
steps to    nd a feasible point. in these cases the method is more e   cient than a
phase i method, which takes a total of around 30 newton steps. for smaller values
of   , the number of newton steps required grows dramatically, approximately as
1/  . for    = 0.01, the infeasible start id77 requires several thousand
iterations to produce a feasible point. in this region the phase i approach is far
more e   cient, requiring only 40 iterations or so.

these results are quite typical. the infeasible start id77 works
very well provided the inequalities are feasible, and not very close to the boundary
between feasible and infeasible. but when the feasible set is just barely nonempty
(as is the case in this example with small   ), a phase i method is far better. another
advantage of the phase i method is that it gracefully handles the infeasible case;
the infeasible start id77, in contrast, simply fails to converge.

11.5 complexity analysis via self-concordance

using the complexity analysis of newton   s method for self-concordant functions
(  9.6.4, page 503, and   10.2.4, page 531), we can give a complexity analysis of
the barrier method. the analysis applies to many common problems, and leads
to several interesting conclusions: it gives a rigorous bound on the total number
of newton steps required to solve a problem using the barrier method, and it
justi   es our observation that the centering problems do not become more di   cult
as t increases.

586

11

interior-point methods

11.5.1 self-concordance assumption

we make two assumptions.

    the function tf0 +    is closed and self-concordant for all t     t(0).
    the sublevel sets of (11.1) are bounded.

the second assumption implies that the centering problem has bounded sublevel
sets (see exercise 11.3), and, therefore, the centering problem is solvable. the
bounded sublevel set assumption also implies that the hessian of tf0 +    is positive
de   nite everywhere (see exercise 11.14). while the self-concordance assumption
restricts the complexity analysis to a particular class of problems, it is important
to emphasize that the barrier method works well in general, whether or not the
self-concordance assumption holds.

the self-concordance assumption holds for a variety of problems, including all

linear and quadratic problems. if the functions fi are linear or quadratic, then

tf0    

mxi=1

log(   fi)

is self-concordant for all values of t     0 (see   9.6). the complexity analysis given
below therefore applies to lps, qps, and qcqps.
in other cases, it is possible to reformulate the problem so the assumption of
self-concordance holds. as an example, consider the linear inequality constrained
id178 maximization problem

minimize pn

subject to f x (cid:22) g
ax = b.

i=1 xi log xi

the function

tf0(x) +   (x) = t

nxi=1

xi log xi    

mxi=1

log(gi     f t

i x),

1 , . . . , f t

where f t
m are the rows of f , is not closed (unless f x (cid:22) g implies x (cid:23) 0), or
self-concordant. we can, however, add the redundant inequality constraints x (cid:23) 0
to obtain the equivalent problem

i=1 xi log xi

minimize pn

subject to f x (cid:22) g
ax = b
x (cid:23) 0.

(11.23)

for this problem we have

tf0(x) +   (x) = t

nxi=1

xi log xi    

nxi=1

log xi    

mxi=1

log(gi     f t

i x),

11.5 complexity analysis via self-concordance

587

which is self-concordant and closed, for any t     0. (the function ty log y     log y
is self-concordant on r++, for all t     0; see exercise 11.13.) the complexity
analysis therefore applies to the reformulated linear inequality constrained id178
maximization problem (11.23).

as a more exotic example, consider the gp

minimize

subject to

f0(x) = log(cid:16)pk0
log(cid:16)pki

k=1 exp(at

k=1 exp(at

0kx + b0k)(cid:17)
ikx + bik)(cid:17)     0,

i = 1, . . . , m.

it is not clear whether or not the function

tf0(x) +   (x) = t log  k0xk=1

exp(at

0kx + b0k)!    

log     log

mxi=1

kixk=1

exp(at

ikx + bik)!

is self-concordant, so although the barrier method works, the complexity analysis
of this section need not hold.

we can, however, reformulate the gp in a form that de   nitely satis   es the self-
ikx + bik) we introduce

concordance assumption. for each (monomial) term exp(at
a new variable yik that serves as an upper bound,

using these new variables we can express the gp in the form

exp(at

ikx + bik)     yik.

minimize pk0
subject to pki

k=1 y0k
k=1 yik     1,
at
ikx + bik     log yik     0,
yik     0,
i = 0, . . . , m,

i = 1, . . . , m

i = 0, . . . , m,
k = 1, . . . , ki.

k = 1, . . . , ki

the associated logarithmic barrier is

mxi=0

kixk=1(cid:0)    log yik     log(log yik     at

ikx     bik)(cid:1)    

mxi=1

log 1    

yik! ,

kixk=1

which is closed and self-concordant (example 9.8, page 500). since the objective is
linear, it follows that tf0 +    is closed and self-concordant for any t.

11.5.2 newton iterations per centering step

the complexity theory of newton   s method for self-concordant functions, developed
in   9.6.4 (page 503) and   10.2.4 (page 531), shows that the number of newton
iterations required to minimize a closed strictly convex self-concordant function f
is bounded above by

f (x)     p   

  

+ c.

(11.24)

588

11

interior-point methods

here x is the starting point for newton   s method, and p    = inf x f (x) is the optimal
value. the constant    depends only on the backtracking parameters    and   , and
is given by

1
  

=

20     8  
    (1     2  )2 .

the constant c depends only on the tolerance   nt,

c = log2 log2(1/  nt),

and can reasonably be approximated as c = 6. the expression (11.24) is a quite
conservative bound on the number of newton steps required, but our interest in this
section is only to establish a complexity bound, concentrating on how it increases
with problem size and algorithm parameters.

in this section we use this result to derive a bound on the number of newton
steps required for one outer iteration of the barrier method, i.e., for computing
x   (  t), starting from x   (t). to lighten the notation we use x to denote x   (t), the
current iterate, and we use x+ to denote x   (  t), the next iterate. we use    and   
to denote      (t) and      (t), respectively.

the self-concordance assumption implies that

  tf0(x) +   (x)       tf0(x+)       (x+)

  

+ c

(11.25)

is an upper bound on the number of newton steps required to compute x+ = x   (  t),
starting at x = x   (t). unfortunately we do not know x+, and hence the upper
bound (11.25), until we actually compute x+, i.e., carry out the newton algorithm
(whereupon we know the exact number of newton steps required to compute x   (  t),
which defeats the purpose). we can, however, derive an upper bound on (11.25),
as follows:

=   tf0(x)       tf0(x+) +

  tf0(x) +   (x)       tf0(x+)       (x+)
mxi=1
log(     t  ifi(x+))     m log   
mxi=1
  ifi(x+)     m     m log   
      tf0(x)       tf0(x+)       t
  ifi(x+) +   t (ax+     b)!     m     m log   
=   tf0(x)       t f0(x+) +
mxi=1

      tf0(x)       tg(  ,   )     m     m log   
= m(       1     log   ).

this chain of equalities and inequalities needs some explanation. to obtain the
second line from the    rst, we use   i =    1/(tfi(x)). in the    rst inequality we use
the fact that log a     a     1 for a > 0. to obtain the fourth line from the third, we
use ax+ = b, so the extra term   t (ax+     b) is zero. the second inequality follows

11.5 complexity analysis via self-concordance

589

1

0.8

  
g
o
l

0.6

   
1
   
  

0.4

0.2

0
1

1.5

2
  

2.5

3

figure 11.13 the function        1     log   , versus   . the number of newton
steps required for one outer iteration of the barrier method is bounded by
(m/  )(       1     log   ) + c.

from the de   nition of the dual function:

g(  ,   ) = inf

z  f0(z) +
mxi=1
    f0(x+) +

  ifi(z) +   t (az     b)!
mxi=1
  ifi(x+) +   t (ax+     b).

the last line follows from g(  ,   ) = f0(x)     m/t.
m(       1     log   )

the conclusion is that

+ c

  

(11.26)

is an upper bound on (11.25), and therefore an upper bound on the number of
newton steps required for one outer iteration of the barrier method. the function
       1     log    is shown in    gure 11.13. for small    it is approximately quadratic;
for large    it grows approximately linearly. this    ts with our intuition that for   
near one, the number of newton steps required to center is small, whereas for large
  , it could well grow.

the bound (11.26) shows that the number of newton steps required in each
centering step is bounded by a quantity that depends mostly on   , the factor by
which t is updated in each outer step of the barrier method, and m, the number of
inequality constraints in the problem. it also depends, weakly, on the parameters
   and    used in the line search for the inner iterations, and in a very weak way
on the tolerance used to terminate the inner iterations. it is interesting to note
that the bound does not depend on n, the dimension of the variable, or p, the
number of equality constraints, or the particular values of the problem data, i.e.,
the objective and constraint functions (provided the self-concordance assumption
in   11.5.1 holds). finally, we note that it does not depend on t; in particular, as
t        , a uniform bound on the number of newton steps per outer iteration holds.

590

11

interior-point methods

11.5.3 total number of newton iterations

we can now give an upper bound on the total number of newton steps in the barrier
method, not counting the initial centering step (which we will analyze later, as part
of phase i). we multiply (11.26), which bounds the number of newton steps per
outer iteration, by (11.13), the number of outer steps required, to obtain

n =(cid:24) log(m/(t(0)  ))

log   

(cid:25)(cid:18) m(       1     log   )

  

+ c(cid:19) ,

(11.27)

an upper bound on the total number of newton steps required. this formula
shows that when the self-concordance assumption holds, we can bound the number
of newton steps required by the barrier method, for any value of    > 1.

if we    x    and m, the bound n is proportional to log(m/(t(0)  )), which is the
log of the ratio of the initial duality gap m/t(0) to the    nal duality gap   , i.e., the
log of the required duality gap reduction. we can therefore say that the barrier
method converges at least linearly, since the number of steps required to reach a
given precision grows logarithmically with the inverse of the precision.

if   , and the required duality gap reduction factor, are    xed, the bound n grows
linearly with m, the number of inequalities. the bound n is independent of the
other problem dimensions n and p, and the particular problem data or functions.
we will see below that by a particular choice of   , that depends on m, we can
obtain a bound on the number of newton steps that grows only as    m, instead of
as m.

finally, we analyze the bound n as a function of the algorithm parameter
  . as    approaches one, the    rst term in n grows large, and therefore so does
n . this is consistent with our intuition and observation that for    near one, the
number of outer iterations is very large. as    becomes large, the bound n grows
approximately as   / log   , this time because the bound on the number of newton
iterations required per outer iteration grows. this, too, is consistent with our
observations. as a result, the bound n has a minimum value as a function of   .

the variation of the bound with the parameter    is illustrated in    gure 11.14,

which shows the bound (11.27) versus    for the values

c = 6,

   = 1/375,

m/(t(0)  ) = 105,

m = 100.

the bound is qualitatively consistent with intuition, and our observations: it grows
very large as    approaches one, and increases, more slowly, as    becomes large. the
bound n has a minimum at        1.02, which gives a bound on the total number
of newton iterations around 8000. the complexity analysis of newton   s method is
conservative, but the basic trade-o    in the choice of    is re   ected in the plot. (in
practice, far larger values of   , from around 2 to 100, work very well, and require
a total number of newton iterations on the order of a few tens.)

choosing    as a function of m

when    (and the required duality gap reduction) is    xed, the bound (11.27) grows
linearly with m, the number of inequalities. it turns out we can obtain a better

11.5 complexity analysis via self-concordance

591

5 104

4 104

3 104

2 104

1 104

n

0
1

1.1
  

1.2

figure 11.14 the upper bound n on the total number of newton iterations,
given by equation (11.27), for c = 6,    = 1/375, m = 100, and a duality gap
reduction factor m/(t(0)  ) = 105, versus the barrier algorithm parameter   .

exponent for m by making    a function of m. suppose we choose

   = 1 + 1/   m.

(11.28)

then we can bound the second term in (11.27) as

       1     log    = 1/   m     log(1 + 1/   m)
    1/   m     1/   m + 1/(2m)

= 1/(2m)

(using     log(1 + a)        a + a2/2 for a     0). using concavity of the logarithm, we
also have

using these inequalities we can bound the total number of newton steps by

n     (cid:24) log(m/(t(0)  ))

log    = log(1 + 1/   m)     (log 2)/   m.
(cid:25)(cid:18) m(       1     log   )
(cid:25)(cid:18) 1
    (cid:24)   m
= l   m log2(m/(t(0)  ))m(cid:18) 1
    c1 + c2   m,

+ c(cid:19)
+ c(cid:19)

log   
log(m/(t(0)  ))

log 2

2  

2  

  

+ c(cid:19)

(11.29)

where

c1 =

1
2  

+ c,

c2 = log2(m/(t(0)  ))(cid:18) 1

2  

+ c(cid:19) .

592

11

interior-point methods

here c1 depends (and only weakly) on algorithm parameters for the centering
newton steps, and c2 depends on these and the required duality gap reduction.
note that the term log2(m/(t(0)  )) is exactly the number of bits of required duality
gap reduction.
for    xed duality gap reduction, the bound (11.29) grows as    m, whereas the
bound n in (11.27) grows like m, if the parameter    is held constant. for this
reason the barrier method, with parameter value (11.28), is said to be an order
   m method.
in practice, we would not use the value    = 1 + 1/   m, which is far too small,
or even decrease    as a function of m. our only interest in this value of    is that
it (approximately) minimizes our (very conservative) upper bound on the number
of newton steps, and yields an overall estimate that grows as    m, instead of m.

11.5.4 feasibility problems

in this section we analyze the complexity of a (minor) variation on the basic phase i
method described in   11.4.1, used to solve a set of convex inequalities,

f1(x)     0,

. . . ,

fm(x)     0,

(11.30)

where f1, . . . , fm are convex, with continuous second derivatives. (we will consider
equality constraints later.) we assume that the phase i problem

minimize
subject to

s
fi(x)     s,

i = 1, . . . , m

(11.31)

satis   es the conditions in   11.5.1. in particular we assume that the feasible set of
the inequalities (11.30) (which of course can be empty) is contained in a euclidean
ball of radius r:

{x | fi(x)     0, i = 1, . . . , m}     {x | kxk2     r}.

we can interpret r as a prior bound on the norm of any point in the feasible set of
the inequalities. this assumption implies that the sublevel sets of the phase i prob-
lem are bounded. without loss of generality, we will start the phase i method at the
point x = 0. we de   ne f = maxi fi(0), which is the maximum constraint violation,
assumed to be positive (since otherwise x = 0 satis   es the inequalities (11.30)).

we de   ne   p    as the optimal value of the phase i optimization problem (11.31).
the sign of   p    determines whether or not the set of inequalities (11.30) is feasible.
the magnitude of   p    also has a meaning. if   p    is positive and large (say, near f ,
the largest value it can have) it means that the set of inequalities is quite infeasible,
in the sense that for each x, at least one of the inequalities is substantially violated
(by at least   p   ). on the other hand, if   p    is negative and large, it means that
the set of inequalities is quite feasible, in the sense that there is not only an x for
which fi(x) are all nonpositive, but in fact there is an x for which fi(x) are all quite
negative (no more than   p   ). thus, the magnitude |  p   | is a measure of how clearly
the set of inequalities is feasible or infeasible, and therefore related to the di   culty

11.5 complexity analysis via self-concordance

593

of determining feasibility of the inequalities (11.30). in particular, if |  p   | is small,
it means the problem is near the boundary between feasibility and infeasibility.
to determine feasibility of the inequalities, we use a variation on the basic
phase i problem (11.31). we add a redundant linear inequality at x     1, to obtain

minimize
subject to

s
fi(x)     s,
at x     1.

i = 1, . . . , m

(11.32)

we will specify a later. our choice will satisfy kak2     1/r, so kxk2     r implies
at x     1, i.e., the extra constraint is redundant.
we will choose a and s0 so that x = 0, s = s0 is on the central path of the
problem (11.32), with a parameter value t(0), i.e., they minimize

t(0)s    

mxi=1

log(s     fi(x))     log(1     at x).

setting to zero the derivative with respect to s, we get

s0     fi(0)
setting to zero the gradient with respect to x yields

t(0) =

mxi=1

1

.

a =    

mxi=1

1

s0     fi(0)   fi(0).

(11.33)

(11.34)

so it remains only to pick the parameter s0; once we have chosen s0, the vector a
is given by (11.34), and the parameter t(0) is given by (11.33). since x = 0 and
s = s0 must be strictly feasible for the phase i problem (11.32), we must choose
s0 > f .

we must also pick s0 to make sure that kak2     1/r. from (11.34), we have

kak2    

mxi=1

1

s0     fi(0)k   fi(0)k    

mg
s0     f

,

where g = maxi k   fi(0)k2. therefore we can take s0 = mgr + f , which ensures
kak2     1/r, so the extra linear inequality is redundant.

using (11.33), we have

t(0) =

mxi=1

1

mgr + f     fi(0)    

1

mgr

,

since f = maxi fi(0). thus x = 0, s = s0 are on the central path for the phase i
problem (11.32), with initial duality gap

m + 1
t(0)     (m + 1)mgr.

594

11

interior-point methods

to solve the original inequalities (11.30) we need to determine the sign of   p   .
we can stop when either the primal objective value of (11.32) is negative, or the
dual objective value is positive. one of these two cases must occur when the duality
gap for (11.32) is less than |  p   |.
we use the barrier method to solve (11.32), starting from a central point with
duality gap no more than (m + 1)mgr, and terminating when (or before) the
duality gap is less than |  p   |. using the results of the previous section, this requires
no more than

(cid:24)   m + 1 log2

m(m + 1)gr

(cid:25)(cid:18) 1

+ c(cid:19)

2  

|  p   |

(11.35)
newton steps. (here we take    = 1 + 1/   m + 1, which gives a better complexity
exponent for m than a    xed value of   .)
the bound (11.35) grows only slightly faster than    m, and depends weakly on
the algorithm parameters used in the centering steps. it is approximately propor-
tional to log2((gr)/|  p   |), which can be interpreted as a measure of how di   cult
the particular feasibility problem is, or how close it is to the boundary between
feasibility and infeasibility.

feasibility problems with equality constraints

we can apply the same analysis to feasibility problems that include equality con-
straints, by eliminating the equality constraints. this does not a   ect the self-
concordance of the problem, but it does mean that g and r refer to the reduced,
or eliminated, problem.

11.5.5 combined phase i/phase ii complexity

in this section we give an end-to-end complexity analysis for solving the problem

minimize
subject to

f0(x)
fi(x)     0,
ax = b

i = 1, . . . , m

using (a variation on) the barrier method. first we solve the phase i problem

minimize
subject to

s
fi(x)     s,
f0(x)     m
ax = b
at x     1,

i = 1, . . . , m

which we assume satis   es the self-concordance and bounded sublevel set assump-
tions of   11.5.1. here we have added two redundant inequalities to the basic phase i
problem. the constraint f0(x)     m is added to guarantee that the phase i cen-
tral path intersects the central path for phase ii, as described in section   11.4.1
(see (11.21)). the number m is a prior bound on the optimal value of the problem.
the second added constraint is the linear inequality at x     1, where a is chosen

11.5 complexity analysis via self-concordance

595

as described in   11.5.4. we use the barrier method to solve this problem, with
   = 1 + 1/   m + 2, and the starting points x = 0, s = s0 given in   11.5.4.
to either    nd a strictly feasible point, or determine the problem is infeasible,

requires no more than

ni =(cid:24)   m + 2 log2

(m + 1)(m + 2)gr

|  p   |

(cid:25)(cid:18) 1

2  

+ c(cid:19)

(11.36)

newton steps, where g and r are as given in 11.5.4. if the problem is infeasible
we are done; if it is feasible, then we    nd a point in phase i, associated with s = 0,
that lies on the central path of the phase ii problem

minimize
subject to

f0(x)
fi(x)     0,
ax = b
at x     1.

i = 1, . . . , m

the associated initial duality gap of this initial point is no more than (m + 1)(m    
p   ) (see (11.22)). we assume the phase ii problem also satis   es the the self-
concordance and bounded sublevel set assumptions in   11.5.1.
we now proceed to phase ii, again using the barrier method. we must reduce
the duality gap from its initial value, which is no more than (m + 1)(m     p   ), to
some tolerance    > 0. this takes at most

nii =(cid:24)   m + 1 log2

(m + 1)(m     p   )

  

(cid:25)(cid:18) 1

2  

+ c(cid:19)

(11.37)

newton steps.
the total number of newton steps is therefore no more than ni + nii. this
bound grows with the number of inequalities m approximately as    m, and includes
two terms that depend on the particular problem instance,

log2

,

gr
|  p   |

log2

m     p   

  

.

11.5.6 summary

the complexity analysis given in this section is mostly of theoretical interest. in
particular, we remind the reader that the choice    = 1 + 1/   m, discussed in this
section, would be a very poor one to use in practice; its only advantage is that it
results in a bound that grows like    m instead of m. likewise, we do not recommend
adding the redundant inequality at x     1 in practice.
the actual bounds obtained from the analysis given here are far higher than the
numbers of iterations actually observed. even the order in the bound appears to
be conservative. the best bounds on the number of newton steps grow like    m,
whereas practical experience suggests that the number of newton steps hardly
grows at all with m (or any other parameter, in fact).

still, it is comforting to know that when the self-concordance condition holds,
we can give a uniform bound on the number of newton steps required in each

596

11

interior-point methods

centering step of the barrier method. an obvious potential pitfall of the barrier
method is the possibility that as t grows, the associated centering problems might
become more di   cult, requiring more newton steps. while practical experience
suggests that this is not the case, the uniform bound bolsters our con   dence that
it cannot happen.

finally, we mention that it is not yet clear whether or not there is a practical
advantage to formulating a problem so that the self-concordance condition holds.
all we can say is that when the self-concordance condition holds, the barrier method
will work well in practice, and we can give a worst case complexity bound.

11.6 problems with generalized inequalities

in this section we show how the barrier method can be extended to problems with
generalized inequalities. we consider the problem

minimize
subject to

f0(x)
fi(x) (cid:22)ki 0,
ax = b,

i = 1, . . . , m

(11.38)

where f0 : rn     r is convex, fi : rn     rki, i = 1, . . . , k, are ki-convex, and
ki     rki are proper cones. as in   11.1, we assume that the functions fi are twice
continuously di   erentiable, that a     rp  n with rank a = p, and that the problem
is solvable.

the kkt conditions for problem (11.38) are

ax   
=
fi(x   ) (cid:22)ki
     
i (cid:23)k    
i + at      
i=1 dfi(x   )t      
=
t fi(x   )
     
=
i

i

   f0(x   ) +pm

b
0,
0,
0
0,

i = 1, . . . , m
i = 1, . . . , m

i = 1, . . . , m,

(11.39)

where dfi(x   )     rki  n is the derivative of fi at x   . we will assume that prob-
lem (11.38) is strictly feasible, so the kkt conditions are necessary and su   cient
conditions for optimality of x   .

the development of the method is parallel to the case with scalar constraints.
once we develop a generalization of the logarithm function that applies to general
proper cones, we can de   ne a logarithmic barrier function for the problem (11.38).
from that point on, the development is essentially the same as in the scalar case.
in particular, the central path, barrier method, and complexity analysis are very
similar.

11.6 problems with generalized inequalities

597

11.6.1 logarithmic barrier and central path

generalized logarithm for a proper cone
we    rst de   ne the analog of the logarithm, log x, for a proper cone k     rq. we
say that    : rq     r is a generalized logarithm for k if

       is concave, closed, twice continuously di   erentiable, dom    = int k, and

   2  (y)     0 for y     int k.

    there is a constant    > 0 such that for all y    k 0, and all s > 0,

  (sy) =   (y) +    log s.

in other words,    behaves like a logarithm along any ray in the cone k.

we call the constant    the degree of    (since exp    is a homogeneous function of
degree   ). note that a generalized logarithm is only de   ned up to an additive
constant; if    is a generalized logarithm for k, then so is    + a, where a     r. the
ordinary logarithm is, of course, a generalized logarithm for r+.
we will use the following two properties, which are satis   ed by any generalized

logarithm: if y    k 0, then

     (y)    k     0,
which implies    is k-increasing (see   3.6.1), and
yt     (y) =   .

(11.40)

the    rst property is proved in exercise 11.15. the second property follows imme-
diately from di   erentiating   (sy) =   (y) +    log s with respect to s.

example 11.5 nonnegative orthant. the function   (x) =pn

+, with degree n. for x     0,

logarithm for k = rn

i=1 log xi is a generalized

     (x) = (1/x1, . . . , 1/xn),

so      (x)     0, and xt     (x) = n.

example 11.6 second-order cone. the function

is a generalized logarithm for the second-order cone

  (x) = log x2
x     rn+1 (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

  nxi=1

n+1    

x2

nxi=1

i!1/2

x2

i!
    xn+1         

,

k =         

598

11

interior-point methods

with degree 2. the gradient of    at a point x     int k is given by
j = 1, . . . , n

     (x)

   2xj

=

   xj

     (x)
   xn+1

=

2xn+1

(cid:0)x2
n+1    pn
(cid:0)x2
n+1    pn

i=1 x2

i=1 x2

i(cid:1) ,
i(cid:1) .

the identities      (x)     int k     = int k and xt     (x) = 2 are easily veri   ed.

example 11.7 positive semide   nite cone. the function   (x) = log det x is a gen-
eralized logarithm for the cone sp

+. the degree is p, since

log det(sx) = log det x + p log s

for s > 0. the gradient of    at a point x     sp

++ is equal to

     (x) = x    1.

thus, we have      (x) = x    1     0, and the inner product of x and      (x) is equal
to tr(xx    1) = p.

logarithmic barrier functions for generalized inequalities

returning to problem (11.38), let   1, . . . ,   m be generalized logarithms for the
cones k1, . . . , km, respectively, with degrees   1, . . . ,   m. we de   ne the logarithmic
barrier function for problem (11.38) as

  (x) =    

mxi=1

  i(   fi(x)),

dom    = {x | fi(x)     0, i = 1, . . . , m}.

convexity of    follows from the fact that the functions   i are ki-increasing, and
the functions fi are ki-convex (see the composition rule of   3.6.2).
the central path

the next step is to de   ne the central path for problem (11.38). we de   ne the
central point x   (t), for t     0, as the minimizer of tf0 +   , subject to ax = b, i.e.,
as the solution of

minimize
subject to ax = b

tf0(x)    pm

i=1   i(   fi(x))

(assuming the minimizer exists, and is unique). central points are characterized
by the optimality condition

t   f0(x) +      (x) + at   
mxi=1

= t   f0(x) +

dfi(x)t     i(   fi(x)) + at    = 0,

(11.41)

for some        rp, where dfi(x) is the derivative of fi at x.

11.6 problems with generalized inequalities

599

dual points on central path

as in the scalar case, points on the central path give dual feasible points for the
problem (11.38). for i = 1, . . . , m, de   ne

     
i (t) =

1
t      i(   fi(x   (t))),

(11.42)

and let      (t) =   /t, where    is the optimal dual variable in (11.41). we will
show that      
m(t), together with      (t), are dual feasible for the original
problem (11.38).

1(t), . . . ,      

first,      

i (t)    k    

i

rithms. second, it follows from (11.41) that the lagrangian

0, by the monotonicity property (11.40) of generalized loga-

l(x,      (t),      (t)) = f0(x) +

mxi=1

i (t)t fi(x) +      (t)t (ax     b)
     

is minimized over x by x = x   (t). the dual function g evaluated at (     (t),      (t))
is therefore equal to

g(     (t),      (t)) = f0(x   (t)) +

mxi=1

= f0(x   (t)) + (1/t)

= f0(x   (t))     (1/t)

     i(   fi(x   (t)))t fi(x   (t))

i (t)t fi(x   (t)) +      (t)t (ax   (t)     b)
     
mxi=1
mxi=1

  i,

where   i is the degree of   i. in the last line, we use the fact that yt     i(y) =   i
for y    ki 0, and therefore

i = 1, . . . , m.

(11.43)

thus, if we de   ne

i (t)t fi(x   (t)) =      i/t,
     
mxi=1

   =

  i,

then the primal feasible point x   (t) and the dual feasible point (     (t),      (t)) have
duality gap   /t. this is just like the scalar case, except that   , the sum of the
degrees of the generalized logarithms for the cones, appears in place of m, the
number of inequalities.

example 11.8 second-order cone programming. we consider an socp with variable
x     rn:

minimize
subject to

f t x
kaix + bik2     ct

i = 1, . . . , m,
where ai     rni  n. as we have seen in example 11.6, the function

i x + di,

(11.44)

  (y) = log y2

p+1    

i!

y2

pxi=1

600

11

interior-point methods

is a generalized logarithm for the second-order cone in rp+1, with degree 2. the
corresponding logarithmic barrier function for (11.44) is

  (x) =    

log((ct

i x + di)2     kaix + bik2
2),

(11.45)

mxi=1

with dom    = {x | kaix + bik2 < ct
on the central path is tf +      (x   (t)) = 0, where

i x + di, i = 1, . . . , m}. the optimality condition

     (x) =    2

mxi=1

it follows that the point

1

i x + di)2     kaix + bik2
(ct

i x + di)ci     at

2(cid:0)(ct

i (aix + bi)(cid:1) .

z   
i (t) =    
where   i = (ct

(ct

i x   (t) + di),

i = 1, . . . , m,

2, is strictly feasible in the dual problem

2
t  i

w   

i (t) =

(aix   (t) + bi),

2
t  i
i x   (t) + di)2     kaix   (t) + bik2
i=1(bt
i=1(at
kzik2     wi,

maximize    pm
subject to pm
mxi=1(cid:0)(aix   (t) + bi)t z   

i (t) + (ct

i zi + diwi)

i zi + ciwi) = f

i = 1, . . . , m.

i x   (t) + di)w   

i (t)(cid:1) =

2m

t

,

the duality gap associated with x   (t) and (z   (t), w   (t)) is

which agrees with the general formula   /t, since   i = 2.

example 11.9 semide   nite programming in inequality form. we consider the sdp
with variable x     rn,

ct x

minimize
subject to f (x) = x1f1 +        + xnfn + g (cid:22) 0,

where g, f1, . . . , fn     sp. the dual problem is

maximize
subject to

tr(gz)
tr(fiz) + ci = 0,
z (cid:23) 0.

i = 1, . . . , n

using the generalized logarithm log det x for the positive semide   nite cone sp
have the barrier function (for the primal problem)

+, we

  (x) = log det(   f (x)   1)

with dom    = {x | f (x)     0}. for strictly feasible x, the gradient of    is equal to

     (x)

   xi

= tr(   f (x)   1fi),

i = 1, . . . , n,

which gives us the optimality conditions that characterize central points:

tci + tr(   f (x   (t))   1fi) = 0,

i = 1, . . . , n.

hence the matrix

z    (t) =

1
t

(   f (x   (t)))   1

is strictly dual feasible, and the duality gap associated with x   (t) and z    (t) is p/t.

11.6 problems with generalized inequalities

601

11.6.2 barrier method

we have seen that the key properties of the central path generalize to problems
with generalized inequalities.

    computing a point on the central path involves minimizing a twice di   er-
entiable convex function subject to equality constraints (which can be done
using newton   s method).

    with the central point x   (t) we can associate a dual feasible point (     (t),      (t))
in particular, x   (t) is no more than   /t-

with associated duality gap   /t.
suboptimal.

this means we can apply the barrier method, exactly as described in   11.3, to the
problem (11.38). the number of outer iterations, or centering steps, required to
compute a central point with duality gap    starting at x   (t(0)) is equal to

(cid:24) log(  /(t(0)  ))

log   

(cid:25) ,

plus one initial centering step. the only di   erence between this result and the
associated one for the scalar case is that    takes the place of m.

phase i and feasibility problems
the phase i methods described in   11.4 are readily extended to problems with
generalized inequalities. let ei    ki 0 be some given, ki-positive vectors, for
i = 1, . . . , m. to determine feasibility of the equalities and generalized inequalities

f1(x) (cid:22)k1 0,

. . . ,

fl(x) (cid:22)km 0,

ax = b,

we solve the problem

minimize
subject to

s
fi(x) (cid:22)ki sei,
ax = b,

i = 1, . . . , m

with variables x and s     r. the optimal value   p    determines the feasibility
of the equalities and generalized inequalities, exactly as in the case of ordinary
inequalities. when   p    is positive, any dual feasible point with positive objective
gives an alternative that proves the set of equalities and generalized inequalities is
infeasible (see page 270).

11.6.3 examples

a small socp

we solve an socp

minimize
subject to

f t x
kaix + bik2     ct

i x + di,

i = 1, . . . , m,

602

11

interior-point methods

p
a
g

y
t
i
l
a
u
d

102

100

10   2

10   4

10   6

   = 50    = 200

   = 2

0

20

40

60
newton iterations

80

figure 11.15 progress of barrier method for an socp, showing duality gap
versus cumulative number of newton steps.

with x     r50, m = 50, and ai     r5  50. the problem instance was randomly
generated, in such a way that the problem is strictly primal and dual feasible, and
has optimal value p    = 1. we start with a point x(0) on the central path, with a
duality gap of 100.

the barrier method is used to solve the problem, using the barrier function

  (x) =    

mxi=1

log(cid:0)(ct

2(cid:1) .
i x + di)2     kaix + bik2

the centering problems are solved using newton   s method, with the same algorithm
parameters as in the examples of   11.3.2: backtracking parameters    = 0.01,    =
0.5, and a stopping criterion   (x)2/2     10   5.
figure 11.15 shows the duality gap versus cumulative number of newton steps.
the plot is very similar to those for linear and geometric programming, shown
in    gures 11.4 and 11.6, respectively. we see an approximately constant number
of newton steps required per centering step, and therefore approximately linear
convergence of the duality gap. for this example, too, the choice of    has little
e   ect on the total number of newton steps, provided    is at least 10 or so. as in
the examples for linear and geometric programming, a reasonable choice of    is in
the range 10     100, which results in a total number of newton steps around 30 (see
   gure 11.16).

a small sdp

our next example is an sdp

ct x

minimize

subject to pn

i=1 xifi + g (cid:22) 0

(11.46)

11.6 problems with generalized inequalities

603

140

120

100

80

60

40

20

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

0
0

40

80

  

120

160

200

figure 11.16 trade-o    in the choice of the parameter   , for a small socp.
the vertical axis shows the total number of newton steps required to reduce
the duality gap from 100 to 10   3, and the horizontal axis shows   .

with variable x     r100, and fi     s100, g     s100. the problem instance was
generated randomly, in such a way that the problem is strictly primal and dual
feasible, with p    = 1. the initial point is on the central path, with a duality gap
of 100.

we apply the barrier method with logarithmic barrier function

  (x) =     log det    

xifi     g! .

nxi=1

the progress of the barrier method for three values of    is shown in    gure 11.17.
note the similarity with the plots for linear, geometric, and second-order cone
programming, shown in    gures 11.4, 11.6, and 11.15. as in the other examples,
the parameter    has only a small e   ect on the e   ciency, provided it is not too
small. the number of newton steps required to reduce the duality gap by a factor
105, versus   , is shown in    gure 11.18.

a family of sdps

in this section we examine the performance of the barrier method as a function of
the problem dimensions. we consider a family of sdps of the form

1t x

minimize
subject to a + diag(x) (cid:23) 0,

(11.47)

with variable x     rn, and parameter a     sn. the matrices a are generated as
follows. for i     j, the coe   cients aij are generated from independent n (0, 1)
distributions. for i < j, we set aij = aji, so a     sn. we then scale a so that its
(spectral) norm is one.

604

11

interior-point methods

p
a
g

y
t
i
l
a
u
d

102

100

10   2

10   4

10   6

   = 150

   = 50

   = 2

0

20

40

60

newton iterations

80

100

figure 11.17 progress of barrier method for a small sdp, showing duality
gap versus cumulative number of newton steps. three plots are shown,
corresponding to three values of the parameter   : 2, 50, and 150.

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

140

120

100

80

60

40

20

0
0

20

40

60
  

80

100

120

figure 11.18 trade-o    in the choice of the parameter   , for a small sdp.
the vertical axis shows the total number of newton steps required to reduce
the duality gap from 100 to 10   3, and the horizontal axis shows   .

11.6 problems with generalized inequalities

605

105

p
a
g

y
t
i
l
a
u
d

100

10   5
0

10

20

30

40

50

newton iterations

n = 50 n = 500

n = 1000

figure 11.19 progress of barrier method for three randomly generated sdps
of the form (11.47), with di   erent dimensions. the plot shows duality gap
versus cumulative number of newton steps. the number of variables in each
problem is n.

the algorithm parameters are    = 20, and the same parameters for the center-
ing steps as in the examples above: backtracking parameters    = 0.01,    = 0.5,
and stopping criterion   (x)2/2     10   5. the initial point is on the central path
with t(0) = 1 (i.e., gap n). the algorithm is terminated when the initial duality
gap is reduced by a factor 8000, i.e., after completing three outer iterations.

figure 11.19 shows the duality gap versus iteration number for three problem
instances, with dimensions n = 50, n = 500, and n = 1000. the plots look very
much like the others, and very much like the ones for lps.

to examine the e   ect of problem size on the number of newton steps required,
we generate 100 problem instances for each of 20 values of n, ranging from n = 10
to n = 1000. we solve each of these 2000 problems using the barrier method, noting
the number of newton steps required. the results are summarized in    gure 11.20,
which shows the mean and standard deviation in the number of newton steps, for
each value of n. the plot looks very much like the one for lps, shown in    gure 11.8.
in particular, the number of newton steps required grows very slowly, from around
20 to 26 iterations, as the problem dimensions increase by a factor of 100.

11.6.4 complexity analysis via self-concordance

in this section we extend the complexity analysis of the barrier method for problems
with ordinary inequalities (given in   11.5), to problems with generalized inequali-
ties. we have already seen that the number of outer iterations is given by

(cid:24) log(  /t(0)  )

log   

(cid:25) ,

606

11

interior-point methods

35

30

25

20

s
n
o
i
t
a
r
e
t
i

n
o
t
w
e
n

15

101

102
n

103

figure 11.20 average number of newton steps required to solve 100 ran-
domly generated sdps (11.47) for each of 20 values of n, the problem size.
error bars show standard deviation, around the average value, for each value
of n. the growth in the average number of newton steps required, as the
problem dimensions range over a 100 : 1 ratio, is very small.

plus one initial centering step. it remains to bound the number of newton steps
required in each centering step, which we will do using the complexity theory of
newton   s method for self-concordant functions. for simplicity, we will exclude the
cost of the initial centering.

we make the same assumptions as in   11.5: the function tf0 +    is closed and

self-concordant for all t     t(0), and the sublevel sets of (11.38) are bounded.

example 11.10 second-order cone programming. the function

     (x) =     log x2

p+1    

i! ,

x2

pxi=1

is self-concordant (see example 9.8), so the logarithmic barrier function (11.45) sat-
is   es the closedness and self-concordance assumption for the socp (11.44).

example 11.11 semide   nite programming. the self-concordance assumption holds
for general semide   nite programs, using log det x as generalized logarithm for the
positive semide   nite cone. for example, for the standard form sdp

minimize
subject to

tr(cx)
tr(aix) = bi,
x (cid:23) 0,

i = 1, . . . , p

with variable x     sn, the function t(0) tr(cx)     log det x is self-concordant (and
closed), for any t(0)     0.

11.6 problems with generalized inequalities

607

we will see that, exactly as in the scalar case, we have

  tf0(x   (t)) +   (x   (t))       tf0(x   (  t))       (x   (  t))       (       1     log   ).

(11.48)

therefore when the self-concordance and bounded sublevel set conditions hold, the
number of newton steps per centering step is no more than

  (       1     log   )

  

+ c,

exactly as in the barrier method for problems with ordinary inequalities. once
we establish the basic bound (11.48), the complexity analysis for problems with
generalized inequalities is identical to the analysis for problems with ordinary in-
equalities, with one exception:    is the sum of the degrees of the cones, instead of
the number of inequalities.

generalized logarithm for dual cone

we will use conjugates to prove the bound (11.48). let    be a generalized logarithm
for the proper cone k, with degree   . the conjugate of the (convex) function      
is

(     )   (v) = sup

u (cid:0)vt u +   (u)(cid:1) .

this function is convex, and has domain    k     = {v | v    k     0}. de   ne    by

  (v) =    (     )   (   v) = inf

u (cid:0)vt u       (u)(cid:1) ,

dom    = int k    .

(11.49)

the function    is concave, and in fact is a generalized logarithm for the dual cone
k    , with the same parameter    (see exercise 11.17). we call    the dual logarithm
associated with the generalized logarithm   .

from (11.49) we obtain the inequality

  (v) +   (u)     ut v,

(11.50)

which holds for any u    k 0, v    k     0, with equality holding if and only      (u) = v
(or equivalently,      (v) = u).
(this inequality is just a variation on young   s
inequality, for concave functions.)

p+1    pp

example 11.12 second-order cone. the second-order cone has generalized logarithm
  (x) = log(x2
i=1 x2
i )1/2}. the
associated dual logarithm is

i=1 x2

i ), with dom    = {x     rp+1 | xp+1 > (pp
  (y) = log y2

i! + 2     log 4,

y2

pxi=1
with dom    = {y     rp+1 | yp+1 > (pp

p+1    

i )1/2} (see exercise 3.36). except for
a constant, it is the same as the original generalized logarithm for the second-order
cone.

i=1 y2

608

11

interior-point methods

example 11.13 positive semide   nite cone. the dual logarithm associated with
  (x) = log det x, with dom    = sp

++, is

  (y ) = log det y + p,

with domain dom       = sp
logarithm, except for a constant.

++ (see example 3.23). again, it is the same generalized

derivation of the basic bound

to simplify notation, we denote x   (t) as x, x   (  t) as x+,      
  . from t  i =      i(   fi(x)) (in (11.42)) and property (11.43), we conclude that

i (t) as   i, and      (t) as

  i(   fi(x)) +   i(t  i) =    t  t

i fi(x) =   i,

(11.51)

i.e., the inequality (11.50) holds with equality for the pair u =    fi(x) and v = t  i.
the same inequality for the pair u =    fi(x+), v =   t  i gives
i fi(x+),

  i(   fi(x+)) +   i(  t  i)          t  t
which becomes, using logarithmic homogeneity of   i,

  i(   fi(x+)) +   i(t  i) +   i log             t  t

i fi(x+).

subtracting the equality (11.51) from this inequality, we get

     i(   fi(x)) +   i(   fi(x+)) +   i log             i       t  t

i fi(x+),

and summing over i yields

  (x)       (x+) +    log                    t

mxi=1

  t
i fi(x+).

(11.52)

we also have, from the de   nition of the dual function,

f0(x)       /t = g(  ,   )

    f0(x+) +

= f0(x+) +

  t
i fi(x+) +   t (ax+     b)

  t
i fi(x+).

mxi=1
mxi=1

multiplying this inequality by   t and adding to the inequality (11.52), we get

  (x)       (x+) +    log    +   tf0(x)                tf0(x+)       ,

which when re-arranged gives

  tf0(x) +   (x)       tf0(x+)       (x+)       (       1     log   ),

the desired inequality (11.48).

11.7 primal-dual interior-point methods

609

11.7 primal-dual interior-point methods

in this section we describe a basic primal-dual interior-point method. primal-
dual interior-point methods are very similar to the barrier method, with some
di   erences.

    there is only one loop or iteration, i.e., there is no distinction between inner
and outer iterations as in the barrier method. at each iteration, both the
primal and dual variables are updated.

    the search directions in a primal-dual interior-point method are obtained
from newton   s method, applied to modi   ed kkt equations (i.e., the opti-
mality conditions for the logarithmic barrier centering problem). the primal-
dual search directions are similar to, but not quite the same as, the search
directions that arise in the barrier method.

    in a primal-dual interior-point method, the primal and dual iterates are not

necessarily feasible.

primal-dual interior-point methods are often more e   cient than the barrier
method, especially when high accuracy is required, since they can exhibit better
than linear convergence. for several basic problem classes, such as linear, quadratic,
second-order cone, geometric, and semide   nite programming, customized primal-
dual methods outperform the barrier method. for general nonlinear convex op-
timization problems, primal-dual interior-point methods are still a topic of active
research, but show great promise. another advantage of primal-dual algorithms
over the barrier method is that they can work when the problem is feasible, but
not strictly feasible (although we will not pursue this).

in this section we present a basic primal-dual method for (11.1), without conver-
gence analysis. we refer the reader to the references for a more thorough treatment
of primal-dual methods and their convergence analysis.

11.7.1 primal-dual search direction

as in the barrier method, we start with the modi   ed kkt conditions (11.15),
expressed as rt(x,   ,   ) = 0, where we de   ne

and t > 0. here f : rn     rm and its derivative matrix df are given by

   f0(x) + df (x)t    + at   
    diag(  )f (x)     (1/t)1

ax     b

(11.53)

rt(x,   ,   ) =      
f (x) =         

fm(x)

f1(x)

...

          ,

       ,
          .

df (x) =         

   f1(x)t

...

   fm(x)t

if x,   ,    satisfy rt(x,   ,   ) = 0 (and fi(x) < 0), then x = x   (t),    =      (t), and
   =      (t).
in particular, x is primal feasible, and   ,    are dual feasible, with

610

11

interior-point methods

duality gap m/t. the    rst block component of rt,

rdual =    f0(x) + df (x)t    + at   ,

is called the dual residual, and the last block component, rpri = ax     b, is called
the primal residual. the middle block,

rcent =     diag(  )f (x)     (1/t)1,

is the centrality residual, i.e., the residual for the modi   ed complementarity condi-
tion.

now consider the newton step for solving the nonlinear equations rt(x,   ,   ) =
0, for    xed t (without    rst eliminating   , as in   11.3.4), at a point (x,   ,   ) that
satisifes f (x)     0,        0. we will denote the current point and newton step as

y = (x,   ,   ),

   y = (   x,      ,      ),

respectively. the newton step is characterized by the linear equations

      

rt(y +    y)     rt(y) + drt(y)   y = 0,
i.e.,    y =    drt(y)   1rt(y). in terms of x,   , and   , we have
   x
     

   2f0(x) +pm

    diag(  )df (x)

i=1   i   2fi(x)
a

    diag(f (x))

df (x)t

at
0

0

0             

             =          

rdual
rcent
rpri

       .

(11.54)
the primal-dual search direction    ypd = (   xpd,      pd,      pd) is de   ned as the
solution of (11.54).

the primal and dual search directions are coupled, both through the coe   cient
matrix and the residuals. for example, the primal search direction    xpd depends
on the current value of the dual variables    and   , as well as x. we note also that
if x satis   es ax = b, i.e., the primal feasibility residual rpri is zero, then we have
a   xpd = 0, so    xpd de   nes a (primal) feasible direction: for any s, x + s   xpd
will satisfy a(x + s   xpd) = b.

comparison with barrier method search directions

the primal-dual search directions are closely related to the search directions used
in the barrier method, but not quite the same. we start with the linear equa-
tions (11.54) that de   ne the primal-dual search directions. we eliminate the vari-
able      pd, using

     pd =     diag(f (x))   1 diag(  )df (x)   xpd + diag(f (x))   1rcent,

which comes from the second block of equations. substituting this into the    rst
block of equations gives

a

0 (cid:21)(cid:20)    xpd
     pd (cid:21)

(cid:20) hpd at
=    (cid:20) rdual + df (x)t diag(f (x))   1rcent
=    (cid:20)    f0(x) + (1/t)pm

rpri

rpri

i=1

1

(cid:21)

   fi(x)   fi(x) + at   

(cid:21) ,

(11.55)

11.7 primal-dual interior-point methods

611

where

hpd =    2f0(x) +

mxi=1

  i   2fi(x) +

mxi=1

  i

   fi(x)   fi(x)   fi(x)t .

(11.56)

we can compare (11.55) to the equation (11.14), which de   nes the newton step
for the centering problem in the barrier method with parameter t. this equation
can be written as

(cid:21)

a

rpri

0 (cid:21)(cid:20)    xbar
  bar (cid:21)
(cid:20) hbar at
=    (cid:20) t   f0(x) +      (x)
=    (cid:20) t   f0(x) +pm
mxi=1

   fi(x)   2fi(x) +

i=1
rpri

1

mxi=1

where

hbar = t   2f0(x) +

1

   fi(x)   fi(x)

(cid:21) ,

(11.57)

1

fi(x)2   fi(x)   fi(x)t .

(11.58)

(here we give the general expression for the infeasible newton step; if the current x
is feasible, i.e., rpri = 0, then    xbar coincides with the feasible newton step    xnt
de   ned in (11.14).)

our    rst observation is that the two systems of equations (11.55) and (11.57)
are very similar. the coe   cient matrices in (11.55) and (11.57) have the same
structure; indeed, the matrices hpd and hbar are both positive linear combinations
of the matrices
   f1(x)   f1(x)t , . . . ,   fm(x)   fm(x)t .
   2f0(x),
this means that the same method can be used to compute the primal-dual search
directions and the barrier method newton step.

   2f1(x), . . . ,   2fm(x),

we can say more about the relation between the primal-dual equations (11.55)
and the barrier method equations (11.57). suppose we divide the    rst block of
equation (11.57) by t, and de   ne the variable      bar = (1/t)  bar        (where    is
arbitrary). then we obtain

(cid:20) (1/t)hbar at

0 (cid:21)(cid:20)    xbar

     bar (cid:21) =    (cid:20)    f0(x) + (1/t)pm

a

i=1

rpri

1

   fi(x)   fi(x) + at   

(cid:21) .

in this form, the righthand side is identical to the righthand side of the primal-dual
equations (evaluated at the same x,   , and   ). the coe   cient matrices di   er only
in the 1, 1 block:

hpd =    2f0(x) +

(1/t)hbar =    2f0(x) +

mxi=1
mxi=1

  i   2fi(x) +

mxi=1
   tfi(x)   2fi(x) +

1

  i

   fi(x)   fi(x)   fi(x)t ,
mxi=1

tfi(x)2   fi(x)   fi(x)t .

1

when x and    satisfy    fi(x)  i = 1/t, the coe   cient matrices, and therefore also
the search directions, coincide.

612

11

interior-point methods

11.7.2 the surrogate duality gap

in the primal-dual interior-point method the iterates x(k),   (k), and   (k) are not
necessarily feasible, except in the limit as the algorithm converges. this means
that we cannot easily evaluate a duality gap   (k) associated with step k of the
algorithm, as we do in (the outer steps of) the barrier method. instead we de   ne
the surrogate duality gap, for any x that satis   es f (x)     0 and    (cid:23) 0, as

    (x,   ) =    f (x)t   .

(11.59)

the surrogate gap      would be the duality gap, if x were primal feasible and   ,   
were dual feasible, i.e., if rpri = 0 and rdual = 0. note that the value of the
parameter t that corresponds to the surrogate duality gap      is m/    .

11.7.3 primal-dual interior-point method

we can now describe the basic primal-dual interior-point algorithm.

algorithm 11.2 primal-dual interior-point method.

given x that satis   es f1(x) < 0, . . . , fm(x) < 0,        0,    > 1,   feas > 0,    > 0.
repeat

1. determine t. set t :=   m/    .
2. compute primal-dual search direction    ypd.
3. line search and update.

determine step length s > 0 and set y := y + s   ypd.

until krprik2       feas, krdualk2       feas, and            .

in step 1, the parameter t is set to a factor    times m/    , which is the value of t
associated with the current surrogate duality gap     . if x,   , and    were central,
with parameter t (and therefore with duality gap m/t), then in step 1 we would
increase t by the factor   , which is exactly the update used in the barrier method.
values of the parameter    on the order of 10 appear to work well.

the primal-dual interior-point algorithm terminates when x is primal feasible
and   ,    are dual feasible (within the tolerance   feas) and the surrogate gap is
smaller than the tolerance   . since the primal-dual interior-point method often has
faster than linear convergence, it is common to choose   feas and    small.

line search

the line search in the primal-dual interior point method is a standard backtracking
line search, based on the norm of the residual, and modi   ed to ensure that        0
and f (x)     0. we denote the current iterate as x,   , and   , and the next iterate
as x+,   +, and   +, i.e.,

x+ = x + s   xpd,

  + =    + s     pd,

  + =    + s     pd.

11.7 primal-dual interior-point methods

613

the residual, evaluated at y+, will be denoted r+.

we    rst compute the largest positive step length, not exceeding one, that gives

  + (cid:23) 0, i.e.,

smax = sup{s     [0, 1] |    + s      (cid:23) 0}

= min{1, min{     i/     i |      i < 0}} .

we start the backtracking with s = 0.99smax, and multiply s by        (0, 1) until we
have f (x+)     0. we continue multiplying s by    until we have
krt(x+,   +,   +)k2     (1       s)krt(x,   ,   )k2.

common choices for the backtracking parameters    and    are the same as those for
newton   s method:    is typically chosen in the range 0.01 to 0.1, and    is typically
chosen in the range 0.3 to 0.8.

one iteration of the primal-dual interior-point algorithm is the same as one step
of the infeasible id77, applied to solving rt(x,   ,   ) = 0, but modi   ed to
ensure        0 and f (x)     0 (or, equivalently, with dom rt restricted to        0 and
f (x)     0). the same arguments used in the proof of convergence of the infeasible
start id77 show that the line search for the primal-dual method always
terminates in a    nite number of steps.

11.7.4 examples

we illustrate the performance of the primal-dual interior-point method for the
same problems considered in   11.3.2. the only di   erence is that instead of starting
with a point on the central path, as in   11.3.2, we start the primal-dual interior-
point method at a randomly generated x(0), that satis   es f (x)     0, and take
  (0)
i =    1/fi(x(0)), so the initial value of the surrogate gap is      = 100. the
parameter values we use for the primal-dual interior-point method are

   = 10,

   = 0.5,

   = 10   8,

   = 0.01.

small lp and gp
we    rst consider the small lp used in   11.3.2, with m = 100 inequalities and
n = 50 variables. figure 11.21 shows the progress of the primal-dual interior-point
method. two plots are shown: the surrogate gap     , and the norm of the primal
and dual residuals,

rfeas =(cid:0)krprik2

2(cid:1)1/2
2 + krdualk2

,

versus iteration number. (the initial point is primal feasible, so the plot shows the
norm of the dual feasibility residual.) the plots show that the residual converges
to zero rapidly, and becomes zero to numerical precision in 24 iterations. the
surrogate gap also converges rapidly. compared to the barrier method, the primal-
dual interior-point method is faster, especially when high accuracy is required.

figure 11.22 shows the progress of the primal-dual interior-point method on the

gp considered in   11.3.2. the convergence is similar to the lp example.

614

11

interior-point methods

102

100

10   2

    

10   4

10   6

10   8

10   10
0

5

105

100

10   5

s
a
e
f
r

10   10

10   15
0

5

25

30

10

15

20

25

30

iteration number

10

15

20

iteration number

figure 11.21 progress of the primal-dual interior-point method for an lp,
showing surrogate duality gap      and the norm of the primal and dual resid-
uals, versus iteration number. the residual converges rapidly to zero within
24 iterations; the surrogate gap also converges to a very small number in
about 28 iterations. the primal-dual interior-point method converges faster
than the barrier method, especially if high accuracy is required.

102

100

10   2

    

10   4

10   6

10   8

10   10
0

5

105

100

10   5

s
a
e
f
r

10   10

20

25

10   15
0

5

10

15

iteration number

10

15

iteration number

20

25

figure 11.22 progress of primal-dual interior-point method for a gp, show-
ing surrogate duality gap      and the norm of the primal and dual residuals
versus iteration number.

11.8

implementation

615

50

40

30

20

s
n
o
i
t
a
r
e
t
i

10

101

102
m

103

figure 11.23 number of iterations required to solve randomly generated
standard lps of di   erent dimensions, with n = 2m. error bars show stan-
dard deviation, around the average value, for 100 instances of each dimen-
sion. the growth in the number of iterations required, as the problem di-
mensions range over a 100 : 1 ratio, is approximately logarithmic.

a family of lps

here we examine the performance of the primal-dual method as a function of
the problem dimensions, for the same family of standard form lps considered
in   11.3.2. we use the primal-dual interior-point method to solve the same 2000
instances, which consist of 100 instances for each value of m. the primal-dual
algorithm is started at x(0) = 1,   (0) = 1,   (0) = 0, and terminated using tolerance
   = 10   8. figure 11.23 shows the average, and standard deviation, of the number
of iterations required versus m. the number of iterations ranges from 15 to 35,
and grows approximately as the logarithm of m. comparing with the results for
the barrier method shown in    gure 11.8, we see that the number of iterations in
the primal-dual method is only slightly higher, despite the fact that we start at
infeasible starting points, and solve the problem to a much higher accuracy.

11.8 implementation

the main e   ort in the barrier method is computing the newton step for the cen-
tering problem, which consists of solving sets of linear equations of the form

where

a

(cid:20) h at
mxi=1

1

0 (cid:21)(cid:20)    xnt

  nt (cid:21) =    (cid:20) g
0 (cid:21) ,
mxi=1

h = t   2f0(x) +

fi(x)2   fi(x)   fi(x)t +

(11.60)

1

   fi(x)   2fi(x)

616

11

interior-point methods

g = t   f0(x) +

mxi=1

1

   fi(x)   fi(x).

the newton equations for the primal-dual method have exactly the same structure,
so our observations in this section apply to the primal-dual method as well.

the coe   cient matrix of (11.60) has kkt structure, so all of the discussion
in   9.7 and   10.4 applies here. in particular, the equations can be solved by elimi-
nation, and structure such as sparsity or diagonal plus low rank can be exploited.
let us give some generic examples in which the special structure of the kkt equa-
tions can be exploited to compute the newton step more e   ciently.

sparse problems

if the original problem is sparse, which means that the objective and every con-
straint function each depend on only a modest number of variables, then the gradi-
ents and hessian matrices of the objective and constraint functions are all sparse,
as is the coe   cient matrix a. provided m is not too big, the matrix h is then
likely to be sparse, so a sparse matrix method can be used to compute the newton
step. the method will likely work well if there are a few relatively dense rows and
columns in the kkt matrix, which would occur, for example, if there were a few
equality constraints involving a large number of variables.

separable objective and a few linear inequality constraints

suppose the objective function is separable, and there are only a relatively small
number of linear equality and inequality constraints. then    2f0(x) is diagonal,
and the terms    2fi(x) vanish, so the matrix h is diagonal plus low rank. since h
is easily inverted, we can solve the kkt equations e   ciently. the same method
can be applied whenever    2f0(x) is easily inverted, e.g., banded, sparse, or block
diagonal.

11.8.1 standard form id135

we    rst discuss the implementation of the barrier method for the standard form
lp

minimize
subject to ax = b,

ct x

x (cid:23) 0,

with a     rm  n. the newton equations for the centering problem

minimize
subject to ax = b

tct x    pn

i=1 log xi

are given by

(cid:20) diag(x)   2 at

0 (cid:21)(cid:20)    xnt

  nt (cid:21) =(cid:20)    tc + diag(x)   11

a

0

(cid:21) .

11.8

implementation

617

these equations are usually solved by block elimination of    xnt. from the    rst
equation,

   xnt = diag(x)2(   tc + diag(x)   11     at   nt)

=    t diag(x)2c + x     diag(x)2at   nt.

substituting in the second equation yields

a diag(x)2at   nt =    ta diag(x)2c + b.

the coe   cient matrix is positive de   nite since by assumption rank a = m. more-
over if a is sparse, then usually a diag(x)2at is sparse, so a sparse cholesky
factorization can be used.

11.8.2    1-norm approximation

consider the    1-norm approximation problem

minimize

kax     bk1

with a     rm  n. we will discuss the implementation assuming m and n are large,
and a is structured, e.g., sparse, and compare it with the cost of the corresponding
least-squares problem

minimize

kax     bk2
2 .

we start by expressing the    1-norm approximation problem as an lp by intro-

ducing auxiliary variables y     rm:
1t y

minimize

the newton equation for the centering problem is

   b (cid:21) .

subject to (cid:20) a    i

   a    i (cid:21)(cid:20) x
0 d2 (cid:21)(cid:20) a    i

y (cid:21) (cid:22)(cid:20) b
   a    i (cid:21)(cid:20)    xnt

0

   i    i (cid:21)(cid:20) d1
(cid:20) at    at
d1 = diag(b     ax + y)   2,

   ynt (cid:21) =    (cid:20) at g1
d2 = diag(   b + ax + y)   2

g2

(cid:21)

where

and

g1 = diag(b     ax + y)   11     diag(   b + ax + y)   11
g2 = t1     diag(b     ax + y)   11     diag(   b + ax + y)   11.

if we multiply out the lefthand side, this can be simpli   ed as

(cid:20) at (d1 + d2)a    at (d1     d2)

   (d1     d2)a

d1 + d2

(cid:21)(cid:20)    xnt
   ynt (cid:21) =    (cid:20) at g1

g2

(cid:21) .

618

11

interior-point methods

applying block elimination to    ynt, we can reduce this to

at da   xnt =    at g

(11.61)

where

and

d = 4d1d2(d1 + d2)   1 = 2(diag(y)2 + diag(b     ax)2)   1

after solving for    xnt, we obtain    ynt from

g = g1 + (d1     d2)(d1 + d2)   1g2.

   ynt = (d1 + d2)   1(   g2 + (d1     d2)a   xnt).

it is interesting to note that (11.61) are the normal equations of a weighted least-
squares problem

minimize

kd1/2(a   x + d   1g)k2.

in other words, the cost of solving the    1-norm approximation problem is the cost
of solving a relatively small number of weighted least-squares problems with the
same matrix a, and weights that change at each iteration.
if a has structure
that allows us to solve the least-squares problem fast (for example, by exploiting
sparsity), then we can solve (11.61) fast.

11.8.3 semide   nite programming in inequality form

we consider the sdp

ct x

minimize

subject to pn

i=1 xifi + g (cid:22) 0,

with variable x     rn, and parameters f1, . . . , fn, g     sp. the associated centering
problem, using the log-determinant barrier function, is

minimize

tct x     log det(   pn

i=1 xifi     g).

the newton step    xnt is found from h   xnt =    g, where the hessian and gradient
are given by

hij = tr(s   1fis   1fj),
gi = tci + tr(s   1fi),

i, j = 1, . . . , n
i = 1, . . . , n,

where s =    pn

then solve the newton equation via cholesky factorization.

i=1 xifi     g. one standard approach is to form h (and g), and
we    rst consider the unstructured case, i.e., we assume all matrices are dense.
we will also just keep track of the order in the    op count, with respect to the
problem dimensions n and p. we    rst form s, which costs order np2    ops. we
then compute the matrices s   1fi, for each i, via cholesky factorization of s, and
then back substitution with the columns of fi (or forming s   1 and multiplying
by fi). this cost is order p3 for each i, so the total cost is order np3. finally,

11.8

implementation

619

we form hij as the inner product of the matrices s   1fi and s   1fj, which costs
order p2    ops. since we do this for n(n + 1)/2 such pairs, the cost is order n2p2.
solving for the newton direction costs order n3. the dominating order is thus
max{np3, n2p2, n3}.
it is not possible, in general, to exploit sparsity in the matrices fi and g, since
h is often dense, even when fi and g are sparse. one exception is when fi and g
have a common block diagonal structure, in which case all the operations described
above can be carried out block by block.

it is often possible to exploit (common) sparsity in fi and g to form the (dense)
hessian h more e   ciently.
if we can    nd an ordering that results in s having
a reasonably sparse cholesky factor, then we can compute the matrices s   1fi
e   ciently, and form hij far more e   ciently.

one interesting example that arises frequently is an sdp with matrix inequality

this corresponds to fi = eii, where eii is the matrix with i, i entry one and all
others zero. in this case, the matrix h can be found very e   ciently:

diag(x) (cid:22) b.

hij = (s   1)2
ij,

where s = b     diag(x). the cost of forming h is thus the cost of forming s   1,
which is at most (i.e., when no other structure is exploited) order n3.

11.8.4 network rate optimization

we consider a variation on the optimal network    ow problem described in   10.4.3
(page 550), which is sometimes called the network rate optimization problem. the
network is described as a directed graph with l arcs or links. goods, or packets
of information, travel on the network, passing through the links. the network
supports n    ows, with (nonnegative) rates x1, . . . , xn, which are the optimization
variables. each    ow moves along a    xed, or pre-determined, path (or route) in the
network, from a source node to a destination node. each link can support multiple
   ows passing through it. the total tra   c on a link is the sum of the    ow rates of
the    ows that travel over the link. each link has a positive capacity, which is the
maximum total tra   c it can handle.

we can describe these link capacity limits using the    ow-link incidence matrix

a     rl  n, de   ned as

aij =(cid:26) 1    ow j passes through link i

0 otherwise.

the total tra   c on link i is then given by (ax)i, so the link capacity constraints
can be expressed as ax (cid:22) c, where ci is the capacity of link i. usually each path
passes through only a small fraction of the total number of links, so the matrix a
is sparse.

in the network rate problem the paths are    xed (and encoded in the matrix a,
which is a problem parameter); the variables are the    ow rates xi. the objective

620

11

interior-point methods

is to choose the    ow rates to maximize a separable utility function u , given by

u (x) = u1(x1) +        + un(xn).

we assume that each ui (and hence, u ) is concave and nondecreasing. we can
think of ui(xi) as the income derived from supporting the ith    ow at rate xi; u (x)
is then the total income associated with the    ows. the network rate optimization
problem is then

which is a id76 problem.

maximize u (x)
subject to ax (cid:22) c,

x (cid:23) 0,

(11.62)

let us apply the barrier method to solve this problem. at each step we must

minimize a function of the form

   tu (x)    

log(c     ax)i    

log xj,

lxi=1

nxj=1

using newton   s method. the newton step    xnt is found by solving the linear
equations

where

(d0 + at d1a + d2)   xnt =    g,

1 (x), . . . , u       

n (x))

d0 =    t diag(u       
d1 = diag(1/(c     ax)2
d2 = diag(1/x2

1, . . . , 1/x2
n)

1, . . . , 1/(c     ax)2
l)

are diagonal matrices, and g     rn. we can describe the sparsity structure of this
n    n coe   cient matrix precisely:

(d0 + at d1a + d2)ij 6= 0

if and only if    ow i and    ow j share a link. if the paths are relatively short, and
each link has relatively few paths passing through it, then this matrix is sparse, so
a sparse cholesky factorization can be used. we can also solve the newton system
e   ciently when some, but not too many, of the rows and columns are relatively
dense. this occurs when a few of the    ows intersect with a large number of the
other    ows, which might occur if a few    ows are relatively long.

we can also use the matrix inversion lemma to compute the newton step by

solving a system with l    l coe   cient matrix, with form

(d   1

1 + a(d0 + d2)   1at )y =    a(d0 + d2)   1g,

and then computing

   xnt =    (d0 + d2)   1(g + at y).
here too we can precisely describe the sparsity pattern:
1 + a(d0 + d2)   1at )ij 6= 0

(d   1

if and only if there is a path that passes through link i and link j. if most paths
are short, this matrix is sparse. this matrix will be sparse, with a few dense rows
and columns, if there are a few bottlenecks, i.e., a few links over which many    ows
travel.

bibliography

bibliography

621

the early history of the barrier method is described in detail by fiacco and mccormick
[fm90,   1.2]. the method was a popular algorithm for id76 in the 1960s,
along with closely related techniques such as the method of centers (li  e  u and huard
[lh66]; see also exercise 11.11), and penalty (or exterior-point) methods [fm90,   4].
interest declined in the 1970s amid concerns about the ill-conditioning of the newton
equations of the centering problem (11.6) for high values of t.

the barrier method regained popularity in the 1980s, after gill, murray, saunders, tom-
lin, and wright [gms+86] pointed out the close connections with karmarkar   s polynomial-
time projective algorithm for id135 [kar84]. the focus of research through-
out the 1980s remained on linear (and to a lesser extent, quadratic) programming, result-
ing in di   erent variations of the basic interior-point methods, and improved worst-case
complexity results (see gonzaga [gon92]). primal-dual methods emerged as the algo-
rithms of choice for practical implementations (see mehrotra [meh92], lustig, marsten,
and shanno [lms94], wright [wri97]).

in their 1994 book, nesterov and nemirovski extended the complexity theory of linear
programming interior-point methods to nonlinear id76 problems, using
the convergence theory of newton   s method for self-concordant functions. they also
developed interior-point methods for problems with generalized inequalities, and discussed
ways of reformulating problems to satisfy the self-concordance assumption. the geometric
programming reformulation on page 587, for example, is from [nn94,   6.3.1].
as mentioned on page 585, the complexity analysis shows that, contrary to what one might
expect, the centering problems in the barrier method do not become more di   cult as t
increases, at least not in exact arithmetic. practical experience, supported by theoretical
results (forsgren, gill, and wright [fgw02,   4.3.2], nocedal and wright [nw99, page
525]), also indicates that the e   ects of ill-conditioning on the computed solution of the
newton system are more benign than thought earlier.

recent research on interior-point methods has concentrated on extending the primal-dual
methods for id135, which converge faster and reach higher accuracies than
(primal) barrier methods, to nonlinear convex problems. one popular approach, along
the lines of the simple primal-dual method of   11.7, is based on linearizing modi   ed kkt
equations for a id76 problem in standard form, i.e., problem (11.1). more
sophisticated algorithms of this type di   er from algorithm 11.2 in the strategy used to
select t (which is crucial to achieve superlinear asymptotic convergence), and the line
search. we refer to wright [wri97, chapter 8], ralph and wright [rw97], den hertog
[dh93], terlaky [ter96], and the survey by forsgren, gill, and wright [fgw02,   5] for
details and references.

other authors adopt the cone programming framework as starting point for extending
primal-dual interior-point methods for id135 to id76 (see
for example, nesterov and todd [nt98]). this approach has resulted in e   cient and
accurate primal-dual methods for semide   nite and second-order programming (see the
surveys by todd [tod01] and alizadeh and goldfarb [ag03]).

as for id135, primal-dual methods for semide   nite programming are usually
described as variations of newton   s method applied to modi   ed kkt equations. unlike
in id135, however, the linearization can be carried out in many di   erent
ways, which lead to di   erent search directions and algorithms; see helmberg, rendl,
vanderbei, and wolkowicz [hrvw96], kojima, shindo, and harah [ksh97], monteiro
[mon97], nesterov and todd [nt98], zhang [zha98], alizadeh, haeberly, and overton
[aho98], and todd, toh, and t  ut  unc  u [ttt98].

great progress has also been made in the area of initialization and infeasibility detection.
homogeneous self-dual formulations provide an elegant and e   cient alternative to the
classical two-phase approach of   11.4; see ye, todd, and mizuno [ytm94], xu, hung,

622

11

interior-point methods

and ye [xhy96], andersen and ye [ay98] and luo, sturm, and zhang [lsz00] for details.

the primal-dual interior-point methods for semide   nite and second-order cone program-
ming have been implemented in a number of software packages, including sedumi [stu99],
sdpt3 [ttt02], sdpa [fkn98], csdp [bor02], and dsdp [by02], a user-friendly in-
terface to several of these codes is provided by yalmip [l  of04].

the following books document the recent developments in this rapidly advancing    eld
in greater detail: vanderbei [van96], wright [wri97], roos, terlaky, and vial [rtv97]
ye [ye97], wolkowicz, saigal, and vandenberghe [wsv00], ben-tal and nemirovski,
[btn01], renegar [ren01], and peng, roos, and terlaky [prt02].

exercises

exercises

the barrier method

623

11.1 barrier method example. consider the simple problem

minimize
subject to

x2 + 1
2     x     4,

which has feasible set [2, 4], and optimal point x    = 2. plot f0, and tf0 +   , for several
values of t > 0, versus x. label x   (t).

11.2 what happens if the barrier method is applied to the lp

x2

minimize
subject to x1     x2,

0     x2,

with variable x     r2?

11.3 boundedness of centering problem. suppose the sublevel sets of (11.1),

minimize
subject to

f0(x)
fi(x)     0,
ax = b,

i = 1, . . . , m

are bounded. show that the sublevel sets of the associated centering problem,

minimize
subject to ax = b,

tf0(x) +   (x)

are bounded.

11.4 adding a norm bound to ensure strong convexity of the centering problem. suppose we

add the constraint xt x     r2 to the problem (11.1):

minimize
subject to

f0(x)
fi(x)     0,
ax = b
xt x     r2.

i = 1, . . . , m

let      denote the logarithmic barrier function for this modi   ed problem. find a > 0 for
which    2(tf0(x) +     (x)) (cid:23) ai holds, for all feasible x.
constraints, for simplicity)

11.5 barrier method for second-order cone programming. consider the socp (without equality

minimize
subject to

f t x
kaix + bik2     ct

i x + di,

i = 1, . . . , m.

(11.63)

the constraint functions in this problem are not di   erentiable (since the euclidean norm
kuk2 is not di   erentiable at u = 0) so the (standard) barrier method cannot be applied.
in   11.6, we saw that this socp can be solved by an extension of the barrier method
that handles generalized inequalities. (see example 11.8, page 599, and page 601.) in this
exercise, we show how the standard barrier method (with scalar constraint functions) can
be used to solve the socp.
we    rst reformulate the socp as

minimize
subject to

f t x
kaix + bik2
ct
i x + di     0,

2/(ct

i x + di)     ct
i = 1, . . . , m.

i x + di,

i = 1, . . . , m

(11.64)

624

11

interior-point methods

the constraint function

fi(x) = kaix + bik2

i x + di     ct
ct

2

i x     di

is the composition of a quadratic-over-linear function with an a   ne function, and is twice
di   erentiable (and convex), provided we de   ne its domain as dom fi = {x | ct
i x+di > 0}.
note that the two problems (11.63) and (11.64) are not exactly equivalent. if ct
i x   +di = 0
for some i, where x    is the optimal solution of the socp (11.63), then the reformulated
problem (11.64) is not solvable; x    is not in its domain. nevertheless we will see that
the barrier method, applied to (11.64), produces arbitrarily accurate suboptimal solutions
of (11.64), and hence also for (11.63).

(a) form the log barrier    for the problem (11.64). compare it to the log barrier that
arises when the socp (11.63) is solved using the barrier method for generalized
inequalities (in   11.6).

(b) show that if tf t x +   (x) is minimized, the minimizer x   (t) is 2m/t-suboptimal for
the problem (11.63). it follows that the standard barrier method, applied to the
reformulated problem (11.64), solves the socp (11.63), in the sense of producing
arbitrarily accurate suboptimal solutions. this is the case even though the optimal
point x    need not be in the domain of the reformulated problem (11.64).

indicator function bi   (u) (see   11.2.1, page 563). we can also construct barriers from

11.6 general barriers. the log barrier is based on the approximation    (1/t) log(   u) of the
other approximations, which in turn yield generalizations of the central path and barrier
method. let h : r     r be a twice di   erentiable, closed, increasing convex function,
with dom h =    r++.
(this implies h(u)         as u     0.) one such function is
h(u) =     log(   u); another example is h(u) =    1/u (for u < 0).
now consider the optimization problem (without equality constraints, for simplicity)

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m,

where fi are twice di   erentiable. we de   ne the h-barrier for this problem as

  h(x) =

mxi=1

h(fi(x)),

with domain {x | fi(x) < 0, i = 1, . . . , m}. when h(u) =     log(   u), this is the usual
logarithmic barrier; when h(u) =    1/u,   h is called the inverse barrier. we de   ne the
h-central path as

x   (t) = argmin tf0(x) +   h(x),

where t > 0 is a parameter. (we assume that for each t, the minimizer exists and is
unique.)

(a) explain why tf0(x) +   h(x) is convex in x, for each t > 0.
(b) show how to construct a dual feasible    from x   (t). find the associated duality gap.

(c) for what functions h does the duality gap found in part (b) depend only on t and

m (and no other problem data)?

11.7 tangent to central path. this problem concerns dx   (t)/dt, which gives the tangent to the
central path at the point x   (t). for simplicity, we consider a problem without equality
constraints; the results readily generalize to problems with equality constraints.

(a) find an explicit expression for dx   (t)/dt. hint. di   erentiate the centrality equa-

tions (11.7) with respect to t.

exercises

625

(b) show that f0(x   (t)) decreases as t increases. thus, the objective value in the barrier
method decreases, as the parameter t is increased. (we already know that the duality
gap, which is m/t, decreases as t increases.)

11.8 predictor-corrector method for centering problems. in the standard barrier method, x   (  t)
is computed using newton   s method, starting from the initial point x   (t). one alternative

that has been proposed is to make an approximation or predictionbx of x   (  t), and then
start the id77 for computing x   (  t) from bx. the idea is that this should
reduce the number of newton steps, sincebx is (presumably) a better initial point than

x   (t). this method of centering is called a predictor-corrector method, since it    rst makes
a prediction of what x   (  t) is, then corrects the prediction using newton   s method.
the most widely used predictor is the    rst-order predictor, based on the tangent to the
central path, explored in exercise 11.7. this predictor is given by

dx   (t)

dt

(  t     t).

bx = x   (t) +

derive an expression for the    rst-order predictor bx. compare it to the newton update

obtained, i.e., x   (t) +    xnt, where    xnt is the newton step for   tf0(x) +   (x), at x   (t).
what can you say when the objective f0 is linear? (for simplicity, you can consider a
problem without equality constraints.)

11.9 dual feasible points near the central path. consider the problem

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m,

with variable x     rn. we assume the functions fi are convex and twice di   erentiable. (we
assume for simplicity there are no equality constraints.) recall (from   11.2.2, page 565)
that   i =    1/(tfi(x   (t))), i = 1, . . . , m, is dual feasible, and in fact, x   (t) minimizes
l(x,   ). this allows us to evaluate the dual function for   , which turns out to be g(  ) =
f0(x   (t))     m/t. in particular, we conclude that x   (t) is m/t-suboptimal.
in this problem we consider what happens when a point x is close to x   (t), but not quite
centered. (this would occur if the centering steps were terminated early, or not carried
out to full accuracy.) in this case, of course, we cannot claim that   i =    1/(tfi(x)),
i = 1, . . . , m, is dual feasible, or that x is m/t-suboptimal. however, it turns out that
a slightly more complicated formula does yield a dual feasible point, provided x is close
enough to centered.
let    xnt be the newton step at x of the centering problem

a formula that often gives a dual feasible point when    xnt is small (i.e., for x nearly
centered) is

i=1 log(   fi(x)).

minimize

tf0(x)    pm
   tfi(x)(cid:18)1 +    fi(x)t    xnt
   fi(x) (cid:19) ,

1

  i =

i = 1, . . . , m.

in this case, the vector x does not minimize l(x,   ), so there is no general formula for the
dual function value g(  ) associated with   . (if we have an analytical expression for the
dual objective, however, we can simply evaluate g(  ).)
verify that for a qcqp

minimize
subject to

(1/2)xt p0x + qt
(1/2)xt pix + qt

0 x + r0
i x + ri     0,

i = 1, . . . , m,

the formula for    yields a dual feasible point (i.e.,    (cid:23) 0 and l(x,   ) is bounded below)
when    xnt is su   ciently small.

626

11

interior-point methods

hint. de   ne

show that

x0 = x +    xnt,

xi = x    

1

t  ifi(x)

   xnt,

i = 1, . . . , m.

   f0(x0) +

  i   fi(xi) = 0.

mxi=1

now use fi(z)     fi(xi) +    fi(xi)t (z     xi), i = 0, . . . , m, to derive a lower bound on
l(z,   ).
11.10 another parametrization of the central path. we consider the problem (11.1), with central

path x   (t) for t > 0, de   ned as the solution of

minimize
subject to ax = b.

tf0(x)    pm

i=1 log(   fi(x))

in this problem we explore another parametrization of the central path.
for u > p   , let z   (u) denote the solution of

minimize     log(u     f0(x))    pm

subject to ax = b.

i=1 log(   fi(x))

show that the curve de   ned by z   (u), for u > p   , is the central path. (in other words,
for each u > p   , there is a t > 0 for which x   (t) = z   (u), and conversely, for each t > 0,
there is an u > p    for which z   (u) = x   (t)).

11.11 method of analytic centers. in this problem we consider a variation on the barrier method,
based on the parametrization of the central path described in exercise 11.10. for simplic-
ity, we consider a problem with no equality constraints,

minimize
subject to

f0(x)
fi(x)     0,

i = 1, . . . , m.

the method of analytic centers starts with any strictly feasible initial point x(0), and any
u(0) > f0(x(0)). we then set

u(1) =   u(0) + (1       )f0(x(0)),

where        (0, 1) is an algorithm parameter (usually chosen small), and then compute the
next iterate as

x(1) = z   (u(1))

(using newton   s method, starting from x(0)). here z   (s) denotes the minimizer of

    log(s     f0(x))    

log(   fi(x)),

mxi=1

which we assume exists and is unique. this process is then repeated.
the point z   (s) is the analytic center of the inequalities

f0(x)     s,

f1(x)     0, . . . , fm(x)     0,

hence the algorithm name.
show that the method of centers works, i.e., x(k) converges to an optimal point. find a
stopping criterion that guarantees that x is   -suboptimal, where    > 0.
hint. the points x(k) are on the central path; see exercise 11.10. use this to show that

where u and u+ are the values of u on consecutive iterations.

u+     p       

m +   
m + 1

(u     p   ),

exercises

627

11.12 barrier method for convex-concave games. we consider a convex-concave game with

inequality constraints,

minimizew maximizez
subject to

f0(w, z)
fi(w)     0,
  fi(z)     0,

i = 1, . . . , m
i = 1, . . . ,   m.

here w     rn is the variable associated with minimizing the objective, and z     r  n is
the variable associated with maximizing the objective. the constraint functions fi and   fi
are convex and di   erentiable, and the objective function f0 is di   erentiable and convex-
concave, i.e., convex in w, for each z, and concave in z, for each w. we assume for
simplicity that dom f0 = rn    r  n.
a solution or saddle-point for the game is a pair w   , z   , for which

f0(w   , z)     f0(w   , z   )     f0(w, z   )

holds for every feasible w and z. (for background on convex-concave games and functions,
see   5.4.3,   10.3.4 and exercises 3.14, 5.24, 5.25, 10.10, and 10.13.) in this exercise we
show how to solve this game using an extension of the barrier method, and the infeasible
start id77 (see   10.3).
(a) let t > 0. explain why the function

tf0(w, z)    

mxi=1

log(   fi(w)) +

  mxi=1

log(      fi(z))

is convex-concave in (w, z). we will assume that it has a unique saddle-point,
(w   (t), z   (t)), which can be found using the infeasible start id77.

(b) as in the barrier method for solving a id76 problem, we can derive
a simple bound on the suboptimality of (w   (t), z   (t)), which depends only on the
problem dimensions, and decreases to zero as t increases. let w and z denote the
feasible sets for w and z,

w = {w | fi(w)     0, i = 1, . . . , m},

z = {z |   fi(z)     0, i = 1, . . . ,   m}.

show that

f0(w   (t), z   (t))     inf
f0(w   (t), z   (t))     sup

w   w

z   z

f0(w, z   (t)) +

f0(w   (t), z)    

,

m
t
  m
t

,

and therefore

sup
z   z

f0(w   (t), z)     inf

w   w

f0(w, z   (t))    

m +   m

t

.

self-concordance and complexity analysis

11.13 self-concordance and negative id178.

(a) show that the negative id178 function x log x (on r++) is not self-concordant.
(b) show that for any t > 0, tx log x     log x is self-concordant (on r++).

11.14 self-concordance and the centering problem. let    be the logarithmic barrier function of
problem (11.1). suppose that the sublevel sets of (11.1) are bounded, and that tf0 +    is
closed and self-concordant. show that t   2f0(x) +    2  (x)     0, for all x     dom   . hint.
see exercises 9.17 and 11.3.

628

11

interior-point methods

barrier method for generalized inequalities

11.15 generalized logarithm is k-increasing. let    be a generalized logarithm for the proper

cone k. suppose y    k 0.
(a) show that      (y) (cid:23)k     0, i.e., that    is k-nondecreasing. hint. if      (y) 6(cid:23)k     0,
then there is some w    k 0 for which wt     (y)     0. use the inequality   (sw)    
  (y) +      (y)t (sw     y), with s > 0.
(b) now show that      (y)    k     0, i.e., that    is k-increasing. hint. show that
   2  (y)     0,      (y) (cid:23)k     0 imply      (y)    k     0.

11.16 [nn94, page 41] properties of a generalized logarithm. let    be a generalized logarithm
for the proper cone k, with degree   . prove that the following properties hold at any
y    k 0.
(a)      (sy) =      (y)/s for all s > 0.
(b)      (y) =       2  (y)y.
(c) yt     2(y)y =      .
(d)      (y)t   2  (y)   1     (y) =      .

11.17 dual generalized logarithm. let    be a generalized logarithm for the proper cone k, with

degree   . show that the dual generalized logarithm   , de   ned in (11.49), satis   es

  (sv) =   (v) +    log s,

for v    k     0, s > 0.

11.18 is the function

with dom    = {y     rn+1 | yn+1 >pn

order cone in rn+1?

implementation

  (y) = log(cid:18)yn+1    pn

i=1 y2

yn+1 (cid:19) ,

i

i=1 y2

i }, a generalized logarithm for the second-

11.19 yet another method for computing the newton step. show that the newton step for the
barrier method, which is given by the solution of the linear equations (11.14), can be
found by solving a larger set of linear equations with coe   cient matrix

t   2f0(x) +pi

df (x)

a

1

   fi(x)   2fi(x)

df (x)t

    diag(f (x))2

0

      

at
0

0       

where f (x) = (f1(x), . . . , fm(x)).
for what types of problem structure might solving this larger system be interesting?

11.20 network rate optimization via the dual problem. in this problem we examine a dual method
for solving the network rate optimization problem of   11.8.4. to simplify the presentation
we assume that the utility functions ui are strictly concave, with dom ui = r++, and
that they satisfy u    

i (xi)         as xi     0 and u    

i (xi)     0 as xi        .

(a) express the dual problem of (11.62) in terms of the conjugate utility functions

vi = (   ui)   , de   ned as

vi(  ) = sup
x>0

(  x + ui(x)).

show that dom vi =    r++, and that for each    < 0 there is a unique x with
u    
i (x) =      .
(b) describe a barrier method for the dual problem. compare the complexity per iter-
ation with the complexity of the method in   11.8.4. distinguish the same two cases
as in   11.8.4 (at a is sparse and aat is sparse).

exercises

numerical experiments

629

11.21 log-chebyshev approximation with bounds. we consider an approximation problem:    nd
x     rn, that satis   es the variable bounds l (cid:22) x (cid:22) u, and yields ax     b, where b     rm.
you can assume that l     u, and b     0 (for reasons we explain below). we let at
i denote
the ith row of the matrix a.
we judge the approximation ax     b by the maximum fractional deviation, which is

max

i=1,...,n

max{(at

i x)/bi, bi/(at

i x)} = max

i=1,...,n

max{at
min{at

i x, bi}
i x, bi}

,

when ax     0; we de   ne the maximum fractional deviation as     if ax 6    0.
the problem of minimizing the maximum fractional deviation is called the fractional
chebyshev approximation problem, or the logarithmic chebyshev approximation problem,
since it is equivalent to minimizing the objective

i=1,...,n| log at
max

i x     log bi|.

(see also exercise 6.3, part (c).)

(a) formulate the fractional chebyshev approximation problem (with variable bounds)
as a id76 problem with twice di   erentiable objective and constraint
functions.

(b) implement a barrier method that solves the fractional chebyshev approximation
problem. you can assume an initial point x(0), satisfying l     x(0)     u, ax(0)     0, is
known.

11.22 maximum volume rectangle inside a polyhedron. consider the problem described in exer-
cise 8.16, i.e.,    nding the maximum volume rectangle r = {x | l (cid:22) x (cid:22) u} that lies in
a polyhedron described by a set of linear inequalities, p = {x | ax (cid:22) b}. implement a
barrier method for solving this problem. you can assume that b     0, which means that
for small l     0 and u     0, the rectangle r lies inside p.
test your implementation on several simple examples. find the maximum volume rect-
angle that lies in the polyhedron de   ned by

a =               

0    1
2    4
1
2
   4
4
   4
0

               

,

b = 1.

plot this polyhedron, and the maximum volume rectangle that lies inside it.

11.23 sdp bounds and heuristics for the two-way partitioning problem.

in this exercise we
consider the two-way partitioning problem (5.7), described on page 219, and also in ex-
ercise 5.39:

minimize
subject to x2

xt w x
i = 1,

i = 1, . . . , n,

(11.65)

with variable x     rn. we assume, without loss of generality, that w     sn satis   es
wii = 0. we denote the optimal value of the partitioning problem as p   , and x    will
denote an optimal partition. (note that    x    is also an optimal partition.)
the lagrange dual of the two-way partitioning problem (11.65) is given by the sdp

maximize    1t   
subject to w + diag(  ) (cid:23) 0,

(11.66)

630

11

interior-point methods

with variable        rn. the dual of this sdp is
tr(w x)

minimize
subject to x (cid:23) 0
xii = 1,

i = 1, . . . , n,

(11.67)

with variable x     sn.
(this sdp can be interpreted as a relaxation of the two-way
partitioning problem (11.65); see exercise 5.39.) the optimal values of these two sdps
are equal, and give a lower bound, which we denote d   , on the optimal value p   . let      
and x     denote optimal points for the two sdps.

(a) implement a barrier method that solves the sdp (11.66) and its dual (11.67), given
the weight matrix w . explain how you obtain nearly optimal    and x, give for-
mulas for any hessians and gradients that your method requires, and explain how
you compute the newton step. test your implementation on some small problem
instances, comparing the bound you    nd with the optimal value (which can be found
by checking the objective value of all 2n partitions). try your implementation on a
randomly chosen problem instance large enough that you cannot    nd the optimal
partition by exhaustive search (e.g., n = 100).

(b) a heuristic for partitioning. in exercise 5.39, you found that if x     has rank one,
then it must have the form x     = x   (x   )t , where x    is optimal for the two-way
partitioning problem. this suggests the following simple heuristic for    nding a good
partition (if not the best): solve the sdps above, to    nd x     (and the bound d   ).
let v denote an eigenvector of x     associated with its largest eigenvalue, and let
  x = sign(v). the vector   x is our guess for a good partition.
try this heuristic on some small problem instances, and the large problem instance
you used in part (a). compare the objective value of your heuristic partition,   xt w   x,
with the lower bound d   .

(c) a randomized method. another heuristic technique for    nding a good partition,
given the solution x     of the sdp (11.67), is based on randomization. the method
is simple: we generate independent samples x(1), . . . , x(k) from a normal distribution
on rn, with zero mean and covariance x    . for each sample we consider the heuristic
approximate solution   x(k) = sign(x(k)). we then take the best among these, i.e.,
the one with lowest cost. try out this procedure on some small problem instances,
and the large problem instance you considered in part (a).

(d) a greedy heuristic re   nement. suppose you are given a partition x, i.e., xi     {   1, 1},
i = 1, . . . , n. how does the objective value change if we move element i from one
set to the other, i.e., change xi to    xi? now consider the following simple greedy
algorithm: given a starting partition x, move the element that gives the largest
reduction in the objective. repeat this procedure until no reduction in objective
can be obtained by moving an element from one set to the other.
try this heuristic on some problem instances, including the large one, starting from
various initial partitions, including x = 1, the heuristic approximate solution found
in part (b), and the randomly generated approximate solutions found in part (c).
how much does this greedy re   nement improve your approximate solutions from
parts (b) and (c)?

11.24 barrier and primal-dual interior-point methods for quadratic programming. implement
a barrier method, and a primal-dual method, for solving the qp (without equality con-
straints, for simplicity)

(1/2)xt p x + qt x

minimize
subject to ax (cid:22) b,

with a     rm  n. you can assume a strictly feasible initial point is given. test your codes
on several examples. for the barrier method, plot the duality gap versus newton steps.
for the primal-dual interior-point method, plot the surrogate duality gap and the norm
of the dual residual versus iteration number.

appendices

appendix a

mathematical background

in this appendix we give a brief review of some basic concepts from analysis and
id202. the treatment is by no means complete, and is meant mostly to set
out our notation.

a.1 norms

a.1.1 inner product, euclidean norm, and angle

the standard inner product on rn, the set of real n-vectors, is given by

hx, yi = xt y =

xiyi,

nxi=1

for x, y     rn.
euclidean norm, or    2-norm, of a vector x     rn is de   ned as
n)1/2.

in this book we use the notation xt y, instead of hx, yi. the

kxk2 = (xt x)1/2 = (x2

(a.1)
the cauchy-schwartz inequality states that |xt y|     kxk2kyk2 for any x, y     rn.
the (unsigned) angle between nonzero vectors x, y     rn is de   ned as

1 +        + x2

where we take cos   1(u)     [0,   ]. we say x and y are orthogonal if xt y = 0.
by

the standard inner product on rm  n, the set of m    n real matrices, is given

6 (x, y) = cos   1(cid:18) xt y

kxk2kyk2(cid:19) ,

hx, y i = tr(x t y ) =

mxi=1

nxj=1

xijyij,

for x, y     rm  n. (here tr denotes trace of a matrix, i.e., the sum of its diagonal
elements.) we use the notation tr(x t y ) instead of hx, y i. note that the inner

634

a mathematical background

product of two matrices is the inner product of the associated vectors, in rmn,
obtained by listing the coe   cients of the matrices in some order, such as row
major.

the frobenius norm of a matrix x     rm  n is given by

kxkf =(cid:0)tr(x t x)(cid:1)1/2

1/2

=      
mxi=1

nxj=1

x 2

ij      

.

(a.2)

the frobenius norm is the euclidean norm of the vector obtained by listing the
coe   cients of the matrix. (the    2-norm of a matrix is a di   erent norm; see   a.1.5.)
the standard inner product on sn, the set of symmetric n   n matrices, is given
by

hx, y i = tr(xy ) =

xijyij =

nxi=1

nxj=1

nxi=1

xiiyii + 2xi<j

xijyij.

a.1.2 norms, distance, and unit ball

a function f : rn     r with dom f = rn is called a norm if

    f is nonnegative: f (x)     0 for all x     rn
    f is de   nite: f (x) = 0 only if x = 0
    f is homogeneous: f (tx) = |t|f (x), for all x     rn and t     r
    f satis   es the triangle inequality: f (x + y)     f (x) + f (y), for all x, y     rn
we use the notation f (x) = kxk, which is meant to suggest that a norm is a
generalization of the absolute value on r. when we specify a particular norm,
we use the notation kxksymb, where the subscript is a mnemonic to indicate which
norm is meant.
a norm is a measure of the length of a vector x; we can measure the distance

between two vectors x and y as the length of their di   erence, i.e.,

dist(x, y) = kx     yk.

we refer to dist(x, y) as the distance between x and y, in the norm k    k.

the set of all vectors with norm less than or equal to one,

b = {x     rn | kxk     1},

is called the unit ball of the norm k    k. the unit ball satis   es the following prop-
erties:

    b is symmetric about the origin, i.e., x     b if and only if    x     b
    b is convex
    b is closed, bounded, and has nonempty interior

a.1 norms

635

conversely, if c     rn is any set satisfying these three conditions, then it is the
unit ball of a norm, which is given by

kxk = (sup{t     0 | tx     c})   1 .

a.1.3 examples

the simplest example of a norm is the absolute value on r. another simple
example is the euclidean or    2-norm on rn, de   ned above in (a.1). two other
frequently used norms on rn are the sum-absolute-value, or    1-norm, given by

and the chebyshev or       -norm, given by

kxk1 = |x1| +        + |xn|,

kxk    = max{|x1|, . . . ,|xn|}.

these three norms are part of a family parametrized by a constant traditionally
denoted p, with p     1: the    p-norm is de   ned by

kxkp = (|x1|p +        + |xn|p)1/p.

this yields the    1-norm when p = 1 and the euclidean norm when p = 2. it is easy
to show that for any x     rn,
p      kxkp = max{|x1|, . . . ,|xn|},
lim

so the       -norm also    ts in this family, as a limit.

another important family of norms are the quadratic norms. for p     sn

++, we

de   ne the p -quadratic norm as

kxkp = (xt p x)1/2 = kp 1/2xk2.

the unit ball of a quadratic norm is an ellipsoid (and conversely, if the unit ball of
a norm is an ellipsoid, the norm is a quadratic norm).

some common norms on rm  n are the frobenius norm, de   ned above in (a.2),

the sum-absolute-value norm,

kxksav =

mxi=1

nxj=1

|xij|,

and the maximum-absolute-value norm,

kxkmav = max{|xij| | i = 1, . . . , m, j = 1, . . . , n}.

we will encounter several other important norms of matrices in   a.1.5.

636

a mathematical background

a.1.4 equivalence of norms

suppose that k    ka and k    kb are norms on rn. a basic result of analysis is that
there exist positive constants    and    such that, for all x     rn,

  kxka     kxkb       kxka.

this means that the norms are equivalent, i.e., they de   ne the same set of open
subsets, the same set of convergent sequences, and so on (see   a.2).
(we con-
clude that any norms on any    nite-dimensional vector space are equivalent, but on
in   nite-dimensional vector spaces, the result need not hold.) using convex analy-
sis, we can give a more speci   c result: if k    k is any norm on rn, then there exists
a quadratic norm k    kp for which

kxkp     kxk        nkxkp

holds for all x. in other words, any norm on rn can be uniformly approximated,

within a factor of    n, by a quadratic norm. (see   8.4.1.)

a.1.5 operator norms

suppose k    ka and k    kb are norms on rm and rn, respectively. we de   ne the
operator norm of x     rm  n, induced by the norms k    ka and k    kb, as

kxka,b = sup{kxuka | kukb     1} .

(it can be shown that this de   nes a norm on rm  n.)

when k    ka and k    kb are both euclidean norms, the operator norm of x is its

maximum singular value, and is denoted kxk2:

kxk2 =   max(x) = (  max(x t x))1/2.

(this agrees with the euclidean norm on rm, when x     rm  1, so there is no
clash of notation.) this norm is also called the spectral norm or    2-norm of x.
as another example, the norm induced by the       -norm on rm and rn, denoted

kxk   , is the max-row-sum norm,

kxk    = sup{kxuk    | kuk        1} = max

i=1,...,m

nxj=1

|xij|.

the norm induced by the    1-norm on rm and rn, denoted kxk1, is the max-
column-sum norm,

kxk1 = max

j=1,...,n

mxi=1

|xij|.

a.2 analysis

a.1.6 dual norm

637

let k    k be a norm on rn. the associated dual norm, denoted k    k   , is de   ned as

kzk    = sup{zt x | kxk     1}.

(this can be shown to be a norm.) the dual norm can be interpreted as the
operator norm of zt , interpreted as a 1   n matrix, with the norm k  k on rn, and
the absolute value on r:

from the de   nition of dual norm we have the inequality

kzk    = sup{|zt x| | kxk     1}.

zt x     kxkkzk   ,

which holds for all x and z. this inequality is tight, in the following sense: for any
x there is a z for which the inequality holds with equality. (similarly, for any z
there is an x that gives equality.) the dual of the dual norm is the original norm:
we have kxk       = kxk for all x. (this need not hold in in   nite-dimensional vector
spaces.)

the dual of the euclidean norm is the euclidean norm, since

sup{zt x | kxk2     1} = kzk2.

(this follows from the cauchy-schwarz inequality; for nonzero z, the value of x
that maximizes zt x over kxk2     1 is z/kzk2.)
the dual of the       -norm is the    1-norm:

sup{zt x | kxk        1} =

|zi| = kzk1,

nxi=1

and the dual of the    1-norm is the       -norm. more generally, the dual of the    p-norm
is the    q-norm, where q satis   es 1/p + 1/q = 1, i.e., q = p/(p     1).
dual norm is

as another example, consider the    2- or spectral norm on rm  n. the associated

which turns out to be the sum of the singular values,

kzk2    = sup{tr(z t x) | kxk2     1},

kzk2    =   1(z) +        +   r(z) = tr(z t z)1/2,

where r = rank z. this norm is sometimes called the nuclear norm.

a.2 analysis

a.2.1 open and closed sets

an element x     c     rn is called an interior point of c if there exists an    > 0 for
which

{y | ky     xk2       }     c,

638

a mathematical background

i.e., there exists a ball centered at x that lies entirely in c. the set of all points
interior to c is called the interior of c and is denoted int c. (since all norms
on rn are equivalent to the euclidean norm, all norms generate the same set of
interior points.) a set c is open if int c = c, i.e., every point in c is an interior
point. a set c     rn is closed if its complement rn \ c = {x     rn | x 6    c} is
open.

the closure of a set c is de   ned as

cl c = rn \ int(rn \ c),

i.e., the complement of the interior of the complement of c. a point x is in the
closure of c if for every    > 0, there is a y     c with kx     yk2       .
we can also describe closed sets and the closure in terms of convergent sequences
and limit points. a set c is closed if and only if it contains the limit point of every
convergent sequence in it. in other words, if x1, x2, . . . converges to x, and xi     c,
then x     c. the closure of c is the set of all limit points of convergent sequences
in c.

the boundary of the set c is de   ned as

bd c = cl c \ int c.

a boundary point x (i.e., a point x     bd c) satis   es the following property: for
all    > 0, there exists y     c and z 6    c with

ky     xk2       ,

kz     xk2       ,

i.e., there exist arbitrarily close points in c, and also arbitrarily close points not in
c. we can characterize closed and open sets in terms of the boundary operation:
c is closed if it contains its boundary, i.e., bd c     c. it is open if it contains no
boundary points, i.e., c     bd c =    .

a.2.2 supremum and in   mum

suppose c     r. a number a is an upper bound on c if for each x     c, x     a.
the set of upper bounds on a set c is either empty (in which case we say c is
unbounded above), all of r (only when c =    ), or a closed in   nite interval [b,   ).
the number b is called the least upper bound or supremum of the set c, and is
denoted sup c. we take sup    =       , and sup c =     if c is unbounded above.
when sup c     c, we say the supremum of c is attained or achieved.
when the set c is    nite, sup c is the maximum of its elements. some authors
use the notation max c to denote supremum, when it is attained, but we follow
standard mathematical convention, using max c only when the set c is    nite.

we de   ne lower bound, and in   mum, in a similar way. a number a is a lower
bound on c     r if for each x     c, a     x. the in   mum (or greatest lower bound )
of a set c     r is de   ned as inf c =     sup(   c). when c is    nite, the in   mum
is the minimum of its elements. we take inf     =    , and inf c =        if c is
unbounded below, i.e., has no lower bound.

a.3 functions

639

a.3 functions

a.3.1 function notation

our notation for functions is mostly standard, with one exception. when we write

f : a     b

we mean that f is a function on the set dom f     a into the set b; in particular
we can have dom f a proper subset of the set a. thus the notation f : rn     rm
means that f maps (some) n-vectors into m-vectors; it does not mean that f (x)
is de   ned for every x     rn. this convention is similar to function declarations in
computer languages. specifying the data types of the input and output arguments
of a function gives the syntax of that function; it does not guarantee that any input
argument with the speci   ed data type is valid.

as an example consider the function f : sn     r, given by

f (x) = log det x,

(a.3)
++. the notation f : sn     r speci   es the syntax of f : it takes
with dom f = sn
as argument a symmetric n    n matrix, and returns a real number. the notation
dom f = sn
++ speci   es which symmetric n   n matrices are valid input arguments
for f (i.e., only positive de   nite ones). the formula (a.3) speci   es what f (x) is,
for x     dom f .

a.3.2 continuity

a function f : rn     rm is continuous at x     dom f if for all    > 0 there exists a
   such that

y     dom f,

ky     xk2        =    kf (y)     f (x)k2       .

continuity can be described in terms of limits: whenever the sequence x1, x2, . . .
in dom f converges to a point x     dom f , the sequence f (x1), f (x2), . . . converges
to f (x), i.e.,

lim
i      

f (xi) = f ( lim
i      

xi).

a function f is continuous if it is continuous at every point in its domain.

a.3.3 closed functions

a function f : rn     r is said to be closed if, for each        r, the sublevel set

{x     dom f | f (x)       }

is closed. this is equivalent to the condition that the epigraph of f ,

epi f = {(x, t)     rn+1 | x     dom f, f (x)     t},

640

a mathematical background

is closed. (this de   nition is general, but is usually only applied to convex func-
tions.)

if f : rn     r is continuous, and dom f is closed, then f is closed. if f : rn    
r is continuous, with dom f open, then f is closed if and only if f converges to    
along every sequence converging to a boundary point of dom f . in other words, if
limi       xi = x     bd dom f , with xi     dom f , we have limi       f (xi) =    .

example a.1 examples on r.

    the function f : r     r, with f (x) = x log x, dom f = r++, is not closed.
    the function f : r     r, with

f (x) =(cid:26) x log x x > 0

x = 0,

0

dom f = r+,

is closed.

    the function f (x) =     log x, dom f = r++, is closed.

a.4 derivatives

a.4.1 derivative and gradient

suppose f : rn     rm and x     int dom f . the function f is di   erentiable at x if
there exists a matrix df (x)     rm  n that satis   es

lim

z   dom f, z6=x, z   x

kf (z)     f (x)     df (x)(z     x)k2

kz     xk2

= 0,

(a.4)

in which case we refer to df (x) as the derivative (or jacobian) of f at x. (there
can be at most one matrix that satis   es (a.4).) the function f is di   erentiable if
dom f is open, and it is di   erentiable at every point in its domain.

the a   ne function of z given by

f (x) + df (x)(z     x)

is called the    rst-order approximation of f at (or near) x. evidently this function
agrees with f at z = x; when z is close to x, this a   ne function is very close to f .
the derivative can be found by deriving the    rst-order approximation of the
function f at x (i.e., the matrix df (x) that satis   es (a.4)), or from partial deriva-
tives:

df (x)ij =

   fi(x)

   xj

,

i = 1, . . . , m,

j = 1, . . . , n.

a.4 derivatives

641

gradient
when f is real-valued (i.e., f : rn     r) the derivative df (x) is a 1    n matrix,
i.e., it is a row vector. its transpose is called the gradient of the function:

which is a (column) vector, i.e., in rn. its components are the partial derivatives
of f :

   f (x) = df (x)t ,

   f (x)i =

   f (x)
   xi

,

i = 1, . . . , n.

the    rst-order approximation of f at a point x     int dom f can be expressed as
(the a   ne function of z)

f (x) +    f (x)t (z     x).

examples
as a simple example consider the quadratic function f : rn     r,

f (x) = (1/2)xt p x + qt x + r,

where p     sn, q     rn, and r     r. its derivative at x is the row vector df (x) =
xt p + qt , and its gradient is

   f (x) = p x + q.

as a more interesting example, we consider the function f : sn     r, given by

f (x) = log det x,

dom f = sn

++.

one (tedious) way to    nd the gradient of f is to introduce a basis for sn,    nd
the gradient of the associated function, and    nally translate the result back to sn.
instead, we will directly    nd the    rst-order approximation of f at x     sn
++. let
z     sn
++ be close to x, and let    x = z     x (which is assumed to be small). we
have

log det z = log det(x +    x)

= log det(cid:16)x 1/2(i + x    1/2   xx    1/2)x 1/2(cid:17)

= log det x + log det(i + x    1/2   xx    1/2)

= log det x +

log(1 +   i),

nxi=1

where   i is the ith eigenvalue of x    1/2   xx    1/2. now we use the fact that    x is
small, which implies   i are small, so to    rst order we have log(1 +   i)       i. using
this    rst-order approximation in the expression above, we get

log det z     log det x +

  i

nxi=1

= log det x + tr(x    1/2   xx    1/2)
= log det x + tr(x    1   x)

= log det x + tr(cid:0)x    1(z     x)(cid:1) ,

642

a mathematical background

where we have used the fact that the sum of the eigenvalues is the trace, and the
property tr(ab) = tr(ba).

thus, the    rst-order approximation of f at x is the a   ne function of z given

by

f (z)     f (x) + tr(cid:0)x    1(z     x)(cid:1) .

noting that the second term on the righthand side is the standard inner product
of x    1 and z     x, we can identify x    1 as the gradient of f at x. thus, we can
write the simple formula

this result should not be surprising, since the derivative of log x, on r++, is 1/x.

   f (x) = x    1.

a.4.2 chain rule

suppose f : rn     rm is di   erentiable at x     int dom f and g : rm     rp
is di   erentiable at f (x)     int dom g. de   ne the composition h : rn     rp by
h(z) = g(f (z)). then h is di   erentiable at x, with derivative

dh(x) = dg(f (x))df (x).

(a.5)

as an example, suppose f : rn     r, g : r     r, and h(x) = g(f (x)). taking

the transpose of dh(x) = dg(f (x))df (x) yields

   h(x) = g   (f (x))   f (x).

(a.6)

composition with a   ne function
suppose f : rn     rm is di   erentiable, a     rn  p, and b     rn. de   ne g : rp    
rm as g(x) = f (ax + b), with dom g = {x | ax + b     dom f}. the derivative of
g is, by the chain rule (a.5), dg(x) = df (ax + b)a.
when f is real-valued (i.e., m = 1), we obtain the formula for the gradient of

a composition of a function with an a   ne function,

   g(x) = at   f (ax + b).

for example, suppose that f : rn     r, x, v     rn, and we de   ne the function
  f : r     r by   f (t) = f (x + tv). (roughly speaking,   f is f , restricted to the line
{x + tv | t     r}.) then we have

d   f (t) =   f    (t) =    f (x + tv)t v.

(the scalar   f    (0) is the directional derivative of f , at x, in the direction v.)

example a.2 consider the function f : rn     r, with dom f = rn and

f (x) = log

mxi=1

exp(at

i x + bi),

a.4 derivatives

643

where a1, . . . , am     rn, and b1, . . . , bm     r. we can    nd a simple expression for
its gradient by noting that it is the composition of the a   ne function ax + b, where
m, and the function g : rm     r given by g(y) =
a     rm  n with rows at

i=1 exp yi). simple di   erentiation (or the formula (a.6)) shows that

1 , . . . , at

log(pm

   g(y) =

i=1 exp yi         
1pm

exp y1

...

exp ym

          ,

(a.7)

so by the composition formula we have

   f (x) =

1

1t z

at z

where zi = exp(at

i x + bi), i = 1, . . . , m.

example a.3 we derive an expression for    f (x), where

f (x) = log det(f0 + x1f1 +        + xnfn),

where f0, . . . , fn     sp, and

dom f = {x     rn | f0 + x1f1 +        + xnfn     0}.

the function f is the composition of the a   ne mapping from x     rn to f0 + x1f1 +
       + xnfn     sp, with the function log det x. we use the chain rule to evaluate

   f (x)

   xi

= tr(fi    log det(f )) = tr(f    1fi),

where f = f0 + x1f1 +        + xnfn. thus we have
tr(f    1f1)

   f (x) =         

...

tr(f    1fn)

          .

a.4.3 second derivative

in this section we review the second derivative of a real-valued function f : rn    
r. the second derivative or hessian matrix of f at x     int dom f , denoted
   2f (x), is given by

   2f (x)ij =

   2f (x)
   xi   xj

,

i = 1, . . . n,

j = 1, . . . , n,

provided f is twice di   erentiable at x, where the partial derivatives are evaluated
at x. the second-order approximation of f , at or near x, is the quadratic function
of z de   ned by

bf (z) = f (x) +    f (x)t (z     x) + (1/2)(z     x)t   2f (x)(z     x).

644

a mathematical background

this second-order approximation satis   es

lim

z   dom f, z6=x, z   x

|f (z)     bf (z)|

kz     xk2

2

= 0.

not surprisingly, the second derivative can be interpreted as the derivative of
the    rst derivative.
if f is di   erentiable, the gradient mapping is the function
   f : rn     rn, with dom   f = dom f , with value    f (x) at x. the derivative
of this mapping is

d   f (x) =    2f (x).

examples
as a simple example consider the quadratic function f : rn     r,

f (x) = (1/2)xt p x + qt x + r,

where p     sn, q     rn, and r     r. its gradient is    f (x) = p x + q, so its hessian
is given by    2f (x) = p . the second-order approximation of a quadratic function
is itself.
as a more complicated example, we consider again the function f : sn     r,
given by f (x) = log det x, with dom f = sn
++. to    nd the second-order approxi-
mation (and therefore, the hessian), we will derive a    rst-order approximation of
the gradient,    f (x) = x    1. for z     sn
++, and    x = z     x, we
have

++ near x     sn

z    1 = (x +    x)   1

= (cid:16)x 1/2(i + x    1/2   xx    1/2)x 1/2(cid:17)   1
= x    1/2(i + x    1/2   xx    1/2)   1x    1/2
    x    1/2(i     x    1/2   xx    1/2)x    1/2
= x    1     x    1   xx    1,

using the    rst-order approximation (i + a)   1     i     a, valid for a small.
this approximation is enough for us to identify the hessian of f at x. the
hessian is a quadratic form on sn. such a quadratic form is cumbersome to de-
scribe in the general case, since it requires four indices. but from the    rst-order
approximation of the gradient above, the quadratic form can be expressed as

    tr(x    1u x    1v ),

where u, v     sn are the arguments of the quadratic form. (this generalizes the
expression for the scalar case: (log x)       =    1/x2.)

now we have the second-order approximation of f near x:

f (z) = f (x +    x)

    f (x) + tr(x    1   x)     (1/2) tr(x    1   xx    1   x)
    f (x) + tr(cid:0)x    1(z     x)(cid:1)     (1/2) tr(cid:0)x    1(z     x)x    1(z     x)(cid:1) .

a.5 id202

645

a.4.4 chain rule for second derivative

a general chain rule for the second derivative is cumbersome in most cases, so we
will state it only for some special cases that we will need.

composition with scalar function
suppose f : rn     r, g : r     r, and h(x) = g(f (x)). simply working out the
partial derivatives yields

   2h(x) = g   (f (x))   2f (x) + g      (f (x))   f (x)   f (x)t .

(a.8)

composition with a   ne function
suppose f : rn     r, a     rn  m, and b     rn. de   ne g : rm     r by g(x) =
f (ax + b). then we have

   2g(x) = at   2f (ax + b)a.

as an example, consider the restriction of a real-valued function f to a line, i.e.,
the function   f (t) = f (x + tv), where x and v are    xed. then we have

   2   f (t) =   f       (t) = vt   2f (x + tv)v.

example a.4 we consider the function f : rn     r from example a.2,

f (x) = log

exp(at

i x + bi),

mxi=1

where a1, . . . , am     rn, and b1, . . . , bm     r. by noting that f (x) = g(ax + b), where
i=1 exp yi), we can obtain a simple formula for the hessian of f . taking
partial derivatives, or using the formula (a.8), noting that g is the composition of

g(y) = log(pm
log withpm

i=1 exp yi, yields

   2g(y) = diag(   g(y))        g(y)   g(y)t ,

where    g(y) is given in (a.7). by the composition formula we have

   2f (x) = at(cid:18) 1

1t z

diag(z)    

1

(1t z)2 zzt(cid:19) a,

where zi = exp(at

i x + bi), i = 1, . . . , m.

a.5 id202

a.5.1 range and nullspace

let a     rm  n (i.e., a is a real matrix with m rows and n columns). the range
of a, denoted r(a), is the set of all vectors in rm that can be written as linear

646

a mathematical background

combinations of the columns of a, i.e.,

r(a) = {ax | x     rn}.

the range r(a) is a subspace of rm, i.e., it is itself a vector space. its dimension
is the rank of a, denoted rank a. the rank of a can never be greater than the
minimum of m and n. we say a has full rank if rank a = min{m, n}.
into zero by a:

the nullspace (or kernel ) of a, denoted n (a), is the set of all vectors x mapped

the nullspace is a subspace of rn.

n (a) = {x | ax = 0}.

orthogonal decomposition induced by a
if v is a subspace of rn, its orthogonal complement, denoted v    , is de   ned as

v     = {x | zt x = 0 for all z     v}.
(as one would expect of a complement, we have v        = v.)

a basic result of id202 is that, for any a     rm  n, we have

n (a) = r(at )   .

(applying the result to at we also have r(a) = n (at )   .) this result is often
stated as

n (a)

   

    r(at ) = rn.

(a.9)

   

    refers to orthogonal direct sum, i.e., the sum of two subspaces
here the symbol
that are orthogonal. the decomposition (a.9) of rn is called the orthogonal de-
composition induced by a.

a.5.2 symmetric eigenvalue decomposition

suppose a     sn, i.e., a is a real symmetric n    n matrix. then a can be factored
as
(a.10)
where q     rn  n is orthogonal, i.e., satis   es qt q = i, and    = diag(  1, . . . ,   n).
the (real) numbers   i are the eigenvalues of a, and are the roots of the charac-
teristic polynomial det(si     a). the columns of q form an orthonormal set of
eigenvectors of a. the factorization (a.10) is called the spectral decomposition or
(symmetric) eigenvalue decomposition of a.

a = q  qt ,

we order the eigenvalues as   1       2                  n. we use the notation   i(a)
to refer to the ith largest eigenvalue of a     s. we usually write the largest or
maximum eigenvalue as   1(a) =   max(a), and the least or minimum eigenvalue as
  n(a) =   min(a).

a.5 id202

647

the determinant and trace can be expressed in terms of the eigenvalues,

det a =

nyi=1

  i,

tr a =

as can the spectral and frobenius norms,

kak2 = max

i=1,...,n|  i| = max{  1,     n},

de   niteness and matrix inequalities

the largest and smallest eigenvalues satisfy

  i,

nxi=1
kakf =  nxi=1

i!1/2

  2

.

  max(a) = sup
x6=0

xt ax
xt x

,

  min(a) = inf
x6=0

xt ax
xt x

.

in particular, for any x, we have

  min(a)xt x     xt ax       max(a)xt x,

with both inequalities tight for (di   erent) choices of x.

a matrix a     sn is called positive de   nite if for all x 6= 0, xt ax > 0. we
denote this as a     0. by the inequality above, we see that a     0 if and only all
its eigenvalues are positive, i.e.,   min(a) > 0. if    a is positive de   nite, we say a
is negative de   nite, which we write as a     0. we use sn
++ to denote the set of
positive de   nite matrices in sn.

if a satis   es xt ax     0 for all x, we say that a is positive semide   nite or
nonnegative de   nite. if    a is nonnegative de   nite, i.e., if xt ax     0 for all x, we
say that a is negative semide   nite or nonpositive de   nite. we use sn
+ to denote
the set of nonnegative de   nite matrices in sn.

for a, b     sn, we use a     b to mean b     a     0, and so on. these inequal-
ities are called matrix inequalities, or generalized inequalities associated with the
positive semide   nite cone.

symmetric squareroot
let a     sn
the (symmetric) squareroot of a as

+, with eigenvalue decomposition a = q diag(  1, . . . ,   n)qt . we de   ne

a1/2 = q diag(  1/2

1

, . . . ,   1/2

n )qt .

the squareroot a1/2 is the unique symmetric positive semide   nite solution of the
equation x 2 = a.

a.5.3 generalized eigenvalue decomposition

the generalized eigenvalues of a pair of symmetric matrices (a, b)     sn    sn are
de   ned as the roots of the polynomial det(sb     a).

648

a mathematical background

we are usually interested in matrix pairs with b     sn

in this case the
generalized eigenvalues are also the eigenvalues of b   1/2ab   1/2 (which are real).
as with the standard eigenvalue decomposition, we order the generalized eigen-
values in nonincreasing order, as   1       2                  n, and denote the maximum
generalized eigenvalue by   max(a, b).

++.

when b     sn

++, the pair of matrices can be factored as

a = v   v t ,

b = v v t ,

(a.11)

where v     rn  n is nonsingular, and    = diag(  1, . . . ,   n), where   i are the
generalized eigenvalues of the pair (a, b). the decomposition (a.11) is called the
generalized eigenvalue decomposition.

the generalized eigenvalue decomposition is related to the standard eigenvalue
decomposition of the matrix b   1/2ab   1/2. if q  qt is the eigenvalue decompo-
sition of b   1/2ab   1/2, then (a.11) holds with v = b1/2q.

a.5.4 singular value decomposition

suppose a     rm  n with rank a = r. then a can be factored as

a = u   v t ,

(a.12)

where u     rm  r satis   es u t u = i, v     rn  r satis   es v t v = i, and    =
diag(  1, . . . ,   r), with

  1       2                  r > 0.

the factorization (a.12) is called the singular value decomposition (svd) of a.
the columns of u are called left singular vectors of a, the columns of v are right
singular vectors, and the numbers   i are the singular values. the singular value
decomposition can be written

a =

rxi=1

  iuivt
i ,

where ui     rm are the left singular vectors, and vi     rn are the right singular
vectors.
the singular value decomposition of a matrix a is closely related to the eigen-
value decomposition of the (symmetric, nonnegative de   nite) matrix at a. us-
ing (a.12) we can write

at a = v   2v t =(cid:2) v

  v (cid:3)(cid:20)   2

0

0

0 (cid:21)(cid:2) v

,

  v (cid:3)t

where   v is any matrix for which [v   v ] is orthogonal. the righthand expression is
the eigenvalue decomposition of at a, so we conclude that its nonzero eigenvalues
are the singular values of a squared, and the associated eigenvectors of at a are
the right singular vectors of a. a similar analysis of aat shows that its nonzero

a.5 id202

649

eigenvalues are also the squares of the singular values of a, and the associated
eigenvectors are the left singular vectors of a.

the    rst or largest singular value is also written as   max(a). it can be expressed

as

  max(a) = sup
x,y6=0

xt ay
kxk2kyk2

= sup
y6=0

kayk2
kyk2

.

the righthand expression shows that the maximum singular value is the    2 operator
norm of a. the minimum singular value of a     rm  n is given by

  min(a) =(cid:26)   r(a)

0

r = min{m, n}
r < min{m, n},

which is positive if and only if a is full rank.

the singular values of a symmetric matrix are the absolute values of its nonzero
eigenvalues, sorted into descending order. the singular values of a symmetric
positive semide   nite matrix are the same as its nonzero eigenvalues.

the condition number of a nonsingular a     rn  n, denoted cond(a) or   (a),

is de   ned as

cond(a) = kak2ka   1k2 =   max(a)/  min(a).

pseudo-inverse
let a = u   v t be the singular value decomposition of a     rm  n, with rank a =
r. we de   ne the pseudo-inverse or moore-penrose inverse of a as

alternative expressions are

a    = v      1u t     rn  m.

a    = lim
     0

(at a +   i)   1at = lim
     0

at (aat +   i)   1,

where the limits are taken with    > 0, which ensures that the inverses in the
expressions exist. if rank a = n, then a    = (at a)   1at . if rank a = m, then
a    = at (aat )   1. if a is square and nonsingular, then a    = a   1.

the pseudo-inverse comes up in problems involving least-squares, minimum
norm, quadratic minimization, and (euclidean) projection. for example, a   b is a
solution of the least-squares problem

minimize

kax     bk2

2

in general. when the solution is not unique, a   b gives the solution with minimum
(euclidean) norm. as another example, the matrix aa    = u u t gives (euclidean)
projection on r(a). the matrix a   a = v v t gives (euclidean) projection on
r(at ).
the optimal value p    of the (general, nonconvex) quadratic optimization prob-
lem

minimize
where p     sn, can be expressed as

(1/2)xt p x + qt x + r,

p    =(cid:26)    (1/2)qt p    q + r p (cid:23) 0,

      

otherwise.

q     r(p )

(this generalizes the expression p    =    (1/2)qt p    1q + r, valid for p     0.)

650

a mathematical background

a.5.5 schur complement

consider a matrix x     sn partitioned as

bt c (cid:21) ,
x =(cid:20) a b

where a     sk. if det a 6= 0, the matrix

s = c     bt a   1b

is called the schur complement of a in x. schur complements arise in several
contexts, and appear in many important formulas and theorems. for example, we
have

det x = det a det s.

inverse of block matrix

the schur complement comes up in solving linear equations, by eliminating one
block of variables. we start with

bt c (cid:21)(cid:20) x
(cid:20) a b

y (cid:21) =(cid:20) u
v (cid:21) ,

and assume that det a 6= 0. if we eliminate x from the top block equation and
substitute it into the bottom block equation, we obtain v = bt a   1u + sy, so

substituting this into the    rst equation yields

y = s   1(v     bt a   1u).

we can express these two equations as a formula for the inverse of a block matrix:

x =(cid:0)a   1 + a   1bs   1bt a   1(cid:1) u     a   1bs   1v.
=(cid:20) a   1 + a   1bs   1bt a   1    a   1bs   1

   s   1bt a   1

s   1

bt c (cid:21)   1
(cid:20) a b

(cid:21) .

in particular, we see that the schur complement is the inverse of the 2, 2 block
entry of the inverse of x.

minimization and de   niteness

the schur complement arises when you minimize a quadratic form over some of
the variables. suppose a     0, and consider the minimization problem

minimize ut au + 2vt bt u + vt cv

(a.13)

with variable u. the solution is u =    a   1bv, and the optimal value is

inf

u (cid:20) u

v (cid:21)t(cid:20) a b

bt c (cid:21)(cid:20) u

v (cid:21) = vt sv.

(a.14)

from this we can derive the following characterizations of positive de   niteness or
semide   niteness of the block matrix x:

a.5 id202

651

    x     0 if and only if a     0 and s     0.
    if a     0, then x (cid:23) 0 if and only if s (cid:23) 0.

schur complement with singular a

some schur complement results have generalizations to the case when a is singular,
although the details are more complicated. as an example, if a (cid:23) 0 and bv    
r(a), then the quadratic minimization problem (a.13) (with variable u) is solvable,
and has optimal value

vt (c     bt a   b)v,

where a    is the pseudo-inverse of a. the problem is unbounded if bv 6    r(a) or
if a 6(cid:23) 0.
the range condition bv     r(a) can also be expressed as (i     aa   )bv = 0,
so we have the following characterization of positive semide   niteness of the block
matrix x:

x (cid:23) 0        a (cid:23) 0,

(i     aa   )b = 0, c     bt a   b (cid:23) 0.

here the matrix c     bt a   b serves as a generalization of the schur complement,
when a is singular.

652

a mathematical background

bibliography

some basic references for the material in this appendix are rudin [rud76] for analysis, and
strang [str80] and meyer [mey00] for id202. more advanced id202 texts
include horn and johnson [hj85, hj91], parlett [par98], golub and van loan [gl89],
trefethen and bau [tb97], and demmel [dem97].
the concept of closed function (  a.3.3) appears frequently in id76, al-
though the terminology varies. the term is used by rockafellar [roc70, page 51], hiriart-
urruty and lemar  echal [hul93, volume 1, page 149], borwein and lewis [bl00, page
76], and bertsekas, nedi  c, and ozdaglar [ber03, page 28].

appendix b

problems involving two
quadratic functions

in this appendix we consider some optimization problems that involve two quadratic,
but not necessarily convex, functions. several strong results hold for these prob-
lems, even when they are not convex.

b.1 single constraint quadratic optimization

we consider the problem with one constraint

xt a0x + 2bt
minimize
subject to xt a1x + 2bt

0 x + c0
1 x + c1     0,

(b.1)

with variable x     rn, and problem parameters ai     sn, bi     rn, ci     r. we do
not assume that ai (cid:23) 0, so problem (b.1) is not a id76 problem.

the lagrangian of (b.1) is

l(x,   ) = xt (a0 +   a1)x + 2(b0 +   b1)t x + c0 +   c1,

and the dual function is

g(  ) = inf
x

=          

l(x,   )
c0 +   c1     (b0 +   b1)t (a0 +   a1)   (b0 +   b1) a0 +   a1 (cid:23) 0,
      

otherwise

b0 +   b1     r(a0 +   a1)

(see   a.5.4). using a schur complement, we can express the dual problem as

  

maximize
subject to        0

(cid:20) a0 +   a1

(b0 +   b1)t

b0 +   b1

c0 +   c1        (cid:21) (cid:23) 0,

(b.2)

654

b problems involving two quadratic functions

an sdp with two variables   ,        r.
the    rst result is that strong duality holds for problem (b.1) and its lagrange
dual (b.2), provided slater   s constraint quali   cation is satis   ed, i.e., there exists
an x with xt a1x + 2bt
1 x + c1 < 0. in other words, if (b.1) is strictly feasible, the
optimal values of (b.1) and (b.2) are equal. (a proof is given in   b.4.)
relaxation interpretation

the dual of the sdp (b.2) is

minimize
subject to

0 x + c0
1 x + c1     0

tr(a0x) + 2bt
tr(a1x) + 2bt

(cid:20) x x

1 (cid:21) (cid:23) 0,

xt

(b.3)

an sdp with variables x     sn, x     rn. this dual sdp has an interesting
interpretation in terms of the original problem (b.1).

we    rst note that (b.1) is equivalent to

minimize
subject to

tr(a0x) + 2bt
tr(a1x) + 2bt
x = xxt .

0 x + c0
1 x + c1     0

(b.4)

in this formulation we express the quadratic terms xt aix as tr(aixxt ), and then
introduce a new variable x = xxt . problem (b.4) has a linear objective function,
one linear inequality constraint, and a nonlinear equality constraint x = xxt . the
next step is to replace the equality constraint by an inequality x (cid:23) xxt :

minimize
subject to

tr(a0x) + bt
tr(a1x) + bt
x (cid:23) xxt .

0 x + c0
1 x + c1     0

(b.5)

this problem is called a relaxation of (b.4), since we have replaced one of the
constraints with a looser constraint. finally we note that the inequality in (b.5)
can be expressed as a linear matrix inequality by using a schur complement, which
gives (b.3).

a number of interesting facts follow immediately from this interpretation of (b.3)
as a relaxation of (b.1). first, it is obvious that the optimal value of (b.3) is less
than or equal to the optimal value of (b.1), since we minimize the same objec-
tive function over a larger set. second, we can conclude that if x = xxt at the
optimum of (b.3), then x must be optimal in (b.1).

combining the result above, that strong duality holds between (b.1) and (b.2)
(if (b.1) is strictly feasible), with strong duality between the dual sdps (b.2)
and (b.3), we conclude that strong duality holds between the original, nonconvex
quadratic problem (b.1), and the sdp relaxation (b.3), provided (b.1) is strictly
feasible.

b.2 the s-procedure

655

b.2 the s-procedure

the next result is a theorem of alternatives for a pair of (nonconvex) quadratic
inequalities. let a1, a2     sn, b1, b2     rn, c1, c2     r, and suppose there exists an
  x with

  xt a2   x + 2bt

2   x + c2 < 0.

then there exists an x     rn satisfying
1 x + c1 < 0,

xt a1x + 2bt

if and only if there exists no    such that

xt a2x + 2bt

2 x + c2     0,

       0,

(cid:20) a1

bt
1

b1

c1 (cid:21) +   (cid:20) a2

bt
2

b2

c2 (cid:21) (cid:23) 0.

(b.6)

(b.7)

in other words, (b.6) and (b.7) are strong alternatives.

this result is readily shown to be equivalent to the result from   b.1, and a proof
is given in   b.4. here we point out that the two inequality systems are clearly weak
alternatives, since (b.6) and (b.7) together lead to a contradiction:

0     (cid:20) x

1 (cid:21)t(cid:18)(cid:20) a1

= xt a1x + 2bt
< 0.

b1

c1 (cid:21) +   (cid:20) a2

bt
1
1 x + c1 +   (xt a2x + 2bt

c2 (cid:21)(cid:19)(cid:20) x
1 (cid:21)

2 x + c2)

bt
2

b2

this theorem of alternatives is sometimes called the s-procedure, and is usually

stated in the following form: the implication

where fi     sn, gi     rn, hi     r, holds if and only if there exists a    such that

xt f1x + 2gt

1 x + h1     0 =    xt f2x + 2gt
h1 (cid:21) ,

h2 (cid:21) (cid:22)   (cid:20) f1

(cid:20) f2

gt
1

gt
2

g2

g1

       0,

2 x + h2     0,

provided there exists a point   x with   xt f1   x + 2gt
is clear.)

1   x + h1 < 0. (note that su   ciency

example b.1 ellipsoid containment. an ellipsoid e     rn with nonempty interior
can be represented as the sublevel set of a quadratic function,

e = {x | xt f x + 2gt x + h     0},

where f     s++ and h     gt f    1g < 0. suppose   e is another ellipsoid with similar
representation,

with   f     s++,   h       gt   f    1  g < 0. by the s-procedure, we see that e       e if and only
if there is a    > 0 such that

  e = {x | xt   f x + 2  gt x +   h     0},

(cid:20)   f

  gt

  g

  h (cid:21) (cid:22)   (cid:20) f

gt

g

h (cid:21) .

656

b problems involving two quadratic functions

b.3 the    eld of values of two symmetric matrices

the following result is the basis for the proof of the strong duality result in   b.1
and the s-procedure in   b.2. if a, b     sn, then for all x     sn
+, there exists an
x     rn such that

xt ax = tr(ax),

xt bx = tr(bx).

(b.8)

remark b.1 geometric interpretation. this result has an interesting interpretation
in terms of the set

which is a cone in r2. it is the cone generated by the set

w (a, b) = {(xt ax, xt bx) | x     rn},

f (a, b) = {(xt ax, xt bx) | kxk2 = 1},

which is called the 2-dimensional    eld of values of the pair (a, b). geometrically,
w (a, b) is the image of the set of rank-one positive semide   nite matrices under the
linear transformation f : sn     r2 de   ned by

f (x) = (tr(ax), tr(bx)).

the result that for every x     sn

+ there exists an x satisfying (b.8) means that

w (a, b) = f (sn

+).

in other words, w (a, b) is a convex cone.

the proof is constructive and uses induction on the rank of x. suppose it is
true for all x     sn
+ with 1     rank x     k, where k     2, that there exists an x such
that (b.8) holds. then the result also holds if rank x = k + 1, as can be seen as
follows. a matrix x     sn
+ with rank x = k + 1 can be expressed as x = yyt + z
where y 6= 0 and z     sn
+ with rank z = k. by assumption, there exists a z such
that tr(az) = zt az, tr(az) = zt bz. therefore

tr(ax) = tr(a(yyt + zzt )),

tr(bx) = tr(b(yyt + zzt )).

the rank of yyt + zzt is one or two, so by assumption there exists an x such
that (b.8) holds.

it is therefore su   cient to prove the result if rank x     2. if rank x = 0 and
rank x = 1 there is nothing to prove. if rank x = 2, we can factor x as x = v v t
where v     rn  2, with linearly independent columns v1 and v2. without loss of
generality we can assume that v t av is diagonal. (if v t av is not diagonal we
replace v with v p where v t av = p diag(  )p t is the eigenvalue decomposition
of v t av .) we will write v t av and v t bv as

and de   ne

v t av =(cid:20)   1

0

0

v t bv =(cid:20)   1
  2 (cid:21) ,
tr(bx) (cid:21) =(cid:20)   1 +   2
  1 +   2 (cid:21) .
w =(cid:20) tr(ax)

  

  

  2 (cid:21) ,

b.4 proofs of the strong duality results

657

we need to show that w = (xt ax, xt bx) for some x.

we distinguish two cases. first, assume (0,   ) is a linear combination of the

vectors (  1,   1) and (  2,   2):

0 = z1  1 + z2  2,

   = z1  1 + z2  2,

for some z1, z2. in this case we choose x =   v1 +  v2, where    and    are determined
by solving two quadratic equations in two variables

  2 + 2    z1 = 1,

  2 + 2    z2 = 1.

(b.9)

this will give the desired result, since

(  v1 +   v2)t b(  v1 +   v2) (cid:21)
(cid:20) (  v1 +   v2)t a(  v1 +   v2)
=   2(cid:20)   1
  1 (cid:21) + 2    (cid:20) 0
= (  2 + 2    z1)(cid:20)   1
= (cid:20)   1 +   2
  1 +   2 (cid:21) .

   (cid:21) +   2(cid:20)   2
  2 (cid:21)
  1 (cid:21) + (  2 + 2    z2)(cid:20)   2
  2 (cid:21)

it remains to show that the equations (b.9) are solvable. to see this, we    rst note
that    and    must be nonzero, so we can write the equations equivalently as

  2(1 + 2(  /  )z1) = 1,

(  /  )2 + 2(  /  )(z2     z1) = 1.

the equation t2 + 2t(z2     z1) = 1 has a positive and a negative root. at least one
of these roots (the root with the same sign as z1) satis   es 1 + 2tz1 > 0, so we can
choose

   =   1/   1 + 2tz1,

   = t  .

this yields two solutions (  ,   ) that satisfy (b.9). (if both roots of t2 +2t(z2   z1) =
1 satisfy 1 + 2tz1 > 0, we obtain four solutions.)
next, assume that (0,   ) is not a linear combination of (  1,   1) and (  2,   2). in
particular, this means that (  1,   1) and (  2,   2) are linearly dependent. therefore
their sum w = (  1 +   2,   1 +   2) is a nonnegative multiple of (  1,   1), or (  2,   2),
or both. if w =   2(  1,   1) for some   , we can choose x =   v1. if w =   2(  2,   2)
for some   , we can choose x =   v2.

b.4 proofs of the strong duality results

we    rst prove the s-procedure result given in   b.2. the assumption of strict
feasibility of   x implies that the matrix

(cid:20) a2

bt
2

b2

c2 (cid:21)

658

b problems involving two quadratic functions

has at least one negative eigenvalue. therefore

       0,

  (cid:20) a2

bt
2

b2

c2 (cid:21) (cid:23) 0 =       = 0.

we can apply the theorem of alternatives for nonstrict linear matrix inequalities,
given in example 5.14, which states that (b.7) is infeasible if and only if

x (cid:23) 0,

tr(cid:18)x(cid:20) a1

bt
1

b1

c1 (cid:21)(cid:19) < 0,

tr(cid:18)x(cid:20) a2

bt
2

b2

c2 (cid:21)(cid:19)     0

is feasible. from   b.3 this is equivalent to feasibility of

(cid:20) v
w (cid:21)t(cid:20) a1

bt
1

b1

c1 (cid:21)(cid:20) v

w (cid:21) < 0,

(cid:20) v
w (cid:21)t(cid:20) a2

bt
2

b2

c2 (cid:21)(cid:20) v

w (cid:21)     0.

if w 6= 0, then x = v/w is feasible in (b.6).
vt a2v     0, so x =   x + tv satis   es

if w = 0, we have vt a1v < 0,

xt a1x + 2bt
xt a2x + 2bt

1 x + c1 =   xt a1   x + 2bt
2 x + c2 =   xt a2   x + 2bt

1   x + c1 + t2vt a1v + 2t(a1   x + b1)t v
2   x + c2 + t2vt a2v + 2t(a2   x + b2)t v

< 2t(a2   x + b2)t v,

i.e., x becomes feasible as t          , depending on the sign of (a2   x + b2)t v.
finally, we prove the result in   b.1, i.e., that the optimal values of (b.1)
and (b.2) are equal if (b.1) is strictly feasible. to do this we note that    is a
lower bound for the optimal value of (b.1) if

by the s-procedure this is true if and only if there exists a        0 such that

xt a1x + bt

0 x + c0       .

1 x + c1     0 =    xt a0x + bt
(cid:20) a0
c1 (cid:21) (cid:23) 0,

c0        (cid:21) +   (cid:20) a1

bt
0

bt
1

b0

b1

i.e.,   ,    are feasible in (b.2).

bibliography

bibliography

659

the results in this appendix are known under di   erent names in di   erent disciplines.
the term s-procedure is from control; see boyd, el ghaoui, feron, and balakrishnan
[befb94, pages 23, 33] for a survey and references. variations of the s-procedure are
known in id202 in the context of joint diagonalization of a pair of symmetric
matrices; see, for example, calabi [cal64] and uhlig [uhl79]. special cases of the strong
duality result are studied in the nonid135 literature on trust-region methods
(stern and wolkowicz [sw95], nocedal and wright [nw99, page 78]).
brickman [bri61] proves that the    eld of values of a pair of matrices a, b     sn (i.e., the
set f (a, b) de   ned in remark b.1) is a convex set if n > 2, and that the set w (a, b)
is a convex cone (for any n). our proof in   b.3 is based on hestenes [hes68]. many
related results and additional references can be found in horn and johnson [hj91,   1.8]
and ben-tal and nemirovski [btn01,   4.10.5].

appendix c

numerical id202
background

in this appendix we give a brief overview of some basic numerical id202,
concentrating on methods for solving one or more sets of linear equations. we focus
on direct (i.e., noniterative) methods, and how problem structure can be exploited
to improve e   ciency. there are many important issues and methods in numerical
id202 that we do not consider here, including numerical stability, details
of id105s, methods for parallel or multiple processors, and iterative
methods. for these (and other) topics, we refer the reader to the references given
at the end of this appendix.

c.1 matrix structure and algorithm complexity

we concentrate on methods for solving the set of linear equations

ax = b

(c.1)

where a     rn  n and b     rn. we assume a is nonsingular, so the solution is
unique for all values of b, and given by x = a   1b. this basic problem arises in
many optimization algorithms, and often accounts for most of the computation. in
the context of solving the linear equations (c.1), the matrix a is often called the
coe   cient matrix, and the vector b is called the righthand side.

the standard generic methods for solving (c.1) require a computational e   ort
that grows approximately like n3. these methods assume nothing more about a
than nonsingularity, and so are generally applicable. for n several hundred or
smaller, these generic methods are probably the best methods to use, except in the
most demanding real-time applications. for n more than a thousand or so, the
generic methods of solving ax = b become less practical.

662

c numerical id202 background

coe   cient matrix structure

in many cases the coe   cient matrix a has some special structure or form that can
be exploited to solve the equation ax = b more e   ciently, using methods tailored
for the special structure. for example, in the newton system    2f (x)   xnt =
      f (x), the coe   cient matrix is symmetric and positive de   nite, which allows us
to use a solution method that is around twice as fast as the generic method (and
also has better roundo    properties). there are many other types of structure that
can be exploited, with computational savings (or algorithm speedup) that is usually
far more than a factor of two. in many cases, the e   ort is reduced to something
proportional to n2 or even n, as compared to n3 for the generic methods. since
these methods are usually applied when n is at least a hundred, and often far larger,
the savings can be dramatic.

a wide variety of coe   cient matrix structures can be exploited. simple exam-
ples related to the sparsity pattern (i.e., the pattern of zero and nonzero entries
in the matrix) include banded, block diagonal, or sparse matrices. a more subtle
exploitable structure is diagonal plus low rank. many common forms of convex
optimization problems lead to linear equations with coe   cient matrices that have
these exploitable structures. (there are many other matrix structures that can be
exploited, e.g., toeplitz, hankel, and circulant, that we will not consider in this
appendix.)

we refer to a generic method that does not exploit any sparsity pattern in the
matrices as one for dense matrices. we refer to a method that does not exploit any
structure at all in the matrices as one for unstructured matrices.

c.1.1 complexity analysis via    op count

the cost of a numerical id202 algorithm is often expressed by giving the
total number of    oating-point operations or    ops required to carry it out, as a
function of various problem dimensions. we de   ne a    op as one addition, sub-
traction, multiplication, or division of two    oating-point numbers. (some authors
de   ne a    op as one multiplication followed by one addition, so their    op counts
are smaller by a factor up to two.) to evaluate the complexity of an algorithm, we
count the total number of    ops, express it as a function (usually a polynomial) of
the dimensions of the matrices and vectors involved, and simplify the expression
by ignoring all terms except the leading (i.e., highest order or dominant) terms.

as an example, suppose that a particular algorithm requires a total of

m3 + 3m2n + mn + 4mn2 + 5m + 22

   ops, where m and n are problem dimensions. we would normally simplify this
   op count to

m3 + 3m2n + 4mn2

   ops, since these are the leading terms in the problem dimensions m and n.
if
in addition we assumed that m     n, we would further simplify the    op count to
4mn2.

c.1 matrix structure and algorithm complexity

663

flop counts were originally popularized when    oating-point operations were rel-
atively slow, so counting the number gave a good estimate of the total computation
time. this is no longer the case: issues such as cache boundaries and locality of
reference can dramatically a   ect the computation time of a numerical algorithm.
however,    op counts can still give us a good rough estimate of the computation
time of a numerical algorithm, and how the time grows with increasing problem
size. since a    op count no longer accurately predicts the computation time of an
algorithm, we usually pay most attention to its order or orders, i.e., its largest
exponents, and ignore di   erences in    op counts smaller than a factor of two or so.
for example, an algorithm with    op count 5n2 is considered comparable to one
with a    op count 4n2, but faster than an algorithm with    op count (1/3)n3.

c.1.2 cost of basic matrix-vector operations

vector operations
to compute the inner product xt y of two vectors x, y     rn we form the products
xiyi, and then add them, which requires n multiplies and n    1 additions, or 2n    1
   ops. as mentioned above, we keep only the leading term, and say that the inner
product requires 2n    ops, or even more approximately, order n    ops. a scalar-
vector multiplication   x, where        r and x     rn costs n    ops. the addition
x + y of two vectors x, y     rn also costs n    ops.
if the vectors x and y are sparse, i.e., have only a few nonzero terms, these
basic operations can be carried out faster (assuming the vectors are stored using
an appropriate data structure). for example, if x is a sparse vector with n nonzero
entries, then the inner product xt y can be computed in 2n    ops.

matrix-vector multiplication
a matrix-vector multiplication y = ax where a     rm  n costs 2mn    ops: we have
to calculate m components of y, each of which is the product of a row of a with
x, i.e., an inner product of two vectors in rn.

matrix-vector products can often be accelerated by taking advantage of struc-
ture in a. for example, if a is diagonal, then ax can be computed in n    ops,
instead of 2n2    ops for multiplication by a general n   n matrix. more generally, if
a is sparse, with only n nonzero elements (out of mn), then 2n    ops are needed
to form ax, since we can skip multiplications and additions with zero.

as a less obvious example, suppose the matrix a has rank p     min{m, n}, and
is represented (stored) in the factored form a = u v , where u     rm  p, v     rp  n.
then we can compute ax by    rst computing v x (which costs 2pn    ops), and then
computing u (v x) (which costs 2mp    ops), so the total is 2p(m + n)    ops. since
p     min{m, n}, this is small compared to 2mn.
matrix-id127
the matrix-matrix product c = ab, where a     rm  n and b     rn  p, costs 2mnp
   ops. we have mp elements in c to calculate, each of which is an inner product of

664

c numerical id202 background

two vectors of length n. again, we can often make substantial savings by taking
advantage of structure in a and b. for example, if a and b are sparse, we can
accelerate the multiplication by skipping additions and multiplications with zero.
if m = p and we know that c is symmetric, then we can calculate the matrix
product in m2n    ops, since we only have to compute the (1/2)m(m + 1) elements
in the lower triangular part.

to form the product of several matrices, we can carry out the matrix-matrix
multiplications in di   erent ways, which have di   erent    op counts in general. the
simplest example is computing the product d = abc, where a     rm  n, b    
rn  p, and c     rp  q. here we can compute d in two ways, using matrix-matrix
multiplies. one method is to    rst form the product ab (2mnp    ops), and then form
d = (ab)c (2mpq    ops), so the total is 2mp(n+q)    ops. alternatively, we can    rst
form the product bc (2npq    ops), and then form d = a(bc) (2mnq    ops), with a
total of 2nq(m+p)    ops. the    rst method is better when 2mp(n+q) < 2nq(m+p),
i.e., when

1
n

+

1
q

<

1
m

+

1
p

.

this assumes that no structure of the matrices is exploited in carrying out matrix-
matrix products.

for products of more than three matrices, there are many ways to parse the
product into matrix-id127s. although it is not hard to develop an
algorithm that determines the best parsing (i.e., the one with the fewest required
   ops) given the matrix dimensions, in most applications the best parsing is clear.

c.2 solving linear equations with factored matrices

c.2.1 linear equations that are easy to solve

we start by examining some cases for which ax = b is easily solved, i.e., x = a   1b
is easily computed.

diagonal matrices

suppose a is diagonal and nonsingular (i.e., aii 6= 0 for all i). the set of linear
equations ax = b can be written as aiixi = bi, i = 1, . . . , n. the solution is given
by xi = bi/aii, and can be calculated in n    ops.

lower triangular matrices
a matrix a     rn  n is lower triangular if aij = 0 for j > i. a lower triangular
matrix is called unit lower triangular if the diagonal elements are equal to one. a
lower triangular matrix is nonsingular if and only if aii 6= 0 for all i.

c.2 solving linear equations with factored matrices

665

suppose a is lower triangular and nonsingular. the equations ax = b are

               

a11
0
a21
a22
...
...
an1 an2

      
0
      
0
...
. . .
       ann

               

               

x1
x2
...
xn

               

=               

b1
b2
...
bn

.

               

from the    rst row, we have a11x1 = b1, from which we conclude x1 = b1/a11.
from the second row we have a21x1 + a22x2 = b2, so we can express x2 as x2 =
(b2   a21x1)/a22. (we have already computed x1, so every number on the righthand
side is known.) continuing this way, we can express each component of x in terms
of previous components, yielding the algorithm

x1
x2
x3

xn

:= b1/a11
:= (b2     a21x1)/a22
:= (b3     a31x1     a32x2)/a33
...
:= (bn     an1x1     an2x2                an,n   1xn   1)/ann.

this procedure is called forward substitution, since we successively compute the
components of x by substituting the known values into the next equation.

let us give a    op count for forward substitution. we start by calculating x1 (1
   op). we substitute x1 in the second equation to    nd x2 (3    ops), then substitute
x1 and x2 in the third equation to    nd x3 (5    ops), etc. the total number of    ops
is

1 + 3 + 5 +        + (2n     1) = n2.

thus, when a is lower triangular and nonsingular, we can compute x = a   1b in
n2    ops.

if the matrix a has additional structure, in addition to being lower triangular,
then forward substitution can be more e   cient than n2    ops. for example, if a
is sparse (or banded), with at most k nonzero entries per row, then each forward
substitution step requires at most 2k + 1    ops, so the overall    op count is 2(k + 1)n,
or 2kn after dropping the term 2n.

upper triangular matrices
a matrix a     rn  n is upper triangular if at is lower triangular, i.e., if aij = 0 for
j < i. we can solve linear equations with nonsingular upper triangular coe   cient
matrix in a way similar to forward substitution, except that we start by calculating
xn, then xn   1, and so on. the algorithm is

xn
xn   1
xn   2

x1

:= bn/ann
:= (bn   1     an   1,nxn)/an   1,n   1
:= (bn   2     an   2,n   1xn   1     an   2,nxn)/an   2,n   2
...
:= (b1     a12x2     a13x3                a1nxn)/a11.

666

c numerical id202 background

this is called backward substitution or back substitution since we determine the
coe   cients in backward order. the cost to compute x = a   1b via backward
substitution is n2    ops. if a is upper triangular and sparse (or banded), with at
most k nonzero entries per row, then back substitution costs 2kn    ops.

orthogonal matrices
a matrix a     rn  n is orthogonal if at a = i, i.e., a   1 = at . in this case we can
compute x = a   1b by a simple matrix-vector product x = at b, which costs 2n2
in general.

if the matrix a has additional structure, we can compute x = a   1b even more
e   ciently than 2n2    ops. for example, if a has the form a = i     2uut , where
kuk2 = 1, we can compute

x = a   1b = (i     2uut )t b = b     2(ut b)u

by    rst computing ut b, then forming b     2(ut b)u, which costs 4n    ops.
permutation matrices

let    = (  1, . . . ,   n) be a permutation of (1, 2, . . . , n). the associated permutation
matrix a     rn  n is given by

aij =(cid:26) 1 j =   i

0 otherwise.

in each row (or column) of a permutation matrix there is exactly one entry with
value one; all other entries are zero. multiplying a vector by a permutation matrix
simply permutes its coe   cients:

ax = (x  1, . . . , x  n ) .

the inverse of a permutation matrix is the permutation matrix associated with the
inverse permutation      1. this turns out to be at , which shows that permutation
matrices are orthogonal.

if a is a permutation matrix, solving ax = b is very easy: x is obtained by
permuting the entries of b by      1. this requires no    oating point operations,
according to our de   nition (but, depending on the implementation, might involve
copying    oating point numbers). we can reach the same conclusion from the
equation x = at b. the matrix at (like a) has only one nonzero entry per row, with
value one. thus no additions are required, and the only multiplications required
are by one.

c.2.2 the factor-solve method

the basic approach to solving ax = b is based on expressing a as a product of
nonsingular matrices,

a = a1a2        ak,

c.2 solving linear equations with factored matrices

667

so that

we can compute x using this formula, working from right to left:

x = a   1b = a   1

k a   1

k   1        a   1
1 b.

2 a   1
1 b

z1
z2

:= a   1
1 b
2 z1 = a   1
:= a   1
...
:= a   1
x := a   1

zk   1

k   1zk   2 = a   1
k zk   1 = a   1

k   1        a   1
1 b
k        a   1
1 b.
the ith step of this process requires computing zi = a   1
i zi   1, i.e., solving the
linear equations aizi = zi   1. if each of these equations is easy to solve (e.g., if ai
is diagonal, lower or upper triangular, a permutation, etc.), this gives a method for
computing x = a   1b.

the step of expressing a in factored form (i.e., computing the factors ai) is
called the factorization step, and the process of computing x = a   1b recursively,
by solving a sequence problems of the form aizi = zi   1, is often called the solve
step. the total    op count for solving ax = b using this factor-solve method is f +s,
where f is the    op count for computing the factorization, and s is the total    op
count for the solve step. in many cases, the cost of the factorization, f , dominates
the total solve cost s.
in this case, the cost of solving ax = b, i.e., computing
x = a   1b, is just f .

solving equations with multiple righthand sides

suppose we need to solve the equations

ax1 = b1,

ax2 = b2,

. . . ,

axm = bm,

where a     rn  n is nonsingular.
in other words, we need to solve m sets of
linear equations, with the same coe   cient matrix, but di   erent righthand sides.
alternatively, we can think of this as computing the matrix

where

x = a   1b

x =(cid:2) x1 x2

       xm (cid:3)     rn  m,

b =(cid:2) b1

b2

      

bm (cid:3)     rn  m.

to do this, we    rst factor a, which costs f . then for i = 1, . . . , m we compute
a   1bi using the solve step. since we only factor a once, the total e   ort is

f + ms.

in other words, we amortize the factorization cost over the set of m solves. had we
(needlessly) repeated the factorization step for each i, the cost would be m(f + s).
when the factorization cost f dominates the solve cost s, the factor-solve
method allows us to solve a small number of linear systems, with the same co-
e   cient matrix, at essentially the same cost as solving one. this is because the
most expensive step, the factorization, is done only once.

668

c numerical id202 background

we can use the factor-solve method to compute the inverse a   1 by solving
ax = ei for i = 1, . . . , n, i.e., by computing a   1i. this requires one factorization
and n solves, so the cost is f + ns.

c.3 lu, cholesky, and ldlt factorization

c.3.1 lu factorization

every nonsingular matrix a     rn  n can be factored as

a = p lu

where p     rn  n is a permutation matrix, l     rn  n is unit lower triangular, and
u     rn  n is upper triangular and nonsingular. this is called the lu factorization
of a. we can also write the factorization as p t a = lu , where the matrix p t a is
obtained from a by re-ordering the rows. the standard algorithm for computing an
lu factorization is called gaussian elimination with partial pivoting or gaussian
elimination with row pivoting. the cost is (2/3)n3    ops if no structure in a is
exploited, which is the case we consider    rst.

solving sets of linear equations using the lu factorization

the lu factorization, combined with the factor-solve approach, is the standard
method for solving a general set of linear equations ax = b.

algorithm c.1 solving linear equations by lu factorization.

given a set of linear equations ax = b, with a nonsingular.

1. lu factorization. factor a as a = p lu ((2/3)n3    ops).
2. permutation. solve p z1 = b (0    ops).
3. forward substitution. solve lz2 = z1 (n2    ops).
4. backward substitution. solve u x = z2 (n2    ops).

the total cost is (2/3)n3 + 2n2, or (2/3)n3    ops if we keep only the leading term.
if we need to solve multiple sets of linear equations with di   erent righthand

sides, i.e., axi = bi, i = 1, . . . , m, the cost is

(2/3)n3 + 2mn2,

since we factor a once, and carry out m pairs of forward and backward substi-
tutions. for example, we can solve two sets of linear equations, with the same
coe   cient matrix but di   erent righthand sides, at essentially the same cost as
solving one. we can compute the inverse a   1 by solving the equations axi = ei,
where xi is the ith column of a   1, and ei is the ith unit vector. this costs (8/3)n3,
i.e., about 3n3    ops.

if the matrix a has certain structure, for example banded or sparse, the lu fac-
torization can be computed in less than (2/3)n3    ops, and the associated forward
and backward substitutions can also be carried out more e   ciently.

c.3 lu, cholesky, and ldlt factorization

669

lu factorization of banded matrices
suppose the matrix a     rn  n is banded, i.e., aij = 0 if |i     j| > k, where
k < n    1 is called the bandwidth of a. we are interested in the case where k     n,
i.e., the bandwidth is much smaller than the size of the matrix. in this case an
lu factorization of a can be computed in roughly 4nk2    ops. the resulting upper
triangular matrix u has bandwidth at most 2k, and the lower triangular matrix l
has at most k + 1 nonzeros per column, so the forward and back substitutions can
be carried out in order 6nk    ops. therefore if a is banded, the linear equations
ax = b can be solved in about 4nk2    ops.

lu factorization of sparse matrices

when the matrix a is sparse, the lu factorization usually includes both row and
column permutations, i.e., a is factored as

a = p1lu p2,

where p1 and p2 are permutation matrices, l is lower triangular, and u is upper
triangular. if the factors l and u are sparse, the forward and backward substi-
tutions can be carried out e   ciently, and we have an e   cient method for solving
ax = b. the sparsity of the factors l and u depends on the permutations p1 and
p2, which are chosen in part to yield relatively sparse factors.

the cost of computing the sparse lu factorization depends in a complicated
way on the size of a, the number of nonzero elements, its sparsity pattern, and
the particular algorithm used, but is often dramatically smaller than the cost of a
dense lu factorization. in many cases the cost grows approximately linearly with
n, when n is large. this means that when a is sparse, we can solve ax = b very
e   ciently, often with an order approximately n.

c.3.2 cholesky factorization

if a     rn  n is symmetric and positive de   nite, then it can be factored as

a = llt

where l is lower triangular and nonsingular with positive diagonal elements. this
is called the cholesky factorization of a, and can be interpreted as a symmetric
lu factorization (with l = u t ). the matrix l, which is uniquely determined
by a, is called the cholesky factor of a. the cost of computing the cholesky
factorization of a dense matrix, i.e., without exploiting any structure, is (1/3)n3
   ops, half the cost of an lu factorization.

solving positive de   nite sets of equations using cholesky factorization

the cholesky factorization can be used to solve ax = b when a is symmetric
positive de   nite.

670

c numerical id202 background

algorithm c.2 solving linear equations by cholesky factorization.

given a set of linear equations ax = b, with a     sn
1. cholesky factorization. factor a as a = llt ((1/3)n3    ops).
2. forward substitution. solve lz1 = b (n2    ops).
3. backward substitution. solve lt x = z1 (n2    ops).

++.

the total cost is (1/3)n3 + 2n2, or roughly (1/3)n3    ops.

there are specialized algorithms, with a complexity much lower than (1/3)n3,

for cholesky factorization of banded and sparse matrices.

cholesky factorization of banded matrices

if a is symmetric positive de   nite and banded with bandwidth k, then its cholesky
factor l is banded with bandwidth k, and can be calculated in nk2    ops. the cost
of the associated solve step is 4nk    ops.

cholesky factorization of sparse matrices

when a is symmetric positive de   nite and sparse, it is usually factored as

a = p llt p t ,

where p is a permutation matrix and l is lower triangular with positive diagonal
elements. we can also express this as p t ap = llt , i.e., llt is the cholesky
factorization of p t ap . we can interpret this as    rst re-ordering the variables and
equations, and then forming the (standard) cholesky factorization of the resulting
permuted matrix. since p t ap is positive de   nite for any permutation matrix p ,
we are free to choose any permutation matrix; for each choice there is a unique
associated cholesky factor l. the choice of p , however, can greatly a   ect the
sparsity of the factor l, which in turn can greatly a   ect the e   ciency of solving
ax = b. various heuristic methods are used to select a permutation p that leads
to a sparse factor l.

example c.1 cholesky factorization with an arrow sparsity pattern. consider a
sparse matrix of the form

a =(cid:20) 1 ut
u d (cid:21)

where d     rn  n is positive diagonal, and u     rn. it can be shown that a is positive
de   nite if ut d   1u < 1. the cholesky factorization of a is

(cid:20) 1 ut
u d (cid:21) =(cid:20) 1

0 lt (cid:21)
u l (cid:21)(cid:20) 1 ut

0

(c.2)

where l is lower triangular with llt = d    uut . for general u, the matrix d    uut
is dense, so we can expect l to be dense. although the matrix a is very sparse
(most of its rows have just two nonzero elements), its cholesky factors are almost
completely dense.

c.3 lu, cholesky, and ldlt factorization

671

on the other hand, suppose we permute the    rst row and column of a to the end.
after this re-ordering, we obtain the cholesky factorization

(cid:20) d u

1 (cid:21) =(cid:20) d1/2

ut d   1/2    1     ut d   1u (cid:21)(cid:20) d1/2

ut

0

0

d   1/2u

   1     ut d   1u (cid:21) .

now the cholesky factor has a diagonal 1,1 block, so it is very sparse.

this example illustrates that the re-ordering greatly a   ects the sparsity of the cholesky
factors. here it was quite obvious what the best permutation is, and all good re-
ordering heuristics would select this re-ordering and permute the dense row and
column to the end. for more complicated sparsity patterns, it can be very di   cult
to    nd the    best    re-ordering (i.e., resulting in the greatest number of zero elements
in l), but various heuristics provide good suboptimal permutations.

for the sparse cholesky factorization, the re-ordering permutation p is often
determined using only sparsity pattern of the matrix a, and not the particular
numerical values of the nonzero elements of a. once p is chosen, we can also
determine the sparsity pattern of l without knowing the numerical values of the
nonzero entries of a. these two steps combined are called the symbolic factorization
of a, and form the    rst step in a sparse cholesky factorization. in contrast, the
permutation matrices in a sparse lu factorization do depend on the numerical
values in a, in addition to its sparsity pattern.

the symbolic factorization is then followed by the numerical factorization, i.e.,
the calculation of the nonzero elements of l. software packages for sparse cholesky
factorization often include separate routines for the symbolic and the numerical
factorization. this is useful in many applications, because the cost of the symbolic
factorization is signi   cant, and often comparable to the numerical factorization.
suppose, for example, that we need to solve m sets of linear equations

a1x = b1,

a2x = b2,

. . . ,

amx = bm

where the matrices ai are symmetric positive de   nite, with di   erent numerical
values, but the same sparsity pattern. suppose the cost of a symbolic factorization
is fsymb, the cost of a numerical factorization is fnum, and the cost of the solve step
is s. then we can solve the m sets of linear equations in

fsymb + m(fnum + s)

   ops, since we only need to carry out the symbolic factorization once, for all m sets
of equations. if instead we carry out a separate symbolic factorization for each set
of linear equations, the    op count is m(fsymb + fnum + s).

c.3.3 ldlt factorization

every nonsingular symmetric matrix a can be factored as

a = p ldlt p t

672

c numerical id202 background

where p is a permutation matrix, l is lower triangular with positive diagonal
elements, and d is block diagonal, with nonsingular 1    1 and 2    2 diagonal
blocks. this is called an ldlt factorization of a. (the cholesky factorization
can be considered a special case of ldlt factorization, with p = i and d = i.)
an ldlt factorization can be computed in (1/3)n3    ops, if no structure of a is
exploited.

algorithm c.3 solving linear equations by ldlt factorization.

given a set of linear equations ax = b, with a     sn nonsingular.

1. ldlt factorization. factor a as a = p ldlt p ((1/3)n3    ops).
2. permutation. solve p z1 = b (0    ops).
3. forward substitution. solve lz2 = z1 (n2    ops).
4. (block) diagonal solve. solve dz3 = z2 (order n    ops).
5. backward substitution. solve lt z4 = z3 (n2    ops).
6. permutation. solve p t x = z4 (0    ops).

the total cost is, keeping only the dominant term, (1/3)n3    ops.

ldlt factorization of banded and sparse matrices

as with the lu and cholesky factorizations, there are specialized methods for
calculating the ldlt factorization of a sparse or banded matrix. these are similar
to the analogous methods for cholesky factorization, with the additional factor d.
in a sparse ldlt factorization, the permutation matrix p cannot be chosen only
on the basis of the sparsity pattern of a (as in a sparse cholesky factorization); it
also depends on the particular nonzero values in the matrix a.

c.4 block elimination and schur complements

c.4.1 eliminating a block of variables

in this section we describe a general method that can be used to solve ax = b
by    rst eliminating a subset of the variables, and then solving a smaller system
of linear equations for the remaining variables. for a dense unstructured matrix,
this approach gives no advantage. but when the submatrix of a associated with
the eliminated variables is easily factored (for example, if it is block diagonal or
banded) the method can be substantially more e   cient than a general method.

suppose we partition the variable x     rn into two blocks or subvectors,

where x1     rn1 , x2     rn2 . we conformally partition the linear equations ax = b
as

(c.3)

x =(cid:20) x1
x2 (cid:21) ,
(cid:20) a11 a12
a21 a22 (cid:21)(cid:20) x1
x2 (cid:21) =(cid:20) b1
b2 (cid:21)

c.4 block elimination and schur complements

673

where a11     rn1  n1, a22     rn2  n2. assuming that the submatrix a11 is invert-
ible, we can eliminate x1 from the equations, as follows. using the    rst equation,
we can express x1 in terms of x2:

substituting this expression into the second equation yields

x1 = a   1

11 (b1     a12x2).

(a22     a21a   1

11 a12)x2 = b2     a21a   1

11 b1.

(c.4)

(c.5)

we refer to this as the reduced equation obtained by eliminating x1 from the orig-
inal equation. the reduced equation (c.5) and the equation (c.4) together are
equivalent to the original equations (c.3). the matrix appearing in the reduced
equation is called the schur complement of the    rst block a11 in a:

s = a22     a21a   1

11 a12

(see also   a.5.5). the schur complement s is nonsingular if and only if a is
nonsingular.
the two equations (c.5) and (c.4) give us an alternative approach to solving
the original system of equations (c.3). we    rst form the schur complement s, then
   nd x2 by solving (c.5), and then calculate x1 from (c.4). we can summarize this
method as follows.

algorithm c.4 solving linear equations by block elimination.

given a nonsingular set of linear equations (c.3), with a11 nonsingular.

1. form a   1
11 a12 and a   1
11 b1.
11 a12 and   b = b2     a21a   1
2. form s = a22     a21a   1
3. determine x2 by solving sx2 =   b.
4. determine x1 by solving a11x1 = b1     a12x2.

11 b1.

remark c.1 interpretation as block factor-solve. block elimination can be interpreted
in terms of the factor-solve approach described in   c.2.2, based on the factorization

(cid:20) a11 a12
a21 a22 (cid:21) =(cid:20) a11

a21 s (cid:21)(cid:20) i a   1

0

0

i

11 a12

(cid:21) ,

which can be considered a block lu factorization. this block lu factorization sug-
gests the following method for solving (c.3). we    rst do a    block forward substitution   
to solve

and then solve

0

a21 s (cid:21)(cid:20) z1
(cid:20) a11
z2 (cid:21) =(cid:20) b1
b2 (cid:21) ,
(cid:20) i a   1
(cid:21)(cid:20) x1
x2 (cid:21) =(cid:20) z1
z2 (cid:21)

11 a12

0

i

674

c numerical id202 background

by    block backward substitution   . this yields the same expressions as the block
elimination method:

11 b1

z1 = a   1
z2 = s   1(b2     a21z1)
x2 = z2
x1 = z1     a   1

11 a12z2.

in fact, the modern approach to the factor-solve method is based on block factor
and solve steps like these, with the block sizes optimally chosen for the processor (or
processors), cache sizes, etc.

complexity analysis of block elimination method

to analyze the (possible) advantage of solving the set of linear equations using
block elimination, we carry out a    op count. we let f and s denote the cost of
factoring a11 and carrying out the associated solve step, respectively. to keep the
analysis simple we assume (for now) that a12, a22, and a21 are treated as dense,
unstructured matrices. the    op counts for each of the four steps in solving ax = b
using block elimination are:

1. computing a   1

11 a12 and a   1

11 b1 requires factoring a11 and n2 + 1 solves, so

it costs f + (n2 + 1)s, or just f + n2s, dropping the dominated term s.

2. forming the schur complement s requires the matrix multiply a21(a   1
2n1, and an n2    n2 matrix subtraction, which costs n2

11 a12),
which costs 2n2
2 (and
can be dropped). the cost of forming   b = b2    a21a   1
11 b1 is dominated by the
cost of forming s, and so can be ignored. the total cost of step 2, ignoring
dominated terms, is then 2n2

2n1.

3. to compute x2 = s   1  b, we factor s and solve, which costs (2/3)n3
2.
4. forming b1   a12x2 costs 2n1n2 +n1    ops. to compute x1 = a   1

11 (b1   a12x2),
we can use the factorization of a11 already computed in step 1, so only the
solve is necessary, which costs s. both of these costs are dominated by other
terms, and can be ignored.

the total cost is then

   ops.

f + n2s + 2n2

2n1 + (2/3)n3
2

(c.6)

eliminating an unstructured matrix

we    rst consider the case when no structure in a11 is exploited. we factor a11
using a standard lu factorization, so f = (2/3)n3
1, and then solve using a forward
and a backward substitution, so s = 2n2
1. the    op count for solving the equations
via block elimination is then

(2/3)n3

1 + n2(2n2

1) + 2n2

2n1 + (2/3)n3

2 = (2/3)(n1 + n2)3,

c.4 block elimination and schur complements

675

which is the same as just solving the larger set of equations using a standard lu
factorization. in other words, solving a set of equations by block elimination gives
no advantage when no structure of a11 is exploited.

on the other hand, when the structure of a11 allows us to factor and solve
more e   ciently than the standard method, block elimination can be more e   cient
than applying the standard method.

eliminating a diagonal matrix

if a11 is diagonal, no factorization is needed, and we can carry out a solve in n1
   ops, so we have f = 0 and s = n1. substituting these values into (c.6) and
keeping only the leading terms yields

2n2

2n1 + (2/3)n3
2,

   ops, which is far smaller than (2/3)(n1 +n2)3, the cost using the standard method.
in particular, the    op count of the standard method grows cubicly in n1, whereas
for block elimination the    op count grows only linearly in n1.

eliminating a banded matrix

if a11 is banded with bandwidth k, we can carry out the factorization in about
f = 4k2n1    ops, and the solve can be done in about s = 6kn1    ops. the overall
complexity of solving ax = b using block elimination is

4k2n1 + 6n2kn1 + 2n2

2n1 + (2/3)n3
2

2n1+(2/3)n3
   ops. assuming k is small compared to n1 and n2, this simpli   es to 2n2
2,
the same as when a11 is diagonal. in particular, the complexity grows linearly in
n1, as opposed to cubicly in n1 for the standard method.

a matrix for which a11 is banded is sometimes called an arrow matrix since the
sparsity pattern, when n1     n2, looks like an arrow pointing down and right. block
elimination can solve linear equations with arrow structure far more e   ciently than
the standard method.

eliminating a block diagonal matrix

suppose that a11 is block diagonal, with (square) block sizes m1, . . . , mk, where
n1 = m1 +        + mk.
in this case we can factor a11 by factoring each block
separately, and similarly we can carry out the solve step on each block separately.
using standard methods for these we    nd
1 +        + (2/3)m3
k,

1 +        + 2m2
k,

f = (2/3)m3

s = 2m2

so the overall complexity of block elimination is

(2/3)

kxi=1

m3

i + 2n2

kxi=1

m2

i + 2n2
2

kxi=1

mi + (2/3)n3
2.

if the block sizes are small compared to n1 and n1     n2, the savings obtained by
block elimination is dramatic.

676

c numerical id202 background

the linear equations ax = b, where a11 is block diagonal, are called partially
separable for the following reason.
if the subvector x2 is    xed, the remaining
equations decouple into k sets of independent linear equations (which can be solved
separately). the subvector x2 is sometimes called the complicating variable since
the equations are much simpler when x2 is    xed. using block elimination, we
can solve partially separable linear equations far more e   ciently than by using a
standard method.

eliminating a sparse matrix

if a11 is sparse, we can eliminate a11 using a sparse factorization and sparse solve
steps, so the values of f and s in (c.6) are much less than for unstructured a11.
when a11 in (c.3) is sparse and the other blocks are dense, and n2     n1, we
say that a is a sparse matrix with a few dense rows and columns. eliminating
the sparse block a11 provides an e   cient method for solving equations which are
sparse except for a few dense rows and columns.

an alternative is to simply apply a sparse factorization algorithm to the entire
matrix a. most sparse solvers will handle dense rows and columns, and select a
permutation that results in sparse factors, and hence fast factorization and solve
times. this is more straightforward than using block elimination, but often slower,
especially in applications where we can exploit structure in the other blocks (see,
e.g., example c.4).

remark c.2 as already suggested in remark c.1, these two methods for solving sys-
tems with a few dense rows and columns are closely related. applying the elimination
method by factoring a11 and s as

a11 = p1l1u1p2,

s = p3l2u2,

can be interpreted as factoring a as

(cid:20) a11 a12
a21 a22 (cid:21) =
(cid:20) p1
p3 (cid:21)(cid:20)

0

0

l1
3 a21p t

p t

2 u    1

1

0

l2 (cid:21)(cid:20) u1 l   1

0

1 p t
u2

1 a12

(cid:21)(cid:20) p2

0

0

i (cid:21) ,

followed by forward and backward substitutions.

c.4.2 block elimination and structure

symmetry and positive de   niteness

there are variants of the block elimination method that can be used when a is
symmetric, or symmetric and positive de   nite. when a is symmetric, so are a11
and the schur complement s, so a symmetric factorization can be used for a11
and s. symmetry can also be exploited in the other operations, such as the matrix
multiplies. overall the savings over the nonsymmetric case is around a factor of
two.

c.4 block elimination and schur complements

677

positive de   niteness can also be exploited in block elimination. when a is sym-
metric and positive de   nite, so are a11 and the schur complement s, so cholesky
factorizations can be used.

exploiting structure in other blocks

our complexity analysis above assumes that we exploit no structure in the matrices
a12, a21, a22, and the schur complement s, i.e., they are treated as dense. but in
many cases there is structure in these blocks that can be exploited in forming the
schur complement, factoring it, and carrying out the solve steps. in such cases the
computational savings of the block elimination method over a standard method
can be even higher.

example c.2 block triangular equations. suppose that a12 = 0, i.e., the linear
equations ax = b have block lower triangular structure:

(cid:20) a11
a21 a22 (cid:21)(cid:20) x1

x2 (cid:21) =(cid:20) b1
b2 (cid:21) .

0

in this case the schur complement is just s = a22, and the block elimination method
reduces to block forward substitution:

x1

x2

:= a   1
:= a   1

11 b1
22 (b2     a21x1).

example c.3 block diagonal and banded systems. suppose that a11 is block diagonal,
with maximum block size l    l, and that a12, a21, and a22 are banded, say with
bandwidth k. in this case, a   1
11 is also block diagonal, with the same block sizes as
a11. therefore the product a   1
11 a12 is also banded, with bandwidth k + l, and the
schur complement, s = a22     a21a   1
11 a12 is banded with bandwidth 2k + l. this
means that forming the schur complement s can be done more e   ciently, and that
the factorization and solve steps with s can be done e   ciently. in particular, for
   xed maximum block size l and bandwidth k, we can solve ax = b with a number of
   ops that grows linearly with n.

example c.4 kkt structure. suppose that the matrix a has kkt structure, i.e.,

a =(cid:20) a11 a12
0 (cid:21) ,

at
12

where a11     sp
use a cholesky factorization. the schur complement s =    at
de   nite, so we can factor    s using a cholesky factorization.

++, and a12     rp  m with rank a12 = m. since a11     0, we can
11 a12 is negative

12a   1

678

c numerical id202 background

c.4.3 the matrix inversion lemma

the idea of block elimination is to remove variables, and then solve a smaller set of
equations that involve the schur complement of the original matrix with respect to
the eliminated variables. the same idea can be turned around: when we recognize
a matrix as a schur complement, we can introduce new variables, and create a
larger set of equations to solve. in most cases there is no advantage to doing this,
since we end up with a larger set of equations. but when the larger set of equations
has some special structure that can be exploited to solve it, introducing variables
can lead to an e   cient method. the most common case is when another block of
variables can be eliminated from the larger matrix.

we start with the linear equations

(c.7)
where a     rn  n is nonsingular, and b     rn  p, c     rp  n. we introduce a new
variable y = cx, and rewrite the equations as

(a + bc)x = b,

ax + by = b,

y = cx,

or, in matrix form,

(cid:20) a b
c    i (cid:21)(cid:20) x

y (cid:21) =(cid:20) b
0 (cid:21) .

(c.8)

note that our original coe   cient matrix, a + bc, is the schur complement of    i
in the larger matrix that appears in (c.8). if we were to eliminate the variable y
from (c.8), we would get back the original equation (c.7).

in some cases, it can be more e   cient to solve the larger set of equations (c.8)
than the original, smaller set of equations (c.7). this would be the case, for
example, if a, b, and c were relatively sparse, but the matrix a + bc were far
less sparse.

after introducing the new variable y, we can eliminate the original variable x
from the larger set of equations (c.8), using x = a   1(b     by). substituting this
into the second equation y = cx, we obtain

so that

(i + ca   1b)y = ca   1b,

y = (i + ca   1b)   1ca   1b.

using x = a   1(b     by), we get

since b is arbitrary, we conclude that

x =(cid:0)a   1     a   1b(i + ca   1b)   1ca   1(cid:1) b.
(a + bc)   1 = a   1     a   1b(cid:0)i + ca   1b(cid:1)   1

ca   1.

this is known as the matrix inversion lemma, or the sherman-woodbury-morrison
formula.

the matrix inversion lemma has many applications. for example if p is small
(or even just not very large), it gives us a method for solving (a + bc)x = b,
provided we have an e   cient method for solving au = v.

(c.9)

c.4 block elimination and schur complements

679

diagonal or sparse plus low rank

suppose that a is diagonal with nonzero diagonal elements, and we want to solve
an equation of the form (c.7). the straightforward solution would consist in    rst
forming the matrix d = a + bc, and then solving dx = b. if the product bc
is dense, then the complexity of this method is 2pn2    ops to form a + bc, plus
(2/3)n3    ops for the lu factorization of d, so the total cost is

2pn2 + (2/3)n3

   ops. the matrix inversion lemma suggests a more e   cient method. we can
calculate x by evaluating the expression (c.9) from right to left, as follows. we
   rst evaluate z = a   1b (n    ops, since a is diagonal). then we form the matrix
e = i + ca   1b (2p2n    ops). next we solve ew = cz, which is a set of p linear
equations in p variables. the cost is (2/3)p3    ops, plus 2pn to form cz. finally,
we evaluate x = z     a   1bw (2pn    ops for the matrix-vector product bw, plus
lower order terms). the total cost is

2p2n + (2/3)p3

   ops, dropping dominated terms. comparing with the    rst method, we see that
the second method is more e   cient when p < n. in particular if p is small and
   xed, the complexity grows linearly with n.

another important application of the matrix inversion lemma occurs when a is
sparse and nonsingular, and the matrices b and c are dense. again we can compare
two methods. the    rst method is to form the (dense) matrix a + bc, and to
solve (c.7) using a dense lu factorization. the cost of this method is 2pn2+(2/3)n3
   ops. the second method is based on evaluating the expression (c.9), using a
sparse lu factorization of a. speci   cally, suppose that f is the cost of factoring
a as a = p1lu p2, and s is the cost of solving the factored system p1lu p2x = d.
we can evaluate (c.9) from right to left as follows. we    rst factor a, and solve
p + 1 linear systems

az = b,

ad = b,

to    nd z     rn, and d     rn  p. the cost is f + (p + 1)s    ops. next, we form the
matrix e = i + cd, and solve

ew = cz,

which is a set of p linear equations in p variables w. the cost of this step is
2p2n + (2/3)p3 plus lower order terms. finally, we evaluate x = z     dw, at a cost
of 2pn    ops. this gives us a total cost of

f + ps + 2p2n + (2/3)p3

   ops. if f     (2/3)n3 and s     2n2, this is much lower than the complexity of the
   rst method.

remark c.3 the augmented system approach. a di   erent approach to exploiting
sparse plus low rank structure is to solve (c.8) directly using a sparse lu-solver. the
system (c.8) is a set of p + n linear equations in p + n variables, and is sometimes

680

c numerical id202 background

called the augmented system associated with (c.7). if a is very sparse and p is small,
then solving the augmented system using a sparse solver can be much faster than
solving the system (c.7) using a dense solver.

the augmented system approach is closely related to the method that we described
above. suppose

is a sparse lu factorization of a, and

a = p1lu p2

i + ca   1b = p3   l   u

is a dense lu factorization of i + ca   1b. then

(cid:20) a b
c    i (cid:21)
=(cid:20) p1

0

0

p3 (cid:21)(cid:20)

l
3 cp t

p t

2 u    1       l (cid:21)(cid:20) u l   1p t

  u

0

0

1 b

(c.10)

(cid:21)(cid:20) p2

0

0

i (cid:21) ,

and this factorization can be used to solve the augmented system. it can be veri   ed
that this is equivalent to the method based on the matrix inversion lemma that we
described above.

of course, if we solve the augmented system using a sparse lu solver, we have no
control over the permutations that are selected. the solver might choose a factor-
ization di   erent from (c.10), and more expensive to compute. in spite of this, the
augmented system approach remains an attractive option. it is easier to implement
than the method based on the matrix inversion lemma, and it is numerically more
stable.

low rank updates
suppose a     rn  n is nonsingular, u, v     rn with 1 + vt a   1u 6= 0, and we want
to solve two sets of linear equations

ax = b,

(a + uvt )  x = b.

the solution   x of the second system is called a rank-one update of x. the matrix
inversion lemma allows us to calculate the rank-one update   x very cheaply, once
we have computed x. we have

  x = (a + uvt )   1b
1

= (a   1    

1 + vt a   1u
vt x

a   1u.

= x    

1 + vt a   1u

a   1uvt a   1)b

we can therefore solve both systems by factoring a, computing x = a   1b and
w = a   1u, and then evaluating

  x = x    

vt x

1 + vt w

w.

the overall cost is f + 2s, as opposed to 2(f + s) if we were to solve for   x from
scratch.

c.5 solving underdetermined linear equations

681

c.5 solving underdetermined linear equations

to conclude this appendix, we mention a few important facts about underdeter-
mined linear equations

(c.11)
where a     rp  n with p < n. we assume that rank a = p, so there is at least one
solution for all b.
in many applications it is su   cient to    nd just one particular solution   x. in

ax = b,

other situations we might need a complete parametrization of all solutions as

{x | ax = b} = {f z +   x | z     rn   p}

(c.12)

where f is a matrix whose columns form a basis for the nullspace of a.

inverting a nonsingular submatrix of a
the solution of the underdetermined system is straightforward if a p  p nonsingular
submatrix of a is known. we start by assuming that the    rst p columns of a are
independent. then we can write the equation ax = b as

ax =(cid:2) a1 a2 (cid:3)(cid:20) x1

x2 (cid:21) = a1x1 + a2x2 = b,

where a1     rp  p is nonsingular. we can express x1 as
1 b     a   1

1 (b     a2x2) = a   1

x1 = a   1

1 a2x2.

this expression allows us to easily calculate a solution: we simply take   x2 = 0,
  x1 = a   1
1 b. the cost is equal to the cost of solving one square set of p linear
equations a1   x1 = b.

we can also parametrize all solutions of ax = b, using x2     rn   p as a free

parameter. the general solution of ax = b can be expressed as

x =(cid:20) x1

x2 (cid:21) =(cid:20)    a   1

1 a2
i

(cid:21) x2 +(cid:20) a   1

1 b
0

(cid:21) .

this gives a parametrization of the form (c.12) with

f =(cid:20)    a   1

1 a2
i

(cid:21) ,

  x =(cid:20) a   1

1 b
0

(cid:21) .

to summarize, assume that the cost of factoring a1 is f and the cost of solving one
system of the form a1x = d is s. then the cost of    nding one solution of (c.11)
is f + s. the cost of parametrizing all solutions (i.e., calculating f and   x) is
f + s(n     p + 1).
now we consider the general case, when the    rst p columns of a need not be
independent. since rank a = p, we can select a set of p columns of a that is
independent, permute them to the front, and then apply the method described

682

c numerical id202 background

above.
columns of   a = ap are independent, i.e.,

in other words, we    nd a permutation matrix p such that the    rst p

where a1 is invertible. the general solution of   a  x = b, where   x = p t x, is then
given by

  a = ap =(cid:2) a1 a2 (cid:3) ,
(cid:21)   x2 +(cid:20) a   1
(cid:21) .
(cid:21) z + p(cid:20) a   1

  x =(cid:20)    a   1
x = p   x = p(cid:20)    a   1

1 a2
i

1 a2
i

1 b
0

1 b
0

(cid:21) ,

the general solution of ax = b is then given by

where z     rn   p is a free parameter. this idea is useful when it is easy to identify
a nonsingular or easily inverted submatrix of a, for example, a diagonal matrix
with nonzero diagonal elements.

the qr factorization
if c     rn  p with p     n and rank c = p, then it can be factored as

c =(cid:2) q1 q2 (cid:3)(cid:20) r
0 (cid:21) ,

where q1     rn  p and q2     rn  (n   p) satisfy
2 q2 = i,

1 q1 = i,

qt

qt

qt

1 q2 = 0,

and r     rp  p is upper triangular with nonzero diagonal elements. this is called
the qr factorization of c. the qr factorization can be calculated in 2p2(n    p/3)
   ops. (the matrix q is stored in a factored form that makes it possible to e   ciently
compute matrix-vector products qx and qt x.)

the qr factorization can be used to solve the underdetermined set of linear

equations (c.11). suppose

at =(cid:2) q1 q2 (cid:3)(cid:20) r
0 (cid:21)

is the qr factorization of at . substituting in the equations it is clear that   x =
q1r   t b satis   es the equations:

a  x = rt qt

1 q1r   t b = b.

moreover, the columns of q2 form a basis for the nullspace of a, so the complete
solution set can be parametrized as

{x =   x + q2z | z     rn   p}.

the qr factorization method is the most common method for solving under-
determined equations. one drawback is that it is di   cult to exploit sparsity. the
factor q is usually dense, even when c is very sparse.

c.5 solving underdetermined linear equations

683

lu factorization of a rectangular matrix
if c     rn  p with p     n and rank c = p, then it can be factored as

c = p lu

where p     rn  n is a permutation matrix, l     rn  p is unit lower triangular (i.e.,
lij = 0 for i < j and lii = 1), and u     rp  p is nonsingular and upper triangular.
the cost is (2/3)p3 + p2(n     p)    ops if no structure in c is exploited.
if the matrix c is sparse, the lu factorization usually includes row and column
permutations, i.e., we factor c as

c = p1lu p2

where p1, p2     rp  p are permutation matrices. the lu factorization of a sparse
rectangular matrix can be calculated very e   ciently, at a cost that is much lower
than for dense matrices.

the lu factorization can be used to solve underdetermined sets of linear equa-
tions. suppose at = p lu is the lu factorization of the matrix at in (c.11), and
we partition l as

l =(cid:20) l1
l2 (cid:21) ,

where l1     rp  p and l2     r(n   p)  p. it is easily veri   ed that the solution set can
be parametrized as (c.12) with

  x = p(cid:20) l   t

0

1 u    t b

(cid:21) ,

f = p(cid:20)    l   t

1 lt
2
i

(cid:21) .

684

c numerical id202 background

bibliography

standard references for dense numerical id202 are golub and van loan [gl89],
demmel [dem97], trefethen and bau [tb97], and higham [hig96]. the sparse cholesky
factorization is covered in george and liu [gl81]. du   , erisman, and reid [der86] and
du    [duf93] discuss the sparse lu and ldlt factorizations. the books by gill, murray,
and wright [gmw81,   2.2], wright [wri97, chapter 11], and nocedal and wright [nw99,
  a.2] include introductions to numerical id202 that focus on problems arising in
numerical optimization.

high-quality implementations of common dense id202 algorithms are included
in the lapack package [abb+99]. lapack is built upon the basic id202
subprograms (blas), a library of routines for basic vector and matrix operations that can
be easily customized to take advantage of speci   c computer architectures. several codes
for solving sparse linear equations are also available, including spooles [apww99],
superlu [dgl03], umfpack [dav03], and wsmp [gup00], to mention only a few.

references

[abb+99] e. anderson, z. bai, c. bischof, s. blackford, j. demmel, j. dongarra, j. du
croz, a. greenbaum, s. hammarling, a. mckenney, and d. sorensen. la-
pack users    guide. society for industrial and applied mathematics, third
edition, 1999. available from www.netlib.org/lapack.

[ae61]

[ag03]

[aho98]

[ali91]

[and70]

k. j. arrow and a. c. enthoven. quasi-concave programming. econometrica,
29(4):779   800, 1961.

f. alizadeh and d. goldfarb. second-order cone programming. mathematical
programming series b, 95:3   51, 2003.

f. alizadeh, j.-p. a. haeberly, and m. l. overton. primal-dual interior-
point methods for semide   nite programming: convergence rates, stability
and numerical results. siam journal on optimization, 8(3):746   768, 1998.

f. alizadeh. combinatorial optimization with interior-point methods and
semi-de   nite matrices. phd thesis, university of minnesota, 1991.

t. w. anderson. estimation of covariance matrices which are linear com-
binations or whose inverses are linear combinations of given matrices.
in
r. c. bose et al., editor, essays in id203 and statistics, pages 1   24.
university of north carolina press, 1970.

[apww99] c. ashcraft, d. pierce, d. k. wah, and j. wu. the reference man-
ual
for spooles version 2.2: an object oriented software library
for solving sparse linear systems of equations, 1999. available from
www.netlib.org/linalg/spooles/spooles.2.2.html.

[ay98]

[bar02]

[bb65]

[bb91]

[bbi71]

e. d. andersen and y. ye. a computational study of the homogeneous
algorithm for large-scale id76. computational optimization
and applications, 10:243   269, 1998.

a. barvinok. a course in convexity, volume 54 of graduate studies in
mathematics. american mathematical society, 2002.

e. f. beckenbach and r. bellman. inequalities. springer, second edition,
1965.

s. boyd and c. barratt. linear controller design: limits of performance.
prentice-hall, 1991.

a. berman and a. ben-israel. more on linear inequalities with applications to
matrix theory. journal of mathematical analysis and applications, 33:482   
496, 1971.

[bd77]

p. j. bickel and k. a. doksum. mathematical statistics. holden-day, 1977.

[bdx04]

[be93]

s. boyd, p. diaconis, and l. xiao. fastest mixing markov chain on a graph.
siam review, 46(4):667   689, 2004.

s. boyd and l. el ghaoui. method of centers for minimizing generalized
eigenvalues. id202 and its applications, 188:63   111, 1993.

686

references

[befb94]

s. boyd, l. el ghaoui, e. feron, and v. balakrishnan. linear matrix in-
equalities in system and control theory. society for industrial and applied
mathematics, 1994.

[ber73]

[ber90]

[ber99]

[ber03]

[bf48]

[bf63]

a. berman. cones, matrices and mathematical programming. springer, 1973.

m. berger. convexity. the american mathematical monthly, 97(8):650   678,
1990.

d. p. bertsekas. nonid135. athena scienti   c, second edition,
1999.

d. p. bertsekas. convex analysis and optimization. athena scienti   c, 2003.
with a. nedi  c and a. e. ozdaglar.

t. bonnesen and w. fenchel. theorie der konvexen k  orper. chelsea pub-
lishing company, 1948. first published in 1934.

r. bellman and k. fan. on systems of linear inequalities in hermitian matrix
variables. in v. l. klee, editor, convexity, volume vii of proceedings of the
symposia in pure mathematics, pages 1   11. american mathematical society,
1963.

[bgt81]

[bi69]

r. g. bland, d. goldfarb, and m. j. todd. the ellipsoid method: a survey.
operations research, 29(6):1039   1091, 1981.

a. ben-israel. linear equations and inequalities on    nite dimensional, real or
complex vector spaces: a uni   ed theory. journal of mathematical analysis
and applications, 27:367   389, 1969.

[bj  o96]

a. bj  orck. numerical methods for least squares problems. society for in-
dustrial and applied mathematics, 1996.

[bkmr98] a. brooke, d. kendrick, a. meeraus, and r. raman. gams: a user   s guide.

the scienti   c press, 1998.

[bl00]

[bn78]

[bon94]

[bor02]

[bp94]

[bri61]

[bs00]

[bss93]

[bt97]

[btn98]

j. m. borwein and a. s. lewis. convex analysis and nonlinear optimization.
springer, 2000.

o. barndor   -nielsen.
theory. john wiley & sons, 1978.

information and exponential families in statistical

j. v. bondar. comments on and complements to inequalities: theory of ma-
jorization and its applications. id202 and its applications, 199:115   
129, 1994.

b. borchers.
www.id4.edu/~borchers/csdp.html.

csdp user   s guide,

2002.

available

from

a. berman and r. j. plemmons. nonnegative matrices in the mathemati-
cal sciences. society for industrial and applied mathematics, 1994. first
published in 1979 by academic press.

l. brickman. on the    eld of values of a matrix. proceedings of the american
mathematical society, 12:61   66, 1961.

d. bertsimas and j. sethuraman. moment problems and semide   nite opti-
mization. in h. wolkowicz, r. saigal, and l. vandenberghe, editors, hand-
book of semide   nite programming, chapter 16, pages 469   510. kluwer aca-
demic publishers, 2000.

m. s. bazaraa, h. d. sherali, and c. m. shetty. nonid135.
theory and algorithms. john wiley & sons, second edition, 1993.

d. bertsimas and j. n. tsitsiklis.
athena scienti   c, 1997.

introduction to linear optimization.

a. ben-tal and a. nemirovski. robust id76. mathematics
of operations research, 23(4):769   805, 1998.

references

687

[btn99]

[btn01]

[by02]

[byt99]

[cal64]

[cds01]

a. ben-tal and a. nemirovski. robust solutions of uncertain linear programs.
operations research letters, 25(1):1   13, 1999.

a. ben-tal and a. nemirovski. lectures on modern id76.
analysis, algorithms, and engineering applications. society for industrial
and applied mathematics, 2001.

s. j. benson and y. ye. dsdp     a software package implementing the
dual-scaling algorithm for semide   nite programming, 2002. available from
www-unix.mcs.anl.gov/~benson.

e. bai, y. ye, and r. tempo. bounded error parameter estimation: a se-
quential analytic center approach. ieee transactions on automatic control,
44(6):1107   1117, 1999.

e. calabi. linear systems of real quadratic forms. proceedings of the amer-
ican mathematical society, 15(5):844   846, 1964.

s. s. chen, d. l. donoho, and m. a. saunders. atomic decomposition by
basis pursuit. siam review, 43(1):129   159, 2001.

[cggs98] s. chandrasekaran, g. h. golub, m. gu, and a. h. sayed. parameter es-
timation in the presence of bounded data uncertainties. siam journal of
matrix analysis and applications, 19(1):235   252, 1998.

[ch53]

[ck77]

[ct91]

[dan63]

[dav63]

r. courant and d. hilbert. method of mathematical physics. volume 1.
interscience publishers, 1953. tranlated and revised from the 1937 german
original.

b. d. craven and j. j. koliha. generalizations of farkas    theorem. siam
journal on numerical analysis, 8(6), 1977.

t. m. cover and j. a. thomas. elements of id205. john wiley
& sons, 1991.

g. b. dantzig. id135 and extensions. princeton university
press, 1963.

c. davis. notions generalizing convexity for functions de   ned on spaces of
matrices.
in v. l. klee, editor, convexity, volume vii of proceedings of
the symposia in pure mathematics, pages 187   201. american mathematical
society, 1963.

[dav03]

t. a. davis.
umfpack user guide,
www.cise.ufl.edu/research/sparse/umfpack.

2003.

available

from

[ddb95] m. a. dahleh and i. j. diaz-bobillo. control of uncertain systems: a linear

programming approach. prentice-hall, 1995.

[deb59]

[dem97]

[der86]

[dgl03]

[dh93]

[dhs99]

[dik67]

g. debreu. theory of value: an axiomatic analysis of economic equilib-
rium. yale university press, 1959.

j. w. demmel. applied numerical id202. society for industrial and
applied mathematics, 1997.

i. s. du   , a. m. erismann, and j. k. reid. direct methods for sparse
matrices. clarendon press, 1986.

j. w. demmel, j. r. gilbert, and x. s. li. superlu users    guide, 2003.
available from crd.lbl.gov/~xiaoye/superlu.

d. den hertog. interior point approach to linear, quadratic and convex
programming. kluwer, 1993.

r. o. duda, p. e. hart, and d. g. stork. pattern classi   cation. john wiley
& sons, second edition, 1999.

i. dikin. iterative solution of problems of linear and quadratic programming.
soviet mathematics doklady, 8(3):674   675, 1967.

688

references

[dlw00]

[dp00]

[dpz67]

[ds96]

[duf93]

t. n. davidson, z.-q. luo, and k. m. wong. design of orthogonal pulse
shapes for communications via semide   nite programming. ieee transactions
on signal processing, 48(5):1433   1445, 2000.

g. e. dullerud and f. paganini. a course in robust control theory: a
convex approach. springer, 2000.

r. j. du   n, e. l. peterson, and c. zener. geometric programming. theory
and applications. john wiley & sons, 1967.

j. e. dennis and r. s. schnabel. numerical methods for unconstrained opti-
mization and nonlinear equations. society for industrial and applied math-
ematics, 1996. first published in 1983 by prentice-hall.

i. s. du   . the solution of augmented systems. in d. f. gri   ths and g. a.
watson, editors, numerical analysis 1993. proceedings of the 15th dundee
conference, pages 40   55. longman scienti   c & technical, 1993.

[eck80]

j. g. ecker. geometric programming: methods, computations and applica-
tions. siam review, 22(3):338   362, 1980.

[egg58]

h. g. eggleston. convexity. cambridge university press, 1958.

[el97]

[em75]

[en00]

[eol98]

[et99]

[far02]

[fd85]

l. el ghaoui and h. lebret. robust solutions to least-squares problems
with uncertain data. siam journal of matrix analysis and applications,
18(4):1035   1064, 1997.

j. elzinga and t. g. moore. a central cutting plane algorithm for the convex
programming problem. mathematical programming studies, 8:134   145, 1975.

l. el ghaoui and s. niculescu, editors. advances in linear matrix inequality
methods in control. society for industrial and applied mathematics, 2000.

l. el ghaoui, f. oustry, and h. lebret. robust solutions to uncertain
semide   nite programs. siam journal on optimization, 9(1):33   52, 1998.

i. ekeland and r. t  emam. convex analysis and variational inequalities.
classics in applied mathematics. society for industrial and applied mathe-
matics, 1999. originally published in 1976.

j. farkas. theorie der einfachen ungleichungen. journal f  ur die reine und
angewandte mathematik, 124:1   27, 1902.

j. p. fishburn and a. e. dunlop. tilos: a posynomial programming ap-
proach to transistor sizing. in ieee international conference on computer-
aided design: iccad-85. digest of technical papers, pages 326   328. ieee
computer society press, 1985.

[fen83]

w. fenchel. convexity through the ages. in p. m. gruber and j. m. wills,
editors, convexity and its applications, pages 120   130. birkh  auser verlag,
1983.

[fgk99]

r. fourer, d. m. gay, and b. w. kernighan. ampl: a modeling language
for mathematical programming. duxbury press, 1999.

[fgw02] a. forsgren, p. e. gill, and m. h. wright. interior methods for nonlinear

optimization. siam review, 44(4):525   597, 2002.

[fkn98]

k. fujisawa, m. kojima, and k. nakata. sdpa user   s manual, 1998. avail-
able from grid.r.dendai.ac.jp/sdpa.

[fl01]

[fm90]

m. florenzano and c. le van. finite dimensional convexity and optimiza-
tion. number 13 in studies in economic theory. springer, 2001.

a. v. fiacco and g. p. mccormick. nonid135. sequential
unconstrained minimization techniques. society for industrial and applied
mathematics, 1990. first published in 1968 by research analysis corpora-
tion.

references

689

[fre56]

[fw56]

[gau95]

[gi03a]

[gi03b]

[gkt51]

[gl81]

[gl89]

[gls88]

[gly96]

r. j. freund. the introduction of risk into a programming model. econo-
metrica, 24(3):253   263, 1956.

m. frank and p. wolfe. an algorithm for quadratic programming. naval
research logistics quarterly, 3:95   110, 1956.

c. f. gauss. theory of the combination of observations least subject to
errors. society for industrial and applied mathematics, 1995. translated
from original 1820 manuscript by g. w. stewart.

d. goldfarb and g. iyengar. robust convex quadratically constrained pro-
grams. mathematical programming series b, 97:495   515, 2003.

d. goldfarb and g. iyengar. robust portfolio selection problems. mathemat-
ics of operations research, 28(1):1   38, 2003.

d. gale, h. w. kuhn, and a. w. tucker. id135 and the
theory of games. in t. c. koopmans, editor, activity analysis of production
and allocation, volume 13 of cowles commission for research in economics
monographs, pages 317   335. john wiley & sons, 1951.

a. george and j. w.-h. liu. computer solution of large sparse positive
de   nite systems. prentice-hall, 1981.

g. golub and c. f. van loan. matrix computations. johns hopkins uni-
versity press, second edition, 1989.

m. gr  otschel, l. lovasz, and a. schrijver. geometric algorithms and com-
binatorial optimization. springer, 1988.

j.-l. go   n, z.-q. luo, and y. ye. complexity analysis of an interior cutting
plane method for convex feasibility problems. siam journal on optimization,
6:638   652, 1996.

[gms+86] p. e. gill, w. murray, m. a. saunders, j. a. tomlin, and m. h. wright. on
projected newton barrier methods for id135 and an equivalence
to karmarkar   s projective method. mathematical programming, 36:183   209,
1986.

[gmw81] p. e. gill, w. murray, and m. h. wright. practical optimization. academic

press, 1981.

[gon92]

[gow85]

[gup00]

[gw95]

[han98]

c. c. gonzaga. path-following methods for id135. siam re-
view, 34(2):167   224, 1992.

j. c. gower. properties of euclidean and non-euclidean distance matrices.
id202 and its applications, 67:81   97, 1985.

a. gupta. wsmp: watson sparse matrix package. part i     direct solution
of symmetric sparse systems. part ii     direct solution of general sparse
systems, 2000. available from www.cs.umn.edu/~agupta/wsmp.

m. x. goemans and d. p. williamson. improved approximation algorithms
for maximum cut and satis   ability problems using semide   nite programming.
journal of the association for computing machinery, 42(6):1115   1145, 1995.

p. c. hansen. rank-de   cient and discrete ill-posed problems. numerical
aspects of linear inversion. society for industrial and applied mathematics,
1998.

[hbl01] m. del mar hershenson, s. p. boyd, and t. h. lee. optimal design of a cmos
op-amp via geometric programming. ieee transactions on computer-aided
design of integrated circuits and systems, 20(1):1   21, 2001.

[hes68]

[hig96]

m. r. hestenes. pairs of quadratic forms. id202 and its applications,
1:397   407, 1968.

n. j. higham. accuracy and stability of numerical algorithms. society for
industrial and applied mathematics, 1996.

690

references

[hil57]

[hj85]

[hj91]

c. hildreth. a quadratic programming procedure. naval research logistics
quarterly, 4:79   85, 1957.

r. a. horn and c. a. johnson. matrix analysis. cambridge university press,
1985.

r. a. horn and c. a. johnson. topics in matrix analysis. cambridge
university press, 1991.

[hlp52]

g. h. hardy, j. e. littlewood, and g. p  olya. inequalities. cambridge uni-
versity press, second edition, 1952.

[hp94]

r. horst and p. pardalos. handbook of global optimization. kluwer, 1994.

[hrvw96] c. helmberg, f. rendl, r. vanderbei, and h. wolkowicz. an interior-
point method for semide   nite programming. siam journal on optimization,
6:342   361, 1996.

[htf01]

[hub64]

t. hastie, r. tibshirani, and j. friedman. the elements of statistical learn-
ing. data mining, id136, and prediction. springer, 2001.

p. j. huber. robust estimation of a location parameter. the annals of
mathematical statistics, 35(1):73   101, 1964.

[hub81]

p. j. huber. robust statistics. john wiley & sons, 1981.

[hul93]

[hul01]

[isi64]

[jar94]

[jen06]

[joh85]

[kan52]

[kan60]

[kar84]

[kel60]

[kle63]

[kle71]

[kn77]

j.-b. hiriart-urruty and c. lemar  echal. convex analysis and minimization
algorithms. springer, 1993. two volumes.

j.-b. hiriart-urruty and c. lemar  echal. fundamentals of convex analy-
sis. springer, 2001. abridged version of convex analysis and minimization
algorithms volumes 1 and 2.

k. isii. inequalities of the types of chebyshev and cram  er-rao and math-
ematical programming. annals of the institute of statistical mathematics,
16:277   293, 1964.

f. jarre. optimal ellipsoidal approximations around the analytic center.
applied mathematics and optimization, 30:15   19, 1994.

j. l. w. v. jensen. sur les fonctions convexes et les in  egalit  es entre les
valeurs moyennes. acta mathematica, 30:175   193, 1906.

f. john. extremum problems with inequalities as subsidiary conditions. in
j. moser, editor, fritz john, collected papers, pages 543   560. birkh  auser
verlag, 1985. first published in 1948.

l. v. kantorovich. functional analysis and applied mathematics. national
bureau of standards, 1952. translated from russian by c. d. benster. first
published in 1948.

l. v. kantorovich. mathematical methods of organizing and planning pro-
duction. management science, 6(4):366   422, 1960. translated from russian.
first published in 1939.

n. karmarkar. a new polynomial-time algorithm for id135.
combinatorica, 4(4):373   395, 1984.

j. e. kelley. the cutting-plane method for solving convex programs. journal
of the society for industrial and applied mathematics, 8(4):703   712, 1960.

v. l. klee, editor. convexity, volume 7 of proceedings of symposia in pure
mathematics. american mathematical society, 1963.

v. klee. what is a convex set? the american mathematical monthly,
78(6):616   631, 1971.

m. g. krein and a. a. nudelman. the markov moment problem and ex-
tremal problems. american mathematical society, 1977. translated from
russian. first published in 1973.

references

691

[koo51]

t. c. koopmans, editor. activity analysis of production and allocation,
volume 13 of cowles commission for research in economics monographs.
john wiley & sons, 1951.

[ks66]

s. karlin and w. j. studden. tchebyche    systems: with applications in
analysis and statistics. john wiley & sons, 1966.

[ksh97] m. kojima, s. shindoh, and s. hara. interior-point methods for the monotone
semide   nite linear complementarity problem in symmetric matrices. siam
journal on optimization, 7(1):86   125, 1997.

[ksh00]

[ksja91]

[kt51]

[kuh76]

[las95]

[las02]

[lay82]

[lh66]

[lh95]

[lms94]

[lo96]

[l  of04]

[l  ow34]

[lsz00]

[lue68]

[lue69]

[lue84]

t. kailath, a. h. sayed, and b. hassibi. linear estimation. prentice-hall,
2000.

j. m. kleinhaus, g. sigl, f. m. johannes, and k. j. antreich. gordian:
vlsi placement by quadratic programming and slicing optimization. ieee
transactions on computer-aided design of integrated circuits and systems,
10(3):356   200, 1991.

h. w. kuhn and a. w. tucker. nonid135. in j. neyman, ed-
itor, proceedings of the second berkeley symposium on mathematical statis-
tics and id203, pages 481   492. university of california press, 1951.

h. w. kuhn. nonid135. a historical view. in r. w. cottle
and c. e. lemke, editors, nonid135, volume 9 of siam-ams
proceedings, pages 1   26. american mathematical society, 1976.

j. b. lasserre. a new farkas lemma for positive semide   nite matrices. ieee
transactions on automatic control, 40(6):1131   1133, 1995.

j. b. lasserre. bounds on measures satisfying moment conditions. the
annals of applied id203, 12(3):1114   1137, 2002.

s. r. lay. convex sets and their applications. john wiley & sons, 1982.

b. li  e  u and p. huard. la m  ethode des centres dans un espace topologique.
numerische mathematik, 8:56   67, 1966.

c. l. lawson and r. j. hanson. solving least squares problems. society
for industrial and applied mathematics, 1995. first published in 1974 by
prentice-hall.

i. j. lustig, r. e. marsten, and d. f. shanno. interior point methods for
id135: computational state of the art. orsa journal on
computing, 6(1):1   14, 1994.

a. s. lewis and m. l. overton. eigenvalue optimization. acta numerica,
5:149   190, 1996.

j. l  ofberg. yalmip : a toolbox for modeling and optimization in mat-
lab.
in proceedings of the ieee international symposium on com-
puter aided control systems design, pages 284   289, 2004. available from
control.ee.ethz.ch/~joloef/yalmip.php.
k. l  owner.   uber monotone matrixfunktionen. mathematische zeitschrift,
38:177   216, 1934.

z.-q. luo, j. f. sturm, and s. zhang. conic convex programming and self-
dual embedding. optimization methods and software, 14:169   218, 2000.

d. g. luenberger. quasi-convex programming. siam journal on applied
mathematics, 16(5), 1968.

d. g. luenberger. optimization by vector space methods. john wiley &
sons, 1969.

d. g. luenberger. linear and nonid135. addison-wesley,
second edition, 1984.

692

references

[lue95]

[lue98]

[luo03]

d. g. luenberger. microeconomic theory. mcgraw-hill, 1995.

d. g. luenberger. investment science. oxford university press, 1998.

z.-q. luo. applications of id76 in signal processing and
digital communication. mathematical programming series b, 97:177   207,
2003.

[lvbl98] m. s. lobo, l. vandenberghe, s. boyd, and h. lebret. applications of second-
order cone programming. id202 and its applications, 284:193   228,
1998.

[man65]

[man94]

[mar52]

[mar56]

o. mangasarian. linear and nonlinear separation of patterns by linear pro-
gramming. operations research, 13(3):444   452, 1965.

o. mangasarian. nonid135. society for industrial and applied
mathematics, 1994. first published in 1969 by mcgraw-hill.

h. markowitz. portfolio selection. the journal of finance, 7(1):77   91, 1952.

h. markowitz. the optimization of a quadratic function subject to linear
constraints. naval research logistics quarterly, 3:111   133, 1956.

[mdw+02] w.-k. ma, t. n. davidson, k. m. wong, z.-q. luo, and p.-c. ching. quasi-
maximum-likelihood multiuser detection using semi-de   nite relaxation with
application to synchronous cdma. ieee transactions on signal processing,
50:912   922, 2002.

[meh92]

[mey00]

[ml57]

[mo60]

[mo79]

[mon97]

s. mehrotra. on the implementation of a primal-dual interior point method.
siam journal on optimization, 2(4):575   601, 1992.

c. d. meyer. matrix analysis and applied id202. society for in-
dustrial and applied mathematics, 2000.

m. marcus and l. lopes. inequalities for symmetric functions and hermitian
matrices. canadian journal of mathematics, 9:305   312, 1957.

a. w. marshall and i. olkin. multivariate chebyshev inequalities. annals
of mathematical statistics, 32(4):1001   1014, 1960.

a. w. marshall and i. olkin. inequalities: theory of majorization and its
applications. academic press, 1979.

r. d. c. monteiro. primal-dual path-following algorithms for semide   nite
programming. siam journal on optimization, 7(3):663   678, 1997.

[mos02] mosek aps. the mosek optimization tools. user   s manual and refer-

ence, 2002. available from www.mosek.com.

[mot33]

[mp68]

[mr95]

[mz89]

[nes98]

[nes00]

t. motzkin. beitr  age zur theorie der linearen ungleichungen. phd thesis,
university of basel, 1933.

r. f. meyer and j. w. pratt. the consistent assessment and fairing of pref-
erence functions. ieee transactions on systems science and cybernetics,
4(3):270   278, 1968.

r. motwani and p. raghavan. randomized algorithms. cambridge university
press, 1995.

m. morari and e. za   riou. robust process control. prentice-hall, 1989.

y. nesterov. semide   nite relaxations and nonconvex quadratic optimization.
optimization methods and software, 9(1-3):141   160, 1998.

y. nesterov. squared functional systems and optimization problems.
in
j. frenk, c. roos, t. terlaky, and s. zhang, editors, high performance
optimization techniques, pages 405   440. kluwer, 2000.

[nik54]

h. nikaid  o. on von neumann   s minimax theorem. paci   c journal of math-
ematics, 1954.

references

693

[nn94]

[nt98]

y. nesterov and a. nemirovskii. interior-point polynomial methods in con-
vex programming. society for industrial and applied mathematics, 1994.

y. e. nesterov and m. j. todd. primal-dual interior-point methods for self-
scaled cones. siam journal on optimization, 8(2):324   364, 1998.

[nw99]

j. nocedal and s. j. wright. numerical optimization. springer, 1999.

[nwy00] y. nesterov, h. wolkowicz, and y. ye. semide   nite programming relaxations
of nonconvex quadratic optimization. in h. wolkowicz, r. saigal, and l. van-
denberghe, editors, handbook of semide   nite programming, chapter 13, pages
361   419. kluwer academic publishers, 2000.

[ny83]

[or00]

[par71]

[par98]

[par00]

[par03]

[pet76]

[pin95]

[pol87]

[pon67]

[pr  e71]

[pr  e73]

[pr  e80]

a. nemirovskii and d. yudin. problem complexity and method e   ciency in
optimization. john wiley & sons, 1983.

j. m. ortega and w. c. rheinboldt. iterative solution of nonlinear equations
in several variables. society for industrial and applied mathematics, 2000.
first published in 1970 by academic press.

v. pareto. manual of political economy. a. m. kelley publishers, 1971.
translated from the french edition. first published in italian in 1906.

b. n. parlett. the symmetric eigenvalue problem. society for industrial and
applied mathematics, 1998. first published in 1980 by prentice-hall.

p. a. parrilo. structured semide   nite programs and semialgebraic geometry
methods in robustness and optimization. phd thesis, california institute of
technology, 2000.

p. a. parrilo. semide   nite programming relaxations for semialgebraic prob-
lems. mathematical programming series b, 96:293   320, 2003.

e. l. peterson. geometric programming. siam review, 18(1):1   51, 1976.

j. pinter. global optimization in action, volume 6 of nonconvex optimiza-
tion and its applications. kluwer, 1995.

b. t. polyak. introduction to optimization. optimization software, 1987.
translated from russian.

j. ponstein. seven kinds of convexity. siam review, 9(1):115   119, 1967.

a. pr  ekopa. logarithmic concave measures with application to stochastic
programming. acta scientiarum mathematicarum, 32:301   315, 1971.

a. pr  ekopa. on logarithmic concave measures and functions. acta scien-
tiarum mathematicarum, 34:335   343, 1973.

a. pr  ekopa. logarithmic concave measures and related topics. in m. a. h.
dempster, editor, stochastic programming, pages 63   82. academic press,
1980.

[pro01]

j. g. proakis. digital communications. mcgraw-hill, fourth edition, 2001.

[prt02]

[ps98]

[psu88]

[puk93]

[ren01]

j. peng, c. roos, and t. terlaky. self-regularity. a new paradigm for
primal-dual interior-point algorithms. princeton university press, 2002.

c. h. papadimitriou and k. steiglitz. combinatorial optimization. algo-
rithms and complexity. dover publications, 1998. first published in 1982 by
prentice-hall.

a. l. peressini, f. e. sullivan, and j. j. uhl. the mathematics of nonlinear
programming. undergraduate texts in mathematics. springer, 1988.

f. pukelsheim. optimal design of experiments. wiley & sons, 1993.

j. renegar. a mathematical view of interior-point methods in convex op-
timization. society for industrial and applied mathematics, 2001.

[roc70]

r. t. rockafellar. convex analysis. princeton university press, 1970.

694

references

[roc89]

[roc93]

[rof92]

[ros65]

[ros99]

[rtv97]

r. t. rockafellar. conjugate duality and optimization. society for industrial
and applied mathematics, 1989. first published in 1974.
r. t. rockafellar. lagrange multipliers and optimality. siam review,
35:183   283, 1993.
l. rudin, s. j. osher, and e. fatemi. nonlinear total variation based noise
removal algorithms. physica d, 60:259   268, 1992.
j. b. rosen. pattern separation by convex programming. journal of mathe-
matical analysis and applications, 10:123   134, 1965.
s. m. ross. an introduction to mathematical finance: options and other
topics. cambridge university press, 1999.
c. roos, t. terlaky, and j.-ph. vial. theory and algorithms for linear
optimization. an interior point approach. john wiley & sons, 1997.

[rud76] w. rudin. principles of mathematical analysis. mcgraw-hill, 1976.
[rv73]
[rw97]

a. w. roberts and d. e. varberg. convex functions. academic press, 1973.
d. ralph and s. j. wright. superlinear convergence of an interior-point
method for monotone variational inequalities. in m. c. ferris and j.-s. pang,
editors, complementarity and variational problems: state of the art, pages
345   385. society for industrial and applied mathematics, 1997.
c. v. rao, s. j. wright, and j. b. rawlings. application of interior-point
methods to model predictive control. journal of optimization theory and
applications, 99(3):723   757, 1998.
i. j. schoenberg. remarks to maurice fr  echet   s article    sur la d  e   nition
axiomatique d   une classe d   espaces distanci  es vectoriellement applicable sur
l   espace de hilbert   . annals of mathematics, 38(3):724   732, 1935.
s. schaible. bibliography in fractional programming. zeitschrift f  ur opera-
tions research, 26:211   241, 1982.
s. schaible. fractional programming. zeitschrift f  ur operations research,
27:39   54, 1983.
a. schrijver. theory of linear and integer programming. john wiley &
sons, 1986.
l. l. scharf. statistical signal processing. detection, estimation, and time
series analysis. addison wesley, 1991. with c  edric demeure.
g. sigl, k. doll, and f. m. johannes. analytical placement: a linear or
quadratic objective function? in proceedings of the 28th acm/ieee design
automation conference, pages 427   432, 1991.
c. scherer, p. gahinet, and m. chilali. multiobjective output-feedback
control via lmi optimization.
ieee transactions on automatic control,
42(7):896   906, 1997.
n. sherwani. algorithms for vlsi design automation. kluwer academic
publishers, third edition, 1999.
n. z. shor. minimization methods for non-di   erentiable functions. springer
series in computational mathematics. springer, 1985.
n. z. shor. the development of numerical methods for nonsmooth optimiza-
tion in the ussr. in j. k. lenstra, a. h. g. rinnooy kan, and a. schri-
jver, editors, history of mathematical programming. a collection of personal
reminiscences, pages 135   139. centrum voor wiskunde en informatica and
north-holland, amsterdam, 1991.
g. sonnevend. an    analytical centre    for polyhedrons and new classes of
global algorithms for linear (smooth, convex) programming. in lecture notes
in control and information sciences, volume 84, pages 866   878. springer,
1986.

[rwr98]

[sch35]

[sch82]

[sch83]

[sch86]

[sch91]

[sdj91]

[sgc97]

[she99]

[sho85]

[sho91]

[son86]

695

references

[spv99]

a. sei   , k. ponnambalam, and j. vlach. a uni   ed approach to statisti-
cal design centering of integrated circuits with correlated parameters. ieee
transactions on circuits and systems     i. fundamental theory and appli-
cations, 46(1):190   196, 1999.

[srvk93]

s. s. sapatnekar, v. b. rao, p. m. vaidya, and s.-m. kang. an exact
solution to the transistor sizing problem for cmos circuits using convex
optimization. ieee transactions on computer-aided design of integrated
circuits and systems, 12(11):1621   1634, 1993.

[ss01]

[str80]

[stu99]

[sw70]

[sw95]

[ta77]

[tb97]

[ter96]

[tib96]

[tik90]

[tit75]

[tke88]

[tod01]

[tod02]

b. sch  olkopf and a. smola. learning with kernels: support vector machines,
id173, optimization, and beyond. mit press, 2001.

g. strang. id202 and its applications. academic press, 1980.

j. f. sturm. using sedumi 1.02, a matlab toolbox for optimization over
symmetric cones. optimization methods and software, 11-12:625   653, 1999.
available from sedumi.mcmaster.ca.

j. stoer and c. witzgall. convexity and optimization in finite dimensions i.
springer-verlag, 1970.

r. j. stern and h. wolkowicz. inde   nite trust region subproblems and non-
symmetric eigenvalue perturbations. siam journal on optimization, 15:286   
313, 1995.

a. n. tikhonov and v. y. arsenin.
v. h. winston & sons, 1977. translated from russian.

solutions of ill-posed problems.

l. n. trefethen and d. bau, iii. numerical id202. society for
industrial and applied mathematics, 1997.

t. terlaky, editor. interior point methods of mathematical programming,
volume 5 of applied optimization. kluwer academic publishers, 1996.

r. tibshirani. regression shrinkage and selection via the lasso. journal of
the royal statistical society, series b, 58(1):267   288, 1996.

v. m. tikhomorov. convex analysis. in r. v. gamkrelidze, editor, analy-
sis ii: convex analysis and approximation theory, volume 14, pages 1   92.
springer, 1990.

d. m. titterington. optimal design: some geometrical aspects of d-
optimality. biometrika, 62(2):313   320, 1975.
s. tarasov, l. khachiyan, and i. `erlikh. the method of inscribed ellipsoids.
soviet mathematics doklady, 37(1):226   230, 1988.

m. j. todd. semide   nite optimization. acta numerica, 10:515   560, 2001.

m. j. todd. the many facets of id135. mathematical program-
ming series b, 91:417   436, 2002.

[ttt98] m. j. todd, k. c. toh, and r. h. t  ut  unc  u. on the nesterov-todd direction
in semide   nite programming. siam journal on optimization, 8(3):769   796,
1998.

[ttt02]

[tuy98]

[uhl79]

k. c. toh, r. h. t  ut  unc  u, and m. j. todd. sdpt3. a matlab soft-
ware for semide   nite-quadratic-id135, 2002. available from
www.math.nus.edu.sg/~mattohkc/sdpt3.html.

h. tuy. convex analysis and global optimization, volume 22 of nonconvex
optimization and its applications. kluwer, 1998.

f. uhlig. a recurring theorem about pairs of quadratic forms and extensions.
a survey. id202 and its applications, 25:219   237, 1979.

[val64]

f. a. valentine. convex sets. mcgraw-hill, 1964.

696

references

[van84]

[van96]

[van97]

[vap00]

[vav91]

[vb95]

[vn63]

[vn46]

[vnm53]

[vt84]

[web71]

g. n. vanderplaats. numerical optimization techniques for engineering
design. mcgraw-hill, 1984.

r. j. vanderbei. id135: foundations and extensions. kluwer,
1996.

r. j. vanderbei.
www.orfe.princeton.edu/~rvdb.

loqo user   s manual, 1997.

available from

v. n. vapnik. the nature of statistical learning theory. springer, second
edition, 2000.

s. a. vavasis. nonlinear optimization: complexity issues. oxford university
press, 1991.

l. vandenberghe and s. boyd. semide   nite programming. siam review,
pages 49   95, 1995.

j. von neumann. discussion of a maximum problem. in a. h. taub, editor,
john von neumann. collected works, volume vi, pages 89   95. pergamon
press, 1963. unpublished working paper from 1947.

j. von neumann. a model of general economic equilibrium. review of eco-
nomic studies, 13(1):1   9, 1945-46.

j. von neumann and o. morgenstern. theory of games and economic be-
havior. princeton university press, third edition, 1953. first published in
1944.

j. van tiel. convex analysis. an introductory text. john wiley & sons,
1984.

a. weber. theory of the location of industries. russell & russell, 1971.
translated from german by c. j. friedrich. first published in 1929.

[web94]

r. webster. convexity. oxford university press, 1994.

[whi71]

p. whittle. optimization under constraints. john wiley & sons, 1971.

[wol81]

[wri97]

[wsv00]

[xhy96]

[ye97]

[ye99]

h. wolkowicz. some applications of optimization in matrix theory. linear
algebra and its applications, 40:101   118, 1981.

s. j. wright. primal-dual interior-point methods. society for industrial and
applied mathematics, 1997.

h. wolkowicz, r. saigal, and l. vandenberghe, editors. handbook of semidef-
inite programming. kluwer academic publishers, 2000.

x. xu, p. hung, and y. ye. a simpli   ed homogeneous and self-dual lin-
ear programming algorithm and its implementation. annals of operations
research, 62:151   172, 1996.

y. ye. interior point algorithms. theory and analysis. john wiley & sons,
1997.

y. ye. approximating quadratic programming with bound and quadratic
constraints. mathematical programming, 84:219   226, 1999.

[ytm94] y. ye, m. j. todd, and s. mizuno. an o(   nl)-iteration homogeneous and
self-dual id135 algorithm. mathematics of operations research,
19:53   67, 1994.

[zen71]

[zha98]

c. zener. engineering design by geometric programming. john wiley &
sons, 1971.

y. zhang. on extending some primal-dual interior-point algorithms from
id135 to semide   nite programming. siam journal on opti-
mization, 8(2):365   386, 1998.

notation

some speci   c sets

r
rn
rm  n
r+, r++
c
cn
cm  n
z
z+
sn
+, sn
sn

++

real numbers.
real n-vectors (n    1 matrices).
real m    n matrices.
nonnegative, positive real numbers.
complex numbers.
complex n-vectors.
complex m    n matrices.
integers.
nonnegative integers.
symmetric n    n matrices.
symmetric positive semide   nite, positive de   nite, n    n
matrices.

vectors and matrices

vector with all components one.
ith standard basis vector.
identity matrix.
transpose of matrix x.
hermitian (complex conjugate) transpose of matrix x.
trace of matrix x.
ith largest eigenvalue of symmetric matrix x.

1
ei
i
x t
x h
tr x
  i(x)
  max(x),   min(x) maximum, minimum eigenvalue of symmetric matrix x.
  i(x)
  max(x),   min(x) maximum, minimum singular value of matrix x.
x    
x     y
v    
diag(x)
diag(x, y, . . .)
rank a
r(a)
n (a)

moore-penrose or pseudo-inverse of matrix x.
vectors x and y are orthogonal: xt y = 0.
orthogonal complement of subspace v .
diagonal matrix with diagonal entries x1, . . . , xn.
block diagonal matrix with diagonal blocks x, y, . . ..
rank of matrix a.
range of matrix a.
nullspace of matrix a.

ith largest singular value of matrix x.

698

notation

norms and distances

k    k
k    k   
kxk2
kxk1
kxk   
kxk2
b(c, r)
dist(a, b)

a norm.
dual of norm k    k.
euclidean (or    2-) norm of vector x.
   1-norm of vector x.
      -norm of vector x.
spectral norm (maximum singular value) of matrix x.
ball with center c and radius r.
distance between sets (or points) a and b.

generalized inequalities

x (cid:22) y
x     y
x (cid:22) y
x     y
x (cid:22)k y
x    k y
x (cid:22)k     y
x    k     y

componentwise inequality between vectors x and y.
strict componentwise inequality between vectors x and y
matrix inequality between symmetric matrices x and y .
strict matrix inequality between symmetric matrices x
and y .
generalized inequality induced by proper cone k.
strict generalized inequality induced by proper cone k.
dual generalized inequality.
dual strict generalized inequality.

topology and convex analysis

card c
int c
relint c
cl c
bd c
conv c
a    c
k    
ic
sc
f    

id203

e x
prob s
var x
n (c,   )
  

cardinality of set c.
interior of set c.
relative interior of set c.
closure of set c.
boundary of set c: bd c = cl c \ int c.
convex hull of set c.
a   ne hull of set c.
dual cone associated with k.
indicator function of set c.
support function of set c.
conjugate function of f .

expected value of random vector x.
id203 of event s.
variance of scalar random variable x.
gaussian distribution with mean c, covariance (matrix)   .
cumulative distribution function of n (0, 1) random vari-
able.

notation

699

functions and derivatives
f : a     b
dom f
epi f
   f
   2f
df

f is a function on the set dom f     a into the set b.
domain of function f .
epigraph of function f .
gradient of function f .
hessian of function f .
derivative (jacobian) matrix of function f .

index

a-optimal experiment design, 387
abstract form convex problem, 137
active constraint, 128
activity planning, 149, 195
a    (a   ne hull), 23
a   ne

combination, 22
dimension, 23
function, 36

composition, 79, 95, 508, 642, 645

hull, 23
independence, 32
invariance, 486

analytic center, 449
newton decrement, 487
newton step, 527
newton   s method, 494, 496
self-concordance, 498

set, 21

separation from convex set, 49

algorithm, see method
alignment constraint, 442
allocation

asset, 155, 186, 209
power, 196, 210, 212, 245
resource, 253, 523, 559

alternatives, 258, 285

generalized inequalities, 54, 269
linear discrimination, 423
linear inequalities, 50, 63
linear matrix inequality, 287
nonconvex quadratic, 655
strong, 260
weak, 258

amplitude distribution, 294, 304
analytic center, 141, 419, 449, 458, 519, 535,

541, 546, 547

a   ne invariance, 449
dual, 276, 525
e   cient line search, 518
ellipsoid, 420, 450
linear matrix inequality, 422, 459, 508,

553

method, 626
ml interpretation, 450
quadratic inequalities, 519

angle, 633

approximation, 448
constraint, 406
problem, 405, 408

anisotropy, 461
approximate id77, 519
approximation

chebyshev, 6, 293
complex, 197
   tting angles, 448
   1-norm, 193, 294
least-squares, 293
log-chebyshev, 344
matrix norm, 194
minimax, 293
monomial, 199
penalty function, 294, 353
regularized, 305
residual, 291
robust, 318
sparse, 333
total variation, 312
variable bounds, 301
width, 121
with constraints, 301

arbitrage-free price, 263
arithmetic mean, 75
arithmetic-geometric mean inequality, 78
arrow matrix, 670
asymptotic cone, 66

backtracking line search, 464
backward substitution, 666
ball, 30, 634

euclidean, 29

banded matrix, 510, 546, 553, 669, 675
bandwidth allocation, 210
barrier

cone, 66
function, 563
method, 568

complexity, 585, 595
convergence analysis, 577
convex-concave game, 627
generalized inequalities, 596, 601, 605
infeasible start, 571

702

index

linear program, 616
second-order cone program, 601
semide   nite program, 602, 618

second-order cone programming, 599
semide   nite programming, 600
tangent, 624

basis, 405

dictionary, 333
dual, 407
functions, 326
lagrange, 326
over-complete, 333
pursuit, 310, 333, 580
well conditioned, 407

bayesian

classi   cation, 428
detector, 367
estimation, 357

bd (boundary), 50, 638
best linear unbiased estimator, 176
binary hypothesis testing, 370
bisection method, 249, 430

quasiid76, 146

blas, 684
block

elimination, 546, 554, 672
lu factorization, 673
matrix inverse, 650
separable, 552
tridiagonal, 553

boolean linear program

lagrangian relaxation, 276
lp relaxation, 194

boundary, 638
bounding box, 433
bounds

chebyshev, 150, 374
cherno   , 379
convex function values, 338
correlation coe   cients, 408
expected values, 361
for global optimization, 11
probabilities, 361

box constraints, 129

certi   cate

infeasibility, 259, 582
suboptimality, 241, 568

chain rule, 642

second derivative, 645

change of variable, 130
chebyshev

approximation, 6, 293

lower bounds via least-squares, 274
robust, 323

bounds, 150, 374
center, 148, 416
inequalities, 150, 154
norm, 635

cherno    bounds, 379
cholesky factorization, 118, 406, 509, 546,

617, 669

banded matrix, 670
sparse matrix, 670

circuit design, 2, 17, 432, 446
cl (closure), 638
classi   cation, 422

bayesian, 428
linear, 423
logistic, 427
nonlinear, 429
polynomial, 430
quadratic, 429
support vector, 425

closed

function, 458, 529, 577, 639
set, 637
sublevel set assumption, 457, 529

closure, 638
combination

a   ne, 22
conic, 25
convex, 24

cantilever beam, 163, 199
capacity of communication channel, 207
card (cardinality), 98

   1-norm heuristic, 310

cauchy-schwartz inequality, 633
ceiling, 96
center

analytic, 419
chebyshev, 148, 416
maximum volume ellipsoid, 418

central path, 564
duality, 565
generalized inequalities, 598
kkt conditions, 567
predictor-corrector, 625

communication channel

capacity, 207

dual, 279

power allocation, 245

complementary slackness, 242

generalized inequalities, 267

complex

norm approximation, 197
semide   nite program, 202

complexity

barrier method, 585

generalized inequalities, 605

linear equations, 662
second-order cone program, 606
semide   nite program, 608

index

703

componentwise inequality, 32, 43
composition, 83

a   ne function, 508, 642, 645
quasiconvexity, 102
self-concordance, 499

concave

function, 67
maximization problem, 137

cond (condition number), 649
condition number, 203, 407, 649

ellipsoid, 461
gradient method, 473
newton   s method, 495
set, 461

id155, 42, 357, 428
cone, 25

barrier, 66
dual, 51
euclidean, 449
hyperbolic, 39
in r2, 64
lexicographic, 64
lorentz, 31
moments, 66
monotone nonnegative, 64
normal, 66
pointed, 43
positive semide   nite, 34, 64
program, 168

dual, 266

proper, 43
recession, 66
second-order, 31, 449
separation, 66
solid, 43

conic

combination, 25
form problem, 168, 201
hull, 25

conjugate

and lagrange dual, 221
function, 90

self-concordance, 517

logarithm, 607

constraint

active, 128
box, 129
explicit, 134
hyperbolic, 197
implicit, 134
kinematic, 247
quali   cations, 226
redundant, 128
set, 127

consumer preference, 339
continuous function, 639

control

model predictive, 17
optimal, 194, 303, 552

conv (convex hull), 24
convergence

infeasible id77, 536
linear, 467
id77, 529
quadratic, 489, 539

convex

combination, 24
cone, 25
equality constraints, 191
function, 67

bounded, 114
bounding values, 338
   rst-order conditions, 69
interpolation, 337
inverse, 114
level set, 113
over concave function, 103
product, 119

geometric program, 162
hull, 24

function, 119
minimizing over, 207
optimization, 2, 7, 136

abstract form, 137

set, 23

image under linear-fractional func-

tion, 62

separating hyperplane, 403
separation from a   ne set, 49

convex-concave, 238

fractional problem, 191
function, 115

saddle-point property, 281

game, 540, 542, 560

barrier method, 627
bounded inverse derivative condition,

559

id77, 540
newton step, 559

convexity

   rst-order condition, 69
matrix, 110
midpoint, 60
second-order conditions, 71
strong, 459, 558

coordinate projection, 38
copositive matrix, 65, 202
correlation coe   cient, 406

bounding, 408

cost, 127

random, 154
risk-sensitive, 155

704

index

covariance

estimation, 355
estimation error, 384
incomplete information, 171

covering ellipsoid, 275
cumulant generating function, 106
cumulative distribution, 107

log-concavity, 124

curve

minimum length piecewise-linear, 547
optimal trade-o   , 182
piecewise-arc, 453

d-optimal experiment design, 387
damped newton step, 489
data    tting, 2, 291
de-noising, 310
deadzone-linear penalty function, 295, 434

dual, 345
decomposition

eigenvalue, 646
generalized eigenvalue, 647
orthogonal, 646
singular value, 648

deconvolution, 307
degenerate ellipsoid, 30
density function

log-concave, 104, 124, 352

depth, 416
derivative, 640

chain rule, 642
directional, 642
pricing, 264
second, 643

descent

direction, 463
feasible, 527

method, 463

gradient, 466
steepest, 475

design

circuit, 2, 17, 432, 446
detector, 364
of experiments, 384
optimal, 292, 303

detector

bayes, 367
design, 364
map, 369
minimax, 367
ml, 369
randomized, 365
robust, 372
determinant, 73

derivative, 641

device sizing, 2
diagonal plus low rank, 511, 678

diagonal scaling, 163
dictionary, 333
diet problem, 148
di   erentiable function, 640
directional derivative, 642
dirichlet density, 124
discrete memoryless channel, 207
discrimination, 422
dist (distance), 46, 634
distance, 46, 634

between polyhedra, 154, 403
between sets, 402
constraint, 443
maximum id203, 118
ratio function, 97
to farthest point in set, 81
to set, 88, 397

distribution

amplitude, 294
gaussian, 104
laplacian, 352
maximum id178, 362
poisson, 353
wishart, 105

dom (domain), 639
domain

function, 639
problem, 127

dual

basis, 407
cone, 51

logarithm, 607
properties, 64

feasibility equations, 521
feasible, 216
function, 216

geometric interpretation, 232

generalized inequality, 53

characterization of minimal points,

54

least-squares, 218
logarithm, 607
id77, 557
norm, 93, 637
problem, 223
residual, 532
spectral norm, 637
stopping criterion, 242
variable, 215

duality, 215

central path, 565
game interpretation, 239
gap, 241

optimal, 226
surrogate, 612

multicriterion interpretation, 236

index

705

price interpretation, 240
saddle-point interpretation, 237
strong, 226
weak, 225

dynamic activity planning, 149

e-optimal experiment design, 387
eccentricity, 461
ei (ith unit vector), 33
eigenvalue

decomposition, 646
generalized, 647
interlacing theorem, 122
maximum, 82, 203
optimization, 203
spread, 203
sum of k largest, 118
electronic device sizing, 2
elementary symmetric functions, 122
elimination

banded matrix, 675
block, 546
constraints, 132
equality constraints, 523, 542
variables, 672

ellipsoid, 29, 39, 635

condition number, 461
covering, 275
degenerate, 30
intersection, 262
l  owner-john, 410
maximum volume, 414
minimum volume, 410
separation, 197
via analytic center, 420
volume, 407

embedded optimization, 3
id178, 72, 90, 117

maximization, 537, 558, 560, 562

dual function, 222
self-concordance, 497

epigraph, 75

problem, 134

equality

constrained minimization, 521
constraint, 127
convex, 191
elimination, 132, 523, 542

equations

kkt, 243
normal, 458

equivalent

norms, 636
problems, 130

estimation, 292

bayesian, 357
covariance, 355

least-squares, 177
linear measurements, 352
maximum a posteriori, 357
noise free, 303
nonparametric distribution, 359
statistical, 351

euclidean

ball, 29
distance

matrix, 65
problems, 405

norm, 633
projection via pseudo-inverse, 649

exact line search, 464
exchange rate, 184
expanded set, 61
experiment design, 384

a-optimal, 387
d-optimal, 387
dual, 276
e-optimal, 387

explanatory variables, 353
explicit constraint, 134
exponential, 71

distribution, 105
matrix, 110

extended-value extension, 68
extrapolation, 333
extremal volume ellipsoids, 410

facility location problem, 432
factor-solve method, 666
factorization

block lu, 673
cholesky, 118, 546, 669
ldlt, 671
lu, 668
qr, 682
symbolic, 511
farkas lemma, 263
fastest mixing markov chain, 173

dual, 286

feasibility

methods, 579
problem, 128

feasible, 127

descent direction, 527
dual, 216
point, 127
problem, 127
set, 127

fenchel   s inequality, 94
   rst-order

approximation, 640
condition

convexity, 69
monotonicity, 109

706

index

quasiconvexity, 99, 121

   tting

minimum norm, 331
polynomial, 331
spline, 331

   oor planning, 438

geometric program, 444

   op count, 662
   ow

optimal, 193, 550, 619

forward substitution, 665
fractional program

generalized, 205
frobenius norm, 634
scaling, 163, 478
fuel use map, 194, 213
function

a   ne, 36
barrier, 563
closed, 458, 529, 577, 639
composition, 83
concave, 67
conjugate, 90, 221
continuous, 639
convex, 67
convex hull, 119
convex-concave, 115
derivative, 640
di   erentiable, 640
domain, 639
dual, 216
elementary symmetric, 122
extended-value extension, 68
   rst-order approximation, 640
   tting, 324
gradient, 641
huber, 345
interpolation, 324, 329
lagrange dual, 216
lagrangian, 215
legendre transform, 95
likelihood, 351
linear-fractional, 41
log barrier, 563
log-concave, 104
log-convex, 104
matrix monotone, 108
monomial, 160
monotone, 115
notation, 14, 639
objective, 127
penalty, 294
perspective, 39, 89, 117
piecewise-linear, 119, 326
pointwise maximum, 80
posynomial, 160

projection, 397
projective, 41
quasiconvex, 95
quasilinear, 122
self-concordant, 497
separable, 249
support, 63
unimodal, 95
utility, 115, 211, 339

game, 238

advantage of going second, 240
barrier method, 627
bounded inverse condition, 559
continuous, 239
convex-concave, 540, 542, 560

newton step, 559

duality, 231
duality interpretation, 239
matrix, 230

gamma function, 104

log-convexity, 123

gauss-id77, 520
gaussian distribution

log-concavity, 104, 123

generalized

eigenvalue

decomposition, 647
minimization, 204
quasiconvexity, 102

fractional program, 205
geometric program, 200
inequality, 43

barrier method, 596, 601
central path, 598
dual, 53, 264
log barrier, 598
logarithm, 597
optimization problem, 167
theorem of alternatives, 269
linear-fractional program, 152
logarithm, 597

dual, 607
positive semide   nite cone, 598
second-order cone, 597

posynomial, 200

geometric

mean, 73, 75

conjugate, 120
maximizing, 198
program, 160, 199

barrier method, 573
convex form, 162
dual, 256
   oor planning, 444
sensitivity analysis, 284
unconstrained, 254, 458

index

707

global optimization, 10

bounds, 11

gp, see geometric program
gradient, 641

conjugate, 121
log barrier, 564
method, 466

and condition number, 473
projected, 557

gram matrix, 405

halfspace, 27

voronoi description, 60

hankel matrix, 65, 66, 170, 204
harmonic mean, 116, 198

log-concavity, 122

hessian, 71, 643

conjugate, 121
lipschitz continuity, 488
log barrier, 564
sparse, 511

h  older   s inequality, 78
huber penalty function, 190, 299, 345
hull

a   ne, 23
conic, 25
convex, 24

hybrid vehicle, 212
hyperbolic

cone, 39
constraint, 197
set, 61

hyperplane, 27

separating, 46, 195, 423
supporting, 50

hypothesis testing, 364, 370

iid noise, 352
implementation

equality constrained methods, 542
interior-point methods, 615
line search, 508
newton   s method, 509
unconstrained methods, 508

implicit constraint, 134
lagrange dual, 257

indicator function, 68, 92

linear approximation, 218
projection and separation, 401

induced norm, 636
inequality

arithmetic-geometric mean, 75, 78
cauchy-schwartz, 633
chebyshev, 150, 154
componentwise, 32, 43
constraint, 127
fenchel   s, 94

form linear program, 147

dual, 225

generalized, 43
h  older   s, 78
information, 115
jensen   s, 77
matrix, 43, 647
triangle, 634
young   s, 94, 120

inexact line search, 464
infeasibility certi   cate, 259
infeasible

barrier method, 571
id77, 531, 534, 558

convergence analysis, 536
phase i, 582

problem, 127

weak duality, 273

in   mum, 638
information inequality, 115
inner product, 633
input design, 307
interior, 637

relative, 23

interior-point

method, 561

implementation, 615

primal-dual method, 609

internal rate of return, 97
interpolation, 324, 329

least-norm, 333
with convex function, 337

intersection

ellipsoids, 262
sets, 36

int (interior), 637
inverse

convex function, 114
linear-fractional function, 62

investment

log-optimal, 559
return, 208

irr (internal rate of return), 97

jacobian, 640
jensen   s inequality, 77

quasiconvex function, 98

karush-kuhn-tucker, see kkt
kinematic constraints, 247
kkt

conditions, 243

central path, 567
generalized inequalities, 267
mechanics interpretation, 246
modi   ed, 577

708

index

supporting hyperplane interpretation,

283

matrix, 522

bounded inverse assumption, 530
nonsingularity, 523, 547

system, 677

nonsingularity, 557
solving, 542

id181, 90, 115, 362

   1-norm

approximation, 294, 353, 514

barrier method, 617

id173, 308
steepest descent method, 477

lagrange

basis, 326
dual function, 216
dual problem, 223
multiplier, 215

contact force interpretation, 247
price interpretation, 253

lagrangian, 215

relaxation, 276, 654

lapack, 684
laplace transform, 106
laplacian distribution, 352
ldlt factorization, 671
least-norm

interpolation, 333
problem, 131, 302

least-penalty problem, 304

statistical interpretation, 359

least-squares, 4, 131, 153, 177, 293, 304, 458

convex function    t, 338
cost as function of weights, 81
dual function, 218
regularized, 184, 205
robust, 190, 300, 323
strong duality, 227
legendre transform, 95
length, 96, 634
level set

convex function, 113

lexicographic cone, 64
likelihood function, 351
likelihood ratio test, 371
line, 21

search, 464, 514

backtracking, 464
exact, 464
implementation, 508
pre-computation, 518
primal-dual interior-point method, 612

convergence, 467
discrimination, 423
equality constraint
eliminating, 132

equations

banded, 669
block elimination, 672
easy, 664
factor-solve method, 666
kkt system, 677
lapack, 684
least-squares, 304
low rank update, 680
lower triangular, 664
multiple righthand sides, 667
newton system, 510
orthogonal, 666
schur complement, 672
software, 684
solution set, 22
solving, 661
sparse solution, 304
symmetric positive de   nite, 669
underdetermined, 681
upper triangular, 665

estimation, 292

best unbiased, 176
facility location, 432
inequalities

alternative, 261
analytic center, 458
log-barrier, 499
solution set, 27, 31
theorem of alternatives, 50, 54

matrix inequality, 38, 76, 82

alternative, 270
analytic center, 422, 459, 508, 553
multiple, 169
strong alternatives, 287

program, 1, 6, 146

barrier method, 571, 574
boolean, 194, 276
central path, 565
dual, 224, 274
dual function, 219
inequality form, 147
primal-dual interior-point method, 613
random constraints, 157
random cost, 154
relaxation of boolean, 194
robust, 157, 193, 278
standard form, 146
strong duality, 227, 280

segment, 21

linear

classi   cation, 423

separation

ellipsoids, 197

linear-fractional

index

709

function, 41

composition, 102
image of convex set, 62
inverse, 62
quasiconvexity, 97

program, 151

generalized, 152

linearized optimality condition, 485
lmi, see linear matrix inequality
locally optimal, 9, 128, 138
location, 432
log barrier, 563

generalized inequalities, 597, 598
gradient and hessian, 564
linear inequalities, 499
linear matrix inequality, 459
penalty function, 295

log-chebyshev approximation, 344, 629
log-concave

density, 104, 352
function, 104

log-convex function, 104
log-convexity

perron-frobenius eigenvalue, 200
second-order conditions, 105

log-determinant, 499

function, 73
gradient, 641
hessian, 644

log-likelihood function, 352
log-optimal investment, 209, 559
log-sum-exp

function, 72, 93
gradient, 642

logarithm, 71
dual, 607
generalized inequality, 597
self-concordance, 497

logistic

classi   cation, 427
function, 122
model, 210
regression, 354

lorentz cone, 31
low rank update, 680
lower triangular matrix, 664
l  owner-john ellipsoid, 410
lp, see linear progam
   p-norm, 635

dual, 637

lu factorization, 668

manufacturing yield, 211
map, see maximum a posteriori id203
markov chain

equilibrium distribution, 285
estimation, 394

fastest mixing, 173

dual, 286

markowitz portfolio optimization, 155
matrix

arrow, 670
banded, 510, 546, 553, 669, 675
block inverse, 650
completion problem, 204
condition number, 649
convexity, 110, 112
copositive, 65, 202
detection probabilities, 366
diagonal plus low rank, 511, 678
euclidean distance, 65
exponential, 110
factorization, 666
fractional function, 76, 82, 89
fractional minimization, 198
game, 230
gram, 405
hankel, 65, 66, 170, 204
hessian, 643
inequality, 43, 647
inverse

matrix convexity, 124

inversion lemma, 515, 678
kkt, 522

nonsingularity, 557
low rank update, 680
minimal upper bound, 180
monotone function, 108
multiplication, 663
node incidence, 551
nonnegative, 165
nonnegative de   nite, 647
norm, 82

approximation, 194
minimization, 169

orthogonal, 666
p0, 202
permutation, 666
positive de   nite, 647
positive semide   nite, 647
power, 110, 112
pseudo-inverse, 649
quadratic function, 111
sparse, 511
square-root, 647

max function, 72

conjugate, 120

max-min

inequality, 238
property, 115, 237

max-row-sum norm, 194, 636
maximal element, 45
maximization problem, 129

710

index

concave, 137

maximum

a posteriori id203 estimation, 357
determinant matrix completion, 204
eigenvalue, 82, 203
element, 45
id178, 254, 558

distribution, 362
dual, 248
strong duality, 228

likelihood

detector, 369
estimation, 351

id203 distance, 118
singular value, 82, 649

dual, 637
minimization, 169
norm, 636

volume

ellipsoid, 414
rectangle, 449, 629

mean

harmonic, 116

method

analytic centers, 626
barrier, 568
bisection, 146
descent, 463
factor-solve, 666
feasibility, 579
gauss-newton, 520
infeasible start newton, 534
interior-point, 561
local optimization, 9
newton   s, 484
phase i, 579
primal-dual, 609
randomized, 11
sequential unconstrained minimization,

569

steepest descent, 475

midpoint convexity, 60
minimal

element, 45

via dual inequalities, 54

surface, 159

minimax

angle    tting, 448
approximation, 293
detector, 367

minimization

equality constrained, 521

minimizing

sequence, 457

minimum

element, 45

via dual inequalities, 54

fuel optimal control, 194
length piecewise-linear curve, 547
norm

   tting, 331

singular value, 649
variance linear unbiased estimator, 176
volume ellipsoid
dual, 222, 228

minkowski function, 119
mixed strategy matrix game, 230
ml, see maximum likelihood
model predictive control, 17
moment, 66

bounds, 170
function

log-concavity, 123

generating function, 106
multidimensional, 204

monomial, 160

approximation, 199

monotone

mapping, 115
nonnegative cone, 64
vector function, 108

monotonicity

   rst-order condition, 109

moore-penrose inverse, 649
motzkin   s theorem, 447
multicriterion

detector design, 368
optimization, 181
problem, 181

scalarization, 183

multidimensional moments, 204
multiplier, 215
mutual information, 207

n (nullspace), 646
network

optimal    ow, 193, 550
rate optimization, 619, 628

newton

decrement, 486, 515, 527
infeasible start method, 531
method, 484

a   ne invariance, 494, 496
approximate, 519
convergence analysis, 529, 536
convex-concave game, 540
dual, 557
equality constraints, 525, 528
implementing, 509
infeasible, 558
self-concordance, 531
trust region, 515

step

index

711

a   ne invariance, 527
equality constraints, 526
primal-dual, 532

system, 510

neyman-pearson lemma, 371
node incidence matrix, 551
nonconvex

optimization, 9
quadratic problem

strong duality, 229

nonlinear

classi   cation, 429
facility location problem, 434
optimization, 9
programming, 9

nonnegative

de   nite matrix, 647
matrix, 165
orthant, 32, 43

minimization, 142

polynomial, 44, 65

nonparametric distribution estimation, 359
norm, 72, 93, 634

approximation, 291
by quadratic, 636
dual, 254
dual function, 221
weighted, 293

ball, 30
cone, 31

dual, 52

conjugate, 93
dual, 637
equivalence, 636
euclidean, 633
frobenius, 634
induced, 636
matrix, 82
max-row-sum, 636
maximum singular value, 636
operator, 636
quadratic, 635

approximation, 413

spectral, 636
sum-absolute-value, 635

normal

cone, 66
distribution

log-concavity, 104
equations, 458, 510
vector, 27

normalized id178, 90
nuclear norm, 637
nullspace, 646

objective function, 127
open set, 637

operator norm, 636
optimal

activity levels, 195
allocation, 523
consumption, 208
control, 194, 303, 552
hybrid vehicle, 212
minimum fuel, 194

design, 292, 303
detector design, 364
duality gap, 226
input design, 307
lagrange multipliers, 223
locally, 9
network    ow, 550
pareto, 57
point, 128

local, 138

resource allocation, 559
set, 128
trade-o    analysis, 182
value, 127, 175

bound via dual function, 216

optimality

conditions, 241

generalized inequalities, 266
kkt, 243
linearized, 485, 526

optimization

convex, 7
embedded, 3
global, 10
local, 9
multicriterion, 181
nonlinear, 9
over polynomials, 203
problem, 127

epigraph form, 134
equivalent, 130
feasibility, 128
feasible, 127
generalized inequalities, 167
maximization, 129
optimal value, 127
perturbation analysis, 249, 250
sensitivity analysis, 250
standard form, 127
symmetry, 189
recourse, 211, 519
robust, 208
two-stage, 211, 519
variable, 127
vector objective, 174

optimizing

over some variables, 133

option pricing, 285

712

index

oracle problem description, 136
ordering

lexicographic, 64

orthogonal

complement, 27
decomposition, 646
matrix, 666

outliers, 298
outward normal vector, 27
over-complete basis, 333

parameter problem description, 136
parametric distribution estimation, 351
pareto optimal, 57, 177, 206
partial

ordering via cone, 43
sum, 62

partitioning problem, 219, 629

dual, 226
dual function, 220
eigenvalue bound, 220
semide   nite program relaxation, 285

pattern recognition, 422
penalty function

approximation, 294
deadzone-linear, 295
huber, 299
log barrier, 295
robust, 299, 343
statistical interpretation, 353

permutation matrix, 666
perron-frobenius eigenvalue, 165

log-convexity, 200
perspective, 39, 89, 117

conjugate, 120
function, 207
image of polyhedron, 62

perturbed optimization problem, 250
phase i method, 579
complexity, 592
infeasible start, 582
sum of infeasibilities, 580

piecewise

arc, 453
polynomial, 327

piecewise-linear

curve

minimum length, 547

function, 80, 119, 326

conjugate, 120

minimization, 150, 562

dual, 275

pin-hole camera, 39
placement, 432

quadratic, 434

point

minimal, 45

minimum, 45

pointed cone, 43
pointwise maximum, 80
poisson distribution, 353
polyhedral uncertainty

robust linear program, 278

polyhedron, 31, 38

chebyshev center, 148, 417
convex hull description, 34
distance between, 154, 403
euclidean projection on, 398
image under perspective, 62
volume, 108
voronoi description, 60

polynomial

classi   cation, 430
   tting, 326, 331
interpolation, 326
log-concavity, 123
nonnegative, 44, 65, 203
piecewise, 327
positive semide   nite, 203
sum of squares, 203
trigonometric, 116, 326

polytope, 31
portfolio

bounding risk, 171
diversi   cation constraint, 279
log-optimal, 209
loss risk constraints, 158
optimization, 2, 155
risk-return trade-o   , 185

positive

de   nite matrix, 647
semide   nite

cone, 34, 36, 64
matrix, 647
matrix completion, 204
polynomial, 203

posynomial, 160

generalized, 200
two-term, 200

power allocation, 196

broadcast channel, 210
communication channel, 210
hybrid vehicle, 212

power function, 71
conjugate, 120
log-concavity, 104

pre-computation for line search, 518
predictor-corrector method, 625
preference relation, 340
present value, 97
price, 57

arbitrage-free, 263
interpretation of duality, 240

index

option, 285
shadow, 241

primal residual, 532
primal-dual

method, 609

geometric program, 613
linear program, 613

newton step, 532
search direction, 609

id203

conditional, 42
distribution

convex sets, 62
maximum distance, 118

simplex, 33

problem

conic form, 168
control, 303
convex, 136
data, 136
dual, 223
equality constrained, 521
estimation, 292
euclidean distance and angle, 405
   oor planning, 438
lagrange dual, 223
least-norm, 302
least-penalty, 304
location, 432
matrix completion, 204
maximization, 129
multicriterion, 181
norm approximation, 291
optimal design, 292, 303
partitioning, 629
placement, 432
quasiconvex, 137
regression, 291
regressor selection, 310
unbounded below, 128
unconstrained, 457
unconstrained quadratic, 458

product

convex functions, 119
inner, 633

production frontier, 57
program

geometric, 160
linear, 146
quadratic, 152
quadratically constrained quadratic, 152
semide   nite, 168, 201

projected gradient method, 557
projection

coordinate, 38
euclidean, 649

713

function, 397
indicator and support function, 401
on a   ne set, 304
on set, 397
on subspace, 292
projective function, 41
proper cone, 43
psd (positive semide   nite), 203
pseudo-inverse, 88, 141, 153, 177, 185, 305,

649

qcqp (quadratically constrained quadratic

program), 152

qp (quadratic program), 152
qr factorization, 682
quadratic

convergence, 489, 539
discrimination, 429
function

convexity, 71
gradient, 641
hessian, 644
minimizing, 140, 514

inequalities

analytic center, 519

inequality

solution set, 61

matrix function, 111
minimization, 458, 649

equality constraints, 522

norm, 635

approximation, 636

norm approximation, 413
optimization, 152, 196
placement, 434
problem

strong duality, 229

program, 152

primal-dual interior-point method, 630
robust, 198

smoothing, 312

quadratic-over-linear function, 72, 76

minimizing, 514

quadratically constrained quadratic program,

152, 196

strong duality, 227

quartile, 62, 117
quasi-id77s, 496
quasiconvex

function, 95

convex representation, 103
   rst-order conditions, 99, 121
jensen   s inequality, 98
second-order conditions, 101

optimization, 137

via convex feasibility, 145

quasilinear function, 122

714

index

r (range), 645
r (reals), 14
r+ (nonnegative reals), 14
r++ (positive reals), 14
rn
randomized

+ (nonnegative orthant), 32

algorithm, 11
detector, 365, 395
strategy, 230

range, 645
rank, 645

quasiconcavity, 98

ratio of distances, 97
recession cone, 66
reconstruction, 310
recourse, 211, 519
rectangle, 61

maximum volume, 449, 629

redundant constraint, 128
regression, 153, 291

logistic, 354
robust, 299

regressor, 291

selection, 310, 334

id173, 5

   1, 308
smoothing, 307
tikhonov, 306

regularized

approximation, 305
least-squares, 184, 205

relative

id178, 90
interior, 23
positioning constraint, 439

residual, 291

amplitude distribution, 296
dual, 532
primal, 532

resource allocation, 559
restricted set, 61
riccati recursion, 553
riesz-fej  er theorem, 348
risk-return trade-o   , 185
risk-sensitive cost, 155
robust

approximation, 318
chebyshev approximation, 323
detector, 372
least-squares, 190, 300, 323
linear discrimination, 424
linear program, 157, 193, 278
optimization, 208
penalty function, 299, 343
quadratic program, 198
regression, 299

sn (symmetric n    n matrices), 34
standard inner product, 633
sn
+ (positive semide   nite n    n matrices),
saddle-point, 115

34

convex-concave function, 281
duality interpretation, 237
via newton   s method, 627
scalarization, 178, 206, 306, 368
duality interpretation, 236
multicriterion problem, 183

scaling, 38
schur complement, 76, 88, 124, 133, 546,

650, 672

sdp, see semide   nite program
search direction, 463

newton, 484, 525
primal-dual, 609
second derivative, 643

chain rule, 645

second-order

conditions

convexity, 71
log-convexity, 105
quasiconvexity, 101

cone, 31, 449

generalized logarithm, 597

cone program, 156

barrier method, 601
central path, 599
complexity, 606
dual, 287

segment, 21
self-concordance, 496, 516

barrier method complexity, 585
composition, 499
conjugate function, 517
id77 with equality constraints,

531

semide   nite program, 168, 201
barrier method, 602, 618
central path, 600
complex, 202
complexity, 608
dual, 265
relaxation

partitioning problem, 285

sensitivity analysis, 250

geometric program, 284

separable

block, 552
function, 249

separating

a   ne and convex set, 49
cones, 66
convex sets, 403, 422

index

715

hyperplane, 46, 195, 423
converse theorem, 50
duality proof, 235
polyhedra, 278
theorem proof, 46

point and convex set, 49, 399
point and polyhedron, 401
sphere, 195
strictly, 49

set

a   ne, 21
boundary, 638
closed, 637
closure, 638
condition number, 461
convex, 23
distance between, 402
distance to, 397
eccentricity, 461
expanded, 61
hyperbolic, 61
intersection, 36
open, 637
projection, 397
rectangle, 61
restricted, 61
slab, 61
sublevel, 75
sum, 38
superlevel, 75
wedge, 61
width, 461

shadow price, 241, 253
signomial, 200
simplex, 32

id203, 33
unit, 33
volume, 407
singular value, 82

decomposition, 648

slab, 61
slack variable, 131
slater   s condition, 226

generalized inequalities, 265
proof of strong duality, 234

smoothing, 307, 310
quadratic, 312

socp, see second-order cone program
solid cone, 43
solution set

linear equations, 22
linear inequality, 27
linear matrix inequality, 38
quadratic inequality, 61
strict linear inequalities, 63

sparse

approximation, 333
description, 334
matrix, 511

cholesky factorization, 670
lu factorization, 669

solution, 304
vectors, 663

spectral

decomposition, 646
norm, 636

dual, 637
minimization, 169

sphere

separating, 195

spline, 327

   tting, 331

spread of eigenvalues, 203
square-root of matrix, 647
standard form

cone program, 168

dual, 266

linear program, 146

dual, 224

standard inner product, 633

sn, 633

statistical estimation, 351
steepest descent method, 475

   1-norm, 477

step length, 463
stopping criterion via duality, 242
strict

linear inequalities, 63
separation, 49

strong

alternatives, 260
convexity, 459, 558
duality, 226

linear program, 280
max-min property, 238

convex-concave function, 281

sublevel set, 75

closedness assumption, 457
condition number, 461

suboptimality

certi   cate, 241
condition, 460

substitution of variable, 130
sum

of k largest, 80

conjugate, 120
solving via dual, 278

of squares, 203
partial, 62
sets, 38

sos (sum of squares), 203

sum-absolute-value norm, 635

716

index

sumt (sequential unconstrained minimiza-

tion method), 569

superlevel set, 75
support function, 63, 81, 92, 120

projection and separation, 401

support vector classi   er, 425
supporting hyperplane, 50
converse theorem, 63
kkt conditions, 283
theorem, 51

supremum, 638
surface

area, 159
optimal trade-o   , 182
surrogate duality gap, 612
svd (singular value decomposition), 648
symbolic factorization, 511
symmetry, 189

constraint, 442

theorem

alternatives, 50, 54, 258

generalized inequalities, 269

eigenvalue interlacing, 122
gauss-markov, 188
motzkin, 447
perron-frobenius, 165
riesz-fej  er, 348
separating hyperplane, 46
slater, 226
supporting hyperplane, 51

tikhonov id173, 306
time-frequency analysis, 334
total variation reconstruction, 312
trade-o    analysis, 182
transaction fee, 155
translation, 38
triangle inequality, 634
triangularization, 326
trigonometric polynomial, 116, 326
trust region, 302

id77, 515
problem, 229

upper triangular matrix, 665
utility function, 115, 130, 211, 339

variable

change of, 130
dual, 215
elimination, 672
explanatory, 353
optimization, 127
slack, 131

vector

normal, 27
optimization, 174

scalarization, 178

veri   cation, 10
volume

ellipsoid, 407
polyhedron, 108
simplex, 407

von neuman growth problem, 152
voronoi region, 60

water-   lling method, 245
weak

alternatives, 258
duality, 225

infeasible problems, 273

max-min inequality, 281

wedge, 61
weight vector, 179
weighted

least-squares, 5
norm approximation, 293

well conditioned basis, 407
width, 461
wireless communication system, 196
wishart distribution, 105
worst-case

analysis, 10
robust approximation, 319

yield function, 107, 211
young   s inequality, 94, 120

two-stage optimization, 519
two-way partitioning problem, see partition-

z (integers), 697

ing problem

unbounded below, 128
uncertainty ellipsoid, 322
unconstrained minimization, 457

method, 568

underdetermined linear equations, 681
uniform distribution, 105
unimodal function, 95
unit

ball, 634
simplex, 33

