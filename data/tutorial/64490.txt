machine learning for systems

and

systems for machine learning

jeff dean

google brain team

g.co/brain

presenting the work of many people at google

systems for machine learning

google confidential + proprietary (permission granted  to share within nist)

general purpose processor performance trends

single-core 
performance 
plateauing 
after 
decades of 
exponential 
growth

graph from 40 years of microprocessor trend data, karl rupp, cc-by 4.0.

just when deep learning is creating insatiable 
computation demands
training powerful but computationally-expensive deep models on:
    terabyte or petabyte-sized training datasets

plus techniques like automl (   learning to learn   , neural architecture 
search, etc.) can multiply desired training computation by 5-1000x

id136 using expensive deep models in systems with:
    hundreds of thousands of requests per second
    latency requirements of tens of milliseconds
    billions of users

more computational power needed

deep learning is transforming how we 

design computers

google confidential + proprietary (permission granted  to share within nist)

special computation properties

reduced
precision

ok

about 1.2
   about 0.6
about 0.7

not

1.21042
   0.61127
0.73989343

special computation properties

reduced
precision

ok

handful of 
specific 
operations

about 1.2
   about 0.6
about 0.7

not

1.21042
   0.61127
0.73989343

  

=

google-designed chip for neural net id136

in production use for ~36 months: used on search 
queries, for id4, for speech, for 
image recognition, for alphago match,    

in-datacenter performance analysis of a tensor processing 
unit, jouppi, young, patil, patterson et al., isca 2017, 
arxiv.org/abs/1704.04760

tpuv1 is a huge help for id136

but what about training?

speeding up training hugely important:

for researcher productivity, and 

for increasing scale of problems that can be tackled

tensor processing unit v2

google-designed device for neural net training and id136

tensor processing unit v2

google-designed device for neural net training and id136

tpuv2 chip

hbm
8 gb

core

core

scalar/vector 

units

scalar/vector 

units

hbm
8 gb

    16 gb of hbm
    600 gb/s mem bw
    scalar/vector units: 

32b float

    mxu: 32b float 

accumulation but 
reduced precision for 
multipliers

    45 tflops

mxu

128x128

mxu

128x128

tensor processing unit v2

    180 teraflops of computation, 64 gb of hbm memory, 2400 gb/s mem bw
    designed to be connected together into larger configurations

tpu pod 

64 2nd-gen tpus

11.5 petaflops

4 terabytes of hbm memory

programmed via tensorflow

same program will run w/only minor modifications on cpus, gpus, & tpus

same program scales via synchronous data parallelism without modification 

on tpu pods 

offered via google cloud

cloud tpu - host w/180 tflops tpuv2 device attached

g.co/tpusignup

accelerated id202 (xla)

    jit / aot compiler for id202
    targets multiple backends, e.g. cpus, gpus, and tpus
    compiler, runtime, and accelerator-specific optimizer
    compiler plus cpu and gpu backends open-sourced

as part of tensorflow

the life of a neural network:

model.py

tf estimator code

tf graph

accelerated id202 (xla)

    jit / aot compiler for id202
    targets multiple backends, e.g. cpus, gpus, and tpus
    compiler, runtime, and accelerator-specific optimizer
    compiler plus cpu and gpu backends open-sourced

as part of tensorflow

the life of a neural network:

model.py

tf estimator code

tf graph

xla

target-independent 

optimizations

xla

target-specific 
code generation

some tpu success stories

internal search ranking model training:

14.2x: ~9 hours on 1/4 pod vs. ~132 hours on 275 high end cpu machines 

internal image model training:

9.8x: ~22 hours on 1/4 pod vs. ~216 hours on previous production setup

wavenet production model id136:

generates speech at 20x real time

some tpu success stories

resnet-50 to >76% accuracy:

1402 minutes (23 hours 22 minutes) on single tpuv2 device
45 minutes on 1/2 pod (32 tpuv2 devices, 31.2x speedup)

resnet-50 to 75% accuracy:

22 minutes on full pod (64 tpuv2 devices)

same code, no special tricks

some tpu success stories

resnet-50 to >76% accuracy:

1402 minutes (23 hours 22 minutes) on single tpuv2 device
45 minutes on 1/2 pod (32 tpuv2 devices, 31.2x speedup)

resnet-50 to 75% accuracy:

22 minutes on full pod (64 tpuv2 devices)

same code, no special tricks

plug: 
come see sam smith   s talk on    don't decay the learning rate, increase the batch 
size    tomorrow at 8:50 am and chris ying   s talk    id163 is the new mnist    at 
9:30 am, both in the deep learning at supercomputing scale workshop in 101b

tpu scaling for resnet-50

batch size
(i/o tokens)

# tpus

time to
ppl=4.8

16k / 16k

32k / 32k

256k / 256k

1m / 1m

1

4

16

64

17.9 hours

3.5 hours

1.1 hours

0.5 hours

more than just id163

transformer model from "attention is 
all you need"
(2017 a. vaswani et. al.,  nips 2017)

wmt   14 english-german translation 
task

adam optimizer - same learning rate 
schedule across configurations

making 1000 cloud tpus available for free to top researchers who are 

committed to open machine learning research

we   re excited to see what researchers will do with much more computation!

g.co/tpusignup 

what should we build in future ml 

accelerators?

google confidential + proprietary (permission granted  to share within nist)

ml arxiv papers per year

if you start an asic machine learning accelerator 

design today, ...

starts to get deployed into production in ~2 years

must remain relevant through ~5 years from now

can we see the future clearly enough?

what should we bet on?

some example questions

precision:

will very-low precision training (1-4 bit weights, 1-4 bit activations)
work in general across all problems we care about?

sparsity and embeddings: how should we handle:

dynamic routing like the sparsely-gated mixture of experts work (iclr   17)
very large embeddings for some problems (e.g. 1b items x 1000d)

batch size:

should we build machines for very large batch sizes?  or batch size 1?

training algorithms:

will sgd-like algorithms remain the dominant training paradigm?
or will large-batch second-order methods like k-fac be better?

machine learning for systems

google confidential + proprietary (permission granted  to share within nist)

learning should be used throughout our 
computing systems
traditional low-level systems code (operating systems, 
compilers, storage systems) does not make extensive use of 
machine learning today
this should change!
a few examples and some opportunities...

machine learning for

higher performance machine learning 

models

google confidential + proprietary (permission granted  to share within nist)

for large models, model parallelism is important

for large models, model parallelism is important

but getting good performance given multiple 

computing devices is non-trivial and non-obvious

softmax

attention

lstm 2

lstm 1

a

a

_
_

b

b

c

c

d

d

a

b

c

a

b

c 

d

softmax

attention

lstm 2

gpu2

lstm 1

gpu1

gpu4

gpu3

a

b

c 

d

a

a

_
_

b

b

c

c

d

d

a

b

c

id23 for

higher performance machine learning models

device placement optimization with id23, 
azalia mirhoseini, hieu pham, quoc le, mohammad norouzi, samy bengio, benoit steiner, yuefeng zhou, 
naveen kumar, rasmus larsen, and jeff dean, icml 2017, arxiv.org/abs/1706.04972

id23 for

higher performance machine learning models

placement model 
(trained via rl) gets 
graph as input + set 
of devices, outputs 
device placement for 
each graph node

device placement optimization with id23, 
azalia mirhoseini, hieu pham, quoc le, mohammad norouzi, samy bengio, benoit steiner, yuefeng zhou, 
naveen kumar, rasmus larsen, and jeff dean, icml 2017, arxiv.org/abs/1706.04972

id23 for

higher performance machine learning models

placement model 
(trained via rl) gets 
graph as input + set 
of devices, outputs 
device placement for 
each graph node

measured time 
per step gives 
rl reward signal

device placement optimization with id23, 
azalia mirhoseini, hieu pham, quoc le, mohammad norouzi, samy bengio, benoit steiner, yuefeng zhou, 
naveen kumar, rasmus larsen, and jeff dean, icml 2017, arxiv.org/abs/1706.04972

device placement with id23

placement model (trained 
via rl) gets graph as input 
+ set of devices, outputs 
device placement for each 
graph node

measured time 
per step gives 
rl reward signal

+19.3% faster vs. expert human for neural 
translation model

+19.7% faster vs. expert human for inceptionv3 
image model

device placement optimization with id23, 
azalia mirhoseini, hieu pham, quoc le, mohammad norouzi, samy bengio, benoit steiner, yuefeng zhou, 
naveen kumar, rasmus larsen, and jeff dean, icml 2017, arxiv.org/abs/1706.04972

device placement with id23

placement model (trained 
via rl) gets graph as input 
+ set of devices, outputs 
device placement for each 
graph node

measured time 
per step gives 
rl reward signal

plug: come see azalia mirhoseini   s talk on    learning device 
placement    tomorrow at 1:30 pm in the deep learning at 
supercomputing scale workshop in 101b

+19.3% faster vs. expert human for neural 
translation model

+19.7% faster vs. expert human for inceptionv3 
image model

device placement optimization with id23, 
azalia mirhoseini, hieu pham, quoc le, mohammad norouzi, samy bengio, benoit steiner, yuefeng zhou, 
naveen kumar, rasmus larsen, and jeff dean, icml 2017, arxiv.org/abs/1706.04972

learned index structures

not

conventional index structures

google confidential + proprietary (permission granted  to share within nist)

b-trees are models

the case for learned index structures, tim kraska, alex beutel, ed chi, jeffrey dean & neoklis polyzotis, arxiv.org/abs/1712.01208

indices as cdfs

the case for learned index structures, tim kraska, alex beutel, ed chi, jeffrey dean & neoklis polyzotis, arxiv.org/abs/1712.01208

does it work?

index of 200m web service log records

type

config

lookup time

speedup vs. btree

size (mb)

size vs. btree

btree

page size: 128

learned index

2nd stage size: 10000

learned index

2nd stage size: 50000

learned index

2nd stage size: 100000

learned index

2nd stage size: 200000

260 ns

222 ns

162 ns

144 ns

126 ns

1.0x 12.98 mb

1.17x

1.60x

0.15 mb

0.76 mb

1.67x

1.53 mb

2.06x

3.05 mb

1.0x

0.01x

0.05x

0.12x

0.23x

the case for learned index structures, tim kraska, alex beutel, ed chi, jeffrey dean & neoklis polyzotis, arxiv.org/abs/1712.01208

hash tables

the case for learned index structures, tim kraska, alex beutel, ed chi, jeffrey dean & neoklis polyzotis, arxiv.org/abs/1712.01208

bloom filters

~2x space improvement over
bloom filter at same false positive rate
the case for learned index structures, tim kraska, alex beutel, ed chi, jeffrey dean & neoklis polyzotis, arxiv.org/abs/1712.01208

model is simple id56
w is number of units in id56 layer
e is width of character embedding

machine learning for improving 

datacenter efficiency

google confidential + proprietary (permission granted  to share within nist)

machine learning to reduce cooling cost in datacenters

ml control on

ml control off

collaboration between deepmind and google datacenter operations teams.
see https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/ 

where else could we use learning?

google confidential + proprietary (permission granted  to share within nist)

computer systems are filled with heuristics

compilers, networking code, operating systems,    
heuristics have to work well    in general case   
generally don   t adapt to actual pattern of usage
generally don   t take into account available context

anywhere we   re using heuristics to make a 
decision!
compilers: instruction scheduling, register allocation, loop 
nest parallelization strategies,    
networking: tcp window size decisions, backoff for 
retransmits, data compression, ...
operating systems: process scheduling, buffer cache 
insertion/replacement, file system prefetching,    
job scheduling systems: which tasks/vms to co-locate on 
same machine, which tasks to pre-empt, ...
asic design: physical circuit layout, test case selection,    

anywhere we   ve punted to a user-tunable 
performance option!
many programs have huge numbers of tunable command-line 
flags, usually not changed from their defaults

--eventmanager_threads=16
--bigtable_scheduler_batch_size=8
--mapreduce_merge_memory=134217728
--lexicon_cache_size=1048576
--storage_server_rpc_freelist_size=128
...

meta-learn everything
ml:
    learning placement decisions
    learning fast kernel implementations
    learning optimization update rules
    learning input preprocessing pipeline steps
    learning id180
    learning model architectures for specific device types, or that are fast 

for id136 on mobile device x,  learning which pre-trained 
components to reuse,    

computer architecture/datacenter networking design:
    learning best design properties by exploring design space 

automatically (via simulator)

keys for success in these settings

(1) having a numeric metric to measure and optimize
(2) having a clean interface to easily integrate learning into 

all of these kinds of systems

current work: exploring apis and implementations
basic ideas:

make a sequence of choices in some context
eventually get feedback about those choices
make this all work with very low overhead, even in

distributed settings

support many implementations of core interfaces

conclusions

ml hardware is at its infancy.  
even faster systems and wider 
deployment will lead to many 
more breakthroughs across a 
wide range of domains.

learning in the core of all of our 
computer systems will make 
them better/more adaptive.  
there are many opportunities for 
this.

more info about our work at g.co/brain 

