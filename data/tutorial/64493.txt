deep learning

ian goodfellow
yoshua bengio
aaron courville

contents

website

acknowledgments

notation

1 introduction

1.1 who should read this book? . . . . . . . . . . . . . . . . . . . .
1.2
historical trends in deep learning . . . . . . . . . . . . . . . . .

i applied math and machine learning basics

2 id202

2.1
scalars, vectors, matrices and tensors . . . . . . . . . . . . . . .
2.2 multiplying matrices and vectors . . . . . . . . . . . . . . . . . .
identity and inverse matrices
2.3
. . . . . . . . . . . . . . . . . . . .
2.4
linear dependence and span . . . . . . . . . . . . . . . . . . . .
norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5
special kinds of matrices and vectors
2.6
. . . . . . . . . . . . . . .
eigendecomposition . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7
2.8
singular value decomposition . . . . . . . . . . . . . . . . . . . .
the moore-penrose pseudoinverse . . . . . . . . . . . . . . . . . .
2.9
2.10 the trace operator
. . . . . . . . . . . . . . . . . . . . . . . . .
2.11 the determinant . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.12 example: principal components analysis
. . . . . . . . . . . . .

3 id203 and id205

3.1 why id203? . . . . . . . . . . . . . . . . . . . . . . . . . . .

i

viii

ix

xiii

1
8
12

27

29
29
32
34
35
37
38
40
42
43
44
45
45

51
52

contents

random variables
. . . . . . . . . . . . . . . . . . . . . . . . . .
3.2
id203 distributions . . . . . . . . . . . . . . . . . . . . . . .
3.3
3.4 marginal id203 . . . . . . . . . . . . . . . . . . . . . . . . .
id155 . . . . . . . . . . . . . . . . . . . . . . .
3.5
the chain rule of conditional probabilities . . . . . . . . . . . .
3.6
3.7
independence and conditional independence . . . . . . . . . . . .
expectation, variance and covariance . . . . . . . . . . . . . . .
3.8
. . . . . . . . . . . . . . . . .
3.9
common id203 distributions
3.10 useful properties of common functions
. . . . . . . . . . . . . .
3.11 bayes    rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3.12 technical details of continuous variables
. . . . . . . . . . . . .
3.13 id205 . . . . . . . . . . . . . . . . . . . . . . . . . .
3.14 structured probabilistic models . . . . . . . . . . . . . . . . . . .

4 numerical computation

4.1 over   ow and under   ow . . . . . . . . . . . . . . . . . . . . . . .
4.2
poor conditioning . . . . . . . . . . . . . . . . . . . . . . . . . .
4.3 gradient-based optimization . . . . . . . . . . . . . . . . . . . .
constrained optimization . . . . . . . . . . . . . . . . . . . . . .
4.4
4.5
example: linear least squares . . . . . . . . . . . . . . . . . . .

54
54
56
57
57
58
58
60
65
68
69
71
73

78
78
80
80
91
94

5 machine learning basics

96
learning algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
97
5.1
capacity, over   tting and under   tting . . . . . . . . . . . . . . . 108
5.2
hyperparameters and validation sets . . . . . . . . . . . . . . . . 118
5.3
5.4
estimators, bias and variance . . . . . . . . . . . . . . . . . . . . 120
5.5 id113 . . . . . . . . . . . . . . . . . . 129
bayesian statistics
5.6
. . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.7
supervised learning algorithms . . . . . . . . . . . . . . . . . . . 137
unsupervised learning algorithms
5.8
. . . . . . . . . . . . . . . . . 142
5.9
stochastic id119 . . . . . . . . . . . . . . . . . . . . . 149
5.10 building a machine learning algorithm . . . . . . . . . . . . . . 151
5.11 challenges motivating deep learning . . . . . . . . . . . . . . . . 152

ii deep networks: modern practices

162

6 deep feedforward networks

164
6.1
example: learning xor . . . . . . . . . . . . . . . . . . . . . . . 167
6.2 gradient-based learning . . . . . . . . . . . . . . . . . . . . . . . 172

ii

contents

6.3
6.4
6.5

6.6

hidden units
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187
architecture design . . . . . . . . . . . . . . . . . . . . . . . . . . 193
back-propagation and other di   erentiation
algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
historical notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220

7 id173 for deep learning

224
7.1
parameter norm penalties . . . . . . . . . . . . . . . . . . . . . . 226
7.2
norm penalties as constrained optimization . . . . . . . . . . . . 233
7.3
id173 and under-constrained problems
. . . . . . . . . 235
7.4 dataset augmentation . . . . . . . . . . . . . . . . . . . . . . . . 236
noise robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
7.5
7.6
semi-supervised learning . . . . . . . . . . . . . . . . . . . . . . 240
7.7 multitask learning . . . . . . . . . . . . . . . . . . . . . . . . . . 241
early stopping . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
7.8
7.9
parameter tying and parameter sharing . . . . . . . . . . . . . . 249
7.10 sparse representations . . . . . . . . . . . . . . . . . . . . . . . . 251
7.11 id112 and other ensemble methods . . . . . . . . . . . . . . . 253
7.12 dropout . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
7.13 adversarial training . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.14 tangent distance, tangent prop and manifold

tangent classi   er . . . . . . . . . . . . . . . . . . . . . . . . . . . 267

8 optimization for training deep models

271
how learning di   ers from pure optimization . . . . . . . . . . . 272
8.1
challenges in neural network optimization . . . . . . . . . . . . 279
8.2
basic algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
8.3
parameter initialization strategies
. . . . . . . . . . . . . . . . . 296
8.4
algorithms with adaptive learning rates . . . . . . . . . . . . . 302
8.5
8.6
approximate second-order methods . . . . . . . . . . . . . . . . 307
8.7 optimization strategies and meta-algorithms . . . . . . . . . . . 313

9 convolutional networks

326
9.1
the convolution operation . . . . . . . . . . . . . . . . . . . . . 327
9.2 motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
9.3
pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
convolution and pooling as an in   nitely strong prior . . . . . . . 339
9.4
variants of the basic convolution function . . . . . . . . . . . . 342
9.5
9.6
structured outputs . . . . . . . . . . . . . . . . . . . . . . . . . . 352
9.7 data types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354

iii

contents

e   cient convolution algorithms
random or unsupervised features

9.8
9.9
9.10 the neuroscienti   c basis for convolutional

. . . . . . . . . . . . . . . . . . 356
. . . . . . . . . . . . . . . . . 356

networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 358
9.11 convolutional networks and the history of deep learning . . . . 365

10 sequence modeling: recurrent and recursive nets

367
10.1 unfolding computational graphs . . . . . . . . . . . . . . . . . . 369
10.2 recurrent neural networks
. . . . . . . . . . . . . . . . . . . . . 372
10.3 bidirectional id56s . . . . . . . . . . . . . . . . . . . . . . . . . . 388
10.4 encoder-decoder sequence-to-sequence

architectures

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
10.5 deep recurrent networks
. . . . . . . . . . . . . . . . . . . . . . 392
10.6 id56s . . . . . . . . . . . . . . . . . . . . . . 394
10.7 the challenge of long-term dependencies . . . . . . . . . . . . . 396
10.8 echo state networks . . . . . . . . . . . . . . . . . . . . . . . . . 399
10.9 leaky units and other strategies for multiple

time scales . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
10.10 the long short-term memory and other gated id56s . . . . . . 404
10.11 optimization for long-term dependencies . . . . . . . . . . . . . 408
10.12 explicit memory . . . . . . . . . . . . . . . . . . . . . . . . . . . 412

11 practical methodology

416
11.1 performance metrics . . . . . . . . . . . . . . . . . . . . . . . . . 417
11.2 default baseline models . . . . . . . . . . . . . . . . . . . . . . . 420
11.3 determining whether to gather more data . . . . . . . . . . . . 421
11.4 selecting hyperparameters . . . . . . . . . . . . . . . . . . . . . . 422
11.5 debugging strategies . . . . . . . . . . . . . . . . . . . . . . . . . 431
11.6 example: multi-digit number recognition . . . . . . . . . . . . . 435

12 applications

438
12.1 large-scale deep learning . . . . . . . . . . . . . . . . . . . . . . 438
12.2 id161 . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
12.3 id103 . . . . . . . . . . . . . . . . . . . . . . . . . . 453
12.4 natural language processing . . . . . . . . . . . . . . . . . . . . 456
12.5 other applications . . . . . . . . . . . . . . . . . . . . . . . . . . 473

iv

contents

iii deep learning research

482

13 linear factor models

485
13.1 probabilistic pca and factor analysis . . . . . . . . . . . . . . . 486
13.2 independent component analysis (ica) . . . . . . . . . . . . . . 487
13.3 slow feature analysis
. . . . . . . . . . . . . . . . . . . . . . . . 489
13.4 sparse coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
13.5 manifold interpretation of pca . . . . . . . . . . . . . . . . . . . 496

14 autoencoders

499
14.1 undercomplete autoencoders
. . . . . . . . . . . . . . . . . . . . 500
14.2 regularized autoencoders . . . . . . . . . . . . . . . . . . . . . . 501
14.3 representational power, layer size and depth . . . . . . . . . . . 505
14.4 stochastic encoders and decoders . . . . . . . . . . . . . . . . . . 506
14.5 denoising autoencoders . . . . . . . . . . . . . . . . . . . . . . . 507
14.6 learning manifolds with autoencoders . . . . . . . . . . . . . . . 513
14.7 contractive autoencoders . . . . . . . . . . . . . . . . . . . . . . 518
14.8 predictive sparse decomposition . . . . . . . . . . . . . . . . . . 521
14.9 applications of autoencoders . . . . . . . . . . . . . . . . . . . . 522

15 representation learning

524
15.1 greedy layer-wise unsupervised pretraining . . . . . . . . . . . 526
15.2 id21 and id20 . . . . . . . . . . . . . 534
15.3 semi-supervised disentangling of causal factors
. . . . . . . . . 539
15.4 distributed representation . . . . . . . . . . . . . . . . . . . . . . 544
15.5 exponential gains from depth . . . . . . . . . . . . . . . . . . . 550
15.6 providing clues to discover underlying causes
. . . . . . . . . . 552

16 structured probabilistic models for deep learning

555
16.1 the challenge of unstructured modeling . . . . . . . . . . . . . . 556
16.2 using graphs to describe model structure . . . . . . . . . . . . . 560
16.3 sampling from id114 . . . . . . . . . . . . . . . . . . 577
16.4 advantages of structured modeling . . . . . . . . . . . . . . . . . 579
16.5 learning about dependencies . . . . . . . . . . . . . . . . . . . . 579
16.6 id136 and approximate id136 . . . . . . . . . . . . . . . . 580
16.7 the deep learning approach to structured

probabilistic models

. . . . . . . . . . . . . . . . . . . . . . . . . 581

17 monte carlo methods

17.1 sampling and monte carlo methods

587
. . . . . . . . . . . . . . . . 587

v

contents

17.2 importance sampling . . . . . . . . . . . . . . . . . . . . . . . . . 589
17.3 id115 methods . . . . . . . . . . . . . . . . 592
17.4 id150 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
17.5 the challenge of mixing between separated

modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 597

18 confronting the partition function

603
18.1 the log-likelihood gradient
. . . . . . . . . . . . . . . . . . . . 604
18.2 stochastic maximum likelihood and contrastive divergence . . . 605
18.3 pseudolikelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . 613
18.4 score matching and ratio matching . . . . . . . . . . . . . . . . 615
18.5 denoising score matching . . . . . . . . . . . . . . . . . . . . . . 617
18.6 noise-contrastive estimation . . . . . . . . . . . . . . . . . . . . 618
18.7 estimating the partition function . . . . . . . . . . . . . . . . . . 621

19 approximate id136

629
19.1 id136 as optimization . . . . . . . . . . . . . . . . . . . . . . 631
19.2 expectation maximization . . . . . . . . . . . . . . . . . . . . . . 632
19.3 map id136 and sparse coding . . . . . . . . . . . . . . . . . 633
19.4 variational id136 and learning . . . . . . . . . . . . . . . . . 636
19.5 learned approximate id136 . . . . . . . . . . . . . . . . . . . 648

20 deep generative models

651
20.1 id82s . . . . . . . . . . . . . . . . . . . . . . . . . 651
20.2 restricted id82s . . . . . . . . . . . . . . . . . . . 653
20.3 id50 . . . . . . . . . . . . . . . . . . . . . . . . . 657
20.4 deep id82s . . . . . . . . . . . . . . . . . . . . . . 660
20.5 id82s for real-valued data . . . . . . . . . . . . . 673
20.6 convolutional id82s . . . . . . . . . . . . . . . . . 679
20.7 id82s for structured or sequential outputs . . . . 681
20.8 other id82s
. . . . . . . . . . . . . . . . . . . . . 683
20.9 back-propagation through random operations . . . . . . . . . . 684
20.10 directed generative nets . . . . . . . . . . . . . . . . . . . . . . . 688
20.11 drawing samples from autoencoders . . . . . . . . . . . . . . . . 707
20.12 generative stochastic networks . . . . . . . . . . . . . . . . . . . 710
20.13 other generation schemes . . . . . . . . . . . . . . . . . . . . . . 712
20.14 evaluating generative models . . . . . . . . . . . . . . . . . . . . 713
20.15 conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 716

bibliography

717

vi

contents

index

773

vii

website

www.deeplearningbook.org

this book is accompanied by the above website. the website provides a
variety of supplementary material, including exercises, lecture slides, corrections of
mistakes, and other resources that should be useful to both readers and instructors.

viii

bibliography

abadi, m., agarwal, a., barham, p., brevdo, e., chen, z., citro, c., corrado, g. s., davis,
a., dean, j., devin, m., ghemawat, s., goodfellow, i., harp, a., irving, g., isard, m.,
jia, y., jozefowicz, r., kaiser, l., kudlur, m., levenberg, j., man  , d., monga, r.,
moore, s., murray, d., olah, c., schuster, m., shlens, j., steiner, b., sutskever, i.,
talwar, k., tucker, p., vanhoucke, v., vasudevan, v., vi  gas, f., vinyals, o., warden,
p., wattenberg, m., wicke, m., yu, y., and zheng, x. (2015). tensorflow: large-scale
machine learning on heterogeneous systems. software available from tensor   ow.org. 25,
210, 441

ackley, d. h., hinton, g. e., and sejnowski, t. j. (1985). a learning algorithm for

id82s. cognitive science, 9, 147   169. 567, 651

alain, g. and bengio, y. (2013). what regularized auto-encoders learn from the data

generating distribution. in iclr   2013, arxiv:1211.4246 . 504, 509, 512, 518

alain, g., bengio, y., yao, l.,   ric thibodeau-laufer, yosinski, j., and vincent, p. (2015).

gsns: generative stochastic networks. arxiv:1503.05571. 507, 709

allen, r. b. (1987). several studies on natural language and back-propagation. in ieee
first international conference on neural networks, volume 2, pages 335   341, san
diego. 468

anderson, e. (1935). the irises of the gasp   peninsula. bulletin of the american iris

society, 59, 2   5. 19

ba, j., mnih, v., and kavukcuoglu, k. (2014). multiple object recognition with visual

attention. arxiv:1412.7755 . 688

bachman, p. and precup, d. (2015). variational generative stochastic networks with
collaborative shaping. in proceedings of the 32nd international conference on machine
learning, icml 2015, lille, france, 6-11 july 2015 , pages 1964   1972. 713

bacon, p.-l., bengio, e., pineau, j., and precup, d. (2015). conditional computation in
neural networks using a decision-theoretic approach. in 2nd multidisciplinary conference
on id23 and decision making (rldm 2015). 445

717

bibliography

bagnell, j. a. and bradley, d. m. (2009). di   erentiable sparse coding. in d. koller,
d. schuurmans, y. bengio, and l. bottou, editors, advances in neural information
processing systems 21 (nips   08), pages 113   120. 494

bahdanau, d., cho, k., and bengio, y. (2015). id4 by jointly
learning to align and translate. in iclr   2015, arxiv:1409.0473 . 25, 99, 392, 412, 415,
459, 470, 471

bahl, l. r., brown, p., de souza, p. v., and mercer, r. l. (1987). id103
with continuous-parameter id48. computer, speech and language, 2,
219   234. 453

baldi, p. and hornik, k. (1989). neural networks and principal component analysis:

learning from examples without local minima. neural networks, 2, 53   58. 283

baldi, p., brunak, s., frasconi, p., soda, g., and pollastri, g. (1999). exploiting the
past and the future in protein secondary structure prediction. bioinformatics, 15(11),
937   946. 388

baldi, p., sadowski, p., and whiteson, d. (2014). searching for exotic particles in

high-energy physics with deep learning. nature communications, 5. 26

ballard, d. h., hinton, g. e., and sejnowski, t. j. (1983). parallel vision computation.

nature. 447

barlow, h. b. (1989). unsupervised learning. neural computation, 1, 295   311. 144

barron, a. e. (1993). universal approximation bounds for superpositions of a sigmoidal

function. ieee trans. on id205, 39, 930   945. 195

bartholomew, d. j. (1987). latent variable models and factor analysis. oxford university

press. 486

basilevsky, a. (1994). statistical factor analysis and related methods: theory and

applications. wiley. 486

bastien, f., lamblin, p., pascanu, r., bergstra, j., goodfellow, i. j., bergeron, a.,
bouchard, n., and bengio, y. (2012). theano: new features and speed improvements.
deep learning and unsupervised id171 nips 2012 workshop. 25, 80, 210,
218, 441

basu, s. and christensen, j. (2013). teaching classi   cation boundaries to humans. in

aaai   2013 . 325

baxter, j. (1995). learning internal representations. in proceedings of the 8th international
conference on computational learning theory (colt   95), pages 311   320, santa cruz,
california. acm press. 241

718

bibliography

bayer, j. and osendorfer, c. (2014). learning stochastic recurrent networks. arxiv

e-prints. 262

becker, s. and hinton, g. (1992). a self-organizing neural network that discovers surfaces

in random-dot stereograms. nature, 355, 161   163. 539

behnke, s. (2001). learning iterative image reconstruction in the neural abstraction

pyramid. int. j. computational intelligence and applications, 1(4), 427   438. 511

beiu, v., quintana, j. m., and avedillo, m. j. (2003). vlsi implementations of threshold
logic-a comprehensive survey. neural networks, ieee transactions on, 14(5), 1217   
1243. 446

belkin, m. and niyogi, p. (2002). laplacian eigenmaps and spectral techniques for
embedding and id91. in t. dietterich, s. becker, and z. ghahramani, editors,
advances in neural information processing systems 14 (nips   01), cambridge, ma.
mit press. 240

belkin, m. and niyogi, p. (2003). laplacian eigenmaps for id84 and

data representation. neural computation, 15(6), 1373   1396. 160, 516

bengio, e., bacon, p.-l., pineau, j., and precup, d. (2015a). conditional computation in

neural networks for faster models. arxiv:1511.06297. 445

bengio, s. and bengio, y. (2000a). taking on the curse of dimensionality in joint
distributions using neural networks. ieee transactions on neural networks, special
issue on data mining and knowledge discovery, 11(3), 550   557. 703

bengio, s., vinyals, o., jaitly, n., and shazeer, n. (2015b). scheduled sampling for
sequence prediction with recurrent neural networks. technical report, arxiv:1506.03099.
378

bengio, y. (1991). arti   cial neural networks and their application to sequence recognition.

ph.d. thesis, mcgill university, (computer science), montreal, canada. 402

bengio, y. (2000). gradient-based optimization of hyperparameters. neural computation,

12(8), 1889   1900. 430

bengio, y. (2002). new distributed probabilistic language models. technical report 1215,

dept. iro, universit   de montr  al. 462

bengio, y. (2009). learning deep architectures for ai . now publishers. 197, 621

bengio, y. (2013). deep learning of representations:

in statistical
language and speech processing, volume 7978 of lecture notes in computer science,
pages 1   37. springer, also in arxiv at http://arxiv.org/abs/1305.0445. 443

looking forward.

bengio, y. (2015). early id136 in energy-based models approximates back-propagation.

technical report arxiv:1510.02777, universite de montreal. 653

719

bibliography

bengio, y. and bengio, s. (2000b). modeling high-dimensional discrete data with multi-

layer neural networks. in nips 12 , pages 400   406. mit press. 702, 703, 705, 707

bengio, y. and delalleau, o. (2009). justifying and generalizing contrastive divergence.

neural computation, 21(6), 1601   1621. 509, 609

bengio, y. and grandvalet, y. (2004). no unbiased estimator of the variance of k-fold
cross-validation. in s. thrun, l. saul, and b. sch  lkopf, editors, advances in neural
information processing systems 16 (nips   03), cambridge, ma. mit press, cambridge.
120

bengio, y. and lecun, y. (2007). scaling learning algorithms towards ai. in large scale

kernel machines. 18

bengio, y. and monperrus, m. (2005). non-local manifold tangent learning. in l. saul,
y. weiss, and l. bottou, editors, advances in neural information processing systems
17 (nips   04), pages 129   136. mit press. 157, 518

bengio, y. and s  n  cal, j.-s. (2003). quick training of probabilistic neural nets by

importance sampling. in proceedings of aistats 2003 . 465

bengio, y. and s  n  cal, j.-s. (2008). adaptive importance sampling to accelerate training
of a neural probabilistic language model. ieee trans. neural networks, 19(4), 713   722.
465

bengio, y., de mori, r., flammia, g., and kompe, r. (1991). phonetically motivated
acoustic parameters for continuous id103 using arti   cial neural networks.
in proceedings of eurospeech   91 . 23, 454

bengio, y., de mori, r., flammia, g., and kompe, r. (1992). neural network-gaussian
mixture hybrid for id103 or density estimation. in nips 4 , pages 175   182.
morgan kaufmann. 454

bengio, y., frasconi, p., and simard, p. (1993). the problem of learning long-term
in ieee international conference on neural

dependencies in recurrent networks.
networks, pages 1183   1195, san francisco. ieee press. (invited paper). 398

bengio, y., simard, p., and frasconi, p. (1994). learning long-term dependencies with

id119 is di   cult. ieee tr. neural nets. 17, 396, 398, 399, 407

bengio, y., latendresse, s., and dugas, c. (1999). gradient-based learning of hyper-

parameters. learning conference, snowbird. 430

bengio, y., ducharme, r., and vincent, p. (2001). a neural probabilistic language model.
in t. k. leen, t. g. dietterich, and v. tresp, editors, nips   2000 , pages 932   938. mit
press. 17, 442, 458, 461, 467, 472, 477

bengio, y., ducharme, r., vincent, p., and jauvin, c. (2003). a neural probabilistic

language model. jmlr, 3, 1137   1155. 461, 467

720

bibliography

bengio, y., le roux, n., vincent, p., delalleau, o., and marcotte, p. (2006a). convex

neural networks. in nips   2005 , pages 123   130. 255

bengio, y., delalleau, o., and le roux, n. (2006b). the curse of highly variable functions

for local kernel machines. in nips   2005 . 155

bengio, y., larochelle, h., and vincent, p. (2006c). non-local manifold parzen windows.

in nips   2005 . mit press. 157, 517

bengio, y., lamblin, p., popovici, d., and larochelle, h. (2007). greedy layer-wise

training of deep networks. in nips   2006 . 13, 18, 197, 319, 320, 526, 528

bengio, y., louradour, j., collobert, r., and weston, j. (2009). curriculum learning. in

icml   09 . 324

bengio, y., mesnil, g., dauphin, y., and rifai, s. (2013a). better mixing via deep

representations. in icml   2013 . 601

bengio, y., l  onard, n., and courville, a. (2013b). estimating or propagating gradients
through stochastic neurons for conditional computation. arxiv:1308.3432. 443, 445,
685, 688

bengio, y., yao, l., alain, g., and vincent, p. (2013c). generalized denoising auto-

encoders as generative models. in nips   2013 . 504, 708, 709

bengio, y., courville, a., and vincent, p. (2013d). representation learning: a review and
new perspectives. ieee trans. pattern analysis and machine intelligence (pami),
35(8), 1798   1828. 552

bengio, y., thibodeau-laufer, e., alain, g., and yosinski, j. (2014). deep generative

stochastic networks trainable by backprop. in icml   2014 . 708, 709, 710, 711

bennett, c. (1976). e   cient estimation of free energy di   erences from monte carlo data.

journal of computational physics, 22(2), 245   268. 627

bennett, j. and lanning, s. (2007). the net   ix prize. 475

berger, a. l., della pietra, v. j., and della pietra, s. a. (1996). a maximum id178
approach to natural language processing. computational linguistics, 22, 39   71. 468

berglund, m. and raiko, t. (2013). stochastic gradient estimate variance in contrastive

divergence and persistent contrastive divergence. corr, abs/1312.6002. 612

bergstra, j. (2011).

incorporating complex cells into neural networks for pattern

classi   cation. ph.d. thesis, universit   de montr  al. 252

bergstra, j. and bengio, y. (2009). slow, decorrelated features for pretraining complex

cell-like networks. in nips   2009 . 490

721

bibliography

bergstra, j. and bengio, y. (2012). random search for hyper-parameter optimization. j.

machine learning res., 13, 281   305. 428, 429

bergstra, j., breuleux, o., bastien, f., lamblin, p., pascanu, r., desjardins, g., turian,
j., warde-farley, d., and bengio, y. (2010). theano: a cpu and gpu math expression
compiler. in proc. scipy. 25, 80, 210, 218, 441

bergstra, j., bardenet, r., bengio, y., and k  gl, b. (2011). algorithms for hyper-parameter

optimization. in nips   2011 . 430

berkes, p. and wiskott, l. (2005). slow feature analysis yields a rich repertoire of complex

cell properties. journal of vision, 5(6), 579   602. 491

bertsekas, d. p. and tsitsiklis, j. (1996). neuro-id145. athena scienti   c.

104

besag, j. (1975). statistical analysis of non-lattice data. the statistician, 24(3), 179   195.

613

bishop, c. m. (1994). mixture density networks. 185

bishop, c. m. (1995a). id173 and complexity control in feed-forward networks.
in proceedings international conference on arti   cial neural networks icann   95 ,
volume 1, page 141   148. 238, 247

bishop, c. m. (1995b). training with noise is equivalent to tikhonov id173.

neural computation, 7(1), 108   116. 238

bishop, c. m. (2006). pattern recognition and machine learning. springer. 96, 142

blum, a. l. and rivest, r. l. (1992). training a 3-node neural network is np-complete.

289

blumer, a., ehrenfeucht, a., haussler, d., and warmuth, m. k. (1989). learnability and

the vapnik   chervonenkis dimension. journal of the acm , 36(4), 929      865. 112

bonnet, g. (1964). transformations des signaux al  atoires    travers les syst  mes non

lin  aires sans m  moire. annales des t  l  communications, 19(9   10), 203   220. 685

bordes, a., weston, j., collobert, r., and bengio, y. (2011). learning structured

embeddings of knowledge bases. in aaai 2011 . 479

bordes, a., glorot, x., weston, j., and bengio, y. (2012). joint learning of words and
meaning representations for open-text id29. aistats   2012 . 396, 479, 480

bordes, a., glorot, x., weston, j., and bengio, y. (2013a). a semantic matching energy
function for learning with multi-relational data. machine learning: special issue on
learning semantics. 479

722

bibliography

bordes, a., usunier, n., garcia-duran, a., weston, j., and yakhnenko, o. (2013b).
translating embeddings for modeling multi-relational data. in c. burges, l. bottou,
m. welling, z. ghahramani, and k. weinberger, editors, advances in neural information
processing systems 26 , pages 2787   2795. curran associates, inc. 479

bornschein, j. and bengio, y. (2015). reweighted wake-sleep.

arxiv:1406.2751 . 690

in iclr   2015,

bornschein, j., shabanian, s., fischer, a., and bengio, y. (2015). training bidirectional

helmholtz machines. technical report, arxiv:1506.03877. 690

boser, b. e., guyon, i. m., and vapnik, v. n. (1992). a training algorithm for opti-
mal margin classi   ers. in colt    92: proceedings of the    fth annual workshop on
computational learning theory, pages 144   152, new york, ny, usa. acm. 17, 139

bottou, l. (1998). online algorithms and stochastic approximations. in d. saad, editor,
online learning in neural networks. cambridge university press, cambridge, uk. 292

bottou, l. (2011). from machine learning to machine reasoning. technical report,

arxiv.1102.1808. 394, 396

bottou, l. (2015). multilayer neural networks. deep learning summer school. 434

bottou, l. and bousquet, o. (2008). the tradeo   s of large scale learning. in nips   2008 .

279, 292

boulanger-lewandowski, n., bengio, y., and vincent, p. (2012). modeling temporal
dependencies in high-dimensional sequences: application to polyphonic music generation
and transcription. in icml   12 . 682

boureau, y., ponce, j., and lecun, y. (2010). a theoretical analysis of feature pooling in
vision algorithms. in proc. international conference on machine learning (icml   10).
339

boureau, y., le roux, n., bach, f., ponce, j., and lecun, y. (2011). ask the locals:
multi-way local pooling for image recognition. in proc. international conference on
id161 (iccv   11). ieee. 339

bourlard, h. and kamp, y. (1988). auto-association by multilayer id88s and

singular value decomposition. biological cybernetics, 59, 291   294. 499

bourlard, h. and wellekens, c. (1989). speech pattern discrimination and multi-layered

id88s. computer speech and language, 3, 1   19. 454

boyd, s. and vandenberghe, l. (2004). id76. cambridge university

press, new york, ny, usa. 91

723

bibliography

brady, m. l., raghavan, r., and slawny, j. (1989). back-propagation fails to separate
where id88s succeed. ieee transactions on circuits and systems, 36, 665   674.
282

brakel, p., stroobandt, d., and schrauwen, b. (2013). training energy-based models for
time-series imputation. journal of machine learning research, 14, 2771   2797. 671,
695

brand, m. (2003). charting a manifold. in nips   2002 , pages 961   968. mit press. 160,

516

breiman, l. (1994). id112 predictors. machine learning, 24(2), 123   140. 253

breiman, l., friedman, j. h., olshen, r. a., and stone, c. j. (1984). classi   cation and

regression trees. wadsworth international group, belmont, ca. 142

bridle, j. s. (1990). alphanets: a recurrent    neural    network architecture with a hidden

markov model interpretation. speech communication, 9(1), 83   92. 182

briggman, k., denk, w., seung, s., helmstaedter, m. n., and turaga, s. c. (2009).
maximin a   nity learning of image segmentation. in nips   2009 , pages 1865   1873. 353

brown, p. f., cocke, j., pietra, s. a. d., pietra, v. j. d., jelinek, f., la   erty, j. d.,
mercer, r. l., and roossin, p. s. (1990). a statistical approach to machine translation.
computational linguistics, 16(2), 79   85. 19

brown, p. f., pietra, v. j. d., desouza, p. v., lai, j. c., and mercer, r. l. (1992). class-
based id165 models of natural language. computational linguistics, 18, 467   479.
458

bryson, a. and ho, y. (1969). applied optimal control: optimization, estimation, and

control. blaisdell pub. co. 221

bryson, jr., a. e. and denham, w. f. (1961). a steepest-ascent method for solving
optimum programming problems. technical report br-1303, raytheon company,
missle and space division. 221

bucilu  a, c., caruana, r., and niculescu-mizil, a. (2006). model compression.

in
proceedings of the 12th acm sigkdd international conference on knowledge discovery
and data mining, pages 535   541. acm. 443

burda, y., grosse, r., and salakhutdinov, r. (2015). importance weighted autoencoders.

arxiv preprint arxiv:1509.00519 . 695

cai, m., shi, y., and liu, j. (2013). deep maxout neural networks for id103.
in automatic id103 and understanding (asru), 2013 ieee workshop
on, pages 291   296. ieee. 190

724

bibliography

carreira-perpi  an, m. a. and hinton, g. e. (2005). on contrastive divergence learning.
in r. g. cowell and z. ghahramani, editors, proceedings of the tenth international
workshop on arti   cial intelligence and statistics (aistats   05), pages 33   40. society
for arti   cial intelligence and statistics. 609

caruana, r. (1993). multitask connectionist learning. in proc. 1993 connectionist models

summer school, pages 372   379. 241

cauchy, a. (1847). m  thode g  n  rale pour la r  solution de syst  mes d     quations simul-
tan  es. in compte rendu des s  ances de l   acad  mie des sciences, pages 536   538. 81,
221

cayton, l. (2005). algorithms for manifold learning. technical report cs2008-0923,

ucsd. 160

chandola, v., banerjee, a., and kumar, v. (2009). anomaly detection: a survey. acm

computing surveys (csur), 41(3), 15. 100

chapelle, o., weston, j., and sch  lkopf, b. (2003). cluster kernels for semi-supervised
learning. in s. becker, s. thrun, and k. obermayer, editors, advances in neural
information processing systems 15 (nips   02), pages 585   592, cambridge, ma. mit
press. 240

chapelle, o., sch  lkopf, b., and zien, a., editors (2006). semi-supervised learning. mit

press, cambridge, ma. 240, 539

chellapilla, k., puri, s., and simard, p. (2006). high performance convolutional neural
networks for document processing.
in guy lorette, editor, tenth international
workshop on frontiers in handwriting recognition, la baule (france). universit   de
rennes 1, suvisoft. http://www.suvisoft.com. 22, 23, 440

chen, b., ting, j.-a., marlin, b. m., and de freitas, n. (2010). deep learning of invariant
spatio-temporal features from video. nips*2010 deep learning and unsupervised
id171 workshop. 354

chen, s. f. and goodman, j. t. (1999). an empirical study of smoothing techniques for

id38. computer, speech and language, 13(4), 359   393. 457, 468

chen, t., du, z., sun, n., wang, j., wu, c., chen, y., and temam, o. (2014a). diannao:
a small-footprint high-throughput accelerator for ubiquitous machine-learning. in pro-
ceedings of the 19th international conference on architectural support for programming
languages and operating systems, pages 269   284. acm. 446

chen, t., li, m., li, y., lin, m., wang, n., wang, m., xiao, t., xu, b., zhang, c.,
and zhang, z. (2015). mxnet: a    exible and e   cient machine learning library for
heterogeneous distributed systems. arxiv preprint arxiv:1512.01274 . 25

725

bibliography

chen, y., luo, t., liu, s., zhang, s., he, l., wang, j., li, l., chen, t., xu, z., sun, n.,
et al. (2014b). dadiannao: a machine-learning supercomputer. in microarchitecture
(micro), 2014 47th annual ieee/acm international symposium on, pages 609   622.
ieee. 446

chilimbi, t., suzue, y., apacible, j., and kalyanaraman, k. (2014). project adam:
building an e   cient and scalable deep learning training system. in 11th usenix
symposium on operating systems design and implementation (osdi   14). 442

cho, k., raiko, t., and ilin, a. (2010). parallel tempering is e   cient for learning restricted

id82s. in ijid98   2010 . 601, 612

cho, k., raiko, t., and ilin, a. (2011). enhanced gradient and adaptive learning rate for

training restricted id82s. in icml   2011 , pages 105   112. 670

cho, k., van merri  nboer, b., gulcehre, c., bougares, f., schwenk, h., and bengio, y.
(2014a). learning phrase representations using id56 encoder-decoder for statistical
machine translation. in proceedings of the empiricial methods in natural language
processing (emnlp 2014). 390, 469, 470

cho, k., van merri  nboer, b., bahdanau, d., and bengio, y. (2014b). on the prop-
erties of id4: encoder-decoder approaches. arxiv e-prints,
abs/1409.1259. 407

choromanska, a., hena   , m., mathieu, m., arous, g. b., and lecun, y. (2014). the

loss surface of multilayer networks. 282, 283

chorowski, j., bahdanau, d., cho, k., and bengio, y. (2014). end-to-end continuous
id103 using attention-based recurrent nn: first results. arxiv:1412.1602.
455

chrisman, l. (1991). learning recursive distributed representations for holistic computa-

tion. connection science, 3(4), 345   366. 468

christianson, b. (1992). automatic hessians by reverse accumulation. ima journal of

numerical analysis, 12(2), 135   150. 220

chrupala, g., kadar, a., and alishahi, a. (2015). learning language through pictures.

arxiv 1506.03694. 407

chung, j., gulcehre, c., cho, k., and bengio, y. (2014). empirical evaluation of gated
recurrent neural networks on sequence modeling. nips   2014 deep learning workshop,
arxiv 1412.3555. 407, 455

chung, j., g  l  ehre,   ., cho, k., and bengio, y. (2015a). gated feedback recurrent

neural networks. in icml   15 . 407

chung, j., kastner, k., dinh, l., goel, k., courville, a., and bengio, y. (2015b). a

recurrent latent variable model for sequential data. in nips   2015 . 694

726

bibliography

ciresan, d., meier, u., masci, j., and schmidhuber, j. (2012). multi-column deep neural

network for tra   c sign classi   cation. neural networks, 32, 333   338. 24, 197

ciresan, d. c., meier, u., gambardella, l. m., and schmidhuber, j. (2010). deep big
simple neural nets for handwritten digit recognition. neural computation, 22, 1   14.
22, 23, 441

coates, a. and ng, a. y. (2011). the importance of encoding versus training with sparse

coding and vector quantization. in icml   2011 . 23, 252, 494

coates, a., lee, h., and ng, a. y. (2011). an analysis of single-layer networks in
unsupervised id171. in proceedings of the thirteenth international conference
on arti   cial intelligence and statistics (aistats 2011). 357, 450

coates, a., huval, b., wang, t., wu, d., catanzaro, b., and andrew, n. (2013).
deep learning with cots hpc systems. in s. dasgupta and d. mcallester, editors,
proceedings of the 30th international conference on machine learning (icml-13),
volume 28 (3), pages 1337   1345. jmlr workshop and conference proceedings. 22, 23,
358, 442

cohen, n., sharir, o., and shashua, a. (2015). on the expressive power of deep learning:

a tensor analysis. arxiv:1509.05009. 552

collobert, r. (2004). large scale machine learning. ph.d. thesis, universit   de paris vi,

lip6. 193

collobert, r. (2011). deep learning for e   cient discriminative parsing. in aistats   2011 .

99, 473

collobert, r. and weston, j. (2008a). a uni   ed architecture for natural language processing:

deep neural networks with multitask learning. in icml   2008 . 466, 473

collobert, r. and weston, j. (2008b). a uni   ed architecture for natural language

processing: deep neural networks with multitask learning. in icml   2008 . 533

collobert, r., bengio, s., and bengio, y. (2001). a parallel mixture of id166s for very

large scale problems. technical report idiap-rr-01-12, idiap. 445

collobert, r., bengio, s., and bengio, y. (2002). parallel mixture of id166s for very large

scale problems. neural computation, 14(5), 1105   1114. 445

collobert, r., weston, j., bottou, l., karlen, m., kavukcuoglu, k., and kuksa, p. (2011a).
natural language processing (almost) from scratch. the journal of machine learning
research, 12, 2493   2537. 324, 473, 533, 534

collobert, r., kavukcuoglu, k., and farabet, c. (2011b). torch7: a matlab-like environ-

ment for machine learning. in biglearn, nips workshop. 25, 209, 441

727

bibliography

comon, p. (1994). independent component analysis - a new concept? signal processing,

36, 287   314. 487

cortes, c. and vapnik, v. (1995). support vector networks. machine learning, 20,

273   297. 17, 139

couprie, c., farabet, c., najman, l., and lecun, y. (2013). indoor semantic segmentation
using depth information. in international conference on learning representations
(iclr2013). 24, 197

courbariaux, m., bengio, y., and david, j.-p. (2015). low precision arithmetic for deep

learning. in arxiv:1412.7024, iclr   2015 workshop. 447

courville, a., bergstra, j., and bengio, y. (2011). unsupervised models of images by

spike-and-slab rbms. in icml   11 . 558, 677

courville, a., desjardins, g., bergstra, j., and bengio, y. (2014). the spike-and-slab
rbm and extensions to discrete and sparse data distributions. pattern analysis and
machine intelligence, ieee transactions on, 36(9), 1874   1887. 679

cover, t. m. and thomas, j. a. (2006). elements of id205, 2nd edition.

wiley-interscience. 71

cox, d. and pinto, n. (2011). beyond simple features: a large-scale feature search
approach to unconstrained face recognition. in automatic face & gesture recognition
and workshops (fg 2011), 2011 ieee international conference on, pages 8   15. ieee.
357

cram  r, h. (1946). mathematical methods of statistics. princeton university press. 133,

292

crick, f. h. c. and mitchison, g. (1983). the function of dream sleep. nature, 304,

111   114. 607

cybenko, g. (1989). approximation by superpositions of a sigmoidal function. mathematics

of control, signals, and systems, 2, 303   314. 194

dahl, g. e., ranzato, m., mohamed, a., and hinton, g. e. (2010). phone recognition

with the mean-covariance restricted id82. in nips   2010 . 24

dahl, g. e., yu, d., deng, l., and acero, a. (2012). context-dependent pre-trained deep
neural networks for large vocabulary id103. ieee transactions on audio,
speech, and language processing, 20(1), 33   42. 454

dahl, g. e., sainath, t. n., and hinton, g. e. (2013). improving deep neural networks

for lvcsr using recti   ed linear units and dropout. in icassp   2013 . 454

dahl, g. e., jaitly, n., and salakhutdinov, r. (2014). multi-task neural networks for

qsar predictions. arxiv:1406.1231. 26

728

bibliography

dauphin, y. and bengio, y. (2013). stochastic ratio matching of rbms for sparse

high-dimensional inputs. in nips26 . nips foundation. 617

dauphin, y., glorot, x., and bengio, y. (2011). large-scale learning of embeddings with

reconstruction sampling. in icml   2011 . 466

dauphin, y., pascanu, r., gulcehre, c., cho, k., ganguli, s., and bengio, y. (2014).
identifying and attacking the saddle point problem in high-dimensional non-convex
optimization. in nips   2014 . 282, 283, 284

davis, a., rubinstein, m., wadhwa, n., mysore, g., durand, f., and freeman, w. t.
(2014). the visual microphone: passive recovery of sound from video. acm transactions
on graphics (proc. siggraph), 33(4), 79:1   79:10. 447

dayan, p. (1990). reinforcement comparison. in connectionist models: proceedings of

the 1990 connectionist summer school, san mateo, ca. 688

dayan, p. and hinton, g. e. (1996). varieties of helmholtz machine. neural networks,

9(8), 1385   1403. 690

dayan, p., hinton, g. e., neal, r. m., and zemel, r. s. (1995). the helmholtz machine.

neural computation, 7(5), 889   904. 690

dean, j., corrado, g., monga, r., chen, k., devin, m., le, q., mao, m., ranzato, m.,
senior, a., tucker, p., yang, k., and ng, a. y. (2012). large scale distributed deep
networks. in nips   2012 . 25, 442

dean, t. and kanazawa, k. (1989). a model for reasoning about persistence and causation.

computational intelligence, 5(3), 142   150. 659

deerwester, s., dumais, s. t., furnas, g. w., landauer, t. k., and harshman, r. (1990).
indexing by latent semantic analysis. journal of the american society for information
science, 41(6), 391   407. 472, 477

delalleau, o. and bengio, y. (2011). shallow vs. deep sum-product networks. in nips.

18, 551

deng, j., dong, w., socher, r., li, l.-j., li, k., and fei-fei, l. (2009). id163: a

large-scale hierarchical image database. in cvpr09 . 19

deng, j., berg, a. c., li, k., and fei-fei, l. (2010a). what does classifying more than
10,000 image categories tell us? in proceedings of the 11th european conference on
id161: part v , eccv   10, pages 71   84, berlin, heidelberg. springer-verlag.
19

deng, l. and yu, d. (2014). deep learning     methods and applications. foundations and

trends in signal processing. 455

729

bibliography

deng, l., seltzer, m., yu, d., acero, a., mohamed, a., and hinton, g. (2010b). binary
coding of speech spectrograms using a deep auto-encoder. in interspeech 2010 , makuhari,
chiba, japan. 24

denil, m., bazzani, l., larochelle, h., and de freitas, n. (2012). learning where to attend
with deep architectures for image tracking. neural computation, 24(8), 2151   2184. 361

denton, e., chintala, s., szlam, a., and fergus, r. (2015). deep generative image models

using a laplacian pyramid of adversarial networks. nips. 698, 699, 714

desjardins, g. and bengio, y. (2008). empirical evaluation of convolutional rbms for
vision. technical report 1327, d  partement d   informatique et de recherche op  ra-
tionnelle, universit   de montr  al. 679

desjardins, g., courville, a. c., bengio, y., vincent, p., and delalleau, o. (2010).
tempered id115 for training of restricted id82s. in
international conference on arti   cial intelligence and statistics, pages 145   152. 601,
612

desjardins, g., courville, a., and bengio, y. (2011). on tracking the partition function.

in nips   2011 . 628

desjardins, g., simonyan, k., pascanu, r., et al. (2015). natural neural networks. in

advances in neural information processing systems, pages 2062   2070. 316

devlin, j., zbib, r., huang, z., lamar, t., schwartz, r., and makhoul, j. (2014). fast
and robust neural network joint models for id151. in proc.
acl   2014 . 468

devroye, l. (2013). non-uniform random variate generation. springerlink : b  cher.

springer new york. 691

dicarlo, j. j. (2013). mechanisms underlying visual object recognition: humans vs.

neurons vs. machines. nips tutorial. 25, 360

dinh, l., krueger, d., and bengio, y. (2014). nice: non-linear independent components

estimation. arxiv:1410.8516. 489

donahue, j., hendricks, l. a., guadarrama, s., rohrbach, m., venugopalan, s., saenko,
k., and darrell, t. (2014). long-term recurrent convolutional networks for visual
recognition and description. arxiv:1411.4389. 100

donoho, d. l. and grimes, c. (2003). hessian eigenmaps: new locally linear embedding
techniques for high-dimensional data. technical report 2003-08, dept. statistics,
stanford university. 160, 516

dosovitskiy, a., springenberg, j. t., and brox, t. (2015). learning to generate chairs with
convolutional neural networks. in proceedings of the ieee conference on computer
vision and pattern recognition, pages 1538   1546. 692, 701

730

bibliography

doya, k. (1993). bifurcations of recurrent neural networks in id119 learning.

ieee transactions on neural networks, 1, 75   80. 396, 399

dreyfus, s. e. (1962). the numerical solution of variational problems. journal of

mathematical analysis and applications, 5(1), 30   45. 221

dreyfus, s. e. (1973). the computational solution of optimal control problems with time

lag. ieee transactions on automatic control, 18(4), 383   385. 221

drucker, h. and lecun, y. (1992). improving generalisation performance using double

back-propagation. ieee transactions on neural networks, 3(6), 991   997. 269

duchi, j., hazan, e., and singer, y. (2011). adaptive subgradient methods for online

learning and stochastic optimization. journal of machine learning research. 303

dudik, m., langford, j., and li, l. (2011). doubly robust policy evaluation and learning.
in proceedings of the 28th international conference on machine learning, icml    11.
477

dugas, c., bengio, y., b  lisle, f., and nadeau, c. (2001). incorporating second-order
functional knowledge for better option pricing. in t. leen, t. dietterich, and v. tresp,
editors, advances in neural information processing systems 13 (nips   00), pages
472   478. mit press. 66, 193

dziugaite, g. k., roy, d. m., and ghahramani, z. (2015). training generative neural net-
works via maximum mean discrepancy optimization. arxiv preprint arxiv:1505.03906 .
699

el hihi, s. and bengio, y. (1996). hierarchical recurrent neural networks for long-term

dependencies. in nips   1995 . 394, 403

elkahky, a. m., song, y., and he, x. (2015). a multi-view deep learning approach for
cross domain user modeling in id126s. in proceedings of the 24th
international conference on world wide web, pages 278   288. 475

elman, j. l. (1993). learning and development in neural networks: the importance of

starting small. cognition, 48, 781   799. 324

erhan, d., manzagol, p.-a., bengio, y., bengio, s., and vincent, p. (2009). the di   culty
of training deep architectures and the e   ect of unsupervised pre-training. in proceedings
of aistats   2009 . 197

erhan, d., bengio, y., courville, a., manzagol, p., vincent, p., and bengio, s. (2010).
why does unsupervised pre-training help deep learning? j. machine learning res.
527, 531, 532

fahlman, s. e., hinton, g. e., and sejnowski, t. j. (1983). massively parallel architectures
in proceedings of the national

for ai: netl, thistle, and id82s.
conference on arti   cial intelligence aaai-83 . 567, 651

731

bibliography

fang, h., gupta, s., iandola, f., srivastava, r., deng, l., doll  r, p., gao, j., he, x.,
mitchell, m., platt, j. c., zitnick, c. l., and zweig, g. (2015). from captions to visual
concepts and back. arxiv:1411.4952. 100

farabet, c., lecun, y., kavukcuoglu, k., culurciello, e., martini, b., akselrod, p., and
talay, s. (2011). large-scale fpga-based convolutional networks. in r. bekkerman,
m. bilenko, and j. langford, editors, scaling up machine learning: parallel and
distributed approaches. cambridge university press. 521

farabet, c., couprie, c., najman, l., and lecun, y. (2013). learning hierarchical features
for scene labeling. ieee transactions on pattern analysis and machine intelligence,
35(8), 1915   1929. 24, 197, 353

fei-fei, l., fergus, r., and perona, p. (2006). id62 of object categories.
ieee transactions on pattern analysis and machine intelligence, 28(4), 594   611. 536

finn, c., tan, x. y., duan, y., darrell, t., levine, s., and abbeel, p. (2015). learning
visual feature spaces for robotic manipulation with deep spatial autoencoders. arxiv
preprint arxiv:1509.06113 . 25

fisher, r. a. (1936). the use of multiple measurements in taxonomic problems. annals

of eugenics, 7, 179   188. 19, 103

f  ldi  k, p. (1989). adaptive network for optimal linear feature extraction. in international
joint conference on neural networks (ijid98), volume 1, pages 401   405, washington
1989. ieee, new york. 490

forcada, m. and   eco, r. (1997). learning recursive distributed representations for
holistic computation. in biological and arti   cial computation: from neuroscience to
technology, pages 453   462. 468

franzius, m., sprekeler, h., and wiskott, l. (2007). slowness and sparseness lead to place,

head-direction, and spatial-view cells. 491

franzius, m., wilbert, n., and wiskott, l. (2008). invariant object recognition with slow
feature analysis. in arti   cial neural networks-icann 2008 , pages 961   970. springer.
492

frasconi, p., gori, m., and sperduti, a. (1997). on the e   cient classi   cation of data
structures by neural networks. in proc. int. joint conf. on arti   cial intelligence. 394,
396

frasconi, p., gori, m., and sperduti, a. (1998). a general framework for adaptive
processing of data structures. ieee transactions on neural networks, 9(5), 768   786.
394, 396

freund, y. and schapire, r. e. (1996a). experiments with a new boosting algorithm. in
machine learning: proceedings of thirteenth international conference, pages 148   156,
usa. acm. 255

732

bibliography

freund, y. and schapire, r. e. (1996b). game theory, on-line prediction and boosting. in
proceedings of the ninth annual conference on computational learning theory, pages
325   332. 255

frey, b. j. (1998). id114 for machine learning and digital communication.

mit press. 702

frey, b. j., hinton, g. e., and dayan, p. (1996). does the wake-sleep algorithm learn good
density estimators? in d. touretzky, m. mozer, and m. hasselmo, editors, advances
in neural information processing systems 8 (nips   95), pages 661   670. mit press,
cambridge, ma. 649

frobenius, g. (1908).   ber matrizen aus positiven elementen, s. b. preuss. akad. wiss.

berlin, germany. 594

fukushima, k. (1975). cognitron: a self-organizing multilayered neural network. biological

cybernetics, 20, 121   136. 15, 222, 526

fukushima, k. (1980). neocognitron: a self-organizing neural network model for a
mechanism of pattern recognition una   ected by shift in position. biological cybernetics,
36, 193   202. 15, 22, 23, 222, 361

gal, y. and ghahramani, z. (2015). bayesian convolutional neural networks with bernoulli

approximate variational id136. arxiv preprint arxiv:1506.02158 . 261

gallinari, p., lecun, y., thiria, s., and fogelman-soulie, f. (1987). memoires associatives

distribuees. in proceedings of cognitiva 87 , paris, la villette. 511

garcia-duran, a., bordes, a., usunier, n., and grandvalet, y. (2015). combining two
and three-way embeddings models for link prediction in knowledge bases. arxiv preprint
arxiv:1506.00999 . 479

garofolo, j. s., lamel, l. f., fisher, w. m., fiscus, j. g., and pallett, d. s. (1993).
darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1.
nasa sti/recon technical report n , 93, 27403. 454

garson, j. (1900). the metric system of identi   cation of criminals, as used in great
britain and ireland. the journal of the anthropological institute of great britain and
ireland, (2), 177   227. 19

gers, f. a., schmidhuber, j., and cummins, f. (2000). learning to forget: continual

prediction with lstm. neural computation, 12(10), 2451   2471. 404, 408

ghahramani, z. and hinton, g. e. (1996). the em algorithm for mixtures of factor
analyzers. technical report crg-tr-96-1, dpt. of comp. sci., univ. of toronto. 485

gillick, d., brunk, c., vinyals, o., and subramanya, a. (2015). multilingual language

processing from bytes. arxiv preprint arxiv:1512.00103 . 472

733

bibliography

girshick, r., donahue, j., darrell, t., and malik, j. (2015). region-based convolutional

networks for accurate id164 and segmentation. 421

giudice, m. d., manera, v., and keysers, c. (2009). programmed to learn? the ontogeny

of mirror neurons. dev. sci., 12(2), 350      363. 653

glorot, x. and bengio, y. (2010). understanding the di   culty of training deep feedforward

neural networks. in aistats   2010 . 299

glorot, x., bordes, a., and bengio, y. (2011a). deep sparse recti   er neural networks. in

aistats   2011 . 15, 171, 193, 222

glorot, x., bordes, a., and bengio, y. (2011b). id20 for large-scale

sentiment classi   cation: a deep learning approach. in icml   2011 . 504, 535

goldberger, j., roweis, s., hinton, g. e., and salakhutdinov, r. (2005). neighbourhood
components analysis. in l. saul, y. weiss, and l. bottou, editors, advances in neural
information processing systems 17 (nips   04). mit press. 113

gong, s., mckenna, s., and psarrou, a. (2000). dynamic vision: from images to face

recognition. imperial college press. 161, 516

goodfellow, i., le, q., saxe, a., and ng, a. (2009). measuring invariances in deep

networks. in nips   2009 , pages 646   654. 252

goodfellow, i., koenig, n., muja, m., pantofaru, c., sorokin, a., and takayama, l. (2010).
help me help you: interfaces for personal robots. in proc. of human robot interaction
(hri), osaka, japan. acm press, acm press. 98

goodfellow, i. j. (2010). technical report: multidimensional, downsampled convolution

for autoencoders. technical report, universit   de montr  al. 350

goodfellow, i. j. (2014). on distinguishability criteria for estimating generative models.
in international conference on learning representations, workshops track. 620, 697

goodfellow, i. j., courville, a., and bengio, y. (2011). spike-and-slab sparse coding
in nips workshop on challenges in learning

for unsupervised feature discovery.
id187. 530, 536

goodfellow, i. j., warde-farley, d., mirza, m., courville, a., and bengio, y. (2013a).
maxout networks. in s. dasgupta and d. mcallester, editors, icml   13 , pages 1319   
1327. 190, 261, 338, 359, 450

goodfellow, i. j., mirza, m., courville, a., and bengio, y. (2013b). multi-prediction deep
id82s. in nips26 . nips foundation. 98, 615, 668, 669, 670, 671, 672,
695

734

bibliography

goodfellow, i. j., warde-farley, d., lamblin, p., dumoulin, v., mirza, m., pascanu, r.,
bergstra, j., bastien, f., and bengio, y. (2013c). pylearn2: a machine learning research
library. arxiv preprint arxiv:1308.4214 . 25, 441

goodfellow, i. j., courville, a., and bengio, y. (2013d). scaling up spike-and-slab models
for unsupervised id171. ieee transactions on pattern analysis and machine
intelligence, 35(8), 1902   1914. 493, 494, 495, 647, 679

goodfellow, i. j., mirza, m., xiao, d., courville, a., and bengio, y. (2014a). an empirical
investigation of catastrophic forgeting in gradient-based neural networks. in iclr   2014 .
191

goodfellow, i. j., shlens, j., and szegedy, c. (2014b). explaining and harnessing adver-

sarial examples. corr, abs/1412.6572. 265, 266, 269, 553, 554

goodfellow, i. j., pouget-abadie, j., mirza, m., xu, b., warde-farley, d., ozair, s.,
courville, a., and bengio, y. (2014c). id3. in nips   2014 .
542, 685, 696, 698, 701

goodfellow, i. j., bulatov, y., ibarz, j., arnoud, s., and shet, v. (2014d). multi-digit
number recognition from street view imagery using deep convolutional neural networks.
in international conference on learning representations. 25, 99, 197, 198, 199, 385,
417, 444

goodfellow, i. j., vinyals, o., and saxe, a. m. (2015). qualitatively characterizing neural
network optimization problems. in international conference on learning representa-
tions. 282, 283, 284, 287

goodman, j. (2001). classes for fast maximum id178 training.

in international

conference on acoustics, speech and signal processing (icassp), utah. 462

gori, m. and tesi, a. (1992). on the problem of local minima in id26. ieee
transactions on pattern analysis and machine intelligence, pami-14(1), 76   86. 282

gosset, w. s. (1908). the probable error of a mean. biometrika, 6(1), 1   25. originally

published under the pseudonym    student   . 19

gouws, s., bengio, y., and corrado, g. (2014). bilbowa: fast bilingual distributed
representations without word alignments. technical report, arxiv:1410.2455. 472, 537

graf, h. p. and jackel, l. d. (1989). analog electronic neural network circuits. circuits

and devices magazine, ieee, 5(4), 44   49. 446

graves, a. (2011). practical variational id136 for neural networks. in nips   2011 . 238

graves, a. (2012). supervised sequence labelling with recurrent neural networks. studies

in computational intelligence. springer. 368, 388, 407, 455

735

bibliography

graves, a. (2013). generating sequences with recurrent neural networks. technical report,

arxiv:1308.0850. 186, 404, 411, 415

graves, a. and jaitly, n. (2014). towards end-to-end id103 with recurrent

neural networks. in icml   2014 . 404

graves, a. and schmidhuber, j. (2005). framewise phoneme classi   cation with bidirec-
tional lstm and other neural network architectures. neural networks, 18(5), 602   610.
388

graves, a. and schmidhuber, j. (2009). o   ine handwriting recognition with multidi-
mensional recurrent neural networks. in d. koller, d. schuurmans, y. bengio, and
l. bottou, editors, nips   2008 , pages 545   552. 388

graves, a., fern  ndez, s., gomez, f., and schmidhuber, j. (2006). connectionist temporal
classi   cation: labelling unsegmented sequence data with recurrent neural networks. in
icml   2006 , pages 369   376, pittsburgh, usa. 455

graves, a., liwicki, m., bunke, h., schmidhuber, j., and fern  ndez, s. (2008). uncon-
strained on-line handwriting recognition with recurrent neural networks. in j. platt,
d. koller, y. singer, and s. roweis, editors, nips   2007 , pages 577   584. 388

graves, a., liwicki, m., fern  ndez, s., bertolami, r., bunke, h., and schmidhuber, j.
(2009). a novel connectionist system for unconstrained handwriting recognition. pattern
analysis and machine intelligence, ieee transactions on, 31(5), 855   868. 404

graves, a., mohamed, a., and hinton, g. (2013). id103 with deep recurrent
neural networks. in icassp   2013 , pages 6645   6649. 388, 393, 394, 404, 406, 407, 455

graves, a., wayne, g., and danihelka, i. (2014). id63s. arxiv:1410.5401.

25, 412

grefenstette, e., hermann, k. m., suleyman, m., and blunsom, p. (2015). learning to

transduce with unbounded memory. in nips   2015 . 412

gre   , k., srivastava, r. k., koutn  k, j., steunebrink, b. r., and schmidhuber, j. (2015).

lstm: a search space odyssey. arxiv preprint arxiv:1503.04069 . 408

gregor, k. and lecun, y. (2010a). emergence of complex-like cells in a temporal product

network with local receptive    elds. technical report, arxiv:1006.0448. 346

gregor, k. and lecun, y. (2010b). learning fast approximations of sparse coding. in
l. bottou and m. littman, editors, proceedings of the twenty-seventh international
conference on machine learning (icml-10). acm. 650

gregor, k., danihelka, i., mnih, a., blundell, c., and wierstra, d. (2014). deep
autoregressive networks. in international conference on machine learning (icml   2014).
690

736

bibliography

gregor, k., danihelka, i., graves, a., and wierstra, d. (2015). draw: a recurrent neural

network for image generation. arxiv preprint arxiv:1502.04623 . 694

gretton, a., borgwardt, k. m., rasch, m. j., sch  lkopf, b., and smola, a. (2012). a
kernel two-sample test. the journal of machine learning research, 13(1), 723   773.
700

g  l  ehre,   . and bengio, y. (2013). knowledge matters: importance of prior information
for optimization. in international conference on learning representations (iclr   2013).
25

guo, h. and gelfand, s. b. (1992). classi   cation trees with neural network feature

extraction. neural networks, ieee transactions on, 3(6), 923   933. 445

gupta, s., agrawal, a., gopalakrishnan, k., and narayanan, p. (2015). deep learning

with limited numerical precision. corr, abs/1502.02551. 447

gutmann, m. and hyvarinen, a. (2010). noise-contrastive estimation: a new estima-
tion principle for unnormalized statistical models. in proceedings of the thirteenth
international conference on arti   cial intelligence and statistics (aistats   10). 618

hadsell, r., sermanet, p., ben, j., erkan, a., han, j., muller, u., and lecun, y.
(2007). online learning for o   road robots: spatial label propagation to learn long-range
traversability. in proceedings of robotics: science and systems, atlanta, ga, usa. 448

hajnal, a., maass, w., pudlak, p., szegedy, m., and turan, g. (1993). threshold circuits

of bounded depth. j. comput. system. sci., 46, 129   154. 196

h  stad, j. (1986). almost optimal lower bounds for small depth circuits. in proceedings
of the 18th annual acm symposium on theory of computing, pages 6   20, berkeley,
california. acm press. 196

h  stad, j. and goldmann, m. (1991). on the power of small-depth threshold circuits.

computational complexity, 1, 113   129. 196

hastie, t., tibshirani, r., and friedman, j. (2001). the elements of statistical learning:
data mining, id136 and prediction. springer series in statistics. springer verlag.
142

he, k., zhang, x., ren, s., and sun, j. (2015). delving deep into recti   ers: surpassing
human-level performance on id163 classi   cation. arxiv preprint arxiv:1502.01852 .
24, 190

hebb, d. o. (1949). the organization of behavior. wiley, new york. 13, 16, 653

hena   , m., jarrett, k., kavukcuoglu, k., and lecun, y. (2011). unsupervised learning

of sparse features for scalable audio classi   cation. in ismir   11 . 521

737

bibliography

henderson, j. (2003). inducing history representations for broad coverage statistical

parsing. in hlt-naacl, pages 103   110. 473

henderson, j. (2004). discriminative training of a neural network statistical parser. in
proceedings of the 42nd annual meeting on association for computational linguistics,
page 95. 473

henniges, m., puertas, g., bornschein, j., eggert, j., and l  cke, j. (2010). binary sparse
coding. in latent variable analysis and signal separation, pages 450   457. springer.
638

herault, j. and ans, b. (1984). circuits neuronaux    synapses modi   ables: d  codage de
messages composites par apprentissage non supervis  . comptes rendus de l   acad  mie
des sciences, 299(iii-13), 525      528. 487

hinton, g. (2012). neural networks for machine learning. coursera, video lectures. 303

hinton, g., deng, l., dahl, g. e., mohamed, a., jaitly, n., senior, a., vanhoucke, v.,
nguyen, p., sainath, t., and kingsbury, b. (2012a). deep neural networks for acoustic
modeling in id103. ieee signal processing magazine, 29(6), 82   97. 24,
454

hinton, g., vinyals, o., and dean, j. (2015). distilling the knowledge in a neural network.

arxiv preprint arxiv:1503.02531 . 443

hinton, g. e. (1989). connectionist learning procedures. arti   cial intelligence, 40,

185   234. 490

hinton, g. e. (1990). mapping part-whole hierarchies into connectionist networks. arti   cial

intelligence, 46(1), 47   75. 412

hinton, g. e. (1999). products of experts. in icann   1999 . 567

hinton, g. e. (2000). training products of experts by minimizing contrastive divergence.
technical report id197u tr 2000-004, gatsby unit, university college london. 608,
673

hinton, g. e. (2006). to recognize shapes,    rst learn to generate images. technical report

utml tr 2006-003, university of toronto. 526, 592

hinton, g. e. (2007a). how to do id26 in a brain.

nips   2007 deep learning workshop. 653

invited talk at the

hinton, g. e. (2007b). learning multiple layers of representation. trends in cognitive

sciences, 11(10), 428   434. 657

hinton, g. e. (2010). a practical guide to training restricted id82s.
technical report utml tr 2010-003, department of computer science, university of
toronto. 608

738

bibliography

hinton, g. e. and ghahramani, z. (1997). generative models for discovering sparse
distributed representations. philosophical transactions of the royal society of london.
144

hinton, g. e. and mcclelland, j. l. (1988). learning representations by recirculation. in

nips   1987 , pages 358   366. 499

hinton, g. e. and roweis, s. (2003). stochastic neighbor embedding. in nips   2002 . 516

hinton, g. e. and salakhutdinov, r. (2006). reducing the dimensionality of data with

neural networks. science, 313(5786), 504   507. 506, 522, 526, 527, 531

hinton, g. e. and sejnowski, t. j. (1986). learning and relearning in id82s.
in d. e. rumelhart and j. l. mcclelland, editors, parallel distributed processing,
volume 1, chapter 7, pages 282   317. mit press, cambridge. 567, 651

hinton, g. e. and sejnowski, t. j. (1999). unsupervised learning: foundations of neural

computation. mit press. 539

hinton, g. e. and shallice, t. (1991). lesioning an attractor network: investigations of

acquired dyslexia. psychological review, 98(1), 74. 13

hinton, g. e. and zemel, r. s. (1994). autoencoders, minimum description length, and

helmholtz free energy. in nips   1993 . 499

hinton, g. e., sejnowski, t. j., and ackley, d. h. (1984). id82s: constraint
satisfaction networks that learn. technical report tr-cmu-cs-84-119, carnegie-mellon
university, dept. of computer science. 567, 651

hinton, g. e., mcclelland, j., and rumelhart, d. (1986). distributed representations.
in d. e. rumelhart and j. l. mcclelland, editors, parallel distributed processing:
explorations in the microstructure of cognition, volume 1, pages 77   109. mit press,
cambridge. 16, 221, 524

hinton, g. e., revow, m., and dayan, p. (1995a). recognizing handwritten digits using
mixtures of linear models. in g. tesauro, d. touretzky, and t. leen, editors, advances
in neural information processing systems 7 (nips   94), pages 1015   1022. mit press,
cambridge, ma. 485

hinton, g. e., dayan, p., frey, b. j., and neal, r. m. (1995b). the wake-sleep algorithm

for unsupervised neural networks. science, 268, 1558   1161. 501, 649

hinton, g. e., dayan, p., and revow, m. (1997). modelling the manifolds of images of

handwritten digits. ieee transactions on neural networks, 8, 65   74. 496

hinton, g. e., welling, m., teh, y. w., and osindero, s. (2001). a new view of ica. in
proceedings of 3rd international conference on independent component analysis and
blind signal separation (ica   01), pages 746   751, san diego, ca. 487

739

bibliography

hinton, g. e., osindero, s., and teh, y. (2006). a fast learning algorithm for deep belief

nets. neural computation, 18, 1527   1554. 13, 18, 23, 141, 526, 527, 657, 658

hinton, g. e., deng, l., yu, d., dahl, g. e., mohamed, a., jaitly, n., senior, a.,
vanhoucke, v., nguyen, p., sainath, t. n., and kingsbury, b. (2012b). deep neural
networks for acoustic modeling in id103: the shared views of four research
groups. ieee signal process. mag., 29(6), 82   97. 99

hinton, g. e., srivastava, n., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2012c).
improving neural networks by preventing co-adaptation of feature detectors. technical
report, arxiv:1207.0580. 235, 260, 264

hinton, g. e., vinyals, o., and dean, j. (2014). dark knowledge. invited talk at the

baylearn bay area machine learning symposium. 443

hochreiter, s. (1991). untersuchungen zu dynamischen neuronalen netzen. diploma

thesis, t.u. m  nchen. 17, 396, 398

hochreiter, s. and schmidhuber, j. (1995). simplifying neural nets by discovering    at
minima. in advances in neural information processing systems 7 , pages 529   536. mit
press. 239

hochreiter, s. and schmidhuber, j. (1997). long short-term memory. neural computation,

9(8), 1735   1780. 17, 404, 407

hochreiter, s., bengio, y., and frasconi, p. (2001). gradient    ow in recurrent nets: the
di   culty of learning long-term dependencies. in j. kolen and s. kremer, editors, field
guide to dynamical recurrent networks. ieee press. 407

holi, j. l. and hwang, j.-n. (1993). finite precision error analysis of neural network
hardware implementations. computers, ieee transactions on, 42(3), 281   290. 446

holt, j. l. and baker, t. e. (1991). back propagation simulations using limited preci-
sion calculations. in neural networks, 1991., ijid98-91-seattle international joint
conference on, volume 2, pages 121   126. ieee. 446

hornik, k., stinchcombe, m., and white, h. (1989). multilayer feedforward networks are

universal approximators. neural networks, 2, 359   366. 194

hornik, k., stinchcombe, m., and white, h. (1990). universal approximation of an
unknown mapping and its derivatives using multilayer feedforward networks. neural
networks, 3(5), 551   560. 194

hsu, f.-h. (2002). behind deep blue: building the computer that defeated the world

chess champion. princeton university press, princeton, nj, usa. 2

huang, f. and ogata, y. (2002). generalized pseudo-likelihood estimates for markov
random    elds on lattice. annals of the institute of statistical mathematics, 54(1), 1   18.
614

740

bibliography

huang, p.-s., he, x., gao, j., deng, l., acero, a., and heck, l. (2013). learning deep
structured semantic models for web search using clickthrough data. in proceedings of
the 22nd acm international conference on conference on information & knowledge
management, pages 2333   2338. acm. 475

hubel, d. and wiesel, t. (1968). receptive    elds and functional architecture of monkey

striate cortex. journal of physiology (london), 195, 215   243. 358

hubel, d. h. and wiesel, t. n. (1959). receptive    elds of single neurons in the cat   s

striate cortex. journal of physiology, 148, 574   591. 358

hubel, d. h. and wiesel, t. n. (1962). receptive    elds, binocular interaction, and
functional architecture in the cat   s visual cortex. journal of physiology (london), 160,
106   154. 358

huszar, f. (2015). how (not) to train your generative model: schedule sampling, likelihood,

adversary? arxiv:1511.05101 . 694

hutter, f., hoos, h., and leyton-brown, k. (2011). sequential model-based optimization
for general algorithm con   guration. in lion-5 . extended version as ubc tech report
tr-2010-10. 430

hyotyniemi, h. (1996). turing machines are recurrent neural networks. in step   96 , pages

13   24. 372

hyv  rinen, a. (1999). survey on independent component analysis. neural computing

surveys, 2, 94   128. 487

hyv  rinen, a. (2005). estimation of non-normalized statistical models using score matching.

journal of machine learning research, 6, 695   709. 509, 615

hyv  rinen, a. (2007a). connections between score matching, contrastive divergence,
and pseudolikelihood for continuous-valued variables. ieee transactions on neural
networks, 18, 1529   1531. 616

hyv  rinen, a. (2007b). some extensions of score matching. computational statistics and

data analysis, 51, 2499   2512. 616

hyv  rinen, a. and hoyer, p. o. (1999). emergence of topography and complex cell
properties from natural images using extensions of ica. in nips, pages 827   833. 489

hyv  rinen, a. and pajunen, p. (1999). nonlinear independent component analysis:

existence and uniqueness results. neural networks, 12(3), 429   439. 489

hyv  rinen, a., karhunen, j., and oja, e. (2001a). independent component analysis.

wiley-interscience. 487

hyv  rinen, a., hoyer, p. o., and inki, m. o. (2001b). topographic independent component

analysis. neural computation, 13(7), 1527   1558. 489

741

bibliography

hyv  rinen, a., hurri, j., and hoyer, p. o. (2009). natural image statistics: a probabilistic

approach to early computational vision. springer-verlag. 364

iba, y. (2001). extended ensemble monte carlo. international journal of modern physics,

c12, 623   656. 601

inayoshi, h. and kurita, t. (2005).

improved generalization by adding both auto-
association and hidden-layer noise to neural-network-based-classi   ers. ieee workshop
on machine learning for signal processing, pages 141   -146. 511

io   e, s. and szegedy, c. (2015). batch id172: accelerating deep network training

by reducing internal covariate shift. 98, 313, 316

jacobs, r. a. (1988). increased rates of convergence through learning rate adaptation.

neural networks, 1(4), 295   307. 303

jacobs, r. a., jordan, m. i., nowlan, s. j., and hinton, g. e. (1991). adaptive mixtures

of local experts. neural computation, 3, 79   87. 185, 445

jaeger, h. (2003). adaptive nonlinear system identi   cation with echo state networks. in

advances in neural information processing systems 15 . 399

jaeger, h. (2007a). discovering multiscale dynamical features with hierarchical echo state

networks. technical report, jacobs university. 394

jaeger, h. (2007b). echo state network. scholarpedia, 2(9), 2330. 399

jaeger, h. (2012). long short-term memory in echo state networks: details of a simulation

study. technical report, technical report, jacobs university bremen. 400

jaeger, h. and haas, h. (2004). harnessing nonlinearity: predicting chaotic systems and

saving energy in wireless communication. science, 304(5667), 78   80. 23, 399

jaeger, h., lukosevicius, m., popovici, d., and siewert, u. (2007). optimization and
applications of echo state networks with leaky- integrator neurons. neural networks,
20(3), 335   352. 403

jain, v., murray, j. f., roth, f., turaga, s., zhigulin, v., briggman, k. l., helmstaedter,
m. n., denk, w., and seung, h. s. (2007). supervised learning of image restoration
with convolutional networks.
in id161, 2007. iccv 2007. ieee 11th
international conference on, pages 1   8. ieee. 352

jaitly, n. and hinton, g. (2011). learning a better representation of speech soundwaves
in acoustics, speech and signal processing

using restricted id82s.
(icassp), 2011 ieee international conference on, pages 5884   5887. ieee. 453

jaitly, n. and hinton, g. e. (2013). vocal tract length perturbation (vtlp) improves

id103. in icml   2013 . 237

742

bibliography

jarrett, k., kavukcuoglu, k., ranzato, m., and lecun, y. (2009). what is the best
multi-stage architecture for object recognition? in iccv   09 . 15, 22, 23, 171, 190, 222,
357, 358, 521

jarzynski, c. (1997). nonequilibrium equality for free energy di   erences. phys. rev. lett.,

78, 2690   2693. 623, 626

jaynes, e. t. (2003). id203 theory: the logic of science. cambridge university

press. 51

jean, s., cho, k., memisevic, r., and bengio, y. (2014). on using very large target

vocabulary for id4. arxiv:1412.2007. 469, 470

jelinek, f. and mercer, r. l. (1980). interpolated estimation of markov source parameters
from sparse data. in e. s. gelsema and l. n. kanal, editors, pattern recognition in
practice. north-holland, amsterdam. 457, 468

jia, y. (2013). ca   e: an open source convolutional architecture for fast feature embedding.

http://caffe.berkeleyvision.org/. 25, 209

jia, y., huang, c., and darrell, t. (2012). beyond spatial pyramids: receptive    eld
in id161 and pattern recognition

learning for pooled image features.
(cvpr), 2012 ieee conference on, pages 3370   3377. ieee. 339

jim, k.-c., giles, c. l., and horne, b. g. (1996). an analysis of noise in recurrent neural
networks: convergence and generalization. ieee transactions on neural networks,
7(6), 1424   1438. 238

jordan, m. i. (1998). learning in id114. kluwer, dordrecht, netherlands. 17

joulin, a. and mikolov, t. (2015). inferring algorithmic patterns with stack-augmented

recurrent nets. arxiv preprint arxiv:1503.01007 . 412

jozefowicz, r., zaremba, w., and sutskever, i. (2015). an empirical evaluation of recurrent

network architectures. in icml   2015 . 302, 407, 408

judd, j. s. (1989). neural network design and the complexity of learning. mit press.

289

jutten, c. and herault, j. (1991). blind separation of sources, part i: an adaptive

algorithm based on neuromimetic architecture. signal processing, 24, 1   10. 487

kahou, s. e., pal, c., bouthillier, x., froumenty, p., g  l  ehre, c., memisevic, r., vincent,
p., courville, a., bengio, y., ferrari, r. c., mirza, m., jean, s., carrier, p. l., dauphin,
y., boulanger-lewandowski, n., aggarwal, a., zumer, j., lamblin, p., raymond,
j.-p., desjardins, g., pascanu, r., warde-farley, d., torabi, a., sharma, a., bengio,
e., c  t  , m., konda, k. r., and wu, z. (2013). combining modality speci   c deep
neural networks for emotion recognition in video. in proceedings of the 15th acm on
international conference on multimodal interaction. 197

743

bibliography

kalchbrenner, n. and blunsom, p. (2013). recurrent continuous translation models. in

emnlp   2013 . 469, 470

kalchbrenner, n., danihelka, i., and graves, a. (2015). grid long short-term memory.

arxiv preprint arxiv:1507.01526 . 390

kamyshanska, h. and memisevic, r. (2015). the potential energy of an autoencoder.

ieee transactions on pattern analysis and machine intelligence. 511

karpathy, a. and li, f.-f. (2015). deep visual-semantic alignments for generating image

descriptions. in cvpr   2015 . arxiv:1412.2306. 100

karpathy, a., toderici, g., shetty, s., leung, t., sukthankar, r., and fei-fei, l. (2014).

large-scale video classi   cation with convolutional neural networks. in cvpr. 19

karush, w. (1939). minima of functions of several variables with inequalities as side

constraints. master   s thesis, dept. of mathematics, univ. of chicago. 93

katz, s. m. (1987). estimation of probabilities from sparse data for the language model
component of a speech recognizer. ieee transactions on acoustics, speech, and signal
processing, assp-35(3), 400   401. 457, 468

kavukcuoglu, k., ranzato, m., and lecun, y. (2008). fast id136 in sparse coding
algorithms with applications to object recognition. technical report, computational and
biological learning lab, courant institute, nyu. tech report cbll-tr-2008-12-01.
521

kavukcuoglu, k., ranzato, m.-a., fergus, r., and lecun, y. (2009). learning invariant

features through topographic    lter maps. in cvpr   2009 . 521

kavukcuoglu, k., sermanet, p., boureau, y.-l., gregor, k., mathieu, m., and lecun, y.
(2010). learning convolutional feature hierarchies for visual recognition. in nips   2010 .
358, 521

kelley, h. j. (1960). gradient theory of optimal    ight paths. ars journal, 30(10),

947   954. 221

khan, f., zhu, x., and mutlu, b. (2011). how do humans teach: on curriculum learning
and teaching dimension. in advances in neural information processing systems 24
(nips   11), pages 1449   1457. 324

kim, s. k., mcafee, l. c., mcmahon, p. l., and olukotun, k. (2009). a highly scalable
restricted id82 fpga implementation. in field programmable logic
and applications, 2009. fpl 2009. international conference on, pages 367   372. ieee.
446

kindermann, r. (1980). markov random fields and their applications (contemporary

mathematics ; v. 1). american mathematical society. 563

744

bibliography

kingma, d. and ba, j. (2014). adam: a method for stochastic optimization. arxiv

preprint arxiv:1412.6980 . 305

kingma, d. and lecun, y. (2010). regularized estimation of image statistics by score

matching. in nips   2010 . 509, 618

kingma, d., rezende, d., mohamed, s., and welling, m. (2014). semi-supervised learning

with deep generative models. in nips   2014 . 421

kingma, d. p. (2013). fast gradient-based id136 with continuous latent variable

models in auxiliary form. technical report, arxiv:1306.0733. 650, 685, 693

kingma, d. p. and welling, m. (2014a). auto-encoding id58. in proceedings

of the international conference on learning representations (iclr). 685, 696

kingma, d. p. and welling, m. (2014b). e   cient gradient-based id136 through
transformations between bayes nets and neural nets. technical report, arxiv:1402.0480.
685

kirkpatrick, s., jr., c. d. g., , and vecchi, m. p. (1983). optimization by simulated

annealing. science, 220, 671   680. 323

kiros, r., salakhutdinov, r., and zemel, r. (2014a). multimodal neural language models.

in icml   2014 . 100

kiros, r., salakhutdinov, r., and zemel, r. (2014b). unifying visual-semantic embeddings

with multimodal neural language models. arxiv:1411.2539 [cs.lg]. 100, 404

klementiev, a., titov, i., and bhattarai, b. (2012). inducing crosslingual distributed

representations of words. in proceedings of coling 2012 . 472, 537

knowles-barley, s., jones, t. r., morgan, j., lee, d., kasthuri, n., lichtman, j. w., and
p   ster, h. (2014). deep learning for the connectome. gpu technology conference. 26

koller, d. and friedman, n. (2009). probabilistic id114: principles and

techniques. mit press. 580, 592, 643

konig, y., bourlard, h., and morgan, n. (1996). remap: recursive estimation and
maximization of a posteriori probabilities     application to transition-based connectionist
id103. in d. touretzky, m. mozer, and m. hasselmo, editors, advances in
neural information processing systems 8 (nips   95). mit press, cambridge, ma. 454

koren, y. (2009). the bellkor solution to the net   ix grand prize. 255, 475

kotzias, d., denil, m., de freitas, n., and smyth, p. (2015). from group to individual

labels using deep features. in acm sigkdd. 104

koutnik, j., gre   , k., gomez, f., and schmidhuber, j. (2014). a clockwork id56. in

icml   2014 . 403

745

bibliography

ko  isk  , t., hermann, k. m., and blunsom, p. (2014). learning bilingual word repre-

sentations by marginalizing alignments. in proceedings of acl. 470

krause, o., fischer, a., glasmachers, t., and igel, c. (2013). approximation properties
of dbns with binary hidden units and real-valued visible units. in icml   2013 . 551

krizhevsky, a. (2010). convolutional id50 on cifar-10. technical report,
university of toronto. unpublished manuscript: http://www.cs.utoronto.ca/ kriz/conv-
cifar10-aug2010.pdf. 441

krizhevsky, a. and hinton, g. (2009). learning multiple layers of features from tiny

images. technical report, university of toronto. 19, 558

krizhevsky, a. and hinton, g. e. (2011). using very deep autoencoders for content-based

id162. in esann . 523

krizhevsky, a., sutskever, i., and hinton, g. (2012). id163 classi   cation with deep

convolutional neural networks. in nips   2012 . 22, 23, 24, 98, 197, 365, 449, 453

krueger, k. a. and dayan, p. (2009). flexible shaping: how learning in small steps helps.

cognition, 110, 380   394. 324

kuhn, h. w. and tucker, a. w. (1951). nonid135. in proceedings of the
second berkeley symposium on mathematical statistics and id203, pages 481   492,
berkeley, calif. university of california press. 93

kumar, a., irsoy, o., su, j., bradbury, j., english, r., pierce, b., ondruska, p., iyyer,
m., gulrajani, i., and socher, r. (2015). ask me anything: dynamic memory networks
for natural language processing. arxiv:1506.07285 . 412, 480

kumar, m. p., packer, b., and koller, d. (2010). self-paced learning for latent variable

models. in nips   2010 . 324

lang, k. j. and hinton, g. e. (1988). the development of the time-delay neural network
architecture for id103. technical report cmu-cs-88-152, carnegie-mellon
university. 361, 368, 402

lang, k. j., waibel, a. h., and hinton, g. e. (1990). a time-delay neural network

architecture for isolated word recognition. neural networks, 3(1), 23   43. 368

langford, j. and zhang, t. (2008). the epoch-greedy algorithm for contextual multi-armed

bandits. in nips   2008 , pages 1096      1103. 476

lappalainen, h., giannakopoulos, x., honkela, a., and karhunen, j. (2000). nonlinear
independent component analysis using id108: experiments and discussion.
in proc. ica. citeseer. 489

larochelle, h. and bengio, y. (2008). classi   cation using discriminative restricted

id82s. in icml   2008 . 240, 252, 528, 683, 712

746

bibliography

larochelle, h. and hinton, g. e. (2010). learning to combine foveal glimpses with a
third-order id82. in advances in neural information processing systems
23 , pages 1243   1251. 361

larochelle, h. and murray, i. (2011). the neural autoregressive distribution estimator.

in aistats   2011 . 702, 705, 706

larochelle, h., erhan, d., and bengio, y. (2008). zero-data learning of new tasks. in

aaai conference on arti   cial intelligence. 537

larochelle, h., bengio, y., louradour, j., and lamblin, p. (2009). exploring strategies for
training deep neural networks. journal of machine learning research, 10, 1   40. 533

lasserre, j. a., bishop, c. m., and minka, t. p. (2006). principled hybrids of generative and
discriminative models. in proceedings of the id161 and pattern recognition
conference (cvpr   06), pages 87   94, washington, dc, usa. ieee computer society.
240, 250

le, q., ngiam, j., chen, z., hao chia, d. j., koh, p. w., and ng, a. (2010). tiled
convolutional neural networks.
in j. la   erty, c. k. i. williams, j. shawe-taylor,
r. zemel, and a. culotta, editors, advances in neural information processing systems
23 (nips   10), pages 1279   1287. 346

le, q., ngiam, j., coates, a., lahiri, a., prochnow, b., and ng, a. (2011). on optimization

methods for deep learning. in proc. icml   2011 . acm. 312

le, q., ranzato, m., monga, r., devin, m., corrado, g., chen, k., dean, j., and ng,
a. (2012). building high-level features using large scale unsupervised learning. in
icml   2012 . 22, 23

le roux, n. and bengio, y. (2008). representational power of restricted boltzmann
machines and id50. neural computation, 20(6), 1631   1649. 551, 652

le roux, n. and bengio, y. (2010). id50 are compact universal approxi-

mators. neural computation, 22(8), 2192   2207. 551

lecun, y. (1985). une proc  dure d   apprentissage pour r  seau    seuil assym  trique. in
cognitiva 85: a la fronti  re de l   intelligence arti   cielle, des sciences de la connaissance
et des neurosciences, pages 599   604, paris 1985. cesta, paris. 221

lecun, y. (1986). learning processes in an asymmetric threshold network. in f. fogelman-
souli  , e. bienenstock, and g. weisbuch, editors, disordered systems and biological
organization, pages 233   240. springer-verlag, les houches, france. 345

lecun, y. (1987). mod  les connexionistes de l   apprentissage. ph.d. thesis, universit   de

paris vi. 17, 499, 511

lecun, y. (1989). generalization and network design strategies. technical report

crg-tr-89-4, university of toronto. 326, 345

747

bibliography

lecun, y., jackel, l. d., boser, b., denker, j. s., graf, h. p., guyon, i., henderson, d.,
howard, r. e., and hubbard, w. (1989). handwritten digit recognition: applications
of neural network chips and automatic learning. ieee communications magazine,
27(11), 41   46. 362

lecun, y., bottou, l., orr, g. b., and m  ller, k.-r. (1998a). e   cient backprop. in
neural networks, tricks of the trade, lecture notes in computer science lncs 1524.
springer verlag. 307, 424

lecun, y., bottou, l., bengio, y., and ha   ner, p. (1998b). gradient based learning

applied to document recognition. proc. ieee. 15, 17, 19, 23, 365, 453, 455

lecun, y., kavukcuoglu, k., and farabet, c. (2010). convolutional networks and
applications in vision. in circuits and systems (iscas), proceedings of 2010 ieee
international symposium on, pages 253   256. ieee. 365

l   ecuyer, p. (1994). e   ciency improvement and variance reduction. in proceedings of

the 1994 winter simulation conference, pages 122      132. 687

lee, c.-y., xie, s., gallagher, p., zhang, z., and tu, z. (2014). deeply-supervised nets.

arxiv preprint arxiv:1409.5185 . 322

lee, h., battle, a., raina, r., and ng, a. (2007). e   cient sparse coding algorithms.
in b. sch  lkopf, j. platt, and t. ho   man, editors, advances in neural information
processing systems 19 (nips   06), pages 801   808. mit press. 635

lee, h., ekanadham, c., and ng, a. (2008). sparse deep belief net model for visual area

v2. in nips   07 . 252

lee, h., grosse, r., ranganath, r., and ng, a. y. (2009). convolutional deep belief
networks for scalable unsupervised learning of hierarchical representations. in l. bottou
and m. littman, editors, proceedings of the twenty-sixth international conference on
machine learning (icml   09). acm, montreal, canada. 357, 680, 681

lee, y. j. and grauman, k. (2011). learning the easy things    rst: self-paced visual

category discovery. in cvpr   2011 . 324

leibniz, g. w. (1676). memoir using the chain rule. (cited in tmme 7:2&3 p 321-332,

2010). 220

lenat, d. b. and guha, r. v. (1989). building large knowledge-based systems; representa-
tion and id136 in the cyc project. addison-wesley longman publishing co., inc.
2

leshno, m., lin, v. y., pinkus, a., and schocken, s. (1993). multilayer feedforward
networks with a nonpolynomial activation function can approximate any function.
neural networks, 6, 861      867. 195, 196

748

bibliography

levenberg, k. (1944). a method for the solution of certain non-linear problems in least

squares. quarterly journal of applied mathematics, ii(2), 164   168. 308

l   h  pital, g. f. a. (1696). analyse des in   niment petits, pour l   intelligence des lignes

courbes. paris: l   imprimerie royale. 220

li, y., swersky, k., and zemel, r. s. (2015). generative moment matching networks.

corr, abs/1502.02761. 699

lin, t., horne, b. g., tino, p., and giles, c. l. (1996). learning long-term dependencies
is not as di   cult with narx recurrent neural networks. ieee transactions on neural
networks, 7(6), 1329   1338. 402

lin, y., liu, z., sun, m., liu, y., and zhu, x. (2015). learning entity and relation

embeddings for id13 completion. in proc. aaai   15 . 479

linde, n. (1992). the machine that changed the world, episode 4. documentary miniseries.

2

lindsey, c. and lindblad, t. (1994). review of hardware neural networks: a user   s
perspective. in proc. third workshop on neural networks: from biology to high
energy physics, pages 195      202, isola d   elba, italy. 446

linnainmaa, s. (1976). taylor expansion of the accumulated rounding error. bit

numerical mathematics, 16(2), 146   160. 221

long, p. m. and servedio, r. a. (2010). restricted id82s are hard to
approximately evaluate or simulate. in proceedings of the 27th international conference
on machine learning (icml   10). 655

lotter, w., kreiman, g., and cox, d. (2015). unsupervised learning of visual structure

using predictive generative networks. arxiv preprint arxiv:1511.06380 . 542, 543

lovelace, a. (1842). notes upon l. f. menabrea   s    sketch of the analytical engine

invented by charles babbage   . 1

lu, l., zhang, x., cho, k., and renals, s. (2015). a study of the recurrent neural network

encoder-decoder for large vocabulary id103. in proc. interspeech. 455

lu, t., p  l, d., and p  l, m. (2010). contextual multi-armed bandits. in international

conference on arti   cial intelligence and statistics, pages 485   492. 476

luenberger, d. g. (1984). linear and nonid135. addison wesley. 312

luko  evi  ius, m. and jaeger, h. (2009). reservoir computing approaches to recurrent

neural network training. computer science review, 3(3), 127   149. 399

749

bibliography

luo, h., shen, r., niu, c., and ullrich, c. (2011). learning class-relevant features and
class-irrelevant features via a hybrid third-order rbm. in international conference on
arti   cial intelligence and statistics, pages 470   478. 683

luo, h., carrier, p. l., courville, a., and bengio, y. (2013). texture modeling with

convolutional spike-and-slab rbms and deep extensions. in aistats   2013 . 100

lyu, s. (2009). interpretation and generalization of score matching. in proceedings of the

twenty-   fth conference in uncertainty in arti   cial intelligence (uai   09). 616

ma, j., sheridan, r. p., liaw, a., dahl, g. e., and svetnik, v. (2015). deep neural nets
as a method for quantitative structure     activity relationships. j. chemical information
and modeling. 528

maas, a. l., hannun, a. y., and ng, a. y. (2013). recti   er nonlinearities improve neural
network acoustic models. in icml workshop on deep learning for audio, speech, and
language processing. 190

maass, w. (1992). bounds for the computational power and learning complexity of analog
neural nets (extended abstract). in proc. of the 25th acm symp. theory of computing,
pages 335   344. 196

maass, w., schnitger, g., and sontag, e. d. (1994). a comparison of the computational
power of sigmoid and boolean threshold circuits. theoretical advances in neural
computation and learning, pages 127   151. 196

maass, w., natschlaeger, t., and markram, h. (2002). real-time computing without
stable states: a new framework for neural computation based on perturbations. neural
computation, 14(11), 2531   2560. 399

mackay, d. (2003). id205, id136 and learning algorithms. cambridge

university press. 71

maclaurin, d., duvenaud, d., and adams, r. p. (2015). gradient-based hyperparameter

optimization through reversible learning. arxiv preprint arxiv:1502.03492 . 430

mao, j., xu, w., yang, y., wang, j., huang, z., and yuille, a. l. (2015). deep captioning

with multimodal recurrent neural networks. in iclr   2015 . arxiv:1410.1090. 100

marcotte, p. and savard, g. (1992). novel approaches to the discrimination problem.

zeitschrift f  r operations research (theory), 36, 517   545. 273

marlin, b. and de freitas, n. (2011). asymptotic e   ciency of deterministic estimators for
discrete energy-based models: ratio matching and pseudolikelihood. in uai   2011 . 615,
617

750

bibliography

marlin, b., swersky, k., chen, b., and de freitas, n. (2010). inductive principles for
restricted id82 learning. in proceedings of the thirteenth international
conference on arti   cial intelligence and statistics (aistats   10), volume 9, pages
509   516. 611, 616, 617

marquardt, d. w. (1963). an algorithm for least-squares estimation of non-linear param-
eters. journal of the society of industrial and applied mathematics, 11(2), 431   441.
308

marr, d. and poggio, t. (1976). cooperative computation of stereo disparity. science,

194. 361

martens, j. (2010). deep learning via hessian-free optimization.

in l. bottou and
m. littman, editors, proceedings of the twenty-seventh international conference on
machine learning (icml-10), pages 735   742. acm. 300

martens, j. and medabalimi, v. (2014). on the expressive e   ciency of sum product

networks. arxiv:1411.7717 . 551

martens, j. and sutskever, i. (2011). learning recurrent neural networks with hessian-free

optimization. in proc. icml   2011 . acm. 408

mase, s. (1995). consistency of the maximum pseudo-likelihood estimator of continuous
state space gibbsian processes. the annals of applied id203, 5(3), pp. 603   612.
614

mcclelland, j., rumelhart, d., and hinton, g. (1995). the appeal of parallel distributed
processing. in computation & intelligence, pages 305   341. american association for
arti   cial intelligence. 16

mcculloch, w. s. and pitts, w. (1943). a logical calculus of ideas immanent in nervous

activity. bulletin of mathematical biophysics, 5, 115   133. 13, 14

mead, c. and ismail, m. (2012). analog vlsi implementation of neural systems, volume 80.

springer science & business media. 446

melchior, j., fischer, a., and wiskott, l. (2013). how to center binary deep boltzmann

machines. arxiv preprint arxiv:1311.1354 . 670

memisevic, r. and hinton, g. e. (2007). unsupervised learning of image transformations.
in proceedings of the id161 and pattern recognition conference (cvpr   07).
683

memisevic, r. and hinton, g. e. (2010). learning to represent spatial transformations
with factored higher-order id82s. neural computation, 22(6), 1473   1492.
683

751

bibliography

mesnil, g., dauphin, y., glorot, x., rifai, s., bengio, y., goodfellow, i., lavoie, e.,
muller, x., desjardins, g., warde-farley, d., vincent, p., courville, a., and bergstra,
j. (2011). unsupervised and id21 challenge: a deep learning approach. in
jmlr w&cp: proc. unsupervised and id21, volume 7. 197, 530, 536

mesnil, g., rifai, s., dauphin, y., bengio, y., and vincent, p. (2012). sur   ng on the

manifold. learning workshop, snowbird. 707

miikkulainen, r. and dyer, m. g. (1991). natural language processing with modular

pdp networks and distributed lexicon. cognitive science, 15, 343   399. 472

mikolov, t. (2012). statistical language models based on neural networks. ph.d. thesis,

brno university of technology. 410

mikolov, t., deoras, a., kombrink, s., burget, l., and cernocky, j. (2011a). empirical
evaluation and combination of advanced id38 techniques. in proc. 12th an-
nual conference of the international speech communication association (interspeech
2011). 467

mikolov, t., deoras, a., povey, d., burget, l., and cernocky, j. (2011b). strategies for

training large scale neural network language models. in proc. asru   2011 . 324, 467

mikolov, t., chen, k., corrado, g., and dean, j. (2013a). e   cient estimation of word rep-
resentations in vector space. in international conference on learning representations:
workshops track. 534

mikolov, t., le, q. v., and sutskever, i. (2013b). exploiting similarities among languages

for machine translation. technical report, arxiv:1309.4168. 537

minka, t. (2005). divergence measures and message passing. microsoft research cambridge

uk tech rep msrtr2005173 , 72(tr-2005-173). 623

minsky, m. l. and papert, s. a. (1969). id88s. mit press, cambridge. 14

mirza, m. and osindero, s. (2014). conditional generative adversarial nets. arxiv preprint

arxiv:1411.1784 . 698

mishkin, d. and matas, j. (2015). all you need is a good init.

arxiv:1511.06422 . 301

arxiv preprint

misra, j. and saha, i. (2010). arti   cial neural networks in hardware: a survey of two

decades of progress. neurocomputing, 74(1), 239   255. 446

mitchell, t. m. (1997). machine learning. mcgraw-hill, new york. 97

miyato, t., maeda, s., koyama, m., nakae, k., and ishii, s. (2015). distributional
smoothing with virtual adversarial training. in iclr. preprint: arxiv:1507.00677. 266

752

bibliography

mnih, a. and gregor, k. (2014). neural variational id136 and learning in belief

networks. in icml   2014 . 688, 690

mnih, a. and hinton, g. e. (2007). three new id114 for statistical language
modelling. in z. ghahramani, editor, proceedings of the twenty-fourth international
conference on machine learning (icml   07), pages 641   648. acm. 460

mnih, a. and hinton, g. e. (2009). a scalable hierarchical distributed language model.
in d. koller, d. schuurmans, y. bengio, and l. bottou, editors, advances in neural
information processing systems 21 (nips   08), pages 1081   1088. 462

mnih, a. and kavukcuoglu, k. (2013). learning id27s e   ciently with noise-
contrastive estimation. in c. burges, l. bottou, m. welling, z. ghahramani, and
k. weinberger, editors, advances in neural information processing systems 26 , pages
2265   2273. curran associates, inc. 467, 620

mnih, a. and teh, y. w. (2012). a fast and simple algorithm for training neural

probabilistic language models. in icml   2012 , pages 1751   1758. 467

mnih, v. and hinton, g. (2010). learning to detect roads in high-resolution aerial images.

in proceedings of the 11th european conference on id161 (eccv). 100

mnih, v., larochelle, h., and hinton, g. (2011). conditional restricted boltzmann
machines for structure output prediction. in proc. conf. on uncertainty in arti   cial
intelligence (uai). 682

mnih, v., kavukcuoglo, k., silver, d., graves, a., antonoglou, i., and wierstra, d. (2013).
playing atari with deep id23. technical report, arxiv:1312.5602. 104

mnih, v., heess, n., graves, a., and kavukcuoglu, k. (2014). recurrent models of visual
attention. in z. ghahramani, m. welling, c. cortes, n. lawrence, and k. weinberger,
editors, nips   2014 , pages 2204   2212. 688

mnih, v., kavukcuoglo, k., silver, d., rusu, a. a., veness, j., bellemare, m. g., graves,
a., riedmiller, m., fidgeland, a. k., ostrovski, g., petersen, s., beattie, c., sadik, a.,
antonoglou, i., king, h., kumaran, d., wierstra, d., legg, s., and hassabis, d. (2015).
human-level control through deep id23. nature, 518, 529   533. 25

mobahi, h. and fisher, iii, j. w. (2015). a theoretical analysis of optimization by

gaussian continuation. in aaai   2015 . 323

mobahi, h., collobert, r., and weston, j. (2009). deep learning from temporal coherence
in video. in l. bottou and m. littman, editors, proceedings of the 26th international
conference on machine learning, pages 737   744, montreal. omnipress. 490

mohamed, a., dahl, g., and hinton, g. (2009). id50 for phone recognition.

454

753

bibliography

mohamed, a., sainath, t. n., dahl, g., ramabhadran, b., hinton, g. e., and picheny,
m. a. (2011). id50 using discriminative features for phone recognition. in
acoustics, speech and signal processing (icassp), 2011 ieee international conference
on, pages 5060   5063. ieee. 454

mohamed, a., dahl, g., and hinton, g. (2012a). acoustic modeling using deep belief
networks. ieee trans. on audio, speech and language processing, 20(1), 14   22. 454

mohamed, a., hinton, g., and penn, g. (2012b). understanding how id50
perform acoustic modelling. in acoustics, speech and signal processing (icassp),
2012 ieee international conference on, pages 4273   4276. ieee. 454

moller, m. f. (1993). a scaled conjugate gradient algorithm for fast supervised learning.

neural networks, 6, 525   533. 312

montavon, g. and muller, k.-r. (2012). deep id82s and the centering
trick. in g. montavon, g. orr, and k.-r. m  ller, editors, neural networks: tricks of
the trade, volume 7700 of lecture notes in computer science, pages 621   637. preprint:
http://arxiv.org/abs/1203.3783. 670

mont  far, g. (2014). universal approximation depth and errors of narrow belief networks

with discrete units. neural computation, 26. 551

mont  far, g. and ay, n. (2011). re   nements of universal approximation results for
id50 and restricted id82s. neural computation, 23(5),
1306   1319. 551

montufar, g. f., pascanu, r., cho, k., and bengio, y. (2014). on the number of linear

regions of deep neural networks. in nips   2014 . 18, 196, 197

mor-yosef, s., samuelo   , a., modan, b., navot, d., and schenker, j. g. (1990). ranking
the risk factors for cesarean: id28 analysis of a nationwide study. obstet
gynecol, 75(6), 944   7. 3

morin, f. and bengio, y. (2005). hierarchical probabilistic neural network language

model. in aistats   2005 . 462, 464

mozer, m. c. (1992). the induction of multiscale temporal structure. in j. m. s. hanson
and r. lippmann, editors, advances in neural information processing systems 4
(nips   91), pages 275   282, san mateo, ca. morgan kaufmann. 403

murphy, k. p. (2012). machine learning: a probabilistic perspective. mit press,

cambridge, ma, usa. 60, 96, 142

nair, v. and hinton, g. (2010). recti   ed linear units improve restricted boltzmann

machines. in icml   2010 . 15, 171, 193

754

bibliography

nair, v. and hinton, g. e. (2009). 3d object recognition with deep belief nets. in y. bengio,
d. schuurmans, j. d. la   erty, c. k. i. williams, and a. culotta, editors, advances in
neural information processing systems 22 , pages 1339   1347. curran associates, inc.
683

narayanan, h. and mitter, s. (2010). sample complexity of testing the manifold hypothesis.

in nips   2010 . 160

naumann, u. (2008). optimal jacobian accumulation is np-complete. mathematical

programming, 112(2), 427   441. 218

navigli, r. and velardi, p. (2005). structural semantic interconnections: a knowledge-
based approach to id51. ieee trans. pattern analysis and
machine intelligence, 27(7), 1075      1086. 480

neal, r. and hinton, g. (1999). a view of the em algorithm that justi   es incremental,
sparse, and other variants. in m. i. jordan, editor, learning in id114. mit
press, cambridge, ma. 632

neal, r. m. (1990). learning stochastic feedforward networks. technical report. 689

neal, r. m. (1993). probabilistic id136 using markov chain monte-carlo methods.
technical report crg-tr-93-1, dept. of computer science, university of toronto. 676

neal, r. m. (1994). sampling from multimodal distributions using tempered transitions.

technical report 9421, dept. of statistics, university of toronto. 601

neal, r. m. (1996). bayesian learning for neural networks. lecture notes in statistics.

springer. 262

neal, r. m. (2001). annealed importance sampling. statistics and computing, 11(2),

125   139. 623, 625, 626, 627

neal, r. m. (2005). estimating ratios of normalizing constants using linked importance

sampling. 627

nesterov, y. (1983). a method of solving a convex programming problem with convergence

rate o(1/k2). soviet mathematics doklady, 27, 372   376. 296

nesterov, y. (2004). introductory lectures on id76 : a basic course. applied

optimization. kluwer academic publ., boston, dordrecht, london. 296

netzer, y., wang, t., coates, a., bissacco, a., wu, b., and ng, a. y. (2011). reading
digits in natural images with unsupervised id171. deep learning and
unsupervised id171 workshop, nips. 19

ney, h. and kneser, r. (1993). improved id91 techniques for class-based statistical
language modelling. in european conference on speech communication and technology
(eurospeech), pages 973   976, berlin. 458

755

bibliography

ng,

a.

(2015).

advice

for

applying

machine

learning.

https://see.stanford.edu/materials/aimlcs229/ml-advice.pdf. 416

niesler, t. r., whittaker, e. w. d., and woodland, p. c. (1998). comparison of part-of-
speech and automatically derived category-based language models for id103.
in international conference on acoustics, speech and signal processing (icassp),
pages 177   180. 458

ning, f., delhomme, d., lecun, y., piano, f., bottou, l., and barbano, p. e. (2005).
toward automatic phenotyping of developing embryos from videos. image processing,
ieee transactions on, 14(9), 1360   1371. 353

nocedal, j. and wright, s. (2006). numerical optimization. springer. 90, 93

norouzi, m. and fleet, d. j. (2011). minimal loss hashing for compact binary codes. in

icml   2011 . 523

nowlan, s. j. (1990). competing experts: an experimental investigation of associative

mixture models. technical report crg-tr-90-5, university of toronto. 445

nowlan, s. j. and hinton, g. e. (1992). simplifying neural networks by soft weight-sharing.

neural computation, 4(4), 473   493. 137

olshausen, b. and field, d. j. (2005). how close are we to understanding v1? neural

computation, 17, 1665   1699. 15

olshausen, b. a. and field, d. j. (1996). emergence of simple-cell receptive    eld properties
by learning a sparse code for natural images. nature, 381, 607   609. 144, 252, 364, 492

olshausen, b. a., anderson, c. h., and van essen, d. c. (1993). a neurobiological
model of visual attention and invariant pattern recognition based on dynamic routing
of information. j. neurosci., 13(11), 4700   4719. 445

opper, m. and archambeau, c. (2009). the variational gaussian approximation revisited.

neural computation, 21(3), 786   792. 685

oquab, m., bottou, l., laptev, i., and sivic, j. (2014). learning and transferring mid-level
image representations using convolutional neural networks. in id161 and
pattern recognition (cvpr), 2014 ieee conference on, pages 1717   1724. ieee. 534

osindero, s. and hinton, g. e. (2008). modeling image patches with a directed hierarchy
of markov random    elds. in j. platt, d. koller, y. singer, and s. roweis, editors,
advances in neural information processing systems 20 (nips   07), pages 1121   1128,
cambridge, ma. mit press. 630

ovid and martin, c. (2004). metamorphoses. w.w. norton. 1

756

bibliography

paccanaro, a. and hinton, g. e. (2000). extracting distributed representations of concepts
and relations from positive and negative propositions. in international joint conference
on neural networks (ijid98), como, italy. ieee, new york. 479

paine, t. l., khorrami, p., han, w., and huang, t. s. (2014). an analysis of unsupervised

pre-training in light of recent advances. arxiv preprint arxiv:1412.6597 . 530

palatucci, m., pomerleau, d., hinton, g. e., and mitchell, t. m. (2009). zero-shot
learning with semantic output codes. in y. bengio, d. schuurmans, j. d. la   erty,
c. k. i. williams, and a. culotta, editors, advances in neural information processing
systems 22 , pages 1410   1418. curran associates, inc. 537

parker, d. b. (1985). learning-logic. technical report tr-47, center for comp. research

in economics and management sci., mit. 221

pascanu, r., mikolov, t., and bengio, y. (2013). on the di   culty of training recurrent

neural networks. in icml   2013 . 285, 396, 399, 403, 409, 410, 411

pascanu, r., g  l  ehre,   ., cho, k., and bengio, y. (2014a). how to construct deep

recurrent neural networks. in iclr   2014 . 18, 262, 393, 394, 406, 455

pascanu, r., montufar, g., and bengio, y. (2014b). on the number of id136 regions
of deep feed forward networks with piece-wise linear activations. in iclr   2014 . 548

pati, y., rezaiifar, r., and krishnaprasad, p. (1993). orthogonal matching pursuit:
recursive function approximation with applications to wavelet decomposition. in pro-
ceedings of the 27 th annual asilomar conference on signals, systems, and computers,
pages 40   44. 252

pearl, j. (1985). id110s: a model of self-activated memory for evidential
in proceedings of the 7th conference of the cognitive science society,

reasoning.
university of california, irvine, pages 329   334. 560

pearl, j. (1988). probabilistic reasoning in intelligent systems: networks of plausible

id136. morgan kaufmann. 52

perron, o. (1907). zur theorie der matrices. mathematische annalen, 64(2), 248   263. 594

petersen, k. b. and pedersen, m. s. (2006). the matrix cookbook. version 20051003. 29

peterson, g. b. (2004). a day of great illumination: b. f. skinner   s discovery of shaping.

journal of the experimental analysis of behavior, 82(3), 317   328. 324

pham, d.-t., garat, p., and jutten, c. (1992). separation of a mixture of independent
sources through a maximum likelihood approach. in eusipco, pages 771   774. 487

757

bibliography

pham, p.-h., jelaca, d., farabet, c., martini, b., lecun, y., and culurciello, e. (2012).
neuflow: data   ow vision processing system-on-a-chip. in circuits and systems (mws-
cas), 2012 ieee 55th international midwest symposium on, pages 1044   1047. ieee.
446

pinheiro, p. h. o. and collobert, r. (2014). recurrent convolutional neural networks for

scene labeling. in icml   2014 . 352, 353

pinheiro, p. h. o. and collobert, r. (2015). from image-level to pixel-level labeling with
convolutional networks. in conference on id161 and pattern recognition
(cvpr). 352, 353

pinto, n., cox, d. d., and dicarlo, j. j. (2008). why is real-world visual object recognition

hard? plos comput biol, 4. 451

pinto, n., stone, z., zickler, t., and cox, d. (2011). scaling up biologically-inspired
id161: a case study in unconstrained face recognition on facebook.
in
id161 and pattern recognition workshops (cvprw), 2011 ieee computer
society conference on, pages 35   42. ieee. 357

pollack, j. b. (1990). recursive distributed representations. arti   cial intelligence, 46(1),

77   105. 394

polyak, b. and juditsky, a. (1992). acceleration of stochastic approximation by averaging.

siam j. control and optimization, 30(4), 838   855. 318

polyak, b. t. (1964). some methods of speeding up the convergence of iteration methods.

ussr computational mathematics and mathematical physics, 4(5), 1   17. 292

poole, b., sohl-dickstein, j., and ganguli, s. (2014). analyzing noise in autoencoders

and deep networks. corr, abs/1406.1831. 237

poon, h. and domingos, p. (2011). sum-product networks: a new deep architecture. in
proceedings of the twenty-seventh conference in uncertainty in arti   cial intelligence
(uai), barcelona, spain. 551

presley, r. k. and haggard, r. l. (1994). a    xed point implementation of the backpropa-
gation learning algorithm. in southeastcon   94. creative technology transfer-a global
a   air., proceedings of the 1994 ieee, pages 136   138. ieee. 446

price, r. (1958). a useful theorem for nonlinear devices having gaussian inputs. ieee

transactions on id205, 4(2), 69   72. 685

quiroga, r. q., reddy, l., kreiman, g., koch, c., and fried, i. (2005). invariant visual
representation by single neurons in the human brain. nature, 435(7045), 1102   1107.
360

758

bibliography

radford, a., metz, l., and chintala, s. (2015). unsupervised representation learning with
deep convolutional id3. arxiv preprint arxiv:1511.06434 .
549, 550, 698, 699

raiko, t., yao, l., cho, k., and bengio, y. (2014).

iterative neural autoregressive

distribution estimator (nade-k). technical report, arxiv:1406.1485. 671, 706

raina, r., madhavan, a., and ng, a. y. (2009). large-scale deep unsupervised learning
using graphics processors. in l. bottou and m. littman, editors, proceedings of the
twenty-sixth international conference on machine learning (icml   09), pages 873   880,
new york, ny, usa. acm. 23, 441

ramsey, f. p. (1926). truth and id203. in r. b. braithwaite, editor, the foundations
of mathematics and other logical essays, chapter 7, pages 156   198. mcmaster university
archive for the history of economic thought. 54

ranzato, m. and hinton, g. h. (2010). modeling pixel means and covariances using

factorized third-order id82s. in cvpr   2010 , pages 2551   2558. 676

ranzato, m., poultney, c., chopra, s., and lecun, y. (2007a). e   cient learning of sparse

representations with an energy-based model. in nips   2006 . 13, 18, 504, 526, 528

ranzato, m., huang, f., boureau, y., and lecun, y. (2007b). unsupervised learning of
invariant feature hierarchies with applications to object recognition. in proceedings of
the id161 and pattern recognition conference (cvpr   07). ieee press. 358

ranzato, m., boureau, y., and lecun, y. (2008). sparse id171 for deep belief

networks. in nips   2007 . 504

ranzato, m., krizhevsky, a., and hinton, g. e. (2010a). factored 3-way restricted
id82s for modeling natural images. in proceedings of aistats 2010 .
675

ranzato, m., mnih, v., and hinton, g. (2010b). generating more realistic images using

gated mrfs. in nips   2010 . 677

rao, c. (1945). information and the accuracy attainable in the estimation of statistical

parameters. bulletin of the calcutta mathematical society, 37, 81   89. 133, 292

rasmus, a., valpola, h., honkala, m., berglund, m., and raiko, t. (2015). semi-supervised

learning with ladder network. arxiv preprint arxiv:1507.02672 . 421, 528

recht, b., re, c., wright, s., and niu, f. (2011). hogwild: a lock-free approach to

parallelizing stochastic id119. in nips   2011 . 442

reichert, d. p., seri  s, p., and storkey, a. j. (2011). neuronal adaptation for sampling-
based probabilistic id136 in perceptual bistability. in advances in neural information
processing systems, pages 2357   2365. 663

759

bibliography

rezende, d. j., mohamed, s., and wierstra, d. (2014). stochastic id26
in icml   2014 . preprint:

and approximate id136 in deep generative models.
arxiv:1401.4082. 650, 685, 693

rifai, s., vincent, p., muller, x., glorot, x., and bengio, y. (2011a). contractive auto-
encoders: explicit invariance during feature extraction. in icml   2011 . 518, 519, 520,
521

rifai, s., mesnil, g., vincent, p., muller, x., bengio, y., dauphin, y., and glorot, x.

(2011b). higher order contractive auto-encoder. in ecml pkdd. 518, 519

rifai, s., dauphin, y., vincent, p., bengio, y., and muller, x. (2011c). the manifold

tangent classi   er. in nips   2011 . 268, 269, 520

rifai, s., bengio, y., dauphin, y., and vincent, p. (2012). a generative process for

sampling contractive auto-encoders. in icml   2012 . 707

ringach, d. and shapley, r. (2004). reverse correlation in neurophysiology. cognitive

science, 28(2), 147   166. 362

roberts, s. and everson, r. (2001). independent component analysis: principles and

practice. cambridge university press. 489

robinson, a. j. and fallside, f. (1991). a recurrent error propagation network speech

recognition system. computer speech and language, 5(3), 259   274. 23, 454

rockafellar, r. t. (1997). convex analysis. princeton landmarks in mathematics. 91

romero, a., ballas, n., ebrahimi kahou, s., chassang, a., gatta, c., and bengio, y.

(2015). fitnets: hints for thin deep nets. in iclr   2015, arxiv:1412.6550 . 321

rosen, j. b. (1960). the gradient projection method for nonid135. part i.
linear constraints. journal of the society for industrial and applied mathematics, 8(1),
pp. 181   217. 91

rosenblatt, f. (1958). the id88: a probabilistic model for information storage and

organization in the brain. psychological review, 65, 386   408. 13, 14, 23

rosenblatt, f. (1962). principles of neurodynamics. spartan, new york. 14, 23

roweis, s. and saul, l. k. (2000). nonlinear id84 by locally linear

embedding. science, 290(5500). 160, 516

roweis, s., saul, l., and hinton, g. (2002). global coordination of local linear models. in
t. dietterich, s. becker, and z. ghahramani, editors, advances in neural information
processing systems 14 (nips   01), cambridge, ma. mit press. 485

rubin, d. b. et al. (1984). bayesianly justi   able and relevant frequency calculations for

the applied statistician. the annals of statistics, 12(4), 1151   1172. 712

760

bibliography

rumelhart, d., hinton, g., and williams, r. (1986a). learning representations by

back-propagating errors. nature, 323, 533   536. 13, 17, 22, 200, 221, 367, 472, 477

rumelhart, d. e., hinton, g. e., and williams, r. j. (1986b). learning internal represen-
tations by error propagation. in d. e. rumelhart and j. l. mcclelland, editors, parallel
distributed processing, volume 1, chapter 8, pages 318   362. mit press, cambridge. 19,
23, 221

rumelhart, d. e., mcclelland, j. l., and the pdp research group (1986c). parallel
distributed processing: explorations in the microstructure of cognition. mit press,
cambridge. 16

russakovsky, o., deng, j., su, h., krause, j., satheesh, s., ma, s., huang, z., karpathy,
a., khosla, a., bernstein, m., berg, a. c., and fei-fei, l. (2014a). id163 large
scale visual recognition challenge. 19

russakovsky, o., deng, j., su, h., krause, j., satheesh, s., ma, s., huang, z., karpathy,
a., khosla, a., bernstein, m., et al. (2014b). id163 large scale visual recognition
challenge. arxiv preprint arxiv:1409.0575 . 24

russel, s. j. and norvig, p. (2003). arti   cial intelligence: a modern approach. prentice

hall. 84

rust, n., schwartz, o., movshon, j. a., and simoncelli, e. (2005). spatiotemporal

elements of macaque v1 receptive    elds. neuron, 46(6), 945   956. 361

sainath, t., mohamed, a., kingsbury, b., and ramabhadran, b. (2013). deep convolu-

tional neural networks for lvcsr. in icassp 2013 . 455

salakhutdinov, r. (2010). learning in markov random    elds using tempered transitions. in
y. bengio, d. schuurmans, c. williams, j. la   erty, and a. culotta, editors, advances
in neural information processing systems 22 (nips   09). 601

salakhutdinov, r. and hinton, g. (2009a). deep id82s. in proceedings of
the international conference on arti   cial intelligence and statistics, volume 5, pages
448   455. 22, 23, 527, 660, 663, 668, 669

salakhutdinov, r. and hinton, g. (2009b). semantic hashing. in international journal of

approximate reasoning. 522

salakhutdinov, r. and hinton, g. e. (2007a). learning a nonlinear embedding by
preserving class neighbourhood structure. in proceedings of the eleventh international
conference on arti   cial intelligence and statistics (aistats   07), san juan, porto
rico. omnipress. 525

salakhutdinov, r. and hinton, g. e. (2007b). semantic hashing. in sigir   2007 . 522

761

bibliography

salakhutdinov, r. and hinton, g. e. (2008). using deep belief nets to learn covariance
kernels for gaussian processes. in j. platt, d. koller, y. singer, and s. roweis, editors,
advances in neural information processing systems 20 (nips   07), pages 1249   1256,
cambridge, ma. mit press. 240

salakhutdinov, r. and larochelle, h. (2010). e   cient learning of deep id82s.
in proceedings of the thirteenth international conference on arti   cial intelligence and
statistics (aistats 2010), jmlr w&cp, volume 9, pages 693   700. 650

salakhutdinov, r. and mnih, a. (2008). probabilistic id105. in nips   2008 .

475

salakhutdinov, r. and murray, i. (2008). on the quantitative analysis of deep belief
networks. in w. w. cohen, a. mccallum, and s. t. roweis, editors, proceedings of
the twenty-   fth international conference on machine learning (icml   08), volume 25,
pages 872   879. acm. 626, 659

salakhutdinov, r., mnih, a., and hinton, g. (2007). restricted id82s for

collaborative    ltering. in icml. 475

sanger, t. d. (1994). neural network learning control of robot manipulators using
gradually increasing task di   culty. ieee transactions on robotics and automation,
10(3). 324

saul, l. k. and jordan, m. i. (1996). exploiting tractable substructures in intractable
networks. in d. touretzky, m. mozer, and m. hasselmo, editors, advances in neural
information processing systems 8 (nips   95). mit press, cambridge, ma. 636

saul, l. k., jaakkola, t., and jordan, m. i. (1996). mean    eld theory for sigmoid belief

networks. journal of arti   cial intelligence research, 4, 61   76. 23, 689

savich, a. w., moussa, m., and areibi, s. (2007). the impact of arithmetic representation
on implementing mlp-bp on fpgas: a study. neural networks, ieee transactions on,
18(1), 240   252. 446

saxe, a. m., koh, p. w., chen, z., bhand, m., suresh, b., and ng, a. (2011). on random

weights and unsupervised id171. in proc. icml   2011 . acm. 357

saxe, a. m., mcclelland, j. l., and ganguli, s. (2013). exact solutions to the nonlinear

dynamics of learning in deep linear neural networks. in iclr. 282, 283, 299

schaul, t., antonoglou, i., and silver, d. (2014). unit tests for stochastic optimization.

in international conference on learning representations. 306

schmidhuber, j. (1992). learning complex, extended sequences using the principle of

history compression. neural computation, 4(2), 234   242. 394

schmidhuber, j. (2012). self-delimiting neural networks. arxiv preprint arxiv:1210.0118 .

384

762

bibliography

schmidhuber, j. and heil, s. (1996). sequential neural text compression. ieee transac-

tions on neural networks, 7(1), 142   146. 472

sch  lkopf, b. and smola, a. j. (2002). learning with kernels: support vector machines,

id173, optimization, and beyond. mit press. 700

sch  lkopf, b., smola, a., and m  ller, k.-r. (1998). nonlinear component analysis as a

kernel eigenvalue problem. neural computation, 10, 1299   1319. 160, 516

sch  lkopf, b., burges, c. j. c., and smola, a. j. (1999). advances in kernel methods    

support vector learning. mit press, cambridge, ma. 17, 140

sch  lkopf, b., janzing, d., peters, j., sgouritsa, e., zhang, k., and mooij, j. (2012). on

causal and anticausal learning. in icml   2012 , pages 1255   1262. 543

schuster, m. (1999). on supervised learning from sequential data with applications for

id103. 186

schuster, m. and paliwal, k. (1997). id182. ieee

transactions on signal processing, 45(11), 2673   2681. 388

schwenk, h. (2007). continuous space language models. computer speech and language,

21, 492   518. 461

schwenk, h. (2010). continuous space language models for id151.

the prague bulletin of mathematical linguistics, 93, 137   146. 468

schwenk, h. (2014). cleaned subset of wmt    14 dataset. 19

schwenk, h. and bengio, y. (1998). training methods for adaptive boosting of neural net-
works. in m. jordan, m. kearns, and s. solla, editors, advances in neural information
processing systems 10 (nips   97), pages 647   653. mit press. 255

schwenk, h. and gauvain, j.-l. (2002). connectionist id38 for large
vocabulary continuous id103. in international conference on acoustics,
speech and signal processing (icassp), pages 765   768, orlando, florida. 461

schwenk, h., costa-juss  , m. r., and fonollosa, j. a. r. (2006). continuous space
in international workshop on spoken

language models for the iwslt 2006 task.
language translation, pages 166   173. 468

seide, f., li, g., and yu, d. (2011). conversational speech transcription using context-

dependent deep neural networks. in interspeech 2011 , pages 437   440. 24

sejnowski, t. (1987). higher-order id82s. in aip conference proceedings
151 on neural networks for computing, pages 398   403. american institute of physics
inc. 683

763

bibliography

series, p., reichert, d. p., and storkey, a. j. (2010). hallucinations in charles bonnet
syndrome induced by homeostasis: a deep id82 model. in advances in
neural information processing systems, pages 2020   2028. 663

sermanet, p., chintala, s., and lecun, y. (2012). convolutional neural networks applied

to house numbers digit classi   cation. corr, abs/1204.3968. 452

sermanet, p., kavukcuoglu, k., chintala, s., and lecun, y. (2013). pedestrian detection
with unsupervised multi-stage id171. in proc. international conference on
id161 and pattern recognition (cvpr   13). ieee. 24, 197

shilov, g. (1977). id202. dover books on mathematics series. dover publications.

29

siegelmann, h. (1995). computation beyond the turing limit. science, 268(5210),

545   548. 372

siegelmann, h. and sontag, e. (1991). turing computability with neural nets. applied

mathematics letters, 4(6), 77   80. 372

siegelmann, h. t. and sontag, e. d. (1995). on the computational power of neural nets.

journal of computer and systems sciences, 50(1), 132   150. 372, 399

sietsma, j. and dow, r. (1991). creating arti   cial neural networks that generalize. neural

networks, 4(1), 67   79. 237

simard, d., steinkraus, p. y., and platt, j. c. (2003). best practices for convolutional

neural networks. in icdar   2003 . 365

simard, p. and graf, h. p. (1994). id26 without multiplication. in advances

in neural information processing systems, pages 232   239. 446

simard, p., victorri, b., lecun, y., and denker, j. (1992). tangent prop - a formalism
for specifying selected invariances in an adaptive network. in nips   1991 . 267, 268, 269,
350

simard, p. y., lecun, y., and denker, j. (1993). e   cient pattern recognition using a

new transformation distance. in nips   92 . 267

simard, p. y., lecun, y. a., denker, j. s., and victorri, b. (1998). transformation
invariance in pattern recognition     tangent distance and tangent propagation. lecture
notes in computer science, 1524. 267

simons, d. j. and levin, d. t. (1998). failure to detect changes to people during a

real-world interaction. psychonomic bulletin & review, 5(4), 644   649. 541

simonyan, k. and zisserman, a. (2015). very deep convolutional networks for large-scale

image recognition. in iclr. 319

764

bibliography

sj  berg, j. and ljung, l. (1995). overtraining, id173 and searching for a minimum,
with application to neural networks. international journal of control, 62(6), 1391   1407.
247

skinner, b. f. (1958). reinforcement today. american psychologist, 13, 94   99. 324

smolensky, p. (1986). information processing in dynamical systems: foundations of
harmony theory. in d. e. rumelhart and j. l. mcclelland, editors, parallel distributed
processing, volume 1, chapter 6, pages 194   281. mit press, cambridge. 568, 584, 653

snoek, j., larochelle, h., and adams, r. p. (2012). practical bayesian optimization of

machine learning algorithms. in nips   2012 . 430

socher, r., huang, e. h., pennington, j., ng, a. y., and manning, c. d. (2011a). dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. in nips   2011 .
394, 396

socher, r., manning, c., and ng, a. y. (2011b). parsing natural scenes and natural lan-
guage with id56s. in proceedings of the twenty-eighth international
conference on machine learning (icml   2011). 394

socher, r., pennington, j., huang, e. h., ng, a. y., and manning, c. d. (2011c).
in

semi-supervised recursive autoencoders for predicting sentiment distributions.
emnlp   2011 . 394

socher, r., perelygin, a., wu, j. y., chuang, j., manning, c. d., ng, a. y., and potts,
c. (2013a). recursive deep models for semantic compositionality over a sentiment
treebank. in emnlp   2013 . 394, 396

socher, r., ganjoo, m., manning, c. d., and ng, a. y. (2013b). zero-shot learning through
cross-modal transfer. in 27th annual conference on neural information processing
systems (nips 2013). 537

sohl-dickstein, j., weiss, e. a., maheswaranathan, n., and ganguli, s. (2015). deep

unsupervised learning using nonequilibrium thermodynamics. 712

sohn, k., zhou, g., and lee, h. (2013). learning and selecting features jointly with

point-wise gated id82s. in icml   2013 . 683

solomono   , r. j. (1989). a system for incremental learning based on algorithmic proba-

bility. 324

sontag, e. d. (1998). vc dimension of neural networks. nato asi series f computer

and systems sciences, 168, 69   96. 545, 549

sontag, e. d. and sussman, h. j. (1989). id26 can give rise to spurious local

minima even for networks without hidden layers. complex systems, 3, 91   106. 281

765

bibliography

sparkes, b. (1996). the red and the black: studies in greek pottery. routledge. 1

spitkovsky, v. i., alshawi, h., and jurafsky, d. (2010). from baby steps to leapfrog: how

   less is more    in unsupervised id33. in hlt   10 . 324

squire, w. and trapp, g. (1998). using complex variables to estimate derivatives of real

functions. siam rev., 40(1), 110      112. 434

srebro, n. and shraibman, a. (2005). rank, trace-norm and max-norm. in proceedings of
the 18th annual conference on learning theory, pages 545   560. springer-verlag. 235

srivastava, n. (2013). improving neural networks with dropout. master   s thesis, u.

toronto. 533

srivastava, n. and salakhutdinov, r. (2012). multimodal learning with deep boltzmann

machines. in nips   2012 . 539

srivastava, n., salakhutdinov, r. r., and hinton, g. e. (2013). modeling documents with

deep id82s. arxiv preprint arxiv:1309.6865 . 660

srivastava, n., hinton, g., krizhevsky, a., sutskever, i., and salakhutdinov, r. (2014).
dropout: a simple way to prevent neural networks from over   tting. journal of machine
learning research, 15, 1929   1958. 255, 261, 262, 264, 669

srivastava, r. k., gre   , k., and schmidhuber, j. (2015). id199.

arxiv:1505.00387 . 322

steinkrau, d., simard, p. y., and buck, i. (2005). using gpus for machine learning
algorithms. 2013 12th international conference on document analysis and recognition,
0, 1115   1119. 440

stoyanov, v., ropson, a., and eisner, j. (2011). empirical risk minimization of graphical
model parameters given approximate id136, decoding, and model structure. in
proceedings of the 14th international conference on arti   cial intelligence and statistics
(aistats), volume 15 of jmlr workshop and conference proceedings, pages 725   733,
fort lauderdale. supplementary material (4 pages) also available. 671, 695

sukhbaatar, s., szlam, a., weston, j., and fergus, r. (2015). weakly supervised memory

networks. arxiv preprint arxiv:1503.08895 . 412

supancic, j. and ramanan, d. (2013). self-paced learning for long-term tracking. in

cvpr   2013 . 324

sussillo, d. (2014). id93: training very deep nonlinear feed-forward networks

with smart initialization. corr, abs/1412.6558. 287, 300, 302, 398

sutskever, i. (2012). training recurrent neural networks. ph.d. thesis, department of

computer science, university of toronto. 401, 408

766

bibliography

sutskever, i. and hinton, g. e. (2008). deep narrow sigmoid belief networks are universal

approximators. neural computation, 20(11), 2629   2636. 689

sutskever, i. and tieleman, t. (2010). on the convergence properties of contrastive
divergence. in y. w. teh and m. titterington, editors, proc. of the international
conference on arti   cial intelligence and statistics (aistats), volume 9, pages 789   795.
610

sutskever, i., hinton, g., and taylor, g. (2009). the recurrent temporal restricted

id82. in nips   2008 . 682

sutskever, i., martens, j., and hinton, g. e. (2011). generating text with recurrent

neural networks. in icml   2011 , pages 1017   1024. 472

sutskever, i., martens, j., dahl, g., and hinton, g. (2013). on the importance of

initialization and momentum in deep learning. in icml. 296, 401, 408

sutskever, i., vinyals, o., and le, q. v. (2014). sequence to sequence learning with

neural networks. in nips   2014, arxiv:1409.3215 . 25, 99, 390, 404, 407, 469, 470

sutton, r. and barto, a. (1998). id23: an introduction. mit press.

104

sutton, r. s., mcallester, d., singh, s., and mansour, y. (2000). id189
for id23 with function approximation. in nips   1999 , pages 1057   
   1063. mit press. 688

swersky, k., ranzato, m., buchman, d., marlin, b., and de freitas, n. (2011). on
autoencoders and score matching for energy based models. in icml   2011 . acm. 509

swersky, k., snoek, j., and adams, r. p. (2014). freeze-thaw bayesian optimization.

arxiv preprint arxiv:1406.3896 . 431

szegedy, c., liu, w., jia, y., sermanet, p., reed, s., anguelov, d., erhan, d., vanhoucke,
v., and rabinovich, a. (2014a). going deeper with convolutions. technical report,
arxiv:1409.4842. 22, 23, 197, 255, 265, 322, 341

szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d., goodfellow, i. j., and
fergus, r. (2014b). intriguing properties of neural networks. iclr, abs/1312.6199.
265, 266, 269

szegedy, c., vanhoucke, v., io   e, s., shlens, j., and wojna, z. (2015). rethinking the

inception architecture for id161. arxiv e-prints. 240, 318

taigman, y., yang, m., ranzato, m., and wolf, l. (2014). deepface: closing the gap to

human-level performance in face veri   cation. in cvpr   2014 . 98

tandy, d. w. (1997). works and days: a translation and commentary for the social

sciences. university of california press. 1

767

bibliography

tang, y. and eliasmith, c. (2010). deep networks for robust visual recognition. in
proceedings of the 27th international conference on machine learning, june 21-24,
2010, haifa, israel. 237

tang, y., salakhutdinov, r., and hinton, g. (2012). deep mixtures of factor analysers.

arxiv preprint arxiv:1206.4635 . 485

taylor, g. and hinton, g. (2009). factored conditional restricted id82s
for modeling motion style.
in l. bottou and m. littman, editors, proceedings of
the twenty-sixth international conference on machine learning (icml   09), pages
1025   1032, montreal, quebec, canada. acm. 682

taylor, g., hinton, g. e., and roweis, s. (2007). modeling human motion using binary
latent variables. in b. sch  lkopf, j. platt, and t. ho   man, editors, advances in neural
information processing systems 19 (nips   06), pages 1345   1352. mit press, cambridge,
ma. 682

teh, y., welling, m., osindero, s., and hinton, g. e. (2003). energy-based models
for sparse overcomplete representations. journal of machine learning research, 4,
1235   1260. 487

tenenbaum, j., de silva, v., and langford, j. c. (2000). a global geometric framework
for nonlinear id84. science, 290(5500), 2319   2323. 160, 516, 532

theis, l., van den oord, a., and bethge, m. (2015). a note on the evaluation of generative

models. arxiv:1511.01844. 694, 715

thompson, j., jain, a., lecun, y., and bregler, c. (2014). joint training of a convolutional

network and a graphical model for human pose estimation. in nips   2014 . 353

thrun, s. (1995). learning to play the game of chess. in nips   1994 . 269

tibshirani, r. j. (1995). regression shrinkage and selection via the lasso. journal of the

royal statistical society b, 58, 267   288. 233

tieleman, t. (2008). training restricted id82s using approximations to
the likelihood gradient. in w. w. cohen, a. mccallum, and s. t. roweis, editors, pro-
ceedings of the twenty-   fth international conference on machine learning (icml   08),
pages 1064   1071. acm. 610

tieleman, t. and hinton, g. (2009). using fast weights to improve persistent contrastive
divergence. in l. bottou and m. littman, editors, proceedings of the twenty-sixth
international conference on machine learning (icml   09), pages 1033   1040. acm.
612

tipping, m. e. and bishop, c. m. (1999). probabilistic principal components analysis.

journal of the royal statistical society b, 61(3), 611   622. 487

768

bibliography

torralba, a., fergus, r., and weiss, y. (2008). small codes and large databases for
recognition. in proceedings of the id161 and pattern recognition conference
(cvpr   08), pages 1   8. 522, 523

touretzky, d. s. and minton, g. e. (1985). symbols among the neurons: details of
a connectionist id136 architecture. in proceedings of the 9th international joint
conference on arti   cial intelligence - volume 1 , ijcai   85, pages 238   243, san francisco,
ca, usa. morgan kaufmann publishers inc. 16

tu, k. and honavar, v. (2011). on the utility of curricula in unsupervised learning of

probabilistic grammars. in ijcai   2011 . 324

turaga, s. c., murray, j. f., jain, v., roth, f., helmstaedter, m., briggman, k., denk,
w., and seung, h. s. (2010). convolutional networks can learn to generate a   nity
graphs for image segmentation. neural computation, 22(2), 511   538. 353

turian, j., ratinov, l., and bengio, y. (2010). word representations: a simple and
general method for semi-supervised learning. in proc. acl   2010 , pages 384   394. 533

t  scher, a., jahrer, m., and bell, r. m. (2009). the bigchaos solution to the net   ix

grand prize. 475

uria, b., murray, i., and larochelle, h. (2013). rnade: the real-valued neural autoregres-

sive density-estimator. in nips   2013 . 706

uria, b., murray, i., and larochelle, h. (2014). a deep and tractable density estimator.

in icml   2014 . 186, 706, 707

van den o  rd, a., dieleman, s., and schrauwen, b. (2013). deep content-based music

recommendation. in nips   2013 . 475

van der maaten, l. and hinton, g. e. (2008). visualizing data using id167. j. machine

learning res., 9. 473, 516

vanhoucke, v., senior, a., and mao, m. z. (2011). improving the speed of neural networks
on cpus. in proc. deep learning and unsupervised id171 nips workshop.
439, 447

vapnik, v. n. (1982). estimation of dependences based on empirical data. springer-

verlag, berlin. 112

vapnik, v. n. (1995). the nature of statistical learning theory. springer, new york.

112

vapnik, v. n. and chervonenkis, a. y. (1971). on the uniform convergence of relative
frequencies of events to their probabilities. theory of id203 and its applications,
16, 264   280. 112

769

bibliography

vincent, p. (2011). a connection between score matching and denoising autoencoders.

neural computation, 23(7). 509, 511, 708

vincent, p. and bengio, y. (2003). manifold parzen windows. in nips   2002 . mit press.

517

vincent, p., larochelle, h., bengio, y., and manzagol, p.-a. (2008). extracting and

composing robust features with denoising autoencoders. in icml 2008 . 237, 511

vincent, p., larochelle, h., lajoie, i., bengio, y., and manzagol, p.-a. (2010). stacked
denoising autoencoders: learning useful representations in a deep network with a local
denoising criterion. j. machine learning res., 11. 511

vincent, p., de br  bisson, a., and bouthillier, x. (2015). e   cient exact gradient update
for training deep networks with very large sparse targets. in c. cortes, n. d. lawrence,
d. d. lee, m. sugiyama, and r. garnett, editors, advances in neural information
processing systems 28 , pages 1108   1116. curran associates, inc. 460

vinyals, o., kaiser, l., koo, t., petrov, s., sutskever, i., and hinton, g. (2014a).

grammar as a foreign language. technical report, arxiv:1412.7449. 404

vinyals, o., toshev, a., bengio, s., and erhan, d. (2014b). show and tell: a neural image

caption generator. arxiv 1411.4555. 404

vinyals, o., fortunato, m., and jaitly, n. (2015a). id193. arxiv preprint

arxiv:1506.03134 . 412

vinyals, o., toshev, a., bengio, s., and erhan, d. (2015b). show and tell: a neural image

caption generator. in cvpr   2015 . arxiv:1411.4555. 100

viola, p. and jones, m. (2001). robust real-time id164. in international

journal of id161. 444

visin, f., kastner, k., cho, k., matteucci, m., courville, a., and bengio, y. (2015).
renet: a recurrent neural network based alternative to convolutional networks. arxiv
preprint arxiv:1505.00393 . 390

von melchner, l., pallas, s. l., and sur, m. (2000). visual behaviour mediated by retinal

projections directed to the auditory pathway. nature, 404(6780), 871   876. 15

wager, s., wang, s., and liang, p. (2013). dropout training as adaptive id173.

in advances in neural information processing systems 26 , pages 351   359. 262

waibel, a., hanazawa, t., hinton, g. e., shikano, k., and lang, k. (1989). phoneme
recognition using time-delay neural networks. ieee transactions on acoustics, speech,
and signal processing, 37, 328   339. 368, 448, 454

wan, l., zeiler, m., zhang, s., lecun, y., and fergus, r. (2013). id173 of neural

networks using dropconnect. in icml   2013 . 263

770

bibliography

wang, s. and manning, c. (2013). fast dropout training. in icml   2013 . 263

wang, z., zhang, j., feng, j., and chen, z. (2014a). id13 and text jointly

embedding. in proc. emnlp   2014 . 479

wang, z., zhang, j., feng, j., and chen, z. (2014b). id13 embedding by

translating on hyperplanes. in proc. aaai   2014 . 479

warde-farley, d., goodfellow, i. j., courville, a., and bengio, y. (2014). an empirical

analysis of dropout in piecewise linear networks. in iclr   2014 . 259, 263, 264

wawrzynek, j., asanovic, k., kingsbury, b., johnson, d., beck, j., and morgan, n.

(1996). spert-ii: a vector microprocessor system. computer, 29(3), 79   86. 446

weaver, l. and tao, n. (2001). the optimal reward baseline for gradient-based reinforce-

ment learning. in proc. uai   2001 , pages 538   545. 688

weinberger, k. q. and saul, l. k. (2004). unsupervised learning of image manifolds by

semide   nite programming. in cvpr   2004 , pages 988   995. 160, 516

weiss, y., torralba, a., and fergus, r. (2008). spectral hashing.

1753   1760. 523

in nips, pages

welling, m., zemel, r. s., and hinton, g. e. (2002). self supervised boosting. in advances

in neural information processing systems, pages 665   672. 699

welling, m., hinton, g. e., and osindero, s. (2003a). learning sparse topographic

representations with products of student-t distributions. in nips   2002 . 677

welling, m., zemel, r., and hinton, g. e. (2003b). self-supervised boosting. in s. becker,
s. thrun, and k. obermayer, editors, advances in neural information processing
systems 15 (nips   02), pages 665   672. mit press. 621

welling, m., rosen-zvi, m., and hinton, g. e. (2005). exponential family harmoniums
with an application to information retrieval. in l. saul, y. weiss, and l. bottou,
editors, advances in neural information processing systems 17 (nips   04), volume 17,
cambridge, ma. mit press. 673

werbos, p. j. (1981). applications of advances in nonlinear sensitivity analysis.

proceedings of the 10th ifip conference, 31.8 - 4.9, nyc , pages 762   770. 221

in

weston, j., bengio, s., and usunier, n. (2010). large scale image annotation: learning to

rank with joint word-image embeddings. machine learning, 81(1), 21   35. 396

weston, j., chopra, s., and bordes, a. (2014). memory networks. arxiv preprint

arxiv:1410.3916 . 412, 480

widrow, b. and ho   , m. e. (1960). adaptive switching circuits. in 1960 ire wescon

convention record, volume 4, pages 96   104. ire, new york. 14, 19, 22, 23

771

bibliography

wikipedia (2015). list of animals by number of neurons     wikipedia, the free encyclopedia.

[online; accessed 4-march-2015]. 22, 23

williams, c. k. i. and agakov, f. v. (2002). products of gaussians and probabilistic

minor component analysis. neural computation, 14(5), 1169   1182. 679

williams, c. k. i. and rasmussen, c. e. (1996). gaussian processes for regression. in
d. touretzky, m. mozer, and m. hasselmo, editors, advances in neural information
processing systems 8 (nips   95), pages 514   520. mit press, cambridge, ma. 140

williams, r. j. (1992). simple statistical gradient-following algorithms connectionist

id23. machine learning, 8, 229   256. 685, 686

williams, r. j. and zipser, d. (1989). a learning algorithm for continually running fully

recurrent neural networks. neural computation, 1, 270   280. 219

wilson, d. r. and martinez, t. r. (2003). the general ine   ciency of batch training for

id119 learning. neural networks, 16(10), 1429   1451. 276

wilson, j. r. (1984). variance reduction techniques for digital simulation. american

journal of mathematical and management sciences, 4(3), 277      312. 687

wiskott, l. and sejnowski, t. j. (2002). slow feature analysis: unsupervised learning of

invariances. neural computation, 14(4), 715   770. 489, 490

wolpert, d. and macready, w. (1997). no free lunch theorems for optimization. ieee

transactions on evolutionary computation, 1, 67   82. 289

wolpert, d. h. (1996). the lack of a priori distinction between learning algorithms. neural

computation, 8(7), 1341   1390. 114

wu, r., yan, s., shan, y., dang, q., and sun, g. (2015). deep image: scaling up image

recognition. arxiv:1501.02876. 442

wu, z. (1997). global continuation for distance geometry problems. siam journal of

optimization, 7, 814   836. 323

xiong, h. y., barash, y., and frey, b. j. (2011). bayesian prediction of tissue-regulated
splicing using rna sequence and cellular context. bioinformatics, 27(18), 2554   2562.
262

xu, k., ba, j. l., kiros, r., cho, k., courville, a., salakhutdinov, r., zemel, r. s., and
bengio, y. (2015). show, attend and tell: neural image id134 with visual
attention. in icml   2015, arxiv:1502.03044 . 100, 404, 688

yildiz, i. b., jaeger, h., and kiebel, s. j. (2012). re-visiting the echo state property.

neural networks, 35, 1   9. 400

772

bibliography

yosinski, j., clune, j., bengio, y., and lipson, h. (2014). how transferable are features

in deep neural networks? in nips   2014 . 321, 534

younes, l. (1998). on the convergence of markovian stochastic algorithms with rapidly
decreasing ergodicity rates. in stochastics and stochastics models, pages 177   228. 610

yu, d., wang, s., and deng, l. (2010). sequential labeling using deep-structured
conditional random    elds. ieee journal of selected topics in signal processing. 319

zaremba, w. and sutskever, i. (2014). learning to execute. arxiv 1410.4615. 325

zaremba, w. and sutskever, i. (2015). id23 id63s.

arxiv:1505.00521 . 415

zaslavsky, t. (1975). facing up to arrangements: face-count formulas for partitions
of space by hyperplanes. number no. 154 in memoirs of the american mathematical
society. american mathematical society. 548

zeiler, m. d. and fergus, r. (2014). visualizing and understanding convolutional networks.

in eccv   14 . 6

zeiler, m. d., ranzato, m., monga, r., mao, m., yang, k., le, q., nguyen, p., senior,
a., vanhoucke, v., dean, j., and hinton, g. e. (2013). on recti   ed linear units for
speech processing. in icassp 2013 . 454

zhou, b., khosla, a., lapedriza, a., oliva, a., and torralba, a. (2015). object detectors

emerge in deep scene id98s. iclr   2015, arxiv:1412.6856. 549

zhou, j. and troyanskaya, o. g. (2014). deep supervised and convolutional generative

stochastic network for protein secondary structure prediction. in icml   2014 . 711

zhou, y. and chellappa, r. (1988). computation of optical    ow using a neural network.
in neural networks, 1988., ieee international conference on, pages 71   78. ieee. 335

z  hrer, m. and pernkopf, f. (2014). general stochastic networks for classi   cation. in

nips   2014 . 711

773

