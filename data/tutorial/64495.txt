graph representation 

learning
jure leskovec

networks

jure leskovec, stanford

2

why networks?

universal language for describing complex data
   networks from science, nature, and technology 

are more similar than one would expect 

data availability (+computational challenges)

   web/mobile, bio, health, and medical

shared vocabulary between fields

   computer science, social science, physics, 

statistics, biology

impact!

   social networking, social media, drug design

jure leskovec, stanford

3

many data are networks

social networks

economic networks

b

biomedical networks

a

c

information networks: 

web & citations

internet

jure leskovec, stanford university

networks of neurons

figure 3: higher-order cluster in the c. elegans neuronal network (28). a: the 4-node
   bi-fan    motif, which is over-expressed in the neuronal networks (1). intuitively, this motif
describes a cooperative propagation of information from the nodes on the left to the nodes on
the right. b: the best higher-order cluster in the c. elegans frontal neuronal network based on
the motif in (a). the cluster contains three ring motor neurons (rmel/v/r; cyan) with many

4

networks: common language

movie 1

actor 1

movie 2

actor 2
movie 3

actor 4

friend

co-worker

mary

peter

brothers

friend

tom

actor 3

albert

protein 1

protein 2

protein 5

protein 9

|n|=4
|e|=4

jure leskovec, stanford university

5

tasks on networks

classical ml tasks in networks:
   node classification

   predict a type of a given node

   link prediction

   predict whether two nodes are linked

   community detection

   identify densely linked clusters of nodes

   network similarity

   how similar are two (sub)networks

jure leskovec, stanford university

6

example: node classification

?

?

?

?

?

machine 
learning

many possible ways to create node features:
   node degree, id95 score, motifs,    
   degree of neighbors, id95 of 

neighbors,    

jure leskovec, stanford university

7

machine learning lifecycle

network 

data

node 
features

learning 
algorithm  

model

feature 
engineering

automatically 
learn the features

downstream 
prediction task

(supervised) machine learning lifecycle: 
this feature, that feature. 
every single time!

jure leskovec, stanford university

8

id171 in graphs
this talk: id171 

for networks!

node!:#      &

u

vector

   &

feature representation, 

embedding

jure leskovec, stanford university

9

what is network embedding?

why learn embeddings?
    we map each node in a network into a low-
the goal is to map each node into a 
dimensional space
low-dimensional space
    distributed representation for nodes
   distributed representation for nodes
    similarity between nodes indicate the link 
   similarity between nodes indicates link 
strength
strength 
   encodes network information and generate 
    encode network information and generate node 
node representation
representation

jure leskovec, stanford university

17

10

example
example
    zachary   s karate network:

   zachary   s karate club network:

deepwalk: online learning of social representations. b. perozzi, r. al-rfou, s skiena, kdd 2014.

jure leskovec, stanford university

11

why is it hard?

images have fixed 2d structure
   can define convolutions (id98s)

jure leskovec, stanford university

12

why is it hard?

text and speech have linear 1d structure

   can define sliding windows

but graphs are non-euclidean!

   graphs have arbitrary size
   node numbering is arbitrary 
(node isomorphism problem)
   much more complex structure

graph-structured data

what if our data looks like this?

input

jure leskovec, stanford university

13

this talk: outline
id171 for networks:
1)    linearizing    the graph

   create a    sentence    for each node 

using id93
   node2vec

2) graph convolution networks

   propagate information between the 

nodes of the graph
   graphsage

jure leskovec, stanford university

14

node2vec: 
unsupervised 
id171

node2vec: scalable id171 for networks
a. grover, j. leskovec. kdd 2016.
predicting multicellular function through multi-layer tissue networks. 
m. zitnik, j. leskovec. bioinformatics, 33 (14): i190-i198, 2017.

jure leskovec, stanford university

15

unsupervised id171
   intuition: find embedding of nodes to 

!-dimensions that preserves similarity
   given a node ", how do we define 
  #$"     neighbourhood of " obtained 
by some strategy %

   idea: learn node embedding such 

that nearby nodes are close together

nearby nodes?

jure leskovec, stanford university

16

id171 as optimization

   given !=($,&)
   goal is to learn (:*      -
   where ( is a table lookup
   we directly    learn    coordinates ((*) of *
   given node *, we want to learn feature 
representation ((*) that is predictive of 
nodes in *   s neighborhood ./(*)
max3 4logpr(./(*)|	(*)
<	   >

jure leskovec, stanford university

17

(1)

max

two standard assumptions:

in order to make the optimization problem tractable, we make

f is a matrix of size |v |     d parameters. for every source node
[25, 28]. we seek to optimize the following objective function,
in order to make the optimization problem tractable, we make
u 2 v , we de   ne ns(u)     v as a network neighborhood of node
which predicts which nodes are members of u   s network neighbor-
two standard assumptions:
unsupervised id171
u generated through a neighborhood sampling strategy s.
hood ns(u) based on the learned node features f:
    conditional independence. we factorize the likelihood by as-
we proceed by extending the skip-gram architecture to networks
suming that the likelihood of observing a neighborhood node
log p r(ns(u)|f (u))
[25, 28]. we seek to optimize the following objective function,
is independent of observing any other neighborhood node
given the feature representation of the source.
which predicts which nodes are members of u   s network neighbor-
hood ns(u) based on the learned node features f:
p r(ni|f (u))
log p r(ns(u)|f (u))

goal: find embedding !(#) that predicts 
nearby nodes %&# :
f xu2v
p r(ns(u)|f (u)) = yni2ns (u)
f xu2v
p r(ns(u)|f (u)) = yni2ns (u)
estimate !(#) using stochastic id119.
pv2v exp(f (v)    f (u))

    conditional independence. we factorize the likelihood by as-
suming that the likelihood of observing a neighborhood node
    symmetry in feature space. a source node and neighbor-
is independent of observing any other neighborhood node
assume conditional likelihood factorizes:
hood node have a symmetric effect over each other in fea-
given the feature representation of the source.
ture space. accordingly, we model the conditional likeli-
hood of every source-neighborhood node pair as a softmax
two standard assumptions:
unit parametrized by a dot product of their features.
then softmax:
    conditional independence. we factorize the likelihood by as-
suming that the likelihood of observing a neighborhood node
    symmetry in feature space. a source node and neighbor-
hood node have a symmetric effect over each other in fea-
is independent of observing any other neighborhood node
ture space. accordingly, we model the conditional likeli-
given the feature representation of the source.
with the above assumptions, the objective in eq. 1 simpli   es to:
hood of every source-neighborhood node pair as a softmax

in order to make the optimization problem tractable, we make

exp(f (ni)    f (u))

p r(ni|f (u)) =

p r(ni|f (u))

max

jure leskovec, stanford university

18

between two kinds of similarities: structural equivalence and ho-
mophily [13]. under the homophily hypothesis [7, 42] nodes that
are highly interconnected and belong to similar network clusters
or communities tend to share features and thus should be embed-
ded closely together (e.g., nodes s1 and u in fig. 1 belong to the
same network community). in contrast, under the structural equiv-
alence [11, 40] nodes that have similar structural roles in networks
tend to share features and thus should be embedded closely together
(e.g., nodes u and s6 in fig. 1 act as hubs of their corresponding
communities).
lence does not emphasize connectivity; nodes could be far apart in
the network and still have the same structural role. in real-world,
these equivalence notions are not exclusive; networks commonly
exhibit both behaviors where properties of some nodes exhibit ho-
mophily while others re   ect structural equivalence.

in producing representations that re   ect either of the equivalences.
in particular, the neighborhoods sampled by the bfs lead to em-
beddings that correspond to structural equivalence. intuitively, we

jure@cs.stanford.edu

jure leskovec
stanford university

two classic strategies to define a 

how to determine !"#
neighborhood !"# of a given node #:
!$%"# ={	)*,),,)-}
!/%"# ={	)0,)1,)2}

and edges. a typical solution involves hand-engineering domain-
speci   c features based on expert knowledge. even if one discounts
the tedious work of feature engineering, such features are usually
designed for speci   c tasks and do not generalize across different

s5 
local microscopic view
global macroscopic view

figure 1: bfs and dfs search strategies from node u (k = 3).

bfs 
dfs 

jure leskovec, stanford university

s3 

s4 

s9 

s6 

s7 

s2 

s8 

s1 

u 

19

prediction tasks over nodes and edges in networks require careful
effort in engineering features for learning algorithms. recent re-
search in the broader    eld of representation learning has led to sig-
ni   cant progress in automating prediction by learning the features
themselves. however, present approaches are largely insensitive to

here we propose node2vec, an algorithmic framework for learn-
ing feature representations for nodes in networks. in node2vec, we
learn a mapping of nodes to a low-dimensional space of features
that maximizes the likelihood of preserving distances between net-
work neighborhoods of nodes. we de   ne a    exible notion of node   s
network neighborhood and design a biased random walk proce-

bfs vs. dfs

u

bfs:

micro-view of 
neighbourhood

dfs:

macro-view of 
neighbourhood

jure leskovec, stanford university

20

interpolating bfs and dfs

biased random walk ! that given a node 
" generates neighborhood #$"
   return parameter %:
   in-out parameter &:
   intuitively, & is the    ratio    of bfs vs. dfs

   moving outwards (dfs) vs. inwards (bfs)

   two parameters:

   return back to the previous node

jure leskovec, stanford university

21

biased id93

biased 2nd-order id93 explore 
network neighborhoods:
same distance to #

   rnd. walk started at ! and is now at "
   insight: neighbors of " can only be:
farther from #

s2

s3

u

s1

w

closer to #

idea: remember where that walk came from

jure leskovec, stanford university

22

biased id93

   walker is at w. where to go next?
1
1/",1/$,1 are 
1/$
1/"
  ",$ model transition probabilities
   "     return parameter
   $        walk away    parameter

unnormalized
probabilities

s2

s3

s1

w

u

jure leskovec, stanford university

23

biased id93

   walker is at w. where to go next?
1/"11/#
w    s1
   bfs-like walk: low value of "
   dfs-like walk: low value of #
$%(') are the nodes visited by the walker

1
1/"

1/#

s2
s3

unnormalized
transition prob.

s2

s3

s1

w

u

jure leskovec, stanford university

24

node2vec algorithm

1) simulate ! id93 of length "
starting from each node #

2) optimize the node2vec objective using 
stochastic id119

linear-time complexity
all 3 steps are individually parallelizable

jure leskovec, stanford university

25

experiments: micro vs. macro
network of character interactions in a novel

spectral id91
deepwalk
line
node2vec
node2vec settings (p,q)
gain of node2vec [%]

0.0405
0.2110
0.0784
0.2581
0.25, 0.25

0.0681
0.1768
0.1447
0.1791
4, 1
1.3

blogcatalog

algorithm

dataset

22.3

0.0395
0.1274
0.1164
0.1552
4, 0.5
21.8

ppi wikipedia

!=1,%=2

figure 3: complementary visualizations of les mis  rables co-
appearance network generated by node2vec with label colors
re   ecting homophily (top) and structural equivalence (bottom).

microscopic view of the 
network neighbourhood

also exclude a recent approach, grarep [6], that generalizes line
to incorporate information from network neighborhoods beyond 2-
hops, but does not scale and hence, provides an unfair comparison
with other neural embedding based id171 methods. apart
from spectral id91 which has a slightly higher time complex-
ity since it involves id105, our experiments stand out

jure leskovec, stanford university

table 2: macro-f1 scores for multilabel classi   cation on blog-
catalog, ppi (homo sapiens) and wikipedia word cooccur-
rence networks with a balanced 50% train-test split.
labeled data with a grid search over p, q 2 {0.25, 0.50, 1, 2, 4}.
under the above experimental settings, we present our results for
two tasks under consideration.
4.3 multi-label classi   cation

!=1,%=0.5

in the multi-label classi   cation setting, every node is assigned
one or more labels from a    nite set l. during the training phase, we
observe a certain fraction of nodes and all their labels. the task is
to predict the labels for the remaining nodes. this is a challenging
task especially if l is large. we perform multi-label classi   cation
on the following datasets:
    blogcatalog [44]: this is a network of social relationships
macroscopic view of the 
of the bloggers listed on the blogcatalog website. the la-
bels represent blogger interests inferred through the meta-
network neighbourhood
data provided by the bloggers. the network has 10,312 nodes,
333,983 edges and 39 different labels.

    protein-protein interactions (ppi) [5]: we use a subgraph
of the ppi network for homo sapiens. the subgraph cor-
responds to the graph induced by nodes for which we could
obtain labels from the hallmark gene sets [21] and represent

26

node classification

spectral
id91
deepwalk
line
node2vec

p, q
% gain

blogcatalog wiki-pos

0.0405
0.2110
0.0784
0.2581

0.0395
0.1274
0.1164
0.1552

macro-f1 score

0.25, 0.25

22.3

4, 0.5
21.8

outperforms in all cases, beating 
closest benchmark by up to 22%.

jure leskovec, stanford university

27

incomplete network data

0.20

0.15

0.10

0.05

0.00

0.0

0.20

0.15

0.10

0.05

e
c
n
a
e
m
r
o
c
r
s
o

f
1
r
f
e
-
o
p
r
c
 
e
a
m
v
i
t
c
d
e
r
p

i

0.2

0.1
fraction of missing edges

0.3

0.4

0.5

0.6

0.00

0.0

jure leskovec, stanford university

0.2

0.1
fraction of additional edges

0.3

0.4

0.5

0.6

28

e
c
n
a
e
m
r
o
c
r
s
o

f
1
r
f
e
-
o
p
r
c
 
e
a
m
v
i
t
c
d
e
r
p

i

multi-layer networks

extending node2vec to 
multi-layer networks

predicting multicellular function through multi-layer tissue networks. m. zitnik, j. 
leskovec. bioinformatics, 33 (14): i190-i198, 2017.

jure leskovec, stanford university

29

multi-layer networks

   given layers !" and 
hierarchy #
structure captured by #

   output: features 
of nodes in layers 
and in internal 
levels of the hierarchy

   aim to capture multilevel hierarchical 

jure leskovec, stanford university

30

through ancestors in the hierarchy.

multi-layer networks

is to learn the functions f1, f2, . . . , ft that map from nodes in each layer
to feature representations. ohmnet achieves this goal by    tting its feature
learning model to a given multi-layer network and a given hierarchy, i.e.,
by    nding the mapping functions f1, f2, . . . , ft that maximize the data

layer network, we model the fact that the    medulla    layer is part of the
   brainstem    layer, which is, in turn, part of the    brain    layer. we use the
dependencies among the layers to de   ne a joint objective for id173
of the learned features of proteins.

we propose to use the hierarchy in the learning process by
incorporating a recursive structure into the id173 term for every
object in the hierarchy. speci   cally, we propose the following form of
   for internal hierarchy:
id173 for node u that resides in hierarchy i:
1
2kfi(u)   f   (i)(u)k2
2.

   for nodes in leaves !"
   #$(&) in layer " is close to #((&) in parent )(")
f1,f2,...,f|m|xi2t

this recursive form of id173 enforces the features of node u in
the hierarchy i to be similar to the features of node u in i   s parent    (i)
cj ,
under the euclidean norm. when regularizing features of all nodes in the
elements i of the hierarchy, we obtain:

given the data, ohmnet aims to solve the following maximum

   i     xj2m

likelihood optimization problem:

use node2vec objective

max

ci(u) =

(3)

    third, this model is more ef   cient than the fully pairwise model.
in the fully pairwise model, the dependencies between layers are
modeled by pairwise comparisons of nodes across all pairs of layers,
which takes o(t 2n ) time, where t is the number of layers and
n is the number of nodes. in contrast, ohmnet models inter-layer
dependencies according to the parent-child relationships speci   ed by
the hierarchy, which takes only o(|m|n ) time. since ohmnet   s
hierarchy is a tree, it holds that|m|     t 2, meaning that the proposed
model scales more easily to large multi-layer networks than the fully
pairwise model.

    finally, the hierarchy is a natural way to represent and model biological
systems spanning many different biological scales (carvunis and
ideker, 2014; greene et al., 2015; yu et al., 2016).

per-layer 
node2vec
ci(u),

jure leskovec, stanford university

hierarchical 
dependency
(4)

3.3 full ohmnet model

31

which includes the single-layer network objectives for all network layers,

ci = xu2li

implications

   nodes in different layers representing 
the same entity/node have the same 
features in hierarchy ancestors

   we learn feature representations at 

multiple scales:
   features of nodes in the layers 
   features of nodes in non-leaves in the 

hierarchy

jure leskovec, stanford university

32

application: protein function
   proteins are worker 

molecules
   understanding protein 

function has great biomedical 
and pharmaceutical 
implications

   function of proteins 

depends on their tissue 
context 
[greene et al., nat genet    15]

jure leskovec, stanford university

g1

g2

g3

g4

33

parietallobe
parietallobe

temporallobe
temporallobe

hindbrain
hindbrain

experiments: biological nets
107 genome-wide 
tissue-specific 
protein interaction 
networks
   584 tissue-specific cellular functions 
   examples (tissue, cellular function): 

placenta
placenta
oviduct
oviduct
femalereproductivesystem
femalereproductivesystem

eyeeye
choroid
choroid
nervoussystem
nervoussystem

pancreas
pancreas
hepatocyte
hepatocyte

reproductivesystem
reproductivesystem

corpuscallosum
corpuscallosum

endocrinegland
endocrinegland

pancreaticislet
pancreaticislet

bloodplasma
bloodplasma

integument
integument

spinalcord
spinalcord

spermatid
spermatid

basophil
basophil

retina
retina

ponspons

lens
lens

gliaglia

   (renal cortex, cortex development)
   (artery, pulmonary artery morphogenesis)

jure leskovec, stanford university

34

tissue specific prediction

tissues

42% improvement over 
state-of-the-art baseline

jure leskovec, stanford university

35

brain tissues

brain

brainstem

cerebellum

frontal
lobe

parietal

lobe

occipital

lobe

temporal

lobe

midbrain

substantia

nigra

pons

medulla
oblongata

9 brain tissue ppi networks

in two-level hierarchy

jure leskovec, stanford university

36

embedding brain networks
   do embeddings match anatomy?

jure leskovec, stanford university

37

node2vec: summary
task-independent id171 in 
networks:
   an explicit locality preserving objective 

for id171

   biased id93 capture diversity 

of network patterns

   scalable and robust algorithm

jure leskovec, stanford university

38

a different setting

   so far: node2vec

   unsupervised (task-agnostic)
   nodes have not attributes

   next: graphsage

   supervised (task-specific)
   nodes have attributes

   text, image, etc.

jure leskovec, stanford university

39

graphsage: 

supervised feature 

learning

inductive representation learning on large graphs. 
w. hamilton, r. ying, j. leskovec. neural information processing systems (nips), 2017.
representation learning on graphs: methods and applications. 
w. hamilton, r. ying, j. leskovec. ieee data engineering bulletin, 2017.

jure leskovec, stanford university

40

idea: convolutional networks
id98 on an image:

goal is to generalize convolutions beyond simple lattices
leverage node features/attributes (e.g., text, images)

jure leskovec, stanford university

41

from images to networks
convolutional neural networks (on grids)
single id98 layer with 3x3 filter:
single id98 layer with 3x3 filter:

(animation by  
vincent dumoulin)

image

transform information at the neighbors and combine it

   transform    messages       " from neighbors: #"	   "
   add them up:    #"	   "
"

graph

jure leskovec, stanford university

42

graph-structured data
graph-structured data

real-world graphs

hidden layer

what if our data looks like this?
what if our data looks like this?

but what if your graphs look like this?

input

input

hidden layer

or this:
or this:

or this:

relu

relu

   
   

   

   examples:

social networks, information networks, 
id13s, communication 
networks, web graph,    

jure leskovec, stanford university

end-to-end learning on graphs with id197s

thomas kipf

43

6

a na  ve approach

a na  ve approach

    take adjacency matrix     and feature matrix   

    concatenate them  

   join adjacency matrix and features
   feed them into a deep neural net:

    feed them into deep (fully connected) neural net 

[a, x]

    done?

a

c

b

d

e

a
b
c
d
e

a    b    c    d    e
feat
0     1     1     1     0          1     0
1     0     0     1     1          0     0
1     0     0     1     0          0     1
1     1     1     0     1          1     1
0     1     0     1     0          1     0

  !(#) parameters

   issues with this idea:

problems:
    huge number of parameters 
    no inductive learning possible

   not applicable to graphs of different sizes
   not invariant to node ordering

jure leskovec, stanford university

end-to-end learning on graphs with id197s

thomas kipf

44

?

hidden layer

hidden layer

graph-structured data

id197
id197: 

learning convolutional neural networks for graphs

what if our data looks like this?

input

a sequence of words. however, for numerous graph col-
lections a problem-speci   c ordering (spatial, temporal, or
otherwise) is missing and the nodes of the graphs are not
in correspondence. in these instances, one has to solve two
problems: (i) determining the node sequences for which
neighborhood graphs are created and (ii) computing a nor-
malization of neighborhood graphs, that is, a unique map-
ping from a graph representation into a vector space rep-
resentation. the proposed approach, termed patchy-san,
addresses these two problems for arbitrary graphs. for each
input graph, it    rst determines nodes (and their order) for
which neighborhood graphs are created. for each of these
nodes, a neighborhood consisting of exactly k nodes is ex-
tracted and normalized, that is, it is uniquely mapped to a
space with a    xed linear order. the normalized neighbor-

or this:

relu

   

   

problem: for a given subgraph how to come 
with canonical node ordering

figure 2. an illustration of the proposed architecture. a node
sequence is selected from a graph via a graph labeling procedure.
for some nodes in the sequence, a local neighborhood graph is as-
sembled and normalized. the normalized neighborhoods are used

learning convolutional neural networks for graphs. m. niepert,  m. ahmed, k. kutzkov icml. 2016.

jure leskovec, stanford university

45

......neighborhood graph constructionconvolutional architecturenode sequence selectiongraph id172our approach: graphsage
idea: node   s neighborhood defines a 

computation graph
!
!

determine node 
computation graph

propagate and

transform information

learn how to propagate information across 

the graph to compute node features

semi-supervised classification with id197. t. n. kipf, m. welling, iclr 2017

jure leskovec, stanford university

46

our approach: graphsage

6

6

update for node !:
   #(%&')=*+,- .%   #%, 0 *+,-(1%   2%)
transform 6   s own 
9+1=> level
features of neighbors :
features from level 9
features of node 6
      #5 = attributes of node 6
         : aggregator function (e.g., avg., lstm, max-pooling)

2   4#

transform and aggregate

jure leskovec, stanford

47

graphsage: example

q(1)

w(1)

q(1)

w(1)

w(2)

q(2)

w(1)

q(1)

supervised training to identify parameters: w(k), q(k)

w(2)
q(2)

w(2)
q(2)

w(2)
q(2)

w(1)
q(1)

jure leskovec, stanford university

48

graphsage: benefits

   can use different aggregators !

   mean (simple element-wise mean), lstm (to a random 

order of nodes),  max-pooling (element-wise max)

   can use different id168s:

   cross id178, hinge loss, ranking loss

   model has a constant number of parameters
   fast scalable id136
   can be applied to any node in any network

jure leskovec, stanford university

49

application: pinterest
human curated collection of pins

pin: a visual bookmark someone 
has saved from the internet to a 
board they   ve created.
pin: image, text, link

board: a greater collection of ideas (pins having sth. in common). 

jure leskovec, stanford university

50

pinterest graph

q

graph: 2b pins, 1b boards, 17b edges
   graph is dynamic: need to apply to 
new nodes without model retraining
   rich node features: content, image

jure leskovec, stanford university

51

task: item-item recs

related pin recommendations
   given user is looking at pin q, what 

pin x are they going to save next:

query

positive

rnd. negative

hard negative

jure leskovec, stanford university

52

graphsage training
   leverage inductive capability, and 

train on individual subgraphs
   300 million nodes, 1 billion edges, 

1.2 billion pin pairs (q, x)

   large batch size: 2048 per minibatch

jure leskovec, stanford university

53

graphsage: id136

   use mapreduce for

model id136

   avoids repeated computation

jure leskovec, stanford university

54

experiments
related pin recommendations
   given user is looking at pin q, predict 
what pin x are they going to save next 
   baselines for comparison

   visual: vgg-16 visual features
   annotation: id97 model
   combined: combine visual and annotation
   rw: random-walk based algorithm
   graphsage
   setup: embed 2b pins, perform nearest 
neighbor to generate recommendations

jure leskovec, stanford university

55

results: ranking

task: given q, rank x as high as possible 
among 2b pins
   hit-rate: pct. p was among top-k
   mrr: mean reciprocal rank

method
visual

annotation
combined
graphsage

hit-rate

17%
14%
27%
46%

mrr
0.23
0.19
0.37
0.56

jure leskovec, stanford university

56

results: user study

   user study: which recommendation 

do you prefer?
win

method

lose draw fraction of wins

graphsage vs. 

visual

graphsage vs. 

annotation

graphsage vs. 

rw

26.7% 18.6% 54.7%

58.9%

28.4% 16.1% 55.5%

63.8%

32.2% 21.4% 46.4%

60.1%

jure leskovec, stanford university

57

example recommendations

gs

jure leskovec, stanford university

58

graphsage: summary

   graph convolution networks

   generalize beyond simple convolutions

   fuses node features & graph info
   state-of-the-art accuracy for node 

classification and link prediction.

   model size independent of graph size; 

can scale to billions of nodes
   largest embedding to date (3b nodes, 17b edges)
   leads to significant performance gains

jure leskovec, stanford university

59

conclusion
id171 

for networks

node!:#      &

u

vec

   &

feature representation, 

embedding

jure leskovec, stanford university

60

conclusion

results from the past 1-2 years have shown:
   representation learning paradigm can be 

extended to graphs

   no feature engineering necessary
   can effectively combine node attribute data 

with the network information

   state-of-the-art results in a number of 

domains/tasks

   use end-to-end training instead of 

multi-stage approaches for better performance

jure leskovec, stanford university

61

conclusion

next steps:
   multimodal & dynamic/evolving settings
   domain-specific adaptations 

(e.g. for recommender systems) 

   graph generation 
   prediction beyond simple parwise edges

   multi-hop edge prediction

   theory

jure leskovec, stanford university

62

phd students

industry partnerships

claire
donnat

mitchell
gordon

david
hallac

emma
pierson

geet
sethi

funding

rex
ying

tim
himabindu
althoff
lakkaraju
post-doctoral fellows

will

hamilton

david
jurgens

marinka
zitnik

michele
catasta

srijan
kumar

research
staff

stephen

bach

peter
kacin

rok
sosic

collaborators
dan jurafsky, linguistics, stanford university
christian danescu-miculescu-mizil, information science, cornell university
stephen boyd, electrical engineering, stanford university
david gleich, computer science, purdue university
vs subrahmanian, computer science, university of maryland
sarah kunz, medicine, harvard university
russ altman, medicine, stanford university
jochen profit, medicine, stanford university
eric horvitz, microsoft research
jon kleinberg, computer science, cornell university
sendhill mullainathan, economics, harvard university
scott delp, bioengineering, stanford university
jens ludwig, harris public policy, university of chicago

jure leskovec, stanford university

63

post-doc positions open!

email us at jure@cs.stanford.edu

64

references

   node2vec: scalable id171 for networks

a. grover, j. leskovec. kdd 2016.

   predicting multicellular function through multi-layer tissue 

networks. m. zitnik, j. leskovec. bioinformatics, 2017.
inductive representation learning on large graphs. 
w. hamilton, r. ying, j. leskovec. nips 2017

  

   representation learning on graphs: methods and 

applications. w. hamilton, r. ying, j. leskovec.
ieee data engineering bulletin, 2017.

   code:

   http://snap.stanford.edu/node2vec
   http://snap.stanford.edu/graphsage

jure leskovec, stanford university

65

