the role of causality 
for interpretability 

bernhard sch  lkopf 

max planck institute for intelligent systems 

http://www.tuebingen.mpg.de/~bs 

l. s. lowry (1887-1976) 

bernhard sch  lkopf 

dependence vs. causation 

h. sies, nature 332:495 (1988); 
from r. wilson, http://users.physics.harvard.edu/~wilson/soundscience/alf_science.html 

bernhard sch  lkopf 

common cause principle 
(reichenbach)

(i) if x and y are sta-
tistically dependent,
then there exists z
causally in   uencing
both;

(ii) z

screens x
and y from each
other
(given z,
x und y become
independent)

z 

special case: 

by permission of the  
university of pittsburgh.  
all rights reserved. 

x 

y 

x 

y 

x 

y 

p(x,y) 

functional causal model (pearl et al.) 

    set of observables x1, . . . , xn on a dag g
    arrows represent direct causal links
    xi := fi(pai, ui) with
independent rvs u1, . . . , un.

    entails observational distribution
p(x1, . . . , xn) satisfying the
causal markov condition:

source: 
http://bayes.cs.ucla.edu/jp_home.html 

conditioned on its parents, xj is independent of its non-descendants

(g, p) are a    graphical model    (lauritzen, 1996)

    the distribution of the independent noises thus picks up a footprint
of the graph topology
    can be assayed using conditional independence testing (for n > 2)

bernhard sch  lkopf 

what is cause and what is effect? 

bernhard sch  lkopf 

    intervention on a: raise the city,    nd that t changes
    hypothetical intervention on a: still expect that t
changes, since we can think of a physical mechanism
p(t|a) that is independent of p(a)
    we expect that p(t|a) is invariant across, say, di   er-
ent countries in a similar climate zone

bernhard sch  lkopf 

independence/invariance 
principle: 
the causal generative 
process is composed of 
autonomous modules that 
do not inform or 
influence each other. 

    factorization

p(x1, . . . , xn) =

nyi=1

p (xi | pai)

according to the causal graph, with independent causal conditionals
    a change in a real world distribution always comes from a change
in a causal conditional (i.e., structural assignment/function and/or
noise variable)
    changing one mechanism p (xi | pai) does not change the other
p (xj | paj) (j 6= i)
    this does not hold if we factorize according to another graph! such
a factorization will change simultaneously in non-interpretable ways.

cf. independence of mechanisms (janzing & sch  olkopf, 2010), independence of cause and mechanism
(janzing et al., 2012), autonomy, (structural) invariance, separability, exogeneity, stability, modular-

       abstraction challenge for unsupervised learning:   
    why is modelling

p(acoustics)

so much worse than modelling

p(acoustics|phonemes) p(phonemes)?

(yoshua bengio, cifar workshop talk, 2017)

from perona, 2017; 
cf. lopez-paz et al., 2016 

gedankenexperiment 

particles scattered at an object

    incoming beam:    cause   
    scattering at object:    mechanism   
    outgoing beam:    e   ect   , contains information about the object

independence assumption 

    s initial state of a physical system
    m the system dynamics applied for some    xed time

independence principle: s and m are algorithmically indepen-
dent

i(s : m) += 0,

i.e., knowing s does not enable a shorter description of m and vice
versa.

thermodynamic arrow of time 

theorem [non-decrease of id178]. let m be a bijective map
on the set of states of a system then i(s : m) += 0 implies

k(m(s))

+

  k(s)

proof idea: if m(s) admits a shorter description than s, knowing m admits a shorter
description of s: just describe m(s) and then apply m 1.

janzing, chaves, sch  olkopf: algorithmic independence of initial condition and
dynamical law in thermodynamics and causal id136. new j. of physics, 2016

independence of input and mechanism 

 

       no added noise 
       assumption: y = f(x) with invertible f 

? 

x 

y 

daniusis, janzing, mooij, zscheischler, steudel, zhang, sch  lkopf:  
inferring deterministic causal relations, uai 2010 

bernhard sch  lkopf 

causal independence implies anticausal dependence 

assume that f is a monotonically increasing bijection of [0, 1].
view px and log f 0 as rvs on the prob. space [0, 1] w. lebesgue measure.

postulate (independence of mechanism and input):

cov (log f 0, px) = 0

note: this is equivalent to

z 1

0

log f0(x)p(x)dx =z 1

0

log f0(x)dx,

since cov (log f 0, px) = e [ log f 0    px]   e [ log f 0] e [ px] = e [ log f 0    px]  
e [ log f 0].
proposition: if f 6= id,

cov (log f  10, py) > 0.

bernhard sch  lkopf 

benchmark dataset with 106 cause-e   ect pairs

http://webdav.tuebingen.mpg.de/cause-effect/

mooij et al: distinguishing cause from e   ect using observational data: meth-
ods and benchmarks, jmlr 2016

bernhard sch  lkopf 

independent mechanisms in machine learning 
nyi=1
       semi-supervised learning (sch  lkopf et al., icml 2012) 
       causal conditionals are robust (icml 2012) 
       use them for causal id136 in multi-task settings 

p(x1, . . . , xn) =

(zhang et al., icml 2013 & aaai 2015, rojas-carulla et al. 2016) 

p (xi | pai)

       related: transportability (bareinboim & pearl, 2012) 

bernhard sch  lkopf 

learning independent mechanisms  
parascandolo, rojas-carulla, kilbertus, sch  lkopf 
http://arxiv.org/abs/1712.00961 

method 

    experts pre-trained to

approximate the identity

training progress 

randomly samples inputs 
 
 
 
output from the best expert 

accuracy of a id98 trained on mnist for different 
test sets 

too many or too few experts     proportion of data    won    

towards learning causal mechanisms 
 
       learn general scms (cf. sch  lkopf, janzing, lopez-paz 2016) 
       data from multiple tasks in multiple environments 
       the functions in the scm should learn components that are 
robust across tasks, i.e., causal (independent) mechanisms 

       competitive training procedure 

   

x 

z 

   

v 

w 

y 

   

a modeling taxonomy 

statistical
model

causal
model

di   erential
equation
model

i.i.d. prediction, pattern recognition,
   generalization   
learn from data
predict under shift & intervention,
   horizontal generalization   
provide physical insight,
understand predictions
think/reason,
   act in an imagined space    (k. lorenz)

y

y

n

n

n

y

(y)

y

(y)

?

y

n

y

y

?

bernhard sch  lkopf 

conclusions 

dominik janzing, kun zhang, jonas peters, niki kilbertus, mateo rojas-carulla, giambattista parascandolo
 
humans build models for the purpose of intervention and often communicate them 
in causal language. 
 
causal conditionals correspond to physical mechanisms, are more robust and 
generalize across tasks.  
 
the independent mechanisms assumption can be used in causal id136 and 
machine learning, leading to models that can be easier to understand, provided the 
underlying physical model is understandable. 
 

bernhard sch  lkopf 

  

peters, janzing, sch  lkopf. elements of causal id136: 
foundations and learning algorithms. 
mit press, 2017, http://www.math.ku.dk/~peters/jonas_files/
bookdraft11-online-2017-06-28.pdf 

we are looking for postdocs --- bs@tuebingen.mpg.de 
   and scientists+engineers at amazon research (new t  bingen lab) 
 

https://youtu.be/8yoqdaooo9a 

bernhard sch  lkopf 

