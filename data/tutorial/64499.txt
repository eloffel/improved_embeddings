manipulating	and	measuring	

model	interpretability

jenn	wortman	vaughan

microsoft	research

based	on	joint	work	with	forough poursabzi-sangdeh,	

dan	goldstein,	jake	hofman,	&	hanna	wallach

approach	1:	design	simple	models

    =    $    $ +	   +	    )    )

generalized	additive	models

(lou,	caruana,	et	al.,	2012&2013)

point	systems

(jung	et	al.,	2017;	ustun &	rudin,	2015,	etc.)

classic	methods:	decision	trees,	rule	lists	(if-then-else),	

rule	sets,	(sparse)	linear	models,	   

approach	2:	design	simple	explanations	

for	(potentially	complex)	models

interpretable	local	
approximations

(ribeiro	et	al.,	2016;	lundberg	and	

lee,	2017)

model	visualizations
(e.g.,	work	at	google	pair)

interpretability?

a	legal	necessity

the	data	controller	shall	provide	   meaningful	
information	about	the	logic	involved,	as	well	as	
the	significance	and	the	envisaged	consequences	
of	such	processing.   	

interpretability	is	a	latent	property

number	of	
features

linear?

ui

clear	vs.	
black	box

   

interpretability

trust

ability	to	
debug
ability	to	
simulate

   

different	users	and	different	needs

y
l
l

a
b
o
g

l

	
l

e
d
o
m

d
n
a
t
s
r
e
d
n
u

	

	
r
e
t
t
e
b
e
k
a
m
approach	

s
n
o
i
s
i
c
e
d

a

s
l
e
d
o
m
g
u
b
e
d

	

i

s
a
b
	
s
s
e
s
s
a

t
s
u
r
t
	
e
r
i
p
s
n

i

approach	

c

	

l

e
g
n
i
s
n
a
p
x
e

l

i

n
o
i
t
c
i
d
e
r
p

ceos

data	scientists

lay	people

regulators

approach	

b

properties	of	the	
system	design

properties	of	
human	behavior

number	of	
features

linear?

ui

clear	vs.	
black	box

   

interpretability

trust

ability	to	
debug
ability	to	
simulate

   

interpretability	is	not	a	purely	
computational	problem.
we	need	interdisciplinary	
approaches	to	address	it.

can	we	learn	from	psychology?

    psychologists	and	social	scientists	have	been	

studying	trust	in	models	since	the	1950s
    e.g.,	literature	on	algorithm	aversion

    general	approach:	run	randomized	human	

subject	experiments	to	isolate	and	measure	the	
impact	of	different	factors	on	trust

our	goal:	apply	this	approach	to	
understand	the	fundamental	
properties	of	human	behavior	
pertinent	to	interpretability	

properties	of	the	
system	design

properties	of	
human	behavior

number	of	
features

linear?

ui

clear	vs.	
black	box

   

interpretability

trust

ability	to	
debug
ability	to	
simulate

   

initial	experiment

    we	ran	a	randomized	human	subject	experiment	

on	1250	participants	from	mechanical	turk

    we	varied

    the	number	of	features
    black-box	vs.	visible	(   clear   )	internals

    we	measured

    trust	in	the	model
    simulatability
    error	of	the	end	user   s	predictions

focus	on	lay	people

	

l

e
g
n
i
s
n
a
p
x
e

l

i

n
o
i
t
c
i
d
e
r
p

y
l
l

a
b
o
g

l

	
l

e
d
o
m

d
n
a
t
s
r
e
d
n
u

	
r
e
t
t
e
b
e
k
a
m

	

s
n
o
i
s
i
c
e
d

s
l
e
d
o
m
g
u
b
e
d

	

i

s
a
b
	
s
s
e
s
s
a

ceos

data	scientists

lay	people

regulators

?

?

t
s
u
r
t
	
e
r
i
p
s
n

i

?

predictive	task

    participants	asked	to	predict	the	prices	of	upper	
west	side	apartments	with	the	help	of	a	model

experimental	conditions

2-feature,	
black-box

2-feature,	
clear

8-feature,	
black-box

8-feature,	
clear

experimental	conditions

2-feature,	
black-box

2-feature,	
clear

8-feature,	
black-box

8-feature,	
clear

experimental	conditions

2-feature,	
black-box

2-feature,	
clear

8-feature,	
black-box

8-feature,	
clear

experiment	interface

experiment	interface

pre-registered	hypotheses

1. the	clear,	2-feature	model	will	be	easiest	for	

participants	to	simulate.

2. participants	will	follow	the	clear,	2-feature	model	

more	than	the	black-box,	8-feature	model.

3. behavior	will	vary	across	conditions	when	an	

unusual	example	leads	a	model	to	make	a	highly	
inaccurate	prediction.

simulation	error

bb2

clear2

bb8

clear8

|model	prediction	    guess	of	model	prediction|

pre-registered	hypotheses

1. the	clear,	2-feature	model	will	be	easiest	for	

participants	to	simulate.

2. participants	will	follow	the	clear,	2-feature	model	

more	than	the	black-box,	8-feature	model.

3. behavior	will	vary	across	conditions	when	an	

unusual	example	leads	a	model	to	make	a	highly	
inaccurate	prediction.

deviation	from	the	model

bb2

clear2

bb8

clear8

|model	prediction	    participant   s	prediction|

prediction	error

bb2

clear2

bb8

clear8

none

|actual	price	    participant   s	prediction|

pre-registered	hypotheses

1. the	clear,	2-feature	model	will	be	easiest	for	

participants	to	simulate.

2. participants	will	follow	the	clear,	2-feature	model	

more	than	the	black-box,	8-feature	model.

3. behavior	will	vary	across	conditions	when	an	

unusual	example	leads	a	model	to	make	a	highly	
inaccurate	prediction.

a	bad	prediction

do	people	deviate	when	they	should?

bb2

clear2

bb8

clear8

|model	prediction	    participant   s	prediction|

potential	concern:	maybe	new	york	
city	prices	are	too	crazy.

scaled	down	prices:	hypotheses
1. the	clear,	2-feature	model	will	be	easiest	for	

participants	to	simulate.

2. participants	will	follow	the	clear,	2-feature	model	

more	than	the	black-box,	8-feature	model.

3. participants	will	follow	the	clear	models	more	

than	black-box	when	an	unusual	example	leads	a	
model	to	make	a	highly	inaccurate	prediction.

scaled	down	prices:	simulation	error

bb2

clear2

bb8

clear8

|model	prediction	    guess	of	model	prediction|

scaled	down	prices:	hypotheses
1. the	clear,	2-feature	model	will	be	easiest	for	

participants	to	simulate.

2. participants	will	follow	the	clear,	2-feature	model	

more	than	the	black-box,	8-feature	model.

3. participants	will	follow	the	clear	models	more	

than	black-box	when	an	unusual	example	leads	a	
model	to	make	a	highly	inaccurate	prediction.

scaled	down	prices:	 deviation

bb2

clear2

bb8

clear8

|model	prediction	    participant   s	prediction|

scaled	down	prices:	prediction	error

bb2

clear2

bb8

clear8

none

|actual	price	    participant   s	prediction|

scaled	down	prices:	hypotheses
1. the	clear,	2-feature	model	will	be	easiest	for	

participants	to	simulate.

2. participants	will	follow	the	clear,	2-feature	model	

more	than	the	black-box,	8-feature	model.

3. participants	will	follow	the	clear	models	more	

than	black-box	when	an	unusual	example	leads	a	
model	to	make	a	highly	inaccurate	prediction.

do	people	deviate	when	they	should?

bb2

clear2

bb8

clear8

|model	prediction	    participant   s	prediction|

self-reported	trust

bb2

clear2

bb8

clear8

summary	of	results

    as	hypothesized,	participants	are	better	able	to	
simulate	the	clear,	2-feature	model	compared	
with	the	black-box,	8-feature	model.

    there	is	no	significant	difference	in	participants   	

deviation	from	the	model	across	conditions.

    when	given	a	   weird   	example	in	which	the	

model	is	wrong,	participants	in	the	clear	
conditions	deviate	less	than	those	in	black-box.	

properties	of	the	
system	design

properties	of	
human	behavior

number	of	
features

linear?

ui

clear	vs.	
black	box

   

interpretability

trust

ability	to	
debug
ability	to	
simulate

   

different	users	and	different	needs

	

l

e
g
n
i
s
n
a
p
x
e

l

i

n
o
i
t
c
i
d
e
r
p

y
l
l

a
b
o
g

l

	
l

e
d
o
m

d
n
a
t
s
r
e
d
n
u

	
r
e
t
t
e
b
e
k
a
m

	

s
n
o
i
s
i
c
e
d

s
l
e
d
o
m
g
u
b
e
d

	

i

s
a
b
	
s
s
e
s
s
a

ceos

data	scientists

lay	people

regulators

?

?

t
s
u
r
t
	
e
r
i
p
s
n

i

?

interpretability	is	not	a	purely	
computational	problem.
we	need	interdisciplinary	
approaches	to	address	it.

thanks!

http://jennwv.com
jenn@microsoft.com

