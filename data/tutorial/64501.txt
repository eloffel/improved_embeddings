deep	learning	for	robotics

pieter	abbeel

embodied	intelligence
uc	berkeley
gradescope

ai	for	robotic	automation
ai	research
ai	for	grading	homework	and	exams

research	in	this	talk	was	done	at	openai and	uc	berkeley

[pr1	robot	-- wyrobek,	berger,	van	der	loos,	salisbury,	icra	2008]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

many	unsolved	pieces	to	the	ai	robotics	puzzle

n faster	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

many	unsolved	pieces	to	the	ai	robotics	puzzle

n faster	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

reinforcement	learning	(rl)

robot + 

environment

max

   

e[

hxt=0

r(st)|      ]

[image	credit:	sutton	and	barto 1998]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

reinforcement	learning	(rl)

robot + 

environment

max

   

e[

hxt=0

r(st)|      ]

n compared	to	

supervised	learning,	
additional	challenges:
n credit	assignment

n

n

stability
exploration

[image	credit:	sutton	and	barto 1998]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

deep	rl	success	stories

id25 mnih et	al,	nips	2013	/	nature	2015

mcts guo et	al,	nips	2014;	trpo schulman,	levine,	moritz,	jordan,	abbeel,	icml	2015; a3c mnih et	al,	
icml	2016;	dueling	id25	wang	et	al	icml	2016;	double	id25	van	hasselt	et	al,	aaai	2016;	prioritized	

experience	replay	schaul et	al,	iclr	2016;	bootstrapped	id25	osband et	al,	2016;	q-ensembles chen	et	al,	

2017;	rainbow hessel	et	al,	2017;	   

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

deep	rl	success	stories

alphago silver	et	al,	nature	2015

alphagozero silver	et	al,	nature	2017

alphazero silver	et	al,	2017

tian	et	al,	2016;	maddison	et	al,	2014;	clark	et	al,	2015

deep	rl	success	stories

n super-human	agent	on	dota2	1v1,	enabled	by

n reinforcement	learning
n self-play
n enough	computation

[openai,	2017]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

deep	rl	success	stories

^		trpo schulman	et	al,	2015	+		gae schulman	et	al,	2016

see	also:	ddpg lillicrap et	al	2015;	svg heess et	al,	2015;	q-prop gu et	al,	2016;	scaling	up	es	salimans et	

al,	2017;	ppo schulman	et	al,	2017;	parkour heess et	al,	2017;	

deep	rl	success	stories

guided	policy	search levine*,	finn*,	darrell,	abbeel,	jmlr	2016

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

[geng*,	zhang*,	bruce*,	caluwaerts,	vespignani,	sunspiral,	abbeel,	levine,	icra	2017]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

mastery?	yes

deep	rl	(id25)

score: 18.9

vs.

human

score: 9.3

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

but	how	good	is	the	learning?

vs.

deep	rl	(id25)

score: 18.9

#experience	
measured	in	real	
time:	40	days

   slow   

human

score: 9.3

#experience	
measured	in	real	
time:	2	hours

   fast   

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

humans	vs.	did25

humans	after	15	minutes	tend	to	outperform	did25	after	115	hours

black	dots:	human	play
blue	curve:	mean	of	human	play
blue	dashed	line:	   expert   	human	play	

red	dashed	lines:	
did25	after	10,	25,	200m	frames
(~	46,	115,	920	hours)

[tsividis,	pouncy,	xu,	tenenbaum,	gershman,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

how	to	bridge	this	gap?

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

starting	observations

n trpo,	id25,	a3c,	ddpg,	ppo,	rainbow,	    are	fully	general	rl	

algorithms
n i.e.,	for	any	environment	that	can	be	mathematically	defined,	these	

algorithms	are	equally	applicable

n environments	encountered	in	real	world

=	tiny,	tiny	subset	of	all	environments	that	could	be	defined

(e.g.	they	all	satisfy	our	universe   s	physics)

can	we	develop	   fast   	rl	algorithms	that	take	advantage	of	this?

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

reinforcement	learning

agent

r

s

environment

a

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

reinforcement	learning

rl	agent

rl	algorithm

policy	

r

o

environment

a

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

reinforcement	learning

rl	agent

rl	agent

rl	algorithm

rl	algorithm

policy	

policy	

   

r

s

environment	a

a

r

s

environment	b

a

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

reinforcement	learning

rl	agent

rl	agent

rl	algorithm

rl	algorithm

policy	

policy	

r

o

environment	a

a

r

o

environment	b

a

traditional	rl	research:
    human	experts	develop	

the	rl	algorithm

    after	many	years,	still	

no	rl	algorithms	nearly	
as	good	as	humans   

   
alternative:
    could	we	learn	a	better	

rl	algorithm?	

    or	even	learn	a	better	

entire	agent?

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-reinforcement	learning

meta-training	environments

environment	a

environment	b

   

meta	rl
algorithm

"fast   	rl
agent

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-reinforcement	learning

meta-training	environments

environment	a

environment	b

   

meta	rl
algorithm

"fast   	rl
agent

r,o

environment	f

a

testing	environments

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-reinforcement	learning

meta-training	environments

environment	a

environment	b

   

meta	rl
algorithm

"fast   	rl
agent

r,o

environment	g

a

testing	environments

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-reinforcement	learning

meta-training	environments

environment	a

environment	b

   

meta	rl
algorithm

"fast   	rl
agent

r,o

environment	h

a

testing	environments

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

formalizing	learning	to	reinforcement	learn

max

   

eme    (k)

m " kxk=1

r(    (k)

m ) | rlagent   #

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

formalizing	learning	to	reinforcement	learn

max

   

eme    (k)

m " kxk=1

r(    (k)

m ) | rlagent   #

m : sample mdp
    (k)
m : k   th trajectory in mdp m

meta-train:

max

    xm2mtrain

e    (k)

m " kxk=1

r(    (k)

m ) | rlagent   #

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]		also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

representing																

rlagent   
m ) | rlagent   #

r(    (k)

max

    xm2mtrain

e    (k)

m " kxk=1

n rlagent =	id56	=	generic	computation	architecture

n different	weights	in	the	id56	means	different	rl	algorithm	and	prior
n different	activations	in	the	id56	means	different	current	policy
n meta-train	objective	can	be	optimized	with	an	existing	(slow)	rl	algorithm

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]	 also:	[wang	et	al,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

evaluation:	multi-armed	bandits

n multi-armed	bandits setting

n

each	bandit	has	its	own	distribution	
over	pay-outs
each	episode	=	choose	1	bandit
n good	rl	agent	should	explore	

n

bandits	sufficiently,	yet	also	exploit	
the	good/best	ones

n provably	(asymptotically)	optimal	
rl	algorithms have	been	invented	
by	humans:	gittins	index,	ucb1,	
thompson	sampling,	   

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

evaluation:	multi-armed	bandits

we	consider	bayesian	evaluation	setting.			some	of	these	prior	works	also	have	adversarial	
guarantees,	which	we	don   t	consider	here.

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

evaluation:	locomotion	    half	cheetah
n task	    reward	based	on	target	running	direction	+	speed

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

evaluation:	locomotion	    half	cheetah
n task	    reward	based	on	target	running	direction	+	speed

n result	of	meta-training	=	a	single	agent	(the	   fast	rl	agent   ),	

which	masters	each	task	almost	instantly	within	1st episode

evaluation:	locomotion	    ant
n task	    reward	based	on	target	running	direction	+	speed

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

evaluation:	locomotion	    ant
n task	    reward	based	on	target	running	direction	+	speed

n result	of	meta-training	=	a	single	agent	(the	   fast	rl	agent   ),	

which	masters	each	task	almost	instantly	within	1st episode

evaluation:	visual	navigation

agent	input:	current	image
agent	action:	straight	/	2	degrees	left	/	2	degrees	right
map	just	shown	for	our	purposes,	but	not	available	to	agent

agent   s view

maze

related	work:	mirowski,	et	al,	2016;	jaderberg et	al,	2016;	mnih et	al,	2016;	wang	et	al,	2016

evaluation:	visual	navigation

agent	input:	current	image
agent	action:	straight	/	2	degrees	left	/	2	degrees	right
map	just	shown	for	our	purposes,	but	not	available	to	agent

before learning-to-learn

after learning-to-learn

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-learning	curves

[duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

other	architectures
n simple	neural	attentive	meta-learner	(snail)	

[mishra,	rohaninejad,	chen,	abbeel,	2017]

n model-agnostic	meta-learning	(maml)

[finn,	abbeel,	levine,	2017;	finn*,	yu*,	zhang,	abbeel,	levine	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

simple	neural	attentive	meta-learner
n like	rl2	but:
replace	the	lstm	with	
dilated	temporal	
convolution	(like	wavenet)	
+	attention

[wavenet:	van	den	oord	et	al,	2016]
[attention-is-all-you-need:	vaswani et	al,	2017]

[mishra*,	rohaninejad*,	chen,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

bandits

[mishra*,	rohaninejad*,	chen,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

vision-based	navigation	in	unknown	maze

[mishra*,	rohaninejad*,	chen,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

agent	dropped	in	new	maze

[mishra*,	rohaninejad*,	chen,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

other	architectures
n simple	neural	attentive	meta-learner	(snail)	

[mishra,	rohaninejad,	chen,	abbeel,	2017]

n model-agnostic	meta-learning	(maml)

[finn,	abbeel,	levine,	2017;	finn*,	yu*,	zhang,	abbeel,	levine	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

model-agnostic	meta-learning	(maml)
n starting	observation:

n computer	vision	practice:

n train	on	id163		
n fine-tune	on	actual	task	
works	really	well!

[deng	et	al.	   09]

[decaf:	donahue et	al.	   14;	   ]

n questions:

n how	to	generalize	this	to	behavior	learning?
n and	can	we	explicitly	train	end-to-end	for	being	maximally	ready	for	

efficient	fine-tuning?

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

model-agnostic	meta-learning	(maml)
key	idea: end-to-end	learning	of	parameter	vector	  	that

is	good	init	for	fine-tuning	for	many	tasks

finetuned parameters

maml	test	time:

fine-tuning:

maml	training:

pretrained	parameters

train	data	for
new	task

[finn,	abbeel,	levine	icml	2017]

finetuned parameters

model-agnostic	meta-learning	(maml)

parameter	vector
being	meta-learned

optimal	parameter	
vector	for	task	i

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

maml	evaluation
n experiments:	competitive	performance	on

n bandits
n cheetah
n ant

n theory:	maml	is	fully	general	

finn	and	levine	2017:	meta-learning	and	universality:	deep	representations	and	
gradient	descent	can	approximate	any	learning	algorithm

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta	learning	for	optimization

task distribution: different neural networks, weight initializations, and/or 
different id168s
n bengio	et	al.,	(1990)	learning	a	synaptic	learning	rule

n naik	et	al.,	(1992)	meta-neural	networks	that	learn	by	learning

n hochreiter	et	al.,	(2001)	learning	to	learn	using	gradient	descent

n younger	et	al.,	(2001),	meta	learning	with	back	propagation

n andrychowicz	et	al.,	(2016)	learning	to	learn	by	gradient	descent	by	gradient	descent

n chen	et	al.,	(2016)	learning	to	learn	for	global	optimization	of	black	box	functions

n wichrowska	et	al.,	(2017)	learned	optimizers	that	scale	and	generalize

n ke	et	al.,	(2017)	learning	to	optimize	neural	nets

n wu	et	al.,	(2017)	understanding	short-horizon	bias	in	stochastic	meta-optimization

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta	learning	for	classification

task distribution: different classification datasets (input: images, output: class labels)
n hochreiter et	al.,	(2001)	learning	to	learn	using	gradient	descent
n younger	et	al.,	(2001),	meta	learning	with	back	propagation
n koch	et	al.,	(2015)	siamese	neural	networks	for	one-shot	image	recognition
n santoro	et	al.,	(2016)	meta-learning	with	memory-augmented	neural	networks
n vinyals	et	al.,	(2016)	matching	networks	for	one	shot	learning
n edwards	et	al.,	(2016)	towards	a	neural	statistician
n ravi	et	al.,	(2017)	optimization	as	a	model	for	few-shot	learning
n munkhdalai	et	al.,	(2017)	meta	networks
n snell	et	al.,	(2017)	prototypical	networks	for	few-shot	learning
n shyam	et	al.,	(2017)	attentive	recurrent	comparators
n finn	et	al.,	(2017)	model-agnostic	meta-learning	for	fast	adaptation	of	deep	networks
n mehrotra	et	al.,	(2017)	generative	adversarial	residual	pairwise	networks	for	one	shot	learning
n mishra	et	al.,	(2017)	meta-learning	with	temporal	convolutions
n li	et	al.,	(2017)	meta-sgd:	learning	to	learn	quickly	for	few	shot	learning
n finn	and	levine,	(2017)	meta-learning	and	universality:	deep	representations	and	gradient	descent	can	approximate	

any	learning	algorithm

n anon@openreview,	(2017)	recasting	gradient-based	meta-learning	as	hierarchical	bayes

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta	learning	for	generative	models
task distribution: different unsupervised datasets (e.g. collection of images)

n rezende et	al.,	(2016)	one-shot	generalization	in	deep	generative	models
n edwards	et	al.,	(2016)	towards	a	neural	statistician
n bartunov	et	al.,	(2016)	fast	adaptation	in	generative	models	with	generative	matching	

networks

n bornschein et	al.,	(2017)	variational memory	addressing	in	generative	models
n reed	et	al.,	(2017)	few-shot	autoregressive	density	estimation:	towards	learning	to	

learn	distributions

meta	learning	for	rl

task distribution: different environments
n schmidhuber.	evolutionary	principles	in	self-referential	learning.	(1987)
n wiering,	schmidhuber.	solving	pomdps	with	levin	search	and	eira.	(1996)
n schmidhuber,	zhao,	wiering.	shifting	inductive	bias	with	success-story	algorithm,	adaptive	levin	search,	

and	incremental	self-improvement.	(mlj	1997)

n schmidhuber,	zhao,	schraudolph.	reinforcement	learning	with	self-modifying	policies	(1998)
n zhao,	schmidhuber.	solving	a	complex	prisoner   s	dilemma	with	self-modifying	policies.	(1998)
n schmidhuber.	a	general	method	for	incremental	self-improvement	and	multiagent learning.	(1999)
n singh,	lewis,	barto.	where	do	rewards	come	from?	(2009)
n singh,	lewis,	barto.	intrinsically	motivated	reinforcement	learning:	an	evolutionary	perspective		(2010)
n niekum,	spector,	barto.	evolution	of	reward	functions	for	reinforcement	learning	(2011)
n duan et	al.,	(2016)	rl2:	fast	reinforcement	learning	via	slow	reinforcement	learning
n wang	et	al.,	(2016)	learning	to	reinforcement	learn
n finn	et	al.,	(2017)	model-agnostic	meta-learning
n mishra,	rohinenjad et	al.,	(2017)	simple	neural	attentive meta-learner
n frans et	al.,	(2017)	meta-learning	shared	hierarchies

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

many	pieces	to	the	ai	robotics	puzzle
n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

slide	credit:	john	schulman

pieter	abbeel	

slide	credit:	john	schulman

pieter	abbeel	

slide	credit:	john	schulman

pieter	abbeel	

slide	credit:	john	schulman

pieter	abbeel	

hierarchical	reinforcement	learning

n dayan	and	hinton,	1993	feudal	rl
n parr	and	russell,	1998	rl	with	hierarchies	of	machines
n sutton,	precup,	singh,	1999	options
n precup,	2000	temporal	abstraction	in	reinforcement	learning
n dietterich et	al,	2000	maxq
n fox,	moshkovitz,	tishby,	2016	principled	option	learning
n heess et	al,	2016	learning	and	transfer	of	modulated	locomotor	controllers
n vezhnevets et	al	2017	feudal	networks	for	hrl
n bacon,	harb,	precup,	2017		option-critic
n florensa,	duan,	abbeel,	2017	snns	for	hrl
n andreas,	klein,	levine,	2017	policy	sketches

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-learning	formulation

n agent	has	to	solve	a	distribution	of	related	long-horizon	tasks,	
with	the	goal	of	learning	new	tasks	in	the	distribution	quickly	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-learning	shared	hierarchies

goal:	find	subpolicies that	enable	fast	

learning	of	master	policy

[frans,	ho,	chen,	abbeel,	schulman,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

meta-learning	shared	hierarchies

rl2	meta-learning	objective:

max

   

eme    (k)

m " kxk=1

r(    (k)

m ) | rlagent   #

mlsh	meta-learning	objective:

=	find	a	set	of	subpolicies that	enable	

fast	learning	of	the	master	policy

mlsh -- experiment	1:	moving	bandits

   

hope	for
   
    high	level	policy:	standard	bandit	problem

learned	subpolicies: low	level	control	for	each	of	the	targets

episode	duration	=	50,		subpolicy duration	=	10

experiment	2:	maze	navigation

n episode	duration	=	1000
n subpolicy duration	=	200

   

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

experiment	3:	maze	navigation

n episode	duration	=	1000
n subpolicy duration	=	200

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

discovered	three	gaits

mlsh	agent	was	trained	on	nine	separate	mazes.
it	discovered	sub-policies	for	upwards,	rightwards,	and	downwards	movement.

many	pieces	to	the	ai	robotics	puzzle
n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

imitation	learning	in	robotics

[abbeel	et	al.	2008]

[kolter	et	al.	2008]

[ziebart	et	al.	2008]

[schulman	et	al.	2013]

[finn	et	al.	2016]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

imitation	learning

imitation	learning

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

one-shot	imitation	learning

[duan,	andrychowicz,	stadie,	ho,	schneider,	sutskever,	abbeel,	zaremba,	2017]	

one-shot	imitation	learning

[duan,	andrychowicz,	stadie,	ho,	schneider,	sutskever,	abbeel,	zaremba,	2017]	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

one-shot	imitation	learning

[duan,	andrychowicz,	stadie,	ho,	schneider,	sutskever,	abbeel,	zaremba,	2017]	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

learning	a	one-shot	imitator

demo	1	of	task	i

demo	2	of	task	i

one-shot	
imitator

[figure	credit:	bradly stadie]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

proof-of-concept:	block	stacking

n each	task	is	specified	by	a	

desired	final	layout
n example:	abcd

n    place	c	on	top	of	d,	
place	b	on	top	of	c,	
place	a	on	top	of	b.   

[duan,	andrychowicz,	stadie,	ho,	schneider,	sutskever,	abbeel,	zaremba,	2017]	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

evaluation

[duan,	andrychowicz,	stadie,	ho,	schneider,	sutskever,	abbeel,	zaremba,	2017]	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

learning	a	one-shot	imitator	with	maml
n meta-learning	loss:

n task	loss	=	behavioral	cloning	loss:					[pomerleau   89,sammut   92]

[finn*,	yu*,	zhang,	abbeel,	levine,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

robot	experiments:	learning	to	place
n meta-testing	targets	/	objects

n meta-training	targets	/	objects

1,300	demonstrations	for	meta-training

[finn*,	yu*,	zhang,	abbeel,	levine,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

robot	experiments:	learning	to	place

1	demo

imitation

[finn*,	yu*,	zhang,	abbeel,	levine,	2017]

succes rate:	90%

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

robot	experiments:	learning	to	place

1	demo

imitation

[finn*,	yu*,	zhang,	abbeel,	levine,	2017]

succes rate:	90%

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

many	pieces	to	the	ai	robotics	puzzle
n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

why	lifelong	learning?

n current	machine	learning	paradigm:

n step	1:	run	machine	learning
n step	2:	deploy
n all	learning	happens	ahead	of	time

n but:	real-world	deployment	will	face	ever	changing	situations

   requires	learning	during	deployment

related	work

n

n

n

n

n mccloskey	and	cohen,	catastrophic	id136	in	connectionist	networks:	the	sequential	learning	

problem	(1989)
thrun, is	learning	the	n-th thing	any	easier	than	learning	the	first?	(nips	1996)
thrun,	lifelong	learning	algorithms	(1998)
schmidhuber,	zhao,	schraudolph,	reinforcement	learning	with	self-modifying	policies	(1998)
bowling,	convergence	and	regret	in	multi-agent	learning	(2005)
conitzer and	sandholm,	awesome	(2007)

n
n hadsell	et	al,	learning	long-range	vision	for	autonomous	off-road	driving	(2009)
n
n mitchell	et	al,	never	ending	learning	(2015)
n

silver,	yang,	li,	lifelong	ml	systems:	beyond	learning	algorithms	(2013)

rusu et	al,	progressive	nns	(2016)
kirkpatrick	et	al,	overcoming	catastrophic	forgetting	in	nns	(2016)

n
n gradient	episodic	memory	for	continual	learning,	david	lopez-paz, marc'aurelio ranzato (2017)
n

finn,	abbeel,	levine,	lifelong	few-shot	learning	(2017)
tessler et	al,	a	deep	hierarchical	approach	to	lifelong	learning	in	minecraft	(2017)

n

continuous	adaptation

n a	formulation

n train	agent	to	be	good	at	dealing	with	non-stationary	environments:

n example	sources	of	nonstationarity:

n dynamics
n competitors	who	learn

[al-shedivat,	bansal,	burda,	sutskever,	mordatch,	abbeel,	2017]

robosumo

[al-shedivat,	bansal,	burda,	sutskever,	mordatch,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

continual	learning	-- robosumo

n ant	(4	legs)	meta-trained				vs.					bug	(6	legs)	regular	training

[al-shedivat,	bansal,	burda,	sutskever,	mordatch,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

robosumo     population	dynamics

[al-shedivat,	bansal,	burda,	sutskever,	mordatch,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

many	pieces	to	the	ai	robotics	puzzle
n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

motivation	for	simulation

compared	to	the	real	world,	simulated	data	collection	is   
n less	expensive
n faster	/	more	scalable
n less	dangerous
n easier	to	label

how	can	we	learn	useful	real-
world	skills	in	the	simulator?

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

approach	1	    use	realistic	simulated	data

simulation

real	
world
carefully	match	the	simulation	to	
the	world	[1,2,3,4]

[1]	stephen	james,	edward	johns.		3d	simulation	for	robot	arm	control	
with	deep	id24 (2016)
[2]	johns,	leutenegger,	davision.		deep	learning	a	grasp	function	for	
grasping	under	gripper	pose	uncertainty	(2016)	
[3]	mahler	et	al,	dex-net	3.0	(2017)
[4]	koenemann et	al.		whole-body	model-predictive	control	applied	to	
the	hrp-2	humanoid.	(2015)

gta	v

real	
world

augment	simulated	data	with	real	
data	[5,6]

[5]	stephan	r	richter,	vibhav vineet,	stefan	roth,	and	vladlen
koltun.	playing	for	data:	ground	truth	from	computer	games	
(2016)
[6]	bousmalis et	al.	using	simulation	and	domain	adaptation	to	
improve	efficiency	of	robotic	grasping	(2017)

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

approach	2	    domain	confusion	/	adaptation	

eric	tzeng,	judy	hoffman,	ning zhang,	kate	saenko,	
trevor	darrell.	deep	domain	confusion:	maximizing	
for	domain	invariance.	arxiv preprint	arxiv:1412.3474,	
2014.	

andrei	a	rusu,	matej	vecerik,	thomas	rotho	  rl,	
nicolas	heess,	razvan	pascanu,	and	raia	hadsell.	sim-
to-real	robot	learning	from	pixels	with	progressive	
nets.	arxiv	preprint	arxiv:1610.04286,	2016.	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

approach	3     domain	randomization

if	the	model	sees	enough	simulated	variation,	the	real	world	

may	look	like	just	the	next	simulator	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

domain	randomization

(cad)  2	rl:	real	single-image	flight	without	a	single	real	image.	

n quadcopter	collision	

avoidance

n ~500	semi-realistic	

textures,	12	floor	plans

n ~40-50%	of	1000m	

trajectories	are	
collision-free

[3]	fereshteh sadeghi and	sergey	levine.	(cad)  2	rl:	real	single-image	flight	without	a	single	real	image.	arxiv preprint	
arxiv:1611.04201,	2016.	

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

domain	randomization	for	pose	estimation

n precise	object	pose	

localization

n 100k	images	with	simple	

randomly	generated	
textures

[tobin,	fong,	ray,	schneider,	zaremba,	abbeel,	2017]

application	of	our	method

[tobin,	fong,	ray,	schneider,	zaremba,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

pre-training	is	not	necessary

[tobin,	fong,	ray,	schneider,	zaremba,	abbeel,	2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

many	pieces	to	the	ai	robotics	puzzle
n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted	from	real	world	experience

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

reward	signal	in	reinforcement	learning?

[source:	yann	lecun]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

reward	signal	in	reinforcement	learning?

[source:	yann	lecun]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

reward	signal	in	reinforcement	learning?

auxiliary	losses	in	deep	rl:
    schmidhuber,	1990
    schmidhuber,	1991
   
lin	and	mitchell,	1992
   
jaderberg et	al	2016
    mirowski et	al	2017
    shelhamer et	al	2016
    pinto	and	gupta	2016
   
   
       

lample and	chaplot,	2017
liu,	gupta,	et	al	2017

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

reward	signal	in	reinforcement	learning?

standard	rl

hindsight	experience	replay

[andrychowicz et	al,	nips	2017]

see	also:	schmidhuber and	huber	(1990);	caruana (1998);	da	silva	et	al	(2012);	kober et	al	(2012);	
devin	et	al	(2016);	pinto	and	gupta	(2016);	foster	and	dayan	(2002);	sutton	et	al	(2011);	bakker	and	
schmidhuber (2014);	vezhnevets et	al	(2017)
most	closely	related:	schaul et	al,	2015	universal	value	functions

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

hindsight	experience	replay

n main	idea:	get	reward	signal	from	any	experience	by	simply	assuming	the	

goal	equals	whatever	happened

n representation	learned:	universal	q-function	q(s, g, a)

n replay	buffer:

n actual	experience:		

(st, gt, at, rt, st+1)

n experience	replay:

n hindsight	replay:		

xk from replay bu   er
xk from replay bu   er

(q   (sk, gk, ak)   (rk +   max

a

q   old(sk+1, gk, a))2

(q   (sk, g0k,ak)   (r0k +   max

a

q   old(sk+1, g0k, a))2

[andrychowicz et	al,	2017]			[related	work:	cabi et	al,	2017]

pieter	abbeel	-- embody.ai /	uc	berkeley	/	gradescope

hindsight	experience	replay:	pushing,	sliding,	pick-and-place

[andrychowicz,	wolski,	ray,	schneider,	fong,	welinder,	mcgrew,	tobin,	abbeel,	zaremba,	nips2017]

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

quantitative	evaluation

~3s/episode	   1.5	days	for	50	epochs,	6	days	for	200	epochs

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

many	exciting	challenges	in	ai	for	robotics!

n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted

n safe	learning
n value	alignment
n planning	+	learning
n    

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

many	exciting	challenges	in	ai	for	robotics!

n safe	learning
n value	alignment
recurring	theme:	
n planning	+	learning
meta-learning
n    

n fast	reinforcement	learning
n long	horizon	reasoning
n taskability
n lifelong	learning
n leverage	simulation
n maximize	signal	extracted

enables	discovering	algorithms	that	are	

powered	by	data/experience	(vs.	just	human	ingenuity)

requires	more	compute

which	is	something	we	are	continuing	to	get

pieter	abbeel	-- embody.ai	/	uc	berkeley	/	gradescope

chelsea	finn	[3,6]

sergey	levine	[3,6]

yan	duan [1,5]

john	schulman	[1,4,5] xi	chen	[1,2,4] peter	bartlett	[1] ilya	sutskever [1,5,7] marcin	andrychowicz [5,9]

kevin	frans [4]

jonathan	ho	[4,5] jonas	schneider	[5,8,	9] wojciech zaremba [5,8,9] josh	tobin	[8,9]

rachel	fong	[8,9] alex	ray	[8,9]

bradly	stadie [5]

peter	welinder [9]

bob	mcgrew	[9]

filip	wolski [9]

nikhil	mishra	[3]

m.	rohaninejad [3]

tianhe yu	[5]

maruan

trapit

yura burda [7]

igor	mordatch [7]

al-shedivat [7]

bansal	[7]

[1]	rl2,	duan,	schulman,	chen,	bartlett,	sutskever,	abbeel,	2016
[2]	simple	neural	attentive	meta-learner,	mishra*,	rohaninejad*,	chen,	abbeel,	2017
[3]	maml,	finn,	abbeel,	levine,	2017
[4]	meta-learning	shared	hierarchies,	frans,	ho,	chen,	abbeel,	schulman,	2017
[5]	one-shot	imitation,	duan,	andrychowicz,	stadie,	ho,	et	al,	2017
[6]	one-shot	visual	imitation	learning,	finn*,	yu*,	zhang,	abbeel,	levine,	2017

[7]	continuous	adaptation,	al-shedivat,	bansal,	burda,	sutskever,	
mordatch,	abbeel,	2017
[8]	domain	randomization	for	transferring	deep	neural	nets	from	sim	
to	real	world,	tobin,	fong,	ray,	schneider,	zaremba,	abbeel,	2017	
[9]	hindsight	experience	replay,	andrychowicz,	wolski,	ray,	schneider,	
fong,	welinder,	mcgrew,	tobin,	abbeel,	zaremba,	2017

