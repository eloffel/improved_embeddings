interpretable	ml	symposium	

spotlight	talks	

causal generative neural networks

learning functional causal models and simulate intervention

olivier goudet   , diviyan kalainathan   , philippe caillou   , isabelle guyon      ,
david lopez-paz   , mich`ele sebag   
    tau team, inria-universit  e paris-saclay, france
    chalearn, berkeley, california
    facebook ai research

1

goals

    inputs: observational data
    outputs: fully directed acyclic graph, generative model

2

new framework: causal generative neural networks (cgnns)

3

learning distributions with cgnn

find the graph g minimizing mmd(g) +   |g|

4

cgnn in a nutshell

    advantages:

    exploitation of both conditional independencies and

distribution asymmetries of the data

    model explicitly multivariate dependencies between inputs and

output (xor learnable)

    non-parametric
    a simulation tool for practitioners

    main limitation: computationally expensive
    working paper available on arxiv:
causal generative neural networks

    code available at:

https://github.com/diviyan-kalainathan/

causaldiscoverytoolbox

5

interpretable	ml	symposium	

spotlight	talks	

detecting bias in black-box models
using transparent model distillation

sarah tan*, rich caruana^, giles hooker*, yin lou`
*cornell university
^microsoft research
`airbnb, inc.

risk of default: 7

black-box risk score

default: yes or no

actual outcome

risk of default: 7

model

distillation

black-box risk score

transparent model

default: yes or no

actual outcome

risk of default: 7

model

distillation

black-box risk score

transparent model

compare,

detect differences

default: yes or no

 bias

actual outcome

detecting bias in compas risk score 

detecting bias in compas risk score 

transparent model distillation lets us:

1. mimic black-box model to see what   s inside
a. check if black-box model used a protected variable

2. detect differences between black-box model and outcome model

a.
b.
c.

find variables predictive of outcomes but likely ignored by black-box model
find variables not predictive of outcomes but likely used by black-box model
exclude / include variables to detect bias

interpretable	ml	symposium	

spotlight	talks	

the doctor just won   t accept that!

zachary c. lipton

https://arxiv.org/abs/1711.08037

doctors want explanations!

overlooked questions

    what do stakeholders want?

    are these demands well de   ned?

    are they reasonable?

    can a supervised learning algorithm provide them?

    addressing fundamental questions or placating users?

blindspots in ml culture

    ml long on technical work, short on critical thinking

    technical confs publish nebulous claims, but not papers 
that address their foundations

    interpretable ml is touted to address the demands of 
stakeholders, but where are they?

thanks!

papers:
the mythos of model interpretability (icml whi 2016, forthcoming in cacm)
https://arxiv.org/abs/1606.03490 
the doctor just won   t accept that (here!)
https://arxiv.org/abs/1711.08037 
contact:
zlipton@cmu.edu
@zacharylipton
approximatelycorrect.com

5

interpretable	ml	symposium	

spotlight	talks	

the intriguing properties of model explanations

maruan al-shedivat       avinava dubey       eric p. xing

interpretable ml symposium

nips 2017

what is explanation?

explanation is a simple model that approximates 
the decision boundary of a complex model.

questions:

decision 
boundary

?

explanation

post-hoc (lime)
1) fit the model
2) fit an explanation

joint (cen)
learn a model that 
generates explanations

1. how do feature selection and feature 

noise affect explanations?

2. when explanation is a part of the learning 

and prediction process, how does that 
affect performance of the model?

3. what insights we can gain by visualizing 

and inspecting explanations?

are explanations consistent?

explanations are as good as the features they 
use to explain predictions. what is the effect of 
feature noise on the generated explanations?

post-hoc (lime)

joint (cen)

post-hoc explanations may overfit!

corrupt interpretable features with noise

(more details on the poster)

how using explanations affects performance?

explanations may affect predictive performance 
of the model. but why and how?

performance for the best models of each type

turns out:

    abundant data regime

cens perform as well as their vanilla deep 
network counterparts.
    scarce data regime

   learning to explain    regularizes the model 
and improves performance.

(more details on the poster)

visualising and inspecting explanations

for more, please come and see our poster!

interpretable	ml	symposium	

spotlight	talks	

using kl-divergence to focus deep 
visual explanation 

housam khalifa bashier babiker
randy goebel
alberta machine intelligence  institute
university of alberta
edmonton, alberta 
canada
khalifab, rgoebel@ualberta.ca

this dog is not standing

lecun, bengio, hinton 436 | nature | vol 521 | 28 may 2015 

spotlight presentation, nips 2017 symposium on interpretable machine learning, december 7, 2017

deep visual explanation

   

   

input stream

classifier
performance

spotlight presentation, nips 2017 symposium on interpretable machine learning, december 7, 2017

deep visual explanation of what?

kullback   leibler
divergence:  a measure 
of how one id203 
distribution diverges 
from a second expected 
id203 distribution.

dkl(p  q) is often 
called the information 
gain achieved if 
distribution estimate p
is used instead of 
distribution estimate q

spotlight presentation, nips 2017 symposium on interpretable machine learning, december 7, 2017

dve heuristic: kl-divergence

    compute the joint probabilities for both ground truth and predicted 

label

    estimate the gradient    between the joint probabilities computed

ki kj refer, respectively, to a
ground truth label or an 
estimated label

    approximate the weights y, y   , to capture the discriminative pixels 

employed by the network to make decision.

z is the gradient determined from the joint 
probabilities of each distribution. this is the actual 
kl-divergence gradient.

the final discriminative map arises by considering each feature map

the attribute weights are based on the 
mean and the standard deviation of the 
gradient

   

spotlight presentation, nips 2017 symposium on interpretable machine learning, december 7, 2017

deep visual explanation

input

heat map

explanation

*

*

=

=

spotlight presentation, nips 2017 symposium on interpretable machine learning, december 7, 2017

interpretable	ml	symposium	

spotlight	talks	

grounding	visual	explanations

lisa	anne	hendricks1,	ronghang hu1,	trevor	darrell1,	zeynep akata2

1uc	berkeley,	2university	of	amsterdam

visual	explanations	discuss	discriminative	features	in	an	image	which	justify	deep	network	classification	decisions.

explanation	[1]:this	is	a	cardinal	because	this	is	a	
red	bird	with	a	black	cheek	patch	and	a	red	beak.

[1]	hendricks	et	al.	generating	visual	explanations.	eccv	2016.

grounding	visual	explanations.
hendricks,	hu,	darrell,	akata
nips	interpretable	ml	symposium

visual	explanations:		are	they	image	relevant?

explanation	[1]:		this	is	a	rufous	hummingbird
because	this	is	a	green	bird	with	a	red	throat	and	

a	long	black	beak.

ours:	this	is	a	rufous	hummingbird because	this	
is	a	small	brown	bird	with	a	white	breast and	long	

black	beak.

[1]	hendricks	et	al.	generating	visual	explanations.	eccv	2016.

grounding	visual	explanations.
hendricks,	hu,	darrell,	akata
nips	interpretable	ml	symposium

phrase-critic

grounding	visual	explanations.
hendricks,	hu,	darrell,	akata
nips	interpretable	ml	symposium

quantitative	results

higher is 

better

grounding	visual	explanations.
hendricks,	hu,	darrell,	akata
nips	interpretable	ml	symposium

this	is	a	golden	winged	warbler	because   

qualitative	results

ours:		   this	is	a	grey	bird	with	with	a	yellow	crown	and	a	black	cheek	patch.
phrase-critic	score:		2.36
[1]:		   this	is	a	grey	bird	with	a	yellow	wing	and	a	black	crown.
phrase-critic	score:		0.24

this	is	a	hooded	oriole	because   

ours:		   this	is	a	yellow	bird	with	a	black	throat	and	a	black	beak.
phrase-critic	score:		3.35
[1]:		   this	is	a	yellow	bird	with	a	black	wing	and	a	black	beak.
phrase-critic	score:		2.50

grounding	visual	explanations.
hendricks,	hu,	darrell,	akata
nips	interpretable	ml	symposium

interpretable	ml	symposium	

spotlight	talks	

with even amount of white space

photos placed in horizontal position 

id203 series expansion classifier 
that is interpretable by design

between photos and header

sapan agarwal and corey hudson

sandia national laboratories is a multimission laboratory managed and operated by national technology and engineering solutions of sandia, llc, a wholly 
owned subsidiary of honeywell international, inc., for the u.s. department of energy   s national nuclear security administration under contract de-na0003525.

want a fast, adaptable, explainable 

classifier

find the id203 of outcome      = 1

given measured features     1 =     1   ,     2 =     2   ,     2 =     3   

estimate           = 1|     1 =     1   ,     2 =     2   ,     3 =     3   

from training data, measure the following probabilities:

          = 1 = 0.8

          = 1|     1 =     1    = 0.001
          = 1|     2 =     2    = 0.9
          = 1|     3 =     3    = 0.7

          = 1|     1 =     1   ,     2 =     2    = 0

how do we combine these to get the best overall 
id203 estimate?

2

no good 1st principles method

    bayesian methods degenerate to values of 0 and 1
    all measured values are estimates
    one route to determine the importance of features is by averaging the 

probabilities

          = 1|     1 =     1   ,     2 =     2   ,     3 =     3       

                =1|         =           

       1

    need to account for many effects:

    as a p approaches 0 or 1, it should be given more weight
    measured probabilities with less supporting data should be given less weight
    measured probabilities that give new information should possibly be given 

more weight

use a hierarchical weighted average

3

preliminary results

dataset

new classifier

id79s

1984 congressional voting
records

96.8%

e. coli promotor gene sequences

95.3%

spect heart data

lymphography

85.0%

86.4%

96.8%

94.3%   

85.0%

87.2%

hyper parameters for both classifiers optimized using 4 fold cross validation 

4

use model to analyze a misclassified 
result

the classifier predicted with 70% id203 that this member of 
congress would be a republican when they are a democrat. 

features

counts 
rep.

counts 
dem.

id203 
rep.

weight cumulative 

weight

x3=1, x9=1, x15=0

x9=1, x15=0

x3=1, x8=0, x9=1, x14=0

x3=1, x7=0, x9=1, x14=0

x2=1, x10=1, x11=0

x1=1, x2=1, x10=1

16

16

47

47

0

0

0

0

0

0

58

38

100% 12.5%

100%

100%

100%

0%

0%

7.0%

4.8%

4.7%

4.0%

3.1%

12.5%

19.4%

24.2%

28.9%

32.9%

36.0%

feature #

id203
republican

weight

feature #

id203
republican

weight

95%
94%
94%
14%
23%
21%
89%
53%

x9=1
x3=1
x15=0
x2=1
x10=1
x11=0
x14=0
x1=1
republican features have higher certainty and 
therefore higher weight

x8=0
x7=0
x6=0
x0=0
x13=1
x4=1
x12=1
x5=1

86%
83%
29%
78%
55%
69%
52%
56%

17%
16%
14%
8%
8%
6%
5%
4%

4%
4%
3%
3%
2%
2%
2%
2%

1984 congressional voting 

records dataset
house bill description

feature 
number

0 handicapped infants
1 water project cost sharing
2 adoption of the budget 

resolution

3 physician fee freeze
4 el salvador aid
5 religious groups in 

schools

6 anti-satellite test ban
7 aid to nicaraguan contras
8 mx missile
9 immigration

10 synfuels corporation 

cutback

11 education spending
12 superfund right to sue
13 crime
14 duty free exports
15 export administration act 

south africa

5

interpretable	ml	symposium	

spotlight	talks	

   

   

   

   

   

   

   

   

   

   

   

   

   

2(w1+w2)3+1

   

   

   

   

min                (fcqwz)
(c,q,w,z)         

s.t.

vi  = fcqwz(x(i)),       i   i
    (v,y)       

    fcqwz

        

        

        

    (x,y)

c,q,w,z

v

   

   

      
    m m
    d

   m m d

+        ,    

   

   

   

   

d 

d = 3       2(     +      )     3       2    

3 c  2m 3 c  2m (m+c) 3   2

c4   3

  m

3 c  2m c3   2

c3   2

  

3 c  2m 3 c  2m

  m+  

c  +  

  

  

   

   

   

interpretable	ml	symposium	

spotlight	talks	

   i know it when i see it   . visualization 

and intuitive interpretability

fabian o   ert 

machine learning and the arts research group /


experimental visualization lab


media arts and technology program

university of california, santa barbara


o   ert@ucsb.edu

zentralwerkstatt.org

what is the epistemic value of intuitively interpretable images?

   intuitively interpretable    images

training

id84

id173

visualization

   intuitively interpretable    images

anonymus, 1892. rabbit-duck illusion. 

fliegende bl  tter 97, p. 17.

beyer, k., goldstein, j., ramakrishnan, r., shaft, u., 1999. when is    nearest neighbor    
meaningful?, in: international conference on database theory. springer, pp. 217   235.

derrida, j., 1982. di     rance, in: margins of philosophy. university of chicago press. 

szegedy, c., zaremba, w., sutskever, i., bruna, j., erhan, d., goodfellow, i., fergus, r., 2013. 
intriguing properties of neural networks. arxiv preprint arxiv:1312.6199.

gabriel goh: image synthesis from yahoo   s open_nsfw

goh, g., 2016. image synthesis from 
yahoo   s open_nsfw. blog: gabriel 
goh.

nguyen, a., dosovitskiy, a., yosinski, 
j., brox, t., clune, j., 2016. 
synthesizing the preferred inputs for 
neurons in neural networks via deep 
generator networks, in: advances in 
neural information processing 
systems. pp. 3387   3395. 

id163 classes for 1k    least pornographic    images

id163 classes for 1k    least pornographic    images

   cli      , 0.98

   cli      , 0.97

   cli      , 0.95

   cli      , 0.93

   cli      , 0.96

   cli      , 0.95

   cli      , 0.96

   valley   , 0.53

   dam   , 0.93

   cli      , 0.93

   cli      , 0.92

   cli      , 0.96

