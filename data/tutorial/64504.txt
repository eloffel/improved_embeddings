interpretability for ai safety

victoria krakovna

interpretability and long-term safety

    how can interpretability help us build safer ai systems?

    ensuring safe behavior generalizes
    identifying causes of unsafe behavior

    for example, what is an rl agent 'thinking' when it exploits a 

loophole in its reward function?

    interpretability is particularly crucial if we want to identify 

potential safety issues before deploying the system
    e.g. if certain errors are unacceptable even during training

    as we build more and more general ai systems, what kind of 

understanding is most helpful for safety?

interpretability for ai safety     victoria krakovna

ai safety problems

long-term ai safety
    reliably specifying human preferences and values to 

advanced ai systems

    setting incentives for ai systems that are aligned with 

these preferences
research challenges
    specification problems: if some important variables or 

considerations are omitted from the objective specification 
(e.g. reward function)

    robustness problems: issues that arise even when 

specification is correct (e.g. during learning)

papers: concrete problems in ai safety, ai safety gridworlds

interpretability for ai safety     victoria krakovna

specification: reward gaming

problem
    difficult to specify reward functions to correctly 

reflect human preferences

    rl agents can find shortcuts to getting lots of 
reward without achieving the intended objective

source: faulty reward functions post 

(amodei and clark)

interpretability for ai safety     victoria krakovna

specification: off switch

problem
    we want to be able to shut down our agents
    agents have an incentive to avoid shutdown if it 

results in getting less reward

    don't want agents to seek shutdown either - 

need indifference to shutdown

source: the off switch presentation (hadfield-menell)

interpretability for ai safety     victoria krakovna

specification: side effects
problem
    want agents to avoid unnecessary disruptions to 

the environment while achieving the objective

    need general solutions that don't rely on specifying 

a penalty for every possible disruption

interpretability for ai safety     victoria krakovna

robustness: distributional shift

problem
    we often apply our systems in a different regime 

from the training regime

    want them to adapt or fail gracefully

interpretability for ai safety     victoria krakovna

robustness: unsafe exploration

problem
    there are some errors we don't want our agent to 

make even during training 

    want the agent to always follow safety constraints 

to avoid damage to itself or its surroundings

interpretability for ai safety     victoria krakovna

relevant forms of interpretability

global: representations

analyzing representations 
for specific units / layers

analyzing representations in 

id23

learning disentangled representations

source: feature visualization post 

(olah et al)

source: understanding id25 paper 

(zahavy et al)

source: beta-vae paper (higgins et al)

interpretability for ai safety     victoria krakovna

relevant forms of interpretability

local: what influences a prediction / decision?

influential features

influential data points

text explanations

source: lime paper (ribeiro et al)

source: influence functions paper 

(koh and liang)

source: visual explanations paper 

(hendricks et al)

interpretability for ai safety     victoria krakovna

specification: reward gaming

source: faulty reward functions post 

(amodei and clark)

problem
    difficult to specify reward functions to correctly 

reflect human preferences

    rl agents can find shortcuts to getting lots of 
reward without achieving the intended objective

desired interpretability examples
    does the agent have representations that 
indicate understanding of the objective? 
(eg "racetrack", "finish line")

    to what extent are influential data points 

associated with the objective?

interpretability for ai safety     victoria krakovna

specification: off switch

problem
    we want to be able to shut down our agents
    agents have an incentive to avoid shutdown if it 

results in getting less reward

source: the off switch presentation (hadfield-menell)

    don't want agents to seek shutdown either - 

need indifference to shutdown
desired interpretability examples
    does the agent have a representation of an 

'off switch'? is it being used in decisions?

    an explanation why the agent took a longer 

path to press the button

interpretability for ai safety     victoria krakovna

specification: side effects
problem
    want agents to avoid unnecessary disruptions to 

the environment while achieving the objective

    need general solutions that don't rely on specifying 

a penalty for every possible disruption
desired interpretability examples
    does the agent have/use representations like 

'broken' or 'stuck'? (or something else related to 
reversibility)

    how much do features corresponding to other 

objects in the room influence its decisions?

interpretability for ai safety     victoria krakovna

robustness: distributional shift

problem
    we often apply our systems in a different regime 

from the training regime

    want them to adapt or fail gracefully

desired interpretability examples
how much do decisions rely on 
representations or features that 
are specific to the training setting?

interpretability for ai safety     victoria krakovna

robustness: unsafe exploration

problem
    there are some errors we don't want our agent to 

make even during training 

    want the agent to always follow safety constraints 

to avoid damage to itself or its surroundings

desired interpretability examples
    does the agent have/use representations 

like 'broken' or 'danger'?

    does the agent's explanation of its chosen 

actions refer to safety constraints?

interpretability for ai safety     victoria krakovna

summary: desired interpretability

interpretation

representations

influential data 
points

influential features

reward 
gaming

objective-
related?

objective-
related?

off switch

side effects

distributional 
shift

unsafe 
exploration

"turn off"

"broken", 
"stuck"

specific to 
training?

"broken", 
"danger"

being 
interrupted?

other objects in 
environment

other objects in 
environment

specific to 
training?

explanations

objective- 
related?

why disabled 
off switch?

why caused 
disruption?

dangerous 
situations

proximity to 
bad states

aware of safety 
constraints?

interpretability for ai safety     victoria krakovna

summary: desired interpretability

    both global and local forms of interpretability 

are helpful for safety

will i get 
switched off?

    identifying representations is particularly 

useful for all the safety problems:
    identify representations without easy 

visualizations, like "off switch"

    check if the agent uses specific 

representations

    need more work on interpretability of 
id23 agents, not just 
image classifiers

will i get 
stuck in that 
state?

interpretability for ai safety     victoria krakovna

safety as a target

    what can safety do for interpretability?
    interpretability would be less important if our ai systems were 100% robust 

and made no mistakes

    what is interpretability? what kind of understanding is important?

    whatever can contribute to ensuring safety!

    safety questions can serve as grounding for interpretability questions

    a nail for the interpretability hammer

interpretability for ai safety     victoria krakovna

takeaways

    interpretability is important for long-term safety, and safety can serve as 

grounding for interpretability

    think about how your interpretability methods can apply to advanced ai 

systems

    if you're interested in the intersection of interpretability and long-term safety, 

consider applying for a future of life institute grant

    join us in working on these challenging problems and making advanced ai 

safer!

thank you!

interpretability for ai safety     victoria krakovna

