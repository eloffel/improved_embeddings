differentially private 

machine learning
theory, algorithms, and applications

kamalika chaudhuri 

(ucsd)

anand d. sarwate 

(rutgers)

logistics and goals

    tutorial time: 2 hr (15 min break after    rst hour)
    what this tutorial will do: 

    motivate and de   ne differential privacy
    provide overview of common methods and tools to 

design differentially private ml algorithms 

    what this tutorial will not do: 

    provide detailed results on the state of the art in 
differentially private versions of speci   c problems

learning outcomes

at the end of the tutorial, you should be able to:
    explain the de   nition of differential privacy,
    design basic differentially private machine learning 

algorithms using standard tools,

    try different approaches for introducing differential 

privacy into optimization methods,

    understand the basics of privacy risk accounting,
    understand how these ideas blend together in more 

complex systems.

motivating 

differential privacy

sensitive data

medical records

genetic data

search logs

aol violates privacy

aol violates privacy

net   ix violates privacy [ns08]

movies%

user%1%
user%2%
user%3%

2-8 movie-ratings and dates for alice reveals:

whether alice is in the dataset or not
alice   s other movie ratings

high-dimensional data is unique

example: ucsd employee salary table

position

gender

department

ethnicity

faculty

female

cse

se asian

salary

-

one employee (kamalika)    ts description!

simply anonymizing data is unsafe!

disease association studies [wlwtz09]

cancer

healthy

correlations

correlations

correlation (r2 values),  alice   s dna reveals:
if alice is in the cancer set or healthy set

simply anonymizing data is unsafe!
statistics on small data sets is unsafe!

privacy

data size

accuracy

privacy de   nition

the setting

privacy barrier

privacy-preserving

sanitizer

private
data set
d

data

non-public

sanitizer

(sensitive)

statistics 

summary
statistic

synthetic
data release
dataset

ml model
public

public

property of sanitizer

privacy barrier

privacy-preserving

sanitizer

private
data set
d

non-public

sanitizer 

data 

summary
statistic
statistics 

synthetic
dataset
data release
ml model
public
public 

aggregate information computable

individual information protected 
(robust to side-information)

differential privacy

data  +

data  +

algorithm

outcome

algorithm

outcome

participation of a person does not change outcome  

since a person has agency, they can decide 

to participate in a dataset or not  

adversary

prior knowledge:

a   s genetic pro   le
a smokes

case 1:

study

a has
cancer

cancer

[ study violates a   s privacy ]

adversary

prior knowledge:

a   s genetic pro   le
a smokes

case 1:

study

adversary

a has
cancer

cancer

[ study violates a   s privacy ]

case 2:

study

prior knowledge:

a   s genetic pro   le
a smokes

a probably 
has cancer

smoking causes cancer

[ study does not violate privacy]

differential privacy [dmns06]

data  +

data  +

algorithm

outcome

algorithm

outcome

participation of a person does not change outcome  

since a person has agency, they can decide 

to participate in a dataset or not  

how to ensure this?

   through randomness

random 
variables

a(

data  +

a(

data  +

)

)

have close 
distributions

how to ensure this?

random 
variables

a(

data  +

a(

data  +

)

)

have close 
distributions

randomness:  added by randomized algorithm a

closeness: likelihood ratio at every point bounded

differential privacy [dmns06]

d
p[a(d) = t] 

d   

  

t

p[a(d   ) = t] 

for all d, d    that differ in one person   s value,
if a =   -differentially private randomized algorithm, then:

   

sup

t

    log

p(a(d) = t)

p(a(d0) = t)           

differential privacy [dmns06]

d
p[a(d) = t] 

d   

  

t

p[a(d   ) = t] 

for all d, d    that differ in one person   s value,
if a =   -differentially private randomized algorithm, then:

   

max-divergence of 
p(a(d)) and p(a(d   ))

sup

t

    log

p(a(d) = t)

p(a(d0) = t)           

approx. differential privacy [dkm+06]

d
p[a(d) = t] 

d   

  

t

p[a(d   ) = t] 

for all d, d    that differ in one person   s value,
if a =        -differentially private randomized algorithm, then:

(   ,  )

s,pr(a(d)2s)> h log

max

pr(a(d) 2 s)    

pr(a(d0) 2 s i        

properties of 

differential privacy

property 1: post-processing invariance

privacy barrier

private
data set
d

di   erentially

private
algorithm

(   1,  1)

non-private 

post-

processing

i v e

i v a t

legitimate user 1

(   1,  1)

d a t a   d e r

data derivative

legitimate user 2

(   1,  1)

data derivative

adversary

(   1,  1)

risk doesn   t increase if you don   t touch the data again

property 2: graceful composition

privacy barrier

preprocessing

("1,  1)

private
data set

d

a1
training

(   1,  1)

("2,  2)

(   2,  2)

a2
cross-
validation
   

("3,  3)

testing
a

("4,  4)

(xi

   i,xi

 i)

total privacy loss is the sum of privacy losses

(better composition possible     coming up later)

how to achieve  
differential privacy?

tools for differentially private 

algorithm design

    global sensitivity method [dmns06]
    exponential mechanism [mt07]

many others we will not cover [dl09, nrs07,    ]

global sensitivity method [dmns06]

problem:
given function f, sensitive dataset d
find a differentially private approximation to f(d)

example: f(d) = mean of data points in d

the global sensitivity method [dmns06]

given:  a function f, sensitive dataset d
de   ne:

dist(d, d   ) = #records that d, d    differ by

the global sensitivity method [dmns06]

given:  a function f, sensitive dataset d
de   ne:

dist(d, d   ) = #records that d, d    differ by

global sensitivity of f:

the global sensitivity method [dmns06]

given:  a function f, sensitive dataset d
de   ne:

dist(d, d   ) = #records that d, d    differ by

global sensitivity of f:

s(f) =

| f(d) - f(d   )|

domain(d)

d   

d

the global sensitivity method [dmns06]

given:  a function f, sensitive dataset d
de   ne:

dist(d, d   ) = #records that d, d    differ by

global sensitivity of f:

s(f) =

dist(d, d   ) = 1

| f(d) - f(d   )|

domain(d)

d   

d

the global sensitivity method [dmns06]

given:  a function f, sensitive dataset d
de   ne:

dist(d, d   ) = #records that d, d    differ by

global sensitivity of f:

s(f) =

max

dist(d, d   ) = 1

| f(d) - f(d   )|

domain(d)

d   

d

d   

d

laplace mechanism

global sensitivity of f is s(f) =

max

dist(d, d   ) = 1

| f(d) - f(d   )|

output       

where

     -differentially
private

laplace distribution:

gaussian mechanism

global sensitivity of f is s(f) =

max

dist(d, d   ) = 1

| f(d) - f(d   )|

output       

where

z    

s(f )
    n (0, 2 ln(1.25/ ))

(   ,  )-differentially 

private

example 1: mean

f(d) = mean(d), where each record is a scalar in [0,1] 

example 1: mean

f(d) = mean(d), where each record is a scalar in [0,1] 

global sensitivity of f = 1/n

example 1: mean

f(d) = mean(d), where each record is a scalar in [0,1] 

global sensitivity of f = 1/n
laplace mechanism:

output       

where z    

1
n   

lap(0, 1)

how to get differential privacy?

    global sensitivity method [dmns06]
    two variants: laplace and gaussian
    exponential mechanism [mt07]

many others we will not cover [dl09, nrs07,    ]

exponential mechanism [mt07]

problem:
given function f(w, d), sensitive data d
find differentially private approximation to

w    = argmax

w

f (w, d)

example: f(w, d) = accuracy of classi   er w on dataset d

the exponential mechanism [mt07]

suppose for any w, 

|f (w, d)   f (w, d0)|     s

when d and d    differ in 1 record.  sample w from:

p(w) / e   f (w,d)/2s

for   -differential privacy.

   

argmax f(w, d)

f(w, d)

pr(w)

w

w

example: parameter tuning

given validation data d,  k classi   ers w1, .., wk 
(privately)    nd the classi   er with highest accuracy on d 

here, f(w, d) = classi   cation accuracy of w on d
for any w, any d and d    that differ by one record,

|f (w, d)   f (w, d0)|    

1
|d|

so, the exponential mechanism outputs wi with prob:

pr(wi) / e   |d|f (wi,d)/2

[cms11, cv13]

summary

    motivation

    what is differential privacy?

    basic differential privacy algorithm design tools:

    the global sensitivity method 

    laplace mechanism
    gaussian mechanism

    exponential mechanism

differential privacy in 

estimation and prediction

estimation and prediction problems

private
data set
d

sample size n

privacy barrier

f (w,  )

risk functional

(",  )

dp estimate of
argmin

f (w, d)

w

  w

private estimator
e[f (   w, z)]   e[f (w   , z)]

statistical estimation: estimate a parameter or 
predictor using private data that has good expected 
performance on future data.

goal: good privacy-accuracy-sample size tradeoff

privacy and accuracy make different 

assumptions about the data

private
data set
d

sample size n

privacy barrier

f (w,  )

risk functional

(",  )

dp estimate of
argmin

f (w, d)

w

  w

private estimator
e[f (   w, z)]   e[f (w   , z)]

privacy     differential privacy makes no assumptions on 
the data distribution: privacy holds unconditionally.

accuracy     accuracy measured w.r.t a    true population 
distribution   : expected excess statistical risk.

statistical learning as risk minimization

w    = argmin

w

1
n

nxi=1

`(w, (xi, yi)) +  r(w)

    empirical risk minimization (erm) is a common paradigm 

for prediction problems.

    produces a predictor w for a label/response y given a 

vector of features/covariates x. 

    typically use a convex id168 and regularizer to 

   prevent over   tting.   

why is erm not private?

w   
+ ++
+
+
   
   
+   
   
   
   
w0   
+ +
+
+
   
   
+   
   
   
+
   

private
data set
d

private
data set
d0

easy for adversary to tell the 
difference between d and d   

d or d0?

w

adversary

[cms11, rbht12]

kernel learning: even worse

    kernel-based methods produce a classi   er that is a 

function of the data points.

    even adversary with black-box access to w could 

potentially learn those points.

w(x) =

nxi=1

   ik(x, xi)

   

+ ++
+
+
   
+   
   
   
   

[cms11]

privacy is compatible with learning

    good learning algorithms generalize to the population 

distribution, not individuals.

    stable learning algorithms generalize [be02].

    differential privacy can be interpreted as a form of stability 

that also implies generalization [dfh+15,bns+16].

    two parts of the same story:   

privacy implies generalization asymptotically.   
tradeoffs between privacy-accuracy-sample size for    nite n.   

revisiting erm

w    = argmin

w

1
n

nxi=1

`(w, (xi, yi)) +  r(w)

    learning using (convex) optimization uses three steps:

1. read in the data

2.

form the objective function

3. perform the minimization

input perturbation
objective perturbation
output perturbation

    we can try to introduce privacy in each step!

privacy in erm: options

input 

perturbation

input 

perturbation

non-private
non-private
preprocessing
preprocessing

private
private
input 
private
perturbation
database
database
database

input 

perturbation

d
d
d

input 

perturbation

input 

perturbation

sanitized 
non-private
dp
dp
optimization
database
optimization
preprocessing
w   
noise addition 
randomization

(",  )

(",  )

or

sanitized
dataset

 non-
  wobjective
private
algorithm

 non-
private
learning

  woutput
  winput

(",  )

privacy
barrier

(",  )

privacy
barrier

privacy
privacy
barrier
barrier

local privacy

input 

perturbation

input 

perturbation

input 

perturbation

input 

perturbation

input 

perturbation

input 

perturbation

(",  )

privacy
barrier

sanitized 
database

 non-
private
algorithm

    local privacy: data contributors sanitize data before collection.

    classical technique: randomized response [w65].

   

interactive variant can be minimax optimal [djw13].

input perturbation

private
database

d

dp

preprocessing

(",  )

sanitized
dataset

 non-
private
learning

  winput

privacy
barrier

   

input perturbation: add noise to the input data.

    advantages: easy to implement, results in reusable 

sanitized data set.

[djw13,fts17]

output perturbation

private
database

d

non-private
preprocessing

non-private
optimization
w   
noise addition 
randomization

or

(",  )

privacy
barrier

  woutput

    compute the minimizer and add noise.

    does not require re-engineering baseline algorithms

noise depends on the sensitivity of the argmin.

[cms11, rbht12]

objective perturbation

non-private
preprocessing

dp

optimization

(",  )

private
database

d

privacy
barrier

  wobjective

objective perturbation

j(w) =

1
n

nxi=1

`(w, (xi, yi)) +  r(w)

a. add a random term to the objective:

  wpriv = argmin

w

 j(w) + w>b 

b. do a randomized approximation of the objective:

  wpriv = argmin

w

  j(w)

randomness depends on the sensitivity properties of j(w).

[cms11, zzx+12]

sensitivity of the argmin

w    = argmin

  wpriv = argmin
w

w

j(w)

 j(w) + w>b 

    non-private optimization solves 

rj(w) = 0 =) w   

    generate vector analogue of laplace: 

b     p(z) / e "/2kzk

    private optimization solves

rj(w) =  b =) wpriv

    have to bound the sensitivity of the gradient.

theoretical bounds on excess risk

input perturbation
output perturbation
objective perturbation

!
!
!

n"
n"
n"

  o pd log(1/ )
  o pd log(1/ )
  o pd log(1/ )
  o    d
  o    d
n"3/2   
n"   

(quadratic loss) 
[cms11, bst14]

[fts17]

same important parameters:
    privacy parameters (  ,   )

    data dimension d

    data bounds

kxik     b

    analytical properties of the loss 

(lipschitz, smoothness)

    id173 parameter

 

typical empirical results

in general:

[cms11]

[jt14]

    objective perturbation 

empirically outperforms output 
perturbation.

    gaussian mechanism with (  ,   ) 
guarantees outperform laplace -
like mechanisms with   -
guarantees.

    loss vs. non-private methods is 

very dataset-dependent.

gaps between theory and practice

    theoretical analysis is for    xed privacy parameters     

how should we choose them in practice? 

    given a data set, can i tell what the privacy-utility-

sample-size tradeoff is?

    what about more general optimization problems/

algorithms?

    what about scaling (computationally) to large data sets?

summary

    training does not on its own guarantee privacy. 

    there are many ways to incorporate dp into prediction 

and learning using erm with different privacy-accuracy-
sample size tradeoffs.

    good dp algorithms should generalize since they learn 

about populations, not individuals.

    theory and experiment show that (  ,   )-dp algorithms 
have better accuracy than   -dp algorithms at the cost of 
a weaker privacy guarantee.

break

differential privacy and 
optimization algorithms

scaling up private optimization

    large data sets are challenging for optimization:   

(cid:15482) batch methods not feasible

    using more data can help our tradeoffs look better:   

(cid:15482) better privacy and accuracy

    online learning involves multiple releases:    

(cid:15482) potential for more privacy loss

goal: guarantee privacy using the optimization algorithm.

stochastic id119

    stochastic id119 (sgd) is a moderately 

popular method for optimization
    stochastic gradients are random    
(cid:15482) already noisy (cid:15482) already private?

    optimization is iterative    

(cid:15482) intermediate results leak information

non-private sgd

j(w) =

1
n

nxi=1

`(w, (xi, yi)) +  r(w)

w0 = 0
for t = 1, 2, . . . , t

    select a random data point

   

take a gradient step

it     unif{1, 2, . . . , n}
gt = r`(wt 1, (xit, yit)) +  rr(wt 1)
wt =    w (wt 1      tgt)

  w = wt

private sgd with noise

j(w) =

1
n

nxi=1

`(w, (xi, yi)) +  r(w)

w0 = 0
for t = 1, 2, . . . , t

    select random data point

    add noise to gradient

it     unif{1, 2, . . . , n}
zt     p(", )(z)
  gt = zt + r`(wt 1, (xit, yit)) +  rr(wt 1)
wt =    w (wt 1      t  gt)

  w = wt

[scs15]

choosing a noise distribution

   laplace    mechanism

p(z) / e("/2)kzk

gaussian mechanism

p(z) i.i.d.    n   0,   o    log(1/ )

"2

      

    have to choose noise according to the sensitivity of the 

(",  )   dp

"   dp

gradient:

max
d,d0

w krj(w; d)   rj(w; d0)k
max

    sensitivity depends on the data distribution, lipschitz 

parameter of the loss, etc.

private sgd with randomized selection

j(w) =

1
n

nxi=1

`(w, (xi, yi)) +  r(w)

    select random data point

    randomly select unbiased 

gradient estimate

w0 = 0
for t = 1, 2, . . . , t

it     unif{1, 2, . . . , n}
gt = r`(wt 1, (xit, yit)) +  rr(wt 1)
  gt     p(", ),g(z)
wt =    w (wt 1      t  gt)

  w = wt

[djw13]

randomized directions

[djw13]

1

1 + e   

=

l
kgk

g w.p.

1
2

+ kgk
2l

e   

1 + e   

b

select hemisphere in 
direction of gradient 

or opposite.

pick uniformly from 
the hemisphere and 

take a step

    need to have control of gradient norms:

kgk     l

    keep some id203 of going in the wrong direction.

why does dp-sgd work?

zt

zt
+
g
=
  g

g

=

l
kgk

g w.p.

1
2

+ kgk
2l

e   

1 + e   

b

1

1 + e   

noisy gradient 

random gradient 

choose noise distribution using 
the sensitivity of the gradient.

randomly select direction biased 
towards the true gradient.

both methods 

    guarantee dp at each iteration.
    ensure unbiased estimate of g to guarantee convergence

making dp-sgd more practical

   sgd is robust to noise    

    true up to a point     for small epsilon (more privacy), the 

gradients can become too noisy.

    solution 1: more iterations ([bst14]: need           )

o(n2)

    solution 2: use standard tricks: mini-batching, etc. [scs13]

    solution 3: use better analysis to show the privacy loss is 

not so bad [bst14][acg+16]

randomly sampling data can amplify 

privacy

privacy barrier
privacy barrier

"
2    

di   erentially 
di   erentially 
private algorithm
private algorithm

private
private
data set
data set
d
d

subsample

 n

    suppose we have an algorithm a which is   -differentially 

private for        1.

    sample   n entries of d uniformly at random and run a 

on those.

    randomized method guarantees 2    -differential privacy.

[bbkn14,bst14]

summary

    stochastic id119 can be made differentially 

private in several ways by randomizing the gradient.

    keeping gradient estimates unbiased will help ensure 

convergence. 

    standard approach for variance reduction/stability (such 

as minibatching) can help with performance.

    random subsampling of the data can amplify privacy 

guarantees.

accounting for total 

privacy risk

measuring total privacy loss

privacy barrier

private
data set
d

di   erentially

private
algorithm

(   1,  1)

non-private 

post-

processing

i v e

i v a t

legitimate user 1

(   1,  1)

d a t a   d e r

data derivative

legitimate user 2

(   1,  1)

data derivative

adversary

(   1,  1)

post processing invariance: risk doesn   t increase if you don   t 
touch the data again
    more complex algorithms have multiple stages    
(cid:15482) all stages have to guarantee dp
    need a way to do privacy accounting: what is lost over time/

multiple queries?

a simple example

privacy barrier

preprocessing

("1,  1)

private
data set

d

training

("2,  2)

cross-
validation

("3,  3)

testing

("4,  4)

composition property of 

differential privacy

basic composition: privacy loss is additive:

    apply r algorithms with 

(   i,  i) : i = 1, 2, . . . , r

    total privacy loss:

  rxi=1

   i,

 i!

rxi=1

    worst-case analysis: each result exposes the worst 

privacy risk. 

what composition says about 

multi-stage methods

privacy barrier

preprocessing

("1,  1)

private
data set

d

training

("2,  2)

cross-
validation

("3,  3)

testing

("4,  4)

total privacy loss is the sum of the privacy losses   

an open question: privacy 
allocation across stages

compositions means we have a privacy budget.

how should we allocate privacy risk across different stages 
of a pipeline?

    noisy features + accurate training?

    clean features + sloppy training?

it   s application dependent! still an open question   

a closer look at    and   

gaussian noise of a given variance produces a spectrum of 
(  ,   ) guarantees:

 1
 2

   1

   2

p(   2|d)
p(   1|d)

p(   2|d0)
p(   1|d0)

p(   2|d)
p(   2|d0)     exp("2)
p(   1|d)
p(   1|d0)     exp("1)

privacy loss as a random 

variable

spectrum of (  ,   ) 
guarantees means we can 
trade off    and    when 
analyzing a particular 
mechanism. 

actual privacy loss is a random variable that depends on d:

zd,d0 = log

p(a(d) = t)
p(a(d0) = t)

w.p. p(a(d) = t)

random privacy loss

zd,d0 = log

p(a(d) = t)
p(a(d0) = t)

w.p. p(a(d) = t)

    bounding the max loss over (d,d   ) is still a random 

variable.

    sequentially computing functions on private data is like 

sequentially sampling independent privacy losses.

    concentration of measure shows that the loss is much 

closer to its expectation.

strong composition bounds
z

(",  ), (",  ), . . . , (",  )

    given only the (  ,   ) 

}|

k times

{

guarantees for k 
algorithms operating 
on the data.

 (k   2i)", 1   (1    )k(1    i) 
`  e(k `)"   e(k 2i+`)" 
`=0 k
 i = pi 1

(1 + e")k

    composition again 
gives a family of    
(  ,   ) tradeoffs: can 
quantify privacy loss 
by choosing any 
valid (  ,   ) pair.

[drv10, kov15]

moments accountant
("1,  1)

privacy barrier

preprocessing

private
data set

a1
training

a2
cross-
validation
   

("2,  2)
a
("3,  3)

(   ,  )

d
basic idea: directly calculate parameters (  ,  )
from composing a sequence of mechanisms

("4,  4)

testing

more ef   cient than composition theorems

[acg+16]

how to compose directly?

given datasets d and d    with one different record,
mechanism a, de   ne privacy loss random variable as:

zd,d0 = log

p(a(d) = t)
p(a(d0) = t)

, w.p. p(a(d) = t)
- properties of zd,d       

related to privacy loss of a 

-

if max absolute value of zd,d      
over all d, d    is   , then a is    
(  ,0)-differentially private

how to compose directly?

given datasets d and d    with one different record,
mechanism a, de   ne privacy loss random variable as:

zd,d0 = log

p(a(d) = t)
p(a(d0) = t)

, w.p. p(a(d) = t)

challenge: to reason about
the worst case over all d, d   

key idea in [acg+16]: use
moment generating functions

accounting for moments   

privacy barrier

preprocessing

("1,  1)

private
data set

d

three steps:

a1
training

a2
cross-
validation
   

testing

("2,  2)
a
("3,  3)

("4,  4)

(   ,  )

1. calculate moment generating functions for a1, a2, ..
2. compose
3. calculate    nal privacy parameters

[acg+16]

1. stepwise moments
("1,  1)

privacy barrier

preprocessing

private
data set

d

a1
training

a2
cross-
validation
   

("2,  2)
a
("3,  3)

("4,  4)
de   ne: stepwise moment at time t of at at any s:

testing

   at(s) = sup
d,d0

log e[eszd,d0 ]

(d and d    differ
by one record)

[acg+16]

2. compose

privacy barrier

preprocessing

("1,  1)

private
data set

d

a1
training

a2
cross-
validation
   

testing

("2,  2)
a
("3,  3)

("4,  4)

theorem:  suppose a = (a1,    , at). for any s:

   a(s)    

   at(s)

txt=1

[acg+16]

3. final calculation
("1,  1)

privacy barrier

preprocessing

a1
training

a2
cross-
validation
   

("2,  2)
a
("3,  3)

(   ,  )

private
data set

d

("4,  4)
theorem: for any   , mechanism a is (  ,  )-dp for 

testing

  = min

exp(   a(s)   s   )

use theorem to    nd best    for a given    from closed form
or by searching over s1, s2, .., sk

[acg+16]

s

example: composing gaussian 

privacy barrier

mechanism

preprocessing

("1,  1)

private
data set

d

a1
training

a2
cross-
validation
   

testing

("2,  2)
a
("3,  3)

("4,  4)

(   ,  )

suppose at answers a query with global sensitivity 1 by 
adding n(0, 1) noise

1. stepwise moments
("1,  1)

privacy barrier

preprocessing

private
data set

d

a1
training

a2
cross-
validation
   

testing

("2,  2)
a
("3,  3)

("4,  4)

suppose at answers a query with global sensitivity 1 by 
adding n(0, 1) noise

simple algebra gives for any s:

   at(s) =

s(s + 1)

2

2. compose

privacy barrier

preprocessing

("1,  1)

private
data set

d

a1
training

a2
cross-
validation
   

testing

("2,  2)
a
("3,  3)

("4,  4)

suppose at answers a query with global sensitivity 1 by 
adding n(0, 1) noise
   a(s)    

   at(s) =

t s(s + 1)

2

txt=1

3. final calculation
("1,  1)

privacy barrier

preprocessing

private
data set

d

a1
training

a2
cross-
validation
   

testing

("2,  2)
a
("3,  3)

("4,  4)

(   ,  )

find lowest    for a given    (or vice versa) by solving:

  = min

s

exp(t s(s + 1)/2   s   )

in this case, solution can be found in closed form.

how does it compare?

n
o

l
i
s
p
e

[drv10]
(better than linear)

moments 
accountant

#rounds of composition

[acg+16]

how does it compare  

on real data?

em for mog
with gaussian 
mechanism
  = 10 4

[pfcw17]

summary

    practical machine learning looks at the data many times.

    post-processing invariance means we just have to track 

the cumulative privacy loss.

    good composition methods use the fact that the actual 
privacy loss may behave much better than the worst-case 
bound.

    the moments accountant method tracks the actual 

privacy loss more accurately: better analysis for better 
privacy guarantees.

applications to modern 

machine learning

when is differential privacy practical?

differential privacy is best suited for understanding 
population-level statistics and structure:

   

id136s about the population should not depend 
strongly on individuals.

    large sample sizes usually mean lower sensitivity and 

less noise.

to build and analyze systems we have to leverage post-
processing invariance and composition properties.

differential privacy in practice

google: rappor for 
tracking statistics in 
chrome.

apple: various iphone 
usage statistics.

census: 2020 us census 
will use differential privacy.

mostly focused on count and average statistics

challenges for machine learning 

applications

differentially private ml is complicated because real ml 
algorithms are complicated:

    multi-stage pipelines, parameter tuning, etc.

    need to    play around    with the data before 
committing to a particular pipeline/algorithm.

       modern    ml approaches (= deep learning) have 

many parameters and less theoretical guidance.

some selected examples

l1, l2, . . .

select random

lot

xl1, xl2, . . .

compute 
gradients

new network

weights
   t

clip and
add noise

update

parameters

privacy
barrier

private
data set
d

moments 
accountant

(",  )

l
a
n
i
g
i
r
o

r
o
i
r
e
t
s
o
p

d
e
s
s
e
c
o
r
p

r
o
i
r
e
t
s
o
p

for today, we will describe some recent examples:

1. differentially private deep learning [acg+16]

2. differential privacy and bayesian id136

differential privacy and deep learning

main idea: train a deep 
network using differentially 
private sgd and use 
moments accountant to 
track privacy loss.

additional components: 
gradient clipping, 
minibatching, data 
augmentation, etc.

[acg+16]

overview of the algorithm

private
data set
d

moments 
accountant

(",  )

l1, l2, . . .

select random

lot

xl1, xl2, . . .

compute 
gradients

new network

weights
   t

clip and
add noise

update

parameters

privacy
barrier

[acg+16]

effectiveness of dp deep learning

empirical results on mnist and cifar: 

    training and test error come close to baseline non-

private deep learning methods.

    to get moderate loss in performance, epsilon and 

delta are not    negligible   

[acg+16]

moving forward in deep learning

this is a good proof of concept for differential privacy for 
deep neural networks. there are lots of interesting 
ways to expand this.

   

just used one nn model: what about other 
architectures? id56s? gans?

    can id173 methods for deep learning (e.g. 

dropout) help with privacy?

    what are good rules of thumb for lot/batch size, 

learning rate, # of hidden units, etc?

[acg+16]

differentially private  
bayesian id136

data x = { x1, x2,     }
model class     

}

related through
likelihood p(x|   )

+

=

prior    (   )

data x

posterior p(   |x)

find differentially private approx to posterior

differentially private  
bayesian id136

approximation

    general methods for private posterior 
    a special case: exponential families
    variational id136

how to make posterior private?

option 1: direct posterior sampling [dmnr14]

not differentially private except under restrictive 
conditions - likelihood ratios may be unbounded!

[gsc17] answer changes under a new relaxation 
r  nyi differential privacy [m17]

how to make posterior private?

option 2: one posterior sample (ops) method [wfs15]

original posteriors

processed posteriors

1. truncate posterior so that likelihood ratio is 
bounded in the truncated region. 

2. raise truncated posterior to a higher temperature

how to make posterior private?

option 2: one posterior sample (ops) method:

advantage:  general

pitfalls: 
    intractable - only exact 
distribution private
    low statistical ef   ciency
even for large n

how to make posterior private?

option 3: approximate the ops distribution via 
stochastic gradient mcmc [wfs15]

original posteriors

processed posteriors
advantage:  noise added during stochastic gradient 
mcmc contributes to privacy

disadvantage: statistical ef   ciency lower than exact ops

differentially private  
bayesian id136

approximation

    general methods for private posterior 
    a special case: exponential families 
    variational id136

exponential family posteriors

(non-private) posterior comes from exp. family:

p(   |x) / e   (   )>(pi t (xi)) b(   )

given data x1, x2,    

posterior depends on data through suf   cient statistic t

exponential family posteriors

(non-private) posterior comes from exp. family:

p(   |x) / e   (   )>(pi t (xi)) b(   )

given data x1, x2,    , suf   cient statistic t

private sampling:
1. if  t is bounded, add noise to             to get private 
version t    
2. sample from the perturbed posterior:

xi

t (xi)

p(   |x) / e   (   )>t 0 b(   )

[zrd16, fgwc16]

how well does it work?

   1.5 x 105

d
o
o
h

i
l

e
k

i
l

   
g
o

l
 
t

e
s
   
t
s
e
t

   2

   2.5

   3

   3.5

   4

   4.5

   5
 
10   1

 

non   private id48
non   private naive bayes
laplace mechanism id48
ops id48 (truncation multiplier = 100)
100

101

epsilon (total)

statistically ef   cient
performance worse than non-private, better than ops
can do id136 in relatively complex systems by building 
up on this method     eg, time series id91 in id48s

differentially private  
bayesian id136

approximation

    general methods for private posterior 
    a special case: exponential families
    variational id136

variational id136

key idea: start with 
a stochastic 
variational id136 
method, and make 
each step private by 
adding laplace noise. 
use moments 
accountant and 
subsampling to track 
privacy loss. 

[jdh16, pfcw16]

summary

    two examples of differentially private complex 
machine learning algorithms
    deep learning 
    bayesian id136

summary, conclusions, 
and where to look next

summary

1. differential privacy: basic de   nitions and mechanisms

2. differential privacy and statistical learning: erm and 

sgd

3. composition and tracking privacy loss

4. applications of differential privacy in ml: deep learning 

and bayesian methods

things we didn   t cover   

    synthetic data generation.

   

interactive data analysis.

    statistical/estimation theory and fundamental limits

    id171 and id84

    systems questions for large-scale deployment

        and many others   

where to learn more

several video lectures and other more technical 
introductions available from the simons institute for the 
theory of computing:

https://simons.berkeley.edu/workshops/bigdata2013-4

monograph by dwork and roth:

http://www.nowpublishers.com/article/details/tcs-042

final takeaways

    differential privacy measures the risk incurred by 

algorithms operating on private data.

    commonly-used tools in machine learning can be made 

differentially private.

    accounting for total privacy loss can enable more 

complex private algorithms.

    still lots of work to be done in both theory and practice.

thanks!

this work was supported by 

national science foundation (iis-1253942, ccf-1453432, satc-1617849)

national institutes of health (1r01da040487-01a1) 

of   ce of naval research (n00014-16-1-2616) 

darpa and the us navy (n66001-15-c-4070)

google faculty research award

