deep	reinforcement	learning		

through	

	policy	op7miza7on	

	

pieter	abbeel	
john	schulman	

open	ai		/	berkeley	ai	research	lab	

	

	

reinforcement	learning	

ut

[figure	source:	subon	&	barto,	1998]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

policy	opmmizamon	

      (u|s)

ut

[figure	source:	subon	&	barto,	1998]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

policy	opmmizamon	

n    consider	control	policy	parameterized	

by	parameter	vector	

   

      (u|s)

ut

max

   

e[

hxt=0

r(st)|      ]

	

n    oqen	stochasmc	policy	class	(smooths	

out	the	problem):	
																				:	id203	of	acmon	u	in	state	s		
      (u|s)

[figure	source:	subon	&	barto,	1998]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

why	policy	opmmizamon	

n    oqen						can	be	simpler	than	q	or	v	

   

n    e.g.,	robomc	grasp	

n    v:	doesn   t	prescribe	acmons	

n    would	need	dynamics	model	(+	compute	1	bellman	back-up)	

n    q:	need	to	be	able	to	e   ciently	solve	

arg max

q   (s, u)

u

n    challenge	for	conmnuous	/	high-dimensional	acmon	spaces*	

*some	recent	work	(parmally)	addressing	this:		

	naf:	gu,	lillicrap,	sutskever,	levine	icml	2016	
	input	convex	nns:	amos,	xu,	kolter	arxiv	2016		

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

example	policy	opmmizamon	success	stories	

kohl	and	stone,	2004	

ng	et	al,	2004	

tedrake	et	al,	2005	

kober	and	peters,	2009	

mnih	et	al,	2015	

(a3c)	

silver	et	al,	2014	

lillicrap	et	al,	2015	

(dpg)	

(ddpg)	

schulman	et	al,	

2016	(trpo	+	gae)	

levine*,	finn*,	et	

al,	2016	
(gps)	

silver*,	huang*,	et	

al,	2016	

(alphago**)	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

policy	opmmizamon	in	the	rl	landscape	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

policy	opmmizamon	in	the	rl	landscape	

id25:	mnih	et	al,	nature	2015	
double	id25:	van	hasselt	et	al,	aaai	2015	
dueling	architecture:	wang	et	al,	icml	2016	
priorimzed	replay:	schaul	et	al,	iclr	2016	
david	silver	icml	2016	tutorial	

outline		

n    derivamve	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ramo	(lr)	policy	gradient	

n    derivamon	/	connecmon	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n   

n   

n   

variance	reducmon	using	value	funcmons	(actor-crimc)	(->	gae,	a3c)	

pathwise	derivamves	(pd)		(->	dpg,	ddpg,	svg)	

stochasmc	computamon	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

inverse	reinforcement	learning	

outline		

n    deriva've	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ramo	(lr)	policy	gradient	

n    derivamon	/	connecmon	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n   

n   

n   

variance	reducmon	using	value	funcmons	(actor-crimc)	(->	gae,	a3c)	

pathwise	derivamves	(pd)		(->	dpg,	ddpg,	svg)	

stochasmc	computamon	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

inverse	reinforcement	learning	

cross-id178	method	

max

   

u (   ) = max

   

e[

hxt=0

r(st)|      ]

n    views	u	as	a	black	box	

n   

ignores	all	other	informamon	
other	than	u	collected	during	
episode		

		=	evolumonary	algorithm	

					populamon:		

p  (i)(   )

	

cem:	
		for	iter	i	=	1,	2,	   	
						for	populamon	member	e	=	1,	2,	...	
										sample	
   (e)     p  (i)(   )
										execute	roll-outs	under	
										store	
							endfor	
	
		
												where					indexes	over	top		p	%	
				endfor	
										

   x  e

  (i+1) = arg max

log p  (   (  e))

(   (e), u (e))

      (e)

					

  e

cross-id178	method	

n    can	work	embarrassingly	well	

[nips	2013]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

closely	related	approaches	

n   

reward	weighted	regression	(rwr)	

(   (e), u (e))

n   

n   

n   

dayan	&	hinton,	nc	1997;	peters	&	schaal,	icml	2007	

  (i+1) = arg max

q(u (e), p  (   (e))) log p  (   (e))

policy	improvement	with	path	integrals	(pi2)	

n   

pi2:	theodorou,	buchli,	schaal	jmlr2010;	kappen,	2007;	(pi2-cma:	stulp	&	sigaud	icml2012)	

   xe

   xe

  (i+1) = arg max

exp( u (e)) log p  (   (e))

covariance	matrix	adaptamon	evolumonary	strategy	(cma-es)	

cma:	hansen	&	ostermeier	1996;		(cma-es:	hansen,	muller,	koumoutsakos	2003)	

n   

	

n   

power	

(  (i+1),    (i+1)) = arg max

  ,    x  e

w(u (  e)) log n (   (  e);   ,    )

n   

kober	&	peters,	nips	2007	(also	applies	importance	sampling	for	sample	re-use)	

  (i+1) =   (i) + xe

(   (e)     (i))u (e)! / xe

u (e)!

applicamons	

covariance	matrix	adaptamon	(cma)	has	
become	standard	in	graphics	[hansen,	
ostermeier,	1996]	

power	[kober&peters,	mlj	2011]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

cross-id178	/	evolumonary	methods	

n    full	episode	evaluamon,	parameter	perturbamon	

n    simple	

n    main	caveat:	best	when	number	of	parameters	is	relamvely	small	

n   

i.e.,	number	of	populamon	members	comparable	to	or	larger	than	number	of	
(e   ecmve)	parameters	

	  	in	pracmce	ok	if	low-dimensional	  	and	willing	to	do	do	many	runs	
  	easy-to-implement	baseline,	great	for	comparisons!	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

black	box	gradient	computamon	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

challenge:	noise	can	dominate	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

solumon	1:		average	over	many	samples	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

solumon	2:	fix	random	seed	

   xed	random	
seed	sample	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

solumon	2:	fix	random	seed	

n    randomness	in	policy	and	dynamics	

n    but	can	oqen	only	control	randomness	in	policy..	

n    example:		wind	in   uence	on	a	helicopter	is	stochasmc,	but	if	

we	assume	the	same	wind	pabern	across	trials,	this	will	make	
the	di   erent	choices	of	  	more	readily	comparable	

n    note:	equally	applicable	to	evolu2onary	methods	

[ng	&	jordan,	2000]	provide	theoremcal	analysis	of	gains	from	   xing	randomness	(   pegasus   )	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

[policy search was done in simulation] 

[ng + al, iser 2004] 

learning	to	hover	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

outline		

n    derivamve	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ra'o	(lr)	policy	gradient	

n    deriva'on	/	connec'on	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n   

n   

n   

variance	reducmon	using	value	funcmons	(actor-crimc)	(->	gae,	a3c)	

pathwise	derivamves	(pd)		(->	dpg,	ddpg,	svg)	

stochasmc	computamon	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

inverse	reinforcement	learning	

likelihood	ramo	policy	gradient	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleb,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleb,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleb,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleb,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleb,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	policy	gradient	

[aleksandrov,	sysoyev,	&	shemeneva,	1968]	
[rubinstein,	1969]	
[glynn,	1986]	
[reinforce,	williams	1992]	
[gpomdp,	baxter	&	bartleb,	2001]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

derivamon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

derivamon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

derivamon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

derivamon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

r    u (   )|   =   old

note:	suggests	we	can	also	look	at	more	than	just	gradient!	

[tang&abbeel,	nips	2011]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

derivamon	from	importance	sampling	

p (   |   old)

r(    ) 
u (   ) = e         old    p (   |   )
r(    ) 
r   u (   ) = e         old   r   p (   |   )
= e         old   r    p (   |   )|   old
r(    ) 
r(    )   
= e         old   r    log p (   |   )|   old

p (   |   old)

p (   |   old)

suggests	we	can	also	look	at	more	than	just	gradient!	

r    u (   )|   =   old

[tang&abbeel,	nips	2011]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

e.g.,	can	use	importance	sampled	objecmve	as	   surrogate	loss   	(locally)	

likelihood	ramo	gradient:	validity	

ru (   )       g =

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n    valid	even	if	r	is	disconmnuous,	and	
unknown,	or	sample	space	(of	paths)	
is	a	discrete	set		

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	gradient:	intuimon	

ru (   )       g =
n    gradient	tries	to:	

1
m

mxi=1

r    log p (    (i);    )r(    (i))

n   

increase	id203	of	paths	with	
posimve	r	

n    decrease	id203	of	paths	with	

negamve	r	

!	likelihood	ramo	changes	probabilimes	of	experienced	paths,	
does	not	try	to	change	the	paths	(see	path	derivamve	later)	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	acmons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	acmons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	acmons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

let   s	decompose	path	into	states	and	acmons	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	gradient	esmmate	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	gradient	esmmate	

n    as	formulated	thus	far:	unbiased	but	very	noisy	
n    fixes	that	lead	to	real-world	pracmcality	

n    baseline	
n    temporal	structure	

n    also:	kl-divergence	trust	region	/	natural	gradient	(=	general	trick,	
equally	applicable	to	perturbamon	analysis	and	   nite	di   erences)	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

likelihood	ramo	gradient:	baseline	

ru (   )       g =

1
m

r    log p (    (i);    )r(    (i))

mxi=1

n    to	build	intuimon,	let   s	assume	r	>	0	

n    then	tries	to	increase	probabilimes	of	all	paths	

      consider	baseline	b:	

ru (   )       g =

						good	choices	for	b?	
1
b = e [r(    )]    
m

r(    (i))

mxi=1

1
m

mxi=1

r    log p (    (i);    )(r(    (i))   b)

smll	unbiased	

[williams	1992]	

p (    ;    )r    log p (    ;    )b
p (    ;    )r   p (    ;    )
p (    ;    )

b

e [r    log p (    ;    )b]
=x   
=x   
=x    r   p (    ;    )b
p (    )b!
=r    x   

=r    (b)
=0

[see:	greensmith,	bartleb,	baxter,	jmlr	2004	

for	variance	reducmon	techniques.]		

	

	

likelihood	ramo	and	temporal	structure	
	current	esmmate:	

n   

r    log p (    (i);    )(r(    (i))   b)

n    future	acmons	do	not	depend	on	past	rewards,	hence	can	lower	variance	

by	instead	using:	

n    good	choice	for	b?		

1
m

mxi=1

h 1xt=0

r    log       (u(i)

t

|s(i)

r(s(i)

k , u(i)

k )!
k )   b(s(i)

r(s(i)
t

t )   b!

, u(i)

|s(i)

t )! h 1xt=0
t ) h 1xk=t

  g =

1
m

=

1
m

mxi=1
mxi=1 h 1xt=0

r    log       (u(i)

t

	expected	return:		

b(st) = e [rt + rt+1 + rt+2 + . . . + rh 1]

  	increase	logprob	of	acmon	propormonally	to	how	much	its	returns	are	beber	than	the	
expected	return	under	the	current	policy	

[policy	gradient	theorem:	subon	et	al,	nips	1999;	gpomdp:	bartleb	&	
baxter,	jair	2001;	survey:	peters	&	schaal,	iros	2006]		

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

pseudo-code	reinforce	aka	vanilla	policy	gradient	

~	[williams,	1992]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

outline		

n    derivamve	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ramo	(lr)	policy	gradient	

n    derivamon	/	connecmon	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n    variance	reduc'on	using	value	func'ons	(actor-cri'c)	(->	gae,	a3c)	

n   

n   

pathwise	deriva'ves	(pd)		(->	dpg,	ddpg,	svg)	

stochas'c	computa'on	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

inverse	reinforcement	learning	

trust region policy optimization

desiderata

desiderata for policy optimization method:

i stable, monotonic improvement. (how to choose stepsizes?)
i good sample e ciency

step sizes

why are step sizes a big deal in rl?

i supervised learning

i step too far ! next updates will    x it

i id23

i step too far ! bad policy
i next batch: collected under bad policy
i can   t recover, collapse in performance!

surrogate objective

i let    (   ) denote the expected return of    
i we collect data with    old. want to optimize some objective to get a new

policy    

i de   ne l   old(   ) to be the    surrogate objective   1

l(   ) = e   old       (a | s)
r   l(      )     old
= r      (      )     old

   old(a | s)

a   old(s, a) 

(policy gradient)

i local approximation to the performance of the policy; does not depend on

parameterization of    

1s. kakade and j. langford.    approximately optimal approximate id23   .

in: icml. vol. 2. 2002, pp. 267   274.

improvement theory

i theory: bound the di   erence between l   old(   ) and    (   ), the performance of

the policy

i result:    (   )   l   old(   )   c    maxs kl[   old(   | s),    (   | s)], where
i monotonic improvement guaranteed (mm algorithm)

c = 2    /(1    )2

practical algorithm: trpo

i constrained optimization problem

max

   

l(   ), subject to kl[   old,    ]      

where l(   ) = e   old       (a | s)

   old(a | s)

a   old(s, a) 

i construct loss from empirical data

  l(   ) =

nxn=1

   (an | sn)
   old(an | sn)

  an

i make quadratic approximation and solve with conjugate gradient algorithm

j. schulman, s. levine, p. moritz, et al.    trust region policy optimization   .

in: icml. 2015

practical algorithm: trpo

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
compute policy gradient g
use cg (with hessian-vector products) to compute f  1g
do line search on surrogate loss and kl constraint

end for

j. schulman, s. levine, p. moritz, et al.    trust region policy optimization   .

in: icml. 2015

practical algorithm: trpo

applied to

i locomotion controllers in 2d

i atari games with pixel input

j. schulman, s. levine, p. moritz, et al.    trust region policy optimization   .

in: icml. 2015

   proximal    policy optimization

i use penalty instead of constraint

minimize

   

nxn=1

      (an | sn)
      old(an | sn)

  an    kl[      old,       ]

   proximal    policy optimization

i use penalty instead of constraint

minimize

   

nxn=1

      (an | sn)
      old(an | sn)

  an    kl[      old,       ]

i pseudocode:

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
do sgd on above objective for some number of epochs
if kl too high, increase  . if kl too low, decrease  .

end for

   proximal    policy optimization

i use penalty instead of constraint

minimize

   

nxn=1

      (an | sn)
      old(an | sn)

  an    kl[      old,       ]

i pseudocode:

for iteration=1, 2, . . . do

run policy for t timesteps or n trajectories
estimate advantage function at all timesteps
do sgd on above objective for some number of epochs
if kl too high, increase  . if kl too low, decrease  .

end for

i     same performance as trpo, but only    rst-order optimization

variance reduction using value functions

variance reduction

i now, we have the following policy gradient formula:

r   e    [r] = e   "t 1xt=0

r    log    (at | st,    )a   (st, at)#

i a    is not known, but we can plug in   at, an advantage estimator
i previously, we showed that taking

  at = rt + rt+1 + rt+2 +         b(st)

for any function b(st), gives an unbiased policy gradient estimator.
b(st)     v    (st) gives variance reduction.

the delayed reward problem

i with id189, we are confounding the e   ect of multiple

actions:

  at = rt + rt+1 + rt+2 +         b(st)

mixes e   ect of at, at+1, at+2, . . .
i snr of   at scales roughly as 1/t

i only at contributes to signal a   (st, at), but at+1, at+2, . . . contribute to

noise.

variance reduction with discounts

i discount factor  , 0 < < 1, downweights the e   ect of rewars that are far

in the future   ignore long term dependencies

i we can form an advantage estimator using the discounted return:

  a 
t = rt +  rt+1 +  2rt+2 + . . .

discounted return

|

{z

 b(st)

}

reduces to our previous estimator when   = 1.

i so advantage has expectation zero, we should    t baseline to be discounted

value function

v    ,  (s) = e      r0 +  r1 +  2r2 + . . . | s0 = s   

i discount   is similar to using a horizon of 1/(1    ) timesteps
i   a 
t

is a biased estimator of the advantage function

value functions in the future

i baseline accounts for and removes the e   ect of past actions

i can also use the value function to estimate future rewards

rt +  v (st+1)
rt +  rt+1 +  2v (st+2)
. . .
rt +  rt+1 +  2rt+2 + . . .

cut o    at one timestep

cut o    at two timesteps

1 timesteps (no v )

value functions in the future

i subtracting out baselines, we get advantage estimators

  a(1)
t = rt +  v (st+1) v (st)
  a(2)
t = rt + rt+1 +  2v (st+2) v (st)
. . .
  a(1)
t = rt +  rt+1 +  2rt+2 + . . . v (st)

i   a(1)

t

has low variance but high bias,   a(1)

t

has high variance but low bias.

i using intermediate k (say, 20) gives an intermediate amount of bias and variance

finite-horizon methods: advantage actor-critic

i a2c / a3c uses this    xed-horizon advantage estimator

v. mnih, a. p. badia, m. mirza, et al.    asynchronous methods for deep id23   .

in: icml (2016)

finite-horizon methods: advantage actor-critic

i a2c / a3c uses this    xed-horizon advantage estimator

i pseudocode

for iteration=1, 2, . . . do

agent acts for t timesteps (e.g., t = 20),
for each timestep t, compute

  rt = rt +  rt+1 +        +  t t+1rt 1 +  t tv (st)
  at =   rt   v (st)

  rt is target value function, in regression problem
  at is estimated advantage function

compute loss gradient g = r   pt

t=1h  log       (at | st)   at + c(v (s)     rt)2i

g is plugged into a stochastic id119 variant, e.g., adam.

end for

v. mnih, a. p. badia, m. mirza, et al.    asynchronous methods for deep id23   .

in: icml (2016)

a3c video

a3c results

td( ) methods: generalized advantage estimation

i recall,    nite-horizon advantage estimators

  a(k)
t = rt +  rt+1 +        +  k 1rt+k 1 +  k v (st+k )   v (st)

i de   ne the td error  t = rt +  v (st+1)   v (st)
i by a telescoping sum,

  a(k)
t =  t +   t+1 +        +  k 1 t+k 1
i take exponentially weighted average of    nite-horizon estimators:

  a  =   a(1)

t +     a(2)

t +  2   a(3)

t + . . .

i we obtain

  a 
t =  t + (  ) t+1 + (  )2 t+2 + . . .

i this scheme named generalized advantage estimation (gae) in [1], though versions have

appeared earlier, e.g., [2]. related to td( )

j. schulman, p. moritz, s. levine, et al.    high-dimensional continuous control using generalized advantage estimation   .

in: icml. 2015

h. kimura and s. kobayashi.    an analysis of actor/critic algorithms using eligibility traces: id23 with imperfect value

function.    in: icml. 1998, pp. 278   286

choosing parameters  ,  

performance as  ,   are varied

trpo+gae video

pathwise derivative id189

deriving the policy gradient, reparameterized

i episodic mdp:

   

. . .

. . .

st

at

rt

s1

a1

s2

a2

want to compute r   e [rt ]. we   ll use r    log    (at | st;    )

deriving the policy gradient, reparameterized

i episodic mdp:

   

. . .

. . .

st

at

rt

s1

a1

s2

a2

want to compute r   e [rt ]. we   ll use r    log    (at | st;    )

i reparameterize: at =    (st, zt;    ). zt is noise from    xed distribution.

   

. . .

. . .

. . .

st

at

zt

rt

s1

a1

z1

s2

a2

z2

deriving the policy gradient, reparameterized

i episodic mdp:

   

. . .

. . .

st

at

rt

s1

a1

s2

a2

want to compute r   e [rt ]. we   ll use r    log    (at | st;    )

i reparameterize: at =    (st, zt;    ). zt is noise from    xed distribution.

   

. . .

. . .

. . .

st

at

zt

rt

s1

a1

z1

s2

a2

z2

i only works if p(s2 | s1, a1) is known   _

using a q-function

   

. . .

. . .

. . .

st

at

zt

rt

s1

a1

z1

s2

a2

z2

d

d   e [rt ] = e" txt=1
= e" txt=1

drt
dat

dat

d   # = e" txt=1

d
dat

e [rt | at]

dat

d   #

dq(st, at)

dat

dat

d   # = e" txt=1

q(st,    (st, zt;    ))#

d
d   

svg(0) algorithm

i learn q  to approximate q    ,  , and use it to compute gradient estimates.

n. heess, g. wayne, d. silver, et al.    learning continuous control policies by stochastic value gradients   .

in: nips. 2015

svg(0) algorithm

i learn q  to approximate q    ,  , and use it to compute gradient estimates.
i pseudocode:

for iteration=1, 2, . . . do

execute policy        to collect t timesteps of data
t=1 q(st,    (st, zt;    ))
t=1(q (st, at)     qt)2, e.g. with td( )

update        using g / r   pt
update q  using g / r pt

end for

n. heess, g. wayne, d. silver, et al.    learning continuous control policies by stochastic value gradients   .

in: nips. 2015

svg(1) algorithm

   

. . .

. . .

. . .

st

at

zt

rt

s1

a1

z1

s2

a2

z2

i instead of learning q, we learn

i state-value function v     v    , 
i dynamics model f , approximating st+1 = f (st, at) +    t

i given transition (st, at, st+1), infer    t = st+1   f (st, at)
i q(st, at) = e [rt +  v (st+1)] = e [rt +  v (f (st, at) +    t)], and at =    (st,   ,    t)

svg(1) algorithm

   

. . .

. . .

. . .

st

at

zt

rt

s1

a1

z1

s2

a2

z2

i just learn dynamics model f

i given whole trajectory, infer all noise variables

i freeze all policy and dynamics noise, di   erentiate through entire deterministic

computation graph

svg results

i applied to 2d robotics tasks

i overall: di   erent gradient estimators behave similarly

n. heess, g. wayne, d. silver, et al.    learning continuous control policies by stochastic value gradients   .

in: nips. 2015

deterministic policy gradient

i for gaussian actions, variance of score function policy gradient estimator goes to

in   nity as variance goes to zero

i but svg(0) gradient is    ne when   ! 0

r   xt

q(st,    (st,   ,    t))

i problem: there   s no exploration.

i solution: add noise to the policy, but estimate q with td(0), so it   s valid

o   -policy

i policy gradient is a little biased (even with q = q   ), but only because state

distribution is o      it gets the right gradient at every state

d. silver, g. lever, n. heess, et al.    deterministic policy gradient algorithms   .

in: icml. 2014

deep deterministic policy gradient

i incorporate replay bu   er and target network ideas from id25 for increased

stability

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

deep deterministic policy gradient

i incorporate replay bu   er and target network ideas from id25 for increased

stability

i use lagged (polyak-averaging) version of q  and        for    tting q  (towards

q    ,  ) with td(0)

  qt = rt +  q 0(st+1,    (st+1;    0))

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

deep deterministic policy gradient

i incorporate replay bu   er and target network ideas from id25 for increased

stability

i use lagged (polyak-averaging) version of q  and        for    tting q  (towards

q    ,  ) with td(0)

  qt = rt +  q 0(st+1,    (st+1;    0))

i pseudocode:

for iteration=1, 2, . . . do

act for several timesteps, add data to replay bu   er
sample minibatch

update        using g / r   pt
update q  using g / r pt

end for

t=1 q(st,    (st, zt;    ))
t=1(q (st, at)     qt)2,

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

ddpg results

applied to 2d and 3d robotics tasks and driving with pixel input

t. p. lillicrap, j. j. hunt, a. pritzel, et al.    continuous control with deep id23   .

in: iclr (2015)

id189: comparison

i two kinds of policy gradient estimator

i reinforce / score function estimator: r log    (a | s)   a.
i pathwise derivative estimators (di   erentiate wrt action)

i learn q or v for variance reduction, to estimate   a

i svg(0) / dpg: d
i svg(1): d
i svg(1):

da q(s, a) (learn q)
da (r +  v (s0)) (learn f , v )
d
dat

(rt +  rt+1 +  2rt+2 + . . . ) (learn f )

i pathwise derivative methods more sample-e cient when they work (maybe),

but work less generally due to high bias

id189: comparison

y. duan, x. chen, r. houthooft, et al.    benchmarking deep id23 for continuous control   .

in: icml (2016)

stochastic computation graphs

gradients of expectations

want to compute r   e [f ]. where   s    ?
i in distribution, e.g., ex   p(   |    ) [f (x)]

i r   ex [f (x)] = ex [f (x)r    log px (x;    )] .
i score function estimator
i example: reinforce policy gradients, where x is the trajectory

i outside distribution: ez   n (0,1) [f (   , z)]

r   ez [f (x(z,    ))] = ez [r   f (x(z,    ))] .

i pathwise derivative estimator
i example: svg policy gradient

i often, we can reparametrize, to change from one form to another

i what if f depends on     in complicated way, a   ecting distribution and f ?

m. c. fu.    gradient estimation   .

in: handbooks in operations research and management science 13 (2006), pp. 575   616

stochastic computation graphs

john schulman1, nicolas heess2, th  ophane weber2, pieter abbeel1 

  baseline(all nodes not in   uenced by x)

1university of california, berkeley              2google deepmind

=

 
    

         xstochastic node x

deterministically
in   uenced by    

     
log p(x | parents(x))      xcost node c
     ex [f (x)] = ex   f (x)
     ez [f (x(z,    ))] = ez     

c

score function estimator: given x   px(x;    )

 
  log p(b | a, d)

 a
    

 a

 a
    

  log p(b | a, d)
pathwise derivative estimator: given z   pz(z), deterministic x(z,    )
  log p(b | a, d)
  

  log p(b | a, d)
 
c

 a

 a

 a

overview

i stochastic computation graph is a dag, each node corresponds to a
(17)

distribution is different from the distribution we are evaluating: for parameter        ,     =    old is
used for sampling, but we are evaluating at     =    new.

generalize id26 to deal with random variables 
that we can   t differentiate through.  
why can   t we differentiate through them?

deterministic or stochastic operation

1) discrete random variables 
2) unmodeled external world, in rl / control

    dv

i can automatically derive unbiased gradient estimators, with variance
(18)

why   s it useful?  
1) no need to  rederive every time  
2) enable generic software
computation graphs

    

pv(v | depsv\   ,    new)
pv(v | depsv\   ,    old)

ev c |    new [  c] = ev c |    old      c  v c,
    
     + 1    
    ev c |    old      c    log      v c,
  c#     es |    old"xc c
p(v | depsv\   ,    old)    qv#
log  p(v | depsv\   ,    new)
  c +xv s
log p(v | depsv\   ,    new)   qv# + const
= es |    old"xv s

pv(v | depsv\   ,    new)
pv(v | depsv\   ,    old)

es |    new"xc c

stochastic computation graphs

    dv

l

l

where the second line used the inequality x   log x + 1, and the sign is reversed since   c is negative.
summing over c   c and rearranging we get

reduction

 c
 b

 e
 d

  log p(d |  )

  

(c + e)

 c
 b

 e
 d

(c + e)

  log p(d |  )

  

 c
 b

 b
 d

c

 d

 c
 b

 b
 d

     e    xcost node c

 
      log p(b | a, d)

  log p(b | a, d)
  
 d 
      log p(b | a, d)

c    = xstochastic node x

 d
deterministically
in   uenced by    
c +

  log p(b | a, d)

 d 

c +

 d

 d

 e

 d

 d

 e

c

(19)

(20)

equation (20) allows for majorization-minimization algorithms (like the em algorithm) to be used
to optimize with respect to    . in fact, similar equations have been derived by interpreting rewards
(negative costs) as probabilities, and then taking the variational lower bound on log-id203 (e.g.,
[24]).

stochastic node

=

motivation / applications
it   s all about gradients of expectations!

c examples
c.1 generalized em algorithm and variational id136.

variational id136
j. schulman, n. heess, t. weber, et al.    gradient estimation using stochastic computation graphs   .

the generalized em algorithm maximizes likelihood in a probabilistic model with latent variables
[18]. suppose the probabilistic model de   nes a id203 distribution p(x, z;    ) where x is ob-
served, z is a latent variable, and     is a parameter of the distribution. the generalized em algorithm
maximizes the variational lower bound, which is de   ned by an expectation over q:

policy gradients in 

id23

in: nips. 2015

  baseline(all nodes not in   uenced by x)

just differentiate the    surrogate    function

 
    

         xstochastic node x

2
deterministically
in   uenced by    

     

log p(x | parents(x))      xcost node c

2

under certain conditions, surrogate is a lower bound on expected 
cost, related to variational lower bound. see appendix c
equivalently, use backprop, but introduce terms 
   logp(x) * (sum of downstream costs)  
at stochastic nodes. see algorithm 1 in paper.

worked example

  

  

a

d

c

b

e

i l = c + e. want to compute d
i treat stochastic nodes (b, d) as constants, and introduce losses logprob     (futurecost) at
i obtain unbiased gradient estimate by di   erentiating surrogate:

each stochastic node

d    e [l] and d

d e [l].

surrogate(   ,  ) = c + e

|{z}(1)

+ log p(  b | a, d)  c
|
}

{z

(2)

(1): how parameters in   uence cost through deterministic dependencies
(2): how parameters a   ect distribution over random variables.

outline		

n    derivamve	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ramo	(lr)	policy	gradient	

n    derivamon	/	connecmon	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n   

n   

n   

variance	reducmon	using	value	funcmons	(actor-crimc)	(->	gae,	a3c)	

pathwise	derivamves	(pd)		(->	dpg,	ddpg,	svg)	

stochasmc	computamon	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

inverse	reinforcement	learning	

goal	

tyt=1

n    find	parameterized	policy																				that	opmmizes:	

      (ut|xt)

j(   ) =

txt=1

e      (xt,ut)[l(xt, ut)]

n    notamon:		

      (    ) = p(x1)

p(xt+1|xt, ut)      (ut|xt)

    = {x1, u1, . . . , xt , ut}

n    rl	takes	lots	of	data   	can	we	reduce	to	supervised	learning?	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

na  ve	solumon	

n    step	1:	

n    consider	sampled	problem	instances		
n    find	a	trajectory-centric	controller																					for	each	problem	instance		

i = 1, 2, . . . , i

   i(ut|xt)

n    step	2:	

n    supervised	training	of	neural	net	to	match	all		
dkl(pi(    )||      (    ))

         arg min

   i(ut|xt)

    xi

n   

issues:	
n    compounding	error	(ross,	gordon,	bagnell	jmlr	2011	   dagger   )	
n    mismatch	train	vs.	test			e.g.,	blind	peg,	vision,   	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

(generic)	guided	policy	search	

n    opmmizamon	formulamon:	

parmcular	form	of	the	constraint	varies	depending	on	the	speci   c	method:	
dual	gradient	descent:	levine	and	abbeel,	nips	2014	
penalty	methods:	mordatch,	lowrey,	andrew,	popovic,	todorov,	nips	2016	
admm:	mordatch	and	todorov,	rss	2014	
bregman	admm:	levine,	finn,	darrell,	abbeel,	jmlr	2016	
mirror	descent:	montgomery,	levine,	nips	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

[levine	&	abbeel,	nips	2014]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

[levine	&	abbeel,	nips	2014]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

comparison	

[levine,	wagener,	abbeel,	icra	2015]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

block	stacking	   	learning	the	controller	for	a	single	instance	

[levine,	wagener,	abbeel,	icra	2015]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

linear-gaussian	controller	learning	curves	

[levine,	wagener,	abbeel,	icra	2015]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

instrumented training 
test time 

training time 

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

architecture	(92,000	parameters)	

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

experimental tasks 

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

learning	

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

learned	skills	

[levine*,	finn*,	darrell,	abbeel,	jmlr	2016	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

pi-gps	

n    uses	pi2	(rather	than	ilqg)	as	the	trajectory	opmmizer	

n   

in	these	experiments:	

n    pi2	opmmizes	over	sequence	of	linear	feedback	controllers	
n    pi2	inimalized	from	demonstramons	

n    neural	net	architecture:	

[chebotar,	kalakrishnan,	yahya,	li,	schaal,	levine,	arxiv	2016]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

outline		

n    derivamve	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ramo	(lr)	policy	gradient	

n    derivamon	/	connecmon	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n    actor-crimc																			(->	gae,	a3c)	

n   

n   

path	derivamves	(pd)		(->	dpg,	ddpg,	svg)	

stochasmc	computamon	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

current	fron'ers	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

current	fronmers	(+pointers	to	some	representamve	recent	work)	

n    o   -policy	policy	gradients	/	o   -policy	actor	crimc	/	connect	with	id24	

n   

n   

ddpg	[lillicrap	et	al,	2015];	q-prop	[gu	et	al,	2016];	doubly	robust	[dudik	et	al,	2011],	   	

pgq	[o   donoghue	et	al,	2016];	acer	[wang	et	al,	2016];	q(lambda)	[harutyunyan	et	al,	2016];	retrace(lambda)	[munos	et	al,	
2016]   	

n   

exploramon	

n   

vime	[houthooq	et	al,	2016];	count-based	exploramon	[bellemare	et	al,	2016];	#exploramon	[tang	et	al,	2016];	curiosity	
[schmidhueber,	1991];	   	

n    auxiliary	objecmves	

n   

learning	to	navigate	[mirowski	et	al,	2016];	rl	with	unsupervised	auxiliary	tasks	[jaderberg	et	al,	2016],	   	

n    mulm-task	and	transfer	(incl.	sim2real)	

n   

deepdriving	[chen	et	al,	2015];	progressive	nets	[rusu	et	al,	2016];	flight	without	a	real	image	[sadeghi	&	levine,	2016];	
sim2real	visuomotor	[tzeng	et	al,	2016];	sim2real	inverse	dynamics	[chrismano	et	al,	2016];	modular	nns	[devin*,	gupta*,	
et	al	2016]	

n   

language	

n   

learning	to	communicate	[foerster	et	al,	2016];	mulmtask	rl	w/policy	sketches	[andreas	et	al,	2016];	learning	language	
through	interacmon	[wang	et	al,	2016]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

current	fronmers	(+pointers	to	some	representamve	recent	work)	

meta-rl	

n   

rl2:	fast	rl	through	slow	rl	[duan	et	al.,	2016];	learning	to	reinforcement	learn	[wang	et	al,	2016];	learning	to	experiment	[denil	et	al,	2016];		
learning	to	learn	for	black-box	opt.	[chen	et	al,	2016],	   	

24/7	data	collecmon	

n   

learning	to	grasp	from	50k	tries	[pinto&gupta,	2015];	learning	hand-eye	coordinamon	[levine	et	al,	2016];	learning	to	poke	by	poking	[agrawal	et	
al,	2016]	

safety	

n   

survey:	garcia	and	fernandez,	jmlr	2015	

architectures	

n    memory,	acmve	percepmon	in	minecraq	[oh	et	al,	2016];	drqn	[hausknecht&stone,	2015];	dueling	networks	[wang	et	al,	2016];	   	

inverse	rl	

n   

generamve	adversarial	imitamon	learning	[ho	et	al,	2016];	guided	cost	learning	[finn	et	al,	2016];	maxent	deep	rl	[wulfmeier	et	al,	2016];	   	

model-based	rl	

n   

deep	visual	foresight	[finn	&	levine,	2016];	embed	to	control	[waber	et	al.,	2015];	spamal	autoencoders	visuomotor	learning	[finn	et	al,	2015];	pilco	[deisenroth	et	al,	
2015]	

hierarchical	rl	

n    modulated	locomotor	controllers	[heess	et	al,	2016];	straw	[vezhnevets	et	al,	2016];	opmon-crimc	[bacon	et	al,	2016];	h-id25	[kulkarni	et	al,	

2016];	hierarchical	lifelong	learning	in	minecraq	[tessler	et	al,	2016]	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

n   

n   

n   

n   

n   

n   

n   

	

how	to	learn	more	and	get	started?		

n    (1)	deep	rl	courses	

n    cs294-112	deep	reinforcement	learning	(uc	berkeley):	

hbp://rll.berkeley.edu/deeprlcourse/	by	sergey	levine,	john	schulman,	chelsea	
finn	
hbp://www0.cs.ucl.ac.uk/sta   /d.silver/web/teaching.html	by	david	silver	

n    compm050/compgi13	reinforcement	learning	(ucl):	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

how	to	learn	more	and	get	started?	

n    (2)	deep	rl	code	bases	

n   

rllab:	hbps://github.com/openai/rllab			
duan,	chen,	houthooq,	schulman	et	al	

n    gps:	hbp://rll.berkeley.edu/gps/	
finn,	zhang,	fu,	tan,	mccarthy,	
schar   ,	stadie,	levine	

		

	
n    rlpy:	

hbps://rlpy.readthedocs.io/en/latest/	
geramifard,	klein,	dann,	dabney,	how	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

how	to	learn	more	and	get	started?	

	

n    (3)	environments	

n    deepmind	lab	/	labyrinth	(deepmind)	

n    arcade	learning	environment	(ale)	

(bellemare	et	al,	jair	2013)	

	

   	

n    openai	gym:	hbps://gym.openai.com/	

n    mujoco:		hbp://mujoco.org	(todorov)	

n    minecrao	(microsoq)	

n    universe:	hbps://universe.openai.com/	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

universe	

a	so:ware	pla<orm	for	measuring	and	training	an	ai's	general	

intelligence	across	the	world's	supply	of	games,	websites	and	other	

applica2ons.	

	

hbps://universe.openai.com	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

universe	--	games	

release	consists	of	a	thousand	environments	including	flash	games,	browser	tasks,	
and	games	like	slither.io,	starcraq	and	gta	v.	

hbps://universe.openai.com	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

universe	   	world	of	bits	(wob):	   mini-wob   	
ai	follows	instrucmons	

hbps://universe.openai.com	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

universe	   	world	of	bits:	real	browser	tasks	
ai	books	plane	mckets		

hbps://universe.openai.com	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

universe	   	world	of	bits:	educamonal	games	
ai	goes	to	school	j	

hbps://universe.openai.com	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

universe	

n    opportunimes:	

n    train	agents	on	universe	tasks.		

n    grant	us	permission	to	use	your	game,	program,	website,	or	app	

n   

integrate	new	environments.	

n    contribute	demonstramons.	

hbps://universe.openai.com	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

summary	

n    derivamve	free	methods	

n   

cross	id178	method	(cem)	/	finite	di   erences	/	fixing	random	seed	

n   

likelihood	ramo	(lr)	policy	gradient	

n    derivamon	/	connecmon	w/importance	sampling	

n    natural	gradient	/	trust	regions	(->	trpo)	

n    actor-crimc																			(->	gae,	a3c)	

n   

n   

path	derivamves	(pd)		(->	dpg,	ddpg,	svg)	

stochasmc	computamon	graphs	(generalizes	lr	/	pd)	

n    guided	policy	search	(gps)	

n   

current	fronmers	

john	schulman	&	pieter	abbeel	   	openai	+	uc	berkeley	

