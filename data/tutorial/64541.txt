gen. independence/complexity

doc summarization

data summarization

end

submodularity in speech/nlp

je   rey a. bilmes

professor

departments of electrical engineering
& computer science and engineering

university of washington, seattle

http://melodi.ee.washington.edu/~bilmes

wednesday, june 11th, 2014

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

1 / 48

gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

2 / 48

gen. independence/complexity

doc summarization

data summarization

end

acknowledgments

joint work with: hui lin, kai wei, yuzong liu, katrin kirchho   , sebastian
tschiatschek, and rishabh iyer

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

3 / 48

gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

4 / 48

gen. independence/complexity

doc summarization

data summarization

end

two equivalent submodular de   nitions

de   nition (submodular)

a function f : 2v     r is submodular if for any a, b     v , we have that:

f (a) + f (b)     f (a     b) + f (a     b)

(1)

de   nition (submodular (diminishing returns))

a function f : 2v     r is submodular if for any a     b     v , and v     v \ b, we

have that:

f (a     {v})     f (a)     f (b     {v})     f (b)

(2)

this means that the incremental    value   ,    gain   , or    cost    of v decreases
(diminishes) as the context in which v is considered grows from a to b.
j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

5 / 48

gen. independence/complexity

doc summarization

data summarization

end

example submodular: number of colors of balls in urns

consider an urn containing colored balls. given a set s of balls, f (s) counts
the number of distinct colors.

submodularity: incremental value of object diminishes in a larger context
(diminishing returns). thus, f is submodular.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

6 / 48

initial value: 2 (colors in urn).new value with added blue ball: 3initial value: 3 (colors in urn).new value with added blue ball: 3gen. independence/complexity

doc summarization

data summarization

end

discrete optimization

we are given a    nite set of objects v of size n = |v|.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

7 / 48

gen. independence/complexity

doc summarization

data summarization

end

discrete optimization

we are given a    nite set of objects v of size n = |v|.
there are 2n such subsets (denoted 2v ) of the form a     v .

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

7 / 48

gen. independence/complexity

doc summarization

data summarization

end

discrete optimization

we are given a    nite set of objects v of size n = |v|.
there are 2n such subsets (denoted 2v ) of the form a     v .
we have a function f : 2v     r that judges the quality (or value, or cost, or
etc.) of each subset. f (a) = some real number.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

7 / 48

gen. independence/complexity

doc summarization

data summarization

end

discrete optimization

we are given a    nite set of objects v of size n = |v|.
there are 2n such subsets (denoted 2v ) of the form a     v .
we have a function f : 2v     r that judges the quality (or value, or cost, or
etc.) of each subset. f (a) = some real number.
unconstrained minimization & maximization:

min
x   v

f (x )

(3)

max
x   v

f (x )

(4)

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

7 / 48

gen. independence/complexity

doc summarization

data summarization

end

discrete optimization

we are given a    nite set of objects v of size n = |v|.
there are 2n such subsets (denoted 2v ) of the form a     v .
we have a function f : 2v     r that judges the quality (or value, or cost, or
etc.) of each subset. f (a) = some real number.
unconstrained minimization & maximization:

min
x   v

f (x )

(3)

max
x   v

f (x )

(4)

without knowing anything about f , it takes 2n queries to be able to o   er
any quality assurance on a candidate solution. otherwise, solution can be
unboundedly poor.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

7 / 48

gen. independence/complexity

doc summarization

data summarization

end

discrete optimization

we are given a    nite set of objects v of size n = |v|.
there are 2n such subsets (denoted 2v ) of the form a     v .
we have a function f : 2v     r that judges the quality (or value, or cost, or
etc.) of each subset. f (a) = some real number.
unconstrained minimization & maximization:

min
x   v

f (x )

(3)

max
x   v

f (x )

(4)

without knowing anything about f , it takes 2n queries to be able to o   er
any quality assurance on a candidate solution. otherwise, solution can be
unboundedly poor.
when f is submodular, eq. (3) is polytime, and eq. (4) is constant-factor
approximable.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

7 / 48

gen. independence/complexity

doc summarization

data summarization

end

constrained discrete optimization

often, we are interested only in a subset of the set of possible subsets,
namely s     2v .

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

8 / 48

gen. independence/complexity

doc summarization

data summarization

end

constrained discrete optimization

often, we are interested only in a subset of the set of possible subsets,
namely s     2v .
example: only sets having bounded size s = {s     v : |s|     k} or within a

s   s w (s)     b(cid:9) where w is a cost vector.

budget(cid:8)s     v :(cid:80)

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

8 / 48

gen. independence/complexity

doc summarization

data summarization

end

constrained discrete optimization

budget(cid:8)s     v :(cid:80)

often, we are interested only in a subset of the set of possible subsets,
namely s     2v .
example: only sets having bounded size s = {s     v : |s|     k} or within a
example: sets could correspond to combinatorial object (i.e., feasible s
might be trees, matchings, paths, vertex covers, or cuts).

s   s w (s)     b(cid:9) where w is a cost vector.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

8 / 48

gen. independence/complexity

doc summarization

data summarization

end

constrained discrete optimization

budget(cid:8)s     v :(cid:80)

s   s w (s)     b(cid:9) where w is a cost vector.

often, we are interested only in a subset of the set of possible subsets,
namely s     2v .
example: only sets having bounded size s = {s     v : |s|     k} or within a
example: sets could correspond to combinatorial object (i.e., feasible s
might be trees, matchings, paths, vertex covers, or cuts).
ex: s might be a function of some g (e.g., sub-level sets of g ,
s = {s     v : g (s)       }, sup-level sets s = {s     v : g (s)       }).

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

8 / 48

gen. independence/complexity

doc summarization

data summarization

end

constrained discrete optimization

budget(cid:8)s     v :(cid:80)

s   s w (s)     b(cid:9) where w is a cost vector.

often, we are interested only in a subset of the set of possible subsets,
namely s     2v .
example: only sets having bounded size s = {s     v : |s|     k} or within a
example: sets could correspond to combinatorial object (i.e., feasible s
might be trees, matchings, paths, vertex covers, or cuts).
ex: s might be a function of some g (e.g., sub-level sets of g ,
s = {s     v : g (s)       }, sup-level sets s = {s     v : g (s)       }).
constrained discrete optimization problems:

maximize
subject to

f (s)
s     s

(5)

minimize
subject to

f (s)
s     s

(6)

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

8 / 48

gen. independence/complexity

doc summarization

data summarization

end

constrained discrete optimization

budget(cid:8)s     v :(cid:80)

s   s w (s)     b(cid:9) where w is a cost vector.

often, we are interested only in a subset of the set of possible subsets,
namely s     2v .
example: only sets having bounded size s = {s     v : |s|     k} or within a
example: sets could correspond to combinatorial object (i.e., feasible s
might be trees, matchings, paths, vertex covers, or cuts).
ex: s might be a function of some g (e.g., sub-level sets of g ,
s = {s     v : g (s)       }, sup-level sets s = {s     v : g (s)       }).
constrained discrete optimization problems:

maximize
subject to

f (s)
s     s

(5)

minimize
subject to

f (s)
s     s

(6)

fortunately, when f (and g ) are submodular, solving these problems can
often be done with guarantees (and often e   ciently)!

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

8 / 48

gen. independence/complexity

doc summarization

data summarization

end

where is submodularity useful as a model?

useful as a model of a physical process. meaning of the value depends on if
we either wish to maximize or minimize it.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

9 / 48

gen. independence/complexity

doc summarization

data summarization

end

where is submodularity useful as a model?

useful as a model of a physical process. meaning of the value depends on if
we either wish to maximize or minimize it.
for maximization: diversity, coverage, span, and information.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

9 / 48

gen. independence/complexity

doc summarization

data summarization

end

where is submodularity useful as a model?

useful as a model of a physical process. meaning of the value depends on if
we either wish to maximize or minimize it.
for maximization: diversity, coverage, span, and information.
for minimization: cooperative costs, complexity, roughness, and irregularity.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

9 / 48

gen. independence/complexity

doc summarization

data summarization

end

where is submodularity useful as a model?

useful as a model of a physical process. meaning of the value depends on if
we either wish to maximize or minimize it.
for maximization: diversity, coverage, span, and information.
for minimization: cooperative costs, complexity, roughness, and irregularity.
in speech/text/nlp, there are many instances of problems that are
inherently discrete.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

9 / 48

gen. independence/complexity

doc summarization

data summarization

end

generalized information/complexity functions

id178, given a joint distribution p(xv ) over |v| random variables:

f (a) = h(xa) =    

p(xa) log p(xa)

(7)

(cid:88)

xa

with p(xv ) joint id203 distribution over xv .

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

10 / 48

gen. independence/complexity

doc summarization

data summarization

end

generalized information/complexity functions

id178, given a joint distribution p(xv ) over |v| random variables:

f (a) = h(xa) =    

p(xa) log p(xa)

(7)

(cid:88)

xa

with p(xv ) joint id203 distribution over xv .
many other functions are submodular. e.g., set cover, graph cut, bipartite
neighborhoods, sums of weighted concave composed with additive functions,
matroid rank, etc.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

10 / 48

gen. independence/complexity

doc summarization

data summarization

end

generalized information/complexity functions

id178, given a joint distribution p(xv ) over |v| random variables:

f (a) = h(xa) =    

p(xa) log p(xa)

(7)

(cid:88)

xa

with p(xv ) joint id203 distribution over xv .
many other functions are submodular. e.g., set cover, graph cut, bipartite
neighborhoods, sums of weighted concave composed with additive functions,
matroid rank, etc.
all submodular functions express a form of    abstract independence    or
   generalized complexity   

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

10 / 48

gen. independence/complexity

doc summarization

data summarization

end

generalized information

given submodular f , there a notion of    independence    , i.e., a      b:

f (a     b) = f (a) + f (b),

and a notion of    conditional independence    , i.e., a      b|c :

f (a     b     c ) + f (c ) = f (a     c ) + f (b     c )
and a notion of    dependence    (conditioning reduces valuation):

f (a|b) (cid:44) f (a     b)     f (b) < f (a),

and a notion of    conditional mutual information   

if (a; b|c ) (cid:44) f (a     c ) + f (b     c )     f (a     b     c )     f (c )     0

and notions of    information amongst a collection of sets   , e.g.:

(8)

(9)

(10)

if (s1; s2; . . . ; sk) =

f (sk)     f (s1     s2                sk)

(11)

k(cid:88)

i=1

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

11 / 48

gen. independence/complexity

doc summarization

data summarization

end

submodular approaches to big data summarization

we are given a set indexed by v

approach: 1)    nd a good function f : 2v     r+ that represents information

in v . 2) then optimize f to obtain a subset.

1) heuristic: design f by hand, hoping that f is a good proxy for the
information within v . acknowledge that f is a surrogate objective,
guarantees are only in terms of f .

2) more promising approach: attempt to learn f , or some aspects of a good f ,

in some fashion based on training data.
we report on both kinds of results for document summarization, speech
training data subset selection, subset selection in statistical machine
translation problems, and image summarization.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

12 / 48

gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

13 / 48

gen. independence/complexity

doc summarization

data summarization

end

extractive document summarization

we extract sentences (green) as a summary of the full document

the summary on the left is a subset of the summary on the right.
consider adding a new (blue) sentence to each of the two summaries.
marginal bene   t of adding the new (blue) sentence to the smaller (left)
summary is no less than the marginal bene   t of adding blue sentence to the
larger (right) summary.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

14 / 48

   gen. independence/complexity

doc summarization

data summarization

end

submodularity for document summarization?

as further evidence for submodularity   s appropriateness,     many instances of
submodularity   s use in the nlp community, originally unbeknownst to the
authors.

e.g., maximum marginal relevance (mmr) (carbonell & goldstein, 1998)
has a diminishing returns property.
modi   ed mmr - (mcdonald, 2007)
concept-based approaches (filatova & hatzivassiloglou 2004; takamura &
okumura, 2009; riedhammer et al., 2010; qazvinian et al., 2010).
two standard methods for automatic evaluation of candidate summarizes
are submodular, including id8-n (lin 2004, described on next slide) and
pyramid (nenkova & passonneau, 2004).
both id8-n and pyramid are parameterized by good quality summarizes
produced by humans, used only for evaluation.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

15 / 48

gen. independence/complexity

doc summarization

data summarization

end

nist   s id8-n (lin-04) evaluation function

http://www.nist.gov/tac/2011/summarization: nist   s id8-n recall
score, a widely used standard, turns out to be submodular:

(cid:80)k

i=1

(cid:80)
(cid:80)k

e   ri

i=1

(cid:80)

min(ce(s), re,i )

,

e   ri

re,i

fid8-n(s) (cid:44)

s is the candidate summary (set of sentences extracted from the ground set v )

ce : 2v     z+ is the number of times an id165 e occurs in summary s, clearly

a modular function for each e.
ri is the set of id165s contained in the reference summary i (given k
reference summaries).
and re,i is the number of times id165 e occurs in reference summary i.
id8 is based on a collection of human generated summaries, so the
measure can be only used to evaluate a summary.
j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

16 / 48

gen. independence/complexity

doc summarization

data summarization

end

coverage function

coverage function

(cid:88)

i   v

l(s) =

min{ci (s),    ci (v )}

ci measures how well i is covered by s.
one simple possible ci (that we use) is:
ci (s) =

(cid:88)

j   s

wi,j ,

where wi,j     0 measures the similarity between i and j.
with this ci , l(s) is monotone submodular, as required.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

17 / 48

gen. independence/complexity

doc summarization

data summarization

end

diversity reward function

diversity reward function

k(cid:88)

(cid:115) (cid:88)

i=1

j   pi   s

rj .

r(s) =

pi , i = 1,       k is a partition of the ground set v
rj     0: singleton reward of j, which represents the importance of j to the
summary.
square root over the sum of rewards of sentences belong to the same
partition (diminishing returns).
r(s) is monotone submodular as well.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

18 / 48

gen. independence/complexity

doc summarization

data summarization

end

diversity reward function mixtures

alternatively, we can utilize multiple partitions/id91s, produce a diversity
reward function for each one, and mix them together.

multi-resolution diversity reward

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)

k1(cid:88)

i=1

j   p (1)

i    s

k2(cid:88)

i=1

rj +   2

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)

j   p (2)

i    s

rj +       

r(s) =   1

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

19 / 48

gen. independence/complexity

doc summarization

data summarization

end

id170: approach with id136

constraints speci   ed in id136 form:

minimize

w   0,  t

subject to

1
t

  t +

(cid:88)
  
2 (cid:107)w(cid:107)2
w(cid:62)ft(y(t))     max
y   yt
  t     0,   t.

t

(cid:0)w(cid:62)ft(y) + (cid:96)t(y)(cid:1)

      t,   t

(12)

(13)

(14)

exponential set of constraints reduced to an embedded optimization
problem,    id136.   

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

20 / 48

gen. independence/complexity

doc summarization

data summarization

end

learning submodular mixtures: unconstrained form

unconstrained form with hinge-loss

(cid:20)

(cid:88)

t

max
y   yt

1
t

min
w   0

(cid:0)w(cid:62)ft(y) + (cid:96)t(y)(cid:1)

(cid:21)

+

  
2 (cid:107)w(cid:107)2

(15)

subgradient approach: to compute a subgradient, we must solve the
following embedded optimization problem

    w(cid:62)ft(y(t))
(cid:0)w(cid:62)ft(y) + (cid:96)t(y)(cid:1)

max
y   yt

(16)

convex in w, and w(cid:62)ft(y) presumably submodular, but what about (cid:96)t(y)?
often one uses hamming loss (in general id170 problems),
but here we ask that that (cid:96)t(y) is also submodular.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

21 / 48

gen. independence/complexity

doc summarization

data summarization

end

duc evaluations

duc (document understanding conference) data http://duc.nist.gov/
standard evaluation of extractive document summarization managed by
nist in the years 2004-2007.
tasks are both query independent (duc    04) and query dependent
summarization (duc    05-   07), which is more like web search.
standard measure of evaluation performance is the id8 measure.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

22 / 48

gen. independence/complexity

doc summarization

data summarization

end

duc-04 results
id8-1: higher is better

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

23 / 48

lin & bilmes,naacl 2010lin & bilmes,acl 2011lin & bilmes,201236.537.037.538.038.539.039.540.040.541.0duc-04 best systemnon-monotone submodular obj.fidelity+diversitysubmodular mixtureid8-1 recall (%)id8-1 f-measure (%)gen. independence/complexity

doc summarization

data summarization

end

duc-05 results
id8-2: higher is better

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

24 / 48

lin & bilmes,acl 2011lin & bilmes, 20126.36.87.37.88.38.8duc-05 best systemfidelity+diversitysubmodular mixtureid8-2 recall (%)id8-2 f-measure (%)gen. independence/complexity

doc summarization

data summarization

end

duc-06 results
id8-2: higher is better

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

25 / 48

lin & bilmes,acl 2011lin & bilmes, 20129.39.49.59.69.79.89.910.0duc-06 best systemfidelity+diversitysubmodular mixtureid8-2 recall (%)id8-2 f-measure (%)gen. independence/complexity

doc summarization

data summarization

end

duc-07 results
id8-2: higher is better

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

26 / 48

lin & bilmes,acl 2011lin & bilmes, 201212.2012.2512.3012.3512.4012.4512.5012.55duc-07 best systemfidelity+diversitysubmodular mixtureid8-2 recall (%)id8-2 f-measure (%)gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

27 / 48

gen. independence/complexity

doc summarization

data summarization

end

as the data set size grow . . .

there is no data like more data

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

28 / 48

andrew ng   s stanford machine learning class, 2011gen. independence/complexity

doc summarization

data summarization

end

as the data set size grow . . .

there is no data like more data     rather, more data is like no data.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

28 / 48

andrew ng   s stanford machine learning class, 2011gen. independence/complexity

doc summarization

data summarization

end

as the data set size grow . . .

there is no data like more data     rather, more data is like no data.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

28 / 48

andrew ng   s stanford machine learning class, 2011as the data set sizes grow . . .

andrew ng   s stanford machine learning class, 2011(riccardi & hakkani-t  r, 2005, id103)(callison-burch&bloodgood, 2010, machine translation)tong & koller, 2001(soon, ng, lim, 2001, coreference resolution)50100150200250number of training examples00.20.40.60.81accuracy(kadie, 1995, generic classi(cid:31)cation)sentiment tutorial, chris potts, stanford ling., 201178808284868890927001.7503.5007.00010.50014.00017.50021.000overall accuracy (%)training set sizeid166sdts( kavzoglu & colkesen, 2012, image classi(cid:31)cation)speed, memory, attention, problem solvingplaying game luminosityhttp://www.lumosity.com/blog/how-much-and-how-often-should-i-train/gen. independence/complexity

doc summarization

data summarization

end

submodularity and learning curves

diminishing returns: the more you have, the less valuable is anything you don   t
have. bad for complex machine learning systems (e.g., deep models, id166s).

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

30 / 48

gen. independence/complexity

doc summarization

data summarization

end

submodularity and learning curves

diminishing returns: the more you have, the less valuable is anything you don   t
have. bad for complex machine learning systems (e.g., deep models, id166s).

proposition

let v = {1, 2, . . . , n} be a    nite ground set, and let f : 2v     r be a set
function. if for all permutations    of v , we have that for all i     j:

f (  j|si   1)     f (  j|sj   1)

(17)

with si = {  1,   2, . . . ,   i}, then f is submodular.

learning curves might not be exactly submodular, but submodularity seems
a reasonable model.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

30 / 48

gen. independence/complexity

doc summarization

data summarization

end

practical goals: submodular proxies

key question: can statistical predictions be cost e   ective using small data?

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

31 / 48

gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

32 / 48

gen. independence/complexity

doc summarization

data summarization

end

speech subset selection: two forms

corpus summarization: given a large set of speech utterances (training
corpus) v = {v1, v2, . . . , vn}, choose a small subset a     v that is
representative of v .
goal: training on summary should yield highest accuracy possible.
in this work, concentrate on drastic reductions in training set (one to two
orders magnitude) to reduce model design iteration cycle time.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

33 / 48

gen. independence/complexity

doc summarization

data summarization

end

corpus summarization: motivation

large vocabulary id103 training is both resource (disk,
memory) and time consuming.
particularly acute with recent models (e.g., deep neural networks) that can
take appreciable time to train.
training data can be redundant: why waste time/resources training on
information you already know?
switchboard, switchboard cellular, & fisher: very large vocabulary,    uent
spontaneous speech, much more di   cult, state of the art id103
system, supervised labels.
probably more true as the data set sizes grow.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

34 / 48

gen. independence/complexity

doc summarization

data summarization

end

submodular switchboard selection: gmm and dnn

1%

5%

10%

20%

all

rand

he (words)

he (3-phones)
sm (3-phones)

52.1   1.5

49.6
47.5
47.5

38.2  0.2

36.5
37.6
35.7

35.1  0.3

34.8
34.2
33.3

34.4  0.2

n/a
n/a
32.6

31.0

table : word error rates, random (rand), histogram-id178 (he), the submodular
(sm) system. histogram-id178 results saturate after 10%.

1%

5%

10%

20%

all

rand
he (3-phones)
sm (3-phones)

43.7  0.5

42.8
41.1

34.3  0.9

33.9
31.8

31.5  0.5

31.3
29.3

29.8  0.2

n/a
28.2

26.0

table : word error rates for dnn system.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

35 / 48

gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

36 / 48

gen. independence/complexity

doc summarization

data summarization

end

data subset selection for machine translation

id151 (smt): automatically translate from one
human language to another.
common problems in smt: 1) test data is from a target domain while
training data is mixed-domain; 2) phrase translation table, when based on all
training data, can be massive.
solution: choose and then train using only a (domain-speci   c) subset of
training data.
many previous approaches (e.g., id165 overlap (ittycheriah & roukos,
2007), coverage of unseen id165s (eck et al. 2005), feature decay
approach (bi  cici & yuret, 2011-2013)) are inadvertently submodular.
some (e.g., moore & lewis, 2010) are only modular.
we approach directly using submodular functions.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

37 / 48

gen. independence/complexity

doc summarization

data summarization

end

feature based submodular functions

v is ground set of data items (sentences) and u is a set of features,
id165s in our work (but could be parse-based or deep features as well).

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

38 / 48

gen. independence/complexity

doc summarization

data summarization

end

feature based submodular functions

(cid:88)

v is ground set of data items (sentences) and u is a set of features,
id165s in our work (but could be parse-based or deep features as well).
feature-based submodular functions:

where wu > 0 is a feature weight, mu(x ) =(cid:80)

u   u

f (x ) =

wu  u(mu(x ))

modular function speci   c to feature u, mu(x) is a relevance score, a
non-negative scalar score indicating the relevance of feature u in object x,
and   u is a u-speci   c non-negative non-decreasing concave function.

x   x mu(x) is a non-negative

(18)

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

38 / 48

gen. independence/complexity

doc summarization

data summarization

end

results: nist 2009 arabic     english

method

data subset sizes

rand
xent
sm-1
sm-2
sm-3
sm-4
sm-5
sm-6
100%

10%

20%

30%

40%

0.3991 (   0.004)
0.4235 (   0.004)

0.4313
0.4335
0.4306
0.4313
0.4286
0.4356   

0.4142 (   0.003)
0.4292 (   0.002)

0.4345
0.4380
0.4319
0.4345
0.4364
0.4359

0.4205 (   0.002)
0.4290 (   0.003)

0.4333
0.4328
0.4374
0.4333
0.4327
0.4384   

0.4220 (   0.002)
0.4292 (   0.001)

0.4351
0.4365
0.4319
0.4351
0.4328
0.4366

0.4257

id7 on nist 2009 test set for random (rand), cross-id178 (xent), and various
submodular (sm) data selection methods. bold = signi   cant over best xent system.
j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

39 / 48

gen. independence/complexity

doc summarization

data summarization

end

outline

1 submodularity: generalized independence or complexity

2 document summarization

3 data summarization

speech summarization
selection in id151
image summarization

4 end

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

40 / 48

gen. independence/complexity

doc summarization

data summarization

end

image collections

many images, also that have a higher level gestalt than just a few.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

41 / 48

gen. independence/complexity

doc summarization

data summarization

end

image summarization

task: summarize collection
of images by representative
subset of the images

applications:

summarizing your
holiday pictures.
summarizing image
search results
e   cient browsing of
image collections
video frame
summarization

   

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

42 / 48

gen. independence/complexity

doc summarization

data summarization

end

image summarization - data collection

data statistics

14 image collections with 100 pictures each
    400 human summaries for every image collection, via amazon turk,
about 5500 summaries total!

example collections:

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

43 / 48

gen. independence/complexity

doc summarization

data summarization

end

image summarization

super-pixel based v-id8

whole collection:

3 best summaries:

3 medium summaries:

3 worst summaries:

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

44 / 48

gen. independence/complexity

doc summarization

data summarization

end

image summarization

learning to summarize

de   ne submodular functions for measuring

coverage (how well a subset of images covers all images),
diversity (how di   erent is a given subset of images),
feature functions (how present are certain visual words),

learn large-margin mixture of these functions on 13 out of 14 image
collections and test on held out image collection

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

45 / 48

gen. independence/complexity

doc summarization

data summarization

end

image summarization

early results - learnt mixture using max-margin

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

46 / 48

f(   )=0f(v)=1greedyminaverageprunedrandommaxoflearnedmixtureaverageprunedhumangreedymaxgen. independence/complexity

doc summarization

data summarization

end

other applications of submodularity in nlp

alignment between two bi-text strings e and f via submodular maximization
subject to matroid constraints (generalizing bipartite matching). lin &
bilmes, naacl/hlt 2011.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

47 / 48

gen. independence/complexity

doc summarization

data summarization

end

other applications of submodularity in nlp

alignment between two bi-text strings e and f via submodular maximization
subject to matroid constraints (generalizing bipartite matching). lin &
bilmes, naacl/hlt 2011.
balanced id91 via symmetric submodular functions. application to
word id91 with bipartite neighborhood functions (a words neighbors are
its features, which is the contexts in which the word occurs). used for
constructed class-based or factored language models. narasimhan & bilmes,
ijcai 2007.

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

47 / 48

gen. independence/complexity

doc summarization

data summarization

end

recent tutorials on submodularity

deep mathematical properties of submodularity

with applications to machine learning

a tutorial at nips 2013

http://nips.cc/conferences/2013/program/event.php?id=3688

mathematical properties of submodularity

with applications to machine learning

a tutorial at mlss 2014

http://mlss2014.hiit.fi/

j. bilmes

submodularity in speech/nlp -     microsoft research faculty summit, 2014

page

48 / 48

