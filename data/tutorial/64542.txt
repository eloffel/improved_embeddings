information science and statistics

series editors:
m. jordan
j. kleinberg
b. scho  lkopf

information science and statistics 

akaike and kitagawa: the practice of time series analysis. 
bishop:  pattern recognition and machine learning. 
cowell, dawid, lauritzen, and spiegelhalter: probabilistic networks and

id109. 

doucet, de freitas, and gordon: sequential monte carlo methods in practice. 
fine: feedforward neural network methodology. 
hawkins and olwell: cumulative sum charts and charting for quality improvement. 
jensen: id110s and decision graphs. 
marchette: computer intrusion detection and network monitoring:

a statistical viewpoint. 

rubinstein and kroese: the cross-id178 method:  a unified approach to 

combinatorial optimization, monte carlo simulation, and machine learning. 

studen  : probabilistic conditional independence structures.
vapnik: the nature of statistical learning theory, second edition.  
wallace: statistical and inductive id136 by minimum massage length. 

christopher m. bishop

pattern recognition and
machine learning

christopher m. bishop f.r.eng.
assistant director
microsoft research ltd
cambridge cb3 0fb, u.k.
cmbishop@microsoft.com
http://research.microsoft.com/   cmbishop

series editors
michael jordan
department of computer

science and department
of statistics

university of california,

berkeley

berkeley, ca 94720
usa

professor jon kleinberg
department of computer

science

cornell university
ithaca, ny 14853
usa

bernhard scho  lkopf
max planck institute for
biological cybernetics

spemannstrasse 38
72076 tu  bingen
germany

library of congress control number: 2006922522

isbn-10: 0-387-31073-8
isbn-13: 978-0387-31073-2

printed on acid-free paper.
   2006 springer science+business media, llc
all rights reserved. this work may not be translated or copied in whole or in part without the written permission of the publisher
(springer science+business media, llc, 233 spring street, new york, ny 10013, usa), except for brief excerpts in connection
with reviews or scholarly analysis. use in connection with any form of information storage and retrieval, electronic adaptation,
computer software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
the use in this publication of trade names, trademarks, service marks, and similar terms, even if they are not identified as such,
is not to be taken as an expression of opinion as to whether or not they are subject to proprietary rights.

printed in singapore.

(kyo)

9 8 7 6 5 4 3 2 1

springer.com

this book is dedicated to my family:

jenna, mark, and hugh

total eclipse of the sun, antalya, turkey, 29 march 2006.

preface

pattern recognition has its origins in engineering, whereas machine learning grew
out of computer science. however, these activities can be viewed as two facets of
the same    eld, and together they have undergone substantial development over the
past ten years. in particular, bayesian methods have grown from a specialist niche to
become mainstream, while id114 have emerged as a general framework
for describing and applying probabilistic models. also, the practical applicability of
bayesian methods has been greatly enhanced through the development of a range of
approximate id136 algorithms such as id58 and expectation propa-
gation. similarly, new models based on kernels have had signi   cant impact on both
algorithms and applications.

this new textbook re   ects these recent developments while providing a compre-
hensive introduction to the    elds of pattern recognition and machine learning. it is
aimed at advanced undergraduates or    rst year phd students, as well as researchers
and practitioners, and assumes no previous knowledge of pattern recognition or ma-
chine learning concepts. knowledge of multivariate calculus and basic id202
is required, and some familiarity with probabilities would be helpful though not es-
sential as the book includes a self-contained introduction to basic id203 theory.
because this book has broad scope, it is impossible to provide a complete list of
references, and in particular no attempt has been made to provide accurate historical
attribution of ideas. instead, the aim has been to give references that offer greater
detail than is possible here and that hopefully provide entry points into what, in some
cases, is a very extensive literature. for this reason, the references are often to more
recent textbooks and review articles rather than to original sources.

the book is supported by a great deal of additional material, including lecture
slides as well as the complete set of    gures used in the book, and the reader is
encouraged to visit the book web site for the latest information:

http://research.microsoft.com/   cmbishop/prml

vii

viii

preface

exercises
the exercises that appear at the end of every chapter form an important com-
ponent of the book. each exercise has been carefully chosen to reinforce concepts
explained in the text or to develop and generalize them in signi   cant ways, and each
is graded according to dif   culty ranging from ((cid:1)), which denotes a simple exercise
taking a few minutes to complete, through to ((cid:1) (cid:1) (cid:1)), which denotes a signi   cantly
more complex exercise.

it has been dif   cult to know to what extent these solutions should be made
widely available. those engaged in self study will    nd worked solutions very ben-
e   cial, whereas many course tutors request that solutions be available only via the
publisher so that the exercises may be used in class. in order to try to meet these
con   icting requirements, those exercises that help amplify key points in the text, or
that    ll in important details, have solutions that are available as a pdf    le from the
book web site. such exercises are denoted by www . solutions for the remaining
exercises are available to course tutors by contacting the publisher (contact details
are given on the book web site). readers are strongly encouraged to work through
the exercises unaided, and to turn to the solutions only as required.

although this book focuses on concepts and principles, in a taught course the
students should ideally have the opportunity to experiment with some of the key
algorithms using appropriate data sets. a companion volume (bishop and nabney,
2008) will deal with practical aspects of pattern recognition and machine learning,
and will be accompanied by matlab software implementing most of the algorithms
discussed in this book.

acknowledgements
first of all i would like to express my sincere thanks to markus svens  en who
has provided immense help with preparation of    gures and with the typesetting of
the book in latex. his assistance has been invaluable.

i am very grateful to microsoft research for providing a highly stimulating re-
search environment and for giving me the freedom to write this book (the views and
opinions expressed in this book, however, are my own and are therefore not neces-
sarily the same as those of microsoft or its af   liates).

springer has provided excellent support throughout the    nal stages of prepara-
tion of this book, and i would like to thank my commissioning editor john kimmel
for his support and professionalism, as well as joseph piliero for his help in design-
ing the cover and the text format and maryann brickner for her numerous contribu-
tions during the production phase. the inspiration for the cover design came from a
discussion with antonio criminisi.

i also wish to thank oxford university press for permission to reproduce ex-
cerpts from an earlier textbook, neural networks for pattern recognition (bishop,
1995a). the images of the mark 1 id88 and of frank rosenblatt are repro-
duced with the permission of arvin calspan advanced technology center. i would
also like to thank asela gunawardana for plotting the spectrogram in figure 13.1,
and bernhard sch  olkopf for permission to use his kernel pca code to plot fig-
ure 12.17.

preface

ix

many people have helped by proofreading draft material and providing com-
ments and suggestions, including shivani agarwal, c  edric archambeau, arik azran,
andrew blake, hakan cevikalp, michael fourman, brendan frey, zoubin ghahra-
mani, thore graepel, katherine heller, ralf herbrich, geoffrey hinton, adam jo-
hansen, matthew johnson, michael jordan, eva kalyvianaki, anitha kannan, julia
lasserre, david liu, tom minka, ian nabney, tonatiuh pena, yuan qi, sam roweis,
balaji sanjiya, toby sharp, ana costa e silva, david spiegelhalter, jay stokes, tara
symeonides, martin szummer, marshall tappen, ilkay ulusoy, chris williams, john
winn, and andrew zisserman.

finally, i would like to thank my wife jenna who has been hugely supportive

throughout the several years it has taken to write this book.

chris bishop
cambridge
february 2006

mathematical notation

i have tried to keep the mathematical content of the book to the minimum neces-
sary to achieve a proper understanding of the    eld. however, this minimum level is
nonzero, and it should be emphasized that a good grasp of calculus, id202,
and id203 theory is essential for a clear understanding of modern pattern recog-
nition and machine learning techniques. nevertheless, the emphasis in this book is
on conveying the underlying concepts rather than on mathematical rigour.

i have tried to use a consistent notation throughout the book, although at times
this means departing from some of the conventions used in the corresponding re-
search literature. vectors are denoted by lower case bold roman letters such as
x, and all vectors are assumed to be column vectors. a superscript t denotes the
transpose of a matrix or vector, so that xt will be a row vector. uppercase bold
roman letters, such as m, denote matrices. the notation (w1, . . . , wm ) denotes a
row vector with m elements, while the corresponding column vector is written as
w = (w1, . . . , wm )t.

the notation [a, b] is used to denote the closed interval from a to b, that is the
interval including the values a and b themselves, while (a, b) denotes the correspond-
ing open interval, that is the interval excluding a and b. similarly, [a, b) denotes an
interval that includes a but excludes b. for the most part, however, there will be
little need to dwell on such re   nements as whether the end points of an interval are
included or not.
the m    m identity matrix (also known as the unit matrix) is denoted im ,
which will be abbreviated to i where there is no ambiguity about it dimensionality.
it has elements iij that equal 1 if i = j and 0 if i (cid:2)= j.

functional is discussed in appendix d.

a functional is denoted f[y] where y(x) is some function. the concept of a
the notation g(x) = o(f(x)) denotes that |f(x)/g(x)| is bounded as x        .

for instance if g(x) = 3x2 + 2, then g(x) = o(x2).

the expectation of a function f(x, y) with respect to a random variable x is de-
noted by ex[f(x, y)]. in situations where there is no ambiguity as to which variable
is being averaged over, this will be simpli   ed by omitting the suf   x, for instance

xi

xii

mathematical notation

e[x]. if the distribution of x is conditioned on another variable z, then the corre-
sponding conditional expectation will be written ex[f(x)|z]. similarly, the variance
is denoted var[f(x)], and for vector variables the covariance is written cov[x, y]. we
shall also use cov[x] as a shorthand notation for cov[x, x]. the concepts of expecta-
tions and covariances are introduced in section 1.2.2.

if we have n values x1, . . . , xn of a d-dimensional vector x = (x1, . . . , xd)t,
we can combine the observations into a data matrix x in which the nth row of x
corresponds to the row vector xt
n. thus the n, i element of x corresponds to the
ith element of the nth observation xn. for the case of one-dimensional variables we
shall denote such a matrix by x, which is a column vector whose nth element is xn.
note that x (which has dimensionality n) uses a different typeface to distinguish it
from x (which has dimensionality d).

contents

preface

mathematical notation

contents

1

introduction
1.1
1.2

. . . . . . .

example: polynomial curve fitting . . . . . . .
. . . . .
id203 theory . . . . .
1.2.1
. . . .
id203 densities
1.2.2 expectations and covariances
1.2.3 bayesian probabilities
. . .
1.2.4 the gaussian distribution . . . . . . . .
1.2.5 curve    tting re-visited . . .
1.2.6 bayesian curve    tting . . .

. . . . .
. . . . .

. . . . .

. . . . .

. . .

. . . . . . .

. . . .
. . . . . . .

. . . . .

1.3 model selection . . . . . .
1.4
1.5 decision theory . . . . . .

the curse of dimensionality . . . .

. . . . .

. . . . .

. . . . . . .

. . . . .

. . . . . . .

1.5.1 minimizing the misclassi   cation rate
1.5.2 minimizing the expected loss
1.5.3 the reject option . . .
. . .
1.5.4
id136 and decision . . .
1.5.5 id168s for regression . . . .
id205 . . . . .
1.6.1 relative id178 and mutual information . . . .

. . . . . . .
. . . . . . .
. . . . .
. . . .

. . . . . .
. . . . .

. . . . .
. . . . .

. . . . . . .

. . . . .

. . . . .

. . .

1.6

exercises

. . .

.

.

.

.

.

.

. . . . . .

. . . .

. . . . . . . .

. . . . . . .
. . . . .
. . . . . . .
. . . . . . .
. . . .
. . . . . . .
. . . .

. . .
. . . . .
.
. . . .
.
. . . . . . .
. . . .
.
. . . . .
.
. . . .
. . . .
.
. . . . .
. . . .
.
. . . . .
. . . . . .
. . . . . .
.
. . . .
. . . .
.
. . . . .
. . . . .
. . . . . .
. . . .

. . .

vii

xi

xiii

1
4
12
17
19
21
24
28
30
32
33
38
39
41
42
42
46
48
55
58

xiii

xiv

contents

2

id203 distributions
2.1 binary variables . . . . . .

. . . . . . .

. . . .

. . . . . . .

. . . . . . .
. . . .

. . . . . . .
. . . .
. . . . .
. . . . .

2.1.1 the beta distribution . . . .

. . . . .

2.2 multinomial variables . . .

. . . . .

. . . . . . .

2.3

2.4

sequential estimation . . . .

. . .
2.2.1 the dirichlet distribution . . . . . .
the gaussian distribution . . . . .
. . . . . . .
2.3.1 conditional gaussian distributions . . . .
2.3.2 marginal gaussian distributions . . . . .
2.3.3 bayes    theorem for gaussian variables . . . . . .
2.3.4 maximum likelihood for the gaussian . . . . . .
2.3.5
2.3.6 bayesian id136 for the gaussian . . .
student   s t-distribution . . .
2.3.7
2.3.8
periodic variables . . . . . .
2.3.9 mixtures of gaussians . . .
the exponential family . . . . . .
2.4.1 maximum likelihood and suf   cient statistics
2.4.2 conjugate priors
2.4.3 noninformative priors

. . . . . . .
. . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .

. . . . .
. . . . .
. . . . .
. . . . .

. . . . .

.

. . . . .
. . .
2.5 nonparametric methods . . . . . .

. . . . .
. . . . .
. . . . .

. . .
.
. . . .
. . . . .
. . .
. . . . . .
. . . . .
. . . . .
.
. . . . .
.
. . . . .
. . . .
.
. . . . .
. . . .
. . . .
. . . .
. . . .

67
68
71
74
76
78
85
88
90
93
94
97
. 102
. 105
. 110
. 113
. . . . . . . . 116
. . . .
. 117
. . . .
. 117
. . . .
. 120
. . . . . 122
. . . . . 124
. 127

. . . . . . .
. . . . . . .
. . . . . . .
. . . . .
. . . . .
. . . . . . . . .

2.5.1 kernel density estimators . . . . . .
2.5.2 nearest-neighbour methods

. . .
. . . . . . .
. .

. . . . . . . .

exercises

. . .

.

.

.

.

.

.

. . . . .

3 linear models for regression

3.1

. . . . .

linear basis function models . . .
3.1.1 maximum likelihood and least squares . . . . . .
3.1.2 geometry of least squares
3.1.3
3.1.4 regularized least squares . . . . . .
3.1.5 multiple outputs
the bias-variance decomposition . . . . . . . .

sequential learning . . . . .

. . . . . . . .

. . . . . .

. . . . .

. . . . .

. . .

3.3.1
3.3.2
3.3.3 equivalent kernel . . .

3.2
3.3 bayesian id75 . . . .
parameter distribution . . .
predictive distribution . . .
. . .
3.4 bayesian model comparison . . . .
the evidence approximation . . .
3.5
3.5.1 evaluation of the evidence function . . .
3.5.2 maximizing the evidence function . . . .
3.5.3 effective number of parameters
. . . . .
. . . . . .
limitations of fixed basis functions
. .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . .

3.6
exercises

. . . . . . . .

. . . . .

. . .

.

.

.

.

.

.

. . . . . . .

. . . .
. . . . .

137
. 138
. 140
. . . . . 143
. . . .
. 143
. . . . . 144
. . . .
. 146
. . . . . 147
. . . .
. 152
. 152
. . . .
. 156
. . . .
. 159
. . . .
. . . .
. 161
. . . .
. 165
. . . . . 166
. . . . . 168
. . . . . 170
. . . . . 172
. 173

. . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . . . . . .

contents

xv

. . . .
. . . .
. . . .

. . . . .
. . . . .
. . . .
. . . . .

.

4.2

. . . . .

. . . . .
. . . . .
. . . . .

. . . . . . .
. . . . . . .
. . . . . . .

fisher   s linear discriminant . . . . . . . .
. . .
fisher   s discriminant for multiple classes
. . .
. . . . .

4 linear models for classi   cation
4.1 discriminant functions . . .
. . .
4.1.1 two classes .
4.1.2 multiple classes . . .
4.1.3 least squares for classi   cation . . .
4.1.4
4.1.5 relation to least squares . . . . . .
4.1.6
4.1.7 the id88 algorithm . . . . . .
probabilistic generative models . . . . . .
4.2.1 continuous inputs
4.2.2 maximum likelihood solution . . . . . .
4.2.3 discrete features . . . . . .
4.2.4 exponential family . . . . .
probabilistic discriminative models . . . . . . .
4.3.1
fixed basis functions . . . .
4.3.2 id28 . . . . .
4.3.3
. . .
4.3.4 multiclass id28 . . . . . .
4.3.5
4.3.6 canonical link functions . . . . . . . . .
the laplace approximation . . . .
4.4.1 model comparison and bic . . . . . .

iterative reweighted least squares

probit regression .

. . . . .
. . . . .

. . . . .
. . . . .

. . . . .

. . . . .

. . . . .

. . . . .

. . .

4.3

4.4

. .

. . . . . . .
. . . . .
. . . . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . . . .

. . . . .
. . . . .

. . . . . . .
. . . . .
. . . . . . .

. . . . .

4.5 bayesian id28 . . .
4.5.1 laplace approximation . . .
4.5.2
predictive distribution . . .
exercises

. . . . .
. . . . .
. . . . .
. . . . . . . .

. . . . .

. . .

.

.

.

.

.

.

. . . . . . .
. . . . . . .
. . . . . . .
. .

. . . .
. . . .
. . . .
. . . . . . . . .

5 neural networks

5.1

feed-forward network functions
5.1.1 weight-space symmetries

. . . . . . . .
. . . . . . .

. . . . .

. . . . .

5.2 network training . . . . . .

. . . . .

. . . . . . .

. . . .
. . . . . . .
. . . . .

. . . . .

. . . . .

. . . . .

parameter optimization . . .

5.2.1
5.2.2 local quadratic approximation . . . . . .
5.2.3 use of gradient information . . . . . .
5.2.4 id119 optimization . . . . . .
error id26 . . .
5.3.1 evaluation of error-function derivatives . . . . .
5.3.2 a simple example
5.3.3 ef   ciency of id26 . . . . . .
5.3.4 the jacobian matrix . . . .
the hessian matrix . . . . .
5.4.1 diagonal approximation . . . . . . . .
5.4.2 outer product approximation . . . .
5.4.3

. . . . .
. . . . . . .

inverse hessian . . .

. . . . . . .

. . . . . . .

. . . . . . .

. . . . . . .

. . . . .
. . . . . . .
. . . .

. . . . .
. . . .

. . . . .

. . . . .

. . . . .

. . . .

. . .

5.3

5.4

. . . . . . .

. . . . .

. . . . . . .

179
. . . . . 181
. . . . . 181
. . . . . 182
. 184
. . . . . 186
. . . . . 189
. 191
. . . . . 192
. 196
. . . . . . .
. . . .
. 198
. . . . . 200
. 202
. . . .
. . . .
. 202
. . . . . 203
. . . .
. 204
. 205
. . . .
. . . . . . 207
. . . . . . 209
. . . .
. 210
. . . . . 212
. 213
. . . .
. . . . . . 216
. 217
. 217
. 218
. 220

225
. . . . . 227
. . . . . . 231
. . . . . 232
. . . .
. 236
. . . . . 237
. . . . . . 239
. . . . . 240
. . . . . 241
. . . . . . 242
. . . 245
. . . . . 246
. . . .
. 247
. . . . . 249
. . . . . . 250
. . 251
. . . .
. . . 252

. . . .

xvi

contents

finite differences . . . . . .

5.4.4
5.4.5 exact evaluation of the hessian . . . . .
5.4.6
fast multiplication by the hessian . . . .
. . . . . . .

5.5 id173 in neural networks

. . . . .

invariances . .

5.5.1 consistent gaussian priors . . . . . . .
5.5.2 early stopping . . .
5.5.3
. . .
5.5.4 tangent propagation . . . .
5.5.5 training with transformed data . . . . . .
5.5.6 convolutional networks
. . .
5.5.7

. . . . . . .
. . . . . . .

. . . .
. . . .

. . . . . .

. . . . .

soft weight sharing . . . . .
5.6 mixture density networks . . . . .
5.7 bayesian neural networks . . . . .

. . . . .
. . . . .
. . . . .

5.7.1
posterior parameter distribution . . . . .
5.7.2 hyperparameter optimization . . . . . .
5.7.3 bayesian neural networks for classi   cation . . .

exercises

. . .

.

.

.

.

.

.

. . . . .

. . . . . . . .

. .

6 kernel methods

6.1 dual representations . . . .
6.2 constructing kernels . . . .
6.3 id80s . . . . . .

. . . . .
. . . . .

. . . . . . .
. . . . . . .

. . . . .

. . . .
. . . .

. . . . . . .

6.3.1 nadaraya-watson model

. . . . . . . . .

6.4 gaussian processes . . . . .

. . . . .

. . . . . . .

6.4.1 id75 revisited . . . . .
. . .
6.4.2 gaussian processes for regression . . . .
6.4.3 learning the hyperparameters . . . . . .
6.4.4 automatic relevance determination . . .
6.4.5 gaussian processes for classi   cation . . .
6.4.6 laplace approximation . . .
6.4.7 connection to neural networks . . . . . .
. .

. . . . . . . .

. . . . .

. . . . .

. . .

.

.

.

.

.

.

exercises

7

sparse kernel machines
7.1 maximum margin classi   ers

. . .

. . . . .

7.1.1 overlapping class distributions . . . . .
7.1.2 relation to id28 . . . . .
7.1.3 multiclass id166s . . . . . .
7.1.4
id166s for regression . . . .
7.1.5 computational learning theory . . . . . .

. . . . .
. . . . .

7.2 relevance vector machines

. . . .
7.2.1 rvm for regression . .
. . .
7.2.2 analysis of sparsity . . . . .
7.2.3 rvm for classi   cation . . .

exercises

. . .

.

.

.

.

.

.

. . . . .

. . . . .
. . . . .
. . . . .
. . . . .
. . . . . . . .

. . . . . . . .

. . . . .
. . . . .
. . . . .

. . . . .

. . . . . . .
. . . . . . .

. . . . . . .
. . . . .
. . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . .
. . . . .

. . .

. 252
. . . . . 253
. . . . . 254
. . . . . 256
. . . . . . 257
. . . 259
. . . 261
. . . .
. 263
. . . . . 265
. . . . . 267
. 269
. . . .
. . . .
. 272
. . . .
. 277
. . . . . 278
. . . . . 280
. 281
. 284

. . . . .
. . . . . . . . .

. . . . .
. . . .

. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .

291
. . . . . 293
. . . . . 294
. 299
. . . . . 301
. . . . . 303
. . . 304
. . . 306
. . . 311
. . . 312
. . . 313
. . . .
. 315
. . . . . 319
. 320

. . . . . . .
. . . . .
. . . . . . . . .

. . . .

. . . . . . .

. . . . .
. . . . .

325
. 326
. . . . . . 331
. . . . . . 336
. 338
. . . .
. . . .
. 339
. . . . . 344
. . . .
. 345
. 345
. . . .
. 349
. . . .
. 353
. . . .
. . . . . . . . .
. 357

. . . . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .
. .

contents

xvii

8 id114

8.1 id110s . . . . .

. . . . .

. . . . . . .

8.1.1 example: polynomial regression . . . . .
8.1.2 generative models . . . . .
8.1.3 discrete variables .
. . . . .
8.1.4 linear-gaussian models . . . . . . . . .

. . . . .
. . . . .

. . . .

. . . . . . .

8.2 conditional independence . . . . .
. . .

8.2.1 three example graphs
8.2.2 d-separation . . . .

. . . . .
. . . . .

8.3 markov random fields

. . . . . .

. . . . .

. . . . . . .

. . . .

. . . . . . .

8.4

. . . . . .

id136 on a chain .

8.3.1 conditional independence properties . . .
factorization properties
8.3.2
. . .
illustration: image de-noising . . . . . .
8.3.3
8.3.4 relation to directed graphs . . . . .
. . .
id136 in id114 . . .
8.4.1
. . .
8.4.2 trees
8.4.3
8.4.4 the sum-product algorithm . . . . . . .
8.4.5 the max-sum algorithm . . . . . .
8.4.6 exact id136 in general graphs
8.4.7 loopy belief propagation . . . . . .
8.4.8 learning the graph structure . . . . . .

. . .
. . . .
. . .

. . . . .
. . . . .

factor graphs .

. . . . . . .

. . . . . .

. . . .

. . . .

. . .

.

.

.

.

. . . . . . .

. . . . . . .

. . . . .

. . . . .
. . . . .
. . . . .

. . . . .

. . . . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . . . .

. . . . . . .
. . . . .
. . . . .
. . . . .
. . . . .
. . . . . . .
. . . . . . .
. . .

359
. . . . . 360
. . . 362
. 365
. . . .
. . . .
. 366
. . . . . 370
. . . .
. 372
. 373
. . . .
. . . 378
. . . .
. 383
. . . . . 383
. . . . . 384
. . . . . 387
. . . . . 390
. . . .
. 393
. . . .
. 394
. . . . . 398
. . . 399
. . . . . . 402
. . . . . 411
. . . . . 416
. . . . . 417
. . . . . . 418
. 418

exercises

. . .

.

.

.

.

.

.

. . . . .

. . . . . . . .

. .

. . . . . . . . .

9 mixture models and em

9.1 id116 id91 . . . .

. . . . .

. . . . . . .

9.1.1

image segmentation and compression . . . . . .

9.2 mixtures of gaussians . . .

. . . . .

. . . . . . .

9.2.1 maximum likelihood . . . .
9.2.2 em for gaussian mixtures . . . . .

. . . . .

. . .

9.3 an alternative view of em . . . .

. . . . .

9.3.1 gaussian mixtures revisited . . . . . .
9.3.2 relation to id116 . . . .
9.3.3 mixtures of bernoulli distributions . . . .
9.3.4 em for bayesian id75 . . . .
the em algorithm in general . . .

. . . . .

. . . . .
. . . . . . . .

. . .

.

.

.

.

.

.

. . . . .

9.4
exercises

. . . .

. . . .
. . . . . . .
. . . . .
. . . . . . .

423
. . . . . 424
. . . . . . 428
. . . . . 430
. . . .
. 432
. . . . . 435
. 439
. . . .
. . . . . . 441
. . . .
. 443
. . . . . 444
. . . . . 448
. . . .
. 450
. 455
. . . . . . . . .

. . . . . . .
. . . . .
. . . . .
. . . . . . .
. .

. . . . .

10 approximate id136

10.1 variational id136 . . . .

. . . . .

10.1.1 factorized distributions . . .
10.1.2 properties of factorized approximations . . . . .
10.1.3 example: the univariate gaussian . . . .
10.1.4 model comparison . . . . .

. . . . .
. . . . . . .

. . . . .

. . . . .

. . . . . . .

. . . .
. . . . . . .

10.2 illustration: variational mixture of gaussians . . . . . .

461
. . . . . 462
. 464
. . . .
. 466
. . . . .
. . . . . 470
. . . .
. 473
. 474
. . . . .

xviii

contents

10.2.1 variational distribution . . .
10.2.2 variational lower bound . . . . . .
10.2.3 predictive density . . . . . .
10.2.4 determining the number of components . . . . .
10.2.5 induced factorizations

. . . . . . .
. . . . .
. . . . . . .

. . . . .

. . . . .

. . .

. . .
10.3 variational id75 . . .
10.3.1 variational distribution . . .
10.3.2 predictive distribution . . .
10.3.3 lower bound . . . .

10.4 exponential family distributions

. . . . .
. . . . .
. . . . .
. . . . .

. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .

. . . . . . .

. . . .
. . . . . . . .

. . . . . . .
. . . . .

10.4.1 variational message passing . . . . . .

. . . . .

10.5 local variational methods . . . . .
10.6 variational id28 . . . . . .

. . . . .

. . . . . . .

. . . .
. . . . . . .

. . . . .

10.6.1 variational posterior distribution . . . . .
10.6.2 optimizing the variational parameters . . . . . .
10.6.3 id136 of hyperparameters
10.7 expectation propagation . . . . . .

. . . . . . .
10.7.1 example: the clutter problem . . . . . .
10.7.2 expectation propagation on graphs . . . .
. .

. . . . . . . .

exercises

. . . . .

. . . . .

. . .

. . .

.

.

.

.

.

.

. . . .
. . . . .
. . . . .
. . . . . . . . .

. . . . . . .

. . . . . . .

. . . .
. 475
. . . . . 481
. . . .
. 482
. 483
. . . . .
. 485
. . . .
. 486
. . . .
. . . .
. 486
. 488
. . . .
. . . 489
. . . . . 490
. . . . . . 491
. 493
. 498
. . . 498
. . . . . . 500
. 502
. . . . . . 505
. . . . . 511
. . . . . 513
. 517

11 sampling methods

11.1 basic sampling algorithms

. . . .
. . .
11.1.1 standard distributions
11.1.2 rejection sampling . . . . .
11.1.3 adaptive rejection sampling . . . . . . .
11.1.4 importance sampling . . . .
11.1.5 sampling-importance-resampling . . . .
11.1.6 sampling and the em algorithm . . . . .

. . . . .
. . . . .
. . . . .

. . . . .

11.2 id115 . . . .

. . . . .

. . . . . . .
. . . . . . .
. . . . . . .
. . . . .
. . . . . . .
. . . . .
. . . . .
. . . . . . .

11.2.1 markov chains
. . . .
11.2.2 the metropolis-hastings algorithm . . .

. . . . . . .

. . .

11.3 id150 . . . . . .
11.4 slice sampling . . . . . . .
11.5 the hybrid monte carlo algorithm . . . . . . .

. . . . . . .
. . . . . . .

. . . . .
. . . . .

11.5.1 dynamical systems . . . . .
11.5.2 hybrid monte carlo . . . .

. . . . .
. . . . .

11.6 estimating the partition function . . . . . . . .
exercises
. .

. . . . . . . .

. . . . .

. . .

.

.

.

.

.

.

. . . . . . .
. . . . .
. . . .
. . . .
. . . . .
. . . . . . .
. . . . . . .
. . . . .
. . . . . . . . .

12 continuous latent variables

12.1 principal component analysis . . .

. . . . .

12.1.1 maximum variance formulation . . . . .
12.1.2 minimum-error formulation . . . . . . .
12.1.3 applications of pca . . . .
12.1.4 pca for high-dimensional data

. . . . .

. . . . .

. . . . . . .
. . . . .
. . . . .
. . . . . . .
. . . . .

523
. 526
. . . .
. 526
. . . .
. . . .
. 528
. . . . . 530
. . . .
. 532
. . . . . 534
. . . . . 536
. 537
. . . .
. . . 539
. . . . . 541
. . . . . 542
. . . . . 546
. . . . . 548
. . . .
. 548
. . . .
. 552
. . . . . 554
. 556

559
. . . .
. 561
. . . . . 561
. . . . . 563
. . . .
. 565
. . . . . 569

contents

xix

12.2 probabilistic pca . . . . .

. . . . . . .

. . . .
12.2.1 maximum likelihood pca . . . . . . . .
12.2.2 em algorithm for pca . . .
12.2.3 bayesian pca . . .
12.2.4 factor analysis . . .

. . . . . . .
. . . . . . .

. . . .
. . . .

. . . . .

.

.

.

. . . .

. . . . . . .

12.3 kernel pca .
12.4 nonlinear latent variable models . . . . . . . .
12.4.1 independent component analysis . . . . .
12.4.2 autoassociative neural networks . . . . .
12.4.3 modelling nonlinear manifolds . . . . . .
. .

. . . . . . . .

exercises

. . . . .

. . . .

. . .

.

.

.

.

.

.

. . . . . . .
. . . . .
. . . . . . .

. . . . . . .
. . . . . . .
. . . . . . . .

. . . 570
. . . . . 574
. . . .
. 577
. . . 580
. . . 583
. . . 586
. . . . . 591
. . . . . 591
. . . . . 592
. . . . . 595
. 599

. . . . .
. . . . .
. . . . .
. . . . .
. . . . . . . . .

13 sequential data

. . . .
. . . . . . .
. . . . .
. . . . .

13.1 markov models . .
13.2 id48

. . . . .

. . . . .

. . . . . . .

. . . . . .

. . . . .

13.2.1 maximum likelihood for the id48 . . .
13.2.2 the forward-backward algorithm . . . .
13.2.3 the sum-product algorithm for the id48 . . . .
13.2.4 scaling factors
13.2.5 the viterbi algorithm . . . .
13.2.6 extensions of the hidden markov model . . . . .

. . . . . . .

. . . . .

. . . .

. . .

. . . . . . .

. . . . . . .

13.3 linear dynamical systems . . . . .
. . .
13.3.1 id136 in lds .
. .
. . .
13.3.2 learning in lds . . .
13.3.3 extensions of lds . . . . .
13.3.4 particle    lters . . . .
. . . . .

exercises

. . .

.

.

.

.

.

. . . . .
. . . . .
. . . . .
. . . . .

. . . . . . .
. . . . . . .
. . . . . . .
. . . . . . .

. . . . . . .
. . . .

. . . .
. . . . . . .

. . . . . . .
. . .

. . . . .

605
. . . . . 607
. . . .
. 610
. . . . . 615
. . . . . 618
. 625
. . . 627
. 629
. 631
. 635
. 638
. 642
. 644
. . . 645
. . . . . . . 646

. . . .
. . . . .
. . . .
. . . .
. . . .
. . . .

14 combining models

14.1 bayesian model averaging . . . . .
14.2 committees . . . . . .
14.3 boosting .

. . . . .
. . . . . . .
14.3.1 minimizing exponential error
14.3.2 error functions for boosting . . . . . .

. . . . . . .
. . . .
. . . . . .

. . . . .

. . .

. . .

.

.

.

.

14.4 tree-based models . . . . .
14.5 conditional mixture models . . . .

. . . . .

. . . . .

. . . . . . .

14.5.1 mixtures of id75 models . . .
. . . . . . .
14.5.2 mixtures of logistic models
. . .
14.5.3 mixtures of experts . .
. . . . .
. . . . .
. . . . . . . .

. . .

.

.

.

.

.

.

exercises

. . .

. . . . .

. . . . . . .

. . . . . . . .

. . . . . . .
. . . .

653
. . . .
. 654
. . . . . 655
. 657
. . . 659
. . . . . . 661
. . . . . 663
. . . .
. 666
. . . . . 667
. . . . . 670
. 672
. . . .
. . . . . . . . .
. 674

. . . .
. . . . . . .
. . . . .
. . . . .
. . . . . . .
. .

appendix a data sets

appendix b id203 distributions

appendix c properties of matrices

677

685

695

xx

contents

appendix d calculus of variations

appendix e lagrange multipliers

references

index

703

707

711

729

1

introduction

the problem of searching for patterns in data is a fundamental one and has a long and
successful history. for instance, the extensive astronomical observations of tycho
brahe in the 16th century allowed johannes kepler to discover the empirical laws of
planetary motion, which in turn provided a springboard for the development of clas-
sical mechanics. similarly, the discovery of regularities in atomic spectra played a
key role in the development and veri   cation of quantum physics in the early twenti-
eth century. the    eld of pattern recognition is concerned with the automatic discov-
ery of regularities in data through the use of computer algorithms and with the use of
these regularities to take actions such as classifying the data into different categories.
consider the example of recognizing handwritten digits, illustrated in figure 1.1.
each digit corresponds to a 28  28 pixel image and so can be represented by a vector
x comprising 784 real numbers. the goal is to build a machine that will take such a
vector x as input and that will produce the identity of the digit 0, . . . , 9 as the output.
this is a nontrivial problem due to the wide variability of handwriting. it could be

1

2

1. introduction

figure 1.1 examples of hand-written dig-

its taken from us zip codes.

tackled using handcrafted rules or heuristics for distinguishing the digits based on
the shapes of the strokes, but in practice such an approach leads to a proliferation of
rules and of exceptions to the rules and so on, and invariably gives poor results.
far better results can be obtained by adopting a machine learning approach in
which a large set of n digits {x1, . . . , xn} called a training set is used to tune the
parameters of an adaptive model. the categories of the digits in the training set
are known in advance, typically by inspecting them individually and hand-labelling
them. we can express the category of a digit using target vector t, which represents
the identity of the corresponding digit. suitable techniques for representing cate-
gories in terms of vectors will be discussed later. note that there is one such target
vector t for each digit image x.

the result of running the machine learning algorithm can be expressed as a
function y(x) which takes a new digit image x as input and that generates an output
vector y, encoded in the same way as the target vectors. the precise form of the
function y(x) is determined during the training phase, also known as the learning
phase, on the basis of the training data. once the model is trained it can then de-
termine the identity of new digit images, which are said to comprise a test set. the
ability to categorize correctly new examples that differ from those used for train-
ing is known as generalization. in practical applications, the variability of the input
vectors will be such that the training data can comprise only a tiny fraction of all
possible input vectors, and so generalization is a central goal in pattern recognition.
for most practical applications, the original input variables are typically prepro-
cessed to transform them into some new space of variables where, it is hoped, the
pattern recognition problem will be easier to solve. for instance, in the digit recogni-
tion problem, the images of the digits are typically translated and scaled so that each
digit is contained within a box of a    xed size. this greatly reduces the variability
within each digit class, because the location and scale of all the digits are now the
same, which makes it much easier for a subsequent pattern recognition algorithm
to distinguish between the different classes. this pre-processing stage is sometimes
also called feature extraction. note that new test data must be pre-processed using
the same steps as the training data.

pre-processing might also be performed in order to speed up computation. for
example, if the goal is real-time face detection in a high-resolution video stream,
the computer must handle huge numbers of pixels per second, and presenting these
directly to a complex pattern recognition algorithm may be computationally infeasi-
ble. instead, the aim is to    nd useful features that are fast to compute, and yet that

1. introduction

3

also preserve useful discriminatory information enabling faces to be distinguished
from non-faces. these features are then used as the inputs to the pattern recognition
algorithm. for instance, the average value of the image intensity over a rectangular
subregion can be evaluated extremely ef   ciently (viola and jones, 2004), and a set of
such features can prove very effective in fast face detection. because the number of
such features is smaller than the number of pixels, this kind of pre-processing repre-
sents a form of id84. care must be taken during pre-processing
because often information is discarded, and if this information is important to the
solution of the problem then the overall accuracy of the system can suffer.

applications in which the training data comprises examples of the input vectors
along with their corresponding target vectors are known as supervised learning prob-
lems. cases such as the digit recognition example, in which the aim is to assign each
input vector to one of a    nite number of discrete categories, are called classi   cation
problems. if the desired output consists of one or more continuous variables, then
the task is called regression. an example of a regression problem would be the pre-
diction of the yield in a chemical manufacturing process in which the inputs consist
of the concentrations of reactants, the temperature, and the pressure.

in other pattern recognition problems, the training data consists of a set of input
vectors x without any corresponding target values. the goal in such unsupervised
learning problems may be to discover groups of similar examples within the data,
where it is called id91, or to determine the distribution of data within the input
space, known as density estimation, or to project the data from a high-dimensional
space down to two or three dimensions for the purpose of visualization.

finally, the technique of id23 (sutton and barto, 1998) is con-
cerned with the problem of    nding suitable actions to take in a given situation in
order to maximize a reward. here the learning algorithm is not given examples of
optimal outputs, in contrast to supervised learning, but must instead discover them
by a process of trial and error. typically there is a sequence of states and actions in
which the learning algorithm is interacting with its environment. in many cases, the
current action not only affects the immediate reward but also has an impact on the re-
ward at all subsequent time steps. for example, by using appropriate reinforcement
learning techniques a neural network can learn to play the game of backgammon to a
high standard (tesauro, 1994). here the network must learn to take a board position
as input, along with the result of a dice throw, and produce a strong move as the
output. this is done by having the network play against a copy of itself for perhaps a
million games. a major challenge is that a game of backgammon can involve dozens
of moves, and yet it is only at the end of the game that the reward, in the form of
victory, is achieved. the reward must then be attributed appropriately to all of the
moves that led to it, even though some moves will have been good ones and others
less so. this is an example of a credit assignment problem. a general feature of re-
inforcement learning is the trade-off between exploration, in which the system tries
out new kinds of actions to see how effective they are, and exploitation, in which
the system makes use of actions that are known to yield a high reward. too strong
a focus on either exploration or exploitation will yield poor results. reinforcement
learning continues to be an active area of machine learning research. however, a

4

1. introduction

figure 1.2 plot of a training data set of n =
10 points, shown as blue circles,
each comprising an observation
of the input variable x along with
the corresponding target variable
t. the green curve shows the
function sin(2  x) used to gener-
ate the data. our goal is to pre-
dict the value of t for some new
value of x, without knowledge of
the green curve.

t

1

0

   1

0

x

1

detailed treatment lies beyond the scope of this book.

although each of these tasks needs its own tools and techniques, many of the
key ideas that underpin them are common to all such problems. one of the main
goals of this chapter is to introduce, in a relatively informal way, several of the most
important of these concepts and to illustrate them using simple examples. later in
the book we shall see these same ideas re-emerge in the context of more sophisti-
cated models that are applicable to real-world pattern recognition applications. this
chapter also provides a self-contained introduction to three important tools that will
be used throughout the book, namely id203 theory, decision theory, and infor-
mation theory. although these might sound like daunting topics, they are in fact
straightforward, and a clear understanding of them is essential if machine learning
techniques are to be used to best effect in practical applications.

1.1. example: polynomial curve fitting

we begin by introducing a simple regression problem, which we shall use as a run-
ning example throughout this chapter to motivate a number of key concepts. sup-
pose we observe a real-valued input variable x and we wish to use this observation to
predict the value of a real-valued target variable t. for the present purposes, it is in-
structive to consider an arti   cial example using synthetically generated data because
we then know the precise process that generated the data for comparison against any
learned model. the data for this example is generated from the function sin(2  x)
with random noise included in the target values, as described in detail in appendix a.
now suppose that we are given a training set comprising n observations of x,
written x     (x1, . . . , xn )t, together with corresponding observations of the values
of t, denoted t     (t1, . . . , tn)t. figure 1.2 shows a plot of a training set comprising
n = 10 data points. the input data set x in figure 1.2 was generated by choos-
ing values of xn, for n = 1, . . . , n, spaced uniformly in range [0, 1], and the target
data set t was obtained by    rst computing the corresponding values of the function

1.1. example: polynomial curve fitting

5

sin(2  x) and then adding a small level of random noise having a gaussian distri-
bution (the gaussian distribution is discussed in section 1.2.4) to each such point in
order to obtain the corresponding value tn. by generating data in this way, we are
capturing a property of many real data sets, namely that they possess an underlying
regularity, which we wish to learn, but that individual observations are corrupted by
random noise. this noise might arise from intrinsically stochastic (i.e. random) pro-
cesses such as radioactive decay but more typically is due to there being sources of
variability that are themselves unobserved.

(cid:1)t of the target variable for some new value(cid:1)x of the input variable. as we shall see
set. furthermore the observed data are corrupted with noise, and so for a given(cid:1)x
there is uncertainty as to the appropriate value for(cid:1)t. id203 theory, discussed

later, this involves implicitly trying to discover the underlying function sin(2  x).
this is intrinsically a dif   cult problem as we have to generalize from a    nite data

our goal is to exploit this training set in order to make predictions of the value

in section 1.2, provides a framework for expressing such uncertainty in a precise
and quantitative manner, and decision theory, discussed in section 1.5, allows us to
exploit this probabilistic representation in order to make predictions that are optimal
according to appropriate criteria.

for the moment, however, we shall proceed rather informally and consider a
simple approach based on curve    tting. in particular, we shall    t the data using a
polynomial function of the form

m(cid:2)

y(x, w) = w0 + w1x + w2x2 + . . . + wm xm =

wjxj

(1.1)

j=0

where m is the order of the polynomial, and xj denotes x raised to the power of j.
the polynomial coef   cients w0, . . . , wm are collectively denoted by the vector w.
note that, although the polynomial function y(x, w) is a nonlinear function of x, it
is a linear function of the coef   cients w. functions, such as the polynomial, which
are linear in the unknown parameters have important properties and are called linear
models and will be discussed extensively in chapters 3 and 4.

the values of the coef   cients will be determined by    tting the polynomial to the
training data. this can be done by minimizing an error function that measures the
mis   t between the function y(x, w), for any given value of w, and the training set
data points. one simple choice of error function, which is widely used, is given by
the sum of the squares of the errors between the predictions y(xn, w) for each data
point xn and the corresponding target values tn, so that we minimize

e(w) =

1
2

{y(xn, w)     tn}2

(1.2)

n(cid:2)

n=1

where the factor of 1/2 is included for later convenience. we shall discuss the mo-
tivation for this choice of error function later in this chapter. for the moment we
simply note that it is a nonnegative quantity that would be zero if, and only if, the

6

1. introduction

figure 1.3 the error

function (1.2) corre-
sponds to (one half of) the sum of
the squares of the displacements
(shown by the vertical green bars)
of each data point from the function
y(x, w).

t

tn

y(xn, w)

xn

x

exercise 1.1

function y(x, w) were to pass exactly through each training data point. the geomet-
rical interpretation of the sum-of-squares error function is illustrated in figure 1.3.

we can solve the curve    tting problem by choosing the value of w for which
e(w) is as small as possible. because the error function is a quadratic function of
the coef   cients w, its derivatives with respect to the coef   cients will be linear in the
elements of w, and so the minimization of the error function has a unique solution,
denoted by w(cid:1), which can be found in closed form. the resulting polynomial is
given by the function y(x, w(cid:1)).

there remains the problem of choosing the order m of the polynomial, and as
we shall see this will turn out to be an example of an important concept called model
comparison or model selection. in figure 1.4, we show four examples of the results
of    tting polynomials having orders m = 0, 1, 3, and 9 to the data set shown in
figure 1.2.

we notice that the constant (m = 0) and    rst order (m = 1) polynomials
give rather poor    ts to the data and consequently rather poor representations of the
function sin(2  x). the third order (m = 3) polynomial seems to give the best    t
to the function sin(2  x) of the examples shown in figure 1.4. when we go to a
much higher order polynomial (m = 9), we obtain an excellent    t to the training
data. in fact, the polynomial passes exactly through each data point and e(w(cid:1)) = 0.
however, the    tted curve oscillates wildly and gives a very poor representation of
the function sin(2  x). this latter behaviour is known as over-   tting.

as we have noted earlier, the goal is to achieve good generalization by making
accurate predictions for new data. we can obtain some quantitative insight into the
dependence of the generalization performance on m by considering a separate test
set comprising 100 data points generated using exactly the same procedure used
to generate the training set points but with new choices for the random noise values
included in the target values. for each choice of m, we can then evaluate the residual
value of e(w(cid:1)) given by (1.2) for the training data, and we can also evaluate e(w(cid:1))
for the test data set. it is sometimes more convenient to use the root-mean-square

t

1

0

   1

t

1

0

   1

0

0

1.1. example: polynomial curve fitting

7

m = 1

x

1

m = 9

m = 0

t

1

0

   1

x

1

0

m = 3

t

1

0

   1

x

1

0

x

1

figure 1.4 plots of polynomials having various orders m, shown as red curves,    tted to the data set shown in
figure 1.2.

(cid:3)

(rms) error de   ned by

erms =

2e(w(cid:1))/n

(1.3)

in which the division by n allows us to compare different sizes of data sets on
an equal footing, and the square root ensures that erms is measured on the same
scale (and in the same units) as the target variable t. graphs of the training and
test set rms errors are shown, for various values of m, in figure 1.5. the test
set error is a measure of how well we are doing in predicting the values of t for
new data observations of x. we note from figure 1.5 that small values of m give
relatively large values of the test set error, and this can be attributed to the fact that
the corresponding polynomials are rather in   exible and are incapable of capturing
the oscillations in the function sin(2  x). values of m in the range 3 (cid:1) m (cid:1) 8
give small values for the test set error, and these also give reasonable representations
of the generating function sin(2  x), as can be seen, for the case of m = 3, from
figure 1.4.

8

1. introduction

figure 1.5 graphs of

the root-mean-square
error, de   ned by (1.3), evaluated
on the training set and on an inde-
pendent test set for various values
of m.

training
test

1

s
m
r
e

0.5

0

0

3

m

6

9

for m = 9, the training set error goes to zero, as we might expect because
this polynomial contains 10 degrees of freedom corresponding to the 10 coef   cients
w0, . . . , w9, and so can be tuned exactly to the 10 data points in the training set.
however, the test set error has become very large and, as we saw in figure 1.4, the
corresponding function y(x, w(cid:1)) exhibits wild oscillations.

this may seem paradoxical because a polynomial of given order contains all
lower order polynomials as special cases. the m = 9 polynomial is therefore capa-
ble of generating results at least as good as the m = 3 polynomial. furthermore, we
might suppose that the best predictor of new data would be the function sin(2  x)
from which the data was generated (and we shall see later that this is indeed the
case). we know that a power series expansion of the function sin(2  x) contains
terms of all orders, so we might expect that results should improve monotonically as
we increase m.

we can gain some insight into the problem by examining the values of the co-
ef   cients w(cid:1) obtained from polynomials of various order, as shown in table 1.1.
we see that, as m increases, the magnitude of the coef   cients typically gets larger.
in particular for the m = 9 polynomial, the coef   cients have become    nely tuned
to the data by developing large positive and negative values so that the correspond-

table 1.1 table of the coef   cients w(cid:1) for
polynomials of various order.
observe how the typical mag-
nitude of
the coef   cients in-
creases dramatically as the or-
der of the polynomial increases.

w(cid:1)
0
w(cid:1)
1
w(cid:1)
2
w(cid:1)
3
w(cid:1)
4
w(cid:1)
5
w(cid:1)
6
w(cid:1)
7
w(cid:1)
8
w(cid:1)
9

0.19

m = 0 m = 1 m = 6
0.31
7.99
-25.43
17.37

0.82
-1.27

m = 9
0.35
232.37
-5321.83
48568.31
-231639.30
640042.26
-1061800.52
1042400.18
-557682.99
125201.43

1.1. example: polynomial curve fitting

9

t

1

0

   1

n = 15

t

1

0

   1

n = 100

0

x

1

0

x

1

figure 1.6 plots of the solutions obtained by minimizing the sum-of-squares error function using the m = 9
polynomial for n = 15 data points (left plot) and n = 100 data points (right plot). we see that increasing the
size of the data set reduces the over-   tting problem.

ing polynomial function matches each of the data points exactly, but between data
points (particularly near the ends of the range) the function exhibits the large oscilla-
tions observed in figure 1.4. intuitively, what is happening is that the more    exible
polynomials with larger values of m are becoming increasingly tuned to the random
noise on the target values.

it is also interesting to examine the behaviour of a given model as the size of the
data set is varied, as shown in figure 1.6. we see that, for a given model complexity,
the over-   tting problem become less severe as the size of the data set increases.
another way to say this is that the larger the data set, the more complex (in other
words more    exible) the model that we can afford to    t to the data. one rough
heuristic that is sometimes advocated is that the number of data points should be
no less than some multiple (say 5 or 10) of the number of adaptive parameters in
the model. however, as we shall see in chapter 3, the number of parameters is not
necessarily the most appropriate measure of model complexity.

also, there is something rather unsatisfying about having to limit the number of
parameters in a model according to the size of the available training set. it would
seem more reasonable to choose the complexity of the model according to the com-
plexity of the problem being solved. we shall see that the least squares approach
to    nding the model parameters represents a speci   c case of maximum likelihood
(discussed in section 1.2.5), and that the over-   tting problem can be understood as
a general property of maximum likelihood. by adopting a bayesian approach, the
over-   tting problem can be avoided. we shall see that there is no dif   culty from
a bayesian perspective in employing models for which the number of parameters
greatly exceeds the number of data points. indeed, in a bayesian model the effective
number of parameters adapts automatically to the size of the data set.

for the moment, however, it is instructive to continue with the current approach
and to consider how in practice we can apply it to data sets of limited size where we

section 3.4

10

1. introduction

t

1

0

   1

ln    =    18

t

1

0

   1

ln    = 0

0

x

1

0

x

1

figure 1.7 plots of m = 9 polynomials    tted to the data set shown in figure 1.2 using the regularized error
function (1.4) for two values of the id173 parameter    corresponding to ln    =    18 and ln    = 0. the
case of no regularizer, i.e.,    = 0, corresponding to ln    =       , is shown at the bottom right of figure 1.4.

n(cid:2)

(cid:4)e(w) =

may wish to use relatively complex and    exible models. one technique that is often
used to control the over-   tting phenomenon in such cases is that of id173,
which involves adding a penalty term to the error function (1.2) in order to discourage
the coef   cients from reaching large values. the simplest such penalty term takes the
form of a sum of squares of all of the coef   cients, leading to a modi   ed error function
of the form

1
2

{y(xn, w)     tn}2 +   
2

(cid:6)w(cid:6)2

(1.4)

n=1
0 + w2
1 + . . . + w2

where (cid:6)w(cid:6)2     wtw = w2
m , and the coef   cient    governs the rel-
ative importance of the id173 term compared with the sum-of-squares error
term. note that often the coef   cient w0 is omitted from the regularizer because its
inclusion causes the results to depend on the choice of origin for the target variable
(hastie et al., 2001), or it may be included but with its own id173 coef   cient
(we shall discuss this topic in more detail in section 5.5.1). again, the error function
in (1.4) can be minimized exactly in closed form. techniques such as this are known
in the statistics literature as shrinkage methods because they reduce the value of the
coef   cients. the particular case of a quadratic regularizer is called ridge regres-
sion (hoerl and kennard, 1970). in the context of neural networks, this approach is
known as weight decay.

figure 1.7 shows the results of    tting the polynomial of order m = 9 to the
same data set as before but now using the regularized error function given by (1.4).
we see that, for a value of ln    =    18, the over-   tting has been suppressed and we
now obtain a much closer representation of the underlying function sin(2  x). if,
however, we use too large a value for    then we again obtain a poor    t, as shown in
figure 1.7 for ln    = 0. the corresponding coef   cients from the    tted polynomials
are given in table 1.2, showing that id173 has the desired effect of reducing

exercise 1.2

1.1. example: polynomial curve fitting

11

table 1.2 table of the coef   cients w(cid:1) for m =
9 polynomials with various values for
the id173 parameter   . note
that ln    =        corresponds to a
model with no id173, i.e., to
the graph at the bottom right in fig-
ure 1.4. we see that, as the value of
   increases, the typical magnitude of
the coef   cients gets smaller.

ln    =        ln    =    18
0.35
4.74
-0.77
-31.97
-3.89
55.28
41.32
-45.95
-91.53
72.68

0.35
232.37
-5321.83
48568.31
-231639.30
640042.26
-1061800.52
1042400.18
-557682.99
125201.43

ln    = 0
0.13
-0.05
-0.06
-0.05
-0.03
-0.02
-0.01
-0.00
0.00
0.01

w(cid:1)
0
w(cid:1)
1
w(cid:1)
2
w(cid:1)
3
w(cid:1)
4
w(cid:1)
5
w(cid:1)
6
w(cid:1)
7
w(cid:1)
8
w(cid:1)
9

the magnitude of the coef   cients.

the impact of the id173 term on the generalization error can be seen by
plotting the value of the rms error (1.3) for both training and test sets against ln   ,
as shown in figure 1.8. we see that in effect    now controls the effective complexity
of the model and hence determines the degree of over-   tting.

the issue of model complexity is an important one and will be discussed at
length in section 1.3. here we simply note that, if we were trying to solve a practical
application using this approach of minimizing an error function, we would have to
   nd a way to determine a suitable value for the model complexity. the results above
suggest a simple way of achieving this, namely by taking the available data and
partitioning it into a training set, used to determine the coef   cients w, and a separate
validation set, also called a hold-out set, used to optimize the model complexity
(either m or   ).
in many cases, however, this will prove to be too wasteful of
valuable training data, and we have to seek more sophisticated approaches.

so far our discussion of polynomial curve    tting has appealed largely to in-
tuition. we now seek a more principled approach to solving problems in pattern
recognition by turning to a discussion of id203 theory. as well as providing the
foundation for nearly all of the subsequent developments in this book, it will also

section 1.3

figure 1.8 graph of the root-mean-square er-
ror (1.3) versus ln    for the m = 9
polynomial.

1

training
test

s
m
r
e

0.5

0

   35

   30

ln   

   25

   20

12

1. introduction

give us some important insights into the concepts we have introduced in the con-
text of polynomial curve    tting and will allow us to extend these to more complex
situations.

1.2. id203 theory

a key concept in the    eld of pattern recognition is that of uncertainty. it arises both
through noise on measurements, as well as through the    nite size of data sets. prob-
ability theory provides a consistent framework for the quanti   cation and manipula-
tion of uncertainty and forms one of the central foundations for pattern recognition.
when combined with decision theory, discussed in section 1.5, it allows us to make
optimal predictions given all the information available to us, even though that infor-
mation may be incomplete or ambiguous.

we will introduce the basic concepts of id203 theory by considering a sim-
ple example. imagine we have two boxes, one red and one blue, and in the red box
we have 2 apples and 6 oranges, and in the blue box we have 3 apples and 1 orange.
this is illustrated in figure 1.9. now suppose we randomly pick one of the boxes
and from that box we randomly select an item of fruit, and having observed which
sort of fruit it is we replace it in the box from which it came. we could imagine
repeating this process many times. let us suppose that in so doing we pick the red
box 40% of the time and we pick the blue box 60% of the time, and that when we
remove an item of fruit from a box we are equally likely to select any of the pieces
of fruit in the box.

in this example, the identity of the box that will be chosen is a random variable,
which we shall denote by b. this random variable can take one of two possible
values, namely r (corresponding to the red box) or b (corresponding to the blue
box). similarly, the identity of the fruit is also a random variable and will be denoted
by f . it can take either of the values a (for apple) or o (for orange).

to begin with, we shall de   ne the id203 of an event to be the fraction
of times that event occurs out of the total number of trials, in the limit that the total
number of trials goes to in   nity. thus the id203 of selecting the red box is 4/10

figure 1.9 we use a simple example of two
coloured boxes each containing fruit
(apples shown in green and or-
anges shown in orange) to intro-
duce the basic ideas of id203.

1.2. id203 theory

13

figure 1.10 we can derive the sum and product rules of id203 by
considering two random variables, x, which takes the values {xi} where
i = 1, . . . , m, and y , which takes the values {yj} where j = 1, . . . , l.
in this illustration we have m = 5 and l = 3.
if we consider a total
number n of instances of these variables, then we denote the number
of instances where x = xi and y = yj by nij, which is the number of
points in the corresponding cell of the array. the number of points in
column i, corresponding to x = xi, is denoted by ci, and the number of
points in row j, corresponding to y = yj, is denoted by rj.

yj

ci
}

nij

xi

}

rj

and the id203 of selecting the blue box is 6/10. we write these probabilities
as p(b = r) = 4/10 and p(b = b) = 6/10. note that, by de   nition, probabilities
must lie in the interval [0, 1]. also, if the events are mutually exclusive and if they
include all possible outcomes (for instance, in this example the box must be either
red or blue), then we see that the probabilities for those events must sum to one.

we can now ask questions such as:    what is the overall id203 that the se-
lection procedure will pick an apple?   , or    given that we have chosen an orange,
what is the id203 that the box we chose was the blue one?   . we can answer
questions such as these, and indeed much more complex questions associated with
problems in pattern recognition, once we have equipped ourselves with the two el-
ementary rules of id203, known as the sum rule and the product rule. having
obtained these rules, we shall then return to our boxes of fruit example.

in order to derive the rules of id203, consider the slightly more general ex-
ample shown in figure 1.10 involving two random variables x and y (which could
for instance be the box and fruit variables considered above). we shall suppose that
x can take any of the values xi where i = 1, . . . , m, and y can take the values yj
where j = 1, . . . , l. consider a total of n trials in which we sample both of the
variables x and y , and let the number of such trials in which x = xi and y = yj
be nij. also, let the number of trials in which x takes the value xi (irrespective
of the value that y takes) be denoted by ci, and similarly let the number of trials in
which y takes the value yj be denoted by rj.

the id203 that x will take the value xi and y will take the value yj is
written p(x = xi, y = yj) and is called the joint id203 of x = xi and
y = yj. it is given by the number of points falling in the cell i,j as a fraction of the
total number of points, and hence

p(x = xi, y = yj) = nij
n

(1.5)
here we are implicitly considering the limit n        . similarly, the id203 that
x takes the value xi irrespective of the value of y is written as p(x = xi) and is
given by the fraction of the total number of points that fall in column i, so that

.

because the number of instances in column i in figure 1.10 is just the sum of the
number of instances in each cell of that column, we have ci =
j nij and therefore,

p(x = xi) = ci
n

.

(cid:5)

(1.6)

14

1. introduction

from (1.5) and (1.6), we have

p(x = xi) =

l(cid:2)

j=1

p(x = xi, y = yj)

(1.7)

which is the sum rule of id203. note that p(x = xi) is sometimes called the
marginal id203, because it is obtained by marginalizing, or summing out, the
other variables (in this case y ).
if we consider only those instances for which x = xi, then the fraction of
such instances for which y = yj is written p(y = yj|x = xi) and is called the
id155 of y = yj given x = xi.
it is obtained by    nding the
fraction of those points in column i that fall in cell i,j and hence is given by

p(y = yj|x = xi) = nij
ci

.

from (1.5), (1.6), and (1.8), we can then derive the following relationship

p(x = xi, y = yj) = nij
n

= nij
ci

   ci
n

= p(y = yj|x = xi)p(x = xi)

(1.8)

(1.9)

which is the product rule of id203.

so far we have been quite careful to make a distinction between a random vari-
able, such as the box b in the fruit example, and the values that the random variable
can take, for example r if the box were the red one. thus the id203 that b takes
the value r is denoted p(b = r). although this helps to avoid ambiguity, it leads
to a rather cumbersome notation, and in many cases there will be no need for such
pedantry. instead, we may simply write p(b) to denote a distribution over the ran-
dom variable b, or p(r) to denote the distribution evaluated for the particular value
r, provided that the interpretation is clear from the context.

with this more compact notation, we can write the two fundamental rules of

id203 theory in the following form.

the rules of id203

sum rule

p(x) =

(cid:2)

y

p(x, y )

product rule

p(x, y ) = p(y |x)p(x).

(1.10)

(1.11)

here p(x, y ) is a joint id203 and is verbalized as    the id203 of x and
y    . similarly, the quantity p(y |x) is a id155 and is verbalized as
   the id203 of y given x   , whereas the quantity p(x) is a marginal id203

1.2. id203 theory

15

and is simply    the id203 of x   . these two simple rules form the basis for all
of the probabilistic machinery that we use throughout this book.

from the product rule, together with the symmetry property p(x, y ) = p(y, x),
we immediately obtain the following relationship between conditional probabilities

p(y |x) = p(x|y )p(y )

p(x)

(1.12)

which is called bayes    theorem and which plays a central role in pattern recognition
and machine learning. using the sum rule, the denominator in bayes    theorem can
be expressed in terms of the quantities appearing in the numerator

(cid:2)

p(x) =

p(x|y )p(y ).

(1.13)

y

we can view the denominator in bayes    theorem as being the id172 constant
required to ensure that the sum of the id155 on the left-hand side of
(1.12) over all values of y equals one.

in figure 1.11, we show a simple example involving a joint distribution over two
variables to illustrate the concept of marginal and conditional distributions. here
a    nite sample of n = 60 data points has been drawn from the joint distribution
and is shown in the top left. in the top right is a histogram of the fractions of data
points having each of the two values of y . from the de   nition of id203, these
fractions would equal the corresponding probabilities p(y ) in the limit n        . we
can view the histogram as a simple way to model a id203 distribution given only
a    nite number of points drawn from that distribution. modelling distributions from
data lies at the heart of statistical pattern recognition and will be explored in great
detail in this book. the remaining two plots in figure 1.11 show the corresponding
histogram estimates of p(x) and p(x|y = 1).

let us now return to our example involving boxes of fruit. for the moment, we
shall once again be explicit about distinguishing between the random variables and
their instantiations. we have seen that the probabilities of selecting either the red or
the blue boxes are given by

p(b = r) = 4/10
p(b = b) = 6/10

(1.14)
(1.15)

respectively. note that these satisfy p(b = r) + p(b = b) = 1.

now suppose that we pick a box at random, and it turns out to be the blue box.
then the id203 of selecting an apple is just the fraction of apples in the blue
box which is 3/4, and so p(f = a|b = b) = 3/4. in fact, we can write out all four
conditional probabilities for the type of fruit, given the selected box

p(f = a|b = r) = 1/4
p(f = o|b = r) = 3/4
p(f = a|b = b) = 3/4
p(f = o|b = b) = 1/4.

(1.16)
(1.17)
(1.18)
(1.19)

16

1. introduction

p(x, y )

p(y )

y = 2

y = 1

x

p(x)

p(x|y = 1)

x

x

figure 1.11 an illustration of a distribution over two variables, x, which takes 9 possible values, and y , which
takes two possible values. the top left    gure shows a sample of 60 points drawn from a joint id203 distri-
bution over these variables. the remaining    gures show histogram estimates of the marginal distributions p(x)
and p(y ), as well as the conditional distribution p(x|y = 1) corresponding to the bottom row in the top left
   gure.

again, note that these probabilities are normalized so that

p(f = a|b = r) + p(f = o|b = r) = 1

and similarly

p(f = a|b = b) + p(f = o|b = b) = 1.

(1.20)

(1.21)

we can now use the sum and product rules of id203 to evaluate the overall

id203 of choosing an apple

p(f = a) = p(f = a|b = r)p(b = r) + p(f = a|b = b)p(b = b)

=

1
4

   4
10

+

3
4

   6
10

=

11
20

(1.22)

from which it follows, using the sum rule, that p(f = o) = 1     11/20 = 9/20.

1.2. id203 theory

17

suppose instead we are told that a piece of fruit has been selected and it is an
orange, and we would like to know which box it came from. this requires that
we evaluate the id203 distribution over boxes conditioned on the identity of
the fruit, whereas the probabilities in (1.16)   (1.19) give the id203 distribution
over the fruit conditioned on the identity of the box. we can solve the problem of
reversing the id155 by using bayes    theorem to give
   20
9

2
3 .
from the sum rule, it then follows that p(b = b|f = o) = 1     2/3 = 1/3.

p(b = r|f = o) = p(f = o|b = r)p(b = r)

   4
10

p(f = o)

(1.23)

3
4

=

=

we can provide an important interpretation of bayes    theorem as follows. if
we had been asked which box had been chosen before being told the identity of
the selected item of fruit, then the most complete information we have available is
provided by the id203 p(b). we call this the prior id203 because it is the
id203 available before we observe the identity of the fruit. once we are told that
the fruit is an orange, we can then use bayes    theorem to compute the id203
p(b|f ), which we shall call the posterior id203 because it is the id203
obtained after we have observed f . note that in this example, the prior id203
of selecting the red box was 4/10, so that we were more likely to select the blue box
than the red one. however, once we have observed that the piece of selected fruit is
an orange, we    nd that the posterior id203 of the red box is now 2/3, so that
it is now more likely that the box we selected was in fact the red one. this result
accords with our intuition, as the proportion of oranges is much higher in the red box
than it is in the blue box, and so the observation that the fruit was an orange provides
signi   cant evidence favouring the red box. in fact, the evidence is suf   ciently strong
that it outweighs the prior and makes it more likely that the red box was chosen
rather than the blue one.

finally, we note that if the joint distribution of two variables factorizes into the
product of the marginals, so that p(x, y ) = p(x)p(y ), then x and y are said to
be independent. from the product rule, we see that p(y |x) = p(y ), and so the
conditional distribution of y given x is indeed independent of the value of x. for
instance, in our boxes of fruit example, if each box contained the same fraction of
apples and oranges, then p(f|b) = p (f ), so that the id203 of selecting, say,
an apple is independent of which box is chosen.

1.2.1 id203 densities
as well as considering probabilities de   ned over discrete sets of events, we
also wish to consider probabilities with respect to continuous variables. we shall
limit ourselves to a relatively informal discussion. if the id203 of a real-valued
variable x falling in the interval (x, x +   x) is given by p(x)  x for   x     0, then
p(x) is called the id203 density over x. this is illustrated in figure 1.12. the
id203 that x will lie in an interval (a, b) is then given by

(cid:6) b

p(x     (a, b)) =

p(x) dx.

a

(1.24)

18

1. introduction

figure 1.12 the concept of id203 for
discrete variables can be ex-
tended to that of a id203
density p(x) over a continuous
variable x and is such that the
id203 of x lying in the inter-
val (x, x +   x) is given by p(x)  x
for   x     0. the id203
density can be expressed as the
derivative of a cumulative distri-
bution function p (x).

p(x)

p (x)

  x

x

because probabilities are nonnegative, and because the value of x must lie some-
where on the real axis, the id203 density p(x) must satisfy the two conditions

(cid:6)    

p(x) (cid:2) 0

p(x) dx = 1.

      

(1.25)

(1.26)

under a nonlinear change of variable, a id203 density transforms differently
from a simple function, due to the jacobian factor. for instance, if we consider

a change of variables x = g(y), then a function f(x) becomes(cid:4)f(y) = f(g(y)).

now consider a id203 density px(x) that corresponds to a density py(y) with
respect to the new variable y, where the suf   ces denote the fact that px(x) and py(y)
are different densities. observations falling in the range (x, x +   x) will, for small
values of   x, be transformed into the range (y, y +   y) where px(x)  x (cid:8) py(y)  y,
and hence

(cid:7)(cid:7)(cid:7)(cid:7) dx

(cid:7)(cid:7)(cid:7)(cid:7)

py(y) = px(x)

dy
= px(g(y))|g

(cid:2)(y)| .

(1.27)

exercise 1.4

one consequence of this property is that the concept of the maximum of a id203
density is dependent on the choice of variable.
the id203 that x lies in the interval (      , z) is given by the cumulative

distribution function de   ned by

(cid:6) z

p (z) =

p(x) dx

      

(1.28)

which satis   es p

(cid:2)(x) = p(x), as shown in figure 1.12.

if we have several continuous variables x1, . . . , xd, denoted collectively by the
vector x, then we can de   ne a joint id203 density p(x) = p(x1, . . . , xd) such

1.2. id203 theory

19

that the id203 of x falling in an in   nitesimal volume   x containing the point x
is given by p(x)  x. this multivariate id203 density must satisfy

(cid:6)

p(x) (cid:2) 0
p(x) dx = 1

(1.29)

(1.30)

in which the integral is taken over the whole of x space. we can also consider joint
id203 distributions over a combination of discrete and continuous variables.

note that if x is a discrete variable, then p(x) is sometimes called a id203
mass function because it can be regarded as a set of    id203 masses    concentrated
at the allowed values of x.

the sum and product rules of id203, as well as bayes    theorem, apply
equally to the case of id203 densities, or to combinations of discrete and con-
tinuous variables. for instance, if x and y are two real variables, then the sum and
product rules take the form

(cid:6)

p(x) =

p(x, y) dy
p(x, y) = p(y|x)p(x).

(1.31)

(1.32)

a formal justi   cation of the sum and product rules for continuous variables (feller,
1966) requires a branch of mathematics called measure theory and lies outside the
scope of this book. its validity can be seen informally, however, by dividing each
real variable into intervals of width     and considering the discrete id203 dis-
tribution over these intervals. taking the limit         0 then turns sums into integrals
and gives the desired result.

1.2.2 expectations and covariances
one of the most important operations involving probabilities is that of    nding
weighted averages of functions. the average value of some function f(x) under a
id203 distribution p(x) is called the expectation of f(x) and will be denoted by
e[f]. for a discrete distribution, it is given by

(cid:2)

e[f] =

p(x)f(x)

(1.33)

x

so that the average is weighted by the relative probabilities of the different values
of x. in the case of continuous variables, expectations are expressed in terms of an
integration with respect to the corresponding id203 density

(cid:6)

e[f] =

p(x)f(x) dx.

(1.34)

in either case, if we are given a    nite number n of points drawn from the id203
distribution or id203 density, then the expectation can be approximated as a

20

1. introduction

   nite sum over these points

n(cid:2)

n=1

e[f] (cid:8) 1
n

f(xn).

(1.35)

we shall make extensive use of this result when we discuss sampling methods in
chapter 11. the approximation in (1.35) becomes exact in the limit n        .

sometimes we will be considering expectations of functions of several variables,
in which case we can use a subscript to indicate which variable is being averaged
over, so that for instance

(1.36)
denotes the average of the function f(x, y) with respect to the distribution of x. note
that ex[f(x, y)] will be a function of y.

ex[f(x, y)]

we can also consider a conditional expectation with respect to a conditional

distribution, so that

ex[f|y] =

p(x|y)f(x)

(cid:2)
(cid:8)
(f(x)     e[f(x)])2(cid:9)

x

with an analogous de   nition for continuous variables.

the variance of f(x) is de   ned by

var[f] = e

(1.37)

(1.38)

exercise 1.5

exercise 1.6

and provides a measure of how much variability there is in f(x) around its mean
value e[f(x)]. expanding out the square, we see that the variance can also be written
in terms of the expectations of f(x) and f(x)2

var[f] = e[f(x)2]     e[f(x)]2.

(1.39)

in particular, we can consider the variance of the variable x itself, which is given by

var[x] = e[x2]     e[x]2.

for two random variables x and y, the covariance is de   ned by

cov[x, y] = ex,y [{x     e[x]}{y     e[y]}]

= ex,y[xy]     e[x]e[y]

(1.40)

(1.41)

which expresses the extent to which x and y vary together. if x and y are indepen-
dent, then their covariance vanishes.

in the case of two vectors of random variables x and y, the covariance is a matrix

(cid:8){x     e[x]}{yt     e[yt]}(cid:9)

cov[x, y] = ex,y

= ex,y[xyt]     e[x]e[yt].

(1.42)

if we consider the covariance of the components of a vector x with each other, then
we use a slightly simpler notation cov[x]     cov[x, x].

1.2. id203 theory

21

1.2.3 bayesian probabilities
so far in this chapter, we have viewed probabilities in terms of the frequencies
of random, repeatable events. we shall refer to this as the classical or frequentist
interpretation of id203. now we turn to the more general bayesian view, in
which probabilities provide a quanti   cation of uncertainty.

consider an uncertain event, for example whether the moon was once in its own
orbit around the sun, or whether the arctic ice cap will have disappeared by the end
of the century. these are not events that can be repeated numerous times in order
to de   ne a notion of id203 as we did earlier in the context of boxes of fruit.
nevertheless, we will generally have some idea, for example, of how quickly we
think the polar ice is melting. if we now obtain fresh evidence, for instance from a
new earth observation satellite gathering novel forms of diagnostic information, we
may revise our opinion on the rate of ice loss. our assessment of such matters will
affect the actions we take, for instance the extent to which we endeavour to reduce
the emission of greenhouse gasses. in such circumstances, we would like to be able
to quantify our expression of uncertainty and make precise revisions of uncertainty in
the light of new evidence, as well as subsequently to be able to take optimal actions
or decisions as a consequence. this can all be achieved through the elegant, and very
general, bayesian interpretation of id203.

the use of id203 to represent uncertainty, however, is not an ad-hoc choice,
but is inevitable if we are to respect common sense while making rational coherent
id136s. for instance, cox (1946) showed that if numerical values are used to
represent degrees of belief, then a simple set of axioms encoding common sense
properties of such beliefs leads uniquely to a set of rules for manipulating degrees of
belief that are equivalent to the sum and product rules of id203. this provided
the    rst rigorous proof that id203 theory could be regarded as an extension of
boolean logic to situations involving uncertainty (jaynes, 2003). numerous other
authors have proposed different sets of properties or axioms that such measures of
uncertainty should satisfy (ramsey, 1931; good, 1950; savage, 1961; definetti,
1970; lindley, 1982). in each case, the resulting numerical quantities behave pre-
cisely according to the rules of id203. it is therefore natural to refer to these
quantities as (bayesian) probabilities.

in the    eld of pattern recognition, too, it is helpful to have a more general no-

thomas bayes
1701   1761

thomas bayes was born in tun-
bridge wells and was a clergyman
as well as an amateur scientist and
a mathematician. he studied logic
and theology at edinburgh univer-
sity and was elected fellow of the
royal society in 1742. during the 18th century, is-
sues regarding id203 arose in connection with

gambling and with the new concept of insurance. one
particularly important problem concerned so-called in-
verse id203. a solution was proposed by thomas
bayes in his paper    essay towards solving a problem
in the doctrine of chances   , which was published in
1764, some three years after his death, in the philo-
sophical transactions of the royal society.
in fact,
bayes only formulated his theory for the case of a uni-
form prior, and it was pierre-simon laplace who inde-
pendently rediscovered the theory in general form and
who demonstrated its broad applicability.

22

1. introduction

tion of id203. consider the example of polynomial curve    tting discussed in
section 1.1. it seems reasonable to apply the frequentist notion of id203 to the
random values of the observed variables tn. however, we would like to address and
quantify the uncertainty that surrounds the appropriate choice for the model param-
eters w. we shall see that, from a bayesian perspective, we can use the machinery
of id203 theory to describe the uncertainty in model parameters such as w, or
indeed in the choice of model itself.

bayes    theorem now acquires a new signi   cance. recall that in the boxes of fruit
example, the observation of the identity of the fruit provided relevant information
that altered the id203 that the chosen box was the red one. in that example,
bayes    theorem was used to convert a prior id203 into a posterior id203
by incorporating the evidence provided by the observed data. as we shall see in
detail later, we can adopt a similar approach when making id136s about quantities
such as the parameters w in the polynomial curve    tting example. we capture our
assumptions about w, before observing the data, in the form of a prior id203
distribution p(w). the effect of the observed data d = {t1, . . . , tn} is expressed
through the id155 p(d|w), and we shall see later, in section 1.2.5,
how this can be represented explicitly. bayes    theorem, which takes the form

p(w|d) = p(d|w)p(w)

p(d)

(1.43)
then allows us to evaluate the uncertainty in w after we have observed d in the form
of the posterior id203 p(w|d).
the quantity p(d|w) on the right-hand side of bayes    theorem is evaluated for
the observed data set d and can be viewed as a function of the parameter vector
w, in which case it is called the likelihood function. it expresses how probable the
observed data set is for different settings of the parameter vector w. note that the
likelihood is not a id203 distribution over w, and its integral with respect to w
does not (necessarily) equal one.

given this de   nition of likelihood, we can state bayes    theorem in words

posterior     likelihood    prior

(1.44)

(cid:6)

where all of these quantities are viewed as functions of w. the denominator in
(1.43) is the id172 constant, which ensures that the posterior distribution
on the left-hand side is a valid id203 density and integrates to one. indeed,
integrating both sides of (1.43) with respect to w, we can express the denominator
in bayes    theorem in terms of the prior distribution and the likelihood function

p(d) =

p(d|w)p(w) dw.

(1.45)
in both the bayesian and frequentist paradigms, the likelihood function p(d|w)
plays a central role. however, the manner in which it is used is fundamentally dif-
ferent in the two approaches. in a frequentist setting, w is considered to be a    xed
parameter, whose value is determined by some form of    estimator   , and error bars

1.2. id203 theory

23

on this estimate are obtained by considering the distribution of possible data sets d.
by contrast, from the bayesian viewpoint there is only a single data set d (namely
the one that is actually observed), and the uncertainty in the parameters is expressed
through a id203 distribution over w.
a widely used frequentist estimator is maximum likelihood, in which w is set
to the value that maximizes the likelihood function p(d|w). this corresponds to
choosing the value of w for which the id203 of the observed data set is maxi-
mized. in the machine learning literature, the negative log of the likelihood function
is called an error function. because the negative logarithm is a monotonically de-
creasing function, maximizing the likelihood is equivalent to minimizing the error.
one approach to determining frequentist error bars is the bootstrap (efron, 1979;
hastie et al., 2001), in which multiple data sets are created as follows. suppose our
original data set consists of n data points x = {x1, . . . , xn}. we can create a new
data set xb by drawing n points at random from x, with replacement, so that some
points in x may be replicated in xb, whereas other points in x may be absent from
xb. this process can be repeated l times to generate l data sets each of size n and
each obtained by sampling from the original data set x. the statistical accuracy of
parameter estimates can then be evaluated by looking at the variability of predictions
between the different bootstrap data sets.

one advantage of the bayesian viewpoint is that the inclusion of prior knowl-
edge arises naturally. suppose, for instance, that a fair-looking coin is tossed three
times and lands heads each time. a classical maximum likelihood estimate of the
id203 of landing heads would give 1, implying that all future tosses will land
heads! by contrast, a bayesian approach with any reasonable prior will lead to a
much less extreme conclusion.

there has been much controversy and debate associated with the relative mer-
its of the frequentist and bayesian paradigms, which have not been helped by the
fact that there is no unique frequentist, or even bayesian, viewpoint. for instance,
one common criticism of the bayesian approach is that the prior distribution is of-
ten selected on the basis of mathematical convenience rather than as a re   ection of
any prior beliefs. even the subjective nature of the conclusions through their de-
pendence on the choice of prior is seen by some as a source of dif   culty. reducing
the dependence on the prior is one motivation for so-called noninformative priors.
however, these lead to dif   culties when comparing different models, and indeed
bayesian methods based on poor choices of prior can give poor results with high
con   dence. frequentist evaluation methods offer some protection from such prob-
lems, and techniques such as cross-validation remain useful in areas such as model
comparison.

this book places a strong emphasis on the bayesian viewpoint, re   ecting the
huge growth in the practical importance of bayesian methods in the past few years,
while also discussing useful frequentist concepts as required.

although the bayesian framework has its origins in the 18th century, the prac-
tical application of bayesian methods was for a long time severely limited by the
dif   culties in carrying through the full bayesian procedure, particularly the need to
marginalize (sum or integrate) over the whole of parameter space, which, as we shall

section 2.1

section 2.4.3

section 1.3

24

1. introduction

see, is required in order to make predictions or to compare different models. the
development of sampling methods, such as id115 (discussed in
chapter 11) along with dramatic improvements in the speed and memory capacity
of computers, opened the door to the practical use of bayesian techniques in an im-
pressive range of problem domains. monte carlo methods are very    exible and can
be applied to a wide range of models. however, they are computationally intensive
and have mainly been used for small-scale problems.

more recently, highly ef   cient deterministic approximation schemes such as
id58 and expectation propagation (discussed in chapter 10) have been
developed. these offer a complementary alternative to sampling methods and have
allowed bayesian techniques to be used in large-scale applications (blei et al., 2003).

1.2.4 the gaussian distribution
we shall devote the whole of chapter 2 to a study of various id203 dis-
tributions and their key properties. it is convenient, however, to introduce here one
of the most important id203 distributions for continuous variables, called the
normal or gaussian distribution. we shall make extensive use of this distribution in
the remainder of this chapter and indeed throughout much of the book.

for the case of a single real-valued variable x, the gaussian distribution is de-

(cid:12)

(cid:13)

   ned by

x|  ,   2

=

1

(2    2)1/2 exp

    1
2  2 (x       )2

(1.46)

n(cid:10)

(cid:11)

which is governed by two parameters:   , called the mean, and   2, called the vari-
ance. the square root of the variance, given by   , is called the standard deviation,
and the reciprocal of the variance, written as    = 1/  2, is called the precision. we
shall see the motivation for these terms shortly. figure 1.13 shows a plot of the
gaussian distribution.

from the form of (1.46) we see that the gaussian distribution satis   es

n (x|  ,   2) > 0.

(1.47)

exercise 1.7

also it is straightforward to show that the gaussian is normalized, so that

pierre-simon laplace
1749   1827

it
is said that laplace was seri-
ously lacking in modesty and at one
point declared himself
to be the
best mathematician in france at the
time, a claim that was arguably true.
as well as being proli   c in mathe-
matics, he also made numerous contributions to as-
tronomy, including the nebular hypothesis by which the

earth is thought to have formed from the condensa-
tion and cooling of a large rotating disk of gas and
dust. in 1812 he published the    rst edition of th  eorie
analytique des probabilit  es, in which laplace states
that    id203 theory is nothing but common sense
reduced to calculation   . this work included a discus-
sion of the inverse id203 calculation (later termed
bayes    theorem by poincar  e), which he used to solve
problems in life expectancy, jurisprudence, planetary
masses, triangulation, and error estimation.

figure 1.13 plot of the univariate gaussian
showing the mean    and the
standard deviation   .

n (x|  ,   2)

1.2. id203 theory

25

2  

  

dx = 1.

(cid:6)    

      

n(cid:10)

x|  ,   2

(cid:11)

x

(1.48)

thus (1.46) satis   es the two requirements for a valid id203 density.

we can readily    nd expectations of functions of x under the gaussian distribu-

tion. in particular, the average value of x is given by

(cid:11)

x|  ,   2

(cid:6)    

      

n(cid:10)
n(cid:10)

(cid:6)    

(cid:11)

x|  ,   2

e[x] =

x dx =   .

(1.49)

because the parameter    represents the average value of x under the distribution, it
is referred to as the mean. similarly, for the second order moment

e[x2] =

      

x2 dx =   2 +   2.

(1.50)

from (1.49) and (1.50), it follows that the variance of x is given by

var[x] = e[x2]     e[x]2 =   2

(1.51)

(cid:12)

and hence   2 is referred to as the variance parameter. the maximum of a distribution
is known as its mode. for a gaussian, the mode coincides with the mean.

we are also interested in the gaussian distribution de   ned over a d-dimensional

vector x of continuous variables, which is given by

1

1

n (x|  ,   ) =

(x       )t  

   1(x       )

|  |1/2 exp

(2  )d/2

(1.52)
where the d-dimensional vector    is called the mean, the d    d matrix    is called
the covariance, and |  | denotes the determinant of   . we shall make use of the
multivariate gaussian distribution brie   y in this chapter, although its properties will
be studied in detail in section 2.3.

   1
2

(cid:13)

exercise 1.8

exercise 1.9

26

1. introduction

figure 1.14 illustration of the likelihood function for
a gaussian distribution, shown by the
red curve. here the black points de-
note a data set of values {xn}, and
the likelihood function given by (1.53)
corresponds to the product of the blue
values. maximizing the likelihood in-
volves adjusting the mean and vari-
ance of the gaussian so as to maxi-
mize this product.

p(x)

n (xn|  ,   2)

xn

x

now suppose that we have a data set of observations x = (x1, . . . , xn )t, rep-
resenting n observations of the scalar variable x. note that we are using the type-
face x to distinguish this from a single observation of the vector-valued variable
(x1, . . . , xd)t, which we denote by x. we shall suppose that the observations are
drawn independently from a gaussian distribution whose mean    and variance   2
are unknown, and we would like to determine these parameters from the data set.
data points that are drawn independently from the same distribution are said to be
independent and identically distributed, which is often abbreviated to i.i.d. we have
seen that the joint id203 of two independent events is given by the product of
the marginal probabilities for each event separately. because our data set x is i.i.d.,
we can therefore write the id203 of the data set, given    and   2, in the form

n(cid:14)

n(cid:10)

(cid:11)

p(x|  ,   2) =

xn|  ,   2

.

(1.53)

section 1.2.5

n=1

when viewed as a function of    and   2, this is the likelihood function for the gaus-
sian and is interpreted diagrammatically in figure 1.14.

one common criterion for determining the parameters in a id203 distribu-
tion using an observed data set is to    nd the parameter values that maximize the
likelihood function. this might seem like a strange criterion because, from our fore-
going discussion of id203 theory, it would seem more natural to maximize the
id203 of the parameters given the data, not the id203 of the data given the
parameters. in fact, these two criteria are related, as we shall discuss in the context
of curve    tting.

for the moment, however, we shall determine values for the unknown parame-
ters    and   2 in the gaussian by maximizing the likelihood function (1.53). in prac-
tice, it is more convenient to maximize the log of the likelihood function. because
the logarithm is a monotonically increasing function of its argument, maximization
of the log of a function is equivalent to maximization of the function itself. taking
the log not only simpli   es the subsequent mathematical analysis, but it also helps
numerically because the product of a large number of small probabilities can easily
under   ow the numerical precision of the computer, and this is resolved by computing
instead the sum of the log probabilities. from (1.46) and (1.53), the log likelihood

exercise 1.11

section 1.1

exercise 1.12

function can be written in the form

(cid:10)
x|  ,   2

(cid:11)

ln p

=     1
2  2

1.2. id203 theory

27

(xn       )2     n
2

ln   2     n
2

ln(2  ).

(1.54)

n(cid:2)

n=1

maximizing (1.54) with respect to   , we obtain the maximum likelihood solution
given by

  ml =

(1.55)
which is the sample mean, i.e., the mean of the observed values {xn}. similarly,
maximizing (1.54) with respect to   2, we obtain the maximum likelihood solution
for the variance in the form

xn

n=1

  2
ml =

1
n

(xn       ml)2

(1.56)

n(cid:2)

1
n

n(cid:2)

n=1

which is the sample variance measured with respect to the sample mean   ml. note
that we are performing a joint maximization of (1.54) with respect to    and   2, but
in the case of the gaussian distribution the solution for    decouples from that for   2
so that we can    rst evaluate (1.55) and then subsequently use this result to evaluate
(1.56).

later in this chapter, and also in subsequent chapters, we shall highlight the sig-
ni   cant limitations of the maximum likelihood approach. here we give an indication
of the problem in the context of our solutions for the maximum likelihood param-
eter settings for the univariate gaussian distribution. in particular, we shall show
that the maximum likelihood approach systematically underestimates the variance
of the distribution. this is an example of a phenomenon called bias and is related
to the problem of over-   tting encountered in the context of polynomial curve    tting.
we    rst note that the maximum likelihood solutions   ml and   2
ml are functions of
the data set values x1, . . . , xn . consider the expectations of these quantities with
respect to the data set values, which themselves come from a gaussian distribution
with parameters    and   2. it is straightforward to show that

(cid:15)

(cid:16)

e[  ml] =   

e[  2

ml] =

n     1
n

  2

(1.57)

(1.58)

so that on average the maximum likelihood estimate will obtain the correct mean but
will underestimate the true variance by a factor (n     1)/n. the intuition behind
this result is given by figure 1.15.

from (1.58) it follows that the following estimate for the variance parameter is

unbiased

(cid:4)  2 = n

n     1   2

ml =

1

n     1

(xn       ml)2.

(1.59)

n(cid:2)

n=1

28

1. introduction

figure 1.15 illustration of how bias arises in using max-
imum likelihood to determine the variance
of a gaussian. the green curve shows
the true gaussian distribution from which
data is generated, and the three red curves
show the gaussian distributions obtained
by    tting to three data sets, each consist-
ing of two data points shown in blue, us-
ing the maximum likelihood results (1.55)
and (1.56). averaged across the three data
sets, the mean is correct, but the variance
is systematically under-estimated because
it is measured relative to the sample mean
and not relative to the true mean.

(a)

(b)

(c)

in section 10.1.3, we shall see how this result arises automatically when we adopt a
bayesian approach.
note that the bias of the maximum likelihood solution becomes less signi   cant
as the number n of data points increases, and in the limit n         the maximum
likelihood solution for the variance equals the true variance of the distribution that
generated the data. in practice, for anything other than small n, this bias will not
prove to be a serious problem. however, throughout this book we shall be interested
in more complex models with many parameters, for which the bias problems asso-
ciated with maximum likelihood will be much more severe. in fact, as we shall see,
the issue of bias in maximum likelihood lies at the root of the over-   tting problem
that we encountered earlier in the context of polynomial curve    tting.

1.2.5 curve    tting re-visited
we have seen how the problem of polynomial curve    tting can be expressed in
terms of error minimization. here we return to the curve    tting example and view it
from a probabilistic perspective, thereby gaining some insights into error functions
and id173, as well as taking us towards a full bayesian treatment.

section 1.1

the goal in the curve    tting problem is to be able to make predictions for the
target variable t given some new value of the input variable x on the basis of a set of
training data comprising n input values x = (x1, . . . , xn )t and their corresponding
target values t = (t1, . . . , tn )t. we can express our uncertainty over the value of
the target variable using a id203 distribution. for this purpose, we shall assume
that, given the value of x, the corresponding value of t has a gaussian distribution
with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). thus
we have

p(t|x, w,   ) = n(cid:10)

t|y(x, w),   

(cid:11)

(1.60)

   1

where, for consistency with the notation in later chapters, we have de   ned a preci-
sion parameter    corresponding to the inverse variance of the distribution. this is
illustrated schematically in figure 1.16.

figure 1.16 schematic illustration of a gaus-
sian conditional distribution for t given x given by
(1.60), in which the mean is given by the polyno-
mial function y(x, w), and the precision is given
by the parameter   , which is related to the vari-
ance by      1 =   2.

t

y(x0, w)

1.2. id203 theory

29

y(x, w)

p(t|x0, w,   )

x0

2  

x

we now use the training data {x, t} to determine the values of the unknown
parameters w and    by maximum likelihood. if the data are assumed to be drawn
independently from the distribution (1.60), then the likelihood function is given by

n(cid:14)

n(cid:10)

n=1

(cid:11)

p(t|x, w,   ) =

tn|y(xn, w),   

   1

.

(1.61)

as we did in the case of the simple gaussian distribution earlier, it is convenient to
maximize the logarithm of the likelihood function. substituting for the form of the
gaussian distribution, given by (1.46), we obtain the log likelihood function in the
form

ln p(t|x, w,   ) =       
2

{y(xn, w)     tn}2 + n
2

ln        n
2

ln(2  ).

(1.62)

n(cid:2)

n=1

consider    rst the determination of the maximum likelihood solution for the polyno-
mial coef   cients, which will be denoted by wml. these are determined by maxi-
mizing (1.62) with respect to w. for this purpose, we can omit the last two terms
on the right-hand side of (1.62) because they do not depend on w. also, we note
that scaling the log likelihood by a positive constant coef   cient does not alter the
location of the maximum with respect to w, and so we can replace the coef   cient
  /2 with 1/2. finally, instead of maximizing the log likelihood, we can equivalently
minimize the negative log likelihood. we therefore see that maximizing likelihood is
equivalent, so far as determining w is concerned, to minimizing the sum-of-squares
error function de   ned by (1.2). thus the sum-of-squares error function has arisen as
a consequence of maximizing likelihood under the assumption of a gaussian noise
distribution.

we can also use maximum likelihood to determine the precision parameter    of

the gaussian conditional distribution. maximizing (1.62) with respect to    gives

{y(xn, wml)     tn}2

.

(1.63)

n(cid:2)

n=1

1
  ml

=

1
n

30

1. introduction

section 1.2.4

p(t|x, wml,   ml) = n(cid:10)
(cid:17)

(cid:18)(m +1)/2

again we can    rst determine the parameter vector wml governing the mean and sub-
sequently use this to    nd the precision   ml as was the case for the simple gaussian
distribution.

having determined the parameters w and   , we can now make predictions for
new values of x. because we now have a probabilistic model, these are expressed
in terms of the predictive distribution that gives the id203 distribution over t,
rather than simply a point estimate, and is obtained by substituting the maximum
likelihood parameters into (1.60) to give

(cid:11)

t|y(x, wml),   

   1
ml

.

(1.64)

now let us take a step towards a more bayesian approach and introduce a prior
distribution over the polynomial coef   cients w. for simplicity, let us consider a
gaussian distribution of the form

(cid:20)

(cid:19)

      
2

exp

wtw

(1.65)

p(w|  ) = n (w|0,   

   1i) =

  
2  

where    is the precision of the distribution, and m +1 is the total number of elements
in the vector w for an m th order polynomial. variables such as   , which control
the distribution of model parameters, are called hyperparameters. using bayes   
theorem, the posterior distribution for w is proportional to the product of the prior
distribution and the likelihood function

p(w|x, t,   ,   )     p(t|x, w,   )p(w|  ).

(1.66)

we can now determine w by    nding the most probable value of w given the data,
in other words by maximizing the posterior distribution. this technique is called
maximum posterior, or simply map. taking the negative logarithm of (1.66) and
combining with (1.62) and (1.65), we    nd that the maximum of the posterior is
given by the minimum of

{y(xn, w)     tn}2 +   
2

wtw.

(1.67)

n(cid:2)

n=1

  
2

thus we see that maximizing the posterior distribution is equivalent to minimizing
the regularized sum-of-squares error function encountered earlier in the form (1.4),
with a id173 parameter given by    =   /  .

1.2.6 bayesian curve    tting
although we have included a prior distribution p(w|  ), we are so far still mak-
ing a point estimate of w and so this does not yet amount to a bayesian treatment. in
a fully bayesian approach, we should consistently apply the sum and product rules
of id203, which requires, as we shall see shortly, that we integrate over all val-
ues of w. such marginalizations lie at the heart of bayesian methods for pattern
recognition.

1.2. id203 theory

31

in the curve    tting problem, we are given the training data x and t, along with
a new test point x, and our goal is to predict the value of t. we therefore wish
to evaluate the predictive distribution p(t|x, x, t). here we shall assume that the
parameters    and    are    xed and known in advance (in later chapters we shall discuss
how such parameters can be inferred from data in a bayesian setting).

a bayesian treatment simply corresponds to a consistent application of the sum
and product rules of id203, which allow the predictive distribution to be written
in the form

(cid:6)

p(t|x, x, t) =

p(t|x, w)p(w|x, t) dw.

(1.68)
here p(t|x, w) is given by (1.60), and we have omitted the dependence on    and
   to simplify the notation. here p(w|x, t) is the posterior distribution over param-
eters, and can be found by normalizing the right-hand side of (1.66). we shall see
in section 3.3 that, for problems such as the curve-   tting example, this posterior
distribution is a gaussian and can be evaluated analytically. similarly, the integra-
tion in (1.68) can also be performed analytically with the result that the predictive
distribution is given by a gaussian of the form

p(t|x, x, t) = n(cid:10)

t|m(x), s2(x)

(cid:11)

n(cid:2)

where the mean and variance are given by

s2(x) =   

here the matrix s is given by

m(x) =     (x)ts

  (xn)tn
   1 +   (x)ts  (x).

n=1

n(cid:2)

n=1

  (xn)  (x)t

s   1 =   i +   

(1.69)

(1.70)

(1.71)

(1.72)

where i is the unit matrix, and we have de   ned the vector   (x) with elements
  i(x) = xi for i = 0, . . . , m.

we see that the variance, as well as the mean, of the predictive distribution in
(1.69) is dependent on x. the    rst term in (1.71) represents the uncertainty in the
predicted value of t due to the noise on the target variables and was expressed already
   1
in the maximum likelihood predictive distribution (1.64) through   
ml. however, the
second term arises from the uncertainty in the parameters w and is a consequence
of the bayesian treatment. the predictive distribution for the synthetic sinusoidal
regression problem is illustrated in figure 1.17.

32

1. introduction

figure 1.17 the predictive distribution result-
ing from a bayesian treatment of
polynomial curve    tting using an
m = 9 polynomial, with the    xed
parameters    = 5    10   3 and    =
11.1 (corresponding to the known
noise variance), in which the red
curve denotes the mean of
the
predictive distribution and the red
region corresponds to   1 stan-
dard deviation around the mean.

t

1

0

   1

0

x

1

1.3. model selection

in our example of polynomial curve    tting using least squares, we saw that there was
an optimal order of polynomial that gave the best generalization. the order of the
polynomial controls the number of free parameters in the model and thereby governs
the model complexity. with regularized least squares, the id173 coef   cient
   also controls the effective complexity of the model, whereas for more complex
models, such as mixture distributions or neural networks there may be multiple pa-
rameters governing complexity.
in a practical application, we need to determine
the values of such parameters, and the principal objective in doing so is usually to
achieve the best predictive performance on new data. furthermore, as well as    nd-
ing the appropriate values for complexity parameters within a given model, we may
wish to consider a range of different types of model in order to    nd the best one for
our particular application.

we have already seen that, in the maximum likelihood approach, the perfor-
mance on the training set is not a good indicator of predictive performance on un-
seen data due to the problem of over-   tting. if data is plentiful, then one approach is
simply to use some of the available data to train a range of models, or a given model
with a range of values for its complexity parameters, and then to compare them on
independent data, sometimes called a validation set, and select the one having the
best predictive performance. if the model design is iterated many times using a lim-
ited size data set, then some over-   tting to the validation data can occur and so it may
be necessary to keep aside a third test set on which the performance of the selected
model is    nally evaluated.

in many applications, however, the supply of data for training and testing will be
limited, and in order to build good models, we wish to use as much of the available
data as possible for training. however, if the validation set is small, it will give a
relatively noisy estimate of predictive performance. one solution to this dilemma is
to use cross-validation, which is illustrated in figure 1.18. this allows a proportion
(s     1)/s of the available data to be used for training while making use of all of the

1.4. the curse of dimensionality

33

figure 1.18 the technique of s-fold cross-validation,

illus-
trated here for the case of s = 4, involves tak-
ing the available data and partitioning it into s
groups (in the simplest case these are of equal
size). then s     1 of the groups are used to train
a set of models that are then evaluated on the re-
maining group. this procedure is then repeated
for all s possible choices for the held-out group,
indicated here by the red blocks, and the perfor-
mance scores from the s runs are then averaged.

run 1

run 2

run 3

run 4

data to assess performance. when data is particularly scarce, it may be appropriate
to consider the case s = n, where n is the total number of data points, which gives
the leave-one-out technique.

one major drawback of cross-validation is that the number of training runs that
must be performed is increased by a factor of s, and this can prove problematic for
models in which the training is itself computationally expensive. a further problem
with techniques such as cross-validation that use separate data to assess performance
is that we might have multiple complexity parameters for a single model (for in-
stance, there might be several id173 parameters). exploring combinations
of settings for such parameters could, in the worst case, require a number of training
runs that is exponential in the number of parameters. clearly, we need a better ap-
proach. ideally, this should rely only on the training data and should allow multiple
hyperparameters and model types to be compared in a single training run. we there-
fore need to    nd a measure of performance which depends only on the training data
and which does not suffer from bias due to over-   tting.

historically various    information criteria    have been proposed that attempt to
correct for the bias of maximum likelihood by the addition of a penalty term to
compensate for the over-   tting of more complex models. for example, the akaike
information criterion, or aic (akaike, 1974), chooses the model for which the quan-
tity
(1.73)
is largest. here p(d|wml) is the best-   t log likelihood, and m is the number of
adjustable parameters in the model. a variant of this quantity, called the bayesian
information criterion, or bic, will be discussed in section 4.4.1. such criteria do
not take account of the uncertainty in the model parameters, however, and in practice
they tend to favour overly simple models. we therefore turn in section 3.4 to a fully
bayesian approach where we shall see how complexity penalties arise in a natural
and principled way.

ln p(d|wml)     m

1.4. the curse of dimensionality

in the polynomial curve    tting example we had just one input variable x. for prac-
tical applications of pattern recognition, however, we will have to deal with spaces

34

1. introduction

figure 1.19 scatter plot of the oil    ow data
for input variables x6 and x7, in
which red denotes the    homoge-
nous    class, green denotes the
   annular    class, and blue denotes
the    laminar    class. our goal
is
to classify the new test point de-
noted by         .

2

1.5

x7

1

0.5

0

0

0.25

0.5
x6

0.75

1

of high dimensionality comprising many input variables. as we now discuss, this
poses some serious challenges and is an important factor in   uencing the design of
pattern recognition techniques.

in order to illustrate the problem we consider a synthetically generated data set
representing measurements taken from a pipeline containing a mixture of oil, wa-
ter, and gas (bishop and james, 1993). these three materials can be present in one
of three different geometrical con   gurations known as    homogenous   ,    annular   , and
   laminar   , and the fractions of the three materials can also vary. each data point com-
prises a 12-dimensional input vector consisting of measurements taken with gamma
ray densitometers that measure the attenuation of gamma rays passing along nar-
row beams through the pipe. this data set is described in detail in appendix a.
figure 1.19 shows 100 points from this data set on a plot showing two of the mea-
surements x6 and x7 (the remaining ten input values are ignored for the purposes of
this illustration). each data point is labelled according to which of the three geomet-
rical classes it belongs to, and our goal is to use this data as a training set in order to
be able to classify a new observation (x6, x7), such as the one denoted by the cross
in figure 1.19. we observe that the cross is surrounded by numerous red points, and
so we might suppose that it belongs to the red class. however, there are also plenty
of green points nearby, so we might think that it could instead belong to the green
class. it seems unlikely that it belongs to the blue class. the intuition here is that the
identity of the cross should be determined more strongly by nearby points from the
training set and less strongly by more distant points. in fact, this intuition turns out
to be reasonable and will be discussed more fully in later chapters.

how can we turn this intuition into a learning algorithm? one very simple ap-
proach would be to divide the input space into regular cells, as indicated in fig-
ure 1.20. when we are given a test point and we wish to predict its class, we    rst
decide which cell it belongs to, and we then    nd all of the training data points that

1.4. the curse of dimensionality

35

figure 1.20 illustration of a simple approach
to the solution of a classi   cation
problem in which the input space
is divided into cells and any new
test point is assigned to the class
that has a majority number of rep-
resentatives in the same cell as
the test point. as we shall see
shortly,
this simplistic approach
has some severe shortcomings.

2

1.5

x7

1

0.5

0

0

0.25

0.5
x6

0.75

1

fall in the same cell. the identity of the test point is predicted as being the same
as the class having the largest number of training points in the same cell as the test
point (with ties being broken at random).

there are numerous problems with this naive approach, but one of the most se-
vere becomes apparent when we consider its extension to problems having larger
numbers of input variables, corresponding to input spaces of higher dimensionality.
the origin of the problem is illustrated in figure 1.21, which shows that, if we divide
a region of a space into regular cells, then the number of such cells grows exponen-
tially with the dimensionality of the space. the problem with an exponentially large
number of cells is that we would need an exponentially large quantity of training data
in order to ensure that the cells are not empty. clearly, we have no hope of applying
such a technique in a space of more than a few variables, and so we need to    nd a
more sophisticated approach.

we can gain further insight into the problems of high-dimensional spaces by
returning to the example of polynomial curve    tting and considering how we would

section 1.1

of

figure 1.21 illustration
the
curse of dimensionality, showing
how the number of regions of a
regular grid grows exponentially
with the dimensionality d of
the
space. for clarity, only a subset of
the cubical regions are shown for
d = 3.

x2

x2

x1

d = 1

x1

x3

d = 2

d = 3

x1

36

1. introduction

extend this approach to deal with input spaces having several variables. if we have
d input variables, then a general polynomial with coef   cients up to order 3 would
take the form

d(cid:2)

d(cid:2)

d(cid:2)

d(cid:2)

d(cid:2)

d(cid:2)

y(x, w) = w0 +

wixi +

wijxixj +

wijkxixjxk.

(1.74)

exercise 1.16

i=1

i=1

j=1

i=1

j=1

k=1

as d increases, so the number of independent coef   cients (not all of the coef   cients
are independent due to interchange symmetries amongst the x variables) grows pro-
portionally to d3. in practice, to capture complex dependencies in the data, we may
need to use a higher-order polynomial. for a polynomial of order m, the growth in
the number of coef   cients is like dm . although this is now a power law growth,
rather than an exponential growth, it still points to the method becoming rapidly
unwieldy and of limited practical utility.

our geometrical intuitions, formed through a life spent in a space of three di-
mensions, can fail badly when we consider spaces of higher dimensionality. as a
simple example, consider a sphere of radius r = 1 in a space of d dimensions, and
ask what is the fraction of the volume of the sphere that lies between radius r = 1    
and r = 1. we can evaluate this fraction by noting that the volume of a sphere of
radius r in d dimensions must scale as rd, and so we write

vd(r) = kdrd

(1.75)

exercise 1.18

where the constant kd depends only on d. thus the required fraction is given by

exercise 1.20

vd(1)     vd(1      )

vd(1)

= 1     (1      )d

(1.76)

which is plotted as a function of   for various values of d in figure 1.22. we see
that, for large d, this fraction tends to 1 even for small values of  . thus, in spaces
of high dimensionality, most of the volume of a sphere is concentrated in a thin shell
near the surface!

as a further example, of direct relevance to pattern recognition, consider the
behaviour of a gaussian distribution in a high-dimensional space. if we transform
from cartesian to polar coordinates, and then integrate out the directional variables,
we obtain an expression for the density p(r) as a function of radius r from the origin.
thus p(r)  r is the id203 mass inside a thin shell of thickness   r located at
radius r. this distribution is plotted, for various values of d, in figure 1.23, and we
see that for large d the id203 mass of the gaussian is concentrated in a thin
shell.

the severe dif   culty that can arise in spaces of many dimensions is sometimes
called the curse of dimensionality (bellman, 1961). in this book, we shall make ex-
tensive use of illustrative examples involving input spaces of one or two dimensions,
because this makes it particularly easy to illustrate the techniques graphically. the
reader should be warned, however, that not all intuitions developed in spaces of low
dimensionality will generalize to spaces of many dimensions.

figure 1.22 plot of the fraction of the volume of
a sphere lying in the range r = 1    
to r = 1 for various values of the
dimensionality d.

1.4. the curse of dimensionality

37

n
o

i
t
c
a
r
f
 

e
m
u
o
v

l

1

0.8

0.6

0.4

0.2

0

0

d = 20

d = 5

d = 2

d = 1

0.2

0.4

0.6

0.8

1

 

although the curse of dimensionality certainly raises important issues for pat-
tern recognition applications, it does not prevent us from    nding effective techniques
applicable to high-dimensional spaces. the reasons for this are twofold. first, real
data will often be con   ned to a region of the space having lower effective dimension-
ality, and in particular the directions over which important variations in the target
variables occur may be so con   ned. second, real data will typically exhibit some
smoothness properties (at least locally) so that for the most part small changes in the
input variables will produce small changes in the target variables, and so we can ex-
ploit local interpolation-like techniques to allow us to make predictions of the target
variables for new values of the input variables. successful pattern recognition tech-
niques exploit one or both of these properties. consider, for example, an application
in manufacturing in which images are captured of identical planar objects on a con-
veyor belt, in which the goal is to determine their orientation. each image is a point

figure 1.23 plot of the id203 density with
to radius r of a gaus-
respect
sian distribution for various values
of
in a
high-dimensional space, most of the
id203 mass of a gaussian is lo-
cated within a thin shell at a speci   c
radius.

the dimensionality d.

2

)
r
(
p

1

d = 1

d = 2

d = 20

0

0

2
r

4

38

1. introduction

in a high-dimensional space whose dimensionality is determined by the number of
pixels. because the objects can occur at different positions within the image and
in different orientations, there are three degrees of freedom of variability between
images, and a set of images will live on a three dimensional manifold embedded
within the high-dimensional space. due to the complex relationships between the
object position or orientation and the pixel intensities, this manifold will be highly
nonlinear. if the goal is to learn a model that can take an input image and output the
orientation of the object irrespective of its position, then there is only one degree of
freedom of variability within the manifold that is signi   cant.

1.5. decision theory

we have seen in section 1.2 how id203 theory provides us with a consistent
mathematical framework for quantifying and manipulating uncertainty. here we
turn to a discussion of decision theory that, when combined with id203 theory,
allows us to make optimal decisions in situations involving uncertainty such as those
encountered in pattern recognition.

suppose we have an input vector x together with a corresponding vector t of
target variables, and our goal is to predict t given a new value for x. for regression
problems, t will comprise continuous variables, whereas for classi   cation problems
t will represent class labels. the joint id203 distribution p(x, t) provides a
complete summary of the uncertainty associated with these variables. determination
of p(x, t) from a set of training data is an example of id136 and is typically a
very dif   cult problem whose solution forms the subject of much of this book. in
a practical application, however, we must often make a speci   c prediction for the
value of t, or more generally take a speci   c action based on our understanding of the
values t is likely to take, and this aspect is the subject of decision theory.

consider, for example, a medical diagnosis problem in which we have taken an
x-ray image of a patient, and we wish to determine whether the patient has cancer
or not. in this case, the input vector x is the set of pixel intensities in the image,
and output variable t will represent the presence of cancer, which we denote by the
class c1, or the absence of cancer, which we denote by the class c2. we might, for
instance, choose t to be a binary variable such that t = 0 corresponds to class c1 and
t = 1 corresponds to class c2. we shall see later that this choice of label values is
particularly convenient for probabilistic models. the general id136 problem then
involves determining the joint distribution p(x,ck), or equivalently p(x, t), which
gives us the most complete probabilistic description of the situation. although this
can be a very useful and informative quantity, in the end we must decide either to
give treatment to the patient or not, and we would like this choice to be optimal
in some appropriate sense (duda and hart, 1973). this is the decision step, and
it is the subject of decision theory to tell us how to make optimal decisions given
the appropriate probabilities. we shall see that the decision stage is generally very
simple, even trivial, once we have solved the id136 problem.

here we give an introduction to the key ideas of decision theory as required for

1.5. decision theory

39

the rest of the book. further background, as well as more detailed accounts, can be
found in berger (1985) and bather (2000).

before giving a more detailed analysis, let us    rst consider informally how we
might expect probabilities to play a role in making decisions. when we obtain the
x-ray image x for a new patient, our goal is to decide which of the two classes to
assign to the image. we are interested in the probabilities of the two classes given
the image, which are given by p(ck|x). using bayes    theorem, these probabilities
can be expressed in the form

p(ck|x) = p(x|ck)p(ck)

p(x)

.

(1.77)

note that any of the quantities appearing in bayes    theorem can be obtained from
the joint distribution p(x,ck) by either marginalizing or conditioning with respect to
the appropriate variables. we can now interpret p(ck) as the prior id203 for the
class ck, and p(ck|x) as the corresponding posterior id203. thus p(c1) repre-
sents the id203 that a person has cancer, before we take the x-ray measurement.
similarly, p(c1|x) is the corresponding id203, revised using bayes    theorem in
light of the information contained in the x-ray. if our aim is to minimize the chance
of assigning x to the wrong class, then intuitively we would choose the class having
the higher posterior id203. we now show that this intuition is correct, and we
also discuss more general criteria for making decisions.

1.5.1 minimizing the misclassi   cation rate
suppose that our goal is simply to make as few misclassi   cations as possible.
we need a rule that assigns each value of x to one of the available classes. such a
rule will divide the input space into regions rk called decision regions, one for each
class, such that all points in rk are assigned to class ck. the boundaries between
decision regions are called decision boundaries or decision surfaces. note that each
decision region need not be contiguous but could comprise some number of disjoint
regions. we shall encounter examples of decision boundaries and decision regions in
later chapters. in order to    nd the optimal decision rule, consider    rst of all the case
of two classes, as in the cancer problem for instance. a mistake occurs when an input
vector belonging to class c1 is assigned to class c2 or vice versa. the id203 of
this occurring is given by

p(mistake) = p(x     r1,c2) + p(x     r2,c1)

p(x,c2) dx +

p(x,c1) dx.

(1.78)

(cid:6)

=

r1

(cid:6)

r2

we are free to choose the decision rule that assigns each point x to one of the two
classes. clearly to minimize p(mistake) we should arrange that each x is assigned to
whichever class has the smaller value of the integrand in (1.78). thus, if p(x,c1) >
p(x,c2) for a given value of x, then we should assign that x to class c1. from the
product rule of id203 we have p(x,ck) = p(ck|x)p(x). because the factor
p(x) is common to both terms, we can restate this result as saying that the minimum

40

1. introduction

(cid:1)x

x0

p(x,c1)

p(x,c2)

r1

x

r2

figure 1.24 schematic illustration of the joint probabilities p(x,ck) for each of two classes plotted
against x, together with the decision boundary x = bx. values of x (cid:2) bx are classi   ed as
class c2 and hence belong to decision region r2, whereas points x < bx are classi   ed
as c1 and belong to r1. errors arise from the blue, green, and red regions, so that for
x < bx the errors are due to points from class c2 being misclassi   ed as c1 (represented by
the sum of the red and green regions), and conversely for points in the region x (cid:2) bx the
errors are due to points from class c1 being misclassi   ed as c2 (represented by the blue
region). as we vary the location bx of the decision boundary, the combined areas of the
blue and green regions remains constant, whereas the size of the red region varies. the
optimal choice for bx is where the curves for p(x,c1) and p(x,c2) cross, corresponding to
bx = x0, because in this case the red region disappears. this is equivalent to the minimum
misclassi   cation rate decision rule, which assigns each value of x to the class having the
higher posterior id203 p(ck|x).

id203 of making a mistake is obtained if each value of x is assigned to the class
for which the posterior id203 p(ck|x) is largest. this result is illustrated for
two classes, and a single input variable x, in figure 1.24.

for the more general case of k classes, it is slightly easier to maximize the

id203 of being correct, which is given by

p(x     rk,ck)

k(cid:2)
k(cid:2)

k=1

(cid:6)

rk

k=1

p(correct) =

=

p(x,ck) dx

(1.79)

which is maximized when the regions rk are chosen such that each x is assigned
to the class for which p(x,ck) is largest. again, using the product rule p(x,ck) =
p(ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see
that each x should be assigned to the class having the largest posterior id203
p(ck|x).

figure 1.25 an example of a loss matrix with ele-
ments lkj for the cancer treatment problem. the rows
correspond to the true class, whereas the columns cor-
respond to the assignment of class made by our deci-
sion criterion.

cancer
normal

1.5. decision theory

(cid:15) cancer normal

(cid:16)

0
1

1000

0

41

1.5.2 minimizing the expected loss
for many applications, our objective will be more complex than simply mini-
mizing the number of misclassi   cations. let us consider again the medical diagnosis
problem. we note that, if a patient who does not have cancer is incorrectly diagnosed
as having cancer, the consequences may be some patient distress plus the need for
further investigations. conversely, if a patient with cancer is diagnosed as healthy,
the result may be premature death due to lack of treatment. thus the consequences
of these two types of mistake can be dramatically different. it would clearly be better
to make fewer mistakes of the second kind, even if this was at the expense of making
more mistakes of the    rst kind.

we can formalize such issues through the introduction of a id168, also
called a cost function, which is a single, overall measure of loss incurred in taking
any of the available decisions or actions. our goal is then to minimize the total loss
incurred. note that some authors consider instead a utility function, whose value
they aim to maximize. these are equivalent concepts if we take the utility to be
simply the negative of the loss, and throughout this text we shall use the id168
convention. suppose that, for a new value of x, the true class is ck and that we assign
x to class cj (where j may or may not be equal to k). in so doing, we incur some
level of loss that we denote by lkj, which we can view as the k, j element of a loss
matrix. for instance, in our cancer example, we might have a loss matrix of the form
shown in figure 1.25. this particular loss matrix says that there is no loss incurred
if the correct decision is made, there is a loss of 1 if a healthy patient is diagnosed as
having cancer, whereas there is a loss of 1000 if a patient having cancer is diagnosed
as healthy.

the optimal solution is the one which minimizes the id168. however,
the id168 depends on the true class, which is unknown. for a given input
vector x, our uncertainty in the true class is expressed through the joint id203
distribution p(x,ck) and so we seek instead to minimize the average loss, where the
average is computed with respect to this distribution, which is given by

lkjp(x,ck) dx.

(1.80)

(cid:5)
each x can be assigned independently to one of the decision regions rj. our goal
is to choose the regions rj in order to minimize the expected loss (1.80), which
k lkjp(x,ck). as before, we can use
implies that for each x we should minimize
the product rule p(x,ck) = p(ck|x)p(x) to eliminate the common factor of p(x).
thus the decision rule that minimizes the expected loss is the one that assigns each

(cid:2)

(cid:2)

(cid:6)

e[l] =

k

j

rj

42

1. introduction

figure 1.26 illustration of the reject option.

inputs
x such that the larger of the two poste-
rior probabilities is less than or equal to
some threshold    will be rejected.

1.0
  

p(c1|x)

p(c2|x)

0.0

reject region

x

new x to the class j for which the quantity(cid:2)

lkjp(ck|x)

k

(1.81)

is a minimum. this is clearly trivial to do, once we know the posterior class proba-
bilities p(ck|x).

1.5.3 the reject option
we have seen that classi   cation errors arise from the regions of input space
where the largest of the posterior probabilities p(ck|x) is signi   cantly less than unity,
or equivalently where the joint distributions p(x,ck) have comparable values. these
are the regions where we are relatively uncertain about class membership. in some
applications, it will be appropriate to avoid making decisions on the dif   cult cases
in anticipation of a lower error rate on those examples for which a classi   cation de-
cision is made. this is known as the reject option. for example, in our hypothetical
medical illustration, it may be appropriate to use an automatic system to classify
those x-ray images for which there is little doubt as to the correct class, while leav-
ing a human expert to classify the more ambiguous cases. we can achieve this by
introducing a threshold    and rejecting those inputs x for which the largest of the
posterior probabilities p(ck|x) is less than or equal to   . this is illustrated for the
case of two classes, and a single continuous input variable x, in figure 1.26. note
that setting    = 1 will ensure that all examples are rejected, whereas if there are k
classes then setting    < 1/k will ensure that no examples are rejected. thus the
fraction of examples that get rejected is controlled by the value of   .

we can easily extend the reject criterion to minimize the expected loss, when
a loss matrix is given, taking account of the loss incurred when a reject decision is
made.

1.5.4 id136 and decision
we have broken the classi   cation problem down into two separate stages, the
id136 stage in which we use training data to learn a model for p(ck|x), and the

exercise 1.24

1.5. decision theory

43

subsequent decision stage in which we use these posterior probabilities to make op-
timal class assignments. an alternative possibility would be to solve both problems
together and simply learn a function that maps inputs x directly into decisions. such
a function is called a discriminant function.

in fact, we can identify three distinct approaches to solving decision problems,
all of which have been used in practical applications. these are given, in decreasing
order of complexity, by:

(a) first solve the id136 problem of determining the class-conditional densities
p(x|ck) for each class ck individually. also separately infer the prior class
probabilities p(ck). then use bayes    theorem in the form

p(ck|x) = p(x|ck)p(ck)

(1.82)
to    nd the posterior class probabilities p(ck|x). as usual, the denominator
in bayes    theorem can be found in terms of the quantities appearing in the
numerator, because

p(x)

(cid:2)

p(x) =

k

(1.83)
equivalently, we can model the joint distribution p(x,ck) directly and then
normalize to obtain the posterior probabilities. having found the posterior
probabilities, we use decision theory to determine class membership for each
new input x. approaches that explicitly or implicitly model the distribution of
inputs as well as outputs are known as generative models, because by sampling
from them it is possible to generate synthetic data points in the input space.

p(x|ck)p(ck).

(b) first solve the id136 problem of determining the posterior class probabilities
p(ck|x), and then subsequently use decision theory to assign each new x to
one of the classes. approaches that model the posterior probabilities directly
are called discriminative models.

(c) find a function f(x), called a discriminant function, which maps each input x
directly onto a class label. for instance, in the case of two-class problems,
f(  ) might be binary valued and such that f = 0 represents class c1 and f = 1
represents class c2. in this case, probabilities play no role.

let us consider the relative merits of these three alternatives. approach (a) is the
most demanding because it involves    nding the joint distribution over both x and
ck. for many applications, x will have high dimensionality, and consequently we
may need a large training set in order to be able to determine the class-conditional
densities to reasonable accuracy. note that the class priors p(ck) can often be esti-
mated simply from the fractions of the training set data points in each of the classes.
one advantage of approach (a), however, is that it also allows the marginal density
of data p(x) to be determined from (1.83). this can be useful for detecting new data
points that have low id203 under the model and for which the predictions may

44

1. introduction

s
e

i
t
i
s
n
e
d

 
s
s
a
c

l

5

4

3

2

1

0

0

p(x|c2)

p(x|c1)

0.2

0.4

0.6

0.8

1

x

1.2

1

0.8

0.6

0.4

0.2

0

0

p(c1|x)

p(c2|x)

0.2

0.4

x

0.6

0.8

1

figure 1.27 example of the class-conditional densities for two classes having a single input variable x (left
plot) together with the corresponding posterior probabilities (right plot). note that the left-hand mode of the
class-conditional density p(x|c1), shown in blue on the left plot, has no effect on the posterior probabilities. the
vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassi   cation
rate.

be of low accuracy, which is known as outlier detection or novelty detection (bishop,
1994; tarassenko, 1995).

however, if we only wish to make classi   cation decisions, then it can be waste-
ful of computational resources, and excessively demanding of data, to    nd the joint
distribution p(x,ck) when in fact we only really need the posterior probabilities
p(ck|x), which can be obtained directly through approach (b). indeed, the class-
conditional densities may contain a lot of structure that has little effect on the pos-
terior probabilities, as illustrated in figure 1.27. there has been much interest in
exploring the relative merits of generative and discriminative approaches to machine
learning, and in    nding ways to combine them (jebara, 2004; lasserre et al., 2006).
an even simpler approach is (c) in which we use the training data to    nd a
discriminant function f(x) that maps each x directly onto a class label, thereby
combining the id136 and decision stages into a single learning problem. in the
example of figure 1.27, this would correspond to    nding the value of x shown by
the vertical green line, because this is the decision boundary giving the minimum
id203 of misclassi   cation.
with option (c), however, we no longer have access to the posterior probabilities
p(ck|x). there are many powerful reasons for wanting to compute the posterior
probabilities, even if we subsequently use them to make decisions. these include:

minimizing risk. consider a problem in which the elements of the loss matrix are
subjected to revision from time to time (such as might occur in a    nancial

1.5. decision theory

45

application). if we know the posterior probabilities, we can trivially revise the
minimum risk decision criterion by modifying (1.81) appropriately. if we have
only a discriminant function, then any change to the loss matrix would require
that we return to the training data and solve the classi   cation problem afresh.

reject option. posterior probabilities allow us to determine a rejection criterion that
will minimize the misclassi   cation rate, or more generally the expected loss,
for a given fraction of rejected data points.

compensating for class priors. consider our medical x-ray problem again, and
suppose that we have collected a large number of x-ray images from the gen-
eral population for use as training data in order to build an automated screening
system. because cancer is rare amongst the general population, we might    nd
that, say, only 1 in every 1,000 examples corresponds to the presence of can-
cer. if we used such a data set to train an adaptive model, we could run into
severe dif   culties due to the small proportion of the cancer class. for instance,
a classi   er that assigned every point to the normal class would already achieve
99.9% accuracy and it would be dif   cult to avoid this trivial solution. also,
even a large data set will contain very few examples of x-ray images corre-
sponding to cancer, and so the learning algorithm will not be exposed to a
broad range of examples of such images and hence is not likely to generalize
well. a balanced data set in which we have selected equal numbers of exam-
ples from each of the classes would allow us to    nd a more accurate model.
however, we then have to compensate for the effects of our modi   cations to
the training data. suppose we have used such a modi   ed data set and found
models for the posterior probabilities. from bayes    theorem (1.82), we see that
the posterior probabilities are proportional to the prior probabilities, which we
can interpret as the fractions of points in each class. we can therefore simply
take the posterior probabilities obtained from our arti   cially balanced data set
and    rst divide by the class fractions in that data set and then multiply by the
class fractions in the population to which we wish to apply the model. finally,
we need to normalize to ensure that the new posterior probabilities sum to one.
note that this procedure cannot be applied if we have learned a discriminant
function directly instead of determining posterior probabilities.

combining models. for complex applications, we may wish to break the problem
into a number of smaller subproblems each of which can be tackled by a sep-
arate module. for example, in our hypothetical medical diagnosis problem,
we may have information available from, say, blood tests as well as x-ray im-
ages. rather than combine all of this heterogeneous information into one huge
input space, it may be more effective to build one system to interpret the x-
ray images and a different one to interpret the blood data. as long as each of
the two models gives posterior probabilities for the classes, we can combine
the outputs systematically using the rules of id203. one simple way to
do this is to assume that, for each class separately, the distributions of inputs
for the x-ray images, denoted by xi, and the blood data, denoted by xb, are

46

1. introduction

independent, so that

p(xi, xb|ck) = p(xi|ck)p(xb|ck).

(1.84)

section 8.2

section 8.2.2

section 1.1

appendix d

this is an example of conditional independence property, because the indepen-
dence holds when the distribution is conditioned on the class ck. the posterior
id203, given both the x-ray and blood data, is then given by

p(ck|xi, xb)     p(xi, xb|ck)p(ck)
    p(xi|ck)p(xb|ck)p(ck)
    p(ck|xi)p(ck|xb)

p(ck)

(1.85)
thus we need the class prior probabilities p(ck), which we can easily estimate
from the fractions of data points in each class, and then we need to normalize
the resulting posterior probabilities so they sum to one. the particular condi-
tional independence assumption (1.84) is an example of the naive bayes model.
note that the joint marginal distribution p(xi, xb) will typically not factorize
under this model. we shall see in later chapters how to construct models for
combining data that do not require the conditional independence assumption
(1.84).

1.5.5 id168s for regression
so far, we have discussed decision theory in the context of classi   cation prob-
lems. we now turn to the case of regression problems, such as the curve    tting
example discussed earlier. the decision stage consists of choosing a speci   c esti-
mate y(x) of the value of t for each input x. suppose that in doing so, we incur a
loss l(t, y(x)). the average, or expected, loss is then given by

(cid:6)(cid:6)

e[l] =

l(t, y(x))p(x, t) dx dt.

(1.86)

a common choice of id168 in regression problems is the squared loss given
by l(t, y(x)) = {y(x)     t}2. in this case, the expected loss can be written

e[l] =

{y(x)     t}2p(x, t) dx dt.

(1.87)

(cid:6)(cid:6)

our goal is to choose y(x) so as to minimize e[l].
if we assume a completely
   exible function y(x), we can do this formally using the calculus of variations to
give

(cid:6)

{y(x)     t}p(x, t) dt = 0.

(1.88)

solving for y(x), and using the sum and product rules of id203, we obtain

  e[l]
  y(x)

= 2

(cid:6)

(cid:6)

y(x) =

tp(x, t) dt

=

p(x)

tp(t|x) dt = et[t|x]

(1.89)

figure 1.28 the regression function y(x),
which minimizes the expected
squared loss, is given by the
mean of the conditional distri-
bution p(t|x).

t

y(x0)

1.5. decision theory

47

y(x)

p(t|x0)

x0

x

which is the conditional average of t conditioned on x and is known as the regression
function. this result is illustrated in figure 1.28. it can readily be extended to mul-
tiple target variables represented by the vector t, in which case the optimal solution
is the conditional average y(x) = et[t|x].

exercise 1.25

we can also derive this result in a slightly different way, which will also shed
light on the nature of the regression problem. armed with the knowledge that the
optimal solution is the conditional expectation, we can expand the square term as
follows

{y(x)     t}2 = {y(x)     e[t|x] + e[t|x]     t}2
= {y(x)     e[t|x]}2 + 2{y(x)     e[t|x]}{e[t|x]     t} + {e[t|x]     t}2

where, to keep the notation uncluttered, we use e[t|x] to denote et[t|x]. substituting
into the id168 and performing the integral over t, we see that the cross-term
vanishes and we obtain an expression for the id168 in the form
{e[t|x]     t}2p(x) dx.

{y(x)     e[t|x]}2

p(x) dx +

e[l] =

(cid:6)

(cid:6)

(1.90)

the function y(x) we seek to determine enters only in the    rst term, which will be
minimized when y(x) is equal to e[t|x], in which case this term will vanish. this
is simply the result that we derived previously and that shows that the optimal least
squares predictor is given by the conditional mean. the second term is the variance
of the distribution of t, averaged over x.
it represents the intrinsic variability of
the target data and can be regarded as noise. because it is independent of y(x), it
represents the irreducible minimum value of the id168.

as with the classi   cation problem, we can either determine the appropriate prob-
abilities and then use these to make optimal decisions, or we can build models that
make decisions directly. indeed, we can identify three distinct approaches to solving
regression problems given, in order of decreasing complexity, by:
(a) first solve the id136 problem of determining the joint density p(x, t). then
normalize to    nd the conditional density p(t|x), and    nally marginalize to    nd
the conditional mean given by (1.89).

48

1. introduction

(b) first solve the id136 problem of determining the conditional density p(t|x),
and then subsequently marginalize to    nd the conditional mean given by (1.89).

(c) find a regression function y(x) directly from the training data.
the relative merits of these three approaches follow the same lines as for classi   ca-
tion problems above.

the squared loss is not the only possible choice of id168 for regression.
indeed, there are situations in which squared loss can lead to very poor results and
where we need to develop more sophisticated approaches. an important example
concerns situations in which the conditional distribution p(t|x) is multimodal, as
often arises in the solution of inverse problems. here we consider brie   y one simple
generalization of the squared loss, called the minkowski loss, whose expectation is
given by

(cid:6)(cid:6)

|y(x)     t|qp(x, t) dx dt

e[lq] =

(1.91)
which reduces to the expected squared loss for q = 2. the function |y     t|q is
plotted against y     t for various values of q in figure 1.29. the minimum of e[lq]
is given by the conditional mean for q = 2, the conditional median for q = 1, and
the conditional mode for q     0.

section 5.6

exercise 1.27

1.6.

id205

in this chapter, we have discussed a variety of concepts from id203 theory and
decision theory that will form the foundations for much of the subsequent discussion
in this book. we close this chapter by introducing some additional concepts from
the    eld of id205, which will also prove useful in our development of
pattern recognition and machine learning techniques. again, we shall focus only on
the key concepts, and we refer the reader elsewhere for more detailed discussions
(viterbi and omura, 1979; cover and thomas, 1991; mackay, 2003) .

we begin by considering a discrete random variable x and we ask how much
information is received when we observe a speci   c value for this variable. the
amount of information can be viewed as the    degree of surprise    on learning the
value of x. if we are told that a highly improbable event has just occurred, we will
have received more information than if we were told that some very likely event
has just occurred, and if we knew that the event was certain to happen we would
receive no information. our measure of information content will therefore depend
on the id203 distribution p(x), and we therefore look for a quantity h(x) that
is a monotonic function of the id203 p(x) and that expresses the information
content. the form of h(  ) can be found by noting that if we have two events x
and y that are unrelated, then the information gain from observing both of them
should be the sum of the information gained from each of them separately, so that
h(x, y) = h(x) + h(y). two unrelated events will be statistically independent and
so p(x, y) = p(x)p(y). from these two relationships, it is easily shown that h(x)
must be given by the logarithm of p(x) and so we have

exercise 1.28

q

|
t

   
y
|

q

|
t

   
y
|

2

1

0
   2

2

1

0
   2

q = 0.3

   1

0
y     t

1

2

q = 2

   1

0
y     t

1

2

q

|
t

   
y
|

q

|
t

   
y
|

2

1

0
   2

2

1

0
   2

1.6. id205

49

q = 1

   1

0
y     t

1

2

q = 10

   1

0
y     t

1

2

figure 1.29 plots of the quantity lq = |y     t|q for various values of q.

h(x) =     log2 p(x)

(1.92)

where the negative sign ensures that information is positive or zero. note that low
id203 events x correspond to high information content. the choice of basis
for the logarithm is arbitrary, and for the moment we shall adopt the convention
prevalent in id205 of using logarithms to the base of 2. in this case, as
we shall see shortly, the units of h(x) are bits (   binary digits   ).

now suppose that a sender wishes to transmit the value of a random variable to
a receiver. the average amount of information that they transmit in the process is
obtained by taking the expectation of (1.92) with respect to the distribution p(x) and
is given by

(cid:2)

h[x] =    

p(x) log2 p(x).

(1.93)

x

this important quantity is called the id178 of the random variable x. note that
limp   0 p ln p = 0 and so we shall take p(x) ln p(x) = 0 whenever we encounter a
value for x such that p(x) = 0.

so far we have given a rather heuristic motivation for the de   nition of informa-

50

1. introduction

tion (1.92) and the corresponding id178 (1.93). we now show that these de   nitions
indeed possess useful properties. consider a random variable x having 8 possible
states, each of which is equally likely. in order to communicate the value of x to
a receiver, we would need to transmit a message of length 3 bits. notice that the
id178 of this variable is given by

h[x] =    8    1
8

log2

1
8

= 3 bits.

4 , 1

now consider an example (cover and thomas, 1991) of a variable having 8 pos-
sible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by
16 , 1
( 1
2 , 1
8 , 1
h[x] =    1
2

64). the id178 in this case is given by
    4
64

64 , 1
64 , 1
    1
1
2
4

    1
16

= 2 bits.

    1
8

64 , 1

1
16

1
64

log2

log2

log2

log2

log2

1
4

1
8

we see that the nonuniform distribution has a smaller id178 than the uniform one,
and we shall gain some insight into this shortly when we discuss the interpretation of
id178 in terms of disorder. for the moment, let us consider how we would transmit
the identity of the variable   s state to a receiver. we could do this, as before, using
a 3-bit number. however, we can take advantage of the nonuniform distribution by
using shorter codes for the more probable events, at the expense of longer codes for
the less probable events, in the hope of getting a shorter average code length. this
can be done by representing the states {a, b, c, d, e, f, g, h} using, for instance, the
following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111.
the average length of the code that has to be transmitted is then

average code length =

1
2

   1 +

1
4

   2 +

1
8

   3 +

1
16

   4 + 4    1
64

   6 = 2 bits

which again is the same as the id178 of the random variable. note that shorter code
strings cannot be used because it must be possible to disambiguate a concatenation
of such strings into its component parts. for instance, 11001110 decodes uniquely
into the state sequence c, a, d.

this relation between id178 and shortest coding length is a general one. the
noiseless coding theorem (shannon, 1948) states that the id178 is a lower bound
on the number of bits needed to transmit the state of a random variable.

from now on, we shall switch to the use of natural logarithms in de   ning en-
tropy, as this will provide a more convenient link with ideas elsewhere in this book.
in this case, the id178 is measured in units of    nats    instead of bits, which differ
simply by a factor of ln 2.

we have introduced the concept of id178 in terms of the average amount of
information needed to specify the state of a random variable. in fact, the concept of
id178 has much earlier origins in physics where it was introduced in the context
of equilibrium thermodynamics and later given a deeper interpretation as a measure
of disorder through developments in statistical mechanics. we can understand this
alternative view of id178 by considering a set of n identical objects that are to be
divided amongst a set of bins, such that there are ni objects in the ith bin. consider

1.6. id205

51

the number of different ways of allocating the objects to the bins. there are n
ways to choose the    rst object, (n     1) ways to choose the second object, and
so on, leading to a total of n! ways to allocate all n objects to the bins, where n!
(pronounced    factorial n   ) denotes the product n   (n    1)          2  1. however,
we don   t wish to distinguish between rearrangements of objects within each bin. in
the ith bin there are ni! ways of reordering the objects, and so the total number of
ways of allocating the n objects to the bins is given by

w = n!(cid:21)

i ni!

(1.94)

which is called the multiplicity. the id178 is then de   ned as the logarithm of the
multiplicity scaled by an appropriate constant

h =

1
n

ln w =

1
n

ln n!     1
n

ln ni!.

(1.95)

we now consider the limit n        , in which the fractions ni/n are held    xed, and
apply stirling   s approximation

(cid:2)

i

which gives

h =     lim
n      

(cid:5)

i

ln n! (cid:8) n ln n     n

(cid:18)

(cid:17)

(cid:18)

(cid:2)

(cid:17)

ni
n

ln

ni
n

=    

(cid:2)

i

pi ln pi

(1.96)

(1.97)

i ni = n. here pi = limn      (ni/n) is the id203
where we have used
of an object being assigned to the ith bin. in physics terminology, the speci   c ar-
rangements of objects in the bins is called a microstate, and the overall distribution
of occupation numbers, expressed through the ratios ni/n, is called a macrostate.
the multiplicity w is also known as the weight of the macrostate.

we can interpret the bins as the states xi of a discrete random variable x, where

p(x = xi) = pi. the id178 of the random variable x is then

h[p] =    

p(xi) ln p(xi).

(1.98)

(cid:2)

i

appendix e

distributions p(xi) that are sharply peaked around a few values will have a relatively
low id178, whereas those that are spread more evenly across many values will
have higher id178, as illustrated in figure 1.30. because 0 (cid:1) pi (cid:1) 1, the id178
is nonnegative, and it will equal its minimum value of 0 when one of the pi =
1 and all other pj(cid:4)=i = 0. the maximum id178 con   guration can be found by
maximizing h using a lagrange multiplier to enforce the id172 constraint
on the probabilities. thus we maximize

(cid:22)(cid:2)

(cid:23)

p(xi) ln p(xi) +   

i

i

p(xi)     1

(1.99)

(cid:2)

(cid:4)h =    

52

1. introduction

s
e

i
t
i
l
i

b
a
b
o
r
p

h = 1.77

0.5

0.25

0

s
e

i
t
i
l
i

b
a
b
o
r
p

h = 3.09

0.5

0.25

0

figure 1.30 histograms of two id203 distributions over 30 bins illustrating the higher value of the id178
h for the broader distribution. the largest id178 would arise from a uniform distribution that would give h =
    ln(1/30) = 3.40.

exercise 1.29

from which we    nd that all of the p(xi) are equal and are given by p(xi) = 1/m
where m is the total number of states xi. the corresponding value of the id178
is then h = ln m. this result can also be derived from jensen   s inequality (to be
discussed shortly). to verify that the stationary point is indeed a maximum, we can
evaluate the second derivative of the id178, which gives

   (cid:4)h

   p(xi)   p(xj)

=    iij

1
pi

(1.100)

where iij are the elements of the identity matrix.

we can extend the de   nition of id178 to include distributions p(x) over con-
tinuous variables x as follows. first divide x into bins of width    . then, assuming
p(x) is continuous, the mean value theorem (weisstein, 1999) tells us that, for each
such bin, there must exist a value xi such that

(cid:6) (i+1)   

p(x) dx = p(xi)   .

(1.101)

i   

we can now quantize the continuous variable x by assigning any value x to the value
xi whenever x falls in the ith bin. the id203 of observing the value xi is then
p(xi)   . this gives a discrete distribution for which the id178 takes the form

h    =    

p(xi)    ln (p(xi)   ) =    

p(xi)    ln p(xi)     ln    

(1.102)

(cid:2)

i

(cid:2)

i

(cid:5)

i p(xi)    = 1, which follows from (1.101). we now omit
where we have used
the second term     ln     on the right-hand side of (1.102) and then consider the limit

1.6. id205

53

        0. the    rst term on the right-hand side of (1.102) will approach the integral of
p(x) ln p(x) in this limit so that

(cid:25)

(cid:6)

p(xi)    ln p(xi)

=    

p(x) ln p(x) dx

(1.103)

(cid:24)(cid:2)

i

lim
      0

where the quantity on the right-hand side is called the differential id178. we see
that the discrete and continuous forms of the id178 differ by a quantity ln    , which
diverges in the limit         0. this re   ects the fact that to specify a continuous
variable very precisely requires a large number of bits. for a density de   ned over
multiple continuous variables, denoted collectively by the vector x, the differential
id178 is given by

(cid:6)

h[x] =    

p(x) ln p(x) dx.

(1.104)

in the case of discrete distributions, we saw that the maximum id178 con-
   guration corresponded to an equal distribution of probabilities across the possible
states of the variable. let us now consider the maximum id178 con   guration for
a continuous variable. in order for this maximum to be well de   ned, it will be nec-
essary to constrain the    rst and second moments of p(x) as well as preserving the
id172 constraint. we therefore maximize the differential id178 with the

ludwig boltzmann
1844   1906

ludwig eduard boltzmann was an
austrian physicist who created the
   eld of statistical mechanics. prior
to boltzmann,
the concept of en-
tropy was already known from
classical thermodynamics where it
quanti   es the fact that when we take energy from a
system, not all of that energy is typically available
to do useful work. boltzmann showed that the ther-
modynamic id178 s, a macroscopic quantity, could
be related to the statistical properties at the micro-
scopic level. this is expressed through the famous
equation s = k ln w in which w represents the
number of possible microstates in a macrostate, and
k (cid:3) 1.38    10   23 (in units of joules per kelvin) is
known as boltzmann   s constant. boltzmann   s ideas
were disputed by many scientists of they day. one dif-
   culty they saw arose from the second law of thermo-

dynamics, which states that the id178 of a closed
system tends to increase with time. by contrast, at
the microscopic level the classical newtonian equa-
tions of physics are reversible, and so they found it
dif   cult to see how the latter could explain the for-
mer. they didn   t fully appreciate boltzmann   s argu-
ments, which were statistical in nature and which con-
cluded not that id178 could never decrease over
time but simply that with overwhelming id203 it
would generally increase. boltzmann even had a long-
running dispute with the editor of the leading german
physics journal who refused to let him refer to atoms
and molecules as anything other than convenient the-
oretical constructs. the continued attacks on his work
lead to bouts of depression, and eventually he com-
mitted suicide. shortly after boltzmann   s death, new
experiments by perrin on colloidal suspensions veri-
   ed his theories and con   rmed the value of the boltz-
mann constant. the equation s = k ln w is carved on
boltzmann   s tombstone.

54

1. introduction

three constraints

(cid:6)    
(cid:6)    

      

p(x) dx = 1

xp(x) dx =   

      

(x       )2p(x) dx =   2.

(cid:6)    

      

(1.105)

(1.106)

(1.107)

appendix e

appendix d

exercise 1.34

exercise 1.35

the constrained maximization can be performed using lagrange multipliers so that
we maximize the following functional with respect to p(x)
p(x) dx     1

p(x) ln p(x) dx +   1

   

(cid:16)

(cid:15)(cid:6)    
(cid:15)(cid:6)    

      

(cid:16)

(cid:16)

xp(x) dx       

+   3

(x       )2p(x) dx       2

.

(cid:6)    
(cid:15)(cid:6)    

      

+  2

      

using the calculus of variations, we set the derivative of this functional to zero giving

p(x) = exp

.

(1.108)

the lagrange multipliers can be found by back substitution of this result into the
three constraint equations, leading    nally to the result

(cid:27)

      

(cid:26)   1 +   1 +   2x +   3(x       )2
(cid:13)

(cid:12)
   (x       )2
2  2

(2    2)1/2 exp

1

p(x) =

(1.109)

and so the distribution that maximizes the differential id178 is the gaussian. note
that we did not constrain the distribution to be nonnegative when we maximized the
id178. however, because the resulting distribution is indeed nonnegative, we see
with hindsight that such a constraint is not necessary.

if we evaluate the differential id178 of the gaussian, we obtain

(cid:26)

h[x] =

1
2

(cid:27)

1 + ln(2    2)

.

(1.110)

thus we see again that the id178 increases as the distribution becomes broader,
i.e., as   2 increases. this result also shows that the differential id178, unlike the
discrete id178, can be negative, because h(x) < 0 in (1.110) for   2 < 1/(2  e).

suppose we have a joint distribution p(x, y) from which we draw pairs of values
of x and y. if a value of x is already known, then the additional information needed
to specify the corresponding value of y is given by     ln p(y|x). thus the average
additional information needed to specify y can be written as

(cid:6)(cid:6)

h[y|x] =    

p(y, x) ln p(y|x) dy dx

(1.111)

1.6. id205

55

exercise 1.37

which is called the conditional id178 of y given x. it is easily seen, using the
product rule, that the conditional id178 satis   es the relation

h[x, y] = h[y|x] + h[x]

(1.112)

where h[x, y] is the differential id178 of p(x, y) and h[x] is the differential en-
tropy of the marginal distribution p(x). thus the information needed to describe x
and y is given by the sum of the information needed to describe x alone plus the
additional information required to specify y given x.

1.6.1 relative id178 and mutual information
so far in this section, we have introduced a number of concepts from information
theory, including the key notion of id178. we now start to relate these ideas to
pattern recognition. consider some unknown distribution p(x), and suppose that
we have modelled this using an approximating distribution q(x). if we use q(x) to
construct a coding scheme for the purpose of transmitting values of x to a receiver,
then the average additional amount of information (in nats) required to specify the
value of x (assuming we choose an ef   cient coding scheme) as a result of using q(x)
instead of the true distribution p(x) is given by
p(x) ln q(x) dx    

kl(p(cid:6)q) =    

p(x) ln p(x) dx

(cid:15)

(cid:16)

(cid:6)

   

(cid:12)

(cid:13)

(cid:6)
(cid:6)

=    

p(x) ln

q(x)
p(x)

dx.

(1.113)

this is known as the relative id178 or id181, or kl diver-
gence (kullback and leibler, 1951), between the distributions p(x) and q(x). note
that it is not a symmetrical quantity, that is to say kl(p(cid:6)q) (cid:2)    kl(q(cid:6)p).
we now show that the id181 satis   es kl(p(cid:6)q) (cid:2) 0 with
equality if, and only if, p(x) = q(x). to do this we    rst introduce the concept of
convex functions. a function f(x) is said to be convex if it has the property that
every chord lies on or above the function, as shown in figure 1.31. any value of x
in the interval from x = a to x = b can be written in the form   a + (1       )b where
0 (cid:1)    (cid:1) 1. the corresponding point on the chord is given by   f(a) + (1       )f(b),

claude shannon
1916   2001

after graduating from michigan and
mit, shannon joined the at&t bell
telephone laboratories in 1941. his
paper    a mathematical theory of
communication    published in the
bell system technical journal in
1948 laid the foundations for modern information the-

ory. this paper introduced the word    bit   , and his con-
cept that information could be sent as a stream of 1s
and 0s paved the way for the communications revo-
lution.
it is said that von neumann recommended to
shannon that he use the term id178, not only be-
cause of its similarity to the quantity used in physics,
but also because    nobody knows what id178 really
is, so in any discussion you will always have an advan-
tage   .

56

1. introduction

figure 1.31 a convex function f (x) is one for which ev-
ery chord (shown in blue) lies on or above
the function (shown in red).

f(x)

chord

a

x  
x  

b

x

and the corresponding value of the function is f (  a + (1       )b). convexity then
implies

f(  a + (1       )b) (cid:1)   f(a) + (1       )f(b).

(1.114)

exercise 1.36

exercise 1.38

this is equivalent to the requirement that the second derivative of the function be
everywhere positive. examples of convex functions are x ln x (for x > 0) and x2. a
function is called strictly convex if the equality is satis   ed only for    = 0 and    = 1.
if a function has the opposite property, namely that every chord lies on or below the
function, it is called concave, with a corresponding de   nition for strictly concave. if
a function f(x) is convex, then    f(x) will be concave.

using the technique of proof by induction, we can show from (1.114) that a

convex function f(x) satis   es

(cid:22)
m(cid:2)

(cid:23)

m(cid:2)

f

  ixi

(cid:5)
i   i = 1, for any set of points {xi}. the result (1.115) is
where   i (cid:2) 0 and
known as jensen   s inequality. if we interpret the   i as the id203 distribution
over a discrete variable x taking the values {xi}, then (1.115) can be written

  if(xi)

(1.115)

(cid:1)

i=1

i=1

(1.116)
where e[  ] denotes the expectation. for continuous variables, jensen   s inequality
takes the form

(cid:15)(cid:6)

(cid:16)

(cid:6)

f (e[x]) (cid:1) e[f(x)]

f

xp(x) dx

f(x)p(x) dx.

(1.117)

we can apply jensen   s inequality in the form (1.117) to the kullback-leibler

divergence (1.113) to give

(cid:6)

(cid:12)

kl(p(cid:6)q) =    

p(x) ln

q(x)
p(x)

dx (cid:2)     ln

q(x) dx = 0

(1.118)

(cid:1)

(cid:13)

(cid:6)

(cid:28)

1.6. id205

57

where we have used the fact that     ln x is a convex function, together with the nor-
q(x) dx = 1. in fact,     ln x is a strictly convex function,
malization condition
so the equality will hold if, and only if, q(x) = p(x) for all x. thus we can in-
terpret the id181 as a measure of the dissimilarity of the two
distributions p(x) and q(x).

we see that there is an intimate relationship between data compression and den-
sity estimation (i.e., the problem of modelling an unknown id203 distribution)
because the most ef   cient compression is achieved when we know the true distri-
bution. if we use a distribution that is different from the true one, then we must
necessarily have a less ef   cient coding, and on average the additional information
that must be transmitted is (at least) equal to the id181 be-
tween the two distributions.

suppose that data is being generated from an unknown distribution p(x) that we
wish to model. we can try to approximate this distribution using some parametric
distribution q(x|  ), governed by a set of adjustable parameters   , for example a
multivariate gaussian. one way to determine    is to minimize the kullback-leibler
divergence between p(x) and q(x|  ) with respect to   . we cannot do this directly
because we don   t know p(x). suppose, however, that we have observed a    nite set
of training points xn, for n = 1, . . . , n, drawn from p(x). then the expectation
with respect to p(x) can be approximated by a    nite sum over these points, using
(1.35), so that

{    ln q(xn|  ) + ln p(xn)} .

(1.119)

kl(p(cid:6)q) (cid:8) n(cid:2)

n=1

the second term on the right-hand side of (1.119) is independent of   , and the    rst
term is the negative log likelihood function for    under the distribution q(x|  ) eval-
uated using the training set. thus we see that minimizing this kullback-leibler
divergence is equivalent to maximizing the likelihood function.

now consider the joint distribution between two sets of variables x and y given
by p(x, y). if the sets of variables are independent, then their joint distribution will
factorize into the product of their marginals p(x, y) = p(x)p(y). if the variables are
not independent, we can gain some idea of whether they are    close    to being indepen-
dent by considering the id181 between the joint distribution
and the product of the marginals, given by

i[x, y]     kl(p(x, y)(cid:6)p(x)p(y))

(cid:6)(cid:6)

(cid:15)

(cid:16)

=    

p(x, y) ln

p(x)p(y)
p(x, y)

dx dy

(1.120)

which is called the mutual information between the variables x and y. from the
properties of the id181, we see that i(x, y) (cid:2) 0 with equal-
ity if, and only if, x and y are independent. using the sum and product rules of
id203, we see that the mutual information is related to the conditional id178
through

i[x, y] = h[x]     h[x|y] = h[y]     h[y|x].

(1.121)

exercise 1.41

58

1. introduction

thus we can view the mutual information as the reduction in the uncertainty about x
by virtue of being told the value of y (or vice versa). from a bayesian perspective,
we can view p(x) as the prior distribution for x and p(x|y) as the posterior distribu-
tion after we have observed new data y. the mutual information therefore represents
the reduction in uncertainty about x as a consequence of the new observation y.

exercises

1.1 ((cid:1)) www consider the sum-of-squares error function given by (1.2) in which
the function y(x, w) is given by the polynomial (1.1). show that the coef   cients
w = {wi} that minimize this error function are given by the solution to the following
set of linear equations

m(cid:2)

where

n(cid:2)

n=1

aij =

aijwj = ti

j=0

(xn)i+j,

ti =

(1.122)

(xn)itn.

(1.123)

n(cid:2)

n=1

here a suf   x i or j denotes the index of a component, whereas (x)i denotes x raised
to the power of i.

1.2 ((cid:1)) write down the set of coupled linear equations, analogous to (1.122), satis   ed
by the coef   cients wi which minimize the regularized sum-of-squares error function
given by (1.4).

1.3 ((cid:1) (cid:1)) suppose that we have three coloured boxes r (red), b (blue), and g (green).
box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange,
and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. if a box is chosen
at random with probabilities p(r) = 0.2, p(b) = 0.2, p(g) = 0.6, and a piece of
fruit is removed from the box (with equal id203 of selecting any of the items in
the box), then what is the id203 of selecting an apple? if we observe that the
selected fruit is in fact an orange, what is the id203 that it came from the green
box?

1.4 ((cid:1) (cid:1)) www consider a id203 density px(x) de   ned over a continuous vari-
able x, and suppose that we make a nonlinear change of variable using x = g(y),
so that the density transforms according to (1.27). by differentiating (1.27), show

that the location(cid:1)y of the maximum of the density in y is not in general related to the
location(cid:1)x of the maximum of the density over x by the simple functional relation
(cid:1)x = g((cid:1)y) as a consequence of the jacobian factor. this shows that the maximum

of a id203 density (in contrast to a simple function) is dependent on the choice
of variable. verify that, in the case of a linear transformation, the location of the
maximum transforms in the same way as the variable itself.

1.5 ((cid:1)) using the de   nition (1.38) show that var[f(x)] satis   es (1.39).

(cid:16)

(1.124)

dx dy.

(1.125)

1.6 ((cid:1)) show that if two variables x and y are independent, then their covariance is

zero.

exercises

59

1.7 ((cid:1) (cid:1)) www in this exercise, we prove the id172 condition (1.48) for the

univariate gaussian. to do this consider, the integral

which we can evaluate by    rst writing its square in the form

(cid:16)

    1
2  2 x2

dx

(cid:15)

i =

(cid:6)    
(cid:6)    

      

exp

(cid:15)

(cid:6)    

i 2 =

exp

      

      

2  2 y2

2  2 x2     1
    1
(cid:11)1/2

(cid:10)

now make the transformation from cartesian coordinates (x, y) to polar coordinates
(r,   ) and then substitute u = r2. show that, by performing the integrals over    and
u, and then taking the square root of both sides, we obtain

(1.126)
finally, use this result to show that the gaussian distribution n (x|  ,   2) is normal-
ized.

.

i =

2    2

1.8 ((cid:1) (cid:1)) www by using a change of variables, verify that the univariate gaussian
distribution given by (1.46) satis   es (1.49). next, by differentiating both sides of the
id172 condition

(cid:6)    

      

n(cid:10)

(cid:11)

x|  ,   2

dx = 1

(1.127)

with respect to   2, verify that the gaussian satis   es (1.50). finally, show that (1.51)
holds.

1.9 ((cid:1)) www show that the mode (i.e. the maximum) of the gaussian distribution
(1.46) is given by   . similarly, show that the mode of the multivariate gaussian
(1.52) is given by   .

1.10 ((cid:1)) www suppose that the two variables x and z are statistically independent.

show that the mean and variance of their sum satis   es

e[x + z] = e[x] + e[z]

var[x + z] = var[x] + var[z].

(1.128)
(1.129)

1.11 ((cid:1)) by setting the derivatives of the log likelihood function (1.54) with respect to   

and   2 equal to zero, verify the results (1.55) and (1.56).

60

1. introduction

1.12 ((cid:1) (cid:1)) www using the results (1.49) and (1.50), show that

e[xnxm] =   2 + inm  2

(1.130)

where xn and xm denote data points sampled from a gaussian distribution with mean
   and variance   2, and inm satis   es inm = 1 if n = m and inm = 0 otherwise.
hence prove the results (1.57) and (1.58).

1.13 ((cid:1)) suppose that the variance of a gaussian is estimated using the result (1.56) but
with the maximum likelihood estimate   ml replaced with the true value    of the
mean. show that this estimator has the property that its expectation is given by the
true variance   2.

1.14 ((cid:1) (cid:1))

show that an arbitrary square matrix with elements wij can be written in
the form wij = ws
ij are symmetric and anti-symmetric
matrices, respectively, satisfying ws
ji for all i and j. now
consider the second order term in a higher order polynomial in d dimensions, given
by

ij =    wa

ij where ws

ij and wa

ji and wa

ij = ws

ij + wa

d(cid:2)

d(cid:2)

show that

d(cid:2)

d(cid:2)

wijxixj.

d(cid:2)

d(cid:2)

i=1

j=1

wijxixj =

(1.131)

ws

ijxixj

(1.132)

i=1

j=1

i=1

j=1

so that the contribution from the anti-symmetric matrix vanishes. we therefore see
that, without loss of generality, the matrix of coef   cients wij can be chosen to be
symmetric, and so not all of the d2 elements of this matrix can be chosen indepen-
dently. show that the number of independent parameters in the matrix ws
ij is given
by d(d + 1)/2.

1.15 ((cid:1) (cid:1) (cid:1)) www in this exercise and the next, we explore how the number of indepen-
dent parameters in a polynomial grows with the order m of the polynomial and with
the dimensionality d of the input space. we start by writing down the m th order
term for a polynomial in d dimensions in the form

d(cid:2)

d(cid:2)

       d(cid:2)

i1=1

i2=1

im =1

wi1i2      im xi1xi2        xim .

(1.133)

the coef   cients wi1i2      im comprise dm elements, but the number of independent
parameters is signi   cantly fewer due to the many interchange symmetries of the
factor xi1xi2        xim . begin by showing that the redundancy in the coef   cients can
be removed by rewriting this m th order term in the form

d(cid:2)

i1(cid:2)

       im   1(cid:2)

i1=1

i2=1

im =1

(cid:4)wi1i2      im xi1xi2        xim .

(1.134)

note that the precise relationship between the(cid:4)w coef   cients and w coef   cients need

exercises

61

not be made explicit. use this result to show that the number of independent param-
eters n(d, m), which appear at order m, satis   es the following recursion relation

d(cid:2)

i=1

n(d, m) =

n(i, m     1).

(i + m     2)!
(i     1)! (m     1)!

=

(d + m     1)!
(d     1)! m!

d(cid:2)

i=1

(1.135)

(1.136)

next use proof by induction to show that the following result holds

which can be done by    rst proving the result for d = 1 and arbitrary m by making
use of the result 0! = 1, then assuming it is correct for dimension d and verifying
that it is correct for dimension d + 1. finally, use the two previous results, together
with proof by induction, to show

(d + m     1)!
(d     1)! m! .

n(d, m) =

(1.137)
to do this,    rst show that the result is true for m = 2, and any value of d (cid:2) 1,
by comparison with the result of exercise 1.14. then make use of (1.135), together
with (1.136), to show that, if the result holds at order m     1, then it will also hold at
order m

1.16 ((cid:1) (cid:1) (cid:1)) in exercise 1.15, we proved the result (1.135) for the number of independent
parameters in the m th order term of a d-dimensional polynomial. we now    nd an
expression for the total number n(d, m) of independent parameters in all of the
terms up to and including the m6th order. first show that n(d, m) satis   es

m(cid:2)

n(d, m) =

n(d, m)

(1.138)

where n(d, m) is the number of independent parameters in the term of order m.
now make use of the result (1.137), together with proof by induction, to show that

m=0

n(d, m) =

(d + m)!

d! m!

.

(1.139)

this can be done by    rst proving that the result holds for m = 0 and arbitrary
d (cid:2) 1, then assuming that it holds at order m, and hence showing that it holds at
order m + 1. finally, make use of stirling   s approximation in the form

n! (cid:8) nne

   n

(1.140)
for large n to show that, for d (cid:10) m, the quantity n(d, m) grows like dm ,
and for m (cid:10) d it grows like m d. consider a cubic (m = 3) polynomial in d
dimensions, and evaluate numerically the total number of independent parameters
for (i) d = 10 and (ii) d = 100, which correspond to typical small-scale and
medium-scale machine learning applications.

62

1. introduction

1.17 ((cid:1) (cid:1)) www the gamma function is de   ned by
ux   1e

  (x)    

   u du.

(cid:6)    

(1.141)

using integration by parts, prove the relation   (x + 1) = x  (x). show also that
  (1) = 1 and hence that   (x + 1) = x! when x is an integer.

0

1.18 ((cid:1) (cid:1)) www we can use the result (1.126) to derive an expression for the surface
area sd, and the volume vd, of a sphere of unit radius in d dimensions. to do this,
consider the following result, which is obtained by transforming from cartesian to
polar coordinates

(cid:6)    

d(cid:14)

      

i=1

(cid:6)    

0

   x2
e

i dxi = sd

   r2
e

rd   1 dr.

(1.142)

using the de   nition (1.141) of the gamma function, together with (1.126), evaluate
both sides of this equation, and hence show that

sd =

2  d/2
  (d/2) .

(1.143)

next, by integrating with respect to radius from 0 to 1, show that the volume of the
unit sphere in d dimensions is given by

finally, use the results   (1) = 1 and   (3/2) =
(1.144) reduce to the usual expressions for d = 2 and d = 3.

vd = sd
d

.

   
  /2 to show that (1.143) and

(1.144)

1.19 ((cid:1) (cid:1)) consider a sphere of radius a in d-dimensions together with the concentric
hypercube of side 2a, so that the sphere touches the hypercube at the centres of each
of its sides. by using the results of exercise 1.18, show that the ratio of the volume
of the sphere to the volume of the cube is given by

volume of sphere
volume of cube

=

  d/2

d2d   1  (d/2) .

(1.145)

now make use of stirling   s formula in the form
  (x + 1) (cid:8) (2  )1/2e

   xxx+1/2

(1.146)
which is valid for x (cid:10) 1, to show that, as d        , the ratio (1.145) goes to zero.
show also that the ratio of the distance from the centre of the hypercube to one of
d, which
the corners, divided by the perpendicular distance to one of the sides, is
therefore goes to     as d        . from these results we see that, in a space of high
dimensionality, most of the volume of a cube is concentrated in the large number of
corners, which themselves become very long    spikes   !

   

exercises

63

1.20 ((cid:1) (cid:1)) www in this exercise, we explore the behaviour of the gaussian distribution
in high-dimensional spaces. consider a gaussian distribution in d dimensions given
by

1

p(x) =

(2    2)d/2 exp

.

(1.147)

(cid:16)

(cid:15)
   (cid:6)x(cid:6)2
2  2

(cid:15)

    r2
2  2

(2    2)d/2 exp

where sd is the surface area of a unit sphere in d dimensions. show that the function
d  . by considering

we wish to    nd the density with respect to radius in polar coordinates in which the
direction variables have been integrated out. to do this, show that the integral of
the id203 density over a thin shell of radius r and thickness  , where   (cid:12) 1, is
given by p(r)  where

(cid:16)
p(r) = sdrd   1
p(r) has a single stationary point located, for large d, at(cid:1)r (cid:8)    
p((cid:1)r +  ) where   (cid:12)(cid:1)r, show that for large d,
p((cid:1)r +  ) = p((cid:1)r) exp
which shows that(cid:1)r is a maximum of the radial id203 density and also that p(r)
decays exponentially away from its maximum at(cid:1)r with length scale   . we have
already seen that    (cid:12) (cid:1)r for large d, and so we see that most of the id203
density p(x) is larger at the origin than at the radius(cid:1)r by a factor of exp(d/2).

mass is concentrated in a thin shell at large radius. finally, show that the id203

    3 2
2  2

(1.148)

(1.149)

(cid:15)

(cid:16)

we therefore see that most of the id203 mass in a high-dimensional gaussian
distribution is located at a different radius from the region of high id203 density.
this property of distributions in spaces of high dimensionality will have important
consequences when we consider bayesian id136 of model parameters in later
chapters.

1.21 ((cid:1) (cid:1)) consider two nonnegative numbers a and b, and show that, if a (cid:1) b, then
a (cid:1) (ab)1/2. use this result to show that, if the decision regions of a two-class
classi   cation problem are chosen to minimize the id203 of misclassi   cation,
this id203 will satisfy

(cid:6)

p(mistake) (cid:1)

{p(x,c1)p(x,c2)}1/2 dx.

(1.150)

1.22 ((cid:1)) www given a loss matrix with elements lkj, the expected risk is minimized
if, for each x, we choose the class that minimizes (1.81). verify that, when the
loss matrix is given by lkj = 1     ikj, where ikj are the elements of the identity
matrix, this reduces to the criterion of choosing the class having the largest posterior
id203. what is the interpretation of this form of loss matrix?

1.23 ((cid:1)) derive the criterion for minimizing the expected loss when there is a general

loss matrix and general prior probabilities for the classes.

64

1. introduction

1.24 ((cid:1) (cid:1)) www consider a classi   cation problem in which the loss incurred when
an input vector from class ck is classi   ed as belonging to class cj is given by the
loss matrix lkj, and for which the loss incurred in selecting the reject option is   .
find the decision criterion that will give the minimum expected loss. verify that this
reduces to the reject criterion discussed in section 1.5.3 when the loss matrix is given
by lkj = 1     ikj. what is the relationship between    and the rejection threshold   ?
1.25 ((cid:1)) www consider the generalization of the squared id168 (1.87) for a
single target variable t to the case of multiple target variables described by the vector
t given by

(cid:6)(cid:6)

e[l(t, y(x))] =

(cid:6)y(x)     t(cid:6)2p(x, t) dx dt.

(1.151)

using the calculus of variations, show that the function y(x) for which this expected
loss is minimized is given by y(x) = et[t|x]. show that this result reduces to (1.89)
for the case of a single target variable t.

1.26 ((cid:1)) by expansion of the square in (1.151), derive a result analogous to (1.90) and
hence show that the function y(x) that minimizes the expected squared loss for the
case of a vector t of target variables is again given by the conditional expectation of
t.

1.27 ((cid:1) (cid:1)) www consider the expected loss for regression problems under the lq loss
function given by (1.91). write down the condition that y(x) must satisfy in order
to minimize e[lq]. show that, for q = 1, this solution represents the conditional
median, i.e., the function y(x) such that the id203 mass for t < y(x) is the
same as for t (cid:2) y(x). also show that the minimum expected lq loss for q     0 is
given by the conditional mode, i.e., by the function y(x) equal to the value of t that
maximizes p(t|x) for each x.

1.28 ((cid:1)) in section 1.6, we introduced the idea of id178 h(x) as the information gained
on observing the value of a random variable x having distribution p(x). we saw
that, for independent variables x and y for which p(x, y) = p(x)p(y), the id178
functions are additive, so that h(x, y) = h(x) + h(y). in this exercise, we derive the
relation between h and p in the form of a function h(p). first show that h(p2) =
2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer.
hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer. this
implies that h(px) = xh(p) where x is a positive rational number, and hence by
continuity when it is a positive real number. finally, show that this implies h(p)
must take the form h(p)     ln p.

1.29 ((cid:1)) www consider an m-state discrete random variable x, and use jensen   s in-
equality in the form (1.115) to show that the id178 of its distribution p(x) satis   es
h[x] (cid:1) ln m.

1.30 ((cid:1) (cid:1)) evaluate the id181 (1.113) between two gaussians

p(x) = n (x|  ,   2) and q(x) = n (x|m, s2).

table 1.3 the joint distribution p(x, y) for two binary variables

x and y used in exercise 1.39.

exercises

65

y

0
1/3
0

1
1/3
1/3

x

0
1

1.31 ((cid:1) (cid:1)) www consider two variables x and y having joint distribution p(x, y). show

that the differential id178 of this pair of variables satis   es

h[x, y] (cid:1) h[x] + h[y]

(1.152)

with equality if, and only if, x and y are statistically independent.

1.32 ((cid:1)) consider a vector x of continuous variables with distribution p(x) and corre-
sponding id178 h[x]. suppose that we make a nonsingular linear transformation
of x to obtain a new variable y = ax. show that the corresponding id178 is given
by h[y] = h[x] + ln|a| where |a| denotes the determinant of a.

1.33 ((cid:1) (cid:1)) suppose that the conditional id178 h[y|x] between two discrete random
variables x and y is zero. show that, for all values of x such that p(x) > 0, the
variable y must be a function of x, in other words for each x there is only one value
of y such that p(y|x) (cid:2)= 0.

1.34 ((cid:1) (cid:1)) www use the calculus of variations to show that the stationary point of the
functional (1.108) is given by (1.108). then use the constraints (1.105), (1.106),
and (1.107) to eliminate the lagrange multipliers and hence show that the maximum
id178 solution is given by the gaussian (1.109).

1.35 ((cid:1)) www use the results (1.106) and (1.107) to show that the id178 of the

univariate gaussian (1.109) is given by (1.110).

1.36 ((cid:1)) a strictly convex function is de   ned as one for which every chord lies above
the function. show that this is equivalent to the condition that the second derivative
of the function be positive.

1.37 ((cid:1)) using the de   nition (1.111) together with the product rule of id203, prove

the result (1.112).

1.38 ((cid:1) (cid:1)) www using proof by induction, show that the inequality (1.114) for convex

functions implies the result (1.115).

1.39 ((cid:1) (cid:1) (cid:1)) consider two binary variables x and y having the joint distribution given in

table 1.3.
evaluate the following quantities

(a) h[x]
(b) h[y]

(c) h[y|x]
(d) h[x|y]

(e) h[x, y]
(f) i[x, y].

draw a diagram to show the relationship between these various quantities.

66

1. introduction

1.40 ((cid:1)) by applying jensen   s inequality (1.115) with f(x) = ln x, show that the arith-

metic mean of a set of real numbers is never less than their geometrical mean.

1.41 ((cid:1)) www using the sum and product rules of id203, show that the mutual

information i(x, y) satis   es the relation (1.121).

2

id203
distributions

in chapter 1, we emphasized the central role played by id203 theory in the
solution of pattern recognition problems. we turn now to an exploration of some
particular examples of id203 distributions and their properties. as well as be-
ing of great interest in their own right, these distributions can form building blocks
for more complex models and will be used extensively throughout the book. the
distributions introduced in this chapter will also serve another important purpose,
namely to provide us with the opportunity to discuss some key statistical concepts,
such as bayesian id136, in the context of simple models before we encounter
them in more complex situations in later chapters.

one role for the distributions discussed in this chapter is to model the prob-
ability distribution p(x) of a random variable x, given a    nite set x1, . . . , xn of
observations. this problem is known as density estimation. for the purposes of
this chapter, we shall assume that the data points are independent and identically
distributed. it should be emphasized that the problem of density estimation is fun-

67

68

2. id203 distributions

damentally ill-posed, because there are in   nitely many id203 distributions that
could have given rise to the observed    nite data set. indeed, any distribution p(x)
that is nonzero at each of the data points x1, . . . , xn is a potential candidate. the
issue of choosing an appropriate distribution relates to the problem of model selec-
tion that has already been encountered in the context of polynomial curve    tting in
chapter 1 and that is a central issue in pattern recognition.

we begin by considering the binomial and multinomial distributions for discrete
random variables and the gaussian distribution for continuous random variables.
these are speci   c examples of parametric distributions, so-called because they are
governed by a small number of adaptive parameters, such as the mean and variance in
the case of a gaussian for example. to apply such models to the problem of density
estimation, we need a procedure for determining suitable values for the parameters,
given an observed data set. in a frequentist treatment, we choose speci   c values
for the parameters by optimizing some criterion, such as the likelihood function. by
contrast, in a bayesian treatment we introduce prior distributions over the parameters
and then use bayes    theorem to compute the corresponding posterior distribution
given the observed data.

we shall see that an important role is played by conjugate priors, that lead to
posterior distributions having the same functional form as the prior, and that there-
fore lead to a greatly simpli   ed bayesian analysis. for example, the conjugate prior
for the parameters of the multinomial distribution is called the dirichlet distribution,
while the conjugate prior for the mean of a gaussian is another gaussian. all of these
distributions are examples of the exponential family of distributions, which possess
a number of important properties, and which will be discussed in some detail.

one limitation of the parametric approach is that it assumes a speci   c functional
form for the distribution, which may turn out to be inappropriate for a particular
application. an alternative approach is given by nonparametric density estimation
methods in which the form of the distribution typically depends on the size of the data
set. such models still contain parameters, but these control the model complexity
rather than the form of the distribution. we end this chapter by considering three
nonparametric methods based respectively on histograms, nearest-neighbours, and
kernels.

2.1. binary variables

we begin by considering a single binary random variable x     {0, 1}. for example,
x might describe the outcome of    ipping a coin, with x = 1 representing    heads   ,
and x = 0 representing    tails   . we can imagine that this is a damaged coin so that
the id203 of landing heads is not necessarily the same as that of landing tails.
the id203 of x = 1 will be denoted by the parameter    so that

p(x = 1|  ) =   

(2.1)

2.1. binary variables

69

where 0 (cid:1)    (cid:1) 1, from which it follows that p(x = 0|  ) = 1       . the id203
distribution over x can therefore be written in the form
bern(x|  ) =   x(1       )1   x

(2.2)

exercise 2.1

which is known as the bernoulli distribution. it is easily veri   ed that this distribution
is normalized and that it has mean and variance given by

e[x] =   
var[x] =   (1       ).

(2.3)
(2.4)
now suppose we have a data set d = {x1, . . . , xn} of observed values of x.
we can construct the likelihood function, which is a function of   , on the assumption
that the observations are drawn independently from p(x|  ), so that

p(d|  ) =

p(xn|  ) =

  xn(1       )1   xn.

(2.5)

n(cid:14)

n(cid:14)

n=1

n=1

in a frequentist setting, we can estimate a value for    by maximizing the likelihood
function, or equivalently by maximizing the logarithm of the likelihood. in the case
of the bernoulli distribution, the log likelihood function is given by

ln p(d|  ) =

ln p(xn|  ) =

{xn ln    + (1     xn) ln(1       )} .

(2.6)

n(cid:2)

n=1

at this point, it is worth noting that the log likelihood function depends on the n
observations xn only through their sum
n xn. this sum provides an example of a
suf   cient statistic for the data under this distribution, and we shall study the impor-
tant role of suf   cient statistics in some detail. if we set the derivative of ln p(d|  )
with respect to    equal to zero, we obtain the maximum likelihood estimator

n(cid:2)
(cid:5)

n=1

n(cid:2)

  ml =

1
n

xn

n=1

(2.7)

section 2.4

jacob bernoulli
1654   1705

jacob bernoulli, also known as
jacques or james bernoulli, was a
swiss mathematician and was the
   rst of many in the bernoulli family
to pursue a career in science and
mathematics. although compelled
to study philosophy and theology against his will by
his parents, he travelled extensively after graduating
in order to meet with many of the leading scientists of

his time, including boyle and hooke in england. when
he returned to switzerland, he taught mechanics and
became professor of mathematics at basel in 1687.
unfortunately, rivalry between jacob and his younger
brother johann turned an initially productive collabora-
tion into a bitter and public dispute. jacob   s most sig-
ni   cant contributions to mathematics appeared in the
artofconjecture published in 1713, eight years after
his death, which deals with topics in id203 the-
ory including what has become known as the bernoulli
distribution.

70

2. id203 distributions

figure 2.1 histogram plot of the binomial dis-
tribution (2.9) as a function of m for
n = 10 and    = 0.25.

0.3

0.2

0.1

0

0

1

2

3

4

5
m

6

7

8

9

10

which is also known as the sample mean. if we denote the number of observations
of x = 1 (heads) within this data set by m, then we can write (2.7) in the form

  ml = m
n

(2.8)

so that the id203 of landing heads is given, in this maximum likelihood frame-
work, by the fraction of observations of heads in the data set.

now suppose we    ip a coin, say, 3 times and happen to observe 3 heads. then
n = m = 3 and   ml = 1. in this case, the maximum likelihood result would
predict that all future observations should give heads. common sense tells us that
this is unreasonable, and in fact this is an extreme example of the over-   tting associ-
ated with maximum likelihood. we shall see shortly how to arrive at more sensible
conclusions through the introduction of a prior distribution over   .

we can also work out the distribution of the number m of observations of x = 1,
given that the data set has size n. this is called the binomial distribution, and
from (2.5) we see that it is proportional to   m(1       )n   m. in order to obtain the
id172 coef   cient we note that out of n coin    ips, we have to add up all
of the possible ways of obtaining m heads, so that the binomial distribution can be
written

(cid:15)

(cid:16)

bin(m|n,   ) =

  m(1       )n   m

n
m

(cid:15)

(cid:16)

n
m

   

n!

(n     m)!m!

(2.9)

(2.10)

where

exercise 2.3

is the number of ways of choosing m objects out of a total of n identical objects.
figure 2.1 shows a plot of the binomial distribution for n = 10 and    = 0.25.

the mean and variance of the binomial distribution can be found by using the
result of exercise 1.10, which shows that for independent events the mean of the
sum is the sum of the means, and the variance of the sum is the sum of the variances.
because m = x1 + . . . + xn , and for each observation the mean and variance are

2.1. binary variables

71

given by (2.3) and (2.4), respectively, we have

e[m]     n(cid:2)

var[m]     n(cid:2)

mbin(m|n,   ) = n   

m=0

(m     e[m])2 bin(m|n,   ) = n   (1       ).

(2.11)

(2.12)

exercise 2.4

these results can also be proved directly using calculus.

m=0

2.1.1 the beta distribution
we have seen in (2.8) that the maximum likelihood setting for the parameter   
in the bernoulli distribution, and hence in the binomial distribution, is given by the
fraction of the observations in the data set having x = 1. as we have already noted,
this can give severely over-   tted results for small data sets. in order to develop a
bayesian treatment for this problem, we need to introduce a prior distribution p(  )
over the parameter   . here we consider a form of prior distribution that has a simple
interpretation as well as some useful analytical properties. to motivate this prior,
we note that the likelihood function takes the form of the product of factors of the
form   x(1       )1   x. if we choose a prior to be proportional to powers of    and
(1       ), then the posterior distribution, which is proportional to the product of the
prior and the likelihood function, will have the same functional form as the prior.
this property is called conjugacy and we will see several examples of it later in this
chapter. we therefore choose a prior, called the beta distribution, given by

exercise 2.5

beta(  |a, b) =

  (a + b)

  (a)  (b)   a   1(1       )b   1

(2.13)

where   (x) is the gamma function de   ned by (1.141), and the coef   cient in (2.13)
ensures that the beta distribution is normalized, so that
beta(  |a, b) d   = 1.

(2.14)

(cid:6) 1

exercise 2.6

the mean and variance of the beta distribution are given by

0

e[  ] =

var[  ] =

a

a + b
(a + b)2(a + b + 1) .

ab

(2.15)

(2.16)

the parameters a and b are often called hyperparameters because they control the
distribution of the parameter   . figure 2.2 shows plots of the beta distribution for
various values of the hyperparameters.

the posterior distribution of    is now obtained by multiplying the beta prior
(2.13) by the binomial likelihood function (2.9) and normalizing. keeping only the
factors that depend on   , we see that this posterior distribution has the form

p(  |m, l, a, b)       m+a   1(1       )l+b   1

(2.17)

72

2. id203 distributions

a = 0.1
b = 0.1

a = 2

b = 3

3

2

1

0

0

3

2

1

0

0

0.5

  

1

a = 1
b = 1

0

0.5

  

1

a = 8
b = 4

3

2

1

0

3

2

1

0.5

  

1

0

0

0.5

  

1

figure 2.2 plots of the beta distribution beta(  |a, b) given by (2.13) as a function of    for various values of the
hyperparameters a and b.

where l = n     m, and therefore corresponds to the number of    tails    in the coin
example. we see that (2.17) has the same functional dependence on    as the prior
distribution, re   ecting the conjugacy properties of the prior with respect to the like-
lihood function. indeed, it is simply another beta distribution, and its id172
coef   cient can therefore be obtained by comparison with (2.13) to give

p(  |m, l, a, b) =

  (m + a + l + b)

  (m + a)  (l + b)   m+a   1(1       )l+b   1.

(2.18)

we see that the effect of observing a data set of m observations of x = 1 and
l observations of x = 0 has been to increase the value of a by m, and the value of
b by l, in going from the prior distribution to the posterior distribution. this allows
us to provide a simple interpretation of the hyperparameters a and b in the prior as
an effective number of observations of x = 1 and x = 0, respectively. note that
a and b need not be integers. furthermore, the posterior distribution can act as the
prior if we subsequently observe additional data. to see this, we can imagine taking
observations one at a time and after each observation updating the current posterior

prior

2

1

0

0

2

1

0

0

0.5
  

1

likelihood function

0.5
  

1

2.1. binary variables

73

posterior

2

1

0

0

0.5
  

1

figure 2.3 illustration of one step of sequential bayesian id136. the prior is given by a beta distribution
with parameters a = 2, b = 2, and the likelihood function, given by (2.9) with n = m = 1, corresponds to a
single observation of x = 1, so that the posterior is given by a beta distribution with parameters a = 3, b = 2.

distribution by multiplying by the likelihood function for the new observation and
then normalizing to obtain the new, revised posterior distribution. at each stage, the
posterior is a beta distribution with some total number of (prior and actual) observed
values for x = 1 and x = 0 given by the parameters a and b. incorporation of an
additional observation of x = 1 simply corresponds to incrementing the value of a
by 1, whereas for an observation of x = 0 we increment b by 1. figure 2.3 illustrates
one step in this process.

section 2.3.5

(cid:6) 1

we see that this sequential approach to learning arises naturally when we adopt
a bayesian viewpoint. it is independent of the choice of prior and of the likelihood
function and depends only on the assumption of i.i.d. data. sequential methods make
use of observations one at a time, or in small batches, and then discard them before
the next observations are used. they can be used, for example, in real-time learning
scenarios where a steady stream of data is arriving, and predictions must be made
before all of the data is seen. because they do not require the whole data set to be
stored or loaded into memory, sequential methods are also useful for large data sets.
maximum likelihood methods can also be cast into a sequential framework.
if our goal is to predict, as best we can, the outcome of the next trial, then we
must evaluate the predictive distribution of x, given the observed data set d. from
the sum and product rules of id203, this takes the form

p(x = 1|d) =

p(x = 1|  )p(  |d) d   =

  p(  |d) d   = e[  |d].

0

(2.19)
using the result (2.18) for the posterior distribution p(  |d), together with the result
(2.15) for the mean of the beta distribution, we obtain
m + a

0

p(x = 1|d) =

(2.20)

m + a + l + b

which has a simple interpretation as the total fraction of observations (both real ob-
servations and    ctitious prior observations) that correspond to x = 1. note that in
the limit of an in   nitely large data set m, l         the result (2.20) reduces to the
maximum likelihood result (2.8). as we shall see, it is a very general property that
the bayesian and maximum likelihood results will agree in the limit of an in   nitely

(cid:6) 1

74

2. id203 distributions

exercise 2.7

exercise 2.8

large data set. for a    nite data set, the posterior mean for    always lies between the
prior mean and the maximum likelihood estimate for    corresponding to the relative
frequencies of events given by (2.7).

from figure 2.2, we see that as the number of observations increases, so the
posterior distribution becomes more sharply peaked. this can also be seen from
the result (2.16) for the variance of the beta distribution, in which we see that the
variance goes to zero for a         or b        . in fact, we might wonder whether it is
a general property of bayesian learning that, as we observe more and more data, the
uncertainty represented by the posterior distribution will steadily decrease.

to address this, we can take a frequentist view of bayesian learning and show
that, on average, such a property does indeed hold. consider a general bayesian
id136 problem for a parameter    for which we have observed a data set d, de-
scribed by the joint distribution p(  ,d). the following result

where

e  [  ]    
ed[e  [  |d]]    

e  [  ] = ed [e  [  |d]]

(cid:6)
(cid:6) (cid:12)(cid:6)

p(  )   d  

  p(  |d) d  

(2.21)

(2.22)

(2.23)

(cid:13)

p(d) dd

says that the posterior mean of   , averaged over the distribution generating the data,
is equal to the prior mean of   . similarly, we can show that

var  [  ] = ed [var  [  |d]] + vard [e  [  |d]] .

(2.24)

the term on the left-hand side of (2.24) is the prior variance of   . on the right-
hand side, the    rst term is the average posterior variance of   , and the second term
measures the variance in the posterior mean of   . because this variance is a positive
quantity, this result shows that, on average, the posterior variance of    is smaller than
the prior variance. the reduction in variance is greater if the variance in the posterior
mean is greater. note, however, that this result only holds on average, and that for a
particular observed data set it is possible for the posterior variance to be larger than
the prior variance.

2.2. multinomial variables

binary variables can be used to describe quantities that can take one of two possible
values. often, however, we encounter discrete variables that can take on one of k
possible mutually exclusive states. although there are various alternative ways to
express such variables, we shall see shortly that a particularly convenient represen-
tation is the 1-of-k scheme in which the variable is represented by a k-dimensional
vector x in which one of the elements xk equals 1, and all remaining elements equal

2.2. multinomial variables

75

(cid:2)
(cid:2)

x

n(cid:14)

k(cid:14)

n=1

k=1

k(cid:14)

k=1

k(cid:2)

k=1

k(cid:14)
(cid:2)

k=1

0. so, for instance if we have a variable that can take k = 6 states and a particular
observation of the variable happens to correspond to the state where x3 = 1, then x
will be represented by

note that such vectors satisfy
by the parameter   k, then the distribution of x is given

x = (0, 0, 1, 0, 0, 0)t.

(2.25)
k=1 xk = 1. if we denote the id203 of xk = 1

(cid:5)k

p(x|  ) =

  xk
k

(2.26)

(cid:5)

where    = (  1, . . . ,   k)t, and the parameters   k are constrained to satisfy   k (cid:2) 0
k   k = 1, because they represent probabilities. the distribution (2.26) can be
and
regarded as a generalization of the bernoulli distribution to more than two outcomes.
it is easily seen that the distribution is normalized

p(x|  ) =

  k = 1

(2.27)

and that

e[x|  ] =

(2.28)
now consider a data set d of n independent observations x1, . . . , xn . the

x

p(x|  )x = (  1, . . . ,   m )t =   .

corresponding likelihood function takes the form

p(d|  ) =

  xnk
k =

p

(
  
k

n xnk)

=

  mk
k .

(2.29)

k(cid:14)

k=1

we see that the likelihood function depends on the n data points only through the
k quantities

mk =

xnk

n

(2.30)

which represent the number of observations of xk = 1. these are called the suf   cient
statistics for this distribution.
in order to    nd the maximum likelihood solution for   , we need to maximize
ln p(d|  ) with respect to   k taking account of the constraint that the   k must sum
to one. this can be achieved using a lagrange multiplier    and maximizing

k(cid:2)

(cid:22)

k(cid:2)

(cid:23)

mk ln   k +   

  k     1

.

k=1

k=1

setting the derivative of (2.31) with respect to   k to zero, we obtain

  k =    mk/  .

(2.31)

(2.32)

section 2.4

appendix e

76

2. id203 distributions

(cid:5)
we can solve for the lagrange multiplier    by substituting (2.32) into the constraint
k   k = 1 to give    =    n. thus we obtain the maximum likelihood solution in

the form

k = mk
  ml
n

(2.33)

which is the fraction of the n observations for which xk = 1.

we can consider the joint distribution of the quantities m1, . . . , mk, conditioned
on the parameters    and on the total number n of observations. from (2.29) this
takes the form

(cid:15)

(cid:16) k(cid:14)

mult(m1, m2, . . . , mk|  , n) =

n

m1m2 . . . mk

k=1

  mk
k

(2.34)

which is known as the multinomial distribution. the id172 coef   cient is the
number of ways of partitioning n objects into k groups of size m1, . . . , mk and is
given by

(cid:15)

n

m1m2 . . . mk

n!

=

m1!m2! . . . mk! .

note that the variables mk are subject to the constraint

(cid:16)
k(cid:2)

k=1

mk = n.

(2.35)

(2.36)

2.2.1 the dirichlet distribution
we now introduce a family of prior distributions for the parameters {  k} of
the multinomial distribution (2.34). by inspection of the form of the multinomial
distribution, we see that the conjugate prior is given by

    k   1

k

(2.37)

p(  |  )     k(cid:14)

k=1

(cid:5)

where 0 (cid:1)   k (cid:1) 1 and
k   k = 1. here   1, . . . ,   k are the parameters of the
distribution, and    denotes (  1, . . . ,   k)t. note that, because of the summation
constraint, the distribution over the space of the {  k} is con   ned to a simplex of
dimensionality k     1, as illustrated for k = 3 in figure 2.4.

exercise 2.9

the normalized form for this distribution is by

dir(  |  ) =

  (  0)

  (  1)         (  k)

    k   1

k

(2.38)

k(cid:14)

k=1

which is called the dirichlet distribution. here   (x) is the gamma function de   ned
by (1.141) while

k(cid:2)

  0 =

  k.

k=1

(2.39)

2.2. multinomial variables

77

figure 2.4 the dirichlet distribution over three variables   1,   2,   3
is con   ned to a simplex (a bounded linear manifold) of
the form shown, as a consequence of the constraints
0 (cid:1)   k (cid:1) 1 and

k   k = 1.

p

  2

  3

  1

plots of the dirichlet distribution over the simplex, for various settings of the param-
eters   k, are shown in figure 2.5.
posterior distribution for the parameters {  k} in the form

multiplying the prior (2.38) by the likelihood function (2.34), we obtain the

p(  |d,   )     p(d|  )p(  |  )     k(cid:14)

k=1

    k+mk   1

k

.

(2.40)

we see that the posterior distribution again takes the form of a dirichlet distribution,
con   rming that the dirichlet is indeed a conjugate prior for the multinomial. this
allows us to determine the id172 coef   cient by comparison with (2.38) so
that

p(  |d,   ) = dir(  |   + m)

=

  (  0 + n)

  (  1 + m1)         (  k + mk)

k(cid:14)

k=1

    k+mk   1

k

(2.41)

where we have denoted m = (m1, . . . , mk)t. as for the case of the binomial
distribution with its beta prior, we can interpret the parameters   k of the dirichlet
prior as an effective number of observations of xk = 1.

note that two-state quantities can either be represented as binary variables and

lejeune dirichlet
1805   1859

johann peter gustav
lejeune
dirichlet was a modest and re-
served mathematician who made
contributions in number theory, me-
chanics, and astronomy, and who
gave the    rst rigorous analysis of
fourier series. his family originated from richelet
in belgium, and the name lejeune dirichlet comes

from    le jeune de richelet    (the young person from
richelet). dirichlet   s    rst paper, which was published
in 1825, brought him instant fame. it concerned fer-
mat   s last theorem, which claims that there are no
positive integer solutions to xn + yn = zn for n > 2.
dirichlet gave a partial proof for the case n = 5, which
was sent to legendre for review and who in turn com-
pleted the proof. later, dirichlet gave a complete proof
for n = 14, although a full proof of fermat   s last theo-
rem for arbitrary n had to wait until the work of andrew
wiles in the closing years of the 20th century.

78

2. id203 distributions

figure 2.5 plots of the dirichlet distribution over three variables, where the two horizontal axes are coordinates
in the plane of the simplex and the vertical axis corresponds to the value of the density. here {  k} = 0.1 on the
left plot, {  k} = 1 in the centre plot, and {  k} = 10 in the right plot.

modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled
using the multinomial distribution (2.34) with k = 2.

2.3. the gaussian distribution

the gaussian, also known as the normal distribution, is a widely used model for the
distribution of continuous variables. in the case of a single variable x, the gaussian
distribution can be written in the form
1

(cid:1)

(cid:2)

n (x|  ,   2) =

(2.42)

    1
2  2 (x       )2

n (x|  ,   ) =

where    is the mean and   2 is the variance. for a d-dimensional vector x, the
multivariate gaussian distribution takes the form
   1
2

(2.43)
where    is a d-dimensional mean vector,    is a d    d covariance matrix, and |  |
denotes the determinant of   .

(x       )t     1(x       )

|  |1/2 exp

(2  )d/2

1

1

(cid:2)

(2    2)1/2 exp
(cid:1)

section 1.6

exercise 2.14

the gaussian distribution arises in many different contexts and can be motivated
from a variety of different perspectives. for example, we have already seen that for
a single real variable, the distribution that maximizes the id178 is the gaussian.
this property applies also to the multivariate gaussian.

another situation in which the gaussian distribution arises is when we consider
the sum of multiple random variables. the central limit theorem (due to laplace)
tells us that, subject to certain mild conditions, the sum of a set of random variables,
which is of course itself a random variable, has a distribution that becomes increas-
ingly gaussian as the number of terms in the sum increases (walker, 1969). we can

2.3. the gaussian distribution

79

n = 1

3

2

1

0

0

n = 2

3

2

1

0

0

n = 10

3

2

1

0

0

0.5

1

0.5

1

0.5

1

figure 2.6 histogram plots of the mean of n uniformly distributed numbers for various values of n. we
observe that as n increases, the distribution tends towards a gaussian.

illustrate this by considering n variables x1, . . . , xn each of which has a uniform
distribution over the interval [0, 1] and then considering the distribution of the mean
(x1 +        + xn )/n. for large n, this distribution tends to a gaussian, as illustrated
in practice, the convergence to a gaussian as n increases can be
in figure 2.6.
very rapid. one consequence of this result is that the binomial distribution (2.9),
which is a distribution over m de   ned by the sum of n observations of the random
binary variable x, will tend to a gaussian as n         (see figure 2.1 for the case of
n = 10).

the gaussian distribution has many important analytical properties, and we shall
consider several of these in detail. as a result, this section will be rather more tech-
nically involved than some of the earlier sections, and will require familiarity with
various matrix identities. however, we strongly encourage the reader to become pro-
   cient in manipulating gaussian distributions using the techniques presented here as
this will prove invaluable in understanding the more complex models presented in
later chapters.

we begin by considering the geometrical form of the gaussian distribution. the

appendix c

carl friedrich gauss
1777   1855

it
is said that when gauss went
to elementary school at age 7, his
teacher b  uttner, trying to keep the
class occupied, asked the pupils to
sum the integers from 1 to 100. to
the teacher   s amazement, gauss
arrived at the answer in a matter of moments by noting
that the sum can be represented as 50 pairs (1 + 100,
2+99, etc.) each of which added to 101, giving the an-
swer 5,050. it is now believed that the problem which
was actually set was of the same form but somewhat
harder in that the sequence had a larger starting value
and a larger increment. gauss was a german math-

ematician and scientist with a reputation for being a
hard-working perfectionist. one of his many contribu-
tions was to show that least squares can be derived
under the assumption of normally distributed errors.
he also created an early formulation of non-euclidean
geometry (a self-consistent geometrical theory that vi-
olates the axioms of euclid) but was reluctant to dis-
cuss it openly for fear that his reputation might suffer
if it were seen that he believed in such a geometry.
at one point, gauss was asked to conduct a geodetic
survey of the state of hanover, which led to his for-
mulation of the normal distribution, now also known
as the gaussian. after his death, a study of his di-
aries revealed that he had discovered several impor-
tant mathematical results years or even decades be-
fore they were published by others.

80

2. id203 distributions

functional dependence of the gaussian on x is through the quadratic form

   2 = (x       )t  

   1(x       )

(2.44)

which appears in the exponent. the quantity     is called the mahalanobis distance
from    to x and reduces to the euclidean distance when    is the identity matrix. the
gaussian distribution will be constant on surfaces in x-space for which this quadratic
form is constant.

first of all, we note that the matrix    can be taken to be symmetric, without
loss of generality, because any antisymmetric component would disappear from the
exponent. now consider the eigenvector equation for the covariance matrix

  ui =   iui

(2.45)

exercise 2.17

exercise 2.18

where i = 1, . . . , d. because    is a real, symmetric matrix its eigenvalues will be
real, and its eigenvectors can be chosen to form an orthonormal set, so that

where iij is the i, j element of the identity matrix and satis   es

ut
i uj = iij

(cid:12)

iij =

if i = j

1,
0, otherwise.

exercise 2.19

the covariance matrix    can be expressed as an expansion in terms of its eigenvec-
tors in the form

d(cid:2)

i=1

d(cid:2)

i=1

   =

  iuiut
i

   1 =
  

1
  i

uiut
i .

and similarly the inverse covariance matrix   

   1 can be expressed as

substituting (2.49) into (2.44), the quadratic form becomes

d(cid:2)

i=1

y2
i
  i

   2 =

where we have de   ned
(2.51)
we can interpret {yi} as a new coordinate system de   ned by the orthonormal vectors
ui that are shifted and rotated with respect to the original xi coordinates. forming
the vector y = (y1, . . . , yd)t, we have

i (x       ).

yi = ut

y = u(x       )

(2.52)

(2.46)

(2.47)

(2.48)

(2.49)

(2.50)

2.3. the gaussian distribution

81

x2

figure 2.7 the red curve shows the ellip-
tical surface of constant proba-
bility density for a gaussian in
a two-dimensional space x =
(x1, x2) on which the density
is exp(   1/2) of
its value at
x =   . the major axes of
the ellipse are de   ned by the
eigenvectors ui of the covari-
ance matrix, with correspond-
ing eigenvalues   i.

1/2
2

  

u2

  

y2

y1

u1

1/2
1

  

x1

appendix c

where u is a matrix whose rows are given by ut
i . from (2.46) it follows that u is
an orthogonal matrix, i.e., it satis   es uut = i, and hence also utu = i, where i
is the identity matrix.

the quadratic form, and hence the gaussian density, will be constant on surfaces
if all of the eigenvalues   i are positive, then these
for which (2.51) is constant.
surfaces represent ellipsoids, with their centres at    and their axes oriented along ui,
and with scaling factors in the directions of the axes given by   
, as illustrated in
figure 2.7.

1/2
i

for the gaussian distribution to be well de   ned, it is necessary for all of the
eigenvalues   i of the covariance matrix to be strictly positive, otherwise the dis-
tribution cannot be properly normalized. a matrix whose eigenvalues are strictly
positive is said to be positive de   nite. in chapter 12, we will encounter gaussian
distributions for which one or more of the eigenvalues are zero, in which case the
distribution is singular and is con   ned to a subspace of lower dimensionality. if all
of the eigenvalues are nonnegative, then the covariance matrix is said to be positive
semide   nite.

now consider the form of the gaussian distribution in the new coordinate system
de   ned by the yi. in going from the x to the y coordinate system, we have a jacobian
matrix j with elements given by

jij =    xi
   yj

= uji

(2.53)

where uji are the elements of the matrix ut. using the orthonormality property of
the matrix u, we see that the square of the determinant of the jacobian matrix is

(cid:7)(cid:7)ut

(cid:7)(cid:7)2 =

(cid:7)(cid:7)ut

(cid:7)(cid:7)|u| =

(cid:7)(cid:7)utu

(cid:7)(cid:7) = |i| = 1

(2.54)
and hence |j| = 1. also, the determinant |  | of the covariance matrix can be written

|j|2 =

82

2. id203 distributions

as the product of its eigenvalues, and hence

d(cid:14)

j=1

|  |1/2 =

1/2
j

.

  

thus in the yj coordinate system, the gaussian distribution takes the form

d(cid:14)

j=1

p(y) = p(x)|j| =

1

(2    j)1/2 exp

    y2
j
2  j

(cid:12)

(cid:13)

(2.55)

(2.56)

which is the product of d independent univariate gaussian distributions. the eigen-
vectors therefore de   ne a new set of shifted and rotated coordinates with respect
to which the joint id203 distribution factorizes into a product of independent
distributions. the integral of the distribution in the y coordinate system is then

(cid:12)

(cid:13)

(cid:6)

p(y) dy =

1

(2    j)1/2 exp

    y2
j
2  j

dyj = 1

(2.57)

(cid:6)    

d(cid:14)

      

j=1

where we have used the result (1.48) for the id172 of the univariate gaussian.
this con   rms that the multivariate gaussian (2.43) is indeed normalized.

we now look at the moments of the gaussian distribution and thereby provide an
interpretation of the parameters    and   . the expectation of x under the gaussian
distribution is given by

(cid:13)

exp

(x       )t  

   1(x       )

x dx

e[x] =

1

(2  )d/2

1

|  |1/2

1

1

(cid:6)
(cid:6)

(cid:12)
(cid:12)

   1
2
   1
2

(cid:13)

   1z

=

exp

|  |1/2

(2  )d/2

(2.58)
where we have changed variables using z = x       . we now note that the exponent
is an even function of the components of z and, because the integrals over these are
taken over the range (      ,   ), the term in z in the factor (z +   ) will vanish by
symmetry. thus

(z +   ) dz

zt  

e[x] =   

(2.59)

and so we refer to    as the mean of the gaussian distribution.

we now consider second order moments of the gaussian. in the univariate case,
we considered the second order moment given by e[x2]. for the multivariate gaus-
sian, there are d2 second order moments given by e[xixj], which we can group
together to form the matrix e[xxt]. this matrix can be written as

(cid:6)

(cid:12)

(cid:13)

e[xxt] =

1

(cid:6)

|  |1/2

1

(2  )d/2
1
1

=

(2  )d/2

|  |1/2

exp

(cid:12)

exp

   1
2

(cid:13)

zt  

   1z

   1
2

(x       )t  

   1(x       )

xxt dx

(z +   )(z +   )t dz

2.3. the gaussian distribution

83

where again we have changed variables using z = x       . note that the cross-terms
involving   zt and   tz will again vanish by symmetry. the term     t is constant
and can be taken outside the integral, which itself is unity because the gaussian
distribution is normalized. consider the term involving zzt. again, we can make
use of the eigenvector expansion of the covariance matrix given by (2.45), together
with the completeness of the set of eigenvectors, to write

z =

yjuj

(2.60)

(cid:6)

(cid:12)
d(cid:2)

   1
2

d(cid:2)

(cid:13)
(cid:6)

exp

zt  

   1z

zzt dz

1

|  |1/2

uiut
j

exp

i=1

j=1

k=1

(cid:24)

    d(cid:2)

(cid:25)

y2
k
2  k

yiyj dy

where yj = ut

j z, which gives

1

(2  )d/2

1

|  |1/2
1

(2  )d/2

d(cid:2)

=

=

d(cid:2)

j=1

uiut

i   i =   

(2.61)

i=1

where we have made use of the eigenvector equation (2.45), together with the fact
that the integral on the right-hand side of the middle line vanishes by symmetry
unless i = j, and in the    nal line we have made use of the results (1.50) and (2.55),
together with (2.48). thus we have

e[xxt] =     t +   .

(2.62)

for single random variables, we subtracted the mean before taking second mo-
ments in order to de   ne a variance. similarly, in the multivariate case it is again
convenient to subtract off the mean, giving rise to the covariance of a random vector
x de   ned by

cov[x] = e

(2.63)
for the speci   c case of a gaussian distribution, we can make use of e[x] =   ,
together with the result (2.62), to give

(x     e[x])(x     e[x])t

(cid:8)

(cid:9)

.

cov[x] =   .

(2.64)

because the parameter matrix    governs the covariance of x under the gaussian
distribution, it is called the covariance matrix.

although the gaussian distribution (2.43) is widely used as a density model, it
suffers from some signi   cant limitations. consider the number of free parameters in
the distribution. a general symmetric covariance matrix    will have d(d + 1)/2
independent parameters, and there are another d independent parameters in   , giv-
ing d(d + 3)/2 parameters in total. for large d, the total number of parameters

exercise 2.21

84

2. id203 distributions

figure 2.8 contours of constant
id203 density for a gaussian
distribution in two dimensions in
which the covariance matrix is (a) of
general form, (b) diagonal, in which
the elliptical contours are aligned
with the coordinate axes, and (c)
proportional to the identity matrix, in
which the contours are concentric
circles.

x2

x2

x2

x1

x1

x1

(a)

(b)

(c)

therefore grows quadratically with d, and the computational task of manipulating
and inverting large matrices can become prohibitive. one way to address this prob-
lem is to use restricted forms of the covariance matrix. if we consider covariance
matrices that are diagonal, so that    = diag(  2
i ), we then have a total of 2d inde-
pendent parameters in the density model. the corresponding contours of constant
density are given by axis-aligned ellipsoids. we could further restrict the covariance
matrix to be proportional to the identity matrix,    =   2i, known as an isotropic co-
variance, giving d + 1 independent parameters in the model and spherical surfaces
of constant density. the three possibilities of general, diagonal, and isotropic covari-
ance matrices are illustrated in figure 2.8. unfortunately, whereas such approaches
limit the number of degrees of freedom in the distribution and make inversion of the
covariance matrix a much faster operation, they also greatly restrict the form of the
id203 density and limit its ability to capture interesting correlations in the data.
a further limitation of the gaussian distribution is that it is intrinsically uni-
modal (i.e., has a single maximum) and so is unable to provide a good approximation
to multimodal distributions. thus the gaussian distribution can be both too    exible,
in the sense of having too many parameters, while also being too limited in the range
of distributions that it can adequately represent. we will see later that the introduc-
tion of latent variables, also called hidden variables or unobserved variables, allows
both of these problems to be addressed. in particular, a rich family of multimodal
distributions is obtained by introducing discrete latent variables leading to mixtures
of gaussians, as discussed in section 2.3.9. similarly, the introduction of continuous
latent variables, as described in chapter 12, leads to models in which the number of
free parameters can be controlled independently of the dimensionality d of the data
space while still allowing the model to capture the dominant correlations in the data
set. indeed, these two approaches can be combined and further extended to derive
a very rich set of id187 that can be adapted to a broad range of prac-
tical applications. for instance, the gaussian version of the markov random    eld,
which is widely used as a probabilistic model of images, is a gaussian distribution
over the joint space of pixel intensities but rendered tractable through the imposition
of considerable structure re   ecting the spatial organization of the pixels. similarly,
the linear dynamical system, used to model time series data for applications such
as tracking, is also a joint gaussian distribution over a potentially large number of
observed and latent variables and again is tractable due to the structure imposed on
the distribution. a powerful framework for expressing the form and properties of

section 8.3

section 13.3

2.3. the gaussian distribution

85

such complex distributions is that of probabilistic id114, which will form
the subject of chapter 8.

2.3.1 conditional gaussian distributions
an important property of the multivariate gaussian distribution is that if two
sets of variables are jointly gaussian, then the conditional distribution of one set
conditioned on the other is again gaussian. similarly, the marginal distribution of
either set is also gaussian.
consider    rst the case of conditional distributions. suppose x is a d-dimensional
vector with gaussian distribution n (x|  ,   ) and that we partition x into two dis-
joint subsets xa and xb. without loss of generality, we can take xa to form the    rst
m components of x, with xb comprising the remaining d     m components, so that

we also de   ne corresponding partitions of the mean vector    given by

and of the covariance matrix    given by

x =

xa
xb

(cid:15)
(cid:15)

(cid:16)
(cid:16)

.

  a
  b

   =

(cid:15)

   =

  aa   ab
  ba   bb

(cid:16)

.

(cid:16)

note that the symmetry   t =    of the covariance matrix implies that   aa and   bb
are symmetric, while   ba =   t
ab.

in many situations, it will be convenient to work with the inverse of the covari-

ance matrix

         

   1

(2.68)

which is known as the precision matrix. in fact, we shall see that some properties
of gaussian distributions are most naturally expressed in terms of the covariance,
whereas others take a simpler form when viewed in terms of the precision. we
therefore also introduce the partitioned form of the precision matrix

(cid:15)

   =

  aa   ab
  ba   bb

exercise 2.22

corresponding to the partitioning (2.65) of the vector x. because the inverse of a
symmetric matrix is also symmetric, we see that   aa and   bb are symmetric, while
  t
ab =   ba. it should be stressed at this point that, for instance,   aa is not simply
given by the inverse of   aa. in fact, we shall shortly examine the relation between
the inverse of a partitioned matrix and the inverses of its partitions.
let us begin by    nding an expression for the conditional distribution p(xa|xb).
from the product rule of id203, we see that this conditional distribution can be

(2.65)

(2.66)

(2.67)

(2.69)

86

2. id203 distributions

evaluated from the joint distribution p(x) = p(xa, xb) simply by    xing xb to the
observed value and normalizing the resulting expression to obtain a valid id203
distribution over xa.
instead of performing this id172 explicitly, we can
obtain the solution more ef   ciently by considering the quadratic form in the exponent
of the gaussian distribution given by (2.44) and then reinstating the id172
coef   cient at the end of the calculation. if we make use of the partitioning (2.65),
(2.66), and (2.69), we obtain

   1(x       ) =

   1
2

(x       )t  
   1
2
   1
2

(xa       a)t  ab(xb       b)
(xa       a)t  aa(xa       a)     1
2
(xb       b)t  bb(xb       b).
(xb       b)t  ba(xa       a)     1
2

(2.70)

we see that as a function of xa, this is again a quadratic form, and hence the cor-
responding conditional distribution p(xa|xb) will be gaussian. because this distri-
bution is completely characterized by its mean and its covariance, our goal will be
to identify expressions for the mean and covariance of p(xa|xb) by inspection of
(2.70).

this is an example of a rather common operation associated with gaussian
distributions, sometimes called    completing the square   , in which we are given a
quadratic form de   ning the exponent terms in a gaussian distribution, and we need
to determine the corresponding mean and covariance. such problems can be solved
straightforwardly by noting that the exponent in a general gaussian distribution
n (x|  ,   ) can be written

   1
2

(x       )t  

   1(x       ) =    1
2

xt  

   1x + xt  

   1   + const

(2.71)

   1 and the coef   cient of the linear term in x to   

where    const    denotes terms which are independent of x, and we have made use of
the symmetry of   . thus if we take our general quadratic form and express it in
the form given by the right-hand side of (2.71), then we can immediately equate the
matrix of coef   cients entering the second order term in x to the inverse covariance
   1  , from which we can
matrix   
obtain   .
now let us apply this procedure to the conditional gaussian distribution p(xa|xb)
for which the quadratic form in the exponent is given by (2.70). we will denote the
mean and covariance of this distribution by   a|b and   a|b, respectively. consider
the functional dependence of (2.70) on xa in which xb is regarded as a constant. if
we pick out all terms that are second order in xa, we have

   1
2

xt
a   aaxa

(2.72)

from which we can immediately conclude that the covariance (inverse precision) of
p(xa|xb) is given by

  a|b =   

   1
aa .

(2.73)

2.3. the gaussian distribution

87

now consider all of the terms in (2.70) that are linear in xa
a {  aa  a       ab(xb       b)}
xt

(2.74)

where we have used   t
the coef   cient of xa in this expression must equal   

ba =   ab. from our discussion of the general form (2.71),

   1
a|b  a|b and hence

  a|b =   a|b {  aa  a       ab(xb       b)}

=   a       

aa   ab(xb       b)
   1

(2.75)

where we have made use of (2.73).

the results (2.73) and (2.75) are expressed in terms of the partitioned precision
matrix of the original joint distribution p(xa, xb). we can also express these results
in terms of the corresponding partitioned covariance matrix. to do this, we make use
of the following identity for the inverse of a partitioned matrix
   mbd   1

(cid:16)   1

(cid:15)

(cid:15)

(cid:16)

m

=

   d   1cm d   1 + d   1cmbd   1

a b
c d

(2.76)

exercise 2.24

where we have de   ned

(2.77)
the quantity m   1 is known as the schur complement of the matrix on the left-hand
side of (2.76) with respect to the submatrix d. using the de   nition

m = (a     bd   1c)   1.

(cid:16)   1

(cid:15)

(cid:16)

(cid:15)

  aa   ab
  ba   bb

=

  aa   ab
  ba   bb

and making use of (2.76), we have

  aa = (  aa       ab  
   1
bb   ba)   1
  ab =    (  aa       ab  
   1
bb   ba)   1  ab  

   1
bb .

(2.78)

(2.79)
(2.80)

from these we obtain the following expressions for the mean and covariance of the
conditional distribution p(xa|xb)

bb (xb       b)
   1
  a|b =   a +   ab  
  a|b =   aa       ab  
   1
bb   ba.

(2.81)
(2.82)
comparing (2.73) and (2.82), we see that the conditional distribution p(xa|xb) takes
a simpler form when expressed in terms of the partitioned precision matrix than
when it is expressed in terms of the partitioned covariance matrix. note that the
mean of the conditional distribution p(xa|xb), given by (2.81), is a linear function of
xb and that the covariance, given by (2.82), is independent of xa. this represents an
example of a linear-gaussian model.

section 8.1.4

88

2. id203 distributions

2.3.2 marginal gaussian distributions
we have seen that if a joint distribution p(xa, xb) is gaussian, then the condi-
tional distribution p(xa|xb) will again be gaussian. now we turn to a discussion of
the marginal distribution given by

(cid:6)

p(xa) =

p(xa, xb) dxb

(2.83)

which, as we shall see, is also gaussian. once again, our strategy for evaluating this
distribution ef   ciently will be to focus on the quadratic form in the exponent of the
joint distribution and thereby to identify the mean and covariance of the marginal
distribution p(xa).

the quadratic form for the joint distribution can be expressed, using the par-
titioned precision matrix, in the form (2.70). because our goal is to integrate out
xb, this is most easily achieved by    rst considering the terms involving xb and then
completing the square in order to facilitate integration. picking out just those terms
that involve xb, we have
   1
2

bb m)t  bb(xb     
   1

b m =    1
2

   1
bb m (2.84)

b   bbxb+xt
xt

   1
bb m)+

(xb     

mt  

1
2

where we have de   ned

m =   bb  b       ba(xa       a).

(2.85)

we see that the dependence on xb has been cast into the standard quadratic form of a
gaussian distribution corresponding to the    rst term on the right-hand side of (2.84),
plus a term that does not depend on xb (but that does depend on xa). thus, when
we take the exponential of this quadratic form, we see that the integration over xb
required by (2.83) will take the form
(xb       

bb m)t  bb(xb       
   1

   1
bb m)

(cid:12)

(cid:13)

(2.86)

dxb.

(cid:6)

exp

   1
2

this integration is easily performed by noting that it is the integral over an unnor-
malized gaussian, and so the result will be the reciprocal of the id172 co-
ef   cient. we know from the form of the normalized gaussian given by (2.43), that
this coef   cient is independent of the mean and depends only on the determinant of
the covariance matrix. thus, by completing the square with respect to xb, we can
integrate out xb and the only term remaining from the contributions on the left-hand
side of (2.84) that depends on xa is the last term on the right-hand side of (2.84) in
which m is given by (2.85). combining this term with the remaining terms from

2.3. the gaussian distribution

89

(2.70) that depend on xa, we obtain

1
2

[  bb  b       ba(xa       a)]t   

bb [  bb  b       ba(xa       a)]
   1

a (  aa  a +   ab  b) + const

   1
xt
a   aaxa + xt
2
a (  aa       ab  
=    1
   1
xt
bb   ba)xa
2
a (  aa       ab  
   1
bb   ba)   1  a + const
+xt

(2.87)

where    const    denotes quantities independent of xa. again, by comparison with
(2.71), we see that the covariance of the marginal distribution of p(xa) is given by

  a = (  aa       ab  

   1
bb   ba)   1.

similarly, the mean is given by

  a(  aa       ab  

   1
bb   ba)  a =   a

where we have used (2.88). the covariance in (2.88) is expressed in terms of the
partitioned precision matrix given by (2.69). we can rewrite this in terms of the
corresponding partitioning of the covariance matrix given by (2.67), as we did for
the conditional distribution. these partitioned matrices are related by

(2.88)

(2.89)

(2.90)

(cid:16)   1

  aa   ab
  ba   bb

=

(cid:15)
(cid:10)

  aa   ab
  ba   bb

(cid:16)
(cid:15)
(cid:11)   1 =   aa.

making use of (2.76), we then have

  aa       ab  

   1
bb   ba

(2.91)
thus we obtain the intuitively satisfying result that the marginal distribution p(xa)
has mean and covariance given by

e[xa] =   a

cov[xa] =   aa.

(2.92)
(2.93)

we see that for a marginal distribution, the mean and covariance are most simply ex-
pressed in terms of the partitioned covariance matrix, in contrast to the conditional
distribution for which the partitioned precision matrix gives rise to simpler expres-
sions.

our results for the marginal and conditional distributions of a partitioned gaus-

sian are summarized below.

partitioned gaussians
given a joint gaussian distribution n (x|  ,   ) with          

   1 and

(cid:15)

(cid:16)

xa
xb

(cid:15)

(cid:16)

  a
  b

x =

,

   =

(2.94)

90

2. id203 distributions

1

xb

0.5

xb = 0.7

p(xa|xb = 0.7)

10

5

p(xa, xb)

p(xa)

0

0

0.5

xa

1

0

0

0.5

xa

1

figure 2.9 the plot on the left shows the contours of a gaussian distribution p(xa, xb) over two variables, and
the plot on the right shows the marginal distribution p(xa) (blue curve) and the conditional distribution p(xa|xb)
for xb = 0.7 (red curve).

(cid:15)

(cid:16)

(cid:15)

(cid:16)

   =

  aa   ab
  ba   bb

,    =

  aa   ab
  ba   bb

conditional distribution:

p(xa|xb) = n (x|  a|b,   

   1
aa )

  a|b =   a       

aa   ab(xb       b).
   1

marginal distribution:

p(xa) = n (xa|  a,   aa).

.

(2.95)

(2.96)
(2.97)

(2.98)

we illustrate the idea of conditional and marginal distributions associated with

a multivariate gaussian using an example involving two variables in figure 2.9.

2.3.3 bayes    theorem for gaussian variables
in sections 2.3.1 and 2.3.2, we considered a gaussian p(x) in which we parti-
tioned the vector x into two subvectors x = (xa, xb) and then found expressions for
the conditional distribution p(xa|xb) and the marginal distribution p(xa). we noted
that the mean of the conditional distribution p(xa|xb) was a linear function of xb.
here we shall suppose that we are given a gaussian marginal distribution p(x) and a
gaussian conditional distribution p(y|x) in which p(y|x) has a mean that is a linear
function of x, and a covariance which is independent of x. this is an example of

2.3. the gaussian distribution

91

a linear gaussian model (roweis and ghahramani, 1999), which we shall study in
greater generality in section 8.1.4. we wish to    nd the marginal distribution p(y)
and the conditional distribution p(x|y). this is a problem that will arise frequently
in subsequent chapters, and it will prove convenient to derive the general results here.

we shall take the marginal and conditional distributions to be

p(x) = n(cid:10)
p(y|x) = n(cid:10)

(cid:11)

(cid:11)

   1

x|  ,   
y|ax + b, l   1

where   , a, and b are parameters governing the means, and    and l are precision
matrices. if x has dimensionality m and y has dimensionality d, then the matrix a
has size d    m.

first we    nd an expression for the joint distribution over x and y. to do this, we

de   ne

z =

(2.99)
(2.100)

(2.101)

(cid:15)

(cid:16)

x
y

and then consider the log of the joint distribution

ln p(z) = ln p(x) + ln p(y|x)

=    1
2
   1
2

(x       )t  (x       )
(y     ax     b)tl(y     ax     b) + const

(2.102)

where    const    denotes terms independent of x and y. as before, we see that this is a
quadratic function of the components of z, and hence p(z) is gaussian distribution.
to    nd the precision of this gaussian, we consider the second order terms in (2.102),
which can be written as

(cid:15)

(cid:16)t(cid:15)

xt(   + atla)x     1
   1
2
2
=    1
   la
2

x
y

ytly +

ytlax +

   + atla    atl

1
2

l

1
xtatly
2
=    1
2

ztrz

(2.103)

and so the gaussian distribution over z has precision (inverse covariance) matrix
given by

r =

   + atla    atl

   la

l

.

(2.104)

(cid:16)

(cid:16)(cid:15)
(cid:16)

x
y

(cid:15)

(cid:15)

exercise 2.29

the covariance matrix is found by taking the inverse of the precision, which can be
done using the matrix inversion formula (2.76) to give

cov[z] = r   1 =

   1
   1at
   1 l   1 + a  

  

  
a  

   1at

.

(2.105)

(cid:16)

92

2. id203 distributions

similarly, we can    nd the mean of the gaussian distribution over z by identify-

ing the linear terms in (2.102), which are given by

xt         xtatlb + ytlb =

         atlb

lb

.

(2.106)

(cid:16)

using our earlier result (2.71) obtained by completing the square over the quadratic
form of a multivariate gaussian, we    nd that the mean of z is given by

(cid:15)

(cid:16)t(cid:15)

x
y

(cid:16)

         atlb

lb

(cid:15)
(cid:15)

e[z] = r   1

exercise 2.30

making use of (2.105), we then obtain

e[z] =

(cid:16)

.

.

(2.107)

(2.108)

  

a   + b

section 2.3

section 2.3

next we    nd an expression for the marginal distribution p(y) in which we have
marginalized over x. recall that the marginal distribution over a subset of the com-
ponents of a gaussian random vector takes a particularly simple form when ex-
pressed in terms of the partitioned covariance matrix. speci   cally, its mean and
covariance are given by (2.92) and (2.93), respectively. making use of (2.105) and
(2.108) we see that the mean and covariance of the marginal distribution p(y) are
given by

e[y] = a   + b
cov[y] = l   1 + a  

   1at.

(2.109)
(2.110)

a special case of this result is when a = i, in which case it reduces to the convolu-
tion of two gaussians, for which we see that the mean of the convolution is the sum
of the mean of the two gaussians, and the covariance of the convolution is the sum
of their covariances.
finally, we seek an expression for the conditional p(x|y). recall that the results
for the conditional distribution are most easily expressed in terms of the partitioned
precision matrix, using (2.73) and (2.75). applying these results to (2.105) and
(2.108) we see that the conditional distribution p(x|y) has mean and covariance
given by

e[x|y] = (   + atla)   1
cov[x|y] = (   + atla)   1.

atl(y     b) +     

(2.111)
(2.112)

(cid:26)

(cid:27)

the evaluation of this conditional can be seen as an example of bayes    theorem.
we can interpret the distribution p(x) as a prior distribution over x. if the variable
y is observed, then the conditional distribution p(x|y) represents the corresponding
posterior distribution over x. having found the marginal and conditional distribu-
tions, we effectively expressed the joint distribution p(z) = p(x)p(y|x) in the form
p(x|y)p(y). these results are summarized below.

2.3. the gaussian distribution

93

marginal and conditional gaussians

given a marginal gaussian distribution for x and a conditional gaussian distri-
bution for y given x in the form

p(x) = n (x|  ,   
p(y|x) = n (y|ax + b, l   1)

   1)

(2.113)
(2.114)

the marginal distribution of y and the conditional distribution of x given y are
given by

p(y) = n (y|a   + b, l   1 + a  
   1at)
p(x|y) = n (x|  {atl(y     b) +     },   )

where

   = (   + atla)   1.

(2.115)
(2.116)

(2.117)

2.3.4 maximum likelihood for the gaussian
given a data set x = (x1, . . . , xn )t in which the observations {xn} are as-
sumed to be drawn independently from a multivariate gaussian distribution, we can
estimate the parameters of the distribution by maximum likelihood. the log likeli-
hood function is given by

ln p(x|  ,   ) =     n d
2

ln(2  )    n
2

ln|  |   1
2

(xn     )t  

   1(xn     ). (2.118)

by simple rearrangement, we see that the likelihood function depends on the data set
only through the two quantities

n(cid:2)

n=1

n(cid:2)

n=1

n=1

xnxt
n.

(2.119)

n(cid:2)

xn,

appendix c

these are known as the suf   cient statistics for the gaussian distribution. using
(c.19), the derivative of the log likelihood with respect to    is given by

ln p(x|  ,   ) =

   
     

   1(xn       )
  

(2.120)

and setting this derivative to zero, we obtain the solution for the maximum likelihood
estimate of the mean given by

  ml =

1
n

xn

(2.121)

n(cid:2)

n=1

n(cid:2)

n=1

94

2. id203 distributions

exercise 2.34

which is the mean of the observed set of data points. the maximization of (2.118)
with respect to    is rather more involved. the simplest approach is to ignore the
symmetry constraint and show that the resulting solution is symmetric as required.
alternative derivations of this result, which impose the symmetry and positive de   -
niteness constraints explicitly, can be found in magnus and neudecker (1999). the
result is as expected and takes the form

n(cid:2)

n=1

  ml =

1
n

(xn       ml)(xn       ml)t

(2.122)

which involves   ml because this is the result of a joint maximization with respect
to    and   . note that the solution (2.121) for   ml does not depend on   ml, and so
we can    rst evaluate   ml and then use this to evaluate   ml.

if we evaluate the expectations of the maximum likelihood solutions under the

exercise 2.35

true distribution, we obtain the following results

n(cid:2)

(cid:4)   =

e[  ml] =   

e[  ml] = n     1
this bias by de   ning a different estimator(cid:4)   given by

we see that the expectation of the maximum likelihood estimate for the mean is equal
to the true mean. however, the maximum likelihood estimate for the covariance has
an expectation that is less than the true value, and hence it is biased. we can correct

  .

n

(2.123)

(2.124)

clearly from (2.122) and (2.124), the expectation of(cid:4)   is equal to   .

n=1

(xn       ml)(xn       ml)t.

1

n     1

(2.125)

2.3.5 sequential estimation
our discussion of the maximum likelihood solution for the parameters of a gaus-
sian distribution provides a convenient opportunity to give a more general discussion
of the topic of sequential estimation for maximum likelihood. sequential methods
allow data points to be processed one at a time and then discarded and are important
for on-line applications, and also where large data sets are involved so that batch
processing of all data points at once is infeasible.

consider the result (2.121) for the maximum likelihood estimator of the mean
ml when it is based on n observations. if we

  ml, which we will denote by   (n )

2.3. the gaussian distribution

95

figure 2.10 a schematic illustration of two correlated ran-
dom variables z and   ,
together with the
regression function f (  ) given by the con-
ditional expectation e[z|  ].
the robbins-
monro algorithm provides a general sequen-
tial procedure for    nding the root   (cid:1) of such
functions.

z

f(  )

  

  (cid:1)

dissect out the contribution from the    nal data point xn , we obtain

n(cid:2)

  (n )

ml =

=

1
n

n=1

xn

1
n

xn +

n   1(cid:2)
1
n
xn + n     1
1
n
ml +

n=1

=
=   (n   1)

xn

  (n   1)

ml

n
(xn       (n   1)
1
n

).

(2.126)
this result has a nice interpretation, as follows. after observing n     1 data points
we have estimated    by   (n   1)
. we now observe data point xn , and we obtain our
revised estimate   (n )
ml by moving the old estimate a small amount, proportional to
1/n, in the direction of the    error signal    (xn       (n   1)
). note that, as n increases,
so the contribution from successive data points gets smaller.

ml

ml

ml

the result (2.126) will clearly give the same answer as the batch result (2.121)
because the two formulae are equivalent. however, we will not always be able to de-
rive a sequential algorithm by this route, and so we seek a more general formulation
of sequential learning, which leads us to the robbins-monro algorithm. consider a
pair of random variables    and z governed by a joint distribution p(z,   ). the con-
ditional expectation of z given    de   nes a deterministic function f(  ) that is given
by

(cid:6)

f(  )     e[z|  ] =

zp(z|  ) dz

(2.127)

and is illustrated schematically in figure 2.10. functions de   ned in this way are
called regression functions.

our goal is to    nd the root   (cid:1) at which f(  (cid:1)) = 0. if we had a large data set
of observations of z and   , then we could model the regression function directly and
then obtain an estimate of its root. suppose, however, that we observe values of
z one at a time and we wish to    nd a corresponding sequential estimation scheme
for   (cid:1). the following general procedure for solving such problems was given by

96

2. id203 distributions

(cid:8)

(cid:9)

robbins and monro (1951). we shall assume that the conditional variance of z is
   nite so that

(2.128)
and we shall also, without loss of generality, consider the case where f(  ) > 0 for
   >   (cid:1) and f(  ) < 0 for    <   (cid:1), as is the case in figure 2.10. the robbins-monro
procedure then de   nes a sequence of successive estimates of the root   (cid:1) given by

e

(z     f)2 |   

<    

  (n ) =   (n   1) + an   1z(  (n   1))

(2.129)

where z(  (n )) is an observed value of z when    takes the value   (n ). the coef   cients
{an} represent a sequence of positive numbers that satisfy the conditions

n       an = 0
lim
an =    

   (cid:2)
   (cid:2)

n =1

n <    .
a2

(2.130)

(2.131)

(2.132)

n =1

it can then be shown (robbins and monro, 1951; fukunaga, 1990) that the sequence
of estimates given by (2.129) does indeed converge to the root with id203 one.
note that the    rst condition (2.130) ensures that the successive corrections decrease
in magnitude so that the process can converge to a limiting value. the second con-
dition (2.131) is required to ensure that the algorithm does not converge short of the
root, and the third condition (2.132) is needed to ensure that the accumulated noise
has    nite variance and hence does not spoil convergence.

now let us consider how a general maximum likelihood problem can be solved
sequentially using the robbins-monro algorithm. by de   nition, the maximum like-
lihood solution   ml is a stationary point of the log likelihood function and hence
satis   es

(cid:24)

(2.133)
exchanging the derivative and the summation, and taking the limit n         we have

  ml

n=1

= 0.

   
     

1
n

ln p(xn|  )

(cid:25)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)

n(cid:2)

(cid:29)

(cid:30)

lim
n      

1
n

ln p(xn|  ) = ex

   
     

   
     

ln p(x|  )

(2.134)

n(cid:2)

n=1

and so we see that    nding the maximum likelihood solution corresponds to    nd-
ing the root of a regression function. we can therefore apply the robbins-monro
procedure, which now takes the form

  (n ) =   (n   1) + an   1

   

     (n   1) ln p(xn|  (n   1)).

(2.135)

2.3. the gaussian distribution

97

z

figure 2.11 in the case of a gaussian distribution, with   
corresponding to the mean   , the regression
function illustrated in figure 2.10 takes the form
of a straight line, as shown in red.
in this
case, the random variable z corresponds to the
derivative of the log likelihood function and is
given by (x       ml)/  2, and its expectation that
de   nes the regression function is a straight line
given by (         ml)/  2. the root of the regres-
sion function corresponds to the maximum like-
lihood estimator   ml.

  ml

p(z|  )

  

as a speci   c example, we consider once again the sequential estimation of the
mean of a gaussian distribution, in which case the parameter   (n ) is the estimate
(n )
ml of the mean of the gaussian, and the random variable z is given by
  

1

  2 (x       ml).

z =    

ln p(x|  ml,   2) =

     ml

(2.136)
thus the distribution of z is gaussian with mean          ml, as illustrated in fig-
ure 2.11. substituting (2.136) into (2.135), we obtain the univariate form of (2.126),
provided we choose the coef   cients an to have the form an =   2/n. note that
although we have focussed on the case of a single variable, the same technique,
together with the same restrictions (2.130)   (2.132) on the coef   cients an , apply
equally to the multivariate case (blum, 1965).

2.3.6 bayesian id136 for the gaussian
the maximum likelihood framework gave point estimates for the parameters   
and   . now we develop a bayesian treatment by introducing prior distributions
over these parameters. let us begin with a simple example in which we consider a
single gaussian random variable x. we shall suppose that the variance   2 is known,
and we consider the task of inferring the mean    given a set of n observations
x = {x1, . . . , xn}. the likelihood function, that is the id203 of the observed
data given   , viewed as a function of   , is given by

(cid:24)

n(cid:2)

(cid:25)

1

p(x|  ) =

p(xn|  ) =

(2.137)
again we emphasize that the likelihood function p(x|  ) is not a id203 distri-
bution over    and is not normalized.

(2    2)n/2 exp

n=1

n=1

.

we see that the likelihood function takes the form of the exponential of a quad-
ratic form in   . thus if we choose a prior p(  ) given by a gaussian, it will be a

(xn       )2

    1
2  2

n(cid:14)

98

2. id203 distributions

conjugate distribution for this likelihood function because the corresponding poste-
rior will be a product of two exponentials of quadratic functions of    and hence will
also be gaussian. we therefore take our prior distribution to be

p(  ) = n(cid:10)

  |  0,   2

0

(cid:11)

and the posterior distribution is given by

p(  |x)     p(x|  )p(  ).

exercise 2.38

simple manipulation involving completing the square in the exponent shows that the
posterior distribution is given by

p(  |x) = n(cid:10)

  |  n ,   2

n

(cid:11)

(2.138)

(2.139)

(2.140)

(2.141)

(2.142)

where

  n =

1
  2
n

=

0

  2

0 +   2   0 + n   2
+ n
  2

n   2

n   2
1
  2
0

0 +   2   ml

in which   ml is the maximum likelihood solution for    given by the sample mean

n(cid:2)

  ml =

1
n

xn.

n=1

(2.143)

it is worth spending a moment studying the form of the posterior mean and
variance. first of all, we note that the mean of the posterior distribution given by
(2.141) is a compromise between the prior mean   0 and the maximum likelihood
solution   ml. if the number of observed data points n = 0, then (2.141) reduces
to the prior mean as expected. for n        , the posterior mean is given by the
maximum likelihood solution. similarly, consider the result (2.142) for the variance
of the posterior distribution. we see that this is most naturally expressed in terms
of the inverse variance, which is called the precision. furthermore, the precisions
are additive, so that the precision of the posterior is given by the precision of the
prior plus one contribution of the data precision from each of the observed data
points. as we increase the number of observed data points, the precision steadily
increases, corresponding to a posterior distribution with steadily decreasing variance.
with no observed data points, we have the prior variance, whereas if the number of
data points n        , the variance   2
n goes to zero and the posterior distribution
becomes in   nitely peaked around the maximum likelihood solution. we therefore
see that the maximum likelihood result of a point estimate for    given by (2.143) is
recovered precisely from the bayesian formalism in the limit of an in   nite number
0         in which the
of observations. note also that for    nite n, if we take the limit   2
prior has in   nite variance then the posterior mean (2.141) reduces to the maximum
n =   2/n.
likelihood result, while from (2.142) the posterior variance is given by   2

2.3. the gaussian distribution

99

figure 2.12 illustration of bayesian id136 for
the mean    of a gaussian distri-
bution, in which the variance is as-
sumed to be known. the curves
show the prior distribution over   
(the curve labelled n = 0), which
in this case is itself gaussian, along
with the posterior distribution given
by (2.140) for increasing numbers n
of data points. the data points are
generated from a gaussian of mean
0.8 and variance 0.1, and the prior is
chosen to have mean 0. in both the
prior and the likelihood function, the
variance is set to the true value.

5

0
   1

n = 0

n = 10

n = 2

n = 1

0

1

exercise 2.40

section 2.3.5

we illustrate our analysis of bayesian id136 for the mean of a gaussian
distribution in figure 2.12. the generalization of this result to the case of a d-
dimensional gaussian random variable x with known covariance and unknown mean
is straightforward.

we have already seen how the maximum likelihood expression for the mean of
a gaussian can be re-cast as a sequential update formula in which the mean after
observing n data points was expressed in terms of the mean after observing n     1
data points together with the contribution from data point xn . in fact, the bayesian
paradigm leads very naturally to a sequential view of the id136 problem. to see
this in the context of the id136 of the mean of a gaussian, we write the posterior
distribution with the contribution from the    nal data point xn separated out so that

(cid:31)

n   1(cid:14)

 

p(  |d)    

p(  )

p(xn|  )

p(xn|  ).

(2.144)

n=1

the term in square brackets is (up to a id172 coef   cient) just the posterior
distribution after observing n     1 data points. we see that this can be viewed as
a prior distribution, which is combined using bayes    theorem with the likelihood
function associated with data point xn to arrive at the posterior distribution after
observing n data points. this sequential view of bayesian id136 is very general
and applies to any problem in which the observed data are assumed to be independent
and identically distributed.

so far, we have assumed that the variance of the gaussian distribution over the
data is known and our goal is to infer the mean. now let us suppose that the mean
is known and we wish to infer the variance. again, our calculations will be greatly
simpli   ed if we choose a conjugate form for the prior distribution. it turns out to be
most convenient to work with the precision        1/  2. the likelihood function for   
takes the form
p(x|  ) =

   1)       n/2 exp

n (xn|  ,   

(xn       )2

n(cid:2)

n(cid:14)

(cid:24)

(cid:25)

(2.145)

.

      
2

n=1

n=1

100

2. id203 distributions

2

1

0

0

a = 0.1
b = 0.1

2

1

a = 1
b = 1

2

1

a = 4
b = 6

  

1

0

0

2

  

1

0

0

2

  

1

2

figure 2.13 plot of the gamma distribution gam(  |a, b) de   ned by (2.146) for various values of the parameters
a and b.

the corresponding conjugate prior should therefore be proportional to the product
of a power of    and the exponential of a linear function of   . this corresponds to
the gamma distribution which is de   ned by

gam(  |a, b) =

1

  (a) ba  a   1 exp(   b  ).

(2.146)

exercise 2.41

exercise 2.42

here   (a) is the gamma function that is de   ned by (1.141) and that ensures that
(2.146) is correctly normalized. the gamma distribution has a    nite integral if a > 0,
and the distribution itself is    nite if a (cid:2) 1. it is plotted, for various values of a and
b, in figure 2.13. the mean and variance of the gamma distribution are given by

(2.147)

e[  ] = a
b
var[  ] = a
b2 .

(cid:24)

n(cid:2)

n=1

(2.148)
consider a prior distribution gam(  |a0, b0). if we multiply by the likelihood

function (2.145), then we obtain a posterior distribution

(cid:25)

p(  |x)       a0   1  n/2 exp

   b0         
2

(xn       )2

(2.149)

which we recognize as a gamma distribution of the form gam(  |an , bn) where

an = a0 + n
2
1
2

bn = b0 +

n(cid:2)

n=1

(xn       )2 = b0 + n

2   2

ml

(2.150)

(2.151)

where   2
ml is the maximum likelihood estimator of the variance. note that in (2.149)
there is no need to keep track of the id172 constants in the prior and the
likelihood function because, if required, the correct coef   cient can be found at the
end using the normalized form (2.146) for the gamma distribution.

2.3. the gaussian distribution

101

from (2.150), we see that the effect of observing n data points is to increase
the value of the coef   cient a by n/2. thus we can interpret the parameter a0 in
the prior in terms of 2a0    effective    prior observations. similarly, from (2.151) we
see that the n data points contribute n   2
ml is
the variance, and so we can interpret the parameter b0 in the prior as arising from
the 2a0    effective    prior observations having variance 2b0/(2a0) = b0/a0. recall
that we made an analogous interpretation for the dirichlet prior. these distributions
are examples of the exponential family, and we shall see that the interpretation of
a conjugate prior in terms of effective    ctitious data points is a general one for the
exponential family of distributions.

ml/2 to the parameter b, where   2

section 2.2

instead of working with the precision, we can consider the variance itself. the
conjugate prior in this case is called the inverse gamma distribution, although we
shall not discuss this further because we will    nd it more convenient to work with
the precision.

now suppose that both the mean and the precision are unknown. to    nd a

conjugate prior, we consider the dependence of the likelihood function on    and   

(cid:13)

n(cid:14)

n=1

(cid:15)
(cid:15)

(cid:12)

(cid:16)1/2
(cid:16)(cid:30)n

exp

      
2

(cid:24)

p(x|  ,   ) =

(cid:29)

   

  1/2 exp

(xn       )2

n(cid:2)

n=1

xn       
2

(cid:25)

x2
n

.

(2.152)

n(cid:2)

n=1

exp

    

we now wish to identify a prior distribution p(  ,   ) that has the same functional
dependence on    and    as the likelihood function and that should therefore take the
form

  
2  

        2
2

(cid:15)

p(  ,   )    

  1/2 exp

(cid:29)
(cid:12)

(cid:16)(cid:30)  
(cid:13)

        2
2
(       c/  )2

        
2

    /2 exp

= exp

(2.153)
where c, d, and    are constants. since we can always write p(  ,   ) = p(  |  )p(  ),
we can    nd p(  |  ) and p(  ) by inspection. in particular, we see that p(  |  ) is a
gaussian whose precision is a linear function of    and that p(  ) is a gamma distri-
bution, so that the normalized prior takes the form

  

   

d     c2
2  

p(  ,   ) = n (  |  0, (    )   1)gam(  |a, b)

(2.154)

where we have de   ned new constants given by   0 = c/  , a = 1 +   /2, b =
d   c2/2  . the distribution (2.154) is called the normal-gamma or gaussian-gamma
distribution and is plotted in figure 2.14. note that this is not simply the product
of an independent gaussian prior over    and a gamma prior over   , because the
precision of    is a linear function of   . even if we chose a prior in which    and   
were independent, the posterior distribution would exhibit a coupling between the
precision of    and the value of   .

exp{c         d  }

(cid:12)

(cid:15)

(cid:16)

(cid:13)

102

2. id203 distributions

figure 2.14 contour plot of the normal-gamma
distribution (2.154) for parameter
values   0 = 0,    = 2, a = 5 and
b = 6.

2

  

1

exercise 2.45

0
   2

in the case of the multivariate gaussian distribution n(cid:10)

0
  

(cid:11)

x|  ,   

   1

for a d-
dimensional variable x, the conjugate prior distribution for the mean   , assuming
the precision is known, is again a gaussian. for known mean and unknown precision
matrix   , the conjugate prior is the wishart distribution given by

2

w(  |w,   ) = b|  |(     d   1)/2 exp

(2.155)
where    is called the number of degrees of freedom of the distribution, w is a d  d
scale matrix, and tr(  ) denotes the trace. the id172 constant b is given by

(cid:15)

(cid:16)
   1
2 tr(w   1  )
(cid:15)

   + 1     i

d(cid:14)

i=1

2

(cid:16)(cid:23)   1

b(w,   ) = |w|     /2

2  d/2   d(d   1)/4

  

.

(2.156)

(cid:22)

again, it is also possible to de   ne a conjugate prior over the covariance matrix itself,
rather than over the precision matrix, which leads to the inverse wishart distribu-
tion, although we shall not discuss this further. if both the mean and the precision
are unknown, then, following a similar line of reasoning to the univariate case, the
conjugate prior is given by

p(  ,   |  0,   , w,   ) = n (  |  0, (    )   1)w(  |w,   )

(2.157)

which is known as the normal-wishart or gaussian-wishart distribution.

2.3.7 student   s t-distribution
we have seen that the conjugate prior for the precision of a gaussian is given
by a gamma distribution. if we have a univariate gaussian n (x|  ,   
   1) together
with a gamma prior gam(  |a, b) and we integrate out the precision, we obtain the
marginal distribution of x in the form

section 2.3.6

exercise 2.46

figure 2.15 plot of student   s t-distribution (2.159)
for    = 0 and    = 1 for various values
of   . the limit            corresponds
to a gaussian distribution with mean
   and precision   .

2.3. the gaussian distribution

103

0.5

0.4

0.3

0.2

0.1

0
   5

          
   = 1.0
   = 0.1

0

5

(cid:6)    
(cid:6)    

0

0
ba
  (a)

p(x|  , a, b) =

=

=

n (x|  ,   
bae(   b   )   a   1

(cid:16)1/2(cid:29)

  (a)
1
2  

(cid:15)

   1)gam(  |a, b) d  

(cid:17)

(cid:18)1/2

exp

  
2  
(x       )2

b +

2

(cid:19)
(cid:30)   a   1/2

      
2

(cid:20)

(2.158)

d  

(x       )2

  (a + 1/2)

where we have made the change of variable z =   [b + (x       )2/2]. by convention
(cid:15)
we de   ne new parameters given by    = 2a and    = a/b, in terms of which the
distribution p(x|  , a, b) takes the form

(cid:30)     /2   1/2

(cid:16)1/2(cid:29)

st(x|  ,   ,   ) =

  (  /2 + 1/2)

  (  /2)

  
    

1 +   (x       )2

  

(2.159)

which is known as student   s t-distribution. the parameter    is sometimes called the
precision of the t-distribution, even though it is not in general equal to the inverse
of the variance. the parameter    is called the degrees of freedom, and its effect is
illustrated in figure 2.15. for the particular case of    = 1, the t-distribution reduces
to the cauchy distribution, while in the limit            the t-distribution st(x|  ,   ,   )
becomes a gaussian n (x|  ,   

   1) with mean    and precision   .

from (2.158), we see that student   s t-distribution is obtained by adding up an
in   nite number of gaussian distributions having the same mean but different preci-
sions. this can be interpreted as an in   nite mixture of gaussians (gaussian mixtures
will be discussed in detail in section 2.3.9. the result is a distribution that in gen-
eral has longer    tails    than a gaussian, as was seen in figure 2.15. this gives the t-
distribution an important property called robustness, which means that it is much less
sensitive than the gaussian to the presence of a few data points which are outliers.
the robustness of the t-distribution is illustrated in figure 2.16, which compares the
maximum likelihood solutions for a gaussian and a t-distribution. note that the max-
imum likelihood solution for the t-distribution can be found using the expectation-
maximization (em) algorithm. here we see that the effect of a small number of

exercise 2.47

exercise 12.24

104

2. id203 distributions

0.5

0.4

0.3

0.2

0.1

0
   5

0.5

0.4

0.3

0.2

0.1

0
   5

0

5

10

(a)

0

5

10

(b)

figure 2.16 illustration of the robustness of student   s t-distribution compared to a gaussian. (a) histogram
distribution of 30 data points drawn from a gaussian distribution, together with the maximum likelihood    t ob-
tained from a t-distribution (red curve) and a gaussian (green curve, largely hidden by the red curve). because
the t-distribution contains the gaussian as a special case it gives almost the same solution as the gaussian.
(b) the same data set but with three additional outlying data points showing how the gaussian (green curve) is
strongly distorted by the outliers, whereas the t-distribution (red curve) is relatively unaffected.

outliers is much less signi   cant for the t-distribution than for the gaussian. outliers
can arise in practical applications either because the process that generates the data
corresponds to a distribution having a heavy tail or simply through mislabelled data.
robustness is also an important property for regression problems. unsurprisingly,
the least squares approach to regression does not exhibit robustness, because it cor-
responds to maximum likelihood under a (conditional) gaussian distribution. by
basing a regression model on a heavy-tailed distribution such as a t-distribution, we
obtain a more robust model.

if we go back to (2.158) and substitute the alternative parameters    = 2a,    =

a/b, and    =    b/a, we see that the t-distribution can be written in the form

st(x|  ,   ,   ) =

x|  , (    )   1

gam(  |  /2,   /2) d  .

(2.160)
we can then generalize this to a multivariate gaussian n (x|  ,   ) to obtain the cor-
responding multivariate student   s t-distribution in the form

(cid:11)

(cid:6)    

n(cid:10)
(cid:6)    

0

st(x|  ,   ,   ) =

n (x|  , (    )   1)gam(  |  /2,   /2) d  .

(2.161)

exercise 2.48

using the same technique as for the univariate case, we can evaluate this integral to
give

0

2.3. the gaussian distribution

(cid:29)

(cid:30)   d/2     /2

st(x|  ,   ,   ) =

  (d/2 +   /2)

  (  /2)

|  |1/2
(    )d/2

1 +

   2
  

105

(2.162)

(2.163)

where d is the dimensionality of x, and    2 is the squared mahalanobis distance
de   ned by

   2 = (x       )t  (x       ).

exercise 2.49

this is the multivariate form of student   s t-distribution and satis   es the following
properties

e[x] =   ,

cov[x] =

  

(       2)

   1,
  

mode[x] =   

if
if

   > 1
   > 2

(2.164)

(2.165)

(2.166)

with corresponding results for the univariate case.

2.3.8 periodic variables
although gaussian distributions are of great practical signi   cance, both in their
own right and as building blocks for more complex probabilistic models, there are
situations in which they are inappropriate as density models for continuous vari-
ables. one important case, which arises in practical applications, is that of periodic
variables.

an example of a periodic variable would be the wind direction at a particular
geographical location. we might, for instance, measure values of wind direction on a
number of days and wish to summarize this using a parametric distribution. another
example is calendar time, where we may be interested in modelling quantities that
are believed to be periodic over 24 hours or over an annual cycle. such quantities
can conveniently be represented using an angular (polar) coordinate 0 (cid:1)    < 2  .

and   2 = 359   

we might be tempted to treat periodic variables by choosing some direction
as the origin and then applying a conventional distribution such as the gaussian.
such an approach, however, would give results that were strongly dependent on the
arbitrary choice of origin. suppose, for instance, that we have two observations at
  1 = 1   
, and we model them using a standard univariate gaussian
distribution. if we choose the origin at 0   
, then the sample mean of this data set
will be 180   
with standard deviation 179   
, whereas if we choose the origin at 180   
,
then the mean will be 0   
. we clearly need to
develop a special approach for the treatment of periodic variables.
let us consider the problem of evaluating the mean of a set of observations
d = {  1, . . . ,   n} of a periodic variable. from now on, we shall assume that    is
measured in radians. we have already seen that the simple average (  1+      +  n )/n
will be strongly coordinate dependent. to    nd an invariant measure of the mean, we
note that the observations can be viewed as points on the unit circle and can therefore
be described instead by two-dimensional unit vectors x1, . . . , xn where (cid:5)xn(cid:5) = 1
for n = 1, . . . , n, as illustrated in figure 2.17. we can average the vectors {xn}

and the standard deviation will be 1   

106

2. id203 distributions

figure 2.17 illustration of the representation of val-
ues   n of a periodic variable as two-
dimensional vectors xn living on the unit
circle. also shown is the average x of
those vectors.

x2

x3

x4

  x

  r

    

x2

x1

x1

instead to give

n(cid:2)

n=1

xn

x =

1
n

(2.167)

and then    nd the corresponding angle    of this average. clearly, this de   nition will
ensure that the location of the mean is independent of the origin of the angular coor-
dinate. note that x will typically lie inside the unit circle. the cartesian coordinates
of the observations are given by xn = (cos   n, sin   n), and we can write the carte-
sian coordinates of the sample mean in the form x = (r cos   , r sin   ). substituting
into (2.167) and equating the x1 and x2 components then gives

r cos    =

1
n

cos   n,

r sin    =

1
n

sin   n.

(2.168)

n(cid:2)

n=1

n(cid:2)

n=1

taking the ratio, and using the identity tan    = sin   / cos   , we can solve for    to
give

(cid:12)(cid:5)
(cid:5)

(cid:13)

   = tan   1

n sin   n
n cos   n

.

(2.169)

shortly, we shall see how this result arises naturally as the maximum likelihood
estimator for an appropriately de   ned distribution over a periodic variable.

we now consider a periodic generalization of the gaussian called the von mises
distribution. here we shall limit our attention to univariate distributions, although
periodic distributions can also be found over hyperspheres of arbitrary dimension.
for an extensive discussion of periodic distributions, see mardia and jupp (2000).

by convention, we will consider distributions p(  ) that have period 2  . any
id203 density p(  ) de   ned over    must not only be nonnegative and integrate

2.3. the gaussian distribution

107

figure 2.18 the von mises distribution can be derived by considering
a two-dimensional gaussian of the form (2.173), whose
density contours are shown in blue and conditioning on
the unit circle shown in red.

x2

p(x)

x1

r = 1

(cid:6) 2  

p(  ) (cid:2) 0

to one, but it must also be periodic. thus p(  ) must satisfy the three conditions

(2.170)

(2.171)

(2.172)

p(  ) d   = 1

0

p(   + 2  ) = p(  ).

from (2.172), it follows that p(   + m2  ) = p(  ) for any integer m.

we can easily obtain a gaussian-like distribution that satis   es these three prop-
erties as follows. consider a gaussian distribution over two variables x = (x1, x2)
having mean    = (  1,   2) and a covariance matrix    =   2i where i is the 2    2
identity matrix, so that

p(x1, x2) =

1
2    2 exp

   (x1       1)2 + (x2       2)2

2  2

.

(2.173)

(cid:12)

(cid:13)

the contours of constant p(x) are circles, as illustrated in figure 2.18. now suppose
we consider the value of this distribution along a circle of    xed radius. then by con-
struction this distribution will be periodic, although it will not be normalized. we can
determine the form of this distribution by transforming from cartesian coordinates
(x1, x2) to polar coordinates (r,   ) so that

x2 = r sin   .
we also map the mean    into polar coordinates by writing

x1 = r cos   ,

  1 = r0 cos   0,

  2 = r0 sin   0.

(2.174)

(2.175)

next we substitute these transformations into the two-dimensional gaussian distribu-
tion (2.173), and then condition on the unit circle r = 1, noting that we are interested
only in the dependence on   . focussing on the exponent in the gaussian distribution
we have

(cid:26)

(r cos        r0 cos   0)2 + (r sin        r0 sin   0)2

(cid:27)
0     2r0 cos    cos   0     2r0 sin    sin   0

1 + r2

(cid:26)

(cid:27)

    1
2  2
=     1
2  2
= r0

  2 cos(         0) + const

(2.176)

108

2. id203 distributions

m = 5,   0 =   /4

m = 1,   0 = 3  /4

3  /4

  /4

0

2  

m = 5,   0 =   /4
m = 1,   0 = 3  /4

figure 2.19 the von mises distribution plotted for two different parameter values, shown as a cartesian plot
on the left and as the corresponding polar plot on the right.

exercise 2.51

where    const    denotes terms independent of   , and we have made use of the following
trigonometrical identities

cos2 a + sin2 a = 1

(2.177)
(2.178)
if we now de   ne m = r0/  2, we obtain our    nal expression for the distribution of
p(  ) along the unit circle r = 1 in the form

cos a cos b + sin a sin b = cos(a     b).

p(  |  0, m) =

1

2  i0(m)

exp{m cos(         0)}

(2.179)

which is called the von mises distribution, or the circular normal. here the param-
eter   0 corresponds to the mean of the distribution, while m, which is known as
the concentration parameter, is analogous to the inverse variance (precision) for the
gaussian. the id172 coef   cient in (2.179) is expressed in terms of i0(m),
which is the zeroth-order bessel function of the    rst kind (abramowitz and stegun,
1965) and is de   ned by

(cid:6) 2  

i0(m) =

1
2  

0

exp{m cos   } d  .

(2.180)

exercise 2.52

for large m, the distribution becomes approximately gaussian. the von mises dis-
tribution is plotted in figure 2.19, and the function i0(m) is plotted in figure 2.20.

now consider the maximum likelihood estimators for the parameters   0 and m

for the von mises distribution. the log likelihood function is given by
cos(  n       0).

ln p(d|  0, m) =    n ln(2  )     n ln i0(m) + m

(2.181)

n(cid:2)

n=1

i0(m)

3000

2000

1000

0

0

2.3. the gaussian distribution

109

1

a(m)

0.5

5
m

10

0

0

5
m

10

figure 2.20 plot of the bessel function i0(m) de   ned by (2.180), together with the function a(m) de   ned by
(2.186).

setting the derivative with respect to   0 equal to zero gives

n(cid:2)

n=1

sin(  n       0) = 0.

to solve for   0, we make use of the trigonometric identity

sin(a     b) = cos b sin a     cos a sin b

(cid:13)

(cid:12)(cid:5)
(cid:5)

n sin   n
n cos   n

exercise 2.53

from which we obtain

0 = tan   1
  ml

(2.182)

(2.183)

(2.184)

which we recognize as the result (2.169) obtained earlier for the mean of the obser-
vations viewed in a two-dimensional cartesian space.

similarly, maximizing (2.181) with respect to m, and making use of i

(cid:4)
0(m) =

i1(m) (abramowitz and stegun, 1965), we have

a(m) =

1
n

cos(  n       ml

0

)

(2.185)

n(cid:2)

n=1

where we have substituted for the maximum likelihood solution for   ml
that we are performing a joint optimization over    and m), and we have de   ned

0

(recalling

a(m) = i1(m)
i0(m) .

(2.186)

the function a(m) is plotted in figure 2.20. making use of the trigonometric iden-
tity (2.178), we can write (2.185) in the form
0    

n(cid:2)

n(cid:2)

a(mml) =

cos   ml

sin   ml

(2.187)

cos   n

sin   n

.

0

(cid:23)

(cid:23)

(cid:22)

1
n

n=1

(cid:22)

1
n

n=1

110

2. id203 distributions

100

80

60

40

1

on the left

figure 2.21 plots of the    old faith-
ful    data in which the blue curves
show contours of constant proba-
bility density.
is a
single gaussian distribution which
has been    tted to the data us-
ing maximum likelihood. note that
this distribution fails to capture the
two clumps in the data and indeed
places much of its id203 mass
in the central region between the
clumps where the data are relatively
sparse. on the right the distribution
is given by a linear combination of
two gaussians which has been    tted
to the data by maximum likelihood
using techniques discussed chap-
ter 9, and which gives a better rep-
resentation of the data.

100

80

60

40

1

2

3

4

5

6

2

3

4

5

6

the right-hand side of (2.187) is easily evaluated, and the function a(m) can be
inverted numerically.

for completeness, we mention brie   y some alternative techniques for the con-
struction of periodic distributions. the simplest approach is to use a histogram of
observations in which the angular coordinate is divided into    xed bins. this has the
virtue of simplicity and    exibility but also suffers from signi   cant limitations, as we
shall see when we discuss histogram methods in more detail in section 2.5. another
approach starts, like the von mises distribution, from a gaussian distribution over a
euclidean space but now marginalizes onto the unit circle rather than conditioning
(mardia and jupp, 2000). however, this leads to more complex forms of distribution
and will not be discussed further. finally, any valid distribution over the real axis
(such as a gaussian) can be turned into a periodic distribution by mapping succes-
sive intervals of width 2   onto the periodic variable (0, 2  ), which corresponds to
   wrapping    the real axis around unit circle. again, the resulting distribution is more
complex to handle than the von mises distribution.

one limitation of the von mises distribution is that it is unimodal. by forming
mixtures of von mises distributions, we obtain a    exible framework for modelling
periodic variables that can handle multimodality. for an example of a machine learn-
ing application that makes use of von mises distributions, see lawrence et al. (2002),
and for extensions to modelling conditional densities for regression problems, see
bishop and nabney (1996).

2.3.9 mixtures of gaussians
while the gaussian distribution has some important analytical properties, it suf-
fers from signi   cant limitations when it comes to modelling real data sets. consider
the example shown in figure 2.21. this is known as the    old faithful    data set,
and comprises 272 measurements of the eruption of the old faithful geyser at yel-
lowstone national park in the usa. each measurement comprises the duration of

appendix a

2.3. the gaussian distribution

111

figure 2.22 example of a gaussian mixture distribution
in one dimension showing three gaussians
(each scaled by a coef   cient) in blue and
their sum in red.

p(x)

the eruption in minutes (horizontal axis) and the time in minutes to the next erup-
tion (vertical axis). we see that the data set forms two dominant clumps, and that
a simple gaussian distribution is unable to capture this structure, whereas a linear
superposition of two gaussians gives a better characterization of the data set.

x

such superpositions, formed by taking linear combinations of more basic dis-
tributions such as gaussians, can be formulated as probabilistic models known as
mixture distributions (mclachlan and basford, 1988; mclachlan and peel, 2000).
in figure 2.22 we see that a linear combination of gaussians can give rise to very
complex densities. by using a suf   cient number of gaussians, and by adjusting their
means and covariances as well as the coef   cients in the linear combination, almost
any continuous density can be approximated to arbitrary accuracy.

we therefore consider a superposition of k gaussian densities of the form

p(x) =

  kn (x|  k,   k)

(2.188)

k=1

which is called a mixture of gaussians. each gaussian density n (x|  k,   k) is
called a component of the mixture and has its own mean   k and covariance   k.
contour and surface plots for a gaussian mixture having 3 components are shown in
figure 2.23.

in this section we shall consider gaussian components to illustrate the frame-
work of mixture models. more generally, mixture models can comprise linear com-
binations of other distributions. for instance, in section 9.3.3 we shall consider
mixtures of bernoulli distributions as an example of a mixture model for discrete
variables.

section 9.3.3

the parameters   k in (2.188) are called mixing coef   cients. if we integrate both
sides of (2.188) with respect to x, and note that both p(x) and the individual gaussian
components are normalized, we obtain

k(cid:2)

k(cid:2)

  k = 1.

(2.189)

also, the requirement that p(x) (cid:2) 0, together with n (x|  k,   k) (cid:2) 0, implies
  k (cid:2) 0 for all k. combining this with the condition (2.189) we obtain

k=1

0 (cid:1)   k (cid:1) 1.

(2.190)

112

2. id203 distributions

1

(a)

1

(b)

0.5

0

0.5

0.3

0.2

0.5

0

0

0.5

1

0

0.5

1

figure 2.23 illustration of a mixture of 3 gaussians in a two-dimensional space.
(a) contours of constant
density for each of the mixture components, in which the 3 components are denoted red, blue and green, and
the values of the mixing coef   cients are shown below each component. (b) contours of the marginal id203
density p(x) of the mixture distribution. (c) a surface plot of the distribution p(x).

we therefore see that the mixing coef   cients satisfy the requirements to be probabil-
ities.

from the sum and product rules, the marginal density is given by

p(x) =

p(k)p(x|k)

(2.191)

k(cid:2)

k=1

which is equivalent to (2.188) in which we can view   k = p(k) as the prior prob-
ability of picking the kth component, and the density n (x|  k,   k) = p(x|k) as
the id203 of x conditioned on k. as we shall see in later chapters, an impor-
tant role is played by the posterior probabilities p(k|x), which are also known as
responsibilities. from bayes    theorem these are given by

  k(x)     p(k|x)

(cid:5)
p(k)p(x|k)
(cid:5)
l p(l)p(x|l)
  kn (x|  k,   k)
l   ln (x|  l,   l) .

=

=

we shall discuss the probabilistic interpretation of the mixture distribution in greater
detail in chapter 9.
the form of the gaussian mixture distribution is governed by the parameters   ,
   and   , where we have used the notation        {  1, . . . ,   k},        {  1, . . . ,   k}
and        {  1, . . .   k}. one way to set the values of these parameters is to use
maximum likelihood. from (2.188) the log of the likelihood function is given by

n(cid:2)

(cid:24)
k(cid:2)

(cid:25)
  kn (xn|  k,   k)

n=1

k=1

ln p(x|  ,   ,   ) =

ln

(2.192)

(2.193)

2.4. the exponential family

113

where x = {x1, . . . , xn}. we immediately see that the situation is now much
more complex than with a single gaussian, due to the presence of the summation
over k inside the logarithm. as a result, the maximum likelihood solution for the
parameters no longer has a closed-form analytical solution. one approach to maxi-
mizing the likelihood function is to use iterative numerical optimization techniques
(fletcher, 1987; nocedal and wright, 1999; bishop and nabney, 2008). alterna-
tively we can employ a powerful framework called expectation maximization, which
will be discussed at length in chapter 9.

2.4. the exponential family

the id203 distributions that we have studied so far in this chapter (with the
exception of the gaussian mixture) are speci   c examples of a broad class of distri-
butions called the exponential family (duda and hart, 1973; bernardo and smith,
1994). members of the exponential family have many important properties in com-
mon, and it is illuminating to discuss these properties in some generality.

the exponential family of distributions over x, given parameters   , is de   ned to

be the set of distributions of the form

p(x|  ) = h(x)g(  ) exp

  tu(x)

(2.194)

(cid:27)

where x may be scalar or vector, and may be discrete or continuous. here    are
called the natural parameters of the distribution, and u(x) is some function of x.
the function g(  ) can be interpreted as the coef   cient that ensures that the distribu-
tion is normalized and therefore satis   es

(cid:6)

(cid:26)

g(  )

h(x) exp

  tu(x)

dx = 1

(2.195)

(cid:26)

(cid:27)

where the integration is replaced by summation if x is a discrete variable.

we begin by taking some examples of the distributions introduced earlier in
the chapter and showing that they are indeed members of the exponential family.
consider    rst the bernoulli distribution

p(x|  ) = bern(x|  ) =   x(1       )1   x.

(2.196)

expressing the right-hand side as the exponential of the logarithm, we have

p(x|  ) = exp{x ln    + (1     x) ln(1       )}

= (1       ) exp

ln

  
1       

(cid:16)

(cid:13)

(cid:12)

(cid:15)
(cid:16)

(cid:15)

   = ln

  
1       

x

.

(2.197)

(2.198)

comparison with (2.194) allows us to identify

114

2. id203 distributions

which we can solve for    to give    =   (  ), where

  (  ) =

1

1 + exp(     )

(2.199)

is called the logistic sigmoid function. thus we can write the bernoulli distribution
using the standard representation (2.194) in the form
p(x|  ) =   (     ) exp(  x)

(2.200)
where we have used 1       (  ) =   (     ), which is easily proved from (2.199). com-
parison with (2.194) shows that

u(x) = x
h(x) = 1
g(  ) =   (     ).

m(cid:14)

(cid:24)

m(cid:2)

(cid:25)

(2.201)
(2.202)
(2.203)

next consider the multinomial distribution that, for a single observation x, takes

the form

p(x|  ) =

  xk
k = exp

xk ln   k

(2.204)

k=1

k=1

where x = (x1, . . . , xn )t. again, we can write this in the standard representation
(2.194) so that

(2.205)
where   k = ln   k, and we have de   ned    = (  1, . . . ,   m )t. again, comparing with
(2.194) we have

p(x|  ) = exp(  tx)

u(x) = x
h(x) = 1
g(  ) = 1.

(2.206)
(2.207)
(2.208)

note that the parameters   k are not independent because the parameters   k are sub-
ject to the constraint

m(cid:2)

  k = 1

k=1

(2.209)
so that, given any m     1 of the parameters   k, the value of the remaining parameter
is    xed. in some circumstances, it will be convenient to remove this constraint by
expressing the distribution in terms of only m     1 parameters. this can be achieved
by using the relationship (2.209) to eliminate   m by expressing it in terms of the
remaining {  k} where k = 1, . . . , m     1, thereby leaving m     1 parameters. note
that these remaining parameters are still subject to the constraints

0 (cid:1)   k (cid:1) 1,

  k (cid:1) 1.

(2.210)

m   1(cid:2)

k=1

2.4. the exponential family

115

making use of the constraint (2.209), the multinomial distribution in this representa-
tion then becomes

(cid:25)

exp

(cid:24)

m(cid:2)

k=1

= exp

= exp

xk ln   k

(cid:24)
(cid:24)

m   1(cid:2)
m   1(cid:2)

k=1

k=1

we now identify

(cid:23)

ln

+ ln

(cid:22)
1     m   1(cid:2)
(cid:22)
1     m   1(cid:2)

k=1

k=1

(cid:23)(cid:25)
(cid:23)(cid:25)

  k

  k

.

(2.211)

=   k

(2.212)

xk ln   k +

(cid:22)

xk ln

ln

xk

(cid:23)
(cid:23)

k=1

  k

j=1   j

(cid:22)
1     m   1(cid:2)
1    (cid:5)m   1
(cid:22)
1    (cid:5)
(cid:5)

j   j

  k

  k =

exp(  k)

j exp(  j) .

1 +

(cid:22)

m   1(cid:2)

(cid:23)   1

which we can solve for   k by    rst summing both sides over k and then rearranging
and back-substituting to give

(2.213)

this is called the softmax function, or the normalized exponential. in this represen-
tation, the multinomial distribution therefore takes the form

p(x|  ) =

1 +

exp(  k)

exp(  tx).

(2.214)

this is the standard form of the exponential family, with parameter vector    =
(  1, . . . ,   m   1)t in which

k=1

u(x) = x
h(x) = 1

(cid:22)

g(  ) =

1 +

m   1(cid:2)

k=1

(cid:23)   1

exp(  k)

.

(2.215)
(2.216)

(2.217)

finally, let us consider the gaussian distribution. for the univariate gaussian,

we have

p(x|  ,   2) =

=

1

(2    2)1/2 exp
(2    2)1/2 exp

1

    1
2  2 (x       )2
    1
2  2 x2 +   

  2 x     1

2  2   2

(cid:13)

(2.218)

(2.219)

(cid:12)
(cid:12)

(cid:13)

116

2. id203 distributions

exercise 2.57

exercise 2.58

which, after some simple rearrangement, can be cast in the standard exponential
family form (2.194) with

   =

(cid:16)

(cid:15)
(cid:15)

(cid:16)

  /  2
   1/2  2
x
x2

u(x) =
h(x) = (2  )   1/2
g(  ) = (   2  2)1/2 exp

(2.220)

(2.221)

(2.222)

(2.223)

(cid:15)

(cid:16)

.

  2
1
4  2

(cid:6)

(cid:6)
(cid:6)

(cid:26)
(cid:26)

(cid:27)
(cid:27)

2.4.1 maximum likelihood and suf   cient statistics
let us now consider the problem of estimating the parameter vector    in the gen-
eral exponential family distribution (2.194) using the technique of maximum likeli-
hood. taking the gradient of both sides of (2.195) with respect to   , we have

(cid:26)

(cid:27)

   g(  )

h(x) exp

  tu(x)

dx

+ g(  )

h(x) exp

  tu(x)

u(x) dx = 0.

(2.224)

rearranging, and making use again of (2.195) then gives

    1
g(  )

   g(  ) = g(  )

h(x) exp

  tu(x)

u(x) dx = e[u(x)]

where we have used (2.194). we therefore obtain the result

       ln g(  ) = e[u(x)].

(2.225)

(2.226)

note that the covariance of u(x) can be expressed in terms of the second derivatives
of g(  ), and similarly for higher order moments. thus, provided we can normalize a
distribution from the exponential family, we can always    nd its moments by simple
differentiation.
n(cid:2)
{x1, . . . , xn}, for which the likelihood function is given by

now consider a set of independent identically distributed data denoted by x =

n(cid:14)

(cid:24)

(cid:25)

(cid:22)

(cid:23)

h(xn)

g(  )n exp

  t

u(xn)

.

(2.227)

p(x|  ) =

n=1

n=1

setting the gradient of ln p(x|  ) with respect to    to zero, we get the following
condition to be satis   ed by the maximum likelihood estimator   ml

       ln g(  ml) =

1
n

u(xn)

(2.228)

n(cid:2)

n=1

2.4. the exponential family

117

(cid:5)

which can in principle be solved to obtain   ml. we see that the solution for the
n u(xn), which
maximum likelihood estimator depends on the data only through
is therefore called the suf   cient statistic of the distribution (2.194). we do not need
to store the entire data set itself but only the value of the suf   cient statistic. for
the bernoulli distribution, for example, the function u(x) is given just by x and
so we need only keep the sum of the data points {xn}, whereas for the gaussian
u(x) = (x, x2)t, and so we should keep both the sum of {xn} and the sum of {x2
n}.
if we consider the limit n        , then the right-hand side of (2.228) becomes
e[u(x)], and so by comparing with (2.226) we see that in this limit   ml will equal
the true value   .

in fact, this suf   ciency property holds also for bayesian id136, although
we shall defer discussion of this until chapter 8 when we have equipped ourselves
with the tools of id114 and can thereby gain a deeper insight into these
important concepts.

2.4.2 conjugate priors
we have already encountered the concept of a conjugate prior several times, for
example in the context of the bernoulli distribution (for which the conjugate prior
is the beta distribution) or the gaussian (where the conjugate prior for the mean is
a gaussian, and the conjugate prior for the precision is the wishart distribution). in
general, for a given id203 distribution p(x|  ), we can seek a prior p(  ) that is
conjugate to the likelihood function, so that the posterior distribution has the same
functional form as the prior. for any member of the exponential family (2.194), there
exists a conjugate prior that can be written in the form
p(  |  ,   ) = f(  ,   )g(  )   exp

    t  

(cid:26)

(cid:27)

(2.229)

where f(  ,   ) is a id172 coef   cient, and g(  ) is the same function as ap-
pears in (2.194). to see that this is indeed conjugate, let us multiply the prior (2.229)
by the likelihood function (2.227) to obtain the posterior distribution, up to a nor-
malization coef   cient, in the form

(cid:24)

(cid:22)
n(cid:2)

(cid:23)(cid:25)

p(  |x,   ,   )     g(  )  +n exp

  t

u(xn) +     

.

(2.230)

n=1

this again takes the same functional form as the prior (2.229), con   rming conjugacy.
furthermore, we see that the parameter    can be interpreted as a effective number of
pseudo-observations in the prior, each of which has a value for the suf   cient statistic
u(x) given by   .

2.4.3 noninformative priors
in some applications of probabilistic id136, we may have prior knowledge
that can be conveniently expressed through the prior distribution. for example, if
the prior assigns zero id203 to some value of variable, then the posterior dis-
tribution will necessarily also assign zero id203 to that value, irrespective of

118

2. id203 distributions

any subsequent observations of data. in many cases, however, we may have little
idea of what form the distribution should take. we may then seek a form of prior
distribution, called a noninformative prior, which is intended to have as little in   u-
ence on the posterior distribution as possible (jeffries, 1946; box and tao, 1973;
bernardo and smith, 1994). this is sometimes referred to as    letting the data speak
for themselves   .
if we have a distribution p(x|  ) governed by a parameter   , we might be tempted
to propose a prior distribution p(  ) = const as a suitable prior. if    is a discrete
variable with k states, this simply amounts to setting the prior id203 of each
state to 1/k. in the case of continuous parameters, however, there are two potential
dif   culties with this approach. the    rst is that, if the domain of    is unbounded,
this prior distribution cannot be correctly normalized because the integral over   
diverges. such priors are called improper. in practice, improper priors can often
be used provided the corresponding posterior distribution is proper, i.e., that it can
be correctly normalized. for instance, if we put a uniform prior distribution over
the mean of a gaussian, then the posterior distribution for the mean, once we have
observed at least one data point, will be proper.

a second dif   culty arises from the transformation behaviour of a id203
density under a nonlinear change of variables, given by (1.27). if a function h(  )

is constant, and we change variables to    =   2, then(cid:1)h(  ) = h(  2) will also be

constant. however, if we choose the density p  (  ) to be constant, then the density
of    will be given, from (1.27), by

(cid:7)(cid:7)(cid:7)(cid:7) d  

d  

(cid:7)(cid:7)(cid:7)(cid:7) = p  (  2)2         

p  (  ) = p  (  )

(2.231)

and so the density over    will not be constant. this issue does not arise when we use
maximum likelihood, because the likelihood function p(x|  ) is a simple function of
   and so we are free to use any convenient parameterization. if, however, we are to
choose a prior distribution that is constant, we must take care to use an appropriate
representation for the parameters.

here we consider two simple examples of noninformative priors (berger, 1985).

first of all, if a density takes the form

p(x|  ) = f(x       )

(2.232)

then the parameter    is known as a location parameter. this family of densities

exhibits translation invariance because if we shift x by a constant to give(cid:1)x = x + c,
where we have de   ned(cid:1)   =    + c. thus the density takes the same form in the

p((cid:1)x|(cid:1)  ) = f((cid:1)x    (cid:1)  )

(2.233)

then

new variable as in the original one, and so the density is independent of the choice
of origin. we would like to choose a prior distribution that re   ects this translation
invariance property, and so we choose a prior that assigns equal id203 mass to

2.4. the exponential family

119

an interval a (cid:1)    (cid:1) b as to the shifted interval a     c (cid:1)    (cid:1) b     c. this implies

(cid:6) b

(cid:6) b   c

(cid:6) b

p(  ) d   =

a

a   c

p(  ) d   =

a

p(       c) d  

(2.234)

and because this must hold for all choices of a and b, we have

p(       c) = p(  )

(2.235)

which implies that p(  ) is constant. an example of a location parameter would be
the mean    of a gaussian distribution. as we have seen, the conjugate prior distri-
bution for    in this case is a gaussian p(  |  0,   2
0), and we obtain a
0        . indeed, from (2.141) and (2.142)
noninformative prior by taking the limit   2
we see that this gives a posterior distribution over    in which the contributions from
(cid:17)
the prior vanish.

0) = n (  |  0,   2
(cid:18)

as a second example, consider a density of the form

p(x|  ) =

1
  

f

x
  

(2.236)

exercise 2.59

where    > 0. note that this will be a normalized density provided f(x) is correctly
normalized. the parameter    is known as a scale parameter, and the density exhibits

scale invariance because if we scale x by a constant to give(cid:1)x = cx, then
where we have de   ned(cid:1)   = c  . this transformation corresponds to a change of

p((cid:1)x|(cid:1)  ) =

(cid:15)(cid:1)x(cid:1)  

(2.237)

(cid:16)

1(cid:1)  

f

scale, for example from meters to kilometers if x is a length, and we would like
to choose a prior distribution that re   ects this scale invariance. if we consider an
interval a (cid:1)    (cid:1) b, and a scaled interval a/c (cid:1)    (cid:1) b/c, then the prior should
assign equal id203 mass to these two intervals. thus we have

(cid:6) b

(cid:6) b/c

p(  ) d   =

p(  ) d   =

a

a/c

1
c

  

1
c

d  

(2.238)

(cid:16)

(cid:6) b

(cid:15)

p

a

and because this must hold for choices of a and b, we have

p(  ) = p

  

(2.239)
and hence p(  )     1/  . note that again this is an improper prior because the integral
of the distribution over 0 (cid:1)    (cid:1)     is divergent. it is sometimes also convenient
to think of the prior distribution for a scale parameter in terms of the density of the
log of the parameter. using the transformation rule (1.27) for densities we see that
p(ln   ) = const. thus, for this prior there is the same id203 mass in the range
1 (cid:1)    (cid:1) 10 as in the range 10 (cid:1)    (cid:1) 100 and in 100 (cid:1)    (cid:1) 1000.

(cid:15)

1
c

(cid:16)

1
c

120

2. id203 distributions

(cid:26)   ((cid:4)x/  )2

(cid:27)

an example of a scale parameter would be the standard deviation    of a gaussian

distribution, after we have taken account of the location parameter   , because

where(cid:4)x = x       . as discussed earlier, it is often more convenient to work in terms

n (x|  ,   2)       

   1 exp

of the precision    = 1/  2 rather than    itself. using the transformation rule for
densities, we see that a distribution p(  )     1/   corresponds to a distribution over   
of the form p(  )     1/  . we have seen that the conjugate prior for    was the gamma
distribution gam(  |a0, b0) given by (2.146). the noninformative prior is obtained
as the special case a0 = b0 = 0. again, if we examine the results (2.150) and (2.151)
for the posterior distribution of   , we see that for a0 = b0 = 0, the posterior depends
only on terms arising from the data and not from the prior.

(2.240)

section 2.3

2.5. nonparametric methods

throughout this chapter, we have focussed on the use of id203 distributions
having speci   c functional forms governed by a small number of parameters whose
values are to be determined from a data set. this is called the parametric approach
to density modelling. an important limitation of this approach is that the chosen
density might be a poor model of the distribution that generates the data, which can
result in poor predictive performance. for instance, if the process that generates the
data is multimodal, then this aspect of the distribution can never be captured by a
gaussian, which is necessarily unimodal.

in this    nal section, we consider some nonparametric approaches to density es-
timation that make few assumptions about the form of the distribution. here we shall
focus mainly on simple frequentist methods. the reader should be aware, however,
that nonparametric bayesian methods are attracting increasing interest (walker et al.,
1999; neal, 2000; m  uller and quintana, 2004; teh et al., 2006).

let us start with a discussion of histogram methods for density estimation, which
we have already encountered in the context of marginal and conditional distributions
in figure 1.11 and in the context of the central limit theorem in figure 2.6. here we
explore the properties of histogram density models in more detail, focussing on the
case of a single continuous variable x. standard histograms simply partition x into
distinct bins of width    i and then count the number ni of observations of x falling
in bin i. in order to turn this count into a normalized id203 density, we simply
divide by the total number n of observations and by the width    i of the bins to
obtain id203 values for each bin given by
pi = ni
n   i

(2.241)

p(x) dx = 1. this gives a model for the density
for which it is easily seen that
p(x) that is constant over the width of each bin, and often the bins are chosen to have
the same width    i =    .

(cid:28)

figure 2.24 an illustration of the histogram approach
to density estimation, in which a data set
of 50 data points is generated from the
distribution shown by the green curve.
histogram density estimates, based on
(2.241), with a common bin width     are
shown for various values of    .

2.5. nonparametric methods

121

    = 0.04

    = 0.08

    = 0.25

5

0

5

0

0

5

0

0

0

0.5

0.5

0.5

1

1

1

in figure 2.24, we show an example of histogram density estimation. here
the data is drawn from the distribution, corresponding to the green curve, which is
formed from a mixture of two gaussians. also shown are three examples of his-
togram density estimates corresponding to three different choices for the bin width
   . we see that when     is very small (top    gure), the resulting density model is very
spiky, with a lot of structure that is not present in the underlying distribution that
generated the data set. conversely, if     is too large (bottom    gure) then the result is
a model that is too smooth and that consequently fails to capture the bimodal prop-
erty of the green curve. the best results are obtained for some intermediate value
of     (middle    gure). in principle, a histogram density model is also dependent on
the choice of edge location for the bins, though this is typically much less signi   cant
than the value of    .

note that the histogram method has the property (unlike the methods to be dis-
cussed shortly) that, once the histogram has been computed, the data set itself can
be discarded, which can be advantageous if the data set is large. also, the histogram
approach is easily applied if the data points are arriving sequentially.

in practice, the histogram technique can be useful for obtaining a quick visual-
ization of data in one or two dimensions but is unsuited to most density estimation
applications. one obvious problem is that the estimated density has discontinuities
that are due to the bin edges rather than any property of the underlying distribution
that generated the data. another major limitation of the histogram approach is its
scaling with dimensionality. if we divide each variable in a d-dimensional space
into m bins, then the total number of bins will be m d. this exponential scaling
with d is an example of the curse of dimensionality. in a space of high dimensional-
ity, the quantity of data needed to provide meaningful estimates of local id203
density would be prohibitive.

the histogram approach to density estimation does, however, teach us two im-
portant lessons. first, to estimate the id203 density at a particular location,
we should consider the data points that lie within some local neighbourhood of that
point. note that the concept of locality requires that we assume some form of dis-
tance measure, and here we have been assuming euclidean distance. for histograms,

section 1.4

122

2. id203 distributions

this neighbourhood property was de   ned by the bins, and there is a natural    smooth-
ing    parameter describing the spatial extent of the local region, in this case the bin
width. second, the value of the smoothing parameter should be neither too large nor
too small in order to obtain good results. this is reminiscent of the choice of model
complexity in polynomial curve    tting discussed in chapter 1 where the degree m
of the polynomial, or alternatively the value    of the id173 parameter, was
optimal for some intermediate value, neither too large nor too small. armed with
these insights, we turn now to a discussion of two widely used nonparametric tech-
niques for density estimation, kernel estimators and nearest neighbours, which have
better scaling with dimensionality than the simple histogram model.

2.5.1 kernel density estimators
let us suppose that observations are being drawn from some unknown probabil-
ity density p(x) in some d-dimensional space, which we shall take to be euclidean,
and we wish to estimate the value of p(x). from our earlier discussion of locality,
let us consider some small region r containing x. the id203 mass associated
with this region is given by

(cid:6)

p =

r

p(x) dx.

(2.242)

section 2.1

now suppose that we have collected a data set comprising n observations drawn
from p(x). because each data point has a id203 p of falling within r, the total
number k of points that lie inside r will be distributed according to the binomial
distribution

bin(k|n, p ) =

n!

k!(n     k)! p k(1     p )1   k.

(2.243)

using (2.11), we see that the mean fraction of points falling inside the region is
e[k/n] = p , and similarly using (2.12) we see that the variance around this mean
is var[k/n] = p (1     p )/n. for large n, this distribution will be sharply peaked
around the mean and so
(2.244)
if, however, we also assume that the region r is suf   ciently small that the id203
density p(x) is roughly constant over the region, then we have

k (cid:7) n p.

(2.245)
where v is the volume of r. combining (2.244) and (2.245), we obtain our density
estimate in the form

.

(2.246)

p (cid:7) p(x)v

p(x) = k
n v

note that the validity of (2.246) depends on two contradictory assumptions, namely
that the region r be suf   ciently small that the density is approximately constant over
the region and yet suf   ciently large (in relation to the value of that density) that the
number k of points falling inside the region is suf   cient for the binomial distribution
to be sharply peaked.

2.5. nonparametric methods

123

we can exploit the result (2.246) in two different ways. either we can    x k and
determine the value of v from the data, which gives rise to the k-nearest-neighbour
technique discussed shortly, or we can    x v and determine k from the data, giv-
ing rise to the kernel approach. it can be shown that both the k-nearest-neighbour
density estimator and the kernel density estimator converge to the true id203
density in the limit n         provided v shrinks suitably with n, and k grows with
n (duda and hart, 1973).
we begin by discussing the kernel method in detail, and to start with we take
the region r to be a small hypercube centred on the point x at which we wish to
determine the id203 density. in order to count the number k of points falling
within this region, it is convenient to de   ne the following function

k(u) =

|ui| (cid:1) 1/2,
1,
0, otherwise

i = 1, . . . , d,

(2.247)

(cid:12)

which represents a unit cube centred on the origin. the function k(u) is an example
of a id81, and in this context is also called a parzen window. from (2.247),
the quantity k((x    xn)/h) will be one if the data point xn lies inside a cube of side
h centred on x, and zero otherwise. the total number of data points lying inside this
cube will therefore be

k =

k

.

(2.248)

substituting this expression into (2.246) then gives the following result for the esti-
mated density at x

(cid:17)

x     xn

(cid:18)

h

(cid:17)

(cid:18)

n(cid:2)

n=1

n(cid:2)

n=1

p(x) =

1
n

1
hd k

x     xn

h

(2.249)

where we have used v = hd for the volume of a hypercube of side h in d di-
mensions. using the symmetry of the function k(u), we can now re-interpret this
equation, not as a single cube centred on x but as the sum over n cubes centred on
the n data points xn.

as it stands, the kernel density estimator (2.249) will suffer from one of the same
problems that the histogram method suffered from, namely the presence of arti   cial
discontinuities, in this case at the boundaries of the cubes. we can obtain a smoother
density model if we choose a smoother id81, and a common choice is the
gaussian, which gives rise to the following kernel density model

n(cid:2)

n=1

p(x) =

1
n

1

(2  h2)1/2 exp

(cid:12)
   (cid:5)x     xn(cid:5)2

(cid:13)

2h2

(2.250)

where h represents the standard deviation of the gaussian components. thus our
density model is obtained by placing a gaussian over each data point and then adding
up the contributions over the whole data set, and then dividing by n so that the den-
sity is correctly normalized. in figure 2.25, we apply the model (2.250) to the data

124

2. id203 distributions

figure 2.25 illustration of

the kernel density model
(2.250) applied to the same data set used
to demonstrate the histogram approach in
figure 2.24. we see that h acts as a
smoothing parameter and that if it is set
too small (top panel), the result is a very
noisy density model, whereas if it is set
too large (bottom panel), then the bimodal
nature of the underlying distribution from
which the data is generated (shown by the
green curve) is washed out. the best den-
sity model is obtained for some intermedi-
ate value of h (middle panel).

h = 0.005

h = 0.07

h = 0.2

5

0

5

0

0

5

0

0

0

0.5

0.5

0.5

1

1

1

set used earlier to demonstrate the histogram technique. we see that, as expected,
the parameter h plays the role of a smoothing parameter, and there is a trade-off
between sensitivity to noise at small h and over-smoothing at large h. again, the
optimization of h is a problem in model complexity, analogous to the choice of bin
width in histogram density estimation, or the degree of the polynomial used in curve
   tting.

we can choose any other id81 k(u) in (2.249) subject to the condi-

tions

(cid:6)

k(u) (cid:2) 0,
k(u) du = 1

(2.251)

(2.252)

which ensure that the resulting id203 distribution is nonnegative everywhere
and integrates to one. the class of density model given by (2.249) is called a kernel
density estimator, or parzen estimator. it has a great merit that there is no compu-
tation involved in the    training    phase because this simply requires storage of the
training set. however, this is also one of its great weaknesses because the computa-
tional cost of evaluating the density grows linearly with the size of the data set.

2.5.2 nearest-neighbour methods
one of the dif   culties with the kernel approach to density estimation is that the
parameter h governing the kernel width is    xed for all kernels. in regions of high
data density, a large value of h may lead to over-smoothing and a washing out of
structure that might otherwise be extracted from the data. however, reducing h may
lead to noisy estimates elsewhere in data space where the density is smaller. thus
the optimal choice for h may be dependent on location within the data space. this
issue is addressed by nearest-neighbour methods for density estimation.

we therefore return to our general result (2.246) for local density estimation,
and instead of    xing v and determining the value of k from the data, we consider
a    xed value of k and use the data to    nd an appropriate value for v . to do this,
we consider a small sphere centred on the point x at which we wish to estimate the

2.5. nonparametric methods

125

figure 2.26 illustration of k-nearest-neighbour den-
sity estimation using the same data set
as in figures 2.25 and 2.24. we see
that the parameter k governs the degree
of smoothing, so that a small value of
k leads to a very noisy density model
(top panel), whereas a large value (bot-
tom panel) smoothes out the bimodal na-
ture of the true distribution (shown by the
green curve) from which the data set was
generated.

k = 1

k = 5

k = 30

5

0

5

0

0

5

0

0

0

0.5

0.5

0.5

1

1

1

exercise 2.61

density p(x), and we allow the radius of the sphere to grow until it contains precisely
k data points. the estimate of the density p(x) is then given by (2.246) with v set to
the volume of the resulting sphere. this technique is known as k nearest neighbours
and is illustrated in figure 2.26, for various choices of the parameter k, using the
same data set as used in figure 2.24 and figure 2.25. we see that the value of k
now governs the degree of smoothing and that again there is an optimum choice for
k that is neither too large nor too small. note that the model produced by k nearest
neighbours is not a true density model because the integral over all space diverges.
we close this chapter by showing how the k-nearest-neighbour technique for
density estimation can be extended to the problem of classi   cation. to do this, we
apply the k-nearest-neighbour density estimation technique to each class separately
and then make use of bayes    theorem. let us suppose that we have a data set com-
prising nk points in class ck with n points in total, so that
k nk = n. if we
wish to classify a new point x, we draw a sphere centred on x containing precisely
k points irrespective of their class. suppose this sphere has volume v and contains
kk points from class ck. then (2.246) provides an estimate of the density associated
with each class

(cid:5)

p(x|ck) = kk
nkv

similarly, the unconditional density is given by
p(x) = k
n v

while the class priors are given by

p(ck) = nk
n

.

.

(2.253)

(2.254)

(2.255)

we can now combine (2.253), (2.254), and (2.255) using bayes    theorem to obtain
the posterior id203 of class membership

p(ck|x) = p(x|ck)p(ck)

p(x)

= kk
k

.

(2.256)

126

2. id203 distributions

x2

x2

figure 2.27 (a) in the k-nearest-
neighbour classi   er, a new point,
shown by the black diamond, is clas-
si   ed according to the majority class
membership of the k closest train-
ing data points, in this case k =
3.
in the nearest-neighbour
(k = 1) approach to classi   cation,
the resulting decision boundary is
composed of hyperplanes that form
perpendicular bisectors of pairs of
points from different classes.

(b)

x1

x1

(a)

(b)

if we wish to minimize the id203 of misclassi   cation, this is done by assigning
the test point x to the class having the largest posterior id203, corresponding to
the largest value of kk/k. thus to classify a new point, we identify the k nearest
points from the training data set and then assign the new point to the class having the
largest number of representatives amongst this set. ties can be broken at random.
the particular case of k = 1 is called the nearest-neighbour rule, because a test
point is simply assigned to the same class as the nearest point from the training set.
these concepts are illustrated in figure 2.27.

in figure 2.28, we show the results of applying the k-nearest-neighbour algo-
rithm to the oil    ow data, introduced in chapter 1, for various values of k. as
expected, we see that k controls the degree of smoothing, so that small k produces
many small regions of each class, whereas large k leads to fewer larger regions.

k = 1

x7

2

1

k = 3

x7

2

1

x7

2

1

k = 3 1

0

0

1

x6

2

0

0

1

x6

2

0

0

1

x6

2

figure 2.28 plot of 200 data points from the oil data set showing values of x6 plotted against x7, where the
red, green, and blue points correspond to the    laminar   ,    annular   , and    homogeneous    classes, respectively. also
shown are the classi   cations of the input space given by the k-nearest-neighbour algorithm for various values
of k.

exercises

127

an interesting property of the nearest-neighbour (k = 1) classi   er is that, in the
limit n        , the error rate is never more than twice the minimum achievable error
rate of an optimal classi   er, i.e., one that uses the true class distributions (cover and
hart, 1967) .

as discussed so far, both the k-nearest-neighbour method, and the kernel den-
sity estimator, require the entire training data set to be stored, leading to expensive
computation if the data set is large. this effect can be offset, at the expense of some
additional one-off computation, by constructing tree-based search structures to allow
(approximate) near neighbours to be found ef   ciently without doing an exhaustive
search of the data set. nevertheless, these nonparametric methods are still severely
limited. on the other hand, we have seen that simple parametric models are very
restricted in terms of the forms of distribution that they can represent. we therefore
need to    nd density models that are very    exible and yet for which the complexity
of the models can be controlled independently of the size of the training set, and we
shall see in subsequent chapters how to achieve this.

exercises

erties

2.1 ((cid:12)) www verify that the bernoulli distribution (2.2) satis   es the following prop-

1(cid:2)

p(x|  ) = 1

x=0

e[x] =   
var[x] =   (1       ).

(2.257)

(2.258)
(2.259)

show that the id178 h[x] of a bernoulli distributed random binary variable x is
given by

h[x] =       ln        (1       ) ln(1       ).

(2.260)

2.2 ((cid:12) (cid:12)) the form of the bernoulli distribution given by (2.2) is not symmetric be-
tween the two values of x. in some situations, it will be more convenient to use an
equivalent formulation for which x     {   1, 1}, in which case the distribution can be
written

(cid:16)(1   x)/2(cid:15)

(cid:16)(1+x)/2

(cid:15)

p(x|  ) =

1       
2

1 +   

(2.261)
where        [   1, 1]. show that the distribution (2.261) is normalized, and evaluate its
mean, variance, and id178.

2

2.3 ((cid:12) (cid:12)) www in this exercise, we prove that the binomial distribution (2.9) is nor-
malized. first use the de   nition (2.10) of the number of combinations of m identical
objects chosen from a total of n to show that

(cid:15)

(cid:16)

(cid:15)

(cid:16)

(cid:15)

(cid:16)

n
m

+

n
m     1

=

n + 1

m

.

(2.262)

128

2. id203 distributions

use this result to prove by induction the following result

(cid:15)

n(cid:2)

(cid:16)

n
m

m=0

(1 + x)n =

xm

(2.263)

which is known as the binomial theorem, and which is valid for all real values of x.
finally, show that the binomial distribution is normalized, so that

  m(1       )n   m = 1

(2.264)

(cid:15)

n(cid:2)

(cid:16)

n
m

m=0

(cid:6) 1
(cid:6)    

0

which can be done by    rst pulling out a factor (1       )n out of the summation and
then making use of the binomial theorem.

2.4 ((cid:12) (cid:12)) show that the mean of the binomial distribution is given by (2.11). to do this,
differentiate both sides of the id172 condition (2.264) with respect to    and
then rearrange to obtain an expression for the mean of n. similarly, by differentiating
(2.264) twice with respect to    and making use of the result (2.11) for the mean of
the binomial distribution prove the result (2.12) for the variance of the binomial.

2.5 ((cid:12) (cid:12)) www in this exercise, we prove that the beta distribution, given by (2.13), is

correctly normalized, so that (2.14) holds. this is equivalent to showing that

  a   1(1       )b   1 d   =

  (a)  (b)
  (a + b) .

(2.265)

(cid:6)    

from the de   nition (1.141) of the gamma function, we have

  (a)  (b) =

exp(   x)xa   1 dx

exp(   y)yb   1 dy.

(2.266)

0

0

use this expression to prove (2.265) as follows. first bring the integral over y inside
the integrand of the integral over x, next make the change of variable t = y + x
where x is    xed, then interchange the order of the x and t integrations, and    nally
make the change of variable x = t   where t is    xed.

2.6 ((cid:12)) make use of the result (2.265) to show that the mean, variance, and mode of the

beta distribution (2.13) are given respectively by

e[  ] =

var[  ] =

mode[  ] =

a

a + b

ab

(a + b)2(a + b + 1)
a     1
a + b     2 .

(2.267)

(2.268)

(2.269)

exercises

129

2.7 ((cid:12) (cid:12)) consider a binomial random variable x given by (2.9), with prior distribution
for    given by the beta distribution (2.13), and suppose we have observed m occur-
rences of x = 1 and l occurrences of x = 0. show that the posterior mean value of x
lies between the prior mean and the maximum likelihood estimate for   . to do this,
show that the posterior mean can be written as    times the prior mean plus (1       )
times the maximum likelihood estimate, where 0 (cid:1)    (cid:1) 1. this illustrates the con-
cept of the posterior distribution being a compromise between the prior distribution
and the maximum likelihood solution.

2.8 ((cid:12)) consider two variables x and y with joint distribution p(x, y). prove the follow-

ing two results

e[x] = ey [ex[x|y]]
var[x] = ey [varx[x|y]] + vary [ex[x|y]] .

(2.270)
(2.271)
here ex[x|y] denotes the expectation of x under the conditional distribution p(x|y),
with a similar notation for the conditional variance.

2.9 ((cid:12) (cid:12) (cid:12)) www . in this exercise, we prove the id172 of the dirichlet dis-
tribution (2.38) using induction. we have already shown in exercise 2.5 that the
beta distribution, which is a special case of the dirichlet for m = 2, is normalized.
we now assume that the dirichlet distribution is normalized for m     1 variables
and prove that it is normalized for m variables. to do this, consider the dirichlet
k=1   k = 1 by
distribution over m variables, and take account of the constraint
eliminating   m , so that the dirichlet is written

(cid:5)m
(cid:23)  m   1

m   1(cid:14)

    k   1

k

(cid:22)
1     m   1(cid:2)

pm (  1, . . . ,   m   1) = cm

  j

(2.272)

k=1

j=1

and our goal is to    nd an expression for cm . to do this, integrate over   m   1, taking
care over the limits of integration, and then make a change of variable so that this
integral has limits 0 and 1. by assuming the correct result for cm   1 and making use
of (2.265), derive the expression for cm .

2.10 ((cid:12) (cid:12)) using the property   (x + 1) = x  (x) of the gamma function, derive the
following results for the mean, variance, and covariance of the dirichlet distribution
given by (2.38)

e[  j] =   j
  0
var[  j] =   j(  0       j)
0(  0 + 1)
  2
cov[  j  l] =       j  l
0(  0 + 1) ,
  2

j (cid:9)= l

(2.273)

(2.274)

(2.275)

where   0 is de   ned by (2.39).

130

2. id203 distributions

2.11 ((cid:12)) www by expressing the expectation of ln   j under the dirichlet distribution

(2.38) as a derivative with respect to   j, show that

e[ln   j] =   (  j)       (  0)

where   0 is given by (2.39) and

  (a)     d
da

ln   (a)

is the digamma function.

2.12 ((cid:12)) the uniform distribution for a continuous variable x is de   ned by

u(x|a, b) =

1
b     a

,

a (cid:1) x (cid:1) b.

(2.276)

(2.277)

(2.278)

verify that this distribution is normalized, and    nd expressions for its mean and
variance.

2.13 ((cid:12) (cid:12)) evaluate the id181 (1.113) between two gaussians

p(x) = n (x|  ,   ) and q(x) = n (x|m, l).

2.14 ((cid:12) (cid:12)) www this exercise demonstrates that the multivariate distribution with max-
imum id178, for a given covariance, is a gaussian. the id178 of a distribution
p(x) is given by

(cid:6)

h[x] =    

p(x) ln p(x) dx.

(2.279)

we wish to maximize h[x] over all distributions p(x) subject to the constraints that
p(x) be normalized and that it have a speci   c mean and covariance, so that

(cid:6)
(cid:6)
(cid:6)

p(x) dx = 1

p(x)x dx =   
p(x)(x       )(x       )t dx =   .

(2.280)

(2.281)

(2.282)

by performing a variational maximization of (2.279) and using lagrange multipliers
to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum
likelihood distribution is given by the gaussian (2.43).

2.15 ((cid:12) (cid:12)) show that the id178 of the multivariate gaussian n (x|  ,   ) is given by

h[x] =

1
2

ln|  | + d
2

(1 + ln(2  ))

(2.283)

where d is the dimensionality of x.

exercises

131

2.16 ((cid:12) (cid:12) (cid:12)) www consider two random variables x1 and x2 having gaussian distri-
butions with means   1,   2 and precisions   1,   2 respectively. derive an expression
for the differential id178 of the variable x = x1 + x2. to do this,    rst    nd the
distribution of x by using the relation

p(x|x2)p(x2) dx2

(2.284)

(cid:6)    

p(x) =

      

and completing the square in the exponent. then observe that this represents the
convolution of two gaussian distributions, which itself will be gaussian, and    nally
make use of the result (1.110) for the id178 of the univariate gaussian.

2.17 ((cid:12)) www consider the multivariate gaussian distribution given by (2.43). by
   1 as the sum of a sym-
writing the precision matrix (inverse covariance matrix)   
metric and an anti-symmetric matrix, show that the anti-symmetric term does not
appear in the exponent of the gaussian, and hence that the precision matrix may be
taken to be symmetric without loss of generality. because the inverse of a symmetric
matrix is also symmetric (see exercise 2.22), it follows that the covariance matrix
may also be chosen to be symmetric without loss of generality.

2.18 ((cid:12) (cid:12) (cid:12)) consider a real, symmetric matrix    whose eigenvalue equation is given
by (2.45). by taking the complex conjugate of this equation and subtracting the
original equation, and then forming the inner product with eigenvector ui, show that
the eigenvalues   i are real. similarly, use the symmetry property of    to show that
two eigenvectors ui and uj will be orthogonal provided   j (cid:9)=   i. finally, show that
without loss of generality, the set of eigenvectors can be chosen to be orthonormal,
so that they satisfy (2.46), even if some of the eigenvalues are zero.

2.19 ((cid:12) (cid:12)) show that a real, symmetric matrix    having the eigenvector equation (2.45)
can be expressed as an expansion in the eigenvectors, with coef   cients given by the
   1 has a
eigenvalues, of the form (2.48). similarly, show that the inverse matrix   
representation of the form (2.49).

2.20 ((cid:12) (cid:12)) www a positive de   nite matrix    can be de   ned as one for which the

quadratic form

(2.285)
is positive for any real value of the vector a. show that a necessary and suf   cient
condition for    to be positive de   nite is that all of the eigenvalues   i of   , de   ned
by (2.45), are positive.

at  a

2.21 ((cid:12)) show that a real, symmetric matrix of size d   d has d(d + 1)/2 independent

parameters.

2.22 ((cid:12)) www show that the inverse of a symmetric matrix is itself symmetric.

2.23 ((cid:12) (cid:12)) by diagonalizing the coordinate system using the eigenvector expansion (2.45),
show that the volume contained within the hyperellipsoid corresponding to a constant

132

2. id203 distributions

mahalanobis distance     is given by

vd|  |1/2   d

(2.286)

where vd is the volume of the unit sphere in d dimensions, and the mahalanobis
distance is de   ned by (2.44).

2.24 ((cid:12) (cid:12)) www prove the identity (2.76) by multiplying both sides by the matrix

(cid:15)

(cid:16)

a b
c d

(2.287)

and making use of the de   nition (2.77).

2.25 ((cid:12) (cid:12))

in sections 2.3.1 and 2.3.2, we considered the conditional and marginal distri-
butions for a multivariate gaussian. more generally, we can consider a partitioning
of the components of x into three groups xa, xb, and xc, with a corresponding par-
titioning of the mean vector    and of the covariance matrix    in the form

(cid:22)

(cid:23)

  a
  b
  c

(cid:23)

(cid:22)

  aa   ab   ac
  ba   bb   bc
  ca   cb   cc

   =

,

   =

.

(2.288)

by making use of the results of section 2.3,    nd an expression for the conditional
distribution p(xa|xb) in which xc has been marginalized out.

2.26 ((cid:12) (cid:12)) a very useful result from id202 is the woodbury matrix inversion

formula given by

(a + bcd)   1 = a   1     a   1b(c   1 + da   1b)   1da   1.

(2.289)

by multiplying both sides by (a + bcd) prove the correctness of this result.

2.27 ((cid:12)) let x and z be two independent random vectors, so that p(x, z) = p(x)p(z).
show that the mean of their sum y = x + z is given by the sum of the means of each
of the variable separately. similarly, show that the covariance matrix of y is given by
the sum of the covariance matrices of x and z. con   rm that this result agrees with
that of exercise 1.10.

2.28 ((cid:12) (cid:12) (cid:12)) www consider a joint distribution over the variable

(cid:15)

(cid:16)

x
y

z =

(2.290)

whose mean and covariance are given by (2.108) and (2.105) respectively. by mak-
ing use of the results (2.92) and (2.93) show that the marginal distribution p(x) is
given (2.99). similarly, by making use of the results (2.81) and (2.82) show that the
conditional distribution p(y|x) is given by (2.100).

exercises

133

2.29 ((cid:12) (cid:12)) using the partitioned matrix inversion formula (2.76), show that the inverse of

the precision matrix (2.104) is given by the covariance matrix (2.105).

2.30 ((cid:12)) by starting from (2.107) and making use of the result (2.105), verify the result

(2.108).

2.31 ((cid:12) (cid:12)) consider two multidimensional random vectors x and z having gaussian
distributions p(x) = n (x|  x,   x) and p(z) = n (z|  z,   z) respectively, together
with their sum y = x+z. use the results (2.109) and (2.110) to    nd an expression for
the marginal distribution p(y) by considering the linear-gaussian model comprising
the product of the marginal distribution p(x) and the conditional distribution p(y|x).
2.32 ((cid:12) (cid:12) (cid:12)) www this exercise and the next provide practice at manipulating the
quadratic forms that arise in linear-gaussian models, as well as giving an indepen-
dent check of results derived in the main text. consider a joint distribution p(x, y)
de   ned by the marginal and conditional distributions given by (2.99) and (2.100).
by examining the quadratic form in the exponent of the joint distribution, and using
the technique of    completing the square    discussed in section 2.3,    nd expressions
for the mean and covariance of the marginal distribution p(y) in which the variable
x has been integrated out. to do this, make use of the woodbury matrix inversion
formula (2.289). verify that these results agree with (2.109) and (2.110) obtained
using the results of chapter 2.

2.33 ((cid:12) (cid:12) (cid:12)) consider the same joint distribution as in exercise 2.32, but now use the
technique of completing the square to    nd expressions for the mean and covariance
of the conditional distribution p(x|y). again, verify that these agree with the corre-
sponding expressions (2.111) and (2.112).

2.34 ((cid:12) (cid:12)) www to    nd the maximum likelihood solution for the covariance matrix
of a multivariate gaussian, we need to maximize the log likelihood function (2.118)
with respect to   , noting that the covariance matrix must be symmetric and positive
de   nite. here we proceed by ignoring these constraints and doing a straightforward
maximization. using the results (c.21), (c.26), and (c.28) from appendix c, show
that the covariance matrix    that maximizes the log likelihood function (2.118) is
given by the sample covariance (2.122). we note that the    nal result is necessarily
symmetric and positive de   nite (provided the sample covariance is nonsingular).

2.35 ((cid:12) (cid:12)) use the result (2.59) to prove (2.62). now, using the results (2.59), and (2.62),

show that

e[xnxm] =     t + inm  

(2.291)
where xn denotes a data point sampled from a gaussian distribution with mean   
and covariance   , and inm denotes the (n, m) element of the identity matrix. hence
prove the result (2.124).

2.36 ((cid:12) (cid:12)) www using an analogous procedure to that used to obtain (2.126), derive
an expression for the sequential estimation of the variance of a univariate gaussian

134

2. id203 distributions

distribution, by starting with the maximum likelihood expression

n(cid:2)

n=1

  2
ml =

1
n

(xn       )2.

(2.292)

verify that substituting the expression for a gaussian distribution into the robbins-
monro sequential estimation formula (2.135) gives a result of the same form, and
hence obtain an expression for the corresponding coef   cients an .

2.37 ((cid:12) (cid:12)) using an analogous procedure to that used to obtain (2.126), derive an ex-
pression for the sequential estimation of the covariance of a multivariate gaussian
distribution, by starting with the maximum likelihood expression (2.122). verify that
substituting the expression for a gaussian distribution into the robbins-monro se-
quential estimation formula (2.135) gives a result of the same form, and hence obtain
an expression for the corresponding coef   cients an .

2.38 ((cid:12)) use the technique of completing the square for the quadratic form in the expo-

nent to derive the results (2.141) and (2.142).

2.39 ((cid:12) (cid:12))

starting from the results (2.141) and (2.142) for the posterior distribution
of the mean of a gaussian random variable, dissect out the contributions from the
   rst n     1 data points and hence obtain expressions for the sequential update of
  n and   2
n . now derive the same results starting from the posterior distribution
p(  |x1, . . . , xn   1) = n (  |  n   1,   2
n   1) and multiplying by the likelihood func-
tion p(xn|  ) = n (xn|  ,   2) and then completing the square and normalizing to
obtain the posterior distribution after n observations.

2.40 ((cid:12) (cid:12)) www consider a d-dimensional gaussian random variable x with distribu-
tion n (x|  ,   ) in which the covariance    is known and for which we wish to infer
the mean    from a set of observations x = {x1, . . . , xn}. given a prior distribution
p(  ) = n (  |  0,   0),    nd the corresponding posterior distribution p(  |x).

2.41 ((cid:12)) use the de   nition of the gamma function (1.141) to show that the gamma dis-

tribution (2.146) is normalized.

2.42 ((cid:12) (cid:12)) evaluate the mean, variance, and mode of the gamma distribution (2.146).

2.43 ((cid:12)) the following distribution
p(x|  2, q) =

(cid:16)

(cid:15)
   |x|q
2  2

q

2(2  2)1/q  (1/q)

exp

p(x|  2, q) dx = 1

(cid:6)    

      

(2.293)

(2.294)

is a generalization of the univariate gaussian distribution. show that this distribution
is normalized so that

and that it reduces to the gaussian when q = 2. consider a regression model in
which the target variable is given by t = y(x, w) +   and   is a random noise

exercises

135

variable drawn from the distribution (2.293). show that the log likelihood function
over w and   2, for an observed data set of input vectors x = {x1, . . . , xn} and
corresponding target variables t = (t1, . . . , tn)t, is given by

ln p(t|x, w,   2) =     1
2  2

|y(xn, w)     tn|q     n
q

ln(2  2) + const

(2.295)

n(cid:2)

n=1

where    const    denotes terms independent of both w and   2. note that, as a function
of w, this is the lq error function considered in section 1.5.5.
2.44 ((cid:12) (cid:12)) consider a univariate gaussian distribution n (x|  ,   

   1) having conjugate
gaussian-gamma prior given by (2.154), and a data set x = {x1, . . . , xn} of i.i.d.
observations. show that the posterior distribution is also a gaussian-gamma distri-
bution of the same functional form as the prior, and write down expressions for the
parameters of this posterior distribution.

2.45 ((cid:12)) verify that the wishart distribution de   ned by (2.155) is indeed a conjugate

prior for the precision matrix of a multivariate gaussian.

2.46 ((cid:12)) www verify that evaluating the integral in (2.158) leads to the result (2.159).
2.47 ((cid:12)) www show that in the limit           , the t-distribution (2.159) becomes a
gaussian. hint: ignore the id172 coef   cient, and simply look at the depen-
dence on x.

2.48 ((cid:12)) by following analogous steps to those used to derive the univariate student   s
t-distribution (2.159), verify the result (2.162) for the multivariate form of the stu-
dent   s t-distribution, by marginalizing over the variable    in (2.161). using the
de   nition (2.161), show by exchanging integration variables that the multivariate
t-distribution is correctly normalized.

2.49 ((cid:12) (cid:12)) by using the de   nition (2.161) of the multivariate student   s t-distribution as a
convolution of a gaussian with a gamma distribution, verify the properties (2.164),
(2.165), and (2.166) for the multivariate t-distribution de   ned by (2.162).

2.50 ((cid:12)) show that in the limit           , the multivariate student   s t-distribution (2.162)

reduces to a gaussian with mean    and precision   .

2.51 ((cid:12)) www the various trigonometric identities used in the discussion of periodic

variables in this chapter can be proven easily from the relation

exp(ia) = cos a + i sin a

in which i is the square root of minus one. by considering the identity

exp(ia) exp(   ia) = 1
prove the result (2.177). similarly, using the identity

cos(a     b) = (cid:10) exp{i(a     b)}

(2.296)

(2.297)

(2.298)

136

2. id203 distributions

where (cid:10) denotes the real part, prove (2.178). finally, by using sin(a     b) =
(cid:11) exp{i(a     b)}, where (cid:11) denotes the imaginary part, prove the result (2.183).
for large m, the von mises distribution (2.179) becomes sharply peaked
around the mode   0. by de   ning    = m1/2(         0) and making the taylor ex-
pansion of the cosine function given by

2.52 ((cid:12) (cid:12))

show that as m        , the von mises distribution tends to a gaussian.

cos    = 1       2
2

+ o(  4)

(2.299)

2.53 ((cid:12)) using the trigonometric identity (2.183), show that solution of (2.182) for   0 is

given by (2.184).

2.54 ((cid:12)) by computing    rst and second derivatives of the von mises distribution (2.179),
and using i0(m) > 0 for m > 0, show that the maximum of the distribution occurs
when    =   0 and that the minimum occurs when    =   0 +    (mod 2  ).

2.55 ((cid:12)) by making use of the result (2.168), together with (2.184) and the trigonometric
identity (2.178), show that the maximum likelihood solution mml for the concentra-
tion of the von mises distribution satis   es a(mml) = r where r is the radius of the
mean of the observations viewed as unit vectors in the two-dimensional euclidean
plane, as illustrated in figure 2.17.

2.56 ((cid:12) (cid:12)) www express the beta distribution (2.13), the gamma distribution (2.146),
and the von mises distribution (2.179) as members of the exponential family (2.194)
and thereby identify their natural parameters.

2.57 ((cid:12)) verify that the multivariate gaussian distribution can be cast in exponential
family form (2.194) and derive expressions for   , u(x), h(x) and g(  ) analogous to
(2.220)   (2.223).

2.58 ((cid:12)) the result (2.226) showed that the negative gradient of ln g(  ) for the exponen-
tial family is given by the expectation of u(x). by taking the second derivatives of
(2.195), show that

          ln g(  ) = e[u(x)u(x)t]     e[u(x)]e[u(x)t] = cov[u(x)].

(2.300)

2.59 ((cid:12)) by changing variables using y = x/  , show that the density (2.236) will be

correctly normalized, provided f(x) is correctly normalized.

2.60 ((cid:12) (cid:12)) www consider a histogram-like density model in which the space x is di-
vided into    xed regions for which the density p(x) takes the constant value hi over
the ith region, and that the volume of region i is denoted    i. suppose we have a set
of n observations of x such that ni of these observations fall in region i. using a
lagrange multiplier to enforce the id172 constraint on the density, derive an
expression for the maximum likelihood estimator for the {hi}.

2.61 ((cid:12)) show that the k-nearest-neighbour density model de   nes an improper distribu-

tion whose integral over all space is divergent.

3

linear

models for
regression

the focus so far in this book has been on unsupervised learning, including topics
such as density estimation and data id91. we turn now to a discussion of super-
vised learning, starting with regression. the goal of regression is to predict the value
of one or more continuous target variables t given the value of a d-dimensional vec-
tor x of input variables. we have already encountered an example of a regression
problem when we considered polynomial curve    tting in chapter 1. the polynomial
is a speci   c example of a broad class of functions called id75 models,
which share the property of being linear functions of the adjustable parameters, and
which will form the focus of this chapter. the simplest form of id75
models are also linear functions of the input variables. however, we can obtain a
much more useful class of functions by taking linear combinations of a    xed set of
nonlinear functions of the input variables, known as basis functions. such models
are linear functions of the parameters, which gives them simple analytical properties,
and yet can be nonlinear with respect to the input variables.

137

138

3. linear models for regression

given a training data set comprising n observations {xn}, where n = 1, . . . , n,
together with corresponding target values {tn}, the goal is to predict the value of t
for a new value of x. in the simplest approach, this can be done by directly con-
structing an appropriate function y(x) whose values for new inputs x constitute the
predictions for the corresponding values of t. more generally, from a probabilistic
perspective, we aim to model the predictive distribution p(t|x) because this expresses
our uncertainty about the value of t for each value of x. from this conditional dis-
tribution we can make predictions of t, for any new value of x, in such a way as to
minimize the expected value of a suitably chosen id168. as discussed in sec-
tion 1.5.5, a common choice of id168 for real-valued variables is the squared
loss, for which the optimal solution is given by the conditional expectation of t.

although linear models have signi   cant limitations as practical techniques for
pattern recognition, particularly for problems involving input spaces of high dimen-
sionality, they have nice analytical properties and form the foundation for more so-
phisticated models to be discussed in later chapters.

3.1. linear basis function models

the simplest linear model for regression is one that involves a linear combination of
the input variables

y(x, w) = w0 + w1x1 + . . . + wdxd

(3.1)

where x = (x1, . . . , xd)t. this is often simply known as id75. the key
property of this model is that it is a linear function of the parameters w0, . . . , wd. it is
also, however, a linear function of the input variables xi, and this imposes signi   cant
limitations on the model. we therefore extend the class of models by considering
linear combinations of    xed nonlinear functions of the input variables, of the form

m   1(cid:2)

y(x, w) = w0 +

wj  j(x)

(3.2)

where   j(x) are known as basis functions. by denoting the maximum value of the
index j by m     1, the total number of parameters in this model will be m.

j=1

the parameter w0 allows for any    xed offset in the data and is sometimes called
a bias parameter (not to be confused with    bias    in a statistical sense). it is often
convenient to de   ne an additional dummy    basis function      0(x) = 1 so that

m   1(cid:2)

y(x, w) =

wj  j(x) = wt  (x)

(3.3)

j=0

where w = (w0, . . . , wm   1)t and    = (  0, . . . ,   m   1)t. in many practical ap-
plications of pattern recognition, we will apply some form of    xed pre-processing,

3.1. linear basis function models

139

or feature extraction, to the original data variables. if the original variables com-
prise the vector x, then the features can be expressed in terms of the basis functions
{  j(x)}.

by using nonlinear basis functions, we allow the function y(x, w) to be a non-
linear function of the input vector x. functions of the form (3.2) are called linear
models, however, because this function is linear in w. it is this linearity in the pa-
rameters that will greatly simplify the analysis of this class of models. however, it
also leads to some signi   cant limitations, as we discuss in section 3.6.

the example of polynomial regression considered in chapter 1 is a particular
example of this model in which there is a single input variable x, and the basis func-
tions take the form of powers of x so that   j(x) = xj. one limitation of polynomial
basis functions is that they are global functions of the input variable, so that changes
in one region of input space affect all other regions. this can be resolved by dividing
the input space up into regions and    t a different polynomial in each region, leading
to spline functions (hastie et al., 2001).

there are many other possible choices for the basis functions, for example

(cid:12)
   (x       j)2

(cid:13)

2s2

  j(x) = exp

where the   j govern the locations of the basis functions in input space, and the pa-
rameter s governs their spatial scale. these are usually referred to as    gaussian   
basis functions, although it should be noted that they are not required to have a prob-
abilistic interpretation, and in particular the id172 coef   cient is unimportant
because these basis functions will be multiplied by adaptive parameters wj.

another possibility is the sigmoidal basis function of the form

(cid:18)

(cid:17)

x       j

s

  j(x) =   

where   (a) is the logistic sigmoid function de   ned by

  (a) =

1

1 + exp(   a) .

(3.4)

(3.5)

(3.6)

equivalently, we can use the    tanh    function because this is related to the logistic
sigmoid by tanh(a) = 2  (a)     1, and so a general linear combination of logistic
sigmoid functions is equivalent to a general linear combination of    tanh    functions.
these various choices of basis function are illustrated in figure 3.1.

yet another possible choice of basis function is the fourier basis, which leads to
an expansion in sinusoidal functions. each basis function represents a speci   c fre-
quency and has in   nite spatial extent. by contrast, basis functions that are localized
to    nite regions of input space necessarily comprise a spectrum of different spatial
frequencies. in many signal processing applications, it is of interest to consider ba-
sis functions that are localized in both space and frequency, leading to a class of
functions known as wavelets. these are also de   ned to be mutually orthogonal, to
simplify their application. wavelets are most applicable when the input values live

140

3. linear models for regression

1

0.5

0

   0.5

   1

   1

1

0.75

0.5

0.25

0

   1

0

1

1

0.75

0.5

0.25

0
   1

0

1

0

1

figure 3.1 examples of basis functions, showing polynomials on the left, gaussians of the form (3.4) in the
centre, and sigmoidal of the form (3.5) on the right.

on a regular lattice, such as the successive time points in a temporal sequence, or the
pixels in an image. useful texts on wavelets include ogden (1997), mallat (1999),
and vidakovic (1999).

most of the discussion in this chapter, however, is independent of the particular
choice of basis function set, and so for most of our discussion we shall not specify
the particular form of the basis functions, except for the purposes of numerical il-
lustration. indeed, much of our discussion will be equally applicable to the situation
in which the vector   (x) of basis functions is simply the identity   (x) = x. fur-
thermore, in order to keep the notation simple, we shall focus on the case of a single
target variable t. however, in section 3.1.5, we consider brie   y the modi   cations
needed to deal with multiple target variables.

3.1.1 maximum likelihood and least squares
in chapter 1, we    tted polynomial functions to data sets by minimizing a sum-
of-squares error function. we also showed that this error function could be motivated
as the maximum likelihood solution under an assumed gaussian noise model. let
us return to this discussion and consider the least squares approach, and its relation
to maximum likelihood, in more detail.

as before, we assume that the target variable t is given by a deterministic func-

tion y(x, w) with additive gaussian noise so that

t = y(x, w) +  

(3.7)

where   is a zero mean gaussian random variable with precision (inverse variance)
  . thus we can write

p(t|x, w,   ) = n (t|y(x, w),   

   1).

(3.8)

section 1.5.5

recall that, if we assume a squared id168, then the optimal prediction, for a
new value of x, will be given by the conditional mean of the target variable. in the
case of a gaussian conditional distribution of the form (3.8), the conditional mean

3.1. linear basis function models

(cid:6)

will be simply

e[t|x] =

tp(t|x) dt = y(x, w).

141

(3.9)

note that the gaussian noise assumption implies that the conditional distribution of
t given x is unimodal, which may be inappropriate for some applications. an ex-
tension to mixtures of conditional gaussian distributions, which permit multimodal
conditional distributions, will be discussed in section 14.5.1.
now consider a data set of inputs x = {x1, . . . , xn} with corresponding target
values t1, . . . , tn . we group the target variables {tn} into a column vector that we
denote by t where the typeface is chosen to distinguish it from a single observation
of a multivariate target, which would be denoted t. making the assumption that
these data points are drawn independently from the distribution (3.8), we obtain the
following expression for the likelihood function, which is a function of the adjustable
parameters w and   , in the form

p(t|x, w,   ) =

n (tn|wt  (xn),   

   1)

(3.10)

where we have used (3.3). note that in supervised learning problems such as regres-
sion (and classi   cation), we are not seeking to model the distribution of the input
variables. thus x will always appear in the set of conditioning variables, and so
from now on we will drop the explicit x from expressions such as p(t|x, w,   ) in or-
der to keep the notation uncluttered. taking the logarithm of the likelihood function,
and making use of the standard form (1.46) for the univariate gaussian, we have

ln p(t|w,   ) =

lnn (tn|wt  (xn),   

   1)

= n
2

ln        n
2

ln(2  )       ed(w)

(3.11)

where the sum-of-squares error function is de   ned by

ed(w) =

1
2

{tn     wt  (xn)}2.

(3.12)

having written down the likelihood function, we can use maximum likelihood to
determine w and   . consider    rst the maximization with respect to w. as observed
already in section 1.2.5, we see that maximization of the likelihood function under a
conditional gaussian noise distribution for a linear model is equivalent to minimizing
a sum-of-squares error function given by ed(w). the gradient of the log likelihood
function (3.11) takes the form

    ln p(t|w,   ) =

tn     wt  (xn)

  (xn)t.

(3.13)

(cid:27)

n(cid:14)

n=1

n(cid:2)

n=1

n(cid:2)

n=1

n(cid:2)

(cid:26)

n=1

142

3. linear models for regression

setting this gradient to zero gives

n(cid:2)

n=1
solving for w we obtain

(cid:22)
n(cid:2)
(cid:11)   1   tt

n=1

(cid:10)

0 =

tn  (xn)t     wt

  (xn)  (xn)t

(cid:23)

.

(3.14)

   =

the quantity

wml =

(3.15)
which are known as the normal equations for the least squares problem. here    is an
n  m matrix, called the design matrix, whose elements are given by   nj =   j(xn),
so that

  t  

               0(x1)

  0(x2)

...

  1(x1)
  1(x2)

...

       (cid:10)

  m   1(x1)
  m   1(x2)

      
      
...
         m   1(xn )

...

(cid:11)   1   t

  0(xn )   1(xn )

             .

  

  t  

(3.17)
is known as the moore-penrose pseudo-inverse of the matrix    (rao and mitra,
1971; golub and van loan, 1996). it can be regarded as a generalization of the
notion of matrix inverse to nonsquare matrices. indeed, if    is square and invertible,
then using the property (ab)   1 = b   1a   1 we see that   

          

   1.

at this point, we can gain some insight into the role of the bias parameter w0. if

we make the bias parameter explicit, then the error function (3.12) becomes

(3.16)

ed(w) =

1
2

wj  j(xn)}2.

(3.18)

setting the derivative with respect to w0 equal to zero, and solving for w0, we obtain

n(cid:2)
{tn     w0     m   1(cid:2)
w0 = t     m   1(cid:2)

wj  j

n=1

j=1

(3.19)

where we have de   ned

t =

1
n

n(cid:2)

n=1

tn,

j=1

  j =

1
n

n(cid:2)

n=1

  j(xn).

(3.20)

thus the bias w0 compensates for the difference between the averages (over the
training set) of the target values and the weighted sum of the averages of the basis
function values.

we can also maximize the log likelihood function (3.11) with respect to the noise

precision parameter   , giving

1
  ml

=

1
n

n(cid:2)

n=1

{tn     wt

ml  (xn)}2

(3.21)

3.1. linear basis function models

143

figure 3.2 geometrical interpretation of the least-squares
solution, in an n-dimensional space whose axes
are the values of t1, . . . , tn . the least-squares
regression function is obtained by    nding the or-
thogonal projection of the data vector t onto the
subspace spanned by the basis functions   j(x)
in which each basis function is viewed as a vec-
tor   j of length n with elements   j(xn).

s

t

  1

y

  2

and so we see that the inverse of the noise precision is given by the residual variance
of the target values around the regression function.

3.1.2 geometry of least squares
at this point, it is instructive to consider the geometrical interpretation of the
least-squares solution. to do this we consider an n-dimensional space whose axes
are given by the tn, so that t = (t1, . . . , tn)t is a vector in this space. each basis
function   j(xn), evaluated at the n data points, can also be represented as a vector in
the same space, denoted by   j, as illustrated in figure 3.2. note that   j corresponds
to the jth column of   , whereas   (xn) corresponds to the nth row of   . if the
number m of basis functions is smaller than the number n of data points, then the
m vectors   j(xn) will span a linear subspace s of dimensionality m. we de   ne
y to be an n-dimensional vector whose nth element is given by y(xn, w), where
n = 1, . . . , n. because y is an arbitrary linear combination of the vectors   j, it can
live anywhere in the m-dimensional subspace. the sum-of-squares error (3.12) is
then equal (up to a factor of 1/2) to the squared euclidean distance between y and
t. thus the least-squares solution for w corresponds to that choice of y that lies in
subspace s and that is closest to t. intuitively, from figure 3.2, we anticipate that
this solution corresponds to the orthogonal projection of t onto the subspace s. this
is indeed the case, as can easily be veri   ed by noting that the solution for y is given
by   wml, and then con   rming that this takes the form of an orthogonal projection.
in practice, a direct solution of the normal equations can lead to numerical dif   -
culties when   t   is close to singular. in particular, when two or more of the basis
vectors   j are co-linear, or nearly so, the resulting parameter values can have large
magnitudes. such near degeneracies will not be uncommon when dealing with real
data sets. the resulting numerical dif   culties can be addressed using the technique
of singular value decomposition, or svd (press et al., 1992; bishop and nabney,
2008). note that the addition of a id173 term ensures that the matrix is non-
singular, even in the presence of degeneracies.

3.1.3 sequential learning
batch techniques, such as the maximum likelihood solution (3.15), which in-
volve processing the entire training set in one go, can be computationally costly for
large data sets. as we have discussed in chapter 1, if the data set is suf   ciently large,
it may be worthwhile to use sequential algorithms, also known as on-line algorithms,

exercise 3.2

144

3. linear models for regression

in which the data points are considered one at a time, and the model parameters up-
dated after each such presentation. sequential learning is also appropriate for real-
time applications in which the data observations are arriving in a continuous stream,
and predictions must be made before all of the data points are seen.

we can obtain a sequential learning algorithm by applying the technique of
stochastic id119, also known as sequential id119, as follows. if
the error function comprises a sum over data points e =
n en, then after presen-
tation of pattern n, the stochastic id119 algorithm updates the parameter
vector w using

(cid:5)

(3.22)
where    denotes the iteration number, and    is a learning rate parameter. we shall
discuss the choice of value for    shortly. the value of w is initialized to some starting
vector w(0). for the case of the sum-of-squares error function (3.12), this gives

w(   +1) = w(   )          en

w(   +1) = w(   ) +   (tn     w(   )t  n)  n

(3.23)

where   n =   (xn). this is known as least-mean-squares or the lms algorithm.
the value of    needs to be chosen with care to ensure that the algorithm converges
(bishop and nabney, 2008).

3.1.4 regularized least squares
in section 1.1, we introduced the idea of adding a id173 term to an
error function in order to control over-   tting, so that the total error function to be
minimized takes the form

ed(w) +   ew (w)

(3.24)
where    is the id173 coef   cient that controls the relative importance of the
data-dependent error ed(w) and the id173 term ew (w). one of the sim-
plest forms of regularizer is given by the sum-of-squares of the weight vector ele-
ments

if we also consider the sum-of-squares error function given by

ew (w) =

wtw.

1
2

n(cid:2)

n=1

e(w) =

1
2

{tn     wt  (xn)}2

n(cid:2)

n=1

1
2

{tn     wt  (xn)}2 +   
2

wtw.

(3.25)

(3.26)

(3.27)

then the total error function becomes

this particular choice of regularizer is known in the machine learning literature as
weight decay because in sequential learning algorithms, it encourages weight values
to decay towards zero, unless supported by the data. in statistics, it provides an ex-
ample of a parameter shrinkage method because it shrinks parameter values towards

3.1. linear basis function models

145

q = 0.5

q = 1

q = 2

q = 4

figure 3.3 contours of the id173 term in (3.29) for various values of the parameter q.

(cid:10)

m(cid:2)

zero. it has the advantage that the error function remains a quadratic function of
w, and so its exact minimizer can be found in closed form. speci   cally, setting the
gradient of (3.27) with respect to w to zero, and solving for w as before, we obtain

w =

  i +   t  

(3.28)

this represents a simple extension of the least-squares solution (3.15).

a more general regularizer is sometimes used, for which the regularized error

(cid:11)   1   tt.
m(cid:2)

takes the form

1
2

n(cid:2)

n=1

{tn     wt  (xn)}2 +   
2

j=1

|wj|q

(3.29)

where q = 2 corresponds to the quadratic regularizer (3.27). figure 3.3 shows con-
tours of the id173 function for different values of q.

the case of q = 1 is know as the lasso in the statistics literature (tibshirani,
it has the property that if    is suf   ciently large, some of the coef   cients
1996).
wj are driven to zero, leading to a sparse model in which the corresponding basis
functions play no role. to see this, we    rst note that minimizing (3.29) is equivalent
to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint

|wj|q (cid:1)   

(3.30)

j=1

for an appropriate value of the parameter   , where the two approaches can be related
using lagrange multipliers. the origin of the sparsity can be seen from figure 3.4,
which shows that the minimum of the error function, subject to the constraint (3.30).
as    is increased, so an increasing number of parameters are driven to zero.

id173 allows complex models to be trained on data sets of limited size
without severe over-   tting, essentially by limiting the effective model complexity.
however, the problem of determining the optimal model complexity is then shifted
from one of    nding the appropriate number of basis functions to one of determining
a suitable value of the id173 coef   cient   . we shall return to the issue of
model complexity later in this chapter.

exercise 3.5

appendix e

146

3. linear models for regression

figure 3.4 plot of
the contours
of the unregularized error function
(blue) along with the constraint re-
gion (3.30) for the quadratic regular-
izer q = 2 on the left and the lasso
regularizer q = 1 on the right,
in
which the optimum value for the pa-
rameter vector w is denoted by w(cid:1).
the lasso gives a sparse solution in
which w(cid:1)

1 = 0.

w2

w2

w(cid:1)

w(cid:1)

w1

w1

for the remainder of this chapter we shall focus on the quadratic regularizer

(3.27) both for its practical importance and its analytical tractability.

3.1.5 multiple outputs
so far, we have considered the case of a single target variable t. in some applica-
tions, we may wish to predict k > 1 target variables, which we denote collectively
by the target vector t. this could be done by introducing a different set of basis func-
tions for each component of t, leading to multiple, independent regression problems.
however, a more interesting, and more common, approach is to use the same set of
basis functions to model all of the components of the target vector so that

y(x, w) = wt  (x)

(3.31)
where y is a k-dimensional column vector, w is an m    k matrix of parameters,
and   (x) is an m-dimensional column vector with elements   j(x), with   0(x) = 1
as before. suppose we take the conditional distribution of the target vector to be an
isotropic gaussian of the form

p(t|x, w,   ) = n (t|wt  (x),   

   1i).

(3.32)

if we have a set of observations t1, . . . , tn , we can combine these into a matrix t
of size n    k such that the nth row is given by tt
n. similarly, we can combine the
input vectors x1, . . . , xn into a matrix x. the log likelihood function is then given
by

ln p(t|x, w,   ) =

lnn (tn|wt  (xn),   

   1i)

(cid:15)

(cid:16)

  
2  

      
2

n(cid:2)

n=1

n=1

= n k
2

ln

      tn     wt  (xn)
      2

. (3.33)

n(cid:2)

3.2. the bias-variance decomposition

147

as before, we can maximize this function with respect to w, giving

(cid:10)

  t  

(cid:11)   1   tt.
(cid:11)   1   ttk =   

   

tk

wml =

(cid:10)

wk =

  t  

if we examine this result for each target variable tk, we have

(3.34)

(3.35)

exercise 3.6

where tk is an n-dimensional column vector with components tnk for n = 1, . . . n.
thus the solution to the regression problem decouples between the different target
   
variables, and we need only compute a single pseudo-inverse matrix   
, which is
shared by all of the vectors wk.

the extension to general gaussian noise distributions having arbitrary covari-
ance matrices is straightforward. again, this leads to a decoupling into k inde-
pendent regression problems. this result is unsurprising because the parameters w
de   ne only the mean of the gaussian noise distribution, and we know from sec-
tion 2.3.4 that the maximum likelihood solution for the mean of a multivariate gaus-
sian is independent of the covariance. from now on, we shall therefore consider a
single target variable t for simplicity.

3.2. the bias-variance decomposition

so far in our discussion of linear models for regression, we have assumed that the
form and number of basis functions are both    xed. as we have seen in chapter 1,
the use of maximum likelihood, or equivalently least squares, can lead to severe
over-   tting if complex models are trained using data sets of limited size. however,
limiting the number of basis functions in order to avoid over-   tting has the side
effect of limiting the    exibility of the model to capture interesting and important
trends in the data. although the introduction of id173 terms can control
over-   tting for models with many parameters, this raises the question of how to
determine a suitable value for the id173 coef   cient   . seeking the solution
that minimizes the regularized error function with respect to both the weight vector
w and the id173 coef   cient    is clearly not the right approach since this
leads to the unregularized solution with    = 0.

as we have seen in earlier chapters, the phenomenon of over-   tting is really an
unfortunate property of maximum likelihood and does not arise when we marginalize
over parameters in a bayesian setting. in this chapter, we shall consider the bayesian
view of model complexity in some depth. before doing so, however, it is instructive
to consider a frequentist viewpoint of the model complexity issue, known as the bias-
variance trade-off. although we shall introduce this concept in the context of linear
basis function models, where it is easy to illustrate the ideas using simple examples,
the discussion has more general applicability.

in section 1.5.5, when we discussed decision theory for regression problems,
we considered various id168s each of which leads to a corresponding optimal
prediction once we are given the conditional distribution p(t|x). a popular choice is

148

3. linear models for regression

(cid:6)

(cid:6)

(cid:6)

form

e[l] =

the squared id168, for which the optimal prediction is given by the conditional
expectation, which we denote by h(x) and which is given by

h(x) = e[t|x] =

tp(t|x) dt.

(3.36)

at this point, it is worth distinguishing between the squared id168 arising
from decision theory and the sum-of-squares error function that arose in the maxi-
mum likelihood estimation of model parameters. we might use more sophisticated
techniques than least squares, for example id173 or a fully bayesian ap-
proach, to determine the conditional distribution p(t|x). these can all be combined
with the squared id168 for the purpose of making predictions.

we showed in section 1.5.5 that the expected squared loss can be written in the

{y(x)     h(x)}2

p(x) dx +

{h(x)     t}2p(x, t) dx dt.

(3.37)

recall that the second term, which is independent of y(x), arises from the intrinsic
noise on the data and represents the minimum achievable value of the expected loss.
the    rst term depends on our choice for the function y(x), and we will seek a so-
lution for y(x) which makes this term a minimum. because it is nonnegative, the
smallest that we can hope to make this term is zero. if we had an unlimited supply of
data (and unlimited computational resources), we could in principle    nd the regres-
sion function h(x) to any desired degree of accuracy, and this would represent the
optimal choice for y(x). however, in practice we have a data set d containing only
a    nite number n of data points, and consequently we do not know the regression
function h(x) exactly.

if we model the h(x) using a parametric function y(x, w) governed by a pa-
rameter vector w, then from a bayesian perspective the uncertainty in our model is
expressed through a posterior distribution over w. a frequentist treatment, however,
involves making a point estimate of w based on the data set d, and tries instead
to interpret the uncertainty of this estimate through the following thought experi-
ment. suppose we had a large number of data sets each of size n and each drawn
independently from the distribution p(t, x). for any given data set d, we can run
our learning algorithm and obtain a prediction function y(x;d). different data sets
from the ensemble will give different functions and consequently different values of
the squared loss. the performance of a particular learning algorithm is then assessed
by taking the average over this ensemble of data sets.
d takes the form
(3.38)
because this quantity will be dependent on the particular data set d, we take its aver-
age over the ensemble of data sets. if we add and subtract the quantity ed[y(x;d)]

consider the integrand of the    rst term in (3.37), which for a particular data set

{y(x;d)     h(x)}2.

3.2. the bias-variance decomposition

149

inside the braces, and then expand, we obtain

{y(x;d)     ed[y(x;d)] + ed[y(x;d)]     h(x)}2
= {y(x;d)     ed[y(x;d)]}2 + {ed[y(x;d)]     h(x)}2
+2{y(x;d)     ed[y(x;d)]}{ed[y(x;d)]     h(x)}.

(3.39)
we now take the expectation of this expression with respect to d and note that the
   nal term will vanish, giving

(cid:8){y(x;d)     h(x)}2
(
)*

ed
= {ed[y(x;d)]     h(x)}2

(cid:9)

+

(cid:8){y(x;d)     ed[y(x;d)]}2

)*

(cid:9)
+

(

+ ed

.

(3.40)

(bias)2

variance

we see that the expected squared difference between y(x;d) and the regression
function h(x) can be expressed as the sum of two terms. the    rst term, called the
squared bias, represents the extent to which the average prediction over all data sets
differs from the desired regression function. the second term, called the variance,
measures the extent to which the solutions for individual data sets vary around their
average, and hence this measures the extent to which the function y(x;d) is sensitive
to the particular choice of data set. we shall provide some intuition to support these
de   nitions shortly when we consider a simple example.

so far, we have considered a single input value x. if we substitute this expansion
back into (3.37), we obtain the following decomposition of the expected squared loss

expected loss = (bias)2 + variance + noise

where

(bias)2 =

variance =

noise =

(cid:6)
(cid:6)
(cid:6)

{ed[y(x;d)]     h(x)}2p(x) dx

(cid:8){y(x;d)     ed[y(x;d)]}2

ed
{h(x)     t}2p(x, t) dx dt

(cid:9)

(3.41)

(3.42)

p(x) dx

(3.43)

(3.44)

and the bias and variance terms now refer to integrated quantities.

our goal is to minimize the expected loss, which we have decomposed into the
sum of a (squared) bias, a variance, and a constant noise term. as we shall see, there
is a trade-off between bias and variance, with very    exible models having low bias
and high variance, and relatively rigid models having high bias and low variance.
the model with the optimal predictive capability is the one that leads to the best
balance between bias and variance. this is illustrated by considering the sinusoidal
data set from chapter 1. here we generate 100 data sets, each containing n = 25
data points, independently from the sinusoidal curve h(x) = sin(2  x). the data
sets are indexed by l = 1, . . . , l, where l = 100, and for each data set d(l) we

appendix a

150

3. linear models for regression

t

1

0

   1

t

1

0

   1

t

1

0

   1

0

0

0

ln    = 2.6

t

1

0

   1

x

1

0

x

1

ln    =    0.31

t

1

0

   1

x

1

0

x

1

ln    =    2.4

t

1

0

   1

x

1

0

x

1

figure 3.5 illustration of the dependence of bias and variance on model complexity, governed by a regulariza-
tion parameter   , using the sinusoidal data set from chapter 1. there are l = 100 data sets, each having n = 25
data points, and there are 24 gaussian basis functions in the model so that the total number of parameters is
m = 25 including the bias parameter. the left column shows the result of    tting the model to the data sets for
various values of ln    (for clarity, only 20 of the 100    ts are shown). the right column shows the corresponding
average of the 100    ts (red) along with the sinusoidal function from which the data sets were generated (green).

3.2. the bias-variance decomposition

151

figure 3.6 plot of squared bias and variance,
together with their sum, correspond-
ing to the results shown in fig-
ure 3.5. also shown is the average
test set error for a test data set size
of 1000 points. the minimum value
of (bias)2 + variance occurs around
ln    =    0.31, which is close to the
value that gives the minimum error
on the test data.

0.15

0.12

0.09

0.06

0.03

0
   3

(bias)2
variance
(bias)2 + variance
test error

   2

   1

0

1

2

ln   

   t a model with 24 gaussian basis functions by minimizing the regularized error
function (3.27) to give a prediction function y(l)(x) as shown in figure 3.5. the
top row corresponds to a large value of the id173 coef   cient    that gives low
variance (because the red curves in the left plot look similar) but high bias (because
the two curves in the right plot are very different). conversely on the bottom row, for
which    is small, there is large variance (shown by the high variability between the
red curves in the left plot) but low bias (shown by the good    t between the average
model    t and the original sinusoidal function). note that the result of averaging many
solutions for the complex model with m = 25 is a very good    t to the regression
function, which suggests that averaging may be a bene   cial procedure. indeed, a
weighted averaging of multiple solutions lies at the heart of a bayesian approach,
although the averaging is with respect to the posterior distribution of parameters, not
with respect to multiple data sets.

we can also examine the bias-variance trade-off quantitatively for this example.

the average prediction is estimated from

y(x) =

1
l

l(cid:2)

l=1

y(l)(x)

and the integrated squared bias and integrated variance are then given by

n(cid:2)
n(cid:2)

n=1

l(cid:2)

(cid:26)

1
l

n=1

l=1

{y(xn)     h(xn)}2

(cid:27)2

y(l)(xn)     y(xn)

(bias)2 =

variance =

1
n

1
n

(3.45)

(3.46)

(3.47)

where the integral over x weighted by the distribution p(x) is approximated by a
   nite sum over data points drawn from that distribution. these quantities, along
with their sum, are plotted as a function of ln    in figure 3.6. we see that small
values of    allow the model to become    nely tuned to the noise on each individual

152

3. linear models for regression

data set leading to large variance. conversely, a large value of    pulls the weight
parameters towards zero leading to large bias.

although the bias-variance decomposition may provide some interesting in-
sights into the model complexity issue from a frequentist perspective, it is of lim-
ited practical value, because the bias-variance decomposition is based on averages
with respect to ensembles of data sets, whereas in practice we have only the single
observed data set. if we had a large number of independent training sets of a given
size, we would be better off combining them into a single large training set, which
of course would reduce the level of over-   tting for a given model complexity.

given these limitations, we turn in the next section to a bayesian treatment of
linear basis function models, which not only provides powerful insights into the
issues of over-   tting but which also leads to practical techniques for addressing the
question model complexity.

3.3. bayesian id75

in our discussion of maximum likelihood for setting the parameters of a linear re-
gression model, we have seen that the effective model complexity, governed by the
number of basis functions, needs to be controlled according to the size of the data
set. adding a id173 term to the log likelihood function means the effective
model complexity can then be controlled by the value of the id173 coef   -
cient, although the choice of the number and form of the basis functions is of course
still important in determining the overall behaviour of the model.

this leaves the issue of deciding the appropriate model complexity for the par-
ticular problem, which cannot be decided simply by maximizing the likelihood func-
tion, because this always leads to excessively complex models and over-   tting. in-
dependent hold-out data can be used to determine model complexity, as discussed
in section 1.3, but this can be both computationally expensive and wasteful of valu-
able data. we therefore turn to a bayesian treatment of id75, which will
avoid the over-   tting problem of maximum likelihood, and which will also lead to
automatic methods of determining model complexity using the training data alone.
again, for simplicity we will focus on the case of a single target variable t. ex-
tension to multiple target variables is straightforward and follows the discussion of
section 3.1.5.

3.3.1 parameter distribution
we begin our discussion of the bayesian treatment of id75 by in-
troducing a prior id203 distribution over the model parameters w. for the mo-
ment, we shall treat the noise precision parameter    as a known constant. first note
that the likelihood function p(t|w) de   ned by (3.10) is the exponential of a quadratic
function of w. the corresponding conjugate prior is therefore given by a gaussian
distribution of the form

p(w) = n (w|m0, s0)

(3.48)

having mean m0 and covariance s0.

exercise 3.7

exercise 3.8

3.3. bayesian id75

153

next we compute the posterior distribution, which is proportional to the product
of the likelihood function and the prior. due to the choice of a conjugate gaus-
sian prior distribution, the posterior will also be gaussian. we can evaluate this
distribution by the usual procedure of completing the square in the exponential, and
then    nding the id172 coef   cient using the standard result for a normalized
gaussian. however, we have already done the necessary work in deriving the gen-
eral result (2.116), which allows us to write down the posterior distribution directly
in the form

p(w|t) = n (w|mn , sn)

(cid:10)

(cid:11)

mn = sn
   1
   1
0 +     t  .
n = s
s

s

   1
0 m0 +     tt

(3.49)

(3.50)
(3.51)

where

note that because the posterior distribution is gaussian, its mode coincides with its
mean. thus the maximum posterior weight vector is simply given by wmap = mn .
   1i with        0, the mean mn
if we consider an in   nitely broad prior s0 =   
of the posterior distribution reduces to the maximum likelihood value wml given
by (3.15). similarly, if n = 0, then the posterior distribution reverts to the prior.
furthermore, if data points arrive sequentially, then the posterior distribution at any
stage acts as the prior distribution for the subsequent data point, such that the new
posterior distribution is again given by (3.49).

for the remainder of this chapter, we shall consider a particular form of gaus-
sian prior in order to simplify the treatment. speci   cally, we consider a zero-mean
isotropic gaussian governed by a single precision parameter    so that

p(w|  ) = n (w|0,   

   1i)

(3.52)

and the corresponding posterior distribution over w is then given by (3.49) with

mn =   sn   tt
   1
n =   i +     t  .
s

(3.53)
(3.54)

the log of the posterior distribution is given by the sum of the log likelihood and

the log of the prior and, as a function of w, takes the form

ln p(w|t) =       
2

{tn     wt  (xn)}2       
2

wtw + const.

(3.55)

maximization of this posterior distribution with respect to w is therefore equiva-
lent to the minimization of the sum-of-squares error function with the addition of a
quadratic id173 term, corresponding to (3.27) with    =   /  .

we can illustrate bayesian learning in a linear basis function model, as well as
the sequential update of a posterior distribution, using a simple example involving
straight-line    tting. consider a single input variable x, a single target variable t and

n(cid:2)

n=1

154

3. linear models for regression

a linear model of the form y(x, w) = w0 + w1x. because this has just two adap-
tive parameters, we can plot the prior and posterior distributions directly in parameter
space. we generate synthetic data from the function f(x, a) = a0 + a1x with param-
eter values a0 =    0.3 and a1 = 0.5 by    rst choosing values of xn from the uniform
distribution u(x|   1, 1), then evaluating f(xn, a), and    nally adding gaussian noise
with standard deviation of 0.2 to obtain the target values tn. our goal is to recover
the values of a0 and a1 from such data, and we will explore the dependence on the
size of the data set. we assume here that the noise variance is known and hence we
set the precision parameter to its true value    = (1/0.2)2 = 25. similarly, we    x
the parameter    to 2.0. we shall shortly discuss strategies for determining    and
   from the training data. figure 3.7 shows the results of bayesian learning in this
model as the size of the data set is increased and demonstrates the sequential nature
of bayesian learning in which the current posterior distribution forms the prior when
a new data point is observed. it is worth taking time to study this    gure in detail as
it illustrates several important aspects of bayesian id136. the    rst row of this
   gure corresponds to the situation before any data points are observed and shows a
plot of the prior distribution in w space together with six samples of the function
y(x, w) in which the values of w are drawn from the prior. in the second row, we
see the situation after observing a single data point. the location (x, t) of the data
point is shown by a blue circle in the right-hand column. in the left-hand column is a
plot of the likelihood function p(t|x, w) for this data point as a function of w. note
that the likelihood function provides a soft constraint that the line must pass close to
the data point, where close is determined by the noise precision   . for comparison,
the true parameter values a0 =    0.3 and a1 = 0.5 used to generate the data set
are shown by a white cross in the plots in the left column of figure 3.7. when we
multiply this likelihood function by the prior from the top row, and normalize, we
obtain the posterior distribution shown in the middle plot on the second row. sam-
ples of the regression function y(x, w) obtained by drawing samples of w from this
posterior distribution are shown in the right-hand plot. note that these sample lines
all pass close to the data point. the third row of this    gure shows the effect of ob-
serving a second data point, again shown by a blue circle in the plot in the right-hand
column. the corresponding likelihood function for this second data point alone is
shown in the left plot. when we multiply this likelihood function by the posterior
distribution from the second row, we obtain the posterior distribution shown in the
middle plot of the third row. note that this is exactly the same posterior distribution
as would be obtained by combining the original prior with the likelihood function
for the two data points. this posterior has now been in   uenced by two data points,
and because two points are suf   cient to de   ne a line this already gives a relatively
compact posterior distribution. samples from this posterior distribution give rise to
the functions shown in red in the third column, and we see that these functions pass
close to both of the data points. the fourth row shows the effect of observing a total
of 20 data points. the left-hand plot shows the likelihood function for the 20th data
point alone, and the middle plot shows the resulting posterior distribution that has
now absorbed information from all 20 observations. note how the posterior is much
sharper than in the third row. in the limit of an in   nite number of data points, the

3.3. bayesian id75

155

figure 3.7 illustration of sequential bayesian learning for a simple linear model of the form y(x, w) =

w0 + w1x. a detailed description of this    gure is given in the text.

156

3. linear models for regression

posterior distribution would become a delta function centred on the true parameter
values, shown by the white cross.

other forms of prior over the parameters can be considered. for instance, we

can generalize the gaussian prior to give

(cid:29)

(cid:17)

(cid:18)1/q

q
2

  
2

(cid:30)m

(cid:22)

(cid:23)

m(cid:2)

j=1

p(w|  ) =

1

  (1/q)

exp

      
2

|wj|q

(3.56)

in which q = 2 corresponds to the gaussian distribution, and only in this case is the
prior conjugate to the likelihood function (3.10). finding the maximum of the poste-
rior distribution over w corresponds to minimization of the regularized error function
(3.29). in the case of the gaussian prior, the mode of the posterior distribution was
equal to the mean, although this will no longer hold if q (cid:9)= 2.

3.3.2 predictive distribution
in practice, we are not usually interested in the value of w itself but rather in
making predictions of t for new values of x. this requires that we evaluate the
predictive distribution de   ned by
p(t|t,   ,   ) =

p(t|w,   )p(w|t,   ,   ) dw

(3.57)

(cid:6)

exercise 3.10

exercise 3.11

in which t is the vector of target values from the training set, and we have omitted the
corresponding input vectors from the right-hand side of the conditioning statements
to simplify the notation. the conditional distribution p(t|x, w,   ) of the target vari-
able is given by (3.8), and the posterior weight distribution is given by (3.49). we
see that (3.57) involves the convolution of two gaussian distributions, and so making
use of the result (2.115) from section 8.1.4, we see that the predictive distribution
takes the form

where the variance   2

p(t|x, t,   ,   ) = n (t|mt
n (x) of the predictive distribution is given by

n   (x),   2

n (x))

(3.58)

(3.59)

  2
n (x) =

1
  

+   (x)tsn   (x).

the    rst term in (3.59) represents the noise on the data whereas the second term
re   ects the uncertainty associated with the parameters w. because the noise process
and the distribution of w are independent gaussians, their variances are additive.
note that, as additional data points are observed, the posterior distribution becomes
n +1(x) (cid:1)
narrower. as a consequence it can be shown (qazaz et al., 1997) that   2
n (x). in the limit n        , the second term in (3.59) goes to zero, and the variance
  2
of the predictive distribution arises solely from the additive noise governed by the
parameter   .

as an illustration of the predictive distribution for bayesian id75
models, let us return to the synthetic sinusoidal data set of section 1.1. in figure 3.8,

t

1

0

   1

t

1

0

   1

0

0

3.3. bayesian id75

157

t

1

0

   1

x

1

0

x

1

t

1

0

   1

x

1

0

x

1

figure 3.8 examples of the predictive distribution (3.58) for a model consisting of 9 gaussian basis functions
of the form (3.4) using the synthetic sinusoidal data set of section 1.1. see the text for a detailed discussion.

we    t a model comprising a linear combination of gaussian basis functions to data
sets of various sizes and then look at the corresponding posterior distributions. here
the green curves correspond to the function sin(2  x) from which the data points
were generated (with the addition of gaussian noise). data sets of size n = 1,
n = 2, n = 4, and n = 25 are shown in the four plots by the blue circles. for
each plot, the red curve shows the mean of the corresponding gaussian predictive
distribution, and the red shaded region spans one standard deviation either side of
the mean. note that the predictive uncertainty depends on x and is smallest in the
neighbourhood of the data points. also note that the level of uncertainty decreases
as more data points are observed.

the plots in figure 3.8 only show the point-wise predictive variance as a func-
tion of x. in order to gain insight into the covariance between the predictions at
different values of x, we can draw samples from the posterior distribution over w,
and then plot the corresponding functions y(x, w), as shown in figure 3.9.

158

3. linear models for regression

t

1

0

   1

t

1

0

   1

0

0

t

1

0

   1

x

1

0

x

1

t

1

0

   1

x

1

0

x

1

figure 3.9 plots of the function y(x, w) using samples from the posterior distributions over w corresponding to
the plots in figure 3.8.

if we used localized basis functions such as gaussians, then in regions away
from the basis function centres, the contribution from the second term in the predic-
   1. thus,
tive variance (3.59) will go to zero, leaving only the noise contribution   
the model becomes very con   dent in its predictions when extrapolating outside the
region occupied by the basis functions, which is generally an undesirable behaviour.
this problem can be avoided by adopting an alternative bayesian approach to re-
gression known as a gaussian process.

note that, if both w and    are treated as unknown, then we can introduce a
conjugate prior distribution p(w,   ) that, from the discussion in section 2.3.6, will
be given by a gaussian-gamma distribution (denison et al., 2002). in this case, the
predictive distribution is a student   s t-distribution.

section 6.4

exercise 3.12
exercise 3.13

3.3. bayesian id75

159

figure 3.10 the equivalent ker-
nel k(x, x(cid:1)) for the gaussian basis
functions in figure 3.1, shown as
a plot of x versus x(cid:1)
, together with
three slices through this matrix cor-
responding to three different values
of x. the data set used to generate
this kernel comprised 200 values of
x equally spaced over the interval
(   1, 1).

3.3.3 equivalent kernel
the posterior mean solution (3.53) for the linear basis function model has an in-
teresting interpretation that will set the stage for kernel methods, including gaussian
processes. if we substitute (3.53) into the expression (3.3), we see that the predictive
mean can be written in the form

chapter 6

n(cid:2)

n=1

y(x, mn) = mt

n   (x) =     (x)tsn   tt =

    (x)tsn   (xn)tn

(3.60)

where sn is de   ned by (3.51). thus the mean of the predictive distribution at a point
x is given by a linear combination of the training set target variables tn, so that we
can write

n(cid:2)

y(x, mn) =

k(x, xn)tn

where the function

n=1

k(x, x(cid:4)) =     (x)tsn   (x(cid:4))

(3.61)

(3.62)

is known as the smoother matrix or the equivalent kernel. regression functions, such
as this, which make predictions by taking linear combinations of the training set
target values are known as linear smoothers. note that the equivalent kernel depends
on the input values xn from the data set because these appear in the de   nition of
sn . the equivalent kernel is illustrated for the case of gaussian basis functions in
(cid:4)) have been plotted as a function of
figure 3.10 in which the id81s k(x, x
(cid:4)
for three different values of x. we see that they are localized around x, and so the
x
mean of the predictive distribution at x, given by y(x, mn), is obtained by forming
a weighted combination of the target values in which data points close to x are given
higher weight than points further removed from x. intuitively, it seems reasonable
that we should weight local evidence more strongly than distant evidence. note that
this localization property holds not only for the localized gaussian basis functions
but also for the nonlocal polynomial and sigmoidal basis functions, as illustrated in
figure 3.11.

160

3. linear models for regression

figure 3.11 examples of equiva-
lent kernels k(x, x(cid:1)) for x = 0
plotted as a function of x(cid:1)
, corre-
sponding (left) to the polynomial ba-
sis functions and (right) to the sig-
moidal basis functions shown in fig-
ure 3.1. note that these are local-
ized functions of x(cid:1)
even though the
corresponding basis functions are
nonlocal.

0.04

0.02

0

   1

0.04

0.02

0

   1

0

1

0

1

further insight into the role of the equivalent kernel can be obtained by consid-

ering the covariance between y(x) and y(x(cid:4)), which is given by
cov[y(x), y(x(cid:4))] = cov[  (x)tw, wt  (x(cid:4))]

=   (x)tsn   (x(cid:4)) =   

   1k(x, x(cid:4))

(3.63)

where we have made use of (3.49) and (3.62). from the form of the equivalent
kernel, we see that the predictive mean at nearby points will be highly correlated,
whereas for more distant pairs of points the correlation will be smaller.

the predictive distribution shown in figure 3.8 allows us to visualize the point-
wise uncertainty in the predictions, governed by (3.59). however, by drawing sam-
ples from the posterior distribution over w, and plotting the corresponding model
functions y(x, w) as in figure 3.9, we are visualizing the joint uncertainty in the
posterior distribution between the y values at two (or more) x values, as governed by
the equivalent kernel.

the formulation of id75 in terms of a id81 suggests an
alternative approach to regression as follows. instead of introducing a set of basis
functions, which implicitly determines an equivalent kernel, we can instead de   ne
a localized kernel directly and use this to make predictions for new input vectors x,
given the observed training set. this leads to a practical framework for regression
(and classi   cation) called gaussian processes, which will be discussed in detail in
section 6.4.

we have seen that the effective kernel de   nes the weights by which the training
set target values are combined in order to make a prediction at a new value of x, and
it can be shown that these weights sum to one, in other words

n(cid:2)

k(x, xn) = 1

(3.64)

exercise 3.14

n=1

by noting that the summation is equivalent to considering the predictive mean(cid:1)y(x)

for all values of x. this intuitively pleasing result can easily be proven informally

for a set of target data in which tn = 1 for all n. provided the basis functions are
linearly independent, that there are more data points than basis functions, and that
one of the basis functions is constant (corresponding to the bias parameter), then it is
clear that we can    t the training data exactly and hence that the predictive mean will

be simply(cid:1)y(x) = 1, from which we obtain (3.64). note that the id81 can

3.4. bayesian model comparison

161

chapter 6

be negative as well as positive, so although it satis   es a summation constraint, the
corresponding predictions are not necessarily convex combinations of the training
set target variables.

finally, we note that the equivalent kernel (3.62) satis   es an important property
shared by id81s in general, namely that it can be expressed in the form an
inner product with respect to a vector   (x) of nonlinear functions, so that

k(x, z) =   (x)t  (z)

(3.65)

where   (x) =   1/2s1/2

n   (x).

3.4. bayesian model comparison

in chapter 1, we highlighted the problem of over-   tting as well as the use of cross-
validation as a technique for setting the values of id173 parameters or for
choosing between alternative models. here we consider the problem of model se-
lection from a bayesian perspective.
in this section, our discussion will be very
general, and then in section 3.5 we shall see how these ideas can be applied to the
determination of id173 parameters in id75.

as we shall see, the over-   tting associated with maximum likelihood can be
avoided by marginalizing (summing or integrating) over the model parameters in-
stead of making point estimates of their values. models can then be compared di-
rectly on the training data, without the need for a validation set. this allows all
available data to be used for training and avoids the multiple training runs for each
model associated with cross-validation. it also allows multiple complexity parame-
ters to be determined simultaneously as part of the training process. for example,
in chapter 7 we shall introduce the relevance vector machine, which is a bayesian
model having one complexity parameter for every training data point.

section 1.5.4

the bayesian view of model comparison simply involves the use of probabilities
to represent uncertainty in the choice of model, along with a consistent application
of the sum and product rules of id203. suppose we wish to compare a set of l
models {mi} where i = 1, . . . , l. here a model refers to a id203 distribution
over the observed data d. in the case of the polynomial curve-   tting problem, the
distribution is de   ned over the set of target values t, while the set of input values x
is assumed to be known. other types of model de   ne a joint distributions over x
and t. we shall suppose that the data is generated from one of these models but we
are uncertain which one. our uncertainty is expressed through a prior id203
distribution p(mi). given a training set d, we then wish to evaluate the posterior
distribution

(3.66)
the prior allows us to express a preference for different models. let us simply
assume that all models are given equal prior id203. the interesting term is
the model evidence p(d|mi) which expresses the preference shown by the data for

p(mi|d)     p(mi)p(d|mi).

162

3. linear models for regression

different models, and we shall examine this term in more detail shortly. the model
evidence is sometimes also called the marginal likelihood because it can be viewed
as a likelihood function over the space of models, in which the parameters have been
marginalized out. the ratio of model evidences p(d|mi)/p(d|mj) for two models
is known as a bayes factor (kass and raftery, 1995).

once we know the posterior distribution over models, the predictive distribution

is given, from the sum and product rules, by

l(cid:2)

(cid:6)

p(t|x,d) =

p(t|x,mi,d)p(mi|d).

(3.67)

i=1

this is an example of a mixture distribution in which the overall predictive distribu-
tion is obtained by averaging the predictive distributions p(t|x,mi,d) of individual
models, weighted by the posterior probabilities p(mi|d) of those models. for in-
stance, if we have two models that are a-posteriori equally likely and one predicts
a narrow distribution around t = a while the other predicts a narrow distribution
around t = b, the overall predictive distribution will be a bimodal distribution with
modes at t = a and t = b, not a single model at t = (a + b)/2.

a simple approximation to model averaging is to use the single most probable

model alone to make predictions. this is known as model selection.

for a model governed by a set of parameters w, the model evidence is given,

from the sum and product rules of id203, by

p(d|mi) =

p(d|w,mi)p(w|mi) dw.

(3.68)

chapter 11

from a sampling perspective, the marginal likelihood can be viewed as the proba-
bility of generating the data set d from a model whose parameters are sampled at
random from the prior. it is also interesting to note that the evidence is precisely the
normalizing term that appears in the denominator in bayes    theorem when evaluating
the posterior distribution over parameters because

p(w|d,mi) = p(d|w,mi)p(w|mi)

p(d|mi)

.

(3.69)

we can obtain some insight into the model evidence by making a simple approx-
imation to the integral over parameters. consider    rst the case of a model having a
single parameter w. the posterior distribution over parameters is proportional to
p(d|w)p(w), where we omit the dependence on the model mi to keep the notation
uncluttered. if we assume that the posterior distribution is sharply peaked around the
most probable value wmap, with width    wposterior, then we can approximate the in-
tegral by the value of the integrand at its maximum times the width of the peak. if we
further assume that the prior is    at with width    wprior so that p(w) = 1/   wprior,
then we have

(cid:6)

p(d) =

p(d|w)p(w) dw (cid:7) p(d|wmap)

   wposterior

   wprior

(3.70)

3.4. bayesian model comparison

163

figure 3.12 we can obtain a rough approximation to
the model evidence if we assume that
the posterior distribution over parame-
ters is sharply peaked around its mode
wmap.

   wposterior

and so taking logs we obtain

ln p(d) (cid:7) ln p(d|wmap) + ln

wmap

   wprior

w

(cid:15)

(cid:16)

.

   wposterior

   wprior

(3.71)

this approximation is illustrated in figure 3.12. the    rst term represents the    t to
the data given by the most probable parameter values, and for a    at prior this would
correspond to the log likelihood. the second term penalizes the model according to
its complexity. because    wposterior <    wprior this term is negative, and it increases
in magnitude as the ratio    wposterior/   wprior gets smaller. thus, if parameters are
   nely tuned to the data in the posterior distribution, then the penalty term is large.

for a model having a set of m parameters, we can make a similar approximation
for each parameter in turn. assuming that all parameters have the same ratio of
   wposterior/   wprior, we obtain

(cid:15)

(cid:16)

ln p(d) (cid:7) ln p(d|wmap) + m ln

   wposterior

   wprior

.

(3.72)

section 4.4.1

thus, in this very simple approximation, the size of the complexity penalty increases
linearly with the number m of adaptive parameters in the model. as we increase
the complexity of the model, the    rst term will typically decrease, because a more
complex model is better able to    t the data, whereas the second term will increase
due to the dependence on m. the optimal model complexity, as determined by
the maximum evidence, will be given by a trade-off between these two competing
terms. we shall later develop a more re   ned version of this approximation, based on
a gaussian approximation to the posterior distribution.

we can gain further insight into bayesian model comparison and understand
how the marginal likelihood can favour models of intermediate complexity by con-
sidering figure 3.13. here the horizontal axis is a one-dimensional representation
of the space of possible data sets, so that each point on this axis corresponds to a
speci   c data set. we now consider three models m1, m2 and m3 of successively
increasing complexity. imagine running these models generatively to produce exam-
ple data sets, and then looking at the distribution of data sets that result. any given

164

3. linear models for regression

p(d)

m1

figure 3.13 schematic illustration of

the
distribution of data sets for
three models of different com-
in which m1 is the
plexity,
simplest and m3 is the most
complex. note that the dis-
tributions are normalized.
in
for the partic-
this example,
ular observed data set d0,
the model m2 with intermedi-
ate complexity has the largest
evidence.

m2

d0

m3

d

model can generate a variety of different data sets since the parameters are governed
by a prior id203 distribution, and for any choice of the parameters there may
be random noise on the target variables. to generate a particular data set from a spe-
ci   c model, we    rst choose the values of the parameters from their prior distribution
p(w), and then for these parameter values we sample the data from p(d|w). a sim-
ple model (for example, based on a    rst order polynomial) has little variability and
so will generate data sets that are fairly similar to each other. its distribution p(d)
is therefore con   ned to a relatively small region of the horizontal axis. by contrast,
a complex model (such as a ninth order polynomial) can generate a great variety of
different data sets, and so its distribution p(d) is spread over a large region of the
space of data sets. because the distributions p(d|mi) are normalized, we see that
the particular data set d0 can have the highest value of the evidence for the model
of intermediate complexity. essentially, the simpler model cannot    t the data well,
whereas the more complex model spreads its predictive id203 over too broad a
range of data sets and so assigns relatively small id203 to any one of them.

implicit in the bayesian model comparison framework is the assumption that
the true distribution from which the data are generated is contained within the set of
models under consideration. provided this is so, we can show that bayesian model
comparison will on average favour the correct model. to see this, consider two
models m1 and m2 in which the truth corresponds to m1. for a given    nite data
set, it is possible for the bayes factor to be larger for the incorrect model. however, if
bayes factor in the form (cid:6)
we average the bayes factor over the distribution of data sets, we obtain the expected

p(d|m1) ln p(d|m1)
p(d|m2)

dd

(3.73)

section 1.6.1

where the average has been taken with respect to the true distribution of the data.
this quantity is an example of the id181 and satis   es the prop-
erty of always being positive unless the two distributions are equal in which case it
is zero. thus on average the bayes factor will always favour the correct model.

we have seen that the bayesian framework avoids the problem of over-   tting
and allows models to be compared on the basis of the training data alone. however,

3.5. the evidence approximation

165

a bayesian approach, like any approach to pattern recognition, needs to make as-
sumptions about the form of the model, and if these are invalid then the results can
be misleading. in particular, we see from figure 3.12 that the model evidence can
be sensitive to many aspects of the prior, such as the behaviour in the tails. indeed,
the evidence is not de   ned if the prior is improper, as can be seen by noting that
an improper prior has an arbitrary scaling factor (in other words, the id172
coef   cient is not de   ned because the distribution cannot be normalized). if we con-
sider a proper prior and then take a suitable limit in order to obtain an improper prior
(for example, a gaussian prior in which we take the limit of in   nite variance) then
the evidence will go to zero, as can be seen from (3.70) and figure 3.12. it may,
however, be possible to consider the evidence ratio between two models    rst and
then take a limit to obtain a meaningful answer.

in a practical application, therefore, it will be wise to keep aside an independent

test set of data on which to evaluate the overall performance of the    nal system.

3.5. the evidence approximation

in a fully bayesian treatment of the linear basis function model, we would intro-
duce prior distributions over the hyperparameters    and    and make predictions by
marginalizing with respect to these hyperparameters as well as with respect to the
parameters w. however, although we can integrate analytically over either w or
over the hyperparameters, the complete marginalization over all of these variables
is analytically intractable. here we discuss an approximation in which we set the
hyperparameters to speci   c values determined by maximizing the marginal likeli-
hood function obtained by    rst integrating over the parameters w. this framework
is known in the statistics literature as empirical bayes (bernardo and smith, 1994;
gelman et al., 2004), or type 2 maximum likelihood (berger, 1985), or generalized
maximum likelihood (wahba, 1975), and in the machine learning literature is also
called the evidence approximation (gull, 1989; mackay, 1992a).

if we introduce hyperpriors over    and   , the predictive distribution is obtained

by marginalizing over w,    and    so that

p(t|t) =

p(t|w,   )p(w|t,   ,   )p(  ,   |t) dw d   d  

(3.74)
where p(t|w,   ) is given by (3.8) and p(w|t,   ,   ) is given by (3.49) with mn and
sn de   ned by (3.53) and (3.54) respectively. here we have omitted the dependence
on the input variable x to keep the notation uncluttered. if the posterior distribution

p(  ,   |t) is sharply peaked around values(cid:1)   and(cid:1)  , then the predictive distribution is
obtained simply by marginalizing over w in which    and    are    xed to the values(cid:1)  
and(cid:1)  , so that

p(t|t) (cid:7) p(t|t,(cid:1)  ,(cid:1)  ) =

p(t|w,(cid:1)  )p(w|t,(cid:1)  ,(cid:1)  ) dw.

(3.75)

(cid:6)

(cid:6)(cid:6)(cid:6)

166

3. linear models for regression

from bayes    theorem, the posterior distribution for    and    is given by

if the prior is relatively    at, then in the evidence framework the values of (cid:1)   and
(cid:1)   are obtained by maximizing the marginal likelihood function p(t|  ,   ). we shall

p(  ,   |t)     p(t|  ,   )p(  ,   ).

(3.76)

proceed by evaluating the marginal likelihood for the linear basis function model and
then    nding its maxima. this will allow us to determine values for these hyperpa-
rameters from the training data alone, without recourse to cross-validation. recall
that the ratio   /   is analogous to a id173 parameter.

as an aside it is worth noting that, if we de   ne conjugate (gamma) prior distri-
butions over    and   , then the marginalization over these hyperparameters in (3.74)
can be performed analytically to give a student   s t-distribution over w (see sec-
tion 2.3.7). although the resulting integral over w is no longer analytically tractable,
it might be thought that approximating this integral, for example using the laplace
approximation discussed (section 4.4) which is based on a local gaussian approxi-
mation centred on the mode of the posterior distribution, might provide a practical
alternative to the evidence framework (buntine and weigend, 1991). however, the
integrand as a function of w typically has a strongly skewed mode so that the laplace
approximation fails to capture the bulk of the id203 mass, leading to poorer re-
sults than those obtained by maximizing the evidence (mackay, 1999).

returning to the evidence framework, we note that there are two approaches that
we can take to the maximization of the log evidence. we can evaluate the evidence
function analytically and then set its derivative equal to zero to obtain re-estimation
equations for    and   , which we shall do in section 3.5.2. alternatively we use a
technique called the expectation maximization (em) algorithm, which will be dis-
cussed in section 9.3.4 where we shall also show that these two approaches converge
to the same solution.

3.5.1 evaluation of the evidence function
the marginal likelihood function p(t|  ,   ) is obtained by integrating over the

weight parameters w, so that

(cid:6)

p(t|  ,   ) =

p(t|w,   )p(w|  ) dw.

(3.77)

exercise 3.16

exercise 3.17

one way to evaluate this integral is to make use once again of the result (2.115)
for the conditional distribution in a linear-gaussian model. here we shall evaluate
the integral instead by completing the square in the exponent and making use of the
standard form for the id172 coef   cient of a gaussian.

from (3.11), (3.12), and (3.52), we can write the evidence function in the form

(cid:15)

(cid:16)n/2(cid:17)

(cid:18)m/2

(cid:6)

p(t|  ,   ) =

  
2  

  
2  

exp{   e(w)} dw

(3.78)

exercise 3.18

exercise 3.19

(3.80)

(3.81)

(3.82)

3.5. the evidence approximation

167

where m is the dimensionality of w, and we have de   ned

e(w) =   ed(w) +   ew (w)

=   
2

(cid:5)t       w(cid:5)2 +   
2

wtw.

(3.79)

we recognize (3.79) as being equal, up to a constant of proportionality, to the reg-
ularized sum-of-squares error function (3.27). we now complete the square over w
giving

e(w) = e(mn ) +

(w     mn )ta(w     mn )

1
2

where we have introduced

a =   i +     t  

together with

e(mn) =   
2

(cid:5)t       mn(cid:5)2 +   
2

mt

n mn .

note that a corresponds to the matrix of second derivatives of the error function

a =       e(w)

(3.83)

and is known as the hessian matrix. here we have also de   ned mn given by

mn =   a   1  tt.
   1
n , and hence (3.84) is equivalent to the previous

(3.84)

using (3.54), we see that a = s
de   nition (3.53), and therefore represents the mean of the posterior distribution.

the integral over w can now be evaluated simply by appealing to the standard

result for the id172 coef   cient of a multivariate gaussian, giving

(cid:6)

(cid:6)

(cid:12)

exp{   e(w)} dw
= exp{   e(mn )}
= exp{   e(mn )}(2  )m/2|a|   1/2.

   1
2

exp

(w     mn)ta(w     mn)

(cid:13)

dw

(3.85)

using (3.78) we can then write the log of the marginal likelihood in the form

ln p(t|  ,   ) = m
2

ln    + n
2

ln        e(mn )     1
2

ln|a|     n
2

ln(2  )

(3.86)

which is the required expression for the evidence function.

returning to the polynomial regression problem, we can plot the model evidence
against the order of the polynomial, as shown in figure 3.14. here we have assumed
a prior of the form (1.65) with the parameter       xed at    = 5    10   3. the form
of this plot is very instructive. referring back to figure 1.4, we see that the m = 0
polynomial has very poor    t to the data and consequently gives a relatively low value

168

3. linear models for regression

figure 3.14 plot of the model evidence versus
the order m, for the polynomial re-
gression model, showing that the
evidence favours the model with
m = 3.

   18

   20

   22

   24

   26

0

2

6

8

4

m

for the evidence. going to the m = 1 polynomial greatly improves the data    t, and
hence the evidence is signi   cantly higher. however, in going to m = 2, the data
   t is improved only very marginally, due to the fact that the underlying sinusoidal
function from which the data is generated is an odd function and so has no even terms
in a polynomial expansion. indeed, figure 1.5 shows that the residual data error is
reduced only slightly in going from m = 1 to m = 2. because this richer model
suffers a greater complexity penalty, the evidence actually falls in going from m = 1
to m = 2. when we go to m = 3 we obtain a signi   cant further improvement in
data    t, as seen in figure 1.4, and so the evidence is increased again, giving the
highest overall evidence for any of the polynomials. further increases in the value
of m produce only small improvements in the    t to the data but suffer increasing
complexity penalty, leading overall to a decrease in the evidence values. looking
again at figure 1.5, we see that the generalization error is roughly constant between
m = 3 and m = 8, and it would be dif   cult to choose between these models on
the basis of this plot alone. the evidence values, however, show a clear preference
for m = 3, since this is the simplest model which gives a good explanation for the
observed data.

3.5.2 maximizing the evidence function
let us    rst consider the maximization of p(t|  ,   ) with respect to   . this can

be done by    rst de   ning the following eigenvector equation

(cid:10)

(cid:11)

(3.87)
(cid:2)
from (3.81), it then follows that a has eigenvalues    +   i. now consider the deriva-
tive of the term involving ln|a| in (3.86) with respect to   . we have

(cid:2)

(cid:14)

ui =   iui.

    t  

d
d  

ln|a| = d
d  

ln

(  i +   ) = d
d  

i

i

ln(  i +   ) =

.

(3.88)

1

  i +   

i

thus the stationary points of (3.86) with respect to    satisfy

(cid:2)

0 = m
2  

    1
2

n mn     1
mt
2

1

  i +   

i

.

(3.89)

multiplying through by 2   and rearranging, we obtain

3.5. the evidence approximation

169

(cid:2)

1

  i +   

i

  mt

n mn = m       
(cid:2)

   =

  i

   +   i

i

=   .

(3.90)

.

(3.91)

since there are m terms in the sum over i, the quantity    can be written

exercise 3.20

the interpretation of the quantity    will be discussed shortly. from (3.90) we see
that the value of    that maximizes the marginal likelihood satis   es

   =

  
n mn
mt

.

(3.92)

note that this is an implicit solution for    not only because    depends on   , but also
because the mode mn of the posterior distribution itself depends on the choice of
  . we therefore adopt an iterative procedure in which we make an initial choice for
   and use this to    nd mn , which is given by (3.53), and also to evaluate   , which
is given by (3.91). these values are then used to re-estimate    using (3.92), and the
process repeated until convergence. note that because the matrix   t   is    xed, we
can compute its eigenvalues once at the start and then simply multiply these by    to
obtain the   i.

it should be emphasized that the value of    has been determined purely by look-
ing at the training data. in contrast to maximum likelihood methods, no independent
data set is required in order to optimize the model complexity.

we can similarly maximize the log marginal likelihood (3.86) with respect to   .
to do this, we note that the eigenvalues   i de   ned by (3.87) are proportional to   ,
and hence d  i/d   =   i/   giving

d
d  

ln|a| = d
d  

ln(  i +   ) =

1
  

=   
  

.

(3.93)

the stationary point of the marginal likelihood therefore satis   es

0 = n
2  

exercise 3.22

and rearranging we obtain

1
  

=

    1
2

tn     mt

n   (xn)

n       

n=1

tn     mt

n   (xn)

.

(3.94)

(3.95)

  i +   

i

  i

(cid:2)
(cid:27)2       
(cid:27)2

2  

i

(cid:2)
n(cid:2)
(cid:26)
n(cid:2)

n=1

1

(cid:26)

again, this is an implicit solution for    and can be solved by choosing an initial
value for    and then using this to calculate mn and    and then re-estimate    using
(3.95), repeating until convergence. if both    and    are to be determined from the
data, then their values can be re-estimated together after each update of   .

170

3. linear models for regression

figure 3.15 contours of the likelihood function (red)
and the prior (green) in which the axes in parameter
space have been rotated to align with the eigenvectors
ui of the hessian. for    = 0, the mode of the poste-
rior is given by the maximum likelihood solution wml,
whereas for nonzero    the mode is at wmap = mn . in
the direction w1 the eigenvalue   1, de   ned by (3.87), is
small compared with    and so the quantity   1/(  1 +   )
is close to zero, and the corresponding map value of
w1 is also close to zero. by contrast, in the direction w2
the eigenvalue   2 is large compared with    and so the
quantity   2/(  2 +  ) is close to unity, and the map value
of w2 is close to its maximum likelihood value.

w2

u2

wml

wmap

u1

w1

3.5.3 effective number of parameters
the result (3.92) has an elegant interpretation (mackay, 1992a), which provides
insight into the bayesian solution for   . to see this, consider the contours of the like-
lihood function and the prior as illustrated in figure 3.15. here we have implicitly
transformed to a rotated set of axes in parameter space aligned with the eigenvec-
tors ui de   ned in (3.87). contours of the likelihood function are then axis-aligned
ellipses. the eigenvalues   i measure the curvature of the likelihood function, and
so in figure 3.15 the eigenvalue   1 is small compared with   2 (because a smaller
curvature corresponds to a greater elongation of the contours of the likelihood func-
tion). because     t   is a positive de   nite matrix, it will have positive eigenvalues,
and so the ratio   i/(  i +   ) will lie between 0 and 1. consequently, the quantity   
de   ned by (3.91) will lie in the range 0 (cid:1)    (cid:1) m. for directions in which   i (cid:12)   ,
the corresponding parameter wi will be close to its maximum likelihood value, and
the ratio   i/(  i +   ) will be close to 1. such parameters are called well determined
because their values are tightly constrained by the data. conversely, for directions
in which   i (cid:13)   , the corresponding parameters wi will be close to zero, as will the
ratios   i/(  i +   ). these are directions in which the likelihood function is relatively
insensitive to the parameter value and so the parameter has been set to a small value
by the prior. the quantity    de   ned by (3.91) therefore measures the effective total
number of well determined parameters.

we can obtain some insight into the result (3.95) for re-estimating    by com-
paring it with the corresponding maximum likelihood result given by (3.21). both
of these formulae express the variance (the inverse precision) as an average of the
squared differences between the targets and the model predictions. however, they
differ in that the number of data points n in the denominator of the maximum like-
lihood result is replaced by n        in the bayesian result. we recall from (1.56) that
the maximum likelihood estimate of the variance for a gaussian distribution over a

3.5. the evidence approximation

171

single variable x is given by

  2
ml =

1
n

n(cid:2)

(xn       ml)2

n=1

(3.96)

and that this estimate is biased because the maximum likelihood solution   ml for
the mean has    tted some of the noise on the data. in effect, this has used up one
degree of freedom in the model. the corresponding unbiased estimate is given by
(1.59) and takes the form

  2
map =

1

n     1

(xn       ml)2.

(3.97)

n(cid:2)

n=1

we shall see in section 10.1.3 that this result can be obtained from a bayesian treat-
ment in which we marginalize over the unknown mean. the factor of n     1 in the
denominator of the bayesian result takes account of the fact that one degree of free-
dom has been used in    tting the mean and removes the bias of maximum likelihood.
now consider the corresponding results for the id75 model. the mean
of the target distribution is now given by the function wt  (x), which contains m
parameters. however, not all of these parameters are tuned to the data. the effective
number of parameters that are determined by the data is   , with the remaining m      
parameters set to small values by the prior. this is re   ected in the bayesian result
for the variance that has a factor n        in the denominator, thereby correcting for
the bias of the maximum likelihood result.

we can illustrate the evidence framework for setting hyperparameters using the
sinusoidal synthetic data set from section 1.1, together with the gaussian basis func-
tion model comprising 9 basis functions, so that the total number of parameters in
the model is given by m = 10 including the bias. here, for simplicity of illustra-
tion, we have set    to its true value of 11.1 and then used the evidence framework to
determine   , as shown in figure 3.16.
we can also see how the parameter    controls the magnitude of the parameters
{wi}, by plotting the individual parameters versus the effective number    of param-
eters, as shown in figure 3.17.
if we consider the limit n (cid:12) m in which the number of data points is large in
relation to the number of parameters, then from (3.87) all of the parameters will be
well determined by the data because   t   involves an implicit sum over data points,
and so the eigenvalues   i increase with the size of the data set. in this case,    = m,
and the re-estimation equations for    and    become

   =

   =

m

2ew (mn )

n

2ed(mn )

(3.98)

(3.99)

where ew and ed are de   ned by (3.25) and (3.26), respectively. these results
can be used as an easy-to-compute approximation to the full evidence re-estimation

172

3. linear models for regression

   5

0
ln   

5

   5

0
ln   

5

figure 3.16 the left plot shows    (red curve) and 2  ew (mn ) (blue curve) versus ln    for the sinusoidal
synthetic data set. it is the intersection of these two curves that de   nes the optimum value for    given by the
evidence procedure. the right plot shows the corresponding graph of log evidence ln p(t|  ,   ) versus ln    (red
curve) showing that the peak coincides with the crossing point of the curves in the left plot. also shown is the
test set error (blue curve) showing that the evidence maximum occurs close to the point of best generalization.

formulae, because they do not require evaluation of the eigenvalue spectrum of the
hessian.

figure 3.17 plot of

the 10 parameters wi
from the gaussian basis function
model versus the effective num-
ber of parameters   , in which the
hyperparameter    is varied in the
range 0 (cid:1)    (cid:1)     causing    to
vary in the range 0 (cid:1)    (cid:1) m.

wi

2

1

0

   1

   2

0

8

4

5

2

6

3

1

7

9

0

2

4

6

8

  

10

3.6. limitations of fixed basis functions

throughout this chapter, we have focussed on models comprising a linear combina-
tion of    xed, nonlinear basis functions. we have seen that the assumption of linearity
in the parameters led to a range of useful properties including closed-form solutions
to the least-squares problem, as well as a tractable bayesian treatment. furthermore,
for a suitable choice of basis functions, we can model arbitrary nonlinearities in the

exercises

173

mapping from input variables to targets. in the next chapter, we shall study an anal-
ogous class of models for classi   cation.

it might appear, therefore, that such linear models constitute a general purpose
framework for solving problems in pattern recognition. unfortunately, there are
some signi   cant shortcomings with linear models, which will cause us to turn in
later chapters to more complex models such as support vector machines and neural
networks.

the dif   culty stems from the assumption that the basis functions   j(x) are    xed
before the training data set is observed and is a manifestation of the curse of dimen-
sionality discussed in section 1.4. as a consequence, the number of basis functions
needs to grow rapidly, often exponentially, with the dimensionality d of the input
space.
fortunately, there are two properties of real data sets that we can exploit to help
alleviate this problem. first of all, the data vectors {xn} typically lie close to a non-
linear manifold whose intrinsic dimensionality is smaller than that of the input space
as a result of strong correlations between the input variables. we will see an example
of this when we consider images of handwritten digits in chapter 12. if we are using
localized basis functions, we can arrange that they are scattered in input space only
in regions containing data. this approach is used in id80s
and also in support vector and relevance vector machines. neural network models,
which use adaptive basis functions having sigmoidal nonlinearities, can adapt the
parameters so that the regions of input space over which the basis functions vary
corresponds to the data manifold. the second property is that target variables may
have signi   cant dependence on only a small number of possible directions within the
data manifold. neural networks can exploit this property by choosing the directions
in input space to which the basis functions respond.

exercises

3.1 ((cid:12)) www show that the    tanh    function and the logistic sigmoid function (3.6)

are related by

tanh(a) = 2  (2a)     1.

hence show that a general linear combination of logistic sigmoid functions of the
form

m(cid:2)

j=1

m(cid:2)

j=1

(cid:18)

(cid:18)

(cid:17)

(cid:17)

x       j

s

x       j

s

y(x, w) = w0 +

wj  

y(x, u) = u0 +

uj tanh

(3.100)

(3.101)

(3.102)

is equivalent to a linear combination of    tanh    functions of the form

and    nd expressions to relate the new parameters {u1, . . . , um} to the original pa-
rameters {w1, . . . , wm}.

174

3. linear models for regression

3.2 ((cid:12) (cid:12)) show that the matrix

  (  t  )   1  t

(3.103)
takes any vector v and projects it onto the space spanned by the columns of   . use
this result to show that the least-squares solution (3.15) corresponds to an orthogonal
projection of the vector t onto the manifold s as shown in figure 3.2.

3.3 ((cid:12)) consider a data set in which each data point tn is associated with a weighting

factor rn > 0, so that the sum-of-squares error function becomes

(cid:26)

n(cid:2)

rn

n=1

ed(w) =

1
2

(cid:27)2

tn     wt  (xn)

.

(3.104)

find an expression for the solution w(cid:1) that minimizes this error function. give two
alternative interpretations of the weighted sum-of-squares error function in terms of
(i) data dependent noise variance and (ii) replicated data points.

3.4 ((cid:12)) www consider a linear model of the form

d(cid:2)

y(x, w) = w0 +

wixi

i=1

together with a sum-of-squares error function of the form

n(cid:2)

n=1

ed(w) =

1
2

{y(xn, w)     tn}2

.

(3.105)

(3.106)

now suppose that gaussian noise  i with zero mean and variance   2 is added in-
dependently to each of the input variables xi. by making use of e[ i] = 0 and
e[ i j] =   ij  2, show that minimizing ed averaged over the noise distribution is
equivalent to minimizing the sum-of-squares error for noise-free input variables with
the addition of a weight-decay id173 term, in which the bias parameter w0
is omitted from the regularizer.

3.5 ((cid:12)) www using the technique of lagrange multipliers, discussed in appendix e,
show that minimization of the regularized error function (3.29) is equivalent to mini-
mizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).
discuss the relationship between the parameters    and   .

3.6 ((cid:12)) www consider a linear basis function regression model for a multivariate

target variable t having a gaussian distribution of the form
p(t|w,   ) = n (t|y(x, w),   )

where

y(x, w) = wt  (x)

(3.107)

(3.108)

exercises

175

together with a training data set comprising input basis vectors   (xn) and corre-
sponding target vectors tn, with n = 1, . . . , n. show that the maximum likelihood
solution wml for the parameter matrix w has the property that each column is
given by an expression of the form (3.15), which was the solution for an isotropic
noise distribution. note that this is independent of the covariance matrix   . show
that the maximum likelihood solution for    is given by

(cid:11)(cid:10)

(cid:11)t

tn     wt

ml  (xn)

tn     wt

ml  (xn)

.

(3.109)

n(cid:2)

(cid:10)

n=1

   =

1
n

3.7 ((cid:12)) by using the technique of completing the square, verify the result (3.49) for the
posterior distribution of the parameters w in the linear basis function model in which
mn and sn are de   ned by (3.50) and (3.51) respectively.

3.8 ((cid:12) (cid:12)) www consider the linear basis function model in section 3.1, and suppose
that we have already observed n data points, so that the posterior distribution over
w is given by (3.49). this posterior can be regarded as the prior for the next obser-
vation. by considering an additional data point (xn +1, tn +1), and by completing
the square in the exponential, show that the resulting posterior distribution is again
given by (3.49) but with sn replaced by sn +1 and mn replaced by mn +1.

3.9 ((cid:12) (cid:12)) repeat the previous exercise but instead of completing the square by hand,

make use of the general result for linear-gaussian models given by (2.116).

3.10 ((cid:12) (cid:12)) www by making use of the result (2.115) to evaluate the integral in (3.57),
verify that the predictive distribution for the bayesian id75 model is
given by (3.58) in which the input-dependent variance is given by (3.59).

3.11 ((cid:12) (cid:12)) we have seen that, as the size of a data set increases, the uncertainty associated
with the posterior distribution over model parameters decreases. make use of the
matrix identity (appendix c)

(cid:10)

m + vvt

(cid:11)   1 = m   1     (m   1v)

vtm   1
1 + vtm   1v

(cid:10)

(cid:11)

to show that the uncertainty   2
given by (3.59) satis   es

n (x) associated with the id75 function

n +1(x) (cid:1)   2
  2

n (x).

(3.110)

(3.111)

3.12 ((cid:12) (cid:12)) we saw in section 2.3.6 that the conjugate prior for a gaussian distribution
with unknown mean and unknown precision (inverse variance) is a normal-gamma
distribution. this property also holds for the case of the conditional gaussian dis-
tribution p(t|x, w,   ) of the id75 model. if we consider the likelihood
function (3.10), then the conjugate prior for w and    is given by
   1s0)gam(  |a0, b0).

p(w,   ) = n (w|m0,   

(3.112)

176

3. linear models for regression

show that the corresponding posterior distribution takes the same functional form,
so that

p(w,   |t) = n (w|mn ,   

   1sn)gam(  |an , bn)

(3.113)

and    nd expressions for the posterior parameters mn , sn , an , and bn .

3.13 ((cid:12) (cid:12)) show that the predictive distribution p(t|x, t) for the model discussed in ex-

ercise 3.12 is given by a student   s t-distribution of the form

p(t|x, t) = st(t|  ,   ,   )

(3.114)

and obtain expressions for   ,    and   .

3.14 ((cid:12) (cid:12))

in this exercise, we explore in more detail the properties of the equivalent
kernel de   ned by (3.62), where sn is de   ned by (3.54). suppose that the basis
functions   j(x) are linearly independent and that the number n of data points is
greater than the number m of basis functions. furthermore, let one of the basis
functions be constant, say   0(x) = 1. by taking suitable linear combinations of
these basis functions, we can construct a new basis set   j(x) spanning the same
space but that are orthonormal, so that

n(cid:2)

n=1

  j(xn)  k(xn) = ijk

(3.115)

where ijk is de   ned to be 1 if j = k and 0 otherwise, and we take   0(x) = 1. show
that for    = 0, the equivalent kernel can be written as k(x, x(cid:4)) =   (x)t  (x(cid:4))
where    = (  1, . . . ,   m )t. use this result to show that the kernel satis   es the
summation constraint

n(cid:2)

k(x, xn) = 1.

(3.116)

n=1

3.15 ((cid:12)) www consider a linear basis function model for regression in which the pa-
rameters    and    are set using the evidence framework. show that the function
e(mn ) de   ned by (3.82) satis   es the relation 2e(mn ) = n.

3.16 ((cid:12) (cid:12)) derive the result (3.86) for the log evidence function p(t|  ,   ) of the linear
regression model by making use of (2.115) to evaluate the integral (3.77) directly.

3.17 ((cid:12)) show that the evidence function for the bayesian id75 model can

be written in the form (3.78) in which e(w) is de   ned by (3.79).

3.18 ((cid:12) (cid:12)) www by completing the square over w, show that the error function (3.79)

in bayesian id75 can be written in the form (3.80).

3.19 ((cid:12) (cid:12)) show that the integration over w in the bayesian id75 model gives

the result (3.85). hence show that the log marginal likelihood is given by (3.86).

3.20 ((cid:12) (cid:12)) www starting from (3.86) verify all of the steps needed to show that maxi-
mization of the log marginal likelihood function (3.86) with respect to    leads to the
re-estimation equation (3.92).

exercises

177

3.21 ((cid:12) (cid:12)) an alternative way to derive the result (3.92) for the optimal value of    in the

evidence framework is to make use of the identity

(cid:15)

(cid:16)

ln|a| = tr

d
d  

a   1 d
d  

a

.

(3.117)

prove this identity by considering the eigenvalue expansion of a real, symmetric
matrix a, and making use of the standard results for the determinant and trace of
a expressed in terms of its eigenvalues (appendix c). then make use of (3.117) to
derive (3.92) starting from (3.86).

3.22 ((cid:12) (cid:12)) starting from (3.86) verify all of the steps needed to show that maximiza-
tion of the log marginal likelihood function (3.86) with respect to    leads to the
re-estimation equation (3.95).

3.23 ((cid:12) (cid:12)) www show that the marginal id203 of the data, in other words the

model evidence, for the model described in exercise 3.12 is given by

p(t) =

1

(2  )n/2

ba0
0
ban
n

  (an )
  (a0)

|sn|1/2
|s0|1/2

by    rst marginalizing with respect to w and then with respect to   .

3.24 ((cid:12) (cid:12)) repeat the previous exercise but now use bayes    theorem in the form

p(t) = p(t|w,   )p(w,   )

p(w,   |t)

(3.118)

(3.119)

and then substitute for the prior and posterior distributions and the likelihood func-
tion in order to derive the result (3.118).

4

linear

models for

classi   cation

in the previous chapter, we explored a class of regression models having particularly
simple analytical and computational properties. we now discuss an analogous class
of models for solving classi   cation problems. the goal in classi   cation is to take an
input vector x and to assign it to one of k discrete classes ck where k = 1, . . . , k.
in the most common scenario, the classes are taken to be disjoint, so that each input is
assigned to one and only one class. the input space is thereby divided into decision
regions whose boundaries are called decision boundaries or decision surfaces. in
this chapter, we consider linear models for classi   cation, by which we mean that the
decision surfaces are linear functions of the input vector x and hence are de   ned
by (d     1)-dimensional hyperplanes within the d-dimensional input space. data
sets whose classes can be separated exactly by linear decision surfaces are said to be
linearly separable.

for regression problems, the target variable t was simply the vector of real num-
bers whose values we wish to predict. in the case of classi   cation, there are various

179

180

4. linear models for classification

ways of using target values to represent class labels. for probabilistic models, the
most convenient, in the case of two-class problems, is the binary representation in
which there is a single target variable t     {0, 1} such that t = 1 represents class c1
and t = 0 represents class c2. we can interpret the value of t as the id203 that
the class is c1, with the values of id203 taking only the extreme values of 0 and
1. for k > 2 classes, it is convenient to use a 1-of-k coding scheme in which t is
a vector of length k such that if the class is cj, then all elements tk of t are zero
except element tj, which takes the value 1. for instance, if we have k = 5 classes,
then a pattern from class 2 would be given the target vector

t = (0, 1, 0, 0, 0)t.

(4.1)
again, we can interpret the value of tk as the id203 that the class is ck. for
nonprobabilistic models, alternative choices of target variable representation will
sometimes prove convenient.

in chapter 1, we identi   ed three distinct approaches to the classi   cation prob-
lem. the simplest involves constructing a discriminant function that directly assigns
each vector x to a speci   c class. a more powerful approach, however, models the
id155 distribution p(ck|x) in an id136 stage, and then subse-
quently uses this distribution to make optimal decisions. by separating id136
and decision, we gain numerous bene   ts, as discussed in section 1.5.4. there are
two different approaches to determining the conditional probabilities p(ck|x). one
technique is to model them directly, for example by representing them as parametric
models and then optimizing the parameters using a training set. alternatively, we
can adopt a generative approach in which we model the class-conditional densities
given by p(x|ck), together with the prior probabilities p(ck) for the classes, and then
we compute the required posterior probabilities using bayes    theorem

p(ck|x) = p(x|ck)p(ck)

p(x)

.

(4.2)

we shall discuss examples of all three approaches in this chapter.

in the id75 models considered in chapter 3, the model prediction
y(x, w) was given by a linear function of the parameters w. in the simplest case,
the model is also linear in the input variables and therefore takes the form y(x) =
wtx + w0, so that y is a real number. for classi   cation problems, however, we wish
to predict discrete class labels, or more generally posterior probabilities that lie in
the range (0, 1). to achieve this, we consider a generalization of this model in which
we transform the linear function of w using a nonlinear function f(   ) so that

(cid:10)

(cid:11)

wtx + w0

y(x) = f

(4.3)
in the machine learning literature f(   ) is known as an activation function, whereas
its inverse is called a link function in the statistics literature. the decision surfaces
correspond to y(x) = constant, so that wtx + w0 = constant and hence the deci-
sion surfaces are linear functions of x, even if the function f(  ) is nonlinear. for this
reason, the class of models described by (4.3) are called generalized linear models

.

4.1. discriminant functions

181

(mccullagh and nelder, 1989). note, however, that in contrast to the models used
for regression, they are no longer linear in the parameters due to the presence of the
nonlinear function f(  ). this will lead to more complex analytical and computa-
tional properties than for id75 models. nevertheless, these models are
still relatively simple compared to the more general nonlinear models that will be
studied in subsequent chapters.

the algorithms discussed in this chapter will be equally applicable if we    rst
make a    xed nonlinear transformation of the input variables using a vector of basis
functions   (x) as we did for regression models in chapter 3. we begin by consider-
ing classi   cation directly in the original input space x, while in section 4.3 we shall
   nd it convenient to switch to a notation involving basis functions for consistency
with later chapters.

4.1. discriminant functions

a discriminant is a function that takes an input vector x and assigns it to one of k
classes, denoted ck. in this chapter, we shall restrict attention to linear discriminants,
namely those for which the decision surfaces are hyperplanes. to simplify the dis-
cussion, we consider    rst the case of two classes and then investigate the extension
to k > 2 classes.

4.1.1 two classes
the simplest representation of a linear discriminant function is obtained by tak-

ing a linear function of the input vector so that

y(x) = wtx + w0

(4.4)

where w is called a weight vector, and w0 is a bias (not to be confused with bias in
the statistical sense). the negative of the bias is sometimes called a threshold. an
input vector x is assigned to class c1 if y(x) (cid:2) 0 and to class c2 otherwise. the cor-
responding decision boundary is therefore de   ned by the relation y(x) = 0, which
corresponds to a (d     1)-dimensional hyperplane within the d-dimensional input
space. consider two points xa and xb both of which lie on the decision surface.
because y(xa) = y(xb) = 0, we have wt(xa     xb) = 0 and hence the vector w is
orthogonal to every vector lying within the decision surface, and so w determines the
orientation of the decision surface. similarly, if x is a point on the decision surface,
then y(x) = 0, and so the normal distance from the origin to the decision surface is
given by

wtx

(cid:5)w(cid:5) =     w0(cid:5)w(cid:5) .

(4.5)

we therefore see that the bias parameter w0 determines the location of the decision
surface. these properties are illustrated for the case of d = 2 in figure 4.1.

furthermore, we note that the value of y(x) gives a signed measure of the per-
pendicular distance r of the point x from the decision surface. to see this, consider

182

4. linear models for classification

figure 4.1 illustration of the geometry of a
linear discriminant function in two dimensions.
the decision surface, shown in red, is perpen-
dicular to w, and its displacement from the
origin is controlled by the bias parameter w0.
also, the signed orthogonal distance of a gen-
eral point x from the decision surface is given
by y(x)/(cid:3)w(cid:3).

y > 0

y = 0

y < 0

x2
r1

r2

w

x

y(x)(cid:5)w(cid:5)

x1

x   

   w0(cid:5)w(cid:5)

an arbitrary point x and let x    be its orthogonal projection onto the decision surface,
so that

x = x    + r

w
(cid:5)w(cid:5) .

multiplying both sides of this result by wt and adding w0, and making use of y(x) =
wtx + w0 and y(x   ) = wtx    + w0 = 0, we have

r = y(x)
(cid:5)w(cid:5) .

(4.7)

this result is illustrated in figure 4.1.

as with the id75 models in chapter 3, it is sometimes convenient
to use a more compact notation in which we introduce an additional dummy    input   

value x0 = 1 and then de   ne(cid:4)w = (w0, w) and(cid:4)x = (x0, x) so that

(4.6)

(4.8)

y(x) = (cid:4)wt(cid:4)x.

in this case, the decision surfaces are d-dimensional hyperplanes passing through
the origin of the d + 1-dimensional expanded input space.

4.1.2 multiple classes
now consider the extension of linear discriminants to k > 2 classes. we might
be tempted be to build a k-class discriminant by combining a number of two-class
discriminant functions. however, this leads to some serious dif   culties (duda and
hart, 1973) as we now show.
consider the use of k   1 classi   ers each of which solves a two-class problem of
separating points in a particular class ck from points not in that class. this is known
as a one-versus-the-rest classi   er. the left-hand example in figure 4.2 shows an

4.1. discriminant functions

183

c3

r3

c1

r1

c1

c2

?

r2

c3

c2

?

r1

not c1

r3

not c2

r2

c2

c1

figure 4.2 attempting to construct a k class discriminant from a set of two class discriminants leads to am-
biguous regions, shown in green. on the left is an example involving the use of two discriminants designed to
distinguish points in class ck from points not in class ck. on the right is an example involving three discriminant
functions each of which is used to separate a pair of classes ck and cj.

example involving three classes where this approach leads to regions of input space
that are ambiguously classi   ed.
an alternative is to introduce k(k     1)/2 binary discriminant functions, one
for every possible pair of classes. this is known as a one-versus-one classi   er. each
point is then classi   ed according to a majority vote amongst the discriminant func-
tions. however, this too runs into the problem of ambiguous regions, as illustrated
in the right-hand diagram of figure 4.2.

we can avoid these dif   culties by considering a single k-class discriminant

comprising k linear functions of the form

yk(x) = wt

(4.9)
and then assigning a point x to class ck if yk(x) > yj(x) for all j (cid:9)= k. the decision
boundary between class ck and class cj is therefore given by yk(x) = yj(x) and
hence corresponds to a (d     1)-dimensional hyperplane de   ned by

k x + wk0

(wk     wj)tx + (wk0     wj0) = 0.

(4.10)

this has the same form as the decision boundary for the two-class case discussed in
section 4.1.1, and so analogous geometrical properties apply.

the decision regions of such a discriminant are always singly connected and
convex. to see this, consider two points xa and xb both of which lie inside decision

region rk, as illustrated in figure 4.3. any point(cid:1)x that lies on the line connecting

xa and xb can be expressed in the form

(cid:1)x =   xa + (1       )xb

(4.11)

184

4. linear models for classification

figure 4.3 illustration of the decision regions for a mul-
ticlass linear discriminant, with the decision
if two points xa
boundaries shown in red.
and xb both lie inside the same decision re-
gion rk, then any point bx that lies on the line
connecting these two points must also lie in
rk, and hence the decision region must be
singly connected and convex.

ri

xa

rj

rk

  x

xb

where 0 (cid:1)    (cid:1) 1. from the linearity of the discriminant functions, it follows that

yk((cid:1)x) =   yk(xa) + (1       )yk(xb).

yk(xb) > yj(xb), for all j (cid:9)= k, and hence yk((cid:1)x) > yj((cid:1)x), and so(cid:1)x also lies

(4.12)
because both xa and xb lie inside rk, it follows that yk(xa) > yj(xa), and
inside rk. thus rk is singly connected and convex.

note that for two classes, we can either employ the formalism discussed here,
based on two discriminant functions y1(x) and y2(x), or else use the simpler but
equivalent formulation described in section 4.1.1 based on a single discriminant
function y(x).

we now explore three approaches to learning the parameters of linear discrimi-
nant functions, based on least squares, fisher   s linear discriminant, and the percep-
tron algorithm.

4.1.3 least squares for classi   cation
in chapter 3, we considered models that were linear functions of the parame-
ters, and we saw that the minimization of a sum-of-squares error function led to a
simple closed-form solution for the parameter values. it is therefore tempting to see
if we can apply the same formalism to classi   cation problems. consider a general
classi   cation problem with k classes, with a 1-of-k binary coding scheme for the
target vector t. one justi   cation for using least squares in such a context is that it
approximates the conditional expectation e[t|x] of the target values given the input
vector. for the binary coding scheme, this conditional expectation is given by the
vector of posterior class probabilities. unfortunately, however, these probabilities
are typically approximated rather poorly, indeed the approximations can have values
outside the range (0, 1), due to the limited    exibility of a linear model as we shall
see shortly.

each class ck is described by its own linear model so that

yk(x) = wt

k x + wk0

(4.13)

where k = 1, . . . , k. we can conveniently group these together using vector nota-
tion so that

(4.14)

y(x) =,wt(cid:4)x

185

can then be written as

4.1. discriminant functions

a dummy input x0 = 1. this representation was discussed in detail in section 3.1. a

error function, as we did for regression in chapter 3. consider a training data set
{xn, tn} where n = 1, . . . , n, and de   ne a matrix t whose nth row is the vector tt
n,
n. the sum-of-squares error function

where ,w is a matrix whose kth column comprises the d + 1-dimensional vector
(cid:4)wk = (wk0, wt
k )t and(cid:4)x is the corresponding augmented input vector (1, xt)t with
new input x is then assigned to the class for which the output yk = (cid:4)wt
k(cid:4)x is largest.
we now determine the parameter matrix ,w by minimizing a sum-of-squares
together with a matrix (cid:4)x whose nth row is(cid:4)xt
(cid:19)
(cid:20)
((cid:4)x,w     t)t((cid:4)x,w     t)
setting the derivative with respect to,w to zero, and rearranging, we then obtain the
solution for,w in the form,w = ((cid:4)xt(cid:4)x)   1(cid:4)xtt = (cid:4)x   t
is the pseudo-inverse of the matrix (cid:4)x, as discussed in section 3.1.1. we
where (cid:4)x   
(cid:18)t(cid:4)x.
(cid:17)(cid:4)x   

y(x) =,wt(cid:4)x = tt

then obtain the discriminant function in the form

ed(,w) =

1
2 tr

(4.15)

(4.16)

(4.17)

.

exercise 4.2

section 2.3.7

an interesting property of least-squares solutions with multiple target variables

is that if every target vector in the training set satis   es some linear constraint

attn + b = 0

(4.18)

for some constants a and b, then the model prediction for any value of x will satisfy
the same constraint so that

aty(x) + b = 0.

(4.19)
thus if we use a 1-of-k coding scheme for k classes, then the predictions made
by the model will have the property that the elements of y(x) will sum to 1 for any
value of x. however, this summation constraint alone is not suf   cient to allow the
model outputs to be interpreted as probabilities because they are not constrained to
lie within the interval (0, 1).

the least-squares approach gives an exact closed-form solution for the discrimi-
nant function parameters. however, even as a discriminant function (where we use it
to make decisions directly and dispense with any probabilistic interpretation) it suf-
fers from some severe problems. we have already seen that least-squares solutions
lack robustness to outliers, and this applies equally to the classi   cation application,
as illustrated in figure 4.4. here we see that the additional data points in the right-
hand    gure produce a signi   cant change in the location of the decision boundary,
even though these point would be correctly classi   ed by the original decision bound-
ary in the left-hand    gure. the sum-of-squares error function penalizes predictions
that are    too correct    in that they lie a long way on the correct side of the decision

186

4. linear models for classification

4

2

0

   2

   4

   6

   8

4

2

0

   2

   4

   6

   8

   4

   2

0

2

4

6

8

   4

   2

0

2

4

6

8

figure 4.4 the left plot shows data from two classes, denoted by red crosses and blue circles, together with
the decision boundary found by least squares (magenta curve) and also by the id28 model (green
curve), which is discussed later in section 4.3.2. the right-hand plot shows the corresponding results obtained
when extra data points are added at the bottom left of the diagram, showing that least squares is highly sensitive
to outliers, unlike id28.

boundary. in section 7.1.2, we shall consider several alternative error functions for
classi   cation and we shall see that they do not suffer from this dif   culty.

however, problems with least squares can be more severe than simply lack of
robustness, as illustrated in figure 4.5. this shows a synthetic data set drawn from
three classes in a two-dimensional input space (x1, x2), having the property that lin-
ear decision boundaries can give excellent separation between the classes. indeed,
the technique of id28, described later in this chapter, gives a satisfac-
tory solution as seen in the right-hand plot. however, the least-squares solution gives
poor results, with only a small region of the input space assigned to the green class.
the failure of least squares should not surprise us when we recall that it cor-
responds to maximum likelihood under the assumption of a gaussian conditional
distribution, whereas binary target vectors clearly have a distribution that is far from
gaussian. by adopting more appropriate probabilistic models, we shall obtain clas-
si   cation techniques with much better properties than least squares. for the moment,
however, we continue to explore alternative nonprobabilistic methods for setting the
parameters in the linear classi   cation models.

4.1.4 fisher   s linear discriminant
one way to view a linear classi   cation model is in terms of dimensionality
reduction. consider    rst the case of two classes, and suppose we take the d-

4.1. discriminant functions

187

6

4

2

0

   2

   4

   6

   6

   4

   2

0

2

4

6

6

4

2

0

   2

   4

   6

   6

   4

   2

0

2

4

6

figure 4.5 example of a synthetic data set comprising three classes, with training data points denoted in red
(  ), green (+), and blue (   ). lines denote the decision boundaries, and the background colours denote the
respective classes of the decision regions. on the left is the result of using a least-squares discriminant. we see
that the region of input space assigned to the green class is too small and so most of the points from this class
are misclassi   ed. on the right is the result of using id28s as described in section 4.3.2 showing
correct classi   cation of the training data.

dimensional input vector x and project it down to one dimension using

y = wtx.

(4.20)
if we place a threshold on y and classify y (cid:2)    w0 as class c1, and otherwise class
c2, then we obtain our standard linear classi   er discussed in the previous section.
in general, the projection onto one dimension leads to a considerable loss of infor-
mation, and classes that are well separated in the original d-dimensional space may
become strongly overlapping in one dimension. however, by adjusting the com-
ponents of the weight vector w, we can select a projection that maximizes the class
separation. to begin with, consider a two-class problem in which there are n1 points
of class c1 and n2 points of class c2, so that the mean vectors of the two classes are
given by

xn,

m2 =

xn.

(4.21)

(cid:2)

n     c1

m1 =

1
n1

(cid:2)

n     c2

1
n2

the simplest measure of the separation of the classes, when projected onto w, is the
separation of the projected class means. this suggests that we might choose w so as
to maximize

m2     m1 = wt(m2     m1)

where

mk = wtmk

(4.22)

(4.23)

188

4. linear models for classification

4

2

0

   2

4

2

0

   2

   2

2

6

   2

2

6

figure 4.6 the left plot shows samples from two classes (depicted in red and blue) along with the histograms
resulting from projection onto the line joining the class means. note that there is considerable class overlap in
the projected space. the right plot shows the corresponding projection based on the fisher linear discriminant,
showing the greatly improved class separation.

appendix e
exercise 4.4

(cid:5)

i w2

is the mean of the projected data from class ck. however, this expression can be
made arbitrarily large simply by increasing the magnitude of w. to solve this
i = 1. using
problem, we could constrain w to have unit length, so that
a lagrange multiplier to perform the constrained maximization, we then    nd that
w     (m2     m1). there is still a problem with this approach, however, as illustrated
in figure 4.6. this shows two classes that are well separated in the original two-
dimensional space (x1, x2) but that have considerable overlap when projected onto
the line joining their means. this dif   culty arises from the strongly nondiagonal
covariances of the class distributions. the idea proposed by fisher is to maximize
a function that will give a large separation between the projected class means while
also giving a small variance within each class, thereby minimizing the class overlap.
the projection formula (4.20) transforms the set of labelled data points in x
into a labelled set in the one-dimensional space y. the within-class variance of the
transformed data from class ck is therefore given by
(yn     mk)2

(cid:2)

(4.24)

s2
k =

n   ck

where yn = wtxn. we can de   ne the total within-class variance for the whole
data set to be simply s2
2. the fisher criterion is de   ned to be the ratio of the
between-class variance to the within-class variance and is given by

1 + s2

j(w) =

(m2     m1)2

1 + s2
s2
2

.

(4.25)

exercise 4.5

we can make the dependence on w explicit by using (4.20), (4.23), and (4.24) to
rewrite the fisher criterion in the form

4.1. discriminant functions

189

j(w) =

wtsbw
wtsww

where sb is the between-class covariance matrix and is given by

sb = (m2     m1)(m2     m1)t
and sw is the total within-class covariance matrix, given by

(4.26)

(4.27)

sw =

(xn     m1)(xn     m1)t +

(xn     m2)(xn     m2)t.

(4.28)

(cid:2)

n   c1

(cid:2)

n   c2

differentiating (4.26) with respect to w, we    nd that j(w) is maximized when

(wtsbw)sww = (wtsww)sbw.

(4.29)
from (4.27), we see that sbw is always in the direction of (m2   m1). furthermore,
we do not care about the magnitude of w, only its direction, and so we can drop the
   1
scalar factors (wtsbw) and (wtsww). multiplying both sides of (4.29) by s
w
we then obtain

(4.30)
note that if the within-class covariance is isotropic, so that sw is proportional to the
unit matrix, we    nd that w is proportional to the difference of the class means, as
discussed above.

w     s

w (m2     m1).
   1

the result (4.30) is known as fisher   s linear discriminant, although strictly it
is not a discriminant but rather a speci   c choice of direction for projection of the
data down to one dimension. however, the projected data can subsequently be used
to construct a discriminant, by choosing a threshold y0 so that we classify a new
point as belonging to c1 if y(x) (cid:2) y0 and classify it as belonging to c2 otherwise.
for example, we can model the class-conditional densities p(y|ck) using gaussian
distributions and then use the techniques of section 1.2.4 to    nd the parameters
of the gaussian distributions by maximum likelihood. having found gaussian ap-
proximations to the projected classes, the formalism of section 1.5.1 then gives an
expression for the optimal threshold. some justi   cation for the gaussian assumption
comes from the central limit theorem by noting that y = wtx is the sum of a set of
random variables.

4.1.5 relation to least squares
the least-squares approach to the determination of a linear discriminant was
based on the goal of making the model predictions as close as possible to a set of
target values. by contrast, the fisher criterion was derived by requiring maximum
class separation in the output space. it is interesting to see the relationship between
these two approaches. in particular, we shall show that, for the two-class problem,
the fisher criterion can be obtained as a special case of least squares.

so far we have considered 1-of-k coding for the target values. if, however, we
adopt a slightly different target coding scheme, then the least-squares solution for

190

4. linear models for classification

the weights becomes equivalent to the fisher solution (duda and hart, 1973). in
particular, we shall take the targets for class c1 to be n/n1, where n1 is the number
of patterns in class c1, and n is the total number of patterns. this target value
approximates the reciprocal of the prior id203 for class c1. for class c2, we
shall take the targets to be    n/n2, where n2 is the number of patterns in class c2.

setting the derivatives of e with respect to w0 and w to zero, we obtain respectively

.

(4.31)

the sum-of-squares error function can be written
wtxn + w0     tn

e =

(cid:11)2

n(cid:2)

(cid:10)

n=1

1
2

(cid:10)

n(cid:2)
n(cid:2)
(cid:10)

n=1

n=1

(cid:11)

(cid:11)

wtxn + w0     tn

= 0

wtxn + w0     tn

xn = 0.

from (4.32), and making use of our choice of target coding scheme for the tn, we
obtain an expression for the bias in the form

where we have used

n(cid:2)

n=1

w0 =    wtm

tn = n1

n
n1

    n2

n
n2

= 0

and where m is the mean of the total data set and is given by

(4.32)

(4.33)

(4.34)

(4.35)

n(cid:2)

n=1

m =

1
n

xn =

1
n

(n1m1 + n2m2).

(4.36)

exercise 4.6

after some straightforward algebra, and again making use of the choice of tn, the
second equation (4.33) becomes

(cid:15)

(cid:16)

sw + n1n2
n

sb

w = n(m1     m2)

(4.37)

where sw is de   ned by (4.28), sb is de   ned by (4.27), and we have substituted for
the bias using (4.34). using (4.27), we note that sbw is always in the direction of
(m2     m1). thus we can write

w     s

w (m2     m1)
   1

(4.38)

where we have ignored irrelevant scale factors. thus the weight vector coincides
with that found from the fisher criterion. in addition, we have also found an expres-
sion for the bias value w0 given by (4.34). this tells us that a new vector x should be
classi   ed as belonging to class c1 if y(x) = wt(x   m) > 0 and class c2 otherwise.

4.1. discriminant functions

191

4.1.6 fisher   s discriminant for multiple classes
we now consider the generalization of the fisher discriminant to k > 2 classes,
and we shall assume that the dimensionality d of the input space is greater than the
k x, where
number k of classes. next, we introduce d
k = 1, . . . , d
. these feature values can conveniently be grouped together to form
a vector y. similarly, the weight vectors {wk} can be considered to be the columns
of a matrix w, so that

> 1 linear    features    yk = wt

(cid:4)

(cid:4)

(4.39)
note that again we are not including any bias parameters in the de   nition of y. the
generalization of the within-class covariance matrix to the case of k classes follows
from (4.28) to give

y = wtx.

sw =

sk

k(cid:2)

k=1

xn

(cid:2)

n   ck
1
nk

(cid:2)

n   ck

(xn     mk)(xn     mk)t

(4.40)

(4.41)

(4.42)

where

sk =

mk =

and nk is the number of patterns in class ck. in order to    nd a generalization of the
between-class covariance matrix, we follow duda and hart (1973) and consider    rst
the total covariance matrix

(cid:5)

where m is the mean of the total data set

st =

(xn     m)(xn     m)t

(4.43)

m =

1
n

xn =

1
n

k(cid:2)

k=1

nkmk

(4.44)

and n =
k nk is the total number of data points. the total covariance matrix can
be decomposed into the sum of the within-class covariance matrix, given by (4.40)
and (4.41), plus an additional matrix sb, which we identify as a measure of the
between-class covariance

st = sw + sb

where

sb =

nk(mk     m)(mk     m)t.

(4.45)

(4.46)

n(cid:2)

n=1

n(cid:2)

n=1

k(cid:2)

k=1

192

4. linear models for classification

n   ck

k=1

k(cid:2)
(cid:2)
k(cid:2)
(cid:2)

k=1

n   ck

these covariance matrices have been de   ned in the original x-space. we can now
de   ne similar matrices in the projected d

-dimensional y-space

(cid:4)

sw =

(yn       k)(yn       k)t

(4.47)

(4.48)

(4.49)

(4.50)

and

where

sb =

  k =

1
nk

nk(  k       )(  k       )t
k(cid:2)

yn,

   =

nk  k.

k=1

1
n

(cid:27)

(cid:26)

again we wish to construct a scalar that is large when the between-class covariance
is large and when the within-class covariance is small. there are now many possible
choices of criterion (fukunaga, 1990). one example is given by

j(w) = tr

   1
s
w sb

.

(cid:26)

(cid:27)

this criterion can then be rewritten as an explicit function of the projection matrix
w in the form

j(w) = tr

(wswwt)   1(wsbwt)

.

(4.51)

(cid:4)

maximization of such criteria is straightforward, though somewhat involved, and is
discussed at length in fukunaga (1990). the weight values are determined by those
eigenvectors of s

   1
w sb that correspond to the d

largest eigenvalues.

there is one important result that is common to all such criteria, which is worth
emphasizing. we    rst note from (4.46) that sb is composed of the sum of k ma-
trices, each of which is an outer product of two vectors and therefore of rank 1. in
addition, only (k     1) of these matrices are independent as a result of the constraint
(4.44). thus, sb has rank at most equal to (k     1) and so there are at most (k     1)
nonzero eigenvalues. this shows that the projection onto the (k     1)-dimensional
subspace spanned by the eigenvectors of sb does not alter the value of j(w), and
so we are therefore unable to    nd more than (k     1) linear    features    by this means
(fukunaga, 1990).

4.1.7 the id88 algorithm
another example of a linear discriminant model is the id88 of rosenblatt
(1962), which occupies an important place in the history of pattern recognition al-
gorithms. it corresponds to a two-class model in which the input vector x is    rst
transformed using a    xed nonlinear transformation to give a feature vector   (x),
and this is then used to construct a generalized linear model of the form

(cid:10)

(cid:11)

y(x) = f

wt  (x)

(4.52)

4.1. discriminant functions

193

where the nonlinear activation function f(  ) is given by a step function of the form

f(a) =

+1, a (cid:2) 0
   1, a < 0.

(4.53)

(cid:12)

the vector   (x) will typically include a bias component   0(x) = 1.
in earlier
discussions of two-class classi   cation problems, we have focussed on a target coding
scheme in which t     {0, 1}, which is appropriate in the context of probabilistic
models. for the id88, however, it is more convenient to use target values
t = +1 for class c1 and t =    1 for class c2, which matches the choice of activation
function.

the algorithm used to determine the parameters w of the id88 can most
easily be motivated by error function minimization. a natural choice of error func-
tion would be the total number of misclassi   ed patterns. however, this does not lead
to a simple learning algorithm because the error is a piecewise constant function
of w, with discontinuities wherever a change in w causes the decision boundary to
move across one of the data points. methods based on changing w using the gradi-
ent of the error function cannot then be applied, because the gradient is zero almost
everywhere.

we therefore consider an alternative error function known as the id88 cri-
terion. to derive this, we note that we are seeking a weight vector w such that
patterns xn in class c1 will have wt  (xn) > 0, whereas patterns xn in class c2
have wt  (xn) < 0. using the t     {   1, +1} target coding scheme it follows that
we would like all patterns to satisfy wt  (xn)tn > 0. the id88 criterion
associates zero error with any pattern that is correctly classi   ed, whereas for a mis-
classi   ed pattern xn it tries to minimize the quantity    wt  (xn)tn. the id88
criterion is therefore given by

ep(w) =    

wt  ntn

(4.54)

(cid:2)

n   m

frank rosenblatt
1928   1969

rosenblatt   s id88 played an
important role in the history of ma-
chine learning.
initially, rosenblatt
simulated the id88 on an ibm
704 computer at cornell
in 1957,
but by the early 1960s he had built
special-purpose hardware that provided a direct, par-
allel implementation of id88 learning. many of
his ideas were encapsulated in    principles of neuro-
dynamics: id88s and the theory of brain mech-
anisms    published in 1962. rosenblatt   s work was
criticized by marvin minksy, whose objections were
published in the book    id88s   , co-authored with

seymour papert. this book was widely misinter-
preted at the time as showing that neural networks
were fatally    awed and could only learn solutions for
linearly separable problems.
in fact, it only proved
such limitations in the case of single-layer networks
such as the id88 and merely conjectured (in-
correctly) that they applied to more general network
models. unfortunately, however, this book contributed
to the substantial decline in research funding for neu-
ral computing, a situation that was not reversed un-
til the mid-1980s. today, there are many hundreds,
if not thousands, of applications of neural networks
in widespread use, with examples in areas such as
handwriting recognition and information retrieval be-
ing used routinely by millions of people.

194

4. linear models for classification

section 3.1.3

where m denotes the set of all misclassi   ed patterns. the contribution to the error
associated with a particular misclassi   ed pattern is a linear function of w in regions
of w space where the pattern is misclassi   ed and zero in regions where it is correctly
classi   ed. the total error function is therefore piecewise linear.

we now apply the stochastic id119 algorithm to this error function.

the change in the weight vector w is then given by

w(   +1) = w(   )          ep(w) = w(   ) +     ntn

(4.55)

where    is the learning rate parameter and    is an integer that indexes the steps of
the algorithm. because the id88 function y(x, w) is unchanged if we multiply
w by a constant, we can set the learning rate parameter    equal to 1 without of
generality. note that, as the weight vector evolves during training, the set of patterns
that are misclassi   ed will change.

the id88 learning algorithm has a simple interpretation, as follows. we
cycle through the training patterns in turn, and for each pattern xn we evaluate the
id88 function (4.52).
if the pattern is correctly classi   ed, then the weight
vector remains unchanged, whereas if it is incorrectly classi   ed, then for class c1
we add the vector   (xn) onto the current estimate of weight vector w while for
class c2 we subtract the vector   (xn) from w. the id88 learning algorithm is
illustrated in figure 4.7.

if we consider the effect of a single update in the id88 learning algorithm,
we see that the contribution to the error from a misclassi   ed pattern will be reduced
because from (4.55) we have

   w(   +1)t  ntn =    w(   )t  ntn     (  ntn)t  ntn <    w(   )t  ntn

(4.56)
where we have set    = 1, and made use of (cid:5)  ntn(cid:5)2 > 0. of course, this does
not imply that the contribution to the error function from the other misclassi   ed
patterns will have been reduced. furthermore, the change in weight vector may have
caused some previously correctly classi   ed patterns to become misclassi   ed. thus
the id88 learning rule is not guaranteed to reduce the total error function at
each stage.

however, the id88 convergence theorem states that if there exists an ex-
act solution (in other words, if the training data set is linearly separable), then the
id88 learning algorithm is guaranteed to    nd an exact solution in a    nite num-
ber of steps. proofs of this theorem can be found for example in rosenblatt (1962),
block (1962), nilsson (1965), minsky and papert (1969), hertz et al. (1991), and
bishop (1995a). note, however, that the number of steps required to achieve con-
vergence could still be substantial, and in practice, until convergence is achieved,
we will not be able to distinguish between a nonseparable problem and one that is
simply slow to converge.

even when the data set is linearly separable, there may be many solutions, and
which one is found will depend on the initialization of the parameters and on the or-
der of presentation of the data points. furthermore, for data sets that are not linearly
separable, the id88 learning algorithm will never converge.

4.1. discriminant functions

195

1

0.5

0

   0.5

   1

   1

1

0.5

0

   0.5

   1

   1

1

0.5

0

   0.5

   1

   1

1

0.5

0

   0.5

   1

   1

   0.5

0

0.5

1

   0.5

0

0.5

1

   0.5

0

0.5

1

   0.5

0

0.5

1

figure 4.7 illustration of the convergence of the id88 learning algorithm, showing data points from two
classes (red and blue) in a two-dimensional feature space (  1,   2). the top left plot shows the initial parameter
vector w shown as a black arrow together with the corresponding decision boundary (black line), in which the
arrow points towards the decision region which classi   ed as belonging to the red class. the data point circled
in green is misclassi   ed and so its feature vector is added to the current weight vector, giving the new decision
boundary shown in the top right plot. the bottom left plot shows the next misclassi   ed point to be considered,
indicated by the green circle, and its feature vector is again added to the weight vector giving the decision
boundary shown in the bottom right plot for which all data points are correctly classi   ed.

196

4. linear models for classification

figure 4.8 illustration of the mark 1 id88 hardware. the photograph on the left shows how the inputs
were obtained using a simple camera system in which an input scene, in this case a printed character, was
illuminated by powerful lights, and an image focussed onto a 20    20 array of cadmium sulphide photocells,
giving a primitive 400 pixel image. the id88 also had a patch board, shown in the middle photograph,
which allowed different con   gurations of input features to be tried. often these were wired up at random to
demonstrate the ability of the id88 to learn without the need for precise wiring, in contrast to a modern
digital computer. the photograph on the right shows one of the racks of adaptive weights. each weight was
implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby
allowing the value of the weight to be adjusted automatically by the learning algorithm.

aside from dif   culties with the learning algorithm, the id88 does not pro-
vide probabilistic outputs, nor does it generalize readily to k > 2 classes. the most
important limitation, however, arises from the fact that (in common with all of the
models discussed in this chapter and the previous one) it is based on linear com-
binations of    xed basis functions. more detailed discussions of the limitations of
id88s can be found in minsky and papert (1969) and bishop (1995a).

analogue hardware implementations of the id88 were built by rosenblatt,
based on motor-driven variable resistors to implement the adaptive parameters wj.
these are illustrated in figure 4.8. the inputs were obtained from a simple camera
system based on an array of photo-sensors, while the basis functions    could be
chosen in a variety of ways, for example based on simple    xed functions of randomly
chosen subsets of pixels from the input image. typical applications involved learning
to discriminate simple shapes or characters.

at the same time that the id88 was being developed, a closely related
system called the adaline, which is short for    adaptive linear element   , was being
explored by widrow and co-workers. the functional form of the model was the same
as for the id88, but a different approach to training was adopted (widrow and
hoff, 1960; widrow and lehr, 1990).

4.2. probabilistic generative models

we turn next to a probabilistic view of classi   cation and show how models with
linear decision boundaries arise from simple assumptions about the distribution of
the data. in section 1.5.4, we discussed the distinction between the discriminative
and the generative approaches to classi   cation. here we shall adopt a generative

4.2. probabilistic generative models

197

figure 4.9 plot of the logistic sigmoid function
  (a) de   ned by (4.59), shown in
red, together with the scaled pro-
bit function   (  a), for   2 =   /8,
shown in dashed blue, where   (a)
is de   ned by (4.114). the scal-
ing factor   /8 is chosen so that the
derivatives of the two curves are
equal for a = 0.

1

0.5

0

   5

0

5

approach in which we model the class-conditional densities p(x|ck), as well as the
class priors p(ck), and then use these to compute posterior probabilities p(ck|x)
through bayes    theorem.
c1 can be written as

consider    rst of all the case of two classes. the posterior id203 for class

p(x|c1)p(c1)

p(c1|x) =

p(x|c1)p(c1) + p(x|c2)p(c2)
1 + exp(   a)
a = ln p(x|c1)p(c1)
p(x|c2)p(c2)
and   (a) is the logistic sigmoid function de   ned by

where we have de   ned

=   (a)

=

1

  (a) =

1

1 + exp(   a)

(4.57)

(4.58)

(4.59)

which is plotted in figure 4.9. the term    sigmoid    means s-shaped. this type of
function is sometimes also called a    squashing function    because it maps the whole
real axis into a    nite interval. the logistic sigmoid has been encountered already
in earlier chapters and plays an important role in many classi   cation algorithms. it
satis   es the following symmetry property

as is easily veri   ed. the inverse of the logistic sigmoid is given by

  (   a) = 1       (a)

(cid:17)

(cid:18)

a = ln

  
1       

(4.60)

(4.61)

and is known as the logit function. it represents the log of the ratio of probabilities
ln [p(c1|x)/p(c2|x)] for the two classes, also known as the log odds.

198

4. linear models for classification

note that in (4.57) we have simply rewritten the posterior probabilities in an
equivalent form, and so the appearance of the logistic sigmoid may seem rather vac-
uous. however, it will have signi   cance provided a(x) takes a simple functional
form. we shall shortly consider situations in which a(x) is a linear function of x, in
which case the posterior id203 is governed by a generalized linear model.

for the case of k > 2 classes, we have

p(ck|x) =

=

(cid:5)
p(x|ck)p(ck)
j p(x|cj)p(cj)
(cid:5)
exp(ak)
j exp(aj)

(4.62)

which is known as the normalized exponential and can be regarded as a multiclass
generalization of the logistic sigmoid. here the quantities ak are de   ned by

ak = ln p(x|ck)p(ck).

(4.63)

the normalized exponential is also known as the softmax function, as it represents
a smoothed version of the    max    function because, if ak (cid:12) aj for all j (cid:9)= k, then
p(ck|x) (cid:7) 1, and p(cj|x) (cid:7) 0.

we now investigate the consequences of choosing speci   c forms for the class-
conditional densities, looking    rst at continuous input variables x and then dis-
cussing brie   y the case of discrete inputs.

4.2.1 continuous inputs
let us assume that the class-conditional densities are gaussian and then explore
the resulting form for the posterior probabilities. to start with, we shall assume that
all classes share the same covariance matrix. thus the density for class ck is given
by

(cid:12)

p(x|ck) =

1

(2  )d/2

1

|  |1/2 exp

   1
2

(x       k)t  

(cid:13)
   1(x       k)

consider    rst the case of two classes. from (4.57) and (4.58), we have

p(c1|x) =   (wtx + w0)

where we have de   ned

w =   
w0 =    1
2

   1(  1       2)
   1  1 +
  t

1   

1
2

   1  2 + ln p(c1)
p(c2) .

  t

2   

we see that the quadratic terms in x from the exponents of the gaussian densities
have cancelled (due to the assumption of common covariance matrices) leading to
a linear function of x in the argument of the logistic sigmoid. this result is illus-
trated for the case of a two-dimensional input space x in figure 4.10. the resulting

.

(4.64)

(4.65)

(4.66)

(4.67)

4.2. probabilistic generative models

199

figure 4.10 the left-hand plot shows the class-conditional densities for two classes, denoted red and blue.
on the right is the corresponding posterior id203 p(c1|x), which is given by a logistic sigmoid of a linear
function of x. the surface in the right-hand plot is coloured using a proportion of red ink given by p(c1|x) and a
proportion of blue ink given by p(c2|x) = 1     p(c1|x).

decision boundaries correspond to surfaces along which the posterior probabilities
p(ck|x) are constant and so will be given by linear functions of x, and therefore
the decision boundaries are linear in input space. the prior probabilities p(ck) enter
only through the bias parameter w0 so that changes in the priors have the effect of
making parallel shifts of the decision boundary and more generally of the parallel
contours of constant posterior id203.

for the general case of k classes we have, from (4.62) and (4.63),

ak(x) = wt

k x + wk0

where we have de   ned

wk =   
wk0 =    1
2

   1  k
  t
k   

   1  k + ln p(ck).

(4.68)

(4.69)

(4.70)

we see that the ak(x) are again linear functions of x as a consequence of the cancel-
lation of the quadratic terms due to the shared covariances. the resulting decision
boundaries, corresponding to the minimum misclassi   cation rate, will occur when
two of the posterior probabilities (the two largest) are equal, and so will be de   ned
by linear functions of x, and so again we have a generalized linear model.
if we relax the assumption of a shared covariance matrix and allow each class-
conditional density p(x|ck) to have its own covariance matrix   k, then the earlier
cancellations will no longer occur, and we will obtain quadratic functions of x, giv-
ing rise to a quadratic discriminant. the linear and quadratic decision boundaries
are illustrated in figure 4.11.

200

4. linear models for classification

2.5

2

1.5

1

0.5

0

   0.5

   1

   1.5

   2

   2.5

   2

   1

0

1

2

figure 4.11 the left-hand plot shows the class-conditional densities for three classes each having a gaussian
distribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix.
the right-hand plot shows the corresponding posterior probabilities, in which the rgb colour vector represents
the posterior probabilities for the respective three classes. the decision boundaries are also shown. notice that
the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas
those between the other pairs of classes are quadratic.

4.2.2 maximum likelihood solution
once we have speci   ed a parametric functional form for the class-conditional
densities p(x|ck), we can then determine the values of the parameters, together with
the prior class probabilities p(ck), using maximum likelihood. this requires a data
set comprising observations of x along with their corresponding class labels.
consider    rst the case of two classes, each having a gaussian class-conditional
density with a shared covariance matrix, and suppose we have a data set {xn, tn}
where n = 1, . . . , n. here tn = 1 denotes class c1 and tn = 0 denotes class c2. we
denote the prior class id203 p(c1) =   , so that p(c2) = 1       . for a data point
xn from class c1, we have tn = 1 and hence

p(xn,c1) = p(c1)p(xn|c1) =   n (xn|  1,   ).

similarly for class c2, we have tn = 0 and hence

p(xn,c2) = p(c2)p(xn|c2) = (1       )n (xn|  2,   ).

thus the likelihood function is given by

n(cid:14)

p(t|  ,   1,   2,   ) =

[  n (xn|  1,   )]tn [(1       )n (xn|  2,   )]1   tn

(4.71)

n=1

where t = (t1, . . . , tn)t. as usual, it is convenient to maximize the log of the
likelihood function. consider    rst the maximization with respect to   . the terms in

exercise 4.9

4.2. probabilistic generative models

201

n(cid:2)

n=1

n(cid:2)

the log likelihood function that depend on    are

{tn ln    + (1     tn) ln(1       )} .

(4.72)

setting the derivative with respect to    equal to zero and rearranging, we obtain

1
n

n1

tn = n1
n

=

   =

n=1

n1 + n2

(4.73)
where n1 denotes the total number of data points in class c1, and n2 denotes the total
number of data points in class c2. thus the maximum likelihood estimate for    is
simply the fraction of points in class c1 as expected. this result is easily generalized
to the multiclass case where again the maximum likelihood estimate of the prior
id203 associated with class ck is given by the fraction of the training set points
assigned to that class.
n(cid:2)

now consider the maximization with respect to   1. again we can pick out of

the log likelihood function those terms that depend on   1 giving

n(cid:2)

tn(xn       1)t  

   1(xn       1) + const.

(4.74)

tn lnn (xn|  1,   ) =    1
2

n=1

n=1

setting the derivative with respect to   1 to zero and rearranging, we obtain

(4.75)
which is simply the mean of all the input vectors xn assigned to class c1. by a
similar argument, the corresponding result for   2 is given by

n=1

tnxn

  1 =

  2 =

1
n2

(1     tn)xn

(4.76)

which again is the mean of all the input vectors xn assigned to class c2.

finally, consider the maximum likelihood solution for the shared covariance
matrix   . picking out the terms in the log likelihood function that depend on   , we
have

n(cid:2)
n(cid:2)

n=1

n(cid:2)

n=1

   1(xn       1)

   1
2

tn ln|  |     1
2

   1
2
=     n
2

n=1

(1     tn) ln|  |     1
2
   1s

ln|  |     n

(cid:26)

  

2 tr

tn(xn       1)t  
n(cid:2)
(cid:27)

n=1

(1     tn)(xn       2)t  

   1(xn       2)

(4.77)

n(cid:2)

1
n1

n(cid:2)

n=1

202

4. linear models for classification

where we have de   ned

s1 =

s = n1
n
1
n1
1
n2

s2 =

(cid:2)
(cid:2)

n   c1

n   c2

s2

s1 + n2
n
(xn       1)(xn       1)t

(xn       2)(xn       2)t.

(4.78)

(4.79)

(4.80)

using the standard result for the maximum likelihood solution for a gaussian distri-
bution, we see that    = s, which represents a weighted average of the covariance
matrices associated with each of the two classes separately.

this result is easily extended to the k class problem to obtain the corresponding
maximum likelihood solutions for the parameters in which each class-conditional
density is gaussian with a shared covariance matrix. note that the approach of    tting
gaussian distributions to the classes is not robust to outliers, because the maximum
likelihood estimation of a gaussian is not robust.

4.2.3 discrete features
let us now consider the case of discrete feature values xi. for simplicity, we
begin by looking at binary feature values xi     {0, 1} and discuss the extension to
more general discrete features shortly. if there are d inputs, then a general distribu-
tion would correspond to a table of 2d numbers for each class, containing 2d     1
independent variables (due to the summation constraint). because this grows expo-
nentially with the number of features, we might seek a more restricted representa-
tion. here we will make the naive bayes assumption in which the feature values are
treated as independent, conditioned on the class ck. thus we have class-conditional
distributions of the form

p(x|ck) =

ki(1       ki)1   xi
  xi

(4.81)

d(cid:14)

i=1

exercise 4.10

section 2.3.7

section 8.2.2

which contain d independent parameters for each class. substituting into (4.63) then
gives

ak(x) =

{xi ln   ki + (1     xi) ln(1       ki)} + ln p(ck)

(4.82)

d(cid:2)

exercise 4.11

i=1

which again are linear functions of the input values xi. for the case of k = 2 classes,
we can alternatively consider the logistic sigmoid formulation given by (4.57). anal-
ogous results are obtained for discrete variables each of which can take m > 2
states.

4.2.4 exponential family
as we have seen, for both gaussian distributed and discrete inputs, the posterior
class probabilities are given by generalized linear models with logistic sigmoid (k =

4.3. probabilistic discriminative models

203

2 classes) or softmax (k (cid:2) 2 classes) id180. these are particular cases
of a more general result obtained by assuming that the class-conditional densities
p(x|ck) are members of the exponential family of distributions.
(cid:27)

using the form (2.194) for members of the exponential family, we see that the

distribution of x can be written in the form

(cid:26)

p(x|  k) = h(x)g(  k) exp

  t

k u(x)

.

(4.83)

we now restrict attention to the subclass of such distributions for which u(x) = x.
then we make use of (2.236) to introduce a scaling parameter s, so that we obtain
the restricted set of exponential family class-conditional densities of the form

(cid:15)

(cid:16)

p(x|  k, s) =

1
s

h

1
s

x

g(  k) exp

  t

k x

.

(4.84)

(cid:13)

(cid:12)

1
s

note that we are allowing each class to have its own parameter vector   k but we are
assuming that the classes share the same scale parameter s.

for the two-class problem, we substitute this expression for the class-conditional
densities into (4.58) and we see that the posterior class id203 is again given by
a logistic sigmoid acting on a linear function a(x) which is given by

a(x) = (  1       2)tx + ln g(  1)     ln g(  2) + ln p(c1)     ln p(c2).

(4.85)

similarly, for the k-class problem, we substitute the class-conditional density ex-
pression into (4.63) to give

ak(x) =   t

k x + ln g(  k) + ln p(ck)

(4.86)

and so again is a linear function of x.

4.3. probabilistic discriminative models

for the two-class classi   cation problem, we have seen that the posterior id203
of class c1 can be written as a logistic sigmoid acting on a linear function of x, for a
wide choice of class-conditional distributions p(x|ck). similarly, for the multiclass
case, the posterior id203 of class ck is given by a softmax transformation of a
linear function of x. for speci   c choices of the class-conditional densities p(x|ck),
we have used maximum likelihood to determine the parameters of the densities as
well as the class priors p(ck) and then used bayes    theorem to    nd the posterior class
probabilities.

however, an alternative approach is to use the functional form of the generalized
linear model explicitly and to determine its parameters directly by using maximum
likelihood. we shall see that there is an ef   cient algorithm    nding such solutions
known as iterative reweighted least squares, or irls.

the indirect approach to    nding the parameters of a generalized linear model,
by    tting class-conditional densities and class priors separately and then applying

204

4. linear models for classification

x2

1

0

   1

   1

0

x1

1

1

  2

0.5

0

0

0.5

  1

1

figure 4.12 illustration of the role of nonlinear basis functions in linear classi   cation models. the left plot
shows the original input space (x1, x2) together with data points from two classes labelled red and blue. two
   gaussian    basis functions   1(x) and   2(x) are de   ned in this space with centres shown by the green crosses
and with contours shown by the green circles. the right-hand plot shows the corresponding feature space
(  1,   2) together with the linear decision boundary obtained given by a id28 model of the form
discussed in section 4.3.2. this corresponds to a nonlinear decision boundary in the original input space,
shown by the black curve in the left-hand plot.

bayes    theorem, represents an example of generative modelling, because we could
take such a model and generate synthetic data by drawing values of x from the
marginal distribution p(x). in the direct approach, we are maximizing a likelihood
function de   ned through the conditional distribution p(ck|x), which represents a
form of discriminative training. one advantage of the discriminative approach is
that there will typically be fewer adaptive parameters to be determined, as we shall
see shortly. it may also lead to improved predictive performance, particularly when
the class-conditional density assumptions give a poor approximation to the true dis-
tributions.

4.3.1 fixed basis functions
so far in this chapter, we have considered classi   cation models that work di-
rectly with the original input vector x. however, all of the algorithms are equally
applicable if we    rst make a    xed nonlinear transformation of the inputs using a
vector of basis functions   (x). the resulting decision boundaries will be linear in
the feature space   , and these correspond to nonlinear decision boundaries in the
original x space, as illustrated in figure 4.12. classes that are linearly separable
in the feature space   (x) need not be linearly separable in the original observation
space x. note that as in our discussion of linear models for regression, one of the

4.3. probabilistic discriminative models

205

basis functions is typically set to a constant, say   0(x) = 1, so that the correspond-
ing parameter w0 plays the role of a bias. for the remainder of this chapter, we shall
include a    xed basis function transformation   (x), as this will highlight some useful
similarities to the regression models discussed in chapter 3.
for many problems of practical interest, there is signi   cant overlap between
the class-conditional densities p(x|ck). this corresponds to posterior probabilities
p(ck|x), which, for at least some values of x, are not 0 or 1. in such cases, the opti-
mal solution is obtained by modelling the posterior probabilities accurately and then
applying standard decision theory, as discussed in chapter 1. note that nonlinear
transformations   (x) cannot remove such class overlap. indeed, they can increase
the level of overlap, or create overlap where none existed in the original observation
space. however, suitable choices of nonlinearity can make the process of modelling
the posterior probabilities easier.

such    xed basis function models have important limitations, and these will be
resolved in later chapters by allowing the basis functions themselves to adapt to the
data. notwithstanding these limitations, models with    xed nonlinear basis functions
play an important role in applications, and a discussion of such models will intro-
duce many of the key concepts needed for an understanding of their more complex
counterparts.

(cid:11)

(cid:10)

4.3.2 id28
we begin our treatment of generalized linear models by considering the problem
of two-class classi   cation. in our discussion of generative approaches in section 4.2,
we saw that under rather general assumptions, the posterior id203 of class c1
can be written as a logistic sigmoid acting on a linear function of the feature vector
   so that
(4.87)
with p(c2|  ) = 1     p(c1|  ). here   (  ) is the logistic sigmoid function de   ned by
(4.59). in the terminology of statistics, this model is known as id28,
although it should be emphasized that this is a model for classi   cation rather than
regression.

p(c1|  ) = y(  ) =   

wt  

for an m-dimensional feature space   , this model has m adjustable parameters.
by contrast, if we had    tted gaussian class conditional densities using maximum
likelihood, we would have used 2m parameters for the means and m(m + 1)/2
parameters for the (shared) covariance matrix. together with the class prior p(c1),
this gives a total of m(m +5)/2+1 parameters, which grows quadratically with m,
in contrast to the linear dependence on m of the number of parameters in logistic
regression. for large values of m, there is a clear advantage in working with the
id28 model directly.

we now use maximum likelihood to determine the parameters of the logistic
regression model. to do this, we shall make use of the derivative of the logistic sig-
moid function, which can conveniently be expressed in terms of the sigmoid function
itself

=   (1       ).

d  
da

(4.88)

section 3.6

exercise 4.12

206

4. linear models for classification

for a data set {  n, tn}, where tn     {0, 1} and   n =   (xn), with n =

1, . . . , n, the likelihood function can be written

p(t|w) =

n {1     yn}1   tn
ytn

(4.89)

n(cid:14)

n=1

where t = (t1, . . . , tn )t and yn = p(c1|  n). as usual, we can de   ne an error
function by taking the negative logarithm of the likelihood, which gives the cross-
id178 error function in the form

e(w) =     ln p(t|w) =     n(cid:2)

n=1

{tn ln yn + (1     tn) ln(1     yn)}

(4.90)

exercise 4.13

section 3.1.1

exercise 4.14

n(cid:2)

where yn =   (an) and an = wt  n. taking the gradient of the error function with
respect to w, we obtain

   e(w) =

(yn     tn)  n

(4.91)

n=1

where we have made use of (4.88). we see that the factor involving the derivative
of the logistic sigmoid has cancelled, leading to a simpli   ed form for the gradient
of the log likelihood. in particular, the contribution to the gradient from data point
n is given by the    error    yn     tn between the target value and the prediction of the
model, times the basis function vector   n. furthermore, comparison with (3.13)
shows that this takes precisely the same form as the gradient of the sum-of-squares
error function for the id75 model.

if desired, we could make use of the result (4.91) to give a sequential algorithm
in which patterns are presented one at a time, in which each of the weight vectors is
updated using (3.22) in which    en is the nth term in (4.91).

it is worth noting that maximum likelihood can exhibit severe over-   tting for
data sets that are linearly separable. this arises because the maximum likelihood so-
lution occurs when the hyperplane corresponding to    = 0.5, equivalent to wt   =
0, separates the two classes and the magnitude of w goes to in   nity. in this case, the
logistic sigmoid function becomes in   nitely steep in feature space, corresponding to
a heaviside step function, so that every training point from each class k is assigned
a posterior id203 p(ck|x) = 1. furthermore, there is typically a continuum
of such solutions because any separating hyperplane will give rise to the same pos-
terior probabilities at the training data points, as will be seen later in figure 10.13.
maximum likelihood provides no way to favour one such solution over another, and
which solution is found in practice will depend on the choice of optimization algo-
rithm and on the parameter initialization. note that the problem will arise even if
the number of data points is large compared with the number of parameters in the
model, so long as the training data set is linearly separable. the singularity can be
avoided by inclusion of a prior and    nding a map solution for w, or equivalently by
adding a id173 term to the error function.

4.3. probabilistic discriminative models

207

4.3.3 iterative reweighted least squares
in the case of the id75 models discussed in chapter 3, the maxi-
mum likelihood solution, on the assumption of a gaussian noise model, leads to a
closed-form solution. this was a consequence of the quadratic dependence of the
log likelihood function on the parameter vector w. for id28, there
is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid
function. however, the departure from a quadratic form is not substantial. to be
precise, the error function is concave, as we shall see shortly, and hence has a unique
minimum. furthermore, the error function can be minimized by an ef   cient iterative
technique based on the newton-raphson iterative optimization scheme, which uses a
local quadratic approximation to the log likelihood function. the newton-raphson
update, for minimizing a function e(w), takes the form (fletcher, 1987; bishop and
nabney, 2008)

(4.92)
where h is the hessian matrix whose elements comprise the second derivatives of
e(w) with respect to the components of w.

let us    rst of all apply the newton-raphson method to the id75
model (3.3) with the sum-of-squares error function (3.12). the gradient and hessian
of this error function are given by

w(new) = w(old)     h   1   e(w).

   e(w) =

h =       e(w) =

(wt  n     tn)  n =   t  w       tt

  n  t

n =   t  

(4.93)

(4.94)

n(cid:2)
n(cid:2)

n=1

n=1

section 3.1.1

where    is the n    m design matrix, whose nth row is given by   t
(cid:27)
raphson update then takes the form
  t  w(old)       tt

w(new) = w(old)     (  t  )   1

(cid:26)

n. the newton-

= (  t  )   1  tt

which we recognize as the standard least-squares solution. note that the error func-
tion in this case is quadratic and hence the newton-raphson formula gives the exact
solution in one step.

now let us apply the newton-raphson update to the cross-id178 error function
(4.90) for the id28 model. from (4.91) we see that the gradient and
hessian of this error function are given by

n(cid:2)

   e(w) =

(yn     tn)  n =   t(y     t)

n=1

h =       e(w) =

yn(1     yn)  n  t

n =   tr  

n(cid:2)

n=1

(4.95)

(4.96)

(4.97)

208

4. linear models for classification

id56 = yn(1     yn).

where we have made use of (4.88). also, we have introduced the n    n diagonal
matrix r with elements

(4.98)
we see that the hessian is no longer constant but depends on w through the weight-
ing matrix r, corresponding to the fact that the error function is no longer quadratic.
using the property 0 < yn < 1, which follows from the form of the logistic sigmoid
function, we see that uthu > 0 for an arbitrary vector u, and so the hessian matrix
h is positive de   nite. it follows that the error function is a concave function of w
and hence has a unique minimum.

exercise 4.15

the newton-raphson update formula for the id28 model then be-

comes

w(new) = w(old)     (  tr  )   1  t(y     t)

(cid:26)

(cid:27)
  tr  w(old)       t(y     t)

= (  tr  )   1
= (  tr  )   1  trz

where z is an n-dimensional vector with elements

z =   w(old)     r   1(y     t).

(4.99)

(4.100)

we see that the update formula (4.99) takes the form of a set of normal equations for a
weighted least-squares problem. because the weighing matrix r is not constant but
depends on the parameter vector w, we must apply the normal equations iteratively,
each time using the new weight vector w to compute a revised weighing matrix
r. for this reason, the algorithm is known as iterative reweighted least squares, or
irls (rubin, 1983). as in the weighted least-squares problem, the elements of the
diagonal weighting matrix r can be interpreted as variances because the mean and
variance of t in the id28 model are given by

e[t] =   (x) = y
var[t] = e[t2]     e[t]2 =   (x)       (x)2 = y(1     y)

(4.101)
(4.102)
where we have used the property t2 = t for t     {0, 1}. in fact, we can interpret irls
as the solution to a linearized problem in the space of the variable a = wt  . the
quantity zn, which corresponds to the nth element of z, can then be given a simple
interpretation as an effective target value in this space obtained by making a local
linear approximation to the logistic sigmoid function around the current operating
point w(old)

an(w) (cid:7) an(w(old)) +

dan
dyn
nw(old)     (yn     tn)
yn(1     yn)

w(old)

(tn     yn)

= zn.

(4.103)

=   t

(cid:7)(cid:7)(cid:7)(cid:7)

(4.104)

(4.105)

4.3. probabilistic discriminative models

209

section 4.2

4.3.4 multiclass id28
in our discussion of generative models for multiclass classi   cation, we have
seen that for a large class of distributions, the posterior probabilities are given by a
softmax transformation of linear functions of the feature variables, so that

p(ck|  ) = yk(  ) =

(cid:5)

exp(ak)
j exp(aj)

where the    activations    ak are given by

ak = wt

k   .

there we used maximum likelihood to determine separately the class-conditional
densities and the class priors and then found the corresponding posterior probabilities
using bayes    theorem, thereby implicitly determining the parameters {wk}. here we
consider the use of maximum likelihood to determine the parameters {wk} of this
model directly. to do this, we will require the derivatives of yk with respect to all of
the activations aj. these are given by

exercise 4.17

= yk(ikj     yj)

   yk
   aj

(4.106)

where ikj are the elements of the identity matrix.

next we write down the likelihood function. this is most easily done using
the 1-of-k coding scheme in which the target vector tn for a feature vector   n
belonging to class ck is a binary vector with all elements zero except for element k,
which equals one. the likelihood function is then given by

n(cid:14)

k(cid:14)

n(cid:14)

k(cid:14)

p(t|w1, . . . , wk) =

p(ck|  n)tnk =

ytnk
nk

(4.107)

n=1

k=1

n=1

k=1

where ynk = yk(  n), and t is an n    k matrix of target variables with elements
tnk. taking the negative logarithm then gives

e(w1, . . . , wk) =     ln p(t|w1, . . . , wk) =     n(cid:2)

tnk ln ynk

(4.108)

k(cid:2)

which is known as the cross-id178 error function for the multiclass classi   cation
problem.

n=1

k=1

we now take the gradient of the error function with respect to one of the param-
eter vectors wj. making use of the result (4.106) for the derivatives of the softmax
function, we obtain

   wj e(w1, . . . , wk) =

(ynj     tnj)   n

(4.109)

n(cid:2)

n=1

exercise 4.18

210

4. linear models for classification

(cid:5)

k tnk = 1. once again, we see the same form arising
where we have made use of
for the gradient as was found for the sum-of-squares error function with the linear
model and the cross-id178 error for the id28 model, namely the prod-
uct of the error (ynj     tnj) times the basis function   n. again, we could use this
to formulate a sequential algorithm in which patterns are presented one at a time, in
which each of the weight vectors is updated using (3.22).

we have seen that the derivative of the log likelihood function for a linear regres-
sion model with respect to the parameter vector w for a data point n took the form
of the    error    yn     tn times the feature vector   n. similarly, for the combination
of logistic sigmoid activation function and cross-id178 error function (4.90), and
for the softmax activation function with the multiclass cross-id178 error function
(4.108), we again obtain this same simple form. this is an example of a more general
result, as we shall see in section 4.3.6.

to    nd a batch algorithm, we again appeal to the newton-raphson update to
obtain the corresponding irls algorithm for the multiclass problem. this requires
evaluation of the hessian matrix that comprises blocks of size m    m in which
block j, k is given by

   wj e(w1, . . . , wk) =     n(cid:2)

   wk

ynk(ikj     ynj)  n  t
n.

(4.110)

exercise 4.20

n=1

as with the two-class problem, the hessian matrix for the multiclass logistic regres-
sion model is positive de   nite and so the error function again has a unique minimum.
practical details of irls for the multiclass case can be found in bishop and nabney
(2008).

4.3.5 probit regression
we have seen that, for a broad range of class-conditional distributions, described
by the exponential family, the resulting posterior class probabilities are given by a
logistic (or softmax) transformation acting on a linear function of the feature vari-
ables. however, not all choices of class-conditional density give rise to such a simple
form for the posterior probabilities (for instance, if the class-conditional densities are
modelled using gaussian mixtures). this suggests that it might be worth exploring
other types of discriminative probabilistic model. for the purposes of this chapter,
however, we shall return to the two-class case, and again remain within the frame-
work of generalized linear models so that

p(t = 1|a) = f(a)
where a = wt  , and f(  ) is the activation function.

(4.111)

one way to motivate an alternative choice for the link function is to consider a
noisy threshold model, as follows. for each input   n, we evaluate an = wt  n and
then we set the target value according to

(cid:12)

tn = 1 if an (cid:2)   
tn = 0 otherwise.

(4.112)

4.3. probabilistic discriminative models

211

figure 4.13 schematic example of a id203 density p(  )
shown by the blue curve, given in this example by a mixture
of two gaussians, along with its cumulative distribution function
f (a), shown by the red curve. note that the value of the blue
curve at any point, such as that indicated by the vertical green
line, corresponds to the slope of the red curve at the same point.
conversely, the value of the red curve at this point corresponds
to the area under the blue curve indicated by the shaded green
region. in the stochastic threshold model, the class label takes
the value t = 1 if the value of a = wt   exceeds a threshold, oth-
erwise it takes the value t = 0. this is equivalent to an activation
function given by the cumulative distribution function f (a).

1

0.8

0.6

0.4

0.2

0

0

1

2

3

4

if the value of    is drawn from a id203 density p(  ), then the corresponding
activation function will be given by the cumulative distribution function

(cid:6) a

f(a) =

p(  ) d  

      

(4.113)

as illustrated in figure 4.13.

as a speci   c example, suppose that the density p(  ) is given by a zero mean,
unit variance gaussian. the corresponding cumulative distribution function is given
by

(cid:6) a

  (a) =

      

n (  |0, 1) d  

(4.114)

exercise 4.21

and known as the erf function or error function (not to be confused with the error
function of a machine learning model). it is related to the probit function by

which is known as the probit function. it has a sigmoidal shape and is compared
with the logistic sigmoid function in figure 4.9. note that the use of a more gen-
eral gaussian distribution does not change the model because this is equivalent to
a re-scaling of the linear coef   cients w. many numerical packages provide for the
evaluation of a closely related function de   ned by

erf(a) =

2   
  

exp(     2/2) d  

(4.115)

(cid:6) a

0

(cid:12)

(cid:13)

  (a) =

1
2

1   
2

1 +

erf(a)

.

(4.116)

the generalized linear model based on a probit activation function is known as probit
regression.

we can determine the parameters of this model using maximum likelihood, by a
straightforward extension of the ideas discussed earlier. in practice, the results found
using probit regression tend to be similar to those of id28. we shall,

212

4. linear models for classification

however,    nd another use for the probit model when we discuss bayesian treatments
of id28 in section 4.5.

one issue that can occur in practical applications is that of outliers, which can
arise for instance through errors in measuring the input vector x or through misla-
belling of the target value t. because such points can lie a long way to the wrong side
of the ideal decision boundary, they can seriously distort the classi   er. note that the
logistic and probit regression models behave differently in this respect because the
tails of the logistic sigmoid decay asymptotically like exp(   x) for x        , whereas
for the probit activation function they decay like exp(   x2), and so the probit model
can be signi   cantly more sensitive to outliers.

however, both the logistic and the probit models assume the data is correctly
labelled. the effect of mislabelling is easily incorporated into a probabilistic model
by introducing a id203   that the target value t has been    ipped to the wrong
value (opper and winther, 2000a), leading to a target value distribution for data point
x of the form

p(t|x) = (1      )  (x) +  (1       (x))

=   + (1     2 )  (x)

(4.117)
where   (x) is the activation function with input vector x. here   may be set in
advance, or it may be treated as a hyperparameter whose value is inferred from the
data.

4.3.6 canonical link functions
for the id75 model with a gaussian noise distribution, the error
function, corresponding to the negative log likelihood, is given by (3.12). if we take
the derivative with respect to the parameter vector w of the contribution to the error
function from a data point n, this takes the form of the    error    yn     tn times the
feature vector   n, where yn = wt  n. similarly, for the combination of the logistic
sigmoid activation function and the cross-id178 error function (4.90), and for the
softmax activation function with the multiclass cross-id178 error function (4.108),
we again obtain this same simple form. we now show that this is a general result
of assuming a conditional distribution for the target variable from the exponential
family, along with a corresponding choice for the activation function known as the
canonical link function.

we again make use of the restricted form (4.84) of exponential family distribu-
tions. note that here we are applying the assumption of exponential family distribu-
tion to the target variable t, in contrast to section 4.2.4 where we applied it to the
input vector x. we therefore consider conditional distributions of the target variable
of the form

(cid:19)

(cid:20)

(cid:17)

(cid:18)

p(t|  , s) =

1
s

h

t
s

  t
s

g(  ) exp

.

(4.118)

using the same line of argument as led to the derivation of the result (2.226), we see
that the conditional mean of t, which we denote by y, is given by

y     e[t|  ] =    s

d
d  

ln g(  ).

(4.119)

4.4. the laplace approximation

213

thus y and    must related, and we denote this relation through    =   (y).

following nelder and wedderburn (1972), we de   ne a generalized linear model
to be one for which y is a nonlinear function of a linear combination of the input (or
feature) variables so that
y = f(wt  )
(4.120)
where f(  ) is known as the activation function in the machine learning literature, and
   1(  ) is known as the link function in statistics.
f
(cid:19)

now consider the log likelihood function for this model, which, as a function of

  , is given by

(cid:20)

n(cid:2)

n(cid:2)

ln p(t|  , s) =

ln p(tn|  , s) =

n=1

n=1

ln g(  n) +   ntn
s

+ const

(4.121)

where we are assuming that all observations share a common scale parameter (which
corresponds to the noise variance for a gaussian distribution for instance) and so s
is independent of n. the derivative of the log likelihood with respect to the model
parameters w is then given by

(cid:13)

   w ln p(t|  , s) =

=

d
d  n

ln g(  n) + tn
s

d  n
dyn

dyn
dan

   an

{tn     yn}   

(cid:4)(yn)f

(cid:4)(an)  n

1
s

(4.122)

(cid:12)

n(cid:2)
n(cid:2)

n=1

n=1

where an = wt  n, and we have used yn = f(an) together with the result (4.119)
for e[t|  ]. we now see that there is a considerable simpli   cation if we choose a
particular form for the link function f

   1(y) given by

which gives f(  (y)) = y and hence f
we have a =    and hence f
function reduces to

(cid:4)(a)  

   1(y) =   (y)

f

(4.123)
   1(y),
(cid:4)(y) = 1. in this case, the gradient of the error

(cid:4)(y) = 1. also, because a = f

(cid:4)(  )  

    ln e(w) =

1
s

{yn     tn}  n.

(4.124)

n(cid:2)

n=1

for the gaussian s =   

   1, whereas for the logistic model s = 1.

4.4. the laplace approximation

in section 4.5 we shall discuss the bayesian treatment of id28. as
we shall see, this is more complex than the bayesian treatment of id75
models, discussed in sections 3.3 and 3.5. in particular, we cannot integrate exactly

214

4. linear models for classification

chapter 10
chapter 11

over the parameter vector w since the posterior distribution is no longer gaussian.
it is therefore necessary to introduce some form of approximation. later in the
book we shall consider a range of techniques based on analytical approximations
and numerical sampling.

here we introduce a simple, but widely used, framework called the laplace ap-
proximation, that aims to    nd a gaussian approximation to a id203 density
de   ned over a set of continuous variables. consider    rst the case of a single contin-
uous variable z, and suppose the distribution p(z) is de   ned by

(cid:7)(cid:7)(cid:7)(cid:7)

df(z)
dz

(cid:28)

p(z) =

1
z

f(z)

(4.125)

f(z) dz is the id172 coef   cient. we shall suppose that the
where z =
value of z is unknown. in the laplace method the goal is to    nd a gaussian approx-
imation q(z) which is centred on a mode of the distribution p(z). the    rst step is to
(cid:4)(z0) = 0, or equivalently
   nd a mode of p(z), in other words a point z0 such that p

= 0.

z=z0

(4.126)

a gaussian distribution has the property that its logarithm is a quadratic function
of the variables. we therefore consider a taylor expansion of ln f(z) centred on the
mode z0 so that

ln f(z) (cid:7) ln f(z0)     1

2 a(z     z0)2

a =     d2

dz2 ln f(z)

.

z=z0

(cid:7)(cid:7)(cid:7)(cid:7)

(4.127)

(4.128)

where

(cid:12)

(cid:13)

(cid:13)

note that the    rst-order term in the taylor expansion does not appear since z0 is a
local maximum of the distribution. taking the exponential we obtain

f(z) (cid:7) f(z0) exp

(z     z0)2

    a
2

.

(4.129)

we can then obtain a normalized distribution q(z) by making use of the standard
result for the id172 of a gaussian, so that

(cid:15)

(cid:16)1/2

(cid:12)

q(z) =

a
2  

exp

(z     z0)2

    a
2

.

(4.130)

the laplace approximation is illustrated in figure 4.14. note that the gaussian
approximation will only be well de   ned if its precision a > 0, in other words the
stationary point z0 must be a local maximum, so that the second derivative of f(z)
at the point z0 is negative.

4.4. the laplace approximation

215

0.8

0.6

0.4

0.2

0
   2

   1

0

1

2

3

4

40

30

20

10

0
   2

   1

0

1

2

3

4

figure 4.14 illustration of the laplace approximation applied to the distribution p(z)     exp(   z2/2)  (20z + 4)
where   (z) is the logistic sigmoid function de   ned by   (z) = (1 + e   z)   1. the left plot shows the normalized
distribution p(z) in yellow, together with the laplace approximation centred on the mode z0 of p(z) in red. the
right plot shows the negative logarithms of the corresponding curves.

we can extend the laplace method to approximate a distribution p(z) = f(z)/z
de   ned over an m-dimensional space z. at a stationary point z0 the gradient    f(z)
will vanish. expanding around this stationary point we have

ln f(z) (cid:7) ln f(z0)     1
2

(z     z0)ta(z     z0)

where the m    m hessian matrix a is de   ned by

a =            ln f(z)|z=z0

and     is the gradient operator. taking the exponential of both sides we obtain

(4.131)

(4.132)

(cid:13)

f(z) (cid:7) f(z0) exp

(z     z0)ta(z     z0)

.

(4.133)

(cid:12)

   1
2

(cid:13)

(cid:12)

q(z) =

|a|1/2
(2  )m/2 exp

the distribution q(z) is proportional to f(z) and the appropriate id172 coef-
   cient can be found by inspection, using the standard result (2.43) for a normalized
multivariate gaussian, giving
   1
2

(4.134)
where |a| denotes the determinant of a. this gaussian distribution will be well
de   ned provided its precision matrix, given by a, is positive de   nite, which implies
that the stationary point z0 must be a local maximum, not a minimum or a saddle
point.

(z     z0)ta(z     z0)

= n (z|z0, a   1)

in order to apply the laplace approximation we    rst need to    nd the mode z0,
and then evaluate the hessian matrix at that mode. in practice a mode will typi-
cally be found by running some form of numerical optimization algorithm (bishop

216

4. linear models for classification

and nabney, 2008). many of the distributions encountered in practice will be mul-
timodal and so there will be different laplace approximations according to which
mode is being considered. note that the id172 constant z of the true distri-
bution does not need to be known in order to apply the laplace method. as a result
of the central limit theorem, the posterior distribution for a model is expected to
become increasingly better approximated by a gaussian as the number of observed
data points is increased, and so we would expect the laplace approximation to be
most useful in situations where the number of data points is relatively large.

one major weakness of the laplace approximation is that, since it is based on a
gaussian distribution, it is only directly applicable to real variables. in other cases
it may be possible to apply the laplace approximation to a transformation of the
variable. for instance if 0 (cid:1)    <     then we can consider a laplace approximation
of ln    . the most serious limitation of the laplace framework, however, is that
it is based purely on the aspects of the true distribution at a speci   c value of the
variable, and so can fail to capture important global properties. in chapter 10 we
shall consider alternative approaches which adopt a more global perspective.

4.4.1 model comparison and bic
as well as approximating the distribution p(z) we can also obtain an approxi-

mation to the id172 constant z. using the approximation (4.133) we have

(cid:6)

z =

f(z) dz

(cid:6)

(cid:12)

   1
2

(cid:7) f(z0)

= f(z0)

exp

(2  )m/2
|a|1/2

(cid:13)

(z     z0)ta(z     z0)

dz

(4.135)

where we have noted that the integrand is gaussian and made use of the standard
result (2.43) for a normalized gaussian distribution. we can use the result (4.135) to
obtain an approximation to the model evidence which, as discussed in section 3.4,
plays a central role in bayesian model comparison.
consider a data set d and a set of models {mi} having parameters {  i}. for
each model we de   ne a likelihood function p(d|  i,mi). if we introduce a prior
p(  i|mi) over the parameters, then we are interested in computing the model evi-
dence p(d|mi) for the various models. from now on we omit the conditioning on
mi to keep the notation uncluttered. from bayes    theorem the model evidence is
given by

(cid:6)

p(d) =

p(d|  )p(  ) d  .

(4.136)
identifying f(  ) = p(d|  )p(  ) and z = p(d), and applying the result (4.135), we
obtain

ln p(d) (cid:7) ln p(d|  map) + ln p(  map) + m
2

ln(2  )     1
2

ln|a|

(4.137)

)*

(

+

occam factor

exercise 4.22

exercise 4.23

section 3.5.3

4.5. bayesian id28

217

where   map is the value of    at the mode of the posterior distribution, and a is the
hessian matrix of second derivatives of the negative log posterior

a =           ln p(d|  map)p(  map) =           ln p(  map|d).

(4.138)

the    rst term on the right hand side of (4.137) represents the log likelihood evalu-
ated using the optimized parameters, while the remaining three terms comprise the
   occam factor    which penalizes model complexity.

if we assume that the gaussian prior distribution over parameters is broad, and
that the hessian has full rank, then we can approximate (4.137) very roughly using

ln p(d) (cid:7) ln p(d|  map)     1

2 m ln n

(4.139)

where n is the number of data points, m is the number of parameters in    and
we have omitted additive constants. this is known as the bayesian information
criterion (bic) or the schwarz criterion (schwarz, 1978). note that, compared to
aic given by (1.73), this penalizes model complexity more heavily.

complexity measures such as aic and bic have the virtue of being easy to
evaluate, but can also give misleading results. in particular, the assumption that the
hessian matrix has full rank is often not valid since many of the parameters are not
   well-determined   . we can use the result (4.137) to obtain a more accurate estimate
of the model evidence starting from the laplace approximation, as we illustrate in
the context of neural networks in section 5.7.

4.5. bayesian id28

we now turn to a bayesian treatment of id28. exact bayesian infer-
ence for id28 is intractable. in particular, evaluation of the posterior
distribution would require id172 of the product of a prior distribution and a
likelihood function that itself comprises a product of logistic sigmoid functions, one
for every data point. evaluation of the predictive distribution is similarly intractable.
here we consider the application of the laplace approximation to the problem of
bayesian id28 (spiegelhalter and lauritzen, 1990; mackay, 1992b).

4.5.1 laplace approximation
recall from section 4.4 that the laplace approximation is obtained by    nding
the mode of the posterior distribution and then    tting a gaussian centred at that
mode. this requires evaluation of the second derivatives of the log posterior, which
is equivalent to    nding the hessian matrix.

because we seek a gaussian representation for the posterior distribution, it is

natural to begin with a gaussian prior, which we write in the general form

p(w) = n (w|m0, s0)

(4.140)

218

4. linear models for classification

where m0 and s0 are    xed hyperparameters. the posterior distribution over w is
given by

(4.141)
where t = (t1, . . . , tn)t. taking the log of both sides, and substituting for the prior
distribution using (4.140), and for the likelihood function using (4.89), we obtain

p(w|t)     p(w)p(t|w)

(w     m0)ts

0 (w     m0)
   1

n(cid:2)
ln p(w|t) =    1
2

+

n=1

{tn ln yn + (1     tn) ln(1     yn)} + const

(4.142)

where yn =   (wt  n). to obtain a gaussian approximation to the posterior dis-
tribution, we    rst maximize the posterior distribution to give the map (maximum
posterior) solution wmap, which de   nes the mean of the gaussian. the covariance
is then given by the inverse of the matrix of second derivatives of the negative log
likelihood, which takes the form

sn =           ln p(w|t) = s

   1
0 +

yn(1     yn)  n  t
n.

(4.143)

n(cid:2)

n=1

the gaussian approximation to the posterior distribution therefore takes the form

q(w) = n (w|wmap, sn ).

(4.144)

having obtained a gaussian approximation to the posterior distribution, there
remains the task of marginalizing with respect to this distribution in order to make
predictions.

4.5.2 predictive distribution
the predictive distribution for class c1, given a new feature vector   (x), is
obtained by marginalizing with respect to the posterior distribution p(w|t), which is
itself approximated by a gaussian distribution q(w) so that

p(c1|  , t) =

p(c1|  , w)p(w|t) dw (cid:7)

(4.145)
with the corresponding id203 for class c2 given by p(c2|  , t) = 1    p(c1|  , t).
to evaluate the predictive distribution, we    rst note that the function   (wt  ) de-
pends on w only through its projection onto   . denoting a = wt  , we have

  (wt  )q(w) dw

(cid:6)

  (a     wt  )  (a) da
where   (  ) is the dirac delta function. from this we obtain

  (wt  ) =

(4.146)

(cid:6)

(cid:6)

(cid:6)

(cid:6)

  (wt  )q(w) dw =

  (a)p(a) da

(4.147)

where

(cid:6)

p(a) =

4.5. bayesian id28

219

  (a     wt  )q(w) dw.

(4.148)

we can evaluate p(a) by noting that the delta function imposes a linear constraint
on w and so forms a marginal distribution from the joint distribution q(w) by inte-
grating out all directions orthogonal to   . because q(w) is gaussian, we know from
section 2.3.2 that the marginal distribution will also be gaussian. we can evaluate
the mean and covariance of this distribution by taking moments, and interchanging
the order of integration over a and w, so that

  a = e[a] =

p(a)a da =

q(w)wt   dw = wt

map  

(4.149)

where we have used the result (4.144) for the variational posterior distribution q(w).
similarly

  2
a = var[a] =

(cid:6)
(cid:26)

(cid:27)
(cid:27)

da

a2     e[a]2

p(a)
(wt  )2     (mt

=

q(w)

n   )2

dw =   tsn   .

(4.150)

(cid:6)

(cid:26)

(cid:6)

(cid:6)

note that the distribution of a takes the same form as the predictive distribution
(3.58) for the id75 model, with the noise variance set to zero. thus our
variational approximation to the predictive distribution becomes
  (a)n (a|  a,   2

  (a)p(a) da =

p(c1|t) =

(4.151)

(cid:6)

(cid:6)

a) da.

this result can also be derived directly by making use of the results for the marginal
of a gaussian distribution given in section 2.3.2.

the integral over a represents the convolution of a gaussian with a logistic sig-
moid, and cannot be evaluated analytically. we can, however, obtain a good approx-
imation (spiegelhalter and lauritzen, 1990; mackay, 1992b; barber and bishop,
1998a) by making use of the close similarity between the logistic sigmoid function
  (a) de   ned by (4.59) and the probit function   (a) de   ned by (4.114). in order to
obtain the best approximation to the logistic function we need to re-scale the hori-
zontal axis, so that we approximate   (a) by   (  a). we can    nd a suitable value of
   by requiring that the two functions have the same slope at the origin, which gives
  2 =   /8. the similarity of the logistic sigmoid and the probit function, for this
choice of   , is illustrated in figure 4.9.

the advantage of using a probit function is that its convolution with a gaussian
can be expressed analytically in terms of another probit function. speci   cally we
can show that

(cid:15)

(cid:16)

(cid:6)

  (  a)n (a|  ,   2) da =   

  

(     2 +   2)1/2

.

(4.152)

exercise 4.24

exercise 4.25

exercise 4.26

220

4. linear models for classification

we now apply the approximation   (a) (cid:7)   (  a) to the probit functions appearing
on both sides of this equation, leading to the following approximation for the convo-
lution of a logistic sigmoid with a gaussian

(cid:6)

(cid:10)

(cid:11)

  (a)n (a|  ,   2) da (cid:7)   

  (  2)  

(4.153)

exercises

(cid:11)

(cid:10)

where we have de   ned

(4.154)
applying this result to (4.151) we obtain the approximate predictive distribution

  (  2) = (1 +     2/8)   1/2.

in the form

p(c1|  , t) =   

  (  2

a)  a

where   a and   2
   ned by (4.154).

a are de   ned by (4.149) and (4.150), respectively, and   (  2

(4.155)
a) is de-
note that the decision boundary corresponding to p(c1|  , t) = 0.5 is given by
  a = 0, which is the same as the decision boundary obtained by using the map
value for w. thus if the decision criterion is based on minimizing misclassi   ca-
tion rate, with equal prior probabilities, then the marginalization over w has no ef-
fect. however, for more complex decision criteria it will play an important role.
marginalization of the logistic sigmoid model under a gaussian approximation to
the posterior distribution will be illustrated in the context of variational id136 in
figure 10.13.

(cid:2)

4.1 ((cid:12) (cid:12)) given a set of data points {xn}, we can de   ne the convex hull to be the set of

all points x given by

x =

(cid:5)
(4.156)
n   n = 1. consider a second set of points {yn} together with
separable if there exists a vector(cid:1)w and a scalar w0 such that(cid:1)wtxn + w0 > 0 for all
where   n (cid:2) 0 and
xn, and(cid:1)wtyn + w0 < 0 for all yn. show that if their convex hulls intersect, the two
their corresponding convex hull. by de   nition, the two sets of points will be linearly

  nxn

n

sets of points cannot be linearly separable, and conversely that if they are linearly
separable, their convex hulls do not intersect.

4.2 ((cid:12) (cid:12)) www consider the minimization of a sum-of-squares error function (4.15),
and suppose that all of the target vectors in the training set satisfy a linear constraint

attn + b = 0

(4.157)

where tn corresponds to the nth row of the matrix t in (4.15). show that as a
consequence of this constraint, the elements of the model prediction y(x) given by
the least-squares solution (4.17) also satisfy this constraint, so that

aty(x) + b = 0.

(4.158)

exercises

221

to do so, assume that one of the basis functions   0(x) = 1 so that the corresponding
parameter w0 plays the role of a bias.

4.3 ((cid:12) (cid:12)) extend the result of exercise 4.2 to show that if multiple linear constraints
are satis   ed simultaneously by the target vectors, then the same constraints will also
be satis   ed by the least-squares prediction of a linear model.

4.4 ((cid:12)) www show that maximization of the class separation criterion given by (4.23)
with respect to w, using a lagrange multiplier to enforce the constraint wtw = 1,
leads to the result that w     (m2     m1).

4.5 ((cid:12)) by making use of (4.20), (4.23), and (4.24), show that the fisher criterion (4.25)

can be written in the form (4.26).

4.6 ((cid:12)) using the de   nitions of the between-class and within-class covariance matrices
given by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the
choice of target values described in section 4.1.5, show that the expression (4.33)
that minimizes the sum-of-squares error function can be written in the form (4.37).

4.7 ((cid:12)) www show that the logistic sigmoid function (4.59) satis   es the property

  (   a) = 1       (a) and that its inverse is given by   

   1(y) = ln{y/(1     y)}.

4.8 ((cid:12)) using (4.57) and (4.58), derive the result (4.65) for the posterior class id203
in the two-class generative model with gaussian densities, and verify the results
(4.66) and (4.67) for the parameters w and w0.

4.9 ((cid:12)) www consider a generative classi   cation model for k classes de   ned by
prior class probabilities p(ck) =   k and general class-conditional densities p(  |ck)
where    is the input feature vector. suppose we are given a training data set {  n, tn}
where n = 1, . . . , n, and tn is a binary target vector of length k that uses the 1-of-
k coding scheme, so that it has components tnj = ijk if pattern n is from class ck.
assuming that the data points are drawn independently from this model, show that
the maximum-likelihood solution for the prior probabilities is given by

where nk is the number of data points assigned to class ck.

  k = nk
n

(4.159)

4.10 ((cid:12) (cid:12)) consider the classi   cation model of exercise 4.9 and now suppose that the
class-conditional densities are given by gaussian distributions with a shared covari-
ance matrix, so that

p(  |ck) = n (  |  k,   ).

(4.160)

show that the maximum likelihood solution for the mean of the gaussian distribution
for class ck is given by

tnk  n

(4.161)

n(cid:2)

n=1

  k =

1
nk

222

4. linear models for classification

k(cid:2)

k=1

   =

nk
n

sk

n(cid:2)

n=1

which represents the mean of those feature vectors assigned to class ck. similarly,
show that the maximum likelihood solution for the shared covariance matrix is given
by

(4.162)

(4.163)

where

sk =

1
nk

tnk(  n       k)(  n       k)t.

thus    is given by a weighted average of the covariances of the data associated with
each class, in which the weighting coef   cients are given by the prior probabilities of
the classes.

4.11 ((cid:12) (cid:12)) consider a classi   cation problem with k classes for which the feature vector
   has m components each of which can take l discrete states. let the values of the
components be represented by a 1-of-l binary coding scheme. further suppose that,
conditioned on the class ck, the m components of    are independent, so that the
class-conditional density factorizes with respect to the feature vector components.
show that the quantities ak given by (4.63), which appear in the argument to the
softmax function describing the posterior class probabilities, are linear functions of
the components of   . note that this represents an example of the naive bayes model
which is discussed in section 8.2.2.

4.12 ((cid:12)) www verify the relation (4.88) for the derivative of the logistic sigmoid func-

tion de   ned by (4.59).

4.13 ((cid:12)) www by making use of the result (4.88) for the derivative of the logistic sig-
moid, show that the derivative of the error function (4.90) for the id28
model is given by (4.91).

4.14 ((cid:12)) show that for a linearly separable data set, the maximum likelihood solution
for the id28 model is obtained by    nding a vector w whose decision
boundary wt  (x) = 0 separates the classes and then taking the magnitude of w to
in   nity.

4.15 ((cid:12) (cid:12)) show that the hessian matrix h for the id28 model, given by
(4.97), is positive de   nite. here r is a diagonal matrix with elements yn(1     yn),
and yn is the output of the id28 model for input vector xn. hence show
that the error function is a concave function of w and that it has a unique minimum.
4.16 ((cid:12)) consider a binary classi   cation problem in which each observation xn is known
to belong to one of two classes, corresponding to t = 0 and t = 1, and suppose that
the procedure for collecting training data is imperfect, so that training points are
sometimes mislabelled. for every data point xn, instead of having a value t for the
class label, we have instead a value   n representing the id203 that tn = 1.
given a probabilistic model p(t = 1|  ), write down the log likelihood function
appropriate to such a data set.

exercises

223

4.17 ((cid:12)) www show that the derivatives of the softmax activation function (4.104),

where the ak are de   ned by (4.105), are given by (4.106).

4.18 ((cid:12)) using the result (4.91) for the derivatives of the softmax activation function,

show that the gradients of the cross-id178 error (4.108) are given by (4.109).

4.19 ((cid:12)) www write down expressions for the gradient of the log likelihood, as well
as the corresponding hessian matrix, for the probit regression model de   ned in sec-
tion 4.3.5. these are the quantities that would be required to train such a model using
irls.

4.20 ((cid:12) (cid:12)) show that the hessian matrix for the multiclass id28 problem,
de   ned by (4.110), is positive semide   nite. note that the full hessian matrix for
this problem is of size m k    m k, where m is the number of parameters and k
is the number of classes. to prove the positive semide   nite property, consider the
product uthu where u is an arbitrary vector of length m k, and then apply jensen   s
inequality.

4.21 ((cid:12)) show that the probit function (4.114) and the erf function (4.115) are related by

(4.116).

4.22 ((cid:12)) using the result (4.135), derive the expression (4.137) for the log model evi-

dence under the laplace approximation.

4.23 ((cid:12) (cid:12)) www in this exercise, we derive the bic result (4.139) starting from the
laplace approximation to the model evidence given by (4.137). show that if the
prior over parameters is gaussian of the form p(  ) = n (  |m, v0), the log model
evidence under the laplace approximation takes the form
ln p(d) (cid:7) ln p(d|  map)     1
ln|h| + const
2
where h is the matrix of second derivatives of the log likelihood ln p(d|  ) evaluated
at   map. now assume that the prior is broad so that v
is small and the second
term on the right-hand side above can be neglected. furthermore, consider the case
of independent, identically distributed data so that h is the sum of terms one for each
data point. show that the log model evidence can then be written approximately in
the form of the bic expression (4.139).

0 (  map     m)     1
   1
2

(  map     m)tv

   1
0

4.24 ((cid:12) (cid:12)) use the results from section 2.3.2 to derive the result (4.151) for the marginal-
ization of the id28 model with respect to a gaussian posterior distribu-
tion over the parameters w.

4.25 ((cid:12) (cid:12)) suppose we wish to approximate the logistic sigmoid   (a) de   ned by (4.59)
by a scaled probit function   (  a), where   (a) is de   ned by (4.114). show that if
   is chosen so that the derivatives of the two functions are equal at a = 0, then
  2 =   /8.

224

4. linear models for classification

4.26 ((cid:12) (cid:12))

in this exercise, we prove the relation (4.152) for the convolution of a probit
function with a gaussian distribution. to do this, show that the derivative of the left-
hand side with respect to    is equal to the derivative of the right-hand side, and then
integrate both sides with respect to    and then show that the constant of integration
vanishes. note that before differentiating the left-hand side, it is convenient    rst
to introduce a change of variable given by a =    +   z so that the integral over a
is replaced by an integral over z. when we differentiate the left-hand side of the
relation (4.152), we will then obtain a gaussian integral over z that can be evaluated
analytically.

5

neural

networks

in chapters 3 and 4 we considered models for regression and classi   cation that com-
prised linear combinations of    xed basis functions. we saw that such models have
useful analytical and computational properties but that their practical applicability
was limited by the curse of dimensionality. in order to apply such models to large-
scale problems, it is necessary to adapt the basis functions to the data.

support vector machines (id166s), discussed in chapter 7, address this by    rst
de   ning basis functions that are centred on the training data points and then selecting
a subset of these during training. one advantage of id166s is that, although the
training involves nonlinear optimization, the objective function is convex, and so the
solution of the optimization problem is relatively straightforward. the number of
basis functions in the resulting models is generally much smaller than the number of
training points, although it is often still relatively large and typically increases with
the size of the training set. the relevance vector machine, discussed in section 7.2,
also chooses a subset from a    xed set of basis functions and typically results in much

225

226

5. neural networks

sparser models. unlike the id166 it also produces probabilistic outputs, although this
is at the expense of a nonid76 during training.

an alternative approach is to    x the number of basis functions in advance but
allow them to be adaptive, in other words to use parametric forms for the basis func-
tions in which the parameter values are adapted during training. the most successful
model of this type in the context of pattern recognition is the feed-forward neural
network, also known as the multilayer id88, discussed in this chapter. in fact,
   multilayer id88    is really a misnomer, because the model comprises multi-
ple layers of id28 models (with continuous nonlinearities) rather than
multiple id88s (with discontinuous nonlinearities). for many applications, the
resulting model can be signi   cantly more compact, and hence faster to evaluate, than
a support vector machine having the same generalization performance. the price to
be paid for this compactness, as with the relevance vector machine, is that the like-
lihood function, which forms the basis for network training, is no longer a convex
function of the model parameters. in practice, however, it is often worth investing
substantial computational resources during the training phase in order to obtain a
compact model that is fast at processing new data.

the term    neural network    has its origins in attempts to    nd mathematical rep-
resentations of information processing in biological systems (mcculloch and pitts,
1943; widrow and hoff, 1960; rosenblatt, 1962; rumelhart et al., 1986). indeed,
it has been used very broadly to cover a wide range of different models, many of
which have been the subject of exaggerated claims regarding their biological plau-
sibility. from the perspective of practical applications of pattern recognition, how-
ever, biological realism would impose entirely unnecessary constraints. our focus in
this chapter is therefore on neural networks as ef   cient models for statistical pattern
recognition. in particular, we shall restrict our attention to the speci   c class of neu-
ral networks that have proven to be of greatest practical value, namely the multilayer
id88.

we begin by considering the functional form of the network model, including
the speci   c parameterization of the basis functions, and we then discuss the prob-
lem of determining the network parameters within a maximum likelihood frame-
work, which involves the solution of a nonlinear optimization problem. this requires
the evaluation of derivatives of the log likelihood function with respect to the net-
work parameters, and we shall see how these can be obtained ef   ciently using the
technique of error id26. we shall also show how the id26
framework can be extended to allow other derivatives to be evaluated, such as the
jacobian and hessian matrices. next we discuss various approaches to regulariza-
tion of neural network training and the relationships between them. we also consider
some extensions to the neural network model, and in particular we describe a gen-
eral framework for modelling id155 distributions known as mixture
density networks. finally, we discuss the use of bayesian treatments of neural net-
works. additional background on neural network models can be found in bishop
(1995a).

5.1. feed-forward network functions

227

5.1. feed-forward network functions

the linear models for regression and classi   cation discussed in chapters 3 and 4, re-
spectively, are based on linear combinations of    xed nonlinear basis functions   j(x)
and take the form

(cid:23)

(cid:22)
m(cid:2)

j=1

wj  j(x)

y(x, w) = f

(5.1)
where f(  ) is a nonlinear activation function in the case of classi   cation and is the
identity in the case of regression. our goal is to extend this model by making the
basis functions   j(x) depend on parameters and then to allow these parameters to
be adjusted, along with the coef   cients {wj}, during training. there are, of course,
many ways to construct parametric nonlinear basis functions. neural networks use
basis functions that follow the same form as (5.1), so that each basis function is itself
a nonlinear function of a linear combination of the inputs, where the coef   cients in
the linear combination are adaptive parameters.

this leads to the basic neural network model, which can be described a series
of functional transformations. first we construct m linear combinations of the input
variables x1, . . . , xd in the form

d(cid:2)

aj =

(1)
ji xi + w

(1)
j0

w

(5.2)

i=1

where j = 1, . . . , m, and the superscript (1) indicates that the corresponding param-
(1)
eters are in the    rst    layer    of the network. we shall refer to the parameters w
ji as
(1)
j0 as biases, following the nomenclature of chapter 3.
weights and the parameters w
the quantities aj are known as activations. each of them is then transformed using
a differentiable, nonlinear activation function h(  ) to give

zj = h(aj).

(5.3)

these quantities correspond to the outputs of the basis functions in (5.1) that, in the
context of neural networks, are called hidden units. the nonlinear functions h(  ) are
generally chosen to be sigmoidal functions such as the logistic sigmoid or the    tanh   
function. following (5.1), these values are again linearly combined to give output
unit activations

m(cid:2)

ak =

w

(2)

kj zj + w

(2)
k0

(5.4)

j=1

where k = 1, . . . , k, and k is the total number of outputs. this transformation cor-
(2)
responds to the second layer of the network, and again the w
k0 are bias parameters.
finally, the output unit activations are transformed using an appropriate activation
function to give a set of network outputs yk. the choice of activation function is
determined by the nature of the data and the assumed distribution of target variables

exercise 5.1

228

5. neural networks

figure 5.1 network diagram for the two-
layer neural network corre-
sponding to (5.7). the input,
hidden, and output variables
are represented by nodes, and
the weight parameters are rep-
resented by links between the
nodes,
in which the bias pa-
rameters are denoted by links
coming from additional
input
and hidden variables x0 and
z0. arrows denote the direc-
tion of information    ow through
the network during forward
propagation.

xd

inputs

x1

x0

hidden units

zm

(1)
m d

w

(2)
km

w

yk

outputs

y1

(2)
10

w

z1

z0

and follows the same considerations as for linear models discussed in chapters 3 and
4. thus for standard regression problems, the activation function is the identity so
that yk = ak. similarly, for multiple binary classi   cation problems, each output unit
activation is transformed using a logistic sigmoid function so that

where

yk =   (ak)

  (a) =

1

1 + exp(   a) .

(5.5)

(5.6)

finally, for multiclass problems, a softmax activation function of the form (4.62)
is used. the choice of output unit activation function is discussed in detail in sec-
tion 5.2.

we can combine these various stages to give the overall network function that,

for sigmoidal output unit id180, takes the form

(cid:23)

(cid:23)

(cid:22)
m(cid:2)

(cid:22)
d(cid:2)

yk(x, w) =   

w

(2)
kj h

(1)
ji xi + w

(1)
j0

w

+ w

(2)
k0

(5.7)

j=1

i=1

where the set of all weight and bias parameters have been grouped together into a
vector w. thus the neural network model is simply a nonlinear function from a set
of input variables {xi} to a set of output variables {yk} controlled by a vector w of
adjustable parameters.

this function can be represented in the form of a network diagram as shown
in figure 5.1. the process of evaluating (5.7) can then be interpreted as a forward
propagation of information through the network. it should be emphasized that these
diagrams do not represent probabilistic id114 of the kind to be consid-
ered in chapter 8 because the internal nodes represent deterministic variables rather
than stochastic ones. for this reason, we have adopted a slightly different graphical

d(cid:2)

i=0

5.1. feed-forward network functions

229

notation for the two kinds of model. we shall see later how to give a probabilistic
interpretation to a neural network.

as discussed in section 3.1, the bias parameters in (5.2) can be absorbed into
the set of weight parameters by de   ning an additional input variable x0 whose value
is clamped at x0 = 1, so that (5.2) takes the form

aj =

(1)
ji xi.

w

(5.8)

we can similarly absorb the second-layer biases into the second-layer weights, so
that the overall network function becomes

(cid:22)

m(cid:2)

(cid:22)
d(cid:2)

(cid:23)(cid:23)

yk(x, w) =   

w

(2)
kj h

(1)
ji xi

w

.

(5.9)

j=0

i=0

as can be seen from figure 5.1, the neural network model comprises two stages
of processing, each of which resembles the id88 model of section 4.1.7, and
for this reason the neural network is also known as the multilayer id88, or
mlp. a key difference compared to the id88, however, is that the neural net-
work uses continuous sigmoidal nonlinearities in the hidden units, whereas the per-
ceptron uses step-function nonlinearities. this means that the neural network func-
tion is differentiable with respect to the network parameters, and this property will
play a central role in network training.

if the id180 of all the hidden units in a network are taken to be
linear, then for any such network we can always    nd an equivalent network without
hidden units. this follows from the fact that the composition of successive linear
transformations is itself a linear transformation. however, if the number of hidden
units is smaller than either the number of input or output units, then the transforma-
tions that the network can generate are not the most general possible linear trans-
formations from inputs to outputs because information is lost in the dimensionality
reduction at the hidden units. in section 12.4.2, we show that networks of linear
units give rise to principal component analysis. in general, however, there is little
interest in multilayer networks of linear units.

the network architecture shown in figure 5.1 is the most commonly used one
in practice. however, it is easily generalized, for instance by considering additional
layers of processing each consisting of a weighted linear combination of the form
(5.4) followed by an element-wise transformation using a nonlinear activation func-
tion. note that there is some confusion in the literature regarding the terminology
for counting the number of layers in such networks. thus the network in figure 5.1
may be described as a 3-layer network (which counts the number of layers of units,
and treats the inputs as units) or sometimes as a single-hidden-layer network (which
counts the number of layers of hidden units). we recommend a terminology in which
figure 5.1 is called a two-layer network, because it is the number of layers of adap-
tive weights that is important for determining the network properties.

another generalization of the network architecture is to include skip-layer con-
nections, each of which is associated with a corresponding adaptive parameter. for

230

5. neural networks

figure 5.2 example of a neural network having a
general feed-forward topology. note that
each hidden and output unit has an
associated bias parameter (omitted for
clarity).

x2

inputs

x1

z1

z2

z3

y2

outputs

y1

instance, in a two-layer network these would go directly from inputs to outputs. in
principle, a network with sigmoidal hidden units can always mimic skip layer con-
nections (for bounded input values) by using a suf   ciently small    rst-layer weight
that, over its operating range, the hidden unit is effectively linear, and then com-
pensating with a large weight value from the hidden unit to the output. in practice,
however, it may be advantageous to include skip-layer connections explicitly.

furthermore, the network can be sparse, with not all possible connections within
a layer being present. we shall see an example of a sparse network architecture when
we consider convolutional neural networks in section 5.5.6.

because there is a direct correspondence between a network diagram and its
mathematical function, we can develop more general network mappings by con-
sidering more complex network diagrams. however, these must be restricted to a
feed-forward architecture, in other words to one having no closed directed cycles, to
ensure that the outputs are deterministic functions of the inputs. this is illustrated
with a simple example in figure 5.2. each (hidden or output) unit in such a network
computes a function given by

(cid:22)(cid:2)

(cid:23)

zk = h

wkjzj

j

(5.10)

where the sum runs over all units that send connections to unit k (and a bias param-
eter is included in the summation). for a given set of values applied to the inputs of
the network, successive application of (5.10) allows the activations of all units in the
network to be evaluated including those of the output units.

the approximation properties of feed-forward networks have been widely stud-
ied (funahashi, 1989; cybenko, 1989; hornik et al., 1989; stinchecombe and white,
1989; cotter, 1990; ito, 1991; hornik, 1991; kreinovich, 1991; ripley, 1996) and
found to be very general. neural networks are therefore said to be universal ap-
proximators. for example, a two-layer network with linear outputs can uniformly
approximate any continuous function on a compact input domain to arbitrary accu-
racy provided the network has a suf   ciently large number of hidden units. this result
holds for a wide range of hidden unit id180, but excluding polynomi-
als. although such theorems are reassuring, the key problem is how to    nd suitable
parameter values given a set of training data, and in later sections of this chapter we

figure 5.3 illustration of
the ca-
pability of a multilayer id88
to approximate four different func-
tions comprising (a) f (x) = x2, (b)
f (x) = sin(x), (c), f (x) = |x|,
and (d) f (x) = h(x) where h(x)
is the heaviside step function.
in
each case, n = 50 data points,
shown as blue dots, have been sam-
pled uniformly in x over the interval
(   1, 1) and the corresponding val-
ues of f (x) evaluated. these data
points are then used to train a two-
layer network having 3 hidden units
with    tanh    id180 and
linear output units. the resulting
network functions are shown by the
red curves, and the outputs of the
three hidden units are shown by the
three dashed curves.

5.1. feed-forward network functions

231

(a)

(c)

(b)

(d)

will show that there exist effective solutions to this problem based on both maximum
likelihood and bayesian approaches.

the capability of a two-layer network to model a broad range of functions is
illustrated in figure 5.3. this    gure also shows how individual hidden units work
collaboratively to approximate the    nal function. the role of hidden units in a simple
classi   cation problem is illustrated in figure 5.4 using the synthetic classi   cation
data set described in appendix a.

5.1.1 weight-space symmetries
one property of feed-forward networks, which will play a role when we consider
bayesian model comparison, is that multiple distinct choices for the weight vector
w can all give rise to the same mapping function from inputs to outputs (chen et al.,
1993). consider a two-layer network of the form shown in figure 5.1 with m hidden
units having    tanh    id180 and full connectivity in both layers. if we
change the sign of all of the weights and the bias feeding into a particular hidden
unit, then, for a given input pattern, the sign of the activation of the hidden unit will
be reversed, because    tanh    is an odd function, so that tanh(   a) =     tanh(a). this
transformation can be exactly compensated by changing the sign of all of the weights
leading out of that hidden unit. thus, by changing the signs of a particular group of
weights (and a bias), the input   output mapping function represented by the network
is unchanged, and so we have found two different weight vectors that give rise to
the same mapping function. for m hidden units, there will be m such    sign-   ip   

232

5. neural networks

figure 5.4 example of the solution of a simple two-
class classi   cation problem involving
synthetic data using a neural network
having two inputs, two hidden units with
   tanh    id180, and a single
output having a logistic sigmoid activa-
tion function. the dashed blue lines
show the z = 0.5 contours for each of
the hidden units, and the red line shows
the y = 0.5 decision surface for the net-
work. for comparison, the green line
denotes the optimal decision boundary
computed from the distributions used to
generate the data.

3

2

1

0

   1

   2

   2

   1

0

1

2

symmetries, and thus any given weight vector will be one of a set 2m equivalent
weight vectors .

similarly, imagine that we interchange the values of all of the weights (and the
bias) leading both into and out of a particular hidden unit with the corresponding
values of the weights (and bias) associated with a different hidden unit. again, this
clearly leaves the network input   output mapping function unchanged, but it corre-
sponds to a different choice of weight vector. for m hidden units, any given weight
vector will belong to a set of m! equivalent weight vectors associated with this inter-
change symmetry, corresponding to the m! different orderings of the hidden units.
the network will therefore have an overall weight-space symmetry factor of m!2m .
for networks with more than two layers of weights, the total level of symmetry will
be given by the product of such factors, one for each layer of hidden units.

it turns out that these factors account for all of the symmetries in weight space
(except for possible accidental symmetries due to speci   c choices for the weight val-
ues). furthermore, the existence of these symmetries is not a particular property of
the    tanh    function but applies to a wide range of id180 (k  urkov  a and
kainen, 1994). in many cases, these symmetries in weight space are of little practi-
cal consequence, although in section 5.7 we shall encounter a situation in which we
need to take them into account.

5.2. network training

so far, we have viewed neural networks as a general class of parametric nonlinear
functions from a vector x of input variables to a vector y of output variables. a
simple approach to the problem of determining the network parameters is to make an
analogy with the discussion of polynomial curve    tting in section 1.1, and therefore
to minimize a sum-of-squares error function. given a training set comprising a set
of input vectors {xn}, where n = 1, . . . , n, together with a corresponding set of

5.2. network training

233

n(cid:2)

n=1

target vectors {tn}, we minimize the error function

e(w) =

1
2

(cid:5)y(xn, w)     tn(cid:5)2.

(5.11)

however, we can provide a much more general view of network training by    rst
giving a probabilistic interpretation to the network outputs. we have already seen
many advantages of using probabilistic predictions in section 1.5.4. here it will also
provide us with a clearer motivation both for the choice of output unit nonlinearity
and the choice of error function.

we start by discussing regression problems, and for the moment we consider
a single target variable t that can take any real value. following the discussions
in section 1.2.5 and 3.1, we assume that t has a gaussian distribution with an x-
dependent mean, which is given by the output of the neural network, so that

p(t|x, w) = n(cid:10)

t|y(x, w),   

   1

(cid:11)

(5.12)

where    is the precision (inverse variance) of the gaussian noise. of course this
is a somewhat restrictive assumption, and in section 5.6 we shall see how to extend
this approach to allow for more general conditional distributions. for the conditional
distribution given by (5.12), it is suf   cient to take the output unit activation function
to be the identity, because such a network can approximate any continuous function
from x to y. given a data set of n independent, identically distributed observations
x = {x1, . . . , xn}, along with corresponding target values t = {t1, . . . , tn}, we
can construct the corresponding likelihood function

n(cid:14)

n=1

p(tn|xn, w,   ).

p(t|x, w,   ) =
n(cid:2)

taking the negative logarithm, we obtain the error function

  
2

n=1

{y(xn, w)     tn}2     n
2

ln    + n
2

ln(2  )

(5.13)

which can be used to learn the parameters w and   . in section 5.7, we shall dis-
cuss the bayesian treatment of neural networks, while here we consider a maximum
likelihood approach. note that in the neural networks literature, it is usual to con-
sider the minimization of an error function rather than the maximization of the (log)
likelihood, and so here we shall follow this convention. consider    rst the determi-
nation of w. maximizing the likelihood function is equivalent to minimizing the
sum-of-squares error function given by

e(w) =

1
2

{y(xn, w)     tn}2

(5.14)

n(cid:2)

n=1

234

5. neural networks

where we have discarded additive and multiplicative constants. the value of w found
by minimizing e(w) will be denoted wml because it corresponds to the maximum
likelihood solution. in practice, the nonlinearity of the network function y(xn, w)
causes the error e(w) to be nonconvex, and so in practice local maxima of the
likelihood may be found, corresponding to local minima of the error function, as
discussed in section 5.2.1.

having found wml, the value of    can be found by minimizing the negative log

likelihood to give

1
  ml

=

1
n

n(cid:2)

n=1

{y(xn, wml)     tn}2.

(5.15)

note that this can be evaluated once the iterative optimization required to    nd wml
is completed. if we have multiple target variables, and we assume that they are inde-
pendent conditional on x and w with shared noise precision   , then the conditional
distribution of the target values is given by

(cid:11)

t|y(x, w),   

   1i

.

(5.16)

following the same argument as for a single target variable, we see that the maximum
likelihood weights are determined by minimizing the sum-of-squares error function
(5.11). the noise precision is then given by

p(t|x, w) = n(cid:10)
n(cid:2)

1
  ml

=

1
n k

n=1

(cid:5)y(xn, wml)     tn(cid:5)2

(5.17)

exercise 5.2

exercise 5.3

where k is the number of target variables. the assumption of independence can be
dropped at the expense of a slightly more complex optimization problem.

recall from section 4.3.6 that there is a natural pairing of the error function
(given by the negative log likelihood) and the output unit activation function. in the
regression case, we can view the network as having an output activation function that
is the identity, so that yk = ak. the corresponding sum-of-squares error function
has the property

= yk     tk

   e
   ak

(5.18)

which we shall make use of when discussing error id26 in section 5.3.
now consider the case of binary classi   cation in which we have a single target
variable t such that t = 1 denotes class c1 and t = 0 denotes class c2. following
the discussion of canonical link functions in section 4.3.6, we consider a network
having a single output whose activation function is a logistic sigmoid

y =   (a)    

1

(5.19)
so that 0 (cid:1) y(x, w) (cid:1) 1. we can interpret y(x, w) as the id155
p(c1|x), with p(c2|x) given by 1     y(x, w). the conditional distribution of targets
given inputs is then a bernoulli distribution of the form

1 + exp(   a)

p(t|x, w) = y(x, w)t {1     y(x, w)}1   t

.

(5.20)

5.2. network training

235

if we consider a training set of independent observations, then the error function,
which is given by the negative log likelihood, is then a cross-id178 error function
of the form

{tn ln yn + (1     tn) ln(1     yn)}

(5.21)

e(w) =     n(cid:2)

exercise 5.4

n=1

where yn denotes y(xn, w). note that there is no analogue of the noise precision   
because the target values are assumed to be correctly labelled. however, the model
is easily extended to allow for labelling errors. simard et al. (2003) found that using
the cross-id178 error function instead of the sum-of-squares for a classi   cation
problem leads to faster training as well as improved generalization.

if we have k separate binary classi   cations to perform, then we can use a net-
work having k outputs each of which has a logistic sigmoid activation function.
associated with each output is a binary class label tk     {0, 1}, where k = 1, . . . , k.
if we assume that the class labels are independent, given the input vector, then the
conditional distribution of the targets is

p(t|x, w) =

yk(x, w)tk [1     yk(x, w)]1   tk .

(5.22)

k(cid:14)

k=1

exercise 5.5

exercise 5.6

e(w) =     n(cid:2)

k(cid:2)

taking the negative logarithm of the corresponding likelihood function then gives
the following error function

{tnk ln ynk + (1     tnk) ln(1     ynk)}

(5.23)

n=1

k=1

where ynk denotes yk(xn, w). again, the derivative of the error function with re-
spect to the activation for a particular output unit takes the form (5.18) just as in the
regression case.

it is interesting to contrast the neural network solution to this problem with the
corresponding approach based on a linear classi   cation model of the kind discussed
in chapter 4. suppose that we are using a standard two-layer network of the kind
shown in figure 5.1. we see that the weight parameters in the    rst layer of the
network are shared between the various outputs, whereas in the linear model each
classi   cation problem is solved independently. the    rst layer of the network can
be viewed as performing a nonlinear feature extraction, and the sharing of features
between the different outputs can save on computation and can also lead to improved
generalization.

finally, we consider the standard multiclass classi   cation problem in which each
input is assigned to one of k mutually exclusive classes. the binary target variables
tk     {0, 1} have a 1-of-k coding scheme indicating the class, and the network
outputs are interpreted as yk(x, w) = p(tk = 1|x), leading to the following error
function

tkn ln yk(xn, w).

(5.24)

e(w) =     n(cid:2)

k(cid:2)

n=1

k=1

236

5. neural networks

figure 5.5 geometrical view of the error function e(w) as
a surface sitting over weight space. point wa is
a local minimum and wb is the global minimum.
at any point wc, the local gradient of the error
surface is given by the vector    e.

e(w)

wa

wb

w2

w1

wc

   e

following the discussion of section 4.3.4, we see that the output unit activation
function, which corresponds to the canonical link, is given by the softmax function

yk(x, w) =

exp(ak(x, w))

exp(aj(x, w))

(5.25)

(cid:2)

j

(cid:5)

which satis   es 0 (cid:1) yk (cid:1) 1 and
k yk = 1. note that the yk(x, w) are unchanged
if a constant is added to all of the ak(x, w), causing the error function to be constant
for some directions in weight space. this degeneracy is removed if an appropriate
id173 term (section 5.5) is added to the error function.

exercise 5.7

a particular output unit takes the familiar form (5.18).

once again, the derivative of the error function with respect to the activation for

in summary, there is a natural choice of both output unit activation function
and matching error function, according to the type of problem being solved. for re-
gression we use linear outputs and a sum-of-squares error, for (multiple independent)
binary classi   cations we use logistic sigmoid outputs and a cross-id178 error func-
tion, and for multiclass classi   cation we use softmax outputs with the corresponding
multiclass cross-id178 error function. for classi   cation problems involving two
classes, we can use a single logistic sigmoid output, or alternatively we can use a
network with two outputs having a softmax output activation function.

5.2.1 parameter optimization
we turn next to the task of    nding a weight vector w which minimizes the
chosen function e(w). at this point, it is useful to have a geometrical picture of the
error function, which we can view as a surface sitting over weight space as shown in
figure 5.5. first note that if we make a small step in weight space from w to w+   w
then the change in the error function is   e (cid:7)   wt   e(w), where the vector    e(w)
points in the direction of greatest rate of increase of the error function. because the
error e(w) is a smooth continuous function of w, its smallest value will occur at a

section 5.1.1

point in weight space such that the gradient of the error function vanishes, so that

5.2. network training

237

   e(w) = 0

(5.26)
as otherwise we could make a small step in the direction of       e(w) and thereby
further reduce the error. points at which the gradient vanishes are called stationary
points, and may be further classi   ed into minima, maxima, and saddle points.

our goal is to    nd a vector w such that e(w) takes its smallest value. how-
ever, the error function typically has a highly nonlinear dependence on the weights
and bias parameters, and so there will be many points in weight space at which the
gradient vanishes (or is numerically very small). indeed, from the discussion in sec-
tion 5.1.1 we see that for any point w that is a local minimum, there will be other
points in weight space that are equivalent minima. for instance, in a two-layer net-
work of the kind shown in figure 5.1, with m hidden units, each point in weight
space is a member of a family of m!2m equivalent points.

furthermore, there will typically be multiple inequivalent stationary points and
in particular multiple inequivalent minima. a minimum that corresponds to the
smallest value of the error function for any weight vector is said to be a global
minimum. any other minima corresponding to higher values of the error function
are said to be local minima. for a successful application of neural networks, it may
not be necessary to    nd the global minimum (and in general it will not be known
whether the global minimum has been found) but it may be necessary to compare
several local minima in order to    nd a suf   ciently good solution.
because there is clearly no hope of    nding an analytical solution to the equa-
tion    e(w) = 0 we resort to iterative numerical procedures. the optimization of
continuous nonlinear functions is a widely studied problem and there exists an ex-
tensive literature on how to solve it ef   ciently. most techniques involve choosing
some initial value w(0) for the weight vector and then moving through weight space
in a succession of steps of the form

w(   +1) = w(   ) +    w(   )

(5.27)

where    labels the iteration step. different algorithms involve different choices for
the weight vector update    w(   ). many algorithms make use of gradient information
and therefore require that, after each update, the value of    e(w) is evaluated at
the new weight vector w(   +1). in order to understand the importance of gradient
information, it is useful to consider a local approximation to the error function based
on a taylor expansion.

5.2.2 local quadratic approximation
insight into the optimization problem, and into the various techniques for solv-
ing it, can be obtained by considering a local quadratic approximation to the error
function.

consider the taylor expansion of e(w) around some point(cid:1)w in weight space

e(w) (cid:7) e((cid:1)w) + (w    (cid:1)w)tb +

(w    (cid:1)w)th(w    (cid:1)w)

(5.28)

1
2

238

5. neural networks

(h)ij        e
   wi   wj

(cid:7)(cid:7)(cid:7)(cid:7)
   e (cid:7) b + h(w    (cid:1)w).

w=bw

.

where cubic and higher terms have been omitted. here b is de   ned to be the gradient

of e evaluated at(cid:1)w

b        e|w=bw
and the hessian matrix h =       e has elements

(5.29)

(5.30)

from (5.28), the corresponding local approximation to the gradient is given by

for points w that are suf   ciently close to (cid:1)w, these expressions will give reasonable

(5.31)

approximations for the error and its gradient.

consider the particular case of a local quadratic approximation around a point
w(cid:1) that is a minimum of the error function. in this case there is no linear term,
because    e = 0 at w(cid:1), and (5.28) becomes

e(w) = e(w(cid:1)) +

(w     w(cid:1))th(w     w(cid:1))

1
2

(5.32)

where the hessian h is evaluated at w(cid:1). in order to interpret this geometrically,
consider the eigenvalue equation for the hessian matrix

hui =   iui

(5.33)

where the eigenvectors ui form a complete orthonormal set (appendix c) so that

(5.34)
we now expand (w     w(cid:1)) as a linear combination of the eigenvectors in the form

ut
i uj =   ij.

w     w(cid:1) =

  iui.

(5.35)

(cid:2)

i

this can be regarded as a transformation of the coordinate system in which the origin
is translated to the point w(cid:1), and the axes are rotated to align with the eigenvectors
(through the orthogonal matrix whose columns are the ui), and is discussed in more
detail in appendix c. substituting (5.35) into (5.32), and using (5.33) and (5.34),
allows the error function to be written in the form

e(w) = e(w(cid:1)) +

1
2

  i  2
i .

(cid:2)

i

a matrix h is said to be positive de   nite if, and only if,

vthv > 0

for all v.

(5.36)

(5.37)

w2

the error

figure 5.6 in the neighbourhood of a min-
imum w(cid:1),
function
can be approximated by a
quadratic. contours of con-
stant error are then ellipses
whose axes are aligned with
the eigenvectors ui of the hes-
sian matrix, with lengths that
are inversely proportional to the
square roots of the correspond-
ing eigenvectors   i.

   1/2
2

  

5.2. network training

239

u2

w(cid:1)

u1

w1

   1/2
1

  

because the eigenvectors {ui} form a complete set, an arbitrary vector v can be
written in the form

from (5.33) and (5.34), we then have

(5.38)

(5.39)

exercise 5.10

exercise 5.11

and so h will be positive de   nite if, and only if, all of its eigenvalues are positive.
in the new coordinate system, whose basis vectors are given by the eigenvectors
{ui}, the contours of constant e are ellipses centred on the origin, as illustrated
in figure 5.6. for a one-dimensional weight space, a stationary point w(cid:1) will be a
minimum if

> 0.

w(cid:1)

(5.40)

exercise 5.12

the corresponding result in d-dimensions is that the hessian matrix, evaluated at
w(cid:1), should be positive de   nite.

5.2.3 use of gradient information
as we shall see in section 5.3, it is possible to evaluate the gradient of an error
function ef   ciently by means of the id26 procedure. the use of this
gradient information can lead to signi   cant improvements in the speed with which
the minima of the error function can be located. we can see why this is so, as follows.
in the quadratic approximation to the error function, given in (5.28), the error
surface is speci   ed by the quantities b and h, which contain a total of w (w +
3)/2 independent elements (because the matrix h is symmetric), where w is the
dimensionality of w (i.e., the total number of adaptive parameters in the network).
the location of the minimum of this quadratic approximation therefore depends on
o(w 2) parameters, and we should not expect to be able to locate the minimum until
we have gathered o(w 2) independent pieces of information. if we do not make
use of gradient information, we would expect to have to perform o(w 2) function

exercise 5.13

v =

ciui.

(cid:2)
(cid:2)

i

i

vthv =

c2
i   i

(cid:7)(cid:7)(cid:7)(cid:7)

   2e
   w2

240

5. neural networks

evaluations, each of which would require o(w ) steps. thus, the computational
effort needed to    nd the minimum using such an approach would be o(w 3).
now compare this with an algorithm that makes use of the gradient information.
because each evaluation of    e brings w items of information, we might hope to
   nd the minimum of the function in o(w ) gradient evaluations. as we shall see,
by using error id26, each such evaluation takes only o(w ) steps and so
the minimum can now be found in o(w 2) steps. for this reason, the use of gradient
information forms the basis of practical algorithms for training neural networks.

w(   +1) = w(   )          e(w(   ))

5.2.4 id119 optimization
the simplest approach to using gradient information is to choose the weight
update in (5.27) to comprise a small step in the direction of the negative gradient, so
that

(5.41)
where the parameter    > 0 is known as the learning rate. after each such update, the
gradient is re-evaluated for the new weight vector and the process repeated. note that
the error function is de   ned with respect to a training set, and so each step requires
that the entire training set be processed in order to evaluate    e. techniques that
use the whole data set at once are called batch methods. at each step the weight
vector is moved in the direction of the greatest rate of decrease of the error function,
and so this approach is known as id119 or steepest descent. although
such an approach might intuitively seem reasonable, in fact it turns out to be a poor
algorithm, for reasons discussed in bishop and nabney (2008).

for batch optimization, there are more ef   cient methods, such as conjugate gra-
dients and quasi-id77s, which are much more robust and much faster
than simple id119 (gill et al., 1981; fletcher, 1987; nocedal and wright,
1999). unlike id119, these algorithms have the property that the error
function always decreases at each iteration unless the weight vector has arrived at a
local or global minimum.

in order to    nd a suf   ciently good minimum, it may be necessary to run a
gradient-based algorithm multiple times, each time using a different randomly cho-
sen starting point, and comparing the resulting performance on an independent vali-
dation set.

there is, however, an on-line version of id119 that has proved useful
in practice for training neural networks on large data sets (le cun et al., 1989).
error functions based on maximum likelihood for a set of independent observations
comprise a sum of terms, one for each data point

e(w) =

en(w).

(5.42)

n(cid:2)

n=1

on-line id119, also known as sequential id119 or stochastic
id119, makes an update to the weight vector based on one data point at a
time, so that

w(   +1) = w(   )          en(w(   )).

(5.43)

5.3. error id26

241

this update is repeated by cycling through the data either in sequence or by selecting
points at random with replacement. there are of course intermediate scenarios in
which the updates are based on batches of data points.

one advantage of on-line methods compared to batch methods is that the former
handle redundancy in the data much more ef   ciently. to see, this consider an ex-
treme example in which we take a data set and double its size by duplicating every
data point. note that this simply multiplies the error function by a factor of 2 and so
is equivalent to using the original error function. batch methods will require double
the computational effort to evaluate the batch error function gradient, whereas on-
line methods will be unaffected. another property of on-line id119 is the
possibility of escaping from local minima, since a stationary point with respect to
the error function for the whole data set will generally not be a stationary point for
each data point individually.

nonlinear optimization algorithms, and their practical application to neural net-

work training, are discussed in detail in bishop and nabney (2008).

5.3. error id26

our goal in this section is to    nd an ef   cient technique for evaluating the gradient
of an error function e(w) for a feed-forward neural network. we shall see that
this can be achieved using a local message passing scheme in which information is
sent alternately forwards and backwards through the network and is known as error
id26, or sometimes simply as backprop.

it should be noted that the term id26 is used in the neural com-
puting literature to mean a variety of different things. for instance, the multilayer
id88 architecture is sometimes called a id26 network. the term
id26 is also used to describe the training of a multilayer id88 us-
ing id119 applied to a sum-of-squares error function. in order to clarify
the terminology, it is useful to consider the nature of the training process more care-
fully. most training algorithms involve an iterative procedure for minimization of an
error function, with adjustments to the weights being made in a sequence of steps. at
each such step, we can distinguish between two distinct stages. in the    rst stage, the
derivatives of the error function with respect to the weights must be evaluated. as
we shall see, the important contribution of the id26 technique is in pro-
viding a computationally ef   cient method for evaluating such derivatives. because
it is at this stage that errors are propagated backwards through the network, we shall
use the term id26 speci   cally to describe the evaluation of derivatives.
in the second stage, the derivatives are then used to compute the adjustments to be
made to the weights. the simplest such technique, and the one originally considered
by rumelhart et al. (1986), involves id119. it is important to recognize
that the two stages are distinct. thus, the    rst stage, namely the propagation of er-
rors backwards through the network in order to evaluate derivatives, can be applied
to many other kinds of network and not just the multilayer id88. it can also be
applied to error functions other that just the simple sum-of-squares, and to the eval-

242

5. neural networks

uation of other derivatives such as the jacobian and hessian matrices, as we shall
see later in this chapter. similarly, the second stage of weight adjustment using the
calculated derivatives can be tackled using a variety of optimization schemes, many
of which are substantially more powerful than simple id119.

5.3.1 evaluation of error-function derivatives
we now derive the id26 algorithm for a general network having ar-
bitrary feed-forward topology, arbitrary differentiable nonlinear id180,
and a broad class of error function. the resulting formulae will then be illustrated
using a simple layered network structure having a single layer of sigmoidal hidden
units together with a sum-of-squares error.

many error functions of practical interest, for instance those de   ned by maxi-
mum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data
point in the training set, so that

n(cid:2)

(cid:2)
(cid:2)

i

e(w) =

(5.44)
here we shall consider the problem of evaluating    en(w) for one such term in the
error function. this may be used directly for sequential optimization, or the results
can be accumulated over the training set in the case of batch methods.

n=1

en(w).

consider    rst a simple linear model in which the outputs yk are linear combina-

tions of the input variables xi so that

yk =

wkixi

(5.45)

together with an error function that, for a particular input pattern n, takes the form

where ynk = yk(xn, w). the gradient of this error function with respect to a weight
wji is given by

en =

1
2

k

(ynk     tnk)2

= (ynj     tnj)xni

   en
   wji

(5.46)

(5.47)

which can be interpreted as a    local    computation involving the product of an    error
signal    ynj     tnj associated with the output end of the link wji and the variable xni
associated with the input end of the link. in section 4.3.2, we saw how a similar
formula arises with the logistic sigmoid activation function together with the cross
id178 error function, and similarly for the softmax activation function together
with its matching cross-id178 error function. we shall now see how this simple
result extends to the more complex setting of multilayer feed-forward networks.

in a general feed-forward network, each unit computes a weighted sum of its

(cid:2)

inputs of the form

aj =

wjizi

i

(5.48)

5.3. error id26

243

where zi is the activation of a unit, or input, that sends a connection to unit j, and wji
is the weight associated with that connection. in section 5.1, we saw that biases can
be included in this sum by introducing an extra unit, or input, with activation    xed
at +1. we therefore do not need to deal with biases explicitly. the sum in (5.48) is
transformed by a nonlinear activation function h(  ) to give the activation zj of unit j
in the form

zj = h(aj).

(5.49)

note that one or more of the variables zi in the sum in (5.48) could be an input, and
similarly, the unit j in (5.49) could be an output.

for each pattern in the training set, we shall suppose that we have supplied the
corresponding input vector to the network and calculated the activations of all of
the hidden and output units in the network by successive application of (5.48) and
(5.49). this process is often called forward propagation because it can be regarded
as a forward    ow of information through the network.

now consider the evaluation of the derivative of en with respect to a weight
wji. the outputs of the various units will depend on the particular input pattern n.
however, in order to keep the notation uncluttered, we shall omit the subscript n
from the network variables. first we note that en depends on the weight wji only
via the summed input aj to unit j. we can therefore apply the chain rule for partial
derivatives to give

we now introduce a useful notation

  j        en
   aj

   en
   wji

=    en
   aj

   aj
   wji

.

(5.50)

(5.51)

where the      s are often referred to as errors for reasons we shall see shortly. using
(5.48), we can write

   aj
   wji

= zi.

substituting (5.51) and (5.52) into (5.50), we then obtain

   en
   wji

=   jzi.

(5.52)

(5.53)

equation (5.53) tells us that the required derivative is obtained simply by multiplying
the value of    for the unit at the output end of the weight by the value of z for the unit
at the input end of the weight (where z = 1 in the case of a bias). note that this takes
the same form as for the simple linear model considered at the start of this section.
thus, in order to evaluate the derivatives, we need only to calculate the value of   j
for each hidden and output unit in the network, and then apply (5.53).

as we have seen already, for the output units, we have

  k = yk     tk

(5.54)

244

5. neural networks

figure 5.7 illustration of the calculation of   j for hidden unit j by
id26 of the      s from those units k to which
unit j sends connections. the blue arrow denotes the
direction of information    ow during forward propagation,
and the red arrows indicate the backward propagation
of error information.

zi

wji

  j

zj

wkj

  k

  1

provided we are using the canonical link as the output-unit activation function. to
evaluate the      s for hidden units, we again make use of the chain rule for partial
derivatives,

  j        en
   aj

=

   en
   ak

   ak
   aj

(5.55)

where the sum runs over all units k to which unit j sends connections. the arrange-
ment of units and weights is illustrated in figure 5.7. note that the units labelled k
could include other hidden units and/or output units. in writing down (5.55), we are
making use of the fact that variations in aj give rise to variations in the error func-
tion only through variations in the variables ak. if we now substitute the de   nition
of    given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the
following id26 formula

(cid:2)

k

(cid:2)

  j = h

(cid:4)(aj)

wkj  k

k

(5.56)

which tells us that the value of    for a particular hidden unit can be obtained by
propagating the      s backwards from units higher up in the network, as illustrated
in figure 5.7. note that the summation in (5.56) is taken over the    rst index on
wkj (corresponding to backward propagation of information through the network),
whereas in the forward propagation equation (5.10) it is taken over the second index.
because we already know the values of the      s for the output units, it follows that
by recursively applying (5.56) we can evaluate the      s for all of the hidden units in a
feed-forward network, regardless of its topology.

the id26 procedure can therefore be summarized as follows.

error id26

1. apply an input vector xn to the network and forward propagate through
the network using (5.48) and (5.49) to    nd the activations of all the hidden
and output units.

2. evaluate the   k for all the output units using (5.54).
3. backpropagate the      s using (5.56) to obtain   j for each hidden unit in the

network.

4. use (5.53) to evaluate the required derivatives.

5.3. error id26

245

for batch methods, the derivative of the total error e can then be obtained by
repeating the above steps for each pattern in the training set and then summing over
all patterns:

   e
   wji

=

   en
   wji

.

(5.57)

(cid:2)

n

in the above derivation we have implicitly assumed that each hidden or output unit in
the network has the same activation function h(  ). the derivation is easily general-
ized, however, to allow different units to have individual id180, simply
by keeping track of which form of h(  ) goes with which unit.

5.3.2 a simple example
the above derivation of the id26 procedure allowed for general
forms for the error function, the id180, and the network topology. in
order to illustrate the application of this algorithm, we shall consider a particular
example. this is chosen both for its simplicity and for its practical importance, be-
cause many applications of neural networks reported in the literature make use of
this type of network. speci   cally, we shall consider a two-layer network of the form
illustrated in figure 5.1, together with a sum-of-squares error, in which the output
units have linear id180, so that yk = ak, while the hidden units have
logistic sigmoid id180 given by

where

h(a)     tanh(a)
tanh(a) = ea     e
   a
ea + e   a .

a useful feature of this function is that its derivative can be expressed in a par-

ticularly simple form:

(5.60)
we also consider a standard sum-of-squares error function, so that for pattern n the
error is given by

h

(cid:4)(a) = 1     h(a)2.

(yk     tk)2

(5.61)

k(cid:2)

k=1

en =

1
2

where yk is the activation of output unit k, and tk is the corresponding target, for a
particular input pattern xn.

for each pattern in the training set in turn, we    rst perform a forward propagation

using

aj =

(5.58)

(5.59)

(5.62)

(5.63)

(5.64)

d(cid:2)
m(cid:2)

i=0

j=0

(1)
ji xi

w

zj = tanh(aj)

yk =

w

(2)
kj zj.

246

5. neural networks

next we compute the      s for each output unit using
  k = yk     tk.

then we backpropagate these to obtain   s for the hidden units using

k(cid:2)

  j = (1     z2
j )

wkj  k.

k=1

(5.65)

(5.66)

finally, the derivatives with respect to the    rst-layer and second-layer weights are
given by

   en
(1)
ji

   w

=   jxi,

   en
(2)
kj

   w

=   kzj.

(5.67)

5.3.3 ef   ciency of id26
one of the most important aspects of id26 is its computational ef   -
ciency. to understand this, let us examine how the number of computer operations
required to evaluate the derivatives of the error function scales with the total number
w of weights and biases in the network. a single evaluation of the error function
(for a given input pattern) would require o(w ) operations, for suf   ciently large w .
this follows from the fact that, except for a network with very sparse connections,
the number of weights is typically much greater than the number of units, and so the
bulk of the computational effort in forward propagation is concerned with evaluat-
ing the sums in (5.48), with the evaluation of the id180 representing a
small overhead. each term in the sum in (5.48) requires one multiplication and one
addition, leading to an overall computational cost that is o(w ).

an alternative approach to id26 for computing the derivatives of the
error function is to use    nite differences. this can be done by perturbing each weight
in turn, and approximating the derivatives by the expression

= en(wji +  )     en(wji)

   en
   wji

(5.68)
where   (cid:13) 1. in a software simulation, the accuracy of the approximation to the
derivatives can be improved by making   smaller, until numerical roundoff problems
arise. the accuracy of the    nite differences method can be improved signi   cantly
by using symmetrical central differences of the form

 

+ o( )

= en(wji +  )     en(wji      )

2 

   en
   wji

+ o( 2).

(5.69)

exercise 5.14

in this case, the o( ) corrections cancel, as can be veri   ed by taylor expansion on
the right-hand side of (5.69), and so the residual corrections are o( 2). the number
of computational steps is, however, roughly doubled compared with (5.68).

the main problem with numerical differentiation is that the highly desirable
o(w ) scaling has been lost. each forward propagation requires o(w ) steps, and

5.3. error id26

247

figure 5.8 illustration of a modular pattern
recognition system in which the
jacobian matrix can be used
to backpropagate error signals
from the outputs through to ear-
lier modules in the system.

u

x

v

z

w

y

there are w weights in the network each of which must be perturbed individually, so
that the overall scaling is o(w 2).

however, numerical differentiation plays an important role in practice, because a
comparison of the derivatives calculated by id26 with those obtained us-
ing central differences provides a powerful check on the correctness of any software
implementation of the id26 algorithm. when training networks in prac-
tice, derivatives should be evaluated using id26, because this gives the
greatest accuracy and numerical ef   ciency. however, the results should be compared
with numerical differentiation using (5.69) for some test cases in order to check the
correctness of the implementation.

5.3.4 the jacobian matrix
we have seen how the derivatives of an error function with respect to the weights
can be obtained by the propagation of errors backwards through the network. the
technique of id26 can also be applied to the calculation of other deriva-
tives. here we consider the evaluation of the jacobian matrix, whose elements are
given by the derivatives of the network outputs with respect to the inputs

jki        yk
   xi

(5.70)

where each such derivative is evaluated with all other inputs held    xed. jacobian
matrices play a useful role in systems built from a number of distinct modules, as
illustrated in figure 5.8. each module can comprise a    xed or adaptive function,
which can be linear or nonlinear, so long as it is differentiable. suppose we wish
to minimize an error function e with respect to the parameter w in figure 5.8. the
derivative of the error function is given by

   e
   w

=

   e
   yk

   yk
   zj

   zj
   w

(5.71)

(cid:2)

k,j

in which the jacobian matrix for the red module in figure 5.8 appears in the middle
term.

because the jacobian matrix provides a measure of the local sensitivity of the
outputs to changes in each of the input variables, it also allows any known errors    xi

248

5. neural networks

(cid:2)

associated with the inputs to be propagated through the trained network in order to
estimate their contribution    yk to the errors at the outputs, through the relation

   yk (cid:7)

   yk
   xi

   xi

(5.72)
which is valid provided the |   xi| are small. in general, the network mapping rep-
resented by a trained neural network will be nonlinear, and so the elements of the
jacobian matrix will not be constants but will depend on the particular input vector
used. thus (5.72) is valid only for small perturbations of the inputs, and the jacobian
itself must be re-evaluated for each new input vector.

i

the jacobian matrix can be evaluated using a id26 procedure that is
similar to the one derived earlier for evaluating the derivatives of an error function
with respect to the weights. we start by writing the element jki in the form

(cid:2)
(cid:2)

j

j

jki =    yk
   xi

=

=

   yk
   aj

   aj
   xi

wji

   yk
   aj

(5.73)

where we have made use of (5.48). the sum in (5.73) runs over all units j to which
the input unit i sends connections (for example, over all units in the    rst hidden
layer in the layered topology considered earlier). we now write down a recursive
id26 formula to determine the derivatives    yk/   aj

(cid:2)

   yk
   aj

=

   al
   aj

(cid:2)

   yk
   al
(cid:4)(aj)

l

l

= h

wlj

   yk
   al

(5.74)

where the sum runs over all units l to which unit j sends connections (corresponding
to the    rst index of wlj). again, we have made use of (5.48) and (5.49). this
id26 starts at the output units for which the required derivatives can be
found directly from the functional form of the output-unit activation function. for
instance, if we have individual sigmoidal id180 at each output unit,
then

   yk
   aj
whereas for softmax outputs we have

=   kj  

(cid:4)(aj)

(5.75)

(5.76)

=   kjyk     ykyj.

   yk
   aj

we can summarize the procedure for evaluating the jacobian matrix as follows.
apply the input vector corresponding to the point in input space at which the ja-
cobian matrix is to be found, and forward propagate in the usual way to obtain the

5.4. the hessian matrix

249

activations of all of the hidden and output units in the network. next, for each row
k of the jacobian matrix, corresponding to the output unit k, backpropagate using
the recursive relation (5.74), starting with (5.75) or (5.76), for all of the hidden units
in the network. finally, use (5.73) to do the id26 to the inputs. the
jacobian can also be evaluated using an alternative forward propagation formalism,
which can be derived in an analogous way to the id26 approach given
here.

again, the implementation of such algorithms can be checked by using numeri-

exercise 5.15

cal differentiation in the form

= yk(xi +  )     yk(xi      )

2 

   yk
   xi

+ o( 2)

(5.77)

which involves 2d forward propagations for a network having d inputs.

5.4. the hessian matrix

we have shown how the technique of id26 can be used to obtain the    rst
derivatives of an error function with respect to the weights in the network. back-
propagation can also be used to evaluate the second derivatives of the error, given
by

   2e

   wji   wlk

.

(5.78)

note that it is sometimes convenient to consider all of the weight and bias parameters
as elements wi of a single vector, denoted w, in which case the second derivatives
form the elements hij of the hessian matrix h, where i, j     {1, . . . , w} and w is
the total number of weights and biases. the hessian plays an important role in many
aspects of neural computing, including the following:

1. several nonlinear optimization algorithms used for training neural networks
are based on considerations of the second-order properties of the error surface,
which are controlled by the hessian matrix (bishop and nabney, 2008).

2. the hessian forms the basis of a fast procedure for re-training a feed-forward

network following a small change in the training data (bishop, 1991).

3. the inverse of the hessian has been used to identify the least signi   cant weights

in a network as part of network    pruning    algorithms (le cun et al., 1990).

4. the hessian plays a central role in the laplace approximation for a bayesian
neural network (see section 5.7). its inverse is used to determine the predic-
tive distribution for a trained network, its eigenvalues determine the values of
hyperparameters, and its determinant is used to evaluate the model evidence.

various approximation schemes have been used to evaluate the hessian matrix
for a neural network. however, the hessian can also be calculated exactly using an
extension of the id26 technique.

250

5. neural networks

an important consideration for many applications of the hessian is the ef   ciency
with which it can be evaluated. if there are w parameters (weights and biases) in the
network, then the hessian matrix has dimensions w    w and so the computational
effort needed to evaluate the hessian will scale like o(w 2) for each pattern in the
data set. as we shall see, there are ef   cient methods for evaluating the hessian
whose scaling is indeed o(w 2).

5.4.1 diagonal approximation
some of the applications for the hessian matrix discussed above require the
inverse of the hessian, rather than the hessian itself. for this reason, there has
been some interest in using a diagonal approximation to the hessian, in other words
one that simply replaces the off-diagonal elements with zeros, because its inverse is
trivial to evaluate. again, we shall consider an error function that consists of a sum
of terms, one for each pattern in the data set, so that e =
n en. the hessian can
then be obtained by considering one pattern at a time, and then summing the results
over all patterns. from (5.48), the diagonal elements of the hessian, for pattern n,
can be written

(cid:5)

   2en
   w2
ji

=    2en
   a2
j

z2
i .

(5.79)

using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can
be found recursively using the chain rule of id128 to give a backprop-
agation equation of the form

   2en
   a2
j

(cid:4)(aj)2

= h

wkjwk(cid:1)j

   2en
   ak   ak(cid:1)

(cid:4)(cid:4)(aj)

+ h

wkj

   en
   ak

.

(5.80)

(cid:2)

(cid:2)

k

k(cid:1)

if we now neglect off-diagonal elements in the second-derivative terms, we obtain
(becker and le cun, 1989; le cun et al., 1990)

   2en
   a2
j

(cid:4)(aj)2

= h

w2
kj

   2en
   a2
k

(cid:4)(cid:4)(aj)

+ h

wkj

   en
   ak

.

(5.81)

(cid:2)

k

(cid:2)

k

(cid:2)

k

note that the number of computational steps required to evaluate this approximation
is o(w ), where w is the total number of weight and bias parameters in the network,
compared with o(w 2) for the full hessian.

ricotti et al. (1988) also used the diagonal approximation to the hessian, but
they retained all terms in the evaluation of    2en/   a2
j and so obtained exact expres-
sions for the diagonal terms. note that this no longer has o(w ) scaling. the major
problem with diagonal approximations, however, is that in practice the hessian is
typically found to be strongly nondiagonal, and so these approximations, which are
driven mainly be computational convenience, must be treated with care.

5.4. the hessian matrix

251

n(cid:2)

n=1

e =

1
2

n(cid:2)

5.4.2 outer product approximation
when neural networks are applied to regression problems, it is common to use

a sum-of-squares error function of the form

(yn     tn)2

(5.82)

n(cid:2)

where we have considered the case of a single output in order to keep the notation
simple (the extension to several outputs is straightforward). we can then write the
hessian matrix in the form

h =       e =

   yn   yn +

(yn     tn)      yn.

(5.83)

n=1

n=1

if the network has been trained on the data set, and its outputs yn happen to be very
close to the target values tn, then the second term in (5.83) will be small and can
be neglected. more generally, however, it may be appropriate to neglect this term
by the following argument. recall from section 1.5.5 that the optimal function that
minimizes a sum-of-squares loss is the conditional average of the target data. the
quantity (yn     tn) is then a random variable with zero mean. if we assume that its
value is uncorrelated with the value of the second derivative term on the right-hand
side of (5.83), then the whole term will average to zero in the summation over n.

by neglecting the second term in (5.83), we arrive at the levenberg   marquardt
approximation or outer product approximation (because the hessian matrix is built
up from a sum of outer products of vectors), given by

bnbt
n

n=1

(5.84)
where bn =    yn =    an because the activation function for the output units is
simply the identity. evaluation of the outer product approximation for the hessian
is straightforward as it only involves    rst derivatives of the error function, which
can be evaluated ef   ciently in o(w ) steps using standard id26. the
elements of the matrix can then be found in o(w 2) steps by simple multiplication.
it is important to emphasize that this approximation is only likely to be valid for a
network that has been trained appropriately, and that for a general network mapping
the second derivative terms on the right-hand side of (5.83) will typically not be
negligible.

in the case of the cross-id178 error function for a network with logistic sigmoid

output-unit id180, the corresponding approximation is given by

h (cid:7) n(cid:2)

h (cid:7) n(cid:2)

exercise 5.16

exercise 5.17

exercise 5.19

exercise 5.20

an analogous result can be obtained for multiclass networks having softmax output-
unit id180.

n=1

yn(1     yn)bnbt
n.

(5.85)

252

5. neural networks

5.4.3 inverse hessian
we can use the outer-product approximation to develop a computationally ef-
   cient procedure for approximating the inverse of the hessian (hassibi and stork,
1993). first we write the outer-product approximation in matrix notation as

n(cid:2)

bnbt
n

hn =

(5.86)
where bn        wan is the contribution to the gradient of the output unit activation
arising from data point n. we now derive a sequential procedure for building up the
hessian by including data points one at a time. suppose we have already obtained
the inverse hessian using the    rst l data points. by separating off the contribution
from data point l + 1, we obtain

n=1

in order to evaluate the inverse of the hessian, we now consider the matrix identity

(cid:10)

m + vvt

l+1.

hl+1 = hl + bl+1bt

(cid:11)   1 = m   1     (m   1v)

(cid:10)

vtm   1
1 + vtm   1v

(cid:11)

(5.87)

(5.88)

where i is the unit matrix, which is simply a special case of the woodbury identity
(c.7). if we now identify hl with m and bl+1 with v, we obtain

   1
l+1 = h

h

   1
   1
l bl+1bt
l     h
l+1h
   1
l
   1
l+1h
l bl+1

1 + bt

.

(5.89)

exercise 5.21

in this way, data points are sequentially absorbed until l+1 = n and the whole data
set has been processed. this result therefore represents a procedure for evaluating
the inverse of the hessian using a single pass through the data set. the initial matrix
h0 is chosen to be   i, where    is a small quantity, so that the algorithm actually
   nds the inverse of h +   i. the results are not particularly sensitive to the precise
value of   . extension of this algorithm to networks having more than one output is
straightforward.

we note here that the hessian matrix can sometimes be calculated indirectly as
part of the network training algorithm. in particular, quasi-newton nonlinear opti-
mization algorithms gradually build up an approximation to the inverse of the hes-
sian during training. such algorithms are discussed in detail in bishop and nabney
(2008).

5.4.4 finite differences
as in the case of the    rst derivatives of the error function, we can    nd the second
derivatives by using    nite differences, with accuracy limited by numerical precision.
if we perturb each possible pair of weights in turn, we obtain

   wji   wlk

   2e

=

1
4 2

{e(wji +  , wlk +  )     e(wji +  , wlk      )
   e(wji      , wlk +  ) + e(wji      , wlk      )} + o( 2).

(5.90)

5.4. the hessian matrix

253

again, by using a symmetrical central differences formulation, we ensure that the
residual errors are o( 2) rather than o( ). because there are w 2 elements in the
hessian matrix, and because the evaluation of each element requires four forward
propagations each needing o(w ) operations (per pattern), we see that this approach
will require o(w 3) operations to evaluate the complete hessian. it therefore has
poor scaling properties, although in practice it is very useful as a check on the soft-
ware implementation of id26 methods.

a more ef   cient version of numerical differentiation can be found by applying
central differences to the    rst derivatives of the error function, which are themselves
calculated using id26. this gives

(cid:12)

(cid:13)

   2e

   wji   wlk

=

1
2 

   e
   wji

(wlk +  )        e
   wji

(wlk      )

+ o( 2).

(5.91)

because there are now only w weights to be perturbed, and because the gradients
can be evaluated in o(w ) steps, we see that this method gives the hessian in o(w 2)
operations.

5.4.5 exact evaluation of the hessian
so far, we have considered various approximation schemes for evaluating the
hessian matrix or its inverse. the hessian can also be evaluated exactly, for a net-
work of arbitrary feed-forward topology, using extension of the technique of back-
propagation used to evaluate    rst derivatives, which shares many of its desirable
features including computational ef   ciency (bishop, 1991; bishop, 1992). it can be
applied to any differentiable error function that can be expressed as a function of
the network outputs and to networks having arbitrary differentiable activation func-
tions. the number of computational steps needed to evaluate the hessian scales
like o(w 2). similar algorithms have also been considered by buntine and weigend
(1993).

here we consider the speci   c case of a network having two layers of weights,
(cid:4)
for which the required equations are easily derived. we shall use indices i and i
to
to denote inputs, indices j and j
denote outputs. we    rst de   ne

to denoted hidden units, and indices k and k

(cid:4)

(cid:4)

  k =    en
   ak

,

mkk(cid:1)        2en
   ak   ak(cid:1)

(5.92)

where en is the contribution to the error from data point n. the hessian matrix for
this network can then be considered in three separate blocks as follows.

1. both weights in the second layer:

exercise 5.22

   2en
(2)
kj    w

(2)
k(cid:1)j(cid:1)

   w

= zjzj(cid:1)mkk(cid:1).

(5.93)

254

5. neural networks

2. both weights in the    rst layer:

(cid:2)
(cid:2)

k

(cid:2)

k

k(cid:1)

= xixi(cid:1)h

(cid:4)(cid:4)(aj(cid:1))ijj(cid:1)

w

(2)
kj(cid:1)  k

   w

   2en
(1)
(1)
ji    w
j(cid:1)i(cid:1)
+xixi(cid:1)h

3. one weight in each layer:

(cid:4)(aj(cid:1))h

(cid:4)(aj)

(cid:24)

   2en
(1)
ji    w

(2)
kj(cid:1)

   w

= xih

(cid:4)(aj(cid:1))

  kijj(cid:1) + zj

w

(2)
k(cid:1)j(cid:1)w

(2)
kj mkk(cid:1).

(5.94)

(cid:25)

w

(2)
k(cid:1)j(cid:1)hkk(cid:1)

.

(5.95)

(cid:2)

k(cid:1)

exercise 5.23

(cid:4)

here ijj(cid:1) is the j, j
element of the identity matrix. if one or both of the weights is
a bias term, then the corresponding expressions are obtained simply by setting the
appropriate activation(s) to 1. inclusion of skip-layer connections is straightforward.

5.4.6 fast multiplication by the hessian
for many applications of the hessian, the quantity of interest is not the hessian
matrix h itself but the product of h with some vector v. we have seen that the
evaluation of the hessian takes o(w 2) operations, and it also requires storage that is
o(w 2). the vector vth that we wish to calculate, however, has only w elements,
so instead of computing the hessian as an intermediate step, we can instead try to
   nd an ef   cient approach to evaluating vth directly in a way that requires only
o(w ) operations.

to do this, we    rst note that

vth = vt   (   e)

(5.96)
where     denotes the gradient operator in weight space. we can then write down
the standard forward-propagation and id26 equations for the evaluation
of    e and apply (5.96) to these equations to give a set of forward-propagation and
id26 equations for the evaluation of vth (m  ller, 1993; pearlmutter,
1994). this corresponds to acting on the original forward-propagation and back-
propagation equations with a differential operator vt   . pearlmutter (1994) used the
notation r{  } to denote the operator vt   , and we shall follow this convention. the
analysis is straightforward and makes use of the usual rules of id128,
together with the result

r{w} = v.

(5.97)

the technique is best illustrated with a simple example, and again we choose a
two-layer network of the form shown in figure 5.1, with linear output units and a
sum-of-squares error function. as before, we consider the contribution to the error
function from one pattern in the data set. the required vector is then obtained as

5.4. the hessian matrix

255

usual by summing over the contributions from each of the patterns separately. for
the two-layer network, the forward-propagation equations are given by

(cid:2)
(cid:2)

i

aj =

wjixi

zj = h(aj)
yk =

wkjzj.

(5.98)

(5.99)

(5.100)

j

we now act on these equations using the r{  } operator to obtain a set of forward
propagation equations in the form
r{aj} =
r{zj} = h
r{yk} =

(cid:2)
(cid:2)

wkjr{zj} +

(cid:4)(aj)r{aj}

(cid:2)

(5.101)

(5.102)

vkjzj

vjixi

(5.103)

i

j

j

where vji is the element of the vector v that corresponds to the weight wji. quan-
tities of the form r{zj}, r{aj} and r{yk} are to be regarded as new variables
whose values are found using the above equations.

because we are considering a sum-of-squares error function, we have the fol-

lowing standard id26 expressions:
  k = yk     tk
(cid:4)(aj)
  j = h

(cid:2)

wkj  k.

(5.104)
(5.105)

again, we act on these equations with the r{  } operator to obtain a set of backprop-
agation equations in the form

r{  k} = r{yk}
r{  j} = h

(cid:4)(cid:4)(aj)r{aj}

wkj  k

k

vkj  k + h

(cid:4)(aj)

(cid:4)(aj)

+ h

(cid:2)

k

(5.106)

wkjr{  k}.

(5.107)

(cid:2)

k

k

(cid:2)

finally, we have the usual equations for the    rst derivatives of the error

   e
   wkj
   e
   wji

=   kzj

=   jxi

(5.108)

(5.109)

256

5. neural networks

and acting on these with the r{  } operator, we obtain expressions for the elements
of the vector vth

(cid:13)
(cid:13)

(cid:12)
(cid:12)

r

r

   e
   wkj

   e
   wji

= r{  k}zj +   kr{zj}

= xir{  j}.

(5.110)

(5.111)

the implementation of this algorithm involves the introduction of additional
variables r{aj}, r{zj} and r{  j} for the hidden units and r{  k} and r{yk}
for the output units. for each input pattern, the values of these quantities can be
found using the above results, and the elements of vth are then given by (5.110)
and (5.111). an elegant aspect of this technique is that the equations for evaluating
vth mirror closely those for standard forward and backward propagation, and so the
extension of existing software to compute this product is typically straightforward.
if desired, the technique can be used to evaluate the full hessian matrix by
choosing the vector v to be given successively by a series of unit vectors of the
form (0, 0, . . . , 1, . . . , 0) each of which picks out one column of the hessian. this
leads to a formalism that is analytically equivalent to the id26 procedure
of bishop (1992), as described in section 5.4.5, though with some loss of ef   ciency
due to redundant calculations.

5.5. id173 in neural networks

the number of input and outputs units in a neural network is generally determined
by the dimensionality of the data set, whereas the number m of hidden units is a free
parameter that can be adjusted to give the best predictive performance. note that m
controls the number of parameters (weights and biases) in the network, and so we
might expect that in a maximum likelihood setting there will be an optimum value
of m that gives the best generalization performance, corresponding to the optimum
balance between under-   tting and over-   tting. figure 5.9 shows an example of the
effect of different values of m for the sinusoidal regression problem.

the generalization error, however, is not a simple function of m due to the
presence of local minima in the error function, as illustrated in figure 5.10. here
we see the effect of choosing multiple random initializations for the weight vector
for a range of values of m. the overall best validation set performance in this
case occurred for a particular solution having m = 8. in practice, one approach to
choosing m is in fact to plot a graph of the kind shown in figure 5.10 and then to
choose the speci   c solution having the smallest validation set error.

there are, however, other ways to control the complexity of a neural network
model in order to avoid over-   tting. from our discussion of polynomial curve    tting
in chapter 1, we see that an alternative approach is to choose a relatively large value
for m and then to control complexity by the addition of a id173 term to the
error function. the simplest regularizer is the quadratic, giving a regularized error

5.5. id173 in neural networks

257

1

0

   1

m = 1

1

0

   1

m = 3

1

0

   1

m = 10

0

1

0

1

0

1

figure 5.9 examples of two-layer networks trained on 10 data points drawn from the sinusoidal data set. the
graphs show the result of    tting networks having m = 1, 3 and 10 hidden units, respectively, by minimizing a
sum-of-squares error function using a scaled conjugate-gradient algorithm.

of the form

(cid:4)e(w) = e(w) +   

2

wtw.

(5.112)

this regularizer is also known as weight decay and has been discussed at length
in chapter 3. the effective model complexity is then determined by the choice of
the id173 coef   cient   . as we have seen previously, this regularizer can be
interpreted as the negative logarithm of a zero-mean gaussian prior distribution over
the weight vector w.

5.5.1 consistent gaussian priors
one of the limitations of simple weight decay in the form (5.112) is that is
inconsistent with certain scaling properties of network mappings. to illustrate this,
consider a multilayer id88 network having two layers of weights and linear
output units, which performs a mapping from a set of input variables {xi} to a set
of output variables {yk}. the activations of the hidden units in the    rst hidden layer

figure 5.10 plot of the sum-of-squares test-set
error for the polynomial data set ver-
sus the number of hidden units in the
network, with 30 random starts for
each network size, showing the ef-
fect of local minima. for each new
start, the weight vector was initial-
ized by sampling from an isotropic
gaussian distribution having a mean
of zero and a variance of 10.

160

140

120

100

80

60

0

2

4

6

8

10

(5.113)

(5.114)

(5.115)

(5.116)

(5.117)

(5.118)

(5.119)
(5.120)

258

5. neural networks

take the form

zj = h

(cid:23)

wjixi + wj0

(cid:22)(cid:2)
(cid:2)

i

while the activations of the output units are given by

yk =

wkjzj + wk0.

suppose we perform a linear transformation of the input data of the form

j

xi    (cid:4)xi = axi + b.

exercise 5.24

then we can arrange for the mapping performed by the network to be unchanged
by making a corresponding linear transformation of the weights and biases from the
inputs to the units in the hidden layer of the form

wji.

a

1
a

wji

wji    (cid:4)wji =
(cid:2)
wj0    (cid:4)wj0 = wj0     b
yk    (cid:4)yk = cyk + d
wkj    (cid:4)wkj = cwkj
wk0    (cid:4)wk0 = cwk0 + d.

i

similarly, a linear transformation of the output variables of the network of the form

can be achieved by making a transformation of the second-layer weights and biases
using

if we train one network using the original data and one network using data for which
the input and/or target variables are transformed by one of the above linear transfor-
mations, then consistency requires that we should obtain equivalent networks that
differ only by the linear transformation of the weights as given. any regularizer
should be consistent with this property, otherwise it arbitrarily favours one solution
over another, equivalent one. clearly, simple weight decay (5.112), that treats all
weights and biases on an equal footing, does not satisfy this property.

we therefore look for a regularizer which is invariant under the linear trans-
formations (5.116), (5.117), (5.119) and (5.120). these require that the regularizer
should be invariant to re-scaling of the weights and to shifts of the biases. such a
regularizer is given by

(5.121)
where w1 denotes the set of weights in the    rst layer, w2 denotes the set of weights
in the second layer, and biases are excluded from the summations. this regularizer

w   w2

w   w1

w2

w2 +   2
2

  1
2

(cid:2)

(cid:2)

5.5. id173 in neural networks

259

will remain unchanged under the weight transformations provided the id173
parameters are re-scaled using   1     a1/2  1 and   2     c
(cid:2)

the regularizer (5.121) corresponds to a prior of the form

(cid:2)

   1/2  2.

(cid:22)

(cid:23)

p(w|  1,   2)     exp

      1
2

w2       2
2

w   w1

w   w2

w2

.

(5.122)

note that priors of this form are improper (they cannot be normalized) because the
bias parameters are unconstrained. the use of improper priors can lead to dif   culties
in selecting id173 coef   cients and in model comparison within the bayesian
framework, because the corresponding evidence is zero. it is therefore common to
include separate priors for the biases (which then break shift invariance) having their
own hyperparameters. we can illustrate the effect of the resulting four hyperpa-
rameters by drawing samples from the prior and plotting the corresponding network
functions, as shown in figure 5.11.
any number of groups wk so that

more generally, we can consider priors in which the weights are divided into

p(w)     exp

  k(cid:5)w(cid:5)2

k

(cid:23)

(cid:22)

(cid:2)
(cid:2)

k

   1
2

w2
j .

j   wk

where

(cid:5)w(cid:5)2

k =

(5.123)

(5.124)

as a special case of this prior, if we choose the groups to correspond to the sets
of weights associated with each of the input units, and we optimize the marginal
likelihood with respect to the corresponding parameters   k, we obtain automatic
relevance determination as discussed in section 7.2.2.

5.5.2 early stopping
an alternative to id173 as a way of controlling the effective complexity
of a network is the procedure of early stopping. the training of nonlinear network
models corresponds to an iterative reduction of the error function de   ned with re-
spect to a set of training data. for many of the optimization algorithms used for
network training, such as conjugate gradients, the error is a nonincreasing function
of the iteration index. however, the error measured with respect to independent data,
generally called a validation set, often shows a decrease at    rst, followed by an in-
crease as the network starts to over-   t. training can therefore be stopped at the point
of smallest error with respect to the validation data set, as indicated in figure 5.12,
in order to obtain a network having good generalization performance.

the behaviour of the network in this case is sometimes explained qualitatively
in terms of the effective number of degrees of freedom in the network, in which this
number starts out small and then to grows during the training process, corresponding
to a steady increase in the effective complexity of the model. halting training before

260

5. neural networks

  w

1 = 1,   b

1 = 1,   w

2 = 1,   b

2 = 1

   0.5

0

0.5

1

  w

1 = 1000,   b

1 = 100,   w

2 = 1,   b

2 = 1

   0.5

0

0.5

1

4

2

0

   2

   4

   6

   1

5

0

   5

   10

   1

40

20

0

   20

   40

   60

   1

5

0

   5

   10

   1

  w

1 = 1,   b

1 = 1,   w

2 = 10,   b

2 = 1

   0.5

0

0.5

1

  w

1 = 1000,   b

1 = 1000,   w

2 = 1,   b

2 = 1

   0.5

0

0.5

1

figure 5.11 illustration of the effect of the hyperparameters governing the prior distribution over weights and
biases in a two-layer network having a single input, a single linear output, and 12 hidden units having    tanh   
id180. the priors are governed by four hyperparameters   b
2 , which represent
the precisions of the gaussian distributions of the    rst-layer biases,    rst-layer weights, second-layer biases, and
second-layer weights, respectively. we see that the parameter   w
2 governs the vertical scale of functions (note
the different vertical axis ranges on the top two diagrams),   w
1 governs the horizontal scale of variations in the
function values, and   b
2, whose
effect is not illustrated here, governs the range of vertical offsets of the functions.

1 governs the horizontal range over which variations occur. the parameter   b

2, and   w

1 ,   b

1,   w

a minimum of the training error has been reached then represents a way of limiting
the effective network complexity.

in the case of a quadratic error function, we can verify this insight, and show
that early stopping should exhibit similar behaviour to id173 using a sim-
ple weight-decay term. this can be understood from figure 5.13, in which the axes
in weight space have been rotated to be parallel to the eigenvectors of the hessian
matrix. if, in the absence of weight decay, the weight vector starts at the origin and
proceeds during training along a path that follows the local negative gradient vec-
tor, then the weight vector will move initially parallel to the w2 axis through a point

corresponding roughly to (cid:4)w and then move towards the minimum of the error func-
eigenvalues of the hessian. stopping at a point near(cid:4)w is therefore similar to weight

tion wml. this follows from the shape of the error surface and the widely differing

exercise 5.25

decay. the relationship between early stopping and weight decay can be made quan-
titative, thereby showing that the quantity       (where    is the iteration index, and   
is the learning rate parameter) plays the role of the reciprocal of the id173

5.5. id173 in neural networks

261

0.25

0.2

0.15

0

10

20

30

40

50

0.45

0.4

0.35

0

10

20

30

40

50

figure 5.12 an illustration of the behaviour of training set error (left) and validation set error (right) during a
typical training session, as a function of the iteration step, for the sinusoidal data set. the goal of achieving
the best generalization performance suggests that training should be stopped at the point shown by the vertical
dashed lines, corresponding to the minimum of the validation set error.

parameter   . the effective number of parameters in the network therefore grows
during the course of training.

5.5.3 invariances
in many applications of pattern recognition, it is known that predictions should
be unchanged, or invariant, under one or more transformations of the input vari-
ables. for example, in the classi   cation of objects in two-dimensional images, such
as handwritten digits, a particular object should be assigned the same classi   cation
irrespective of its position within the image (translation invariance) or of its size
(scale invariance). such transformations produce signi   cant changes in the raw
data, expressed in terms of the intensities at each of the pixels in the image, and
yet should give rise to the same output from the classi   cation system. similarly
in id103, small levels of nonlinear warping along the time axis, which
preserve temporal ordering, should not change the interpretation of the signal.

if suf   ciently large numbers of training patterns are available, then an adaptive
model such as a neural network can learn the invariance, at least approximately. this
involves including within the training set a suf   ciently large number of examples of
the effects of the various transformations. thus, for translation invariance in an im-
age, the training set should include examples of objects at many different positions.
this approach may be impractical, however, if the number of training examples
is limited, or if there are several invariants (because the number of combinations of
transformations grows exponentially with the number of such transformations). we
therefore seek alternative approaches for encouraging an adaptive model to exhibit
the required invariances. these can broadly be divided into four categories:

1. the training set is augmented using replicas of the training patterns, trans-
formed according to the desired invariances. for instance, in our digit recog-
nition example, we could make multiple copies of each example in which the

262

5. neural networks

figure 5.13 a schematic illustration of why
early stopping can give similar
results to weight decay in the
case of a quadratic error func-
tion. the ellipse shows a con-
tour of constant error, and wml
denotes the minimum of the er-
ror function.
if the weight vector
starts at the origin and moves ac-
cording to the local negative gra-
dient direction, then it will follow
the path shown by the curve. by
stopping training early, a weight
vector ew is found that is qual-
itatively similar to that obtained
with a simple weight-decay reg-
ularizer and training to the mini-
mum of the regularized error, as
can be seen by comparing with
figure 3.15.

w2

(cid:4)w

wml

w1

digit is shifted to a different position in each image.

2. a id173 term is added to the error function that penalizes changes in
the model output when the input is transformed. this leads to the technique of
tangent propagation, discussed in section 5.5.4.

3. invariance is built into the pre-processing by extracting features that are invari-
ant under the required transformations. any subsequent regression or classi-
   cation system that uses such features as inputs will necessarily also respect
these invariances.

4. the    nal option is to build the invariance properties into the structure of a neu-
ral network (or into the de   nition of a id81 in the case of techniques
such as the relevance vector machine). one way to achieve this is through the
use of local receptive    elds and shared weights, as discussed in the context of
convolutional neural networks in section 5.5.6.

approach 1 is often relatively easy to implement and can be used to encourage com-
plex invariances such as those illustrated in figure 5.14. for sequential training
algorithms, this can be done by transforming each input pattern before it is presented
to the model so that, if the patterns are being recycled, a different transformation
(drawn from an appropriate distribution) is added each time. for batch methods, a
similar effect can be achieved by replicating each data point a number of times and
transforming each copy independently. the use of such augmented data can lead to
signi   cant improvements in generalization (simard et al., 2003), although it can also
be computationally costly.

approach 2 leaves the data set unchanged but modi   es the error function through
the addition of a regularizer. in section 5.5.5, we shall show that this approach is
closely related to approach 2.

5.5. id173 in neural networks

263

figure 5.14 illustration of the synthetic warping of a handwritten digit. the original image is shown on the
left. on the right, the top row shows three examples of warped digits, with the corresponding displacement
   elds shown on the bottom row. these displacement    elds are generated by sampling random displacements
   x,    y     (0, 1) at each pixel and then smoothing by convolution with gaussians of width 0.01, 30 and 60
respectively.

one advantage of approach 3 is that it can correctly extrapolate well beyond the
range of transformations included in the training set. however, it can be dif   cult
to    nd hand-crafted features with the required invariances that do not also discard
information that can be useful for discrimination.

5.5.4 tangent propagation
we can use id173 to encourage models to be invariant to transformations
of the input through the technique of tangent propagation (simard et al., 1992).
consider the effect of a transformation on a particular input vector xn. provided the
transformation is continuous (such as translation or rotation, but not mirror re   ection
for instance), then the transformed pattern will sweep out a manifold m within the
d-dimensional input space. this is illustrated in figure 5.15, for the case of d =
2 for simplicity. suppose the transformation is governed by a single parameter   
(which might be rotation angle for instance). then the subspace m swept out by xn

figure 5.15 illustration of a two-dimensional

input space
showing the effect of a continuous transforma-
tion on a particular input vector xn. a one-
dimensional transformation, parameterized by
the continuous variable   , applied to xn causes
it to sweep out a one-dimensional manifold m.
locally, the effect of the transformation can be
approximated by the tangent vector    n.

x2

   n

xn

m

  

x1

264

5. neural networks

will be one-dimensional, and will be parameterized by   . let the vector that results
from acting on xn by this transformation be denoted by s(xn,   ), which is de   ned
so that s(x, 0) = x. then the tangent to the curve m is given by the directional
derivative    =    s/     , and the tangent vector at the point xn is given by

   n =    s(xn,   )

     

.

  =0

(5.125)

under a transformation of the input vector, the network output vector will, in general,
change. the derivative of output k with respect to    is given by

(cid:7)(cid:7)(cid:7)(cid:7)

d(cid:2)

d(cid:2)

   yk
     

=

  =0

i=1

   yk
   xi

   xi
     

  =0

i=1

=

jki  i

(5.126)

(cid:7)(cid:7)(cid:7)(cid:7)

(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)

where jki is the (k, i) element of the jacobian matrix j, as discussed in section 5.3.4.
the result (5.126) can be used to modify the standard error function, so as to encour-
age local invariance in the neighbourhood of the data points, by the addition to the
original error function e of a id173 function     to give a total error function
of the form

where    is a id173 coef   cient and

(cid:22)

(cid:2)

(cid:2)

n

k

    =

1
2

(cid:7)(cid:7)(cid:7)(cid:7)

   ynk
     

  =0

=

1
2

(5.127)

(cid:22)

d(cid:2)

(cid:23)2

jnki  ni

.

(5.128)

n

k

i=1

(cid:4)e = e +      
(cid:23)2
(cid:2)
(cid:2)

exercise 5.26

the id173 function will be zero when the network mapping function is in-
variant under the transformation in the neighbourhood of each pattern vector, and
the value of the parameter    determines the balance between    tting the training data
and learning the invariance property.

in a practical implementation, the tangent vector    n can be approximated us-
ing    nite differences, by subtracting the original vector xn from the corresponding
vector after transformation using a small value of   , and then dividing by   . this is
illustrated in figure 5.16.

the id173 function depends on the network weights through the jaco-
bian j. a id26 formalism for computing the derivatives of the regu-
larizer with respect to the network weights is easily obtained by extension of the
techniques introduced in section 5.3.

if the transformation is governed by l parameters (e.g., l = 3 for the case of
translations combined with in-plane rotations in a two-dimensional image), then the
manifold m will have dimensionality l, and the corresponding regularizer is given
by the sum of terms of the form (5.128), one for each transformation. if several
transformations are considered at the same time, and the network mapping is made
invariant to each separately, then it will be (locally) invariant to combinations of the
transformations (simard et al., 1992).

5.5. id173 in neural networks

265

(a)

(b)

figure 5.16 illustration
showing
(a) the original image x of a hand-
written digit, (b) the tangent vector
   corresponding to an in   nitesimal
clockwise rotation, (c) the result of
adding a small contribution from the
tangent vector to the original image
giving x +     with   = 15 degrees,
and (d) the true image rotated for
comparison.

(c)

(d)

a related technique, called tangent distance, can be used to build invariance
properties into distance-based methods such as nearest-neighbour classi   ers (simard
et al., 1993).

5.5.5 training with transformed data
we have seen that one way to encourage invariance of a model to a set of trans-
formations is to expand the training set using transformed versions of the original
input patterns. here we show that this approach is closely related to the technique of
tangent propagation (bishop, 1995b; leen, 1995).

as in section 5.5.4, we shall consider a transformation governed by a single
parameter    and described by the function s(x,   ), with s(x, 0) = x. we shall
also consider a sum-of-squares error function. the error function for untransformed
inputs can be written (in the in   nite data set limit) in the form
{y(x)     t}2p(t|x)p(x) dx dt

(cid:6)(cid:6)

e =

(5.129)

1
2

as discussed in section 1.5.5. here we have considered a network having a single
output, in order to keep the notation uncluttered.
if we now consider an in   nite
number of copies of each data point, each of which is perturbed by the transformation

266

5. neural networks

in which the parameter    is drawn from a distribution p(  ), then the error function
de   ned over this expanded data set can be written as

{y(s(x,   ))     t}2p(t|x)p(x)p(  ) dx dt d  .

(5.130)

(cid:6)(cid:6)(cid:6)

(cid:4)e =

1
2

we now assume that the distribution p(  ) has zero mean with small variance, so that
we are only considering small transformations of the original input vectors. we can
then expand the transformation function as a taylor series in powers of    to give

s(x,   ) = s(x, 0) +   

   
     

s(x,   )

+   2
2

  =0

   2
     2 s(x,   )

+ o(  3)

  =0

(cid:7)(cid:7)(cid:7)(cid:7)

denotes the second derivative of s(x,   ) with respect to    evaluated at    = 0.

= x +      +

1

2   2   (cid:4) + o(  3)
-

where    (cid:4)
this allows us to expand the model function to give
y(s(x,   )) = y(x) +      t   y(x) +   2
2

(   (cid:4))t    y(x) +    t      y(x)  

+ o(  3).

substituting into the mean error function (5.130) and expanding, we then have

(cid:4)e =

(cid:6)(cid:6)
(cid:6)(cid:6)
(cid:6)(cid:6) (cid:29)

1
2

{y(x)     t}2p(t|x)p(x) dx dt

+ e[  ]

{y(x)     t}   t   y(x)p(t|x)p(x) dx dt

+ e[  2]

(cid:10)

{y(x)     t}1
2

.

(cid:11)2

   t   y(x)

+

p(t|x)p(x) dx dt + o(  3).

(   (cid:4))t    y(x) +    t      y(x)  

(cid:7)(cid:7)(cid:7)(cid:7)

(cid:19)

because the distribution of transformations has zero mean we have e[  ] = 0. also,
we shall denote e[  2] by   . omitting terms of o(  3), the average error function then
becomes

(5.131)
where e is the original sum-of-squares error, and the id173 term     takes the
form

(cid:4)e = e +      

    =

(   (cid:4))t    y(x) +    t      y(x)  

(cid:19)

(cid:6) (cid:29)
(cid:10)

+

{y(x)     e[t|x]}1
2

(cid:30)

(cid:11)2

   t   y(x)

p(x) dx

(5.132)

.

(cid:20)

(cid:20)

in which we have performed the integration over t.

5.5. id173 in neural networks

267

we can further simplify this id173 term as follows. in section 1.5.5 we
saw that the function that minimizes the sum-of-squares error is given by the condi-
tional average e[t|x] of the target values t. from (5.131) we see that the regularized
error will equal the unregularized sum-of-squares plus terms which are o(  ), and so
the network function that minimizes the total error will have the form

thus, to leading order in   , the    rst term in the regularizer vanishes and we are left
with

p(x) dx

(5.134)

y(x) = e[t|x] + o(  ).

(cid:6) (cid:10)

(cid:11)2

    =

1
2

   t   y(x)

(5.133)

exercise 5.27

which is equivalent to the tangent propagation regularizer (5.128).
if we consider the special case in which the transformation of the inputs simply
consists of the addition of random noise, so that x     x +   , then the regularizer
takes the form

(cid:6)

    =

1
2

(cid:5)   y(x)(cid:5)2

p(x) dx

(5.135)

which is known as tikhonov id173 (tikhonov and arsenin, 1977; bishop,
1995b). derivatives of this regularizer with respect to the network weights can be
found using an extended id26 algorithm (bishop, 1993). we see that, for
small noise amplitudes, tikhonov id173 is related to the addition of random
noise to the inputs, which has been shown to improve generalization in appropriate
circumstances (sietsma and dow, 1991).

5.5.6 convolutional networks
another approach to creating models that are invariant to certain transformation
of the inputs is to build the invariance properties into the structure of a neural net-
work. this is the basis for the convolutional neural network (le cun et al., 1989;
lecun et al., 1998), which has been widely applied to image data.

consider the speci   c task of recognizing handwritten digits. each input image
comprises a set of pixel intensity values, and the desired output is a posterior proba-
bility distribution over the ten digit classes. we know that the identity of the digit is
invariant under translations and scaling as well as (small) rotations. furthermore, the
network must also exhibit invariance to more subtle transformations such as elastic
deformations of the kind illustrated in figure 5.14. one simple approach would be to
treat the image as the input to a fully connected network, such as the kind shown in
figure 5.1. given a suf   ciently large training set, such a network could in principle
yield a good solution to this problem and would learn the appropriate invariances by
example.

however, this approach ignores a key property of images, which is that nearby
pixels are more strongly correlated than more distant pixels. many of the modern
approaches to id161 exploit this property by extracting local features that
depend only on small subregions of the image. information from such features can
then be merged in later stages of processing in order to detect higher-order features

268

5. neural networks

input image

convolutional layer

sub-sampling
layer

figure 5.17 diagram illustrating part of a convolutional neural network, showing a layer of convolu-
tional units followed by a layer of subsampling units. several successive pairs of such
layers may be used.

and ultimately to yield information about the image as whole. also, local features
that are useful in one region of the image are likely to be useful in other regions of
the image, for instance if the object of interest is translated.

these notions are incorporated into convolutional neural networks through three
mechanisms: (i) local receptive    elds, (ii) weight sharing, and (iii) subsampling. the
structure of a convolutional network is illustrated in figure 5.17. in the convolutional
layer the units are organized into planes, each of which is called a feature map. units
in a feature map each take inputs only from a small subregion of the image, and all
of the units in a feature map are constrained to share the same weight values. for
instance, a feature map might consist of 100 units arranged in a 10    10 grid, with
each unit taking inputs from a 5  5 pixel patch of the image. the whole feature map
therefore has 25 adjustable weight parameters plus one adjustable bias parameter.
input values from a patch are linearly combined using the weights and the bias, and
the result transformed by a sigmoidal nonlinearity using (5.1). if we think of the units
as feature detectors, then all of the units in a feature map detect the same pattern but
at different locations in the input image. due to the weight sharing, the evaluation
of the activations of these units is equivalent to a convolution of the image pixel
intensities with a    kernel    comprising the weight parameters. if the input image is
shifted, the activations of the feature map will be shifted by the same amount but will
otherwise be unchanged. this provides the basis for the (approximate) invariance of

5.5. id173 in neural networks

269

the network outputs to translations and distortions of the input image. because we
will typically need to detect multiple features in order to build an effective model,
there will generally be multiple feature maps in the convolutional layer, each having
its own set of weight and bias parameters.

the outputs of the convolutional units form the inputs to the subsampling layer
of the network. for each feature map in the convolutional layer, there is a plane of
units in the subsampling layer and each unit takes inputs from a small receptive    eld
in the corresponding feature map of the convolutional layer. these units perform
subsampling. for instance, each subsampling unit might take inputs from a 2    2
unit region in the corresponding feature map and would compute the average of
those inputs, multiplied by an adaptive weight with the addition of an adaptive bias
parameter, and then transformed using a sigmoidal nonlinear activation function.
the receptive    elds are chosen to be contiguous and nonoverlapping so that there
are half the number of rows and columns in the subsampling layer compared with
the convolutional layer. in this way, the response of a unit in the subsampling layer
will be relatively insensitive to small shifts of the image in the corresponding regions
of the input space.

in a practical architecture, there may be several pairs of convolutional and sub-
sampling layers. at each stage there is a larger degree of invariance to input trans-
formations compared to the previous layer. there may be several feature maps in a
given convolutional layer for each plane of units in the previous subsampling layer,
so that the gradual reduction in spatial resolution is then compensated by an increas-
ing number of features. the    nal layer of the network would typically be a fully
connected, fully adaptive layer, with a softmax output nonlinearity in the case of
multiclass classi   cation.

the whole network can be trained by error minimization using id26
to evaluate the gradient of the error function. this involves a slight modi   cation
of the usual id26 algorithm to ensure that the shared-weight constraints
are satis   ed. due to the use of local receptive    elds, the number of weights in
the network is smaller than if the network were fully connected. furthermore, the
number of independent parameters to be learned from the data is much smaller still,
due to the substantial numbers of constraints on the weights.

5.5.7 soft weight sharing
one way to reduce the effective complexity of a network with a large number
of weights is to constrain weights within certain groups to be equal. this is the
technique of weight sharing that was discussed in section 5.5.6 as a way of building
translation invariance into networks used for image interpretation. it is only appli-
cable, however, to particular problems in which the form of the constraints can be
speci   ed in advance. here we consider a form of soft weight sharing (nowlan and
hinton, 1992) in which the hard constraint of equal weights is replaced by a form
of id173 in which groups of weights are encouraged to have similar values.
furthermore, the division of weights into groups, the mean weight value for each
group, and the spread of values within the groups are all determined as part of the
learning process.

exercise 5.28

270

5. neural networks

section 2.3.9

exercise 5.29

recall that the simple weight decay regularizer, given in (5.112), can be viewed
as the negative log of a gaussian prior distribution over the weights. we can encour-
age the weight values to form several groups, rather than just one group, by consid-
ering instead a id203 distribution that is a mixture of gaussians. the centres
and variances of the gaussian components, as well as the mixing coef   cients, will be
considered as adjustable parameters to be determined as part of the learning process.
thus, we have a id203 density of the form

(cid:14)

p(w) =

p(wi)

where

i

  jn (wi|  j,   2
j )

m(cid:2)

j=1

p(wi) =

(5.136)

(5.137)

and   j are the mixing coef   cients. taking the negative logarithm then leads to a
id173 function of the form

(cid:2)

(cid:22)
m(cid:2)

(cid:23)

   (w) =    

ln

  jn (wi|  j,   2
j )

.

(5.138)

i

j=1

the total error function is then given by

(cid:4)e(w) = e(w) +      (w)

(5.139)

where    is the id173 coef   cient. this error is minimized both with respect
to the weights wi and with respect to the parameters {  j,   j,   j} of the mixture
model. if the weights were constant, then the parameters of the mixture model could
be determined by using the em algorithm discussed in chapter 9. however, the dis-
tribution of weights is itself evolving during the learning process, and so to avoid nu-
merical instability, a joint optimization is performed simultaneously over the weights
and the mixture-model parameters. this can be done using a standard optimization
algorithm such as conjugate gradients or quasi-id77s.

in order to minimize the total error function, it is necessary to be able to evaluate
its derivatives with respect to the various adjustable parameters. to do this it is con-
venient to regard the {  j} as prior probabilities and to introduce the corresponding
posterior probabilities which, following (2.192), are given by bayes    theorem in the
form

  j(w) =

(5.140)

the derivatives of the total error function with respect to the weights are then given
by

(cid:5)
  jn (w|  j,   2
j )
k   kn (w|  k,   2
k) .
(cid:2)

=    e
   wi

+   

  j(wi)

j

(wi       j)

  2
j

   (cid:4)e

   wi

.

(5.141)

5.5. id173 in neural networks

271

the effect of the id173 term is therefore to pull each weight towards the
centre of the jth gaussian, with a force proportional to the posterior id203 of
that gaussian for the given weight. this is precisely the kind of effect that we are
seeking.

derivatives of the error with respect to the centres of the gaussians are also

   (cid:4)e

     j

(cid:2)

i

=   

  j(wi)

(  i     wj)

  2
j

(5.142)

exercise 5.30

easily computed to give

exercise 5.31

which has a simple intuitive interpretation, because it pushes   j towards an aver-
age of the weight values, weighted by the posterior probabilities that the respective
weight parameters were generated by component j. similarly, the derivatives with
respect to the variances are given by

(cid:15)

   (cid:4)e

     j

(cid:2)

i

=   

  j(wi)

    (wi       j)2

  3
j

1
  j

(cid:16)

(5.143)

which drives   j towards the weighted average of the squared deviations of the weights
around the corresponding centre   j, where the weighting coef   cients are again given
by the posterior id203 that each weight is generated by component j. note that
in a practical implementation, new variables   j de   ned by

  2
j = exp(  j)

(5.144)

are introduced, and the minimization is performed with respect to the   j. this en-
sures that the parameters   j remain positive. it also has the effect of discouraging
pathological solutions in which one or more of the   j goes to zero, corresponding
to a gaussian component collapsing onto one of the weight parameter values. such
solutions are discussed in more detail in the context of gaussian mixture models in
section 9.2.1.

for the derivatives with respect to the mixing coef   cients   j, we need to take

account of the constraints (cid:2)

  j = 1,

0 (cid:1)   i (cid:1) 1

(5.145)

j

which follow from the interpretation of the   j as prior probabilities. this can be
done by expressing the mixing coef   cients in terms of a set of auxiliary variables
{  j} using the softmax function given by

(cid:5)m

  j =

exp(  j)
k=1 exp(  k)

.

(5.146)

exercise 5.32

the derivatives of the regularized error function with respect to the {  j} then take
the form

272

5. neural networks

figure 5.18 the left    gure shows a two-link robot arm,
in which the cartesian coordinates (x1, x2) of the end ef-
fector are determined uniquely by the two joint angles   1
and   2 and the (   xed) lengths l1 and l2 of the arms. this
is know as the forward kinematics of the arm.
in prac-
tice, we have to    nd the joint angles that will give rise to a
desired end effector position and, as shown in the right    g-
ure, this inversekinematicshas two solutions correspond-
ing to    elbow up    and    elbow down   .

   (cid:4)e

     j

(x1, x2)

(x1, x2)

l2

  2

elbow
up

l1

  1

{  j       j(wi)} .

(cid:2)

i

=

elbow
down

(5.147)

we see that   j is therefore driven towards the average posterior id203 for com-
ponent j.

5.6. mixture density networks

exercise 5.33

the goal of supervised learning is to model a conditional distribution p(t|x), which
for many simple regression problems is chosen to be gaussian. however, practical
machine learning problems can often have signi   cantly non-gaussian distributions.
these can arise, for example, with inverse problems in which the distribution can be
multimodal, in which case the gaussian assumption can lead to very poor predic-
tions.

as a simple example of an inverse problem, consider the kinematics of a robot
arm, as illustrated in figure 5.18. the forward problem involves    nding the end ef-
fector position given the joint angles and has a unique solution. however, in practice
we wish to move the end effector of the robot to a speci   c position, and to do this we
must set appropriate joint angles. we therefore need to solve the inverse problem,
which has two solutions as seen in figure 5.18.

forward problems often corresponds to causality in a physical system and gen-
erally have a unique solution. for instance, a speci   c pattern of symptoms in the
human body may be caused by the presence of a particular disease. in pattern recog-
nition, however, we typically have to solve an inverse problem, such as trying to
predict the presence of a disease given a set of symptoms. if the forward problem
involves a many-to-one mapping, then the inverse problem will have multiple solu-
tions. for instance, several different diseases may result in the same symptoms.

in the robotics example, the kinematics is de   ned by geometrical equations, and
the multimodality is readily apparent. however, in many machine learning problems
the presence of multimodality, particularly in problems involving spaces of high di-
mensionality, can be less obvious. for tutorial purposes, however, we shall consider
a simple toy problem for which we can easily visualize the multimodality. data for
this problem is generated by sampling a variable x uniformly over the interval (0, 1),
to give a set of values {xn}, and the corresponding target values tn are obtained

figure 5.19 on the left is the data
set for a simple    forward problem    in
which the red curve shows the result
of    tting a two-layer neural network
by minimizing the sum-of-squares
error function. the corresponding
inverse problem, shown on the right,
is obtained by exchanging the roles
of x and t. here the same net-
work trained again by minimizing the
sum-of-squares error function gives
a very poor    t to the data due to the
multimodality of the data set.

1

0

5.6. mixture density networks

273

1

0

0

1

0

1

by computing the function xn + 0.3 sin(2  xn) and then adding uniform noise over
the interval (   0.1, 0.1). the inverse problem is then obtained by keeping the same
data points but exchanging the roles of x and t. figure 5.19 shows the data sets for
the forward and inverse problems, along with the results of    tting two-layer neural
networks having 6 hidden units and a single linear output unit by minimizing a sum-
of-squares error function. least squares corresponds to maximum likelihood under
a gaussian assumption. we see that this leads to a very poor model for the highly
non-gaussian inverse problem.
we therefore seek a general framework for modelling id155
distributions. this can be achieved by using a mixture model for p(t|x) in which
both the mixing coef   cients as well as the component densities are    exible functions
of the input vector x, giving rise to the mixture density network. for any given value
of x, the mixture model provides a general formalism for modelling an arbitrary
conditional density function p(t|x). provided we consider a suf   ciently    exible
network, we then have a framework for approximating arbitrary conditional distri-
butions.

here we shall develop the model explicitly for gaussian components, so that

p(t|x) =

t|  k(x),   2

k(x)

.

(5.148)

k=1

this is an example of a heteroscedastic model since the noise variance on the data
is a function of the input vector x. instead of gaussians, we can use other distribu-
tions for the components, such as bernoulli distributions if the target variables are
binary rather than continuous. we have also specialized to the case of isotropic co-
variances for the components, although the mixture density network can readily be
extended to allow for general covariance matrices by representing the covariances
using a cholesky factorization (williams, 1996). even with isotropic components,
the conditional distribution p(t|x) does not assume factorization with respect to the
components of t (in contrast to the standard sum-of-squares regression model) as a
consequence of the mixture distribution.

we now take the various parameters of the mixture model, namely the mixing
k(x), to be governed by

coef   cients   k(x), the means   k(x), and the variances   2

k(cid:2)

  k(x)n(cid:10)

(cid:11)

274

5. neural networks

xd

x1

p(t|x)

  m

  1

  

figure 5.20 the mixturedensitynetwork can represent general id155 densities p(t|x)
by considering a parametric mixture model for the distribution of t whose parameters are
determined by the outputs of a neural network that takes x as its input vector.

t

the outputs of a conventional neural network that takes x as its input. the structure
of this mixture density network is illustrated in figure 5.20. the mixture density
network is closely related to the mixture of experts discussed in section 14.5.3. the
principle difference is that in the mixture density network the same function is used
to predict the parameters of all of the component densities as well as the mixing co-
ef   cients, and so the nonlinear hidden units are shared amongst the input-dependent
functions.

the neural network in figure 5.20 can, for example, be a two-layer network
having sigmoidal (   tanh   ) hidden units. if there are l components in the mixture
model (5.148), and if t has k components, then the network will have l output unit
activations denoted by a  
k that determine the mixing coef   cients   k(x), k outputs
k that determine the kernel widths   k(x), and l    k outputs denoted
denoted by a  
  
kj that determine the components   kj(x) of the kernel centres   k(x). the total
by a
number of network outputs is given by (k + 2)l, as compared with the usual k
outputs for a network, which simply predicts the conditional means of the target
variables.

the mixing coef   cients must satisfy the constraints

  k(x) = 1,

0 (cid:1)   k(x) (cid:1) 1

(5.149)

k(cid:2)

k=1

which can be achieved using a set of softmax outputs

(cid:5)k

exp(a  
k)
l=1 exp(a  
l )

  k(x) =

.

(5.150)

similarly, the variances must satisfy   2
of the exponentials of the corresponding network activations using

k(x) (cid:2) 0 and so can be represented in terms

  k(x) = exp(a  

k).

(5.151)

finally, because the means   k(x) have real components, they can be represented

5.6. mixture density networks

275

directly by the network output activations

  
  kj(x) = a
kj.

(5.152)

the adaptive parameters of the mixture density network comprise the vector w
of weights and biases in the neural network, that can be set by maximum likelihood,
or equivalently by minimizing an error function de   ned to be the negative logarithm
of the likelihood. for independent data, this error function takes the form

e(w) =     n(cid:2)

ln

(cid:24)

k(cid:2)

  k(xn, w)n(cid:10)

tn|  k(xn, w),   2

k(xn, w)

(5.153)

(cid:11)(cid:25)

n=1

k=1

where we have made the dependencies on w explicit.

in order to minimize the error function, we need to calculate the derivatives of
the error e(w) with respect to the components of w. these can be evaluated by
using the standard id26 procedure, provided we obtain suitable expres-
sions for the derivatives of the error with respect to the output-unit activations. these
represent error signals    for each pattern and for each output unit, and can be back-
propagated to the hidden units and the error function derivatives evaluated in the
usual way. because the error function (5.153) is composed of a sum of terms, one
for each training data point, we can consider the derivatives for a particular pattern
n and then    nd the derivatives of e by summing over all patterns.

because we are dealing with mixture distributions, it is convenient to view the
mixing coef   cients   k(x) as x-dependent prior probabilities and to introduce the
corresponding posterior probabilities given by

(cid:5)k

  k(t|x) =   knnk
l=1   lnnl

(5.154)

where nnk denotes n (tn|  k(xn),   2

k(xn)).

the derivatives with respect to the network output activations governing the mix-

exercise 5.34

ing coef   cients are given by

   en
   a  
k

=   k       k.

(5.155)

similarly, the derivatives with respect to the output activations controlling the com-
ponent means are given by

(cid:12)

(cid:13)

   en
  
   a
kl

=   k

  kl     tl

  2
k

(cid:12)(cid:5)t       k(cid:5)2

  3
k

    1
  k

(cid:13)

=      k

   en
   a  
k

.

(5.156)

.

(5.157)

finally, the derivatives with respect to the output activations controlling the compo-
nent variances are given by

exercise 5.35

exercise 5.36

276

5. neural networks

figure 5.21 (a) plot of the mixing
coef   cients   k(x) as a function of
x for the three id81s in a
mixture density network trained on
the data shown in figure 5.19. the
model has three gaussian compo-
nents, and uses a two-layer multi-
layer id88 with    ve    tanh    sig-
moidal units in the hidden layer, and
nine outputs (corresponding to the 3
means and 3 variances of the gaus-
sian components and the 3 mixing
coef   cients). at both small and large
values of x, where the conditional
id203 density of the target data
is unimodal, only one of
the ker-
nels has a high value for its prior
id203, while at intermediate val-
ues of x, where the conditional den-
sity is trimodal, the three mixing co-
ef   cients have comparable values.
(b) plots of the means   k(x) using
the same colour coding as for the
mixing coef   cients.
(c) plot of the
contours of the corresponding con-
ditional id203 density of the tar-
get data for the same mixture den-
sity network.
the ap-
proximate conditional mode, shown
by the red points, of the conditional
density.

(d) plot of

1

0

1

0

0

0

1

(a)

0

(b)

1

0

1

0

1

0

(c)

(d)

1

1

we illustrate the use of a mixture density network by returning to the toy ex-
ample of an inverse problem shown in figure 5.19. plots of the mixing coef   -
cients   k(x), the means   k(x), and the conditional density contours corresponding
to p(t|x), are shown in figure 5.21. the outputs of the neural network, and hence the
parameters in the mixture model, are necessarily continuous single-valued functions
of the input variables. however, we see from figure 5.21(c) that the model is able to
produce a conditional density that is unimodal for some values of x and trimodal for
other values by modulating the amplitudes of the mixing components   k(x).

once a mixture density network has been trained, it can predict the conditional
density function of the target data for any given value of the input vector. this
conditional density represents a complete description of the generator of the data, so
far as the problem of predicting the value of the output vector is concerned. from
this density function we can calculate more speci   c quantities that may be of interest
in different applications. one of the simplest of these is the mean, corresponding to
the conditional average of the target data, and is given by

e [t|x] =

tp(t|x) dt =

  k(x)  k(x)

(5.158)

(cid:6)

k(cid:2)

k=1

5.7. bayesian neural networks

277

exercise 5.37

tional average, to give

where we have used (5.148). because a standard network trained by least squares
is approximating the conditional mean, we see that a mixture density network can
reproduce the conventional least-squares result as a special case. of course, as we
have already noted, for a multimodal distribution the conditional mean is of limited
value.

we can similarly evaluate the variance of the density function about the condi-

s2(x) = e

(cid:8)(cid:5)t     e[t|x](cid:5)2 |x
(cid:9)
           2
k(cid:2)

  k(x)

k(x) +

k=1

=

                 k(x)     k(cid:2)

l=1

               2

  l(x)  l(x)

(5.159)

          (5.160)

where we have used (5.148) and (5.158). this is more general than the corresponding
least-squares result because the variance is a function of x.

we have seen that for multimodal distributions, the conditional mean can give
a poor representation of the data. for instance, in controlling the simple robot arm
shown in figure 5.18, we need to pick one of the two possible joint angle settings
in order to achieve the desired end-effector location, whereas the average of the two
solutions is not itself a solution.
in such cases, the conditional mode may be of
more value. because the conditional mode for the mixture density network does not
have a simple analytical solution, this would require numerical iteration. a simple
alternative is to take the mean of the most probable component (i.e., the one with the
largest mixing coef   cient) at each value of x. this is shown for the toy data set in
figure 5.21(d).

5.7. bayesian neural networks

so far, our discussion of neural networks has focussed on the use of maximum like-
lihood to determine the network parameters (weights and biases). regularized max-
imum likelihood can be interpreted as a map (maximum posterior) approach in
which the regularizer can be viewed as the logarithm of a prior parameter distribu-
tion. however, in a bayesian treatment we need to marginalize over the distribution
of parameters in order to make predictions.

in section 3.3, we developed a bayesian solution for a simple id75
model under the assumption of gaussian noise. we saw that the posterior distribu-
tion, which is gaussian, could be evaluated exactly and that the predictive distribu-
tion could also be found in closed form. in the case of a multilayered network, the
highly nonlinear dependence of the network function on the parameter values means
that an exact bayesian treatment can no longer be found. in fact, the log of the pos-
terior distribution will be nonconvex, corresponding to the multiple local minima in
the error function.

the technique of variational id136, to be discussed in chapter 10, has been
applied to bayesian neural networks using a factorized gaussian approximation

278

5. neural networks

to the posterior distribution (hinton and van camp, 1993) and also using a full-
covariance gaussian (barber and bishop, 1998a; barber and bishop, 1998b). the
most complete treatment, however, has been based on the laplace approximation
(mackay, 1992c; mackay, 1992b) and forms the basis for the discussion given here.
we will approximate the posterior distribution by a gaussian, centred at a mode of
the true posterior. furthermore, we shall assume that the covariance of this gaus-
sian is small so that the network function is approximately linear with respect to the
parameters over the region of parameter space for which the posterior id203 is
signi   cantly nonzero. with these two approximations, we will obtain models that
are analogous to the id75 and classi   cation models discussed in earlier
chapters and so we can exploit the results obtained there. we can then make use of
the evidence framework to provide point estimates for the hyperparameters and to
compare alternative models (for example, networks having different numbers of hid-
den units). to start with, we shall discuss the regression case and then later consider
the modi   cations needed for solving classi   cation tasks.

5.7.1 posterior parameter distribution
consider the problem of predicting a single continuous target variable t from
a vector x of inputs (the extension to multiple targets is straightforward). we shall
suppose that the conditional distribution p(t|x) is gaussian, with an x-dependent
mean given by the output of a neural network model y(x, w), and with precision
(inverse variance)   

p(t|x, w,   ) = n (t|y(x, w),   

   1).

(5.161)

n(cid:14)

similarly, we shall choose a prior distribution over the weights w that is gaussian of
the form

(5.162)
for an i.i.d. data set of n observations x1, . . . , xn , with a corresponding set of target
values d = {t1, . . . , tn}, the likelihood function is given by

p(w|  ) = n (w|0,   

   1i).

p(d|w,   ) =

n (tn|y(xn, w),   

   1)

n=1

and so the resulting posterior distribution is then

p(w|d,   ,   )     p(w|  )p(d|w,   ).

(5.163)

(5.164)

which, as a consequence of the nonlinear dependence of y(x, w) on w, will be non-
gaussian.

we can    nd a gaussian approximation to the posterior distribution by using the
laplace approximation. to do this, we must    rst    nd a (local) maximum of the
posterior, and this must be done using iterative numerical optimization. as usual, it
is convenient to maximize the logarithm of the posterior, which can be written in the

5.7. bayesian neural networks

279

form

ln p(w|d) =       
2

wtw       
2

n(cid:2)

n=1

{y(xn, w)     tn}2 + const

(5.165)

which corresponds to a regularized sum-of-squares error function. assuming for
the moment that    and    are    xed, we can    nd a maximum of the posterior, which
we denote wmap, by standard nonlinear optimization algorithms such as conjugate
gradients, using error id26 to evaluate the required derivatives.

having found a mode wmap, we can then build a local gaussian approximation
by evaluating the matrix of second derivatives of the negative log posterior distribu-
tion. from (5.165), this is given by

a =           ln p(w|d,   ,   ) =   i +   h

(5.166)

where h is the hessian matrix comprising the second derivatives of the sum-of-
squares error function with respect to the components of w. algorithms for comput-
ing and approximating the hessian were discussed in section 5.4. the corresponding
gaussian approximation to the posterior is then given from (4.134) by

q(w|d) = n (w|wmap, a   1).

(5.167)

similarly, the predictive distribution is obtained by marginalizing with respect

to this posterior distribution

p(t|x,d) =

(cid:6)

p(t|x, w)q(w|d) dw.

(5.168)

however, even with the gaussian approximation to the posterior, this integration is
still analytically intractable due to the nonlinearity of the network function y(x, w)
as a function of w. to make progress, we now assume that the posterior distribution
has small variance compared with the characteristic scales of w over which y(x, w)
is varying. this allows us to make a taylor series expansion of the network function
around wmap and retain only the linear terms

y(x, w) (cid:7) y(x, wmap) + gt(w     wmap)

(5.169)

where we have de   ned

g =    wy(x, w)|w=wmap .

(5.170)
with this approximation, we now have a linear-gaussian model with a gaussian
distribution for p(w) and a gaussian for p(t|w) whose mean is a linear function of
w of the form

t|y(x, wmap) + gt(w     wmap),   

   1

.

(5.171)

p(t|x, w,   ) (cid:7) n(cid:10)

p(t|x,d,   ,   ) = n(cid:10)

t|y(x, wmap),   2(x)

(5.172)

(cid:11)

(cid:11)

exercise 5.38

we can therefore make use of the general result (2.115) for the marginal p(t) to give

280

5. neural networks

where the input-dependent variance is given by

   1 + gta   1g.

  2(x) =   

(5.173)
we see that the predictive distribution p(t|x,d) is a gaussian whose mean is given
by the network function y(x, wmap) with the parameter set to their map value. the
variance has two terms, the    rst of which arises from the intrinsic noise on the target
variable, whereas the second is an x-dependent term that expresses the uncertainty
in the interpolant due to the uncertainty in the model parameters w. this should
be compared with the corresponding predictive distribution for the id75
model, given by (3.58) and (3.59).

5.7.2 hyperparameter optimization
so far, we have assumed that the hyperparameters    and    are    xed and known.
we can make use of the evidence framework, discussed in section 3.5, together with
the gaussian approximation to the posterior obtained using the laplace approxima-
tion, to obtain a practical procedure for choosing the values of such hyperparameters.
the marginal likelihood, or evidence, for the hyperparameters is obtained by

integrating over the network weights
p(d|  ,   ) =

(cid:6)

p(d|w,   )p(w|  ) dw.

(5.174)

exercise 5.39

this is easily evaluated by making use of the laplace approximation result (4.135).
taking logarithms then gives
ln p(d|  ,   ) (cid:7)    e(wmap)     1
2

ln|a| + w
2

ln        n
2

ln    + n
2

ln(2  ) (5.175)

where w is the total number of parameters in w, and the regularized error function
is de   ned by

n(cid:2)

n=1

e(wmap) =   
2

{y(xn, wmap)     tn}2 +   
2

wt

mapwmap.

(5.176)

we see that this takes the same form as the corresponding result (3.86) for the linear
regression model.
in the evidence framework, we make point estimates for    and    by maximizing
ln p(d|  ,   ). consider    rst the maximization with respect to   , which can be done
by analogy with the id75 case discussed in section 3.5.2. we    rst de   ne
the eigenvalue equation

(5.177)
where h is the hessian matrix comprising the second derivatives of the sum-of-
squares error function, evaluated at w = wmap. by analogy with (3.92), we obtain

  hui =   iui

   =

  

wt

mapwmap

(5.178)

section 3.5.3

section 5.1.1

5.7. bayesian neural networks

281

where    represents the effective number of parameters and is de   ned by

w(cid:2)

i=1

   =

  i

   +   i

.

(5.179)

note that this result was exact for the id75 case. for the nonlinear neural
network, however, it ignores the fact that changes in    will cause changes in the
hessian h, which in turn will change the eigenvalues. we have therefore implicitly
ignored terms involving the derivatives of   i with respect to   .

similarly, from (3.95) we see that maximizing the evidence with respect to   

gives the re-estimation formula

1
  

=

1

n       

n(cid:2)

n=1

{y(xn, wmap)     tn}2.

(5.180)

as with the linear model, we need to alternate between re-estimation of the hyper-
parameters    and    and updating of the posterior distribution. the situation with
a neural network model is more complex, however, due to the multimodality of the
posterior distribution. as a consequence, the solution for wmap found by maximiz-
ing the log posterior will depend on the initialization of w. solutions that differ only
as a consequence of the interchange and sign reversal symmetries in the hidden units
are identical so far as predictions are concerned, and it is irrelevant which of the
equivalent solutions is found. however, there may be inequivalent solutions as well,
and these will generally yield different values for the optimized hyperparameters.
in order to compare different models, for example neural networks having differ-
ent numbers of hidden units, we need to evaluate the model evidence p(d). this can
be approximated by taking (5.175) and substituting the values of    and    obtained
from the iterative optimization of these hyperparameters. a more careful evaluation
is obtained by marginalizing over    and   , again by making a gaussian approxima-
tion (mackay, 1992c; bishop, 1995a). in either case, it is necessary to evaluate the
determinant |a| of the hessian matrix. this can be problematic in practice because
the determinant, unlike the trace, is sensitive to the small eigenvalues that are often
dif   cult to determine accurately.

the laplace approximation is based on a local quadratic expansion around a
mode of the posterior distribution over weights. we have seen in section 5.1.1 that
any given mode in a two-layer network is a member of a set of m!2m equivalent
modes that differ by interchange and sign-change symmetries, where m is the num-
ber of hidden units. when comparing networks having different numbers of hid-
den units, this can be taken into account by multiplying the evidence by a factor of
m!2m .

5.7.3 bayesian neural networks for classi   cation
so far, we have used the laplace approximation to develop a bayesian treat-
ment of neural network regression models. we now discuss the modi   cations to

282

5. neural networks

exercise 5.40

exercise 5.41

(cid:2)

this framework that arise when it is applied to classi   cation. here we shall con-
sider a network having a single logistic sigmoid output corresponding to a two-class
classi   cation problem. the extension to networks with multiclass softmax outputs
is straightforward. we shall build extensively on the analogous results for linear
classi   cation models discussed in section 4.5, and so we encourage the reader to
familiarize themselves with that material before studying this section.

the log likelihood function for this model is given by

ln p(d|w) =

= 1n {tn ln yn + (1     tn) ln(1     yn)}

(5.181)

n

where tn     {0, 1} are the target values, and yn     y(xn, w). note that there is no
hyperparameter   , because the data points are assumed to be correctly labelled. as
before, the prior is taken to be an isotropic gaussian of the form (5.162).

the    rst stage in applying the laplace framework to this model is to initialize
the hyperparameter   , and then to determine the parameter vector w by maximizing
the log posterior distribution. this is equivalent to minimizing the regularized error
function

e(w) =     ln p(d|w) +   
2

wtw

(5.182)

and can be achieved using error id26 combined with standard optimiza-
tion algorithms, as discussed in section 5.3.

having found a solution wmap for the weight vector, the next step is to eval-
uate the hessian matrix h comprising the second derivatives of the negative log
likelihood function. this can be done, for instance, using the exact method of sec-
tion 5.4.5, or using the outer product approximation given by (5.85). the second
derivatives of the negative log posterior can again be written in the form (5.166), and
the gaussian approximation to the posterior is then given by (5.167).

to optimize the hyperparameter   , we again maximize the marginal likelihood,

ln|a| + w
2

ln    + const

(5.183)

which is easily shown to take the form
ln p(d|  ) (cid:7)    e(wmap)     1
2

where the regularized error function is de   ned by

e(wmap) =     n(cid:2)

n=1

{tn ln yn + (1     tn) ln(1     yn)} +   
2

wt

mapwmap (5.184)

in which yn     y(xn, wmap). maximizing this evidence function with respect to   
again leads to the re-estimation equation given by (5.178).

the use of the evidence procedure to determine    is illustrated in figure 5.22

for the synthetic two-dimensional data discussed in appendix a.

finally, we need the predictive distribution, which is de   ned by (5.168). again,
this integration is intractable due to the nonlinearity of the network function. the

5.7. bayesian neural networks

283

figure 5.22 illustration of

the evidence framework
applied to a synthetic two-class data set.
the green curve shows the optimal de-
cision boundary, the black curve shows
the result of    tting a two-layer network
with 8 hidden units by maximum likeli-
hood, and the red curve shows the re-
sult of including a regularizer in which
   is optimized using the evidence pro-
cedure, starting from the initial value
   = 0. note that the evidence proce-
dure greatly reduces the over-   tting of
the network.

3

2

1

0

   1

   2

   2

   1

0

1

2

simplest approximation is to assume that the posterior distribution is very narrow
and hence make the approximation

p(t|x,d) (cid:7) p(t|x, wmap).

(5.185)

we can improve on this, however, by taking account of the variance of the posterior
distribution. in this case, a linear approximation for the network outputs, as was used
in the case of regression, would be inappropriate due to the logistic sigmoid output-
unit activation function that constrains the output to lie in the range (0, 1). instead,
we make a linear approximation for the output unit activation in the form

a(x, w) (cid:7) amap(x) + bt(w     wmap)

(5.186)
where amap(x) = a(x, wmap), and the vector b        a(x, wmap) can be found by
id26.

(cid:6)

(cid:10)

because we now have a gaussian approximation for the posterior distribution
over w, and a model for a that is a linear function of w, we can now appeal to the
results of section 4.5.2. the distribution of output unit activation values, induced by
the distribution over network weights, is given by

  

p(a|x,d) =

a     amap(x)     bt(x)(w     wmap)

q(w|d) dw (5.187)
where q(w|d) is the gaussian approximation to the posterior distribution given by
(5.167). from section 4.5.2, we see that this distribution is gaussian with mean
amap     a(x, wmap), and variance

(cid:11)

a(x) = bt(x)a   1b(x).
  2

(5.188)

finally, to obtain the predictive distribution, we must marginalize over a using

p(t = 1|x,d) =

  (a)p(a|x,d) da.

(5.189)

(cid:6)

284

5. neural networks

3

2

1

0

   1

   2

3

2

1

0

   1

   2

   2

   1

0

1

2

   2

   1

0

1

2

figure 5.23 an illustration of the laplace approximation for a bayesian neural network having 8 hidden units
with    tanh    id180 and a single logistic-sigmoid output unit. the weight parameters were found using
scaled conjugate gradients, and the hyperparameter    was optimized using the evidence framework. on the left
is the result of using the simple approximation (5.185) based on a point estimate wmap of the parameters,
in which the green curve shows the y = 0.5 decision boundary, and the other contours correspond to output
probabilities of y = 0.1, 0.3, 0.7, and 0.9. on the right is the corresponding result obtained using (5.190). note
that the effect of marginalization is to spread out the contours and to make the predictions less con   dent, so
that at each input point x, the posterior probabilities are shifted towards 0.5, while the y = 0.5 contour itself is
unaffected.

the convolution of a gaussian with a logistic sigmoid is intractable. we therefore
apply the approximation (4.153) to (5.189) giving

p(t = 1|x,d) =   

  (  2

a)btwmap

(5.190)

(cid:10)

(cid:11)

where   (  ) is de   ned by (4.154). recall that both   2

a and b are functions of x.

figure 5.23 shows an example of this framework applied to the synthetic classi-

   cation data set described in appendix a.

exercises

5.1 ((cid:12) (cid:12)) consider a two-layer network function of the form (5.7) in which the hidden-
unit nonlinear id180 g(  ) are given by logistic sigmoid functions of the
form

  (a) = {1 + exp(   a)}   1

.

(5.191)

show that there exists an equivalent network, which computes exactly the same func-
tion, but with hidden unit id180 given by tanh(a) where the tanh func-
tion is de   ned by (5.59). hint:    rst    nd the relation between   (a) and tanh(a), and
then show that the parameters of the two networks differ by linear transformations.

5.2 ((cid:12)) www show that maximizing the likelihood function under the conditional
distribution (5.16) for a multioutput neural network is equivalent to minimizing the
sum-of-squares error function (5.11).

exercises

285

5.3 ((cid:12) (cid:12)) consider a regression problem involving multiple target variables in which it
is assumed that the distribution of the targets, conditioned on the input vector x, is a
gaussian of the form

p(t|x, w) = n (t|y(x, w),   )

(5.192)
where y(x, w) is the output of a neural network with input vector x and weight
vector w, and    is the covariance of the assumed gaussian noise on the targets.
given a set of independent observations of x and t, write down the error function
that must be minimized in order to    nd the maximum likelihood solution for w, if
we assume that    is    xed and known. now assume that    is also to be determined
from the data, and write down an expression for the maximum likelihood solution
for   . note that the optimizations of w and    are now coupled, in contrast to the
case of independent target variables discussed in section 5.2.

5.4 ((cid:12) (cid:12)) consider a binary classi   cation problem in which the target values are t    
{0, 1}, with a network output y(x, w) that represents p(t = 1|x), and suppose that
there is a id203   that the class label on a training data point has been incorrectly
set. assuming independent and identically distributed data, write down the error
function corresponding to the negative log likelihood. verify that the error function
(5.21) is obtained when   = 0. note that this error function makes the model robust
to incorrectly labelled data, in contrast to the usual error function.

5.5 ((cid:12)) www show that maximizing likelihood for a multiclass neural network model
in which the network outputs have the interpretation yk(x, w) = p(tk = 1|x) is
equivalent to the minimization of the cross-id178 error function (5.24).

5.6 ((cid:12)) www show the derivative of the error function (5.21) with respect to the
activation ak for an output unit having a logistic sigmoid activation function satis   es
(5.18).

5.7 ((cid:12)) show the derivative of the error function (5.24) with respect to the activation ak

for output units having a softmax activation function satis   es (5.18).

5.8 ((cid:12)) we saw in (4.88) that the derivative of the logistic sigmoid activation function
can be expressed in terms of the function value itself. derive the corresponding result
for the    tanh    activation function de   ned by (5.59).

5.9 ((cid:12)) www the error function (5.21) for binary classi   cation problems was de-
rived for a network having a logistic-sigmoid output activation function, so that
0 (cid:1) y(x, w) (cid:1) 1, and data having target values t     {0, 1}. derive the correspond-
ing error function if we consider a network having an output    1 (cid:1) y(x, w) (cid:1) 1
and target values t = 1 for class c1 and t =    1 for class c2. what would be the
appropriate choice of output unit activation function?

5.10 ((cid:12)) www consider a hessian matrix h with eigenvector equation (5.33). by
setting the vector v in (5.39) equal to each of the eigenvectors ui in turn, show that
h is positive de   nite if, and only if, all of its eigenvalues are positive.

286

5. neural networks

5.11 ((cid:12) (cid:12)) www consider a quadratic error function de   ned by (5.32), in which the
hessian matrix h has an eigenvalue equation given by (5.33). show that the con-
tours of constant error are ellipses whose axes are aligned with the eigenvectors ui,
with lengths that are inversely proportional to the square root of the corresponding
eigenvalues   i.

5.12 ((cid:12) (cid:12)) www by considering the local taylor expansion (5.32) of an error function
about a stationary point w(cid:1), show that the necessary and suf   cient condition for the
stationary point to be a local minimum of the error function is that the hessian matrix

h, de   ned by (5.30) with(cid:1)w = w(cid:1), be positive de   nite.

5.13 ((cid:12))

show that as a consequence of the symmetry of the hessian matrix h, the
number of independent elements in the quadratic error function (5.28) is given by
w (w + 3)/2.

5.14 ((cid:12)) by making a taylor expansion, verify that the terms that are o( ) cancel on the

right-hand side of (5.69).

5.15 ((cid:12) (cid:12)) in section 5.3.4, we derived a procedure for evaluating the jacobian matrix of a
neural network using a id26 procedure. derive an alternative formalism
for    nding the jacobian based on forward propagation equations.

5.16 ((cid:12)) the outer product approximation to the hessian matrix for a neural network
using a sum-of-squares error function is given by (5.84). extend this result to the
case of multiple outputs.

5.17 ((cid:12)) consider a squared id168 of the form
{y(x, w)     t}2

e =

p(x, t) dx dt

(5.193)

(cid:6)(cid:6)

1
2

where y(x, w) is a parametric function such as a neural network. the result (1.89)
shows that the function y(x, w) that minimizes this error is given by the conditional
expectation of t given x. use this result to show that the second derivative of e with
respect to two elements wr and ws of the vector w, is given by

(cid:6)

   2e

   wr   ws

=

   y
   wr

   y
   ws

p(x) dx.

(5.194)

note that, for a    nite sample from p(x), we obtain (5.84).

5.18 ((cid:12)) consider a two-layer network of the form shown in figure 5.1 with the addition
of extra parameters corresponding to skip-layer connections that go directly from
the inputs to the outputs. by extending the discussion of section 5.3.2, write down
the equations for the derivatives of the error function with respect to these additional
parameters.

5.19 ((cid:12)) www derive the expression (5.85) for the outer product approximation to
the hessian matrix for a network having a single output with a logistic sigmoid
output-unit activation function and a cross-id178 error function, corresponding to
the result (5.84) for the sum-of-squares error function.

exercises

287

5.20 ((cid:12)) derive an expression for the outer product approximation to the hessian matrix
for a network having k outputs with a softmax output-unit activation function and
a cross-id178 error function, corresponding to the result (5.84) for the sum-of-
squares error function.

5.21 ((cid:12) (cid:12) (cid:12)) extend the expression (5.86) for the outer product approximation of the hes-
sian matrix to the case of k > 1 output units. hence, derive a recursive expression
analogous to (5.87) for incrementing the number n of patterns and a similar expres-
sion for incrementing the number k of outputs. use these results, together with the
identity (5.88), to    nd sequential update expressions analogous to (5.89) for    nding
the inverse of the hessian by incrementally including both extra patterns and extra
outputs.

5.22 ((cid:12) (cid:12)) derive the results (5.93), (5.94), and (5.95) for the elements of the hessian
matrix of a two-layer feed-forward network by application of the chain rule of cal-
culus.

5.23 ((cid:12) (cid:12)) extend the results of section 5.4.5 for the exact hessian of a two-layer network

to include skip-layer connections that go directly from inputs to outputs.

5.24 ((cid:12)) verify that the network function de   ned by (5.113) and (5.114) is invariant un-
der the transformation (5.115) applied to the inputs, provided the weights and biases
are simultaneously transformed using (5.116) and (5.117). similarly, show that the
network outputs can be transformed according (5.118) by applying the transforma-
tion (5.119) and (5.120) to the second-layer weights and biases.

5.25 ((cid:12) (cid:12) (cid:12)) www consider a quadratic error function of the form
(w     w(cid:1))th(w     w(cid:1))

e = e0 +

1
2

(5.195)

where w(cid:1) represents the minimum, and the hessian matrix h is positive de   nite and
constant. suppose the initial weight vector w(0) is chosen to be at the origin and is
updated using simple id119

w(   ) = w(     1)          e

(5.196)

where    denotes the step number, and    is the learning rate (which is assumed to be
small). show that, after    steps, the components of the weight vector parallel to the
eigenvectors of h can be written

(   )

j = {1     (1         j)  } w(cid:1)

j

w

(5.197)

where wj = wtuj, and uj and   j are the eigenvectors and eigenvalues, respectively,
of h so that

(5.198)
show that as           , this gives w(   )     w(cid:1) as expected, provided |1         j| < 1.
now suppose that training is halted after a    nite number    of steps. show that the

huj =   juj.

288

5. neural networks

components of the weight vector parallel to the eigenvectors of the hessian satisfy

(   )

j (cid:7) w(cid:1)
| (cid:13) |w(cid:1)

j when   j (cid:12) (    )   1
j| when   j (cid:13) (    )   1.

w
(   )
j

|w

(5.199)

(5.200)

compare this result with the discussion in section 3.5.3 of id173 with simple
weight decay, and hence show that (    )   1 is analogous to the id173 param-
eter   . the above results also show that the effective number of parameters in the
network, as de   ned by (3.91), grows as the training progresses.

5.26 ((cid:12) (cid:12)) consider a multilayer id88 with arbitrary feed-forward topology, which
is to be trained by minimizing the tangent propagation error function (5.127) in
which the regularizing function is given by (5.128). show that the id173
term     can be written as a sum over patterns of terms of the form

(cid:2)
(cid:2)
where g is a differential operator de   ned by

   n =

1
2

k

(gyk)2

g    

   
   xi

  i

i

by acting on the forward propagation equations

.

(cid:2)

(cid:2)

zj = h(aj),

(5.203)
with the operator g, show that    n can be evaluated by forward propagation using
the following equations:

wjizi

aj =

i

  j = h

(cid:4)(aj)  j,

  j =

wji  i.

i

(5.204)

where we have de   ned the new variables

  j     gzj,

  j     gaj.

(5.205)
now show that the derivatives of    n with respect to a weight wrs in the network can
be written in the form

(cid:2)

k

      n
   wrs

=

where we have de   ned

  k {  krzs +   kr  s}

  kr        yk
   ar

,

  kr     g  kr.

write down the id26 equations for   kr, and hence derive a set of back-
propagation equations for the evaluation of the   kr.

(5.201)

(5.202)

(5.206)

(5.207)

exercises

289

5.27 ((cid:12) (cid:12)) www consider the framework for training with transformed data in the
special case in which the transformation consists simply of the addition of random
noise x     x +    where    has a gaussian distribution with zero mean and unit
covariance. by following an argument analogous to that of section 5.5.5, show that
the resulting regularizer reduces to the tikhonov form (5.135).

5.28 ((cid:12)) www consider a neural network, such as the convolutional network discussed
in section 5.5.6, in which multiple weights are constrained to have the same value.
discuss how the standard id26 algorithm must be modi   ed in order to
ensure that such constraints are satis   ed when evaluating the derivatives of an error
function with respect to the adjustable parameters in the network.

5.29 ((cid:12)) www verify the result (5.141).

5.30 ((cid:12)) verify the result (5.142).

5.31 ((cid:12)) verify the result (5.143).
5.32 ((cid:12) (cid:12)) show that the derivatives of the mixing coef   cients {  k}, de   ned by (5.146),

with respect to the auxiliary parameters {  j} are given by

=   jk  j       j  k.

     k
     j

(cid:5)

(5.208)

hence, by making use of the constraint

k   k = 1, derive the result (5.147).

5.33 ((cid:12)) write down a pair of equations that express the cartesian coordinates (x1, x2)
for the robot arm shown in figure 5.18 in terms of the joint angles   1 and   2 and
the lengths l1 and l2 of the links. assume the origin of the coordinate system is
given by the attachment point of the lower arm. these equations de   ne the    forward
kinematics    of the robot arm.

5.34 ((cid:12)) www derive the result (5.155) for the derivative of the error function with
respect to the network output activations controlling the mixing coef   cients in the
mixture density network.

5.35 ((cid:12)) derive the result (5.156) for the derivative of the error function with respect
to the network output activations controlling the component means in the mixture
density network.

5.36 ((cid:12)) derive the result (5.157) for the derivative of the error function with respect to
the network output activations controlling the component variances in the mixture
density network.

5.37 ((cid:12)) verify the results (5.158) and (5.160) for the conditional mean and variance of

the mixture density network model.

5.38 ((cid:12)) using the general result (2.115), derive the predictive distribution (5.172) for

the laplace approximation to the bayesian neural network model.

290

5. neural networks

5.39 ((cid:12)) www make use of the laplace approximation result (4.135) to show that the
evidence function for the hyperparameters    and    in the bayesian neural network
model can be approximated by (5.175).

5.40 ((cid:12)) www outline the modi   cations needed to the framework for bayesian neural
networks, discussed in section 5.7.3, to handle multiclass problems using networks
having softmax output-unit id180.

5.41 ((cid:12) (cid:12)) by following analogous steps to those given in section 5.7.1 for regression
networks, derive the result (5.183) for the marginal likelihood in the case of a net-
work having a cross-id178 error function and logistic-sigmoid output-unit activa-
tion function.

6

kernel
methods

in chapters 3 and 4, we considered linear parametric models for regression and
classi   cation in which the form of the mapping y(x, w) from input x to output y
is governed by a vector w of adaptive parameters. during the learning phase, a
set of training data is used either to obtain a point estimate of the parameter vector
or to determine a posterior distribution over this vector. the training data is then
discarded, and predictions for new inputs are based purely on the learned parameter
vector w. this approach is also used in nonlinear parametric models such as neural
networks.

however, there is a class of pattern recognition techniques, in which the training
data points, or a subset of them, are kept and used also during the prediction phase.
for instance, the parzen id203 density model comprised a linear combination
of    kernel    functions each one centred on one of the training data points. similarly,
in section 2.5.2 we introduced a simple technique for classi   cation called nearest
neighbours, which involved assigning to each new test vector the same label as the

chapter 5

section 2.5.1

291

292

6. kernel methods

closest example from the training set. these are examples of memory-based methods
that involve storing the entire training set in order to make predictions for future data
points. they typically require a metric to be de   ned that measures the similarity of
any two vectors in input space, and are generally fast to    train    but slow at making
predictions for test data points.

many linear parametric models can be re-cast into an equivalent    dual represen-
tation    in which the predictions are also based on linear combinations of a kernel
function evaluated at the training data points. as we shall see, for models which are
based on a    xed nonlinear feature space mapping   (x), the id81 is given
by the relation

k(x, x(cid:4)) =   (x)t  (x(cid:4)).

(6.1)

from this de   nition, we see that the kernel is a symmetric function of its arguments
so that k(x, x(cid:4)) = k(x(cid:4)
, x). the kernel concept was introduced into the    eld of pat-
tern recognition by aizerman et al. (1964) in the context of the method of potential
functions, so-called because of an analogy with electrostatics. although neglected
for many years, it was re-introduced into machine learning in the context of large-
margin classi   ers by boser et al.
(1992) giving rise to the technique of support
vector machines. since then, there has been considerable interest in this topic, both
in terms of theory and applications. one of the most signi   cant developments has
been the extension of kernels to handle symbolic objects, thereby greatly expanding
the range of problems that can be addressed.
the simplest example of a id81 is obtained by considering the identity
mapping for the feature space in (6.1) so that   (x) = x, in which case k(x, x(cid:4)) =
xtx(cid:4)

. we shall refer to this as the linear kernel.
the concept of a kernel formulated as an inner product in a feature space allows
us to build interesting extensions of many well-known algorithms by making use of
the kernel trick, also known as kernel substitution. the general idea is that, if we have
an algorithm formulated in such a way that the input vector x enters only in the form
of scalar products, then we can replace that scalar product with some other choice of
kernel. for instance, the technique of kernel substitution can be applied to principal
component analysis in order to develop a nonlinear variant of pca (sch  olkopf et al.,
1998). other examples of kernel substitution include nearest-neighbour classi   ers
and the kernel fisher discriminant (mika et al., 1999; roth and steinhage, 2000;
baudat and anouar, 2000).

there are numerous forms of id81s in common use, and we shall en-
counter several examples in this chapter. many have the property of being a function
only of the difference between the arguments, so that k(x, x(cid:4)) = k(x     x(cid:4)), which
are known as stationary kernels because they are invariant to translations in input
space. a further specialization involves homogeneous kernels, also known as ra-
dial basis functions, which depend only on the magnitude of the distance (typically
euclidean) between the arguments so that k(x, x(cid:4)) = k((cid:5)x     x(cid:4)(cid:5)).

for recent textbooks on kernel methods, see sch  olkopf and smola (2002), her-

brich (2002), and shawe-taylor and cristianini (2004).

chapter 7

section 12.3

section 6.3

6.1. dual representations

293

6.1. dual representations

n(cid:2)

(cid:26)

n=1

(cid:27)2 +   

2

n(cid:2)

(cid:27)

many linear models for regression and classi   cation can be reformulated in terms of
a dual representation in which the id81 arises naturally. this concept will
play an important role when we consider support vector machines in the next chapter.
here we consider a id75 model whose parameters are determined by
minimizing a regularized sum-of-squares error function given by

j(w) =

1
2

wt  (xn)     tn

wtw

(6.2)

n(cid:2)

(cid:26)

(cid:27)

(cid:26)

where    (cid:2) 0. if we set the gradient of j(w) with respect to w equal to zero, we see
that the solution for w takes the form of a linear combination of the vectors   (xn),
with coef   cients that are functions of w, of the form

w =     1
  

wt  (xn)     tn

n=1

n=1

  (xn) =

an  (xn) =   ta

(6.3)

where    is the design matrix, whose nth row is given by   (xn)t. here the vector
a = (a1, . . . , an )t, and we have de   ned

an =     1
  

wt  (xn)     tn

.

(6.4)

instead of working with the parameter vector w, we can now reformulate the least-
squares algorithm in terms of the parameter vector a, giving rise to a dual represen-
tation. if we substitute w =   ta into j(w), we obtain

j(a) =

1
2

at    t    ta     at    tt +

1
2

ttt +   
2

at    ta

(6.5)

where t = (t1, . . . , tn )t. we now de   ne the gram matrix k =     t, which is an
n    n symmetric matrix with elements

knm =   (xn)t  (xm) = k(xn, xm)

(6.6)
where we have introduced the id81 k(x, x(cid:4)) de   ned by (6.1). in terms of
the gram matrix, the sum-of-squares error function can be written as

j(a) =

1
2

atkka     atkt +

1
2

ttt +   
2

atka.

(6.7)

setting the gradient of j(a) with respect to a to zero, we obtain the following solu-
tion

a = (k +   in)

   1 t.

(6.8)

294

6. kernel methods

if we substitute this back into the id75 model, we obtain the following
prediction for a new input x

y(x) = wt  (x) = at    (x) = k(x)t (k +   in)

   1 t

(6.9)

where we have de   ned the vector k(x) with elements kn(x) = k(xn, x). thus we
see that the dual formulation allows the solution to the least-squares problem to be
expressed entirely in terms of the id81 k(x, x(cid:4)). this is known as a dual
formulation because, by noting that the solution for a can be expressed as a linear
combination of the elements of   (x), we recover the original formulation in terms of
the parameter vector w. note that the prediction at x is given by a linear combination
of the target values from the training set. in fact, we have already obtained this result,
using a slightly different notation, in section 3.3.3.
in the dual formulation, we determine the parameter vector a by inverting an
n    n matrix, whereas in the original parameter space formulation we had to invert
an m    m matrix in order to determine w. because n is typically much larger
than m, the dual formulation does not seem to be particularly useful. however, the
advantage of the dual formulation, as we shall see, is that it is expressed entirely in
terms of the id81 k(x, x(cid:4)). we can therefore work directly in terms of
kernels and avoid the explicit introduction of the feature vector   (x), which allows
us implicitly to use feature spaces of high, even in   nite, dimensionality.

the existence of a dual representation based on the gram matrix is a property of
many linear models, including the id88. in section 6.4, we will develop a dual-
ity between probabilistic linear models for regression and the technique of gaussian
processes. duality will also play an important role when we discuss support vector
machines in chapter 7.

exercise 6.1

exercise 6.2

6.2. constructing kernels

in order to exploit kernel substitution, we need to be able to construct valid kernel
functions. one approach is to choose a feature space mapping   (x) and then use
this to    nd the corresponding kernel, as is illustrated in figure 6.1. here the kernel
function is de   ned for a one-dimensional input space by

k(x, x

(cid:4)) =   (x)t  (x

(cid:4)) =

(cid:4))
  i(x)  i(x

(6.10)

where   i(x) are the basis functions.

i=1

an alternative approach is to construct id81s directly. in this case,
we must ensure that the function we choose is a valid kernel, in other words that it
corresponds to a scalar product in some (perhaps in   nite dimensional) feature space.
as a simple example, consider a id81 given by

m(cid:2)

(cid:10)

(cid:11)2

k(x, z) =

xtz

.

(6.11)

6.2. constructing kernels

295

1

0.5

0

   0.5

   1

   1

0.04

0.02

0

   1

1

0.75

0.5

0.25

0

   1

0.04

0.02

0

   1

1

0.75

0.5

0.25

0
   1

0.04

0.02

0

   1

0

0

1

1

0

1

0

1

0

0

1

1

figure 6.1 illustration of the construction of id81s starting from a corresponding set of basis func-
tions. in each column the lower plot shows the id81 k(x, x(cid:1)) de   ned by (6.10) plotted as a function of
x for x(cid:1) = 0, while the upper plot shows the corresponding basis functions given by polynomials (left column),
   gaussians    (centre column), and logistic sigmoids (right column).

if we take the particular case of a two-dimensional input space x = (x1, x2) we
can expand out the terms and thereby identify the corresponding nonlinear feature
mapping

k(x, z) =

(cid:10)

(cid:11)2 = (x1z1 + x2z2)2

xtz
2z2
1 + 2x1z1x2z2 + x2
1z2
= x2
   
   
2
= (x2
2x1x2, x2
2z1z2, z2
1,
=   (x)t  (z).

2)(z2
1,

2)t
   

(6.12)

2)t and
we see that the feature mapping takes the form   (x) = (x2
1,
therefore comprises all possible second order terms, with a speci   c weighting be-
tween them.

2x1x2, x2

more generally, however, we need a simple way to test whether a function con-
stitutes a valid kernel without having to construct the function   (x) explicitly. a
necessary and suf   cient condition for a function k(x, x(cid:4)) to be a valid kernel (shawe-
taylor and cristianini, 2004) is that the gram matrix k, whose elements are given by
k(xn, xm), should be positive semide   nite for all possible choices of the set {xn}.
note that a positive semide   nite matrix is not the same thing as a matrix whose
elements are nonnegative.

one powerful technique for constructing new kernels is to build them out of
simpler kernels as building blocks. this can be done using the following properties:

appendix c

296

6. kernel methods

techniques for constructing new kernels.
given valid kernels k1(x, x(cid:4)) and k2(x, x(cid:4)), the following new kernels will also
be valid:

k(x, x(cid:4)) = ck1(x, x(cid:4))
k(x, x(cid:4)) = f(x)k1(x, x(cid:4))f(x(cid:4))
k(x, x(cid:4)) = q (k1(x, x(cid:4)))
k(x, x(cid:4)) = exp (k1(x, x(cid:4)))
k(x, x(cid:4)) = k1(x, x(cid:4)) + k2(x, x(cid:4))
k(x, x(cid:4)) = k1(x, x(cid:4))k2(x, x(cid:4))
k(x, x(cid:4)) = k3 (  (x),   (x(cid:4)))
k(x, x(cid:4)) = xtax(cid:4)
k(x, x(cid:4)) = ka(xa, x(cid:4)
k(x, x(cid:4)) = ka(xa, x(cid:4)

(6.13)
(6.14)
(6.15)
(6.16)
(6.17)
(6.18)
(6.19)
(6.20)
(6.21)
(6.22)
where c > 0 is a constant, f(  ) is any function, q(  ) is a polynomial with nonneg-
m , k3(  ,  ) is a valid kernel in
ative coef   cients,   (x) is a function from x to r
m , a is a symmetric positive semide   nite matrix, xa and xb are variables (not
r
necessarily disjoint) with x = (xa, xb), and ka and kb are valid id81s
over their respective spaces.

a) + kb(xb, x(cid:4)
b)
a)kb(xb, x(cid:4)
b)

(cid:10)

(cid:10)

(cid:11)2

equipped with these properties, we can now embark on the construction of more
complex kernels appropriate to speci   c applications. we require that the kernel
k(x, x(cid:4)) be symmetric and positive semide   nite and that it expresses the appropriate
form of similarity between x and x(cid:4)
according to the intended application. here we
consider a few common examples of id81s. for a more extensive discus-
sion of    kernel engineering   , see shawe-taylor and cristianini (2004).

we saw that the simple polynomial kernel k(x, x(cid:4)) =

contains only
if we consider the slightly generalized kernel k(x, x(cid:4)) =
with c > 0, then the corresponding feature mapping   (x) contains con-

terms of degree two.
xtx(cid:4) + c
stant and linear terms as well as terms of order two. similarly, k(x, x(cid:4)) =
contains all monomials of order m. for instance, if x and x(cid:4)
are two images, then
the kernel represents a particular weighted sum of all possible products of m pixels
in the    rst image with m pixels in the second image. this can similarly be gener-
alized to include all terms up to degree m by considering k(x, x(cid:4)) =
with c > 0. using the results (6.17) and (6.18) for combining kernels we see that
these will all be valid id81s.

xtx(cid:4)(cid:11)m
(cid:11)m

xtx(cid:4)(cid:11)2

xtx(cid:4) + c

(cid:10)

(cid:10)

another commonly used kernel takes the form

(cid:10)   (cid:5)x     x(cid:4)(cid:5)2/2  2

(cid:11)

k(x, x(cid:4)) = exp

(6.23)

and is often called a    gaussian    kernel. note, however, that in this context it is
not interpreted as a id203 density, and hence the id172 coef   cient is

6.2. constructing kernels

297

omitted. we can see that this is a valid kernel by expanding the square

to give

k(x, x(cid:4)) = exp

(cid:5)x     x(cid:4)(cid:5)2 = xtx + (x(cid:4))tx(cid:4)     2xtx(cid:4)

(cid:10)   xtx/2  2

(cid:11)

(cid:10)

exp

xtx(cid:4)

/  2

exp

(cid:11)

(cid:10)   (x(cid:4))tx(cid:4)

(cid:11)

/2  2

(6.24)

(6.25)

exercise 6.11

exercise 6.12

and then making use of (6.14) and (6.16), together with the validity of the linear
kernel k(x, x(cid:4)) = xtx(cid:4)
. note that the feature vector that corresponds to the gaussian
kernel has in   nite dimensionality.
kernel substitution in (6.24) to replace xtx(cid:4)
obtain

the gaussian kernel is not restricted to the use of euclidean distance. if we use
with a nonlinear kernel   (x, x(cid:4)), we

(cid:12)

(cid:13)

k(x, x(cid:4)) = exp

    1
2  2 (  (x, x) +   (x(cid:4)

, x(cid:4))     2  (x, x(cid:4)))

.

(6.26)

an important contribution to arise from the kernel viewpoint has been the exten-
sion to inputs that are symbolic, rather than simply vectors of real numbers. kernel
functions can be de   ned over objects as diverse as graphs, sets, strings, and text doc-
uments. consider, for instance, a    xed set and de   ne a nonvectorial space consisting
of all possible subsets of this set. if a1 and a2 are two such subsets then one simple
choice of kernel would be

k(a1, a2) = 2|a1   a2|

(6.27)
where a1     a2 denotes the intersection of sets a1 and a2, and |a| denotes the
number of subsets in a. this is a valid id81 because it can be shown to
correspond to an inner product in a feature space.

one powerful approach to the construction of kernels starts from a probabilistic
generative model (haussler, 1999), which allows us to apply generative models in a
discriminative setting. generative models can deal naturally with missing data and
in the case of id48 can handle sequences of varying length. by
contrast, discriminative models generally give better performance on discriminative
tasks than generative models. it is therefore of some interest to combine these two
approaches (lasserre et al., 2006). one way to combine them is to use a generative
model to de   ne a kernel, and then use this kernel in a discriminative approach.

given a generative model p(x) we can de   ne a kernel by

k(x, x(cid:4)) = p(x)p(x(cid:4)).

(6.28)

this is clearly a valid id81 because we can interpret it as an inner product
in the one-dimensional feature space de   ned by the mapping p(x). it says that two
inputs x and x(cid:4)
are similar if they both have high probabilities. we can use (6.13) and
(6.17) to extend this class of kernels by considering sums over products of different
id203 distributions, with positive weighting coef   cients p(i), of the form

k(x, x(cid:4)) =

p(x|i)p(x(cid:4)|i)p(i).

(6.29)

(cid:2)

i

298

6. kernel methods

section 9.2

section 13.2

exercise 6.13

this is equivalent, up to an overall multiplicative constant, to a mixture distribution
in which the components factorize, with the index i playing the role of a    latent   
variable. two inputs x and x(cid:4)
will give a large value for the id81, and
hence appear similar, if they have signi   cant id203 under a range of different
components. taking the limit of an in   nite sum, we can also consider kernels of the
form

(cid:6)

k(x, x(cid:4)) =

p(x|z)p(x(cid:4)|z)p(z) dz

(6.30)

where z is a continuous latent variable.
now suppose that our data consists of ordered sequences of length l so that
an observation is given by x = {x1, . . . , xl}. a popular generative model for
sequences is the hidden markov model, which expresses the distribution p(x) as a
marginalization over a corresponding sequence of hidden states z = {z1, . . . , zl}.
we can use this approach to de   ne a id81 measuring the similarity of two
sequences x and x(cid:4)

by extending the mixture representation (6.29) to give
k(x, x(cid:4)) =

p(x|z)p(x(cid:4)|z)p(z)

(cid:2)

(6.31)

z

so that both observed sequences are generated by the same hidden sequence z. this
model can easily be extended to allow sequences of differing length to be compared.
an alternative technique for using generative models to de   ne id81s
is known as the fisher kernel (jaakkola and haussler, 1999). consider a parametric
generative model p(x|  ) where    denotes the vector of parameters. the goal is to
   nd a kernel that measures the similarity of two input vectors x and x(cid:4)
induced by the
generative model. jaakkola and haussler (1999) consider the gradient with respect
to   , which de   nes a vector in a    feature    space having the same dimensionality as
  . in particular, they consider the fisher score

g(  , x) =       ln p(x|  )

from which the fisher kernel is de   ned by

k(x, x(cid:4)) = g(  , x)tf   1g(  , x(cid:4)).

here f is the fisher information matrix, given by

(cid:8)

(cid:9)

f = ex

g(  , x)g(  , x)t

(6.34)
where the expectation is with respect to x under the distribution p(x|  ). this can
be motivated from the perspective of information geometry (amari, 1998), which
considers the differential geometry of the space of model parameters. here we sim-
ply note that the presence of the fisher information matrix causes this kernel to be
invariant under a nonlinear re-parameterization of the density model          (  ).

in practice, it is often infeasible to evaluate the fisher information matrix. one
approach is simply to replace the expectation in the de   nition of the fisher informa-
tion with the sample average, giving

(6.32)

(6.33)

g(  , xn)g(  , xn)t.

(6.35)

n(cid:2)

n=1

f (cid:7) 1
n

6.3. id80s

299

section 12.1.3

this is the covariance matrix of the fisher scores, and so the fisher kernel corre-
sponds to a whitening of these scores. more simply, we can just omit the fisher
information matrix altogether and use the noninvariant kernel

k(x, x(cid:4)) = g(  , x)tg(  , x(cid:4)).

(6.36)

an application of fisher kernels to document retrieval is given by hofmann (2000).

a    nal example of a id81 is the sigmoidal kernel given by

(cid:10)

(cid:11)

k(x, x(cid:4)) = tanh

axtx(cid:4) + b

(6.37)

whose gram matrix in general is not positive semide   nite. this form of kernel
has, however, been used in practice (vapnik, 1995), possibly because it gives kernel
expansions such as the support vector machine a super   cial resemblance to neural
network models. as we shall see, in the limit of an in   nite number of basis functions,
a bayesian neural network with an appropriate prior reduces to a gaussian process,
thereby providing a deeper link between neural networks and kernel methods.

section 6.4.7

6.3. id80s

in chapter 3, we discussed regression models based on linear combinations of    xed
basis functions, although we did not discuss in detail what form those basis functions
might take. one choice that has been widely used is that of radial basis functions,
which have the property that each basis function depends only on the radial distance
(typically euclidean) from a centre   j, so that   j(x) = h((cid:5)x       j(cid:5)).
historically, radial basis functions were introduced for the purpose of exact func-
tion interpolation (powell, 1987). given a set of input vectors {x1, . . . , xn} along
with corresponding target values {t1, . . . , tn}, the goal is to    nd a smooth function
f(x) that    ts every target value exactly, so that f(xn) = tn for n = 1, . . . , n. this
is achieved by expressing f(x) as a linear combination of radial basis functions, one
centred on every data point

f(x) =

wnh((cid:5)x     xn(cid:5)).

(6.38)

n=1

the values of the coef   cients {wn} are found by least squares, and because there
are the same number of coef   cients as there are constraints, the result is a function
that    ts every target value exactly. in pattern recognition applications, however, the
target values are generally noisy, and exact interpolation is undesirable because this
corresponds to an over-   tted solution.

expansions in radial basis functions also arise from id173 theory (pog-
gio and girosi, 1990; bishop, 1995a). for a sum-of-squares error function with a
regularizer de   ned in terms of a differential operator, the optimal solution is given
by an expansion in the green   s functions of the operator (which are analogous to the
eigenvectors of a discrete matrix), again with one basis function centred on each data

n(cid:2)

300

6. kernel methods

point. if the differential operator is isotropic then the green   s functions depend only
on the radial distance from the corresponding data point. due to the presence of the
regularizer, the solution no longer interpolates the training data exactly.

another motivation for radial basis functions comes from a consideration of
the interpolation problem when the input (rather than the target) variables are noisy
if the noise on the input variable x is described
(webb, 1994; bishop, 1995a).
by a variable    having a distribution   (  ), then the sum-of-squares error function
becomes

{y(xn +   )     tn}2

  (  ) d  .

(6.39)

(cid:6)

n(cid:2)

n=1

e =

1
2

appendix d
exercise 6.17

using the calculus of variations, we can optimize with respect to the function f(x)
to give

n(cid:2)

y(xn) =

tnh(x     xn)

(6.40)

n=1

where the basis functions are given by

h(x     xn) =   (x     xn)
  (x     xn)

n(cid:2)

.

(6.41)

n=1

we see that there is one basis function centred on every data point. this is known as
the nadaraya-watson model and will be derived again from a different perspective
in section 6.3.1. if the noise distribution   (  ) is isotropic, so that it is a function
only of (cid:5)  (cid:5), then the basis functions will be radial.
n h(x     xn) = 1
for any value of x. the effect of such id172 is shown in figure 6.2. normal-
ization is sometimes used in practice as it avoids having regions of input space where
all of the basis functions take small values, which would necessarily lead to predic-
tions in such regions that are either small or controlled purely by the bias parameter.

note that the basis functions (6.41) are normalized, so that

(cid:5)

another situation in which expansions in normalized radial basis functions arise
is in the application of kernel density estimation to the problem of regression, as we
shall discuss in section 6.3.1.

because there is one basis function associated with every data point, the corre-
sponding model can be computationally costly to evaluate when making predictions
for new data points. models have therefore been proposed (broomhead and lowe,
1988; moody and darken, 1989; poggio and girosi, 1990), which retain the expan-
sion in radial basis functions but where the number m of basis functions is smaller
than the number n of data points. typically, the number of basis functions, and the
locations   i of their centres, are determined based on the input data {xn} alone. the
basis functions are then kept    xed and the coef   cients {wi} are determined by least
squares by solving the usual set of linear equations, as discussed in section 3.1.1.

6.3. id80s

301

1

0.8

0.6

0.4

0.2

0
   1

1

0.8

0.6

0.4

0.2

0
   1

   0.5

0

0.5

1

   0.5

0

0.5

1

figure 6.2 plot of a set of gaussian basis functions on the left, together with the corresponding normalized
basis functions on the right.

section 9.1

section 2.5.1

one of the simplest ways of choosing basis function centres is to use a randomly
chosen subset of the data points. a more systematic approach is called orthogonal
least squares (chen et al., 1991). this is a sequential selection process in which at
each step the next data point to be chosen as a basis function centre corresponds to
the one that gives the greatest reduction in the sum-of-squares error. values for the
expansion coef   cients are determined as part of the algorithm. id91 algorithms
such as id116 have also been used, which give a set of basis function centres that
no longer coincide with training data points.

6.3.1 nadaraya-watson model
in section 3.3.3, we saw that the prediction of a id75 model for a
new input x takes the form of a linear combination of the training set target values
with coef   cients given by the    equivalent kernel    (3.62) where the equivalent kernel
satis   es the summation constraint (3.64).
we can motivate the kernel regression model (3.61) from a different perspective,
starting with kernel density estimation. suppose we have a training set {xn, tn} and
we use a parzen density estimator to model the joint distribution p(x, t), so that

p(x, t) =

1
n

f(x     xn, t     tn)

(6.42)

where f(x, t) is the component density function, and there is one such component
centred on each data point. we now    nd an expression for the regression function
y(x), corresponding to the conditional average of the target variable conditioned on

n(cid:2)

n=1

302

6. kernel methods

the input variable, which is given by

y(x) = e[t|x] =

(cid:6)    

      

tp(t|x) dt

tp(x, t) dt

p(x, t) dt

=

=

(cid:6)
(cid:6)

(cid:6)
(cid:6)
(cid:2)
(cid:2)
(cid:6)    

m

n

      

f(x, t)t dt = 0

g(x     xn)tn
g(x     xm)

k(x, xn)tn

y(x) =

=

k(x, xn) = g(x     xn)
g(x     xm)

(cid:2)
(cid:2)
(cid:2)

m

n

n

(cid:2)
(cid:6)    

m

g(x) =

f(x, t) dt.

      

tf(x     xn, t     tn) dt

f(x     xm, t     tm) dt

.

(6.43)

we now assume for simplicity that the component density functions have zero mean
so that

for all values of x. using a simple change of variable, we then obtain

where n, m = 1, . . . , n and the id81 k(x, xn) is given by

and we have de   ned

the result (6.45) is known as the nadaraya-watson model, or kernel regression
(nadaraya, 1964; watson, 1964). for a localized id81, it has the prop-
erty of giving more weight to the data points xn that are close to x. note that the
kernel (6.46) satis   es the summation constraint

n(cid:2)

n=1

k(x, xn) = 1.

(6.44)

(6.45)

(6.46)

(6.47)

6.4. gaussian processes

303

figure 6.3 illustration of the nadaraya-watson kernel
regression model using isotropic gaussian kernels, for the
sinusoidal data set. the original sine function is shown
by the green curve, the data points are shown in blue,
and each is the centre of an isotropic gaussian kernel.
the resulting regression function, given by the condi-
tional mean, is shown by the red line, along with the two-
standard-deviation region for the conditional distribution
p(t|x) shown by the red shading. the blue ellipse around
each data point shows one standard deviation contour for
the corresponding kernel. these appear noncircular due
to the different scales on the horizontal and vertical axes.

1.5

1

0.5

0

   0.5

   1

   1.5

0

0.2

0.4

0.6

0.8

1

in fact, this model de   nes not only a conditional expectation but also a full

conditional distribution given by

(cid:6)

p(t|x) = p(t, x)

=

p(t, x) dt

(cid:2)
(cid:6)
(cid:2)

n

f(x     xn, t     tn)

f(x     xm, t     tm) dt

(6.48)

exercise 6.18

m

from which other expectations can be evaluated.

as an illustration we consider the case of a single input variable x in which
f(x, t) is given by a zero-mean isotropic gaussian over the variable z = (x, t) with
variance   2. the corresponding conditional distribution (6.48) is given by a gaus-
sian mixture, and is shown, together with the conditional mean, for the sinusoidal
synthetic data set in figure 6.3.

an obvious extension of this model is to allow for more    exible forms of gaus-
sian components, for instance having different variance parameters for the input and
target variables. more generally, we could model the joint distribution p(t, x) using
a gaussian mixture model, trained using techniques discussed in chapter 9 (ghahra-
mani and jordan, 1994), and then    nd the corresponding conditional distribution
p(t|x). in this latter case we no longer have a representation in terms of kernel func-
tions evaluated at the training set data points. however, the number of components
in the mixture model can be smaller than the number of training set points, resulting
in a model that is faster to evaluate for test data points. we have thereby accepted an
increased computational cost during the training phase in order to have a model that
is faster at making predictions.

6.4. gaussian processes

in section 6.1, we introduced kernels by applying the concept of duality to a non-
probabilistic model for regression. here we extend the role of kernels to probabilis-

304

6. kernel methods

tic discriminative models, leading to the framework of gaussian processes. we shall
thereby see how kernels arise naturally in a bayesian setting.

in chapter 3, we considered id75 models of the form y(x, w) =
wt  (x) in which w is a vector of parameters and   (x) is a vector of    xed nonlinear
basis functions that depend on the input vector x. we showed that a prior distribution
over w induced a corresponding prior distribution over functions y(x, w). given a
training data set, we then evaluated the posterior distribution over w and thereby
obtained the corresponding posterior distribution over regression functions, which
in turn (with the addition of noise) implies a predictive distribution p(t|x) for new
input vectors x.

in the gaussian process viewpoint, we dispense with the parametric model and
instead de   ne a prior id203 distribution over functions directly. at    rst sight, it
might seem dif   cult to work with a distribution over the uncountably in   nite space of
functions. however, as we shall see, for a    nite training set we only need to consider
the values of the function at the discrete set of input values xn corresponding to the
training set and test set data points, and so in practice we can work in a    nite space.
models equivalent to gaussian processes have been widely studied in many dif-
ferent    elds. for instance, in the geostatistics literature gaussian process regression
is known as kriging (cressie, 1993). similarly, arma (autoregressive moving aver-
age) models, kalman    lters, and id80s can all be viewed as
forms of gaussian process models. reviews of gaussian processes from a machine
learning perspective can be found in mackay (1998), williams (1999), and mackay
(2003), and a comparison of gaussian process models with alternative approaches is
given in rasmussen (1996). see also rasmussen and williams (2006) for a recent
textbook on gaussian processes.

6.4.1 id75 revisited
in order to motivate the gaussian process viewpoint, let us return to the linear
regression example and re-derive the predictive distribution by working in terms
of distributions over functions y(x, w). this will provide a speci   c example of a
gaussian process.

consider a model de   ned in terms of a linear combination of m    xed basis

functions given by the elements of the vector   (x) so that

y(x) = wt  (x)

(6.49)

where x is the input vector and w is the m-dimensional weight vector. now consider
a prior distribution over w given by an isotropic gaussian of the form

p(w) = n (w|0,   

   1i)

(6.50)

governed by the hyperparameter   , which represents the precision (inverse variance)
of the distribution. for any given value of w, the de   nition (6.49) de   nes a partic-
ular function of x. the id203 distribution over w de   ned by (6.50) therefore
induces a id203 distribution over functions y(x). in practice, we wish to eval-
uate this function at speci   c values of x, for example at the training data points

exercise 2.31

(6.52)

(6.53)

(6.54)

6.4. gaussian processes

305

x1, . . . , xn . we are therefore interested in the joint distribution of the function val-
ues y(x1), . . . , y(xn ), which we denote by the vector y with elements yn = y(xn)
for n = 1, . . . , n. from (6.49), this vector is given by

y =   w

(6.51)
where    is the design matrix with elements   nk =   k(xn). we can    nd the proba-
bility distribution of y as follows. first of all we note that y is a linear combination of
gaussian distributed variables given by the elements of w and hence is itself gaus-
sian. we therefore need only to    nd its mean and covariance, which are given from
(6.50) by

e[y] =   e[w] = 0

(cid:8)

(cid:9)

(cid:8)

(cid:9)

cov[y] = e

yyt

=   e

wwt

  t =

1
  

    t = k

where k is the gram matrix with elements

knm = k(xn, xm) =

and k(x, x(cid:4)) is the id81.

1
  

  (xn)t  (xm)

this model provides us with a particular example of a gaussian process. in gen-
eral, a gaussian process is de   ned as a id203 distribution over functions y(x)
such that the set of values of y(x) evaluated at an arbitrary set of points x1, . . . , xn
jointly have a gaussian distribution. in cases where the input vector x is two di-
mensional, this may also be known as a gaussian random    eld. more generally, a
stochastic process y(x) is speci   ed by giving the joint id203 distribution for
any    nite set of values y(x1), . . . , y(xn ) in a consistent manner.

a key point about gaussian stochastic processes is that the joint distribution
over n variables y1, . . . , yn is speci   ed completely by the second-order statistics,
namely the mean and the covariance. in most applications, we will not have any
prior knowledge about the mean of y(x) and so by symmetry we take it to be zero.
this is equivalent to choosing the mean of the prior over weight values p(w|  ) to
be zero in the basis function viewpoint. the speci   cation of the gaussian process is
then completed by giving the covariance of y(x) evaluated at any two values of x,
which is given by the id81

e [y(xn)y(xm)] = k(xn, xm).

(6.55)

for the speci   c case of a gaussian process de   ned by the id75 model
(6.49) with a weight prior (6.50), the id81 is given by (6.54).

we can also de   ne the id81 directly, rather than indirectly through a
choice of basis function. figure 6.4 shows samples of functions drawn from gaus-
sian processes for two different choices of id81. the    rst of these is a
   gaussian    kernel of the form (6.23), and the second is the exponential kernel given
by

(6.56)
which corresponds to the ornstein-uhlenbeck process originally introduced by uh-
lenbeck and ornstein (1930) to describe brownian motion.

(cid:4)) = exp (      |x     x
(cid:4)|)

k(x, x

306

6. kernel methods

figure 6.4 samples from gaus-
sian processes for a    gaussian    ker-
nel (left) and an exponential kernel
(right).

3

1.5

0

   1.5

   3

   1

   0.5

0

0.5

1

3

1.5

0

   1.5

   3

   1

   0.5

0

0.5

1

6.4.2 gaussian processes for regression
in order to apply gaussian process models to the problem of regression, we need

to take account of the noise on the observed target values, which are given by

tn = yn +  n

(6.57)

where yn = y(xn), and  n is a random noise variable whose value is chosen inde-
pendently for each observation n. here we shall consider noise processes that have
a gaussian distribution, so that

p(tn|yn) = n (tn|yn,   

   1)

(6.58)

where    is a hyperparameter representing the precision of the noise. because the
noise is independent for each data point, the joint distribution of the target values
t = (t1, . . . , tn)t conditioned on the values of y = (y1, . . . , yn)t is given by an
isotropic gaussian of the form

p(t|y) = n (t|y,   

   1in )

(6.59)
where in denotes the n    n unit matrix. from the de   nition of a gaussian process,
the marginal distribution p(y) is given by a gaussian whose mean is zero and whose
covariance is de   ned by a gram matrix k so that

p(y) = n (y|0, k).

(6.60)

the id81 that determines k is typically chosen to express the property
that, for points xn and xm that are similar, the corresponding values y(xn) and
y(xm) will be more strongly correlated than for dissimilar points. here the notion
of similarity will depend on the application.

in order to    nd the marginal distribution p(t), conditioned on the input values
x1, . . . , xn , we need to integrate over y. this can be done by making use of the
results from section 2.3.3 for the linear-gaussian model. using (2.115), we see that
the marginal distribution of t is given by

(cid:6)

p(t) =

p(t|y)p(y) dy = n (t|0, c)

(6.61)

6.4. gaussian processes

307

where the covariance matrix c has elements

c(xn, xm) = k(xn, xm) +   

   1  nm.

(6.62)

this result re   ects the fact that the two gaussian sources of randomness, namely
that associated with y(x) and that associated with  , are independent and so their
covariances simply add.

one widely used id81 for gaussian process regression is given by the
exponential of a quadratic form, with the addition of constant and linear terms to
give

(cid:12)

(cid:13)

k(xn, xm) =   0 exp

+   2 +   3xt

nxm.

(6.63)

(cid:5)xn     xm(cid:5)2

      1
2

note that the term involving   3 corresponds to a parametric model that is a linear
function of the input variables. samples from this prior are plotted for various values
of the parameters   0, . . . ,   3 in figure 6.5, and figure 6.6 shows a set of points sam-
pled from the joint distribution (6.60) along with the corresponding values de   ned
by (6.61).

so far, we have used the gaussian process viewpoint to build a model of the
joint distribution over sets of data points. our goal in regression, however, is to
make predictions of the target variables for new inputs, given a set of training data.
let us suppose that tn = (t1, . . . , tn)t, corresponding to input values x1, . . . , xn ,
comprise the observed training set, and our goal is to predict the target variable tn +1
for a new input vector xn +1. this requires that we evaluate the predictive distri-
bution p(tn +1|tn ). note that this distribution is conditioned also on the variables
x1, . . . , xn and xn +1. however, to keep the notation simple we will not show these
conditioning variables explicitly.
to    nd the conditional distribution p(tn +1|t), we begin by writing down the
joint distribution p(tn +1), where tn +1 denotes the vector (t1, . . . , tn , tn +1)t. we
then apply the results from section 2.3.1 to obtain the required conditional distribu-
tion, as illustrated in figure 6.7.

from (6.61), the joint distribution over t1, . . . , tn +1 will be given by

p(tn +1) = n (tn +1|0, cn +1)

(6.64)
where cn +1 is an (n + 1)    (n + 1) covariance matrix with elements given by
(6.62). because this joint distribution is gaussian, we can apply the results from
section 2.3.1 to    nd the conditional gaussian distribution. to do this, we partition
the covariance matrix as follows

(cid:15)

(cid:16)

cn +1 =

(6.65)
where cn is the n    n covariance matrix with elements given by (6.62) for n, m =
1, . . . , n, the vector k has elements k(xn, xn +1) for n = 1, . . . , n, and the scalar

cn k
kt
c

308

6. kernel methods

(1.00, 4.00, 0.00, 0.00)

(9.00, 4.00, 0.00, 0.00)

(1.00, 64.00, 0.00, 0.00)

3

1.5

0

   1.5

   3

   1

3

1.5

0

   1.5

   3

   1

9

4.5

0

   4.5

0

   0.5
0.5
(1.00, 0.25, 0.00, 0.00)

1

   9

   1

9

4.5

0

   4.5

   9

   1

   0.5

0

0.5

1

   0.5

0

0.5

1

(1.00, 4.00, 10.00, 0.00)

   0.5

0

0.5

1

3

1.5

0

   1.5

   3

   1

4

2

0

   2

   4

   1

0

   0.5
0.5
(1.00, 4.00, 0.00, 5.00)

1

   0.5

0

0.5

1

figure 6.5 samples from a gaussian process prior de   ned by the covariance function (6.63). the title above
each plot denotes (  0,   1,   2,   3).

   1. using the results (2.81) and (2.82), we see that the con-
c = k(xn +1, xn +1) +   
ditional distribution p(tn +1|t) is a gaussian distribution with mean and covariance
given by

   1
m(xn +1) = ktc
n t
  2(xn +1) = c     ktc

   1
n k.

(6.66)
(6.67)

these are the key results that de   ne gaussian process regression. because the vector
k is a function of the test point input value xn +1, we see that the predictive distribu-
tion is a gaussian whose mean and variance both depend on xn +1. an example of
gaussian process regression is shown in figure 6.8.

the only restriction on the id81 is that the covariance matrix given by
(6.62) must be positive de   nite. if   i is an eigenvalue of k, then the corresponding
   1. it is therefore suf   cient that the kernel matrix
eigenvalue of c will be   i +   
k(xn, xm) be positive semide   nite for any pair of points xn and xm, so that   i (cid:2) 0,
because any eigenvalue   i that is zero will still give rise to a positive eigenvalue
for c because    > 0. this is the same restriction on the id81 discussed
earlier, and so we can again exploit all of the techniques in section 6.2 to construct

figure 6.6 illustration of

the sampling of data
points {tn} from a gaussian process.
the blue curve shows a sample func-
tion from the gaussian process prior
over functions, and the red points
show the values of yn obtained by
evaluating the function at a set of in-
put values {xn}. the correspond-
ing values of {tn}, shown in green,
are obtained by adding independent
gaussian noise to each of the {yn}.

t

3

0

   3

   1

suitable kernels.

6.4. gaussian processes

309

0

x

1

note that the mean (6.66) of the predictive distribution can be written, as a func-

tion of xn +1, in the form

n(cid:2)

m(xn +1) =

ank(xn, xn +1)

(6.68)

exercise 6.21

n=1
   1
n t. thus, if the id81 k(xn, xm)
where an is the nth component of c
depends only on the distance (cid:5)xn     xm(cid:5), then we obtain an expansion in radial
basis functions.

the results (6.66) and (6.67) de   ne the predictive distribution for gaussian pro-
cess regression with an arbitrary id81 k(xn, xm). in the particular case in
which the id81 k(x, x(cid:4)) is de   ned in terms of a    nite set of basis functions,
we can derive the results obtained previously in section 3.3.2 for id75
starting from the gaussian process viewpoint.

for such models, we can therefore obtain the predictive distribution either by
taking a parameter space viewpoint and using the id75 result or by taking
a function space viewpoint and using the gaussian process result.
the central computational operation in using gaussian processes will involve
the inversion of a matrix of size n    n, for which standard methods require o(n 3)
computations. by contrast, in the basis function model we have to invert a matrix
sn of size m    m, which has o(m 3) computational complexity. note that for
both viewpoints, the matrix inversion must be performed once for the given training
set. for each new test point, both methods require a vector-matrix multiply, which
has cost o(n 2) in the gaussian process case and o(m 2) for the linear basis func-
tion model. if the number m of basis functions is smaller than the number n of
data points, it will be computationally more ef   cient to work in the basis function

310

6. kernel methods

figure 6.7 illustration of

the mechanism of
gaussian process regression for
the case of one training point and
one test point, in which the red el-
lipses show contours of the joint dis-
tribution p(t1, t2). here t1 is the
training data point, and condition-
ing on the value of t1, correspond-
ing to the vertical blue line, we ob-
tain p(t2|t1) shown as a function of
t2 by the green curve.

1

0

   1

t2

m(x2)

t1

   1

0

1

framework. however, an advantage of a gaussian processes viewpoint is that we
can consider covariance functions that can only be expressed in terms of an in   nite
number of basis functions.

for large training data sets, however, the direct application of gaussian process
methods can become infeasible, and so a range of approximation schemes have been
developed that have better scaling with training set size than the exact approach
(gibbs, 1997; tresp, 2001; smola and bartlett, 2001; williams and seeger, 2001;
csat  o and opper, 2002; seeger et al., 2003). practical issues in the application of
gaussian processes are discussed in bishop and nabney (2008).

we have introduced gaussian process regression for the case of a single tar-
get variable. the extension of this formalism to multiple target variables, known
as co-kriging (cressie, 1993), is straightforward. various other extensions of gaus-

exercise 6.23

figure 6.8 illustration of gaussian process re-
gression applied to the sinusoidal
data set in figure a.6 in which the
three right-most data points have
been omitted. the green curve
shows the sinusoidal function from
which the data points, shown in
blue, are obtained by sampling and
addition of gaussian noise. the
red line shows the mean of
the
gaussian process predictive distri-
bution, and the shaded region cor-
responds to plus and minus two
standard deviations. notice how
the uncertainty increases in the re-
gion to the right of the data points.

1

0.5

0

   0.5

   1

0

0.2

0.4

0.6

0.8

1

6.4. gaussian processes

311

sian process regression have also been considered, for purposes such as modelling
the distribution over low-dimensional manifolds for unsupervised learning (bishop
et al., 1998a) and the solution of stochastic differential equations (graepel, 2003).

6.4.3 learning the hyperparameters
the predictions of a gaussian process model will depend, in part, on the choice
of covariance function. in practice, rather than    xing the covariance function, we
may prefer to use a parametric family of functions and then infer the parameter
values from the data. these parameters govern such things as the length scale of the
correlations and the precision of the noise and correspond to the hyperparameters in
a standard parametric model.
techniques for learning the hyperparameters are based on the evaluation of the
likelihood function p(t|  ) where    denotes the hyperparameters of the gaussian pro-
cess model. the simplest approach is to make a point estimate of    by maximizing
the log likelihood function. because    represents a set of hyperparameters for the
regression problem, this can be viewed as analogous to the type 2 maximum like-
lihood procedure for id75 models. maximization of the log likelihood
can be done using ef   cient gradient-based optimization algorithms such as conjugate
gradients (fletcher, 1987; nocedal and wright, 1999; bishop and nabney, 2008).

the log likelihood function for a gaussian process regression model is easily

evaluated using the standard form for a multivariate gaussian distribution, giving

ln p(t|  ) =    1
2

ln|cn|     1
2

ttc

n t     n
   1
2

ln(2  ).

(6.69)

section 3.5

for nonlinear optimization, we also need the gradient of the log likelihood func-
tion with respect to the parameter vector   . we shall assume that evaluation of the
derivatives of cn is straightforward, as would be the case for the covariance func-
tions considered in this chapter. making use of the result (c.21) for the derivative of
n , together with the result (c.22) for the derivative of ln|cn|, we obtain
   1
c

(cid:15)

(cid:16)

   
     i

   1
n

c

   cn
     i

ln p(t|  ) =    1

+

1
2

   1
ttc
n

   cn
     i

   1
n t.

c

(6.70)
because ln p(t|  ) will in general be a nonconvex function, it can have multiple max-
ima.

2 tr

it is straightforward to introduce a prior over    and to maximize the log poste-
rior using gradient-based methods. in a fully bayesian treatment, we need to evaluate
marginals over    weighted by the product of the prior p(  ) and the likelihood func-
tion p(t|  ). in general, however, exact marginalization will be intractable, and we
must resort to approximations.

the gaussian process regression model gives a predictive distribution whose
mean and variance are functions of the input vector x. however, we have assumed
that the contribution to the predictive variance arising from the additive noise, gov-
erned by the parameter   , is a constant. for some problems, known as heteroscedas-
tic, the noise variance itself will also depend on x. to model this, we can extend the

312

6. kernel methods

for gaussian processes,

figure 6.9 samples from the ard
prior
in
which the id81 is given by
(6.71). the left plot corresponds to
  1 =   2 = 1, and the right plot cor-
responds to   1 = 1,   2 = 0.01.

gaussian process framework by introducing a second gaussian process to represent
the dependence of    on the input x (goldberg et al., 1998). because    is a variance,
and hence nonnegative, we use the gaussian process to model ln   (x).

6.4.4 automatic relevance determination
in the previous section, we saw how maximum likelihood could be used to de-
termine a value for the correlation length-scale parameter in a gaussian process.
this technique can usefully be extended by incorporating a separate parameter for
each input variable (rasmussen and williams, 2006). the result, as we shall see, is
that the optimization of these parameters by maximum likelihood allows the relative
importance of different inputs to be inferred from the data. this represents an exam-
ple in the gaussian process context of automatic relevance determination, or ard,
which was originally formulated in the framework of neural networks (mackay,
1994; neal, 1996). the mechanism by which appropriate inputs are preferred is
discussed in section 7.2.2.

consider a gaussian process with a two-dimensional input space x = (x1, x2),

having a id81 of the form

(cid:25)

k(x, x(cid:4)) =   0 exp

   1
2

  i(xi     x
(cid:4)
i)2

.

(6.71)

(cid:24)

2(cid:2)

i=1

samples from the resulting prior over functions y(x) are shown for two different
settings of the precision parameters   i in figure 6.9. we see that, as a particu-
lar parameter   i becomes small, the function becomes relatively insensitive to the
corresponding input variable xi. by adapting these parameters to a data set using
maximum likelihood, it becomes possible to detect input variables that have little
effect on the predictive distribution, because the corresponding values of   i will be
small. this can be useful in practice because it allows such inputs to be discarded.
ard is illustrated using a simple synthetic data set having three inputs x1, x2 and x3
(nabney, 2002) in figure 6.10. the target variable t, is generated by sampling 100
values of x1 from a gaussian, evaluating the function sin(2  x1), and then adding

6.4. gaussian processes

313

figure 6.10 illustration of automatic rele-
vance determination in a gaus-
sian process for a synthetic prob-
lem having three inputs x1, x2,
and x3,
for which the curves
show the corresponding values of
the hyperparameters   1 (red),   2
(green), and   3 (blue) as a func-
tion of the number of iterations
when optimizing the marginal
likelihood. details are given in
the text. note the logarithmic
scale on the vertical axis.

102

100

10   2

10   4

0

20

40

60

80

100

gaussian noise. values of x2 are given by copying the corresponding values of x1
and adding noise, and values of x3 are sampled from an independent gaussian dis-
tribution. thus x1 is a good predictor of t, x2 is a more noisy predictor of t, and x3
has only chance correlations with t. the marginal likelihood for a gaussian process
with ard parameters   1,   2,   3 is optimized using the scaled conjugate gradients
algorithm. we see from figure 6.10 that   1 converges to a relatively large value,   2
converges to a much smaller value, and   3 becomes very small indicating that x3 is
irrelevant for predicting t.

the ard framework is easily incorporated into the exponential-quadratic kernel
(6.63) to give the following form of id81, which has been found useful for
applications of gaussian processes to a range of regression problems

(cid:24)

d(cid:2)

(cid:25)

d(cid:2)

k(xn, xm) =   0 exp

   1
2

  i(xni     xmi)2

+   2 +   3

xnixmi

(6.72)

i=1

i=1

where d is the dimensionality of the input space.

6.4.5 gaussian processes for classi   cation
in a probabilistic approach to classi   cation, our goal is to model the posterior
probabilities of the target variable for a new input vector, given a set of training
data. these probabilities must lie in the interval (0, 1), whereas a gaussian process
model makes predictions that lie on the entire real axis. however, we can easily
adapt gaussian processes to classi   cation problems by transforming the output of
the gaussian process using an appropriate nonlinear activation function.
consider    rst the two-class problem with a target variable t     {0, 1}. if we de-
   ne a gaussian process over a function a(x) and then transform the function using
a logistic sigmoid y =   (a), given by (4.59), then we will obtain a non-gaussian
stochastic process over functions y(x) where y     (0, 1). this is illustrated for the
case of a one-dimensional input space in figure 6.11 in which the id203 distri-

314

6. kernel methods

10

5

0

   5

   10

   1

   0.5

0

0.5

1

1

0.75

0.5

0.25

0
   1

   0.5

0

0.5

1

figure 6.11 the left plot shows a sample from a gaussian process prior over functions a(x), and the right plot
shows the result of transforming this sample using a logistic sigmoid function.

bution over the target variable t is then given by the bernoulli distribution

p(t|a) =   (a)t(1       (a))1   t.

(6.73)

as usual, we denote the training set inputs by x1, . . . , xn with corresponding
observed target variables t = (t1, . . . , tn)t. we also consider a single test point
xn +1 with target value tn +1. our goal is to determine the predictive distribution
p(tn +1|t), where we have left the conditioning on the input variables implicit. to do
this we introduce a gaussian process prior over the vector an +1, which has compo-
nents a(x1), . . . , a(xn +1). this in turn de   nes a non-gaussian process over tn +1,
and by conditioning on the training data tn we obtain the required predictive distri-
bution. the gaussian process prior for an +1 takes the form
p(an +1) = n (an +1|0, cn +1).

(6.74)

unlike the regression case, the covariance matrix no longer includes a noise term
because we assume that all of the training data points are correctly labelled. how-
ever, for numerical reasons it is convenient to introduce a noise-like term governed
by a parameter    that ensures that the covariance matrix is positive de   nite. thus
the covariance matrix cn +1 has elements given by

c(xn, xm) = k(xn, xm) +     nm

(6.75)

where k(xn, xm) is any positive semide   nite id81 of the kind considered
in section 6.2, and the value of    is typically    xed in advance. we shall assume that
the id81 k(x, x(cid:4)) is governed by a vector    of parameters, and we shall
later discuss how    may be learned from the training data.
for two-class problems, it is suf   cient to predict p(tn +1 = 1|tn ) because the
value of p(tn +1 = 0|tn ) is then given by 1     p(tn +1 = 1|tn ). the required

6.4. gaussian processes

315

predictive distribution is given by

(cid:6)

p(tn +1 = 1|tn) =

p(tn +1 = 1|an +1)p(an +1|tn) dan +1

(6.76)

where p(tn +1 = 1|an +1) =   (an +1).

this integral is analytically intractable, and so may be approximated using sam-
pling methods (neal, 1997). alternatively, we can consider techniques based on
an analytical approximation. in section 4.5.2, we derived the approximate formula
(4.153) for the convolution of a logistic sigmoid with a gaussian distribution. we
can use this result to evaluate the integral in (6.76) provided we have a gaussian
approximation to the posterior distribution p(an +1|tn ). the usual justi   cation for a
gaussian approximation to a posterior distribution is that the true posterior will tend
to a gaussian as the number of data points increases as a consequence of the central
limit theorem. in the case of gaussian processes, the number of variables grows with
the number of data points, and so this argument does not apply directly. however, if
we consider increasing the number of data points falling in a    xed region of x space,
then the corresponding uncertainty in the function a(x) will decrease, again leading
asymptotically to a gaussian (williams and barber, 1998).

three different approaches to obtaining a gaussian approximation have been
considered. one technique is based on variational id136 (gibbs and mackay,
2000) and makes use of the local variational bound (10.144) on the logistic sigmoid.
this allows the product of sigmoid functions to be approximated by a product of
gaussians thereby allowing the marginalization over an to be performed analyti-
cally. the approach also yields a lower bound on the likelihood function p(tn|  ).
the variational framework for gaussian process classi   cation can also be extended
to multiclass (k > 2) problems by using a gaussian approximation to the softmax
function (gibbs, 1997).

a second approach uses expectation propagation (opper and winther, 2000b;
minka, 2001b; seeger, 2003). because the true posterior distribution is unimodal, as
we shall see shortly, the expectation propagation approach can give good results.

(cid:6)

6.4.6 laplace approximation
the third approach to gaussian process classi   cation is based on the laplace
approximation, which we now consider in detail. in order to evaluate the predictive
distribution (6.76), we seek a gaussian approximation to the posterior distribution
over an +1, which, using bayes    theorem, is given by
(cid:6)
p(an +1, an|tn ) dan
(cid:6)
1

p(an +1|tn ) =

p(an +1, an)p(tn|an +1, an) dan
p(an +1|an)p(an )p(tn|an ) dan

p(tn )

=

1

p(an +1|an)p(an|tn ) dan

(6.77)

(cid:6)

p(tn )

=

=

section 2.3

section 10.1

section 10.7

section 4.4

316

6. kernel methods

where we have used p(tn|an +1, an) = p(tn|an). the conditional distribution
p(an +1|an) is obtained by invoking the results (6.66) and (6.67) for gaussian pro-
cess regression, to give

p(an +1|an ) = n (an +1|ktc

n an , c     ktc
   1
   1
n k).

(6.78)

we can therefore evaluate the integral in (6.77) by    nding a laplace approximation
for the posterior distribution p(an|tn ), and then using the standard result for the
convolution of two gaussian distributions.

the prior p(an) is given by a zero-mean gaussian process with covariance ma-

trix cn , and the data term (assuming independence of the data points) is given by

p(tn|an ) =

  (an)tn(1       (an))1   tn =

eantn  (   an).

(6.79)

n=1

n=1

we then obtain the laplace approximation by taylor expanding the logarithm of
p(an|tn ), which up to an additive id172 constant is given by the quantity

n(cid:14)

n(cid:14)

  (an ) = ln p(an) + ln p(tn|an)

=    1
2

    n(cid:2)

at
n c

n an     n
   1
2

ln(2  )     1
2

ln|cn| + tt

n an

ln(1 + ean) + const.

(6.80)

n=1

first we need to    nd the mode of the posterior distribution, and this requires that we
evaluate the gradient of   (an ), which is given by

     (an) = tn       n     c

   1
n an

(6.81)

where   n is a vector with elements   (an). we cannot simply    nd the mode by
setting this gradient to zero, because   n depends nonlinearly on an , and so we
resort to an iterative scheme based on the newton-raphson method, which gives rise
to an iterative reweighted least squares (irls) algorithm. this requires the second
derivatives of   (an ), which we also require for the laplace approximation anyway,
and which are given by

        (an) =    wn     c

   1
n

(6.82)
where wn is a diagonal matrix with elements   (an)(1      (an)), and we have used
the result (4.88) for the derivative of the logistic sigmoid function. note that these
diagonal elements lie in the range (0, 1/4), and hence wn is a positive de   nite
matrix. because cn (and hence its inverse) is positive de   nite by construction, and
because the sum of two positive de   nite matrices is also positive de   nite, we see
that the hessian matrix a =            (an) is positive de   nite and so the posterior
distribution p(an|tn) is log convex and therefore has a single mode that is the global

section 4.3.3

exercise 6.24

6.4. gaussian processes

317

maximum. the posterior distribution is not gaussian, however, because the hessian
is a function of an .

using the newton-raphson formula (4.92), the iterative update equation for an

exercise 6.25

is given by

n = cn (i + wn cn )   1 {tn       n + wn an} .
anew

(6.83)

these equations are iterated until they converge to the mode which we denote by
n . at the mode, the gradient      (an) will vanish, and hence a(cid:1)
a(cid:1)

n will satisfy

n = cn (tn       n).
a(cid:1)

(6.84)

once we have found the mode a(cid:1)

n of the posterior, we can evaluate the hessian

matrix given by

h =            (an) = wn + c

   1
n

(6.85)

where the elements of wn are evaluated using a(cid:1)
proximation to the posterior distribution p(an|tn ) given by

n . this de   nes our gaussian ap-

q(an) = n (an|a(cid:1)

n , h   1).

(6.86)

exercise 6.26

we can now combine this with (6.78) and hence evaluate the integral (6.77). because
this corresponds to a linear-gaussian model, we can use the general result (2.115) to
give

e[an +1|tn ] = kt(tn       n )
var[an +1|tn ] = c     kt(w
   1
n + cn )   1k.

(6.87)
(6.88)
now that we have a gaussian distribution for p(an +1|tn ), we can approximate
the integral (6.76) using the result (4.153). as with the bayesian id28
model of section 4.5, if we are only interested in the decision boundary correspond-
ing to p(tn +1|tn ) = 0.5, then we need only consider the mean and we can ignore
the effect of the variance.
we also need to determine the parameters    of the covariance function. one
approach is to maximize the likelihood function given by p(tn|  ) for which we need
expressions for the log likelihood and its gradient. if desired, suitable id173
terms can also be added, leading to a penalized maximum likelihood solution. the
likelihood function is de   ned by
p(tn|  ) =

p(tn|an )p(an|  ) dan .

(6.89)

(cid:6)

this integral is analytically intractable, so again we make use of the laplace approx-
imation. using the result (4.135), we obtain the following approximation for the log
of the likelihood function

ln p(tn|  ) =   (a(cid:1)

n )     1
2

ln|wn + c

n | + n
   1
2

ln(2  )

(6.90)

318

6. kernel methods

n|  ) + ln p(tn|a(cid:1)

n ) = ln p(a(cid:1)

where   (a(cid:1)
n ). we also need to evaluate the gradient
of ln p(tn|  ) with respect to the parameter vector   . note that changes in    will
cause changes in a(cid:1)
n , leading to additional terms in the gradient. thus, when we
differentiate (6.90) with respect to   , we obtain two sets of terms, the    rst arising
from the dependence of the covariance matrix cn on   , and the rest arising from
dependence of a(cid:1)

n on   .

the terms arising from the explicit dependence on    can be found by using

(6.80) together with the results (c.21) and (c.22), and are given by

    ln p(tn|  )

     j

=

   1
a(cid:1)t
n c
n

   cn
     j

   1
n a(cid:1)

n

c

(cid:29)

1
2
   1

2 tr

(i + cn wn )   1wn

(cid:30)

   cn
     j

.

(6.91)

n , and so   (a(cid:1)

to compute the terms arising from the dependence of a(cid:1)

n on   , we note that
the laplace approximation has been constructed such that   (an ) has zero gradient
at an = a(cid:1)
n) gives no contribution to the gradient as a result of its
dependence on a(cid:1)
n . this leaves the following contribution to the derivative with
respect to a component   j of   
n |
    ln|wn + c
   1

n(cid:2)

   1
2

   a(cid:1)
n

n(cid:2)

(cid:8)

n=1

n=1

=    1
2

   a(cid:1)
n
     j

(cid:9)

(i + cn wn )   1cn

n(1       (cid:1)

n)(1     2  (cid:1)

nn   (cid:1)

n)    a(cid:1)
n
     j

(6.92)

n =   (a(cid:1)

where   (cid:1)
de   nition of wn . we can evaluate the derivative of a(cid:1)
entiating the relation (6.84) with respect to   j to give

n), and again we have used the result (c.22) together with the
n with respect to   j by differ-

   a(cid:1)
n
     j

=    cn
     j

(tn       n )     cn wn

   a(cid:1)
n
     j

.

rearranging then gives

   a(cid:1)
n
     j

= (i + wn cn )   1    cn
     j

(tn       n ).

(6.93)

(6.94)

combining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log
likelihood function, which can be used with standard nonlinear optimization algo-
rithms in order to determine a value for   .

we can illustrate the application of the laplace approximation for gaussian pro-
cesses using the synthetic two-class data set shown in figure 6.12. extension of the
laplace approximation to gaussian processes involving k > 2 classes, using the
softmax activation function, is straightforward (williams and barber, 1998).

appendix a

6.4. gaussian processes

319

2

0

   2

   2

0

2

figure 6.12 illustration of the use of a gaussian process for classi   cation, showing the data on the left together
with the optimal decision boundary from the true distribution in green, and the decision boundary from the
gaussian process classi   er in black. on the right is the predicted posterior id203 for the blue and red
classes together with the gaussian process decision boundary.

6.4.7 connection to neural networks
we have seen that the range of functions which can be represented by a neural
network is governed by the number m of hidden units, and that, for suf   ciently
large m, a two-layer network can approximate any given function with arbitrary
accuracy.
in the framework of maximum likelihood, the number of hidden units
needs to be limited (to a level dependent on the size of the training set) in order
to avoid over-   tting. however, from a bayesian perspective it makes little sense to
limit the number of parameters in the network according to the size of the training
set.

in a bayesian neural network, the prior distribution over the parameter vector
w, in conjunction with the network function f(x, w), produces a prior distribution
over functions from y(x) where y is the vector of network outputs. neal (1996)
has shown that, for a broad class of prior distributions over w, the distribution of
functions generated by a neural network will tend to a gaussian process in the limit
m        . it should be noted, however, that in this limit the output variables of the
neural network become independent. one of the great merits of neural networks is
that the outputs share the hidden units and so they can    borrow statistical strength   
from each other, that is, the weights associated with each hidden unit are in   uenced
by all of the output variables not just by one of them. this property is therefore lost
in the gaussian process limit.

we have seen that a gaussian process is determined by its covariance (kernel)
function. williams (1998) has given explicit forms for the covariance in the case of
two speci   c choices for the hidden unit activation function (probit and gaussian).
these id81s k(x, x(cid:4)) are nonstationary, i.e. they cannot be expressed as
a function of the difference x     x(cid:4)
, as a consequence of the gaussian weight prior
being centred on zero which breaks translation invariance in weight space.

320

6. kernel methods

exercises

by working directly with the covariance function we have implicitly marginal-
ized over the distribution of weights. if the weight prior is governed by hyperpa-
rameters, then their values will determine the length scales of the distribution over
functions, as can be understood by studying the examples in figure 5.11 for the case
of a    nite number of hidden units. note that we cannot marginalize out the hyperpa-
rameters analytically, and must instead resort to techniques of the kind discussed in
section 6.4.

6.1 ((cid:12) (cid:12)) www consider the dual formulation of the least squares id75
problem given in section 6.1. show that the solution for the components an of
the vector a can be expressed as a linear combination of the elements of the vector
  (xn). denoting these coef   cients by the vector w, show that the dual of the dual
formulation is given by the original representation in terms of the parameter vector
w.

6.2 ((cid:12) (cid:12))

in this exercise, we develop a dual formulation of the id88 learning
algorithm. using the id88 learning rule (4.55), show that the learned weight
vector w can be written as a linear combination of the vectors tn  (xn) where tn    
{   1, +1}. denote the coef   cients of this linear combination by   n and derive a
formulation of the id88 learning algorithm, and the predictive function for the
id88, in terms of the   n. show that the feature vector   (x) enters only in the
form of the id81 k(x, x(cid:4)) =   (x)t  (x(cid:4)).

6.3 ((cid:12)) the nearest-neighbour classi   er (section 2.5.2) assigns a new input vector x
to the same class as that of the nearest input vector xn from the training set, where
in the simplest case, the distance is de   ned by the euclidean metric (cid:5)x     xn(cid:5)2. by
expressing this rule in terms of scalar products and then making use of kernel sub-
stitution, formulate the nearest-neighbour classi   er for a general nonlinear kernel.

6.4 ((cid:12))

in appendix c, we give an example of a matrix that has positive elements but
that has a negative eigenvalue and hence that is not positive de   nite. find an example
of the converse property, namely a 2    2 matrix with positive eigenvalues yet that
has at least one negative element.

6.5 ((cid:12)) www verify the results (6.13) and (6.14) for constructing valid kernels.

6.6 ((cid:12)) verify the results (6.15) and (6.16) for constructing valid kernels.

6.7 ((cid:12)) www verify the results (6.17) and (6.18) for constructing valid kernels.

6.8 ((cid:12)) verify the results (6.19) and (6.20) for constructing valid kernels.

6.9 ((cid:12)) verify the results (6.21) and (6.22) for constructing valid kernels.
6.10 ((cid:12)) show that an excellent choice of kernel for learning a function f(x) is given
by k(x, x(cid:4)) = f(x)f(x(cid:4)) by showing that a linear learning machine based on this
kernel will always    nd a solution proportional to f(x).

6.11 ((cid:12)) by making use of the expansion (6.25), and then expanding the middle factor
as a power series, show that the gaussian kernel (6.23) can be expressed as the inner
product of an in   nite-dimensional feature vector.

exercises

321

6.12 ((cid:12) (cid:12)) www consider the space of all possible subsets a of a given    xed set d.
show that the id81 (6.27) corresponds to an inner product in a feature
space of dimensionality 2|d|
de   ned by the mapping   (a) where a is a subset of d
and the element   u (a), indexed by the subset u, is given by

(cid:12)

  u (a) =

if u     a;
1,
0, otherwise.

(6.95)

here u     a denotes that u is either a subset of a or is equal to a.

6.13 ((cid:12)) show that the fisher kernel, de   ned by (6.33), remains invariant if we make
a nonlinear transformation of the parameter vector          (  ), where the function
  (  ) is invertible and differentiable.

6.14 ((cid:12)) www write down the form of the fisher kernel, de   ned by (6.33), for the
case of a distribution p(x|  ) = n (x|  , s) that is gaussian with mean    and    xed
covariance s.

6.15 ((cid:12)) by considering the determinant of a 2    2 gram matrix, show that a positive-

de   nite id81 k(x, x

(cid:4)) satis   es the cauchy-schwartz inequality

k(x1, x2)2 (cid:1) k(x1, x1)k(x2, x2).

(6.96)

6.16 ((cid:12) (cid:12)) consider a parametric model governed by the parameter vector w together
with a data set of input values x1, . . . , xn and a nonlinear feature mapping   (x).
suppose that the dependence of the error function on w takes the form

j(w) = f(wt  (x1), . . . , wt  (xn )) + g(wtw)

(6.97)

where g(  ) is a monotonically increasing function. by writing w in the form

n(cid:2)

w =

  n  (xn) + w   

(6.98)

n=1

show that the value of w that minimizes j(w) takes the form of a linear combination
of the basis functions   (xn) for n = 1, . . . , n.

6.17 ((cid:12) (cid:12)) www consider the sum-of-squares error function (6.39) for data having
noisy inputs, where   (  ) is the distribution of the noise. use the calculus of vari-
ations to minimize this error function with respect to the function y(x), and hence
show that the optimal solution is given by an expansion of the form (6.40) in which
the basis functions are given by (6.41).

322

6. kernel methods

6.18 ((cid:12)) consider a nadaraya-watson model with one input variable x and one target
variable t having gaussian components with isotropic covariances, so that the co-
variance matrix is given by   2i where i is the unit matrix. write down expressions
for the conditional density p(t|x) and for the conditional mean e[t|x] and variance
var[t|x], in terms of the id81 k(x, xn).

6.19 ((cid:12) (cid:12)) another viewpoint on kernel regression comes from a consideration of re-
gression problems in which the input variables as well as the target variables are
corrupted with additive noise. suppose each target value tn is generated as usual
by taking a function y(zn) evaluated at a point zn, and adding gaussian noise. the
value of zn is not directly observed, however, but only a noise corrupted version
xn = zn +   n where the random variable    is governed by some distribution g(  ).
consider a set of observations {xn, tn}, where n = 1, . . . , n, together with a cor-
responding sum-of-squares error function de   ned by averaging over the distribution
of input noise to give

{y(xn       n)     tn}2

g(  n) d  n.

(6.99)

(cid:6)

n(cid:2)

n=1

e =

1
2

by minimizing e with respect to the function y(z) using the calculus of variations
(appendix d), show that optimal solution for y(x) is given by a nadaraya-watson
kernel regression solution of the form (6.45) with a kernel of the form (6.46).

6.20 ((cid:12) (cid:12)) www verify the results (6.66) and (6.67).

6.21 ((cid:12) (cid:12)) www consider a gaussian process regression model in which the kernel
function is de   ned in terms of a    xed set of nonlinear basis functions. show that the
predictive distribution is identical to the result (3.58) obtained in section 3.3.2 for the
bayesian id75 model. to do this, note that both models have gaussian
predictive distributions, and so it is only necessary to show that the conditional mean
and variance are the same. for the mean, make use of the matrix identity (c.6), and
for the variance, make use of the matrix identity (c.7).

6.22 ((cid:12) (cid:12)) consider a regression problem with n training set input vectors x1, . . . , xn
and l test set input vectors xn +1, . . . , xn +l, and suppose we de   ne a gaussian
process prior over functions t(x). derive an expression for the joint predictive dis-
tribution for t(xn +1), . . . , t(xn +l), given the values of t(x1), . . . , t(xn ). show the
marginal of this distribution for one of the test observations tj where n + 1 (cid:1) j (cid:1)
n + l is given by the usual gaussian process regression result (6.66) and (6.67).

6.23 ((cid:12) (cid:12)) www consider a gaussian process regression model in which the target
variable t has dimensionality d. write down the conditional distribution of tn +1
for a test input vector xn +1, given a training set of input vectors x1, . . . , xn +1 and
corresponding target observations t1, . . . , tn .

6.24 ((cid:12)) show that a diagonal matrix w whose elements satisfy 0 < wii < 1 is positive
de   nite. show that the sum of two positive de   nite matrices is itself positive de   nite.

exercises

323

6.25 ((cid:12)) www using the newton-raphson formula (4.92), derive the iterative update
n of the posterior distribution in the gaussian

formula (6.83) for    nding the mode a(cid:1)
process classi   cation model.

6.26 ((cid:12)) using the result (2.115), derive the expressions (6.87) and (6.88) for the mean
and variance of the posterior distribution p(an +1|tn ) in the gaussian process clas-
si   cation model.

6.27 ((cid:12) (cid:12) (cid:12)) derive the result (6.90) for the log likelihood function in the laplace approx-
imation framework for gaussian process classi   cation. similarly, derive the results
(6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood.

7

sparse kernel

machines

in the previous chapter, we explored a variety of learning algorithms based on non-
linear kernels. one of the signi   cant limitations of many such algorithms is that
the id81 k(xn, xm) must be evaluated for all possible pairs xn and xm
of training points, which can be computationally infeasible during training and can
lead to excessive computation times when making predictions for new data points.
in this chapter we shall look at kernel-based algorithms that have sparse solutions,
so that predictions for new inputs depend only on the id81 evaluated at a
subset of the training data points.

we begin by looking in some detail at the support vector machine (id166), which
became popular in some years ago for solving problems in classi   cation, regression,
and novelty detection. an important property of support vector machines is that the
determination of the model parameters corresponds to a id76 prob-
lem, and so any local solution is also a global optimum. because the discussion of
support vector machines makes extensive use of lagrange multipliers, the reader is

325

326

7. sparse kernel machines

encouraged to review the key concepts covered in appendix e. additional infor-
mation on support vector machines can be found in vapnik (1995), burges (1998),
cristianini and shawe-taylor (2000), m  uller et al. (2001), sch  olkopf and smola
(2002), and herbrich (2002).

the id166 is a decision machine and so does not provide posterior probabilities.
we have already discussed some of the bene   ts of determining probabilities in sec-
tion 1.5.4. an alternative sparse kernel technique, known as the relevance vector
machine (rvm), is based on a bayesian formulation and provides posterior proba-
bilistic outputs, as well as having typically much sparser solutions than the id166.

section 7.2

7.1. maximum margin classi   ers

we begin our discussion of support vector machines by returning to the two-class
classi   cation problem using linear models of the form

y(x) = wt  (x) + b

(7.1)
where   (x) denotes a    xed feature-space transformation, and we have made the
bias parameter b explicit. note that we shall shortly introduce a dual representation
expressed in terms of id81s, which avoids having to work explicitly in
feature space. the training data set comprises n input vectors x1, . . . , xn , with
corresponding target values t1, . . . , tn where tn     {   1, 1}, and new data points x
are classi   ed according to the sign of y(x).

we shall assume for the moment that the training data set is linearly separable in
feature space, so that by de   nition there exists at least one choice of the parameters
w and b such that a function of the form (7.1) satis   es y(xn) > 0 for points having
tn = +1 and y(xn) < 0 for points having tn =    1, so that tny(xn) > 0 for all
training data points.

there may of course exist many such solutions that separate the classes exactly.
in section 4.1.7, we described the id88 algorithm that is guaranteed to    nd
a solution in a    nite number of steps. the solution that it    nds, however, will be
dependent on the (arbitrary) initial values chosen for w and b as well as on the
order in which the data points are presented. if there are multiple solutions all of
which classify the training data set exactly, then we should try to    nd the one that
will give the smallest generalization error. the support vector machine approaches
this problem through the concept of the margin, which is de   ned to be the smallest
distance between the decision boundary and any of the samples, as illustrated in
figure 7.1.

in support vector machines the decision boundary is chosen to be the one for
which the margin is maximized. the maximum margin solution can be motivated us-
ing computational learning theory, also known as statistical learning theory. how-
ever, a simple insight into the origins of maximum margin has been given by tong
and koller (2000) who consider a framework for classi   cation based on a hybrid of
generative and discriminative approaches. they    rst model the distribution over in-
put vectors x for each class using a parzen density estimator with gaussian kernels

section 7.1.5

y = 1

y = 0

y =    1

7.1. maximum margin classi   ers

327

y =    1
y = 0

y = 1

margin

figure 7.1 the margin is de   ned as the perpendicular distance between the decision boundary and the closest
of the data points, as shown on the left    gure. maximizing the margin leads to a particular choice of decision
boundary, as shown on the right. the location of this boundary is determined by a subset of the data points,
known as support vectors, which are indicated by the circles.

having a common parameter   2. together with the class priors, this de   nes an opti-
mal misclassi   cation-rate decision boundary. however, instead of using this optimal
boundary, they determine the best hyperplane by minimizing the id203 of error
relative to the learned density model. in the limit   2     0, the optimal hyperplane
is shown to be the one having maximum margin. the intuition behind this result is
that as   2 is reduced, the hyperplane is increasingly dominated by nearby data points
relative to more distant ones. in the limit, the hyperplane becomes independent of
data points that are not support vectors.

we shall see in figure 10.13 that marginalization with respect to the prior distri-
bution of the parameters in a bayesian approach for a simple linearly separable data
set leads to a decision boundary that lies in the middle of the region separating the
data points. the large margin solution has similar behaviour.
recall from figure 4.1 that the perpendicular distance of a point x from a hyper-
plane de   ned by y(x) = 0 where y(x) takes the form (7.1) is given by |y(x)|/(cid:5)w(cid:5).
furthermore, we are only interested in solutions for which all data points are cor-
rectly classi   ed, so that tny(xn) > 0 for all n. thus the distance of a point xn to the
decision surface is given by

.

(7.2)

tny(xn)

(cid:5)w(cid:5)

(cid:5)w(cid:5) = tn(wt  (xn) + b)
(cid:12)

(cid:8)

(cid:10)

1
(cid:5)w(cid:5) min

n

(cid:11)(cid:9)(cid:13)

the margin is given by the perpendicular distance to the closest point xn from the
data set, and we wish to optimize the parameters w and b in order to maximize this
distance. thus the maximum margin solution is found by solving

arg max

(7.3)
where we have taken the factor 1/(cid:5)w(cid:5) outside the optimization over n because w

wt  (xn) + b

tn

w,b

328

7. sparse kernel machines

(cid:10)

(cid:11)

does not depend on n. direct solution of this optimization problem would be very
complex, and so we shall convert it into an equivalent problem that is much easier
to solve. to do this we note that if we make the rescaling w       w and b       b,
then the distance from any point xn to the decision surface, given by tny(xn)/(cid:5)w(cid:5),
is unchanged. we can use this freedom to set

(cid:11)

tn

wt  (xn) + b

= 1

(7.4)

tn

(cid:2) 1,

for the point that is closest to the surface. in this case, all data points will satisfy the
constraints

wt  (xn) + b

n = 1, . . . , n.

(7.5)
this is known as the canonical representation of the decision hyperplane.
in the
case of data points for which the equality holds, the constraints are said to be active,
whereas for the remainder they are said to be inactive. by de   nition, there will
always be at least one active constraint, because there will always be a closest point,
and once the margin has been maximized there will be at least two active constraints.
the optimization problem then simply requires that we maximize (cid:5)w(cid:5)   1, which is
equivalent to minimizing (cid:5)w(cid:5)2, and so we have to solve the optimization problem

(cid:10)

arg min

w,b

(cid:5)w(cid:5)2

1
2

(7.6)

subject to the constraints given by (7.5). the factor of 1/2 in (7.6) is included for
later convenience. this is an example of a quadratic programming problem in which
we are trying to minimize a quadratic function subject to a set of linear inequality
constraints. it appears that the bias parameter b has disappeared from the optimiza-
tion. however, it is determined implicitly via the constraints, because these require
that changes to (cid:5)w(cid:5) be compensated by changes to b. we shall see how this works
shortly.
in order to solve this constrained optimization problem, we introduce lagrange
multipliers an (cid:2) 0, with one multiplier an for each of the constraints in (7.5), giving
the lagrangian function

(cid:5)w(cid:5)2     n(cid:2)

n=1

(cid:26)

l(w, b, a) =

1
2

(cid:27)

tn(wt  (xn) + b)     1

an

(7.7)

appendix e

where a = (a1, . . . , an )t. note the minus sign in front of the lagrange multiplier
term, because we are minimizing with respect to w and b, and maximizing with
respect to a. setting the derivatives of l(w, b, a) with respect to w and b equal to
zero, we obtain the following two conditions

antn  (xn)

n(cid:2)
n(cid:2)

n=1

antn.

n=1

w =

0 =

(7.8)

(7.9)

7.1. maximum margin classi   ers

329

eliminating w and b from l(w, b, a) using these conditions then gives the dual
representation of the maximum margin problem in which we maximize

n(cid:2)

n(cid:2)

an     1
2

n(cid:2)

(cid:4)l(a) =
n(cid:2)

n=1
with respect to a subject to the constraints

n=1

m=1

anamtntmk(xn, xm)

(7.10)

an (cid:2) 0,

n = 1, . . . , n,

antn = 0.

(7.11)

(7.12)

n=1

here the id81 is de   ned by k(x, x(cid:4)) =   (x)t  (x(cid:4)). again, this takes the
form of a quadratic programming problem in which we optimize a quadratic function
of a subject to a set of inequality constraints. we shall discuss techniques for solving
such quadratic programming problems in section 7.1.1.

the solution to a quadratic programming problem in m variables in general has
computational complexity that is o(m 3). in going to the dual formulation we have
turned the original optimization problem, which involved minimizing (7.6) over m
variables, into the dual problem (7.10), which has n variables. for a    xed set of
basis functions whose number m is smaller than the number n of data points, the
move to the dual problem appears disadvantageous. however, it allows the model to
be reformulated using kernels, and so the maximum margin classi   er can be applied
ef   ciently to feature spaces whose dimensionality exceeds the number of data points,
including in   nite feature spaces. the kernel formulation also makes clear the role
of the constraint that the id81 k(x, x(cid:4)) be positive de   nite, because this

ensures that the lagrangian function(cid:4)l(a) is bounded below, giving rise to a well-

de   ned optimization problem.
in order to classify new data points using the trained model, we evaluate the sign
of y(x) de   ned by (7.1). this can be expressed in terms of the parameters {an} and
the id81 by substituting for w using (7.8) to give

n(cid:2)

y(x) =

antnk(x, xn) + b.

(7.13)

n=1

joseph-louis lagrange
1736   1813

although widely considered to be
a french mathematician, lagrange
was born in turin in italy. by the age
of nineteen, he had already made
important contributions mathemat-
ics and had been appointed as pro-
fessor at the royal artillery school in turin. for many

years, euler worked hard to persuade lagrange to
move to berlin, which he eventually did in 1766 where
he succeeded euler as director of mathematics at
the berlin academy. later he moved to paris, nar-
rowly escaping with his life during the french revo-
lution thanks to the personal intervention of lavoisier
(the french chemist who discovered oxygen) who him-
self was later executed at the guillotine. lagrange
made key contributions to the calculus of variations
and the foundations of dynamics.

330

7. sparse kernel machines

in appendix e, we show that a constrained optimization of this form satis   es the
karush-kuhn-tucker (kkt) conditions, which in this case require that the following
three properties hold

an (cid:2) 0
tny(xn)     1 (cid:2) 0
an {tny(xn)     1} = 0.

(7.14)
(7.15)
(7.16)

thus for every data point, either an = 0 or tny(xn) = 1. any data point for
which an = 0 will not appear in the sum in (7.13) and hence plays no role in making
predictions for new data points. the remaining data points are called support vectors,
and because they satisfy tny(xn) = 1, they correspond to points that lie on the
maximum margin hyperplanes in feature space, as illustrated in figure 7.1. this
property is central to the practical applicability of support vector machines. once
the model is trained, a signi   cant proportion of the data points can be discarded and
only the support vectors retained.

having solved the quadratic programming problem and found a value for a, we
can then determine the value of the threshold parameter b by noting that any support
vector xn satis   es tny(xn) = 1. using (7.13) this gives

(cid:23)

amtmk(xn, xm) + b

= 1

(7.17)

(cid:22)(cid:2)

tn

m   s

where s denotes the set of indices of the support vectors. although we can solve
this equation for b using an arbitrarily chosen support vector xn, a numerically more
n = 1,
stable solution is obtained by    rst multiplying through by tn, making use of t2
and then averaging these equations over all support vectors and solving for b to give

(cid:23)

(cid:22)

(cid:2)

n   s

(cid:2)

m   s

b =

1
ns

tn    

amtmk(xn, xm)

(7.18)

where ns is the total number of support vectors.

for later comparison with alternative models, we can express the maximum-
margin classi   er in terms of the minimization of an error function, with a simple
quadratic regularizer, in the form

e   (y(xn)tn     1) +   (cid:5)w(cid:5)2

(7.19)

n=1

where e   (z) is a function that is zero if z (cid:2) 0 and     otherwise and ensures that
the constraints (7.5) are satis   ed. note that as long as the id173 parameter
satis   es    > 0, its precise value plays no role.

figure 7.2 shows an example of the classi   cation resulting from training a sup-
port vector machine on a simple synthetic data set using a gaussian kernel of the

n(cid:2)

7.1. maximum margin classi   ers

331

figure 7.2 example of synthetic data from
two classes in two dimensions
showing contours of constant
y(x) obtained from a support
vector machine having a gaus-
sian id81. also shown
are the decision boundary,
the
margin boundaries, and the sup-
port vectors.

form (6.23). although the data set is not linearly separable in the two-dimensional
data space x, it is linearly separable in the nonlinear feature space de   ned implicitly
by the nonlinear id81. thus the training data points are perfectly separated
in the original data space.

this example also provides a geometrical insight into the origin of sparsity in
the id166. the maximum margin hyperplane is de   ned by the location of the support
vectors. other data points can be moved around freely (so long as they remain out-
side the margin region) without changing the decision boundary, and so the solution
will be independent of such data points.

7.1.1 overlapping class distributions
so far, we have assumed that the training data points are linearly separable in the
feature space   (x). the resulting support vector machine will give exact separation
of the training data in the original input space x, although the corresponding decision
boundary will be nonlinear. in practice, however, the class-conditional distributions
may overlap, in which case exact separation of the training data can lead to poor
generalization.

we therefore need a way to modify the support vector machine so as to allow
some of the training points to be misclassi   ed. from (7.19) we see that in the case
of separable classes, we implicitly used an error function that gave in   nite error
if a data point was misclassi   ed and zero error if it was classi   ed correctly, and
then optimized the model parameters to maximize the margin. we now modify this
approach so that data points are allowed to be on the    wrong side    of the margin
boundary, but with a penalty that increases with the distance from that boundary. for
the subsequent optimization problem, it is convenient to make this penalty a linear
function of this distance. to do this, we introduce slack variables,   n (cid:2) 0 where
n = 1, . . . , n, with one slack variable for each training data point (bennett, 1992;
cortes and vapnik, 1995). these are de   ned by   n = 0 for data points that are on or
inside the correct margin boundary and   n = |tn     y(xn)| for other points. thus a
data point that is on the decision boundary y(xn) = 0 will have   n = 1, and points

332

7. sparse kernel machines

figure 7.3 illustration of the slack variables   n (cid:2) 0.
data points with circles around them are
support vectors.

y =    1
y = 0

   > 1

y = 1

   < 1

   = 0

   = 0

tny(xn) (cid:2) 1       n,

with   n > 1 will be misclassi   ed. the exact classi   cation constraints (7.5) are then
replaced with

n = 1, . . . , n

(7.20)
in which the slack variables are constrained to satisfy   n (cid:2) 0. data points for which
  n = 0 are correctly classi   ed and are either on the margin or on the correct side
of the margin. points for which 0 <   n (cid:1) 1 lie inside the margin, but on the cor-
rect side of the decision boundary, and those data points for which   n > 1 lie on
the wrong side of the decision boundary and are misclassi   ed, as illustrated in fig-
ure 7.3. this is sometimes described as relaxing the hard margin constraint to give a
soft margin and allows some of the training set data points to be misclassi   ed. note
that while slack variables allow for overlapping class distributions, this framework is
still sensitive to outliers because the penalty for misclassi   cation increases linearly
with   .

our goal is now to maximize the margin while softly penalizing points that lie

on the wrong side of the margin boundary. we therefore minimize

n(cid:2)

c

n=1

  n +

(cid:5)w(cid:5)2

1
2

(7.21)

(cid:5)

where the parameter c > 0 controls the trade-off between the slack variable penalty
and the margin. because any point that is misclassi   ed has   n > 1, it follows that
n   n is an upper bound on the number of misclassi   ed points. the parameter c is
therefore analogous to (the inverse of) a id173 coef   cient because it controls
the trade-off between minimizing training errors and controlling model complexity.
in the limit c        , we will recover the earlier support vector machine for separable
data.
  n (cid:2) 0. the corresponding lagrangian is given by

we now wish to minimize (7.21) subject to the constraints (7.20) together with

n(cid:2)

  n    n(cid:2)

an {tny(xn)     1 +   n}    n(cid:2)

n=1

n=1

n=1

  n  n (7.22)

l(w, b, a) =

(cid:5)w(cid:5)2 + c

1
2

appendix e

where {an (cid:2) 0} and {  n (cid:2) 0} are lagrange multipliers. the corresponding set of
kkt conditions are given by

7.1. maximum margin classi   ers

333

an (cid:2) 0
tny(xn)     1 +   n (cid:2) 0
an (tny(xn)     1 +   n) = 0
  n (cid:2) 0
  n (cid:2) 0
  n  n = 0

(7.23)
(7.24)
(7.25)
(7.26)
(7.27)
(7.28)

where n = 1, . . . , n.

we now optimize out w, b, and {  n} making use of the de   nition (7.1) of y(x)

to give

   l
   w

   l
   b

n(cid:2)

antn  (xn)

= 0     w =

= 0     n(cid:2)

n=1

n=1

antn = 0

= 0     an = c       n.

(7.31)
using these results to eliminate w, b, and {  n} from the lagrangian, we obtain the
dual lagrangian in the form

   l
     n

n(cid:2)

n=1

(cid:4)l(a) =

n(cid:2)

n(cid:2)

n=1

m=1

an     1
2

anamtntmk(xn, xm)

(7.32)

which is identical to the separable case, except that the constraints are somewhat
different. to see what these constraints are, we note that an (cid:2) 0 is required because
these are lagrange multipliers. furthermore, (7.31) together with   n (cid:2) 0 implies
an (cid:1) c. we therefore have to minimize (7.32) with respect to the dual variables
{an} subject to

(7.29)

(7.30)

(7.33)

(7.34)

0 (cid:1) an (cid:1) c

n(cid:2)

antn = 0

n=1

for n = 1, . . . , n, where (7.33) are known as box constraints. this again represents
a quadratic programming problem. if we substitute (7.29) into (7.1), we see that
predictions for new data points are again made by using (7.13).

we can now interpret the resulting solution. as before, a subset of the data
points may have an = 0, in which case they do not contribute to the predictive

334

7. sparse kernel machines

model (7.13). the remaining data points constitute the support vectors. these have
an > 0 and hence from (7.25) must satisfy

tny(xn) = 1       n.

(7.35)

if an < c, then (7.31) implies that   n > 0, which from (7.28) requires   n = 0 and
hence such points lie on the margin. points with an = c can lie inside the margin
and can either be correctly classi   ed if   n (cid:1) 1 or misclassi   ed if   n > 1.

to determine the parameter b in (7.1), we note that those support vectors for

which 0 < an < c have   n = 0 so that tny(xn) = 1 and hence will satisfy

(cid:23)

tn

amtmk(xn, xm) + b

= 1.

(7.36)

(cid:23)

again, a numerically stable solution is obtained by averaging to give

amtmk(xn, xm)

(7.37)

where m denotes the set of indices of data points having 0 < an < c.

an alternative, equivalent formulation of the support vector machine, known as
the   -id166, has been proposed by sch  olkopf et al. (2000). this involves maximizing

(cid:22)(cid:2)
(cid:2)

m   s

(cid:22)

(cid:2)

m   s

b =

1
nm

n   m

tn    

(cid:4)l(a) =    1

2

n(cid:2)

n(cid:2)

n=1

m=1

anamtntmk(xn, xm)

(7.38)

subject to the constraints

0 (cid:1) an (cid:1) 1/n

n(cid:2)
n(cid:2)

n=1

antn = 0

an (cid:2)   .

(7.39)

(7.40)

(7.41)

n=1

this approach has the advantage that the parameter   , which replaces c, can be
interpreted as both an upper bound on the fraction of margin errors (points for which
  n > 0 and hence which lie on the wrong side of the margin boundary and which may
or may not be misclassi   ed) and a lower bound on the fraction of support vectors. an
example of the   -id166 applied to a synthetic data set is shown in figure 7.4. here
gaussian kernels of the form exp (     (cid:5)x     x(cid:4)(cid:5)2) have been used, with    = 0.45.

although predictions for new inputs are made using only the support vectors,
the training phase (i.e., the determination of the parameters a and b) makes use of
the whole data set, and so it is important to have ef   cient algorithms for solving

7.1. maximum margin classi   ers

335

figure 7.4 illustration of the   -id166 applied
to a nonseparable data set in two
dimensions. the support vectors
are indicated by circles.

2

0

   2

the quadratic programming problem. we    rst note that the objective function(cid:4)l(a)

   2

0

2

given by (7.10) or (7.32) is quadratic and so any local optimum will also be a global
optimum provided the constraints de   ne a convex region (which they do as a conse-
quence of being linear). direct solution of the quadratic programming problem us-
ing traditional techniques is often infeasible due to the demanding computation and
memory requirements, and so more practical approaches need to be found. the tech-
nique of chunking (vapnik, 1982) exploits the fact that the value of the lagrangian
is unchanged if we remove the rows and columns of the kernel matrix corresponding
to lagrange multipliers that have value zero. this allows the full quadratic pro-
gramming problem to be broken down into a series of smaller ones, whose goal is
eventually to identify all of the nonzero lagrange multipliers and discard the others.
chunking can be implemented using protected conjugate gradients (burges, 1998).
although chunking reduces the size of the matrix in the quadratic function from the
number of data points squared to approximately the number of nonzero lagrange
multipliers squared, even this may be too big to    t in memory for large-scale appli-
cations. decomposition methods (osuna et al., 1996) also solve a series of smaller
quadratic programming problems but are designed so that each of these is of a    xed
size, and so the technique can be applied to arbitrarily large data sets. however, it
still involves numerical solution of quadratic programming subproblems and these
can be problematic and expensive. one of the most popular approaches to training
support vector machines is called sequential minimal optimization, or smo (platt,
1999). it takes the concept of chunking to the extreme limit and considers just two
lagrange multipliers at a time. in this case, the subproblem can be solved analyti-
cally, thereby avoiding numerical quadratic programming altogether. heuristics are
given for choosing the pair of lagrange multipliers to be considered at each step.
in practice, smo is found to have a scaling with the number of data points that is
somewhere between linear and quadratic depending on the particular application.

we have seen that id81s correspond to inner products in feature spaces
that can have high, or even in   nite, dimensionality. by working directly in terms of
the id81, without introducing the feature space explicitly, it might there-
fore seem that support vector machines somehow manage to avoid the curse of di-

336

7. sparse kernel machines

section 1.4

section 4.3.2

mensionality. this is not the case, however, because there are constraints amongst
the feature values that restrict the effective dimensionality of feature space. to see
this consider a simple second-order polynomial kernel that we can expand in terms
of its components

(cid:11)2 = (1 + x1z1 + x2z2)2

k(x, z) =

1z2
= 1 + 2x1z1 + 2x2z2 + x2
   
2x1x2, x2
= (1,
=   (x)t  (z).

   
2x2, x2
1,

2z2
1 + 2x1z1x2z2 + x2
   
2
2z2, z2
1,

2)(1,

2z1,

   

   
2z1z2, z2

2)t
(7.42)

(cid:10)

1 + xtz
   
2x1,

this id81 therefore represents an inner product in a feature space having
six dimensions, in which the mapping from input space to feature space is described
by the vector function   (x). however, the coef   cients weighting these different
features are constrained to have speci   c forms. thus any set of points in the original
two-dimensional space x would be constrained to lie exactly on a two-dimensional
nonlinear manifold embedded in the six-dimensional feature space.

we have already highlighted the fact that the support vector machine does not
provide probabilistic outputs but instead makes classi   cation decisions for new in-
put vectors. veropoulos et al. (1999) discuss modi   cations to the id166 to allow
the trade-off between false positive and false negative errors to be controlled. how-
ever, if we wish to use the id166 as a module in a larger probabilistic system, then
probabilistic predictions of the class label t for new inputs x are required.

to address this issue, platt (2000) has proposed    tting a logistic sigmoid to the
outputs of a previously trained support vector machine. speci   cally, the required
id155 is assumed to be of the form

p(t = 1|x) =    (ay(x) + b)

(7.43)

where y(x) is de   ned by (7.1). values for the parameters a and b are found by
minimizing the cross-id178 error function de   ned by a training set consisting of
pairs of values y(xn) and tn. the data used to    t the sigmoid needs to be independent
of that used to train the original id166 in order to avoid severe over-   tting. this two-
stage approach is equivalent to assuming that the output y(x) of the support vector
machine represents the log-odds of x belonging to class t = 1. because the id166
training procedure is not speci   cally intended to encourage this, the id166 can give
a poor approximation to the posterior probabilities (tipping, 2001).

7.1.2 relation to id28
as with the separable case, we can re-cast the id166 for nonseparable distri-
butions in terms of the minimization of a regularized error function. this will also
allow us to highlight similarities, and differences, compared to the id28
model.
we have seen that for data points that are on the correct side of the margin
boundary, and which therefore satisfy yntn (cid:2) 1, we have   n = 0, and for the

7.1. maximum margin classi   ers

337

e(z)

figure 7.5 plot of the    hinge    error function used
in support vector machines, shown
in blue, along with the error function
for id28, rescaled by a
factor of 1/ ln(2) so that it passes
through the point (0, 1), shown in red.
also shown are the misclassi   cation
error in black and the squared error
in green.

   2

   1

0

1

z

2

n(cid:2)

remaining points we have   n = 1     yntn. thus the objective function (7.21) can be
written (up to an overall multiplicative constant) in the form

esv(yntn) +   (cid:5)w(cid:5)2

(7.44)

n=1

where    = (2c)   1, and esv(  ) is the hinge error function de   ned by

esv(yntn) = [1     yntn]+

(7.45)
where [   ]+ denotes the positive part. the hinge error function, so-called because
of its shape, is plotted in figure 7.5. it can be viewed as an approximation to the
misclassi   cation error, i.e., the error function that ideally we would like to minimize,
which is also shown in figure 7.5.
when we considered the id28 model in section 4.3.2, we found it
convenient to work with target variable t     {0, 1}. for comparison with the support
vector machine, we    rst reformulate maximum likelihood id28 using
the target variable t     {   1, 1}. to do this, we note that p(t = 1|y) =   (y) where
y(x) is given by (7.1), and   (y) is the logistic sigmoid function de   ned by (4.59). it
follows that p(t =    1|y) = 1       (y) =   (   y), where we have used the properties
of the logistic sigmoid function, and so we can write

p(t|y) =   (yt).

(7.46)

exercise 7.6

from this we can construct an error function by taking the negative logarithm of the
likelihood function that, with a quadratic regularizer, takes the form

n(cid:2)

elr(yntn) +   (cid:5)w(cid:5)2.

where

n=1

elr(yt) = ln (1 + exp(   yt)) .

(7.47)

(7.48)

338

7. sparse kernel machines

for comparison with other error functions, we can divide by ln(2) so that the error
function passes through the point (0, 1). this rescaled error function is also plotted
in figure 7.5 and we see that it has a similar form to the support vector error function.
the key difference is that the    at region in esv(yt) leads to sparse solutions.

both the logistic error and the hinge loss can be viewed as continuous approx-
imations to the misclassi   cation error. another continuous error function that has
sometimes been used to solve classi   cation problems is the squared error, which
is again plotted in figure 7.5. it has the property, however, of placing increasing
emphasis on data points that are correctly classi   ed but that are a long way from
the decision boundary on the correct side. such points will be strongly weighted at
the expense of misclassi   ed points, and so if the objective is to minimize the mis-
classi   cation rate, then a monotonically decreasing error function would be a better
choice.

7.1.3 multiclass id166s
the support vector machine is fundamentally a two-class classi   er. in practice,
however, we often have to tackle problems involving k > 2 classes. various meth-
ods have therefore been proposed for combining multiple two-class id166s in order
to build a multiclass classi   er.
one commonly used approach (vapnik, 1998) is to construct k separate id166s,
in which the kth model yk(x) is trained using the data from class ck as the positive
examples and the data from the remaining k     1 classes as the negative examples.
this is known as the one-versus-the-rest approach. however, in figure 4.2 we saw
that using the decisions of the individual classi   ers can lead to inconsistent results
in which an input is assigned to multiple classes simultaneously. this problem is
sometimes addressed by making predictions for new inputs x using

y(x) = max

k

yk(x).

(7.49)

unfortunately, this heuristic approach suffers from the problem that the different
classi   ers were trained on different tasks, and there is no guarantee that the real-
valued quantities yk(x) for different classi   ers will have appropriate scales.

another problem with the one-versus-the-rest approach is that the training sets
are imbalanced. for instance, if we have ten classes each with equal numbers of
training data points, then the individual classi   ers are trained on data sets comprising
90% negative examples and only 10% positive examples, and the symmetry of the
original problem is lost. a variant of the one-versus-the-rest scheme was proposed
by lee et al. (2001) who modify the target values so that the positive class has target
+1 and the negative class has target    1/(k     1).

weston and watkins (1999) de   ne a single objective function for training all
k id166s simultaneously, based on maximizing the margin from each to remaining
classes. however, this can result in much slower training because, instead of solving
k separate optimization problems each over n data points with an overall cost of
o(kn 2), a single optimization problem of size (k     1)n must be solved giving an
overall cost of o(k 2n 2).

7.1. maximum margin classi   ers

339

another approach is to train k(k    1)/2 different 2-class id166s on all possible
pairs of classes, and then to classify test points according to which class has the high-
est number of    votes   , an approach that is sometimes called one-versus-one. again,
we saw in figure 4.2 that this can lead to ambiguities in the resulting classi   cation.
also, for large k this approach requires signi   cantly more training time than the
one-versus-the-rest approach. similarly, to evaluate test points, signi   cantly more
computation is required.

the latter problem can be alleviated by organizing the pairwise classi   ers into
a directed acyclic graph (not to be confused with a probabilistic graphical model)
leading to the dagid166 (platt et al., 2000). for k classes, the dagid166 has a total
of k(k     1)/2 classi   ers, and to classify a new test point only k     1 pairwise
classi   ers need to be evaluated, with the particular classi   ers used depending on
which path through the graph is traversed.

a different approach to multiclass classi   cation, based on error-correcting out-
put codes, was developed by dietterich and bakiri (1995) and applied to support
vector machines by allwein et al. (2000). this can be viewed as a generalization of
the voting scheme of the one-versus-one approach in which more general partitions
of the classes are used to train the individual classi   ers. the k classes themselves
are represented as particular sets of responses from the two-class classi   ers chosen,
and together with a suitable decoding scheme, this gives robustness to errors and to
ambiguity in the outputs of the individual classi   ers. although the application of
id166s to multiclass classi   cation problems remains an open issue, in practice the
one-versus-the-rest approach is the most widely used in spite of its ad-hoc formula-
tion and its practical limitations.

there are also single-class support vector machines, which solve an unsuper-
vised learning problem related to id203 density estimation. instead of mod-
elling the density of data, however, these methods aim to    nd a smooth boundary
enclosing a region of high density. the boundary is chosen to represent a quantile of
the density, that is, the id203 that a data point drawn from the distribution will
land inside that region is given by a    xed number between 0 and 1 that is speci   ed in
advance. this is a more restricted problem than estimating the full density but may
be suf   cient in speci   c applications. two approaches to this problem using support
vector machines have been proposed. the algorithm of sch  olkopf et al. (2001) tries
to    nd a hyperplane that separates all but a    xed fraction    of the training data from
the origin while at the same time maximizing the distance (margin) of the hyperplane
from the origin, while tax and duin (1999) look for the smallest sphere in feature
space that contains all but a fraction    of the data points. for kernels k(x, x(cid:4)) that
are functions only of x     x(cid:4)

, the two algorithms are equivalent.

7.1.4 id166s for regression
we now extend support vector machines to regression problems while at the
same time preserving the property of sparseness. in simple id75, we

section 3.1.4

340

7. sparse kernel machines

figure 7.6 plot of an  -insensitive error function (in
red) in which the error increases lin-
early with distance beyond the insen-
sitive region. also shown for compar-
ison is the quadratic error function (in
green).

e(z)

    

0

 

z

minimize a regularized error function given by

{yn     tn}2 +   
2

(cid:5)w(cid:5)2.

(7.50)

to obtain sparse solutions, the quadratic error function is replaced by an  -insensitive
error function (vapnik, 1995), which gives zero error if the absolute difference be-
tween the prediction y(x) and the target t is less than   where   > 0. a simple
example of an  -insensitive error function, having a linear cost associated with errors
outside the insensitive region, is given by

e (y(x)     t) =

0,
|y(x)     t|      , otherwise

if |y(x)     t| <  ;

and is illustrated in figure 7.6.

we therefore minimize a regularized error function given by

n(cid:2)

n=1

1
2

(cid:12)

n(cid:2)

c

n=1

e (y(xn)     tn) +

(cid:5)w(cid:5)2

1
2

where y(x) is given by (7.1). by convention the (inverse) id173 parameter,
denoted c, appears in front of the error term.
as before, we can re-express the optimization problem by introducing slack
variables. for each data point xn, we now need two slack variables   n (cid:2) 0 and
corresponds to a point for which tn < y(xn)      , as illustrated in figure 7.7.

(cid:1)  n (cid:2) 0, where   n > 0 corresponds to a point for which tn > y(xn) +  , and(cid:1)  n > 0

the condition for a target point to lie inside the  -tube is that yn       (cid:1) tn (cid:1)
yn+ , where yn = y(xn). introducing the slack variables allows points to lie outside
the tube provided the slack variables are nonzero, and the corresponding conditions
are

tn (cid:1) y(xn) +   +   n

tn (cid:2) y(xn)          (cid:1)  n.

(7.51)

(7.52)

(7.53)
(7.54)

7.1. maximum margin classi   ers

341

figure 7.7 illustration of id166 regression, showing
the regression curve together with the  -
insensitive    tube   . also shown are exam-
ples of the slack variables    and b  . points
above the  -tube have    > 0 and b   = 0,
points below the  -tube have    = 0 and
b   > 0, and points inside the  -tube have
   = b   = 0.

y(x)

   > 0

(cid:1)   > 0

y +  

y
y      

x

the error function for support vector regression can then be written as

n(cid:2)

(  n +(cid:1)  n) +

(cid:5)w(cid:5)2

1
2

(7.55)

c

n=1

(7.53) and (7.54). this can be achieved by introducing lagrange multipliers an (cid:2) 0,

which must be minimized subject to the constraints   n (cid:2) 0 and(cid:1)  n (cid:2) 0 as well as
(cid:1)an (cid:2) 0,   n (cid:2) 0, and(cid:1)  n (cid:2) 0 and optimizing the lagrangian
(  n  n +(cid:1)  n
(cid:1)an(  +(cid:1)  n     yn + tn).

(cid:5)w(cid:5)2     n(cid:2)
(  n +(cid:1)  n) +
an(  +   n + yn     tn)     n(cid:2)
grangian with respect to w, b,   n, and(cid:1)  n to zero, giving

we now substitute for y(x) using (7.1) and then set the derivatives of the la-

n(cid:2)
    n(cid:2)

(cid:1)  n)

l = c

(7.56)

1
2

n=1

n=1

n=1

n=1

(an    (cid:1)an)  (xn)

n=1

= 0     w =

n(cid:2)
= 0     n(cid:2)
(an    (cid:1)an) = 0
= 0     (cid:1)an +(cid:1)  n = c.

= 0     an +   n = c

n=1

   l
   w

   l
   b

   l
     n
   l

   (cid:1)  n

(7.57)

(7.58)

(7.59)

(7.60)

exercise 7.7

using these results to eliminate the corresponding variables from the lagrangian, we
see that the dual problem involves maximizing

342

7. sparse kernel machines

n(cid:2)
n(cid:2)
(cid:4)l(a,(cid:1)a) =    1
n(cid:2)
(an +(cid:1)an) +

    

m=1

n=1

2

(an    (cid:1)an)(am    (cid:1)am)k(xn, xm)

n(cid:2)

(an    (cid:1)an)tn

n=1

with respect to {an} and {(cid:1)an}, where we have introduced the kernel k(x, x(cid:4)) =
we note that an (cid:2) 0 and(cid:1)an (cid:2) 0 are both required because these are lagrange
multipliers. also   n (cid:2) 0 and (cid:1)  n (cid:2) 0 together with (7.59) and (7.60), require
an (cid:1) c and(cid:1)an (cid:1) c, and so again we have the box constraints

  (x)t  (x(cid:4)). again, this is a constrained maximization, and to    nd the constraints

n=1

(7.61)

0 (cid:1) an (cid:1) c

0 (cid:1)(cid:1)an (cid:1) c
n(cid:2)

substituting (7.57) into (7.1), we see that predictions for new inputs can be made

together with the condition (7.58).

using

y(x) =

(an    (cid:1)an)k(x, xn) + b

which is again expressed in terms of the id81.

n=1

the corresponding karush-kuhn-tucker (kkt) conditions, which state that at
the solution the product of the dual variables and the constraints must vanish, are
given by

an(  +   n + yn     tn) = 0

(cid:1)an(  +(cid:1)  n     yn + tn) = 0
(c    (cid:1)an)(cid:1)  n = 0.

(c     an)  n = 0

and such points must lie either on or below the lower boundary of the  -tube.

from these we can obtain several useful results. first of all, we note that a coef   cient
an can only be nonzero if   +   n + yn     tn = 0, which implies that the data point
either lies on the upper boundary of the  -tube (  n = 0) or lies above the upper

boundary (  n > 0). similarly, a nonzero value for(cid:1)an implies   +(cid:1)  n     yn + tn = 0,
furthermore, the two constraints   +   n + yn     tn = 0 and   +(cid:1)  n     yn + tn = 0
(cid:1)  n are nonnegative while   is strictly positive, and so for every data point xn, either
an or(cid:1)an (or both) must be zero.
(7.64), in other words those for which either an (cid:9)= 0 or(cid:1)an (cid:9)= 0. these are points that

are incompatible, as is easily seen by adding them together and noting that   n and

the support vectors are those data points that contribute to predictions given by

lie on the boundary of the  -tube or outside the tube. all points within the tube have

(7.62)
(7.63)

(7.64)

(7.65)
(7.66)
(7.67)
(7.68)

an =(cid:1)an = 0. we again have a sparse solution, and the only terms that have to be

7.1. maximum margin classi   ers

343

evaluated in the predictive model (7.64) are those that involve the support vectors.

the parameter b can be found by considering a data point for which 0 < an <
c, which from (7.67) must have   n = 0, and from (7.65) must therefore satisfy
  + yn     tn = 0. using (7.1) and solving for b, we obtain

b = tn           wt  (xn)

= tn           n(cid:2)

(am    (cid:1)am)k(xn, xm)

(7.69)

for which 0 <(cid:1)an < c. in practice, it is better to average over all such estimates of

where we have used (7.57). we can obtain an analogous result by considering a point

m=1

b.

as with the classi   cation case, there is an alternative formulation of the id166
for regression in which the parameter governing complexity has a more intuitive
interpretation (sch  olkopf et al., 2000). in particular, instead of    xing the width   of
the insensitive region, we    x instead a parameter    that bounds the fraction of points
lying outside the tube. this involves maximizing

n(cid:2)
n(cid:2)
(cid:4)l(a,(cid:1)a) =    1
n(cid:2)
(an    (cid:1)an)tn

m=1

n=1

+

2

(an    (cid:1)an)(am    (cid:1)am)k(xn, xm)

subject to the constraints

n=1

0 (cid:1) an (cid:1) c/n

0 (cid:1)(cid:1)an (cid:1) c/n
n(cid:2)
(an    (cid:1)an) = 0
n(cid:2)
(an +(cid:1)an) (cid:1)   c.

n=1

(7.70)

(7.71)
(7.72)

(7.73)

(7.74)

n=1

it can be shown that there are at most   n data points falling outside the insensitive
tube, while at least   n data points are support vectors and so lie either on the tube
or outside it.

the use of a support vector machine to solve a regression problem is illustrated
using the sinusoidal data set in figure 7.8. here the parameters    and c have been
chosen by hand. in practice, their values would typically be determined by cross-
validation.

appendix a

344

7. sparse kernel machines

figure 7.8 illustration of the   -id166 for re-
gression applied to the sinusoidal
synthetic data set using gaussian
kernels. the predicted regression
curve is shown by the red line, and
the  -insensitive tube corresponds
to the shaded region. also, the
data points are shown in green,
and those with support vectors
are indicated by blue circles.

t

1

0

   1

0

x

1

7.1.5 computational learning theory
historically, support vector machines have largely been motivated and analysed
using a theoretical framework known as computational learning theory, also some-
times called statistical learning theory (anthony and biggs, 1992; kearns and vazi-
rani, 1994; vapnik, 1995; vapnik, 1998). this has its origins with valiant (1984)
who formulated the probably approximately correct, or pac, learning framework.
the goal of the pac framework is to understand how large a data set needs to be in
order to give good generalization. it also gives bounds for the computational cost of
learning, although we do not consider these here.
suppose that a data set d of size n is drawn from some joint distribution p(x, t)
where x is the input variable and t represents the class label, and that we restrict
attention to    noise free    situations in which the class labels are determined by some
(unknown) deterministic function t = g(x). in pac learning we say that a function
f(x;d), drawn from a space f of such functions on the basis of the training set
d, has good generalization if its expected error rate is below some pre-speci   ed
threshold  , so that
(7.75)
where i(  ) is the indicator function, and the expectation is with respect to the dis-
tribution p(x, t). the quantity on the left-hand side is a random variable, because
it depends on the training set d, and the pac framework requires that (7.75) holds,
with id203 greater than 1       , for a data set d drawn randomly from p(x, t).
here    is another pre-speci   ed parameter, and the terminology    probably approxi-
mately correct    comes from the requirement that with high id203 (greater than
1      ), the error rate be small (less than  ). for a given choice of model space f, and
for given parameters   and   , pac learning aims to provide bounds on the minimum
size n of data set needed to meet this criterion. a key quantity in pac learning is
the vapnik-chervonenkis dimension, or vc dimension, which provides a measure of
the complexity of a space of functions, and which allows the pac framework to be
extended to spaces containing an in   nite number of functions.

ex,t [i (f(x;d) (cid:9)= t)] <  

the bounds derived within the pac framework are often described as worst-

7.2. relevance vector machines

345

case, because they apply to any choice for the distribution p(x, t), so long as both
the training and the test examples are drawn (independently) from the same distribu-
tion, and for any choice for the function f(x) so long as it belongs to f. in real-world
applications of machine learning, we deal with distributions that have signi   cant reg-
ularity, for example in which large regions of input space carry the same class label.
as a consequence of the lack of any assumptions about the form of the distribution,
the pac bounds are very conservative, in other words they strongly over-estimate
the size of data sets required to achieve a given generalization performance. for this
reason, pac bounds have found few, if any, practical applications.
one attempt to improve the tightness of the pac bounds is the pac-bayesian
framework (mcallester, 2003), which considers a distribution over the space f of
functions, somewhat analogous to the prior in a bayesian treatment. this still con-
siders any possible choice for p(x, t), and so although the bounds are tighter, they
are still very conservative.

7.2. relevance vector machines

support vector machines have been used in a variety of classi   cation and regres-
sion applications. nevertheless, they suffer from a number of limitations, several
of which have been highlighted already in this chapter. in particular, the outputs of
an id166 represent decisions rather than posterior probabilities. also, the id166 was
originally formulated for two classes, and the extension to k > 2 classes is prob-
lematic. there is a complexity parameter c, or    (as well as a parameter   in the case
of regression), that must be found using a hold-out method such as cross-validation.
finally, predictions are expressed as linear combinations of id81s that are
centred on training data points and that are required to be positive de   nite.

the relevance vector machine or rvm (tipping, 2001) is a bayesian sparse ker-
nel technique for regression and classi   cation that shares many of the characteristics
of the id166 whilst avoiding its principal limitations. additionally, it typically leads
to much sparser models resulting in correspondingly faster performance on test data
whilst maintaining comparable generalization error.

in contrast to the id166 we shall    nd it more convenient to introduce the regres-

sion form of the rvm    rst and then consider the extension to classi   cation tasks.

7.2.1 rvm for regression
the relevance vector machine for regression is a linear model of the form studied
in chapter 3 but with a modi   ed prior that results in sparse solutions. the model
de   nes a conditional distribution for a real-valued target variable t, given an input
vector x, which takes the form

p(t|x, w,   ) = n (t|y(x),   

   1)

(7.76)

346

7. sparse kernel machines

m(cid:2)

n(cid:2)

n=1

where    =   
by a linear model of the form

   2 is the noise precision (inverse noise variance), and the mean is given

y(x) =

wi  i(x) = wt  (x)

(7.77)

i=1

with    xed nonlinear basis functions   i(x), which will typically include a constant
term so that the corresponding weight parameter represents a    bias   .

the relevance vector machine is a speci   c instance of this model, which is in-
tended to mirror the structure of the support vector machine. in particular, the basis
functions are given by kernels, with one kernel associated with each of the data
points from the training set. the general expression (7.77) then takes the id166-like
form

y(x) =

wnk(x, xn) + b

(7.78)

where b is a bias parameter. the number of parameters in this case is m = n + 1,
and y(x) has the same form as the predictive model (7.64) for the id166, except that
the coef   cients an are here denoted wn. it should be emphasized that the subsequent
analysis is valid for arbitrary choices of basis function, and for generality we shall
work with the form (7.77). in contrast to the id166, there is no restriction to positive-
de   nite kernels, nor are the basis functions tied in either number or location to the
training data points.

suppose we are given a set of n observations of the input vector x, which we
n with n = 1, . . . , n. the
denote collectively by a data matrix x whose nth row is xt
corresponding target values are given by t = (t1, . . . , tn)t. thus, the likelihood
function is given by

p(t|x, w,   ) =

p(tn|xn, w,   

   1).

(7.79)

next we introduce a prior distribution over the parameter vector w and as in
chapter 3, we shall consider a zero-mean gaussian prior. however, the key differ-
ence in the rvm is that we introduce a separate hyperparameter   i for each of the
weight parameters wi instead of a single shared hyperparameter. thus the weight
prior takes the form

p(w|  ) =

n (wi|0,   

   1
i )

(7.80)

i=1

where   i represents the precision of the corresponding parameter wi, and    denotes
(  1, . . . ,   m )t. we shall see that, when we maximize the evidence with respect
to these hyperparameters, a signi   cant proportion of them go to in   nity, and the
corresponding weight parameters have posterior distributions that are concentrated
at zero. the basis functions associated with these parameters therefore play no role

n(cid:14)

n=1

m(cid:14)

7.2. relevance vector machines

347

in the predictions made by the model and so are effectively pruned out, resulting in
a sparse model.

using the result (3.49) for id75 models, we see that the posterior

distribution for the weights is again gaussian and takes the form

p(w|t, x,   ,   ) = n (w|m,   )

(7.81)

where the mean and covariance are given by

(cid:10)

(cid:11)   1

m =       tt
   =

(7.82)
(7.83)
where    is the n    m design matrix with elements   ni =   i(xn), and a =
diag(  i). note that in the speci   c case of the model (7.78), we have    = k, where
k is the symmetric (n + 1)    (n + 1) kernel matrix with elements k(xn, xm).

a +     t  

the values of    and    are determined using type-2 maximum likelihood, also
known as the evidence approximation, in which we maximize the marginal likeli-
hood function obtained by integrating out the weight parameters
p(t|x, w,   )p(w|  ) dw.

p(t|x,   ,   ) =

(7.84)

(cid:6)

because this represents the convolution of two gaussians, it is readily evaluated to
give the log marginal likelihood in the form

ln p(t|x,   ,   ) = lnn (t|0, c)

(cid:26)

(cid:27)
n ln(2  ) + ln|c| + ttc   1t

=    1
2

where t = (t1, . . . , tn)t, and we have de   ned the n    n matrix c given by

c =   

   1i +   a   1  t.

(7.85)

(7.86)

section 3.5

exercise 7.10

our goal is now to maximize (7.85) with respect to the hyperparameters    and
  . this requires only a small modi   cation to the results obtained in section 3.5 for
the evidence approximation in the id75 model. again, we can identify
two approaches. in the    rst, we simply set the required derivatives of the marginal
likelihood to zero and obtain the following re-estimation equations

exercise 7.12

  new

i

(  new)   1 =

=   i
m2
i
(cid:5)t       m(cid:5)2
i   i

n    (cid:5)

(7.87)

(7.88)

section 3.5.3

where mi is the ith component of the posterior mean m de   ned by (7.82). the
quantity   i measures how well the corresponding parameter wi is determined by the
data and is de   ned by

348

7. sparse kernel machines

  i = 1       i  ii

(7.89)
in which   ii is the ith diagonal component of the posterior covariance    given by
(7.83). learning therefore proceeds by choosing initial values for    and   , evalu-
ating the mean and covariance of the posterior using (7.82) and (7.83), respectively,
and then alternately re-estimating the hyperparameters, using (7.87) and (7.88), and
re-estimating the posterior mean and covariance, using (7.82) and (7.83), until a suit-
able convergence criterion is satis   ed.

exercise 9.23

section 7.2.2

exercise 7.14

section 6.4.2

the second approach is to use the em algorithm, and is discussed in sec-
tion 9.3.4. these two approaches to    nding the values of the hyperparameters that
maximize the evidence are formally equivalent. numerically, however, it is found
that the direct optimization approach corresponding to (7.87) and (7.88) gives some-
what faster convergence (tipping, 2001).
as a result of the optimization, we    nd that a proportion of the hyperparameters
{  i} are driven to large (in principle in   nite) values, and so the weight parameters
wi corresponding to these hyperparameters have posterior distributions with mean
and variance both zero. thus those parameters, and the corresponding basis func-
tions   i(x), are removed from the model and play no role in making predictions for
new inputs. in the case of models of the form (7.78), the inputs xn corresponding to
the remaining nonzero weights are called relevance vectors, because they are iden-
ti   ed through the mechanism of automatic relevance determination, and are analo-
gous to the support vectors of an id166. it is worth emphasizing, however, that this
mechanism for achieving sparsity in probabilistic models through automatic rele-
vance determination is quite general and can be applied to any model expressed as
an adaptive linear combination of basis functions.

having found values   (cid:1) and   (cid:1) for the hyperparameters that maximize the
marginal likelihood, we can evaluate the predictive distribution over t for a new
input x. using (7.76) and (7.81), this is given by

p(t|x, x, t,   (cid:1),   (cid:1)) =

p(t|x, w,   (cid:1))p(w|x, t,   (cid:1),   (cid:1)) dw
t|mt  (x),   2(x)

.

(cid:11)

(7.90)

(cid:6)
= n(cid:10)

thus the predictive mean is given by (7.76) with w set equal to the posterior mean
m, and the variance of the predictive distribution is given by
  2(x) = (  (cid:1))   1 +   (x)t    (x)

(7.91)

where    is given by (7.83) in which    and    are set to their optimized values   (cid:1) and
  (cid:1). this is just the familiar result (3.59) obtained in the context of id75.
recall that for localized basis functions, the predictive variance for id75
models becomes small in regions of input space where there are no basis functions.
in the case of an rvm with the basis functions centred on data points, the model will
therefore become increasingly certain of its predictions when extrapolating outside
the domain of the data (rasmussen and qui  nonero-candela, 2005), which of course
is undesirable. the predictive distribution in gaussian process regression does not

7.2. relevance vector machines

349

figure 7.9 illustration of rvm regression us-
ing the same data set, and the
same gaussian id81s,
as used in figure 7.8 for
the
  -id166 regression model.
the
mean of
the predictive distribu-
tion for the rvm is shown by the
red line, and the one standard-
deviation predictive distribution is
shown by the shaded region.
also, the data points are shown
in green, and the relevance vec-
tors are indicated by blue circles.
note that there are only 3 rele-
vance vectors compared to 7 sup-
port vectors for the   -id166 in fig-
ure 7.8.

t

1

0

   1

0

x

1

suffer from this problem. however, the computational cost of making predictions
with a gaussian processes is typically much higher than with an rvm.

figure 7.9 shows an example of the rvm applied to the sinusoidal regression
data set. here the noise precision parameter    is also determined through evidence
maximization. we see that the number of relevance vectors in the rvm is signif-
icantly smaller than the number of support vectors used by the id166. for a wide
range of regression and classi   cation tasks, the rvm is found to give models that
are typically an order of magnitude more compact than the corresponding support
vector machine, resulting in a signi   cant improvement in the speed of processing on
test data. remarkably, this greater sparsity is achieved with little or no reduction in
generalization error compared with the corresponding id166.

the principal disadvantage of the rvm compared to the id166 is that training
involves optimizing a nonconvex function, and training times can be longer than for a
comparable id166. for a model with m basis functions, the rvm requires inversion
of a matrix of size m    m, which in general requires o(m 3) computation. in the
speci   c case of the id166-like model (7.78), we have m = n +1. as we have noted,
there are techniques for training id166s whose cost is roughly quadratic in n. of
course, in the case of the rvm we always have the option of starting with a smaller
number of basis functions than n + 1. more signi   cantly, in the relevance vector
machine the parameters governing complexity and noise variance are determined
automatically from a single training run, whereas in the support vector machine the
parameters c and   (or   ) are generally found using cross-validation, which involves
multiple training runs. furthermore, in the next section we shall derive an alternative
procedure for training the relevance vector machine that improves training speed
signi   cantly.

7.2.2 analysis of sparsity
we have noted earlier that the mechanism of automatic relevance determination
causes a subset of parameters to be driven to zero. we now examine in more detail

350

7. sparse kernel machines

t2

c

t

t1

t2

  

c

t

t1

figure 7.10 illustration of the mechanism for sparsity in a bayesian id75 model, showing a training
set vector of target values given by t = (t1, t2)t, indicated by the cross, for a model with one basis vector
   = (  (x1),   (x2))t, which is poorly aligned with the target data vector t. on the left we see a model having
only isotropic noise, so that c =      1i, corresponding to    =    , with    set to its most probable value. on
the right we see the same model but with a    nite value of   . in each case the red ellipse corresponds to unit
mahalanobis distance, with |c| taking the same value for both plots, while the dashed green circle shows the
contrition arising from the noise term      1. we see that any    nite value of    reduces the id203 of the
observed data, and so for the most probable solution the basis vector is removed.

the mechanism of sparsity in the context of the relevance vector machine. in the
process, we will arrive at a signi   cantly faster procedure for optimizing the hyper-
parameters compared to the direct techniques given above.

before proceeding with a mathematical analysis, we    rst give some informal
insight into the origin of sparsity in bayesian linear models. consider a data set
comprising n = 2 observations t1 and t2, together with a model having a single
basis function   (x), with hyperparameter   , along with isotropic noise having pre-
cision   . from (7.85), the marginal likelihood is given by p(t|  ,   ) = n (t|0, c) in
which the covariance matrix takes the form

c =

1
  

i +

1
  

    t

(7.92)

where    denotes the n-dimensional vector (  (x1),   (x2))t, and similarly t =
(t1, t2)t. notice that this is just a zero-mean gaussian process model over t with
covariance c. given a particular observation for t, our goal is to    nd   (cid:1) and   (cid:1) by
maximizing the marginal likelihood. we see from figure 7.10 that, if there is a poor
alignment between the direction of    and that of the training data vector t, then the
corresponding hyperparameter    will be driven to    , and the basis vector will be
pruned from the model. this arises because any    nite value for    will always assign
a lower id203 to the data, thereby decreasing the value of the density at t, pro-
vided that    is set to its optimal value. we see that any    nite value for    would cause
the distribution to be elongated in a direction away from the data, thereby increasing
the id203 mass in regions away from the observed data and hence reducing the
value of the density at the target data vector itself. for the more general case of m

7.2. relevance vector machines

351

basis vectors   1, . . . ,   m a similar intuition holds, namely that if a particular basis
vector is poorly aligned with the data vector t, then it is likely to be pruned from the
model.

we now investigate the mechanism for sparsity from a more mathematical per-
spective, for a general case involving m basis functions. to motivate this analysis
we    rst note that, in the result (7.87) for re-estimating the parameter   i, the terms on
the right-hand side are themselves also functions of   i. these results therefore rep-
resent implicit solutions, and iteration would be required even to determine a single
  i with all other   j for j (cid:9)= i    xed.

this suggests a different approach to solving the optimization problem for the
rvm, in which we make explicit all of the dependence of the marginal likelihood
(7.85) on a particular   i and then determine its stationary points explicitly (faul and
tipping, 2002; tipping and faul, 2003). to do this, we    rst pull out the contribution
from   i in the matrix c de   ned by (7.86) to give

(cid:2)

c =   

   1i +

   1
j   j  t

j +   

  

= c   i +   

j(cid:9)=i
   1
i   i  t
i

   1
i   i  t
i

(7.93)

where   i denotes the ith column of   , in other words the n-dimensional vector with
elements (  i(x1), . . . ,   i(xn )), in contrast to   n, which denotes the nth row of   .
the matrix c   i represents the matrix c with the contribution from basis function i
removed. using the matrix identities (c.7) and (c.15), the determinant and inverse
of c can then be written

|c| = |c   i||1 +   
   1   i     c
c   1 = c

   1   i   i|
   1
i   t
i c
   1   i   i  t
   1   i
i c
   1   i   i
  i +   t
i c

.

(7.94)

(7.95)

using these results, we can then write the log marginal likelihood function (7.85) in
the form

(7.96)
where l(     i) is simply the log marginal likelihood with basis function   i omitted,
and the quantity   (  i) is de   ned by

l(  ) = l(     i) +   (  i)

(cid:29)

(cid:30)

  (  i) =

1
2

ln   i     ln (  i + si) + q2

i

  i + si

(7.97)

and contains all of the dependence on   i. here we have introduced the two quantities

si =   t
qi =   t

i c
i c

   1   i   i
   1   i t.

(7.98)
(7.99)

here si is called the sparsity and qi is known as the quality of   i, and as we shall
see, a large value of si relative to the value of qi means that the basis function   i

exercise 7.15

352

7. sparse kernel machines

of

the

figure 7.11 plots
log
likelihood   (  i) versus
marginal
ln   i showing on the left, the single
maximum at a    nite   i for q2
i = 4
and si = 1 (so that q2
i > si) and on
the right, the maximum at   i =    
for q2
i = 1 and si = 2 (so that
q2
i < si).

2

0

   2

   4

2

0

   2

   4

   5

0

5

   5

0

5

is more likely to be pruned from the model. the    sparsity    measures the extent to
which basis function   i overlaps with the other basis vectors in the model, and the
   quality    represents a measure of the alignment of the basis vector   n with the error
between the training set values t = (t1, . . . , tn)t and the vector y   i of predictions
that would result from the model with the vector   i excluded (tipping and faul,
2003).

the stationary points of the marginal likelihood with respect to   i occur when

the derivative

d  (  i)

=   

   1
i s2

i     (q2
2(  i + si)2

i     si)

(7.100)
is equal to zero. there are two possible forms for the solution. recalling that   i (cid:2) 0,
we see that if q2
i > si, we
can solve for   i to obtain

i < si, then   i         provides a solution. conversely, if q2

d  i

  i = s2

i

i     si
q2

.

(7.101)

exercise 7.16

these two solutions are illustrated in figure 7.11. we see that the relative size of
the quality and sparsity terms determines whether a particular basis vector will be
pruned from the model or not. a more complete analysis (faul and tipping, 2002),
based on the second derivatives of the marginal likelihood, con   rms these solutions
are indeed the unique maxima of   (  i).

note that this approach has yielded a closed-form solution for   i, for given
values of the other hyperparameters. as well as providing insight into the origin of
sparsity in the rvm, this analysis also leads to a practical algorithm for optimizing
the hyperparameters that has signi   cant speed advantages. this uses a    xed set
of candidate basis vectors, and then cycles through them in turn to decide whether
each vector should be included in the model or not. the resulting sequential sparse
bayesian learning algorithm is described below.

sequential sparse bayesian learning algorithm

1. if solving a regression problem, initialize   .
2. initialize using one basis function   1, with hyperparameter   1 set using
(7.101), with the remaining hyperparameters   j for j (cid:9)= i initialized to
in   nity, so that only   1 is included in the model.

7.2. relevance vector machines

353

3. evaluate    and m, along with qi and si for all basis functions.
4. select a candidate basis function   i.
5. if q2

i > si, and   i <    , so that the basis vector   i is already included in
i > si, and   i =    , then add   i to the model, and evaluate hyperpa-
i (cid:1) si, and   i <     then remove basis function   i from the model,

the model, then update   i using (7.101).

rameter   i using (7.101).
and set   i =    .

7. if q2

6. if q2

8. if solving a regression problem, update   .
9. if converged terminate, otherwise go to 3.

note that if q2
from the model and no action is required.

i (cid:1) si and   i =    , then the basis function   i is already excluded

in practice, it is convenient to evaluate the quantities

qi =   t
si =   t

i c   1t
i c   1  i.

the quality and sparseness variables can then be expressed in the form

(7.102)
(7.103)

(7.104)

(7.105)
note that when   i =    , we have qi = qi and si = si. using (c.7), we can write

.

si =

qi =     t
si =     t

i t       2  t
i   i       2  t

i       tt

i       t  i

(7.106)
(7.107)

where    and    involve only those basis vectors that correspond to    nite hyperpa-
rameters   i. at each stage the required computations therefore scale like o(m 3),
where m is the number of active basis vectors in the model and is typically much
smaller than the number n of training patterns.

7.2.3 rvm for classi   cation
we can extend the relevance vector machine framework to classi   cation prob-
lems by applying the ard prior over weights to a probabilistic linear classi   cation
model of the kind studied in chapter 4. to start with, we consider two-class prob-
lems with a binary target variable t     {0, 1}. the model now takes the form of a
linear combination of basis functions transformed by a logistic sigmoid function

(cid:10)

(cid:11)

y(x, w) =   

wt  (x)

(7.108)

qi =

  iqi
  i     si
  isi
  i     si

exercise 7.17

354

7. sparse kernel machines

section 4.4

exercise 7.18

where   (  ) is the logistic sigmoid function de   ned by (4.59).
if we introduce a
gaussian prior over the weight vector w, then we obtain the model that has been
considered already in chapter 4. the difference here is that in the rvm, this model
uses the ard prior (7.80) in which there is a separate precision hyperparameter
associated with each weight parameter.

in contrast to the regression model, we can no longer integrate analytically over
the parameter vector w. here we follow tipping (2001) and use the laplace ap-
proximation, which was applied to the closely related problem of bayesian logistic
regression in section 4.5.1.

we begin by initializing the hyperparameter vector   . for this given value of
  , we then build a gaussian approximation to the posterior distribution and thereby
obtain an approximation to the marginal likelihood. maximization of this approxi-
mate marginal likelihood then leads to a re-estimated value for   , and the process is
repeated until convergence.

let us consider the laplace approximation for this model in more detail. for
a    xed value of   , the mode of the posterior distribution over w is obtained by
maximizing

ln p(w|t,   ) = ln{p(t|w)p(w|  )}     ln p(t|  )
{tn ln yn + (1     tn) ln(1     yn)}     1
2

=

n(cid:2)

n=1

wtaw + const (7.109)

where a = diag(  i). this can be done using iterative reweighted least squares
(irls) as discussed in section 4.3.3. for this, we need the gradient vector and
hessian matrix of the log posterior distribution, which from (7.109) are given by

       ln p(w|t,   ) =    (cid:10)

(cid:11)
    ln p(w|t,   ) =   t(t     y)     aw
  tb   + a

(7.110)
(7.111)
where b is an n    n diagonal matrix with elements bn = yn(1     yn), the vector
y = (y1, . . . , yn)t, and    is the design matrix with elements   ni =   i(xn). here
we have used the property (4.88) for the derivative of the logistic sigmoid function.
at convergence of the irls algorithm, the negative hessian represents the inverse
covariance matrix for the gaussian approximation to the posterior distribution.

the mode of the resulting approximation to the posterior distribution, corre-
sponding to the mean of the gaussian approximation, is obtained setting (7.110) to
zero, giving the mean and covariance of the laplace approximation in the form

(cid:10)

w(cid:1) = a   1  t(t     y)
  tb   + a
   =

(cid:11)   1

.

(7.112)

(7.113)

we can now use this laplace approximation to evaluate the marginal likelihood.
using the general result (4.135) for an integral evaluated using the laplace approxi-

exercise 7.19

7.2. relevance vector machines

355

mation, we have

(cid:6)
(cid:7) p(t|w(cid:1))p(w(cid:1)|  )(2  )m/2|  |1/2.

p(t|w)p(w|  ) dw

p(t|  ) =

(7.114)
if we substitute for p(t|w(cid:1)) and p(w(cid:1)|  ) and then set the derivative of the marginal
likelihood with respect to   i equal to zero, we obtain

   1
2

(w(cid:1)

i )2 +

1
2  i

    1
2

  ii = 0.

(7.115)

de   ning   i = 1       i  ii and rearranging then gives

i =   i
  new
(w(cid:1)
i )2

(7.116)

which is identical to the re-estimation formula (7.87) obtained for the regression
rvm.

if we de   ne

(cid:1)t =   w(cid:1) + b   1(t     y)
(cid:19)
(cid:20)
n ln(2  ) + ln|c| + ((cid:1)t)tc   1(cid:1)t

ln p(t|  ,   ) =    1
2

we can write the approximate log marginal likelihood in the form

where

c = b +   a  t.

(7.117)

(7.118)

(7.119)

appendix a

section 13.3

this takes the same form as (7.85) in the regression case, and so we can apply the
same analysis of sparsity and obtain the same fast learning algorithm in which we
fully optimize a single hyperparameter   i at each step.

figure 7.12 shows the relevance vector machine applied to a synthetic classi   -
cation data set. we see that the relevance vectors tend not to lie in the region of the
decision boundary, in contrast to the support vector machine. this is consistent with
our earlier discussion of sparsity in the rvm, because a basis function   i(x) centred
on a data point near the boundary will have a vector   i that is poorly aligned with
the training data vector t.

one of the potential advantages of the relevance vector machine compared with
the id166 is that it makes probabilistic predictions. for example, this allows the rvm
to be used to help construct an emission density in a nonlinear extension of the linear
dynamical system for tracking faces in video sequences (williams et al., 2005).

so far, we have considered the rvm for binary classi   cation problems. for
k > 2 classes, we again make use of the probabilistic approach in section 4.3.4 in
which there are k linear models of the form

ak = wt

k x

(7.120)

356

7. sparse kernel machines

2

0

   2

   2

0

2

figure 7.12 example of the relevance vector machine applied to a synthetic data set, in which the left-hand plot
shows the decision boundary and the data points, with the relevance vectors indicated by circles. comparison
with the results shown in figure 7.4 for the corresponding support vector machine shows that the rvm gives a
much sparser model. the right-hand plot shows the posterior id203 given by the rvm output in which the
proportion of red (blue) ink indicates the id203 of that point belonging to the red (blue) class.

which are combined using a softmax function to give outputs

exp(ak)(cid:2)

exp(aj)

yk(x) =

.

(7.121)

the log likelihood function is then given by

j

ln p(t|w1, . . . , wk) =

n(cid:14)

k(cid:14)

n=1

k=1

ytnk
nk

(7.122)

where the target values tnk have a 1-of-k coding for each data point n, and t is a
matrix with elements tnk. again, the laplace approximation can be used to optimize
the hyperparameters (tipping, 2001), in which the model and its hessian are found
using irls. this gives a more principled approach to multiclass classi   cation than
the pairwise method used in the support vector machine and also provides probabilis-
tic predictions for new data points. the principal disadvantage is that the hessian
matrix has size m k  m k, where m is the number of active basis functions, which
gives an additional factor of k 3 in the computational cost of training compared with
the two-class rvm.

the principal disadvantage of the relevance vector machine is the relatively long
training times compared with the id166. this is offset, however, by the avoidance of
cross-validation runs to set the model complexity parameters. furthermore, because
it yields sparser models, the computation time on test points, which is usually the
more important consideration in practice, is typically much less.

exercises

357

exercises

7.1 ((cid:12) (cid:12)) www suppose we have a data set of input vectors {xn} with corresponding
target values tn     {   1, 1}, and suppose that we model the density of input vec-
tors within each class separately using a parzen kernel density estimator (see sec-
tion 2.5.1) with a kernel k(x, x(cid:4)). write down the minimum misclassi   cation-rate
decision rule assuming the two classes have equal prior id203. show also that,
if the kernel is chosen to be k(x, x(cid:4)) = xtx(cid:4)
, then the classi   cation rule reduces to
simply assigning a new input vector to the class having the closest mean. finally,
show that, if the kernel takes the form k(x, x(cid:4)) =   (x)t  (x(cid:4)), that the classi   cation
is based on the closest mean in the feature space   (x).

7.2 ((cid:12)) show that, if the 1 on the right-hand side of the constraint (7.5) is replaced by
some arbitrary constant    > 0, the solution for the maximum margin hyperplane is
unchanged.

7.3 ((cid:12) (cid:12))

show that, irrespective of the dimensionality of the data space, a data set
consisting of just two data points, one from each class, is suf   cient to determine the
location of the maximum-margin hyperplane.

7.4 ((cid:12) (cid:12)) www show that the value    of the margin for the maximum-margin hyper-

plane is given by

n(cid:2)

1
  2 =

(7.123)
where {an} are given by maximizing (7.10) subject to the constraints (7.11) and
(7.12).

an

n=1

7.5 ((cid:12) (cid:12)) show that the values of    and {an} in the previous exercise also satisfy

  2 = 2(cid:4)l(a)
where(cid:4)l(a) is de   ned by (7.10). similarly, show that
  2 = (cid:5)w(cid:5)2.

1

1

(7.125)
7.6 ((cid:12)) consider the id28 model with a target variable t     {   1, 1}. if
we de   ne p(t = 1|y) =   (y) where y(x) is given by (7.1), show that the negative
log likelihood, with the addition of a quadratic id173 term, takes the form
(7.47).

7.7 ((cid:12)) consider the lagrangian (7.56) for the regression support vector machine. by

setting the derivatives of the lagrangian with respect to w, b,   n, and(cid:1)  n to zero and

then back substituting to eliminate the corresponding variables, show that the dual
lagrangian is given by (7.61).

(7.124)

358

7. sparse kernel machines

7.8 ((cid:12)) www for the regression support vector machine considered in section 7.1.4,
show that all training data points for which   n > 0 will have an = c, and similarly

all points for which(cid:1)  n > 0 will have(cid:1)an = c.

7.9 ((cid:12)) verify the results (7.82) and (7.83) for the mean and covariance of the posterior

distribution over weights in the regression rvm.

7.10 ((cid:12) (cid:12)) www derive the result (7.85) for the marginal likelihood function in the
regression rvm, by performing the gaussian integral over w in (7.84) using the
technique of completing the square in the exponential.

7.11 ((cid:12) (cid:12)) repeat the above exercise, but this time make use of the general result (2.115).

7.12 ((cid:12) (cid:12)) www show that direct maximization of the log marginal likelihood (7.85) for
the regression relevance vector machine leads to the re-estimation equations (7.87)
and (7.88) where   i is de   ned by (7.89).

7.13 ((cid:12) (cid:12))

in the evidence framework for rvm regression, we obtained the re-estimation
formulae (7.87) and (7.88) by maximizing the marginal likelihood given by (7.85).
extend this approach by inclusion of hyperpriors given by gamma distributions of
the form (b.26) and obtain the corresponding re-estimation formulae for    and    by
maximizing the corresponding posterior id203 p(t,   ,   |x) with respect to   
and   .

7.14 ((cid:12) (cid:12)) derive the result (7.90) for the predictive distribution in the relevance vector

machine for regression. show that the predictive variance is given by (7.91).

7.15 ((cid:12) (cid:12)) www using the results (7.94) and (7.95), show that the marginal likelihood
(7.85) can be written in the form (7.96), where   (  n) is de   ned by (7.97) and the
sparsity and quality factors are de   ned by (7.98) and (7.99), respectively.

7.16 ((cid:12)) by taking the second derivative of the log marginal likelihood (7.97) for the
regression rvm with respect to the hyperparameter   i, show that the stationary
point given by (7.101) is a maximum of the marginal likelihood.

7.17 ((cid:12) (cid:12)) using (7.83) and (7.86), together with the matrix identity (c.7), show that
the quantities sn and qn de   ned by (7.102) and (7.103) can be written in the form
(7.106) and (7.107).

7.18 ((cid:12)) www show that the gradient vector and hessian matrix of the log poste-
rior distribution (7.109) for the classi   cation relevance vector machine are given by
(7.110) and (7.111).

7.19 ((cid:12) (cid:12)) verify that maximization of the approximate log marginal likelihood function
(7.114) for the classi   cation relevance vector machine leads to the result (7.116) for
re-estimation of the hyperparameters.

8

graphical
models

probabilities play a central role in modern pattern recognition. we have seen in
chapter 1 that id203 theory can be expressed in terms of two simple equations
corresponding to the sum rule and the product rule. all of the probabilistic infer-
ence and learning manipulations discussed in this book, no matter how complex,
amount to repeated application of these two equations. we could therefore proceed
to formulate and solve complicated probabilistic models purely by algebraic ma-
nipulation. however, we shall    nd it highly advantageous to augment the analysis
using diagrammatic representations of id203 distributions, called probabilistic
id114. these offer several useful properties:

1. they provide a simple way to visualize the structure of a probabilistic model

and can be used to design and motivate new models.

2. insights into the properties of the model, including conditional independence

properties, can be obtained by inspection of the graph.

359

360

8. id114

3. complex computations, required to perform id136 and learning in sophis-
ticated models, can be expressed in terms of graphical manipulations, in which
underlying mathematical expressions are carried along implicitly.

a graph comprises nodes (also called vertices) connected by links (also known
as edges or arcs). in a probabilistic graphical model, each node represents a random
variable (or group of random variables), and the links express probabilistic relation-
ships between these variables. the graph then captures the way in which the joint
distribution over all of the random variables can be decomposed into a product of
factors each depending only on a subset of the variables. we shall begin by dis-
cussing id110s, also known as directed id114, in which the
links of the graphs have a particular directionality indicated by arrows. the other
major class of id114 are markov random    elds, also known as undirected
id114, in which the links do not carry arrows and have no directional
signi   cance. directed graphs are useful for expressing causal relationships between
random variables, whereas undirected graphs are better suited to expressing soft con-
straints between random variables. for the purposes of solving id136 problems,
it is often convenient to convert both directed and undirected graphs into a different
representation called a factor graph.

in this chapter, we shall focus on the key aspects of id114 as needed
for applications in pattern recognition and machine learning. more general treat-
ments of id114 can be found in the books by whittaker (1990), lauritzen
(1996), jensen (1996), castillo et al. (1997), jordan (1999), cowell et al. (1999),
and jordan (2007).

8.1. id110s

in order to motivate the use of directed graphs to describe id203 distributions,
consider    rst an arbitrary joint distribution p(a, b, c) over three variables a, b, and c.
note that at this stage, we do not need to specify anything further about these vari-
ables, such as whether they are discrete or continuous. indeed, one of the powerful
aspects of id114 is that a speci   c graph can make probabilistic statements
for a broad class of distributions. by application of the product rule of id203
(1.11), we can write the joint distribution in the form

p(a, b, c) = p(c|a, b)p(a, b).

(8.1)

a second application of the product rule, this time to the second term on the right-
hand side of (8.1), gives

p(a, b, c) = p(c|a, b)p(b|a)p(a).

(8.2)

note that this decomposition holds for any choice of the joint distribution. we now
represent the right-hand side of (8.2) in terms of a simple graphical model as follows.
first we introduce a node for each of the random variables a, b, and c and associate
each node with the corresponding conditional distribution on the right-hand side of

figure 8.1 a directed graphical model representing the joint probabil-
ity distribution over three variables a, b, and c, correspond-
ing to the decomposition on the right-hand side of (8.2).

a

b

8.1. id110s

361

c

(8.2). then, for each conditional distribution we add directed links (arrows) to the
graph from the nodes corresponding to the variables on which the distribution is
conditioned. thus for the factor p(c|a, b), there will be links from nodes a and b to
node c, whereas for the factor p(a) there will be no incoming links. the result is the
graph shown in figure 8.1. if there is a link going from a node a to a node b, then we
say that node a is the parent of node b, and we say that node b is the child of node a.
note that we shall not make any formal distinction between a node and the variable
to which it corresponds but will simply use the same symbol to refer to both.

an interesting point to note about (8.2) is that the left-hand side is symmetrical
with respect to the three variables a, b, and c, whereas the right-hand side is not.
indeed, in making the decomposition in (8.2), we have implicitly chosen a particular
ordering, namely a, b, c, and had we chosen a different ordering we would have
obtained a different decomposition and hence a different graphical representation.
we shall return to this point later.

for the moment let us extend the example of figure 8.1 by considering the joint
distribution over k variables given by p(x1, . . . , xk). by repeated application of
the product rule of id203, this joint distribution can be written as a product of
conditional distributions, one for each of the variables

p(x1, . . . , xk) = p(xk|x1, . . . , xk   1) . . . p(x2|x1)p(x1).

(8.3)

for a given choice of k, we can again represent this as a directed graph having k
nodes, one for each conditional distribution on the right-hand side of (8.3), with each
node having incoming links from all lower numbered nodes. we say that this graph
is fully connected because there is a link between every pair of nodes.

so far, we have worked with completely general joint distributions, so that the
decompositions, and their representations as fully connected graphs, will be applica-
ble to any choice of distribution. as we shall see shortly, it is the absence of links
in the graph that conveys interesting information about the properties of the class of
distributions that the graph represents. consider the graph shown in figure 8.2. this
is not a fully connected graph because, for instance, there is no link from x1 to x2 or
from x3 to x7.

we shall now go from this graph to the corresponding representation of the joint
id203 distribution written in terms of the product of a set of conditional dis-
tributions, one for each node in the graph. each such conditional distribution will
be conditioned only on the parents of the corresponding node in the graph. for in-
stance, x5 will be conditioned on x1 and x3. the joint distribution of all 7 variables

362

8. id114

figure 8.2 example of a directed acyclic graph describing the joint
distribution over variables x1, . . . , x7. the corresponding
decomposition of the joint distribution is given by (8.4).

x1

x2

x4

x3

x5

x6

x7

is therefore given by

p(x1)p(x2)p(x3)p(x4|x1, x2, x3)p(x5|x1, x3)p(x6|x4)p(x7|x4, x5).

(8.4)

the reader should take a moment to study carefully the correspondence between
(8.4) and figure 8.2.

k(cid:14)

we can now state in general terms the relationship between a given directed
graph and the corresponding distribution over the variables. the joint distribution
de   ned by a graph is given by the product, over all of the nodes of the graph, of
a conditional distribution for each node conditioned on the variables corresponding
to the parents of that node in the graph. thus, for a graph with k nodes, the joint
distribution is given by

p(xk|pak)

p(x) =

k=1

(8.5)
where pak denotes the set of parents of xk, and x = {x1, . . . , xk}. this key
equation expresses the factorization properties of the joint distribution for a directed
graphical model. although we have considered each node to correspond to a single
variable, we can equally well associate sets of variables and vector-valued variables
with the nodes of a graph. it is easy to show that the representation on the right-
hand side of (8.5) is always correctly normalized provided the individual conditional
distributions are normalized.

the directed graphs that we are considering are subject to an important restric-
tion namely that there must be no directed cycles, in other words there are no closed
paths within the graph such that we can move from node to node along links follow-
ing the direction of the arrows and end up back at the starting node. such graphs are
also called directed acyclic graphs, or dags. this is equivalent to the statement that
there exists an ordering of the nodes such that there are no links that go from any
node to any lower numbered node.

8.1.1 example: polynomial regression
as an illustration of the use of directed graphs to describe id203 distri-
butions, we consider the bayesian polynomial regression model introduced in sec-

exercise 8.1

exercise 8.2

8.1. id110s

363

figure 8.3 directed graphical model representing the joint
distribution (8.6) corresponding to the bayesian
polynomial regression model
introduced in sec-
tion 1.2.6.

w

t1

tn

tion 1.2.6. the random variables in this model are the vector of polynomial coef   -
cients w and the observed data t = (t1, . . . , tn)t. in addition, this model contains
the input data x = (x1, . . . , xn )t, the noise variance   2, and the hyperparameter   
representing the precision of the gaussian prior over w, all of which are parameters
of the model rather than random variables. focussing just on the random variables
for the moment, we see that the joint distribution is given by the product of the prior
p(w) and n conditional distributions p(tn|w) for n = 1, . . . , n so that

p(t, w) = p(w)

p(tn|w).

(8.6)

n=1

this joint distribution can be represented by a graphical model shown in figure 8.3.

when we start to deal with more complex models later in the book, we shall    nd
it inconvenient to have to write out multiple nodes of the form t1, . . . , tn explicitly as
in figure 8.3. we therefore introduce a graphical notation that allows such multiple
nodes to be expressed more compactly, in which we draw a single representative
node tn and then surround this with a box, called a plate, labelled with n indicating
that there are n nodes of this kind. re-writing the graph of figure 8.3 in this way,
we obtain the graph shown in figure 8.4.

we shall sometimes    nd it helpful to make the parameters of a model, as well as

its stochastic variables, explicit. in this case, (8.6) becomes

n(cid:14)

n(cid:14)

p(t, w|x,   ,   2) = p(w|  )

p(tn|w, xn,   2).

n=1

correspondingly, we can make x and    explicit in the graphical representation. to
do this, we shall adopt the convention that random variables will be denoted by open
circles, and deterministic parameters will be denoted by smaller solid circles. if we
take the graph of figure 8.4 and include the deterministic parameters, we obtain the
graph shown in figure 8.5.

when we apply a graphical model to a problem in machine learning or pattern
recognition, we will typically set some of the random variables to speci   c observed

figure 8.4 an alternative, more compact, representation of the graph
shown in figure 8.3 in which we have introduced a plate
(the box labelled n) that represents n nodes of which only
a single example tn is shown explicitly.

tn

n

w

364

8. id114

figure 8.5 this shows the same model as in figure 8.4 but
with the deterministic parameters shown explicitly
by the smaller solid nodes.

xn

  

  2

tn

n

w

values, for example the variables {tn} from the training set in the case of polynomial
curve    tting. in a graphical model, we will denote such observed variables by shad-
ing the corresponding nodes. thus the graph corresponding to figure 8.5 in which
the variables {tn} are observed is shown in figure 8.6. note that the value of w is
not observed, and so w is an example of a latent variable, also known as a hidden
variable. such variables play a crucial role in many probabilistic models and will
form the focus of chapters 9 and 12.
having observed the values {tn} we can, if desired, evaluate the posterior dis-
tribution of the polynomial coef   cients w as discussed in section 1.2.5. for the
moment, we note that this involves a straightforward application of bayes    theorem

n(cid:14)

p(w|t)     p(w)

p(tn|w)

(8.7)

n=1

where again we have omitted the deterministic parameters in order to keep the nota-
tion uncluttered.

in general, model parameters such as w are of little direct interest in themselves,
because our ultimate goal is to make predictions for new input values. suppose we

are given a new input value(cid:1)x and we wish to    nd the corresponding id203 dis-
tribution for(cid:1)t conditioned on the observed data. the graphical model that describes

this problem is shown in figure 8.7, and the corresponding joint distribution of all
of the random variables in this model, conditioned on the deterministic parameters,
is then given by

p((cid:1)t, t, w|(cid:1)x, x,   ,   2) =

p(tn|xn, w,   2)

p(w|  )p((cid:1)t|(cid:1)x, w,   2).

(8.8)

(cid:31)

n(cid:14)

 

n=1

figure 8.6 as in figure 8.5 but with the nodes {tn} shaded
to indicate that
the corresponding random vari-
ables have been set to their observed (training set)
values.

xn

  

  2

tn

n

w

8.1. id110s

365

figure 8.7 the polynomial regression model, corresponding
to figure 8.6, showing also a new input value bx
together with the corresponding model prediction
bt.

xn

  

w

tn

n

  2

the required predictive distribution for(cid:1)t is then obtained, from the sum rule of

  t

id203, by integrating out the model parameters w so that

p((cid:1)t|(cid:1)x, x, t,   ,   2)    

p((cid:1)t, t, w|(cid:1)x, x,   ,   2) dw

(cid:6)

  x

where we are implicitly setting the random variables in t to the speci   c values ob-
served in the data set. the details of this calculation were discussed in chapter 3.

8.1.2 generative models
there are many situations in which we wish to draw samples from a given prob-
ability distribution. although we shall devote the whole of chapter 11 to a detailed
discussion of sampling methods, it is instructive to outline here one technique, called
ancestral sampling, which is particularly relevant to id114. consider a
joint distribution p(x1, . . . , xk) over k variables that factorizes according to (8.5)
corresponding to a directed acyclic graph. we shall suppose that the variables have
been ordered such that there are no links from any node to any lower numbered node,
in other words each node has a higher number than any of its parents. our goal is to

draw a sample(cid:1)x1, . . . ,(cid:1)xk from the joint distribution.
distribution p(x1), which we call(cid:1)x1. we then work through each of the nodes in or-

to do this, we start with the lowest-numbered node and draw a sample from the
der, so that for node n we draw a sample from the conditional distribution p(xn|pan)
in which the parent variables have been set to their sampled values. note that at each
stage, these parent values will always be available because they correspond to lower-
numbered nodes that have already been sampled. techniques for sampling from
speci   c distributions will be discussed in detail in chapter 11. once we have sam-
pled from the    nal variable xk, we will have achieved our objective of obtaining a
sample from the joint distribution. to obtain a sample from some marginal distribu-
tion corresponding to a subset of the variables, we simply take the sampled values
for the required nodes and ignore the sampled values for the remaining nodes. for
example, to draw a sample from the distribution p(x2, x4), we simply sample from

the full joint distribution and then retain the values(cid:1)x2,(cid:1)x4 and discard the remaining
values {(cid:1)xj(cid:9)=2,4}.

366

8. id114

figure 8.8 a graphical model representing the process by which
images of objects are created, in which the identity
of an object (a discrete variable) and the position and
orientation of that object (continuous variables) have
independent prior probabilities. the image (a vector
of pixel intensities) has a id203 distribution that
is dependent on the identity of the object as well as
on its position and orientation.

object

position

orientation

image

for practical applications of probabilistic models, it will typically be the higher-
numbered variables corresponding to terminal nodes of the graph that represent the
observations, with lower-numbered nodes corresponding to latent variables. the
primary role of the latent variables is to allow a complicated distribution over the
observed variables to be represented in terms of a model constructed from simpler
(typically exponential family) conditional distributions.

we can interpret such models as expressing the processes by which the observed
data arose. for instance, consider an object recognition task in which each observed
data point corresponds to an image (comprising a vector of pixel intensities) of one
of the objects. in this case, the latent variables might have an interpretation as the
position and orientation of the object. given a particular observed image, our goal is
to    nd the posterior distribution over objects, in which we integrate over all possible
positions and orientations. we can represent this problem using a graphical model
of the form show in figure 8.8.

the graphical model captures the causal process (pearl, 1988) by which the ob-
served data was generated. for this reason, such models are often called generative
models. by contrast, the polynomial regression model described by figure 8.5 is
not generative because there is no id203 distribution associated with the input
variable x, and so it is not possible to generate synthetic data points from this model.
we could make it generative by introducing a suitable prior distribution p(x), at the
expense of a more complex model.

the hidden variables in a probabilistic model need not, however, have any ex-
plicit physical interpretation but may be introduced simply to allow a more complex
joint distribution to be constructed from simpler components.
in either case, the
technique of ancestral sampling applied to a generative model mimics the creation
of the observed data and would therefore give rise to    fantasy    data whose id203
distribution (if the model were a perfect representation of reality) would be the same
as that of the observed data. in practice, producing synthetic observations from a
generative model can prove informative in understanding the form of the id203
distribution represented by that model.

8.1.3 discrete variables
we have discussed the importance of id203 distributions that are members
of the exponential family, and we have seen that this family includes many well-
known distributions as particular cases. although such distributions are relatively
simple, they form useful building blocks for constructing more complex id203

section 2.4

8.1. id110s

367

figure 8.9 (a) this fully-connected graph describes a general distribu-
tion over two k-state discrete variables having a total of
k 2     1 parameters.
(b) by dropping the link between the
nodes, the number of parameters is reduced to 2(k     1).

x1

x1

(a)

(b)

x2

x2

distributions, and the framework of id114 is very useful in expressing the
way in which these building blocks are linked together.

such models have particularly nice properties if we choose the relationship be-
tween each parent-child pair in a directed graph to be conjugate, and we shall ex-
plore several examples of this shortly. two cases are particularly worthy of note,
namely when the parent and child node each correspond to discrete variables and
when they each correspond to gaussian variables, because in these two cases the
relationship can be extended hierarchically to construct arbitrarily complex directed
acyclic graphs. we begin by examining the discrete case.
the id203 distribution p(x|  ) for a single discrete variable x having k

possible states (using the 1-of-k representation) is given by

p(x|  ) =

  xk
k

k=1

(cid:5)
and is governed by the parameters    = (  1, . . . ,   k)t. due to the constraint
k   k = 1, only k     1 values for   k need to be speci   ed in order to de   ne the
now suppose that we have two discrete variables, x1 and x2, each of which has
k states, and we wish to model their joint distribution. we denote the id203 of
observing both x1k = 1 and x2l = 1 by the parameter   kl, where x1k denotes the
kth component of x1, and similarly for x2l. the joint distribution can be written

distribution.

(8.9)

k(cid:14)

p(x1, x2|  ) =

k(cid:14)

k(cid:14)

k=1

l=1

  x1kx2l
kl

.

(cid:5)

(cid:5)

k

l   kl = 1, this distri-
because the parameters   kl are subject to the constraint
bution is governed by k 2     1 parameters. it is easily seen that the total number of
parameters that must be speci   ed for an arbitrary joint distribution over m variables
is km     1 and therefore grows exponentially with the number m of variables.
using the product rule, we can factor the joint distribution p(x1, x2) in the form
p(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the
x1 node to the x2 node as shown in figure 8.9(a). the marginal distribution p(x1)
is governed by k     1 parameters, as before, similarly, the conditional distribution
p(x2|x1) requires the speci   cation of k     1 parameters for each of the k possible
values of x1. the total number of parameters that must be speci   ed in the joint
distribution is therefore (k     1) + k(k     1) = k 2     1 as before.

now suppose that the variables x1 and x2 were independent, corresponding to
the graphical model shown in figure 8.9(b). each variable is then described by

368

8. id114

figure 8.10 this chain of m discrete nodes, each
having k states, requires the speci   cation of k     1 +
(m     1)k(k     1) parameters, which grows linearly
with the length m of the chain. in contrast, a fully con-
nected graph of m nodes would have km     1 param-
eters, which grows exponentially with m.

x1

x2

xm

a separate multinomial distribution, and the total number of parameters would be
2(k     1). for a distribution over m independent discrete variables, each having k
states, the total number of parameters would be m(k     1), which therefore grows
linearly with the number of variables. from a graphical perspective, we have reduced
the number of parameters by dropping links in the graph, at the expense of having a
restricted class of distributions.

more generally, if we have m discrete variables x1, . . . , xm , we can model
the joint distribution using a directed graph with one variable corresponding to each
node. the conditional distribution at each node is given by a set of nonnegative pa-
rameters subject to the usual id172 constraint. if the graph is fully connected
then we have a completely general distribution having km     1 parameters, whereas
if there are no links in the graph the joint distribution factorizes into the product of
the marginals, and the total number of parameters is m(k     1). graphs having in-
termediate levels of connectivity allow for more general distributions than the fully
factorized one while requiring fewer parameters than the general joint distribution.
as an illustration, consider the chain of nodes shown in figure 8.10. the marginal
distribution p(x1) requires k     1 parameters, whereas each of the m     1 condi-
tional distributions p(xi|xi   1), for i = 2, . . . , m, requires k(k     1) parameters.
this gives a total parameter count of k     1 + (m     1)k(k     1), which is quadratic
in k and which grows linearly (rather than exponentially) with the length m of the
chain.

an alternative way to reduce the number of independent parameters in a model
is by sharing parameters (also known as tying of parameters). for instance, in the
chain example of figure 8.10, we can arrange that all of the conditional distributions
p(xi|xi   1), for i = 2, . . . , m, are governed by the same set of k(k   1) parameters.
together with the k   1 parameters governing the distribution of x1, this gives a total
of k 2     1 parameters that must be speci   ed in order to de   ne the joint distribution.
we can turn a graph over discrete variables into a bayesian model by introduc-
ing dirichlet priors for the parameters. from a graphical point of view, each node
then acquires an additional parent representing the dirichlet distribution over the pa-
rameters associated with the corresponding discrete node. this is illustrated for the
chain model in figure 8.11. the corresponding model in which we tie the parame-
ters governing the conditional distributions p(xi|xi   1), for i = 2, . . . , m, is shown
in figure 8.12.

another way of controlling the exponential growth in the number of parameters
in models of discrete variables is to use parameterized models for the conditional
distributions instead of complete tables of id155 values. to illus-
trate this idea, consider the graph in figure 8.13 in which all of the nodes represent
binary variables. each of the parent variables xi is governed by a single parame-

8.1. id110s

figure 8.11 an extension of the model of
figure 8.10 to include dirich-
let priors over
the param-
eters governing the discrete
distributions.

figure 8.12 as in figure 8.11 but with a sin-
gle set of parameters    shared
amongst all of the conditional
distributions p(xi|xi   1).

  1

x1

  1

x1

  2

x2

x2

  

369

  m

xm

xm

ter   i representing the id203 p(xi = 1), giving m parameters in total for the
parent nodes. the conditional distribution p(y|x1, . . . , xm ), however, would require
2m parameters representing the id203 p(y = 1) for each of the 2m possible
settings of the parent variables. thus in general the number of parameters required
to specify this conditional distribution will grow exponentially with m. we can ob-
tain a more parsimonious form for the conditional distribution by using a logistic
sigmoid function acting on a linear combination of the parent variables, giving

(cid:22)

(cid:23)

m(cid:2)

p(y = 1|x1, . . . , xm ) =   

w0 +

wixi

=   (wtx)

(8.10)

section 2.4

i=1

where   (a) = (1+exp(   a))   1 is the logistic sigmoid, x = (x0, x1, . . . , xm )t is an
(m + 1)-dimensional vector of parent states augmented with an additional variable
x0 whose value is clamped to 1, and w = (w0, w1, . . . , wm )t is a vector of m + 1
parameters. this is a more restricted form of conditional distribution than the general
case but is now governed by a number of parameters that grows linearly with m. in
this sense, it is analogous to the choice of a restrictive form of covariance matrix (for
example, a diagonal matrix) in a multivariate gaussian distribution. the motivation
for the logistic sigmoid representation was discussed in section 4.2.

figure 8.13 a graph comprising m parents x1, . . . , xm and a sin-
gle child y, used to illustrate the idea of parameterized
conditional distributions for discrete variables.

x1

xm

y

370

8. id114

8.1.4 linear-gaussian models
in the previous section, we saw how to construct joint id203 distributions
over a set of discrete variables by expressing the variables as nodes in a directed
acyclic graph. here we show how a multivariate gaussian can be expressed as a
directed graph corresponding to a linear-gaussian model over the component vari-
ables. this allows us to impose interesting structure on the distribution, with the
general gaussian and the diagonal covariance gaussian representing opposite ex-
tremes. several widely used techniques are examples of linear-gaussian models,
such as probabilistic principal component analysis, factor analysis, and linear dy-
namical systems (roweis and ghahramani, 1999). we shall make extensive use of
the results of this section in later chapters when we consider some of these techniques
in detail.

consider an arbitrary directed acyclic graph over d variables in which node i
represents a single continuous random variable xi having a gaussian distribution.
the mean of this distribution is taken to be a linear combination of the states of its
parent nodes pai of node i

      xi

(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)

(cid:2)

j   pai

      

p(xi|pai) = n

wijxj + bi, vi

(8.11)

where wij and bi are parameters governing the mean, and vi is the variance of the
conditional distribution for xi. the log of the joint distribution is then the log of the
product of these conditionals over all nodes in the graph and hence takes the form

ln p(x) =

d(cid:2)
=     d(cid:2)

i=1

i=1

ln p(xi|pai)

      xi    

      2

wijxj     bi

(cid:2)

j   pai

(8.12)

+ const

(8.13)

1
2vi

where x = (x1, . . . , xd)t and    const    denotes terms independent of x. we see that
this is a quadratic function of the components of x, and hence the joint distribution
p(x) is a multivariate gaussian.

we can determine the mean and covariance of the joint distribution recursively
as follows. each variable xi has (conditional on the states of its parents) a gaussian
distribution of the form (8.11) and so

   

xi =

wijxj + bi +

vi i

(8.14)

where  i is a zero mean, unit variance gaussian random variable satisfying e[ i] = 0
and e[ i j] = iij, where iij is the i, j element of the identity matrix. taking the
expectation of (8.14), we have

e[xi] =

wij e[xj] + bi.

(8.15)

(cid:2)

j   pai

(cid:2)

j   pai

figure 8.14 a directed graph over three gaussian variables,

with one missing link.

x1

x2

8.1. id110s

371

x3

thus we can    nd the components of e[x] = (e[x1], . . . , e[xd])t by starting at the
lowest numbered node and working recursively through the graph (here we again
assume that the nodes are numbered such that each node has a higher number than
its parents). similarly, we can use (8.14) and (8.15) to obtain the i, j element of the
covariance matrix for p(x) in the form of a recursion relation

cov[xi, xj] = e [(xi     e[xi])(xj     e[xj])]

      (xi     e[xi])
(cid:2)

         (cid:2)

k   paj

= e

=

wjkcov[xi, xk] + iijvj

k   paj

wjk(xk     e[xk]) +

   

vj j

      

         

(8.16)

and so the covariance can similarly be evaluated recursively starting from the lowest
numbered node.

let us consider two extreme cases. first of all, suppose that there are no links
in the graph, which therefore comprises d isolated nodes. in this case, there are no
parameters wij and so there are just d parameters bi and d parameters vi. from
the recursion relations (8.15) and (8.16), we see that the mean of p(x) is given by
(b1, . . . , bd)t and the covariance matrix is diagonal of the form diag(v1, . . . , vd).
the joint distribution has a total of 2d parameters and represents a set of d inde-
pendent univariate gaussian distributions.
now consider a fully connected graph in which each node has all lower num-
bered nodes as parents. the matrix wij then has i     1 entries on the ith row and
hence is a lower triangular matrix (with no entries on the leading diagonal). then
the total number of parameters wij is obtained by taking the number d2 of elements
in a d   d matrix, subtracting d to account for the absence of elements on the lead-
ing diagonal, and then dividing by 2 because the matrix has elements only below the
diagonal, giving a total of d(d   1)/2. the total number of independent parameters
{wij} and {vi} in the covariance matrix is therefore d(d + 1)/2 corresponding to
a general symmetric covariance matrix.

graphs having some intermediate level of complexity correspond to joint gaus-
sian distributions with partially constrained covariance matrices. consider for ex-
ample the graph shown in figure 8.14, which has a link missing between variables
x1 and x3. using the recursion relations (8.15) and (8.16), we see that the mean and
covariance of the joint distribution are given by

(cid:22)

   = (b1, b2 + w21b1, b3 + w32b2 + w32w21b1)t

   =

v1

w21v1
v2 + w2

w21v1

21v1
w32w21v1 w32(v2 + w2

21v1) v3 + w2

w32w21v1
w32(v2 + w2

21v1)
32(v2 + w2

21v1)

(cid:23)

(8.17)

. (8.18)

section 2.3

exercise 8.7

372

8. id114

we can readily extend the linear-gaussian graphical model to the case in which
the nodes of the graph represent multivariate gaussian variables. in this case, we can
write the conditional distribution for node i in the form

      xi

(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)

(cid:2)

j   pai

      

p(xi|pai) = n

wijxj + bi,   i

(8.19)

section 2.3.6

where now wij is a matrix (which is nonsquare if xi and xj have different dimen-
sionalities). again it is easy to verify that the joint distribution over all variables is
gaussian.

note that we have already encountered a speci   c example of the linear-gaussian
relationship when we saw that the conjugate prior for the mean    of a gaussian
variable x is itself a gaussian distribution over   . the joint distribution over x and
   is therefore gaussian. this corresponds to a simple two-node graph in which
the node representing    is the parent of the node representing x. the mean of the
distribution over    is a parameter controlling a prior, and so it can be viewed as a
hyperparameter. because the value of this hyperparameter may itself be unknown,
we can again treat it from a bayesian perspective by introducing a prior over the
hyperparameter, sometimes called a hyperprior, which is again given by a gaussian
distribution. this type of construction can be extended in principle to any level and is
an illustration of a hierarchical bayesian model, of which we shall encounter further
examples in later chapters.

8.2. conditional independence

an important concept for id203 distributions over multiple variables is that of
conditional independence (dawid, 1980). consider three variables a, b, and c, and
suppose that the conditional distribution of a, given b and c, is such that it does not
depend on the value of b, so that

p(a|b, c) = p(a|c).

(8.20)

we say that a is conditionally independent of b given c. this can be expressed in a
slightly different way if we consider the joint distribution of a and b conditioned on
c, which we can write in the form

p(a, b|c) = p(a|b, c)p(b|c)
= p(a|c)p(b|c).

(8.21)

where we have used the product rule of id203 together with (8.20). thus we
see that, conditioned on c, the joint distribution of a and b factorizes into the prod-
uct of the marginal distribution of a and the marginal distribution of b (again both
conditioned on c). this says that the variables a and b are statistically independent,
given c. note that our de   nition of conditional independence will require that (8.20),

8.2. conditional independence

373

figure 8.15 the    rst of three examples of graphs over three variables
a, b, and c used to discuss conditional independence
properties of directed id114.

c

a

b

or equivalently (8.21), must hold for every possible value of c, and not just for some
values. we shall sometimes use a shorthand notation for conditional independence
(dawid, 1979) in which

a        b | c

(8.22)

denotes that a is conditionally independent of b given c and is equivalent to (8.20).

conditional independence properties play an important role in using probabilis-
tic models for pattern recognition by simplifying both the structure of a model and
the computations needed to perform id136 and learning under that model. we
shall see examples of this shortly.

if we are given an expression for the joint distribution over a set of variables in
terms of a product of conditional distributions (i.e., the mathematical representation
underlying a directed graph), then we could in principle test whether any poten-
tial conditional independence property holds by repeated application of the sum and
product rules of id203. in practice, such an approach would be very time con-
suming. an important and elegant feature of id114 is that conditional
independence properties of the joint distribution can be read directly from the graph
without having to perform any analytical manipulations. the general framework
for achieving this is called d-separation, where the    d    stands for    directed    (pearl,
1988). here we shall motivate the concept of d-separation and give a general state-
ment of the d-separation criterion. a formal proof can be found in lauritzen (1996).

8.2.1 three example graphs
we begin our discussion of the conditional independence properties of directed
graphs by considering three simple examples each involving graphs having just three
nodes. together, these will motivate and illustrate the key concepts of d-separation.
the    rst of the three examples is shown in figure 8.15, and the joint distribution
corresponding to this graph is easily written down using the general result (8.5) to
give

(8.23)
if none of the variables are observed, then we can investigate whether a and b are
independent by marginalizing both sides of (8.23) with respect to c to give

p(a, b, c) = p(a|c)p(b|c)p(c).

(cid:2)

p(a, b) =

p(a|c)p(b|c)p(c).

c

in general, this does not factorize into the product p(a)p(b), and so

a (cid:9)       b |    

(8.24)

(8.25)

374

8. id114

figure 8.16 as in figure 8.15 but where we have conditioned on the

value of variable c.

c

a

b

where     denotes the empty set, and the symbol (cid:9)       means that the conditional inde-
pendence property does not hold in general. of course, it may hold for a particular
distribution by virtue of the speci   c numerical values associated with the various
conditional probabilities, but it does not follow in general from the structure of the
graph.

now suppose we condition on the variable c, as represented by the graph of
figure 8.16. from (8.23), we can easily write down the conditional distribution of a
and b, given c, in the form

p(a, b|c) = p(a, b, c)
p(c)

= p(a|c)p(b|c)

and so we obtain the conditional independence property

a        b | c.

we can provide a simple graphical interpretation of this result by considering
the path from node a to node b via c. the node c is said to be tail-to-tail with re-
spect to this path because the node is connected to the tails of the two arrows, and
the presence of such a path connecting nodes a and b causes these nodes to be de-
pendent. however, when we condition on node c, as in figure 8.16, the conditioned
node    blocks    the path from a to b and causes a and b to become (conditionally)
independent.

we can similarly consider the graph shown in figure 8.17. the joint distribution
corresponding to this graph is again obtained from our general formula (8.5) to give

p(a, b, c) = p(a)p(c|a)p(b|c).

(8.26)

first of all, suppose that none of the variables are observed. again, we can test to
see if a and b are independent by marginalizing over c to give

p(a, b) = p(a)

p(c|a)p(b|c) = p(a)p(b|a).

(cid:2)

c

figure 8.17 the second of our three examples of 3-node
graphs used to motivate the conditional indepen-
dence framework for directed id114.

a

c

b

figure 8.18 as in figure 8.17 but now conditioning on node c.

a

c

8.2. conditional independence

which in general does not factorize into p(a)p(b), and so

a (cid:9)       b |    

375

b

(8.27)

as before.

now suppose we condition on node c, as shown in figure 8.18. using bayes   

theorem, together with (8.26), we obtain

p(a, b|c) = p(a, b, c)
p(c)

= p(a)p(c|a)p(b|c)
p(c)
= p(a|c)p(b|c)

and so again we obtain the conditional independence property

a        b | c.

as before, we can interpret these results graphically. the node c is said to be
head-to-tail with respect to the path from node a to node b. such a path connects
nodes a and b and renders them dependent. if we now observe c, as in figure 8.18,
then this observation    blocks    the path from a to b and so we obtain the conditional
independence property a        b | c.

finally, we consider the third of our 3-node examples, shown by the graph in
figure 8.19. as we shall see, this has a more subtle behaviour than the two previous
graphs.

the joint distribution can again be written down using our general result (8.5) to

give

p(a, b, c) = p(a)p(b)p(c|a, b).

(8.28)

consider    rst the case where none of the variables are observed. marginalizing both
sides of (8.28) over c we obtain

p(a, b) = p(a)p(b)

figure 8.19 the last of our three examples of 3-node graphs used to
explore conditional independence properties in graphi-
cal models. this graph has rather different properties
from the two previous examples.

a

b

c

376

8. id114

figure 8.20 as in figure 8.19 but conditioning on the value of node
c. in this graph, the act of conditioning induces a depen-
dence between a and b.

a

b

c

and so a and b are independent with no variables observed, in contrast to the two
previous examples. we can write this result as
a        b |    .

(8.29)

now suppose we condition on c, as indicated in figure 8.20. the conditional distri-
bution of a and b is then given by

p(a, b|c) = p(a, b, c)
p(c)

= p(a)p(b)p(c|a, b)

p(c)

exercise 8.10

which in general does not factorize into the product p(a)p(b), and so

a (cid:9)       b | c.

thus our third example has the opposite behaviour from the    rst two. graphically,
we say that node c is head-to-head with respect to the path from a to b because it
connects to the heads of the two arrows. when node c is unobserved, it    blocks   
the path, and the variables a and b are independent. however, conditioning on c
   unblocks    the path and renders a and b dependent.

there is one more subtlety associated with this third example that we need to
consider. first we introduce some more terminology. we say that node y is a de-
scendant of node x if there is a path from x to y in which each step of the path
follows the directions of the arrows. then it can be shown that a head-to-head path
will become unblocked if either the node, or any of its descendants, is observed.

in summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked
unless it is observed in which case it blocks the path. by contrast, a head-to-head
node blocks a path if it is unobserved, but once the node, and/or at least one of its
descendants, is observed the path becomes unblocked.

it is worth spending a moment to understand further the unusual behaviour of the
graph of figure 8.20. consider a particular instance of such a graph corresponding
to a problem with three binary random variables relating to the fuel system on a car,
as shown in figure 8.21. the variables are called b, representing the state of a
battery that is either charged (b = 1) or    at (b = 0), f representing the state of
the fuel tank that is either full of fuel (f = 1) or empty (f = 0), and g, which is
the state of an electric fuel gauge and which indicates either full (g = 1) or empty

b

f

b

f

b

f

8.2. conditional independence

377

g

g

g

figure 8.21 an example of a 3-node graph used to illustrate the phenomenon of    explaining away   . the three
nodes represent the state of the battery (b), the state of the fuel tank (f ) and the reading on the electric fuel
gauge (g). see the text for details.

(g = 0). the battery is either charged or    at, and independently the fuel tank is
either full or empty, with prior probabilities

p(b = 1) = 0.9
p(f = 1) = 0.9.

given the state of the fuel tank and the battery, the fuel gauge reads full with proba-
bilities given by

p(g = 1|b = 1, f = 1) = 0.8
p(g = 1|b = 1, f = 0) = 0.2
p(g = 1|b = 0, f = 1) = 0.2
p(g = 1|b = 0, f = 0) = 0.1

so this is a rather unreliable fuel gauge! all remaining probabilities are determined
by the requirement that probabilities sum to one, and so we have a complete speci   -
cation of the probabilistic model.

before we observe any data, the prior id203 of the fuel tank being empty
is p(f = 0) = 0.1. now suppose that we observe the fuel gauge and discover that
it reads empty, i.e., g = 0, corresponding to the middle graph in figure 8.21. we
can use bayes    theorem to evaluate the posterior id203 of the fuel tank being
empty. first we evaluate the denominator for bayes    theorem given by
p(g = 0|b, f )p(b)p(f ) = 0.315

p(g = 0) =

(cid:2)

(8.30)

b   {0,1}

f   {0,1}

and similarly we evaluate
p(g = 0|f = 0) =

p(g = 0|b, f = 0)p(b) = 0.81

(8.31)

(cid:2)
(cid:2)

b   {0,1}

and using these results we have

p(f = 0|g = 0) = p(g = 0|f = 0)p(f = 0)

p(g = 0)

(cid:7) 0.257

(8.32)

378

8. id114

and so p(f = 0|g = 0) > p(f = 0). thus observing that the gauge reads empty
makes it more likely that the tank is indeed empty, as we would intuitively expect.
next suppose that we also check the state of the battery and    nd that it is    at, i.e.,
b = 0. we have now observed the states of both the fuel gauge and the battery, as
shown by the right-hand graph in figure 8.21. the posterior id203 that the fuel
tank is empty given the observations of both the fuel gauge and the battery state is
then given by

p(f = 0|g = 0, b = 0) = p(g = 0|b = 0, f = 0)p(f = 0)
f   {0,1} p(g = 0|b = 0, f )p(f )

(cid:7) 0.111 (8.33)

(cid:5)

where the prior id203 p(b = 0) has cancelled between numerator and denom-
inator. thus the id203 that the tank is empty has decreased (from 0.257 to
0.111) as a result of the observation of the state of the battery. this accords with our
intuition that    nding out that the battery is    at explains away the observation that the
fuel gauge reads empty. we see that the state of the fuel tank and that of the battery
have indeed become dependent on each other as a result of observing the reading
on the fuel gauge. in fact, this would also be the case if, instead of observing the
fuel gauge directly, we observed the state of some descendant of g. note that the
id203 p(f = 0|g = 0, b = 0) (cid:7) 0.111 is greater than the prior id203
p(f = 0) = 0.1 because the observation that the fuel gauge reads zero still provides
some evidence in favour of an empty fuel tank.

8.2.2 d-separation
we now give a general statement of the d-separation property (pearl, 1988) for
directed graphs. consider a general directed graph in which a, b, and c are arbi-
trary nonintersecting sets of nodes (whose union may be smaller than the complete
set of nodes in the graph). we wish to ascertain whether a particular conditional
independence statement a        b | c is implied by a given directed acyclic graph. to
do so, we consider all possible paths from any node in a to any node in b. any such
path is said to be blocked if it includes a node such that either

(a) the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the

node is in the set c, or

(b) the arrows meet head-to-head at the node, and neither the node, nor any of its

descendants, is in the set c.

if all paths are blocked, then a is said to be d-separated from b by c, and the joint
distribution over all of the variables in the graph will satisfy a        b | c.

the concept of d-separation is illustrated in figure 8.22. in graph (a), the path
from a to b is not blocked by node f because it is a tail-to-tail node for this path
and is not observed, nor is it blocked by node e because, although the latter is a
head-to-head node, it has a descendant c because is in the conditioning set. thus
the conditional independence statement a        b | c does not follow from this graph.
in graph (b), the path from a to b is blocked by node f because this is a tail-to-tail
node that is observed, and so the conditional independence property a        b | f will

figure 8.22 illustration of the con-
cept of d-separation. see the text for
details.

a

f

a

f

8.2. conditional independence

379

b

e

c

b

e

c

(a)

(b)

be satis   ed by any distribution that factorizes according to this graph. note that this
path is also blocked by node e because e is a head-to-head node and neither it nor its
descendant are in the conditioning set.

for the purposes of d-separation, parameters such as    and   2 in figure 8.5,
indicated by small    lled circles, behave in the same was as observed nodes. how-
ever, there are no marginal distributions associated with such nodes. consequently
parameter nodes never themselves have parents and so all paths through these nodes
will always be tail-to-tail and hence blocked. consequently they play no role in
d-separation.

section 2.3

another example of conditional independence and d-separation is provided by
the concept of i.i.d. (independent identically distributed) data introduced in sec-
tion 1.2.4. consider the problem of    nding the posterior distribution for the mean
of a univariate gaussian distribution. this can be represented by the directed graph
shown in figure 8.23 in which the joint distribution is de   ned by a prior p(  ) to-
gether with a set of conditional distributions p(xn|  ) for n = 1, . . . , n. in practice,
we observe d = {x1, . . . , xn} and our goal is to infer   . suppose, for a moment,
that we condition on    and consider the joint distribution of the observations. using
d-separation, we note that there is a unique path from any xi to any other xj(cid:9)=i and
that this path is tail-to-tail with respect to the observed node   . every such path is
blocked and so the observations d = {x1, . . . , xn} are independent given   , so that

figure 8.23 (a) directed graph corre-
sponding to the problem
of inferring the mean    of
a univariate gaussian dis-
tribution from observations
x1, . . . , xn .
(b) the same
graph drawn using the plate
notation.

n(cid:14)

n=1

  

p(d|  ) =

p(xn|  ).

(8.34)

  

x1

xn

xn

(a)

n

(b)

n

380

8. id114

figure 8.24 a graphical representation of the    naive bayes   
conditioned on the
model
class label z,
the components of the observed
vector x = (x1, . . . , xd)t are assumed to be
independent.

for classi   cation.

z

x1

xd

however, if we integrate over   , the observations are in general no longer indepen-
dent

p(d) =

p(d|  )p(  ) d   (cid:9)=

p(xn).

(8.35)

n(cid:14)

(cid:6)    

section 3.3

0

n=1

independence property

here    is a latent variable, because its value is not observed.

another example of a model representing i.i.d. data is the graph in figure 8.7
corresponding to bayesian polynomial regression. here the stochastic nodes corre-

spond to {tn}, w and(cid:1)t. we see that the node for w is tail-to-tail with respect to
the path from(cid:1)t to any one of the nodes tn and so we have the following conditional
(cid:1)t        tn | w.
(cid:1)t is independent of the training data {t1, . . . , tn}. we can therefore    rst use the
predictions of(cid:1)t for new input observations(cid:1)x.

(8.36)
thus, conditioned on the polynomial coef   cients w, the predictive distribution for

training data to determine the posterior distribution over the coef   cients w and then
we can discard the training data and use the posterior distribution for w to make

a related graphical structure arises in an approach to classi   cation called the
naive bayes model, in which we use conditional independence assumptions to sim-
plify the model structure. suppose our observed variable consists of a d-dimensional
vector x = (x1, . . . , xd)t, and we wish to assign observed values of x to one of k
classes. using the 1-of-k encoding scheme, we can represent these classes by a k-
dimensional binary vector z. we can then de   ne a generative model by introducing
a multinomial prior p(z|  ) over the class labels, where the kth component   k of   
is the prior id203 of class ck, together with a conditional distribution p(x|z)
for the observed vector x. the key assumption of the naive bayes model is that,
conditioned on the class z, the distributions of the input variables x1, . . . , xd are in-
dependent. the graphical representation of this model is shown in figure 8.24. we
see that observation of z blocks the path between xi and xj for j (cid:9)= i (because such
paths are tail-to-tail at the node z) and so xi and xj are conditionally independent
given z. if, however, we marginalize out z (so that z is unobserved) the tail-to-tail
path from xi to xj is no longer blocked. this tells us that in general the marginal
density p(x) will not factorize with respect to the components of x. we encountered
a simple application of the naive bayes model in the context of fusing data from
different sources for medical diagnosis in section 1.5.
if we are given a labelled training set, comprising inputs {x1, . . . , xn} together
with their class labels, then we can    t the naive bayes model to the training data

8.2. conditional independence

381

using maximum likelihood assuming that the data are drawn independently from
the model. the solution is obtained by    tting the model for each class separately
using the correspondingly labelled data. as an example, suppose that the id203
density within each class is chosen to be gaussian. in this case, the naive bayes
assumption then implies that the covariance matrix for each gaussian is diagonal,
and the contours of constant density within each class will be axis-aligned ellipsoids.
the marginal density, however, is given by a superposition of diagonal gaussians
(with weighting coef   cients given by the class priors) and so will no longer factorize
with respect to its components.

the naive bayes assumption is helpful when the dimensionality d of the input
space is high, making density estimation in the full d-dimensional space more chal-
lenging. it is also useful if the input vector contains both discrete and continuous
variables, since each can be represented separately using appropriate models (e.g.,
bernoulli distributions for binary observations or gaussians for real-valued vari-
ables). the conditional independence assumption of this model is clearly a strong
one that may lead to rather poor representations of the class-conditional densities.
nevertheless, even if this assumption is not precisely satis   ed, the model may still
give good classi   cation performance in practice because the decision boundaries can
be insensitive to some of the details in the class-conditional densities, as illustrated
in figure 1.27.

we have seen that a particular directed graph represents a speci   c decomposition
of a joint id203 distribution into a product of conditional probabilities. the
graph also expresses a set of conditional independence statements obtained through
the d-separation criterion, and the d-separation theorem is really an expression of the
equivalence of these two properties. in order to make this clear, it is helpful to think
of a directed graph as a    lter. suppose we consider a particular joint id203
distribution p(x) over the variables x corresponding to the (nonobserved) nodes of
the graph. the    lter will allow this distribution to pass through if, and only if, it can
be expressed in terms of the factorization (8.5) implied by the graph. if we present to
the    lter the set of all possible distributions p(x) over the set of variables x, then the
subset of distributions that are passed by the    lter will be denoted df, for directed
factorization. this is illustrated in figure 8.25. alternatively, we can use the graph as
a different kind of    lter by    rst listing all of the conditional independence properties
obtained by applying the d-separation criterion to the graph, and then allowing a
distribution to pass only if it satis   es all of these properties. if we present all possible
distributions p(x) to this second kind of    lter, then the d-separation theorem tells us
that the set of distributions that will be allowed through is precisely the set df.

it should be emphasized that the conditional independence properties obtained
from d-separation apply to any probabilistic model described by that particular di-
rected graph. this will be true, for instance, whether the variables are discrete or
continuous or a combination of these. again, we see that a particular graph is de-
scribing a whole family of id203 distributions.

at one extreme we have a fully connected graph that exhibits no conditional in-
dependence properties at all, and which can represent any possible joint id203
distribution over the given variables. the set df will contain all possible distribu-

382

8. id114

p(x)

df

figure 8.25 we can view a graphical model (in this case a directed graph) as a    lter in which a prob-
ability distribution p(x) is allowed through the    lter if, and only if, it satis   es the directed
factorization property (8.5). the set of all possible id203 distributions p(x) that pass
through the    lter is denoted df. we can alternatively use the graph to    lter distributions
according to whether they respect all of the conditional independencies implied by the
d-separation properties of the graph. the d-separation theorem says that it is the same
set of distributions df that will be allowed through this second kind of    lter.

tions p(x). at the other extreme, we have the fully disconnected graph, i.e., one
having no links at all. this corresponds to joint distributions which factorize into the
product of the marginal distributions over the variables comprising the nodes of the
graph.
note that for any given graph, the set of distributions df will include any dis-
tributions that have additional independence properties beyond those described by
the graph. for instance, a fully factorized distribution will always be passed through
the    lter implied by any graph over the corresponding set of variables.

we end our discussion of conditional independence properties by exploring the
concept of a markov blanket or markov boundary. consider a joint distribution
p(x1, . . . , xd) represented by a directed graph having d nodes, and consider the
conditional distribution of a particular node with variables xi conditioned on all of
the remaining variables xj(cid:9)=i. using the factorization property (8.5), we can express
this conditional distribution in the form
p(xi|x{j(cid:9)=i}) =

p(x1, . . . , xd)

(cid:6)
(cid:14)
(cid:6) (cid:14)

k

=

p(x1, . . . , xd) dxi

p(xk|pak)

p(xk|pak) dxi

k

in which the integral is replaced by a summation in the case of discrete variables. we
now observe that any factor p(xk|pak) that does not have any functional dependence
on xi can be taken outside the integral over xi, and will therefore cancel between
numerator and denominator. the only factors that remain will be the conditional
distribution p(xi|pai) for node xi itself, together with the conditional distributions
for any nodes xk such that node xi is in the conditioning set of p(xk|pak), in other
words for which xi is a parent of xk. the conditional p(xi|pai) will depend on the
parents of node xi, whereas the conditionals p(xk|pak) will depend on the children

8.3. markov random fields

383

figure 8.26 the markov blanket of a node xi comprises the set
of parents, children and co-parents of the node.
it
has the property that the conditional distribution of
xi, conditioned on all the remaining variables in the
graph, is dependent only on the variables in the
markov blanket.

xi

of xi as well as on the co-parents, in other words variables corresponding to parents
of node xk other than node xi. the set of nodes comprising the parents, the children
and the co-parents is called the markov blanket and is illustrated in figure 8.26. we
can think of the markov blanket of a node xi as being the minimal set of nodes that
isolates xi from the rest of the graph. note that it is not suf   cient to include only the
parents and children of node xi because the phenomenon of explaining away means
that observations of the child nodes will not block paths to the co-parents. we must
therefore observe the co-parent nodes also.

8.3. markov random fields

we have seen that directed id114 specify a factorization of the joint dis-
tribution over a set of variables into a product of local conditional distributions. they
also de   ne a set of conditional independence properties that must be satis   ed by any
distribution that factorizes according to the graph. we turn now to the second ma-
jor class of id114 that are described by undirected graphs and that again
specify both a factorization and a set of conditional independence relations.

a markov random    eld, also known as a markov network or an undirected
graphical model (kindermann and snell, 1980), has a set of nodes each of which
corresponds to a variable or group of variables, as well as a set of links each of
which connects a pair of nodes. the links are undirected, that is they do not carry
arrows. in the case of undirected graphs, it is convenient to begin with a discussion
of conditional independence properties.

8.3.1 conditional independence properties
in the case of directed graphs, we saw that it was possible to test whether a par-
ticular conditional independence property holds by applying a graphical test called
d-separation. this involved testing whether or not the paths connecting two sets of
nodes were    blocked   . the de   nition of blocked, however, was somewhat subtle
due to the presence of paths having head-to-head nodes. we might ask whether it
is possible to de   ne an alternative graphical semantics for id203 distributions
such that conditional independence is determined by simple graph separation. this
is indeed the case and corresponds to undirected id114. by removing the

section 8.2

384

8. id114

figure 8.27 an example of an undirected graph in
which every path from any node in set
a to any node in set b passes through
at least one node in set c. conse-
quently the conditional
independence
property a        b | c holds for any
id203 distribution described by this
graph.

c

b

a

directionality from the links of the graph, the asymmetry between parent and child
nodes is removed, and so the subtleties associated with head-to-head nodes no longer
arise.

suppose that in an undirected graph we identify three sets of nodes, denoted a,

b, and c, and that we consider the conditional independence property

a        b | c.

(8.37)

to test whether this property is satis   ed by a id203 distribution de   ned by a
graph we consider all possible paths that connect nodes in set a to nodes in set
b. if all such paths pass through one or more nodes in set c, then all such paths are
   blocked    and so the conditional independence property holds. however, if there is at
least one such path that is not blocked, then the property does not necessarily hold, or
more precisely there will exist at least some distributions corresponding to the graph
that do not satisfy this conditional independence relation. this is illustrated with an
example in figure 8.27. note that this is exactly the same as the d-separation crite-
rion except that there is no    explaining away    phenomenon. testing for conditional
independence in undirected graphs is therefore simpler than in directed graphs.

an alternative way to view the conditional independence test is to imagine re-
moving all nodes in set c from the graph together with any links that connect to
those nodes. we then ask if there exists a path that connects any node in a to any
node in b. if there are no such paths, then the conditional independence property
must hold.

the markov blanket for an undirected graph takes a particularly simple form,
because a node will be conditionally independent of all other nodes conditioned only
on the neighbouring nodes, as illustrated in figure 8.28.

8.3.2 factorization properties
we now seek a factorization rule for undirected graphs that will correspond to
the above conditional independence test. again, this will involve expressing the joint
distribution p(x) as a product of functions de   ned over sets of variables that are local
to the graph. we therefore need to decide what is the appropriate notion of locality
in this case.

8.3. markov random fields

385

figure 8.28 for an undirected graph, the markov blanket of a node
xi consists of the set of neighbouring nodes.
it has the
property that the conditional distribution of xi, conditioned
on all the remaining variables in the graph, is dependent
only on the variables in the markov blanket.

if we consider two nodes xi and xj that are not connected by a link, then these
variables must be conditionally independent given all other nodes in the graph. this
follows from the fact that there is no direct path between the two nodes, and all other
paths pass through nodes that are observed, and hence those paths are blocked. this
conditional independence property can be expressed as

p(xi, xj|x\{i,j}) = p(xi|x\{i,j})p(xj|x\{i,j})

(8.38)

where x\{i,j} denotes the set x of all variables with xi and xj removed. the factor-
ization of the joint distribution must therefore be such that xi and xj do not appear
in the same factor in order for the conditional independence property to hold for all
possible distributions belonging to the graph.

this leads us to consider a graphical concept called a clique, which is de   ned
as a subset of the nodes in a graph such that there exists a link between all pairs of
nodes in the subset. in other words, the set of nodes in a clique is fully connected.
furthermore, a maximal clique is a clique such that it is not possible to include any
other nodes from the graph in the set without it ceasing to be a clique. these concepts
are illustrated by the undirected graph over four variables shown in figure 8.29. this
graph has    ve cliques of two nodes given by {x1, x2}, {x2, x3}, {x3, x4}, {x4, x2},
and {x1, x3}, as well as two maximal cliques given by {x1, x2, x3} and {x2, x3, x4}.
the set {x1, x2, x3, x4} is not a clique because of the missing link from x1 to x4.

we can therefore de   ne the factors in the decomposition of the joint distribution
to be functions of the variables in the cliques. in fact, we can consider functions
of the maximal cliques, without loss of generality, because other cliques must be
subsets of maximal cliques. thus, if {x1, x2, x3} is a maximal clique and we de   ne
an arbitrary function over this clique, then including another factor de   ned over a
subset of these variables would be redundant.

let us denote a clique by c and the set of variables in that clique by xc. then

figure 8.29 a four-node undirected graph showing a clique (outlined in

green) and a maximal clique (outlined in blue).

x1

x3

x2

x4

386

8. id114

(cid:14)
(cid:14)

c

1
z

(cid:2)

the joint distribution is written as a product of potential functions   c(xc) over the
maximal cliques of the graph

p(x) =

  c(xc).

(8.39)

here the quantity z, sometimes called the partition function, is a id172 con-
stant and is given by

z =

  c(xc)

(8.40)

x

c

which ensures that the distribution p(x) given by (8.39) is correctly normalized.
by considering only potential functions which satisfy   c(xc) (cid:2) 0 we ensure that
p(x) (cid:2) 0. in (8.40) we have assumed that x comprises discrete variables, but the
framework is equally applicable to continuous variables, or a combination of the two,
in which the summation is replaced by the appropriate combination of summation
and integration.

note that we do not restrict the choice of potential functions to those that have a
speci   c probabilistic interpretation as marginal or conditional distributions. this is
in contrast to directed graphs in which each factor represents the conditional distribu-
tion of the corresponding variable, conditioned on the state of its parents. however,
in special cases, for instance where the undirected graph is constructed by starting
with a directed graph, the potential functions may indeed have such an interpretation,
as we shall see shortly.

one consequence of the generality of the potential functions   c(xc) is that
their product will in general not be correctly normalized. we therefore have to in-
troduce an explicit id172 factor given by (8.40). recall that for directed
graphs, the joint distribution was automatically normalized as a consequence of the
id172 of each of the conditional distributions in the factorization.

the presence of this id172 constant is one of the major limitations of
undirected graphs. if we have a model with m discrete nodes each having k states,
then the evaluation of the id172 term involves summing over km states and
so (in the worst case) is exponential in the size of the model. the partition function
is needed for parameter learning because it will be a function of any parameters that
govern the potential functions   c(xc). however, for evaluation of local conditional
distributions, the partition function is not needed because a conditional is the ratio
of two marginals, and the partition function cancels between numerator and denom-
inator when evaluating this ratio. similarly, for evaluating local marginal probabil-
ities we can work with the unnormalized joint distribution and then normalize the
marginals explicitly at the end. provided the marginals only involves a small number
of variables, the evaluation of their id172 coef   cient will be feasible.

so far, we have discussed the notion of conditional independence based on sim-
ple graph separation and we have proposed a factorization of the joint distribution
that is intended to correspond to this conditional independence structure. however,
we have not made any formal connection between conditional independence and
factorization for undirected graphs. to do so we need to restrict attention to poten-
tial functions   c(xc) that are strictly positive (i.e., never zero or negative for any

8.3. markov random fields

387

choice of xc). given this restriction, we can make a precise relationship between
factorization and conditional independence.

to do this we again return to the concept of a graphical model as a    lter, corre-
sponding to figure 8.25. consider the set of all possible distributions de   ned over
a    xed set of variables corresponding to the nodes of a particular undirected graph.
we can de   ne ui to be the set of such distributions that are consistent with the set
of conditional independence statements that can be read from the graph using graph
separation. similarly, we can de   ne uf to be the set of such distributions that can
be expressed as a factorization of the form (8.39) with respect to the maximal cliques
of the graph. the hammersley-clifford theorem (clifford, 1990) states that the sets
ui and uf are identical.

because we are restricted to potential functions which are strictly positive it is

convenient to express them as exponentials, so that

  c(xc) = exp{   e(xc)}

(8.41)

where e(xc) is called an energy function, and the exponential representation is
called the boltzmann distribution. the joint distribution is de   ned as the product of
potentials, and so the total energy is obtained by adding the energies of each of the
maximal cliques.

in contrast to the factors in the joint distribution for a directed graph, the po-
tentials in an undirected graph do not have a speci   c probabilistic interpretation.
although this gives greater    exibility in choosing the potential functions, because
there is no id172 constraint, it does raise the question of how to motivate a
choice of potential function for a particular application. this can be done by view-
ing the potential function as expressing which con   gurations of the local variables
are preferred to others. global con   gurations that have a relatively high id203
are those that    nd a good balance in satisfying the (possibly con   icting) in   uences
of the clique potentials. we turn now to a speci   c example to illustrate the use of
undirected graphs.

8.3.3 illustration: image de-noising
we can illustrate the application of undirected graphs using an example of noise
removal from a binary image (besag, 1974; geman and geman, 1984; besag, 1986).
although a very simple example, this is typical of more sophisticated applications.
let the observed noisy image be described by an array of binary pixel values yi    
{   1, +1}, where the index i = 1, . . . , d runs over all pixels. we shall suppose
that the image is obtained by taking an unknown noise-free image, described by
binary pixel values xi     {   1, +1} and randomly    ipping the sign of pixels with
some small id203. an example binary image, together with a noise corrupted
image obtained by    ipping the sign of the pixels with id203 10%, is shown in
figure 8.30. given the noisy image, our goal is to recover the original noise-free
image.

because the noise level is small, we know that there will be a strong correlation
between xi and yi. we also know that neighbouring pixels xi and xj in an image
are strongly correlated. this prior knowledge can be captured using the markov

388

8. id114

figure 8.30 illustration of image de-noising using a markov random    eld. the top row shows the original
binary image on the left and the corrupted image after randomly changing 10% of the pixels on the right. the
bottom row shows the restored images obtained using iterated conditional models (icm) on the left and using
the graph-cut algorithm on the right.
icm produces an image where 96% of the pixels agree with the original
image, whereas the corresponding number for graph-cut is 99%.

random    eld model whose undirected graph is shown in figure 8.31. this graph has
two types of cliques, each of which contains two variables. the cliques of the form
{xi, yi} have an associated energy function that expresses the correlation between
these variables. we choose a very simple energy function for these cliques of the
form      xiyi where    is a positive constant. this has the desired effect of giving a
lower energy (thus encouraging a higher id203) when xi and yi have the same
sign and a higher energy when they have the opposite sign.
the remaining cliques comprise pairs of variables {xi, xj} where i and j are
indices of neighbouring pixels. again, we want the energy to be lower when the
pixels have the same sign than when they have the opposite sign, and so we choose
an energy given by      xixj where    is a positive constant.

because a potential function is an arbitrary, nonnegative function over a maximal
clique, we can multiply it by any nonnegative functions of subsets of the clique, or

8.3. markov random fields

389

figure 8.31 an undirected graphical model representing a
markov random    eld for image de-noising,
in
which xi is a binary variable denoting the state
of pixel i in the unknown noise-free image, and yi
denotes the corresponding value of pixel i in the
observed noisy image.

yi

xi

equivalently we can add the corresponding energies. in this example, this allows us
to add an extra term hxi for each pixel i in the noise-free image. such a term has
the effect of biasing the model towards pixel values that have one particular sign in
preference to the other.

the complete energy function for the model then takes the form

(cid:2)

i

(cid:2)

{i,j}

e(x, y) = h

xi       

xixj       

(cid:2)

which de   nes a joint distribution over x and y given by
exp{   e(x, y)}.

p(x, y) =

1
z

xiyi

i

(8.42)

(8.43)

we now    x the elements of y to the observed values given by the pixels of the
noisy image, which implicitly de   nes a conditional distribution p(x|y) over noise-
free images. this is an example of the ising model, which has been widely studied in
statistical physics. for the purposes of image restoration, we wish to    nd an image x
having a high id203 (ideally the maximum id203). to do this we shall use
a simple iterative technique called iterated conditional modes, or icm (kittler and
f  oglein, 1984), which is simply an application of coordinate-wise gradient ascent.
the idea is    rst to initialize the variables {xi}, which we do by simply setting xi =
yi for all i. then we take one node xj at a time and we evaluate the total energy
for the two possible states xj = +1 and xj =    1, keeping all other node variables
   xed, and set xj to whichever state has the lower energy. this will either leave
the id203 unchanged, if xj is unchanged, or will increase it. because only
one variable is changed, this is a simple local computation that can be performed
ef   ciently. we then repeat the update for another site, and so on, until some suitable
stopping criterion is satis   ed. the nodes may be updated in a systematic way, for
instance by repeatedly raster scanning through the image, or by choosing nodes at
random.

if we have a sequence of updates in which every site is visited at least once,
and in which no changes to the variables are made, then by de   nition the algorithm

exercise 8.13

390

8. id114

figure 8.32 (a) example of a directed
graph.
(b) the equivalent undirected
graph.

x1

x1

(a)

(b)

x2

x2

xn   1

xn

xn

xn   1

will have converged to a local maximum of the id203. this need not, however,
correspond to the global maximum.

for the purposes of this simple illustration, we have    xed the parameters to be
   = 1.0,    = 2.1 and h = 0. note that leaving h = 0 simply means that the prior
probabilities of the two states of xi are equal. starting with the observed noisy image
as the initial con   guration, we run icm until convergence, leading to the de-noised
image shown in the lower left panel of figure 8.30. note that if we set    = 0,
which effectively removes the links between neighbouring pixels, then the global
most probable solution is given by xi = yi for all i, corresponding to the observed
noisy image.

later we shall discuss a more effective algorithm for    nding high id203 so-
lutions called the max-product algorithm, which typically leads to better solutions,
although this is still not guaranteed to    nd the global maximum of the posterior dis-
tribution. however, for certain classes of model, including the one given by (8.42),
there exist ef   cient algorithms based on graph cuts that are guaranteed to    nd the
global maximum (greig et al., 1989; boykov et al., 2001; kolmogorov and zabih,
2004). the lower right panel of figure 8.30 shows the result of applying a graph-cut
algorithm to the de-noising problem.

exercise 8.14

section 8.4

8.3.4 relation to directed graphs
we have introduced two graphical frameworks for representing id203 dis-
tributions, corresponding to directed and undirected graphs, and it is instructive to
discuss the relation between these. consider    rst the problem of taking a model that
is speci   ed using a directed graph and trying to convert it to an undirected graph. in
some cases this is straightforward, as in the simple example in figure 8.32. here the
joint distribution for the directed graph is given as a product of conditionals in the
form

p(x) = p(x1)p(x2|x1)p(x3|x2)       p(xn|xn   1).

(8.44)

now let us convert this to an undirected graph representation, as shown in fig-
ure 8.32. in the undirected graph, the maximal cliques are simply the pairs of neigh-
bouring nodes, and so from (8.39) we wish to write the joint distribution in the form

p(x) =

1
z

  1,2(x1, x2)  2,3(x2, x3)         n   1,n (xn   1, xn).

(8.45)

figure 8.33 example of a simple
directed graph (a) and the corre-
sponding moral graph (b).

x1

x3

x1

x3

x2

x2

8.3. markov random fields

391

x4

(a)

x4

(b)

this is easily done by identifying

  1,2(x1, x2) = p(x1)p(x2|x1)
  2,3(x2, x3) = p(x3|x2)

  n   1,n (xn   1, xn ) = p(xn|xn   1)

...

where we have absorbed the marginal p(x1) for the    rst node into the    rst potential
function. note that in this case, the partition function z = 1.

let us consider how to generalize this construction, so that we can convert any
distribution speci   ed by a factorization over a directed graph into one speci   ed by a
factorization over an undirected graph. this can be achieved if the clique potentials
of the undirected graph are given by the conditional distributions of the directed
graph. in order for this to be valid, we must ensure that the set of variables that
appears in each of the conditional distributions is a member of at least one clique of
the undirected graph. for nodes on the directed graph having just one parent, this is
achieved simply by replacing the directed link with an undirected link. however, for
nodes in the directed graph having more than one parent, this is not suf   cient. these
are nodes that have    head-to-head    paths encountered in our discussion of conditional
independence. consider a simple directed graph over 4 nodes shown in figure 8.33.
the joint distribution for the directed graph takes the form

p(x) = p(x1)p(x2)p(x3)p(x4|x1, x2, x3).

(8.46)
we see that the factor p(x4|x1, x2, x3) involves the four variables x1, x2, x3, and
x4, and so these must all belong to a single clique if this conditional distribution is
to be absorbed into a clique potential. to ensure this, we add extra links between
all pairs of parents of the node x4. anachronistically, this process of    marrying
the parents    has become known as moralization, and the resulting undirected graph,
after dropping the arrows, is called the moral graph. it is important to observe that
the moral graph in this example is fully connected and so exhibits no conditional
independence properties, in contrast to the original directed graph.

thus in general to convert a directed graph into an undirected graph, we    rst add
additional undirected links between all pairs of parents for each node in the graph and

392

8. id114

section 8.4

section 8.2

then drop the arrows on the original links to give the moral graph. then we initialize
all of the clique potentials of the moral graph to 1. we then take each conditional
distribution factor in the original directed graph and multiply it into one of the clique
potentials. there will always exist at least one maximal clique that contains all of
the variables in the factor as a result of the moralization step. note that in all cases
the partition function is given by z = 1.

the process of converting a directed graph into an undirected graph plays an
important role in exact id136 techniques such as the junction tree algorithm.
converting from an undirected to a directed representation is much less common
and in general presents problems due to the id172 constraints.

we saw that in going from a directed to an undirected representation we had to
discard some conditional independence properties from the graph. of course, we
could always trivially convert any distribution over a directed graph into one over an
undirected graph by simply using a fully connected undirected graph. this would,
however, discard all conditional independence properties and so would be vacuous.
the process of moralization adds the fewest extra links and so retains the maximum
number of independence properties.

we have seen that the procedure for determining the conditional independence
properties is different between directed and undirected graphs. it turns out that the
two types of graph can express different conditional independence properties, and
it is worth exploring this issue in more detail. to do so, we return to the view of
a speci   c (directed or undirected) graph as a    lter, so that the set of all possible
distributions over the given variables could be reduced to a subset that respects the
conditional independencies implied by the graph. a graph is said to be a d map
(for    dependency map   ) of a distribution if every conditional independence statement
satis   ed by the distribution is re   ected in the graph. thus a completely disconnected
graph (no links) will be a trivial d map for any distribution.

alternatively, we can consider a speci   c distribution and ask which graphs have
the appropriate conditional independence properties. if every conditional indepen-
dence statement implied by a graph is satis   ed by a speci   c distribution, then the
graph is said to be an i map (for    independence map   ) of that distribution. clearly a
fully connected graph will be a trivial i map for any distribution.

if it is the case that every conditional independence property of the distribution
is re   ected in the graph, and vice versa, then the graph is said to be a perfect map for

figure 8.34 venn diagram illustrating the set of all distributions
p over a given set of variables, together with the
set of distributions d that can be represented as a
perfect map using a directed graph, and the set u
that can be represented as a perfect map using an
undirected graph.

d

u

p

8.4. id136 in id114

figure 8.35 a directed graph whose conditional

independence
properties cannot be expressed using an undirected
graph over the same three variables.

a

393

b

c

that distribution. a perfect map is therefore both an i map and a d map.

consider the set of distributions such that for each distribution there exists a
directed graph that is a perfect map. this set is distinct from the set of distributions
such that for each distribution there exists an undirected graph that is a perfect map.
in addition there are distributions for which neither directed nor undirected graphs
offer a perfect map. this is illustrated as a venn diagram in figure 8.34.
figure 8.35 shows an example of a directed graph that is a perfect map for
a distribution satisfying the conditional independence properties a        b |     and
a (cid:9)       b | c. there is no corresponding undirected graph over the same three vari-
ables that is a perfect map.
conversely, consider the undirected graph over four variables shown in fig-
ure 8.36. this graph exhibits the properties a (cid:9)       b |    , c        d | a     b and
a        b | c     d. there is no directed graph over four variables that implies the same
set of conditional independence properties.

the graphical framework can be extended in a consistent way to graphs that
include both directed and undirected links. these are called chain graphs (lauritzen
and wermuth, 1989; frydenberg, 1990), and contain the directed and undirected
graphs considered so far as special cases. although such graphs can represent a
broader class of distributions than either directed or undirected alone, there remain
distributions for which even a chain graph cannot provide a perfect map. chain
graphs are not discussed further in this book.

figure 8.36 an undirected graph whose conditional

independence
properties cannot be expressed in terms of a directed
graph over the same variables.

a

b

c

d

8.4.

id136 in id114

we turn now to the problem of id136 in id114, in which some of
the nodes in a graph are clamped to observed values, and we wish to compute the
posterior distributions of one or more subsets of other nodes. as we shall see, we
can exploit the graphical structure both to    nd ef   cient algorithms for id136, and

394

8. id114

figure 8.37 a graphical representation of bayes   

see the text for details.

theorem.

x

y

x

y

x

y

(a)

(b)

(c)

to make the structure of those algorithms transparent. speci   cally, we shall see that
many algorithms can be expressed in terms of the propagation of local messages
around the graph. in this section, we shall focus primarily on techniques for exact
id136, and in chapter 10 we shall consider a number of approximate id136
algorithms.

to start with, let us consider the graphical interpretation of bayes    theorem.
suppose we decompose the joint distribution p(x, y) over two variables x and y into
a product of factors in the form p(x, y) = p(x)p(y|x). this can be represented by
the directed graph shown in figure 8.37(a). now suppose we observe the value of
y, as indicated by the shaded node in figure 8.37(b). we can view the marginal
distribution p(x) as a prior over the latent variable x, and our goal is to infer the
corresponding posterior distribution over x. using the sum and product rules of
id203 we can evaluate

p(y) =

p(y|x

(cid:4))p(x
(cid:4))

(8.47)

(cid:2)

x(cid:1)

which can then be used in bayes    theorem to calculate

p(x|y) = p(y|x)p(x)

(8.48)
thus the joint distribution is now expressed in terms of p(y) and p(x|y). from a
graphical perspective, the joint distribution p(x, y) is now represented by the graph
shown in figure 8.37(c), in which the direction of the arrow is reversed. this is the
simplest example of an id136 problem for a graphical model.

p(y)

.

8.4.1 id136 on a chain
now consider a more complex problem involving the chain of nodes of the form
shown in figure 8.32. this example will lay the foundation for a discussion of exact
id136 in more general graphs later in this section.

speci   cally, we shall consider the undirected graph in figure 8.32(b). we have
already seen that the directed chain can be transformed into an equivalent undirected
chain. because the directed graph does not have any nodes with more than one
parent, this does not require the addition of any extra links, and the directed and
undirected versions of this graph express exactly the same set of conditional inde-
pendence statements.

8.4. id136 in id114

395

the joint distribution for this graph takes the form

p(x) =

1
z

  1,2(x1, x2)  2,3(x2, x3)         n   1,n (xn   1, xn).

(8.49)

we shall consider the speci   c case in which the n nodes represent discrete vari-
ables each having k states, in which case each potential function   n   1,n(xn   1, xn)
comprises an k    k table, and so the joint distribution has (n     1)k 2 parameters.
let us consider the id136 problem of    nding the marginal distribution p(xn)
for a speci   c node xn that is part way along the chain. note that, for the moment,
there are no observed nodes. by de   nition, the required marginal is obtained by
summing the joint distribution over all variables except xn, so that

(cid:2)

(cid:2)

(cid:2)

(cid:2)

      

      

p(xn) =

p(x).

(8.50)

x1

xn   1

xn+1

xn

in a naive implementation, we would    rst evaluate the joint distribution and
then perform the summations explicitly. the joint distribution can be represented as
a set of numbers, one for each possible value for x. because there are n variables
each with k states, there are k n values for x and so evaluation and storage of the
joint distribution, as well as marginalization to obtain p(xn), all involve storage and
computation that scale exponentially with the length n of the chain.

we can, however, obtain a much more ef   cient algorithm by exploiting the con-
ditional independence properties of the graphical model. if we substitute the factor-
ized expression (8.49) for the joint distribution into (8.50), then we can rearrange the
order of the summations and the multiplications to allow the required marginal to be
evaluated much more ef   ciently. consider for instance the summation over xn . the
potential   n   1,n (xn   1, xn) is the only one that depends on xn , and so we can

perform the summation (cid:2)

  n   1,n (xn   1, xn )

(8.51)

xn

   rst to give a function of xn   1. we can then use this to perform the summation
over xn   1, which will involve only this new function together with the potential
  n   2,n   1(xn   2, xn   1), because this is the only other place that xn   1 appears.
similarly, the summation over x1 involves only the potential   1,2(x1, x2) and so
can be performed separately to give a function of x2, and so on. because each
summation effectively removes a variable from the distribution, this can be viewed
as the removal of a node from the graph.

if we group the potentials and summations together in this way, we can express

396

8. id114

the desired marginal in the form

p(xn) =

1

xn   1

z      (cid:2)
(
      (cid:2)
(

xn+1

  n   1,n(xn   1, xn)      

  

      
+

      

(cid:31)(cid:2)

x1

  2,3(x2, x3)

)*

    (xn)

x2

(cid:31)(cid:2)
(cid:31)(cid:2)
)*

xn

    (xn)

  1,2(x1, x2)

 

      
+

  n,n+1(xn, xn+1)      

  n   1,n (xn   1, xn )

      

.

(8.52)

the reader is encouraged to study this re-ordering carefully as the underlying idea
forms the basis for the later discussion of the general sum-product algorithm. here
the key concept that we are exploiting is that multiplication is distributive over addi-
tion, so that

ab + ac = a(b + c)

(8.53)

in which the left-hand side involves three arithmetic operations whereas the right-
hand side reduces this to two operations.
let us work out the computational cost of evaluating the required marginal using
this re-ordered expression. we have to perform n     1 summations each of which is
over k states and each of which involves a function of two variables. for instance,
the summation over x1 involves only the function   1,2(x1, x2), which is a table of
k    k numbers. we have to sum this table over x1 for each value of x2 and so this
has o(k 2) cost. the resulting vector of k numbers is multiplied by the matrix of
numbers   2,3(x2, x3) and so is again o(k 2). because there are n     1 summations
and multiplications of this kind, the total cost of evaluating the marginal p(xn) is
o(n k 2). this is linear in the length of the chain, in contrast to the exponential cost
of a naive approach. we have therefore been able to exploit the many conditional
independence properties of this simple graph in order to obtain an ef   cient calcula-
tion. if the graph had been fully connected, there would have been no conditional
independence properties, and we would have been forced to work directly with the
full joint distribution.

we now give a powerful interpretation of this calculation in terms of the passing
of local messages around on the graph. from (8.52) we see that the expression for the
marginal p(xn) decomposes into the product of two factors times the id172
constant

p(xn) =

    (xn)    (xn).

(8.54)

1
z

we shall interpret     (xn) as a message passed forwards along the chain from node
xn   1 to node xn. similarly,     (xn) can be viewed as a message passed backwards

8.4. id136 in id114

397

    (xn   1)

    (xn)

    (xn)

    (xn+1)

x1

xn   1

xn

xn+1

xn

figure 8.38 the marginal distribution
p(xn) for a node xn along the chain is ob-
tained by multiplying the two messages
    (xn) and     (xn), and then normaliz-
ing. these messages can themselves
be evaluated recursively by passing mes-
sages from both ends of the chain to-
wards node xn.

(cid:2)
(cid:2)

xn   1

      (cid:2)

      

      

along the chain to node xn from node xn+1. note that each of the messages com-
prises a set of k values, one for each choice of xn, and so the product of two mes-
sages should be interpreted as the point-wise multiplication of the elements of the
two messages to give another set of k values.

the message     (xn) can be evaluated recursively because

    (xn) =

=

  n   1,n(xn   1, xn)

xn   2

  n   1,n(xn   1, xn)    (xn   1).

(8.55)

we therefore    rst evaluate

xn   1

    (x2) =

(cid:2)

x1

  1,2(x1, x2)

(8.56)

and then apply (8.55) repeatedly until we reach the desired node. note carefully the
structure of the message passing equation. the outgoing message     (xn) in (8.55)
is obtained by multiplying the incoming message     (xn   1) by the local potential
involving the node variable and the outgoing variable and then summing over the
node variable.

similarly, the message     (xn) can be evaluated recursively by starting with

node xn and using

    (xn) =

=

(cid:2)
(cid:2)

xn+1

      (cid:2)

      

      

  n+1,n(xn+1, xn)

xn+2

  n+1,n(xn+1, xn)    (xn+1).

(8.57)

xn+1

this recursive message passing is illustrated in figure 8.38. the id172 con-
stant z is easily evaluated by summing the right-hand side of (8.54) over all states
of xn, an operation that requires only o(k) computation.

graphs of the form shown in figure 8.38 are called markov chains, and the
corresponding message passing equations represent an example of the chapman-
kolmogorov equations for markov processes (papoulis, 1984).

398

8. id114

now suppose we wish to evaluate the marginals p(xn) for every node n    
{1, . . . , n} in the chain. simply applying the above procedure separately for each
node will have computational cost that is o(n 2m 2). however, such an approach
would be very wasteful of computation. for instance, to    nd p(x1) we need to prop-
agate a message     (  ) from node xn back to node x2. similarly, to evaluate p(x2)
we need to propagate a messages     (  ) from node xn back to node x3. this will
involve much duplicated computation because most of the messages will be identical
in the two cases.

suppose instead we    rst launch a message     (xn   1) starting from node xn
and propagate corresponding messages all the way back to node x1, and suppose we
similarly launch a message     (x2) starting from node x1 and propagate the corre-
sponding messages all the way forward to node xn . provided we store all of the
intermediate messages along the way, then any node can evaluate its marginal sim-
ply by applying (8.54). the computational cost is only twice that for    nding the
marginal of a single node, rather than n times as much. observe that a message
has passed once in each direction across each link in the graph. note also that the
id172 constant z need be evaluated only once, using any convenient node.
if some of the nodes in the graph are observed, then the corresponding variables
are simply clamped to their observed values and there is no summation. to see

this, note that the effect of clamping a variable xn to an observed value (cid:1)xn can
additional function i(xn,(cid:1)xn), which takes the value 1 when xn =(cid:1)xn and the value
contain xn. summations over xn then contain only one term in which xn =(cid:1)xn.

0 otherwise. one such function can then be absorbed into each of the potentials that

be expressed by multiplying the joint distribution by (one or more copies of) an

now suppose we wish to calculate the joint distribution p(xn   1, xn) for two
neighbouring nodes on the chain. this is similar to the evaluation of the marginal
for a single node, except that there are now two variables that are not summed out.
a few moments thought will show that the required joint distribution can be written
in the form

p(xn   1, xn) =

1
z

    (xn   1)  n   1,n(xn   1, xn)    (xn).

(8.58)

thus we can obtain the joint distributions over all of the sets of variables in each
of the potentials directly once we have completed the message passing required to
obtain the marginals.

this is a useful result because in practice we may wish to use parametric forms
for the clique potentials, or equivalently for the conditional distributions if we started
from a directed graph. in order to learn the parameters of these potentials in situa-
tions where not all of the variables are observed, we can employ the em algorithm,
and it turns out that the local joint distributions of the cliques, conditioned on any
observed data, is precisely what is needed in the e step. we shall consider some
examples of this in detail in chapter 13.

8.4.2 trees
we have seen that exact id136 on a graph comprising a chain of nodes can be
performed ef   ciently in time that is linear in the number of nodes, using an algorithm

exercise 8.15

chapter 9

8.4. id136 in id114

399

figure 8.39 examples
tree-
structured graphs, showing (a) an
undirected tree, (b) a directed tree,
and (c) a directed polytree.

of

(a)

(b)

(c)

that can be interpreted in terms of messages passed along the chain. more generally,
id136 can be performed ef   ciently using local message passing on a broader
class of graphs called trees. in particular, we shall shortly generalize the message
passing formalism derived above for chains to give the sum-product algorithm, which
provides an ef   cient framework for exact id136 in tree-structured graphs.

in the case of an undirected graph, a tree is de   ned as a graph in which there
is one, and only one, path between any pair of nodes. such graphs therefore do not
have loops. in the case of directed graphs, a tree is de   ned such that there is a single
node, called the root, which has no parents, and all other nodes have one parent. if
we convert a directed tree into an undirected graph, we see that the moralization step
will not add any links as all nodes have at most one parent, and as a consequence the
corresponding moralized graph will be an undirected tree. examples of undirected
and directed trees are shown in figure 8.39(a) and 8.39(b). note that a distribution
represented as a directed tree can easily be converted into one represented by an
undirected tree, and vice versa.

if there are nodes in a directed graph that have more than one parent, but there is
still only one path (ignoring the direction of the arrows) between any two nodes, then
the graph is a called a polytree, as illustrated in figure 8.39(c). such a graph will
have more than one node with the property of having no parents, and furthermore,
the corresponding moralized undirected graph will have loops.

8.4.3 factor graphs
the sum-product algorithm that we derive in the next section is applicable to
undirected and directed trees and to polytrees. it can be cast in a particularly simple
and general form if we    rst introduce a new graphical construction called a factor
graph (frey, 1998; kschischnang et al., 2001).

exercise 8.18

both directed and undirected graphs allow a global function of several vari-
ables to be expressed as a product of factors over subsets of those variables. factor
graphs make this decomposition explicit by introducing additional nodes for the fac-
tors themselves in addition to the nodes representing the variables. they also allow
us to be more explicit about the details of the factorization, as we shall see.

let us write the joint distribution over a set of variables in the form of a product

of factors

p(x) =

fs(xs)

(8.59)

(cid:14)

where xs denotes a subset of the variables. for convenience, we shall denote the

s

400

8. id114

figure 8.40 example of a factor graph, which corresponds

to the factorization (8.60).

x1

x2

x3

fa

fb

fc

fd

individual variables by xi, however, as in earlier discussions, these can comprise
groups of variables (such as vectors or matrices). each factor fs is a function of a
corresponding set of variables xs.

directed graphs, whose factorization is de   ned by (8.5), represent special cases
of (8.59) in which the factors fs(xs) are local conditional distributions. similarly,
undirected graphs, given by (8.39), are a special case in which the factors are po-
tential functions over the maximal cliques (the normalizing coef   cient 1/z can be
viewed as a factor de   ned over the empty set of variables).

in a factor graph, there is a node (depicted as usual by a circle) for every variable
in the distribution, as was the case for directed and undirected graphs. there are also
additional nodes (depicted by small squares) for each factor fs(xs) in the joint dis-
tribution. finally, there are undirected links connecting each factor node to all of the
variables nodes on which that factor depends. consider, for example, a distribution
that is expressed in terms of the factorization

p(x) = fa(x1, x2)fb(x1, x2)fc(x2, x3)fd(x3).

(8.60)

this can be expressed by the factor graph shown in figure 8.40. note that there are
two factors fa(x1, x2) and fb(x1, x2) that are de   ned over the same set of variables.
in an undirected graph, the product of two such factors would simply be lumped
together into the same clique potential. similarly, fc(x2, x3) and fd(x3) could be
combined into a single potential over x2 and x3. the factor graph, however, keeps
such factors explicit and so is able to convey more detailed information about the
underlying factorization.

x1

x2

x1

x2

x1

f

x3

(b)

x3

(a)

x2

fb

fa

x3

(c)

figure 8.41 (a) an undirected graph with a single clique potential   (x1, x2, x3). (b) a factor graph with factor
f (x1, x2, x3) =   (x1, x2, x3) representing the same distribution as the undirected graph. (c) a different factor
graph representing the same distribution, whose factors satisfy fa(x1, x2, x3)fb(x1, x2) =   (x1, x2, x3).

x1

x2

x1

x2

x1

x2

8.4. id136 in id114

401

f

x3

(b)

fc

fa

fb

x3

(c)

x3

(a)

figure 8.42 (a) a directed graph with the factorization p(x1)p(x2)p(x3|x1, x2). (b) a factor graph representing
the same distribution as the directed graph, whose factor satis   es f (x1, x2, x3) = p(x1)p(x2)p(x3|x1, x2). (c)
a different factor graph representing the same distribution with factors fa(x1) = p(x1), fb(x2) = p(x2) and
fc(x1, x2, x3) = p(x3|x1, x2).

factor graphs are said to be bipartite because they consist of two distinct kinds
of nodes, and all links go between nodes of opposite type. in general, factor graphs
can therefore always be drawn as two rows of nodes (variable nodes at the top and
factor nodes at the bottom) with links between the rows, as shown in the example in
figure 8.40. in some situations, however, other ways of laying out the graph may
be more intuitive, for example when the factor graph is derived from a directed or
undirected graph, as we shall see.

if we are given a distribution that is expressed in terms of an undirected graph,
then we can readily convert it to a factor graph. to do this, we create variable nodes
corresponding to the nodes in the original undirected graph, and then create addi-
tional factor nodes corresponding to the maximal cliques xs. the factors fs(xs) are
then set equal to the clique potentials. note that there may be several different factor
graphs that correspond to the same undirected graph. these concepts are illustrated
in figure 8.41.

similarly, to convert a directed graph to a factor graph, we simply create variable
nodes in the factor graph corresponding to the nodes of the directed graph, and then
create factor nodes corresponding to the conditional distributions, and then    nally
add the appropriate links. again, there can be multiple factor graphs all of which
correspond to the same directed graph. the conversion of a directed graph to a
factor graph is illustrated in figure 8.42.

we have already noted the importance of tree-structured graphs for performing
ef   cient id136. if we take a directed or undirected tree and convert it into a factor
graph, then the result will again be a tree (in other words, the factor graph will have
no loops, and there will be one and only one path connecting any two nodes). in
the case of a directed polytree, conversion to an undirected graph results in loops
due to the moralization step, whereas conversion to a factor graph again results in a
tree, as illustrated in figure 8.43.
in fact, local cycles in a directed graph due to
links connecting parents of a node can be removed on conversion to a factor graph
by de   ning the appropriate factor function, as shown in figure 8.44.

we have seen that multiple different factor graphs can represent the same di-
rected or undirected graph. this allows factor graphs to be more speci   c about the

402

8. id114

(a)

(b)

(c)

figure 8.43 (a) a directed polytree. (b) the result of converting the polytree into an undirected graph showing
the creation of loops. (c) the result of converting the polytree into a factor graph, which retains the tree structure.

precise form of the factorization. figure 8.45 shows an example of a fully connected
undirected graph along with two different factor graphs.
in (b), the joint distri-
bution is given by a general form p(x) = f(x1, x2, x3), whereas in (c), it is given
by the more speci   c factorization p(x) = fa(x1, x2)fb(x1, x3)fc(x2, x3). it should
be emphasized that the factorization in (c) does not correspond to any conditional
independence properties.

8.4.4 the sum-product algorithm
we shall now make use of the factor graph framework to derive a powerful class
of ef   cient, exact id136 algorithms that are applicable to tree-structured graphs.
here we shall focus on the problem of evaluating local marginals over nodes or
subsets of nodes, which will lead us to the sum-product algorithm. later we shall
modify the technique to allow the most probable state to be found, giving rise to the
max-sum algorithm.

also we shall suppose that all of the variables in the model are discrete, and
so marginalization corresponds to performing sums. the framework, however, is
equally applicable to linear-gaussian models in which case marginalization involves
integration, and we shall consider an example of this in detail when we discuss linear
dynamical systems.

section 13.3

figure 8.44 (a) a fragment of a di-
rected graph having a lo-
cal cycle.
(b) conversion
to a fragment of a factor
graph having a tree struc-
ture, in which f (x1, x2, x3) =
p(x1)p(x2|x1)p(x3|x1, x2).

x1

x2

x1

x2

f(x1, x2, x3)

x3

(a)

x3
(b)

x1

x2

x1

x2

x1

fa

x2

8.4. id136 in id114

403

x3

(a)

f(x1, x2, x3)

x3
(b)

fb

fc

x3

(c)

figure 8.45 (a) a fully connected undirected graph. (b) and (c) two factor graphs each of which corresponds
to the undirected graph in (a).

there is an algorithm for exact id136 on directed graphs without loops known
as belief propagation (pearl, 1988; lauritzen and spiegelhalter, 1988), and is equiv-
alent to a special case of the sum-product algorithm. here we shall consider only the
sum-product algorithm because it is simpler to derive and to apply, as well as being
more general.

we shall assume that the original graph is an undirected tree or a directed tree or
polytree, so that the corresponding factor graph has a tree structure. we    rst convert
the original graph into a factor graph so that we can deal with both directed and
undirected models using the same framework. our goal is to exploit the structure of
the graph to achieve two things: (i) to obtain an ef   cient, exact id136 algorithm
for    nding marginals; (ii) in situations where several marginals are required to allow
computations to be shared ef   ciently.

we begin by considering the problem of    nding the marginal p(x) for partic-
ular variable node x. for the moment, we shall suppose that all of the variables
are hidden. later we shall see how to modify the algorithm to incorporate evidence
corresponding to observed variables. by de   nition, the marginal is obtained by sum-
ming the joint distribution over all variables except x so that

p(x) =

p(x)

(8.61)

where x \ x denotes the set of variables in x with variable x omitted. the idea is
to substitute for p(x) using the factor graph expression (8.59) and then interchange
summations and products in order to obtain an ef   cient algorithm. consider the
fragment of graph shown in figure 8.46 in which we see that the tree structure of
the graph allows us to partition the factors in the joint distribution into groups, with
one group associated with each of the factor nodes that is a neighbour of the variable
node x. we see that the joint distribution can be written as a product of the form

p(x) =

fs(x, xs)

(8.62)

(cid:2)

x\x

(cid:14)

s   ne(x)

ne(x) denotes the set of factor nodes that are neighbours of x, and xs denotes the
set of all variables in the subtree connected to the variable node x via the factor node

404

8. id114

figure 8.46 a fragment of a factor graph illustrating the

evaluation of the marginal p(x).

)
s

x

,

x
(
s
f

  fs   x(x)

fs

x

fs, and fs(x, xs) represents the product of all the factors in the group associated
with factor fs.

substituting (8.62) into (8.61) and interchanging the sums and products, we ob-

tain

p(x) =

=

 

(cid:31)(cid:2)

(cid:14)
(cid:14)

s   ne(x)

s   ne(x)

(cid:2)

fs(x, xs)

xs

  fs   x(x).

  fs   x(x)    

fs(x, xs)

here we have introduced a set of functions   fs   x(x), de   ned by

(8.63)

(8.64)

xs

which can be viewed as messages from the factor nodes fs to the variable node x.
we see that the required marginal p(x) is given by the product of all the incoming
messages arriving at node x.

in order to evaluate these messages, we again turn to figure 8.46 and note that
each factor fs(x, xs) is described by a factor (sub-)graph and so can itself be fac-
torized. in particular, we can write

fs(x, xs) = fs(x, x1, . . . , xm )g1 (x1, xs1) . . . gm (xm , xsm )

(8.65)

where, for convenience, we have denoted the variables associated with factor fx, in
addition to x, by x1, . . . , xm . this factorization is illustrated in figure 8.47. note
that the set of variables {x, x1, . . . , xm} is the set of variables on which the factor
 
fs depends, and so it can also be denoted xs, using the notation of (8.59).

substituting (8.65) into (8.64) we obtain

(cid:31)(cid:2)

  fs   x(x) =

=

fs(x, x1, . . . , xm )

fs(x, x1, . . . , xm )

gm(xm, xsm)

xxm
  xm   fs(xm)

(8.66)

(cid:2)
(cid:2)

x1

(cid:2)
(cid:2)

xm

. . .

. . .

x1

xm

(cid:14)
(cid:14)

m   ne(fs)\x

m   ne(fs)\x

8.4. id136 in id114

405

figure 8.47 illustration of the factorization of the subgraph as-

sociated with factor node fs.

xm

  xm   fs(xm )

fs

  fs   x(x)

x

xm

gm(xm, xsm)

(cid:2)

where ne(fs) denotes the set of variable nodes that are neighbours of the factor node
fs, and ne(fs) \ x denotes the same set but with node x removed. here we have
de   ned the following messages from variable nodes to factor nodes

  xm   fs(xm)    

gm(xm, xsm).

(8.67)

xsm

we have therefore introduced two distinct kinds of message, those that go from factor
nodes to variable nodes denoted   f   x(x), and those that go from variable nodes to
factor nodes denoted   x   f (x). in each case, we see that messages passed along a
link are always a function of the variable associated with the variable node that link
connects to.

the result (8.66) says that to evaluate the message sent by a factor node to a vari-
able node along the link connecting them, take the product of the incoming messages
along all other links coming into the factor node, multiply by the factor associated
with that node, and then marginalize over all of the variables associated with the
incoming messages. this is illustrated in figure 8.47. it is important to note that
a factor node can send a message to a variable node once it has received incoming
messages from all other neighbouring variable nodes.

finally, we derive an expression for evaluating the messages from variable nodes
to factor nodes, again by making use of the (sub-)graph factorization. from fig-
ure 8.48, we see that term gm(xm, xsm) associated with node xm is given by a
product of terms fl(xm, xml) each associated with one of the factor nodes fl that is
linked to node xm (excluding node fs), so that

gm(xm, xsm) =

fl(xm, xml)

(8.68)

(cid:14)

l   ne(xm)\fs

where the product is taken over all neighbours of node xm except for node fs. note
that each of the factors fl(xm, xml) represents a subtree of the original graph of
precisely the same kind as introduced in (8.62). substituting (8.68) into (8.67), we

406

8. id114

figure 8.48 illustration of the evaluation of the message sent by a

variable node to an adjacent factor node.

fl

fl

xm

fs

then obtain

  xm   fs(xm) =

=

(cid:14)
(cid:14)

l   ne(xm)\fs

l   ne(xm)\fs

fl(xm, xml)

 

fl(xm, xml)

(cid:31)(cid:2)

xml
  fl   xm(xm)

(8.69)

where we have used the de   nition (8.64) of the messages passed from factor nodes to
variable nodes. thus to evaluate the message sent by a variable node to an adjacent
factor node along the connecting link, we simply take the product of the incoming
messages along all of the other links. note that any variable node that has only
two neighbours performs no computation but simply passes messages through un-
changed. also, we note that a variable node can send a message to a factor node
once it has received incoming messages from all other neighbouring factor nodes.

recall that our goal is to calculate the marginal for variable node x, and that this
marginal is given by the product of incoming messages along all of the links arriving
at that node. each of these messages can be computed recursively in terms of other
messages. in order to start this recursion, we can view the node x as the root of the
tree and begin at the leaf nodes. from the de   nition (8.69), we see that if a leaf node
is a variable node, then the message that it sends along its one and only link is given
by

(8.70)
as illustrated in figure 8.49(a). similarly, if the leaf node is a factor node, we see
from (8.66) that the message sent should take the form

  x   f (x) = 1

  f   x(x) = f(x)

(8.71)

figure 8.49 the sum-product algorithm
begins with messages sent
by the leaf nodes, which de-
pend on whether the leaf
node is (a) a variable node,
or (b) a factor node.

  x   f (x) = 1

  f   x(x) = f(x)

x

f

f

x

(a)

(b)

8.4. id136 in id114

407

as illustrated in figure 8.49(b).

at this point, it is worth pausing to summarize the particular version of the sum-
product algorithm obtained so far for evaluating the marginal p(x). we start by
viewing the variable node x as the root of the factor graph and initiating messages
at the leaves of the graph using (8.70) and (8.71). the message passing steps (8.66)
and (8.69) are then applied recursively until messages have been propagated along
every link, and the root node has received messages from all of its neighbours. each
node can send a message towards the root once it has received messages from all
of its other neighbours. once the root node has received messages from all of its
neighbours, the required marginal can be evaluated using (8.63). we shall illustrate
this process shortly.

to see that each node will always receive enough messages to be able to send out
a message, we can use a simple inductive argument as follows. clearly, for a graph
comprising a variable root node connected directly to several factor leaf nodes, the
algorithm trivially involves sending messages of the form (8.71) directly from the
leaves to the root. now imagine building up a general graph by adding nodes one at
a time, and suppose that for some particular graph we have a valid algorithm. when
one more (variable or factor) node is added, it can be connected only by a single
link because the overall graph must remain a tree, and so the new node will be a leaf
node. it therefore sends a message to the node to which it is linked, which in turn
will therefore receive all the messages it requires in order to send its own message
towards the root, and so again we have a valid algorithm, thereby completing the
proof.

now suppose we wish to    nd the marginals for every variable node in the graph.
this could be done by simply running the above algorithm afresh for each such node.
however, this would be very wasteful as many of the required computations would
be repeated. we can obtain a much more ef   cient procedure by    overlaying    these
multiple message passing algorithms to obtain the general sum-product algorithm
as follows. arbitrarily pick any (variable or factor) node and designate it as the
root. propagate messages from the leaves to the root as before. at this point, the
root node will have received messages from all of its neighbours. it can therefore
send out messages to all of its neighbours. these in turn will then have received
messages from all of their neighbours and so can send out messages along the links
going away from the root, and so on. in this way, messages are passed outwards
from the root all the way to the leaves. by now, a message will have passed in
both directions across every link in the graph, and every node will have received
a message from all of its neighbours. again a simple inductive argument can be
used to verify the validity of this message passing protocol. because every variable
node will have received messages from all of its neighbours, we can readily calculate
the marginal distribution for every variable in the graph. the number of messages
that have to be computed is given by twice the number of links in the graph and
so involves only twice the computation involved in    nding a single marginal. by
comparison, if we had run the sum-product algorithm separately for each node, the
amount of computation would grow quadratically with the size of the graph. note
that this algorithm is in fact independent of which node was designated as the root,

exercise 8.20

408

8. id114

figure 8.50 the sum-product algorithm can be viewed
purely in terms of messages sent out by factor
nodes to other factor nodes.
in this example,
the outgoing message shown by the blue arrow
is obtained by taking the product of all the in-
coming messages shown by green arrows, mul-
tiplying by the factor fs, and marginalizing over
the variables x1 and x2.

x1

x2

fs

x3

exercise 8.21

and indeed the notion of one node having a special status was introduced only as a
convenient way to explain the message passing protocol.

next suppose we wish to    nd the marginal distributions p(xs) associated with
the sets of variables belonging to each of the factors. by a similar argument to that
used above, it is easy to see that the marginal associated with a factor is given by the
product of messages arriving at the factor node and the local factor at that node

p(xs) = fs(xs)

  xi   fs(xi)

(8.72)

(cid:14)

i   ne(fs)

in complete analogy with the marginals at the variable nodes.
if the factors are
parameterized functions and we wish to learn the values of the parameters using
the em algorithm, then these marginals are precisely the quantities we will need to
calculate in the e step, as we shall see in detail when we discuss the hidden markov
model in chapter 13.

the message sent by a variable node to a factor node, as we have seen, is simply
the product of the incoming messages on other links. we can if we wish view the
sum-product algorithm in a slightly different form by eliminating messages from
variable nodes to factor nodes and simply considering messages that are sent out by
factor nodes. this is most easily seen by considering the example in figure 8.50.

so far, we have rather neglected the issue of id172. if the factor graph
was derived from a directed graph, then the joint distribution is already correctly nor-
malized, and so the marginals obtained by the sum-product algorithm will similarly
be normalized correctly. however, if we started from an undirected graph, then in
general there will be an unknown id172 coef   cient 1/z. as with the simple
chain example of figure 8.38, this is easily handled by working with an unnormal-

ized version(cid:4)p(x) of the joint distribution, where p(x) =(cid:4)p(x)/z. we    rst run the
sum-product algorithm to    nd the corresponding unnormalized marginals(cid:4)p(xi). the
(cid:4)p(x) directly.

coef   cient 1/z is then easily obtained by normalizing any one of these marginals,
and this is computationally ef   cient because the id172 is done over a single
variable rather than over the entire set of variables as would be required to normalize

at this point, it may be helpful to consider a simple example to illustrate the
operation of the sum-product algorithm. figure 8.51 shows a simple 4-node factor

figure 8.51 a simple factor graph used to illustrate the

sum-product algorithm.

x1

x2

8.4. id136 in id114

409

x3

fa

fb

fc

x4

graph whose unnormalized joint distribution is given by

(cid:4)p(x) = fa(x1, x2)fb(x2, x3)fc(x2, x4).

(8.73)

in order to apply the sum-product algorithm to this graph, let us designate node x3
as the root, in which case there are two leaf nodes x1 and x4. starting with the leaf
nodes, we then have the following sequence of six messages

the direction of    ow of these messages is illustrated in figure 8.52. once this mes-
sage propagation is complete, we can then propagate messages from the root node
out to the leaf nodes, and these are given by

  x1   fa(x1) = 1
  fa   x2(x2) =

x1
  x4   fc(x4) = 1
  fc   x2(x2) =

fa(x1, x2)

fc(x2, x4)

  x2   fb(x2) =   fa   x2(x2)  fc   x2(x2)
fb(x2, x3)  x2   fb.
  fb   x3(x3) =

  x3   fb(x3) = 1
  fb   x2(x2) =

fb(x2, x3)

  x2   fa(x2) =   fb   x2(x2)  fc   x2(x2)
  fa   x1(x1) =

fa(x1, x2)  x2   fa(x2)

  x2   fc(x2) =   fa   x2(x2)  fb   x2(x2)
  fc   x4(x4) =

fc(x2, x4)  x2   fc(x2).

(cid:2)
(cid:2)
(cid:2)

x4

x2

x3

(cid:2)
(cid:2)
(cid:2)

x2

x2

(8.74)

(8.75)

(8.76)

(8.77)

(8.78)

(8.79)

(8.80)

(8.81)

(8.82)

(8.83)

(8.84)

(8.85)

410

8. id114

x1

x2

x3

x1

x2

x3

x4
(a)

x4
(b)

figure 8.52 flow of messages for the sum-product algorithm applied to the example graph in figure 8.51. (a)
from the leaf nodes x1 and x4 towards the root node x3. (b) from the root node towards the leaf nodes.

(cid:31)(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)

x1

x2

x1

(cid:2)
(cid:2)

x4

=

=

=

one message has now passed in each direction across each link, and we can now
evaluate the marginals. as a simple check, let us verify that the marginal p(x2) is
given by the correct expression. using (8.63) and substituting for the messages using
the above results, we have

(cid:4)p(x2) =   fa   x2(x2)  fb   x2(x2)  fc   x2(x2)
 (cid:31)(cid:2)

 (cid:31)(cid:2)

 

fa(x1, x2)

fb(x2, x3)

fc(x2, x4)

x3

x4

fa(x1, x2)fb(x2, x3)fc(x2, x4)

(cid:4)p(x)

(8.86)

x1

x3

x4

as required.

so far, we have assumed that all of the variables in the graph are hidden. in most
practical applications, a subset of the variables will be observed, and we wish to cal-
culate posterior distributions conditioned on these observations. observed nodes are
easily handled within the sum-product algorithm as follows. suppose we partition x
into hidden variables h and observed variables v, and that the observed value of v

(cid:21)
is denoted(cid:1)v. then we simply multiply the joint distribution p(x) by
i i(vi,(cid:1)vi),
where i(v,(cid:1)v) = 1 if v =(cid:1)v and i(v,(cid:1)v) = 0 otherwise. this product corresponds
to p(h, v = (cid:1)v) and hence is an unnormalized version of p(h|v = (cid:1)v). by run-
p(hi|v =(cid:1)v) up to a id172 coef   cient whose value can be found ef   ciently

ning the sum-product algorithm, we can ef   ciently calculate the posterior marginals

using a local computation. any summations over variables in v then collapse into a
single term.

we have assumed throughout this section that we are dealing with discrete vari-
ables. however, there is nothing speci   c to discrete variables either in the graphical
framework or in the probabilistic construction of the sum-product algorithm. for

8.4. id136 in id114

411

table 8.1 example of a joint distribution over two binary variables for
which the maximum of the joint distribution occurs for dif-
ferent variable values compared to the maxima of the two
marginals.

y = 0
y = 1

x = 0 x = 1
0.4
0.3
0.3
0.0

section 13.3

continuous variables the summations are simply replaced by integrations. we shall
give an example of the sum-product algorithm applied to a graph of linear-gaussian
variables when we consider linear dynamical systems.

8.4.5 the max-sum algorithm
the sum-product algorithm allows us to take a joint distribution p(x) expressed
as a factor graph and ef   ciently    nd marginals over the component variables. two
other common tasks are to    nd a setting of the variables that has the largest prob-
ability and to    nd the value of that id203. these can be addressed through a
closely related algorithm called max-sum, which can be viewed as an application of
id145 in the context of id114 (cormen et al., 2001).

a simple approach to    nding latent variable values having high id203
would be to run the sum-product algorithm to obtain the marginals p(xi) for ev-
ery variable, and then, for each marginal in turn, to    nd the value x(cid:1)
i that maximizes
that marginal. however, this would give the set of values that are individually the
most probable. in practice, we typically wish to    nd the set of values that jointly
have the largest id203, in other words the vector xmax that maximizes the joint
distribution, so that

xmax = arg max

p(x)

x

for which the corresponding value of the joint id203 will be given by

p(xmax) = max

x

p(x).

(8.87)

(8.88)

in general, xmax is not the same as the set of x(cid:1)
i values, as we can easily show using
a simple example. consider the joint distribution p(x, y) over two binary variables
x, y     {0, 1} given in table 8.1. the joint distribution is maximized by setting x =
1 and y = 0, corresponding the value 0.4. however, the marginal for p(x), obtained
by summing over both values of y, is given by p(x = 0) = 0.6 and p(x = 1) = 0.4,
and similarly the marginal for y is given by p(y = 0) = 0.7 and p(y = 1) = 0.3,
and so the marginals are maximized by x = 0 and y = 0, which corresponds to a
value of 0.3 for the joint distribution. in fact, it is not dif   cult to construct examples
for which the set of individually most probable values has id203 zero under the
joint distribution.

we therefore seek an ef   cient algorithm for    nding the value of x that maxi-
mizes the joint distribution p(x) and that will allow us to obtain the value of the
joint distribution at its maximum. to address the second of these problems, we shall
simply write out the max operator in terms of its components

max

x

p(x) = max
x1

. . . max
xm

p(x)

(8.89)

exercise 8.27

412

8. id114

where m is the total number of variables, and then substitute for p(x) using its
expansion in terms of a product of factors. in deriving the sum-product algorithm,
we made use of the distributive law (8.53) for multiplication. here we make use of
the analogous law for the max operator

(8.90)
which holds if a (cid:2) 0 (as will always be the case for the factors in a graphical model).
this allows us to exchange products with maximizations.

max(ab, ac) = a max(b, c)

consider    rst the simple example of a chain of nodes described by (8.49). the

evaluation of the id203 maximum can be written as

(cid:29)

1
z

max

x

p(x) =

=

1
z

max
x1

(cid:29)

       max

xn

max
x1

  1,2(x1, x2)

[  1,2(x1, x2)         n   1,n (xn   1, xn )]
       max

  n   1,n (xn   1, xn )

.

xn

(cid:30)(cid:30)

as with the calculation of marginals, we see that exchanging the max and product
operators results in a much more ef   cient computation, and one that is easily inter-
preted in terms of messages passed from node xn backwards along the chain to node
x1.

we can readily generalize this result to arbitrary tree-structured factor graphs
by substituting the expression (8.59) for the factor graph expansion into (8.89) and
again exchanging maximizations with products. the structure of this calculation is
identical to that of the sum-product algorithm, and so we can simply translate those
results into the present context. in particular, suppose that we designate a particular
variable node as the    root    of the graph. then we start a set of messages propagating
inwards from the leaves of the tree towards the root, with each node sending its
message towards the root once it has received all incoming messages from its other
neighbours. the    nal maximization is performed over the product of all messages
arriving at the root node, and gives the maximum value for p(x). this could be called
the max-product algorithm and is identical to the sum-product algorithm except that
summations are replaced by maximizations. note that at this stage, messages have
been sent from leaves to the root, but not in the other direction.

in practice, products of many small probabilities can lead to numerical under-
   ow problems, and so it is convenient to work with the logarithm of the joint distri-
bution. the logarithm is a monotonic function, so that if a > b then ln a > ln b, and
hence the max operator and the logarithm function can be interchanged, so that

(cid:17)

(cid:18)

ln

max

x

p(x)

= max

x

ln p(x).

the distributive property is preserved because

max(a + b, a + c) = a + max(b, c).

(8.91)

(8.92)

thus taking the logarithm simply has the effect of replacing the products in the
max-product algorithm with sums, and so we obtain the max-sum algorithm. from

8.4. id136 in id114

413

the results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can
readily write down the max-sum algorithm in terms of message passing simply by
replacing    sum    with    max    and replacing products with sums of logarithms to give

      ln f(x, x1, . . . , xm ) +

(cid:2)

       (8.93)

  xm   f (xm)

m   ne(fs)\x

  x   f (x) =

  fl   x(x).

(8.94)

  f   x(x) = max
x1,...,xm

(cid:2)

l   ne(x)\f

the initial messages sent by the leaf nodes are obtained by analogy with (8.70) and
(8.71) and are given by

(8.95)
(8.96)

(8.97)

while at the root node the maximum id203 can then be computed, by analogy
with (8.63), using

  x   f (x) = 0
  f   x(x) = ln f(x)

pmax = max

x

  fs   x(x)

       (cid:2)

s   ne(x)

       (cid:2)

s   ne(x)

       .

       .

so far, we have seen how to    nd the maximum of the joint distribution by prop-
agating messages from the leaves to an arbitrarily chosen root node. the result will
be the same irrespective of which node is chosen as the root. now we turn to the
second problem of    nding the con   guration of the variables for which the joint dis-
tribution attains this maximum value. so far, we have sent messages from the leaves
to the root. the process of evaluating (8.97) will also give the value xmax for the
most probable value of the root node variable, de   ned by

xmax = arg max

x

  fs   x(x)

(8.98)

at this point, we might be tempted simply to continue with the message passing al-
gorithm and send messages from the root back out to the leaves, using (8.93) and
(8.94), then apply (8.98) to all of the remaining variable nodes. however, because
we are now maximizing rather than summing, it is possible that there may be mul-
tiple con   gurations of x all of which give rise to the maximum value for p(x). in
such cases, this strategy can fail because it is possible for the individual variable
values obtained by maximizing the product of messages at each node to belong to
different maximizing con   gurations, giving an overall con   guration that no longer
corresponds to a maximum.

the problem can be resolved by adopting a rather different kind of message
passing from the root node to the leaves. to see how this works, let us return once
again to the simple chain example of n variables x1, . . . , xn each having k states,

414

8. id114

figure 8.53 a lattice, or trellis, diagram show-
ing explicitly the k possible states (one per row
of the diagram) for each of the variables xn in the
in this illustration k = 3. the ar-
chain model.
row shows the direction of message passing in the
max-product algorithm. for every state k of each
variable xn (corresponding to column n of the dia-
gram) the function   (xn) de   nes a unique state at
the previous variable, indicated by the black lines.
the two paths through the lattice correspond to
con   gurations that give the global maximum of the
joint id203 distribution, and either of these
can be found by tracing back along the black lines
in the opposite direction to the arrow.

k = 1

k = 2

k = 3

n     2

n     1

n

n + 1

corresponding to the graph shown in figure 8.38. suppose we take node xn to be
the root node. then in the    rst phase, we propagate messages from the leaf node x1
to the root node using

  xn   fn,n+1(xn) =   fn   1,n   xn(xn)
  fn   1,n   xn(xn) = max
xn   1

ln fn   1,n(xn   1, xn) +   xn   1   f n   1,n(xn)

(cid:8)

(cid:9)

which follow from applying (8.94) and (8.93) to this particular graph. the initial
message sent from the leaf node is simply

the most probable value for xn is then given by

  x1   f1,2(x1) = 0.

(cid:8)

(cid:9)

(8.99)

.

(8.100)

xmax
n = arg max

xn

  fn   1,n   xn (xn )

now we need to determine the states of the previous variables that correspond to the
same maximizing con   guration. this can be done by keeping track of which values
of the variables gave rise to the maximum state of each variable, in other words by
storing quantities given by

  (xn) = arg max

xn   1

ln fn   1,n(xn   1, xn) +   xn   1   f n   1,n(xn)

.

(8.101)

(cid:8)

(cid:9)

to understand better what is happening, it is helpful to represent the chain of vari-
ables in terms of a lattice or trellis diagram as shown in figure 8.53. note that this
is not a probabilistic graphical model because the nodes represent individual states
of variables, while each variable corresponds to a column of such states in the di-
agram. for each state of a given variable, there is a unique state of the previous
variable that maximizes the id203 (ties are broken either systematically or at
random), corresponding to the function   (xn) given by (8.101), and this is indicated

8.4. id136 in id114

415

by the lines connecting the nodes. once we know the most probable value of the    -
nal node xn , we can then simply follow the link back to    nd the most probable state
of node xn   1 and so on back to the initial node x1. this corresponds to propagating
a message back down the chain using

n   1 =   (xmax
xmax
n )

(8.102)

and is known as back-tracking. note that there could be several values of xn   1 all
of which give the maximum value in (8.101). provided we chose one of these values
when we do the back-tracking, we are assured of a globally consistent maximizing
con   guration.

in figure 8.53, we have indicated two paths, each of which we shall suppose
corresponds to a global maximum of the joint id203 distribution. if k = 2
and k = 3 each represent possible values of xmax
n , then starting from either state
and tracing back along the black lines, which corresponds to iterating (8.102), we
obtain a valid global maximum con   guration. note that if we had run a forward
pass of max-sum message passing followed by a backward pass and then applied
(8.98) at each node separately, we could end up selecting some states from one path
and some from the other path, giving an overall con   guration that is not a global
maximizer. we see that it is necessary instead to keep track of the maximizing states
during the forward pass using the functions   (xn) and then use back-tracking to    nd
a consistent solution.

the extension to a general tree-structured factor graph should now be clear. if
a message is sent from a factor node f to a variable node x, a maximization is
performed over all other variable nodes x1, . . . , xm that are neighbours of that fac-
tor node, using (8.93). when we perform this maximization, we keep a record of
which values of the variables x1, . . . , xm gave rise to the maximum. then in the
back-tracking step, having found xmax, we can then use these stored values to as-
sign consistent maximizing states xmax
m . the max-sum algorithm, with
back-tracking, gives an exact maximizing con   guration for the variables provided
the factor graph is a tree. an important application of this technique is for    nding
the most probable sequence of hidden states in a hidden markov model, in which
case it is known as the viterbi algorithm.

, . . . , xmax

1

as with the sum-product algorithm, the inclusion of evidence in the form of
observed variables is straightforward. the observed variables are clamped to their
observed values, and the maximization is performed over the remaining hidden vari-
ables. this can be shown formally by including identity functions for the observed
variables into the factor functions, as we did for the sum-product algorithm.

it is interesting to compare max-sum with the iterated conditional modes (icm)
algorithm described on page 389. each step in icm is computationally simpler be-
cause the    messages    that are passed from one node to the next comprise a single
value consisting of the new state of the node for which the conditional distribution
is maximized. the max-sum algorithm is more complex because the messages are
functions of node variables x and hence comprise a set of k values for each pos-
sible state of x. unlike max-sum, however, icm is not guaranteed to    nd a global
maximum even for tree-structured graphs.

section 13.2

416

8. id114

8.4.6 exact id136 in general graphs
the sum-product and max-sum algorithms provide ef   cient and exact solutions
to id136 problems in tree-structured graphs. for many practical applications,
however, we have to deal with graphs having loops.

the message passing framework can be generalized to arbitrary graph topolo-
gies, giving an exact id136 procedure known as the junction tree algorithm (lau-
ritzen and spiegelhalter, 1988; jordan, 2007). here we give a brief outline of the
key steps involved. this is not intended to convey a detailed understanding of the
algorithm, but rather to give a    avour of the various stages involved. if the starting
point is a directed graph, it is    rst converted to an undirected graph by moraliza-
tion, whereas if starting from an undirected graph this step is not required. next the
graph is triangulated, which involves    nding chord-less cycles containing four or
more nodes and adding extra links to eliminate such chord-less cycles. for instance,
in the graph in figure 8.36, the cycle a   c   b   d   a is chord-less a link could be
added between a and b or alternatively between c and d. note that the joint dis-
tribution for the resulting triangulated graph is still de   ned by a product of the same
potential functions, but these are now considered to be functions over expanded sets
of variables. next the triangulated graph is used to construct a new tree-structured
undirected graph called a join tree, whose nodes correspond to the maximal cliques
of the triangulated graph, and whose links connect pairs of cliques that have vari-
ables in common. the selection of which pairs of cliques to connect in this way is
important and is done so as to give a maximal spanning tree de   ned as follows. of
all possible trees that link up the cliques, the one that is chosen is one for which the
weight of the tree is largest, where the weight for a link is the number of nodes shared
by the two cliques it connects, and the weight for the tree is the sum of the weights
for the links. if the tree is condensed, so that any clique that is a subset of another
clique is absorbed into the larger clique, this gives a junction tree. as a consequence
of the triangulation step, the resulting tree satis   es the running intersection property,
which means that if a variable is contained in two cliques, then it must also be con-
tained in every clique on the path that connects them. this ensures that id136
about variables will be consistent across the graph. finally, a two-stage message
passing algorithm, essentially equivalent to the sum-product algorithm, can now be
applied to this junction tree in order to    nd marginals and conditionals. although
the junction tree algorithm sounds complicated, at its heart is the simple idea that
we have used already of exploiting the factorization properties of the distribution to
allow sums and products to be interchanged so that partial summations can be per-
formed, thereby avoiding having to work directly with the joint distribution. the
role of the junction tree is to provide a precise and ef   cient way to organize these
computations. it is worth emphasizing that this is achieved using purely graphical
operations!

the junction tree is exact for arbitrary graphs and is ef   cient in the sense that
for a given graph there does not in general exist a computationally cheaper approach.
unfortunately, the algorithm must work with the joint distributions within each node
(each of which corresponds to a clique of the triangulated graph) and so the compu-
tational cost of the algorithm is determined by the number of variables in the largest

8.4. id136 in id114

417

clique and will grow exponentially with this number in the case of discrete variables.
an important concept is the treewidth of a graph (bodlaender, 1993), which is de-
   ned in terms of the number of variables in the largest clique. in fact, it is de   ned to
be as one less than the size of the largest clique, to ensure that a tree has a treewidth
of 1. because there in general there can be multiple different junction trees that can
be constructed from a given starting graph, the treewidth is de   ned by the junction
tree for which the largest clique has the fewest variables.
if the treewidth of the
original graph is high, the junction tree algorithm becomes impractical.

8.4.7 loopy belief propagation
for many problems of practical interest, it will not be feasible to use exact in-
ference, and so we need to exploit effective approximation methods. an important
class of such approximations, that can broadly be called variational methods, will be
discussed in detail in chapter 10. complementing these deterministic approaches is
a wide range of sampling methods, also called monte carlo methods, that are based
on stochastic numerical sampling from distributions and that will be discussed at
length in chapter 11.

here we consider one simple approach to approximate id136 in graphs with
loops, which builds directly on the previous discussion of exact id136 in trees.
the idea is simply to apply the sum-product algorithm even though there is no guar-
antee that it will yield good results. this approach is known as loopy belief propa-
gation (frey and mackay, 1998) and is possible because the message passing rules
(8.66) and (8.69) for the sum-product algorithm are purely local. however, because
the graph now has cycles, information can    ow many times around the graph. for
some models, the algorithm will converge, whereas for others it will not.

in order to apply this approach, we need to de   ne a message passing schedule.
let us assume that one message is passed at a time on any given link and in any
given direction. each message sent from a node replaces any previous message sent
in the same direction across the same link and will itself be a function only of the
most recent messages received by that node at previous steps of the algorithm.

we have seen that a message can only be sent across a link from a node when
all other messages have been received by that node across its other links. because
there are loops in the graph, this raises the problem of how to initiate the message
passing algorithm. to resolve this, we suppose that an initial message given by the
unit function has been passed across every link in each direction. every node is then
in a position to send a message.

there are now many possible ways to organize the message passing schedule.
for example, the    ooding schedule simultaneously passes a message across every
link in both directions at each time step, whereas schedules that pass one message at
a time are called serial schedules.

following kschischnang et al. (2001), we will say that a (variable or factor)
node a has a message pending on its link to a node b if node a has received any
message on any of its other links since the last time it send a message to b. thus,
when a node receives a message on one of its links, this creates pending messages
on all of its other links. only pending messages need to be transmitted because

418

8. id114

exercise 8.29

other messages would simply duplicate the previous message on the same link. for
graphs that have a tree structure, any schedule that sends only pending messages
will eventually terminate once a message has passed in each direction across every
link. at this point, there are no pending messages, and the product of the received
messages at every variable give the exact marginal. in graphs having loops, however,
the algorithm may never terminate because there might always be pending messages,
although in practice it is generally found to converge within a reasonable time for
most applications. once the algorithm has converged, or once it has been stopped
if convergence is not observed, the (approximate) local marginals can be computed
using the product of the most recently received incoming messages to each variable
node or factor node on every link.

in some applications, the loopy belief propagation algorithm can give poor re-
sults, whereas in other applications it has proven to be very effective. in particular,
state-of-the-art algorithms for decoding certain kinds of error-correcting codes are
equivalent to loopy belief propagation (gallager, 1963; berrou et al., 1993; mceliece
et al., 1998; mackay and neal, 1999; frey, 1998).

8.4.8 learning the graph structure
in our discussion of id136 in id114, we have assumed that the
structure of the graph is known and    xed. however, there is also interest in go-
ing beyond the id136 problem and learning the graph structure itself from data
(friedman and koller, 2003). this requires that we de   ne a space of possible struc-
tures as well as a measure that can be used to score each structure.

from a bayesian viewpoint, we would ideally like to compute a posterior dis-
tribution over graph structures and to make predictions by averaging with respect
to this distribution. if we have a prior p(m) over graphs indexed by m, then the
posterior distribution is given by

p(m|d)     p(m)p(d|m)

(8.103)
where d is the observed data set. the model evidence p(d|m) then provides the
score for each model. however, evaluation of the evidence involves marginalization
over the latent variables and presents a challenging computational problem for many
models.

exploring the space of structures can also be problematic. because the number
of different graph structures grows exponentially with the number of nodes, it is
often necessary to resort to heuristics to    nd good candidates.

exercises

8.1 ((cid:12)) www by marginalizing out the variables in order, show that the representation
(8.5) for the joint distribution of a directed graph is correctly normalized, provided
each of the conditional distributions is normalized.

8.2 ((cid:12)) www show that the property of there being no directed cycles in a directed
graph follows from the statement that there exists an ordered numbering of the nodes
such that for each node there are no links going to a lower-numbered node.

table 8.2 the joint distribution over three binary variables.

exercises

419

a
0
0
0
0
1
1
1
1

b
0
0
1
1
0
0
1
1

c
0
1
0
1
0
1
0
1

p(a, b, c)

0.192
0.144
0.048
0.216
0.192
0.064
0.048
0.096

8.3 ((cid:12) (cid:12)) consider three binary variables a, b, c     {0, 1} having the joint distribution
given in table 8.2. show by direct evaluation that this distribution has the property
that a and b are marginally dependent, so that p(a, b) (cid:9)= p(a)p(b), but that they
become independent when conditioned on c, so that p(a, b|c) = p(a|c)p(b|c) for
both c = 0 and c = 1.

8.4 ((cid:12) (cid:12)) evaluate the distributions p(a), p(b|c), and p(c|a) corresponding to the joint
distribution given in table 8.2. hence show by direct evaluation that p(a, b, c) =
p(a)p(c|a)p(b|c). draw the corresponding directed graph.

8.5 ((cid:12)) www draw a directed probabilistic graphical model corresponding to the

relevance vector machine described by (7.79) and (7.80).

8.6 ((cid:12)) for the model shown in figure 8.13, we have seen that the number of parameters
required to specify the conditional distribution p(y|x1, . . . , xm ), where xi     {0, 1},
could be reduced from 2m to m + 1 by making use of the logistic sigmoid represen-
tation (8.10). an alternative representation (pearl, 1988) is given by

m(cid:14)

p(y = 1|x1, . . . , xm ) = 1     (1       0)

(1       i)xi

(8.104)

i=1

where the parameters   i represent the probabilities p(xi = 1), and   0 is an additional
parameters satisfying 0 (cid:1)   0 (cid:1) 1. the conditional distribution (8.104) is known as
the noisy-or. show that this can be interpreted as a    soft    (probabilistic) form of the
logical or function (i.e., the function that gives y = 1 whenever at least one of the
xi = 1). discuss the interpretation of   0.

8.7 ((cid:12) (cid:12)) using the recursion relations (8.15) and (8.16), show that the mean and covari-
ance of the joint distribution for the graph shown in figure 8.14 are given by (8.17)
and (8.18), respectively.

8.8 ((cid:12)) www show that a        b, c | d implies a        b | d.
8.9 ((cid:12)) www using the d-separation criterion, show that the conditional distribution
for a node x in a directed graph, conditioned on all of the nodes in the markov
blanket, is independent of the remaining variables in the graph.

420

8. id114

figure 8.54 example of a graphical model used to explore the con-
ditional independence properties of the head-to-head
path a   c   b when a descendant of c, namely the node
d, is observed.

a

b

c

d

8.10 ((cid:12)) consider the directed graph shown in figure 8.54 in which none of the variables
is observed. show that a        b |    . suppose we now observe the variable d. show
that in general a (cid:9)       b | d.

8.11 ((cid:12) (cid:12)) consider the example of the car fuel system shown in figure 8.21, and suppose
that instead of observing the state of the fuel gauge g directly, the gauge is seen by
the driver d who reports to us the reading on the gauge. this report is either that the
gauge shows full d = 1 or that it shows empty d = 0. our driver is a bit unreliable,
as expressed through the following probabilities

p(d = 1|g = 1) = 0.9
p(d = 0|g = 0) = 0.9.

(8.105)
(8.106)

suppose that the driver tells us that the fuel gauge shows empty, in other words
that we observe d = 0. evaluate the id203 that the tank is empty given only
this observation. similarly, evaluate the corresponding id203 given also the
observation that the battery is    at, and note that this second id203 is lower.
discuss the intuition behind this result, and relate the result to figure 8.54.

8.12 ((cid:12)) www show that there are 2m (m   1)/2 distinct undirected graphs over a set of

m distinct random variables. draw the 8 possibilities for the case of m = 3.

8.13 ((cid:12)) consider the use of iterated conditional modes (icm) to minimize the energy
function given by (8.42). write down an expression for the difference in the values
of the energy associated with the two states of a particular variable xj, with all other
variables held    xed, and show that it depends only on quantities that are local to xj
in the graph.

8.14 ((cid:12)) consider a particular case of the energy function given by (8.42) in which the
coef   cients    = h = 0. show that the most probable con   guration of the latent
variables is given by xi = yi for all i.

8.15 ((cid:12) (cid:12)) www show that the joint distribution p(xn   1, xn) for two neighbouring
nodes in the graph shown in figure 8.38 is given by an expression of the form (8.58).

exercises

421

8.16 ((cid:12) (cid:12)) consider the id136 problem of evaluating p(xn|xn ) for the graph shown
in figure 8.38, for all nodes n     {1, . . . , n     1}. show that the message passing
algorithm discussed in section 8.4.1 can be used to solve this ef   ciently, and discuss
which messages are modi   ed and in what way.

8.17 ((cid:12) (cid:12)) consider a graph of the form shown in figure 8.38 having n = 5 nodes, in
which nodes x3 and x5 are observed. use d-separation to show that x2        x5 | x3.
show that if the message passing algorithm of section 8.4.1 is applied to the evalu-
ation of p(x2|x3, x5), the result will be independent of the value of x5.

8.18 ((cid:12) (cid:12)) www show that a distribution represented by a directed tree can trivially
be written as an equivalent distribution over the corresponding undirected tree. also
show that a distribution expressed as an undirected tree can, by suitable normaliza-
tion of the clique potentials, be written as a directed tree. calculate the number of
distinct directed trees that can be constructed from a given undirected tree.

8.19 ((cid:12) (cid:12)) apply the sum-product algorithm derived in section 8.4.4 to the chain-of-
nodes model discussed in section 8.4.1 and show that the results (8.54), (8.55), and
(8.57) are recovered as a special case.

8.20 ((cid:12)) www consider the message passing protocol for the sum-product algorithm on
a tree-structured factor graph in which messages are    rst propagated from the leaves
to an arbitrarily chosen root node and then from the root node out to the leaves. use
proof by induction to show that the messages can be passed in such an order that
at every step, each node that must send a message has received all of the incoming
messages necessary to construct its outgoing messages.

8.21 ((cid:12) (cid:12)) www show that the marginal distributions p(xs) over the sets of variables
xs associated with each of the factors fx(xs) in a factor graph can be found by    rst
running the sum-product message passing algorithm and then evaluating the required
marginals using (8.72).

8.22 ((cid:12)) consider a tree-structured factor graph, in which a given subset of the variable
nodes form a connected subgraph (i.e., any variable node of the subset is connected
to at least one of the other variable nodes via a single factor node). show how the
sum-product algorithm can be used to compute the marginal distribution over that
subset.

8.23 ((cid:12) (cid:12)) www in section 8.4.4, we showed that the marginal distribution p(xi) for a
variable node xi in a factor graph is given by the product of the messages arriving at
this node from neighbouring factor nodes in the form (8.63). show that the marginal
p(xi) can also be written as the product of the incoming message along any one of
the links with the outgoing message along the same link.

8.24 ((cid:12) (cid:12)) show that the marginal distribution for the variables xs in a factor fs(xs) in
a tree-structured factor graph, after running the sum-product message passing algo-
rithm, can be written as the product of the message arriving at the factor node along
all its links, times the local factor f(xs), in the form (8.72).

422

8. id114

8.25 ((cid:12) (cid:12))

in (8.86), we veri   ed that the sum-product algorithm run on the graph in
figure 8.51 with node x3 designated as the root node gives the correct marginal for
x2. show that the correct marginals are obtained also for x1 and x3. similarly, show
that the use of the result (8.72) after running the sum-product algorithm on this graph
gives the correct joint distribution for x1, x2.

8.26 ((cid:12)) consider a tree-structured factor graph over discrete variables, and suppose we
wish to evaluate the joint distribution p(xa, xb) associated with two variables xa and
xb that do not belong to a common factor. de   ne a procedure for using the sum-
product algorithm to evaluate this joint distribution in which one of the variables is
successively clamped to each of its allowed values.

8.27 ((cid:12) (cid:12)) consider two discrete variables x and y each having three possible states, for
example x, y     {0, 1, 2}. construct a joint distribution p(x, y) over these variables

having the property that the value(cid:1)x that maximizes the marginal p(x), along with
the value(cid:1)y that maximizes the marginal p(y), together have id203 zero under
the joint distribution, so that p((cid:1)x,(cid:1)y) = 0.

8.28 ((cid:12) (cid:12)) www the concept of a pending message in the sum-product algorithm for
a factor graph was de   ned in section 8.4.7. show that if the graph has one or more
cycles, there will always be at least one pending message irrespective of how long
the algorithm runs.

8.29 ((cid:12) (cid:12)) www show that if the sum-product algorithm is run on a factor graph with a
tree structure (no loops), then after a    nite number of messages have been sent, there
will be no pending messages.

9

mixture models

and em

if we de   ne a joint distribution over observed and latent variables, the correspond-
ing distribution of the observed variables alone is obtained by marginalization. this
allows relatively complex marginal distributions over observed variables to be ex-
pressed in terms of more tractable joint distributions over the expanded space of
observed and latent variables. the introduction of latent variables thereby allows
complicated distributions to be formed from simpler components. in this chapter,
we shall see that mixture distributions, such as the gaussian mixture discussed in
section 2.3.9, can be interpreted in terms of discrete latent variables. continuous
latent variables will form the subject of chapter 12.

as well as providing a framework for building more complex id203 dis-
tributions, mixture models can also be used to cluster data. we therefore begin our
discussion of mixture distributions by considering the problem of    nding clusters
in a set of data points, which we approach    rst using a nonprobabilistic technique
called the id116 algorithm (lloyd, 1982). then we introduce the latent variable

423

section 9.1

424

9. mixture models and em

section 9.2

section 9.3

section 9.4

view of mixture distributions in which the discrete latent variables can be interpreted
as de   ning assignments of data points to speci   c components of the mixture. a gen-
eral technique for    nding maximum likelihood estimators in latent variable models
is the id83. we    rst of all use the gaussian
mixture distribution to motivate the em algorithm in a fairly informal way, and then
we give a more careful treatment based on the latent variable viewpoint. we shall
see that the id116 algorithm corresponds to a particular nonprobabilistic limit of
em applied to mixtures of gaussians. finally, we discuss em in some generality.

gaussian mixture models are widely used in data mining, pattern recognition,
machine learning, and statistical analysis. in many applications, their parameters are
determined by maximum likelihood, typically using the em algorithm. however, as
we shall see there are some signi   cant limitations to the maximum likelihood ap-
proach, and in chapter 10 we shall show that an elegant bayesian treatment can be
given using the framework of variational id136. this requires little additional
computation compared with em, and it resolves the principal dif   culties of maxi-
mum likelihood while also allowing the number of components in the mixture to be
inferred automatically from the data.

9.1. id116 id91

we begin by considering the problem of identifying groups, or clusters, of data points
in a multidimensional space. suppose we have a data set {x1, . . . , xn} consisting
of n observations of a random d-dimensional euclidean variable x. our goal is to
partition the data set into some number k of clusters, where we shall suppose for
the moment that the value of k is given. intuitively, we might think of a cluster as
comprising a group of data points whose inter-point distances are small compared
with the distances to points outside of the cluster. we can formalize this notion by
   rst introducing a set of d-dimensional vectors   k, where k = 1, . . . , k, in which
  k is a prototype associated with the kth cluster. as we shall see shortly, we can
think of the   k as representing the centres of the clusters. our goal is then to    nd
an assignment of data points to clusters, as well as a set of vectors {  k}, such that
the sum of the squares of the distances of each data point to its closest vector   k, is
a minimum.

it is convenient at this point to de   ne some notation to describe the assignment
of data points to clusters. for each data point xn, we introduce a corresponding set
of binary indicator variables rnk     {0, 1}, where k = 1, . . . , k describing which of
the k clusters the data point xn is assigned to, so that if data point xn is assigned to
cluster k then rnk = 1, and rnj = 0 for j (cid:9)= k. this is known as the 1-of-k coding
scheme. we can then de   ne an objective function, sometimes called a distortion
measure, given by

n(cid:2)

k(cid:2)

j =

rnk(cid:5)xn       k(cid:5)2

(9.1)

which represents the sum of the squares of the distances of each data point to its

n=1

k=1

9.1. id116 id91

425

assigned vector   k. our goal is to    nd values for the {rnk} and the {  k} so as to
minimize j. we can do this through an iterative procedure in which each iteration
involves two successive steps corresponding to successive optimizations with respect
to the rnk and the   k. first we choose some initial values for the   k. then in the    rst
phase we minimize j with respect to the rnk, keeping the   k    xed. in the second
phase we minimize j with respect to the   k, keeping rnk    xed. this two-stage
optimization is then repeated until convergence. we shall see that these two stages
of updating rnk and updating   k correspond respectively to the e (expectation) and
m (maximization) steps of the em algorithm, and to emphasize this we shall use the
terms e step and m step in the context of the id116 algorithm.

section 9.4

consider    rst the determination of the rnk. because j in (9.1) is a linear func-
tion of rnk, this optimization can be performed easily to give a closed form solution.
the terms involving different n are independent and so we can optimize for each
n separately by choosing rnk to be 1 for whichever value of k gives the minimum
value of (cid:5)xn       k(cid:5)2. in other words, we simply assign the nth data point to the
closest cluster centre. more formally, this can be expressed as
if k = arg minj (cid:5)xn       j(cid:5)2
otherwise.

rnk =

(cid:12)

(9.2)

1
0

now consider the optimization of the   k with the rnk held    xed. the objective
function j is a quadratic function of   k, and it can be minimized by setting its
derivative with respect to   k to zero giving

which we can easily solve for   k to give

n(cid:2)

n=1

2

rnk(xn       k) = 0
(cid:5)
n rnkxn(cid:5)

.

n rnk

  k =

(9.3)

(9.4)

the denominator in this expression is equal to the number of points assigned to
cluster k, and so this result has a simple interpretation, namely set   k equal to the
mean of all of the data points xn assigned to cluster k. for this reason, the procedure
is known as the id116 algorithm.

the two phases of re-assigning data points to clusters and re-computing the clus-
ter means are repeated in turn until there is no further change in the assignments (or
until some maximum number of iterations is exceeded). because each phase reduces
the value of the objective function j, convergence of the algorithm is assured. how-
ever, it may converge to a local rather than global minimum of j. the convergence
properties of the id116 algorithm were studied by macqueen (1967).

the id116 algorithm is illustrated using the old faithful data set in fig-
ure 9.1. for the purposes of this example, we have made a linear re-scaling of the
data, known as standardizing, such that each of the variables has zero mean and
unit standard deviation. for this example, we have chosen k = 2, and so in this

exercise 9.1

appendix a

426

9. mixture models and em

(a)

   2

(d)

   2

(g)

2

0

   2

2

0

   2

2

0

   2

0

2

0

2

(b)

   2

(e)

   2

(h)

2

0

   2

2

0

   2

2

0

   2

0

2

0

2

(c)

   2

(f)

   2

(i)

2

0

   2

2

0

   2

2

0

   2

0

2

0

2

   2

0

2

   2

0

2

   2

0

2

figure 9.1 illustration of the id116 algorithm using the re-scaled old faithful data set. (a) green points
denote the data set in a two-dimensional euclidean space. the initial choices for centres   1 and   2 are shown
by the red and blue crosses, respectively. (b) in the initial e step, each data point is assigned either to the red
cluster or to the blue cluster, according to which cluster centre is nearer. this is equivalent to classifying the
points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta
line, they lie on. (c) in the subsequent m step, each cluster centre is re-computed to be the mean of the points
assigned to the corresponding cluster. (d)   (i) show successive e and m steps through to    nal convergence of
the algorithm.

9.1. id116 id91

427

figure 9.2 plot of the cost function j given by
(9.1) after each e step (blue points)
and m step (red points) of the k-
means algorithm for the example
shown in figure 9.1. the algo-
rithm has converged after the third
m step, and the    nal em cycle pro-
duces no changes in either the as-
signments or the prototype vectors.

1000

j

500

0

1

2

3

4

section 9.2.2

section 2.3.5

exercise 9.2

case, the assignment of each data point to the nearest cluster centre is equivalent to a
classi   cation of the data points according to which side they lie of the perpendicular
bisector of the two cluster centres. a plot of the cost function j given by (9.1) for
the old faithful example is shown in figure 9.2.

note that we have deliberately chosen poor initial values for the cluster centres
so that the algorithm takes several steps before convergence. in practice, a better
initialization procedure would be to choose the cluster centres   k to be equal to a
random subset of k data points. it is also worth noting that the id116 algorithm
itself is often used to initialize the parameters in a gaussian mixture model before
applying the em algorithm.

a direct implementation of the id116 algorithm as discussed here can be
relatively slow, because in each e step it is necessary to compute the euclidean dis-
tance between every prototype vector and every data point. various schemes have
been proposed for speeding up the id116 algorithm, some of which are based on
precomputing a data structure such as a tree such that nearby points are in the same
subtree (ramasubramanian and paliwal, 1990; moore, 2000). other approaches
make use of the triangle inequality for distances, thereby avoiding unnecessary dis-
tance calculations (hodgson, 1998; elkan, 2003).

so far, we have considered a batch version of id116 in which the whole data
set is used together to update the prototype vectors. we can also derive an on-line
stochastic algorithm (macqueen, 1967) by applying the robbins-monro procedure
to the problem of    nding the roots of the regression function given by the derivatives
of j in (9.1) with respect to   k. this leads to a sequential update in which, for each
data point xn in turn, we update the nearest prototype   k using

  new

k =   old

k +   n(xn       old
k )

(9.5)

where   n is the learning rate parameter, which is typically made to decrease mono-
tonically as more data points are considered.

the id116 algorithm is based on the use of squared euclidean distance as the
measure of dissimilarity between a data point and a prototype vector. not only does
this limit the type of data variables that can be considered (it would be inappropriate
for cases where some or all of the variables represent categorical labels for instance),

428

9. mixture models and em

section 2.3.7

but it can also make the determination of the cluster means nonrobust to outliers. we
can generalize the id116 algorithm by introducing a more general dissimilarity
measure v(x, x(cid:4)) between two vectors x and x(cid:4)
and then minimizing the following
distortion measure

n(cid:2)

k(cid:2)

(cid:4)j =

rnkv(xn,   k)

(9.6)

n=1

k=1

which gives the k-medoids algorithm. the e step again involves, for given cluster
prototypes   k, assigning each data point to the cluster for which the dissimilarity to
the corresponding prototype is smallest. the computational cost of this is o(kn),
as is the case for the standard id116 algorithm. for a general choice of dissimi-
larity measure, the m step is potentially more complex than for id116, and so it
is common to restrict each cluster prototype to be equal to one of the data vectors as-
signed to that cluster, as this allows the algorithm to be implemented for any choice
of dissimilarity measure v(  ,  ) so long as it can be readily evaluated. thus the m
step involves, for each cluster k, a discrete search over the nk points assigned to that
cluster, which requires o(n 2

k) evaluations of v(  ,  ).

one notable feature of the id116 algorithm is that at each iteration, every
data point is assigned uniquely to one, and only one, of the clusters. whereas some
data points will be much closer to a particular centre   k than to any other centre,
there may be other data points that lie roughly midway between cluster centres. in
the latter case, it is not clear that the hard assignment to the nearest cluster is the
most appropriate. we shall see in the next section that by adopting a probabilistic
approach, we obtain    soft    assignments of data points to clusters in a way that re   ects
the level of uncertainty over the most appropriate assignment. this probabilistic
formulation brings with it numerous bene   ts.

9.1.1 image segmentation and compression
as an illustration of the application of the id116 algorithm, we consider
the related problems of image segmentation and image compression. the goal of
segmentation is to partition an image into regions each of which has a reasonably
homogeneous visual appearance or which corresponds to objects or parts of objects
(forsyth and ponce, 2003). each pixel in an image is a point in a 3-dimensional space
comprising the intensities of the red, blue, and green channels, and our segmentation
algorithm simply treats each pixel in the image as a separate data point. note that
strictly this space is not euclidean because the channel intensities are bounded by
the interval [0, 1]. nevertheless, we can apply the id116 algorithm without dif   -
culty. we illustrate the result of running id116 to convergence, for any particular
value of k, by re-drawing the image replacing each pixel vector with the {r, g, b}
intensity triplet given by the centre   k to which that pixel has been assigned. results
for various values of k are shown in figure 9.3. we see that for a given value of k,
the algorithm is representing the image using a palette of only k colours. it should
be emphasized that this use of id116 is not a particularly sophisticated approach
to image segmentation, not least because it takes no account of the spatial proximity
of different pixels. the image segmentation problem is in general extremely dif   cult

k = 2

k = 3

k = 10

original image

9.1. id116 id91

429

figure 9.3 two examples of the application of the id116 id91 algorithm to image segmentation show-
ing the initial images together with their id116 segmentations obtained using various values of k. this
also illustrates of the use of vector quantization for data compression, in which smaller values of k give higher
compression at the expense of poorer image quality.

and remains the subject of active research and is introduced here simply to illustrate
the behaviour of the id116 algorithm.

we can also use the result of a id91 algorithm to perform data compres-
sion.
it is important to distinguish between lossless data compression, in which
the goal is to be able to reconstruct the original data exactly from the compressed
representation, and lossy data compression, in which we accept some errors in the
reconstruction in return for higher levels of compression than can be achieved in the
lossless case. we can apply the id116 algorithm to the problem of lossy data
compression as follows. for each of the n data points, we store only the identity
k of the cluster to which it is assigned. we also store the values of the k clus-
ter centres   k, which typically requires signi   cantly less data, provided we choose
k (cid:13) n. each data point is then approximated by its nearest centre   k. new data
points can similarly be compressed by    rst    nding the nearest   k and then storing
the label k instead of the original data vector. this framework is often called vector
quantization, and the vectors   k are called code-book vectors.

430

9. mixture models and em

the image segmentation problem discussed above also provides an illustration
of the use of id91 for data compression. suppose the original image has n
pixels comprising {r, g, b} values each of which is stored with 8 bits of precision.
then to transmit the whole image directly would cost 24n bits. now suppose we
   rst run id116 on the image data, and then instead of transmitting the original
pixel intensity vectors we transmit the identity of the nearest vector   k. because
there are k such vectors, this requires log2 k bits per pixel. we must also transmit
the k code book vectors   k, which requires 24k bits, and so the total number of
bits required to transmit the image is 24k + n log2 k (rounding up to the nearest
integer). the original image shown in figure 9.3 has 240    180 = 43, 200 pixels
and so requires 24    43, 200 = 1, 036, 800 bits to transmit directly. by comparison,
the compressed images require 43, 248 bits (k = 2), 86, 472 bits (k = 3), and
173, 040 bits (k = 10), respectively, to transmit. these represent compression ratios
compared to the original image of 4.2%, 8.3%, and 16.7%, respectively. we see that
there is a trade-off between degree of compression and image quality. note that our
aim in this example is to illustrate the id116 algorithm. if we had been aiming to
produce a good image compressor, then it would be more fruitful to consider small
blocks of adjacent pixels, for instance 5   5, and thereby exploit the correlations that
exist in natural images between nearby pixels.

9.2. mixtures of gaussians

in section 2.3.9 we motivated the gaussian mixture model as a simple linear super-
position of gaussian components, aimed at providing a richer class of density mod-
els than the single gaussian. we now turn to a formulation of gaussian mixtures in
terms of discrete latent variables. this will provide us with a deeper insight into this
important distribution, and will also serve to motivate the expectation-maximization
algorithm.

recall from (2.188) that the gaussian mixture distribution can be written as a

linear superposition of gaussians in the form

k(cid:2)

k=1

p(x) =

  kn (x|  k,   k).

(9.7)

let us introduce a k-dimensional binary random variable z having a 1-of-k repre-
sentation in which a particular element zk is equal to 1 and all other elements are
equal to 0. the values of zk therefore satisfy zk     {0, 1} and
k zk = 1, and we
see that there are k possible states for the vector z according to which element is
nonzero. we shall de   ne the joint distribution p(x, z) in terms of a marginal dis-
tribution p(z) and a conditional distribution p(x|z), corresponding to the graphical
model in figure 9.4. the marginal distribution over z is speci   ed in terms of the
mixing coef   cients   k, such that

(cid:5)

p(zk = 1) =   k

9.2. mixtures of gaussians

431

figure 9.4 graphical representation of a mixture model,

in which
the joint distribution is expressed in the form p(x, z) =
p(z)p(x|z).

z

x

where the parameters {  k} must satisfy
k(cid:2)

together with

0 (cid:1)   k (cid:1) 1

  k = 1

k=1

(9.8)

(9.9)

in order to be valid probabilities. because z uses a 1-of-k representation, we can
also write this distribution in the form

p(z) =

  zk
k .

(9.10)

similarly, the conditional distribution of x given a particular value for z is a gaussian

k=1

p(x|zk = 1) = n (x|  k,   k)

which can also be written in the form

p(x|z) =

n (x|  k,   k)zk .

(9.11)

k(cid:14)

k(cid:14)

k=1

exercise 9.3

the joint distribution is given by p(z)p(x|z), and the marginal distribution of x is
then obtained by summing the joint distribution over all possible states of z to give

(cid:2)

k(cid:2)

p(x) =

p(z)p(x|z) =

  kn (x|  k,   k)

(9.12)

z

k=1

(cid:5)

where we have made use of (9.10) and (9.11). thus the marginal distribution of x is
a gaussian mixture of the form (9.7). if we have several observations x1, . . . , xn ,
then, because we have represented the marginal distribution in the form p(x) =
z p(x, z), it follows that for every observed data point xn there is a corresponding

latent variable zn.

we have therefore found an equivalent formulation of the gaussian mixture in-
volving an explicit latent variable.
it might seem that we have not gained much
by doing so. however, we are now able to work with the joint distribution p(x, z)

432

9. mixture models and em

instead of the marginal distribution p(x), and this will lead to signi   cant simpli   ca-
tions, most notably through the introduction of the expectation-maximization (em)
algorithm.
another quantity that will play an important role is the id155
of z given x. we shall use   (zk) to denote p(zk = 1|x), whose value can be found
using bayes    theorem

  (zk)     p(zk = 1|x) =

=

k(cid:2)
p(zk = 1)p(x|zk = 1)
p(zj = 1)p(x|zj = 1)
k(cid:2)
  kn (x|  k,   k)
  jn (x|  j,   j)

j=1

.

(9.13)

section 8.1.2

j=1

we shall view   k as the prior id203 of zk = 1, and the quantity   (zk) as the
corresponding posterior id203 once we have observed x. as we shall see later,
  (zk) can also be viewed as the responsibility that component k takes for    explain-
ing    the observation x.

we can use the technique of ancestral sampling to generate random samples
distributed according to the gaussian mixture model. to do this, we    rst generate a

value for z, which we denote(cid:1)z, from the marginal distribution p(z) and then generate
a value for x from the conditional distribution p(x|(cid:1)z). techniques for sampling from

standard distributions are discussed in chapter 11. we can depict samples from the
joint distribution p(x, z) by plotting points at the corresponding values of x and
then colouring them according to the value of z, in other words according to which
gaussian component was responsible for generating them, as shown in figure 9.5(a).
similarly samples from the marginal distribution p(x) are obtained by taking the
samples from the joint distribution and ignoring the values of z. these are illustrated
in figure 9.5(b) by plotting the x values without any coloured labels.

we can also use this synthetic data set to illustrate the    responsibilities    by eval-
uating, for every data point, the posterior id203 for each component in the
mixture distribution from which this data set was generated. in particular, we can
represent the value of the responsibilities   (znk) associated with data point xn by
plotting the corresponding point using proportions of red, blue, and green ink given
by   (znk) for k = 1, 2, 3, respectively, as shown in figure 9.5(c). so, for instance,
a data point for which   (zn1) = 1 will be coloured red, whereas one for which
  (zn2) =   (zn3) = 0.5 will be coloured with equal proportions of blue and green
ink and so will appear cyan. this should be compared with figure 9.5(a) in which
the data points were labelled using the true identity of the component from which
they were generated.

9.2.1 maximum likelihood
suppose we have a data set of observations {x1, . . . , xn}, and we wish to model
this data using a mixture of gaussians. we can represent this data set as an n    d

9.2. mixtures of gaussians

433

1

(a)

1

(b)

1

(c)

0.5

0

0.5

0

0.5

0

0

0.5

1

0

0.5

1

0

0.5

1

figure 9.5 example of 500 points drawn from the mixture of 3 gaussians shown in figure 2.23. (a) samples
from the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components of the
mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution
p(x), which is obtained by simply ignoring the values of z and just plotting the x values. the data set in (a) is
said to be complete, whereas that in (b) is incomplete. (c) the same samples in which the colours represent the
value of the responsibilities   (znk) associated with data point xn, obtained by plotting the corresponding point
using proportions of red, blue, and green ink given by   (znk) for k = 1, 2, 3, respectively

matrix x in which the nth row is given by xt
n. similarly, the corresponding latent
variables will be denoted by an n    k matrix z with rows zt
n. if we assume that
the data points are drawn independently from the distribution, then we can express
the gaussian mixture model for this i.i.d. data set using the graphical representation
shown in figure 9.6. from (9.7) the log of the likelihood function is given by

n(cid:2)

(cid:24)

k(cid:2)

(cid:25)
  kn (xn|  k,   k)

ln p(x|  ,   ,   ) =

ln

.

(9.14)

n=1

k=1

before discussing how to maximize this function, it is worth emphasizing that
there is a signi   cant problem associated with the maximum likelihood framework
applied to gaussian mixture models, due to the presence of singularities. for sim-
plicity, consider a gaussian mixture whose components have covariance matrices
given by   k =   2
ki, where i is the unit matrix, although the conclusions will hold
for general covariance matrices. suppose that one of the components of the mixture
model, let us say the jth component, has its mean   j exactly equal to one of the data

figure 9.6 graphical representation of a gaussian mixture model
for a set of n i.i.d. data points {xn}, with corresponding
latent points {zn}, where n = 1, . . . , n.

zn

xn

  

  

  

n

434

9. mixture models and em

figure 9.7 illustration of how singularities in the
likelihood function arise with mixtures
of gaussians. this should be com-
pared with the case of a single gaus-
sian shown in figure 1.14 for which no
singularities arise.

p(x)

points so that   j = xn for some value of n. this data point will then contribute a
term in the likelihood function of the form

x

1
  j

n (xn|xn,   2

j i) =

1

.

(2  )1/2

(9.15)
if we consider the limit   j     0, then we see that this term goes to in   nity and
so the log likelihood function will also go to in   nity. thus the maximization of
the log likelihood function is not a well posed problem because such singularities
will always be present and will occur whenever one of the gaussian components
   collapses    onto a speci   c data point. recall that this problem did not arise in the
case of a single gaussian distribution. to understand the difference, note that if a
single gaussian collapses onto a data point it will contribute multiplicative factors
to the likelihood function arising from the other data points and these factors will go
to zero exponentially fast, giving an overall likelihood that goes to zero rather than
in   nity. however, once we have (at least) two components in the mixture, one of
the components can have a    nite variance and therefore assign    nite id203 to
all of the data points while the other component can shrink onto one speci   c data
point and thereby contribute an ever increasing additive value to the log likelihood.
this is illustrated in figure 9.7. these singularities provide another example of the
severe over-   tting that can occur in a maximum likelihood approach. we shall see
that this dif   culty does not occur if we adopt a bayesian approach. for the moment,
however, we simply note that in applying maximum likelihood to gaussian mixture
models we must take steps to avoid    nding such pathological solutions and instead
seek local maxima of the likelihood function that are well behaved. we can hope to
avoid the singularities by using suitable heuristics, for instance by detecting when a
gaussian component is collapsing and resetting its mean to a randomly chosen value
while also resetting its covariance to some large value, and then continuing with the
optimization.

a further issue in    nding maximum likelihood solutions arises from the fact
that for any given maximum likelihood solution, a k-component mixture will have
a total of k! equivalent solutions corresponding to the k! ways of assigning k
sets of parameters to k components. in other words, for any given (nondegenerate)
point in the space of parameter values there will be a further k!   1 additional points
all of which give rise to exactly the same distribution. this problem is known as

section 10.1

9.2. mixtures of gaussians

435

identi   ability (casella and berger, 2002) and is an important issue when we wish to
interpret the parameter values discovered by a model. identi   ability will also arise
when we discuss models having continuous latent variables in chapter 12. however,
for the purposes of    nding a good density model, it is irrelevant because any of the
equivalent solutions is as good as any other.

maximizing the log likelihood function (9.14) for a gaussian mixture model
turns out to be a more complex problem than for the case of a single gaussian. the
dif   culty arises from the presence of the summation over k that appears inside the
logarithm in (9.14), so that the logarithm function no longer acts directly on the
gaussian. if we set the derivatives of the log likelihood to zero, we will no longer
obtain a closed form solution, as we shall see shortly.

one approach is to apply gradient-based optimization techniques (fletcher, 1987;
nocedal and wright, 1999; bishop and nabney, 2008). although gradient-based
techniques are feasible, and indeed will play an important role when we discuss
mixture density networks in chapter 5, we now consider an alternative approach
known as the em algorithm which has broad applicability and which will lay the
foundations for a discussion of variational id136 techniques in chapter 10.

9.2.2 em for gaussian mixtures
an elegant and powerful method for    nding maximum likelihood solutions for
models with latent variables is called the expectation-maximization algorithm, or em
algorithm (dempster et al., 1977; mclachlan and krishnan, 1997). later we shall
give a general treatment of em, and we shall also show how em can be generalized
to obtain the variational id136 framework. initially, we shall motivate the em
algorithm by giving a relatively informal treatment in the context of the gaussian
mixture model. we emphasize, however, that em has broad applicability, and indeed
it will be encountered in the context of a variety of different models in this book.
let us begin by writing down the conditions that must be satis   ed at a maximum
of the likelihood function. setting the derivatives of ln p(x|  ,   ,   ) in (9.14) with
respect to the means   k of the gaussian components to zero, we obtain

  k(xn       k)

(9.16)

0 =     n(cid:2)

n=1

(cid:5)
  kn (xn|  k,   k)
(
+
j   jn (xn|  j,   j)

)*

  (znk)

where we have made use of the form (2.43) for the gaussian distribution. note that
the posterior probabilities, or responsibilities, given by (9.13) appear naturally on
the right-hand side. multiplying by   
(which we assume to be nonsingular) and
rearranging we obtain

   1
k

  k =

1
nk

  (znk)xn

where we have de   ned

nk =

  (znk).

(9.17)

(9.18)

n(cid:2)
n(cid:2)

n=1

n=1

section 10.1

436

9. mixture models and em

section 2.3.4

appendix e

we can interpret nk as the effective number of points assigned to cluster k. note
carefully the form of this solution. we see that the mean   k for the kth gaussian
component is obtained by taking a weighted mean of all of the points in the data set,
in which the weighting factor for data point xn is given by the posterior id203
  (znk) that component k was responsible for generating xn.
if we set the derivative of ln p(x|  ,   ,   ) with respect to   k to zero, and follow
a similar line of reasoning, making use of the result for the maximum likelihood
solution for the covariance matrix of a single gaussian, we obtain

  (znk)(xn       k)(xn       k)t

(9.19)

n(cid:2)

n=1

  k =

1
nk

which has the same form as the corresponding result for a single gaussian    tted to
the data set, but again with each data point weighted by the corresponding poste-
rior id203 and with the denominator given by the effective number of points
associated with the corresponding component.
finally, we maximize ln p(x|  ,   ,   ) with respect to the mixing coef   cients
  k. here we must take account of the constraint (9.9), which requires the mixing
coef   cients to sum to one. this can be achieved using a lagrange multiplier and
maximizing the following quantity

(cid:22)
k(cid:2)

(cid:23)

ln p(x|  ,   ,   ) +   

  k     1

(9.20)

which gives

n(cid:2)

n=1

k=1

(cid:5)
n (xn|  k,   k)
j   jn (xn|  j,   j)

0 =

+   

(9.21)

where again we see the appearance of the responsibilities. if we now multiply both
sides by   k and sum over k making use of the constraint (9.9), we    nd    =    n.
using this to eliminate    and rearranging we obtain

  k = nk
n

(9.22)

so that the mixing coef   cient for the kth component is given by the average respon-
sibility which that component takes for explaining the data points.

it is worth emphasizing that the results (9.17), (9.19), and (9.22) do not con-
stitute a closed-form solution for the parameters of the mixture model because the
responsibilities   (znk) depend on those parameters in a complex way through (9.13).
however, these results do suggest a simple iterative scheme for    nding a solution to
the maximum likelihood problem, which as we shall see turns out to be an instance
of the em algorithm for the particular case of the gaussian mixture model. we
   rst choose some initial values for the means, covariances, and mixing coef   cients.
then we alternate between the following two updates that we shall call the e step

9.2. mixtures of gaussians

437

2

0

   2

l = 1

2

0

   2

   2

0

(a)

2

   2

0

(b)

2

   2

0

(c)

2

l = 2

l = 5

2

0

   2

l = 20

2

0

   2

2

0

   2

2

0

   2

   2

0

(d)

2

   2

0

(e)

2

   2

0

(f)

2

figure 9.8 illustration of the em algorithm using the old faithful set as used for the illustration of the id116
algorithm in figure 9.1. see the text for details.

section 9.4

and the m step, for reasons that will become apparent shortly. in the expectation
step, or e step, we use the current values for the parameters to evaluate the posterior
probabilities, or responsibilities, given by (9.13). we then use these probabilities in
the maximization step, or m step, to re-estimate the means, covariances, and mix-
ing coef   cients using the results (9.17), (9.19), and (9.22). note that in so doing
we    rst evaluate the new means using (9.17) and then use these new values to    nd
the covariances using (9.19), in keeping with the corresponding result for a single
gaussian distribution. we shall show that each update to the parameters resulting
from an e step followed by an m step is guaranteed to increase the log likelihood
function. in practice, the algorithm is deemed to have converged when the change
in the log likelihood function, or alternatively in the parameters, falls below some
threshold. we illustrate the em algorithm for a mixture of two gaussians applied to
the rescaled old faithful data set in figure 9.8. here a mixture of two gaussians
is used, with centres initialized using the same values as for the id116 algorithm
in figure 9.1, and with precision matrices initialized to be proportional to the unit
matrix. plot (a) shows the data points in green, together with the initial con   gura-
tion of the mixture model in which the one standard-deviation contours for the two

438

9. mixture models and em

gaussian components are shown as blue and red circles. plot (b) shows the result
of the initial e step, in which each data point is depicted using a proportion of blue
ink equal to the posterior id203 of having been generated from the blue com-
ponent, and a corresponding proportion of red ink given by the posterior id203
of having been generated by the red component. thus, points that have a signi   cant
id203 for belonging to either cluster appear purple. the situation after the    rst
m step is shown in plot (c), in which the mean of the blue gaussian has moved to
the mean of the data set, weighted by the probabilities of each data point belonging
to the blue cluster, in other words it has moved to the centre of mass of the blue ink.
similarly, the covariance of the blue gaussian is set equal to the covariance of the
blue ink. analogous results hold for the red component. plots (d), (e), and (f) show
the results after 2, 5, and 20 complete cycles of em, respectively. in plot (f) the
algorithm is close to convergence.

note that the em algorithm takes many more iterations to reach (approximate)
convergence compared with the id116 algorithm, and that each cycle requires
signi   cantly more computation. it is therefore common to run the id116 algo-
rithm in order to    nd a suitable initialization for a gaussian mixture model that is
subsequently adapted using em. the covariance matrices can conveniently be ini-
tialized to the sample covariances of the clusters found by the id116 algorithm,
and the mixing coef   cients can be set to the fractions of data points assigned to the
respective clusters. as with gradient-based approaches for maximizing the log like-
lihood, techniques must be employed to avoid singularities of the likelihood function
in which a gaussian component collapses onto a particular data point. it should be
emphasized that there will generally be multiple local maxima of the log likelihood
function, and that em is not guaranteed to    nd the largest of these maxima. because
the em algorithm for gaussian mixtures plays such an important role, we summarize
it below.

em for gaussian mixtures

given a gaussian mixture model, the goal is to maximize the likelihood function
with respect to the parameters (comprising the means and covariances of the
components and the mixing coef   cients).

1. initialize the means   k, covariances   k and mixing coef   cients   k, and

evaluate the initial value of the log likelihood.

2. e step. evaluate the responsibilities using the current parameter values

  (znk) =   kn (xn|  k,   k)
  jn (xn|  j,   j)

k(cid:2)

j=1

.

(9.23)

9.3. an alternative view of em

439

3. m step. re-estimate the parameters using the current responsibilities

n(cid:2)
n(cid:2)

n=1

n=1

  new

k

=

1
nk

  new

k

  new
k

=

1
nk
= nk
n

  (znk)xn

  (znk) (xn       new

k

) (xn       new

k

)t

where

4. evaluate the log likelihood

ln p(x|  ,   ,   ) =

nk =

n(cid:2)

ln

n(cid:2)
(cid:24)

n=1

k(cid:2)

  (znk).

(cid:25)
  kn (xn|  k,   k)

(9.24)

(9.25)

(9.26)

(9.27)

(9.28)

n=1

k=1

and check for convergence of either the parameters or the log likelihood. if
the convergence criterion is not satis   ed return to step 2.

9.3. an alternative view of em

in this section, we present a complementary view of the em algorithm that recog-
nizes the key role played by latent variables. we discuss this approach    rst of all
in an abstract setting, and then for illustration we consider once again the case of
gaussian mixtures.

the goal of the em algorithm is to    nd maximum likelihood solutions for mod-
els having latent variables. we denote the set of all observed data by x, in which the
nth row represents xt
n, and similarly we denote the set of all latent variables by z,
with a corresponding row zt
n. the set of all model parameters is denoted by   , and
so the log likelihood function is given by

(cid:24)(cid:2)

(cid:25)

ln p(x|  ) = ln

p(x, z|  )

.

(9.29)

z

note that our discussion will apply equally well to continuous latent variables simply
by replacing the sum over z with an integral.
a key observation is that the summation over the latent variables appears inside
the logarithm. even if the joint distribution p(x, z|  ) belongs to the exponential

440

9. mixture models and em

family, the marginal distribution p(x|  ) typically does not as a result of this sum-
mation. the presence of the sum prevents the logarithm from acting directly on the
joint distribution, resulting in complicated expressions for the maximum likelihood
solution.
now suppose that, for each observation in x, we were told the corresponding
value of the latent variable z. we shall call {x, z} the complete data set, and we
shall refer to the actual observed data x as incomplete, as illustrated in figure 9.5.
the likelihood function for the complete data set simply takes the form ln p(x, z|  ),
and we shall suppose that maximization of this complete-data log likelihood function
is straightforward.
in practice, however, we are not given the complete data set {x, z}, but only
the incomplete data x. our state of knowledge of the values of the latent variables
in z is given only by the posterior distribution p(z|x,   ). because we cannot use
the complete-data log likelihood, we consider instead its expected value under the
posterior distribution of the latent variable, which corresponds (as we shall see) to the
e step of the em algorithm. in the subsequent m step, we maximize this expectation.
if the current estimate for the parameters is denoted   old, then a pair of successive
e and m steps gives rise to a revised estimate   new. the algorithm is initialized by
choosing some starting value for the parameters   0. the use of the expectation may
seem somewhat arbitrary. however, we shall see the motivation for this choice when
we give a deeper treatment of em in section 9.4.
in the e step, we use the current parameter values   old to    nd the posterior
distribution of the latent variables given by p(z|x,   old). we then use this posterior
distribution to    nd the expectation of the complete-data log likelihood evaluated for
some general parameter value   . this expectation, denoted q(  ,   old), is given by

q(  ,   old) =

p(z|x,   old) ln p(x, z|  ).

(9.30)

(cid:2)

z

in the m step, we determine the revised parameter estimate   new by maximizing this
function

  new = arg max

(9.31)
note that in the de   nition of q(  ,   old), the logarithm acts directly on the joint
distribution p(x, z|  ), and so the corresponding m-step maximization will, by sup-
position, be tractable.

q(  ,   old).

  

section 9.4

the general em algorithm is summarized below. it has the property, as we shall
show later, that each cycle of em will increase the incomplete-data log likelihood
(unless it is already at a local maximum).

the general em algorithm
given a joint distribution p(x, z|  ) over observed variables x and latent vari-
ables z, governed by parameters   , the goal is to maximize the likelihood func-
tion p(x|  ) with respect to   .
1. choose an initial setting for the parameters   old.

9.3. an alternative view of em

441

2. e step evaluate p(z|x,   old).

3. m step evaluate   new given by

q(  ,   old)

  new = arg max

  

(cid:2)

where

q(  ,   old) =

p(z|x,   old) ln p(x, z|  ).

(9.32)

(9.33)

exercise 9.4

z

4. check for convergence of either the log likelihood or the parameter values.

if the convergence criterion is not satis   ed, then let

  old       new

(9.34)

and return to step 2.

the em algorithm can also be used to    nd map (maximum posterior) solutions
for models in which a prior p(  ) is de   ned over the parameters. in this case the e
step remains the same as in the maximum likelihood case, whereas in the m step the
quantity to be maximized is given by q(  ,   old) + ln p(  ). suitable choices for the
prior will remove the singularities of the kind illustrated in figure 9.7.

here we have considered the use of the em algorithm to maximize a likelihood
function when there are discrete latent variables. however, it can also be applied
when the unobserved variables correspond to missing values in the data set. the
distribution of the observed values is obtained by taking the joint distribution of all
the variables and then marginalizing over the missing ones. em can then be used
to maximize the corresponding likelihood function. we shall show an example of
the application of this technique in the context of principal component analysis in
figure 12.11. this will be a valid procedure if the data values are missing at random,
meaning that the mechanism causing values to be missing does not depend on the
unobserved values. in many situations this will not be the case, for instance if a
sensor fails to return a value whenever the quantity it is measuring exceeds some
threshold.

9.3.1 gaussian mixtures revisited
we now consider the application of this latent variable view of em to the spe-
ci   c case of a gaussian mixture model. recall that our goal is to maximize the log
likelihood function (9.14), which is computed using the observed data set x, and we
saw that this was more dif   cult than for the case of a single gaussian distribution
due to the presence of the summation over k that occurs inside the logarithm. sup-
pose then that in addition to the observed data set x, we were also given the values
of the corresponding discrete variables z. recall that figure 9.5(a) shows a    com-
plete    data set (i.e., one that includes labels showing which component generated
each data point) while figure 9.5(b) shows the corresponding    incomplete    data set.
the graphical model for the complete data is shown in figure 9.9.

442

9. mixture models and em

figure 9.9 this shows the same graph as in figure 9.6 except that
we now suppose that the discrete variables zn are ob-
served, as well as the data variables xn.

zn

xn

  

  

  

n

now consider the problem of maximizing the likelihood for the complete data

set {x, z}. from (9.10) and (9.11), this likelihood function takes the form

p(x, z|  ,   ,   ) =

k n (xn|  k,   k)znk
  znk

(9.35)

where znk denotes the kth component of zn. taking the logarithm, we obtain

ln p(x, z|  ,   ,   ) =

znk {ln   k + lnn (xn|  k,   k)} .

(9.36)

n(cid:14)

k(cid:14)

n=1

k=1

n(cid:2)

k(cid:2)

n=1

k=1

comparison with the log likelihood function (9.14) for the incomplete data shows
that the summation over k and the logarithm have been interchanged. the loga-
rithm now acts directly on the gaussian distribution, which itself is a member of
the exponential family. not surprisingly, this leads to a much simpler solution to
the maximum likelihood problem, as we now show. consider    rst the maximization
with respect to the means and covariances. because zn is a k-dimensional vec-
tor with all elements equal to 0 except for a single element having the value 1, the
complete-data log likelihood function is simply a sum of k independent contribu-
tions, one for each mixture component. thus the maximization with respect to a
mean or a covariance is exactly as for a single gaussian, except that it involves only
the subset of data points that are    assigned    to that component. for the maximization
with respect to the mixing coef   cients, we note that these are coupled for different
values of k by virtue of the summation constraint (9.9). again, this can be enforced
using a lagrange multiplier as before, and leads to the result

n(cid:2)

  k =

1
n

znk

n=1

(9.37)

so that the mixing coef   cients are equal to the fractions of data points assigned to
the corresponding components.

thus we see that the complete-data log likelihood function can be maximized
trivially in closed form. in practice, however, we do not have values for the latent
variables so, as discussed earlier, we consider the expectation, with respect to the
posterior distribution of the latent variables, of the complete-data log likelihood.

9.3. an alternative view of em

443

using (9.10) and (9.11) together with bayes    theorem, we see that this posterior
distribution takes the form

p(z|x,   ,   ,   )     n(cid:14)

k(cid:14)

[  kn (xn|  k,   k)]znk .

(9.38)

exercise 9.5
section 8.2

exercise 9.8

n(cid:2)

k(cid:2)

n=1

k=1

and hence factorizes over n so that under the posterior distribution the {zn} are
independent. this is easily veri   ed by inspection of the directed graph in figure 9.6
(cid:2)
and making use of the d-separation criterion. the expected value of the indicator
variable znk under this posterior distribution is then given by
(cid:2)
(cid:8)
znk [  kn (xn|  k,   k)]znk
  jn (xn|  j,   j)
k(cid:2)
  kn (xn|  k,   k)
  jn (xn|  j,   j)

(cid:9)znj

e[znk] =

=   (znk)

(9.39)

znk

=

znj

j=1

which is just the responsibility of component k for data point xn. the expected value
of the complete-data log likelihood function is therefore given by

ez[ln p(x, z|  ,   ,   )] =

  (znk){ln   k + lnn (xn|  k,   k)} .

(9.40)

n=1

k=1

we can now proceed as follows. first we choose some initial values for the param-
eters   old,   old and   old, and use these to evaluate the responsibilities (the e step).
we then keep the responsibilities    xed and maximize (9.40) with respect to   k,   k
and   k (the m step). this leads to closed form solutions for   new,   new and   new
given by (9.17), (9.19), and (9.22) as before. this is precisely the em algorithm for
gaussian mixtures as derived earlier. we shall gain more insight into the role of the
expected complete-data log likelihood function when we give a proof of convergence
of the em algorithm in section 9.4.

9.3.2 relation to id116
comparison of the id116 algorithm with the em algorithm for gaussian
mixtures shows that there is a close similarity. whereas the id116 algorithm
performs a hard assignment of data points to clusters, in which each data point is
associated uniquely with one cluster, the em algorithm makes a soft assignment
based on the posterior probabilities. in fact, we can derive the id116 algorithm
as a particular limit of em for gaussian mixtures as follows.

consider a gaussian mixture model in which the covariance matrices of the
mixture components are given by  i, where   is a variance parameter that is shared

444

9. mixture models and em

by all of the components, and i is the identity matrix, so that

p(x|  k,   k) =

1

(2   )1/2 exp

    1
2 

(cid:5)x       k(cid:5)2

(cid:12)

(cid:13)

.

(9.41)

we now consider the em algorithm for a mixture of k gaussians of this form in
which we treat   as a    xed constant, instead of a parameter to be re-estimated. from
(9.13) the posterior probabilities, or responsibilities, for a particular data point xn,
are given by

(cid:26)   (cid:5)xn       j(cid:5)2/2 
(cid:27) .
  (znk) =   k exp{   (cid:5)xn       k(cid:5)2/2 }

(cid:5)

j   j exp

(9.42)
if we consider the limit       0, we see that in the denominator the term for which
(cid:5)xn       j(cid:5)2 is smallest will go to zero most slowly, and hence the responsibilities
  (znk) for the data point xn all go to zero except for term j, for which the responsi-
bility   (znj) will go to unity. note that this holds independently of the values of the
  k so long as none of the   k is zero. thus, in this limit, we obtain a hard assignment
of data points to clusters, just as in the id116 algorithm, so that   (znk)     rnk
where rnk is de   ned by (9.2). each data point is thereby assigned to the cluster
having the closest mean.

the em re-estimation equation for the   k, given by (9.17), then reduces to the
id116 result (9.4). note that the re-estimation formula for the mixing coef   cients
(9.22) simply re-sets the value of   k to be equal to the fraction of data points assigned
to cluster k, although these parameters no longer play an active role in the algorithm.
finally, in the limit       0 the expected complete-data log likelihood, given by

exercise 9.11

(9.40), becomes

ez[ln p(x, z|  ,   ,   )]        1
2

n(cid:2)

k(cid:2)

n=1

k=1

rnk(cid:5)xn       k(cid:5)2 + const.

(9.43)

thus we see that in this limit, maximizing the expected complete-data log likelihood
is equivalent to minimizing the distortion measure j for the id116 algorithm
given by (9.1).

note that the id116 algorithm does not estimate the covariances of the clus-
ters but only the cluster means. a hard-assignment version of the gaussian mixture
model with general covariance matrices, known as the elliptical id116 algorithm,
has been considered by sung and poggio (1994).

9.3.3 mixtures of bernoulli distributions
so far in this chapter, we have focussed on distributions over continuous vari-
ables described by mixtures of gaussians. as a further example of mixture mod-
elling, and to illustrate the em algorithm in a different context, we now discuss mix-
tures of discrete binary variables described by bernoulli distributions. this model
is also known as latent class analysis (lazarsfeld and henry, 1968; mclachlan and
peel, 2000). as well as being of practical importance in its own right, our discus-
sion of bernoulli mixtures will also lay the foundation for a consideration of hidden
markov models over discrete variables.

section 13.2

9.3. an alternative view of em

445

d(cid:14)

consider a set of d binary variables xi, where i = 1, . . . , d, each of which is

governed by a bernoulli distribution with parameter   i, so that

p(x|  ) =

i (1       i)(1   xi)
  xi

(9.44)

i=1

where x = (x1, . . . , xd)t and    = (  1, . . . ,   d)t. we see that the individual
variables xi are independent, given   . the mean and covariance of this distribution
are easily seen to be

e[x] =   
cov[x] = diag{  i(1       i)}.

now let us consider a    nite mixture of these distributions given by

k(cid:2)

k=1

p(x|  ,   ) =

  kp(x|  k)

where    = {  1, . . . ,   k},    = {  1, . . . ,   k}, and

ki(1       ki)(1   xi).
  xi

(cid:27)     e[x]e[x]t

cov[x] =

  k +   k  t
k

  k

k=1

where   k = diag {  ki(1       ki)}. because the covariance matrix cov[x] is no
longer diagonal, the mixture distribution can capture correlations between the vari-
ables, unlike a single bernoulli distribution.
if we are given a data set x = {x1, . . . , xn} then the log likelihood function

for this model is given by

ln p(x|  ,   ) =

n(cid:2)

(cid:24)
k(cid:2)

(cid:25)
  kp(xn|  k)

ln

.

(9.51)

n=1

k=1

again we see the appearance of the summation inside the logarithm, so that the
maximum likelihood solution no longer has closed form.

we now derive the em algorithm for maximizing the likelihood function for
the mixture of bernoulli distributions. to do this, we    rst introduce an explicit latent

d(cid:14)

i=1

p(x|  k) =
k(cid:2)
k(cid:2)

k=1

  k  k

(cid:26)

e[x] =

(9.45)
(9.46)

(9.47)

(9.48)

(9.49)

(9.50)

exercise 9.12

the mean and covariance of this mixture distribution are given by

446

9. mixture models and em

exercise 9.14

k(cid:14)

k=1

p(x|  k)zk
k(cid:14)

  zk
k .

variable z associated with each instance of x. as in the case of the gaussian mixture,
z = (z1, . . . , zk)t is a binary k-dimensional variable having a single component
equal to 1, with all other components equal to 0. we can then write the conditional
distribution of x, given the latent variable, as

p(x|z,   ) =

(9.52)

while the prior distribution for the latent variables is the same as for the mixture of
gaussians model, so that

p(z|  ) =

(9.53)
if we form the product of p(x|z,   ) and p(z|  ) and then marginalize over z, then we
recover (9.47).

k=1

in order to derive the em algorithm, we    rst write down the complete-data log

likelihood function, which is given by
ln p(x, z|  ,   ) =

(cid:24)

znk

ln   k

n(cid:2)

k(cid:2)

n=1

k=1

+

[xni ln   ki + (1     xni) ln(1       ki)]

(9.54)
where x = {xn} and z = {zn}. next we take the expectation of the complete-data
log likelihood with respect to the posterior distribution of the latent variables to give

i=1

(cid:24)

ez[ln p(x, z|  ,   )] =

  (znk)

ln   k

n(cid:2)

k(cid:2)

n=1

k=1

+

[xni ln   ki + (1     xni) ln(1       ki)]

(9.55)

(cid:25)

(cid:25)

d(cid:2)

d(cid:2)

i=1

where   (znk) = e[znk] is the posterior id203, or responsibility, of component
k given data point xn. in the e step, these responsibilities are evaluated using bayes   
theorem, which takes the form

  (znk) = e[znk] =

=

znk

(cid:9)znj

(cid:2)
(cid:2)
(cid:8)
znk [  kp(xn|  k)]znk
  jp(xn|  j)
k(cid:2)
  kp(xn|  k)
  jp(xn|  j)

znj

.

j=1

(9.56)

9.3. an alternative view of em

447

if we consider the sum over n in (9.55), we see that the responsibilities enter

only through two terms, which can be written as

n(cid:2)

n=1

1
nk

nk =

xk =

  (znk)

n(cid:2)

  (znk)xn

n=1

(9.57)

(9.58)

where nk is the effective number of data points associated with component k. in the
m step, we maximize the expected complete-data log likelihood with respect to the
parameters   k and   . if we set the derivative of (9.55) with respect to   k equal to
zero and rearrange the terms, we obtain

exercise 9.15

exercise 9.16

exercise 9.17

section 9.4

  k = xk.

(9.59)

we see that this sets the mean of component k equal to a weighted mean of the
data, with weighting coef   cients given by the responsibilities that component k takes
for data points. for the maximization with respect to   k, we need to introduce a
k   k = 1. following analogous
lagrange multiplier to enforce the constraint
steps to those used for the mixture of gaussians, we then obtain

(cid:5)

  k = nk
n

(9.60)

which represents the intuitively reasonable result that the mixing coef   cient for com-
ponent k is given by the effective fraction of points in the data set explained by that
component.

note that in contrast to the mixture of gaussians, there are no singularities in
which the likelihood function goes to in   nity. this can be seen by noting that the
likelihood function is bounded above because 0 (cid:1) p(xn|  k) (cid:1) 1. there exist
singularities at which the likelihood function goes to zero, but these will not be
found by em provided it is not initialized to a pathological starting point, because
the em algorithm always increases the value of the likelihood function, until a local
maximum is found. we illustrate the bernoulli mixture model in figure 9.10 by
using it to model handwritten digits. here the digit images have been turned into
binary vectors by setting all elements whose values exceed 0.5 to 1 and setting the
remaining elements to 0. we now    t a data set of n = 600 such digits, comprising
the digits    2   ,    3   , and    4   , with a mixture of k = 3 bernoulli distributions by
running 10 iterations of the em algorithm. the mixing coef   cients were initialized
to   k = 1/k, and the parameters   kj were set to random values chosen uniformly in
j   kj = 1.
the range (0.25, 0.75) and then normalized to satisfy the constraint that
we see that a mixture of 3 bernoulli distributions is able to    nd the three clusters in
the data set corresponding to the different digits.

(cid:5)

the conjugate prior for the parameters of a bernoulli distribution is given by
the beta distribution, and we have seen that a beta prior is equivalent to introducing

448

9. mixture models and em

figure 9.10 illustration of the bernoulli mixture model in which the top row shows examples from the digits data
set after converting the pixel values from grey scale to binary using a threshold of 0.5. on the bottom row the    rst
three images show the parameters   ki for each of the three components in the mixture model. as a comparison,
we also    t the same data set using a single multivariate bernoulli distribution, again using maximum likelihood.
this amounts to simply averaging the counts in each pixel and is shown by the right-most image on the bottom
row.

section 2.1.1

exercise 9.18

exercise 9.19

additional effective observations of x. we can similarly introduce priors into the
bernoulli mixture model, and use em to maximize the posterior id203 distri-
butions.

it is straightforward to extend the analysis of bernoulli mixtures to the case of
multinomial binary variables having m > 2 states by making use of the discrete dis-
tribution (2.26). again, we can introduce dirichlet priors over the model parameters
if desired.

9.3.4 em for bayesian id75
as a third example of the application of em, we return to the evidence ap-
proximation for bayesian id75. in section 3.5.2, we obtained the re-
estimation equations for the hyperparameters    and    by evaluation of the evidence
and then setting the derivatives of the resulting expression to zero. we now turn to
an alternative approach for    nding    and    based on the em algorithm. recall that
our goal is to maximize the evidence function p(t|  ,   ) given by (3.77) with respect
to    and   . because the parameter vector w is marginalized out, we can regard it as
a latent variable, and hence we can optimize this marginal likelihood function using
em. in the e step, we compute the posterior distribution of w given the current set-
ting of the parameters    and    and then use this to    nd the expected complete-data
log likelihood. in the m step, we maximize this quantity with respect to    and   . we
have already derived the posterior distribution of w because this is given by (3.49).
the complete-data log likelihood function is then given by

ln p(t, w|  ,   ) = ln p(t|w,   ) + ln p(w|  )

(9.61)

9.3. an alternative view of em

449

where the likelihood p(t|w,   ) and the prior p(w|  ) are given by (3.10) and (3.52),
respectively, and y(x, w) is given by (3.3). taking the expectation with respect to
the posterior distribution of w then gives

e [ln p(t, w|  ,   )] = m
2
      
2

(cid:15)

(cid:16)

+ n
2

ln

  
2  

(cid:8)

(cid:18)
(cid:8)
(tn     wt  n)2

      
2 e

wtw

(cid:9)
(cid:9)

(cid:17)
n(cid:2)

ln

  
2  

e

n=1

.

(9.62)

exercise 9.20

setting the derivatives with respect to    to zero, we obtain the m step re-estimation
equation

   = m

e [wtw]

=

m

n mn + tr(sn ) .

mt

(9.63)

exercise 9.21

an analogous result holds for   .

note that this re-estimation equation takes a slightly different form from the
corresponding result (3.92) derived by direct evaluation of the evidence function.
however, they each involve computation and inversion (or eigen decomposition) of
an m    m matrix and hence will have comparable computational cost per iteration.
these two approaches to determining    should of course converge to the same
result (assuming they    nd the same local maximum of the evidence function). this
can be veri   ed by    rst noting that the quantity    is de   ned by

   = m       

1

= m       tr(sn ).

(9.64)

m(cid:2)

  i +   

i=1

at a stationary point of the evidence function, the re-estimation equation (3.92) will
be self-consistently satis   ed, and hence we can substitute for    to give

n mn =    = m       tr(sn )

  mt

(9.65)

and solving for    we obtain (9.63), which is precisely the em re-estimation equation.
as a    nal example, we consider a closely related model, namely the relevance
vector machine for regression discussed in section 7.2.1. there we used direct max-
imization of the marginal likelihood to derive re-estimation equations for the hyper-
parameters    and   . here we consider an alternative approach in which we view the
weight vector w as a latent variable and apply the em algorithm. the e step involves
   nding the posterior distribution over the weights, and this is given by (7.81). in the
m step we maximize the expected complete-data log likelihood, which is de   ned by

ew [ln p(t|x, w,   )p(w|  )]

(9.66)

exercise 9.22

where the expectation is taken with respect to the posterior distribution computed
using the    old    parameter values. to compute the new parameter values we maximize
with respect to    and    to give

450

9. mixture models and em

  new

i

=

(  new)   1 =

1

i +   ii
m2
(cid:5)t       mn(cid:5)2 +   

   1

n

(cid:5)

i   i

(9.67)

(9.68)

exercise 9.23

these re-estimation equations are formally equivalent to those obtained by direct
maxmization.

9.4. the em algorithm in general

the expectation maximization algorithm, or em algorithm, is a general technique for
   nding maximum likelihood solutions for probabilistic models having latent vari-
ables (dempster et al., 1977; mclachlan and krishnan, 1997). here we give a very
general treatment of the em algorithm and in the process provide a proof that the
em algorithm derived heuristically in sections 9.2 and 9.3 for gaussian mixtures
does indeed maximize the likelihood function (csisz`ar and tusn`ady, 1984; hath-
away, 1986; neal and hinton, 1999). our discussion will also form the basis for the
derivation of the variational id136 framework.

section 10.1

consider a probabilistic model in which we collectively denote all of the ob-
served variables by x and all of the hidden variables by z. the joint distribution
p(x, z|  ) is governed by a set of parameters denoted   . our goal is to maximize
the likelihood function that is given by
p(x|  ) =

p(x, z|  ).

(cid:2)

(9.69)

z

here we are assuming z is discrete, although the discussion is identical if z com-
prises continuous variables or a combination of discrete and continuous variables,
with summation replaced by integration as appropriate.
we shall suppose that direct optimization of p(x|  ) is dif   cult, but that opti-
mization of the complete-data likelihood function p(x, z|  ) is signi   cantly easier.
next we introduce a distribution q(z) de   ned over the latent variables, and we ob-
serve that, for any choice of q(z), the following decomposition holds

where we have de   ned

l(q,   ) =

q(z) ln

ln p(x|  ) = l(q,   ) + kl(q(cid:5)p)

(cid:2)
(cid:2)

z

z

(cid:12)

(cid:12)

p(x, z|  )

q(z)
p(z|x,   )

q(z)

(cid:13)

(9.70)

(9.71)

(9.72)

(cid:13)

.

kl(q(cid:5)p) =    

q(z) ln

note that l(q,   ) is a functional (see appendix d for a discussion of functionals)
of the distribution q(z), and a function of the parameters   . it is worth studying

9.4. the em algorithm in general

451

figure 9.11 illustration of the decomposition given
by (9.70), which holds for any choice
of distribution q(z).
because the
id181 satis   es
kl(q(cid:3)p) (cid:2) 0, we see that the quan-
tity l(q,   ) is a lower bound on the log
likelihood function ln p(x|  ).

kl(q||p)

l(q,   )

ln p(x|  )

exercise 9.24

section 1.6.1

carefully the forms of the expressions (9.71) and (9.72), and in particular noting that
they differ in sign and also that l(q,   ) contains the joint distribution of x and z
while kl(q(cid:5)p) contains the conditional distribution of z given x. to verify the
decomposition (9.70), we    rst make use of the product rule of id203 to give

ln p(x, z|  ) = ln p(z|x,   ) + ln p(x|  )

(9.73)
which we then substitute into the expression for l(q,   ). this gives rise to two terms,
one of which cancels kl(q(cid:5)p) while the other gives the required log likelihood
ln p(x|  ) after noting that q(z) is a normalized distribution that sums to 1.
from (9.72), we see that kl(q(cid:5)p) is the id181 between
q(z) and the posterior distribution p(z|x,   ). recall that the kullback-leibler di-
vergence satis   es kl(q(cid:5)p) (cid:2) 0, with equality if, and only if, q(z) = p(z|x,   ). it
therefore follows from (9.70) that l(q,   ) (cid:1) ln p(x|  ), in other words that l(q,   )
is a lower bound on ln p(x|  ). the decomposition (9.70) is illustrated in fig-
ure 9.11.

the em algorithm is a two-stage iterative optimization technique for    nding
maximum likelihood solutions. we can use the decomposition (9.70) to de   ne the
em algorithm and to demonstrate that it does indeed maximize the log likelihood.
suppose that the current value of the parameter vector is   old. in the e step, the
lower bound l(q,   old) is maximized with respect to q(z) while holding   old    xed.
the solution to this maximization problem is easily seen by noting that the value
of ln p(x|  old) does not depend on q(z) and so the largest value of l(q,   old) will
occur when the id181 vanishes, in other words when q(z) is
equal to the posterior distribution p(z|x,   old). in this case, the lower bound will
equal the log likelihood, as illustrated in figure 9.12.
in the subsequent m step, the distribution q(z) is held    xed and the lower bound
l(q,   ) is maximized with respect to    to give some new value   new. this will
cause the lower bound l to increase (unless it is already at a maximum), which will
necessarily cause the corresponding log likelihood function to increase. because the
distribution q is determined using the old parameter values rather than the new values
and is held    xed during the m step, it will not equal the new posterior distribution
p(z|x,   new), and hence there will be a nonzero kl divergence. the increase in the
log likelihood function is therefore greater than the increase in the lower bound, as

452

9. mixture models and em

kl(q||p) = 0

figure 9.12 illustration of the e step of
the em algorithm. the q
distribution is set equal to
the posterior distribution for
the current parameter val-
ues   old, causing the lower
bound to move up to the
same value as the log like-
lihood function, with the kl
divergence vanishing.

l(q,   old)

ln p(x|  old)

shown in figure 9.13. if we substitute q(z) = p(z|x,   old) into (9.71), we see that,
after the e step, the lower bound takes the form
l(q,   ) =
p(z|x,   old) ln p(x, z|  )    

p(z|x,   old) ln p(z|x,   old)

(cid:2)

(cid:2)

z

= q(  ,   old) + const

z

(9.74)

where the constant is simply the negative id178 of the q distribution and is there-
fore independent of   . thus in the m step, the quantity that is being maximized is the
expectation of the complete-data log likelihood, as we saw earlier in the case of mix-
tures of gaussians. note that the variable    over which we are optimizing appears
only inside the logarithm. if the joint distribution p(z, x|  ) comprises a member of
the exponential family, or a product of such members, then we see that the logarithm
will cancel the exponential and lead to an m step that will be typically much simpler
than the maximization of the corresponding incomplete-data log likelihood function
p(x|  ).

the operation of the em algorithm can also be viewed in the space of parame-
ters, as illustrated schematically in figure 9.14. here the red curve depicts the (in-

kl(q||p)

figure 9.13 illustration of the m step of the em
the distribution q(z)
algorithm.
is held    xed and the lower bound
l(q,   ) is maximized with respect
to the parameter vector    to give
a revised value   new. because the
kl divergence is nonnegative, this
causes the log likelihood ln p(x|  )
to increase by at least as much as
the lower bound does.

l(q,   new)

ln p(x|  new)

figure 9.14 the em algorithm involves alter-
nately computing a lower bound
on the log likelihood for the cur-
rent parameter values and then
maximizing this bound to obtain
the new parameter values. see
the text for a full discussion.

9.4. the em algorithm in general

453

ln p(x|  )

l (q,   )

  old   new

exercise 9.25

complete data) log likelihood function whose value we wish to maximize. we start
with some initial parameter value   old, and in the    rst e step we evaluate the poste-
rior distribution over latent variables, which gives rise to a lower bound l(  ,   (old))
whose value equals the log likelihood at   (old), as shown by the blue curve. note that
the bound makes a tangential contact with the log likelihood at   (old), so that both
curves have the same gradient. this bound is a convex function having a unique
maximum (for mixture components from the exponential family). in the m step, the
bound is maximized giving the value   (new), which gives a larger value of log likeli-
hood than   (old). the subsequent e step then constructs a bound that is tangential at
  (new) as shown by the green curve.
for the particular case of an independent, identically distributed data set, x
will comprise n data points {xn} while z will comprise n corresponding latent
(cid:21)
variables {zn}, where n = 1, . . . , n. from the independence assumption, we have
n p(xn, zn) and, by marginalizing over the {zn} we have p(x) =
p(x, z) =
n p(xn). using the sum and product rules, we see that the posterior id203

(cid:21)

that is evaluated in the e step takes the form

(cid:2)

p(z|x,   ) = p(x, z|  )
p(x, z|  )

=

z

z

n=1

n(cid:14)
n(cid:14)
(cid:2)

n=1

p(xn, zn|  )

p(xn, zn|  )

n(cid:14)

n=1

=

p(zn|xn,   )

(9.75)

and so the posterior distribution also factorizes with respect to n.
in the case of
the gaussian mixture model this simply says that the responsibility that each of the
mixture components takes for a particular data point xn depends only on the value
of xn and on the parameters    of the mixture components, not on the values of the
other data points.

we have seen that both the e and the m steps of the em algorithm are increas-
ing the value of a well-de   ned bound on the log likelihood function and that the

454

9. mixture models and em

complete em cycle will change the model parameters in such a way as to cause
the log likelihood to increase (unless it is already at a maximum, in which case the
parameters remain unchanged).
we can also use the em algorithm to maximize the posterior distribution p(  |x)
for models in which we have introduced a prior p(  ) over the parameters. to see this,
we note that as a function of   , we have p(  |x) = p(  , x)/p(x) and so

ln p(  |x) = ln p(  , x)     ln p(x).

making use of the decomposition (9.70), we have

ln p(  |x) = l(q,   ) + kl(q(cid:5)p) + ln p(  )     ln p(x)

(cid:2) l(q,   ) + ln p(  )     ln p(x).

(9.76)

(9.77)

where ln p(x) is a constant. we can again optimize the right-hand side alternately
with respect to q and   . the optimization with respect to q gives rise to the same e-
step equations as for the standard em algorithm, because q only appears in l(q,   ).
the m-step equations are modi   ed through the introduction of the prior term ln p(  ),
which typically requires only a small modi   cation to the standard maximum likeli-
hood m-step equations.

the em algorithm breaks down the potentially dif   cult problem of maximizing
the likelihood function into two stages, the e step and the m step, each of which will
often prove simpler to implement. nevertheless, for complex models it may be the
case that either the e step or the m step, or indeed both, remain intractable. this
leads to two possible extensions of the em algorithm, as follows.
the generalized em, or gem, algorithm addresses the problem of an intractable
m step. instead of aiming to maximize l(q,   ) with respect to   , it seeks instead
to change the parameters in such a way as to increase its value. again, because
l(q,   ) is a lower bound on the log likelihood function, each complete em cycle of
the gem algorithm is guaranteed to increase the value of the log likelihood (unless
the parameters already correspond to a local maximum). one way to exploit the
gem approach would be to use one of the nonlinear optimization strategies, such
as the conjugate gradients algorithm, during the m step. another form of gem
algorithm, known as the expectation conditional maximization, or ecm, algorithm,
involves making several constrained optimizations within each m step (meng and
rubin, 1993). for instance, the parameters might be partitioned into groups, and the
m step is broken down into multiple steps each of which involves optimizing one of
the subset with the remainder held    xed.
we can similarly generalize the e step of the em algorithm by performing a
partial, rather than complete, optimization of l(q,   ) with respect to q(z) (neal and
hinton, 1999). as we have seen, for any given value of    there is a unique maximum
of l(q,   ) with respect to q(z) that corresponds to the posterior distribution q  (z) =
p(z|x,   ) and that for this choice of q(z) the bound l(q,   ) is equal to the log
likelihood function ln p(x|  ). it follows that any algorithm that converges to the
global maximum of l(q,   ) will    nd a value of    that is also a global maximum
of the log likelihood ln p(x|  ). provided p(x, z|  ) is a continuous function of   

exercises

455

then, by continuity, any local maximum of l(q,   ) will also be a local maximum of
ln p(x|  ).
consider the case of n independent data points x1, . . . , xn with corresponding
latent variables z1, . . . , zn . the joint distribution p(x, z|  ) factorizes over the data
points, and this structure can be exploited in an incremental form of em in which
at each em cycle only one data point is processed at a time. in the e step, instead
of recomputing the responsibilities for all of the data points, we just re-evaluate the
responsibilities for one data point. it might appear that the subsequent m step would
require computation involving the responsibilities for all of the data points. how-
ever, if the mixture components are members of the exponential family, then the
responsibilities enter only through simple suf   cient statistics, and these can be up-
dated ef   ciently. consider, for instance, the case of a gaussian mixture, and suppose
we perform an update for data point m in which the corresponding old and new
values of the responsibilities are denoted   old(zmk) and   new(zmk). in the m step,
the required suf   cient statistics can be updated incrementally. for instance, for the
means the suf   cient statistics are de   ned by (9.17) and (9.18) from which we obtain

(cid:16)(cid:10)

(cid:11)

  new

k =   old

k +

  new(zmk)       old(zmk)

n new

k

xm       old

k

(cid:15)

(9.78)

(9.79)

together with

n new

k = n old

k +   new(zmk)       old(zmk).

the corresponding results for the covariances and the mixing coef   cients are analo-
gous.

thus both the e step and the m step take    xed time that is independent of the
total number of data points. because the parameters are revised after each data point,
rather than waiting until after the whole data set is processed, this incremental ver-
sion can converge faster than the batch version. each e or m step in this incremental
algorithm is increasing the value of l(q,   ) and, as we have shown above, if the
algorithm converges to a local (or global) maximum of l(q,   ), this will correspond
to a local (or global) maximum of the log likelihood function ln p(x|  ).

9.1 ((cid:12)) www consider the id116 algorithm discussed in section 9.1. show that as
a consequence of there being a    nite number of possible assignments for the set of
discrete indicator variables rnk, and that for each such assignment there is a unique
optimum for the {  k}, the id116 algorithm must converge after a    nite number
of iterations.

9.2 ((cid:12)) apply the robbins-monro sequential estimation procedure described in sec-
tion 2.3.5 to the problem of    nding the roots of the regression function given by
the derivatives of j in (9.1) with respect to   k. show that this leads to a stochastic
id116 algorithm in which, for each data point xn, the nearest prototype   k is
updated using (9.5).

exercise 9.26

exercises

456

9. mixture models and em

9.3 ((cid:12)) www consider a gaussian mixture model in which the marginal distribution
p(z) for the latent variable is given by (9.10), and the conditional distribution p(x|z)
for the observed variable is given by (9.11). show that the marginal distribution
p(x), obtained by summing p(z)p(x|z) over all possible values of z, is a gaussian
mixture of the form (9.7).

9.4 ((cid:12)) suppose we wish to use the em algorithm to maximize the posterior distri-
bution over parameters p(  |x) for a model containing latent variables, where x is
the observed data set. show that the e step remains the same as in the maximum
likelihood case, whereas in the m step the quantity to be maximized is given by
q(  ,   old) + ln p(  ) where q(  ,   old) is de   ned by (9.30).

n(cid:14)

9.5 ((cid:12)) consider the directed graph for a gaussian mixture model shown in figure 9.6.
by making use of the d-separation criterion discussed in section 8.2, show that the
posterior distribution of the latent variables factorizes with respect to the different
data points so that

p(z|x,   ,   ,   ) =

p(zn|xn,   ,   ,   ).

(9.80)

n=1

9.6 ((cid:12) (cid:12)) consider a special case of a gaussian mixture model in which the covari-
ance matrices   k of the components are all constrained to have a common value
  . derive the em equations for maximizing the likelihood function under such a
model.

9.7 ((cid:12)) www verify that maximization of the complete-data log likelihood (9.36) for
a gaussian mixture model leads to the result that the means and covariances of each
component are    tted independently to the corresponding group of data points, and
the mixing coef   cients are given by the fractions of points in each group.

9.8 ((cid:12)) www show that if we maximize (9.40) with respect to   k while keeping the

responsibilities   (znk)    xed, we obtain the closed form solution given by (9.17).

9.9 ((cid:12)) show that if we maximize (9.40) with respect to   k and   k while keeping the
responsibilities   (znk)    xed, we obtain the closed form solutions given by (9.19)
and (9.22).

9.10 ((cid:12) (cid:12)) consider a density model given by a mixture distribution

p(x) =

  kp(x|k)

(9.81)

k=1

and suppose that we partition the vector x into two parts so that x = (xa, xb).
show that the conditional density p(xb|xa) is itself a mixture distribution and    nd
expressions for the mixing coef   cients and for the component densities.

k(cid:2)

exercises

457

9.11 ((cid:12))

in section 9.3.2, we obtained a relationship between id116 and em for
gaussian mixtures by considering a mixture model in which all components have
covariance  i. show that in the limit       0, maximizing the expected complete-
data log likelihood for this model, given by (9.40), is equivalent to minimizing the
distortion measure j for the id116 algorithm given by (9.1).

9.12 ((cid:12)) www consider a mixture distribution of the form

p(x) =

  kp(x|k)

(9.82)

k=1

where the elements of x could be discrete or continuous or a combination of these.
denote the mean and covariance of p(x|k) by   k and   k, respectively. show that
the mean and covariance of the mixture distribution are given by (9.49) and (9.50).

9.13 ((cid:12) (cid:12)) using the re-estimation equations for the em algorithm, show that a mix-
ture of bernoulli distributions, with its parameters set to values corresponding to a
maximum of the likelihood function, has the property that

k(cid:2)

n(cid:2)

e[x] =

1
n

xn     x.

(9.83)

nents have the same mean   k = (cid:1)   for k = 1, . . . , k, then the em algorithm will

hence show that if the parameters of this model are initialized such that all compo-

n=1

converge after one iteration, for any choice of the initial mixing coef   cients, and that
this solution has the property   k = x. note that this represents a degenerate case of
the mixture model in which all of the components are identical, and in practice we
try to avoid such solutions by using an appropriate initialization.

9.14 ((cid:12)) consider the joint distribution of latent and observed variables for the bernoulli
distribution obtained by forming the product of p(x|z,   ) given by (9.52) and p(z|  )
given by (9.53). show that if we marginalize this joint distribution with respect to z,
then we obtain (9.47).

9.15 ((cid:12)) www show that if we maximize the expected complete-data log likelihood
function (9.55) for a mixture of bernoulli distributions with respect to   k, we obtain
the m step equation (9.59).

9.16 ((cid:12)) show that if we maximize the expected complete-data log likelihood function
(9.55) for a mixture of bernoulli distributions with respect to the mixing coef   cients
  k, using a lagrange multiplier to enforce the summation constraint, we obtain the
m step equation (9.60).

9.17 ((cid:12)) www show that as a consequence of the constraint 0 (cid:1) p(xn|  k) (cid:1) 1 for
the discrete variable xn, the incomplete-data log likelihood function for a mixture
of bernoulli distributions is bounded above, and hence that there are no singularities
for which the likelihood goes to in   nity.

458

9. mixture models and em

9.18 ((cid:12) (cid:12)) consider a bernoulli mixture model as discussed in section 9.3.3, together
with a prior distribution p(  k|ak, bk) over each of the parameter vectors   k given
by the beta distribution (2.13), and a dirichlet prior p(  |  ) given by (2.38). derive
the em algorithm for maximizing the posterior id203 p(  ,   |x).
(cid:5)

9.19 ((cid:12) (cid:12)) consider a d-dimensional variable x each of whose components i is itself a
multinomial variable of degree m so that x is a binary vector with components xij
where i = 1, . . . , d and j = 1, . . . , m, subject to the constraint that
j xij = 1 for
all i. suppose that the distribution of these variables is described by a mixture of the
discrete multinomial distributions considered in section 2.2 so that

k=1

k(cid:2)
  kp(x|  k)
d(cid:14)
m(cid:14)
(cid:5)

i=1

xij
kij.

p(x) =

where

(9.84)

p(x|  k) =

  

(9.85)
the parameters   kij represent the probabilities p(xij = 1|  k) and must satisfy
0 (cid:1)   kij (cid:1) 1 together with the constraint
j   kij = 1 for all values of k and i.
given an observed data set {xn}, where n = 1, . . . , n, derive the e and m step
equations of the em algorithm for optimizing the mixing coef   cients   k and the
component parameters   kij of this distribution by maximum likelihood.

j=1

9.20 ((cid:12)) www show that maximization of the expected complete-data log likelihood
function (9.62) for the bayesian id75 model leads to the m step re-
estimation result (9.63) for   .

9.21 ((cid:12) (cid:12)) using the evidence framework of section 3.5, derive the m-step re-estimation
equations for the parameter    in the bayesian id75 model, analogous to
the result (9.63) for   .

9.22 ((cid:12) (cid:12)) by maximization of the expected complete-data log likelihood de   ned by
(9.66), derive the m step equations (9.67) and (9.68) for re-estimating the hyperpa-
rameters of the relevance vector machine for regression.

9.23 ((cid:12) (cid:12)) www in section 7.2.1 we used direct maximization of the marginal like-
lihood to derive the re-estimation equations (7.87) and (7.88) for    nding values of
the hyperparameters    and    for the regression rvm. similarly, in section 9.3.4
we used the em algorithm to maximize the same marginal likelihood, giving the
re-estimation equations (9.67) and (9.68). show that these two sets of re-estimation
equations are formally equivalent.

9.24 ((cid:12)) verify the relation (9.70) in which l(q,   ) and kl(q(cid:5)p) are de   ned by (9.71)

and (9.72), respectively.

exercises

459

9.25 ((cid:12)) www show that the lower bound l(q,   ) given by (9.71), with q(z) =
p(z|x,   (old)), has the same gradient with respect to    as the log likelihood function
ln p(x|  ) at the point    =   (old).

9.26 ((cid:12)) www consider the incremental form of the em algorithm for a mixture of
gaussians, in which the responsibilities are recomputed only for a speci   c data point
xm. starting from the m-step formulae (9.17) and (9.18), derive the results (9.78)
and (9.79) for updating the component means.

9.27 ((cid:12) (cid:12)) derive m-step formulae for updating the covariance matrices and mixing
coef   cients in a gaussian mixture model when the responsibilities are updated in-
crementally, analogous to the result (9.78) for updating the means.

10

approximate

id136

a central task in the application of probabilistic models is the evaluation of the pos-
terior distribution p(z|x) of the latent variables z given the observed (visible) data
variables x, and the evaluation of expectations computed with respect to this dis-
tribution. the model might also contain some deterministic parameters, which we
will leave implicit for the moment, or it may be a fully bayesian model in which any
unknown parameters are given prior distributions and are absorbed into the set of
latent variables denoted by the vector z. for instance, in the em algorithm we need
to evaluate the expectation of the complete-data log likelihood with respect to the
posterior distribution of the latent variables. for many models of practical interest, it
will be infeasible to evaluate the posterior distribution or indeed to compute expec-
tations with respect to this distribution. this could be because the dimensionality of
the latent space is too high to work with directly or because the posterior distribution
has a highly complex form for which expectations are not analytically tractable. in
the case of continuous variables, the required integrations may not have closed-form

461

462

10. approximate id136

analytical solutions, while the dimensionality of the space and the complexity of the
integrand may prohibit numerical integration. for discrete variables, the marginal-
izations involve summing over all possible con   gurations of the hidden variables,
and though this is always possible in principle, we often    nd in practice that there
may be exponentially many hidden states so that exact calculation is prohibitively
expensive.

in such situations, we need to resort to approximation schemes, and these fall
broadly into two classes, according to whether they rely on stochastic or determin-
istic approximations. stochastic techniques such as id115, de-
scribed in chapter 11, have enabled the widespread use of bayesian methods across
many domains. they generally have the property that given in   nite computational
resource, they can generate exact results, and the approximation arises from the use
of a    nite amount of processor time. in practice, sampling methods can be compu-
tationally demanding, often limiting their use to small-scale problems. also, it can
be dif   cult to know whether a sampling scheme is generating independent samples
from the required distribution.

in this chapter, we introduce a range of deterministic approximation schemes,
some of which scale well to large applications. these are based on analytical ap-
proximations to the posterior distribution, for example by assuming that it factorizes
in a particular way or that it has a speci   c parametric form such as a gaussian. as
such, they can never generate exact results, and so their strengths and weaknesses
are complementary to those of sampling methods.

in section 4.4, we discussed the laplace approximation, which is based on a
local gaussian approximation to a mode (i.e., a maximum) of the distribution. here
we turn to a family of approximation techniques called variational id136 or vari-
ational bayes, which use more global criteria and which have been widely applied.
we conclude with a brief introduction to an alternative variational framework known
as expectation propagation.

10.1. variational id136

variational methods have their origins in the 18th century with the work of euler,
lagrange, and others on the calculus of variations. standard calculus is concerned
with    nding derivatives of functions. we can think of a function as a mapping that
takes the value of a variable as the input and returns the value of the function as the
output. the derivative of the function then describes how the output value varies
as we make in   nitesimal changes to the input value. similarly, we can de   ne a
functional as a mapping that takes a function as the input and that returns the value
of the functional as the output. an example would be the id178 h[p], which takes
a id203 distribution p(x) as the input and returns the quantity

(cid:6)

h[p] =

p(x) ln p(x) dx

(10.1)

10.1. variational id136

463

as the output. we can the introduce the concept of a functional derivative, which ex-
presses how the value of the functional changes in response to in   nitesimal changes
to the input function (feynman et al., 1964). the rules for the calculus of variations
mirror those of standard calculus and are discussed in appendix d. many problems
can be expressed in terms of an optimization problem in which the quantity being
optimized is a functional. the solution is obtained by exploring all possible input
functions to    nd the one that maximizes, or minimizes, the functional. variational
methods have broad applicability and include such areas as    nite element methods
(kapur, 1989) and maximum id178 (schwarz, 1988).

although there is nothing intrinsically approximate about variational methods,
they do naturally lend themselves to    nding approximate solutions. this is done
by restricting the range of functions over which the optimization is performed, for
instance by considering only quadratic functions or by considering functions com-
posed of a linear combination of    xed basis functions in which only the coef   cients
of the linear combination can vary. in the case of applications to probabilistic in-
ference, the restriction may for example take the form of factorization assumptions
(jordan et al., 1999; jaakkola, 2001).

now let us consider in more detail how the concept of variational optimization
can be applied to the id136 problem. suppose we have a fully bayesian model in
which all parameters are given prior distributions. the model may also have latent
variables as well as parameters, and we shall denote the set of all latent variables
and parameters by z. similarly, we denote the set of all observed variables by x.
for example, we might have a set of n independent, identically distributed data,
for which x = {x1, . . . , xn} and z = {z1, . . . , zn}. our probabilistic model
speci   es the joint distribution p(x, z), and our goal is to    nd an approximation for
the posterior distribution p(z|x) as well as for the model evidence p(x). as in our
discussion of em, we can decompose the log marginal id203 using

where we have de   ned

l(q) =

ln p(x) = l(q) + kl(q(cid:5)p)

(cid:6)

(cid:6)

q(z) ln

(cid:12)

(cid:13)
(cid:13)

dz

dz.

(cid:12)

p(x, z)

q(z)
p(z|x)
q(z)

kl(q(cid:5)p) =    

q(z) ln

(10.2)

(10.3)

(10.4)

this differs from our discussion of em only in that the parameter vector    no longer
appears, because the parameters are now stochastic variables and are absorbed into
z. since in this chapter we will mainly be interested in continuous variables we have
used integrations rather than summations in formulating this decomposition. how-
ever, the analysis goes through unchanged if some or all of the variables are discrete
simply by replacing the integrations with summations as required. as before, we
can maximize the lower bound l(q) by optimization with respect to the distribution
q(z), which is equivalent to minimizing the kl divergence. if we allow any possible
choice for q(z), then the maximum of the lower bound occurs when the kl diver-
gence vanishes, which occurs when q(z) equals the posterior distribution p(z|x).

464

10. approximate id136

1

0.8

0.6

0.4

0.2

0
   2

   1

0

1

2

3

4

40

30

20

10

0
   2

   1

0

1

2

3

4

figure 10.1 illustration of the variational approximation for the example considered earlier in figure 4.14. the
left-hand plot shows the original distribution (yellow) along with the laplace (red) and variational (green) approx-
imations, and the right-hand plot shows the negative logarithms of the corresponding curves.

however, we shall suppose the model is such that working with the true posterior
distribution is intractable.

we therefore consider instead a restricted family of distributions q(z) and then
seek the member of this family for which the kl divergence is minimized. our goal
is to restrict the family suf   ciently that they comprise only tractable distributions,
while at the same time allowing the family to be suf   ciently rich and    exible that it
can provide a good approximation to the true posterior distribution. it is important to
emphasize that the restriction is imposed purely to achieve tractability, and that sub-
ject to this requirement we should use as rich a family of approximating distributions
as possible. in particular, there is no    over-   tting    associated with highly    exible dis-
tributions. using more    exible approximations simply allows us to approach the true
posterior distribution more closely.
one way to restrict the family of approximating distributions is to use a paramet-
ric distribution q(z|  ) governed by a set of parameters   . the lower bound l(q)
then becomes a function of   , and we can exploit standard nonlinear optimization
techniques to determine the optimal values for the parameters. an example of this
approach, in which the variational distribution is a gaussian and we have optimized
with respect to its mean and variance, is shown in figure 10.1.

10.1.1 factorized distributions
here we consider an alternative way in which to restrict the family of distri-
butions q(z). suppose we partition the elements of z into disjoint groups that we
denote by zi where i = 1, . . . , m. we then assume that the q distribution factorizes
with respect to these groups, so that

q(z) =

qi(zi).

(10.5)

m(cid:14)

i=1

10.1. variational id136

465

it should be emphasized that we are making no further assumptions about the distri-
bution. in particular, we place no restriction on the functional forms of the individual
factors qi(zi). this factorized form of variational id136 corresponds to an ap-
proximation framework developed in physics called mean    eld theory (parisi, 1988).
amongst all distributions q(z) having the form (10.5), we now seek that distri-
bution for which the lower bound l(q) is largest. we therefore wish to make a free
form (variational) optimization of l(q) with respect to all of the distributions qi(zi),
which we do by optimizing with respect to each of the factors in turn. to achieve
this, we    rst substitute (10.5) into (10.3) and then dissect out the dependence on one
of the factors qj(zj). denoting qj(zj) by simply qj to keep the notation uncluttered,
we then obtain
l(q) =

(cid:24)
(cid:6) (cid:14)
(cid:2)
(cid:24)(cid:6)
(cid:6)
(cid:14)
(cid:6)
(cid:6)
qj ln(cid:4)p(x, zj) dzj    
where we have de   ned a new distribution(cid:4)p(x, zj) by the relation
ln(cid:4)p(x, zj) = ei(cid:9)=j[ln p(x, z)] + const.

(10.7)
here the notation ei(cid:9)=j[       ] denotes an expectation with respect to the q distributions
over all variables zi for i (cid:9)= j, so that

(cid:25)
(cid:25)

ln p(x, z)    

qj ln qj dzj + const

qj ln qj dzj + const

ln p(x, z)

dzj    

qi dzi

(10.6)

(cid:6)

ln qi

dz

i(cid:9)=j

=

=

qj

qi

i

i

(cid:6)

ei(cid:9)=j[ln p(x, z)] =

ln p(x, z)

qi dzi.

(10.8)

(cid:14)

i(cid:9)=j

now suppose we keep the {qi(cid:9)=j}    xed and maximize l(q) in (10.6) with re-
spect to all possible forms for the distribution qj(zj). this is easily done by rec-
ognizing that (10.6) is a negative id181 between qj(zj) and

(cid:4)p(x, zj). thus maximizing (10.6) is equivalent to minimizing the kullback-leibler

leonhard euler
1707   1783

euler was a swiss mathematician
and physicist who worked in st.
petersburg and berlin and who is
widely considered to be one of the
greatest mathematicians of all time.
he is certainly the most proli   c, and
his collected works    ll 75 volumes. amongst his many

contributions, he formulated the modern theory of the
function, he developed (together with lagrange) the
calculus of variations, and he discovered the formula
ei   =    1, which relates four of the most important
numbers in mathematics. during the last 17 years of
his life, he was almost totally blind, and yet he pro-
duced nearly half of his results during this period.

466

10. approximate id136

divergence, and the minimum occurs when qj(zj) =(cid:4)p(x, zj). thus we obtain a

general expression for the optimal solution q(cid:1)

j (zj) given by

ln q(cid:1)

j (zj) = ei(cid:9)=j[ln p(x, z)] + const.

(10.9)

it is worth taking a few moments to study the form of this solution as it provides the
basis for applications of variational methods. it says that the log of the optimal so-
lution for factor qj is obtained simply by considering the log of the joint distribution
over all hidden and visible variables and then taking the expectation with respect to
all of the other factors {qi} for i (cid:9)= j.
(cid:6)

the additive constant in (10.9) is set by normalizing the distribution q(cid:1)

thus if we take the exponential of both sides and normalize, we have

exp (ei(cid:9)=j[ln p(x, z)])

j (zj).

q(cid:1)
j (zj) =

.

exp (ei(cid:9)=j[ln p(x, z)]) dzj

in practice, we shall    nd it more convenient to work with the form (10.9) and then re-
instate the id172 constant (where required) by inspection. this will become
clear from subsequent examples.

the set of equations given by (10.9) for j = 1, . . . , m represent a set of con-
sistency conditions for the maximum of the lower bound subject to the factorization
constraint. however, they do not represent an explicit solution because the expres-
sion on the right-hand side of (10.9) for the optimum q(cid:1)
j (zj) depends on expectations
computed with respect to the other factors qi(zi) for i (cid:9)= j. we will therefore seek
a consistent solution by    rst initializing all of the factors qi(zi) appropriately and
then cycling through the factors and replacing each in turn with a revised estimate
given by the right-hand side of (10.9) evaluated using the current estimates for all of
the other factors. convergence is guaranteed because bound is convex with respect
to each of the factors qi(zi) (boyd and vandenberghe, 2004).

10.1.2 properties of factorized approximations
our approach to variational id136 is based on a factorized approximation to
the true posterior distribution. let us consider for a moment the problem of approx-
imating a general distribution by a factorized distribution. to begin with, we discuss
the problem of approximating a gaussian distribution using a factorized gaussian,
which will provide useful insight into the types of inaccuracy introduced in using
factorized approximations. consider a gaussian distribution p(z) = n (z|  ,   
   1)
over two correlated variables z = (z1, z2) in which the mean and precision have
elements

(cid:15)

(cid:15)

(cid:16)

(cid:16)

   =

,

   =

  1
  2

  11   12
  21   22

(10.10)

and   21 =   12 due to the symmetry of the precision matrix. now suppose we
wish to approximate this distribution using a factorized gaussian of the form q(z) =
q1(z1)q2(z2). we    rst apply the general result (10.9) to    nd an expression for the

10.1. variational id136

467

optimal factor q(cid:1)
1(z1). in doing so it is useful to note that on the right-hand side we
only need to retain those terms that have some functional dependence on z1 because
all other terms can be absorbed into the id172 constant. thus we have

ln q(cid:1)

1(z1) = ez2[ln p(z)] + const

(cid:29)

(cid:30)

(z1       1)2  11     (z1       1)  12(z2       2)

   1
2
1  11 + z1  1  11     z1  12 (e[z2]       2) + const.

+ const

(10.11)

= ez2
=    1

2 z2

next we observe that the right-hand side of this expression is a quadratic function of
z1, and so we can identify q(cid:1)(z1) as a gaussian distribution. it is worth emphasizing
that we did not assume that q(zi) is gaussian, but rather we derived this result by
variational optimization of the kl divergence over all possible distributions q(zi).
note also that we do not need to consider the additive constant in (10.9) explicitly
because it represents the id172 constant that can be found at the end by
inspection if required. using the technique of completing the square, we can identify
the mean and precision of this gaussian, giving

section 2.3.1

q(cid:1)(z1) = n (z1|m1,      1
11 )

where

m1 =   1          1

11   12 (e[z2]       2) .
2(z2) is also gaussian and can be written as

by symmetry, q(cid:1)

2(z2) = n (z2|m2,      1
q(cid:1)
22 )

(10.12)

(10.13)

(10.14)

in which

m2 =   2          1

22   21 (e[z1]       1) .

(10.15)
note that these solutions are coupled, so that q(cid:1)(z1) depends on expectations com-
puted with respect to q(cid:1)(z2) and vice versa. in general, we address this by treating
the variational solutions as re-estimation equations and cycling through the variables
in turn updating them until some convergence criterion is satis   ed. we shall see
an example of this shortly. here, however, we note that the problem is suf   ciently
simple that a closed form solution can be found. in particular, because e[z1] = m1
and e[z2] = m2, we see that the two equations are satis   ed if we take e[z1] =   1
and e[z2] =   2, and it is easily shown that this is the only solution provided the dis-
tribution is nonsingular. this result is illustrated in figure 10.2(a). we see that the
mean is correctly captured but that the variance of q(z) is controlled by the direction
of smallest variance of p(z), and that the variance along the orthogonal direction is
signi   cantly under-estimated. it is a general result that a factorized variational ap-
proximation tends to give approximations to the posterior distribution that are too
compact.
by way of comparison, suppose instead that we had been minimizing the reverse
id181 kl(p(cid:5)q). as we shall see, this form of kl divergence

exercise 10.2

468

10. approximate id136

figure 10.2 comparison
of
the
the two alternative forms for
id181.
the
green contours corresponding to
1, 2, and 3 standard deviations for
a correlated gaussian distribution
p(z) over two variables z1 and z2,
and the red contours represent
the corresponding levels for an
q(z)
approximating
over the same variables given by
the product of
two independent
univariate gaussian distributions
whose parameters are obtained by
minimization of
the kullback-
leibler divergence kl(q(cid:3)p), and
the reverse kullback-leibler
(b)
divergence kl(p(cid:3)q).

distribution

(a)

1

z2

0.5

0

0

1

z2

0.5

0

0

z1

1

0.5
(a)

z1

1

0.5
(b)

section 10.7

exercise 10.3

is used in an alternative approximate id136 framework called expectation prop-
agation. we therefore consider the general problem of minimizing kl(p(cid:5)q) when
q(z) is a factorized approximation of the form (10.5). the kl divergence can then
be written in the form

 

kl(p(cid:5)q) =    

p(z)

ln qi(zi)

dz + const

(10.16)

where the constant term is simply the id178 of p(z) and so does not depend on
q(z). we can now optimize with respect to each of the factors qj(zj), which is
easily done using a lagrange multiplier to give

q(cid:1)
j (zj) =

p(z)

dzi = p(zj).

(10.17)

(cid:6)

(cid:6)

(cid:31)
m(cid:2)

i=1

(cid:14)

i(cid:9)=j

in this case, we    nd that the optimal solution for qj(zj) is just given by the corre-
sponding marginal distribution of p(z). note that this is a closed-form solution and
so does not require iteration.

to apply this result to the illustrative example of a gaussian distribution p(z)
over a vector z we can use (2.98), which gives the result shown in figure 10.2(b).
we see that once again the mean of the approximation is correct, but that it places
signi   cant id203 mass in regions of variable space that have very low probabil-
ity.

the difference between these two results can be understood by noting that there

is a large positive contribution to the id181

kl(q(cid:5)p) =    

q(z) ln

dz

(10.18)

(cid:6)

(cid:12)

(cid:13)

p(z)
q(z)

10.1. variational id136

469

(a)

(b)

(c)

figure 10.3 another comparison of the two alternative forms for the id181. (a) the blue
contours show a bimodal distribution p(z) given by a mixture of two gaussians, and the red contours correspond
to the single gaussian distribution q(z) that best approximates p(z) in the sense of minimizing the kullback-
leibler divergence kl(p(cid:3)q). (b) as in (a) but now the red contours correspond to a gaussian distribution q(z)
found by numerical minimization of the id181 kl(q(cid:3)p). (c) as in (b) but showing a different
local minimum of the id181.

from regions of z space in which p(z) is near zero unless q(z) is also close to
zero. thus minimizing this form of kl divergence leads to distributions q(z) that
avoid regions in which p(z) is small. conversely, the id181
kl(p(cid:5)q) is minimized by distributions q(z) that are nonzero in regions where p(z)
is nonzero.

we can gain further insight into the different behaviour of the two kl diver-
gences if we consider approximating a multimodal distribution by a unimodal one,
as illustrated in figure 10.3.
in practical applications, the true posterior distri-
bution will often be multimodal, with most of the posterior mass concentrated in
some number of relatively small regions of parameter space. these multiple modes
may arise through nonidenti   ability in the latent space or through complex nonlin-
ear dependence on the parameters. both types of multimodality were encountered in
chapter 9 in the context of gaussian mixtures, where they manifested themselves as
multiple maxima in the likelihood function, and a variational treatment based on the
minimization of kl(q(cid:5)p) will tend to    nd one of these modes. by contrast, if we
were to minimize kl(p(cid:5)q), the resulting approximations would average across all
of the modes and, in the context of the mixture model, would lead to poor predictive
distributions (because the average of two good parameter values is typically itself
not a good parameter value). it is possible to make use of kl(p(cid:5)q) to de   ne a useful
id136 procedure, but this requires a rather different approach to the one discussed
here, and will be considered in detail when we discuss expectation propagation.

the two forms of id181 are members of the alpha family

section 10.7

470

10. approximate id136

(cid:15)

(cid:6)

of divergences (ali and silvey, 1966; amari, 1985; minka, 2005) de   ned by

4

1    

d  (p(cid:5)q) =

p(x)(1+  )/2q(x)(1     )/2 dx

1       2

(10.19)
where        <    <     is a continuous parameter. the id181
kl(p(cid:5)q) corresponds to the limit        1, whereas kl(q(cid:5)p) corresponds to the limit
          1. for all values of    we have d  (p(cid:5)q) (cid:2) 0, with equality if, and only if,
p(x) = q(x). suppose p(x) is a    xed distribution, and we minimize d  (p(cid:5)q) with
respect to some set of distributions q(x). then for    (cid:1)    1 the divergence is zero
forcing, so that any values of x for which p(x) = 0 will have q(x) = 0, and typically
q(x) will under-estimate the support of p(x) and will tend to seek the mode with the
largest mass. conversely for    (cid:2) 1 the divergence is zero-avoiding, so that values
of x for which p(x) > 0 will have q(x) > 0, and typically q(x) will stretch to cover
all of p(x), and will over-estimate the support of p(x). when    = 0 we obtain a
symmetric divergence that is linearly related to the hellinger distance given by

dh(p(cid:5)q) =

p(x)1/2     q(x)1/2

dx.

(10.20)

(cid:6) (cid:10)

the square root of the hellinger distance is a valid distance metric.

10.1.3 example: the univariate gaussian
we now illustrate the factorized variational approximation using a gaussian dis-
tribution over a single variable x (mackay, 2003). our goal is to infer the posterior
distribution for the mean    and precision    , given a data set d = {x1, . . . , xn} of
observed values of x which are assumed to be drawn independently from the gaus-
sian. the likelihood function is given by

(cid:16)

(cid:25)

p(d|  ,   ) =

(xn       )2

.

(10.21)

(cid:24)

n(cid:2)

n=1

      
2

  
2  

(cid:18)n/2
(cid:17)
p(  |  ) = n(cid:10)

exp

we now introduce conjugate prior distributions for    and    given by

  |  0, (  0  )   1

(10.22)
(10.23)
where gam(  |a0, b0) is the gamma distribution de   ned by (2.146). together these
distributions constitute a gaussian-gamma conjugate prior distribution.

p(  ) = gam(  |a0, b0)

for this simple problem the posterior distribution can be found exactly, and again
takes the form of a gaussian-gamma distribution. however, for tutorial purposes
we will consider a factorized variational approximation to the posterior distribution
given by

q(  ,   ) = q  (  )q   (  ).

(10.24)

(cid:11)

(cid:11)

exercise 10.6

section 2.3.6

exercise 2.44

10.1. variational id136

471

note that the true posterior distribution does not factorize in this way. the optimum
factors q  (  ) and q   (  ) can be obtained from the general result (10.9) as follows.
for q  (  ) we have

ln q(cid:1)

(cid:24)

  (  ) = e   [ln p(d|  ,   ) + ln p(  |  )] + const
(xn       )2

(cid:25)
completing the square over    we see that q  (  ) is a gaussian n(cid:10)

  0(         0)2 +

=     e[  ]
2

n(cid:2)

n=1

(cid:11)

+ const. (10.25)

  |  n ,   

   1
n

with

exercise 10.7

mean and precision given by

  n =   0  0 + n x
  0 + n
  n = (  0 + n)e[  ].

(10.26)

(10.27)
note that for n         this gives the maximum likelihood result in which   n = x
and the precision is in   nite.

similarly, the optimal solution for the factor q   (  ) is given by

   (  ) = e   [ln p(d|  ,   ) + ln p(  |  )] + ln p(  ) + const

ln q(cid:1)

= (a0     1) ln        b0   + n
2

ln   

(cid:31)

n(cid:2)

n=1

      

2 e  

(xn       )2 +   0(         0)2

+ const

(10.28)

and hence q   (  ) is a gamma distribution gam(  |an , bn ) with parameters

an = a0 + n
2
1
2 e  

bn = b0 +

(cid:31)

n(cid:2)

n=1

(xn       )2 +   0(         0)2

.

(10.29)

(10.30)

 

 

again this exhibits the expected behaviour when n        .

it should be emphasized that we did not assume these speci   c functional forms
for the optimal distributions q  (  ) and q   (  ). they arose naturally from the structure
of the likelihood function and the corresponding conjugate priors.

thus we have expressions for the optimal distributions q  (  ) and q   (  ) each of
which depends on moments evaluated with respect to the other distribution. one ap-
proach to    nding a solution is therefore to make an initial guess for, say, the moment
e[  ] and use this to re-compute the distribution q  (  ). given this revised distri-
bution we can then extract the required moments e[  ] and e[  2], and use these to
recompute the distribution q   (  ), and so on. since the space of hidden variables for
this example is only two dimensional, we can illustrate the variational approxima-
tion to the posterior distribution by plotting contours of both the true posterior and
the factorized approximation, as illustrated in figure 10.4.

exercise 10.8

section 10.4.1

472

10. approximate id136

(a)

(c)

  

2

1

0
   1
2

  

1

0
   1

(b)

(d)

  

2

1

0
   1
2

  

1

0

  

1

0

  

1

0

  

1

0
   1

0

  

1

figure 10.4 illustration of variational id136 for the mean    and precision    of a univariate gaussian distribu-
tion. contours of the true posterior distribution p(  ,   |d) are shown in green. (a) contours of the initial factorized
approximation q  (  )q   (   ) are shown in blue. (b) after re-estimating the factor q  (  ). (c) after re-estimating the
factor q   (   ). (d) contours of the optimal factorized approximation, to which the iterative scheme converges, are
shown in red.

appendix b

in general, we will need to use an iterative approach such as this in order to
solve for the optimal factorized posterior distribution. for the very simple example
we are considering here, however, we can    nd an explicit solution by solving the
simultaneous equations for the optimal factors q  (  ) and q   (  ). before doing this,
we can simplify these expressions by considering broad, noninformative priors in
which   0 = a0 = b0 =   0 = 0. although these parameter settings correspond to
improper priors, we see that the posterior distribution is still well de   ned. using the
standard result e[  ] = an /bn for the mean of a gamma distribution, together with
(10.29) and (10.30), we have

(cid:31)

n(cid:2)

n=1

1

e[  ]

= e

1
n

 

(xn       )2

= x2     2xe[  ] + e[  2].

(10.31)

then, using (10.26) and (10.27), we obtain the    rst and second order moments of

10.1. variational id136

473

q  (  ) in the form

e[  ] = x,

e[  2] = x2 +

1

n e[  ] .

(10.32)

exercise 10.9

we can now substitute these moments into (10.31) and then solve for e[  ] to give

(x2     x2)

1

e[  ]

=

=

1

n     1

1

n     1

n(cid:2)

n=1

(xn     x)2.

(10.33)

section 1.2.4

we recognize the right-hand side as the familiar unbiased estimator for the variance
of a univariate gaussian distribution, and so we see that the use of a bayesian ap-
proach has avoided the bias of the maximum likelihood solution.

10.1.4 model comparison
as well as performing id136 over the hidden variables z, we may also
wish to compare a set of candidate models, labelled by the index m, and having
prior probabilities p(m). our goal is then to approximate the posterior probabilities
p(m|x), where x is the observed data. this is a slightly more complex situation
than that considered so far because different models may have different structure
and indeed different dimensionality for the hidden variables z. we cannot there-
fore simply consider a factorized approximation q(z)q(m), but must instead recog-
nize that the posterior over z must be conditioned on m, and so we must consider
q(z, m) = q(z|m)q(m). we can readily verify the following decomposition based
on this variational distribution

ln p(x) = lm    

q(z|m)q(m) ln

where the lm is a lower bound on ln p(x) and is given by

lm =

q(z|m)q(m) ln

p(z, x, m)
q(z|m)q(m)

(cid:13)

p(z, m|x)
q(z|m)q(m)

(cid:12)

(cid:12)

(cid:13)

(10.34)

.

(10.35)

(cid:2)

(cid:2)

m

z

(cid:2)

(cid:2)

m

z

here we are assuming discrete z, but the same analysis applies to continuous latent
variables provided the summations are replaced with integrations. we can maximize
lm with respect to the distribution q(m) using a lagrange multiplier, with the result

q(m)     p(m) exp{lm}.

(10.36)
however, if we maximize lm with respect to the q(z|m), we    nd that the solutions
for different m are coupled, as we expect because they are conditioned on m. we
proceed instead by    rst optimizing each of the q(z|m) individually by optimization

exercise 10.10

exercise 10.11

474

10. approximate id136

of (10.35), and then subsequently determining the q(m) using (10.36). after nor-
malization the resulting values for q(m) can be used for model selection or model
averaging in the usual way.

10.2.

illustration: variational mixture of gaussians

we now return to our discussion of the gaussian mixture model and apply the vari-
ational id136 machinery developed in the previous section. this will provide a
good illustration of the application of variational methods and will also demonstrate
how a bayesian treatment elegantly resolves many of the dif   culties associated with
the maximum likelihood approach (attias, 1999b). the reader is encouraged to work
through this example in detail as it provides many insights into the practical appli-
cation of variational methods. many bayesian models, corresponding to much more
sophisticated distributions, can be solved by straightforward extensions and general-
izations of this analysis.

n(cid:14)

k(cid:14)

n=1

k=1

n(cid:14)

k(cid:14)

n(cid:10)

n=1

k=1

our starting point is the likelihood function for the gaussian mixture model, il-
lustrated by the graphical model in figure 9.6. for each observation xn we have
a corresponding latent variable zn comprising a 1-of-k binary vector with ele-
ments znk for k = 1, . . . , k. as before we denote the observed data set by x =
{x1, . . . , xn}, and similarly we denote the latent variables by z = {z1, . . . , zn}.
from (9.10) we can write down the conditional distribution of z, given the mixing
coef   cients   , in the form

p(z|  ) =

  znk
k

.

(10.37)

(cid:11)znk

similarly, from (9.11), we can write down the conditional distribution of the ob-
served data vectors, given the latent variables and the component parameters

p(x|z,   ,   ) =

xn|  k,   

   1
k

(10.38)

where    = {  k} and    = {  k}. note that we are working in terms of precision
matrices rather than covariance matrices as this somewhat simpli   es the mathemat-
ics.

next we introduce priors over the parameters   ,    and   . the analysis is con-
siderably simpli   ed if we use conjugate prior distributions. we therefore choose a
dirichlet distribution over the mixing coef   cients   

p(  ) = dir(  |  0) = c(  0)

    0   1

k

(10.39)

where by symmetry we have chosen the same parameter   0 for each of the compo-
nents, and c(  0) is the id172 constant for the dirichlet distribution de   ned

k=1

k(cid:14)

section 10.4.1

10.2. illustration: variational mixture of gaussians

475

figure 10.5 directed acyclic graph representing the bayesian mix-
ture of gaussians model, in which the box (plate) de-
notes a set of n i.i.d. observations. here    denotes
{  k} and    denotes {  k}.

  

  

  

zn

xn

n

section 2.2.1

by (b.23). as we have seen, the parameter   0 can be interpreted as the effective
prior number of observations associated with each component of the mixture. if the
value of   0 is small, then the posterior distribution will be in   uenced primarily by
the data rather than by the prior.

similarly, we introduce an independent gaussian-wishart prior governing the

mean and precision of each gaussian component, given by

p(  ,   ) = p(  |  )p(  )

k(cid:14)

n(cid:10)

=

  k|m0, (  0  k)   1

(cid:11) w(  k|w0,   0)

(10.40)

section 2.3.6

k=1

because this represents the conjugate prior distribution when both the mean and pre-
cision are unknown. typically we would choose m0 = 0 by symmetry.

the resulting model can be represented as a directed graph as shown in fig-
ure 10.5. note that there is a link from    to    since the variance of the distribution
over    in (10.40) is a function of   .

this example provides a nice illustration of the distinction between latent vari-
ables and parameters. variables such as zn that appear inside the plate are regarded
as latent variables because the number of such variables grows with the size of the
data set. by contrast, variables such as    that are outside the plate are    xed in
number independently of the size of the data set, and so are regarded as parameters.
from the perspective of id114, however, there is really no fundamental
difference between them.

10.2.1 variational distribution
in order to formulate a variational treatment of this model, we next write down

the joint distribution of all of the random variables, which is given by
p(x, z,   ,   ,   ) = p(x|z,   ,   )p(z|  )p(  )p(  |  )p(  )

(10.41)

in which the various factors are de   ned above. the reader should take a moment to
verify that this decomposition does indeed correspond to the probabilistic graphical
model shown in figure 10.5. note that only the variables x = {x1, . . . , xn} are
observed.

476

10. approximate id136

we now consider a variational distribution which factorizes between the latent

variables and the parameters so that

q(z,   ,   ,   ) = q(z)q(  ,   ,   ).

(10.42)

it is remarkable that this is the only assumption that we need to make in order to
obtain a tractable practical solution to our bayesian mixture model. in particular, the
functional form of the factors q(z) and q(  ,   ,   ) will be determined automatically
by optimization of the variational distribution. note that we are omitting the sub-
scripts on the q distributions, much as we do with the p distributions in (10.41), and
are relying on the arguments to distinguish the different distributions.

the corresponding sequential update equations for these factors can be easily
derived by making use of the general result (10.9). let us consider the derivation of
the update equation for the factor q(z). the log of the optimized factor is given by

ln q(cid:1)(z) = e  ,  ,  [ln p(x, z,   ,   ,   )] + const.

(10.43)

we now make use of the decomposition (10.41). note that we are only interested in
the functional dependence of the right-hand side on the variable z. thus any terms
that do not depend on z can be absorbed into the additive id172 constant,
giving

ln q(cid:1)(z) = e  [ln p(z|  )] + e  ,  [ln p(x|z,   ,   )] + const.

(10.44)

substituting for the two conditional distributions on the right-hand side, and again
absorbing any terms that are independent of z into the additive constant, we have

ln q(cid:1)(z) =

znk ln   nk + const

(10.45)

n(cid:2)

k(cid:2)

n=1

k=1

where we have de   ned

ln   nk = e[ln   k] +

where d is the dimensionality of the data variable x. taking the exponential of both
sides of (10.45) we obtain

   1

2

1

2 e  k,  k

(cid:8)
(cid:9)
2 e [ln|  k| ]     d
ln(2  )
(xn       k)t  k(xn       k)
k(cid:14)
q(cid:1)(z)     n(cid:14)
n(cid:14)

k(cid:14)

  znk
nk .

n=1

k=1

q(cid:1)(z) =

rznk
nk

n=1

k=1

(10.46)

(10.47)

(10.48)

exercise 10.12

requiring that this distribution be normalized, and noting that for each value of n
the quantities znk are binary and sum to 1 over all values of k, we obtain

where

10.2. illustration: variational mixture of gaussians

477

k(cid:2)

rnk =   nk
  nj

.

j=1

(10.49)

we see that the optimal solution for the factor q(z) takes the same functional form
as the prior p(z|  ). note that because   nk is given by the exponential of a real
quantity, the quantities rnk will be nonnegative and will sum to one, as required.

for the discrete distribution q(cid:1)(z) we have the standard result

e[znk] = rnk

(10.50)

from which we see that the quantities rnk are playing the role of responsibilities.
note that the optimal solution for q(cid:1)(z) depends on moments evaluated with respect
to the distributions of other variables, and so again the variational update equations
are coupled and must be solved iteratively.

at this point, we shall    nd it convenient to de   ne three statistics of the observed

data set evaluated with respect to the responsibilities, given by

n(cid:2)

n=1

1
nk

1
nk

nk =

xk =

sk =

rnkxn

rnk(xn     xk)(xn     xk)t.

(10.51)

(10.52)

(10.53)

rnk

n(cid:2)
n(cid:2)

n=1

n=1

note that these are analogous to quantities evaluated in the maximum likelihood em
algorithm for the gaussian mixture model.

now let us consider the factor q(  ,   ,   ) in the variational posterior distribu-

tion. again using the general result (10.9) we have

ln q(cid:1)(  ,   ,   ) = ln p(  ) +

ln p(  k,   k) + ez [ln p(z|  )]

k(cid:2)
e[znk] lnn(cid:10)

k=1

k(cid:2)

n(cid:2)

k=1

n=1

+

(cid:11)

xn|  k,   

   1
k

+ const.

(10.54)

we observe that the right-hand side of this expression decomposes into a sum of
terms involving only    together with terms only involving    and   , which implies
that the variational posterior q(  ,   ,   ) factorizes to give q(  )q(  ,   ). further-
more, the terms involving    and    themselves comprise a sum over k of terms
involving   k and   k leading to the further factorization

q(  ,   ,   ) = q(  )

q(  k,   k).

(10.55)

k(cid:14)

k=1

478

10. approximate id136

identifying the terms on the right-hand side of (10.54) that depend on   , we have

ln q(cid:1)(  ) = (  0     1)

ln   k +

rnk ln   k + const

(10.56)

k(cid:2)

k(cid:2)

n(cid:2)

k=1

k=1

n=1

where we have used (10.50). taking the exponential of both sides, we recognize
q(cid:1)(  ) as a dirichlet distribution

q(cid:1)(  ) = dir(  |  )

(10.57)

exercise 10.13

exercise 10.14

where    has components   k given by

  k =   0 + nk.

(10.58)
finally, the variational posterior distribution q(cid:1)(  k,   k) does not factorize into
the product of the marginals, but we can always use the product rule to write it in the
form q(cid:1)(  k,   k) = q(cid:1)(  k|  k)q(cid:1)(  k). the two factors can be found by inspecting
(10.54) and reading off those terms that involve   k and   k. the result, as expected,
is a gaussian-wishart distribution and is given by
  k|mk, (  k  k)   1

q(cid:1)(  k,   k) = n(cid:10)

(cid:11) w(  k|wk,   k)

(10.59)

where we have de   ned

  k =   0 + nk

mk =

1
  k

(  0m0 + nkxk)

w

= w

   1
k
  k =   0 + nk.

   1
0 + nksk +   0nk
  0 + nk

(xk     m0)(xk     m0)t

(10.60)

(10.61)

(10.62)

(10.63)

these update equations are analogous to the m-step equations of the em algorithm
for the maximum likelihood solution of the mixture of gaussians. we see that the
computations that must be performed in order to update the variational posterior
distribution over the model parameters involve evaluation of the same sums over the
data set, as arose in the maximum likelihood treatment.

in order to perform this variational m step, we need the expectations e[znk] =
rnk representing the responsibilities. these are obtained by normalizing the   nk that
are given by (10.46). we see that this expression involves expectations with respect
to the variational distributions of the parameters, and these are easily evaluated to
give

(cid:8)
(cid:9)
(xn       k)t  k(xn       k)

e  k,  k

(cid:15)
d(cid:2)
k +   k(xn     mk)twk(xn     mk)
   1

(cid:16)

  k + 1     i

(10.64)
+ d ln 2 + ln|wk| (10.65)

= d  

ln(cid:4)  k     e [ln|  k| ] =
ln(cid:4)  k     e [ln   k] =   (  k)       ((cid:1)  )

i=1

  

2

(10.66)

appendix b

(cid:5)

10.2. illustration: variational mixture of gaussians

where we have introduced de   nitions of(cid:4)  k and(cid:4)  k, and   (  ) is the digamma function
de   ned by (b.25), with(cid:1)   =
(cid:12)

if we substitute (10.64), (10.65), and (10.66) into (10.46) and make use of

the standard properties of the wishart and dirichlet distributions.

(10.49), we obtain the following result for the responsibilities

k   k. the results (10.65) and (10.66) follow from

(cid:13)

479

exp

    d
2  k

      k
2

(xn     mk)twk(xn     mk)

.

(10.67)

rnk    (cid:4)  k

(cid:4)  1/2

k

notice the similarity to the corresponding result for the responsibilities in maximum
likelihood em, which from (9.13) can be written in the form

rnk       k|  k|1/2 exp

(10.68)

(cid:12)

   1
2

(cid:13)
(xn       k)t  k(xn       k)

where we have used the precision in place of the covariance to highlight the similarity
to (10.67).

thus the optimization of the variational posterior distribution involves cycling
between two stages analogous to the e and m steps of the maximum likelihood em
algorithm. in the variational equivalent of the e step, we use the current distributions
over the model parameters to evaluate the moments in (10.64), (10.65), and (10.66)
and hence evaluate e[znk] = rnk. then in the subsequent variational equivalent
of the m step, we keep these responsibilities    xed and use them to re-compute the
variational distribution over the parameters using (10.57) and (10.59). in each case,
we see that the variational posterior distribution has the same functional form as the
corresponding factor in the joint distribution (10.41). this is a general result and is
a consequence of the choice of conjugate distributions.

figure 10.6 shows the results of applying this approach to the rescaled old faith-
ful data set for a gaussian mixture model having k = 6 components. we see that
after convergence, there are only two components for which the expected values
of the mixing coef   cients are numerically distinguishable from their prior values.
this effect can be understood qualitatively in terms of the automatic trade-off in a
bayesian model between    tting the data and the complexity of the model, in which
the complexity penalty arises from components whose parameters are pushed away
from their prior values. components that take essentially no responsibility for ex-
plaining the data points have rnk (cid:7) 0 and hence nk (cid:7) 0. from (10.58), we see
that   k (cid:7)   0 and from (10.60)   (10.63) we see that the other parameters revert to
their prior values. in principle such components are    tted slightly to the data points,
but for broad priors this effect is too small to be seen numerically. for the varia-
tional gaussian mixture model the expected values of the mixing coef   cients in the
posterior distribution are given by

section 10.4.1

section 3.4

exercise 10.15

(10.69)
consider a component for which nk (cid:7) 0 and   k (cid:7)   0. if the prior is broad so that
  0     0, then e[  k]     0 and the component plays no role in the model, whereas if

.

e[  k] =   k + nk
k  0 + n

480

10. approximate id136

figure 10.6 id58ian
mixture of k = 6 gaussians ap-
plied to the old faithful data set, in
which the ellipses denote the one
standard-deviation density contours
for each of the components, and the
density of red ink inside each ellipse
corresponds to the mean value of
the mixing coef   cient for each com-
ponent. the number in the top left
of each diagram shows the num-
ber of iterations of variational infer-
ence. components whose expected
mixing coef   cient are numerically in-
distinguishable from zero are not
plotted.

0

60

15

120

the prior tightly constrains the mixing coef   cients so that   0        , then e[  k]    
1/k.

in figure 10.6, the prior over the mixing coef   cients is a dirichlet of the form
(10.39). recall from figure 2.5 that for   0 < 1 the prior favours solutions in which
some of the mixing coef   cients are zero. figure 10.6 was obtained using   0 = 10   3,
and resulted in two components having nonzero mixing coef   cients. if instead we
choose   0 = 1 we obtain three components with nonzero mixing coef   cients, and
for    = 10 all six components have nonzero mixing coef   cients.

as we have seen there is a close similarity between the variational solution for
the bayesian mixture of gaussians and the em algorithm for maximum likelihood.
in fact if we consider the limit n         then the bayesian treatment converges to the
maximum likelihood em algorithm. for anything other than very small data sets,
the dominant computational cost of the variational algorithm for gaussian mixtures
arises from the evaluation of the responsibilities, together with the evaluation and
inversion of the weighted data covariance matrices. these computations mirror pre-
cisely those that arise in the maximum likelihood em algorithm, and so there is little
computational overhead in using this bayesian approach as compared to the tradi-
tional maximum likelihood one. there are, however, some substantial advantages.
first of all, the singularities that arise in maximum likelihood when a gaussian com-
ponent    collapses    onto a speci   c data point are absent in the bayesian treatment.

10.2. illustration: variational mixture of gaussians

481

indeed, these singularities are removed if we simply introduce a prior and then use a
map estimate instead of maximum likelihood. furthermore, there is no over-   tting
if we choose a large number k of components in the mixture, as we saw in fig-
ure 10.6. finally, the variational treatment opens up the possibility of determining
the optimal number of components in the mixture without resorting to techniques
such as cross validation.

section 10.2.4

10.2.2 variational lower bound
we can also straightforwardly evaluate the lower bound (10.3) for this model.
in practice, it is useful to be able to monitor the bound during the re-estimation in
order to test for convergence. it can also provide a valuable check on both the math-
ematical expressions for the solutions and their software implementation, because at
each step of the iterative re-estimation procedure the value of this bound should not
decrease. we can take this a stage further to provide a deeper test of the correctness
of both the mathematical derivation of the update equations and of their software im-
plementation by using    nite differences to check that each update does indeed give
a (constrained) maximum of the bound (svens  en and bishop, 2004).

for the variational mixture of gaussians, the lower bound (10.3) is given by
l =

p(x, z,   ,   ,   )

q(z,   ,   ,   ) ln

q(z,   ,   ,   )
= e[ln p(x, z,   ,   ,   )]     e[ln q(z,   ,   ,   )]
= e[ln p(x|z,   ,   )] + e[ln p(z|  )] + e[ln p(  )] + e[ln p(  ,   )]

d   d   d  

z

   e[ln q(z)]     e[ln q(  )]     e[ln q(  ,   )]

(10.70)

(cid:6)(cid:6)(cid:6)

(cid:2)

(cid:12)

(cid:13)

where, to keep the notation uncluttered, we have omitted the (cid:12) superscript on the
q distributions, along with the subscripts on the expectation operators because each
expectation is taken with respect to all of the random variables in its argument. the
various terms in the bound are easily evaluated to give the following results

exercise 10.16

k(cid:2)

(cid:12)

ln(cid:4)  k     d  

nk

1
2
     k(xk     mk)twk(xk     mk)     d ln(2  )

k       ktr(skwk)
   1

k=1

(cid:13)

n(cid:2)

k(cid:2)

rnk ln(cid:4)  k

e[ln p(x|z,   ,   )] =

e[ln p(z|  )] =

n=1

k=1

e[ln p(  )] = ln c(  0) + (  0     1)

k(cid:2)

k=1

ln(cid:4)  k

(10.71)

(10.72)

(10.73)

482

10. approximate id136

e[ln p(  ,   )] =

1
2

k(cid:2)

k=1

(cid:12)
d ln(  0/2  ) + ln(cid:4)  k     d  0

  k

(cid:13)

     0  k(mk     m0)twk(mk     m0)
(  0     d     1)

k(cid:2)

+

2

k=1

k=1

ln(cid:4)  k     1

k(cid:2)
k(cid:2)
(  k     1) ln(cid:4)  k + ln c(  )
(cid:16)
(cid:12)
ln(cid:4)  k + d

rnk ln rnk

(cid:15)

ln

k=1

  k
2  

1
2

2

2

n(cid:2)
k(cid:2)
k(cid:2)

n=1

k=1

k=1

e[ln q(z)] =

e[ln q(  )] =

e[ln q(  ,   )] =

+ k ln b(w0,   0)

  ktr(w

   1
0 wk)

(10.74)

(10.75)

(10.76)

(10.77)

(cid:13)

    h [q(  k)]

    d
2

where d is the dimensionality of x, h[q(  k)] is the id178 of the wishart distribu-
tion given by (b.82), and the coef   cients c(  ) and b(w,   ) are de   ned by (b.23)
and (b.79), respectively. note that the terms involving expectations of the logs of the
q distributions simply represent the negative entropies of those distributions. some
simpli   cations and combination of terms can be performed when these expressions
are summed to give the lower bound. however, we have kept the expressions sepa-
rate for ease of understanding.

finally, it is worth noting that the lower bound provides an alternative approach
for deriving the variational re-estimation equations obtained in section 10.2.1. to do
this we use the fact that, since the model has conjugate priors, the functional form of
the factors in the variational posterior distribution is known, namely discrete for z,
dirichlet for   , and gaussian-wishart for (  k,   k). by taking general parametric
forms for these distributions we can derive the form of the lower bound as a function
of the parameters of the distributions. maximizing the bound with respect to these
parameters then gives the required re-estimation equations.

exercise 10.18

10.2.3 predictive density
in applications of the bayesian mixture of gaussians model we will often be

interested in the predictive density for a new value(cid:1)x of the observed variable. as-
sociated with this observation will be a corresponding latent variable(cid:1)z, and the pre-

p((cid:1)x|(cid:1)z,   ,   )p((cid:1)z|  )p(  ,   ,   |x) d   d   d  

(10.78)

dictive density is then given by

p((cid:1)x|x) =

(cid:6)(cid:6)(cid:6)

(cid:2)

bz

10.2. illustration: variational mixture of gaussians

483

where p(  ,   ,   |x) is the (unknown) true posterior distribution of the parameters.

using (10.37) and (10.38) we can    rst perform the summation over(cid:1)z to give
p((cid:1)x|x) =

  kn(cid:10)(cid:1)x|  k,   

p(  ,   ,   |x) d   d   d  .

(cid:6)(cid:6)(cid:6)

k(cid:2)

(10.79)

(cid:11)

   1
k

k=1

because the remaining integrations are intractable, we approximate the predictive
density by replacing the true posterior distribution p(  ,   ,   |x) with its variational
approximation q(  )q(  ,   ) to give

p((cid:1)x|x) =

(cid:6)(cid:6)(cid:6)

k(cid:2)

k=1

  kn(cid:10)(cid:1)x|  k,   

   1
k

(cid:11)

q(  )q(  k,   k) d   d  k d  k

(10.80)

exercise 10.19

where we have made use of the factorization (10.55) and in each term we have im-
plicitly integrated out all variables {  j,   j} for j (cid:9)= k the remaining integrations
can now be evaluated analytically giving a mixture of student   s t-distributions

p((cid:1)x|x) =

k(cid:2)

k=1

1(cid:1)  

  kst((cid:1)x|mk, lk,   k + 1     d)

(10.81)

in which the kth component has mean mk, and the precision is given by

lk =

(  k + 1     d)  k

(1 +   k)

wk

(10.82)

exercise 10.20

in which   k is given by (10.63). when the size n of the data set is large the predictive
distribution (10.81) reduces to a mixture of gaussians.

section 10.1.4

exercise 10.21

10.2.4 determining the number of components
we have seen that the variational lower bound can be used to determine a pos-
terior distribution over the number k of components in the mixture model. there
is, however, one subtlety that needs to be addressed. for any given setting of the
parameters in a gaussian mixture model (except for speci   c degenerate settings),
there will exist other parameter settings for which the density over the observed vari-
ables will be identical. these parameter values differ only through a re-labelling of
the components. for instance, consider a mixture of two gaussians and a single ob-
served variable x, in which the parameters have the values   1 = a,   2 = b,   1 = c,
  2 = d,   1 = e,   2 = f. then the parameter values   1 = b,   2 = a,   1 = d,
  2 = c,   1 = f,   2 = e, in which the two components have been exchanged, will
by symmetry give rise to the same value of p(x). if we have a mixture model com-
prising k components, then each parameter setting will be a member of a family of
k! equivalent settings.

in the context of maximum likelihood, this redundancy is irrelevant because the
parameter optimization algorithm (for example em) will, depending on the initial-
ization of the parameters,    nd one speci   c solution, and the other equivalent solu-
tions play no role. in a bayesian setting, however, we marginalize over all possible

484

10. approximate id136

figure 10.7 plot of the variational lower bound
l versus the number k of com-
ponents in the gaussian mixture
model, for the old faithful data,
showing a distinct peak at k =
2 components. for each value
of k, the model
is trained from
100 different random starts, and
the results shown as    +    symbols
plotted with small random hori-
zontal perturbations so that they
can be distinguished. note that
some solutions    nd suboptimal
local maxima, but that this hap-
pens infrequently.

p(d|k)

1

2

3

4

5

6

k

parameter values. we have seen in figure 10.2 that if the true posterior distribution
is multimodal, variational id136 based on the minimization of kl(q(cid:5)p) will tend
to approximate the distribution in the neighbourhood of one of the modes and ignore
the others. again, because equivalent modes have equivalent predictive densities,
this is of no concern provided we are considering a model having a speci   c number
k of components. if, however, we wish to compare different values of k, then we
need to take account of this multimodality. a simple approximate solution is to add
a term ln k! onto the lower bound when used for model comparison and averaging.
figure 10.7 shows a plot of the lower bound, including the multimodality fac-
tor, versus the number k of components for the old faithful data set. it is worth
emphasizing once again that maximum likelihood would lead to values of the likeli-
hood function that increase monotonically with k (assuming the singular solutions
have been avoided, and discounting the effects of local maxima) and so cannot be
used to determine an appropriate model complexity. by contrast, bayesian id136
automatically makes the trade-off between model complexity and    tting the data.

this approach to the determination of k requires that a range of models having
different k values be trained and compared. an alternative approach to determining
a suitable value for k is to treat the mixing coef   cients    as parameters and make
point estimates of their values by maximizing the lower bound (corduneanu and
bishop, 2001) with respect to    instead of maintaining a id203 distribution
over them as in the fully bayesian approach. this leads to the re-estimation equation

n(cid:2)

  k =

1
n

rnk

n=1

(10.83)

and this maximization is interleaved with the variational updates for the q distribution
over the remaining parameters. components that provide insuf   cient contribution

exercise 10.22

section 3.4

exercise 10.23

section 7.2.2

10.2. illustration: variational mixture of gaussians

485

to explaining the data will have their mixing coef   cients driven to zero during the
optimization, and so they are effectively removed from the model through automatic
relevance determination. this allows us to make a single training run in which we
start with a relatively large initial value of k, and allow surplus components to be
pruned out of the model. the origins of the sparsity when optimizing with respect to
hyperparameters is discussed in detail in the context of the relevance vector machine.

10.2.5 induced factorizations
in deriving these variational update equations for the gaussian mixture model,
we assumed a particular factorization of the variational posterior distribution given
by (10.42). however, the optimal solutions for the various factors exhibit additional
factorizations. in particular, the solution for q(cid:1)(  ,   ) is given by the product of an
independent distribution q(cid:1)(  k,   k) over each of the components k of the mixture,
whereas the variational posterior distribution q(cid:1)(z) over the latent variables, given
by (10.48), factorizes into an independent distribution q(cid:1)(zn) for each observation n
(note that it does not further factorize with respect to k because, for each value of n,
the znk are constrained to sum to one over k). these additional factorizations are a
consequence of the interaction between the assumed factorization and the conditional
independence properties of the true distribution, as characterized by the directed
graph in figure 10.5.

we shall refer to these additional factorizations as induced factorizations be-
cause they arise from an interaction between the factorization assumed in the varia-
tional posterior distribution and the conditional independence properties of the true
joint distribution. in a numerical implementation of the variational approach it is
important to take account of such additional factorizations. for instance, it would
be very inef   cient to maintain a full precision matrix for the gaussian distribution
over a set of variables if the optimal form for that distribution always had a diago-
nal precision matrix (corresponding to a factorization with respect to the individual
variables described by that gaussian).

such induced factorizations can easily be detected using a simple graphical test
based on d-separation as follows. we partition the latent variables into three disjoint
groups a, b, c and then let us suppose that we are assuming a factorization between
c and the remaining latent variables, so that

q(a, b, c) = q(a, b)q(c).

(10.84)

using the general result (10.9), together with the product rule for probabilities, we
see that the optimal solution for q(a, b) is given by

ln q(cid:1)(a, b) = ec[ln p(x, a, b, c)] + const
= ec[ln p(a, b|x, c)] + const.

(10.85)
we now ask whether this resulting solution will factorize between a and b, in
other words whether q(cid:1)(a, b) = q(cid:1)(a)q(cid:1)(b). this will happen if, and only if,
ln p(a, b|x, c) = ln p(a|x, c) + ln p(b|x, c), that is, if the conditional inde-
pendence relation

a        b | x, c

(10.86)

486

10. approximate id136

is satis   ed. we can test to see if this relation does hold, for any choice of a and b
by making use of the d-separation criterion.

to illustrate this, consider again the bayesian mixture of gaussians represented
by the directed graph in figure 10.5, in which we are assuming a variational fac-
torization given by (10.42). we can see immediately that the variational posterior
distribution over the parameters must factorize between    and the remaining param-
eters    and    because all paths connecting    to either    or    must pass through
one of the nodes zn all of which are in the conditioning set for our conditional inde-
pendence test and all of which are head-to-tail with respect to such paths.

10.3. variational id75

exercise 10.26

n(cid:14)

as a second illustration of variational id136, we return to the bayesian linear
regression model of section 3.3. in the evidence framework, we approximated the
integration over    and    by making point estimates obtained by maximizing the log
marginal likelihood. a fully bayesian approach would integrate over the hyperpa-
rameters as well as over the parameters. although exact integration is intractable,
we can use variational methods to    nd a tractable approximation. in order to sim-
plify the discussion, we shall suppose that the noise precision parameter    is known,
and is    xed to its true value, although the framework is easily extended to include
the distribution over   . for the id75 model, the variational treatment
will turn out to be equivalent to the evidence framework. nevertheless, it provides a
good exercise in the use of variational methods and will also lay the foundation for
variational treatment of bayesian id28 in section 10.6.

recall that the likelihood function for w, and the prior over w, are given by

n (tn|wt  n,   

   1)

(10.87)

p(t|w) =
p(w|  ) = n (w|0,   

n=1

   1i)

(10.88)
where   n =   (xn). we now introduce a prior distribution over   . from our dis-
cussion in section 2.3.6, we know that the conjugate prior for the precision of a
gaussian is given by a gamma distribution, and so we choose

(10.89)
where gam(  |  ,  ) is de   ned by (b.26). thus the joint distribution of all the variables
is given by

p(  ) = gam(  |a0, b0)

p(t, w,   ) = p(t|w)p(w|  )p(  ).

(10.90)

this can be represented as a directed graphical model as shown in figure 10.8.

10.3.1 variational distribution
our    rst goal is to    nd an approximation to the posterior distribution p(w,   |t).
to do this, we employ the variational framework of section 10.1, with a variational

10.3. variational id75

487

figure 10.8 probabilistic graphical model representing the joint dis-
regression

the bayesian linear

for

tribution (10.90)
model.

  

w

  n

  

tn

n

posterior distribution given by the factorized expression

q(w,   ) = q(w)q(  ).

(10.91)

we can    nd re-estimation equations for the factors in this distribution by making use
of the general result (10.9). recall that for each factor, we take the log of the joint
distribution over all variables and then average with respect to those variables not in
that factor. consider    rst the distribution over   . keeping only terms that have a
functional dependence on   , we have

ln q(cid:1)(  ) = ln p(  ) + ew [ln p(w|  )] + const
= (a0     1) ln        b0   + m
2

ln          

2 e[wtw] + const.

(10.92)

we recognize this as the log of a gamma distribution, and so identifying the coef   -
cients of    and ln    we obtain

q(cid:1)(  ) = gam(  |an , bn )

where

an = a0 + m
2
1
2 e[wtw].

bn = b0 +

similarly, we can    nd the variational re-estimation equation for the posterior
distribution over w. again, using the general result (10.9), and keeping only those
terms that have a functional dependence on w, we have
ln q(cid:1)(w) = ln p(t|w) + e   [ln p(w|  )] + const

(10.96)

n(cid:2)
{wt  n     tn}2     1
(cid:10)
(cid:11)

n=1

=       
2
=    1
2

2 e[  ]wtw + const

wt

e[  ]i +     t  

w +   wt  tt + const.

because this is a quadratic form, the distribution q(cid:1)(w) is gaussian, and so we can
complete the square in the usual way to identify the mean and covariance, giving

q(cid:1)(w) = n (w|mn , sn)

(10.93)

(10.94)

(10.95)

(10.97)

(10.98)

(10.99)

488

10. approximate id136

where

(cid:10)

mn =   sn   tt
sn =

e[  ]i +     t  

(cid:11)   1

.

(10.100)
(10.101)

note the close similarity to the posterior distribution (3.52) obtained when    was
treated as a    xed parameter. the difference is that here    is replaced by its expecta-
tion e[  ] under the variational distribution. indeed, we have chosen to use the same
notation for the covariance matrix sn in both cases.

using the standard results (b.27), (b.38), and (b.39), we can obtain the required

moments as follows

e[  ] = an /bn
e[wwt] = mn mt

n + sn .

(10.102)
(10.103)

the evaluation of the variational posterior distribution begins by initializing the pa-
rameters of one of the distributions q(w) or q(  ), and then alternately re-estimates
these factors in turn until a suitable convergence criterion is satis   ed (usually speci-
   ed in terms of the lower bound to be discussed shortly).

it is instructive to relate the variational solution to that found using the evidence
framework in section 3.5. to do this consider the case a0 = b0 = 0, corresponding
to the limit of an in   nitely broad prior over   . the mean of the variational posterior
distribution q(  ) is then given by

e[  ] = an
bn

= m/2

e[wtw]/2

=

m

n mn + tr(sn) .

mt

(10.104)

comparison with (9.63) shows that in the case of this particularly simple model,
the variational approach gives precisely the same expression as that obtained by
maximizing the evidence function using em except that the point estimate for   
is replaced by its expected value. because the distribution q(w) depends on q(  )
only through the expectation e[  ], we see that the two approaches will give identical
results for the case of an in   nitely broad prior.

10.3.2 predictive distribution
the predictive distribution over t, given a new input x, is easily evaluated for

this model using the gaussian variational posterior for the parameters

(cid:6)
(cid:6)
(cid:6)

p(t|x, t) =
(cid:7)

p(t|x, w)p(w|t) dw
p(t|x, w)q(w) dw
n (t|wt  (x),   

n   (x),   2(x))

=
= n (t|mt

   1)n (w|mn , sn ) dw

(10.105)

10.3. variational id75

489

where we have evaluated the integral by making use of the result (2.115) for the
linear-gaussian model. here the input-dependent variance is given by

  2(x) =

1
  

+   (x)tsn   (x).

(10.106)

note that this takes the same form as the result (3.59) obtained with    xed    except
that now the expected value e[  ] appears in the de   nition of sn .

10.3.3 lower bound
another quantity of importance is the lower bound l de   ned by

l(q) = e[ln p(w,   , t)]     e[ln q(w,   )]
   e  [ln q(w)]w     e[ln q(  )].

= ew[ln p(t|w)] + ew,  [ln p(w|  )] + e  [ln p(  )]

(10.107)

exercise 10.27

evaluation of the various terms is straightforward, making use of results obtained in
previous chapters, and gives

(cid:16)

(cid:15)
(cid:8)
(cid:8)

e[ln p(t|w)]w = n
ln
2
      
2 tr
e[ln p(w|  )]w,   =     m
2
    an
2bn

  
2  

      
2

ttt +   mt

n   tt

(cid:9)

  t  (mn mt

ln(2  ) + m
2

n + sn )
(  (an )     ln bn )

(cid:9)

mt

n mn + tr(sn)

e[ln p(  )]   = a0 ln b0 + (a0     1) [  (an )     ln bn ]

(10.108)

(10.109)

(10.110)

    ln   (an )

   b0
an
bn
ln|sn| + m
1
2
2

[1 + ln(2  )]

   e[ln q(w)]w =
(10.111)
   e[ln q(  )]   = ln   (an )     (an     1)  (an )     ln bn + an . (10.112)
figure 10.9 shows a plot of the lower bound l(q) versus the degree of a polynomial
model for a synthetic data set generated from a degree three polynomial. here the
prior parameters have been set to a0 = b0 = 0, corresponding to the noninformative
prior p(  )     1/  , which is uniform over ln    as discussed in section 2.3.6. as
we saw in section 10.1, the quantity l represents lower bound on the log marginal
likelihood p(t|m) for the model. if we assign equal prior probabilities p(m) to the
different values of m, then we can interpret l as an approximation to the poste-
rior model id203 p(m|t). thus the variational framework assigns the highest
id203 to the model with m = 3. this should be contrasted with the maximum
likelihood result, which assigns ever smaller residual error to models of increasing
complexity until the residual error is driven to zero, causing maximum likelihood to
favour severely over-   tted models.

490

10. approximate id136

figure 10.9 plot of

the lower bound l ver-
sus the order m of the polyno-
mial, for a polynomial model, in
which a set of 10 data points is
generated from a polynomial with
m = 3 sampled over the inter-
val (   5, 5) with additive gaussian
noise of variance 0.09. the value
of the bound gives the log prob-
ability of the model, and we see
that the value of the bound peaks
at m = 3, corresponding to the
true model from which the data
set was generated.

1

3

5

7

9

10.4. exponential family distributions

in chapter 2, we discussed the important role played by the exponential family of
distributions and their conjugate priors. for many of the models discussed in this
book, the complete-data likelihood is drawn from the exponential family. however,
in general this will not be the case for the marginal likelihood function for the ob-
served data. for example, in a mixture of gaussians, the joint distribution of obser-
vations xn and corresponding hidden variables zn is a member of the exponential
family, whereas the marginal distribution of xn is a mixture of gaussians and hence
is not.

up to now we have grouped the variables in the model into observed variables
and hidden variables. we now make a further distinction between latent variables,
denoted z, and parameters, denoted   , where parameters are intensive (   xed in num-
ber independent of the size of the data set), whereas latent variables are extensive
(scale in number with the size of the data set). for example, in a gaussian mixture
model, the indicator variables zkn (which specify which component k is responsible
for generating data point xn) represent the latent variables, whereas the means   k,
precisions   k and mixing proportions   k represent the parameters.
consider the case of independent identically distributed data. we denote the
data values by x = {xn}, where n = 1, . . . n, with corresponding latent variables
z = {zn}. now suppose that the joint distribution of observed and latent variables
is a member of the exponential family, parameterized by natural parameters    so that

p(x, z|  ) =

h(xn, zn)g(  ) exp

  tu(xn, zn)

(cid:26)

(cid:26)

(cid:27)
(cid:27)

.

(10.113)

.

(10.114)

we shall also use a conjugate prior for   , which can be written as

p(  |  0, v0) = f(  0,   0)g(  )  0 exp

  o  t  0

recall that the conjugate prior distribution can be interpreted as a prior number   0
of observations all having the value   0 for the u vector. now consider a variational

n(cid:14)

n=1

(cid:27)

(cid:27)

(cid:27)

(cid:27)

10.4. exponential family distributions

491

distribution that factorizes between the latent variables and the parameters, so that
q(z,   ) = q(z)q(  ). using the general result (10.9), we can solve for the two
factors as follows

ln q(cid:1)(z) = e  [ln p(x, z|  )] + const

=

ln h(xn, zn) + e[  t]u(xn, zn)

+ const. (10.115)

n(cid:2)

(cid:26)

n=1

(cid:21)

thus we see that this decomposes into a sum of independent terms, one for each
value of n, and hence the solution for q(cid:1)(z) will factorize over n so that q(cid:1)(z) =
n q(cid:1)(zn). this is an example of an induced factorization. taking the exponential

section 10.2.5

of both sides, we have

(cid:26)

q(cid:1)(zn) = h(xn, zn)g (e[  ]) exp

e[  t]u(xn, zn)

(10.116)

where the id172 coef   cient has been re-instated by comparison with the
standard form for the exponential family.

similarly, for the variational distribution over the parameters, we have
ln q(cid:1)(  ) = ln p(  |  0,   0) + ez[ln p(x, z|  )] + const

(10.117)

=   0 ln g(  ) +   t  0 +

ln g(  ) +   t

ezn[u(xn, zn)]

+ const. (10.118)

n(cid:2)

(cid:26)

n=1

again, taking the exponential of both sides, and re-instating the id172 coef-
   cient by inspection, we have

(cid:26)

q(cid:1)(  ) = f(  n ,   n )g(  )  n exp

  t  n

where we have de   ned

  n =   0 + n

n(cid:2)

  n =   0 +

ezn[u(xn, zn)].

(10.119)

(10.120)

(10.121)

n=1

note that the solutions for q(cid:1)(zn) and q(cid:1)(  ) are coupled, and so we solve them iter-
atively in a two-stage procedure. in the variational e step, we evaluate the expected
suf   cient statistics e[u(xn, zn)] using the current posterior distribution q(zn) over
the latent variables and use this to compute a revised posterior distribution q(  ) over
the parameters. then in the subsequent variational m step, we use this revised pa-
rameter posterior distribution to    nd the expected natural parameters e[  t], which
gives rise to a revised variational distribution over the latent variables.

10.4.1 variational message passing
we have illustrated the application of variational methods by considering a spe-
ci   c model, the bayesian mixture of gaussians, in some detail. this model can be

492

10. approximate id136

described by the directed graph shown in figure 10.5. here we consider more gen-
erally the use of variational methods for models described by directed graphs and
derive a number of widely applicable results.

the joint distribution corresponding to a directed graph can be written using the

decomposition

p(x) =

p(xi|pai)

(10.122)

where xi denotes the variable(s) associated with node i, and pai denotes the parent
set corresponding to node i. note that xi may be a latent variable or it may belong
to the set of observed variables. now consider a variational approximation in which
the distribution q(x) is assumed to factorize with respect to the xi so that

q(x) =

qi(xi).

(10.123)

(cid:14)

i

(cid:14)

i

note that for observed nodes, there is no factor q(xi) in the variational distribution.
we now substitute (10.122) into our general result (10.9) to give

(cid:31)(cid:2)

 
ln p(xi|pai)

ln q(cid:1)

j (xj) = ei(cid:9)=j

+ const.

(10.124)

i

any terms on the right-hand side that do not depend on xj can be absorbed into
in fact, the only terms that do depend on xj are the con-
the additive constant.
ditional distribution for xj given by p(xj|paj) together with any other conditional
distributions that have xj in the conditioning set. by de   nition, these conditional
distributions correspond to the children of node j, and they therefore also depend on
the co-parents of the child nodes, i.e., the other parents of the child nodes besides
node xj itself. we see that the set of all nodes on which q(cid:1)(xj) depends corresponds
to the markov blanket of node xj, as illustrated in figure 8.26. thus the update
of the factors in the variational posterior distribution represents a local calculation
on the graph. this makes possible the construction of general purpose software for
variational id136 in which the form of the model does not need to be speci   ed in
advance (bishop et al., 2003).

if we now specialize to the case of a model in which all of the conditional dis-
tributions have a conjugate-exponential structure, then the variational update proce-
dure can be cast in terms of a local message passing algorithm (winn and bishop,
2005). in particular, the distribution associated with a particular node can be updated
once that node has received messages from all of its parents and all of its children.
this in turn requires that the children have already received messages from their co-
parents. the evaluation of the lower bound can also be simpli   ed because many of
the required quantities are already evaluated as part of the message passing scheme.
this distributed message passing formulation has good scaling properties and is well
suited to large networks.

10.5. local variational methods

493

10.5. local variational methods

the variational framework discussed in sections 10.1 and 10.2 can be considered a
   global    method in the sense that it directly seeks an approximation to the full poste-
rior distribution over all random variables. an alternative    local    approach involves
   nding bounds on functions over individual variables or groups of variables within
a model. for instance, we might seek a bound on a conditional distribution p(y|x),
which is itself just one factor in a much larger probabilistic model speci   ed by a
directed graph. the purpose of introducing the bound of course is to simplify the
resulting distribution. this local approximation can be applied to multiple variables
in turn until a tractable approximation is obtained, and in section 10.6.1 we shall
give a practical example of this approach in the context of id28. here
we focus on developing the bounds themselves.

we have already seen in our discussion of the id181 that
the convexity of the logarithm function played a key role in developing the lower
bound in the global variational approach. we have de   ned a (strictly) convex func-
tion as one for which every chord lies above the function. convexity also plays a
central role in the local variational framework. note that our discussion will ap-
ply equally to concave functions with    min    and    max    interchanged and with lower
bounds replaced by upper bounds.
let us begin by considering a simple example, namely the function f(x) =
exp(   x), which is a convex function of x, and which is shown in the left-hand plot
of figure 10.10. our goal is to approximate f(x) by a simpler function, in particular
a linear function of x. from figure 10.10, we see that this linear function will be a
lower bound on f(x) if it corresponds to a tangent. we can obtain the tangent line
y(x) at a speci   c value of x, say x =   , by making a    rst order taylor expansion

section 1.6.1

(10.125)
so that y(x) (cid:1) f(x) with equality when x =   . for our example function f(x) =

y(x) = f(  ) + f

(cid:4)(  )(x       )

figure 10.10 in the left-hand    g-
ure the red curve shows the function
exp(   x), and the blue line shows
the tangent at x =    de   ned by
(10.125) with    = 1. this line has
slope    = f(cid:1)(  ) =     exp(     ). note
that any other tangent line, for ex-
ample the ones shown in green, will
have a smaller value of y at x =
  . the right-hand    gure shows the
corresponding plot of
the function
         g(  ), where g(  ) is given by
(10.131), versus    for    = 1,
in
which the maximum corresponds to
   =     exp(     ) =    1/e.

1

0.5

0

0

  

1.5

x

3

0.4

         g(  )

0.2

0
   1

   0.5

  

0

494

10. approximate id136

y

f(x)

y

   g(  )

f(x)

x

  x

x

  x     g(  )

figure 10.11 in the left-hand plot the red curve shows a convex function f (x), and the blue line represents the
linear function   x, which is a lower bound on f (x) because f (x) >   x for all x. for the given value of slope    the
contact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy
(shown by the green dashed lines) given by f (x)       x. this de   nes the dual function g(  ), which corresponds
to the (negative of the) intercept of the tangent line having slope   .

exp(   x), we therefore obtain the tangent line in the form

y(x) = exp(     )     exp(     )(x       )

(10.126)

which is a linear function parameterized by   . for consistency with subsequent
discussion, let us de   ne    =     exp(     ) so that

y(x,   ) =   x        +    ln(     ).

different values of    correspond to different tangent lines, and because all such lines
are lower bounds on the function, we have f(x) (cid:2) y(x,   ). thus we can write the
function in the form

(10.127)

(10.128)

f(x) = max

  

{  x        +    ln(     )} .

we have succeeded in approximating the convex function f(x) by a simpler, lin-
ear function y(x,   ). the price we have paid is that we have introduced a variational
parameter   , and to obtain the tightest bound we must optimize with respect to   .

we can formulate this approach more generally using the framework of convex
duality (rockafellar, 1972; jordan et al., 1999). consider the illustration of a convex
function f(x) shown in the left-hand plot in figure 10.11.
in this example, the
function   x is a lower bound on f(x) but it is not the best lower bound that can
be achieved by a linear function having slope   , because the tightest bound is given
by the tangent line. let us write the equation of the tangent line, having slope    as
  x     g(  ) where the (negative) intercept g(  ) clearly depends on the slope    of the
tangent. to determine the intercept, we note that the line must be moved vertically by
an amount equal to the smallest vertical distance between the line and the function,
as shown in figure 10.11. thus

g(  ) =     min
x
= max

{f(x)       x}
{  x     f(x)} .

x

(10.129)

10.5. local variational methods

495

now, instead of    xing    and varying x, we can consider a particular x and then
adjust    until the tangent plane is tangent at that particular x. because the y value
of the tangent line at a particular x is maximized when that value coincides with its
contact point, we have

f(x) = max

  

{  x     g(  )} .

(10.130)

we see that the functions f(x) and g(  ) play a dual role, and are related through
(10.129) and (10.130).
let us apply these duality relations to our simple example f(x) = exp(   x).
from (10.129) we see that the maximizing value of x is given by    =     ln(     ), and
back-substituting we obtain the conjugate function g(  ) in the form

g(  ) =           ln(     )

(10.131)
as obtained previously. the function          g(  ) is shown, for    = 1 in the right-hand
plot in figure 10.10. as a check, we can substitute (10.131) into (10.130), which
gives the maximizing value of    =     exp(   x), and back-substituting then recovers
the original function f(x) = exp(   x).

for concave functions, we can follow a similar argument to obtain upper bounds,

in which max    is replaced with    min   , so that

f(x) = min
  
g(  ) = min
x

{  x     g(  )}
{  x     f(x)} .

(10.132)

(10.133)

if the function of interest is not convex (or concave), then we cannot directly
apply the method above to obtain a bound. however, we can    rst seek invertible
transformations either of the function or of its argument which change it into a con-
vex form. we then calculate the conjugate function and then transform back to the
original variables.

an important example, which arises frequently in pattern recognition, is the

logistic sigmoid function de   ned by

  (x) =

1

1 + e   x .

(10.134)

as it stands this function is neither convex nor concave. however, if we take the
logarithm we obtain a function which is concave, as is easily veri   ed by    nding the
second derivative. from (10.133) the corresponding conjugate function then takes
the form

g(  ) = min
x

{  x     f(x)} =       ln        (1       ) ln(1       )

(10.135)

which we recognize as the binary id178 function for a variable whose id203
of having the value 1 is   . using (10.132), we then obtain an upper bound on the log
sigmoid

ln   (x) (cid:1)   x     g(  )

(10.136)

exercise 10.30

appendix b

496

10. approximate id136

1

0.5

0
   6

   = 0.2

   = 0.7

0

1

0.5

   = 2.5

6

0
   6

     

0

  

6

figure 10.12 the left-hand plot shows the logistic sigmoid function   (x) de   ned by (10.134) in red, together
with two examples of the exponential upper bound (10.137) shown in blue. the right-hand plot shows the logistic
sigmoid again in red together with the gaussian lower bound (10.144) shown in blue. here the parameter
   = 2.5, and the bound is exact at x =    and x =      , denoted by the dashed green lines.

and taking the exponential, we obtain an upper bound on the logistic sigmoid itself
of the form

  (x) (cid:1) exp(  x     g(  ))

(10.137)

exercise 10.31

which is plotted for two values of    on the left-hand plot in figure 10.12.

we can also obtain a lower bound on the sigmoid having the functional form of
a gaussian. to do this, we follow jaakkola and jordan (2000) and make transforma-
tions both of the input variable and of the function itself. first we take the log of the
logistic function and then decompose it so that
   x) =     ln

ln   (x) =     ln(1 + e

   x/2)

(cid:26)

(cid:27)

= x/2     ln(ex/2 + e

(10.138)
we now note that the function f(x) =     ln(ex/2 + e
   x/2) is a convex function of
the variable x2, as can again be veri   ed by    nding the second derivative. this leads
to a lower bound on f(x), which is a linear function of x2 whose conjugate function
is given by

(cid:18)(cid:20)

(cid:19)

(cid:17)   

   x/2(ex/2 + e
e
   x/2).

g(  ) = max
x2

  x2     f

x2

(cid:18)

.

(10.139)

(10.140)

.

(cid:17)

x
2

the stationarity condition leads to
0 =        dx
dx2

d
dx

f(x) =    +

tanh

1
4x

(cid:29)

(cid:15)

(cid:16)

(cid:30)

if we denote this value of x, corresponding to the contact point of the tangent line
for this particular value of   , by   , then we have

  (  ) =     1
4  

tanh

  
2

=     1
2  

  (  )     1
2

.

(10.141)

10.5. local variational methods

497

instead of thinking of    as the variational parameter, we can let    play this role as
this leads to simpler expressions for the conjugate function, which is then given by

g(  ) =   (  )  2     f(  ) =   (  )  2 + ln(e  /2 + e

     /2).

hence the bound on f(x) can be written as

f(x) (cid:2)   x2     g(  ) =   x2         2     ln(e  /2 + e

     /2).

the bound on the sigmoid then becomes

  (x) (cid:2)   (  ) exp

(x       )/2       (  )(x2       2)

(cid:26)

(cid:27)

(10.142)

(10.143)

(10.144)

section 4.5

section 4.3

where   (  ) is de   ned by (10.141). this bound is illustrated in the right-hand plot of
figure 10.12. we see that the bound has the form of the exponential of a quadratic
function of x, which will prove useful when we seek gaussian representations of
posterior distributions de   ned through logistic sigmoid functions.

the logistic sigmoid arises frequently in probabilistic models over binary vari-
ables because it is the function that transforms a log odds ratio into a posterior prob-
ability. the corresponding transformation for a multiclass distribution is given by
the softmax function. unfortunately, the lower bound derived here for the logistic
sigmoid does not directly extend to the softmax. gibbs (1997) proposes a method
for constructing a gaussian distribution that is conjectured to be a bound (although
no rigorous proof is given), which may be used to apply local variational methods to
multiclass problems.

we shall see an example of the use of local variational bounds in sections 10.6.1.
for the moment, however, it is instructive to consider in general terms how these
bounds can be used. suppose we wish to evaluate an integral of the form

(cid:6)

i =

  (a)p(a) da

(10.145)

where   (a) is the logistic sigmoid, and p(a) is a gaussian id203 density. such
integrals arise in bayesian models when, for instance, we wish to evaluate the pre-
dictive distribution, in which case p(a) represents a posterior parameter distribution.
because the integral is intractable, we employ the variational bound (10.144), which
we write in the form   (a) (cid:2) f(a,   ) where    is a variational parameter. the inte-
gral now becomes the product of two exponential-quadratic functions and so can be
integrated analytically to give a bound on i

f(a,   )p(a) da = f (  ).

(10.146)

(cid:6)

i (cid:2)

we now have the freedom to choose the variational parameter   , which we do by
   nding the value   (cid:1) that maximizes the function f (  ). the resulting value f (  (cid:1))
represents the tightest bound within this family of bounds and can be used as an
approximation to i. this optimized bound, however, will in general not be exact.

498

10. approximate id136

although the bound   (a) (cid:2) f(a,   ) on the logistic sigmoid can be optimized exactly,
the required choice for    depends on the value of a, so that the bound is exact for one
value of a only. because the quantity f (  ) is obtained by integrating over all values
of a, the value of   (cid:1) represents a compromise, weighted by the distribution p(a).

10.6. variational id28

we now illustrate the use of local variational methods by returning to the bayesian
id28 model studied in section 4.5. there we focussed on the use of
the laplace approximation, while here we consider a variational treatment based on
the approach of jaakkola and jordan (2000). like the laplace method, this also
leads to a gaussian approximation to the posterior distribution. however, the greater
   exibility of the variational approximation leads to improved accuracy compared
to the laplace method. furthermore (unlike the laplace method), the variational
approach is optimizing a well de   ned objective function given by a rigourous bound
on the model evidence. id28 has also been treated by dybowski and
roberts (2005) from a bayesian perspective using monte carlo sampling techniques.

10.6.1 variational posterior distribution
here we shall make use of a variational approximation based on the local bounds
introduced in section 10.5. this allows the likelihood function for logistic regres-
sion, which is governed by the logistic sigmoid, to be approximated by the expo-
nential of a quadratic form. it is therefore again convenient to choose a conjugate
gaussian prior of the form (4.140). for the moment, we shall treat the hyperparam-
eters m0 and s0 as    xed constants. in section 10.6.3, we shall demonstrate how the
variational formalism can be extended to the case where there are unknown hyper-
parameters whose values are to be inferred from the data.

in the variational framework, we seek to maximize a lower bound on the marginal
likelihood. for the bayesian id28 model, the marginal likelihood takes
the form

(cid:6) (cid:31)

n(cid:14)

p(tn|w)

p(w) dw.

(10.147)

(cid:6)

p(t) =

we    rst note that the conditional distribution for t can be written as

 

1

p(t|w)p(w) dw =
(cid:15)

p(t|w) =   (a)t {1       (a)}1   t
1    

=

1

n=1

(cid:16)t(cid:15)

1 + e   a

1 + e   a
   a
1 + e   a = eat  (   a)
e

= eat

(cid:16)1   t

where a = wt  . in order to obtain a lower bound on p(t), we make use of the
variational lower bound on the logistic sigmoid function given by (10.144), which

(10.148)

10.6. variational id28

499

(cid:27)

we reproduce here for convenience
  (z) (cid:2)   (  ) exp

(cid:26)

where

  (  ) =

we can therefore write

p(t|w) = eat  (   a) (cid:2) eat  (  ) exp

(cid:30)

(z       )/2       (  )(z2       2)

(cid:29)
(cid:26)   (a +   )/2       (  )(a2       2)
(cid:27)

  (  )     1
2

1
2  

.

(10.149)

(10.150)

.

(10.151)

note that because this bound is applied to each of the terms in the likelihood function
separately, there is a variational parameter   n corresponding to each training set
observation (  n, tn). using a = wt  , and multiplying by the prior distribution, we
obtain the following bound on the joint distribution of t and w
p(t, w) = p(t|w)p(w) (cid:2) h(w,   )p(w)
where    denotes the set {  n} of variational parameters, and
n(cid:14)
      (  n)([wt  n]2       2
n)

wt  ntn     (wt  n +   n)/2

h(w,   ) =

  (  n) exp

(10.152)

(10.153)

(cid:26)

(cid:27)

n=1

.

evaluation of the exact posterior distribution would require id172 of the left-
hand side of this inequality. because this is intractable, we work instead with the
right-hand side. note that the function on the right-hand side cannot be interpreted
as a id203 density because it is not normalized. once it is normalized to give a
variational posterior distribution q(w), however, it no longer represents a bound.
because the logarithm function is monotonically increasing, the inequality a (cid:2)
b implies ln a (cid:2) ln b. this gives a lower bound on the log of the joint distribution
of t and w of the form

ln{p(t|w)p(w)} (cid:2) ln p(w) +

ln   (  n) + wt  ntn

    (wt  n +   n)/2       (  n)([wt  n]2       2
n)

.

(10.154)

(cid:27)

n(cid:2)

(cid:26)

n=1

substituting for the prior p(w), the right-hand side of this inequality becomes, as a
function of w

n(cid:2)

n=1

+

(cid:26)

(w     m0)ts

0 (w     m0)
   1

   1
2
wt  n(tn     1/2)       (  n)wt(  n  t

n)w

(cid:27)

+ const.

(10.155)

500

10. approximate id136

this is a quadratic function of w, and so we can obtain the corresponding variational
approximation to the posterior distribution by identifying the linear and quadratic
terms in w, giving a gaussian variational posterior of the form

where

q(w) = n (w|mn , sn )

(cid:22)

n(cid:2)

mn = sn

s

   1
n = s

   1
0 + 2

s

(tn     1/2)  n

   1
0 m0 +

n(cid:2)

n=1

  (  n)  n  t
n.

(cid:23)

(10.156)

(10.157)

(10.158)

n=1

as with the laplace framework, we have again obtained a gaussian approximation
to the posterior distribution. however, the additional    exibility provided by the vari-
ational parameters {  n} leads to improved accuracy in the approximation (jaakkola
and jordan, 2000).

here we have considered a batch learning context in which all of the training
data is available at once. however, bayesian methods are intrinsically well suited
to sequential learning in which the data points are processed one at a time and then
discarded. the formulation of this variational approach for the sequential case is
straightforward.

note that the bound given by (10.149) applies only to the two-class problem and
so this approach does not directly generalize to classi   cation problems with k > 2
classes. an alternative bound for the multiclass case has been explored by gibbs
(1997).

exercise 10.32

10.6.2 optimizing the variational parameters
we now have a normalized gaussian approximation to the posterior distribution,
which we shall use shortly to evaluate the predictive distribution for new data points.
first, however, we need to determine the variational parameters {  n} by maximizing
the lower bound on the marginal likelihood.

to do this, we substitute the inequality (10.152) back into the marginal likeli-

(cid:6)

hood to give

ln p(t) = ln

(cid:6)

p(t|w)p(w) dw (cid:2) ln

h(w,   )p(w) dw = l(  ).

(10.159)

as with the optimization of the hyperparameter    in the id75 model of
section 3.5, there are two approaches to determining the   n. in the    rst approach, we
recognize that the function l(  ) is de   ned by an integration over w and so we can
view w as a latent variable and invoke the em algorithm. in the second approach,
we integrate over w analytically and then perform a direct maximization over   . let
us begin by considering the em approach.
{  n}, which we denote collectively by   old.

the em algorithm starts by choosing some initial values for the parameters
in the e step of the em algorithm,

(cid:27)

n(cid:2)

(cid:26)

10.6. variational id28

501

we then use these parameter values to    nd the posterior distribution over w, which
is given by (10.156). in the m step, we then maximize the expected complete-data
log likelihood which is given by

q(  ,   old) = e [ln h(w,   )p(w)]

(10.160)

where the expectation is taken with respect to the posterior distribution q(w) evalu-
ated using   old. noting that p(w) does not depend on   , and substituting for h(w,   )
we obtain

q(  ,   old) =

ln   (  n)       n/2       (  n)(  t

n e[wwt]  n       2
n)

+ const

n=1

(10.161)
where    const    denotes terms that are independent of   . we now set the derivative with
respect to   n equal to zero. a few lines of algebra, making use of the de   nitions of
  (  ) and   (  ), then gives

(cid:4)(  n)(  t

n e[wwt]  n       2
n).

0 =   

(10.162)
(cid:4)(  ) is a monotonic function of    for    (cid:2) 0, and that we can
we now note that   
restrict attention to nonnegative values of    without loss of generality due to the
(cid:4)(  ) (cid:9)= 0, and hence we obtain the
symmetry of the bound around    = 0. thus   
following re-estimation equations

(cid:10)

(cid:11)

(  new

n )2 =   t

n e[wwt]  n =   t
n

sn + mn mt
n

  n

(10.163)

where we have used (10.156).

let us summarize the em algorithm for    nding the variational posterior distri-
bution. we    rst initialize the variational parameters   old. in the e step, we evaluate
the posterior distribution over w given by (10.156), in which the mean and covari-
ance are de   ned by (10.157) and (10.158). in the m step, we then use this variational
posterior to compute a new value for    given by (10.163). the e and m steps are
repeated until a suitable convergence criterion is satis   ed, which in practice typically
requires only a few iterations.
an alternative approach to obtaining re-estimation equations for    is to note
that in the integral over w in the de   nition (10.159) of the lower bound l(  ), the
integrand has a gaussian-like form and so the integral can be evaluated analytically.
having evaluated the integral, we can then differentiate with respect to   n. it turns
out that this gives rise to exactly the same re-estimation equations as does the em
approach given by (10.163).
as we have emphasized already, in the application of variational methods it is
useful to be able to evaluate the lower bound l(  ) given by (10.159). the integration
over w can be performed analytically by noting that p(w) is gaussian and h(w,   )
is the exponential of a quadratic function of w. thus, by completing the square
and making use of the standard result for the id172 coef   cient of a gaussian
distribution, we can obtain a closed form solution which takes the form

exercise 10.33

exercise 10.34

exercise 10.35

502

10. approximate id136

6

4

2

0

   2

   4

   6

   4

6

4

2

0

   2

   4

   6

   4

   2

0

2

4

5
2
.
0

.

50
7
9
.
9
0

1
0.0

   2

0

2

4

figure 10.13 illustration of the bayesian approach to id28 for a simple linearly separable data
set. the plot on the left shows the predictive distribution obtained using variational id136. we see that
the decision boundary lies roughly mid way between the clusters of data points, and that the contours of the
predictive distribution splay out away from the data re   ecting the greater uncertainty in the classi   cation of such
regions. the plot on the right shows the decision boundaries corresponding to    ve samples of the parameter
vector w drawn from the posterior distribution p(w|t).

l(  ) =

1
2

+

ln

mt

|sn|
(cid:12)
|s0|     1
n(cid:2)
ln   (  n)     1

n s

2

mt

0 s

   1
0 m0

(cid:13)

1
2

   1
n mn +
2   n       (  n)  2

n

n=1

.

(10.164)

this variational framework can also be applied to situations in which the data
is arriving sequentially (jaakkola and jordan, 2000).
in this case we maintain a
gaussian posterior distribution over w, which is initialized using the prior p(w). as
each data point arrives, the posterior is updated by making use of the bound (10.151)
and then normalized to give an updated posterior distribution.

the predictive distribution is obtained by marginalizing over the posterior dis-
tribution, and takes the same form as for the laplace approximation discussed in
section 4.5.2. figure 10.13 shows the variational predictive distributions for a syn-
thetic data set. this example provides interesting insights into the concept of    large
margin   , which was discussed in section 7.1 and which has qualitatively similar be-
haviour to the bayesian solution.

10.6.3 id136 of hyperparameters
so far, we have treated the hyperparameter    in the prior distribution as a known
constant. we now extend the bayesian id28 model to allow the value of
this parameter to be inferred from the data set. this can be achieved by combining
the global and local variational approximations into a single framework, so as to
maintain a lower bound on the marginal likelihood at each stage. such a combined
approach was adopted by bishop and svens  en (2003) in the context of a bayesian
treatment of the hierarchical mixture of experts model.

10.6. variational id28

503

speci   cally, we consider once again a simple isotropic gaussian prior distribu-

tion of the form

p(w|  ) = n (w|0,   

   1i).

(10.165)
our analysis is readily extended to more general gaussian priors, for instance if we
wish to associate a different hyperparameter with different subsets of the parame-
ters wj. as usual, we consider a conjugate hyperprior over    given by a gamma
distribution

p(  ) = gam(  |a0, b0)

(10.166)

governed by the constants a0 and b0.

the marginal likelihood for this model now takes the form

(cid:6)(cid:6)

p(t) =

p(w,   , t) dw d  

(10.167)

where the joint distribution is given by

p(w,   , t) = p(t|w)p(w|  )p(  ).

(10.168)

we are now faced with an analytically intractable integration over w and   , which
we shall tackle by using both the local and global variational approaches in the same
model

to begin with, we introduce a variational distribution q(w,   ), and then apply

the decomposition (10.2), which in this instance takes the form

(10.169)
where the lower bound l(q) and the id181 kl(q(cid:5)p) are de-
   ned by

ln p(t) = l(q) + kl(q(cid:5)p)
(cid:6)(cid:6)
(cid:6)(cid:6)

q(w,   ) ln

(cid:12)

(cid:13)
p(w,   , t)
q(w,   )
p(w,   |t))
q(w,   )

(cid:12)

q(w,   ) ln

(cid:13)

dw d  

kl(q(cid:5)p) =    

(10.171)
at this point, the lower bound l(q) is still intractable due to the form of the
likelihood factor p(t|w). we therefore apply the local variational bound to each of
the logistic sigmoid factors as before. this allows us to use the inequality (10.152)
and place a lower bound on l(q), which will therefore also be a lower bound on the
log marginal likelihood

dw d  .

l(q) =

(10.170)

ln p(t) (cid:2) l(q) (cid:2)(cid:4)l(q,   )

(cid:6)(cid:6)

(cid:12)

=

q(w,   ) ln

(cid:13)

h(w,   )p(w|  )p(  )

q(w,   )

dw d  .

(10.172)

next we assume that the variational distribution factorizes between parameters and
hyperparameters so that

q(w,   ) = q(w)q(  ).

(10.173)

504

10. approximate id136

with this factorization we can appeal to the general result (10.9) to    nd expressions
for the optimal factors. consider    rst the distribution q(w). discarding terms that
are independent of w, we have

ln q(w) = e   [ln{h(w,   )p(w|  )p(  )}] + const
= ln h(w,   ) + e   [ln p(w|  )] + const.

we now substitute for ln h(w,   ) using (10.153), and for ln p(w|  ) using (10.165),
giving

n(cid:2)

(cid:26)

n=1

(cid:27)

(10.174)

(10.175)

(10.176)

ln q(w) =     e[  ]
2

wtw +

(tn     1/2)wt  n       (  n)wt  n  t
nw

+ const.

we see that this is a quadratic function of w and so the solution for q(w) will be
gaussian. completing the square in the usual way, we obtain

where we have de   ned

   1
n   n =

  

q(w) = n (w|  n ,   n )

n(cid:2)

n=1

(tn     1/2)  n
n(cid:2)

   1
n = e[  ]i + 2

  

  (  n)  n  t
n.

similarly, the optimal solution for the factor q(  ) is obtained from

n=1

ln q(  ) = ew [ln p(w|  )] + ln p(  ) + const.

(cid:8)

(cid:9)

substituting for ln p(w|  ) using (10.165), and for ln p(  ) using (10.166), we obtain

ln q(  ) = m
2

ln          
2 e

wtw

+ (a0     1) ln        b0   + const.

we recognize this as the log of a gamma distribution, and so we obtain

q(  ) = gam(  |an , bn) =

where

0   a0   1e

   b0  

1
  (a0) ab0
(cid:8)

an = a0 + m
2
1
2 ew

bn = b0 +

(cid:9)

.

wtw

(10.177)

(10.178)

(10.179)

10.7. expectation propagation

505

maximizing the lower bound(cid:4)l(q,   ). omitting terms that are independent of   , and

we also need to optimize the variational parameters   n, and this is also done by

integrating over   , we have

(cid:6)

(cid:4)l(q,   ) =

q(w) ln h(w,   ) dw + const.

(10.180)

note that this has precisely the same form as (10.159), and so we can again appeal
to our earlier result (10.163), which can be obtained by direct optimization of the
marginal likelihood function, leading to re-estimation equations of the form

(cid:10)

(cid:11)

(  new

n )2 =   t
n

  n +   n   t
n

  n.

(10.181)

we have obtained re-estimation equations for the three quantities q(w), q(  ),
and   , and so after making suitable initializations, we can cycle through these quan-
tities, updating each in turn. the required moments are given by

(cid:8)

e

(cid:9)

e [  ] = an
bn
wtw

=   n +   t

n   n .

(10.182)

(10.183)

appendix b

10.7. expectation propagation

we conclude this chapter by discussing an alternative form of deterministic approx-
imate id136, known as expectation propagation or ep (minka, 2001a; minka,
2001b). as with the id58 methods discussed so far, this too is based
on the minimization of a id181 but now of the reverse form,
which gives the approximation rather different properties.
consider for a moment the problem of minimizing kl(p(cid:5)q) with respect to q(z)
when p(z) is a    xed distribution and q(z) is a member of the exponential family and
so, from (2.194), can be written in the form

(cid:26)

(cid:27)

q(z) = h(z)g(  ) exp

  tu(z)

.

(10.184)

as a function of   , the id181 then becomes

kl(p(cid:5)q) =     ln g(  )       t

ep(z)[u(z)] + const

(10.185)

where the constant terms are independent of the natural parameters   . we can mini-
mize kl(p(cid:5)q) within this family of distributions by setting the gradient with respect
to    to zero, giving

(10.186)
however, we have already seen in (2.226) that the negative gradient of ln g(  ) is
given by the expectation of u(z) under the distribution q(z). equating these two
results, we obtain

       ln g(  ) = ep(z)[u(z)].

eq(z)[u(z)] = ep(z)[u(z)].

(10.187)

506

10. approximate id136

we see that the optimum solution simply corresponds to matching the expected suf-
   cient statistics. so, for instance, if q(z) is a gaussian n (z|  ,   ) then we minimize
the id181 by setting the mean    of q(z) equal to the mean of
the distribution p(z) and the covariance    equal to the covariance of p(z). this is
sometimes called moment matching. an example of this was seen in figure 10.3(a).
now let us exploit this result to obtain a practical algorithm for approximate
id136. for many probabilistic models, the joint distribution of data d and hidden
variables (including parameters)    comprises a product of factors in the form

p(d,   ) =

fi(  ).

(10.188)

(cid:14)

i

this would arise, for example, in a model for independent, identically distributed
data in which there is one factor fn(  ) = p(xn|  ) for each data point xn, along
with a factor f0(  ) = p(  ) corresponding to the prior. more generally, it would also
apply to any model de   ned by a directed probabilistic graph in which each factor is a
conditional distribution corresponding to one of the nodes, or an undirected graph in
which each factor is a clique potential. we are interested in evaluating the posterior
distribution p(  |d) for the purpose of making predictions, as well as the model
evidence p(d) for the purpose of model comparison. from (10.188) the posterior is
given by

p(  |d) =

fi(  )

(10.189)

and the model evidence is given by
p(d) =

fi(  ) d  .

(10.190)

(cid:14)

i

1
p(d)

(cid:6) (cid:14)

i

here we are considering continuous variables, but the following discussion applies
equally to discrete variables with integrals replaced by summations. we shall sup-
pose that the marginalization over   , along with the marginalizations with respect to
the posterior distribution required to make predictions, are intractable so that some
form of approximation is required.

expectation propagation is based on an approximation to the posterior distribu-

tion which is also given by a product of factors

q(  ) =

in which each factor(cid:4)fi(  ) in the approximation corresponds to one of the factors
obtain a practical algorithm, we need to constrain the factors(cid:4)fi(  ) in some way,

fi(  ) in the true posterior (10.189), and the factor 1/z is the normalizing constant
needed to ensure that the left-hand side of (10.191) integrates to unity. in order to

i

(10.191)

and in particular we shall assume that they come from the exponential family. the
product of the factors will therefore also be from the exponential family and so can

(cid:14)

(cid:4)fi(  )

1
z

10.7. expectation propagation

be described by a    nite set of suf   cient statistics. for example, if each of the(cid:4)fi(  )
ideally we would like to determine the(cid:4)fi(  ) by minimizing the kullback-leibler

is a gaussian, then the overall approximation q(  ) will also be gaussian.

507

divergence between the true posterior and the approximation given by

(cid:22)

(cid:14)

i

                1

z

(cid:14)

i

(cid:23)
(cid:4)fi(  )

kl (p(cid:5)q) = kl

1
p(d)

fi(  )

.

(10.192)

note that this is the reverse form of kl divergence compared with that used in varia-
tional id136. in general, this minimization will be intractable because the kl di-
vergence involves averaging with respect to the true distribution. as a rough approx-
imation, we could instead minimize the kl divergences between the corresponding

pairs fi(  ) and(cid:4)fi(  ) of factors. this represents a much simpler problem to solve,

and has the advantage that the algorithm is noniterative. however, because each fac-
tor is individually approximated, the product of the factors could well give a poor
approximation.

this is similar in spirit to the update of factors in the id58 framework

expectation propagation makes a much better approximation by optimizing each
factor in turn in the context of all of the remaining factors. it starts by initializing

the factors(cid:4)fi(  ), and then cycles through the factors re   ning them one at a time.
considered earlier. suppose we wish to re   ne factor(cid:4)fj(  ). we    rst remove this
(cid:4)fi(  ). conceptually, we will now determine a
revised form of the factor(cid:4)fj(  ) by ensuring that the product
(cid:4)fi(  )

factor from the product to give

(10.193)

i(cid:9)=j

(cid:21)
(cid:14)
qnew(  )    (cid:4)fj(  )
(cid:14)
(cid:4)fi(  )

i(cid:9)=j

is as close as possible to

fj(  )

in which we keep    xed all of the factors(cid:4)fi(  ) for i (cid:9)= j. this ensures that the
to the    clutter problem   . to achieve this, we    rst remove the factor(cid:4)fj(  ) from the

approximation is most accurate in the regions of high posterior id203 as de   ned
by the remaining factors. we shall see an example of this effect when we apply ep

(10.194)

i(cid:9)=j

current approximation to the posterior by de   ning the unnormalized distribution

.

(10.195)

\j(  ) from the product of factors i (cid:9)= j, although
note that we could instead    nd q
in practice division is usually easier. this is now combined with the factor fj(  ) to
give a distribution

(10.196)

\j(  ) = q(  )(cid:4)fj(  )

q

fj(  )q

\j(  )

1
zj

section 10.7.1

508

10. approximate id136

1

0.8

0.6

0.4

0.2

0
   2

   1

0

1

2

3

4

40

30

20

10

0
   2

   1

0

1

2

3

4

figure 10.14 illustration of the expectation propagation approximation using a gaussian distribution for the
example considered earlier in figures 4.14 and 10.1. the left-hand plot shows the original distribution (yellow)
along with the laplace (red), global variational (green), and ep (blue) approximations, and the right-hand plot
shows the corresponding negative logarithms of the distributions. note that the ep distribution is broader than
that variational id136, as a consequence of the different form of kl divergence.

(cid:6)

(cid:15)

where zj is the id172 constant given by

we now determine a revised factor(cid:4)fj(  ) by minimizing the kullback-leibler diver-

(10.197)

fj(  )q

zj =

gence

kl

\j(  )
fj(  )q
zj

.

(10.198)

\j(  ) d  .

             qnew(  )
(cid:16)

this is easily solved because the approximating distribution qnew(  ) is from the ex-
ponential family, and so we can appeal to the result (10.187), which tells us that the
parameters of qnew(  ) are obtained by matching its expected suf   cient statistics to
the corresponding moments of (10.196). we shall assume that this is a tractable oper-
ation. for example, if we choose q(  ) to be a gaussian distribution n (  |  ,   ), then
\j(  ), and    is
   is set equal to the mean of the (unnormalized) distribution fj(  )q
set to its covariance. more generally, it is straightforward to obtain the required ex-
pectations for any member of the exponential family, provided it can be normalized,
because the expected statistics can be related to the derivatives of the id172
coef   cient, as given by (2.226). the ep approximation is illustrated in figure 10.14.

from (10.193), we see that the revised factor (cid:4)fj(  ) can be found by taking

qnew(  ) and dividing out the remaining factors so that

(cid:4)fj(  ) = k

qnew(  )
q\j(  )

(10.199)

where we have used (10.195). the coef   cient k is determined by multiplying both

10.7. expectation propagation

509

(cid:6) (cid:4)fj(  )q
(cid:6)

sides of (10.199) by q

\i(  ) and integrating to give

k =

\j(  ) d  

(10.200)

where we have used the fact that qnew(  ) is normalized. the value of k can therefore
be found by matching zeroth-order moments
\j(  ) d   =

(cid:6) (cid:4)fj(  )q

\j(  ) d  .

fj(  )q

(10.201)

combining this with (10.197), we then see that k = zj and so can be found by
evaluating the integral in (10.197).
in practice, several passes are made through the set of factors, revising each
factor in turn. the posterior distribution p(  |d) is then approximated using (10.191),
and the model evidence p(d) can be approximated by using (10.190) with the factors

fi(  ) replaced by their approximations(cid:4)fi(  ).

expectation propagation
we are given a joint distribution over observed data d and stochastic variables
   in the form of a product of factors

p(d,   ) =

fi(  )

(10.202)

and we wish to approximate the posterior distribution p(  |d) by a distribution
of the form

(cid:14)
(cid:14)

i

1
z

q(  ) =

we also wish to approximate the model evidence p(d).

(cid:4)fi(  ).
1. initialize all of the approximating factors(cid:4)fi(  ).
(cid:4)fi(  ).

2. initialize the posterior approximation by setting

q(  )    

(cid:14)

i

i

3. until convergence:

(a) choose a factor(cid:4)fj(  ) to re   ne.
(b) remove(cid:4)fj(  ) from the posterior by division
\j(  ) = q(  )(cid:4)fj(  )

q

.

(10.203)

(10.204)

(10.205)

510

10. approximate id136

(c) evaluate the new posterior by setting the suf   cient statistics (moments)
\j(  )fj(  ), including evaluation of the

of qnew(  ) equal to those of q
id172 constant

(cid:6)

zj =

\j(  )fj(  ) d  .

q

(10.206)

(d) evaluate and store the new factor

qnew(  )
q\j(  ) .
4. evaluate the approximation to the model evidence

(cid:4)fj(  ) = zj
(cid:6) (cid:14)

p(d) (cid:7)

(cid:4)fi(  ) d  .

(10.207)

(10.208)

i

a special case of ep, known as assumed density    ltering (adf) or moment
matching (maybeck, 1982; lauritzen, 1992; boyen and koller, 1998; opper and
winther, 1999), is obtained by initializing all of the approximating factors except
the    rst to unity and then making one pass through the factors updating each of them
once. assumed density    ltering can be appropriate for on-line learning in which data
points are arriving in a sequence and we need to learn from each data point and then
discard it before considering the next point. however, in a batch setting we have the
opportunity to re-use the data points many times in order to achieve improved ac-
curacy, and it is this idea that is exploited in expectation propagation. furthermore,
if we apply adf to batch data, the results will have an undesirable dependence on
the (arbitrary) order in which the data points are considered, which again ep can
overcome.

one disadvantage of expectation propagation is that there is no guarantee that
the iterations will converge. however, for approximations q(  ) in the exponential
family, if the iterations do converge, the resulting solution will be a stationary point
of a particular energy function (minka, 2001a), although each iteration of ep does
not necessarily decrease the value of this energy function. this is in contrast to
id58, which iteratively maximizes a lower bound on the log marginal
likelihood, in which each iteration is guaranteed not to decrease the bound. it is
possible to optimize the ep cost function directly, in which case it is guaranteed
to converge, although the resulting algorithms can be slower and more complex to
implement.

another difference between id58 and ep arises from the form of
kl divergence that is minimized by the two algorithms, because the former mini-
mizes kl(q(cid:5)p) whereas the latter minimizes kl(p(cid:5)q). as we saw in figure 10.3,
for distributions p(  ) which are multimodal, minimizing kl(p(cid:5)q) can lead to poor
approximations. in particular, if ep is applied to mixtures the results are not sen-
sible because the approximation tries to capture all of the modes of the posterior
distribution. conversely, in logistic-type models, ep often out-performs both local
variational methods and the laplace approximation (kuss and rasmussen, 2006).

10.7. expectation propagation

511

figure 10.15 illustration of the clutter problem
for a data space dimensionality of
d = 1. training data points, de-
noted by the crosses, are drawn
from a mixture of two gaussians
with components shown in red
and green. the goal is to infer the
mean of the green gaussian from
the observed data.

   5

0

  

5

x

10

10.7.1 example: the clutter problem
following minka (2001b), we illustrate the ep algorithm using a simple exam-
ple in which the goal is to infer the mean    of a multivariate gaussian distribution
over a variable x given a set of observations drawn from that distribution. to make
the problem more interesting, the observations are embedded in background clutter,
which itself is also gaussian distributed, as illustrated in figure 10.15. the distribu-
tion of observed values x is therefore a mixture of gaussians, which we take to be
of the form

p(x|  ) = (1     w)n (x|  , i) + wn (x|0, ai)

(10.209)

where w is the proportion of background clutter and is assumed to be known. the
prior over    is taken to be gaussian

p(  ) = n (  |0, bi)

(10.210)

n(cid:14)

and minka (2001a) chooses the parameter values a = 10, b = 100 and w = 0.5.
the joint distribution of n observations d = {x1, . . . , xn} and    is given by

p(d,   ) = p(  )

p(xn|  )

(10.211)

n=1

and so the posterior distribution comprises a mixture of 2n gaussians. thus the
computational cost of solving this problem exactly would grow exponentially with
the size of the data set, and so an exact solution is intractable for moderately large
n.
to apply ep to the clutter problem, we    rst identify the factors f0(  ) = p(  )
and fn(  ) = p(xn|  ). next we select an approximating distribution from the expo-
nential family, and for this example it is convenient to choose a spherical gaussian

q(  ) = n (  |m, vi).

(10.212)

512

10. approximate id136

(10.213)

(cid:4)fn(  ) = snn (  |mn, vni)

the factor approximations will therefore take the form of exponential-quadratic
functions of the form

where n = 1, . . . , n, and we set(cid:4)f0(  ) equal to the prior p(  ). note that the use of
convenient shorthand notation. the approximations(cid:4)fn(  ), for n = 1, . . . , n, can

n (  |  ,  ) does not imply that the right-hand side is a well-de   ned gaussian density
(in fact, as we shall see, the variance parameter vn can be negative) but is simply a
be initialized to unity, corresponding to sn = (2  vn)d/2, vn         and mn = 0,
where d is the dimensionality of x and hence of   . the initial q(  ), de   ned by
(10.191), is therefore equal to the prior.

we then iteratively re   ne the factors by taking one factor fn(  ) at a time and
applying (10.205), (10.206), and (10.207). note that we do not need to revise the
term f0(  ) because an ep update will leave this term unchanged. here we state the
results and leave the reader to    ll in the details.

first we remove the current estimate(cid:4)fn(  ) from q(  ) by division using (10.205)

\n(  ), which has mean and inverse variance given by
n (m     mn)
\nv
m\n = m + v
   1
   1     v
   1
\n)   1 = v
n .

(v

next we evaluate the id172 constant zn using (10.206) to give

zn = (1     w)n (xn|m\n, (v

\n + 1)i) + wn (xn|0, ai).

(10.214)
(10.215)

(10.216)

exercise 10.37

exercise 10.38

to give q

exercise 10.39

similarly, we compute the mean and variance of qnew(  ) by    nding the mean and
variance of q

\n(  )fn(  ) to give

\n

v

m = m\n +   n
v\n + 1
\n)2
\n       n
(v
v\n + 1

v = v

(xn     m\n)
+   n(1       n)

(v

\n)2(cid:5)xn     m\n(cid:5)2
d(v\n + 1)2

(10.217)

(10.218)

where the quantity

we use (10.207) to compute the re   ned factor(cid:4)fn(  ) whose parameters are given by

has a simple interpretation as the id203 of the point xn not being clutter. then

(10.219)

  n = 1     w
zn

n (xn|0, ai)

n = (vnew)   1     (v
   1
v
mn = m\n + (vn + v
sn =

\n)   1
\n)(v
zn

(2  vn)d/2n (mn|m\n, (vn + v\n)i) .

\n)   1(mnew     m\n)

(10.220)
(10.221)

(10.222)

this re   nement process is repeated until a suitable termination criterion is satis   ed,
for instance that the maximum change in parameter values resulting from a complete

10.7. expectation propagation

513

   5

0

5

  

10

   5

0

5

  

10

figure 10.16 examples of the approximation of speci   c factors for a one-dimensional version of the clutter
problem, showing fn(  ) in blue, efn(  ) in red, and q\n(  ) in green. notice that the current form for q\n(  ) controls
the range of    over which efn(  ) will be a good approximation to fn(  ).

pass through all factors is less than some threshold. finally, we use (10.208) to
evaluate the approximation to the model evidence, given by

p(d) (cid:7) (2  vnew)d/2 exp(b/2)

sn(2  vn)   d/2

where

b =

(mnew)tmnew

v

mt
nmn
vn

.

n(cid:14)
(cid:26)
    n(cid:2)

n=1

n=1

(cid:27)

(10.223)

(10.224)

examples factor approximations for the clutter problem with a one-dimensional pa-
rameter space    are shown in figure 10.16. note that the factor approximations can
have in   nite or even negative values for the    variance    parameter vn. this simply
corresponds to approximations that curve upwards instead of downwards and are not
necessarily problematic provided the overall approximate posterior q(  ) has posi-
tive variance. figure 10.17 compares the performance of ep with id58
(mean    eld theory) and the laplace approximation on the clutter problem.

10.7.2 expectation propagation on graphs
so far in our general discussion of ep, we have allowed the factors fi(  ) in the
distribution p(  ) to be functions of all of the components of   , and similarly for the

approximating factors(cid:4)f(  ) in the approximating distribution q(  ). we now consider

situations in which the factors depend only on subsets of the variables. such restric-
tions can be conveniently expressed using the framework of probabilistic graphical
models, as discussed in chapter 8. here we use a factor graph representation because
this encompasses both directed and undirected graphs.

514

10. approximate id136

posterior mean

100

r
o
r
r

e

10   5

laplace

vb

104

ep

106

flops

evidence

vb

10   200

r
o
r
r

e

10   202

laplace

10   204

104

106

flops

ep

figure 10.17 comparison of expectation propagation, variational id136, and the laplace approximation on
the clutter problem. the left-hand plot shows the error in the predicted posterior mean versus the number of
   oating point operations, and the right-hand plot shows the corresponding results for the model evidence.

we shall focus on the case in which the approximating distribution is fully fac-
torized, and we shall show that in this case expectation propagation reduces to loopy
belief propagation (minka, 2001a). to start with, we show this in the context of a
simple example, and then we shall explore the general case.
first of all, recall from (10.17) that if we minimize the kullback-leibler diver-
gence kl(p(cid:5)q) with respect to a factorized distribution q, then the optimal solution
for each factor is simply the corresponding marginal of p.

section 8.4.4

now consider the factor graph shown on the left in figure 10.18, which was
introduced earlier in the context of the sum-product algorithm. the joint distribution
is given by

we seek an approximation q(x) that has the same factorization, so that

p(x) = fa(x1, x2)fb(x2, x3)fc(x2, x4).

q(x)    (cid:4)fa(x1, x2)(cid:4)fb(x2, x3)(cid:4)fc(x2, x4).

(10.225)

(10.226)

note that id172 constants have been omitted, and these can be re-instated at
the end by local id172, as is generally done in belief propagation. now sup-
pose we restrict attention to approximations in which the factors themselves factorize
with respect to the individual variables so that

q(x)    (cid:4)fa1(x1)(cid:4)fa2(x2)(cid:4)fb2(x2)(cid:4)fb3(x3)(cid:4)fc2(x2)(cid:4)fc4(x4)

(10.227)

which corresponds to the factor graph shown on the right in figure 10.18. because
the individual factors are factorized, the overall distribution q(x) is itself fully fac-
torized.

now we apply the ep algorithm using the fully factorized approximation. sup-
pose that we have initialized all of the factors and that we choose to re   ne factor

x1

x2

x3

x1

x2

x3

10.7. expectation propagation

515

fa

fb

  fa1

  fa2

fc

x4

  fb2

  fb3

  fc2

  fc4

x4

figure 10.18 on the left is a simple factor graph from figure 8.51 and reproduced here for convenience. on
the right is the corresponding factorized approximation.

(cid:4)fb(x2, x3) = (cid:4)fb2(x2)(cid:4)fb3(x3). we    rst remove this factor from the approximating

distribution to give

\b(x) =(cid:4)fa1(x1)(cid:4)fa2(x2)(cid:4)fc2(x2)(cid:4)fc4(x4)

q

(10.228)

(10.229)

and we then multiply this by the exact factor fb(x2, x3) to give

the result, as noted above, is that qnew(z) comprises the product of factors, one for
each variable xi, in which each factor is given by the corresponding marginal of

\b(x)fb(x2, x3) =(cid:4)fa1(x1)(cid:4)fa2(x2)(cid:4)fc2(x2)(cid:4)fc4(x4)fb(x2, x3).

(cid:1)p(x) = q
we now    nd qnew(x) by minimizing the id181 kl((cid:1)p(cid:5)qnew).
(cid:1)p(x). these four marginals are given by
(cid:1)p(x1)     (cid:4)fa1(x1)
(cid:1)p(x2)     (cid:4)fa2(x2)(cid:4)fc2(x2)
(cid:19)
(cid:20)
(cid:2)
fb(x2, x3)(cid:4)fa2(x2)(cid:4)fc2(x2)
(cid:1)p(x3)    
(cid:1)p(x4)     (cid:4)fc4(x4)

and qnew(x) is obtained by multiplying these marginals together. we see that the

only factors in q(x) that change when we update(cid:4)fb(x2, x3) are those that involve
the variables in fb namely x2 and x3. to obtain the re   ned factor(cid:4)fb(x2, x3) =
(cid:4)fb2(x2)(cid:4)fb3(x3) we simply divide qnew(x) by q

\b(x), which gives

(cid:2)

fb(x2, x3)

(10.230)

(10.231)

(10.232)

(10.233)

x3

x2

(cid:4)fb2(x2)    
(cid:4)fb3(x3)    

(cid:2)
(cid:2)

x3

x2

fb(x2, x3)

(cid:19)
(cid:20)
fb(x2, x3)(cid:4)fa2(x2)(cid:4)fc2(x2)

.

(10.234)

(10.235)

516

10. approximate id136

section 8.4.4

  fb   x2(x2) sent by factor node fb to variable node x2 and is given by (8.81). simi-

these are precisely the messages obtained using belief propagation in which mes-
sages from variable nodes to factor nodes have been folded into the messages from

factor nodes to variable nodes. in particular, (cid:4)fb2(x2) corresponds to the message
larly, if we substitute (8.78) into (8.79), we obtain (10.235) in which(cid:4)fa2(x2) corre-
sponds to   fa   x2(x2) and(cid:4)fc2(x2) corresponds to   fc   x2(x2), giving the message
(cid:4)fb3(x3) which corresponds to   fb   x3(x3).
factors at a time, for instance if we re   ne only(cid:4)fb3(x3), then(cid:4)fb2(x2) is unchanged
by de   nition, while the re   ned version of(cid:4)fb3(x3) is again given by (10.235). if

this result differs slightly from standard belief propagation in that messages are
passed in both directions at the same time. we can easily modify the ep procedure
to give the standard form of the sum-product algorithm by updating just one of the

we are re   ning only one term at a time, then we can choose the order in which the
re   nements are done as we wish. in particular, for a tree-structured graph we can
follow a two-pass update scheme, corresponding to the standard belief propagation
schedule, which will result in exact id136 of the variable and factor marginals.
the initialization of the approximation factors in this case is unimportant.

now let us consider a general factor graph corresponding to the distribution

(cid:14)

p(  ) =

fi(  i)

(10.236)

(cid:14)

i

(cid:14)

(cid:4)fik(  k)

q(  )    

where   i represents the subset of variables associated with factor fi. we approximate
this using a fully factorized distribution of the form

(10.237)

i

k

where   k corresponds to an individual variable node. suppose that we wish to re   ne

the particular term(cid:4)fjl(  l) keeping all other terms    xed. we    rst remove the term
(cid:4)fj(  j) from q(  ) to give
and then multiply by the exact factor fj(  j). to determine the re   ned term(cid:4)fjl(  l),

(cid:4)fik(  k)

\j(  )    

(cid:14)

(cid:14)

(10.238)

i(cid:9)=j

q

k

we need only consider the functional dependence on   l, and so we simply    nd the
corresponding marginal of

(10.239)
up to a multiplicative constant, this involves taking the marginal of fj(  j) multiplied
\j(  ) that are functions of any of the variables in   j. terms that
by any terms from q

correspond to other factors (cid:4)fi(  i) for i (cid:9)= j will cancel between numerator and

q

\j(  )fj(  j).

(cid:2)

(cid:4)fjl(  l)    

fj(  j)

  m(cid:2)=l     j

\j(  ). we therefore obtain

(cid:14)

(cid:14)

m(cid:9)=l

k

(cid:4)fkm(  m).

denominator when we subsequently divide by q

(10.240)

exercises

517

we recognize this as the sum-product rule in the form in which messages from vari-
able nodes to factor nodes have been eliminated, as illustrated by the example shown

in figure 8.50. the quantity(cid:4)fjm(  m) corresponds to the message   fj     m(  m),

which factor node j sends to variable node m, and the product over k in (10.240)
is over all factors that depend on the variables   m that have variables (other than
variable   l) in common with factor fj(  j). in other words, to compute the outgoing
message from a factor node, we take the product of all the incoming messages from
other factor nodes, multiply by the local factor, and then marginalize.

thus, the sum-product algorithm arises as a special case of expectation propa-
gation if we use an approximating distribution that is fully factorized. this suggests
that more    exible approximating distributions, corresponding to partially discon-
nected graphs, could be used to achieve higher accuracy. another generalization is
to group factors fi(  i) together into sets and to re   ne all the factors in a set together
at each iteration. both of these approaches can lead to improvements in accuracy
(minka, 2001b). in general, the problem of choosing the best combination of group-
ing and disconnection is an open research issue.

we have seen that variational message passing and expectation propagation op-
timize two different forms of the id181. minka (2005) has
shown that a broad range of message passing algorithms can be derived from a com-
mon framework involving minimization of members of the alpha family of diver-
gences, given by (10.19). these include variational message passing, loopy belief
propagation, and expectation propagation, as well as a range of other algorithms,
which we do not have space to discuss here, such as tree-reweighted message pass-
ing (wainwright et al., 2005), fractional belief propagation (wiegerinck and heskes,
2003), and power ep (minka, 2004).

exercises

10.1 ((cid:12)) www verify that the log marginal distribution of the observed data ln p(x)
can be decomposed into two terms in the form (10.2) where l(q) is given by (10.3)
and kl(q(cid:5)p) is given by (10.4).

10.2 ((cid:12)) use the properties e[z1] = m1 and e[z2] = m2 to solve the simultaneous equa-
tions (10.13) and (10.15), and hence show that, provided the original distribution
p(z) is nonsingular, the unique solution for the means of the factors in the approxi-
mation distribution is given by e[z1] =   1 and e[z2] =   2.

10.3 ((cid:12) (cid:12)) www consider a factorized variational distribution q(z) of the form (10.5).
by using the technique of lagrange multipliers, verify that minimization of the
id181 kl(p(cid:5)q) with respect to one of the factors qi(zi),
keeping all other factors    xed, leads to the solution (10.17).

10.4 ((cid:12) (cid:12)) suppose that p(x) is some    xed distribution and that we wish to approximate
it using a gaussian distribution q(x) = n (x|  ,   ). by writing down the form of
the kl divergence kl(p(cid:5)q) for a gaussian q(x) and then differentiating, show that

518

10. approximate id136

minimization of kl(p(cid:5)q) with respect to    and    leads to the result that    is given
by the expectation of x under p(x) and that    is given by the covariance.

10.5 ((cid:12) (cid:12)) www consider a model in which the set of all hidden stochastic variables, de-
noted collectively by z, comprises some latent variables z together with some model
parameters   . suppose we use a variational distribution that factorizes between la-
tent variables and parameters so that q(z,   ) = qz(z)q  (  ), in which the distribution
q  (  ) is approximated by a point estimate of the form q  (  ) =   (         0) where   0
is a vector of free parameters. show that variational optimization of this factorized
distribution is equivalent to an em algorithm, in which the e step optimizes qz(z),
and the m step maximizes the expected complete-data log posterior distribution of   
with respect to   0.

10.6 ((cid:12) (cid:12)) the alpha family of divergences is de   ned by (10.19). show that the kullback-
leibler divergence kl(p(cid:5)q) corresponds to        1. this can be done by writing
p  = exp(  ln p) = 1 +   ln p + o( 2) and then taking       0. similarly show that
kl(q(cid:5)p) corresponds to           1.

10.7 ((cid:12) (cid:12)) consider the problem of inferring the mean and precision of a univariate gaus-
sian using a factorized variational approximation, as considered in section 10.1.3.
show that the factor q  (  ) is a gaussian of the form n (  |  n ,   
   1
n ) with mean and
precision given by (10.26) and (10.27), respectively. similarly show that the factor
q   (  ) is a gamma distribution of the form gam(  |an , bn ) with parameters given by
(10.29) and (10.30).

10.8 ((cid:12)) consider the variational posterior distribution for the precision of a univariate
gaussian whose parameters are given by (10.29) and (10.30). by using the standard
results for the mean and variance of the gamma distribution given by (b.27) and
(b.28), show that if we let n        , this variational posterior distribution has a
mean given by the inverse of the maximum likelihood estimator for the variance of
the data, and a variance that goes to zero.

10.9 ((cid:12) (cid:12)) by making use of the standard result e[  ] = an /bn for the mean of a gamma
distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result
(10.33) for the reciprocal of the expected precision in the factorized variational treat-
ment of a univariate gaussian.

10.10 ((cid:12)) www derive the decomposition given by (10.34) that is used to    nd approxi-

mate posterior distributions over models using variational id136.

10.11 ((cid:12) (cid:12)) www by using a lagrange multiplier to enforce the id172 constraint
on the distribution q(m), show that the maximum of the lower bound (10.35) is given
by (10.36).

10.12 ((cid:12) (cid:12)) starting from the joint distribution (10.41), and applying the general result
(10.9), show that the optimal variational distribution q(cid:1)(z) over the latent variables
for the bayesian mixture of gaussians is given by (10.48) by verifying the steps
given in the text.

exercises

519

10.13 ((cid:12) (cid:12)) www starting from (10.54), derive the result (10.59) for the optimum vari-
ational posterior distribution over   k and   k in the bayesian mixture of gaussians,
and hence verify the expressions for the parameters of this distribution given by
(10.60)   (10.63).

10.14 ((cid:12) (cid:12)) using the distribution (10.59), verify the result (10.64).

10.15 ((cid:12)) using the result (b.17), show that the expected value of the mixing coef   cients

in the variational mixture of gaussians is given by (10.69).

10.16 ((cid:12) (cid:12)) www verify the results (10.71) and (10.72) for the    rst two terms in the

lower bound for the variational gaussian mixture model given by (10.70).

10.17 ((cid:12) (cid:12) (cid:12)) verify the results (10.73)   (10.77) for the remaining terms in the lower bound

for the variational gaussian mixture model given by (10.70).

10.18 ((cid:12) (cid:12) (cid:12))

in this exercise, we shall derive the variational re-estimation equations for
the gaussian mixture model by direct differentiation of the lower bound. to do this
we assume that the variational distribution has the factorization de   ned by (10.42)
and (10.55) with factors given by (10.48), (10.57), and (10.59). substitute these into
(10.70) and hence obtain the lower bound as a function of the parameters of the varia-
tional distribution. then, by maximizing the bound with respect to these parameters,
derive the re-estimation equations for the factors in the variational distribution, and
show that these are the same as those obtained in section 10.2.1.

10.19 ((cid:12) (cid:12)) derive the result (10.81) for the predictive distribution in the variational treat-

ment of the bayesian mixture of gaussians model.

10.20 ((cid:12) (cid:12)) www this exercise explores the id58 solution for the mixture of
gaussians model when the size n of the data set is large and shows that it reduces (as
we would expect) to the maximum likelihood solution based on em derived in chap-
ter 9. note that results from appendix b may be used to help answer this exercise.
first show that the posterior distribution q(cid:1)(  k) of the precisions becomes sharply
peaked around the maximum likelihood solution. do the same for the posterior dis-
tribution of the means q(cid:1)(  k|  k). next consider the posterior distribution q(cid:1)(  )
for the mixing coef   cients and show that this too becomes sharply peaked around
the maximum likelihood solution. similarly, show that the responsibilities become
equal to the corresponding maximum likelihood values for large n, by making use
of the following asymptotic result for the digamma function for large x

  (x) = ln x + o (1/x) .

(10.241)

finally, by making use of (10.80), show that for large n, the predictive distribution
becomes a mixture of gaussians.

10.21 ((cid:12)) show that the number of equivalent parameter settings due to interchange sym-

metries in a mixture model with k components is k!.

520

10. approximate id136

10.22 ((cid:12) (cid:12)) we have seen that each mode of the posterior distribution in a gaussian mix-
ture model is a member of a family of k! equivalent modes. suppose that the result
of running the variational id136 algorithm is an approximate posterior distribu-
tion q that is localized in the neighbourhood of one of the modes. we can then
approximate the full posterior distribution as a mixture of k! such q distributions,
once centred on each mode and having equal mixing coef   cients. show that if we
assume negligible overlap between the components of the q mixture, the resulting
lower bound differs from that for a single component q distribution through the ad-
dition of an extra term ln k!.

10.23 ((cid:12) (cid:12)) www consider a variational gaussian mixture model in which there is no
prior distribution over mixing coef   cients {  k}. instead, the mixing coef   cients are
treated as parameters, whose values are to be found by maximizing the variational
lower bound on the log marginal likelihood. show that maximizing this lower bound
with respect to the mixing coef   cients, using a lagrange multiplier to enforce the
constraint that the mixing coef   cients sum to one, leads to the re-estimation result
(10.83). note that there is no need to consider all of the terms in the lower bound but
only the dependence of the bound on the {  k}.

10.24 ((cid:12) (cid:12)) www we have seen in section 10.2 that the singularities arising in the max-
imum likelihood treatment of gaussian mixture models do not arise in a bayesian
treatment. discuss whether such singularities would arise if the bayesian model
were solved using maximum posterior (map) estimation.

10.25 ((cid:12) (cid:12)) the variational treatment of the bayesian mixture of gaussians, discussed in
section 10.2, made use of a factorized approximation (10.5) to the posterior distribu-
tion. as we saw in figure 10.2, the factorized assumption causes the variance of the
posterior distribution to be under-estimated for certain directions in parameter space.
discuss qualitatively the effect this will have on the variational approximation to the
model evidence, and how this effect will vary with the number of components in
the mixture. hence explain whether the variational gaussian mixture will tend to
under-estimate or over-estimate the optimal number of components.

10.26 ((cid:12) (cid:12) (cid:12)) extend the variational treatment of bayesian id75 to include
a gamma hyperprior gam(  |c0, d0) over    and solve variationally, by assuming a
factorized variational distribution of the form q(w)q(  )q(  ). derive the variational
update equations for the three factors in the variational distribution and also obtain
an expression for the lower bound and for the predictive distribution.

10.27 ((cid:12) (cid:12)) by making use of the formulae given in appendix b show that the variational
lower bound for the linear basis function regression model, de   ned by (10.107), can
be written in the form (10.107) with the various terms de   ned by (10.108)   (10.112).

10.28 ((cid:12) (cid:12) (cid:12)) rewrite the model for the bayesian mixture of gaussians, introduced in
section 10.2, as a conjugate model from the exponential family, as discussed in
section 10.4. hence use the general results (10.115) and (10.119) to derive the
speci   c results (10.48), (10.57), and (10.59).

exercises

521

10.29 ((cid:12)) www show that the function f(x) = ln(x) is concave for 0 < x <    
by computing its second derivative. determine the form of the dual function g(  )
de   ned by (10.133), and verify that minimization of   x     g(  ) with respect to   
according to (10.132) indeed recovers the function ln(x).

10.30 ((cid:12)) by evaluating the second derivative, show that the log logistic function f(x) =
    ln(1 + e
   x) is concave. derive the variational upper bound (10.137) directly by
making a second order taylor expansion of the log logistic function around a point
x =   .

10.31 ((cid:12) (cid:12)) by    nding the second derivative with respect to x, show that the function
f(x) =     ln(ex/2 + e
   x/2) is a concave function of x. now consider the second
derivatives with respect to the variable x2 and hence show that it is a convex function
of x2. plot graphs of f(x) against x and against x2. derive the lower bound (10.144)
on the logistic sigmoid function directly by making a    rst order taylor series expan-
sion of the function f(x) in the variable x2 centred on the value   2.

10.32 ((cid:12) (cid:12)) www consider the variational treatment of id28 with sequen-
tial learning in which data points are arriving one at a time and each must be pro-
cessed and discarded before the next data point arrives. show that a gaussian ap-
proximation to the posterior distribution can be maintained through the use of the
lower bound (10.151), in which the distribution is initialized using the prior, and as
each data point is absorbed its corresponding variational parameter   n is optimized.
10.33 ((cid:12)) by differentiating the quantity q(  ,   old) de   ned by (10.161) with respect to
the variational parameter   n show that the update equation for   n for the bayesian
id28 model is given by (10.163).

10.34 ((cid:12) (cid:12))

in this exercise we derive re-estimation equations for the variational parame-
ters    in the bayesian id28 model of section 4.5 by direct maximization
of the lower bound given by (10.164). to do this set the derivative of l(  ) with re-
spect to   n equal to zero, making use of the result (3.117) for the derivative of the log
of a determinant, together with the expressions (10.157) and (10.158) which de   ne
the mean and covariance of the variational posterior distribution q(w).

10.35 ((cid:12) (cid:12)) derive the result (10.164) for the lower bound l(  ) in the variational logistic
regression model. this is most easily done by substituting the expressions for the
gaussian prior q(w) = n (w|m0, s0), together with the lower bound h(w,   ) on
the likelihood function, into the integral (10.159) which de   nes l(  ). next gather
together the terms which depend on w in the exponential and complete the square
to give a gaussian integral, which can then be evaluated by invoking the standard
result for the id172 coef   cient of a multivariate gaussian. finally take the
logarithm to obtain (10.164).

10.36 ((cid:12) (cid:12)) consider the adf approximation scheme discussed in section 10.7, and show
that inclusion of the factor fj(  ) leads to an update of the model evidence of the
form

pj(d) (cid:7) pj   1(d)zj

(10.242)

522

10. approximate id136

where zj is the id172 constant de   ned by (10.197). by applying this result
recursively, and initializing with p0(d) = 1, derive the result

(cid:14)

p(d) (cid:7)

zj.

j

(10.243)

10.37 ((cid:12)) www consider the expectation propagation algorithm from section 10.7, and
suppose that one of the factors f0(  ) in the de   nition (10.188) has the same expo-
nential family functional form as the approximating distribution q(  ). show that if

the factor(cid:4)f0(  ) is initialized to be f0(  ), then an ep update to re   ne(cid:4)f0(  ) leaves
(cid:4)f0(  ) unchanged. this situation typically arises when one of the factors is the prior

p(  ), and so we see that the prior factor can be incorporated once exactly and does
not need to be re   ned.

10.38 ((cid:12) (cid:12) (cid:12))

in this exercise and the next, we shall verify the results (10.214)   (10.224)
for the expectation propagation algorithm applied to the clutter problem. begin by
using the division formula (10.205) to derive the expressions (10.214) and (10.215)
by completing the square inside the exponential to identify the mean and variance.
also, show that the id172 constant zn, de   ned by (10.206), is given for the
clutter problem by (10.216). this can be done by making use of the general result
(2.115).

10.39 ((cid:12) (cid:12) (cid:12)) show that the mean and variance of qnew(  ) for ep applied to the clutter
problem are given by (10.217) and (10.218). to do this,    rst prove the following
results for the expectations of    and     t under qnew(  )

e[  ] = m\n + v

\n   m\n ln zn

e[  t  ] = 2(v

\n)2   v\n ln zn + 2e[  ]tm\n     (cid:5)m\n(cid:5)2

(10.244)
(10.245)

and then make use of the result (10.216) for zn. next, prove the results (10.220)   
(10.222) by using (10.207) and completing the square in the exponential. finally,
use (10.208) to derive the result (10.223).

11

sampling
methods

for most probabilistic models of practical interest, exact id136 is intractable, and
so we have to resort to some form of approximation. in chapter 10, we discussed
id136 algorithms based on deterministic approximations, which include methods
such as id58 and expectation propagation. here we consider approxi-
mate id136 methods based on numerical sampling, also known as monte carlo
techniques.

although for some applications the posterior distribution over unobserved vari-
ables will be of direct interest in itself, for most situations the posterior distribution
is required primarily for the purpose of evaluating expectations, for example in order
to make predictions. the fundamental problem that we therefore wish to address in
this chapter involves    nding the expectation of some function f(z) with respect to a
id203 distribution p(z). here, the components of z might comprise discrete or
continuous variables or some combination of the two. thus in the case of continuous

523

524

11. sampling methods

figure 11.1 schematic illustration of a function f (z)
whose expectation is to be evaluated with
respect to a distribution p(z).

p(z)

f(z)

z

variables, we wish to evaluate the expectation

(cid:6)

e[f] =

f(z)p(z) dz

(11.1)

exercise 11.1

by

l(cid:2)

(cid:1)f =

1
l

where the integral is replaced by summation in the case of discrete variables. this
is illustrated schematically for a single continuous variable in figure 11.1. we shall
suppose that such expectations are too complex to be evaluated exactly using analyt-
ical techniques.

the general idea behind sampling methods is to obtain a set of samples z(l)
(where l = 1, . . . , l) drawn independently from the distribution p(z). this allows
the expectation (11.1) to be approximated by a    nite sum

as long as the samples z(l) are drawn from the distribution p(z), then e[(cid:1)f] = e[f]
and so the estimator(cid:1)f has the correct mean. the variance of the estimator is given

l=1

(11.2)

f(z(l)).

var[(cid:1)f] =

(cid:8)

(f     e[f])2

1
l

e

(cid:9)

(11.3)

is the variance of the function f(z) under the distribution p(z). it is worth emphasiz-
ing that the accuracy of the estimator therefore does not depend on the dimension-
ality of z, and that, in principle, high accuracy may be achievable with a relatively
small number of samples z(l). in practice, ten or twenty independent samples may
suf   ce to estimate an expectation to suf   cient accuracy.
the problem, however, is that the samples {z(l)} might not be independent, and
so the effective sample size might be much smaller than the apparent sample size.
also, referring back to figure 11.1, we note that if f(z) is small in regions where
p(z) is large, and vice versa, then the expectation may be dominated by regions
of small id203, implying that relatively large sample sizes will be required to
achieve suf   cient accuracy.

for many models, the joint distribution p(z) is conveniently speci   ed in terms
of a graphical model. in the case of a directed graph with no observed variables, it is

11. sampling methods

525

straightforward to sample from the joint distribution (assuming that it is possible to
sample from the conditional distributions at each node) using the following ances-
tral sampling approach, discussed brie   y in section 8.1.2. the joint distribution is
speci   ed by

m(cid:14)

p(z) =

p(zi|pai)

(11.4)

i=1

where zi are the set of variables associated with node i, and pai denotes the set of
variables associated with the parents of node i. to obtain a sample from the joint
distribution, we make one pass through the set of variables in the order z1, . . . , zm
sampling from the conditional distributions p(zi|pai). this is always possible be-
cause at each step all of the parent values will have been instantiated. after one pass
through the graph, we will have obtained a sample from the joint distribution.

now consider the case of a directed graph in which some of the nodes are in-
stantiated with observed values. we can in principle extend the above procedure, at
least in the case of nodes representing discrete variables, to give the following logic
sampling approach (henrion, 1988), which can be seen as a special case of impor-
tance sampling discussed in section 11.1.4. at each step, when a sampled value is
obtained for a variable zi whose value is observed, the sampled value is compared
to the observed value, and if they agree then the sample value is retained and the al-
gorithm proceeds to the next variable in turn. however, if the sampled value and the
observed value disagree, then the whole sample so far is discarded and the algorithm
starts again with the    rst node in the graph. this algorithm samples correctly from
the posterior distribution because it corresponds simply to drawing samples from the
joint distribution of hidden variables and data variables and then discarding those
samples that disagree with the observed data (with the slight saving of not continu-
ing with the sampling from the joint distribution as soon as one contradictory value is
observed). however, the overall id203 of accepting a sample from the posterior
decreases rapidly as the number of observed variables increases and as the number
of states that those variables can take increases, and so this approach is rarely used
in practice.

in the case of id203 distributions de   ned by an undirected graph, there is
no one-pass sampling strategy that will sample even from the prior distribution with
no observed variables. instead, computationally more expensive techniques must be
employed, such as id150, which is discussed in section 11.3.

as well as sampling from conditional distributions, we may also require samples
from a marginal distribution. if we already have a strategy for sampling from a joint
distribution p(u, v), then it is straightforward to obtain samples from the marginal
distribution p(u) simply by ignoring the values for v in each sample.

there are numerous texts dealing with monte carlo methods. those of partic-
ular interest from the statistical id136 perspective include chen et al. (2001),
gamerman (1997), gilks et al. (1996), liu (2001), neal (1996), and robert and
casella (1999). also there are review articles by besag et al. (1995), brooks (1998),
diaconis and saloff-coste (1998), jerrum and sinclair (1996), neal (1993), tierney
(1994), and andrieu et al. (2003) that provide additional information on sampling

526

11. sampling methods

methods for statistical id136.

diagnostic tests for convergence of id115 algorithms are
summarized in robert and casella (1999), and some practical guidance on the use of
sampling methods in the context of machine learning is given in bishop and nabney
(2008).

11.1. basic sampling algorithms

in this section, we consider some simple strategies for generating random samples
from a given distribution. because the samples will be generated by a computer
algorithm they will in fact be pseudo-random numbers, that is, they will be deter-
ministically calculated, but must nevertheless pass appropriate tests for randomness.
generating such numbers raises several subtleties (press et al., 1992) that lie outside
the scope of this book. here we shall assume that an algorithm has been provided
that generates pseudo-random numbers distributed uniformly over (0, 1), and indeed
most software environments have such a facility built in.

dy

(cid:7)(cid:7)(cid:7)(cid:7) dz
(cid:7)(cid:7)(cid:7)(cid:7)
p((cid:1)y) d(cid:1)y

11.1.1 standard distributions
we    rst consider how to generate random numbers from simple nonuniform dis-
tributions, assuming that we already have available a source of uniformly distributed
random numbers. suppose that z is uniformly distributed over the interval (0, 1),
and that we transform the values of z using some function f(  ) so that y = f(z).
the distribution of y will be governed by

p(y) = p(z)

(11.5)

z = h(y)    

where, in this case, p(z) = 1. our goal is to choose the function f(z) such that the
resulting values of y have some speci   c desired distribution p(y). integrating (11.5)
we obtain

(cid:6) y

(11.6)
   1(z), and so we have to
which is the inde   nite integral of p(y). thus, y = h
transform the uniformly distributed random numbers using a function which is the
inverse of the inde   nite integral of the desired distribution. this is illustrated in
figure 11.2.

      

consider for example the exponential distribution
p(y) =    exp(     y)

(11.7)
where 0 (cid:1) y <    . in this case the lower limit of the integral in (11.6) is 0, and so
h(y) = 1     exp(     y). thus, if we transform our uniformly distributed variable z
using y =      

   1 ln(1     z), then y will have an exponential distribution.

exercise 11.2

11.1. basic sampling algorithms

527

figure 11.2 geometrical

interpretation of

the trans-
formation method for generating nonuni-
formly distributed random numbers. h(y)
is the inde   nite integral of the desired dis-
tribution p(y).
if a uniformly distributed
random variable z is transformed using
y = h   1(z), then y will be distributed ac-
cording to p(y).

1

0

h(y)

p(y)

y

another example of a distribution to which the transformation method can be

applied is given by the cauchy distribution

p(y) =

1
  

1

1 + y2 .

(11.8)

exercise 11.3

in this case, the inverse of the inde   nite integral can be expressed in terms of the
   tan    function.

the generalization to multiple variables is straightforward and involves the ja-

cobian of the change of variables, so that

p(y1, . . . , ym ) = p(z1, . . . , zm )

(cid:7)(cid:7)(cid:7)(cid:7)    (z1, . . . , zm )

   (y1, . . . , ym )

(cid:7)(cid:7)(cid:7)(cid:7) .

(11.9)

as a    nal example of the transformation method we consider the box-muller
method for generating samples from a gaussian distribution. first, suppose we gen-
erate pairs of uniformly distributed random numbers z1, z2     (   1, 1), which we can
do by transforming a variable distributed uniformly over (0, 1) using z     2z     1.
2 (cid:1) 1. this leads to a uniform
next we discard each pair unless it satis   es z2
distribution of points inside the unit circle with p(z1, z2) = 1/  , as illustrated in
figure 11.3. then, for each pair z1, z2 we evaluate the quantities

1 + z2

figure 11.3 the box-muller method for generating gaussian dis-
tributed random numbers starts by generating samples
from a uniform distribution inside the unit circle.

1

z2

   1
   1

z1

1

528

11. sampling methods

exercise 11.4

exercise 11.5

y1 = z1

y2 = z2

(cid:16)1/2
(cid:16)1/2

r2

(cid:15)   2 ln z1
(cid:15)   2 ln z2
(cid:7)(cid:7)(cid:7)(cid:7)
(cid:7)(cid:7)(cid:7)(cid:7)    (z1, z2)
(cid:30)(cid:29)

   (y1, y2)

r2

(11.10)

(11.11)

(11.12)

where r2 = z2

1 + z2

2. then the joint distribution of y1 and y2 is given by

p(y1, y2) = p(z1, z2)

(cid:29)

=

1   
2  

exp(   y2

1/2)

(cid:30)

exp(   y2

2/2)

1   
2  

and so y1 and y2 are independent and each has a gaussian distribution with zero
mean and unit variance.

if y has a gaussian distribution with zero mean and unit variance, then   y +   
will have a gaussian distribution with mean    and variance   2. to generate vector-
valued variables having a multivariate gaussian distribution with mean    and co-
variance   , we can make use of the cholesky decomposition, which takes the form
   = llt (press et al., 1992). then, if z is a vector valued random variable whose
components are independent and gaussian distributed with zero mean and unit vari-
ance, then y =    + lz will have mean    and covariance   .

obviously, the transformation technique depends for its success on the ability
to calculate and then invert the inde   nite integral of the required distribution. such
operations will only be feasible for a limited number of simple distributions, and so
we must turn to alternative approaches in search of a more general strategy. here
we consider two techniques called rejection sampling and importance sampling. al-
though mainly limited to univariate distributions and thus not directly applicable to
complex problems in many dimensions, they do form important components in more
general strategies.

11.1.2 rejection sampling
the rejection sampling framework allows us to sample from relatively complex
distributions, subject to certain constraints. we begin by considering univariate dis-
tributions and discuss the extension to multiple dimensions subsequently.

suppose we wish to sample from a distribution p(z) that is not one of the simple,
standard distributions considered so far, and that sampling directly from p(z) is dif-
   cult. furthermore suppose, as is often the case, that we are easily able to evaluate
p(z) for any given value of z, up to some normalizing constant z, so that

where(cid:4)p(z) can readily be evaluated, but zp is unknown.

p(z) =

in order to apply rejection sampling, we need some simpler distribution q(z),
sometimes called a proposal distribution, from which we can readily draw samples.

(11.13)

(cid:4)p(z)

1
zp

figure 11.4 in the rejection sampling method,
samples are drawn from a sim-
ple distribution q(z) and rejected
if they fall
in the grey area be-
tween the unnormalized distribu-
tion ep(z) and the scaled distribu-
tion kq(z). the resulting samples
are distributed according to p(z),
which is the normalized version of
ep(z).

11.1. basic sampling algorithms

529

kq(z0)

kq(z)

  p(z)

u0

z

z0

we next introduce a constant k whose value is chosen such that kq(z) (cid:2)(cid:4)p(z) for

all values of z. the function kq(z) is called the comparison function and is illus-
trated for a univariate distribution in figure 11.4. each step of the rejection sampler
involves generating two random numbers. first, we generate a number z0 from the
distribution q(z). next, we generate a number u0 from the uniform distribution over
[0, kq(z0)]. this pair of random numbers has uniform distribution under the curve

of the function kq(z). finally, if u0 >(cid:4)p(z0) then the sample is rejected, otherwise
ure 11.4. the remaining pairs then have uniform distribution under the curve of(cid:4)p(z),
ples are then accepted with id203(cid:4)p(z)/kq(z), and so the id203 that a

and hence the corresponding z values are distributed according to p(z), as desired.
the original values of z are generated from the distribution q(z), and these sam-

u0 is retained. thus the pair is rejected if it lies in the grey shaded region in fig-

sample will be accepted is given by

{(cid:4)p(z)/kq(z)} q(z) dz
(cid:6) (cid:4)p(z) dz.
the area under the unnormalized distribution(cid:4)p(z) to the area under the curve kq(z).
limitation that kq(z) must be nowhere less than(cid:4)p(z).

we therefore see that the constant k should be as small as possible subject to the

thus the fraction of points that are rejected by this method depends on the ratio of

p(accept) =

(11.14)

(cid:6)

1
k

=

exercise 11.6

as an illustration of the use of rejection sampling, consider the task of sampling

from the gamma distribution

gam(z|a, b) = baza   1 exp(   bz)

  (a)

(11.15)

which, for a > 1, has a bell-shaped form, as shown in figure 11.5. a suitable
proposal distribution is therefore the cauchy (11.8) because this too is bell-shaped
and because we can use the transformation method, discussed earlier, to sample from
it. we need to generalize the cauchy slightly to ensure that it nowhere has a smaller
value than the gamma distribution. this can be achieved by transforming a uniform
random variable y using z = b tan y + c, which gives random numbers distributed
according to.

exercise 11.7

530

11. sampling methods

figure 11.5 plot showing the gamma distribu-
tion given by (11.15) as the green
curve, with a scaled cauchy pro-
posal distribution shown by the red
curve. samples from the gamma
distribution can be obtained by
sampling from the cauchy and
then applying the rejection sam-
pling criterion.

p(z)

0.15

0.1

0.05

0

0

10

20

30

z

q(z) =

(11.16)
the minimum reject rate is obtained by setting c = a     1, b2 = 2a     1 and choos-
ing the constant k to be as small as possible while still satisfying the requirement

kq(z) (cid:2)(cid:4)p(z). the resulting comparison function is also illustrated in figure 11.5.

1 + (z     c)2/b2 .

k

11.1.3 adaptive rejection sampling
in many instances where we might wish to apply rejection sampling, it proves
dif   cult to determine a suitable analytic form for the envelope distribution q(z). an
alternative approach is to construct the envelope function on the    y based on mea-
sured values of the distribution p(z) (gilks and wild, 1992). construction of an
envelope function is particularly straightforward for cases in which p(z) is log con-
cave, in other words when ln p(z) has derivatives that are nonincreasing functions
of z. the construction of a suitable envelope function is illustrated graphically in
figure 11.6.

the function ln p(z) and its gradient are evaluated at some initial set of grid
points, and the intersections of the resulting tangent lines are used to construct the
envelope function. next a sample value is drawn from the envelope distribution.
this is straightforward because the log of the envelope distribution is a succession

exercise 11.9

figure 11.6 in the case of distributions that are
log concave, an envelope function
for use in rejection sampling can be
constructed using the tangent lines
computed at a set of grid points. if a
sample point is rejected, it is added
to the set of grid points and used to
re   ne the envelope distribution.

ln p(z)

z1

z2

z3

z

11.1. basic sampling algorithms

531

figure 11.7 illustrative example of

rejection
sampling involving sampling from a
gaussian distribution p(z) shown by
the green curve, by using rejection
sampling from a proposal distri-
bution q(z) that
is also gaussian
and whose scaled version kq(z) is
shown by the red curve.

0.5

p(z)

0.25

0
   5

0

z

5

of linear functions, and hence the envelope distribution itself comprises a piecewise
exponential distribution of the form

q(z) = ki  i exp{     i(z     zi   1)}

zi   1 < z (cid:1) zi.

(11.17)

once a sample has been drawn, the usual rejection criterion can be applied. if the
sample is accepted, then it will be a draw from the desired distribution. if, however,
the sample is rejected, then it is incorporated into the set of grid points, a new tangent
line is computed, and the envelope function is thereby re   ned. as the number of
grid points increases, so the envelope function becomes a better approximation of
the desired distribution p(z) and the id203 of rejection decreases.

a variant of the algorithm exists that avoids the evaluation of derivatives (gilks,
1992). the adaptive rejection sampling framework can also be extended to distri-
butions that are not log concave, simply by following each rejection sampling step
with a metropolis-hastings step (to be discussed in section 11.2.2), giving rise to
adaptive rejection metropolis sampling (gilks et al., 1995).

clearly for rejection sampling to be of practical value, we require that the com-
parison function be close to the required distribution so that the rate of rejection is
kept to a minimum. now let us examine what happens when we try to use rejection
sampling in spaces of high dimensionality. consider, for the sake of illustration,
a somewhat arti   cial problem in which we wish to sample from a zero-mean mul-
tivariate gaussian distribution with covariance   2
pi, where i is the unit matrix, by
rejection sampling from a proposal distribution that is itself a zero-mean gaussian
distribution having covariance   2
p in order that
there exists a k such that kq(z) (cid:2) p(z). in d-dimensions the optimum value of k
is given by k = (  q/  p)d, as illustrated for d = 1 in figure 11.7. the acceptance
rate will be the ratio of volumes under p(z) and kq(z), which, because both distribu-
tions are normalized, is just 1/k. thus the acceptance rate diminishes exponentially
with dimensionality. even if   q exceeds   p by just one percent, for d = 1, 000 the
acceptance ratio will be approximately 1/20, 000. in this illustrative example the
comparison function is close to the required distribution. for more practical exam-
ples, where the desired distribution may be multimodal and sharply peaked, it will
be extremely dif   cult to    nd a good proposal distribution and comparison function.

qi. obviously, we must have   2

q (cid:2)   2

532

11. sampling methods

figure 11.8 importance sampling addresses the prob-
lem of evaluating the expectation of a func-
tion f (z) with respect to a distribution p(z)
from which it is dif   cult to draw samples di-
instead, samples {z(l)} are drawn
rectly.
from a simpler distribution q(z), and the
corresponding terms in the summation are
weighted by the ratios p(z(l))/q(z(l)).

p(z)

q(z)

f(z)

z

furthermore, the exponential decrease of acceptance rate with dimensionality is a
generic feature of rejection sampling. although rejection can be a useful technique
in one or two dimensions it is unsuited to problems of high dimensionality. it can,
however, play a role as a subroutine in more sophisticated algorithms for sampling
in high dimensional spaces.

11.1.4 importance sampling
one of the principal reasons for wishing to sample from complicated id203
distributions is to be able to evaluate expectations of the form (11.1). the technique
of importance sampling provides a framework for approximating expectations di-
rectly but does not itself provide a mechanism for drawing samples from distribution
p(z).

e[f] (cid:7) l(cid:2)

the    nite sum approximation to the expectation, given by (11.2), depends on
being able to draw samples from the distribution p(z). suppose, however, that it is
impractical to sample directly from p(z) but that we can evaluate p(z) easily for any
given value of z. one simplistic strategy for evaluating expectations would be to
discretize z-space into a uniform grid and to evaluate the integrand as a sum of the
form

p(z(l))f(z(l)).

(11.18)

l=1

an obvious problem with this approach is that the number of terms in the summation
grows exponentially with the dimensionality of z. furthermore, as we have already
noted, the kinds of id203 distributions of interest will often have much of their
mass con   ned to relatively small regions of z space and so uniform sampling will be
very inef   cient because in high-dimensional problems, only a very small proportion
of the samples will make a signi   cant contribution to the sum. we would really like
to choose the sample points to fall in regions where p(z) is large, or ideally where
the product p(z)f(z) is large.

as in the case of rejection sampling, importance sampling is based on the use
of a proposal distribution q(z) from which it is easy to draw samples, as illustrated
in figure 11.8. we can then express the expectation in the form of a    nite sum over

11.1. basic sampling algorithms

533

samples {z(l)} drawn from q(z)

e[f] =

(cid:6)
(cid:6)

=

(cid:7) 1
l

f(z)p(z) dz
f(z) p(z)

l(cid:2)

l=1

q(z) q(z) dz
p(z(l))
q(z(l)) f(z(l)).

(11.19)

the quantities rl = p(z(l))/q(z(l)) are known as importance weights, and they cor-
rect the bias introduced by sampling from the wrong distribution. note that, unlike
rejection sampling, all of the samples generated are retained.

id172 constant, so that p(z) =(cid:4)p(z)/zp where(cid:4)p(z) can be evaluated easily,
distribution q(z) =(cid:4)q(z)/zq, which has the same property. we then have

it will often be the case that the distribution p(z) can only be evaluated up to a

whereas zp is unknown. similarly, we may wish to use an importance sampling

(cid:6)

where(cid:4)rl = (cid:4)p(z(l))/(cid:4)q(z(l)). we can use the same sample set to evaluate the ratio

l=1

(11.20)

e[f] =

(cid:6)

1
l

f(z)

f(z)p(z) dz

= zq
zp
(cid:7) zq
zp

(cid:4)p(z)(cid:4)q(z) q(z) dz
(cid:4)rlf(z(l)).
(cid:6) (cid:4)p(z)(cid:4)q(z) q(z) dz

l(cid:2)
(cid:6) (cid:4)p(z) dz =
l(cid:2)
(cid:4)rl
e[f] (cid:7) l(cid:2)
(cid:4)rl(cid:5)
(cid:4)p(z(l))/q(z(l))
(cid:5)
m(cid:4)rm
m(cid:4)p(z(m))/q(z(m)) .

wlf(z(l))

=

l=1

l=1

=

1
zq
(cid:7) 1
l

zp/zq with the result

zp
zq

and hence

where we have de   ned

wl =

(11.21)

(11.22)

(11.23)

as with rejection sampling, the success of the importance sampling approach
depends crucially on how well the sampling distribution q(z) matches the desired

534

11. sampling methods

distribution p(z). if, as is often the case, p(z)f(z) is strongly varying and has a sig-
ni   cant proportion of its mass concentrated over relatively small regions of z space,
then the set of importance weights {rl} may be dominated by a few weights hav-
ing large values, with the remaining weights being relatively insigni   cant. thus the
effective sample size can be much smaller than the apparent sample size l. the prob-
lem is even more severe if none of the samples falls in the regions where p(z)f(z)
is large. in that case, the apparent variances of rl and rlf(z(l)) may be small even
though the estimate of the expectation may be severely wrong. hence a major draw-
back of the importance sampling method is the potential to produce results that are
arbitrarily in error and with no diagnostic indication. this also highlights a key re-
quirement for the sampling distribution q(z), namely that it should not be small or
zero in regions where p(z) may be signi   cant.

for distributions de   ned in terms of a graphical model, we can apply the impor-
tance sampling technique in various ways. for discrete variables, a simple approach
is called uniform sampling. the joint distribution for a directed graph is de   ned
by (11.4). each sample from the joint distribution is obtained by    rst setting those
variables zi that are in the evidence set equal to their observed values. each of the
remaining variables is then sampled independently from a uniform distribution over
the space of possible instantiations. to determine the corresponding weight associ-

ated with a sample z(l), we note that the sampling distribution(cid:4)q(z) is uniform over
the possible choices for z, and that(cid:4)p(z|x) =(cid:4)p(z), where x denotes the subset of

variables that are observed, and the equality follows from the fact that every sample
z that is generated is necessarily consistent with the evidence. thus the weights rl
are simply proportional to p(z). note that the variables can be sampled in any order.
this approach can yield poor results if the posterior distribution is far from uniform,
as is often the case in practice.

an improvement on this approach is called likelihood weighted sampling (fung
and chang, 1990; shachter and peot, 1990) and is based on ancestral sampling of
the variables. for each variable in turn, if that variable is in the evidence set, then it
is just set to its instantiated value. if it is not in the evidence set, then it is sampled
from the conditional distribution p(zi|pai) in which the conditioning variables are
set to their currently sampled values. the weighting associated with the resulting
sample z is then given by

r(z) =

p(zi|pai)
p(zi|pai)

p(zi|pai)

=

zi   e

1

p(zi|pai).

(11.24)

(cid:14)

zi   e

(cid:14)

zi(cid:9)   e

(cid:14)

this method can be further extended using self-importance sampling (shachter and
peot, 1990) in which the importance sampling distribution is continually updated to
re   ect the current estimated posterior distribution.

11.1.5 sampling-importance-resampling
the rejection sampling method discussed in section 11.1.2 depends in part for
its success on the determination of a suitable value for the constant k. for many
pairs of distributions p(z) and q(z), it will be impractical to determine a suitable

11.1. basic sampling algorithms

535

value for k in that any value that is suf   ciently large to guarantee a bound on the
desired distribution will lead to impractically small acceptance rates.

as in the case of rejection sampling, the sampling-importance-resampling (sir)
approach also makes use of a sampling distribution q(z) but avoids having to de-
termine the constant k. there are two stages to the scheme.
in the    rst stage,
l samples z(1), . . . , z(l) are drawn from q(z). then in the second stage, weights
w1, . . . , wl are constructed using (11.23). finally, a second set of l samples is
drawn from the discrete distribution (z(1), . . . , z(l)) with probabilities given by the
weights (w1, . . . , wl).
the resulting l samples are only approximately distributed according to p(z),
but the distribution becomes correct in the limit l        . to see this, consider the
univariate case, and note that the cumulative distribution of the resampled values is
given by

(cid:2)
(cid:5)
l i(z(l) (cid:1) a)(cid:4)p(z(l))/q(z(l))
(cid:5)
l(cid:4)p(z(l))/q(z(l))

l:z(l)(cid:1)a

wl

p(z (cid:1) a) =

=

(11.25)

where i(.) is the indicator function (which equals 1 if its argument is true and 0
otherwise). taking the limit l        , and assuming suitable regularity of the dis-
tributions, we can replace the sums by integrals weighted according to the original
sampling distribution q(z)

(cid:6)
(cid:6)
(cid:6)

i(z (cid:1) a){(cid:4)p(z)/q(z)} q(z) dz
(cid:6)
{(cid:4)p(z)/q(z)} q(z) dz
i(z (cid:1) a)(cid:4)p(z) dz
(cid:6) (cid:4)p(z) dz

p(z (cid:1) a) =

=

=

i(z (cid:1) a)p(z) dz

(11.26)

which is the cumulative distribution function of p(z). again, we see that the normal-
ization of p(z) is not required.

for a    nite value of l, and a given initial sample set, the resampled values will
only approximately be drawn from the desired distribution. as with rejection sam-
pling, the approximation improves as the sampling distribution q(z) gets closer to
the desired distribution p(z). when q(z) = p(z), the initial samples (z(1), . . . , z(l))
have the desired distribution, and the weights wn = 1/l so that the resampled values
also have the desired distribution.

if moments with respect to the distribution p(z) are required, then they can be

536

11. sampling methods

evaluated directly using the original samples together with the weights, because

e[f(z)] =

f(z)p(z) dz

(cid:6)
(cid:6)
f(z)[(cid:4)p(z)/q(z)]q(z) dz
(cid:6)
[(cid:4)p(z)/q(z)]q(z) dz
(cid:7) l(cid:2)

wlf(zl).

=

(11.27)

l=1

11.1.6 sampling and the em algorithm
in addition to providing a mechanism for direct implementation of the bayesian
framework, monte carlo methods can also play a role in the frequentist paradigm,
for example to    nd maximum likelihood solutions. in particular, sampling methods
can be used to approximate the e step of the em algorithm for models in which the
e step cannot be performed analytically. consider a model with hidden variables
z, visible (observed) variables x, and parameters   . the function that is optimized
with respect to    in the m step is the expected complete-data log likelihood, given
by

(cid:6)

q(  ,   old) =

p(z|x,   old) ln p(z, x|  ) dz.

(11.28)

we can use sampling methods to approximate this integral by a    nite sum over sam-
ples {z(l)}, which are drawn from the current estimate for the posterior distribution
p(z|x,   old), so that

q(  ,   old) (cid:7) 1
l

ln p(z(l), x|  ).

(11.29)

l(cid:2)

l=1

the q function is then optimized in the usual way in the m step. this procedure is
called the monte carlo em algorithm.

it is straightforward to extend this to the problem of    nding the mode of the
posterior distribution over    (the map estimate) when a prior distribution p(  ) has
been de   ned, simply by adding ln p(  ) to the function q(  ,   old) before performing
the m step.

a particular instance of the monte carlo em algorithm, called stochastic em,
arises if we consider a    nite mixture model, and draw just one sample at each e step.
here the latent variable z characterizes which of the k components of the mixture
is responsible for generating each data point. in the e step, a sample of z is taken
from the posterior distribution p(z|x,   old) where x is the data set. this effectively
makes a hard assignment of each data point to one of the components in the mixture.
in the m step, this sampled approximation to the posterior distribution is used to
update the model parameters in the usual way.

11.2. id115

537

now suppose we move from a maximum likelihood approach to a full bayesian
treatment in which we wish to sample from the posterior distribution over the param-
eter vector   . in principle, we would like to draw samples from the joint posterior
p(  , z|x), but we shall suppose that this is computationally dif   cult. suppose fur-
ther that it is relatively straightforward to sample from the complete-data parameter
posterior p(  |z, x). this inspires the data augmentation algorithm, which alter-
nates between two steps known as the i-step (imputation step, analogous to an e
step) and the p-step (posterior step, analogous to an m step).

ip algorithm
i-step. we wish to sample from p(z|x) but we cannot do this directly. we

therefore note the relation

p(z|x) =

p(z|  , x)p(  |x) d  

(11.30)

and hence for l = 1, . . . , l we    rst draw a sample   (l) from the current esti-
mate for p(  |x), and then use this to draw a sample z(l) from p(z|  (l), x).

p-step. given the relation

p(  |x) =

p(  |z, x)p(z|x) dz

(11.31)
we use the samples {z(l)} obtained from the i-step to compute a revised
estimate of the posterior distribution over    given by

p(  |x) (cid:7) 1
l

p(  |z(l), x).

(11.32)

(cid:6)

(cid:6)

l(cid:2)

l=1

by assumption, it will be feasible to sample from this approximation in the
i-step.

note that we are making a (somewhat arti   cial) distinction between parameters   
and hidden variables z. from now on, we blur this distinction and focus simply on
the problem of drawing samples from a given posterior distribution.

11.2. id115

in the previous section, we discussed the rejection sampling and importance sam-
pling strategies for evaluating expectations of functions, and we saw that they suffer
from severe limitations particularly in spaces of high dimensionality. we therefore
turn in this section to a very general and powerful framework called markov chain
monte carlo (mcmc), which allows sampling from a large class of distributions,

538

11. sampling methods

and which scales well with the dimensionality of the sample space. markov chain
monte carlo methods have their origins in physics (metropolis and ulam, 1949),
and it was only towards the end of the 1980s that they started to have a signi   cant
impact in the    eld of statistics.

as with rejection and importance sampling, we again sample from a proposal
distribution. this time, however, we maintain a record of the current state z(   ), and
the proposal distribution q(z|z(   )) depends on this current state, and so the sequence

of samples z(1), z(2), . . . forms a markov chain. again, if we write p(z) =(cid:4)p(z)/zp,
we will assume that(cid:4)p(z) can readily be evaluated for any given value of z, although

section 11.2.1

the value of zp may be unknown. the proposal distribution itself is chosen to be
suf   ciently simple that it is straightforward to draw samples from it directly. at
each cycle of the algorithm, we generate a candidate sample z(cid:1) from the proposal
distribution and then accept the sample according to an appropriate criterion.
in the basic metropolis algorithm (metropolis et al., 1953), we assume that the
proposal distribution is symmetric, that is q(za|zb) = q(zb|za) for all values of
za and zb. the candidate sample is then accepted with id203

a(z(cid:1), z(   )) = min

1,

.

(11.33)

(cid:15)

(cid:16)

(cid:4)p(z(cid:1))
(cid:4)p(z(   ))

this can be achieved by choosing a random number u with uniform distribution over
the unit interval (0, 1) and then accepting the sample if a(z(cid:1), z(   )) > u. note that
if the step from z(   ) to z(cid:1) causes an increase in the value of p(z), then the candidate
point is certain to be kept.

if the candidate sample is accepted, then z(   +1) = z(cid:1), otherwise the candidate
point z(cid:1) is discarded, z(   +1) is set to z(   ) and another candidate sample is drawn
from the distribution q(z|z(   +1)). this is in contrast to rejection sampling, where re-
jected samples are simply discarded. in the metropolis algorithm when a candidate
point is rejected, the previous sample is included instead in the    nal list of samples,
leading to multiple copies of samples. of course, in a practical implementation,
only a single copy of each retained sample would be kept, along with an integer
weighting factor recording how many times that state appears. as we shall see, as
long as q(za|zb) is positive for any values of za and zb (this is a suf   cient but
not necessary condition), the distribution of z(   ) tends to p(z) as           . it should
be emphasized, however, that the sequence z(1), z(2), . . . is not a set of independent
samples from p(z) because successive samples are highly correlated. if we wish to
obtain independent samples, then we can discard most of the sequence and just re-
tain every m th sample. for m suf   ciently large, the retained samples will for all
practical purposes be independent. figure 11.9 shows a simple illustrative exam-
ple of sampling from a two-dimensional gaussian distribution using the metropolis
algorithm in which the proposal distribution is an isotropic gaussian.

further insight into the nature of id115 algorithms can be
gleaned by looking at the properties of a speci   c example, namely a simple random

11.2. id115

539

figure 11.9 a simple illustration using metropo-
lis algorithm to sample from a
gaussian distribution whose one
standard-deviation contour is shown
by the ellipse. the proposal distribu-
tion is an isotropic gaussian distri-
bution whose standard deviation is
0.2. steps that are accepted are
shown as green lines, and rejected
steps are shown in red. a total of
150 candidate samples are gener-
ated, of which 43 are rejected.

3

2.5

2

1.5

1

0.5

0

0

0.5

1

1.5

2

2.5

3

exercise 11.10

walk. consider a state space z consisting of the integers, with probabilities

p(z(   +1) = z(   )) = 0.5
p(z(   +1) = z(   ) + 1) = 0.25
p(z(   +1) = z(   )     1) = 0.25

(11.34)
(11.35)
(11.36)

where z(   ) denotes the state at step    . if the initial state is z(1) = 0, then by sym-
metry the expected state at time    will also be zero e[z(   )] = 0, and similarly it is
easily seen that e[(z(   ))2] =    /2. thus after    steps, the random walk has only trav-
elled a distance that on average is proportional to the square root of    . this square
root dependence is typical of random walk behaviour and shows that id93
are very inef   cient in exploring the state space. as we shall see, a central goal in
designing id115 methods is to avoid random walk behaviour.

11.2.1 markov chains
before discussing id115 methods in more detail, it is use-
ful to study some general properties of markov chains in more detail. in particular,
we ask under what circumstances will a markov chain converge to the desired dis-
tribution. a    rst-order markov chain is de   ned to be a series of random variables
z(1), . . . , z(m ) such that the following conditional independence property holds for
m     {1, . . . , m     1}

p(z(m+1)|z(1), . . . , z(m)) = p(z(m+1)|z(m)).

(11.37)

this of course can be represented as a directed graph in the form of a chain, an ex-
ample of which is shown in figure 8.38. we can then specify the markov chain by
giving the id203 distribution for the initial variable p(z(0)) together with the

540

11. sampling methods

(cid:2)

conditional probabilities for subsequent variables in the form of transition probabil-
ities tm(z(m), z(m+1))     p(z(m+1)|z(m)). a markov chain is called homogeneous
if the transition probabilities are the same for all m.

the marginal id203 for a particular variable can be expressed in terms of

the marginal id203 for the previous variable in the chain in the form

p(z(m+1)) =

p(z(m+1)|z(m))p(z(m)).

(11.38)

z(m)

a distribution is said to be invariant, or stationary, with respect to a markov chain
if each step in the chain leaves that distribution invariant. thus, for a homogeneous
markov chain with transition probabilities t (z(cid:4)
, z), the distribution p(cid:1)(z) is invariant
if
, z)p(cid:1)(z(cid:4)).
t (z(cid:4)

(cid:2)

p(cid:1)(z) =

(11.39)

z(cid:1)

note that a given markov chain may have more than one invariant distribution. for
instance, if the transition probabilities are given by the identity transformation, then
any distribution will be invariant.

a suf   cient (but not necessary) condition for ensuring that the required distribu-
tion p(z) is invariant is to choose the transition probabilities to satisfy the property
of detailed balance, de   ned by

p(cid:1)(z)t (z, z(cid:4)) = p(cid:1)(z(cid:4))t (z(cid:4)

, z)

(11.40)

for the particular distribution p(cid:1)(z).
it is easily seen that a transition id203
that satis   es detailed balance with respect to a particular distribution will leave that
distribution invariant, because

p(cid:1)(z(cid:4))t (z(cid:4)

, z) =

p(cid:1)(z)t (z, z(cid:4)) = p(cid:1)(z)

p(z(cid:4)|z) = p(cid:1)(z).

(11.41)

(cid:2)

z(cid:1)

(cid:2)

z(cid:1)

(cid:2)

z(cid:1)

a markov chain that respects detailed balance is said to be reversible.

our goal is to use markov chains to sample from a given distribution. we can
achieve this if we set up a markov chain such that the desired distribution is invariant.
however, we must also require that for m        , the distribution p(z(m)) converges
to the required invariant distribution p(cid:1)(z), irrespective of the choice of initial dis-
tribution p(z(0)). this property is called ergodicity, and the invariant distribution
is then called the equilibrium distribution. clearly, an ergodic markov chain can
have only one equilibrium distribution. it can be shown that a homogeneous markov
chain will be ergodic, subject only to weak restrictions on the invariant distribution
and the transition probabilities (neal, 1993).

in practice we often construct the transition probabilities from a set of    base   
transitions b1, . . . , bk. this can be achieved through a mixture distribution of the
form

t (z(cid:4)

, z) =

  kbk(z(cid:4)

, z)

(11.42)

k(cid:2)

k=1

11.2. id115

541

(cid:5)

for some set of mixing coef   cients   1, . . . ,   k satisfying   k (cid:2) 0 and
k   k = 1.
alternatively, the base transitions may be combined through successive application,
so that
t (z(cid:4)

, z1) . . . bk   1(zk   2, zk   1)bk(zk   1, z).

(cid:2)

(cid:2)

b1(z(cid:4)

, z) =

(11.43)

. . .

z1

zn   1

if a distribution is invariant with respect to each of the base transitions, then obvi-
ously it will also be invariant with respect to either of the t (z(cid:4)
, z) given by (11.42)
or (11.43). for the case of the mixture (11.42), if each of the base transitions sat-
is   es detailed balance, then the mixture transition t will also satisfy detailed bal-
ance. this does not hold for the transition id203 constructed using (11.43), al-
though by symmetrizing the order of application of the base transitions, in the form
b1, b2, . . . , bk, bk, . . . , b2, b1, detailed balance can be restored. a common ex-
ample of the use of composite transition probabilities is where each base transition
changes only a subset of the variables.

11.2.2 the metropolis-hastings algorithm
earlier we introduced the basic metropolis algorithm, without actually demon-
strating that it samples from the required distribution. before giving a proof, we
   rst discuss a generalization, known as the metropolis-hastings algorithm (hast-
ings, 1970), to the case where the proposal distribution is no longer a symmetric
function of its arguments. in particular at step    of the algorithm, in which the cur-
rent state is z(   ), we draw a sample z(cid:1) from the distribution qk(z|z(   )) and then
accept it with id203 ak(z(cid:1), z   ) where

ak(z(cid:1), z(   )) = min

1,

.

(11.44)

here k labels the members of the set of possible transitions being considered. again,
the evaluation of the acceptance criterion does not require knowledge of the normal-

izing constant zp in the id203 distribution p(z) =(cid:4)p(z)/zp. for a symmetric

proposal distribution the metropolis-hastings criterion (11.44) reduces to the stan-
dard metropolis criterion given by (11.33).

we can show that p(z) is an invariant distribution of the markov chain de   ned
by the metropolis-hastings algorithm by showing that detailed balance, de   ned by
(11.40), is satis   ed. using (11.44) we have

p(z)qk(z|z(cid:4))ak(z(cid:4)

, z) = min (p(z)qk(z|z(cid:4)), p(z(cid:4))qk(z(cid:4)|z))
= min (p(z(cid:4))qk(z(cid:4)|z), p(z)qk(z|z(cid:4)))
= p(z(cid:4))qk(z(cid:4)|z)ak(z, z(cid:4))

(11.45)

as required.

the speci   c choice of proposal distribution can have a marked effect on the
performance of the algorithm. for continuous state spaces, a common choice is a
gaussian centred on the current state, leading to an important trade-off in determin-
if the variance is small, then the
ing the variance parameter of this distribution.

(cid:15)

(cid:16)

(cid:4)p(z(cid:1))qk(z(   )|z(cid:1))
(cid:4)p(z(   ))qk(z(cid:1)|z(   ))

542

11. sampling methods

figure 11.10 schematic illustration of the use of an isotropic
gaussian proposal distribution (blue circle) to
sample from a correlated multivariate gaussian
distribution (red ellipse) having very different stan-
dard deviations in different directions, using the
metropolis-hastings algorithm.
in order to keep
the rejection rate low, the scale    of the proposal
distribution should be on the order of the smallest
standard deviation   min, which leads to random
walk behaviour in which the number of steps sep-
arating states that are approximately independent
is of order (  max/  min)2 where   max is the largest
standard deviation.

  min

  max

  

proportion of accepted transitions will be high, but progress through the state space
takes the form of a slow random walk leading to long correlation times. however,
if the variance parameter is large, then the rejection rate will be high because, in the
kind of complex problems we are considering, many of the proposed steps will be
to states for which the id203 p(z) is low. consider a multivariate distribution
p(z) having strong correlations between the components of z, as illustrated in fig-
ure 11.10. the scale    of the proposal distribution should be as large as possible
without incurring high rejection rates. this suggests that    should be of the same
order as the smallest length scale   min. the system then explores the distribution
along the more extended direction by means of a random walk, and so the number
of steps to arrive at a state that is more or less independent of the original state is
of order (  max/  min)2. in fact in two dimensions, the increase in rejection rate as   
increases is offset by the larger steps sizes of those transitions that are accepted, and
more generally for a multivariate gaussian the number of steps required to obtain
independent samples scales like (  max/  2)2 where   2 is the second-smallest stan-
dard deviation (neal, 1993). these details aside, it remains the case that if the length
scales over which the distributions vary are very different in different directions, then
the metropolis hastings algorithm can have very slow convergence.

11.3. id150

id150 (geman and geman, 1984) is a simple and widely applicable markov
chain monte carlo algorithm and can be seen as a special case of the metropolis-
hastings algorithm.

consider the distribution p(z) = p(z1, . . . , zm ) from which we wish to sample,
and suppose that we have chosen some initial state for the markov chain. each step
of the id150 procedure involves replacing the value of one of the variables
by a value drawn from the distribution of that variable conditioned on the values of
the remaining variables. thus we replace zi by a value drawn from the distribution
p(zi|z\i), where zi denotes the ith component of z, and z\i denotes z1, . . . , zm but
with zi omitted. this procedure is repeated either by cycling through the variables

11.3. id150

543

in some particular order or by choosing the variable to be updated at each step at
random from some distribution.

and at step    of the algorithm we have selected values z
replace z
bution

for example, suppose we have a distribution p(z1, z2, z3) over three variables,
(   )
3 . we    rst
obtained by sampling from the conditional distri-
p(z1|z

(   )
1 by a new value z

(   +1)
1

(   )
1 , z

and z

(11.46)

(   )
2 , z

(   )
3 ).

(   )
2

next we replace z
distribution

(   )
2

by a value z

(   +1)
2

obtained by sampling from the conditional

p(z2|z

(   +1)
1

(   )
3 )

, z

(11.47)

so that the new value for z1 is used straight away in subsequent sampling steps. then
we update z3 with a sample z

drawn from

(   +1)
3

p(z3|z

(   +1)
1

(   +1)
2

)

, z

(11.48)

and so on, cycling through the three variables in turn.

id150
1. initialize {zi : i = 1, . . . , m}
2. for    = 1, . . . , t :
(   +1)
1
(   +1)
2

    sample z
    sample z

    p(z1|z
    p(z2|z

...

(   )
2 , z
(   +1)
1

(   )
3 , . . . , z
, z

(   )
3 , . . . , z

(   )
m ).
(   )
m ).

    sample z

...

    sample z

(   +1)
j

    p(zj|z

(   +1)
1

, . . . , z

(   +1)
j   1 , z

(   )
j+1, . . . , z

(   )
m ).

(   +1)
m

    p(zm|z

(   +1)
1

(   +1)
2

, z

, . . . , z

(   +1)
m   1 ).

josiah willard gibbs
1839   1903

gibbs spent almost his entire life liv-
ing in a house built by his father in
new haven, connecticut.
in 1863,
gibbs was granted the    rst phd in
engineering in the united states,
and in 1871 he was appointed to
the    rst chair of mathematical physics in the united

states at yale, a post for which he received no salary
because at the time he had no publications. he de-
veloped the    eld of vector analysis and made contri-
butions to crystallography and planetary orbits. his
most famous work, entitled ontheequilibriumofhet-
erogeneous substances, laid the foundations for the
science of physical chemistry.

544

11. sampling methods

to show that this procedure samples from the required distribution, we    rst of
all note that the distribution p(z) is an invariant of each of the id150 steps
individually and hence of the whole markov chain. this follows from the fact that
when we sample from p(zi|{z\i), the marginal distribution p(z\i) is clearly invariant
because the value of z\i is unchanged. also, each step by de   nition samples from the
correct conditional distribution p(zi|z\i). because these conditional and marginal
distributions together specify the joint distribution, we see that the joint distribution
is itself invariant.

the second requirement to be satis   ed in order that the id150 proce-
dure samples from the correct distribution is that it be ergodic. a suf   cient condition
for ergodicity is that none of the conditional distributions be anywhere zero. if this
is the case, then any point in z space can be reached from any other point in a    nite
number of steps involving one update of each of the component variables. if this
requirement is not satis   ed, so that some of the conditional distributions have zeros,
then ergodicity, if it applies, must be proven explicitly.

the distribution of initial states must also be speci   ed in order to complete the
algorithm, although samples drawn after many iterations will effectively become
independent of this distribution. of course, successive samples from the markov
chain will be highly correlated, and so to obtain samples that are nearly independent
it will be necessary to subsample the sequence.

we can obtain the id150 procedure as a particular instance of the
metropolis-hastings algorithm as follows. consider a metropolis-hastings sampling
step involving the variable zk in which the remaining variables z\k remain    xed, and
for which the transition id203 from z to z(cid:1) is given by qk(z(cid:1)|z) = p(z(cid:1)
k|z\k).
we note that z(cid:1)\k = z\k because these components are unchanged by the sampling
step. also, p(z) = p(zk|z\k)p(z\k). thus the factor that determines the acceptance
id203 in the metropolis-hastings (11.44) is given by

a(z(cid:1), z) = p(z(cid:1))qk(z|z(cid:1))
p(z)qk(z(cid:1)|z)

=

k|z(cid:1)\k)p(z(cid:1)\k)p(zk|z(cid:1)\k)
k|z\k)

p(z(cid:1)
p(zk|z\k)p(z\k)p(z(cid:1)

= 1

(11.49)

where we have used z(cid:1)\k = z\k. thus the metropolis-hastings steps are always
accepted.

as with the metropolis algorithm, we can gain some insight into the behaviour of
id150 by investigating its application to a gaussian distribution. consider
a correlated gaussian in two variables, as illustrated in figure 11.11, having con-
ditional distributions of width l and marginal distributions of width l. the typical
step size is governed by the conditional distributions and will be of order l. because
the state evolves according to a random walk, the number of steps needed to obtain
independent samples from the distribution will be of order (l/l)2. of course if the
gaussian distribution were uncorrelated, then the id150 procedure would
be optimally ef   cient. for this simple problem, we could rotate the coordinate sys-
tem in order to decorrelate the variables. however, in practical applications it will
generally be infeasible to    nd such transformations.

one approach to reducing random walk behaviour in id150 is called
over-relaxation (adler, 1981). in its original form, this applies to problems for which

11.3. id150

545

z2

l

figure 11.11 illustration of id150 by alter-
nate updates of two variables whose
distribution is a correlated gaussian.
the step size is governed by the stan-
dard deviation of the conditional distri-
bution (green curve), and is o(l), lead-
ing to slow progress in the direction of
elongation of the joint distribution (red
ellipse). the number of steps needed
to obtain an independent sample from
the distribution is o((l/l)2).

l

z1

the conditional distributions are gaussian, which represents a more general class of
distributions than the multivariate gaussian because, for example, the non-gaussian
distribution p(z, y)     exp(   z2y2) has gaussian conditional distributions. at each
step of the id150 algorithm, the conditional distribution for a particular
component zi has some mean   i and some variance   2
i . in the over-relaxation frame-
work, the value of zi is replaced with

i =   i +   (zi       i) +   i(1       2
(cid:4)

i )1/2  

z

(11.50)

i , then so too does z

where    is a gaussian random variable with zero mean and unit variance, and   
is a parameter such that    1 <    < 1. for    = 0, the method is equivalent to
standard id150, and for    < 0 the step is biased to the opposite side of the
mean. this step leaves the desired distribution invariant because if zi has mean   i
(cid:4)
and variance   2
i. the effect of over-relaxation is to encourage
directed motion through state space when the variables are highly correlated. the
framework of ordered over-relaxation (neal, 1999) generalizes this approach to non-
gaussian distributions.
the practical applicability of id150 depends on the ease with which
samples can be drawn from the conditional distributions p(zk|z\k). in the case of
id203 distributions speci   ed using id114, the conditional distribu-
tions for individual nodes depend only on the variables in the corresponding markov
blankets, as illustrated in figure 11.12. for directed graphs, a wide choice of condi-
tional distributions for the individual nodes conditioned on their parents will lead to
conditional distributions for id150 that are log concave. the adaptive re-
jection sampling methods discussed in section 11.1.3 therefore provide a framework
for monte carlo sampling from directed graphs with broad applicability.

if the graph is constructed using distributions from the exponential family, and
if the parent-child relationships preserve conjugacy, then the full conditional distri-
butions arising in id150 will have the same functional form as the orig-

546

11. sampling methods

figure 11.12 the id150 method requires samples
to be drawn from the conditional distribution of a variable condi-
tioned on the remaining variables. for id114, this
conditional distribution is a function only of the states of the
nodes in the markov blanket. for an undirected graph this com-
prises the set of neighbours, as shown on the left, while for a
directed graph the markov blanket comprises the parents, the
children, and the co-parents, as shown on the right.

inal conditional distributions (conditioned on the parents) de   ning each node, and
so standard sampling techniques can be employed. in general, the full conditional
distributions will be of a complex form that does not permit the use of standard sam-
pling algorithms. however, if these conditionals are log concave, then sampling can
be done ef   ciently using adaptive rejection sampling (assuming the corresponding
variable is a scalar).

if, at each stage of the id150 algorithm, instead of drawing a sample
from the corresponding conditional distribution, we make a point estimate of the
variable given by the maximum of the conditional distribution, then we obtain the
iterated conditional modes (icm) algorithm discussed in section 8.3.3. thus icm
can be seen as a greedy approximation to id150.

because the basic id150 technique considers one variable at a time,
there are strong dependencies between successive samples. at the opposite extreme,
if we could draw samples directly from the joint distribution (an operation that we
are supposing is intractable), then successive samples would be independent. we can
hope to improve on the simple gibbs sampler by adopting an intermediate strategy in
which we sample successively from groups of variables rather than individual vari-
ables. this is achieved in the blocking id150 algorithm by choosing blocks
of variables, not necessarily disjoint, and then sampling jointly from the variables in
each block in turn, conditioned on the remaining variables (jensen et al., 1995).

11.4. slice sampling

we have seen that one of the dif   culties with the metropolis algorithm is the sensi-
tivity to step size. if this is too small, the result is slow decorrelation due to random
walk behaviour, whereas if it is too large the result is inef   ciency due to a high rejec-
tion rate. the technique of slice sampling (neal, 2003) provides an adaptive step size
that is automatically adjusted to match the characteristics of the distribution. again

it requires that we are able to evaluate the unnormalized distribution(cid:4)p(z).

consider    rst the univariate case. slice sampling involves augmenting z with
an additional variable u and then drawing samples from the joint (z, u) space. we
shall see another example of this approach when we discuss hybrid monte carlo in
section 11.5. the goal is to sample uniformly from the area under the distribution

  p(z)

u

z(   )

11.4. slice sampling

547

  p(z)

zmin

u

zmax

z

(a)

z(   )

(b)

z

(a) for a given value z(   ), a value of u is chosen uniformly in
figure 11.13 illustration of slice sampling.
the region 0 (cid:1) u (cid:1) ep(z(   )), which then de   nes a    slice    through the distribution, shown by the solid horizontal
(b) because it is infeasible to sample directly from a slice, a new sample of z is drawn from a region
lines.
zmin (cid:1) z (cid:1) zmax, which contains the previous value z(   ).

(11.51)

given by

where zp =

(cid:12)
(cid:1)p(z, u) =
(cid:28)(cid:4)p(z) dz. the marginal distribution over z is given by
(cid:6) ep(z)
(cid:6) (cid:1)p(z, u) du =

if 0 (cid:1) u (cid:1)(cid:4)p(z)
(cid:4)p(z)

1/zp
0

otherwise

du =

1
zp

0

zp

(11.52)

= p(z)

values. this can be achieved by alternately sampling z and u. given the value of z

and so we can sample from p(z) by sampling from(cid:1)p(z, u) and then ignoring the u
we evaluate(cid:4)p(z) and then sample u uniformly in the range 0 (cid:1) u (cid:1)(cid:4)p(z), which is
distribution de   ned by {z :(cid:4)p(z) > u}. this is illustrated in figure 11.13(a).
under(cid:1)p(z, u) invariant, which can be achieved by ensuring that detailed balance is

in practice, it can be dif   cult to sample directly from a slice through the distribu-
tion and so instead we de   ne a sampling scheme that leaves the uniform distribution

straightforward. then we    x u and sample z uniformly from the    slice    through the

satis   ed. suppose the current value of z is denoted z(   ) and that we have obtained
a corresponding sample u. the next value of z is obtained by considering a region
zmin (cid:1) z (cid:1) zmax that contains z(   ). it is in the choice of this region that the adap-
tation to the characteristic length scales of the distribution takes place. we want the
region to encompass as much of the slice as possible so as to allow large moves in z
space while having as little as possible of this region lying outside the slice, because
this makes the sampling less ef   cient.

one approach to the choice of region involves starting with a region containing
z(   ) having some width w and then testing each of the end points to see if they lie
within the slice.
if either end point does not, then the region is extended in that
direction by increments of value w until the end point lies outside the region. a
is then chosen uniformly from this region, and if it lies within the
candidate value z
slice, then it forms z(   +1). if it lies outside the slice, then the region is shrunk such
forms an end point and such that the region still contains z(   ). then another
that z

(cid:4)

(cid:4)

548

11. sampling methods

candidate point is drawn uniformly from this reduced region and so on, until a value
of z is found that lies within the slice.

slice sampling can be applied to multivariate distributions by repeatedly sam-
pling each variable in turn, in the manner of id150. this requires that
we are able to compute, for each component zi, a function that is proportional to
p(zi|z\i).

11.5. the hybrid monte carlo algorithm

as we have already noted, one of the major limitations of the metropolis algorithm
is that it can exhibit random walk behaviour whereby the distance traversed through
the state space grows only as the square root of the number of steps. the problem
cannot be resolved simply by taking bigger steps as this leads to a high rejection rate.
in this section, we introduce a more sophisticated class of transitions based on an
analogy with physical systems and that has the property of being able to make large
changes to the system state while keeping the rejection id203 small. it is ap-
plicable to distributions over continuous variables for which we can readily evaluate
the gradient of the log id203 with respect to the state variables. we will discuss
the dynamical systems framework in section 11.5.1, and then in section 11.5.2 we
explain how this may be combined with the metropolis algorithm to yield the pow-
erful hybrid monte carlo algorithm. a background in physics is not required as this
section is self-contained and the key results are all derived from    rst principles.

11.5.1 dynamical systems
the dynamical approach to stochastic sampling has its origins in algorithms for
simulating the behaviour of physical systems evolving under hamiltonian dynam-
ics. in a id115 simulation, the goal is to sample from a given
id203 distribution p(z). the framework of hamiltonian dynamics is exploited
by casting the probabilistic simulation in the form of a hamiltonian system. in order
to remain in keeping with the literature in this area, we make use of the relevant
dynamical systems terminology where appropriate, which will be de   ned as we go
along.
the dynamics that we consider corresponds to the evolution of the state variable
z = {zi} under continuous time, which we denote by    . classical dynamics is de-
scribed by newton   s second law of motion in which the acceleration of an object is
proportional to the applied force, corresponding to a second-order differential equa-
tion over time. we can decompose a second-order equation into two coupled    rst-
order equations by introducing intermediate momentum variables r, corresponding
to the rate of change of the state variables z, having components

ri =

dzi
d  

(11.53)

where the zi can be regarded as position variables in this dynamics perspective. thus

11.5. the hybrid monte carlo algorithm

549

for each position variable there is a corresponding momentum variable, and the joint
space of position and momentum variables is called phase space.

without loss of generality, we can write the id203 distribution p(z) in the

form

p(z) =

exp (   e(z))

1
zp

(11.54)

where e(z) is interpreted as the potential energy of the system when in state z. the
system acceleration is the rate of change of momentum and is given by the applied
force, which itself is the negative gradient of the potential energy

dri
d  

=        e(z)
   zi

.

(11.55)

it is convenient to reformulate this dynamical system using the hamiltonian

framework. to do this, we    rst de   ne the kinetic energy by

k(r) =

(cid:5)r(cid:5)2 =

1
2

1
2

r2
i .

(11.56)

(cid:2)

i

the total energy of the system is then the sum of its potential and kinetic energies

h(z, r) = e(z) + k(r)

(11.57)

exercise 11.15

where h is the hamiltonian function. using (11.53), (11.55), (11.56), and (11.57),
we can now express the dynamics of the system in terms of the hamiltonian equa-
tions given by

dzi
d  
dri
d  

=    h
   ri
=        h
   zi

.

(11.58)

(11.59)

william hamilton
1805   1865

william rowan hamilton was an
irish mathematician and physicist,
and child prodigy, who was ap-
pointed professor of astronomy at
trinity college, dublin, in 1827, be-
fore he had even graduated. one
of hamilton   s most important contributions was a new
formulation of dynamics, which played a signi   cant
role in the later development of quantum mechanics.

his other great achievement was the development of
quaternions, which generalize the concept of complex
numbers by introducing three distinct square roots of
minus one, which satisfy i2 = j2 = k2 = ijk =    1.
it is said that these equations occurred to him while
walking along the royal canal in dublin with his wife,
on 16 october 1843, and he promptly carved the
equations into the side of broome bridge. although
there is no longer any evidence of the carving, there is
now a stone plaque on the bridge commemorating the
discovery and displaying the quaternion equations.

550

11. sampling methods

(cid:12)
(cid:12)

(cid:2)
(cid:2)

i

i

(cid:12)
(cid:12)

(cid:2)
(cid:2)

i

i

during the evolution of this dynamical system, the value of the hamiltonian h is
constant, as is easily seen by differentiation

dh
d  

=

=

   h
   zi

dzi
d  

   h
   zi

   h
   ri

+    h
   ri
       h
   ri

dri
d  

   h
   zi

= 0.

(11.60)

(cid:13)
(cid:13)

a second important property of hamiltonian dynamical systems, known as li-
ouville   s theorem, is that they preserve volume in phase space. in other words, if
we consider a region within the space of variables (z, r), then as this region evolves
under the equations of hamiltonian dynamics, its shape may change but its volume
will not. this can be seen by noting that the    ow    eld (rate of change of location in
phase space) is given by

(cid:15)

(cid:16)

dz
d  
and that the divergence of this    eld vanishes

v =

,

dr
d  

div v =

=

   
   zi
       
   zi

dzi
d  

+    
   ri

dri
d  

   h
   ri

+    
   ri

   h
   zi

(cid:13)
(cid:13)

(11.61)

= 0.

(11.62)

now consider the joint distribution over phase space whose total energy is the

hamiltonian, i.e., the distribution given by

p(z, r) =

exp(   h(z, r)).

1
zh

(11.63)

using the two results of conservation of volume and conservation of h, it follows
that the hamiltonian dynamics will leave p(z, r) invariant. this can be seen by
considering a small region of phase space over which h is approximately constant.
if we follow the evolution of the hamiltonian equations for a    nite time, then the
volume of this region will remain unchanged as will the value of h in this region, and
hence the id203 density, which is a function only of h, will also be unchanged.
although h is invariant, the values of z and r will vary, and so by integrating
the hamiltonian dynamics over a    nite time duration it becomes possible to make
large changes to z in a systematic way that avoids random walk behaviour.

evolution under the hamiltonian dynamics will not, however, sample ergodi-
cally from p(z, r) because the value of h is constant. in order to arrive at an ergodic
sampling scheme, we can introduce additional moves in phase space that change
the value of h while also leaving the distribution p(z, r) invariant. the simplest
way to achieve this is to replace the value of r with one drawn from its distribution
conditioned on z. this can be regarded as a id150 step, and hence from

11.5. the hybrid monte carlo algorithm

551

exercise 11.16

section 11.3 we see that this also leaves the desired distribution invariant. noting
that z and r are independent in the distribution p(z, r), we see that the conditional
distribution p(r|z) is a gaussian from which it is straightforward to sample.

in a practical application of this approach, we have to address the problem of
performing a numerical integration of the hamiltonian equations. this will neces-
sarily introduce numerical errors and so we should devise a scheme that minimizes
the impact of such errors. in fact, it turns out that integration schemes can be devised
for which liouville   s theorem still holds exactly. this property will be important in
the hybrid monte carlo algorithm, which is discussed in section 11.5.2. one scheme
for achieving this is called the leapfrog discretization and involves alternately updat-

ing discrete-time approximations(cid:1)z and(cid:1)r to the position and momentum variables

using

((cid:1)z(  ))
(cid:1)ri(   +  /2) = (cid:1)ri(  )      
(cid:1)zi(   +  ) = (cid:1)zi(  ) +  (cid:1)ri(   +  /2)
(cid:1)ri(   +  ) = (cid:1)ri(   +  /2)      

   e
   zi

2

   e
   zi

2

((cid:1)z(   +  )).

(11.64)

(11.65)

(11.66)

we see that this takes the form of a half-step update of the momentum variables with
step size  /2, followed by a full-step update of the position variables with step size  ,
followed by a second half-step update of the momentum variables. if several leapfrog
steps are applied in succession, it can be seen that half-step updates to the momentum
variables can be combined into full-step updates with step size  . the successive
updates to position and momentum variables then leapfrog over each other. in order
to advance the dynamics by a time interval    , we need to take    /  steps. the error
involved in the discretized approximation to the continuous time dynamics will go to
zero, assuming a smooth function e(z), in the limit       0. however, for a nonzero
  as used in practice, some residual error will remain. we shall see in section 11.5.2
how the effects of such errors can be eliminated in the hybrid monte carlo algorithm.
in summary then, the hamiltonian dynamical approach involves alternating be-
tween a series of leapfrog updates and a resampling of the momentum variables from
their marginal distribution.

note that the hamiltonian dynamics method, unlike the basic metropolis algo-
rithm, is able to make use of information about the gradient of the log id203
distribution as well as about the distribution itself. an analogous situation is familiar
from the domain of function optimization. in most cases where gradient informa-
tion is available, it is highly advantageous to make use of it. informally, this follows
from the fact that in a space of dimension d, the additional computational cost of
evaluating a gradient compared with evaluating the function itself will typically be a
   xed factor independent of d, whereas the d-dimensional gradient vector conveys
d pieces of information compared with the one piece of information given by the
function itself.

552

11. sampling methods

11.5.2 hybrid monte carlo
as we discussed in the previous section, for a nonzero step size  , the discretiza-
tion of the leapfrog algorithm will introduce errors into the integration of the hamil-
tonian dynamical equations. hybrid monte carlo (duane et al., 1987; neal, 1996)
combines hamiltonian dynamics with the metropolis algorithm and thereby removes
any bias associated with the discretization.

speci   cally, the algorithm uses a markov chain consisting of alternate stochastic
updates of the momentum variable r and hamiltonian dynamical updates using the
leapfrog algorithm. after each application of the leapfrog algorithm, the resulting
candidate state is accepted or rejected according to the metropolis criterion based
on the value of the hamiltonian h. thus if (z, r) is the initial state and (z(cid:1), r(cid:1))
is the state after the leapfrog integration, then this candidate state is accepted with
id203

min (1, exp{h(z, r)     h(z(cid:1), r(cid:1))}) .

(11.67)

if the leapfrog integration were to simulate the hamiltonian dynamics perfectly,
then every such candidate step would automatically be accepted because the value
of h would be unchanged. due to numerical errors, the value of h may sometimes
decrease, and we would like the metropolis criterion to remove any bias due to this
effect and ensure that the resulting samples are indeed drawn from the required dis-
tribution. in order for this to be the case, we need to ensure that the update equations
corresponding to the leapfrog integration satisfy detailed balance (11.40). this is
easily achieved by modifying the leapfrog scheme as follows.

before the start of each leapfrog integration sequence, we choose at random,
with equal id203, whether to integrate forwards in time (using step size  ) or
backwards in time (using step size     ). we    rst note that the leapfrog integration
scheme (11.64), (11.65), and (11.66) is time-reversible, so that integration for l steps
using step size      will exactly undo the effect of integration for l steps using step
size  . next we show that the leapfrog integration preserves phase-space volume
exactly. this follows from the fact that each step in the leapfrog scheme updates
either a zi variable or an ri variable by an amount that is a function only of the other
variable. as shown in figure 11.14, this has the effect of shearing a region of phase
space while not altering its volume.
finally, we use these results to show that detailed balance holds. consider a
small region r of phase space that, under a sequence of l leapfrog iterations of
step size  , maps to a region r(cid:4)
. using conservation of volume under the leapfrog
iteration, we see that if r has volume   v then so too will r(cid:4)
. if we choose an initial
point from the distribution (11.63) and then update it using l leapfrog interactions,
the id203 of the transition going from r to r(cid:4)

is given by

1
zh

exp(   h(r))  v

1
2

min{1, exp(   h(r) + h(r(cid:4)))} .

(11.68)

where the factor of 1/2 arises from the id203 of choosing to integrate with a
positive step size rather than a negative one. similarly, the id203 of starting in

11.5. the hybrid monte carlo algorithm

553

ri

(cid:4)
i

r

zi

(cid:4)
i

z

figure 11.14 each step of the leapfrog algorithm (11.64)   (11.66) modi   es either a position variable zi or a
momentum variable ri. because the change to one variable is a function only of the other, any region in phase
space will be sheared without change of volume.

region r(cid:4)

and integrating backwards in time to end up in region r is given by
1
zh

min{1, exp(   h(r(cid:4)) + h(r))} .

exp(   h(r(cid:4)))  v

1
2

(11.69)

exercise 11.17

it is easily seen that the two probabilities (11.68) and (11.69) are equal, and hence
detailed balance holds. note that this proof ignores any overlap between the regions
r and r(cid:4)

but is easily generalized to allow for such overlap.

it is not dif   cult to construct examples for which the leapfrog algorithm returns
to its starting position after a    nite number of iterations. in such cases, the random
replacement of the momentum values before each leapfrog integration will not be
suf   cient to ensure ergodicity because the position variables will never be updated.
such phenomena are easily avoided by choosing the magnitude of the step size at
random from some small interval, before each leapfrog integration.

we can gain some insight into the behaviour of the hybrid monte carlo algo-
rithm by considering its application to a multivariate gaussian. for convenience,
consider a gaussian distribution p(z) with independent components, for which the
hamiltonian is given by

(cid:2)

i

(cid:2)

i

h(z, r) =

1
2

1
  2
i

z2
i +

1
2

r2
i .

(11.70)

our conclusions will be equally valid for a gaussian distribution having correlated
components because the hybrid monte carlo algorithm exhibits rotational isotropy.
during the leapfrog integration, each pair of phase-space variables zi, ri evolves in-
dependently. however, the acceptance or rejection of the candidate point is based
on the value of h, which depends on the values of all of the variables. thus, a
signi   cant integration error in any one of the variables could lead to a high prob-
ability of rejection. in order that the discrete leapfrog integration be a reasonably

554

11. sampling methods

good approximation to the true continuous-time dynamics, it is necessary for the
leapfrog integration scale   to be smaller than the shortest length-scale over which
the potential is varying signi   cantly. this is governed by the smallest value of   i,
which we denote by   min. recall that the goal of the leapfrog integration in hybrid
monte carlo is to move a substantial distance through phase space to a new state
that is relatively independent of the initial state and still achieve a high id203 of
acceptance. in order to achieve this, the leapfrog integration must be continued for a
number of iterations of order   max/  min.

by contrast, consider the behaviour of a simple metropolis algorithm with an
isotropic gaussian proposal distribution of variance s2, considered earlier. in order
to avoid high rejection rates, the value of s must be of order   min. the exploration of
state space then proceeds by a random walk and takes of order (  max/  min)2 steps
to arrive at a roughly independent state.

11.6. estimating the partition function

as we have seen, most of the sampling algorithms considered in this chapter re-
quire only the functional form of the id203 distribution up to a multiplicative
constant. thus if we write

pe(z) =

exp(   e(z))

1
ze

(11.71)

then the value of the id172 constant ze, also known as the partition func-
tion, is not needed in order to draw samples from p(z). however, knowledge of the
value of ze can be useful for bayesian model comparison since it represents the
model evidence (i.e., the id203 of the observed data given the model), and so
it is of interest to consider how its value might be obtained. we assume that direct
evaluation by summing, or integrating, the function exp(   e(z)) over the state space
of z is intractable.

for model comparison, it is actually the ratio of the partition functions for two
models that is required. multiplication of this ratio by the ratio of prior probabilities
gives the ratio of posterior probabilities, which can then be used for model selection
or model averaging.

one way to estimate a ratio of partition functions is to use importance sampling

(cid:5)
(cid:5)
from a distribution with energy function g(z)
(cid:5)
z exp(   e(z))
(cid:5)
z exp(   g(z))
z exp(   e(z) + g(z)) exp(   g(z))
z exp(   g(z))
(cid:2)

=
= eg(z)[exp(   e + g)]
(cid:7)

exp(   e(z(l)) + g(z(l)))

ze
zg

=

l

(11.72)

l(cid:2)

l=1

11.6. estimating the partition function

555

where {z(l)} are samples drawn from the distribution de   ned by pg(z). if the dis-
tribution pg is one for which the partition function can be evaluated analytically, for
example a gaussian, then the absolute value of ze can be obtained.

this approach will only yield accurate results if the importance sampling distri-
bution pg is closely matched to the distribution pe, so that the ratio pe/pg does not
have wide variations. in practice, suitable analytically speci   ed importance sampling
distributions cannot readily be found for the kinds of complex models considered in
this book.

an alternative approach is therefore to use the samples obtained from a markov
chain to de   ne the importance-sampling distribution. if the transition id203 for
the markov chain is given by t (z, z(cid:4)), and the sample set is given by z(1), . . . , z(l),
then the sampling distribution can be written as

exp (   g(z)) =

1
zg

which can be used directly in (11.72).

t (z(l), z)

(11.73)

methods for estimating the ratio of two partition functions require for their suc-
cess that the two corresponding distributions be reasonably closely matched. this is
especially problematic if we wish to    nd the absolute value of the partition function
for a complex distribution because it is only for relatively simple distributions that
the partition function can be evaluated directly, and so attempting to estimate the
ratio of partition functions directly is unlikely to be successful. this problem can be
tackled using a technique known as chaining (neal, 1993; barber and bishop, 1997),
which involves introducing a succession of intermediate distributions p2, . . . , pm   1
that interpolate between a simple distribution p1(z) for which we can evaluate the
id172 coef   cient z1 and the desired complex distribution pm (z). we then
have

zm
z1

= z2
z1

z3
z2

       zm
zm   1

(11.74)

in which the intermediate ratios can be determined using monte carlo methods as
discussed above. one way to construct such a sequence of intermediate systems
is to use an energy function containing a continuous parameter 0 (cid:1)    (cid:1) 1 that
interpolates between the two distributions

e  (z) = (1       )e1(z) +   em (z).

(11.75)

if the intermediate ratios in (11.74) are to be found using monte carlo, it may be
more ef   cient to use a single markov chain run than to restart the markov chain for
each ratio. in this case, the markov chain is run initially for the system p1 and then
after some suitable number of steps moves on to the next distribution in the sequence.
note, however, that the system must remain close to the equilibrium distribution at
each stage.

556

11. sampling methods

exercises

11.1 ((cid:12)) www show that the    nite sample estimator(cid:1)f de   ned by (11.2) has mean

equal to e[f] and variance given by (11.3).

11.2 ((cid:12)) suppose that z is a random variable with uniform distribution over (0, 1) and
   1(z) where h(y) is given by (11.6). show that y

that we transform z using y = h
has the distribution p(y).

11.3 ((cid:12)) given a random variable z that is uniformly distributed over (0, 1),    nd a trans-

formation y = f(z) such that y has a cauchy distribution given by (11.8).

11.4 ((cid:12) (cid:12))

suppose that z1 and z2 are uniformly distributed over the unit circle, as
shown in figure 11.3, and that we make the change of variables given by (11.10)
and (11.11). show that (y1, y2) will be distributed according to (11.12).

11.5 ((cid:12)) www let z be a d-dimensional random variable having a gaussian distribu-
tion with zero mean and unit covariance matrix, and suppose that the positive de   nite
symmetric matrix    has the cholesky decomposition    = llt where l is a lower-
triangular matrix (i.e., one with zeros above the leading diagonal). show that the
variable y =    + lz has a gaussian distribution with mean    and covariance   .
this provides a technique for generating samples from a general multivariate gaus-
sian using samples from a univariate gaussian having zero mean and unit variance.

11.6 ((cid:12) (cid:12)) www in this exercise, we show more carefully that rejection sampling does
indeed draw samples from the desired distribution p(z). suppose the proposal dis-
tribution is q(z) and show that the id203 of a sample value z being accepted is

given by(cid:4)p(z)/kq(z) where(cid:4)p is any unnormalized distribution that is proportional to
p(z), and the constant k is set to the smallest value that ensures kq(z) (cid:2)(cid:4)p(z) for all

values of z. note that the id203 of drawing a value z is given by the id203
of drawing that value from q(z) times the id203 of accepting that value given
that it has been drawn. make use of this, along with the sum and product rules of
id203, to write down the normalized form for the distribution over z, and show
that it equals p(z).

11.7 ((cid:12)) suppose that z has a uniform distribution over the interval [0, 1]. show that the

variable y = b tan z + c has a cauchy distribution given by (11.16).

11.8 ((cid:12) (cid:12)) determine expressions for the coef   cients ki in the envelope distribution
(11.17) for adaptive rejection sampling using the requirements of continuity and nor-
malization.

11.9 ((cid:12) (cid:12)) by making use of the technique discussed in section 11.1.1 for sampling
from a single exponential distribution, devise an algorithm for sampling from the
piecewise exponential distribution de   ned by (11.17).

11.10 ((cid:12)) show that the simple random walk over the integers de   ned by (11.34), (11.35),
and (11.36) has the property that e[(z(   ))2] = e[(z(     1))2] + 1/2 and hence by
induction that e[(z(   ))2] =    /2.

figure 11.15 a id203 distribution over two variables z1
and z2 that is uniform over the shaded regions
and that is zero everywhere else.

z2

exercises

557

z1

11.11 ((cid:12) (cid:12)) www show that the id150 algorithm, discussed in section 11.3,

satis   es detailed balance as de   ned by (11.40).

11.12 ((cid:12)) consider the distribution shown in figure 11.15. discuss whether the standard
id150 procedure for this distribution is ergodic, and therefore whether it
would sample correctly from this distribution

11.13 ((cid:12) (cid:12)) consider the simple 3-node graph shown in figure 11.16 in which the observed
node x is given by a gaussian distribution n (x|  ,   
   1) with mean    and precision
   . suppose that the marginal distributions over the mean and precision are given
by n (  |  0, s0) and gam(  |a, b), where gam(  |  ,  ) denotes a gamma distribution.
write down expressions for the conditional distributions p(  |x,   ) and p(  |x,   ) that
would be required in order to apply id150 to the posterior distribution
p(  ,   |x).

11.14 ((cid:12)) verify that the over-relaxation update (11.50), in which zi has mean   i and
(cid:4)
i with

variance   i, and where    has zero mean and unit variance, gives a value z
mean   i and variance   2
i .

11.15 ((cid:12)) www using (11.56) and (11.57), show that the hamiltonian equation (11.58)
is equivalent to (11.53). similarly, using (11.57) show that (11.59) is equivalent to
(11.55).

11.16 ((cid:12)) by making use of (11.56), (11.57), and (11.63), show that the conditional dis-

tribution p(r|z) is a gaussian.

figure 11.16 a graph involving an observed gaussian variable x with

prior distributions over its mean    and precision   .

  

  

x

558

11. sampling methods

11.17 ((cid:12)) www verify that the two probabilities (11.68) and (11.69) are equal, and hence

that detailed balance holds for the hybrid monte carlo algorithm.

appendix a

in chapter 9, we discussed probabilistic models having discrete latent variables, such
as the mixture of gaussians. we now explore models in which some, or all, of the
latent variables are continuous. an important motivation for such models is that
many data sets have the property that the data points all lie close to a manifold of
much lower dimensionality than that of the original data space. to see why this
might arise, consider an artificial data set constructed by taking one of the off-line
digits, represented by a 64 x 64 pixel grey-level image, and embedding it in a larger
image of size 100 x 100 by padding with pixels having the value zero (corresponding
to white pixels) in which the location and orientation of the digit is varied at random,
as illustrated in figure 12.1. each of the resulting images is represented by a point in
the 100 x 100 = 10, ooo-dimensional data space. however, across a data set of such
images, there are only three degrees offreedom of variability, corresponding to the
vertical and horizontal translations and the rotations. the data points will therefore
live on a subspace of the data space whose intrinsic dimensionality is three. note

559

560

12. continuous latent variables

figure 12.1 a synthetic data sel obtained by taking one of the off-line digit images and creating multi 
ple copies in each of which the digit has undergone a random displacement and rotation
within some larger image field. the resulting images each have 100 )( 100 = 10.000
pixels.

that the manifold will be nonlinear because. for instance. if we translate the digit
past a particular pixel, that pixel value will go from zero (white) 10 one (black) and
back to zero again. which is clearly a nonlinear function of the digit position.
in
this example. !.he lranslation and rotation parameters are latent variables because we
observe only the image vectors and are not told which values of the translation or
rotation variables were used to create them.

for real digit image data, there will be a funher degree of freedom arising from
scaling. moreover there will be multiple addilional degrees of freedom associaled
wilh more complex deformations due to the variability in an individual's wriling
3s well as lhe differences in writing slyles between individuals.
evenheless. the
number of such degrees of freedom will be small compared to the dimensionality of
ihe data set.

another example is provided by the oil flow data set. in which (for a given ge-
ometrical configuration of the gas, woller, and oil phases) there are only two degrees
of freedom of variability corresponding to the fraction of oil in the pipe and the frac 
tion of water (the fraction of gas ihen being determined). ahhough the data space
comprises 12 measuremenls, a data set of points will lie close to a iwo-dimensional
manifold embedded within this space. in this case, the manifold comprises scveral
distinct segments corresponding to different flow regimes. each such segment being
a (noisy) continuous two-dimensional manifold. if our goal is data compression. or
density modelling, then there can be benefits in exploiling this manifold struclure.

in praclice.

the data points will not be confined precisely to a smooth low 
dimensional manifold, and we can interpret the departures of data points from the
manifold as   noise'. this leads naturally to a generative view of such models in
which we first select a poinl within the manifold according to some latent variable
distribution and then generate an observed data point by :ldding noise, drawn from
some conditional distribution of the data varillbles given the latent varillbles.

thc simplest continuous latent variable model assumes gaussian distributions
for both thc latent and observed variables and makes use of a linear,gaussian de-
pendence of the observed variables on ihe slate of the latent variables. this leads
to a probabilislic fonnulation of the well-known technique of principal component
analysis (pea), as well as 10 a related model called factor analysis.

in this chapter w will begin wilh a slandard, nonprobabilistic treatment of pea.
and thcn we show how pea arises naturally as the maximum likelihood solution 10

appendix a

section 8.1..j

section 12.1

12.1. principal c01n[>om"1 analjsis

561

flgu,e12.2 p'if>cipal compooont a",,~" seeks" $pace
01 !owe, dimensionality. kt"(>wil as !he p<lno>
pal subspace "nd denoted i:jy the magenta "1
line. such itlet the grthogonet [jiojectioh 01
!he data points ('ed doisl onto tp'ns~
"""'imizes the varia,..,., of !he proja<:ted points
(green dois). an "it",nati"" ....finilion 01 pca
is based on m..mizing the """,-<>i  squares
of !he projection errors. ind'cated by the bfi.>e
lines.

s'crio" 12.2

s'di"" 12.4

a panlcula, fonn of linear-gau"ian latem "ariable model. this probabilistic refor 
mulation bring~ many ad\'imlag~s, su~h as tl>l: use i)f em for parameter eslimalion,
rrinciple<j c~tensioos 10 oli~turc, of pea model" and ba)'~sian formulat;ons that
allow tbe number of rrincipal com[>oncnts to be detennined aulomatically from !be
data. finally'. "c disl;us< briefly ""'eral gencrali,ation, of the latent yariable concept
that g<l be~ood tbe linear-gaussian assumption including non  gau"i"n i.tcnt yari 
abies .....hich lea'" to tbe fr.me....ork of indrl"'mj.m compon.nl anal,-.;., as ....ell a,
models ha"ing a nonlinear rclationship bet ....een latent and oose",e<j ,'luiable,.

____'c2=.~1. principal component analysis

principal compooem analy,;" or rca.;s a technique tha! is "'idely u<ed for appli.
cations such as dimensionality .-eduction, lossy data comprc"ion, feature e>tracti"".
and data v;,ualizatioll (jolliffe, 2(02). it;s also kno...." as tile karoan.n  i..,;"" tran,  
f~.

lbcrc an: t....o commonly used definitions of pea that giye rise to the >arne
algorithm. pea can be defined as the unhog<lnal projtttion of the data o/1to a lo....er
dimensionallincar space. kno....n as the pri/lcip.al $uh.   p.aa. soch that the \'ariance of
the projttted data i' ma~imi,e<j (1i",.liing. 1933). equi"alemly,;t can be defined as
tbe linear projection that minimi"'. the average projttlion cost. defined as t~ mean
squa.-ed distance !letween the data [>oint< and tbeir p<ojtttioo, (pearson, 19(1). the
l"j'"oc"s< of onhogonal projection i' illustraled in figute 12.2. we con,ider each of
these definitions in tum.

12,1.1 mllximllm variance lormulation
con,ider a dala set <if obser"\lations {x,,} where" = 1..... s, and x" i, a
euclidean variable "'ilh dimen,ionality d. our goal is to project if>/:: data onto a
'pace ha"ing dimen,ionality m < d" hile ill3jli",i,illg the "ariallce of the projttted
data. for the !noll..nl. we 'hall assume that tbe "alue of m is g;\  en. latcr in this

562

12. continuous latent variables

chapter, we shall consider techniques to determine an appropriate value of iv! from
the data.

to begin with, consider the projection onto a one-dimensional space (m = 1).
we can define the direction of this space using a d-dimensional vector ul, which
for convenience (and without loss of generality) we shall choose to be a unit vector
so that uf ul = 1 (note that we are only interested in the direction defined by ul,
not in the magnitude of ul itself). each data point x n is then projected onto a scalar
value uf x n . the mean of the projected data is ufx where x is the sample set mean
given by

and the variance of the projected data is given by

(12.1)

(12.2)

where s is the data covariance matrix defined by

s = - "(xn - x)(xn - x)t

1 n
nlj
n=l

.

(12.3)

appendix e

we now maximize the projected variance ufsul with respect to ul. clearly, this has
to be a constrained maximization to prevent ilulll ..... 00. the appropriate constraint
comes from the id172 condition uf ul = 1. to enforce this constraint,
we introduce a lagrange multiplier that we shall denote by ai, and then make an
unconstrained maximization of

(12.4)

by setting the derivative with respect to ul equal to zero, we see that this quantity
will have a stationary point when

(12.5)

which says that ul must be an eigenvector of s. if we left-multiply by uf and make
use of uf ul = 1, we see that the variance is given by

and so the variance will be a maximum when we set ul equal to the eigenvector
having the largest eigenvalue ai. this eigenvector is known as the first principal
component.

we can define additional principal components in an incremental fashion by
choosing each new direction to be that which maximizes the projected variance

(12.6)

exercise 12.1

section 12.2.2

appendix c

12.1. principal component analysis

563

amongst all possible directions orthogonal to those already considered. if we con 
sider the general case of an m -dimensional projection space, the optimal linear pro 
jection for which the variance of the projected data is maximized is now defined by
the m eigenvectors u 1, ... , u m of the data covariance matrix s corresponding to the
m largest eigenvalues >'1, ... ,am. this is easily shown using proof by induction.
to summarize, principal component analysis involves evaluating the mean x
and the covariance matrix s of the data set and then finding the m eigenvectors of s
corresponding to the m largest eigenvalues. algorithms for finding eigenvectors and
eigenvalues, as well as additional theorems related to eigenvector decomposition,
can be found in golub and van loan (1996). note that the computational cost of
computing the full eigenvector decomposition for a matrix of size d x dis o(d3).
if we plan to project our data onto the first m principal components, then we only
need to find the first m eigenvalues and eigenvectors. this can be done with more
efficient techniques, such as the power method (golub and van loan, 1996), that
scale like o(md 2 ), or alternatively we can make use of the em algorithm.

12.1.2 minimum-error formulation
we now discuss an alternative formulation of pea based on projection error
minimization. to do this, we introduce a complete orthonormal set of d-dimensional
basis vectors {ui} where i = 1, ... , d that satisfy

(12.7)

because this basis is complete, each data point can be represented exactly by a linear
combination of the basis vectors

d

x n = laniui

i=l

(12.8)

where the coefficients ani will be different for different data points. this simply
corresponds to a rotation of the coordinate system to a new system defined by the
{ui}, and the original d components {xnl' ... , xnd} are replaced by an equivalent
set {anl' ... ,and}. taking the inner product with uj, and making use of the or 
thonormality property, we obtain anj = x;uj, and so without loss of generality we
can write

d

x n = l (x~ui) ui  

(12.9)

i=l

our goal, however, is to approximate this data point using a representation in 
volving a restricted number m < d of variables corresponding to a projection onto
a lower-dimensional subspace. the m -dimensional linear subspace can be repre 
sented, without loss of generality, by the first m of the basis vectors, and so we
approximate each data point x n by
m
xn = l

d
zniui + l

(12.10)

biui

i=l

i=m+l

564

12. continuous latent variables

where the {zni} depend on the particular data point, whereas the {bd are constants
that are the same for all data points. we are free to choose the {ui}, the {zni}, and
the {bd so as to minimize the distortion introduced by the reduction in dimensional 
ity. as our distortion measure, we shall use the squared distance between the original
data point x n and its approximation xn , averaged over the data set, so that our goal
is to minimize

n

j = ~ l ilxn - xn 11 2

.

(12.11)

n=l

consider first of all the minimization with respect to the quantities {zni}. sub 
stituting for xn , setting the derivative with respect to znj to zero, and making use of
the orthonormality conditions, we obtain

where j = 1, ... ,m. similarly, setting the derivative of j with respect to bi to zero,
and again making use of the orthonormality relations, gives

(12.12)

(12.13)
where j = m +1, ... ,d. if we substitute for zni and bi , and make use of the general
expansion (12.9), we obtain

b
j = x uj

-t

x n - x n = l {( x n - x)tud ui

d

i=m+l

(12.14)

from which we see that the displacement vector from x n to xn lies in the space
orthogonal to the principal subspace, because it is a linear combination of {ud for
i = m + 1, ... , d, as illustrated in figure 12.2. this is to be expected because the
projected points xn must lie within the principal subspace, but we can move them
freely within that subspace, and so the minimum error is given by the orthogonal
projection.

we therefore obtain an expression for the distortion measure j as a function

purely of the {ud in the form

1 ~ ~ (t

j = n l

l

_t)2

= l u i sui.

t

d

(12.15)

x n ui - x ui

n=l i=m+l

i=m+l

there remains the task of minimizing j with respect to the {ui}, which must
be a constrained minimization otherwise we will obtain the vacuous result ui = o.
the constraints arise from the orthonormality conditions and, as we shall see, the
solution will be expressed in terms of the eigenvector expansion of the covariance
matrix. before considering a formal solution, let us try to obtain some intuition about
the result by considering the case of a two-dimensional data space d = 2 and a one 
dimensional principal subspace m = 1. we have to choose a direction u2 so as to

12.1. principal component analysis

565

minimize j = uisu2' subject to the id172 constraint ui u2 = 1. using a
lagrange multiplier a2 to enforce the constraint, we consider the minimization of

(12.16)

setting the derivative with respect to u2 to zero, we obtain su2 = a2u2 so that u2
is an eigenvector of s with eigenvalue a2. thus any eigenvector will define a sta 
tionary point of the distortion measure. to find the value of j at the minimum, we
back-substitute the solution for u2 into the distortion measure to give j = a2. we
therefore obtain the minimum value of j by choosing u2 to be the eigenvector corre 
sponding to the smaller of the two eigenvalues. thus we should choose the principal
subspace to be aligned with the eigenvector having the larger eigenvalue. this result
accords with our intuition that, in order to minimize the average squared projection
distance, we should choose the principal component subspace to pass through the
mean of the data points and to be aligned with the directions of maximum variance.
for the case when the eigenvalues are equal, any choice of principal direction will
give rise to the same value of j.

the general solution to the minimization of j for arbitrary d and arbitrary m <
d is obtained by choosing the {ui} to be eigenvectors of the covariance matrix given
by

(12.17)
where i = 1, ... ,d, and as usual the eigenvectors {ui} are chosen to be orthonor 
mal. the corresponding value of the distortion measure is then given by

sui = aiui

d

j= l ai

i=m+l

(12.18)

which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal
to the principal subspace. we therefore obtain the minimum value of j by selecting
these eigenvectors to be those having the d - m smallest eigenvalues, and hence
the eigenvectors defining the principal subspace are those corresponding to the m
largest eigenvalues.

although we have considered m < d, the pca analysis still holds if m =
d, in which case there is no id84 but simply a rotation of the
coordinate axes to align with principal components.

finally, it is worth noting that there exists a closely related linear dimensionality
reduction technique called canonical correlation analysis, or cca (hotelling, 1936;
bach and jordan, 2002). whereas pca works with a single random variable, cca
considers two (or more) variables and tries to find a corresponding pair of linear
subspaces that have high cross-correlation, so that each component within one of the
subspaces is correlated with a single component from the other subspace. its solution
can be expressed in terms of a generalized eigenvector problem.

12.1.3 applications of pea
we can illustrate the use of pca for data compression by considering the off 
line digits data set. because each eigenvector of the covariance matrix is a vector

exercise 12.2

appendix a

566

12. col\'tinuolis latf;i\'t \'ariaiiles

figure 12.3 the mean ~'" x aklog with !he ii"'t lou' pca e;gerrvecl<)rll ul,.

cligits data set. t<>getl'ler with !he correspondi~~.

.. '"

lor the 011-.....

;n the oiigi",,1 d-<limensional space. we can represent tho eigenw:cto<s as imago< of
tho same silo as ,1>0 data poi",,_ 11,. first ih'e .ig.n,'occofs. along wich tl>o corre 
sponding .igen,'slue,. are <iio"'n in figure 12,3, a plo! ofll>o complete spect"'m uf
oigo",  alue,. sone<! into decreasing order. is shown in figure 12.4{ai. the di'tortion
measure j assqciated wilh choo<ing a particular value of m is gi.'en by tho sum
of the eig.n",lues from m + i up to 0 and is pto!ted for different ,'aluo< of .\1 in
figure 12,4(b).

if "'e <utlslitut. (12, 12) and (12.13) into (12.10). we can write the i'ca appro~  

imation to a data "eel'" x~ i" the fonn

'- ~ l{x~",)u,+ i: (xl'u,)u,

._m+l

"

m

.-.

m

- x+l(x~u,-xtu,)u;

    ,
,
,
,

, 10'

(12.19)

(12.20)

" ,

~

,.,

~ ;

"-

"

0

" ,

~

,.,

~ "

,

to'

,
,
"

,
"

fiiiure 12,4 (a) piol at !he ejoi;nv.loo .".,etrum lor the off  1ine digits data set
(b) p10t 01 !he sum at the
<:liscarded ."".ioos, which "'l'fesoots!he s.um-ol  sq"",es distortlon j i<*~ by projecti<xl the data onto
a p<incipal componenl slll>spaee '" dimensionalitv m.

11. 1. i'rindpall.:"l11pon~nt anal~  .i.

567

fiiiur. 1:1:.5 an ",>gi",,1 ~mpie irom li>e 011  _ digils data ...ttoll"1her with its pea re<:onstnxlions
oblair...:! by 'e1aio"li!xl ,if j)<incipal ~n1s 10< various val,," 01 ,if. as ,ii increason
!tie re<:onst,uctiofi ~s more ao::urate and woukl ~ portee! when .-if k d ~
28 x 28 ~ ."-1.

ai''''''''/;'\' a

seer/on 9.1

where we ha"e made moe of the relation
"
,-,

x = l (x'",) u;

(12.21)

which follow. from the completene" of the {u, i, thi. represent. a contpre"ioo
"f the data >ct. ilttau>e for each data poim we ha,.. repla  d the v  dimensiooal
"o<:lor x" wilh an ,i[.din>en,ional "o<:tor having componem, (x~'" _ x'",). 11ie
'mailer the "alue of m. the greater the degree of comp.-e",ion. example. of pea
,""on't""tioos of data points for the digits data set are shown in figure 12.5

anolher application of priocipal compcmenl analy,i. i' to data pre-processing.
in thi' case, lhe goal is no! dimensionality reduc1ion but rather the tmn,formmion of
a data sel in or<k' to standa'lli'.e eenain of ils pmpenies. this can be in'portanl in
allowing .ubsequent pallem ,""ognition algorithm. 10 be applied successfully 10 the
data >ct. typically. il is done wilen the original "ariable. are mea,ured in "arioos dif.
ferent unil' or !la"e significantly difterent ,'ariabilil}'. for instance in the old faithful
data sel. the time betv.-een eruption. i. typicany an order of magni1ude greater than
lhe durali"" of.n erupt;,,". when w'e applied the ".nlcans algorill"" 10 thi< data
set, ".-e first made a separ.te linear re-sealing of the individual "anable' socb thm
each "ariable had zero mean and unit "ariance. llus is known as slllnlardiv  .,g the
dota. and the co\'anance matrix for lhe 'lando,di/,ed dala has components

(12,22)

where <1, is the ,'anaoce of :c,. this i< known as the (",,,el,,,;,,,, matri.' of the original
dota and ha' the propeny thai if t""o rompooent, x; and x, of the data are perfee1ly
correl.ted. then ai _ i.   nd if they a.-e uocorrelated. then ai _ o.

11",,'1""', using pea we can make a it>of'e subst.mial nonnalizat;oo of the data
to gi\'c it zero mean and unit co'  ariance. so that different "anables become derorre 
late<l to do this. we first ""rile the ei8cn"cclor equation (12, 17) in the form

su= ul

(12.23)

568

12. continuous latent variables

100

80

90 00'

,=~o

70

60

50

40

2

0

-2

2

0

-2

0

b o

000

0

08 0

0
0 tj ~

cpo 0

o~
~ooid

2

4

6

-2

0

2

-2

0

2

figure 12.6 illustration of the effects of linear pre-processing applied to the old faithful data set. the plot on
the left shows the original data. the centre plot shows the result of standardizing the individual variables to zero
mean and unit variance. also shown are the principal axes of this normalized data set, plotted over the range
  a~/2. the plot on the right shows the result of whitening of the data to give it zero mean and unit covariance.

where l is a d x d diagonal matrix with elements ai, and u is a d x d orthog 
onal matrix with columns given by ui. then we define, for each data point x n , a
transformed value given by

where x is the sample mean defined by (12.1). clearly, the set {yn} has zero mean,
and its covariance is given by the identity matrix because

(12.24)

n

1~ l l -1/2ut(xn - x)(xn - x)tul-1/2
l ~1/2utsul -1/2 = l -1/2ll-1/2 = i.

n=l

(12.25)

appendix a

appendix a

this operation is known as whitening or sphereing the data and is illustrated for the
old faithful data set in figure 12.6.

it is interesting to compare pca with the fisher linear discriminant which was
discussed in section 4.1.4. both methods can be viewed as techniques for linear
id84. however, pca is unsupervised and depends only on the
values x n whereas fisher linear discriminant also uses class-label information. this
difference is highlighted by the example in figure 12.7.

another common application of principal component analysis is to data visual 
ization. here each data point is projected onto a two-dimensional (m = 2) principal
subspace, so that a data point x n is plotted at cartesian coordinates given by x'j. u1
and x'j. u2, where ul and u2 are the eigenvectors corresponding to the largest and
second largest eigenvalues. an example of such a plot, for the oil flow data set, is
shown in figure 12.8.

12.1. i'<incipal cl)m ..."n~nt anal}'s;,

569

fig"", 12.7

a comparison 01 pro:ipal compo 
mnt analysis ....111 fisha(s linaar
discriminant 101 """", <*man""'"
ality r&duclion. here too data in
two dimansions, belonging to two
classes siiowi1 in red and blue. is
to be pfoi"cled onto a s.ingle di  
mension. pca c/>xlsas the direc  
tion 01 maximum varia""e. siiowi1
try tha ma9""ta co""'. wt11ch leads
to strong class overlap. whereas
!he fisl>ef iimar discfornillant takes
accoun1 <:a too class labels and
leads to a projection onto the g<ean
cum! giving much t>etler class
separation

"

~

.. ,

. "., - . .' ~.   ''''

'~'" ~ .         ._',
..'

':r-'---~+  _..-:--'~-j

~,.,
."
.,':----;!---;-"
3

_.s

0

fig"", 12.8 visualilatlon 01 !he oill'low <lata liet obtained
try projoecting the <lata onto the lirst two prin.
cipal compone<1ts. the <ed, blue, and 9r&en
points corre-spond to !he 'iamini(,
't>omo-
genoous', and '8nnula~ flow oonligurations
",specriveiy.

12.1.4 pea for high-dimensional data
in some application. of plitlcipal component analysis. the number of data points
is smaller than t!>c dimensionality of troe data 'pace. foi" example. ",e might want to
apply pea to a data <el of a few hundred images, each of ,,'hich rorrespooos to a
"eetor in a 'pace of poientially .....ml million dimensiolls (coitesponding tn thfl'e
enlour "alues for each of the pi.",ls in troe image), noie that in a d-<limen,ional space
a set of jy points. ",'here n < d. defines a linear subspa::e ",hose dimensi"nality
is at ""'st n - 1, and so there is linle point in applying pea for ,'alue< of m
thai"'" greater than n -
indeed, if "'e pelf"",, pea we will find that at least
d - n + i of the eigen".lues art lero. eorrespnnding tq eigenvectors aloog ",hose
direclioos the data <el has 10m varianee. funhem>ore. typical algol"ithm, for finding
the eigen,'eet"" of a d x d matrix ha"e a computatiooal eosl thm scales like o( d~j.
aoo so for appliealions such as the image e,ample. a direc' application of pea will
be computatiooally infe,,-sibje.

w. can resoh'e this problem as foil",",'" fir;l. let us define x to be the (n " dj  

i,

570

12. continuous latent variables

dimensional centred data matrix, whose nth row is given by (x n - x)t. the covari 
ance matrix (12.3) can then be written as s = n- 1xtx, and the corresponding
eigenvector equation becomes

t

1
-x xui = aiui
n

.

now pre-multiply both sides by x to give

1
nxx (xui) = ai(xui)'

t

if we now define vi = xui, we obtain

t

1
-xx vi = aivi
n

(12.26)

(12.27)

(12.28)

which is an eigenvector equation for the n x n matrix n- 1xxt . we see that this
has the same n -1 eigenvalues as the original covariance matrix (which itself has an
additional d - n + 1 eigenvalues of value zero). thus we can solve the eigenvector
problem in spaces of lower dimensionality with computational cost o(n3 ) instead
of o(d 3 ). in order to determine the eigenvectors, we multiply both sides of (12.28)
by x t to give

1 t)
nx x (x vi) = ai(x vi)

t

t

(

(12.29)

from which we see that (xtvi) is an eigenvector of s with eigenvalue ai. note,
however, that these eigenvectors need not be normalized. to determine the appropri 
ate id172, we re-scale ui ex: x tvi by a constant such that ilui ii = 1, which,
assuming vi has been normalized to unit length, gives

1

ui = ( nai)1/2 x vi  

t

(12.30)

in summary, to apply this approach we first evaluate xxt and then find its eigen 
vectors and eigenvalues and then compute the eigenvectors in the original data space
using (12.30).

12.2. probabilistic pea

the formulation of pca discussed in the previous section was based on a linear
projection of the data onto a subspace of lower dimensionality than the original data
space. we now show that pca can also be expressed as the maximum likelihood
solution of a probabilistic latent variable model. this reformulation of pca, known
as probabilistic pea, brings several advantages compared with conventional pca:

    probabilistic pca represents a constrained form of the gaussian distribution
in which the number of free parameters can be restricted while still allowing
the model to capture the dominant correlations in a data set.

section 12.2.2

12.2. probabilistic pea

571

    we can derive an em algorithm for pca that is computationally efficient in
situations where only a few leading eigenvectors are required and that avoids
having to evaluate the data covariance matrix as an intermediate step.

    the combination of a probabilistic model and em allows us to deal with miss 

ing values in the data set.

    mixtures of probabilistic pca models can be formulated in a principled way

and trained using the em algorithm.

section 12.2.3

    probabilistic pca forms the basis for a bayesian treatment of pca in which
the dimensionality of the principal subspace can be found automatically from
the data.

    the existence of a likelihood function allows direct comparison with other
probabilistic density models. by contrast, conventional pca will assign a low
reconstruction cost to data points that are close to the principal subspace even
if they lie arbitrarily far from the training data.

    probabilistic pca can be used to model class-conditional densities and hence

be applied to classification problems.

    the probabilistic pca model can be run generatively to provide samples from

the distribution.

this formulation of pca as a probabilistic model was proposed independently by
tipping and bishop (1997, 1999b) and by roweis (1998). as we shall see later, it is
closely related to factor analysis (basilevsky, 1994).

probabilistic pca is a simple example of the linear-gaussian framework, in
which all of the marginal and conditional distributions are gaussian. we can formu 
late probabilistic pca by first introducing an explicit latent variable z corresponding
to the principal-component subspace. next we define a gaussian prior distribution
p(z) over the latent variable, together with a gaussian conditional distribution p(xl z)
for the observed variable x conditioned on the value of the latent variable. specifi 
cally, the prior distribution over z is given by a zero-mean unit-covariance gaussian

p(z) = n(zio, i).

(12.31)

similarly, the conditional distribution of the observed variable x, conditioned on the
value of the latent variable z, is again gaussian, of the form

p(xlz) = n(xlwz + j-l, a 2i)

(12.32)

in which the mean of x is a general linear function of z governed by the d x m
matrix wand the d-dimensional vector j-l. note that this factorizes with respect to
the elements of x, in other words this is an example of the naive bayes model. as
we shall see shortly, the columns of w span a linear subspace within the data space
that corresponds to the principal subspace. the other parameter in this model is the
scalar a 2 governing the variance of the conditional distribution. note that there is no

section 8.1.4

section 8.2.2

572

11. continuous lat!::nt vanim1li::s

/.-,,,,,,,

,

,

flgu.. 12.9 i\n ~i"'tfat"" oilt>e ii"""fative vi&w oi1t>e p<ot>abi!;st", pea modeifof" two-dimensiooal <!ala
space and a on&-<lirnent.ionallat/l<1t space, an ob&erved <!ala point x is generated by first drawing a value i
fof 1t>e iat&n1 vafiatlle /f(lm ~s prior dist,~t"" p(~) and itlen drawing a val"" fof x lrom an iso/fopk: gaussian
distr~t"" (iijust,al&(l by the red cir<:ie's) having mean wi +" and coy8r1.once ,,'1 the l/f&er\ ellips.&$ show l!le
density """toors!of the marg'''''1 dis1r1bulion pix).

loss of ge""rajity in assuming a zero mean. unit co\'ariance gau"ian for the latent
distributi"n ii{z) because a more id197eral gau"i3n di"ributi"n would gi"e rise to an
equivalent probabili"ic n>odel.

we can view the probabilistic pea model from a geoerati"e \'iew""int in "hich
a sampled '-alue of the ob""yed ,..riable is obiained by first choo,ing a ,..iue for
the latent ,'ariahle aod then >ampling the oo",,,'e;j ,-ariable cooditioned on this lao
tent \'alue, specifically, the v-dimen'ional oo"''''ed '-ariable x is defined by a lin  
ea, tran,formati,," of the '\/  dimen,i"nal latcnt '-ariable z plu, additi'-e gaussian
'noise' , <0 that

,,=\vz+,,+~

(12.33)

w!>ere z is an m-di""'nsional gaussian lalent variable. and .. is a v  dimensi"nal
,ero-mean gau..ian-distributed noi.., "ariable witb co'-ariance ,,21. this generative
process is illustrated in figure 12.9. noie that this frame".-orl< is based on a mapping
from latent ,pace 10 data space. in contrast 10 the nl()l'(: c(""'cnti,,,,"1 "iew "f i'ca
dis.cus"'d alx",e, 11ie ",,'e= mapping, from data space to the latent space. ,,-ill he
oolained ,honly using ha ycs   lhwn:m.

suf!ll'osc we wish 10 deten"ine the "alues ofll>o parameters \v. i' and ,,' using
maximum likelihuo<l, to write """"n lhe likeliltood function, we need an ""pression
for tl>o marginal distributioo p{") of tl>o ~,,'ed ...riahle_ this is exprt__sed. fmn'
the sum aod p,oduct rules "fid203, in the form

(11,34)

e,e,,-ise 12,7

ll""aus(: this corresponds to a linear  gau"i,n lt1(llicl thi< marginal di,tribulion is
again gaussian. atld is given by

,,(,,) _ n{xllf,c)

(ius)

12.2. probabilistic pea

573

where the d x d covariance matrix c is defined by
21.

c = wwt + 0-

(12.36)

this result can also be derived more directly by noting that the predictive distribution
will be gaussian and then evaluating its mean and covariance using (12.33). this
gives

ie[x]
cov[x]

ie[wz + jl + e] = jl
ie [(wz + e)(wz + e)t]
ie [wzztwt] + ie[eet] = wwt + 0-

21

(12.37)

(12.38)

where we have used the fact that z and e are independent random variables and hence
are uncorrelated.

intuitively, we can think of the distribution p(x) as being defined by taking an
isotropic gaussian 'spray can' and moving it across the principal subspace spraying
2 and weighted by the prior distribution.
gaussian ink with density determined by 0-
the accumulated ink density gives rise to a 'pancake' shaped distribution represent 
ing the marginal density p(x).

the predictive distribution p(x) is governed by the parameters jl, w, and 0-

2
   
however, there is redundancy in this parameterization corresponding to rotations of
the latent space coordinates. to see this, consider a matrix w = wr where r is
an orthogonal matrix. using the orthogonality property rrt = i, we see that the
quantity wwt that appears in the covariance matrix c takes the form

(12.39)

and hence is independent of r. thus there is a whole family of matrices w all of
which give rise to the same predictive distribution. this invariance can be understood
in terms of rotations within the latent space. we shall return to a discussion of the
number of independent parameters in this model later.

when we evaluate the predictive distribution, we require c- 1 , which involves
the inversion of a d x d matrix. the computation required to do this can be reduced
by making use of the matrix inversion identity (c.7) to give
c- 1 = 0-- 11 - 0--2wm- 1w t

(12.40)

where the m x m matrix m is defined by

m = wtw + 0-

21.

(12.41)
because we invert m rather than inverting c directly, the cost of evaluating c- 1 is
reduced from o(d3 ) to o(m3 ).

as well as the predictive distribution p(x), we will also require the posterior
distributionp(zlx), which can again be written down directly using the result (2.116)
for linear-gaussian models to give

note that the posterior mean depends on x, whereas the posterior covariance is in 
dependent of x.

(12.42)

exercise 12.8

574

12. continuous latent variables

figure 12.10 the probabilistic pea model for a data set of n obser 
vations of x can be expressed as a directed graph in
which each observation x n is associated with a value
zn of the latent variable.

..-+--w

n

12.2.1 maximum likelihood pea
we next consider the determination of the model parameters using maximum
likelihood. given a data set x = {xn } of observed data points, the probabilistic
pea model can be expressed as a directed graph, as shown in figure 12.10. the
corresponding log likelihood function is given, from (12.35), by

inp(xijl, w,o' 2

n

) = l ln p(xn iw,jl,o'2
1""

n=l

n

n

)

nd

--2- ln(2n) - 2 ln ie[ - 2 l,..(xn - jl) c- (xn - jl).

t

1

(12.43)

n=l

setting the derivative of the log likelihood with respect to jl equal to zero gives the
expected result jl = x where x is the data mean defined by (12.1). back-substituting
we can then write the log likelihood function in the form

inp(xiw, jl, 0'2) = -2 {d in(2n) + in ie[ + tr (c-1s)}

n

(12.44)

where s is the data covariance matrix defined by (12.3). because the log likelihood
is a quadratic function of jl, this solution represents the unique maximum, as can be
confirmed by computing second derivatives.

maximization with respect to w and 0'2 is more complex but nonetheless has
an exact closed-form solution. it was shown by tipping and bishop (1999b) that all
of the stationary points of the log likelihood function can be written as

(12.45)

where u m is a d x m matrix whose columns are given by any subset (of size m)
of the eigenvectors of the data covariance matrix s, the m x m diagonal matrix
l m has elements given by the corresponding eigenvalues ..\, and r is an arbitrary
m x m orthogonal matrix.

furthermore, tipping and bishop (1999b) showed that the maximum of the like 
lihood function is obtained when the m eigenvectors are chosen to be those whose
eigenvalues are the m largest (all other solutions being saddle points). a similar re 
sult was conjectured independently by roweis (1998), although no proof was given.

12.2. probabilistic pea

575

again, we shall assume that the eigenvectors have been arranged in order of decreas 
ing values of the corresponding eigenvalues, so that the m principal eigenvectors are
ul,"" um. in this case, the columns of w define the principal subspace of stan 
dard pca. the corresponding maximum likelihood solution for (j'2 is then given by

1

(j'~l = d-m l ai

d

i=m+l

(12.46)

so that (j'~l is the average variance associated with the discarded dimensions.

because r is orthogonal, it can be interpreted as a rotation matrix in the m x m
latent space. if we substitute the solution for w into the expression for c, and make
use of the orthogonality property rrt = i, we see that c is independent of r.
this simply says that the predictive density is unchanged by rotations in the latent
space as discussed earlier. for the particular case of r = i, we see that the columns
of w are the principal component eigenvectors scaled by the variance parameters
ai - (j'2. the interpretation of these scaling factors is clear once we recognize that
for a convolution of independent gaussian distributions (in this case the latent space
distribution and the noise model) the variances are additive. thus the variance ai
in the direction of an eigenvector ui is composed of the sum of a contribution ai  
(j'2 from the projection of the unit-variance latent space distribution into data space
through the corresponding column of w, plus an isotropic contribution of variance
(j'2 which is added in all directions by the noise model.

it is worth taking a moment to study the form of the covariance matrix given
by (12.36). consider the variance of the predictive distribution along some direction
specified by the unit vector v, where vtv = 1, which is given by vtcv. first
suppose that v is orthogonal to the principal subspace, in other words it is given by
some linear combination of the discarded eigenvectors. then v tv = 0 and hence
v tcv = (j'2. thus the model predicts a noise variance orthogonal to the principal
subspace, which, from (12.46), is just the average of the discarded eigenvalues. now
suppose that v = ui where ui is one of the retained eigenvectors defining the prin 
cipal subspace. then vtcv = (ai - (j'2) + (j'2 = ai. in other words, this model
correctly captures the variance of the data along the principal axes, and approximates
the variance in all remaining directions with a single average value (j'2.

one way to construct the maximum likelihood density model would simply be
to find the eigenvectors and eigenvalues of the data covariance matrix and then to
evaluate wand (j'2 using the results given above.
in this case, we would choose
r = i for convenience. however, if the maximum likelihood solution is found by
numerical optimization of the likelihood function, for instance using an algorithm
such as conjugate gradients (fletcher, 1987; nocedal and wright, 1999; bishop and
nabney, 2008) or through the em algorithm, then the resulting value of r is es 
sentially arbitrary. this implies that the columns of w need not be orthogonal. if
an orthogonal basis is required, the matrix w can be post-processed appropriately
(golub and van loan, 1996). alternatively, the em algorithm can be modified in
such a way as to yield orthonormal principal directions, sorted in descending order
of the corresponding eigenvalues, directly (ahn and oh, 2003).

section 12.2.2

576

12. continuous latent variables

the rotational invariance in latent space represents a form of statistical noniden 
tifiability, analogous to that encountered for mixture models in the case of discrete
latent variables. here there is a continuum of parameters all of which lead to the
same predictive density, in contrast to the discrete nonidentifiability associated with
component re-labelling in the mixture setting.

if we consider the case of m = d, so that there is no reduction of dimension 
ality, then u m = u and l m = l. making use of the orthogonality properties
uut = i and rrt = i, we see that the covariance c of the marginal distribution
for x becomes

(12.47)

and so we obtain the standard maximum likelihood solution for an unconstrained
gaussian distribution in which the covariance matrix is given by the sample covari 
ance.

conventional pca is generally formulated as a projection of points from the d 
dimensional data space onto an m -dimensional linear subspace. probabilistic pca,
however, is most naturally expressed as a mapping from the latent space into the data
space via (12.33). for applications such as visualization and data compression, we
can reverse this mapping using bayes' theorem. any point x in data space can then
be summarized by its posterior mean and covariance in latent space. from (12.42)
the mean is given by

where m is given by (12.41). this projects to a point in data space given by

wle[zlx] + j-l.

(12.48)

(12.49)

section 3.3.1

note that this takes the same form as the equations for regularized id75
and is a consequence of maximizing the likelihood function for a linear gaussian
model. similarly, the posterior covariance is given from (12.42) by 0-2m- 1 and is
independent of x.

if we take the limit 0-

2

----t 0, then the posterior mean reduces to

(12.50)

exercise 12.11

exercise 12.12

section 2.3

which represents an orthogonal projection of the data point onto the latent space,
and so we recover the standard pca model. the posterior covariance in this limit is
2 > 0, the latent projection
zero, however, and the density becomes singular. for 0-
is shifted towards the origin, relative to the orthogonal projection.

finally, we note that an important role for the probabilistic pca model is in
defining a multivariate gaussian distribution in which the number of degrees of free 
dom, in other words the number of independent parameters, can be controlled whilst
still allowing the model to capture the dominant correlations in the data. recall
that a general gaussian distribution has d(d + 1)/2 independent parameters in its
covariance matrix (plus another d parameters in its mean). thus the number of
parameters scales quadratically with d and can become excessive in spaces of high

12.2. probabilistic pea

577

dimensionality. if we restrict the covariance matrix to be diagonal, then it has only d
independent parameters, and so the number of parameters now grows linearly with
dimensionality. however, it now treats the variables as if they were independent and
hence can no longer express any correlations between them. probabilistic pea pro 
vides an elegant compromise in which the m most significant correlations can be
captured while still ensuring that the total number of parameters grows only linearly
with d. we can see this by evaluating the number of degrees of freedom in the
ppca model as follows. the covariance matrix c depends on the parameters w,
, giving a total parameter count of dm + 1. however,
which has size d x m, and a 2
we have seen that there is some redundancy in this parameterization associated with
rotations of the coordinate system in the latent space. the orthogonal matrix r that
expresses these rotations has size m x m. in the first column of this matrix there are
m - 1 independent parameters, because the column vector must be normalized to
unit length. in the second column there are m - 2 independent parameters, because
the column must be normalized and also must be orthogonal to the previous column,
and so on. summing this arithmetic series, we see that r has a total of m(m -1)/2
independent parameters. thus the number of degrees of freedom in the covariance
matrix c is given by

dm + 1 - m(m - 1)/2.

(12.51)

exercise 12.14

section 12.2.4

section 9.4

the number of independent parameters in this model therefore only grows linearly
with d, for fixed m. if we take m = d - 1, then we recover the standard result
for a full covariance gaussian. in this case, the variance along d - 1 linearly in 
dependent directions is controlled by the columns of w, and the variance along the
remaining direction is given by a 2
. if m = 0, the model is equivalent to the isotropic
covariance case.

12.2.2 em algorithm for pea
as we have seen, the probabilistic pca model can be expressed in terms of a
marginalization over a continuous latent space z in which for each data point x n ,
there is a corresponding latent variable zn. we can therefore make use of the em
algorithm to find maximum likelihood estimates of the model parameters. this may
seem rather pointless because we have already obtained an exact closed-form so 
lution for the maximum likelihood parameter values. however, in spaces of high
dimensionality, there may be computational advantages in using an iterative em
procedure rather than working directly with the sample covariance matrix. this em
procedure can also be extended to the factor analysis model, for which there is no
closed-form solution. finally, it allows missing data to be handled in a principled
way.

we can derive the em algorithm for probabilistic pca by following the general
framework for em. thus we write down the complete-data log likelihood and take
its expectation with respect to the posterior distribution of the latent distribution
evaluated using 'old' parameter values. maximization of this expected complete 
data log likelihood then yields the 'new' parameter values. because the data points

578

12. continuous latent variables

are assumed independent, the complete-data log likelihood function takes the form

inp (x, zijl, w, (j2) = l {lnp(xnlzn ) + lnp(zn)}

n

(12.52)

n=l

where the nth row of the matrix z is given by zn. we already know that the exact
maximum likelihood solution for jl is given by the sample mean x defined by (12.1),
and it is convenient to substitute for jl at this stage. making use of the expressions
(12.31) and (12.32) for the latent and conditional distributions, respectively, and tak 
ing the expectation with respect to the posterior distribution over the latent variables,
we obtain

note that this depends on the posterior distribution only through the sufficient statis 
tics of the gaussian. thus in the e step, we use the old parameter values to evaluate

m-1wt(xn - x)
(j2m- 1 + le[zn]le[zn]t

(12.54)
(12.55)

which follow directly from the posterior distribution (12.42) together with the stan 
dard result le[znz~] = cov[zn] + je[zn]je[zn]t. here m is defined by (12.41).

in the m step, we maximize with respect to wand (j2, keeping the posterior
statistics fixed. maximization with respect to (t2 is straightforward. for the maxi 
mization with respect to w we make use of (c.24), and obtain the m-step equations

exercise 12.15

w new

2(jnew =

[t,exn -x)ilizn]t] [t,il[znz~]]-'
nd l {llxn - xl1 2
+tr (je[znzj]w~eww new )}.

n=l

1

n

- 2le[zn]tw~ew(xn - x)

(12.56)

(12.57)

the em algorithm for probabilistic pca proceeds by initializing the parameters
and then alternately computing the sufficient statistics of the latent space posterior
distribution using (12.54) and (12.55) in the e step and revising the parameter values
using (12.56) and (12.57) in the m step.

one of the benefits of the em algorithm for pca is computational efficiency
for large-scale applications (roweis, 1998). unlike conventional pca based on an

12.2. probabilistic pea

579

eigenvector decomposition of the sample covariance matrix, the em approach is
iterative and so might appear to be less attractive. however, each cycle of the em
algorithm can be computationally much more efficient than conventional pca in
spaces of high dimensionality. to see this, we note that the eigendecomposition of
the covariance matrix requires o(d 3 ) computation. often we are interested only
in the first m eigenvectors and their corresponding eigenvalues, in which case we
can use algorithms that are 0 (md 2 ). however, the evaluation of the covariance
matrix itself takes 0 (nd 2
) computations, where n is the number of data points.
algorithms such as the snapshot method (sirovich, 1987), which assume that the
eigenvectors are linear combinations of the data vectors, avoid direct evaluation of
the covariance matrix but are o(n3 ) and hence unsuited to large data sets. the em
algorithm described here also does not construct the covariance matrix explicitly.
instead, the most computationally demanding steps are those involving sums over
the data set that are 0 (n d m). for large d, and m    d, this can be a significant
saving compared to 0 (nd 2
) and can offset the iterative nature of the em algorithm.
note that this em algorithm can be implemented in an on-line form in which
each d-dimensional data point is read in and processed and then discarded before
the next data point is considered. to see this, note that the quantities evaluated in
the e step (an m-dimensional vector and an m x m matrix) can be computed for
each data point separately, and in the m step we need to accumulate sums over data
points, which we can do incrementally. this approach can be advantageous if both
nand d are large.

because we now have a fully probabilistic model for pca, we can deal with
missing data, provided that it is missing at random, by marginalizing over the dis 
tribution of the unobserved variables. again these missing values can be treated
using the em algorithm. we give an example of the use of this approach for data
visualization in figure 12.11.

another elegant feature ofthe em approach is that we can take the limit a 2

----t 0,
corresponding to standard pca, and still obtain a valid em-like algorithm (roweis,
1998). from (12.55), we see that the only quantity we need to compute in the estep
is je[zn]. furthermore, the m step is simplifie~ because m = wtw. to emphasize
the simplicity of the algorithm, let us define x to be a matrix of size n x d whose
nth row is given by the vector x n - x and similarly define 0 to be a matrix of size
d x m whose nth row is given by the vector je[zn]. the estep (12.54) of the em
algorithm for pca then becomes

o = (w~d wold)-lw~dx

and the m step (12.56) takes the form

w new = xtot(oot)-l.

(12.58)

(12.59)

again these can be implemented in an on-line form. these equations have a simple
interpretation as follows. from our earlier discussion, we see that the e step involves
an orthogonal projection of the data points onto the current estimate for the principal
subspace. correspondingly, the m step represents a re-estimation of the principal

580

12. contlnljoljs i"ht;i'it vi\ riarles

fig".. 12.11 probabilistic pca visoo,zsbon 01 a portion 0i1he "" !low data setlo< ihe !irsl 100 (lata   einls, the
left..,...nd plot oiiows ihe i'o"leoo< mean proj9c1ions oilhfi (lata poims on lhe principal subspace. the ,;gtri  hi\nd
plot is obtained by firsl ran<lomly omitting 30% 0i1he variable .aloo. and lhen us>rlg em 10 mndie i"" mi......
values. note i!iai eac/1 data poinl1hen nos allea., one missing mea.u,ement but lhoallhe plot i. ""ry ..mia, to
lhe ona obtained wit"""l miss.... vall>ll$

ewrrise /2,/7

subspace to minimize !he squared reoonslructioo error in 'oihich the proje<:tion, are
c.,n.

we ean gh'e a ,imple physical analogy for this em algorithm. which is easily
visualized for d = 2 and m = 1. coo,ider a collectioo nf data point'
,n twi)
dimension', aod let tile u""'-dimensiunal principal subspace be represented by a <ohd
rod. now atlach each data point to the nxi via a ,pring oo<:)"ing hooi;:e', law ("umj
energy i, propol1ional 10 ,lie square of lile spring". length).
in ll1e e 'tel', we keep
the nxi hed and allow the attachment point' tn ,iide up and <i<,wn ll1e nxi '" a, to
minimize ll1e e",,'lly, this cau",. each attachment point (independently) 10 position
itself at the orthogonal pmjeclion of the c~sponding data point onto the nxi.
in
the m 'tel'. we keep the attachment poiol' fil<ed and then release tile nxi and allow it
to m'>,'e 10 tile minimum energy posilion. 11ie e and m 'teps are then repeated until
a ,uitable c""vergence cri.eri"" is ..,isfled. a. is illuslrated in figure 12.12.

12.2.3 bayesian pea
s<j far in oilr di",""ioo of pea. we have ",'.nled ihal tile '"ine ,ii for ,lie
dl,nen,ionalit)" of tile principal .ubspace is gi"en,
in praclice. ".-e nlmt cooose a
suilable ,..i"" according 10 the application. for ,isuali,a,ion. we ge""",ny choose
.\1 = 2. whereas for oiher application, the approrrialc choice for ,1/ ma)" be less
dea,. one appmao:h i. 10 pi", the eigen"alue 'peclrum for lhe data set. analog,   ."
10 the example in figure 12.4 for the off_line digits dala sci, and look to see if lite
eige",,,i .... nmurally form two groups comprising a set of ,mall ,'alues separated by
a ,ign;flcant gap from a ",t of relativel)" large ,'alues, indicating a natural cholcc f<>r
ai, in practice. such a gap i, oflen ''''' seen

, ",

0

-,

,
, ",

o
-,

-,

   

,

,

,

0

-,

,

-,

-,

   ,
....
~,    

0

o

, ,<,

0

-,

,

-,

-,

0

o

,

,

,

0

o

flgu.. 12.12 synt"'elic <lata illustrating too em algorithm !of pca defined by (12.58) and (1259). (8) a data
set x with the data points shown in 1ji'e  l, t"ll"tm' w'i1!1l!>e t'im pmdpal """"""",is (shown as eigenveclor1
scaled by it>e squafll 'oois 04 the eigej'l\lllluel). (b) initial configurat"'" 01 too principalsul>sl>a<:<t defined by w,
shown in md, too"lhfir with the fk'(ijeclions 01 the latll<11 points z inlo too <lata space, giitoo by zwt , shown in
cyan, (oj alter""" m step, too laten! si'b  l p>as been update<! wiih z r>el(l nxed. (d) me' tt>e success.... e slep,
it>e ""'-'eo 01 z havu been up<!atll<:1. ~ '" ihogoooal r>rojecliqn$, with w h&k! fixed. (e) aft.... tile se<:o<l<l m
s!flp. <') after l!ie mc()<>;l e st"l'

s,uion i.j

be<:au,", th~ pm/xlhi li>lic pea modd has a well  defined likelillood f"flction, we
<wid employ cros,-,-.1idation to delermine the \"ajue of di"",nsiooa!ity by "'iecting
tit<: large,t log likelihood t>i1 a '-alidation data set such an opprooch. hov.  ~\-er. can
become computationally ro<lly. p3rticularl)' if we cqnsid<:,     probabilistic mixlure
of pea modds (tipping and bishop. 1999a) in "hich we seek 10 <!etermi'" the
appropriate dimen,ionalily ",paraltly for toch componenl in lt1e mixm""

gi'-en thai w. ha,-e a probabilislic formulalion of pea, il s  ms natural 10 s  k

u buye,ian approach 10 model seleclion. to do thi,. ,,"'e nee<! 10 marginalize 001
the model paramele" /'. \v. und ,,' wilh ""peel to appropriate prior distribution'.
this can be done by u,ing a ,-ariation.l framework to .pproxim'le the allulylic.lly
intractable murginaliuoi;oo, (bi,hop. 1mb). 1lic marginal likelihood v.lues. given
by ttle ,'ari.,ionallower bour.d, cun lhen be c<>mpun:d for a r.nge of different '"tue'
"f;\i ar.d itie '"iue giving iht largest marginal likelihood ",iecloo_

l1ere we consider. simpler approach introducoo by b.ased on the rddmu "p-

582

12. continuous latent variables

figure 12.13 probabilistic graphical model for bayesian pea in
which the distribution over the parameter matrix w
is governed by a vector a of hyperparameters.

w

n

proximation, which is appropriate when the number of data points is relatively large
and the corresponding posterior distribution is tightly peaked (bishop, 1999a).
it
involves a specific choice of prior over w that allows surplus dimensions in the
principal subspace to be pruned out of the model. this corresponds to an example of
automatic relevance determination, or ard, discussed in section 7.2.2. specifically,
we define an independent gaussian prior over each column of w, which represent
the vectors defining the principal subspace. each such gaussian has an independent
variance governed by a precision hyperparameter o:i so that

(12.60)

where wi is the i th column of w. the resulting model can be represented using the
directed graph shown in figure 12.13.

the values for o:i will be found iteratively by maximizing the marginallikeli 
hood function in which w has been integrated out. as a result of this optimization,
some of the o:i may be driven to infinity, with the corresponding parameters vec 
tor wi being driven to zero (the posterior distribution becomes a delta function at
the origin) giving a sparse solution. the effective dimensionality of the principal
subspace is then determined by the number of finite o:i values, and the correspond 
ing vectors wi can be thought of as 'relevant' for modelling the data distribution.
in this way, the bayesian approach is automatically making the trade-off between
improving the fit to the data, by using a larger number of vectors wi with their cor 
responding eigenvalues ai each tuned to the data, and reducing the complexity of
the model by suppressing some of the wi vectors. the origins of this sparsity were
discussed earlier in the context of relevance vector machines.

the values of o:i are re-estimated during training by maximizing the log marginal

likelihood given by

p(xla, j-l, 0'2) = jp(xiw, j-l, o'2)p(wla) dw

(12.61)

where the log ofp(xiw, j-l, 0'2) is given by (12.43). note that for simplicity we also
treat j-l and 0'2 as parameters to be estimated, rather than defining priors over these
parameters.

section 7.2

12.2. probabilistic pea

583

section 4.4

section 3.5.3

because this integration is intractable, we make use of the laplace approxima 
tion. if we assume that the posterior distribution is sharply peaked, as will occur for
sufficiently large data sets, then the re-estimation equations obtained by maximizing
the marginal likelihood with respect to ai take the simple form

which follows from (3.98), noting that the dimensionality of wi is d. these re 
estimations are interleaved with the em algorithm updates for determining wand
a 2
    the e-step equations are again given by (12.54) and (12.55). similarly, the m 
step equation for a 2 is again given by (12.57). the only change is to the m-step
equation for w, which is modified to give

(12.62)

(12.63)

where a = diag(ai)' the value of i-" is given by the sample mean, as before.

if we choose m = d - 1 then, if all ai values are finite, the model represents
a full-covariance gaussian, while if all the ai go to infinity the model is equivalent
to an isotropic gaussian, and so the model can encompass all pennissible values for
the effective dimensionality of the principal subspace. it is also possible to consider
smaller values of m, which will save on computational cost but which will limit
the maximum dimensionality of the subspace. a comparison of the results of this
algorithm with standard probabilistic pca is shown in figure 12.14.

bayesian pca provides an opportunity to illustrate the id150 algo 
rithm discussed in section 11.3. figure 12.15 shows an example of the samples
from the hyperparameters in ai for a data set in d = 4 dimensions in which the di 
mensionality of the latent space is m = 3 but in which the data set is generated from
a probabilistic pca model having one direction of high variance, with the remaining
directions comprising low variance noise. this result shows clearly the presence of
three distinct modes in the posterior distribution. at each step of the iteration, one of
the hyperparameters has a small value and the remaining two have large values, so
that two of the three latent variables are suppressed. during the course of the gibbs
sampling, the solution makes sharp transitions between the three modes.

the model described here involves a prior only over the matrix w. a fully
bayesian treatment of pca, including priors over 1-", a 2 , and n, and solved us 
ing variational methods, is described in bishop (1999b). for a discussion of vari 
ous bayesian approaches to detennining the appropriate dimensionality for a pca
model, see minka (2001c).

12.2.4 factor analysis
factor analysis is a linear-gaussian latent variable model that is closely related
to probabilistic pca. its definition differs from that of probabilistic pca only in that
the conditional distribution of the observed variable x given the latent variable z is

584

12. continuous latent variables

   
      
           
               
   
   
   
   
           
          

   

   
   

       

              
   
          
   
      
       
       

   
   

   
                    
       
   
       
   

figure 12.14 'hinloo' diagrams of the matrix w in which each element 01 the matrix is depicted as
a square (white lor positive and black lor negative values) whose area is proportional
to the magnitude of that element. the synthetic data sel comprises 300 data points in
d = 10 dimensions sampled from a gaussian distribution having standard deviation 1.0
in 3 directions and standard deviation 0.5 in the remaining 7 directions for a data set in
d = 10 dimensions having at = 3 directions with larger variance than the remaining 7
directions. the left-hand plol shows the result irom maximum likelihood probabilistic pca,
and the left  hand plot shows the corresponding resuft from bayesian pea. we see how
the bayesian model is able to discover the appropriate dimensionality by suppressing the
6 surplus degrees of freedom.

taken to have a diagonal rather than an isotropic covariance so that

p(xlz) = n(xlwz + 1'. \ii)

(12.64)

where ill is a d x d diagonal matrix. note that the factor analysis model, in common
with probabilistic pca. assumes that the observed variables xl, ... ,xo are indepen 
dent. given the latent variable z. in essence. the factor analysis model is explaining
the observed covariance structure of the data by representing the independent vari 
ance associated with each coordinate in the matrix 1j.' and capturing the covariance
between variables in the matrix w.
in the factor analysis literature. the columns
of w. which capture the correlations between observed variables. are calledfaclor
loadings. and the diagonal elements of 1j.'. which represent the independent noise
variances for each of the variables, are called llniqllenesses.

the origins of factor analysis are as old as those of pca. and discussions of
factor analysis can be found in the books by everitt (1984). bartholomew (1987),
and basilevsky (1994). links between factor analysis and pca were investigated
by lilwley (1953) and anderson (1963) who showed that at stationary points of
the likelihood function. for a faclor analysis model with 1j.' = (121, the columns of
w are scaled eigenvectors of the sample covariance matrix. and (12 is the average
of the discarded eigenvalues. later. tipping and bishop (1999b) showed that the
maximum of the log likelihood function occurs when the eigenvectors comprising
ware chosen to be the principal eigenvectors.

making use of (2.115). we see that the marginal distribution for the observed

12.2.l'ru":ohilislk i'ca

585

flliure12.15 gillbs .,,,,>p!j"lllo< bay<lslan
pca sh<ming plots oj ino,
versus ~eralion number br
three
showing
tr"""tions
tbe
th"'" moots <a !he posterior
distribution.

betw.....

values.

"

eurr"e 12,19

s"na" 12.4

,-"riabl, i' gi,-,n by 1'(x) ~ n( xlj', c) whe ... now

c=wwt+'i'.

(12,6~)

as with probabilistic pc a, thi, momi is im-"ri.rrl to 'olalions in 11><0 latent 'pace.

histoocally, factor anal)',;s has been lhe ,ubjerl of col1tro,-ersy wroe" a!tempt<
h",-e bttn "'a<k: to place an intc'p"'t"lioo on the ind;vidual faclon (the coofdinates
in z_space). which h3.\ pm"en problematic due to lr.e """i<lcmifiabilily of factof
analysis associmed with mation' in this 'pace. from oor perspeoh-e, howe,-er. we
shall .iew factor analysis as a form of lalent "ariable densily model. in which the
form of tl>c lalent 'pace i' of interest but no! the particular choicc of coordinates
used to descrit>c il. if we wish to remove the degeneracy a'sociated with latent 'pace
roiations. ""e mu,t con'ider non-gaussian latent ,-"riable di'tribution,. gi"irrg rise 10
independent component .n.lysi, (lca) models.

we can detenni"e the parameters i'. \v. "nd .... in the fac!of an.ly,i, model by
muimum likelihood. 11.. solution for i' i' ag"in given by the ",,,,pie "'ean. how  
eyc'. "nli~e probabili,tic l'ca.lllcre i' no longer a closed-form maximum likelihood
solution for \v. ",'hich mu.\ltherdorc be found i'er.li,'c1)'_ because faclor anal)',i. is
a latent variable model thi' can be don. using an em algorilhm (r.bin and thayer.
1982) !h"t is "nalogou, to the one used (of pml>;lbili.tie pea. specihcally. lhe e-'lep
eqnjtioo, are g;'-en by

e[zoj

= gwt",-'(xn - xl

e[z"z~j _ g + e[zo]e[z,,]t

where ""e h",'e defi""d

(12,66)
(1267)

(l2,6h)

noie th"t thi' i' e.pre,<ed in a for'" thai in,-oh'es inycrsi"n of mal rices "f silo ,\ i x ,if
rathe'lhan d x d (ex"",,,, for tbe d x d diagooal matrix oj' "'hose in,-erse i.' 'ri"ial

586

12. continuous latent variables

exercise 12.22

to compute in o(d) steps), which is convenient because often m    d. similarly,
the m-step equations take the form

w new

[~(x"-xllllizn]"] [~ill[znz~i]-'
diag{s-w.'w ~ ~1ll[zn](xn _ xl"}

(12.69)

(12.70)

where the 'diag' operator sets all of the nondiagonal elements of a matrix to zero. a
bayesian treatment of the factor analysis model can be obtained by a straightforward
application of the techniques discussed in this book.

another difference between probabilistic pca and factor analysis concerns their
different behaviour under transformations of the data set. for pca and probabilis 
tic pca, if we rotate the coordinate system in data space, then we obtain exactly
the same fit to the data but with the w matrix transformed by the corresponding
rotation matrix. however, for factor analysis, the analogous property is that if we
make a component-wise re-scaling of the data vectors, then this is absorbed into a
corresponding re-scaling of the elements of \)i.

exercise 12.25

12.3. kernel pea

in chapter 6, we saw how the technique of kernel substitution allows us to take an
algorithm expressed in terms of scalar products of the form x t x' and generalize
that algorithm by replacing the scalar products with a nonlinear kernel. here we
apply this technique of kernel substitution to principal component analysis, thereby
obtaining a nonlinear generalization called kernel pea (scholkopf et al., 1998).

consider a data set {xn } of observations, where n = 1, ... , n, in a space of
dimensionality d. in order to keep the notation uncluttered, we shall assume that
we have already subtracted the sample mean from each of the vectors x n , so that
ln x n = o. the first step is to express conventional pca in such a form that the
data vectors {x n } appear only in the form of the scalar products x~ x m . recall that
the principal components are defined by the eigenvectors ui of the covariance matrix

where i = 1, ... ,d. here the d x d sample covariance matrix s is defined by

sui = aiui

(12.71)

and the eigenvectors are normalized such that ut ui = 1.

now consider a nonlinear transformation   (x) into an m -dimensional feature
space, so that each data point x n is thereby projected onto a point   (xn ). we can

(12.72)

12.3. k~mci l'co'.

587

...'\

figu'.12.16 sctiematic _,.lion 01 kernel pea. a <utll hi in lhe oflglnal <uta space l~'_ plot} ..
pfoleled by' """'*- tranllklfmalion ~,,} 1nio. fa.tur. space (fight_ plot). by i*b'~ pca in the
!hue 111**. we oblao'ilha pmeiilai ~"lls. "'who<:tllha tnt ie ......... in blue.,.., .. ojano4ed by lha
_
plolklio". onio lhe iirsl poiridl* '""" ......... 11.
"""'*' _od 110 ""'*- poof&cllu. in .... <lfisionai oillll 111**. hole iiuiiin gmefm' la nol pox ' .. 110
'. . . . . . 1ha,...iiio_ poi ........ 00i' ........ by._1n "apam.

v, tha gr-. .... in imiun apam indicma iha _

__ ptrform )l-.bnl pea ill
fnlllk lopice. ..-iudl,mpiiclily   lii'il's     - '_
....-.. model ill
onpnll cbuo ~ as ,1lu>tr*d in fllift 12-16.
princlpai
"io..it. . lei '" los!oulllt 1nl illt~ diu. ~ lobo halnro mean,
fu

ji) !hal l. 4>("'.j .. o. we dwl rctlll'1l 10 itl,~ pol'" .ihonly. 1llt.1f " .if "'"""*
co\-.ullcc mmfu ,n (~ .if*'e ,~ l"" by

,
..,
.~. l <>(x.j4>(x.)t

c -

and ,l~ ",,,,n'"mol" opan,ion i'   lined by

cv, = a,v,

; = 1... ,. m. our goal is 10 soh'" lhis eigen"lilue problem wilhoul ha"inlllo work
."plici,ly in ,he f.liture 'pace. from !he definilion of c. lhe .ill"""""l"'" equal ions

lell' u, thai y, !-ali,fies ."s l <bc".) {<b(x.lt v,} - )"y,

(12.7~)

..,

........ ..-" .- ln1 (proo.idcd a, > 0) tilt "cc'lor v, is li''n by     ii_ rombllla"on
of illt d>(

j..... jo <;.- he "-"llm iillhc (orm

(12.73)

(12,74)

(12.76)

,
...
'"' l 11,.4>(".).

v,

588

12. continuous latent variables

substituting this expansion back into the eigenvector equation, we obtain

1 n
n l

n=l

n
  (xn)  (xn)t l

n
aim  (xm ) = ai l

m=l

n=l

ain  (xn ),

(12.77)

the key step is now to express this in terms of the id81 k(xn , x m ) =

  (xn)t  (xm ), which we do by multiplying both sides by   (xz)t to give

1 n
m
n lk(xi'xn ) l

n=l

m=l

aimk(xn,xm ) = ai laink(xi'xn ),

(12.78)

n

n=l

this can be written in matrix notation as

where ai is an n-dimensional column vector with elements ani for n = 1, ... ,n.
we can find solutions for ai by solving the following eigenvalue problem

(12.80)

(12.79)

exercise 12.26

in which we have removed a factor of k from both sides of (12.79). note that
the solutions of (12.79) and (12.80) differ only by eigenvectors of k having zero
eigenvalues that do not affect the principal components projection.

the id172 condition for the coefficients ai is obtained by requiring that
the eigenvectors in feature space be normalized. using (12.76) and (12.80), we have

1 = v;vi = l l

n

n

n=l m=l

ainaim  (xn)t  (xm ) = a;k~ = aina;ai'

(12.81)

having solved the eigenvector problem, the resulting principal component pro 
jections can then also be cast in terms of the id81 so that, using (12.76),
the projection of a point x onto eigenvector i is given by

yi(x) =   (x)tvi = l ain  (x)t   (xn ) = l aink(x, x n )

n

n

n=l

n=l

(12.82)

and so again is expressed in terms of the id81.

in the original d-dimensional x space there are d orthogonal eigenvectors and
hence we can find at most d linear principal components. the dimensionality m
of the feature space, however, can be much larger than d (even infinite), and thus
we can find a number of nonlinear principal components that can exceed d. note,
however, that the number of nonzero eigenvalues cannot exceed the number n of
data points, because (even if m > n) the covariance matrix in feature space has
rank at most equal to n. this is reflected in the fact that kernel pca involves the
eigenvector expansion of the n x n matrix k.

12.3. kernel pca

589

so far we have assumed that the projected data set given by   (xn ) has zero
mean, which in general will not be the case. we cannot simply compute and then
subtract off the mean, since we wish to avoid working directly in feature space, and
so again, we formulate the algorithm purely in-!erms of the id81. the
projected data points after centralizing, denoted   (xn ), are given by

and the corresponding elements of the gram matrix are given by

k nm =

  (xn)t  (xm )

  (xn)t  (xm )

1 n
- n l

z=l

  (xn)t  (xz)

1 n

1

n

n

- n l  (xz)t  (xm ) + n2 ll  (xj)t  (xz)

z=l

j=l z=l

1 n
k(xn , x m ) - n l

k(xz, x m )

z=l

- n lk(xn,xz) + n2 llk(xj,xl)'

1

n

n

j=l 1=1

1 n

z=l

this can be expressed in matrix notation as

(12.83)

(12.84)

(12.85)

exercise 12.27

~

where in denotes the n x n matrix in which every element takes the value l/n.
thus we can evaluate k using only the id81 and then use k to determine
the eigenvalues and eigenvectors. note that the standard pca algorithm is recovered
as a special case if we use a linear kernel k(x, x') = xtx/. figure 12.17 shows an
example of kernel pca applied to a synthetic data set (scholkopf et al., 1998). here
a 'gaussian' kernel of the form

~

k(x, x') = exp(-llx - x/11 2/0.1)

(12.86)

is applied to a synthetic data set. the lines correspond to contours along which the
projection onto the corresponding principal component, defined by

n
  (x?vi = l

n=l

aink(x, x n )

(12.87)

is constant.

590

12. continuous latent valuables

_.

figure 12.11 e"llmple 01 kernel pca, with a gaussian kernel awiioo 10 a synthetic <lata sat in two <:iirnensions,
showing !he firsl flight eigenfunclions along w~h l!>eir e9tnvailnls. the oootours am lines along which !he
projoc1ion onlo t"" coffaspmding principal componen1ls co<>stam, nola haw ihe firsl two~ ....,..rat.
!he th"'" dusters. !he ""'" ill"'"~ spiii ""'*' oilhe eluste, into hamos. and t"" loliowing ihree

~ again spi~!he duste," into halves along directions orthogonal 10 tho premous splils,

one obvioo' dls.ajmota~eof i:emel !'ca is thaf if invoh'es finding lhe elgen"e< 
tors of the n x n malri>: k raw. ihan lhe d x d malri, s of cor,..emionallinear
!'ca. and!io in prac1lce for large data "'1' appro,lmation< are often us(:d

finally. ""e ooie that i" <tandard linear i'ca, we often retain some redoce<l num  
ber l < dof eigenvectors and then appro,lmale 0 data vttl<:>r xn b}' its projection
i~ 0,,1" lhe l-dimensional principal subspace, defined by

,

i~-l:  ",)""

(12.88)

i" kernell'ca. this will in gencr~1 not be flo'slble, to see thl', ooie ihat the map 
ping 4'(x) maps the d-dimensional x space i"t" 0 d-dimensioo.l manijqiii in lhe
m-dimemioo.l femure space <1>. tlie: .'ector x i' koown a< lhe f'",.imagr of lhe
c","""ponding poi"l 4'(x). however, fhe projec1ioo of poinl> in feature <j'3c" ""to
the linear rca ,ub,p""" in that 'pace will typically"''' lie on fhe nonlinear d 
dimensional manifold and !io will nul ha.,. a c"""",pondlng p",.lmo~ein dolo spa<."c,

technlque< ho.-e lherefore bttn proposed for finding approximale pre-image< ib""lr
nat.. 2(04).

12.4. nonlinear latent variable models

591

12.4. nonlinear latent variable models

in this chapter, we have focussed on the simplest class of models having continuous
latent variables, namely those based on linear-gaussian distributions. as well as
having great practical importance, these models are relatively easy to analyse and
to fit to data and can also be used as components in more complex models. here
we consider briefly some generalizations of this framework to models that are either
nonlinear or non-gaussian, or both.

in fact, the issues of nonlinearity and non-gaussianity are related because a
general id203 density can be obtained from a simple fixed reference density,
such as a gaussian, by making a nonlinear change of variables. this idea forms the
basis of several practical latent variable models as we shall see shortly.

exercise 12.28

independent component analysis

12.4.1
we begin by considering models in which the observed variables are related
linearly to the latent variables, but for which the latent distribution is non-gaussian.
an important class of such models, known as independent component analysis, or
lea, arises when we consider a distribution over the latent variables that factorizes,
so that

m

p(z) = iip(zj).

j=l

(12.89)

to understand the role of such models, consider a situation in which two people
are talking at the same time, and we record their voices using two microphones.
if we ignore effects such as time delay and echoes, then the signals received by
the microphones at any point in time will be given by linear combinations of the
amplitudes of the two voices. the coefficients of this linear combination will be
constant, and if we can infer their values from sample data, then we can invert the
mixing process (assuming it is nonsingular) and thereby obtain two clean signals
each of which contains the voice of just one person. this is an example of a problem
called blind source separation in which 'blind' refers to the fact that we are given
only the mixed data, and neither the original sources nor the mixing coefficients are
observed (cardoso, 1998).

this type of problem is sometimes addressed using the following approach
(mackay, 2003) in which we ignore the temporal nature of the signals and treat the
successive samples as i.i.d. we consider a generative model in which there are two
latent variables corresponding to the unobserved speech signal amplitudes, and there
are two observed variables given by the signal values at the microphones. the latent
variables have a joint distribution that factorizes as above, and the observed variables
are given by a linear combination of the latent variables. there is no need to include
a noise distribution because the number of latent variables equals the number of ob 
served variables, and therefore the marginal distribution of the observed variables
will not in general be singular, so the observed variables are simply deterministic
linear combinations of the latent variables. given a data set of observations, the

592

12. continuous latent variables

likelihood function for this model is a function of the coefficients in the linear com 
bination. the log likelihood can be maximized using gradient-based optimization
giving rise to a particular version of independent component analysis.

the success of this approach requires that the latent variables have non-gaussian
distributions. to see this, recall that in probabilistic pca (and in factor analysis) the
latent-space distribution is given by a zero-mean isotropic gaussian. the model
therefore cannot distinguish between two different choices for the latent variables
where these differ simply by a rotation in latent space. this can be verified directly
by noting that the marginal density (12.35), and hence the likelihood function, is
unchanged if we make the transformation w -) wr where r is an orthogonal
matrix satisfying rrt = i, because the matrix c given by (12.36) is itself invariant.
extending the model to allow more general gaussian latent distributions does not
change this conclusion because, as we have seen, such a model is equivalent to the
zero-mean isotropic gaussian latent variable model.

another way to see why a gaussian latent variable distribution in a linear model
is insufficient to find independent components is to note that the principal compo 
nents represent a rotation of the coordinate system in data space such as to diagonal 
ize the covariance matrix, so that the data distribution in the new coordinates is then
uncorrelated. although zero correlation is a necessary condition for independence
it is not, however, sufficient.
in practice, a common choice for the latent-variable
distribution is given by

exercise 12.29

1

p(z) = --,.-----,-
7fcosh(zj)

j

1

(12.90)

which has heavy tails compared to a gaussian, reflecting the observation that many
real-world distributions also exhibit this property.

the original ica model (bell and sejnowski, 1995) was based on the optimiza 
tion of an objective function defined by information maximization. one advantage
of a probabilistic latent variable formulation is that it helps to motivate and formu 
late generalizations of basic ica. for instance, independent factor analysis (attias,
1999a) considers a model in which the number of latent and observed variables can
differ, the observed variables are noisy, and the individual latent variables have flex 
ible distributions modelled by mixtures of gaussians. the log likelihood for this
model is maximized using em, and the reconstruction of the latent variables is ap 
proximated using a variational approach. many other types of model have been
considered, and there is now a huge literature on ica and its applications (jutten
and herault, 1991; comon et at., 1991; amari et at., 1996; pearlmutter and parra,
1997; hyvarinen and oja, 1997; hinton et at., 2001; miskin and mackay, 2001;
hojen-sorensen et at., 2002; choudrey and roberts, 2003; chan et at., 2003; stone,
2004).

12.4.2 autoassociative neural networks
in chapter 5 we considered neural networks in the context of supervised learn 
ing, where the role of the network is to predict the output variables given values

12.4. nonlinear latent variable models

593

figure 12.18 an autoassociative multilayer id88 having
two layers of weights. such a network is trained to
map input vectors onto themselves by minimiza 
tion ot a sum-ot-squares error. even with non 
linear units in the hidden layer, such a network
is equivalent to linear principal component anal 
ysis. links representing bias parameters have
been omitted for clarity.

inputs

outputs

for the input variables. however, neural networks have also been applied to un 
supervised learning where they have been used for id84. this
is achieved by using a network having the same number of outputs as inputs, and
optimizing the weights so as to minimize some measure of the reconstruction error
between inputs and outputs with respect to a set of training data.

consider first a multilayer id88 of the form shown in figure 12.18, hav 
ing d inputs, d output units and m hidden units, with m < d. the targets used
to train the network are simply the input vectors themselves, so that the network
is attempting to map each input vector onto itself. such a network is said to form
an autoassociative mapping. since the number of hidden units is smaller than the
number of inputs, a perfect reconstruction of all input vectors is not in general pos 
sible. we therefore determine the network parameters w by minimizing an error
function which captures the degree of mismatch between the input vectors and their
reconstructions. in particular, we shall choose a sum-of-squares error of the form

1 n

e(w) = "2 l ily(xn , w) - xn 11 2

   

n=l

(12.91)

if the hidden units have linear activations functions, then it can be shown that the
error function has a unique global minimum, and that at this minimum the network
performs a projection onto the m -dimensional subspace which is spanned by the first
m principal components of the data (bourlard and kamp, 1988; baldi and hornik,
1989). thus, the vectors of weights which lead into the hidden units in figure 12.18
form a basis set which spans the principal subspace. note, however, that these vec 
tors need not be orthogonal or normalized. this result is unsurprising, since both
principal component analysis and the neural network are using linear dimensionality
reduction and are minimizing the same sum-of-squares error function.

it might be thought that the limitations of a linear id84 could
be overcome by using nonlinear (sigmoidal) id180 for the hidden units
in the network in figure 12.18. however, even with nonlinear hidden units, the min 
imum error solution is again given by the projection onto the principal component
subspace (bourlard and kamp, 1988). there is therefore no advantage in using two 
layer neural networks to perform id84. standard techniques for
principal component analysis (based on singular value decomposition) are guaran 
teed to give the correct solution in finite time, and they also generate an ordered set
of eigenvalues with corresponding orthonormal eigenvectors.

594

12. continuous latent variables

figure 12.19 addition of extra hidden lay 
ers of noolinear units gives an
auloassocialive network which
can perform a noolinear dimen 
siooality reduction.

f,    

f,    

inputs

x,

outputs

x,

the situation is different, however. if additional hidden layers are pcrmillcd in
the network. consider the four-layer autoassociativc network shown in figure 12.19.
again the output units are linear, and the m units in the second hidden layer can also
be linear. however, the first and third hidden layers have sigmoidal nonlinear activa 
tion functions. the network is again trained by minimization of the error function
(12.91). we can view this network as two successive functional mappings f] and
f 2 , as indicated in figure 12.19. the first mapping f] projects the original d 
dimensional data onto an ai-dimensional subspace s defined by the activations of
the units in the second hidden layer. because of the presence of the first hidden layer
of nonlinear units. this mapping is very general. and in particular is not restricted to
being linear. similarly. the second half of the network defines an arbitrary functional
mapping from the m -dimensional space back into the original d-dimensional input
space. this has a simple geometrical interpretation. as indicated for the case d = 3
and m = 2 in figure 12.20.

such a network effectively perfonns a nonlinear principal component analysis.

x3

"f,

   

x,

"

figure 12.20 geometrical interpretation of the mappings performed by the network in figure 12.1 g for the case
of 0 = 3 inputs and ai = 2 units in the middle hidden layer. the function f, maps from an m-dimensional
space s into a d-dimensiooal space and therefore defines the way in which the space s is embedded within the
original x-space. since the mapping f, can be r"i()(llinear, the embedding 01 s can be nonplanar, as indicated
in the figure. the mapping f. then defines a projectiorl of points in the original d-dimensional space into the
m -dimensional subspace s.

12.4. nonlinear latent variable models

595

it has the advantage of not being limited to linear transformations, although it con 
tains standard principal component analysis as a special case. however, training
the network now involves a nonlinear optimization problem, since the error function
(12.91) is no longer a quadratic function of the network parameters. computation 
ally intensive nonlinear optimization techniques must be used, and there is the risk of
finding a suboptimal local minimum of the error function. also, the dimensionality
of the subspace must be specified before training the network.

12.4.3 modelling nonlinear manifolds
as we have already noted, many natural sources of data correspond to low 
dimensional, possibly noisy, nonlinear manifolds embedded within the higher di 
mensional observed data space. capturing this property explicitly can lead to im 
proved density modelling compared with more general methods. here we consider
briefly a range of techniques that attempt to do this.

one way to model the nonlinear structure is through a combination of linear
models, so that we make a piece-wise linear approximation to the manifold. this can
be obtained, for instance, by using a id91 technique such as k -means based on
euclidean distance to partition the data set into local groups with standard pca ap 
plied to each group. a better approach is to use the reconstruction error for cluster
assignment (kambhatla and leen, 1997; hinton et al., 1997) as then a common cost
function is being optimized in each stage. however, these approaches still suffer
from limitations due to the absence of an overall density model. by using prob 
abilistic pca it is straightforward to define a fully probabilistic model simply by
considering a mixture distribution in which the components are probabilistic pca
models (tipping and bishop, 1999a). such a model has both discrete latent vari 
ables, corresponding to the discrete mixture, as well as continuous latent variables,
and the likelihood function can be maximized using the em algorithm. a fully
bayesian treatment, based on variational id136 (bishop and winn, 2000), allows
the number of components in the mixture, as well as the effective dimensionalities
of the individual models, to be inferred from the data. there are many variants of
this model in which parameters such as the w matrix or the noise variances are tied
across components in the mixture, or in which the isotropic noise distributions are
replaced by diagonal ones, giving rise to a mixture of factor analysers (ghahramani
and hinton, 1996a; ghahramani and beal, 2000). the mixture of probabilistic pca
models can also be extended hierarchically to produce an interactive data visualiza 
tion algorithm (bishop and tipping, 1998).

an alternative to considering a mixture of linear models is to consider a single
nonlinear model. recall that conventional pca finds a linear subspace that passes
close to the data in a least-squares sense. this concept can be extended to one 
dimensional nonlinear surfaces in the form of principal curves (hastie and stuetzle,
1989). we can describe a curve in a d-dimensional data space using a vector-valued
function f ().), which is a vector each of whose elements is a function of the scalar )..
there are many possible ways to parameterize the curve, of which a natural choice
is the arc length along the curve. for any given point xin data space, we can find
the point on the curve that is closest in euclidean distance. we denote this point by

596

12. continuous latent variables

>.. = gf(x) because it depends on the particular curve f(>"). for a continuous data
density p(x), a principal curve is defined as one for which every point on the curve
is the mean of all those points in data space that project to it, so that

je [xlgf(x) = >..] = f(>").

(12.92)

for a given continuous density, there can be many principal curves. in practice, we
are interested in finite data sets, and we also wish to restrict attention to smooth
curves. hastie and stuetzle (1989) propose a two-stage iterative procedure for find 
ing such principal curves, somewhat reminiscent of the em algorithm for pca. the
curve is initialized using the first principal component, and then the algorithm alter 
nates between a data projection step and curve re-estimation step. in the projection
step, each data point is assigned to a value of >.. corresponding to the closest point
on the curve. then in the re-estimation step, each point on the curve is given by
a weighted average of those points that project to nearby points on the curve, with
points closest on the curve given the greatest weight. in the case where the subspace
is constrained to be linear, the procedure converges to the first principal component
and is equivalent to the power method for finding the largest eigenvector of the co 
variance matrix. principal curves can be generalized to multidimensional manifolds
called principal surfaces although these have found limited use due to the difficulty
of data smoothing in higher dimensions even for two-dimensional manifolds.

pca is often used to project a data set onto a lower-dimensional space, for ex 
ample two dimensional, for the purposes of visualization. another linear technique
with a similar aim is multidimensional scaling, or mds (cox and cox, 2000). it finds
a low-dimensional projection of the data such as to preserve, as closely as possible,
the pairwise distances between data points, and involves finding the eigenvectors of
the distance matrix. in the case where the distances are euclidean, it gives equivalent
results to pca. the mds concept can be extended to a wide variety of data types
specified in terms of a similarity matrix, giving nonmetric mds.

two other nonprobabilistic methods for id84 and data vi 
sualization are worthy of mention. locally linear embedding, or lle (roweis and
saul, 2000) first computes the set of coefficients that best reconstructs each data
point from its neighbours. these coefficients are arranged to be invariant to rota 
tions, translations, and scalings of that data point and its neighbours, and hence they
characterize the local geometrical properties of the neighbourhood. lle then maps
the high-dimensional data points down to a lower dimensional space while preserv 
if the local neighbourhood for a particular
ing these neighbourhood coefficients.
data point can be considered linear, then the transformation can be achieved using
a combination of translation, rotation, and scaling, such as to preserve the angles
formed between the data points and their neighbours. because the weights are in 
variant to these transformations, we expect the same weight values to reconstruct the
data points in the low-dimensional space as in the high-dimensional data space. in
spite of the nonlinearity, the optimization for lle does not exhibit local minima.

in isometric feature mapping, or isomap (tenenbaum et ai., 2000), the goal is
to project the data to a lower-dimensional space using mds, but where the dissim 
ilarities are defined in terms of the geodesic distances measured along the mani-

12.4. nonlinear latent variable models

597

fold. for instance, if two points lie on a circle, then the geodesic is the arc-length
distance measured around the circumference of the circle not the straight line dis 
tance measured along the chord connecting them. the algorithm first defines the
neighbourhood for each data point, either by finding the k nearest neighbours or by
finding all points within a sphere of radius e. a graph is then constructed by link 
ing all neighbouring points and labelling them with their euclidean distance. the
geodesic distance between any pair of points is then approximated by the sum of
the arc lengths along the shortest path connecting them (which itself is found using
standard algorithms). finally, metric mds is applied to the geodesic distance matrix
to find the low-dimensional projection.

our focus in this chapter has been on models for which the observed vari 
ables are continuous. we can also consider models having continuous latent vari 
ables together with discrete observed variables, giving rise to latent trait models
(bartholomew, 1987). in this case, the marginalization over the continuous latent
variables, even for a linear relationship between latent and observed variables, can 
not be performed analytically, and so more sophisticated techniques are required.
tipping (1999) uses variational id136 in a model with a two-dimensional latent
space, allowing a binary data set to be visualized analogously to the use of pca to
visualize continuous data. note that this model is the dual of the bayesian logistic
regression problem discussed in section 4.5. in the case of id28 we
have n observations of the feature vector <l>n which are parameterized by a single
parameter vector w, whereas in the latent space visualization model there is a single
latent space variable x (analogous to <1   and n copies of the latent variable w n . a
generalization of probabilistic latent variable models to general exponential family
distributions is described in collins et al. (2002).

we have already noted that an arbitrary distribution can be formed by taking a
gaussian random variable and transforming it through a suitable nonlinearity. this
is exploited in a general latent variable model called a density network (mackay,
1995; mackay and gibbs, 1999) in which the nonlinear function is governed by a
multilayered neural network. if the network has enough hidden units, it can approx 
imate a given nonlinear function to any desired accuracy. the downside of having
such a flexible model is that the marginalization over the latent variables, required in
order to obtain the likelihood function, is no longer analytically tractable. instead,
the likelihood is approximated using monte carlo techniques by drawing samples
from the gaussian prior. the marginalization over the latent variables then becomes
a simple sum with one term for each sample. however, because a large number
of sample points may be required in order to give an accurate representation of the
marginal, this procedure can be computationally costly.

if we consider more restricted forms for the nonlinear function, and make an ap 
propriate choice of the latent variable distribution, then we can construct a latent vari 
able model that is both nonlinear and efficient to train. the generative topographic
mapping, or gtm (bishop et ai., 1996; bishop et ai., 1997a; bishop et ai., 1998b)
uses a latent distribution that is defined by a finite regular grid of delta functions over
the (typically two-dimensional) latent space. marginalization over the latent space
then simply involves summing over the contributions from each of the grid locations.

chapter 5

chapter jj

598

12. continuous latent va k1ahu':s

?lot ot trle oillkyw <:lata wllisualiz.ed using pea on the left and gtm on itle ngr,t fof tile gtm
flliu.e 12.21
model. each <lata poinils plollfld at tile mean ot its posm'k>< dislribution in ..tent s;>ace, tile "","ineanty mlhe
gtm 1tlod8i._lha sepamlion betwoon the groups of data points to be .....n """. ckl.arfy,

.   .'"

ch"l'l~f j

s~etioo /.4

the no"liotar mapping is gi,'en by a id75 model thai allow, for general
iio/llinearily while being a linear fuoction of tile adapli'-e parameler<, noie thai tilt
usual limitation of id75 models arising from the en"", of dimen,iooalily
does 1101 arise in the contr~1 of lhe gt~i si""'e the "\3nifold generall)' ha< t,,'o di"ltn  
sions irrespecti'-e of the dimensionality of the data space, a coo""!",,nce of illese
11"0 cooices is that the likelihood funclion can be e~pressed analytically in dosed
form and can be optimilc<.! efficiently o,ing the em algorithm_ the resolting gtm
model his a lwo-dimensional nonlinear manifold 10 tile dala sel. and by e"alualing
the posterior distrilj",lion (wer latent space for the data poi"", they can he projectt<j
back to the lalent 'ji'k'" for .'isualilalion purposes, figure 12,21 sl"""s a comparison
of the oil data..,1 "isualired wilh lincar pea and wilh lhe iio/lhnear gt~i,
tilt gtm can be seen as a probabilistic "'rsion of an earlier nlod<l callm the '''if
org"nidng ""'p. or som (kohonen. 1982: kobonen. (995). which also represents
a iwo-dimensiooal iio/llincar manifoid as a regular array of disc"'le points. the
som i' somewhat remin;""'nt of the k  trlcan, algorithm in that data points are
a.,igr.ed to nearby prolol)'j>c '-eclon thai are lhen subsc<juenlly updale<!.
initially.
lhe proioi)'jl('s are distribuled at random, and during the training process they 'selr
organize' so as to apl'ro~imalea smoolh manifold. unlike k -mean'. how'e"e.. the
som is tioi optimizing any well.ddine<! cost function (erwin .. al.. 1992) making"
difficult to s." the parameters of the model and 10 assess con'-ergence. there i' also
no guarantee that the '",if-<>rganilalion' will take place .. this is depen""nl 00 the
choice of appropriate paranlttcr "aloc' f,,, any particular data sel.

by oofitrast, gtm optimize, the log likelihood functioo, and the resolting model
define' a probabilily den,ity in dma ,pace,
in fael il corre,ponds to a con,m,incd
mi,ture of gaussian, in which the component.' ,h.re a conlnlon ".riance.   nd the
mean, are con'trained to lie on a 'mooih tw-o-diitlcn,iooal n1anifold. this proba-

section 6.4

exercises

appendix e

exercises

599

bilistic foundation also makes it very straightforward to define generalizations of
gtm (bishop et al., 1998a) such as a bayesian treatment, dealing with missing val-
ues, a principled extension to discrete variables, the use of gaussian processes to
define the manifold, or a hierarchical gtm model (tino and nabney, 2002).

because the manifold in gtm is defined as a continuous surface, not just at the
prototype vectors as in the som, it is possible to compute the magnification factors
corresponding to the local expansions and compressions of the manifold needed to
fit the data set (bishop et al., 1997b) as well as the directional curvatures of the
manifold (tino et al., 2001). these can be visualized along with the projected data
and provide additional insight into the model.

12.1 (* *) lib in this exercise, we use proof by induction to show that the linear
projection onto an m -dimensional subspace that maximizes the variance of the pro 
jected data is defined by the m eigenvectors of the data covariance matrix s, given
by (12.3), corresponding to the m largest eigenvalues. in section 12.1, this result
was proven for the case of m = 1. now suppose the result holds for some general
value of m and show that it consequently holds for dimensionality m + 1. to do
this, first set the derivative of the variance of the projected data with respect to a
vector um+1 defining the new direction in data space equal to zero. this should
be done subject to the constraints that um +l be orthogonal to the existing vectors
u1,"" um, and also that it be normalized to unit length. use lagrange multipli-
ers to enforce these constraints. then make use of the orthonormality properties of
the vectors u1,"" um to show that the new vector um+1 is an eigenvector of s.
finally, show that the variance is maximized if the eigenvector is chosen to be the
one corresponding to eigenvector am+1 where the eigenvalues have been ordered in
decreasing value.

12.2 (**)

show that the minimum value of the pca distortion measure j given by
(12.15) with respect to the ui, subject to the orthonormality constraints (12.7), is
obtained when the ui are eigenvectors of the data covariance matrix s. to do this,
introduce a matrix h of lagrange multipliers, one for each constraint, so that the
modified distortion measure, in matrix notation reads

] = tr { utsu} + tr { h(i - utu) }

(12.93)

where uis a m~trix of dimensio~d x (d - m) whose columns are gi:::..en b~ ui.
now minimize j with respect to u and show that the s~ution satisfies su = uh.
clearly, one possible solution is that the columns of u are eigenvectors of s, in
which case h is a diagonal matrix containing the corresponding eigenvalues. to
obtain the general solution, show that h can be assumed to be a symmetr~ ma~ix,
and by using its eigenvect  r expansion show that the general solution to su =~uh
gives the same value for j as the specific solution in which the columns of u are

600

12. continuous latent variables

the eigenvectors of s. because these solutions are all equivalent, it is convenient to
choose the eigenvector solution.

12.3 (*) verify that the eigenvectors defined by (12.30) are normalized to unit length,

assuming that the eigenvectors vi have unit length.

12.4 (*) imm suppose we replace the zero-mean, unit-covariance latent space distri 
bution (12.31) in the probabilistic pca model by a general gaussian distribution of
the formn(zlm, ~). by redefining the parameters of the model, show that this leads
to an identical model for the marginal distribution p(x) over the observed variables
for any valid choice of m and ~.

12.5 (* *) let x be a d-dimensional random variable having a gaussian distribution
given by n(xijl, ~), and consider the m-dimensional random variable given by
y = ax + b where a is an m x d matrix. show that y also has a gaussian
distribution, and find expressions for its mean and covariance. discuss the form of
this gaussian distribution for m < d, for m = d, and for m > d.

12.6 (*) imm draw a directed probabilistic graph for the probabilistic pca model
described in section 12.2 in which the components of the observed variable x are
shown explicitly as separate nodes. hence verify that the probabilistic pca model
has the same independence structure as the naive bayes model discussed in sec 
tion 8.2.2.

12.7 (* *) by making use of the results (2.270) and (2.271) for the mean and covariance
of a general distribution, derive the result (12.35) for the marginal distribution p(x)
in the probabilistic pca model.

12.8 (* *)imm by making use of the result (2.116), show that the posterior distribution

p(zlx) for the probabilistic pca model is given by (12.42).

12.9 (*) verify that maximizing the log likelihood (12.43) for the probabilistic pca
model with respect to the parameter jl gives the result jlml = x where x is the
mean of the data vectors.

12.10 (**) by evaluating the second derivatives of the log likelihood function (12.43) for
the probabilistic pca model with respect to the parameter jl, show that the stationary
point jlml = x represents the unique maximum.
(* *) imm show that in the limit (y2 -. 0, the posterior mean for the probabilistic
pca model becomes an orthogonal projection onto the principal subspace, as in
conventional pca.

12.11

12.12 (* *) for (y2 > 0 show that the posterior mean in the probabilistic pca model is

shifted towards the origin relative to the orthogonal projection.

12.13 (* *) show that the optimal reconstruction of a data point under probabilistic pca,

according to the least squares projection cost of conventional pca, is given by

(12.94)

exercises

601

12.14 (*) the number of independent parameters in the covariance matrix for the proba 
bilistic pca model with an m -dimensional latent space and a d-dimensional data
space is given by (12.51). verify that in the case of m = d - 1, the number of
independent parameters is the same as in a general covariance gaussian, whereas for

m =   it is the same as for a gaussian with an isotropic covariance.

12.15 (**) iiii!i derive the m-step equations (12.56) and (12.57) for the probabilistic
pca model by maximization of the expected complete-data log likelihood function
given by (12.53).

12.16 (* * *)

in figure 12.11, we showed an application of probabilistic pca to a data set
in which some of the data values were missing at random. derive the em algorithm
for maximizing the likelihood function for the probabilistic pca model in this situ 
ation. note that the {zn}, as well as the missing data values that are components of
the vectors {x n }, are now latent variables. show that in the special case in which all
of the data values are observed, this reduces to the em algorithm for probabilistic
pca derived in section 12.2.2.

12.17 (**) iiii!i let w be a d x m matrix whose columns define a linear subspace
of dimensionality m embedded within a data space of dimensionality d, and let j1
be a d-dimensional vector. given a data set {x n } where n = 1, ... , n, we can
approximate the data points using a linear mapping from a set of m -dimensional
vectors {zn}, so that xn is approximated by w zn + j1. the associated sum-of 
squares reconstruction cost is given by

n

j = l ilxn - j1- wzn 11 2

.

n=l

(12.95)

first show that minimizing j with respect to j1leads to an analogous expression with
x n and zn replaced by zero-mean variables x n - x and zn - z, respectively, where x
and z denote sample means. then show that minimizing j with respect to zn, where
w is kept fixed, gives rise to the pca estep (12.58), and that minimizing j with
respect to w, where {zn} is kept fixed, gives rise to the pca m step (12.59).

12.18 (*) derive an expression for the number of independent parameters in the factor

analysis model described in section 12.2.4.

12.19 (**) iiii!i show that the factor analysis model described in section 12.2.4 is

invariant under rotations of the latent space coordinates.

12.20 (**) by considering second derivatives, show that the only stationary point of
the log likelihood function for the factor analysis model discussed in section 12.2.4
with respect to the parameter j1 is given by the sample mean defined by (12.1).
furthermore, show that this stationary point is a maximum.

12.21 (**) derive the formulae (12.66) and (12.67) for the e step of the em algorithm
for factor analysis. note that from the result of exercise 12.20, the parameter j1 can
be replaced by the sample mean x.

602

12. continuous latent variables

12.22 (* *) write down an expression for the expected complete-data log likelihood func 
tion for the factor analysis model, and hence derive the corresponding m step equa 
tions (12.69) and (12.70).

12.23 (*) iii!i draw a directed probabilistic graphical model representing a discrete
mixture of probabilistic pca models in which each pca model has its own values
of w, jl, and 0-
    now draw a modified graph in which these parameter values are
shared between the components of the mixture.

2

12.24 (***) we saw in section 2.3.7 that student's t-distribution can be viewed as an
infinite mixture of gaussians in which we marginalize with respect to a continu 
ous latent variable. by exploiting this representation, formulate an em algorithm
for maximizing the log likelihood function for a multivariate student's t-distribution
given an observed set of data points, and derive the forms of the e and m step equa 
tions.

12.25 (**) iii!i consider a linear-gaussian latent-variable model having a latent space
distribution p(z) = n(xio, i) and a conditional distribution for the observed vari 
able p(xlz) = n(xlwz + il, <p) where <p is an arbitrary symmetric, positive 
definite noise covariance matrix. now suppose that we make a nonsingular linear
transformation of the data variables x ---t ax, where a is a d x d matrix.
if
jlml' w ml and <pml represent the maximum likelihood solution corresponding to
the original untransformed data, show that ajlml' awml, and a <pmlat will rep 
resent the corresponding maximum likelihood solution for the transformed data set.
finally, show that the form of the model is preserved in two cases: (i) a is a diagonal
matrix and <p is a diagonal matrix. this corresponds to the case of factor analysis.
the transformed <p remains diagonal, and hence factor analysis is covariant under
component-wise re-scaling of the data variables; (ii) a is orthogonal and <p is pro 
21. this corresponds to probabilistic pca.
portional to the unit matrix so that <p = 0-
the transformed <p matrix remains proportional to the unit matrix, and hence proba 
bilistic pca is covariant under a rotation of the axes of data space, as is the case for
conventional pca.

\

12.26 (**) show that any vector ai that satisfies (12.80) will also satisfy (12.79). also,
show that for any solution of (12.80) having eigenvalue a, we can add any multiple
of an eigenvector of k having zero eigenvalue, and obtain a solution to (12.79)
that also has eigenvalue a. finally, show that such modifications do not affect the
principal-component projection given by (12.82).

12.27 (* *) show that the conventional linear pca algorithm is recovered as a special case

of kernel pca if we choose the linear id81 given by k(x, x') = x t x'.

12.28 (* *) iii!i use the transformation property (1.27) of a id203 density under
a change of variable to show that any density p(y) can be obtained from a fixed
density q(x) that is everywhere nonzero by making a nonlinear change of variable
y = f(x) in which f(x) is a monotonic function so that 0 :::; j'(x) < 00. write
down the differential equation satisfied by f (x) and draw a diagram illustrating the
transformation of the density.

exercises

603

12.29 (**)em suppose that two variables zl and z2 are independent so thatp(zl' z2) =
p(zl)p(z2)' show that the covariance matrix between these variables is diagonal.
this shows that independence is a sufficient condition for two variables to be un 
correlated. now consider two variables yl and y2 in which -1 :0;; yl
:0;; 1 and
y2 = yg. write down the conditional distribution p(y2iyl) and observe that this is
dependent on yb showing that the two variables are not independent. now show
that the covariance matrix between these two variables is again diagonal. to do this,
use the relation p(yl, y2) = p(yi )p(y2iyl) to show that the off-diagonal terms are
zero. this counter-example shows that zero correlation is not a sufficient condition
for independence.

13

sequential

data

so far in this book, we have focussed primarily on sets of data points that were as-
sumed to be independent and identically distributed (i.i.d.). this assumption allowed
us to express the likelihood function as the product over all data points of the prob-
ability distribution evaluated at each data point. for many applications, however,
the i.i.d. assumption will be a poor one. here we consider a particularly important
class of such data sets, namely those that describe sequential data. these often arise
through measurement of time series, for example the rainfall measurements on suc-
cessive days at a particular location, or the daily values of a currency exchange rate,
or the acoustic features at successive time frames used for id103. an
example involving speech data is shown in figure 13.1. sequential data can also
arise in contexts other than time series, for example the sequence of nucleotide base
pairs along a strand of dna or the sequence of characters in an english sentence.
for convenience, we shall sometimes refer to    past    and    future    observations in a
sequence. however, the models explored in this chapter are equally applicable to all

605

606

13. sequential data

figure 13.1 example of a spectro-
gram of the spoken words    bayes    theo-
rem    showing a plot of the intensity of the
spectral coef   cients versus time index.

forms of sequential data, not just temporal sequences.

it is useful to distinguish between stationary and nonstationary sequential dis-
tributions. in the stationary case, the data evolves in time, but the distribution from
which it is generated remains the same. for the more complex nonstationary situa-
tion, the generative distribution itself is evolving with time. here we shall focus on
the stationary case.

for many applications, such as    nancial forecasting, we wish to be able to pre-
dict the next value in a time series given observations of the previous values. in-
tuitively, we expect that recent observations are likely to be more informative than
more historical observations in predicting future values. the example in figure 13.1
shows that successive observations of the speech spectrum are indeed highly cor-
related. furthermore, it would be impractical to consider a general dependence of
future observations on all previous observations because the complexity of such a
model would grow without limit as the number of observations increases. this leads
us to consider markov models in which we assume that future predictions are inde-

13.1. markov models

607

figure 13.2 the simplest approach to
modelling a sequence of ob-
servations is to treat
them
as independent, correspond-
ing to a graph without links.

x1

x2

x3

x4

pendent of all but the most recent observations.

although such models are tractable, they are also severely limited. we can ob-
tain a more general framework, while still retaining tractability, by the introduction
of latent variables, leading to state space models. as in chapters 9 and 12, we shall
see that complex models can thereby be constructed from simpler components (in
particular, from distributions belonging to the exponential family) and can be read-
ily characterized using the framework of probabilistic id114. here we
focus on the two most important examples of state space models, namely the hid-
den markov model, in which the latent variables are discrete, and linear dynamical
systems, in which the latent variables are gaussian. both models are described by di-
rected graphs having a tree structure (no loops) for which id136 can be performed
ef   ciently using the sum-product algorithm.

13.1. markov models

the easiest way to treat sequential data would be simply to ignore the sequential
aspects and treat the observations as i.i.d., corresponding to the graph in figure 13.2.
such an approach, however, would fail to exploit the sequential patterns in the data,
such as correlations between observations that are close in the sequence. suppose,
for instance, that we observe a binary variable denoting whether on a particular day
it rained or not. given a time series of recent observations of this variable, we wish
to predict whether it will rain on the next day. if we treat the data as i.i.d., then the
only information we can glean from the data is the relative frequency of rainy days.
however, we know in practice that the weather often exhibits trends that may last for
several days. observing whether or not it rains today is therefore of signi   cant help
in predicting if it will rain tomorrow.

to express such effects in a probabilistic model, we need to relax the i.i.d. as-
sumption, and one of the simplest ways to do this is to consider a markov model.
first of all we note that, without loss of generality, we can use the product rule to
express the joint distribution for a sequence of observations in the form

n(cid:14)

p(x1, . . . , xn ) =

p(xn|x1, . . . , xn   1).

(13.1)

n=1

if we now assume that each of the conditional distributions on the right-hand side
is independent of all previous observations except the most recent, we obtain the
   rst-order markov chain, which is depicted as a graphical model in figure 13.3. the

608

13. sequential data

figure 13.3 a    rst-order markov chain of ob-
servations {xn} in which the dis-
tribution p(xn|xn   1) of a particu-
lar observation xn is conditioned
on the value of the previous ob-
servation xn   1.

x1

x2

x3

x4

joint distribution for a sequence of n observations under this model is given by

p(x1, . . . , xn ) = p(x1)

p(xn|xn   1).

(13.2)

section 8.2

exercise 13.1

n=2

from the d-separation property, we see that the conditional distribution for observa-
tion xn, given all of the observations up to time n, is given by
p(xn|x1, . . . , xn   1) = p(xn|xn   1)

(13.3)

which is easily veri   ed by direct evaluation starting from (13.2) and using the prod-
uct rule of id203. thus if we use such a model to predict the next observation
in a sequence, the distribution of predictions will depend only on the value of the im-
mediately preceding observation and will be independent of all earlier observations.
in most applications of such models, the conditional distributions p(xn|xn   1)
that de   ne the model will be constrained to be equal, corresponding to the assump-
tion of a stationary time series. the model is then known as a homogeneous markov
chain. for instance, if the conditional distributions depend on adjustable parameters
(whose values might be inferred from a set of training data), then all of the condi-
tional distributions in the chain will share the same values of those parameters.

although this is more general than the independence model, it is still very re-
strictive. for many sequential observations, we anticipate that the trends in the data
over several successive observations will provide important information in predict-
ing the next value. one way to allow earlier observations to have an in   uence is to
move to higher-order markov chains. if we allow the predictions to depend also on
the previous-but-one value, we obtain a second-order markov chain, represented by
the graph in figure 13.4. the joint distribution is now given by

n(cid:14)

n(cid:14)

p(x1, . . . , xn ) = p(x1)p(x2|x1)

p(xn|xn   1, xn   2).

(13.4)

again, using d-separation or by direct evaluation, we see that the conditional distri-
bution of xn given xn   1 and xn   2 is independent of all observations x1, . . . xn   3.

n=3

figure 13.4 a second-order markov chain, in
which the conditional distribution
of a particular observation xn
depends on the values of the two
previous observations xn   1 and
xn   2.

x1

x2

x3

x4

figure 13.5 we can represent sequen-
tial data using a markov chain of latent
variables, with each observation condi-
tioned on the state of the corresponding
latent variable. this important graphical
structure forms the foundation both for the
hidden markov model and for linear dy-
namical systems.

z1

x1

z2

x2

13.1. markov models

zn   1

zn

609

zn+1

xn   1

xn

xn+1

each observation is now in   uenced by two previous observations. we can similarly
consider extensions to an m th order markov chain in which the conditional distri-
bution for a particular variable depends on the previous m variables. however, we
have paid a price for this increased    exibility because the number of parameters in
the model is now much larger. suppose the observations are discrete variables hav-
ing k states. then the conditional distribution p(xn|xn   1) in a    rst-order markov
chain will be speci   ed by a set of k     1 parameters for each of the k states of xn   1
giving a total of k(k     1) parameters. now suppose we extend the model to an
m th order markov chain, so that the joint distribution is built up from conditionals
p(xn|xn   m , . . . , xn   1). if the variables are discrete, and if the conditional distri-
butions are represented by general id155 tables, then the number
of parameters in such a model will have km   1(k     1) parameters. because this
grows exponentially with m, it will often render this approach impractical for larger
values of m.

for continuous variables, we can use linear-gaussian conditional distributions
in which each node has a gaussian distribution whose mean is a linear function
of its parents. this is known as an autoregressive or ar model (box et al., 1994;
thiesson et al., 2004). an alternative approach is to use a parametric model for
p(xn|xn   m , . . . , xn   1) such as a neural network. this technique is sometimes
called a tapped delay line because it corresponds to storing (delaying) the previous
m values of the observed variable in order to predict the next value. the number
of parameters can then be much smaller than in a completely general model (for ex-
ample it may grow linearly with m), although this is achieved at the expense of a
restricted family of conditional distributions.

suppose we wish to build a model for sequences that is not limited by the
markov assumption to any order and yet that can be speci   ed using a limited number
of free parameters. we can achieve this by introducing additional latent variables to
permit a rich class of models to be constructed out of simple components, as we did
with mixture distributions in chapter 9 and with continuous latent variable models in
chapter 12. for each observation xn, we introduce a corresponding latent variable
zn (which may be of different type or dimensionality to the observed variable). we
now assume that it is the latent variables that form a markov chain, giving rise to the
graphical structure known as a state space model, which is shown in figure 13.5. it
satis   es the key conditional independence property that zn   1 and zn+1 are indepen-
dent given zn, so that

zn+1        zn   1 | zn.

(13.5)

610

13. sequential data

the joint distribution for this model is given by

p(x1, . . . , xn , z1, . . . , zn) = p(z1)

p(zn|zn   1)

(cid:31)

n(cid:14)

n=2

 

n(cid:14)

n=1

p(xn|zn).

(13.6)

using the d-separation criterion, we see that there is always a path connecting any
two observed variables xn and xm via the latent variables, and that this path is never
blocked. thus the predictive distribution p(xn+1|x1, . . . , xn) for observation xn+1
given all previous observations does not exhibit any conditional independence prop-
erties, and so our predictions for xn+1 depends on all previous observations. the
observed variables, however, do not satisfy the markov property at any order. we
shall discuss how to evaluate the predictive distribution in later sections of this chap-
ter.

there are two important models for sequential data that are described by this
graph. if the latent variables are discrete, then we obtain the hidden markov model,
or id48 (elliott et al., 1995). note that the observed variables in an id48 may
be discrete or continuous, and a variety of different conditional distributions can be
used to model them. if both the latent and the observed variables are gaussian (with
a linear-gaussian dependence of the conditional distributions on their parents), then
we obtain the linear dynamical system.

section 13.2

section 13.3

13.2. id48

the hidden markov model can be viewed as a speci   c instance of the state space
model of figure 13.5 in which the latent variables are discrete. however, if we
examine a single time slice of the model, we see that it corresponds to a mixture
distribution, with component densities given by p(x|z).
it can therefore also be
interpreted as an extension of a mixture model in which the choice of mixture com-
ponent for each observation is not selected independently but depends on the choice
of component for the previous observation. the id48 is widely used in speech
recognition (jelinek, 1997; rabiner and juang, 1993), natural language modelling
(manning and sch  utze, 1999), on-line handwriting recognition (nag et al., 1986),
and for the analysis of biological sequences such as proteins and dna (krogh et al.,
1994; durbin et al., 1998; baldi and brunak, 2001).

as in the case of a standard mixture model, the latent variables are the discrete
multinomial variables zn describing which component of the mixture is responsible
for generating the corresponding observation xn. again, it is convenient to use a
1-of-k coding scheme, as used for mixture models in chapter 9. we now allow the
id203 distribution of zn to depend on the state of the previous latent variable
zn   1 through a conditional distribution p(zn|zn   1). because the latent variables are
k-dimensional binary variables, this conditional distribution corresponds to a table
of numbers that we denote by a, the elements of which are known as transition
probabilities. they are given by ajk     p(znk = 1|zn   1,j = 1), and because they
are probabilities, they satisfy 0 (cid:1) ajk (cid:1) 1 with
k ajk = 1, so that the matrix a

(cid:5)

13.2. id48

611

figure 13.6 transition diagram showing a model whose la-
tent variables have three possible states corre-
sponding to the three boxes. the black lines
denote the elements of the transition matrix
ajk.

a22

a21

a12

k = 2

a32

a23

k = 1

a11

k = 3

a31

a13

a33

has k(k   1) independent parameters. we can then write the conditional distribution
explicitly in the form

p(zn|zn   1,a) =

zn   1,j znk
jk

.

a

(13.7)

the initial latent node z1 is special in that it does not have a parent node, and so
it has a marginal distribution p(z1) represented by a vector of probabilities    with
elements   k     p(z1k = 1), so that

p(z1|  ) =

  z1k
k

(13.8)

k(cid:14)

k(cid:14)

k=1

j=1

k(cid:14)

k=1

(cid:5)

where

k   k = 1.

the transition matrix is sometimes illustrated diagrammatically by drawing the
states as nodes in a state transition diagram as shown in figure 13.6 for the case of
k = 3. note that this does not represent a probabilistic graphical model, because
the nodes are not separate variables but rather states of a single variable, and so we
have shown the states as boxes rather than circles.

section 8.4.5

it is sometimes useful to take a state transition diagram, of the kind shown in
figure 13.6, and unfold it over time. this gives an alternative representation of the
transitions between latent states, known as a lattice or trellis diagram, and which is
shown for the case of the hidden markov model in figure 13.7.
the speci   cation of the probabilistic model is completed by de   ning the con-
ditional distributions of the observed variables p(xn|zn,   ), where    is a set of pa-
rameters governing the distribution. these are known as emission probabilities, and
might for example be given by gaussians of the form (9.11) if the elements of x are
continuous variables, or by id155 tables if x is discrete. because
xn is observed, the distribution p(xn|zn,   ) consists, for a given value of   , of a
vector of k numbers corresponding to the k possible states of the binary vector zn.

612

13. sequential data

figure 13.7 if we unfold the state transition dia-
gram of figure 13.6 over time, we obtain a lattice,
or trellis, representation of the latent states. each
column of this diagram corresponds to one of the
latent variables zn.

k = 1

k = 2

k = 3

a11

a11

a11

a33

n     2

n     1

a33

n

a33

n + 1

we can represent the emission probabilities in the form

p(xn|zn,   ) =

p(xn|  k)znk .

(13.9)

k(cid:14)

k=1

we shall focuss attention on homogeneous models for which all of the condi-
tional distributions governing the latent variables share the same parameters a, and
similarly all of the emission distributions share the same parameters    (the extension
to more general cases is straightforward). note that a mixture model for an i.i.d. data
set corresponds to the special case in which the parameters ajk are the same for all
values of j, so that the conditional distribution p(zn|zn   1) is independent of zn   1.
this corresponds to deleting the horizontal links in the graphical model shown in
figure 13.5.

the joint id203 distribution over both latent and observed variables is then

given by

p(x, z|  ) = p(z1|  )

(cid:31)

n(cid:14)

 

n(cid:14)

p(zn|zn   1, a)

p(xm|zm,   )

(13.10)

exercise 13.4

n=2

m=1

where x = {x1, . . . , xn}, z = {z1, . . . , zn}, and    = {  , a,   } denotes the set
of parameters governing the model. most of our discussion of the hidden markov
model will be independent of the particular choice of the emission probabilities.
indeed, the model is tractable for a wide range of emission distributions including
discrete tables, gaussians, and mixtures of gaussians. it is also possible to exploit
discriminative models such as neural networks. these can be used to model the
emission density p(x|z) directly, or to provide a representation for p(z|x) that can
be converted into the required emission density p(x|z) using bayes    theorem (bishop
et al., 2004).

we can gain a better understanding of the hidden markov model by considering
it from a generative point of view. recall that to generate samples from a mixture of

1

0.5

k = 1

0

0

k = 3

k = 2

0.5

13.2. id48

613

1

0.5

1

0

0

0.5

1

figure 13.8 illustration of sampling from a hidden markov model having a 3-state latent variable z and a
gaussian emission model p(x|z) where x is 2-dimensional. (a) contours of constant id203 density for the
emission distributions corresponding to each of the three states of the latent variable. (b) a sample of 50 points
drawn from the hidden markov model, colour coded according to the component that generated them and with
lines connecting the successive observations. here the transition matrix was    xed so that in any state there is a
5% id203 of making a transition to each of the other states, and consequently a 90% id203 of remaining
in the same state.

gaussians, we    rst chose one of the components at random with id203 given by
the mixing coef   cients   k and then generate a sample vector x from the correspond-
ing gaussian component. this process is repeated n times to generate a data set of
n independent samples. in the case of the hidden markov model, this procedure is
modi   ed as follows. we    rst choose the initial latent variable z1 with probabilities
governed by the parameters   k and then sample the corresponding observation x1.
now we choose the state of the variable z2 according to the transition probabilities
p(z2|z1) using the already instantiated value of z1. thus suppose that the sample for
z1 corresponds to state j. then we choose the state k of z2 with probabilities ajk
for k = 1, . . . , k. once we know z2 we can draw a sample for x2 and also sample
the next latent variable z3 and so on. this is an example of ancestral sampling for
a directed graphical model. if, for instance, we have a model in which the diago-
nal transition elements akk are much larger than the off-diagonal elements, then a
typical data sequence will have long runs of points generated from a single compo-
nent, with infrequent transitions from one component to another. the generation of
samples from a hidden markov model is illustrated in figure 13.8.

there are many variants of the standard id48 model, obtained for instance by
imposing constraints on the form of the transition matrix a (rabiner, 1989). here we
mention one of particular practical importance called the left-to-right id48, which
is obtained by setting the elements ajk of a to zero if k < j, as illustrated in the

section 8.1.2

614

13. sequential data

figure 13.9 example of the state transition diagram for a 3-state
left-to-right hidden markov model. note that once a
state has been vacated, it cannot later be re-entered.

a11

a22

a33

a12

a23

k = 1

a13

k = 2

k = 3

state transition diagram for a 3-state id48 in figure 13.9. typically for such models
the initial state probabilities for p(z1) are modi   ed so that p(z11) = 1 and p(z1j) = 0
for j (cid:9)= 1, in other words every sequence is constrained to start in state j = 1. the
transition matrix may be further constrained to ensure that large changes in the state
index do not occur, so that ajk = 0 if k > j +    . this type of model is illustrated
using a lattice diagram in figure 13.10.

many applications of id48, for example id103,
or on-line character recognition, make use of left-to-right architectures. as an illus-
tration of the left-to-right hidden markov model, we consider an example involving
handwritten digits. this uses on-line data, meaning that each digit is represented
by the trajectory of the pen as a function of time in the form of a sequence of pen
coordinates, in contrast to the off-line digits data, discussed in appendix a, which
comprises static two-dimensional pixellated images of the ink. examples of the on-
line digits are shown in figure 13.11. here we train a hidden markov model on a
subset of data comprising 45 examples of the digit    2   . there are k = 16 states,
each of which can generate a line segment of    xed length having one of 16 possible
angles, and so the emission distribution is simply a 16    16 table of probabilities
associated with the allowed angle values for each state index value. transition prob-
abilities are all set to zero except for those that keep the state index k the same or
that increment it by 1, and the model parameters are optimized using 25 iterations of
em. we can gain some insight into the resulting model by running it generatively, as
shown in figure 13.11.

figure 13.10 lattice diagram for a 3-state left-
to-right id48 in which the state index k is allowed
to increase by at most 1 at each transition.

k = 1

k = 2

k = 3

a11

a11

a11

a33

n     2

n     1

a33

n

a33

n + 1

13.2. id48

615

figure 13.11 top row: examples of on-line handwritten
digits. bottom row: synthetic digits sam-
pled generatively from a left-to-right hid-
den markov model that has been trained
on a data set of 45 handwritten digits.

one of the most powerful properties of id48 is their ability to
exhibit some degree of invariance to local warping (compression and stretching) of
the time axis. to understand this, consider the way in which the digit    2    is written
in the on-line handwritten digits example. a typical digit comprises two distinct
sections joined at a cusp. the    rst part of the digit, which starts at the top left, has a
sweeping arc down to the cusp or loop at the bottom left, followed by a second more-
or-less straight sweep ending at the bottom right. natural variations in writing style
will cause the relative sizes of the two sections to vary, and hence the location of the
cusp or loop within the temporal sequence will vary. from a generative perspective
such variations can be accommodated by the hidden markov model through changes
in the number of transitions to the same state versus the number of transitions to the
successive state. note, however, that if a digit    2    is written in the reverse order, that
is, starting at the bottom right and ending at the top left, then even though the pen tip
coordinates may be identical to an example from the training set, the id203 of
the observations under the model will be extremely small. in the id103
context, warping of the time axis is associated with natural variations in the speed of
speech, and again the hidden markov model can accommodate such a distortion and
not penalize it too heavily.

13.2.1 maximum likelihood for the id48
if we have observed a data set x = {x1, . . . , xn}, we can determine the param-
eters of an id48 using maximum likelihood. the likelihood function is obtained
from the joint distribution (13.10) by marginalizing over the latent variables

(cid:2)

p(x|  ) =

p(x, z|  ).

(13.11)

z

because the joint distribution p(x, z|  ) does not factorize over n (in contrast to the
mixture distribution considered in chapter 9), we cannot simply treat each of the
summations over zn independently. nor can we perform the summations explicitly
because there are n variables to be summed over, each of which has k states, re-
sulting in a total of k n terms. thus the number of terms in the summation grows

616

13. sequential data

exponentially with the length of the chain. in fact, the summation in (13.11) cor-
responds to summing over exponentially many paths through the lattice diagram in
figure 13.7.

we have already encountered a similar dif   culty when we considered the infer-
ence problem for the simple chain of variables in figure 8.32. there we were able
to make use of the conditional independence properties of the graph to re-order the
summations in order to obtain an algorithm whose cost scales linearly, instead of
exponentially, with the length of the chain. we shall apply a similar technique to the
hidden markov model.

a further dif   culty with the expression (13.11) for the likelihood function is that,
because it corresponds to a generalization of a mixture distribution, it represents a
summation over the emission models for different settings of the latent variables.
direct maximization of the likelihood function will therefore lead to complex ex-
pressions with no closed-form solutions, as was the case for simple mixture models
(recall that a mixture model for i.i.d. data is a special case of the id48).

section 9.2

we therefore turn to the expectation maximization algorithm to    nd an ef   cient
framework for maximizing the likelihood function in id48. the
em algorithm starts with some initial selection for the model parameters, which we
denote by   old. in the e step, we take these parameter values and    nd the posterior
distribution of the latent variables p(z|x,   old). we then use this posterior distri-
bution to evaluate the expectation of the logarithm of the complete-data likelihood
function, as a function of the parameters   , to give the function q(  ,   old) de   ned
by

(cid:2)

q(  ,   old) =

p(z|x,   old) ln p(x, z|  ).

(13.12)

z

at this point, it is convenient to introduce some notation. we shall use   (zn) to
denote the marginal posterior distribution of a latent variable zn, and   (zn   1, zn) to
denote the joint posterior distribution of two successive latent variables, so that

  (zn) = p(zn|x,   old)

  (zn   1, zn) = p(zn   1, zn|x,   old).

(13.13)
(13.14)

for each value of n, we can store   (zn) using a set of k nonnegative numbers
that sum to unity, and similarly we can store   (zn   1, zn) using a k    k matrix of
nonnegative numbers that again sum to unity. we shall also use   (znk) to denote the
id155 of znk = 1, with a similar use of notation for   (zn   1,j, znk)
and for other probabilistic variables introduced later. because the expectation of a
binary random variable is just the id203 that it takes the value 1, we have

  (znk) = e[znk] =

  (z)znk

(13.15)

  (zn   1,j, znk) = e[zn   1,jznk] =

z

  (z)zn   1,jznk.

(13.16)

if we substitute the joint distribution p(x, z|  ) given by (13.10) into (13.12),

z

(cid:2)

(cid:2)

exercise 13.5

exercise 13.6

  k =

ajk =

  (z1j)

  (z1k)

k(cid:2)
n(cid:2)
k(cid:2)
n(cid:2)

n=2

j=1

13.2. id48

617

and make use of the de   nitions of    and    , we obtain

n(cid:2)

k(cid:2)

k(cid:2)

q(  ,   old) =

  (z1k) ln   k +

  (zn   1,j, znk) ln ajk

n=2

j=1

k=1

  (znk) ln p(xn|  k).

(13.17)

k(cid:2)
n(cid:2)

k=1

+

k(cid:2)

n=1

k=1

the goal of the e step will be to evaluate the quantities   (zn) and   (zn   1, zn) ef   -
ciently, and we shall discuss this in detail shortly.
in the m step, we maximize q(  ,   old) with respect to the parameters    =
{  , a,   } in which we treat   (zn) and   (zn   1, zn) as constant. maximization with
respect to    and a is easily achieved using appropriate lagrange multipliers with
the results

(13.18)

  (zn   1,j, znk)

.

(13.19)

  (zn   1,j, znl)

l=1

n=2

the em algorithm must be initialized by choosing starting values for    and a, which
should of course respect the summation constraints associated with their probabilis-
tic interpretation. note that any elements of    or a that are set to zero initially will
remain zero in subsequent em updates. a typical initialization procedure would
involve selecting random starting values for these parameters subject to the summa-
tion and non-negativity constraints. note that no particular modi   cation to the em
results are required for the case of left-to-right models beyond choosing initial values
for the elements ajk in which the appropriate elements are set to zero, because these
will remain zero throughout.

to maximize q(  ,   old) with respect to   k, we notice that only the    nal term
in (13.17) depends on   k, and furthermore this term has exactly the same form as
the data-dependent term in the corresponding function for a standard mixture dis-
tribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a
gaussian mixture. here the quantities   (znk) are playing the role of the responsibil-
ities. if the parameters   k are independent for the different components, then this
term decouples into a sum of terms one for each value of k, each of which can be
maximized independently. we are then simply maximizing the weighted log likeli-
hood function for the emission density p(x|  k) with weights   (znk). here we shall
suppose that this maximization can be done ef   ciently. for instance, in the case of

618

13. sequential data

gaussian emission densities we have p(x|  k) = n (x|  k,   k), and maximization
of the function q(  ,   old) then gives

  (znk)xn

n(cid:2)
n(cid:2)
n(cid:2)

n=1

n=1

  k =

  (znk)

(13.20)

  (znk)(xn       k)(xn       k)t

  k =

n=1

.

(13.21)

  (znk)

for the case of discrete multinomial observed variables, the conditional distribution
of the observations takes the form

p(x|z) =

  xizk
ik

(13.22)

and the corresponding m-step equations are given by

n(cid:2)

n=1

d(cid:14)

k(cid:14)

i=1

k=1

n(cid:2)
n(cid:2)

n=1

  (znk)xni

  ik =

.

  (znk)

(13.23)

n=1

an analogous result holds for bernoulli observed variables.

the em algorithm requires initial values for the parameters of the emission dis-
tribution. one way to set these is    rst to treat the data initially as i.i.d. and    t the
emission density by maximum likelihood, and then use the resulting values to ini-
tialize the parameters for em.

13.2.2 the forward-backward algorithm
next we seek an ef   cient procedure for evaluating the quantities   (znk) and
  (zn   1,j, znk), corresponding to the e step of the em algorithm. the graph for the
hidden markov model, shown in figure 13.5, is a tree, and so we know that the
posterior distribution of the latent variables can be obtained ef   ciently using a two-
stage message passing algorithm. in the particular context of the hidden markov
model, this is known as the forward-backward algorithm (rabiner, 1989), or the
baum-welch algorithm (baum, 1972). there are in fact several variants of the basic
algorithm, all of which lead to the exact marginals, according to the precise form of

exercise 13.8

section 8.4

13.2. id48

619

the messages that are propagated along the chain (jordan, 2007). we shall focus on
the most widely used of these, known as the alpha-beta algorithm.

as well as being of great practical importance in its own right, the forward-
backward algorithm provides us with a nice illustration of many of the concepts
introduced in earlier chapters. we shall therefore begin in this section with a    con-
ventional    derivation of the forward-backward equations, making use of the sum
and product rules of id203, and exploiting conditional independence properties
which we shall obtain from the corresponding graphical model using d-separation.
then in section 13.2.3, we shall see how the forward-backward algorithm can be
obtained very simply as a speci   c example of the sum-product algorithm introduced
in section 8.4.4.
it is worth emphasizing that evaluation of the posterior distributions of the latent
variables is independent of the form of the emission density p(x|z) or indeed of
whether the observed variables are continuous or discrete. all we require is the
values of the quantities p(xn|zn) for each value of zn for every n. also, in this
section and the next we shall omit the explicit dependence on the model parameters
  old because these    xed throughout.

we therefore begin by writing down the following conditional independence

properties (jordan, 2007)

p(x|zn) = p(x1, . . . , xn|zn)

p(xn+1, . . . , xn|zn)
p(x1, . . . , xn   1|xn, zn) = p(x1, . . . , xn   1|zn)
p(x1, . . . , xn   1|zn   1, zn) = p(x1, . . . , xn   1|zn   1)
p(xn+1, . . . , xn|zn, zn+1) = p(xn+1, . . . , xn|zn+1)
p(xn+2, . . . , xn|zn+1, xn+1) = p(xn+2, . . . , xn|zn+1)
p(x|zn   1, zn) = p(x1, . . . , xn   1|zn   1)

(13.24)
(13.25)
(13.26)
(13.27)
(13.28)
p(xn|zn)p(xn+1, . . . , xn|zn) (13.29)
(13.30)
(13.31)
where x = {x1, . . . , xn}. these relations are most easily proved using d-separation.
for instance in the    rst of these results, we note that every path from any one of the
nodes x1, . . . , xn   1 to the node xn passes through the node zn, which is observed.
because all such paths are head-to-tail, it follows that the conditional independence
property must hold. the reader should take a few moments to verify each of these
properties in turn, as an exercise in the application of d-separation. these relations
can also be proved directly, though with signi   cantly greater effort, from the joint
distribution for the hidden markov model using the sum and product rules of proba-
bility.

p(xn +1|x, zn +1) = p(xn +1|zn +1)

p(zn +1|zn , x) = p(zn +1|zn )

let us begin by evaluating   (znk). recall that for a discrete multinomial ran-
dom variable the expected value of one of its components is just the id203 of
that component having the value 1. thus we are interested in    nding the posterior
distribution p(zn|x1, . . . , xn ) of zn given the observed data set x1, . . . , xn . this

exercise 13.10

620

13. sequential data

represents a vector of length k whose entries correspond to the expected values of
znk. using bayes    theorem, we have

  (zn) = p(zn|x) = p(x|zn)p(zn)

p(x)

.

(13.32)

note that the denominator p(x) is implicitly conditioned on the parameters   old
of the id48 and hence represents the likelihood function. using the conditional
independence property (13.24), together with the product rule of id203, we
obtain

  (zn) = p(x1, . . . , xn, zn)p(xn+1, . . . , xn|zn)

p(x)

=   (zn)  (zn)

p(x)

(13.33)

where we have de   ned

  (zn)     p(x1, . . . , xn, zn)
  (zn)     p(xn+1, . . . , xn|zn).

(13.34)
(13.35)
the quantity   (zn) represents the joint id203 of observing all of the given
data up to time n and the value of zn, whereas   (zn) represents the conditional
id203 of all future data from time n + 1 up to n given the value of zn. again,
  (zn) and   (zn) each represent set of k numbers, one for each of the possible
settings of the 1-of-k coded binary vector zn. we shall use the notation   (znk) to
denote the value of   (zn) when znk = 1, with an analogous interpretation of   (znk).
we now derive recursion relations that allow   (zn) and   (zn) to be evaluated
ef   ciently. again, we shall make use of conditional independence properties, in
particular (13.25) and (13.26), together with the sum and product rules, allowing us
to express   (zn) in terms of   (zn   1) as follows

  (zn) = p(x1, . . . , xn, zn)

= p(x1, . . . , xn|zn)p(zn)
= p(xn|zn)p(x1, . . . , xn   1|zn)p(zn)
= p(xn|zn)p(x1, . . . , xn   1, zn)
= p(xn|zn)

p(x1, . . . , xn   1, zn   1, zn)

= p(xn|zn)

= p(xn|zn)

= p(xn|zn)

p(x1, . . . , xn   1, zn|zn   1)p(zn   1)

p(x1, . . . , xn   1|zn   1)p(zn|zn   1)p(zn   1)

p(x1, . . . , xn   1, zn   1)p(zn|zn   1)

zn   1

(cid:2)
(cid:2)
(cid:2)
(cid:2)

zn   1

zn   1

zn   1

(cid:2)

zn   1

making use of the de   nition (13.34) for   (zn), we then obtain
  (zn   1)p(zn|zn   1).

  (zn) = p(xn|zn)

(13.36)

13.2. id48

621

figure 13.12 illustration of the forward recursion (13.36) for
evaluation of the    variables.
in this fragment
of the lattice, we see that the quantity   (zn1)
is obtained by taking the elements   (zn   1,j) of
  (zn   1) at step n   1 and summing them up with
weights given by aj1, corresponding to the val-
ues of p(zn|zn   1), and then multiplying by the
data contribution p(xn|zn1).

  (zn   1,1)

  (zn,1)

k = 1

a11

a21

p(xn|zn,1)

  (zn   1,2)

k = 2

  (zn   1,3)

k = 3

n     1

a31

n

it is worth taking a moment to study this recursion relation in some detail. note
that there are k terms in the summation, and the right-hand side has to be evaluated
for each of the k values of zn so each step of the    recursion has computational
cost that scaled like o(k 2). the forward recursion equation for   (zn) is illustrated
using a lattice diagram in figure 13.12.

in order to start this recursion, we need an initial condition that is given by

  (z1) = p(x1, z1) = p(z1)p(x1|z1) =

{  kp(x1|  k)}z1k

(13.37)

which tells us that   (z1k), for k = 1, . . . , k, takes the value   kp(x1|  k). starting
at the    rst node of the chain, we can then work along the chain and evaluate   (zn)
for every latent node. because each step of the recursion involves multiplying by a
k    k matrix, the overall cost of evaluating these quantities for the whole chain is
of o(k 2n).

we can similarly    nd a recursion relation for the quantities   (zn) by making

use of the conditional independence properties (13.27) and (13.28) giving

k(cid:14)

k=1

  (zn) = p(xn+1, . . . , xn|zn)

p(xn+1, . . . , xn , zn+1|zn)

p(xn+1, . . . , xn|zn, zn+1)p(zn+1|zn)

p(xn+1, . . . , xn|zn+1)p(zn+1|zn)

p(xn+2, . . . , xn|zn+1)p(xn+1|zn+1)p(zn+1|zn).

zn+1

(cid:2)
(cid:2)
(cid:2)
(cid:2)

zn+1

zn+1

zn+1

=

=

=

=

622

13. sequential data

figure 13.13 illustration of

the backward recursion
(13.38) for evaluation of the    variables. in
this fragment of the lattice, we see that the
quantity   (zn1) is obtained by taking the
components   (zn+1,k) of   (zn+1) at step
n + 1 and summing them up with weights
given by the products of a1k, correspond-
ing to the values of p(zn+1|zn) and the cor-
responding values of the emission density
p(xn|zn+1,k).

  (zn,1)

  (zn+1,1)

k = 1

a11

a12

p(xn|zn+1,1)

  (zn+1,2)

k = 2

a13

k = 3

n

p(xn|zn+1,2)

  (zn+1,3)

n + 1

p(xn|zn+1,3)

(cid:2)

making use of the de   nition (13.35) for   (zn), we then obtain

  (zn) =

  (zn+1)p(xn+1|zn+1)p(zn+1|zn).

(13.38)

zn+1

note that in this case we have a backward message passing algorithm that evaluates
  (zn) in terms of   (zn+1). at each step, we absorb the effect of observation xn+1
through the emission id203 p(xn+1|zn+1), multiply by the transition matrix
p(zn+1|zn), and then marginalize out zn+1. this is illustrated in figure 13.13.

again we need a starting condition for the recursion, namely a value for   (zn ).
this can be obtained by setting n = n in (13.33) and replacing   (zn ) with its
de   nition (13.34) to give

p(zn|x) = p(x, zn )  (zn )

p(x)

(13.39)

which we see will be correct provided we take   (zn ) = 1 for all settings of zn .

in the m step equations, the quantity p(x) will cancel out, as can be seen, for

instance, in the m-step equation for   k given by (13.20), which takes the form

n(cid:2)
n(cid:2)

n=1

n(cid:2)
n(cid:2)

n=1

  (znk)xn

  (znk)  (znk)xn

  k =

=

  (znk)

.

(13.40)

  (znk)  (znk)

n=1

n=1

however, the quantity p(x) represents the likelihood function whose value we typ-
ically wish to monitor during the em optimization, and so it is useful to be able to
evaluate it. if we sum both sides of (13.33) over zn, and use the fact that the left-hand
side is a normalized distribution, we obtain

p(x) =

  (zn)  (zn).

(13.41)

(cid:2)

zn

13.2. id48

623

thus we can evaluate the likelihood function by computing this sum, for any conve-
nient choice of n. for instance, if we only want to evaluate the likelihood function,
then we can do this by running the    recursion from the start to the end of the chain,
and then use this result for n = n, making use of the fact that   (zn ) is a vector of
1s. in this case no    recursion is required, and we simply have

(cid:2)

p(x) =

  (zn ).

(13.42)

zn

let us take a moment to interpret this result for p(x). recall that to compute the
likelihood we should take the joint distribution p(x, z) and sum over all possible
values of z. each such value represents a particular choice of hidden state for every
time step, in other words every term in the summation is a path through the lattice
diagram, and recall that there are exponentially many such paths. by expressing
the likelihood function in the form (13.42), we have reduced the computational cost
from being exponential in the length of the chain to being linear by swapping the
order of the summation and multiplications, so that at each time step n we sum
the contributions from all paths passing through each of the states znk to give the
intermediate quantities   (zn).
next we consider the evaluation of the quantities   (zn   1, zn), which correspond
to the values of the conditional probabilities p(zn   1, zn|x) for each of the k    k
settings for (zn   1, zn). using the de   nition of   (zn   1, zn), and applying bayes   
theorem, we have
  (zn   1, zn) = p(zn   1, zn|x)
= p(x|zn   1, zn)p(zn   1, zn)
= p(x1, . . . , xn   1|zn   1)p(xn|zn)p(xn+1, . . . , xn|zn)p(zn|zn   1)p(zn   1)
=   (zn   1)p(xn|zn)p(zn|zn   1)  (zn)

p(x)

p(x)

(13.43)

p(x)

where we have made use of the conditional independence property (13.29) together
with the de   nitions of   (zn) and   (zn) given by (13.34) and (13.35). thus we can
calculate the   (zn   1, zn) directly by using the results of the    and    recursions.

let us summarize the steps required to train a hidden markov model using
the em algorithm. we    rst make an initial selection of the parameters   old where
       (  , a,   ). the a and    parameters are often initialized either uniformly or
randomly from a uniform distribution (respecting their non-negativity and summa-
tion constraints). initialization of the parameters    will depend on the form of the
distribution. for instance in the case of gaussians, the parameters   k might be ini-
tialized by applying the id116 algorithm to the data, and   k might be initialized
to the covariance matrix of the corresponding id116 cluster. then we run both
the forward    recursion and the backward    recursion and use the results to evaluate
  (zn) and   (zn   1, zn). at this stage, we can also evaluate the likelihood function.

624

13. sequential data

this completes the e step, and we use the results to    nd a revised set of parameters
  new using the m-step equations from section 13.2.1. we then continue to alternate
between e and m steps until some convergence criterion is satis   ed, for instance
when the change in the likelihood function is below some threshold.
note that in these recursion relations the observations enter through conditional
distributions of the form p(xn|zn). the recursions are therefore independent of
the type or dimensionality of the observed variables or the form of this conditional
distribution, so long as its value can be computed for each of the k possible states
of zn. since the observed variables {xn} are    xed, the quantities p(xn|zn) can be
pre-computed as functions of zn at the start of the em algorithm, and remain    xed
throughout.

exercise 13.12

we have seen in earlier chapters that the maximum likelihood approach is most
effective when the number of data points is large in relation to the number of parame-
ters. here we note that a hidden markov model can be trained effectively, using max-
imum likelihood, provided the training sequence is suf   ciently long. alternatively,
we can make use of multiple shorter sequences, which requires a straightforward
modi   cation of the hidden markov model em algorithm. in the case of left-to-right
models, this is particularly important because, in a given observation sequence, a
given state transition corresponding to a nondiagonal element of a will seen at most
once.
another quantity of interest is the predictive distribution, in which the observed
data is x = {x1, . . . , xn} and we wish to predict xn +1, which would be important
for real-time applications such as    nancial forecasting. again we make use of the
sum and product rules together with the conditional independence properties (13.29)
and (13.31) giving

p(xn +1|x) =

=

=

=

=

=

zn+1

(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)

zn+1

zn+1

zn+1

zn+1
1

p(x)

p(xn +1, zn +1|x)

p(xn +1|zn +1)p(zn +1|x)

p(xn +1|zn +1)

p(xn +1|zn +1)

p(zn +1, zn|x)

p(zn +1|zn )p(zn|x)

(cid:2)
(cid:2)
(cid:2)

zn

zn

zn

p(xn +1|zn +1)

(cid:2)

p(zn +1|zn ) p(zn , x)
p(x)

(cid:2)

p(xn +1|zn +1)

p(zn +1|zn)  (zn ) (13.44)

zn+1

zn

which can be evaluated by    rst running a forward    recursion and then computing
the    nal summations over zn and zn +1. the result of the    rst summation over zn
can be stored and used once the value of xn +1 is observed in order to run the   
recursion forward to the next step in order to predict the subsequent value xn +2.

figure 13.14 a fragment of
the fac-
tor graph representation for the hidden
markov model.

  

z1

13.2. id48

625

zn   1

  n

zn

g1

gn   1

gn

x1

xn   1

xn

note that in (13.44), the in   uence of all data from x1 to xn is summarized in the k
values of   (zn ). thus the predictive distribution can be carried forward inde   nitely
using a    xed amount of storage, as may be required for real-time applications.

here we have discussed the estimation of the parameters of an id48 using max-
imum likelihood. this framework is easily extended to regularized maximum likeli-
hood by introducing priors over the model parameters   , a and    whose values are
then estimated by maximizing their posterior id203. this can again be done us-
ing the em algorithm in which the e step is the same as discussed above, and the m
step involves adding the log of the prior distribution p(  ) to the function q(  ,   old)
before maximization and represents a straightforward application of the techniques
developed at various points in this book. furthermore, we can use variational meth-
ods to give a fully bayesian treatment of the id48 in which we marginalize over the
parameter distributions (mackay, 1997). as with maximum likelihood, this leads to
a two-pass forward-backward recursion to compute posterior probabilities.

13.2.3 the sum-product algorithm for the id48
the directed graph that represents the hidden markov model, shown in fig-
ure 13.5, is a tree and so we can solve the problem of    nding local marginals for the
hidden variables using the sum-product algorithm. not surprisingly, this turns out to
be equivalent to the forward-backward algorithm considered in the previous section,
and so the sum-product algorithm therefore provides us with a simple way to derive
the alpha-beta recursion formulae.

we begin by transforming the directed graph of figure 13.5 into a factor graph,
of which a representative fragment is shown in figure 13.14. this form of the fac-
tor graph shows all variables, both latent and observed, explicitly. however, for
the purpose of solving the id136 problem, we shall always be conditioning on
the variables x1, . . . , xn , and so we can simplify the factor graph by absorbing the
emission probabilities into the transition id203 factors. this leads to the sim-
pli   ed factor graph representation in figure 13.15, in which the factors are given
by

h(z1) = p(z1)p(x1|z1)

fn(zn   1, zn) = p(zn|zn   1)p(xn|zn).

(13.45)
(13.46)

section 10.1

section 8.4.4

626

13. sequential data

figure 13.15 a simpli   ed form of fac-
tor graph to describe the hidden markov
model.

h

fn

z1

zn   1

zn

to derive the alpha-beta algorithm, we denote the    nal hidden variable zn as
the root node, and    rst pass messages from the leaf node h to the root. from the
general results (8.66) and (8.69) for message propagation, we see that the messages
which are propagated in the hidden markov model take the form

  zn   1   fn(zn   1) =   fn   1   zn   1(zn   1)

  fn   zn(zn) =

fn(zn   1, zn)  zn   1   fn(zn   1)

(13.47)

(13.48)

(cid:2)

zn   1

these equations represent the propagation of messages forward along the chain and
are equivalent to the alpha recursions derived in the previous section, as we shall
now show. note that because the variable nodes zn have only two neighbours, they
perform no computation.
sion for the f     z messages of the form

we can eliminate   zn   1   fn(zn   1) from (13.48) using (13.47) to give a recur-

(cid:2)

  fn   zn(zn) =

fn(zn   1, zn)  fn   1   zn   1(zn   1).

(13.49)

if we now recall the de   nition (13.46), and if we de   ne

zn   1

  (zn) =   fn   zn(zn)

(13.50)

then we obtain the alpha recursion given by (13.36). we also need to verify that
the quantities   (zn) are themselves equivalent to those de   ned previously. this
is easily done by using the initial condition (8.71) and noting that   (z1) is given
by h(z1) = p(z1)p(x1|z1) which is identical to (13.37). because the initial    is
the same, and because they are iteratively computed using the same equation, all
subsequent    quantities must be the same.

next we consider the messages that are propagated from the root node back to

the leaf node. these take the form

(cid:2)

  fn+1   fn(zn) =

fn+1(zn, zn+1)  fn+2   fn+1(zn+1)

(13.51)

zn+1

where, as before, we have eliminated the messages of the type z     f since the
variable nodes perform no computation. using the de   nition (13.46) to substitute
for fn+1(zn, zn+1), and de   ning

  (zn) =   fn+1   zn(zn)

(13.52)

13.2. id48

627

we obtain the beta recursion given by (13.38). again, we can verify that the beta
variables themselves are equivalent by noting that (8.70) implies that the initial mes-
sage send by the root variable node is   zn   fn (zn ) = 1, which is identical to the
initialization of   (zn ) given in section 13.2.2.

the sum-product algorithm also speci   es how to evaluate the marginals once all
the messages have been evaluated. in particular, the result (8.63) shows that the local
marginal at the node zn is given by the product of the incoming messages. because
we have conditioned on the variables x = {x1, . . . , xn}, we are computing the
joint distribution

p(zn, x) =   fn   zn(zn)  fn+1   zn(zn) =   (zn)  (zn).

(13.53)

dividing both sides by p(x), we then obtain
  (zn) = p(zn, x)
p(x)

=   (zn)  (zn)

p(x)

(13.54)

exercise 13.11

in agreement with (13.33). the result (13.43) can similarly be derived from (8.72).

13.2.4 scaling factors
there is an important issue that must be addressed before we can make use of the
forward backward algorithm in practice. from the recursion relation (13.36), we note
that at each step the new value   (zn) is obtained from the previous value   (zn   1)
by multiplying by quantities p(zn|zn   1) and p(xn|zn). because these probabilities
are often signi   cantly less than unity, as we work our way forward along the chain,
the values of   (zn) can go to zero exponentially quickly. for moderate lengths of
chain (say 100 or so), the calculation of the   (zn) will soon exceed the dynamic
range of the computer, even if double precision    oating point is used.

in the case of i.i.d. data, we implicitly circumvented this problem with the eval-
uation of likelihood functions by taking logarithms. unfortunately, this will not help
here because we are forming sums of products of small numbers (we are in fact im-
plicitly summing over all possible paths through the lattice diagram of figure 13.7).
we therefore work with re-scaled versions of   (zn) and   (zn) whose values remain
of order unity. as we shall see, the corresponding scaling factors cancel out when
we use these re-scaled quantities in the em algorithm.

in (13.34), we de   ned   (zn) = p(x1, . . . , xn, zn) representing the joint distri-
bution of all the observations up to xn and the latent variable zn. now we de   ne a
normalized version of    given by

(cid:1)  (zn) = p(zn|x1, . . . , xn) =

  (zn)

p(x1, . . . , xn)

(13.55)

which we expect to be well behaved numerically because it is a id203 distribu-
tion over k variables for any value of n. in order to relate the scaled and original al-
pha variables, we introduce scaling factors de   ned by conditional distributions over
the observed variables

cn = p(xn|x1, . . . , xn   1).

(13.56)

628

13. sequential data

from the product rule, we then have

n(cid:14)

p(x1, . . . , xn) =

cm

(13.57)

cm

m=1

m=1

and so

(cid:22)

(13.58)

n(cid:14)

(cid:2)

  (zn) = p(zn|x1, . . . , xn)p(x1, . . . , xn) =

cn(cid:1)  (zn) = p(xn|zn)

(cid:23)(cid:1)  (zn).
we can then turn the recursion equation (13.36) for    into one for(cid:1)   given by
(cid:1)  (zn   1)p(zn|zn   1).
note that at each stage of the forward message passing phase, used to evaluate(cid:1)  (zn),
that normalizes the right-hand side of (13.59) to give(cid:1)  (zn).
we can similarly de   ne re-scaled variables(cid:1)  (zn) using
(cid:23)(cid:1)  (zn)
tities(cid:1)  (zn) are simply the ratio of two conditional probabilities

which will again remain within machine precision because, from (13.35), the quan-

we have to evaluate and store cn, which is easily done because it is the coef   cient

n(cid:14)

  (zn) =

(13.59)

(13.60)

(cid:22)

m=n+1

zn   1

cm

p(xn+1, . . . , xn|zn)

p(xn+1, . . . , xn|x1, . . . , xn) .

(13.61)

the recursion result (13.38) for    then gives the following recursion for the re-scaled
variables

(cid:1)  (zn+1)p(xn+1|zn+1)p(zn+1|zn).

(13.62)

cn+1

(cid:1)  (zn) =
(cid:2)
(cid:1)  (zn) =

zn+1

in applying this recursion relation, we make use of the scaling factors cn that were
previously computed in the    phase.

from (13.57), we see that the likelihood function can be found using

n(cid:14)

p(x) =

cn.

n=1

(13.63)

exercise 13.15

similarly, using (13.33) and (13.43), together with (13.63), we see that the required
marginals are given by

  (zn) = (cid:1)  (zn)(cid:1)  (zn)

  (zn   1, zn) = cn(cid:1)  (zn   1)p(xn|zn)p(zn|z   1)(cid:1)  (zn).

(13.64)
(13.65)

section 13.3

13.2. id48

629

finally, we note that there is an alternative formulation of the forward-backward
algorithm (jordan, 2007) in which the backward pass is de   ned by a recursion based

the quantities   (zn) = (cid:1)  (zn)(cid:1)  (zn) instead of using(cid:1)  (zn). this         recursion
requires that the forward pass be completed    rst so that all the quantities (cid:1)  (zn)

are available for the backward pass, whereas the forward and backward passes of
the         algorithm can be done independently. although these two algorithms have
comparable computational cost, the         version is the most commonly encountered
one in the case of id48, whereas for linear dynamical systems a
recursion analogous to the         form is more usual.

13.2.5 the viterbi algorithm
in many applications of id48, the latent variables have some
meaningful interpretation, and so it is often of interest to    nd the most probable
sequence of hidden states for a given observation sequence. for instance in speech
recognition, we might wish to    nd the most probable phoneme sequence for a given
series of acoustic observations. because the graph for the hidden markov model is
a directed tree, this problem can be solved exactly using the max-sum algorithm.
we recall from our discussion in section 8.4.5 that the problem of    nding the most
probable sequence of latent states is not the same as that of    nding the set of states
that are individually the most probable. the latter problem can be solved by    rst
running the forward-backward (sum-product) algorithm to    nd the latent variable
marginals   (zn) and then maximizing each of these individually (duda et al., 2001).
however, the set of such states will not, in general, correspond to the most probable
sequence of states. in fact, this set of states might even represent a sequence having
zero id203, if it so happens that two successive states, which in isolation are
individually the most probable, are such that the transition matrix element connecting
them is zero.

in practice, we are usually interested in    nding the most probable sequence of
states, and this can be solved ef   ciently using the max-sum algorithm, which in the
context of id48 is known as the viterbi algorithm (viterbi, 1967).
note that the max-sum algorithm works with log probabilities and so there is no
need to use re-scaled variables as was done with the forward-backward algorithm.
figure 13.16 shows a fragment of the hidden markov model expanded as lattice
diagram. as we have already noted, the number of possible paths through the lattice
grows exponentially with the length of the chain. the viterbi algorithm searches this
space of paths ef   ciently to    nd the most probable path with a computational cost
that grows only linearly with the length of the chain.

as with the sum-product algorithm, we    rst represent the hidden markov model
as a factor graph, as shown in figure 13.15. again, we treat the variable node zn
as the root, and pass messages to the root starting with the leaf nodes. using the
results (8.93) and (8.94), we see that the messages passed in the max-sum algorithm
are given by

  zn   fn+1(zn) =   fn   zn(zn)

  fn+1   zn+1(zn+1) = max
zn

ln fn+1(zn, zn+1) +   zn   fn+1(zn)

(cid:27)

(13.66)
. (13.67)

(cid:26)

630

13. sequential data

figure 13.16 a fragment of
the id48 lattice
showing two possible paths. the viterbi algorithm
ef   ciently determines the most probable path from
amongst the exponentially many possibilities. for
any given path, the corresponding id203 is
given by the product of the elements of the tran-
sition matrix ajk, corresponding to the probabil-
ities p(zn+1|zn) for each segment of the path,
along with the emission densities p(xn|k) asso-
ciated with each node on the path.

k = 1

k = 2

k = 3

n     2

n     1

n

n + 1

if we eliminate   zn   fn+1(zn) between these two equations, and make use of (13.46),
we obtain a recursion for the f     z messages of the form

  (zn+1) = ln p(xn+1|zn+1) + max

zn

{ln p(x+1|zn) +   (zn)}

(13.68)

where we have introduced the notation   (zn)       fn   zn(zn).
from (8.95) and (8.96), these messages are initialized using

  (z1) = ln p(z1) + ln p(x1|z1).

(13.69)

where we have used (13.45). note that to keep the notation uncluttered, we omit
the dependence on the model parameters    that are held    xed when    nding the most
probable sequence.

exercise 13.16

the viterbi algorithm can also be derived directly from the de   nition (13.6) of
the joint distribution by taking the logarithm and then exchanging maximizations
and summations. it is easily seen that the quantities   (zn) have the probabilistic
interpretation

  (zn) = max

p(x1, . . . , xn, z1, . . . , zn).

z1,...,zn   1

(13.70)

once we have completed the    nal maximization over zn , we will obtain the
value of the joint distribution p(x, z) corresponding to the most probable path. we
also wish to    nd the sequence of latent variable values that corresponds to this path.
to do this, we simply make use of the back-tracking procedure discussed in sec-
tion 8.4.5. speci   cally, we note that the maximization over zn must be performed
for each of the k possible values of zn+1. suppose we keep a record of the values
of zn that correspond to the maxima for each value of the k values of zn+1. let us
denote this function by   (kn) where k     {1, . . . , k}. once we have passed mes-
sages to the end of the chain and found the most probable state of zn , we can then
use this function to backtrack along the chain by applying it recursively

kmax
n =   (kmax

n+1).

(13.71)

13.2. id48

631

intuitively, we can understand the viterbi algorithm as follows. naively, we
could consider explicitly all of the exponentially many paths through the lattice,
evaluate the id203 for each, and then select the path having the highest proba-
bility. however, we notice that we can make a dramatic saving in computational cost
as follows. suppose that for each path we evaluate its id203 by summing up
products of transition and emission probabilities as we work our way forward along
each path through the lattice. consider a particular time step n and a particular state
k at that time step. there will be many possible paths converging on the correspond-
ing node in the lattice diagram. however, we need only retain that particular path
that so far has the highest id203. because there are k states at time step n, we
need to keep track of k such paths. at time step n + 1, there will be k 2 possible
paths to consider, comprising k possible paths leading out of each of the k current
states, but again we need only retain k of these corresponding to the best path for
each state at time n+1. when we reach the    nal time step n we will discover which
state corresponds to the overall most probable path. because there is a unique path
coming into that state we can trace the path back to step n     1 to see what state it
occupied at that time, and so on back through the lattice to the state n = 1.

13.2.6 extensions of the hidden markov model
the basic hidden markov model, along with the standard training algorithm
based on maximum likelihood, has been extended in numerous ways to meet the
requirements of particular applications. here we discuss a few of the more important
examples.

we see from the digits example in figure 13.11 that id48 can
be quite poor generative models for the data, because many of the synthetic digits
look quite unrepresentative of the training data. if the goal is sequence classi   ca-
tion, there can be signi   cant bene   t in determining the parameters of hidden markov
models using discriminative rather than maximum likelihood techniques. suppose
we have a training set of r observation sequences xr, where r = 1, . . . , r, each of
which is labelled according to its class m, where m = 1, . . . , m. for each class, we
have a separate hidden markov model with its own parameters   m, and we treat the
problem of determining the parameter values as a standard classi   cation problem in
which we optimize the cross-id178

ln p(mr|xr).

(13.72)

using bayes    theorem this can be expressed in terms of the sequence probabilities
associated with the id48

r(cid:2)

r=1

(cid:24)

r(cid:2)

r=1

ln

(cid:5)m

p(xr|  r)p(mr)
l=1 p(xr|  l)p(lr)

(cid:25)

(13.73)

where p(m) is the prior id203 of class m. optimization of this cost function
is more complex than for maximum likelihood (kapadia, 1998), and in particular

632

13. sequential data

figure 13.17 section of an autoregressive hidden
markov model, in which the distribution
of the observation xn depends on a
subset of the previous observations as
well as on the hidden state zn.
in this
example, the distribution of xn depends
on the two previous observations xn   1
and xn   2.

zn   1

zn

zn+1

xn   1

xn

xn+1

requires that every training sequence be evaluated under each of the models in or-
der to compute the denominator in (13.73). id48, coupled with
discriminative training methods, are widely used in id103 (kapadia,
1998).

a signi   cant weakness of the hidden markov model is the way in which it rep-
resents the distribution of times for which the system remains in a given state. to see
the problem, note that the id203 that a sequence sampled from a given hidden
markov model will spend precisely t steps in state k and then make a transition to a
different state is given by

p(t ) = (akk)t (1     akk)     exp (   t ln akk)

(13.74)

and so is an exponentially decaying function of t . for many applications, this will
be a very unrealistic model of state duration. the problem can be resolved by mod-
elling state duration directly in which the diagonal coef   cients akk are all set to zero,
and each state k is explicitly associated with a id203 distribution p(t|k) of pos-
sible duration times. from a generative point of view, when a state k is entered, a
value t representing the number of time steps that the system will remain in state k
is then drawn from p(t|k). the model then emits t values of the observed variable
xt, which are generally assumed to be independent so that the corresponding emis-
t=1 p(xt|k). this approach requires some straightforward
sion density is simply
modi   cations to the em optimization procedure (rabiner, 1989).

(cid:21)t

another limitation of the standard id48 is that it is poor at capturing long-
range correlations between the observed variables (i.e., between variables that are
separated by many time steps) because these must be mediated via the    rst-order
markov chain of hidden states. longer-range effects could in principle be included
by adding extra links to the graphical model of figure 13.5. one way to address this
is to generalize the id48 to give the autoregressive hidden markov model (ephraim
et al., 1989), an example of which is shown in figure 13.17. for discrete observa-
tions, this corresponds to expanded tables of conditional probabilities for the emis-
sion distributions. in the case of a gaussian emission density, we can use the linear-
gaussian framework in which the conditional distribution for xn given the values
of the previous observations, and the value of zn, is a gaussian whose mean is a
linear combination of the values of the conditioning variables. clearly the number
of additional links in the graph must be limited to avoid an excessive the number of
free parameters. in the example shown in figure 13.17, each observation depends on

figure 13.18 example of an input-output hidden
markov model.
in this case, both the
emission probabilities and the transition
probabilities depend on the values of a
sequence of observations u1, . . . , un .

13.2. id48

633

un   1

un

un+1

zn   1

zn

zn+1

xn   1

xn

xn+1

the two preceding observed variables as well as on the hidden state. although this
graph looks messy, we can again appeal to d-separation to see that in fact it still has
a simple probabilistic structure. in particular, if we imagine conditioning on zn we
see that, as with the standard id48, the values of zn   1 and zn+1 are independent,
corresponding to the conditional independence property (13.5). this is easily veri-
   ed by noting that every path from node zn   1 to node zn+1 passes through at least
one observed node that is head-to-tail with respect to that path. as a consequence,
we can again use a forward-backward recursion in the e step of the em algorithm to
determine the posterior distributions of the latent variables in a computational time
that is linear in the length of the chain. similarly, the m step involves only a minor
modi   cation of the standard m-step equations. in the case of gaussian emission
densities this involves estimating the parameters using the standard id75
equations, discussed in chapter 3.

we have seen that the autoregressive id48 appears as a natural extension of the
standard id48 when viewed as a graphical model. in fact the probabilistic graphical
modelling viewpoint motivates a plethora of different graphical structures based on
the id48. another example is the input-output hidden markov model (bengio and
frasconi, 1995), in which we have a sequence of observed variables u1, . . . , un , in
addition to the output variables x1, . . . , xn , whose values in   uence either the dis-
tribution of latent variables or output variables, or both. an example is shown in
figure 13.18. this extends the id48 framework to the domain of supervised learn-
ing for sequential data. it is again easy to show, through the use of the d-separation
criterion, that the markov property (13.5) for the chain of latent variables still holds.
to verify this, simply note that there is only one path from node zn   1 to node zn+1
and this is head-to-tail with respect to the observed node zn. this conditional inde-
pendence property again allows the formulation of a computationally ef   cient learn-
ing algorithm. in particular, we can determine the parameters    of the model by
maximizing the likelihood function l(  ) = p(x|u,   ) where u is a matrix whose
rows are given by ut
n. as a consequence of the conditional independence property
(13.5) this likelihood function can be maximized ef   ciently using an em algorithm
in which the e step involves forward and backward recursions.

another variant of the id48 worthy of mention is the factorial hidden markov
model (ghahramani and jordan, 1997), in which there are multiple independent

exercise 13.18

634

13. sequential data

figure 13.19 a factorial hidden markov model com-
prising two markov chains of latent vari-
ables. for continuous observed variables
x, one possible choice of emission model
is a linear-gaussian density in which the
mean of the gaussian is a linear combi-
nation of the states of the corresponding
latent variables.

z(2)
n   1

z(2)
n

z(2)
n+1

z(1)
n   1

z(1)
n

z(1)
n+1

xn   1

xn

xn+1

markov chains of latent variables, and the distribution of the observed variable at
a given time step is conditional on the states of all of the corresponding latent vari-
ables at that same time step. figure 13.19 shows the corresponding graphical model.
the motivation for considering factorial id48 can be seen by noting that in order to
represent, say, 10 bits of information at a given time step, a standard id48 would
need k = 210 = 1024 latent states, whereas a factorial id48 could make use of 10
binary latent chains. the primary disadvantage of factorial id48s, however, lies in
the additional complexity of training them. the m step for the factorial id48 model
is straightforward. however, observation of the x variables introduces dependencies
between the latent chains, leading to dif   culties with the e step. this can be seen
by noting that in figure 13.19, the variables z(1)
n are connected by a path
which is head-to-head at node xn and hence they are not d-separated. the exact e
step for this model does not correspond to running forward and backward recursions
along the m markov chains independently. this is con   rmed by noting that the key
conditional independence property (13.5) is not satis   ed for the individual markov
chains in the factorial id48 model, as is shown using d-separation in figure 13.20.
now suppose that there are m chains of hidden nodes and for simplicity suppose
that all latent variables have the same number k of states. then one approach would
be to note that there are km combinations of latent variables at a given time step

n and z(2)

figure 13.20 example of a path, highlighted in green,
which is head-to-head at the observed
nodes xn   1 and xn+1, and head-to-tail
at the unobserved nodes z(2)
n and
z(2)
n+1. thus the path is not blocked and
so the conditional independence property
(13.5) does not hold for the individual la-
tent chains of the factorial id48 model.
as a consequence, there is no ef   cient
exact e step for this model.

n   1, z(2)

z(2)
n   1

z(2)
n

z(2)
n+1

z(1)
n   1

z(1)
n

z(1)
n+1

xn   1

xn

xn+1

section 10.1

13.3. linear dynamical systems

635

and so we can transform the model into an equivalent standard id48 having a single
chain of latent variables each of which has km latent states. we can then run the
standard forward-backward recursions in the e step. this has computational com-
plexity o(n k 2m ) that is exponential in the number m of latent chains and so will
be intractable for anything other than small values of m. one solution would be
to use sampling methods (discussed in chapter 11). as an elegant deterministic al-
ternative, ghahramani and jordan (1997) exploited variational id136 techniques
to obtain a tractable algorithm for approximate id136. this can be done using
a simple variational posterior distribution that is fully factorized with respect to the
latent variables, or alternatively by using a more powerful approach in which the
variational distribution is described by independent markov chains corresponding to
the chains of latent variables in the original model. in the latter case, the variational
id136 algorithms involves running independent forward and backward recursions
along each chain, which is computationally ef   cient and yet is also able to capture
correlations between variables within the same chain.

clearly, there are many possible probabilistic structures that can be constructed
according to the needs of particular applications. id114 provide a general
technique for motivating, describing, and analysing such structures, and variational
methods provide a powerful framework for performing id136 in those models for
which exact solution is intractable.

13.3. linear dynamical systems

in order to motivate the concept of linear dynamical systems, let us consider the
following simple problem, which often arises in practical settings. suppose we wish
to measure the value of an unknown quantity z using a noisy sensor that returns a
observation x representing the value of z plus zero-mean gaussian noise. given a
single measurement, our best guess for z is to assume that z = x. however, we
can improve our estimate for z by taking lots of measurements and averaging them,
because the random noise terms will tend to cancel each other. now let   s make the
situation more complicated by assuming that we wish to measure a quantity z that
is changing over time. we can take regular measurements of x so that at some point
in time we have obtained x1, . . . , xn and we wish to    nd the corresponding values
z1, . . . , xn . if we simply average the measurements, the error due to random noise
will be reduced, but unfortunately we will just obtain a single averaged estimate, in
which we have averaged over the changing value of z, thereby introducing a new
source of error.

intuitively, we could imagine doing a bit better as follows. to estimate the value
of zn , we take only the most recent few measurements, say xn   l, . . . , xn and just
average these. if z is changing slowly, and the random noise level in the sensor is
high, it would make sense to choose a relatively long window of observations to
average. conversely, if the signal is changing quickly, and the noise levels are small,
we might be better just to use xn directly as our estimate of zn . perhaps we could
do even better if we take a weighted average, in which more recent measurements

636

13. sequential data

make a greater contribution than less recent ones.

although this sort of intuitive argument seems plausible, it does not tell us how
to form a weighted average, and any sort of hand-crafted weighing is hardly likely
to be optimal. fortunately, we can address problems such as this much more sys-
tematically by de   ning a probabilistic model that captures the time evolution and
measurement processes and then applying the id136 and learning methods devel-
oped in earlier chapters. here we shall focus on a widely used model known as a
linear dynamical system.

as we have seen, the id48 corresponds to the state space model shown in
figure 13.5 in which the latent variables are discrete but with arbitrary emission
id203 distributions. this graph of course describes a much broader class of
id203 distributions, all of which factorize according to (13.6). we now consider
extensions to other distributions for the latent variables. in particular, we consider
continuous latent variables in which the summations of the sum-product algorithm
become integrals. the general form of the id136 algorithms will, however, be
the same as for the hidden markov model. it is interesting to note that, historically,
id48 and linear dynamical systems were developed independently.
once they are both expressed as id114, however, the deep relationship
between them immediately becomes apparent.

one key requirement is that we retain an ef   cient algorithm for id136 which
is linear in the length of the chain. this requires that, for instance, when we take
x1, . . . , xn, and multiply by the transition id203 p(zn|zn   1) and the emission
id203 p(xn|zn) and then marginalize over zn   1, we obtain a distribution over

a quantity(cid:1)  (zn   1), representing the posterior id203 of zn given observations
zn that is of the same functional form as that over (cid:1)  (zn   1). that is to say, the

distribution must not become more complex at each stage, but must only change in
its parameter values. not surprisingly, the only distributions that have this property
of being closed under multiplication are those belonging to the exponential family.
here we consider the most important example from a practical perspective,
which is the gaussian. in particular, we consider a linear-gaussian state space model
so that the latent variables {zn}, as well as the observed variables {xn}, are multi-
variate gaussian distributions whose means are linear functions of the states of their
parents in the graph. we have seen that a directed graph of linear-gaussian units
is equivalent to a joint gaussian distribution over all of the variables. furthermore,

marginals such as(cid:1)  (zn) are also gaussian, so that the functional form of the mes-
each of which has a mean that is linear in zn. then even if(cid:1)  (z1) is gaussian, the
quantity(cid:1)  (z2) will be a mixture of k gaussians, (cid:1)  (z3) will be a mixture of k 2

sages is preserved and we will obtain an ef   cient id136 algorithm. by contrast,
suppose that the emission densities p(xn|zn) comprise a mixture of k gaussians

gaussians, and so on, and exact id136 will not be of practical value.

we have seen that the hidden markov model can be viewed as an extension of
the mixture models of chapter 9 to allow for sequential correlations in the data.
in a similar way, we can view the linear dynamical system as a generalization of the
continuous latent variable models of chapter 12 such as probabilistic pca and factor
analysis. each pair of nodes {zn, xn} represents a linear-gaussian latent variable

13.3. linear dynamical systems

637

model for that particular observation. however, the latent variables {zn} are no
longer treated as independent but now form a markov chain.

because the model is represented by a tree-structured directed graph, id136
problems can be solved ef   ciently using the sum-product algorithm. the forward re-
cursions, analogous to the    messages of the hidden markov model, are known as the
kalman    lter equations (kalman, 1960; zarchan and musoff, 2005), and the back-
ward recursions, analogous to the    messages, are known as the kalman smoother
equations, or the rauch-tung-striebel (rts) equations (rauch et al., 1965). the
kalman    lter is widely used in many real-time tracking applications.

because the linear dynamical system is a linear-gaussian model, the joint distri-
bution over all variables, as well as all marginals and conditionals, will be gaussian.
it follows that the sequence of individually most probable latent variable values is
the same as the most probable latent sequence. there is thus no need to consider the
analogue of the viterbi algorithm for the linear dynamical system.

because the model has linear-gaussian conditional distributions, we can write

the transition and emission distributions in the general form
p(zn|zn   1) = n (zn|azn   1,   )
p(xn|zn) = n (xn|czn,   ).

the initial latent variable also has a gaussian distribution which we write as

p(z1) = n (z1|  0, v0).

(13.75)
(13.76)

(13.77)

exercise 13.19

exercise 13.24

note that in order to simplify the notation, we have omitted additive constant terms
from the means of the gaussians. in fact, it is straightforward to include them if
desired. traditionally, these distributions are more commonly expressed in an equiv-
alent form in terms of noisy linear equations given by

zn = azn   1 + wn
xn = czn + vn
z1 =   0 + u

(13.78)
(13.79)
(13.80)

where the noise terms have the distributions

w     n (w|0,   )
v     n (v|0,   )
u     n (u|0, v0).

(13.81)
(13.82)
(13.83)
the parameters of the model, denoted by    = {a,   , c,   ,   0, v0}, can be
determined using maximum likelihood through the em algorithm. in the e step, we
need to solve the id136 problem of determining the local posterior marginals for
the latent variables, which can be solved ef   ciently using the sum-product algorithm,
as we discuss in the next section.

638

13. sequential data

13.3.1 id136 in lds
we now turn to the problem of    nding the marginal distributions for the latent
variables conditional on the observation sequence. for given parameter settings, we
also wish to make predictions of the next latent state zn and of the next observation
xn conditioned on the observed data x1, . . . , xn   1 for use in real-time applications.
these id136 problems can be solved ef   ciently using the sum-product algorithm,
which in the context of the linear dynamical system gives rise to the kalman    lter
and kalman smoother equations.

it is worth emphasizing that because the linear dynamical system is a linear-
gaussian model, the joint distribution over all latent and observed variables is simply
a gaussian, and so in principle we could solve id136 problems by using the
standard results derived in previous chapters for the marginals and conditionals of a
multivariate gaussian. the role of the sum-product algorithm is to provide a more
ef   cient way to perform such computations.

linear dynamical systems have the identical factorization, given by (13.6), to
id48, and are again described by the factor graphs in figures 13.14
and 13.15. id136 algorithms therefore take precisely the same form except that
summations over latent variables are replaced by integrations. we begin by consid-
ering the forward equations in which we treat zn as the root node, and propagate
messages from the leaf node h(z1) to the root. from (13.77), the initial message will
be gaussian, and because each of the factors is gaussian, all subsequent messages
will also be gaussian. by convention, we shall propagate messages that are nor-
malized marginal distributions corresponding to p(zn|x1, . . . , xn), which we denote
by

this is precisely analogous to the propagation of scaled variables(cid:1)  (zn) given by

(cid:1)  (zn) = n (zn|  n, vn).

(13.84)

(13.59) in the discrete case of the hidden markov model, and so the recursion equa-
tion now takes the form

(cid:6) (cid:1)  (zn   1)p(zn|zn   1) dzn   1.

cn(cid:1)  (zn) = p(xn|zn)

(13.85)
substituting for the conditionals p(zn|zn   1) and p(xn|zn), using (13.75) and (13.76),
respectively, and making use of (13.84), we see that (13.85) becomes

(cid:6)
id98 (zn|  n, vn) = n (xn|czn,   )

n (zn|azn   1,   )n (zn   1|  n   1, vn   1) dzn   1.
(cid:6)

here we are supposing that   n   1 and vn   1 are known, and by evaluating the inte-
gral in (13.86), we wish to determine values for   n and vn. the integral is easily
evaluated by making use of the result (2.115), from which it follows that
n (zn|azn   1,   )n (zn   1|  n   1, vn   1) dzn   1

(13.86)

= n (zn|a  n   1, pn   1)

(13.87)

13.3. linear dynamical systems

639

where we have de   ned

pn   1 = avn   1at +   .

(13.88)

we can now combine this result with the    rst factor on the right-hand side of (13.86)
by making use of (2.115) and (2.116) to give

  n = a  n   1 + kn(xn     ca  n   1)
vn = (i     knc)pn   1
cn = n (xn|ca  n   1, cpn   1ct +   ).

(13.89)
(13.90)
(13.91)

here we have made use of the matrix inverse identities (c.5) and (c.7) and also
de   ned the kalman gain matrix

kn = pn   1ct

cpn   1ct +   

.

(13.92)

thus, given the values of   n   1 and vn   1, together with the new observation xn,
we can evaluate the gaussian marginal for zn having mean   n and covariance vn,
as well as the id172 coef   cient cn.

the initial conditions for these recursion equations are obtained from

c1(cid:1)  (z1) = p(z1)p(x1|z1).

(13.93)
because p(z1) is given by (13.77), and p(x1|z1) is given by (13.76), we can again
make use of (2.115) to calculate c1 and (2.116) to calculate   1 and v1 giving

(cid:11)   1

  1 =   0 + k1(x1     c  0)
v1 = (i     k1c)v0
c1 = n (x1|c  0, cv0ct +   )

(cid:11)   1

where

k1 = v0ct

cv0ct +   

.

(13.94)
(13.95)
(13.96)

(13.97)

(cid:10)

(cid:10)

similarly, the likelihood function for the linear dynamical system is given by (13.63)
in which the factors cn are found using the kalman    ltering equations.

we can interpret the steps involved in going from the posterior marginal over
zn   1 to the posterior marginal over zn as follows.
in (13.89), we can view the
quantity a  n   1 as the prediction of the mean over zn obtained by simply taking the
mean over zn   1 and projecting it forward one step using the transition id203
matrix a. this predicted mean would give a predicted observation for xn given by
cazn   1 obtained by applying the emission id203 matrix c to the predicted
hidden state mean. we can view the update equation (13.89) for the mean of the
hidden variable distribution as taking the predicted mean a  n   1 and then adding
a correction that is proportional to the error xn     cazn   1 between the predicted
observation and the actual observation. the coef   cient of this correction is given by
the kalman gain matrix. thus we can view the kalman    lter as a process of making
successive predictions and then correcting these predictions in the light of the new
observations. this is illustrated graphically in figure 13.21.

640

13. sequential data

zn   1

zn

zn

figure 13.21 the linear dynamical system can be viewed as a sequence of steps in which increasing un-
certainty in the state variable due to diffusion is compensated by the arrival of new data. in the left-hand plot,
the blue curve shows the distribution p(zn   1|x1, . . . , xn   1), which incorporates all the data up to step n     1.
the diffusion arising from the nonzero variance of the transition id203 p(zn|zn   1) gives the distribution
p(zn|x1, . . . , xn   1), shown in red in the centre plot. note that this is broader and shifted relative to the blue curve
(which is shown dashed in the centre plot for comparison). the next data observation xn contributes through the
emission density p(xn|zn), which is shown as a function of zn in green on the right-hand plot. note that this is not
a density with respect to zn and so is not normalized to one. inclusion of this new data point leads to a revised
distribution p(zn|x1, . . . , xn) for the state density shown in blue. we see that observation of the data has shifted
and narrowed the distribution compared to p(zn|x1, . . . , xn   1) (which is shown in dashed in the right-hand plot
for comparison).

exercise 13.27

exercise 13.28

if we consider a situation in which the measurement noise is small compared
to the rate at which the latent variable is evolving, then we    nd that the posterior
distribution for zn depends only on the current measurement xn, in accordance with
the intuition from our simple example at the start of the section. similarly, if the
latent variable is evolving slowly relative to the observation noise level, we    nd that
the posterior mean for zn is obtained by averaging all of the measurements obtained
up to that time.

one of the most important applications of the kalman    lter is to tracking, and
this is illustrated using a simple example of an object moving in two dimensions in
figure 13.22.

so far, we have solved the id136 problem of    nding the posterior marginal
for a node zn given observations from x1 up to xn. next we turn to the problem of
   nding the marginal for a node zn given all observations x1 to xn . for temporal
data, this corresponds to the inclusion of future as well as past observations. al-
though this cannot be used for real-time prediction, it plays a key role in learning the
parameters of the model. by analogy with the hidden markov model, this problem
can be solved by propagating messages from node xn back to node x1 and com-
bining this information with that obtained during the forward message passing stage

used to compute the(cid:1)  (zn).
of   (zn) =(cid:1)  (zn)(cid:1)  (zn) rather than in terms of(cid:1)  (zn). because   (zn) must also be

in the lds literature, it is usual to formulate this backward recursion in terms

gaussian, we write it in the form

  (zn) =(cid:1)  (zn)(cid:1)  (zn) = n (zn|(cid:1)  n,(cid:1)vn).

(13.98)

to derive the required recursion, we start from the backward recursion (13.62) for

13.3. linear dynamical systems

641

figure 13.22 an illustration of a linear dy-
namical system being used to
track a moving object. the blue
points indicate the true positions
of the object in a two-dimensional
space at successive time steps,
the green points denote noisy
measurements of the positions,
and the red crosses indicate the
means of the inferred posterior
distributions of the positions ob-
tained by running the kalman    l-
tering equations.
the covari-
the inferred positions
ances of
are indicated by the red ellipses,
which correspond to contours
having one standard deviation.

cn+1

(cid:1)  (zn) =

(cid:1)  (zn), which, for continuous latent variables, can be written in the form
(cid:6) (cid:1)  (zn+1)p(xn+1|zn+1)p(zn+1|zn) dzn+1.
we now multiply both sides of (13.99) by(cid:1)  (zn) and substitute for p(xn+1|zn+1)
(cid:1)  n =   n + jn
(cid:1)vn = vn + jn

and p(zn+1|zn) using (13.75) and (13.76). then we make use of (13.89), (13.90)
and (13.91), together with (13.98), and after some manipulation we obtain

(cid:10)(cid:1)  n+1     a  n
(cid:11)
(cid:18)
(cid:17)(cid:1)vn+1     pn

(13.100)

(13.99)

(13.101)

jt
n

where we have de   ned

   1

(13.102)
and we have made use of avn = pnjt
n. note that these recursions require that the
forward pass be completed    rst so that the quantities   n and vn will be available
for the backward pass.

jn = vnat (pn)

  (zn   1, zn) = (cn)

can be obtained from (13.65) in the form

for the em algorithm, we also require the pairwise posterior marginals, which

   1(cid:1)  (zn   1)p(xn|zn)p(zn|z   1)(cid:1)  (zn)

n (zn   1|  n   1, vn   1)n (zn|azn   1,   )n (xn|czn,   )n (zn|(cid:1)  n,(cid:1)vn)
substituting for(cid:1)  (zn) using (13.84) and rearranging, we see that   (zn   1, zn) is a

cn(cid:1)  (zn)

gaussian with mean given with components   (zn   1) and   (zn), and a covariance
between zn and zn   1 given by

(13.103)

=

.

cov[zn, zn   1] = jn   1

(13.104)

(cid:1)vn.

exercise 13.29

exercise 13.31

642

13. sequential data

13.3.2 learning in lds
so far, we have considered the id136 problem for linear dynamical systems,
assuming that the model parameters    = {a,   , c,   ,   0, v0} are known. next, we
consider the determination of these parameters using maximum likelihood (ghahra-
mani and hinton, 1996b). because the model has latent variables, this can be ad-
dressed using the em algorithm, which was discussed in general terms in chapter 9.
we can derive the em algorithm for the linear dynamical system as follows. let
us denote the estimated parameter values at some particular cycle of the algorithm
by   old. for these parameter values, we can run the id136 algorithm to determine
the posterior distribution of the latent variables p(z|x,   old), or more precisely those
in particular, we shall
local posterior marginals that are required in the m step.
require the following expectations

e [zn] = (cid:1)  n
(cid:9)
(cid:9)

(cid:1)vn +(cid:1)  n(cid:1)  t
= (cid:1)vn +(cid:1)  n(cid:1)  t

= jn   1

n   1
znzt
n

n

(cid:8)

(cid:8)

e

e

znzt

n   1

(13.105)
(13.106)

(13.107)

where we have used (13.104).

now we consider the complete-data log likelihood function, which is obtained

by taking the logarithm of (13.6) and is therefore given by

n(cid:2)

ln p(x, z|  ) = ln p(z1|  0, v0) +

ln p(zn|zn   1, a,   )

n(cid:2)

+

n=2

ln p(xn|zn, c,   )

(13.108)

n=1

in which we have made the dependence on the parameters explicit. we now take the
expectation of the complete-data log likelihood with respect to the posterior distri-
bution p(z|x,   old) which de   nes the function

q(  ,   old) = ez|  old [ln p(x, z|  )] .

(13.109)

in the m step, this function is maximized with respect to the components of   .

consider    rst the parameters   0 and v0. if we substitute for p(z1|  0, v0) in
(13.108) using (13.77), and then take the expectation with respect to z, we obtain

(cid:29)

(cid:30)
0 (z1       0)
   1

+ const

q(  ,   old) =    1
2

ln|v0|     ez|  old

(z1       0)tv

1
2

where all terms not dependent on   0 or v0 have been absorbed into the additive
constant. maximization with respect to   0 and v0 is easily performed by making
use of the maximum likelihood solution for a gaussian distribution discussed in
section 2.3.4, giving

exercise 13.32

13.3. linear dynamical systems

643

(13.110)
(13.111)
similarly, to optimize a and   , we substitute for p(zn|zn   1, a,   ) in (13.108)

1 ]     e[z1]e[zt
1 ].

= e[z1]
= e[z1zt

  new
vnew

0

0

using (13.75) giving

(cid:31)

q(  ,   old) =     n     1
   ez|  old

n(cid:2)

2

1
2

n=2

ln|  |

 

(zn     azn   1)t  

   1(zn     azn   1)

+ const

(13.112)

in which the constant comprises terms that are independent of a and   . maximizing
with respect to these parameters then gives

anew =

e

znzt

n   1

zn   1zt

n   1

(cid:22)
n(cid:2)

(cid:8)
n(cid:2)

(cid:8)

(cid:26)

e

(cid:9)(cid:23)(cid:22)
n(cid:2)
(cid:8)
(cid:9)     anew
(cid:8)

n=2

e

znzt
n

n=2
anew + anew

(cid:9)(cid:23)   1
(cid:9)
(cid:20)

(cid:8)

(cid:9)

e

zn   1zt
n

e

zn   1zt

n   1

(anew)t

(13.113)

.

(13.114)

  new =

n=2

(cid:8)

(cid:9)

1

n     1
znzt

n   1

   e

note that anew must be evaluated    rst, and the result can then be used to determine
  new.
p(xn|zn, c,   ) in (13.108) using (13.76) giving

finally, in order to determine the new values of c and   , we substitute for

maximizing with respect to c and    then gives

q(  ,   old) =     n
2

(xn     czn)t  

   1(xn     czn)

+ const.

 

ln|  |

(cid:31)

1
2

   ez|  old
(cid:22)
n(cid:2)
n(cid:2)

n=1

1
n
   xne

n=1

(cid:26)
(cid:8)

n(cid:2)

n=1

(cid:8)

(cid:9)

cnew =

xne

zt
n

  new =

n     cnew

xnxt

e [zn] xt
n

(cid:9)(cid:23)(cid:22)
n(cid:2)

e

n=1

(cid:8)

znzt
n

(cid:9)(cid:23)   1
(cid:9)

(cid:8)

zt
n

cnew + cnew

e

znzt
n

cnew

(13.115)

(cid:27)

.

(13.116)

exercise 13.33

exercise 13.34

644

13. sequential data

we have approached parameter learning in the linear dynamical system using
maximum likelihood. inclusion of priors to give a map estimate is straightforward,
and a fully bayesian treatment can be found by applying the analytical approxima-
tion techniques discussed in chapter 10, though a detailed treatment is precluded
here due to lack of space.

13.3.3 extensions of lds
as with the hidden markov model, there is considerable interest in extending
the basic linear dynamical system in order to increase its capabilities. although the
assumption of a linear-gaussian model leads to ef   cient algorithms for id136
and learning, it also implies that the marginal distribution of the observed variables
is simply a gaussian, which represents a signi   cant limitation. one simple extension
of the linear dynamical system is to use a gaussian mixture as the initial distribution
for z1.
if this mixture has k components, then the forward recursion equations
(13.85) will lead to a mixture of k gaussians over each hidden variable zn, and so
the model is again tractable.

for many applications, the gaussian emission density is a poor approximation.
if instead we try to use a mixture of k gaussians as the emission density, then the

posterior(cid:1)  (z1) will also be a mixture of k gaussians. however, from (13.85) the
posterior(cid:1)  (z2) will comprise a mixture of k 2 gaussians, and so on, with(cid:1)  (zn)

being given by a mixture of k n gaussians. thus the number of components grows
exponentially with the length of the chain, and so this model is impractical.

chapter 10

more generally, introducing transition or emission models that depart from the
linear-gaussian (or other exponential family) model leads to an intractable infer-
ence problem. we can make deterministic approximations such as assumed den-
sity    ltering or expectation propagation, or we can make use of sampling methods,
as discussed in section 13.3.4. one widely used approach is to make a gaussian
approximation by linearizing around the mean of the predicted distribution, which
gives rise to the extended kalman    lter (zarchan and musoff, 2005).

as with id48, we can develop interesting extensions of the ba-
sic linear dynamical system by expanding its graphical representation. for example,
the switching state space model (ghahramani and hinton, 1998) can be viewed as
a combination of the hidden markov model with a set of linear dynamical systems.
the model has multiple markov chains of continuous linear-gaussian latent vari-
ables, each of which is analogous to the latent chain of the linear dynamical system
discussed earlier, together with a markov chain of discrete variables of the form used
in a hidden markov model. the output at each time step is determined by stochas-
tically choosing one of the continuous latent chains, using the state of the discrete
latent variable as a switch, and then emitting an observation from the corresponding
conditional output distribution. exact id136 in this model is intractable, but vari-
ational methods lead to an ef   cient id136 scheme involving forward-backward
recursions along each of the continuous and discrete markov chains independently.
note that, if we consider multiple chains of discrete latent variables, and use one as
the switch to select from the remainder, we obtain an analogous model having only
discrete latent variables known as the switching hidden markov model.

13.3. linear dynamical systems

645

chapter 11

13.3.4 particle    lters
for dynamical systems which do not have a linear-gaussian, for example, if
they use a non-gaussian emission density, we can turn to sampling methods in order
to    nd a tractable id136 algorithm. in particular, we can apply the sampling-
importance-resampling formalism of section 11.1.5 to obtain a sequential monte
carlo algorithm known as the particle    lter.

consider the class of distributions represented by the graphical model in fig-
ure 13.5, and suppose we are given the observed values xn = (x1, . . . , xn) and
we wish to draw l samples from the posterior distribution p(zn|xn). using bayes   
theorem, we have

e[f(zn)] =

f(zn)p(zn|xn) dzn
f(zn)p(zn|xn, xn   1) dzn
f(zn)p(xn|zn)p(zn|xn   1) dzn

p(xn|zn)p(zn|xn   1) dzn

=

(cid:6)
(cid:6)
(cid:6)
(cid:6)
(cid:7) l(cid:2)

=

l=1

w(l)

n f(z(l)
n )

(13.117)

where {z(l)
n } is a set of samples drawn from p(zn|xn   1) and we have made use of
the conditional independence property p(xn|zn, xn   1) = p(xn|zn), which follows
from the graph in figure 13.5. the sampling weights {w

n } are de   ned by

(l)

(cid:5)l

p(xn|z(l)
n )
m=1 p(xn|z(m)
n )

w(l)

n =

(13.118)

where the same samples are used in the numerator as in the denominator. thus the
(cid:5)
posterior distribution p(zn|xn) is represented by the set of samples {z(l)
n } together
with the corresponding weights {w
(l)
n 1
and

n }. note that these weights satisfy 0 (cid:1) w

(l)
n = 1.

(l)

because we wish to    nd a sequential sampling scheme, we shall suppose that
a set of samples and weights have been obtained at time step n, and that we have
subsequently observed the value of xn+1, and we wish to    nd the weights and sam-
ples at time step n + 1. we    rst sample from the distribution p(zn+1|xn). this is

l w

646

13. sequential data

straightforward since, again using bayes    theorem

(cid:6)
(cid:6)
(cid:6)
(cid:6)
(cid:2)

p(zn+1|xn) =

=

=

=

=

p(zn+1|zn, xn)p(zn|xn) dzn
p(zn+1|zn)p(zn|xn) dzn
p(zn+1|zn)p(zn|xn, xn   1) dzn
p(zn+1|zn)p(xn|zn)p(zn|xn   1) dzn

(cid:6)

p(xn|zn)p(zn|xn   1) dzn

n p(zn+1|z(l)
w(l)
n )

where we have made use of the conditional independence properties

l

p(zn+1|zn, xn) = p(zn+1|zn)
p(xn|zn, xn   1) = p(xn|zn)

(13.119)

(13.120)
(13.121)

which follow from the application of the d-separation criterion to the graph in fig-
ure 13.5. the distribution given by (13.119) is a mixture distribution, and samples
can be drawn by choosing a component l with id203 given by the mixing coef-
   cients w(l) and then drawing a sample from the corresponding component.

in summary, we can view each step of the particle    lter algorithm as comprising
two stages. at time step n, we have a sample representation of the posterior dis-
tribution p(zn|xn) expressed as samples {z(l)
n }.
this can be viewed as a mixture representation of the form (13.119). to obtain the
corresponding representation for the next time step, we    rst draw l samples from
the mixture distribution (13.119), and then for each sample we use the new obser-
n+1). this is
vation xn+1 to evaluate the corresponding weights w
illustrated, for the case of a single variable z, in figure 13.23.

n } with corresponding weights {w

n+1     p(xn+1|z(l)

(l)

(l)

the particle    ltering, or sequential monte carlo, approach has appeared in the
literature under various names including the bootstrap    lter (gordon et al., 1993),
survival of the    ttest (kanazawa et al., 1995), and the condensation algorithm (isard
and blake, 1998).

exercises

13.1 ((cid:12)) www use the technique of d-separation, discussed in section 8.2, to verify
that the markov model shown in figure 13.3 having n nodes in total satis   es the
conditional independence properties (13.3) for n = 2, . . . , n. similarly, show that
a model described by the graph in figure 13.4 in which there are n nodes in total

p(zn|xn)

p(zn+1|xn)

p(xn+1|zn+1)

p(zn+1|xn+1)

exercises

647

z

figure 13.23 schematic illustration of the operation of the particle    lter for a one-dimensional latent
space. at time step n, the posterior p(zn|xn) is represented as a mixture distribution,
shown schematically as circles whose sizes are proportional to the weights w(l)
n . a set of
l samples is then drawn from this distribution and the new weights w(l)
n+1 evaluated using
p(xn+1|z(l)

n+1).

satis   es the conditional independence properties

p(xn|x1, . . . , xn   1) = p(xn|xn   1, xn   2)

(13.122)

for n = 3, . . . , n.

13.2 ((cid:12) (cid:12)) consider the joint id203 distribution (13.2) corresponding to the directed
graph of figure 13.3. using the sum and product rules of id203, verify that
this joint distribution satis   es the conditional independence property (13.3) for n =
2, . . . , n. similarly, show that the second-order markov model described by the
joint distribution (13.4) satis   es the conditional independence property

p(xn|x1, . . . , xn   1) = p(xn|xn   1, xn   2)

(13.123)

for n = 3, . . . , n.

13.3 ((cid:12)) by using d-separation, show that the distribution p(x1, . . . , xn ) of the observed
data for the state space model represented by the directed graph in figure 13.5 does
not satisfy any conditional independence properties and hence does not exhibit the
markov property at any    nite order.

13.4 ((cid:12) (cid:12)) www consider a hidden markov model in which the emission densities are
represented by a parametric model p(x|z, w), such as a id75 model or
a neural network, in which w is a vector of adaptive parameters. describe how the
parameters w can be learned from data using maximum likelihood.

648

13. sequential data

13.5 ((cid:12) (cid:12)) verify the m-step equations (13.18) and (13.19) for the initial state probabili-
ties and transition id203 parameters of the hidden markov model by maximiza-
tion of the expected complete-data log likelihood function (13.17), using appropriate
lagrange multipliers to enforce the summation constraints on the components of   
and a.

13.6 ((cid:12))

show that if any elements of the parameters    or a for a hidden markov
model are initially set to zero, then those elements will remain zero in all subsequent
updates of the em algorithm.

13.7 ((cid:12)) consider a hidden markov model with gaussian emission densities. show that
maximization of the function q(  ,   old) with respect to the mean and covariance
parameters of the gaussians gives rise to the m-step equations (13.20) and (13.21).

13.8 ((cid:12) (cid:12)) www for a hidden markov model having discrete observations governed by
a multinomial distribution, show that the conditional distribution of the observations
given the hidden variables is given by (13.22) and the corresponding m step equa-
tions are given by (13.23). write down the analogous equations for the conditional
distribution and the m step equations for the case of a hidden markov with multiple
binary output variables each of which is governed by a bernoulli conditional dis-
tribution. hint: refer to sections 2.1 and 2.2 for a discussion of the corresponding
maximum likelihood solutions for i.i.d. data if required.

13.9 ((cid:12) (cid:12)) www use the d-separation criterion to verify that the conditional indepen-
dence properties (13.24)   (13.31) are satis   ed by the joint distribution for the hidden
markov model de   ned by (13.6).

13.10 ((cid:12) (cid:12) (cid:12)) by applying the sum and product rules of id203, verify that the condi-
tional independence properties (13.24)   (13.31) are satis   ed by the joint distribution
for the hidden markov model de   ned by (13.6).

13.11 ((cid:12) (cid:12)) starting from the expression (8.72) for the marginal distribution over the vari-
ables of a factor in a factor graph, together with the results for the messages in the
sum-product algorithm obtained in section 13.2.3, derive the result (13.43) for the
joint posterior distribution over two successive latent variables in a hidden markov
model.

13.12 ((cid:12) (cid:12)) suppose we wish to train a hidden markov model by maximum likelihood
using data that comprises r independent sequences of observations, which we de-
note by x(r) where r = 1, . . . , r. show that in the e step of the em algorithm,
we simply evaluate posterior probabilities for the latent variables by running the   
and    recursions independently for each of the sequences. also show that in the
m step, the initial id203 and transition id203 parameters are re-estimated

using modi   ed forms of (13.18 ) and (13.19) given by

(r)
1k )

r=1

  (z

r(cid:2)
k(cid:2)
r(cid:2)
n(cid:2)
r(cid:2)
r(cid:2)
k(cid:2)
n(cid:2)

  (z

n=2

r=1

r=1

j=1

r=1

l=1

n=2

(r)
1j )

  (z

(r)
n   1,j, z

(r)
n,k)

  (z

(r)
n   1,j, z

(r)
n,l)

  k =

ajk =

exercises

649

(13.124)

(13.125)

where, for notational convenience, we have assumed that the sequences are of the
same length (the generalization to sequences of different lengths is straightforward).
similarly, show that the m-step equation for re-estimation of the means of gaussian
emission models is given by

r(cid:2)
n(cid:2)
n(cid:2)
r(cid:2)

n=1

r=1

  k =

  (z

(r)

nk )x(r)

n

.

  (z

(r)
nk )

(13.126)

r=1

n=1

note that the m-step equations for other emission model parameters and distributions
take an analogous form.

13.13 ((cid:12) (cid:12)) www use the de   nition (8.64) of the messages passed from a factor node
to a variable node in a factor graph, together with the expression (13.6) for the joint
distribution in a hidden markov model, to show that the de   nition (13.50) of the
alpha message is the same as the de   nition (13.34).

13.14 ((cid:12) (cid:12)) use the de   nition (8.67) of the messages passed from a factor node to a
variable node in a factor graph, together with the expression (13.6) for the joint
distribution in a hidden markov model, to show that the de   nition (13.52) of the
beta message is the same as the de   nition (13.35).

13.15 ((cid:12) (cid:12)) use the expressions (13.33) and (13.43) for the marginals in a hidden markov
model to derive the corresponding results (13.64) and (13.65) expressed in terms of
re-scaled variables.

13.16 ((cid:12) (cid:12) (cid:12))

in this exercise, we derive the forward message passing equation for the
viterbi algorithm directly from the expression (13.6) for the joint distribution. this
involves maximizing over all of the hidden variables z1, . . . , zn . by taking the log-
arithm and then exchanging maximizations and summations, derive the recursion

650

13. sequential data

(13.68) where the quantities   (zn) are de   ned by (13.70). show that the initial
condition for this recursion is given by (13.69).

13.17 ((cid:12)) www show that the directed graph for the input-output hidden markov model,
given in figure 13.18, can be expressed as a tree-structured factor graph of the form
shown in figure 13.15 and write down expressions for the initial factor h(z1) and
for the general factor fn(zn   1, zn) where 2 (cid:1) n (cid:1) n.

13.18 ((cid:12) (cid:12) (cid:12)) using the result of exercise 13.17, derive the recursion equations, includ-
ing the initial conditions, for the forward-backward algorithm for the input-output
hidden markov model shown in figure 13.18.

13.19 ((cid:12)) www the kalman    lter and smoother equations allow the posterior distribu-
tions over individual latent variables, conditioned on all of the observed variables,
to be found ef   ciently for linear dynamical systems. show that the sequence of
latent variable values obtained by maximizing each of these posterior distributions
individually is the same as the most probable sequence of latent values. to do this,
simply note that the joint distribution of all latent and observed variables in a linear
dynamical system is gaussian, and hence all conditionals and marginals will also be
gaussian, and then make use of the result (2.98).

13.20 ((cid:12) (cid:12)) www use the result (2.115) to prove (13.87).

13.21 ((cid:12) (cid:12)) use the results (2.115) and (2.116), together with the matrix identities (c.5)
and (c.7), to derive the results (13.89), (13.90), and (13.91), where the kalman gain
matrix kn is de   ned by (13.92).

13.22 ((cid:12) (cid:12)) www using (13.93), together with the de   nitions (13.76) and (13.77) and

the result (2.115), derive (13.96).

13.23 ((cid:12) (cid:12)) using (13.93), together with the de   nitions (13.76) and (13.77) and the result

(2.116), derive (13.94), (13.95) and (13.97).

13.24 ((cid:12) (cid:12)) www consider a generalization of (13.75) and (13.76) in which we include

constant terms a and c in the gaussian means, so that

p(zn|zn   1) = n (zn|azn   1 + a,   )
p(xn|zn) = n (xn|czn + c,   ).

(13.127)
(13.128)

show that this extension can be re-case in the framework discussed in this chapter by
de   ning a state vector z with an additional component    xed at unity, and then aug-
menting the matrices a and c using extra columns corresponding to the parameters
a and c.

13.25 ((cid:12) (cid:12))

in this exercise, we show that when the kalman    lter equations are applied
to independent observations, they reduce to the results given in section 2.3 for the
maximum likelihood solution for a single gaussian distribution. consider the prob-
lem of    nding the mean    of a single gaussian random variable x, in which we are
given a set of independent observations {x1, . . . , xn}. to model this we can use

exercises

651

a linear dynamical system governed by (13.75) and (13.76), with latent variables
{z1, . . . , zn} in which c becomes the identity matrix and where the transition prob-
ability a = 0 because the observations are independent. let the parameters m0
and v0 of the initial state be denoted by   0 and   2
0, respectively, and suppose that
   becomes   2. write down the corresponding kalman    lter equations starting from
the general results (13.89) and (13.90), together with (13.94) and (13.95). show that
these are equivalent to the results (2.141) and (2.142) obtained directly by consider-
ing independent data.

13.26 ((cid:12) (cid:12) (cid:12)) consider a special case of the linear dynamical system of section 13.3 that is
equivalent to probabilistic pca, so that the transition matrix a = 0, the covariance
   = i, and the noise covariance    =   2i. by making use of the matrix inversion
identity (c.7) show that, if the emission density matrix c is denoted w, then the
posterior distribution over the hidden states de   ned by (13.89) and (13.90) reduces
to the result (12.42) for probabilistic pca.

13.27 ((cid:12)) www consider a linear dynamical system of the form discussed in sec-
tion 13.3 in which the amplitude of the observation noise goes to zero, so that    = 0.
show that the posterior distribution for zn has mean xn and zero variance. this
accords with our intuition that if there is no noise, we should just use the current
observation xn to estimate the state variable zn and ignore all previous observations.

13.28 ((cid:12) (cid:12) (cid:12)) consider a special case of the linear dynamical system of section 13.3 in
which the state variable zn is constrained to be equal to the previous state variable,
which corresponds to a = i and    = 0. for simplicity, assume also that v0        
so that the initial conditions for z are unimportant, and the predictions are determined
purely by the data. use proof by induction to show that the posterior mean for state
zn is determined by the average of x1, . . . , xn. this corresponds to the intuitive
result that if the state variable is constant, our best estimate is obtained by averaging
the observations.

13.29 ((cid:12) (cid:12) (cid:12))

starting from the backwards recursion equation (13.99), derive the rts
smoothing equations (13.100) and (13.101) for the gaussian linear dynamical sys-
tem.

13.30 ((cid:12) (cid:12)) starting from the result (13.65) for the pairwise posterior marginal in a state
space model, derive the speci   c form (13.103) for the case of the gaussian linear
dynamical system.

13.31 ((cid:12) (cid:12)) starting from the result (13.103) and by substituting for(cid:1)  (zn) using (13.84),

verify the result (13.104) for the covariance between zn and zn   1.

13.32 ((cid:12) (cid:12)) www verify the results (13.110) and (13.111) for the m-step equations for

  0 and v0 in the linear dynamical system.

13.33 ((cid:12) (cid:12)) verify the results (13.113) and (13.114) for the m-step equations for a and   

in the linear dynamical system.

652

13. sequential data

13.34 ((cid:12) (cid:12)) verify the results (13.115) and (13.116) for the m-step equations for c and   

in the linear dynamical system.

14

combining

models

in earlier chapters, we have explored a range of different models for solving classi   -
cation and regression problems. it is often found that improved performance can be
obtained by combining multiple models together in some way, instead of just using
a single model in isolation. for instance, we might train l different models and then
make predictions using the average of the predictions made by each model. such
combinations of models are sometimes called committees. in section 14.2, we dis-
cuss ways to apply the committee concept in practice, and we also give some insight
into why it can sometimes be an effective procedure.

one important variant of the committee method, known as boosting, involves
training multiple models in sequence in which the error function used to train a par-
ticular model depends on the performance of the previous models. this can produce
substantial improvements in performance compared to the use of a single model and
is discussed in section 14.3.

instead of averaging the predictions of a set of models, an alternative form of

653

654

14. combining models

model combination is to select one of the models to make the prediction, in which
the choice of model is a function of the input variables. thus different models be-
come responsible for making predictions in different regions of input space. one
widely used framework of this kind is known as a decision tree in which the selec-
tion process can be described as a sequence of binary selections corresponding to
the traversal of a tree structure and is discussed in section 14.4. in this case, the
individual models are generally chosen to be very simple, and the overall    exibility
of the model arises from the input-dependent selection process. id90 can
be applied to both classi   cation and regression problems.

one limitation of id90 is that the division of input space is based on
hard splits in which only one model is responsible for making predictions for any
given value of the input variables. the decision process can be softened by moving
to a probabilistic framework for combining models, as discussed in section 14.5. for
example, if we have a set of k models for a conditional distribution p(t|x, k) where
x is the input variable, t is the target variable, and k = 1, . . . , k indexes the model,
then we can form a probabilistic mixture of the form

k(cid:2)

p(t|x) =

  k(x)p(t|x, k)

(14.1)

k=1

in which   k(x) = p(k|x) represent the input-dependent mixing coef   cients. such
models can be viewed as mixture distributions in which the component densities, as
well as the mixing coef   cients, are conditioned on the input variables and are known
as mixtures of experts. they are closely related to the mixture density network model
discussed in section 5.6.

14.1. bayesian model averaging

section 9.2

it is important to distinguish between model combination methods and bayesian
model averaging, as the two are often confused. to understand the difference, con-
sider the example of density estimation using a mixture of gaussians in which several
gaussian components are combined probabilistically. the model contains a binary
latent variable z that indicates which component of the mixture is responsible for
generating the corresponding data point. thus the model is speci   ed in terms of a
joint distribution

(14.2)
and the corresponding density over the observed variable x is obtained by marginal-
izing over the latent variable

p(x, z)

p(x) =

p(x, z).

(14.3)

(cid:2)

z

14.2. committees

655

in the case of our gaussian mixture example, this leads to a distribution of the form

p(x) =

  kn (x|  k,   k)

(14.4)

with the usual interpretation of the symbols. this is an example of model combi-
 
nation. for independent, identically distributed data, we can use (14.3) to write the
marginal id203 of a data set x = {x1, . . . , xn} in the form

(cid:31)(cid:2)

n(cid:14)

n(cid:14)

p(x) =

p(xn) =

p(xn, zn)

.

(14.5)

n=1

n=1

zn

thus we see that each observed data point xn has a corresponding latent variable zn.
now suppose we have several different models indexed by h = 1, . . . , h with
prior probabilities p(h). for instance one model might be a mixture of gaussians and
another model might be a mixture of cauchy distributions. the marginal distribution
over the data set is given by

k(cid:2)

k=1

h(cid:2)

p(x) =

p(x|h)p(h).

(14.6)

h=1

this is an example of bayesian model averaging. the interpretation of this summa-
tion over h is that just one model is responsible for generating the whole data set,
and the id203 distribution over h simply re   ects our uncertainty as to which
model that is. as the size of the data set increases, this uncertainty reduces, and
the posterior probabilities p(h|x) become increasingly focussed on just one of the
models.

this highlights the key difference between bayesian model averaging and model
combination, because in bayesian model averaging the whole data set is generated
by a single model. by contrast, when we combine multiple models, as in (14.5), we
see that different data points within the data set can potentially be generated from
different values of the latent variable z and hence by different components.
although we have considered the marginal id203 p(x), the same consid-
erations apply for the predictive density p(x|x) or for conditional distributions such
as p(t|x, x, t).

exercise 14.1

14.2. committees

section 3.2

the simplest way to construct a committee is to average the predictions of a set of
individual models. such a procedure can be motivated from a frequentist perspective
by considering the trade-off between bias and variance, which decomposes the er-
ror due to a model into the bias component that arises from differences between the
model and the true function to be predicted, and the variance component that repre-
sents the sensitivity of the model to the individual data points. recall from figure 3.5

656

14. combining models

that when we trained multiple polynomials using the sinusoidal data, and then aver-
aged the resulting functions, the contribution arising from the variance term tended to
cancel, leading to improved predictions. when we averaged a set of low-bias mod-
els (corresponding to higher order polynomials), we obtained accurate predictions
for the underlying sinusoidal function from which the data were generated.

in practice, of course, we have only a single data set, and so we have to    nd
a way to introduce variability between the different models within the committee.
one approach is to use bootstrap data sets, discussed in section 1.2.3. consider a
regression problem in which we are trying to predict the value of a single continuous
variable, and suppose we generate m bootstrap data sets and then use each to train
a separate copy ym(x) of a predictive model where m = 1, . . . , m. the committee
prediction is given by

ym(x).

(14.7)

m(cid:2)

m=1

ycom(x) =

1
m

this procedure is known as bootstrap aggregation or id112 (breiman, 1996).

suppose the true regression function that we are trying to predict is given by
h(x), so that the output of each of the models can be written as the true value plus
an error in the form

ym(x) = h(x) +  m(x).

(14.8)

the average sum-of-squares error then takes the form

ex

(14.9)
where ex[  ] denotes a frequentist expectation with respect to the distribution of the
input vector x. the average error made by the models acting individually is therefore

= ex

 m(x)2

(cid:8)

(cid:9)

(cid:9)

 m(x)2

.

(14.10)

similarly, the expected error from the committee (14.7) is given by

(cid:8){ym(x)     h(x)}2(cid:9)
m(cid:2)

eav =

ex

m=1

(cid:8)

1
m

      (cid:24)
      (cid:24)

m(cid:2)
m(cid:2)

m=1

m=1

1
m

1
m

ecom = ex

= ex

(cid:25)2

      

ym(x)     h(x)

(cid:25)2

      

 m(x)

if we assume that the errors have zero mean and are uncorrelated, so that

ex [ m(x)] = 0
ex [ m(x) l(x)] = 0,

m (cid:9)= l

(14.11)

(14.12)
(14.13)

exercise 14.2

then we obtain

14.3. boosting

657

ecom =

1
m

eav.

(14.14)

this apparently dramatic result suggests that the average error of a model can be
reduced by a factor of m simply by averaging m versions of the model. unfortu-
nately, it depends on the key assumption that the errors due to the individual models
are uncorrelated. in practice, the errors are typically highly correlated, and the reduc-
tion in overall error is generally small. it can, however, be shown that the expected
committee error will not exceed the expected error of the constituent models, so that
ecom (cid:1) eav. in order to achieve more signi   cant improvements, we turn to a more
sophisticated technique for building committees, known as boosting.

exercise 14.3

14.3. boosting

boosting is a powerful technique for combining multiple    base    classi   ers to produce
a form of committee whose performance can be signi   cantly better than that of any
of the base classi   ers. here we describe the most widely used form of boosting
algorithm called adaboost, short for    adaptive boosting   , developed by freund and
schapire (1996). boosting can give good results even if the base classi   ers have a
performance that is only slightly better than random, and hence sometimes the base
classi   ers are known as weak learners. originally designed for solving classi   cation
problems, boosting can also be extended to regression (friedman, 2001).

the principal difference between boosting and the committee methods such as
id112 discussed above, is that the base classi   ers are trained in sequence, and
each base classi   er is trained using a weighted form of the data set in which the
weighting coef   cient associated with each data point depends on the performance
of the previous classi   ers. in particular, points that are misclassi   ed by one of the
base classi   ers are given greater weight when used to train the next classi   er in
the sequence. once all the classi   ers have been trained, their predictions are then
combined through a weighted majority voting scheme, as illustrated schematically
in figure 14.1.

consider a two-class classi   cation problem, in which the training data comprises
input vectors x1, . . . , xn along with corresponding binary target variables t1, . . . , tn
where tn     {   1, 1}. each data point is given an associated weighting parameter
wn, which is initially set 1/n for all data points. we shall suppose that we have
a procedure available for training a base classi   er using weighted data to give a
function y(x)     {   1, 1}. at each stage of the algorithm, adaboost trains a new
classi   er using a data set in which the weighting coef   cients are adjusted according
to the performance of the previously trained classi   er so as to give greater weight
to the misclassi   ed data points. finally, when the desired number of base classi   ers
have been trained, they are combined to form a committee using coef   cients that
give different weight to different base classi   ers. the precise form of the adaboost
algorithm is given below.

658

14. combining models

figure 14.1 schematic illustration of

the
boosting framework.
each
base classi   er ym(x) is trained
on a weighted form of the train-
ing set (blue arrows) in which
the weights w(m)
depend on
the performance of
the pre-
vious base classi   er ym   1(x)
(green arrows). once all base
classi   ers have been trained,
they are combined to give
the    nal classi   er ym (x) (red
arrows).

n

{w

(1)

n }

{w

(2)

n }

{w

(m )
n

}

y1(x)

y2(x)

ym (x)

(cid:22)
m(cid:2)

ym (x) = sign

(cid:23)

  mym(x)

m

adaboost
1. initialize the data weighting coef   cients {wn} by setting w

(1)
n = 1/n for

n = 1, . . . , n.

2. for m = 1, . . . , m:

(a) fit a classi   er ym(x) to the training data by minimizing the weighted

error function

jm =

n(cid:2)

n=1

n i(ym(xn) (cid:9)= tn)
w(m)

(14.15)

where i(ym(xn) (cid:9)= tn) is the indicator function and equals 1 when
ym(xn) (cid:9)= tn and 0 otherwise.
n(cid:2)

(b) evaluate the quantities

 m =

n=1

(14.16)

and then use these to evaluate

  m = ln

.

(14.17)

(c) update the data weighting coef   cients

w(m+1)

n

= w(m)

n

exp{  mi(ym(xn) (cid:9)= tn)}

(14.18)

n i(ym(xn) (cid:9)= tn)
n(cid:2)
w(m)
(cid:12)

(cid:13)

w(m)

n=1

n

1      m
 m

3. make predictions using the    nal model, which is given by

14.3. boosting

659

(cid:22)

m(cid:2)

(cid:23)

ym (x) = sign

  mym(x)

.

(14.19)

m=1

we see that the    rst base classi   er y1(x) is trained using weighting coef   -
(1)
n that are all equal, which therefore corresponds to the usual procedure
cients w
for training a single classi   er. from (14.18), we see that in subsequent iterations
(m)
the weighting coef   cients w
are increased for data points that are misclassi   ed
n
and decreased for data points that are correctly classi   ed. successive classi   ers are
therefore forced to place greater emphasis on points that have been misclassi   ed by
previous classi   ers, and data points that continue to be misclassi   ed by successive
classi   ers receive ever greater weight. the quantities  m represent weighted mea-
sures of the error rates of each of the base classi   ers on the data set. we therefore
see that the weighting coef   cients   m de   ned by (14.17) give greater weight to the
more accurate classi   ers when computing the overall output given by (14.19).

the adaboost algorithm is illustrated in figure 14.2, using a subset of 30 data
points taken from the toy classi   cation data set shown in figure a.7. here each base
learners consists of a threshold on one of the input variables. this simple classi   er
corresponds to a form of decision tree known as a    decision stumps   , i.e., a deci-
sion tree with a single node. thus each base learner classi   es an input according to
whether one of the input features exceeds some threshold and therefore simply parti-
tions the space into two regions separated by a linear decision surface that is parallel
to one of the axes.

section 14.4

14.3.1 minimizing exponential error
boosting was originally motivated using statistical learning theory, leading to
upper bounds on the generalization error. however, these bounds turn out to be too
loose to have practical value, and the actual performance of boosting is much better
than the bounds alone would suggest. friedman et al. (2000) gave a different and
very simple interpretation of boosting in terms of the sequential minimization of an
exponential error function.

consider the exponential error function de   ned by
exp{   tnfm(xn)}

e =

n(cid:2)

n=1

(14.20)

where fm(x) is a classi   er de   ned in terms of a linear combination of base classi   ers
yl(x) of the form

fm(x) =

(14.21)
and tn     {   1, 1} are the training set target values. our goal is to minimize e with
respect to both the weighting coef   cients   l and the parameters of the base classi   ers
yl(x).

l=1

  lyl(x)

m(cid:2)

1
2

660

14. combining models

m = 2

   1

0

1

2

m = 10

m = 1

   1

0

1

2

m = 6

2

0

   2

2

0

   2

2

0

   2

2

0

   2

2

0

   2

2

0

   2

m = 3

   1

0

1

2

m = 150

   1

0

1

2

   1

0

1

2

   1

0

1

2

figure 14.2 illustration of boosting in which the base learners consist of simple thresholds applied to one or
other of the axes. each    gure shows the number m of base learners trained so far, along with the decision
boundary of the most recent base learner (dashed black line) and the combined decision boundary of the en-
semble (solid green line). each data point is depicted by a circle whose radius indicates the weight assigned to
that data point when training the most recently added base learner. thus, for instance, we see that points that
are misclassi   ed by the m = 1 base learner are given greater weight when training the m = 2 base learner.

instead of doing a global error function minimization, however, we shall sup-
pose that the base classi   ers y1(x), . . . , ym   1(x) are    xed, as are their coef   cients
  1, . . . ,   m   1, and so we are minimizing only with respect to   m and ym(x). sep-
arating off the contribution from base classi   er ym(x), we can then write the error
function in the form

(cid:12)

(cid:12)

n(cid:2)
n(cid:2)

n=1

n=1

e =

=

exp

   tnfm   1(xn)     1

2 tn  mym(xn)

(cid:13)

w(m)

n

exp

   1
2 tn  mym(xn)

(cid:13)

(14.22)

(m)

n = exp{   tnfm   1(xn)} can be viewed as constants
where the coef   cients w
if we denote by tm the set of
because we are optimizing only   m and ym(x).
data points that are correctly classi   ed by ym(x), and if we denote the remaining
misclassi   ed points by mm, then we can in turn rewrite the error function in the

14.3. boosting

661

(cid:2)

form

e = e

     m/2

w(m)

n + e  m/2

n   tm
= (e  m/2     e

     m/2)

n(cid:2)

n=1

(cid:2)

n   mm

w(m)

n

n i(ym(xn) (cid:9)= tn) + e
w(m)

     m/2

n(cid:2)

n=1

w(m)
n .

(14.23)

when we minimize this with respect to ym(x), we see that the second term is con-
stant, and so this is equivalent to minimizing (14.15) because the overall multiplica-
tive factor in front of the summation does not affect the location of the minimum.
similarly, minimizing with respect to   m, we obtain (14.17) in which  m is de   ned
by (14.16).

from (14.22) we see that, having found   m and ym(x), the weights on the data

(cid:12)

(cid:13)

   1
2 tn  mym(xn)

.

(14.24)

points are updated using

w(m+1)

n

= w(m)

n

exp

making use of the fact that

tnym(xn) = 1     2i(ym(xn) (cid:9)= tn)

(14.25)

(m)
we see that the weights w
n

are updated at the next iteration using
exp(     m/2) exp{  mi(ym(xn) (cid:9)= tn)} .

w(m+1)

= w(m)

n

(14.26)
because the term exp(     m/2) is independent of n, we see that it weights all data
points by the same factor and so can be discarded. thus we obtain (14.18).

n

finally, once all the base classi   ers are trained, new data points are classi   ed by
evaluating the sign of the combined function de   ned according to (14.21). because
the factor of 1/2 does not affect the sign it can be omitted, giving (14.19).

14.3.2 error functions for boosting
the exponential error function that is minimized by the adaboost algorithm
differs from those considered in previous chapters. to gain some insight into the
nature of the exponential error function, we    rst consider the expected error given
by

(cid:6)

ex,t [exp{   ty(x)}] =

exp{   ty(x)}p(t|x)p(x) dx.

(14.27)

(cid:2)

t

if we perform a variational minimization with respect to all possible functions y(x),
we obtain

(cid:12)

(cid:13)

y(x) =

ln

1
2

p(t = 1|x)
p(t =    1|x)

(14.28)

exercise 14.6

exercise 14.7

662

14. combining models

figure 14.3 plot of the exponential (green) and
rescaled cross-id178 (red) error
functions along with the hinge er-
ror
(blue) used in support vector
machines, and the misclassi   cation
for large
error (black). note that
negative values of z = ty(x),
the
cross-id178 gives a linearly in-
creasing penalty, whereas the expo-
nential loss gives an exponentially in-
creasing penalty.

e(z)

   2

   1

0

1

z

2

which is half the log-odds. thus the adaboost algorithm is seeking the best approx-
imation to the log odds ratio, within the space of functions represented by the linear
combination of base classi   ers, subject to the constrained minimization resulting
from the sequential optimization strategy. this result motivates the use of the sign
function in (14.19) to arrive at the    nal classi   cation decision.

we have already seen that the minimizer y(x) of the cross-id178 error (4.90)
for two-class classi   cation is given by the posterior class id203. in the case
of a target variable t     {   1, 1}, we have seen that the error function is given by
ln(1 + exp(   yt)). this is compared with the exponential error function in fig-
ure 14.3, where we have divided the cross-id178 error by a constant factor ln(2)
so that it passes through the point (0, 1) for ease of comparison. we see that both
can be seen as continuous approximations to the ideal misclassi   cation error func-
tion. an advantage of the exponential error is that its sequential minimization leads
to the simple adaboost scheme. one drawback, however, is that it penalizes large
negative values of ty(x) much more strongly than cross-id178. in particular, we
see that for large negative values of ty, the cross-id178 grows linearly with |ty|,
whereas the exponential error function grows exponentially with |ty|. thus the ex-
ponential error function will be much less robust to outliers or misclassi   ed data
points. another important difference between cross-id178 and the exponential er-
ror function is that the latter cannot be interpreted as the log likelihood function of
any well-de   ned probabilistic model. furthermore, the exponential error does not
generalize to classi   cation problems having k > 2 classes, again in contrast to the
cross-id178 for a probabilistic model, which is easily generalized to give (4.108).
the interpretation of boosting as the sequential optimization of an additive model
under an exponential error (friedman et al., 2000) opens the door to a wide range
of boosting-like algorithms, including multiclass extensions, by altering the choice
of error function. it also motivates the extension to regression problems (friedman,
2001). if we consider a sum-of-squares error function for regression, then sequential
minimization of an additive model of the form (14.21) simply involves    tting each
new base classi   er to the residual errors tn   fm   1(xn) from the previous model. as
we have noted, however, the sum-of-squares error is not robust to outliers, and this

section 7.1.2

exercise 14.8

section 4.3.4

exercise 14.9

figure 14.4 comparison of

the squared error
(green) with the absolute error (red)
showing how the latter places much
less emphasis on large errors and
hence is more robust to outliers and
mislabelled data points.

14.4. tree-based models

663

e(z)

   1

0

1

z

can be addressed by basing the boosting algorithm on the absolute deviation |y     t|
instead. these two error functions are compared in figure 14.4.

14.4. tree-based models

there are various simple, but widely used, models that work by partitioning the
input space into cuboid regions, whose edges are aligned with the axes, and then
assigning a simple model (for example, a constant) to each region. they can be
viewed as a model combination method in which only one model is responsible
for making predictions at any given point in input space. the process of selecting
a speci   c model, given a new input x, can be described by a sequential decision
making process corresponding to the traversal of a binary tree (one that splits into
two branches at each node). here we focus on a particular tree-based framework
called classi   cation and regression trees, or cart (breiman et al., 1984), although
there are many other variants going by such names as   and c4.5 (quinlan, 1986;
quinlan, 1993).

figure 14.5 shows an illustration of a recursive binary partitioning of the input
space, along with the corresponding tree structure. in this example, the    rst step

figure 14.5 illustration of a two-dimensional

in-
put space that has been partitioned
into    ve regions using axis-aligned
boundaries.

x2

  3

  2

b

a

e

c

d

  1

  4

x1

664

14. combining models

figure 14.6 binary tree corresponding to the par-
input space shown in fig-

titioning of
ure 14.5.

x1 >   1

x2 (cid:1)   2

x2 >   3

x1 (cid:1)   4

a

b

c

d

e

divides the whole of the input space into two regions according to whether x1 (cid:1)   1
or x1 >   1 where   1 is a parameter of the model. this creates two subregions, each
of which can then be subdivided independently. for instance, the region x1 (cid:1)   1
is further subdivided according to whether x2 (cid:1)   2 or x2 >   2, giving rise to the
regions denoted a and b. the recursive subdivision can be described by the traversal
of the binary tree shown in figure 14.6. for any new input x, we determine which
region it falls into by starting at the top of the tree at the root node and following
a path down to a speci   c leaf node according to the decision criteria at each node.
note that such id90 are not probabilistic id114.

within each region, there is a separate model to predict the target variable. for
instance, in regression we might simply predict a constant over each region, or in
classi   cation we might assign each region to a speci   c class. a key property of tree-
based models, which makes them popular in    elds such as medical diagnosis, for
example, is that they are readily interpretable by humans because they correspond
to a sequence of binary decisions applied to the individual input variables. for in-
stance, to predict a patient   s disease, we might    rst ask    is their temperature greater
than some threshold?   . if the answer is yes, then we might next ask    is their blood
pressure less than some threshold?   . each leaf of the tree is then associated with a
speci   c diagnosis.

in order to learn such a model from a training set, we have to determine the
structure of the tree, including which input variable is chosen at each node to form
the split criterion as well as the value of the threshold parameter   i for the split. we
also have to determine the values of the predictive variable within each region.

consider    rst a regression problem in which the goal is to predict a single target
variable t from a d-dimensional vector x = (x1, . . . , xd)t of input variables. the
training data consists of input vectors {x1, . . . , xn} along with the corresponding
continuous labels {t1, . . . , tn}. if the partitioning of the input space is given, and we
minimize the sum-of-squares error function, then the optimal value of the predictive
variable within any given region is just given by the average of the values of tn for
those data points that fall in that region.

now consider how to determine the structure of the decision tree. even for a
   xed number of nodes in the tree, the problem of determining the optimal structure
(including choice of input variable for each split as well as the corresponding thresh-

exercise 14.10

14.4. tree-based models

665

olds) to minimize the sum-of-squares error is usually computationally infeasible due
to the combinatorially large number of possible solutions. instead, a greedy opti-
mization is generally done by starting with a single root node, corresponding to the
whole input space, and then growing the tree by adding nodes one at a time. at each
step there will be some number of candidate regions in input space that can be split,
corresponding to the addition of a pair of leaf nodes to the existing tree. for each
of these, there is a choice of which of the d input variables to split, as well as the
value of the threshold. the joint optimization of the choice of region to split, and the
choice of input variable and threshold, can be done ef   ciently by exhaustive search
noting that, for a given choice of split variable and threshold, the optimal choice of
predictive variable is given by the local average of the data, as noted earlier. this
is repeated for all possible choices of variable to be split, and the one that gives the
smallest residual sum-of-squares error is retained.

given a greedy strategy for growing the tree, there remains the issue of when
to stop adding nodes. a simple approach would be to stop when the reduction in
residual error falls below some threshold. however, it is found empirically that often
none of the available splits produces a signi   cant reduction in error, and yet after
several more splits a substantial error reduction is found. for this reason, it is com-
mon practice to grow a large tree, using a stopping criterion based on the number
of data points associated with the leaf nodes, and then prune back the resulting tree.
the pruning is based on a criterion that balances residual error against a measure of
model complexity. if we denote the starting tree for pruning by t0, then we de   ne
t     t0 to be a subtree of t0 if it can be obtained by pruning nodes from t0 (in
other words, by collapsing internal nodes by combining the corresponding regions).
suppose the leaf nodes are indexed by    = 1, . . . ,|t|, with leaf node    representing
a region r   of input space having n   data points, and |t| denoting the total number
of leaf nodes. the optimal prediction for region r   is then given by

y   =

tn

(14.29)

and the corresponding contribution to the residual sum-of-squares is then

q   (t ) =

{tn     y  }2

.

the pruning criterion is then given by

c(t ) =

q   (t ) +   |t|

   =1

the id173 parameter    determines the trade-off between the overall residual
sum-of-squares error and the complexity of the model as measured by the number
|t| of leaf nodes, and its value is chosen by cross-validation.

for classi   cation problems, the process of growing and pruning the tree is sim-
ilar, except that the sum-of-squares error is replaced by a more appropriate measure

(cid:2)

xn   r  

1
n  

(cid:2)

xn   r  

|t|(cid:2)

(14.30)

(14.31)

666

14. combining models

of performance. if we de   ne p   k to be the proportion of data points in region r  
assigned to class k, where k = 1, . . . , k, then two commonly used choices are the
cross-id178

q   (t ) =

p   k ln p   k

k(cid:2)
k(cid:2)

k=1

and the gini index

q   (t ) =

p   k (1     p   k) .

(14.32)

(14.33)

exercise 14.11

k=1

these both vanish for p   k = 0 and p   k = 1 and have a maximum at p   k = 0.5. they
encourage the formation of regions in which a high proportion of the data points are
assigned to one class. the cross id178 and the gini index are better measures than
the misclassi   cation rate for growing the tree because they are more sensitive to the
node probabilities. also, unlike misclassi   cation rate, they are differentiable and
hence better suited to gradient based optimization methods. for subsequent pruning
of the tree, the misclassi   cation rate is generally used.

the human interpretability of a tree model such as cart is often seen as its
major strength. however, in practice it is found that the particular tree structure that
is learned is very sensitive to the details of the data set, so that a small change to the
training data can result in a very different set of splits (hastie et al., 2001).

there are other problems with tree-based methods of the kind considered in
this section. one is that the splits are aligned with the axes of the feature space,
which may be very suboptimal. for instance, to separate two classes whose optimal
decision boundary runs at 45 degrees to the axes would need a large number of
axis-parallel splits of the input space as compared to a single non-axis-aligned split.
furthermore, the splits in a decision tree are hard, so that each region of input space
is associated with one, and only one, leaf node model. the last issue is particularly
problematic in regression where we are typically aiming to model smooth functions,
and yet the tree model produces piecewise-constant predictions with discontinuities
at the split boundaries.

14.5. conditional mixture models

we have seen that standard id90 are restricted by hard, axis-aligned splits of
the input space. these constraints can be relaxed, at the expense of interpretability,
by allowing soft, probabilistic splits that can be functions of all of the input variables,
not just one of them at a time. if we also give the leaf models a probabilistic inter-
pretation, we arrive at a fully probabilistic tree-based model called the hierarchical
mixture of experts, which we consider in section 14.5.3.

an alternative way to motivate the hierarchical mixture of experts model is to
start with a standard probabilistic mixtures of unconditional density models such as
gaussians and replace the component densities with conditional distributions. here
we consider mixtures of id75 models (section 14.5.1) and mixtures of

chapter 9

14.5. conditional mixture models

667

id28 models (section 14.5.2). in the simplest case, the mixing coef   -
cients are independent of the input variables. if we make a further generalization to
allow the mixing coef   cients also to depend on the inputs then we obtain a mixture
of experts model. finally, if we allow each component in the mixture model to be
itself a mixture of experts model, then we obtain a hierarchical mixture of experts.

14.5.1 mixtures of id75 models
one of the many advantages of giving a probabilistic interpretation to the lin-
ear regression model is that it can then be used as a component in more complex
probabilistic models. this can be done, for instance, by viewing the conditional
distribution representing the id75 model as a node in a directed prob-
abilistic graph. here we consider a simple example corresponding to a mixture of
id75 models, which represents a straightforward extension of the gaus-
sian mixture model discussed in section 9.2 to the case of conditional gaussian
distributions.

we therefore consider k id75 models, each governed by its own
weight parameter wk. in many applications, it will be appropriate to use a common
noise variance, governed by a precision parameter   , for all k components, and this
is the case we consider here. we will once again restrict attention to a single target
variable t, though the extension to multiple outputs is straightforward. if we denote
the mixing coef   cients by   k, then the mixture distribution can be written

p(t|  ) =

  kn (t|wt

   1)

k   ,   

(14.34)
where    denotes the set of all adaptive parameters in the model, namely w = {wk},
(cid:22)
   = {  k}, and   . the log likelihood function for this model, given a data set of
k(cid:2)
observations {  n, tn}, then takes the form

n(cid:2)

(cid:23)

k=1

ln p(t|  ) =

ln

  kn (tn|wt

k   n,   

   1)

(14.35)

where t = (t1, . . . , tn)t denotes the vector of target variables.

n=1

k=1

in order to maximize this likelihood function, we can once again appeal to the
em algorithm, which will turn out to be a simple extension of the em algorithm for
unconditional gaussian mixtures of section 9.2. we can therefore build on our expe-
rience with the unconditional mixture and introduce a set z = {zn} of binary latent
variables where znk     {0, 1} in which, for each data point n, all of the elements
k = 1, . . . , k are zero except for a single value of 1 indicating which component
of the mixture was responsible for generating that data point. the joint distribution
over latent and observed variables can be represented by the graphical model shown
in figure 14.7.

the complete-data log likelihood function then takes the form

k(cid:2)

ln p(t, z|  ) =

znk ln

  kn (tn|wt

k   n,   

   1)

.

(14.36)

n(cid:2)

k(cid:2)

n=1

k=1

(cid:26)

(cid:27)

exercise 14.12

exercise 14.13

668

14. combining models

figure 14.7 probabilistic directed graph representing a mixture of

id75 models, de   ned by (14.35).

  

  

w

zn

  n

tn

n

the em algorithm begins by    rst choosing an initial value   old for the model param-
eters. in the e step, these parameter values are then used to evaluate the posterior
probabilities, or responsibilities, of each component k for every data point n given
by

(cid:5)
  nk = e[znk] = p(k|  n,   old) =   kn (tn|wt
j   jn (tn|wt

k   n,   
j   n,      1) .

(14.37)

   1)

exercise 14.14

k(cid:2)

the responsibilities are then used to determine the expectation, with respect to the
posterior distribution p(z|t,   old), of the complete-data log likelihood, which takes
n(cid:2)
(cid:27)
the form
q(  ,   old) = ez [ln p(t, z|  )] =
(cid:5)

in the m step, we maximize the function q(  ,   old) with respect to   , keeping the
  nk    xed. for the optimization with respect to the mixing coef   cients   k we need
k   k = 1, which can be done with the aid of a
to take account of the constraint
lagrange multiplier, leading to an m-step re-estimation equation for   k in the form

ln   k + lnn (tn|wt

k   n,   

   1)

(cid:26)

  nk

n=1

k=1

.

n(cid:2)

  k =

1
n

  nk.

n=1

(14.38)

note that this has exactly the same form as the corresponding result for a simple
mixture of unconditional gaussians given by (9.22).

next consider the maximization with respect to the parameter vector wk of the
kth id75 model. substituting for the gaussian distribution, we see that
the function q(  ,   old), as a function of the parameter vector wk, takes the form

(cid:12)

n(cid:2)

  nk

n=1

(cid:10)

      
2

tn     wt

k   n

(cid:13)

(cid:11)2

q(  ,   old) =

+ const

(14.39)

where the constant term includes the contributions from other weight vectors wj for
j (cid:9)= k. note that the quantity we are maximizing is similar to the (negative of the)
standard sum-of-squares error (3.12) for a single id75 model, but with
the inclusion of the responsibilities   nk. this represents a weighted least squares

14.5. conditional mixture models

669

problem, in which the term corresponding to the nth data point carries a weighting
coef   cient given by     nk, which could be interpreted as an effective precision for
each data point. we see that each component id75 model in the mixture,
governed by its own parameter vector wk, is    tted separately to the whole data set in
the m step, but with each data point n weighted by the responsibility   nk that model
k takes for that data point. setting the derivative of (14.39) with respect to wk equal
to zero gives

n(cid:2)

(cid:10)

0 =

  nk

n=1

(cid:11)

tn     wt

k   n

  n

(14.40)

which we can write in matrix notation as

(14.41)
where rk = diag(  nk) is a diagonal matrix of size n    n. solving for wk, we
obtain

0 =   trk(t       wk)
(cid:10)

(14.42)
this represents a set of modi   ed normal equations corresponding to the weighted
least squares problem, of the same form as (4.99) found in the context of logistic
regression. note that after each e step, the matrix rk will change and so we will
have to solve the normal equations afresh in the subsequent m step.

finally, we maximize q(  ,   old) with respect to   . keeping only terms that

(cid:11)   1   trkt.

  trk  

wk =

depend on   , the function q(  ,   old) can be written

(cid:10)

(cid:13)

(cid:11)2

q(  ,   old) =

1
2

ln          
2

tn     wt

k   n

.

(14.43)

(cid:12)

n(cid:2)

k(cid:2)

  nk

n=1

k=1

setting the derivative with respect to    equal to zero, and rearranging, we obtain the
m-step equation for    in the form

n(cid:2)

k(cid:2)

(cid:10)

1
  

=

1
n

  nk

n=1

k=1

tn     wt

k   n

(cid:11)2

.

(14.44)

in figure 14.8, we illustrate this em algorithm using the simple example of
   tting a mixture of two straight lines to a data set having one input variable x and
one target variable t. the predictive density (14.34) is plotted in figure 14.9 using
the converged parameter values obtained from the em algorithm, corresponding to
the right-hand plot in figure 14.8. also shown in this    gure is the result of    tting
a single id75 model, which gives a unimodal predictive density. we see
that the mixture model gives a much better representation of the data distribution,
and this is re   ected in the higher likelihood value. however, the mixture model
also assigns signi   cant id203 mass to regions where there is no data because its
predictive distribution is bimodal for all values of x. this problem can be resolved by
extending the model to allow the mixture coef   cients themselves to be functions of
x, leading to models such as the mixture density networks discussed in section 5.6,
and hierarchical mixture of experts discussed in section 14.5.3.

670

14. combining models

1.5
1
0.5
0
   0.5
   1
   1.5

1

0.8

0.6

0.4

0.2

1.5
1
0.5
0
   0.5
   1
   1.5

1.5
1
0.5
0
   0.5
   1
   1.5

   1

   0.5

0

0.5

1

   1

   0.5

0

0.5

1

   1

   0.5

0

0.5

1

1

0.8

0.6

0.4

0.2

1

0.8

0.6

0.4

0.2

0

   1

   0.5

0

0.5

1

0

   1

   0.5

0

0.5

1

0

   1

   0.5

0

0.5

1

figure 14.8 example of a synthetic data set, shown by the green points, having one input variable x and one
target variable t, together with a mixture of two id75 models whose mean functions y(x, wk), where
k     {1, 2}, are shown by the blue and red lines. the upper three plots show the initial con   guration (left), the
result of running 30 iterations of em (centre), and the result after 50 iterations of em (right). here    was initialized
to the reciprocal of the true variance of the set of target values. the lower three plots show the corresponding
responsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the
posterior id203 of the blue line for that data point (and similarly for the red segment).

14.5.2 mixtures of logistic models
because the id28 model de   nes a conditional distribution for the
target variable, given the input vector, it is straightforward to use it as the component
distribution in a mixture model, thereby giving rise to a richer family of conditional
distributions compared to a single id28 model. this example involves
a straightforward combination of ideas encountered in earlier sections of the book
and will help consolidate these for the reader.

the conditional distribution of the target variable, for a probabilistic mixture of

k id28 models, is given by

k(cid:2)
(cid:10)

k=1

p(t|  ,   ) =

  kyt

k [1     yk]1   t
(cid:11)

(14.45)

where    is the feature vector, yk =   
denotes the adjustable parameters namely {  k} and {wk}.

is the output of component k, and   
now suppose we are given a data set {  n, tn}. the corresponding likelihood

k   

wt

14.5. conditional mixture models

671

figure 14.9 the left plot shows the predictive conditional density corresponding to the converged solution in
figure 14.8. this gives a log likelihood value of    3.0. a vertical slice through one of these plots at a particular
value of x represents the corresponding conditional distribution p(t|x), which we see is bimodal. the plot on the
right shows the predictive density for a single id75 model    tted to the same data set using maximum
likelihood. this model has a smaller log likelihood of    27.6.

(cid:23)

(cid:27)znk

(cid:22)

k(cid:2)

n=1

k=1

n(cid:14)

k(cid:14)

(cid:26)

function is then given by

p(t|  ) =

n(cid:14)

  kytn

nk [1     ynk]1   tn

(14.46)

where ynk =   (wt
k   n) and t = (t1, . . . , tn)t. we can maximize this likelihood
function iteratively by making use of the em algorithm. this involves introducing
latent variables znk that correspond to a 1-of-k coded binary indicator variable for
each data point n. the complete-data likelihood function is then given by

p(t, z|  ) =

  kytn

nk [1     ynk]1   tn

(14.47)

n=1

k=1

where z is the matrix of latent variables with elements znk. we initialize the em
algorithm by choosing an initial value   old for the model parameters. in the e step,
we then use these parameter values to evaluate the posterior probabilities of the com-
ponents k for each data point n, which are given by
  nk = e[znk] = p(k|  n,   old) =   kytn
j   jytn

nk [1     ynk]1   tn
nj [1     ynj]1   tn

(cid:5)

(14.48)

.

these responsibilities are then used to    nd the expected complete-data log likelihood
as a function of   , given by

q(  ,   old) = ez [ln p(t, z|  )]

n(cid:2)

k(cid:2)

n=1

k=1

=

  nk {ln   k + tn ln ynk + (1     tn) ln (1     ynk)} .

(14.49)

672

14. combining models

(cid:5)

n(cid:2)

1
n

the m step involves maximization of this function with respect to   , keeping   old,
and hence   nk,    xed. maximization with respect to   k can be done in the usual way,
k   k = 1, giving
with a lagrange multiplier to enforce the summation constraint
the familiar result

  k =

n=1

  nk.

(14.50)
to determine the {wk}, we note that the q(  ,   old) function comprises a sum
over terms indexed by k each of which depends only on one of the vectors wk, so
that the different vectors are decoupled in the m step of the em algorithm. in other
words, the different components interact only via the responsibilities, which are    xed
during the m step. note that the m step does not have a closed-form solution and
must be solved iteratively using, for instance, the iterative reweighted least squares
(irls) algorithm. the gradient and the hessian for the vector wk are given by

   kq =

  nk(tn     ynk)  n
n(cid:2)

hk =       k   kq =

  nkynk(1     ynk)  n  t

n

(14.51)

(14.52)

n(cid:2)

n=1

section 4.3.3

n=1

where    k denotes the gradient with respect to wk. for    xed   nk, these are indepen-
dent of {wj} for j (cid:9)= k and so we can solve for each wk separately using the irls
algorithm. thus the m-step equations for component k correspond simply to    tting
a single id28 model to a weighted data set in which data point n carries
a weight   nk. figure 14.10 shows an example of the mixture of id28
models applied to a simple classi   cation problem. the extension of this model to a
mixture of softmax models for more than two classes is straightforward.

section 4.3.3

exercise 14.16

14.5.3 mixtures of experts
in section 14.5.1, we considered a mixture of id75 models, and in
section 14.5.2 we discussed the analogous mixture of linear classi   ers. although
these simple mixtures extend the    exibility of linear models to include more com-
plex (e.g., multimodal) predictive distributions, they are still very limited. we can
further increase the capability of such models by allowing the mixing coef   cients
themselves to be functions of the input variable, so that

p(t|x) =

  k(x)pk(t|x).

(14.53)

k=1

this is known as a mixture of experts model (jacobs et al., 1991) in which the mix-
ing coef   cients   k(x) are known as gating functions and the individual component
densities pk(t|x) are called experts. the notion behind the terminology is that differ-
ent components can model the distribution in different regions of input space (they

k(cid:2)

14.5. conditional mixture models

673

figure 14.10 illustration of a mixture of id28 models. the left plot shows data points drawn
from two classes denoted red and blue, in which the background colour (which varies from pure red to pure blue)
denotes the true id203 of the class label. the centre plot shows the result of    tting a single id28
model using maximum likelihood, in which the background colour denotes the corresponding id203 of the
class label. because the colour is a near-uniform purple, we see that the model assigns a id203 of around
0.5 to each of the classes over most of input space. the right plot shows the result of    tting a mixture of two
id28 models, which now gives much higher id203 to the correct labels for many of the points
in the blue class.

(cid:5)

are    experts    at making predictions in their own regions), and the gating functions
determine which components are dominant in which region.
the gating functions   k(x) must satisfy the usual constraints for mixing co-
ef   cients, namely 0 (cid:1)   k(x) (cid:1) 1 and
k   k(x) = 1. they can therefore be
represented, for example, by linear softmax models of the form (4.104) and (4.105).
if the experts are also linear (regression or classi   cation) models, then the whole
model can be    tted ef   ciently using the em algorithm, with iterative reweighted
least squares being employed in the m step (jordan and jacobs, 1994).

such a model still has signi   cant limitations due to the use of linear models
for the gating and expert functions. a much more    exible model is obtained by
using a multilevel gating function to give the hierarchical mixture of experts, or
hme model (jordan and jacobs, 1994). to understand the structure of this model,
imagine a mixture distribution in which each component in the mixture is itself a
mixture distribution. for simple unconditional mixtures, this hierarchical mixture is
trivially equivalent to a single    at mixture distribution. however, when the mixing
coef   cients are input dependent, this hierarchical model becomes nontrivial. the
hme model can also be viewed as a probabilistic version of id90 discussed
in section 14.4 and can again be trained ef   ciently by maximum likelihood using an
em algorithm with irls in the m step. a bayesian treatment of the hme has been
given by bishop and svens  en (2003) based on variational id136.

we shall not discuss the hme in detail here. however, it is worth pointing out
the close connection with the mixture density network discussed in section 5.6. the
principal advantage of the mixtures of experts model is that it can be optimized by
em in which the m step for each mixture component and gating model involves
a id76 (although the overall optimization is nonconvex). by con-
trast, the advantage of the mixture density network approach is that the component

exercise 14.17

section 4.3.3

674

14. combining models

exercises

densities and the mixing coef   cients share the hidden units of the neural network.
furthermore, in the mixture density network, the splits of the input space are further
relaxed compared to the hierarchical mixture of experts in that they are not only soft,
and not constrained to be axis aligned, but they can also be nonlinear.

14.1 ((cid:12) (cid:12)) www consider a set models of the form p(t|x, zh,   h, h) in which x is the
input vector, t is the target vector, h indexes the different models, zh is a latent vari-
able for model h, and   h is the set of parameters for model h. suppose the models
have prior probabilities p(h) and that we are given a training set x = {x1, . . . , xn}
and t = {t1, . . . , tn}. write down the formulae needed to evaluate the predic-
tive distribution p(t|x, x, t) in which the latent variables and the model index are
marginalized out. use these formulae to highlight the difference between bayesian
averaging of different models and the use of latent variables within a single model.
14.2 ((cid:12)) the expected sum-of-squares error eav for a simple committee model can
be de   ned by (14.10), and the expected error of the committee itself is given by
(14.11). assuming that the individual errors satisfy (14.12) and (14.13), derive the
result (14.14).

14.3 ((cid:12)) www by making use of jensen   s inequality (1.115), for the special case of
the convex function f(x) = x2, show that the average expected sum-of-squares
error eav of the members of a simple committee model, given by (14.10), and the
expected error ecom of the committee itself, given by (14.11), satisfy

ecom (cid:1) eav.

(14.54)

14.4 ((cid:12) (cid:12)) by making use of jensen   s in equality (1.115), show that the result (14.54)
derived in the previous exercise hods for any error function e(y), not just sum-of-
squares, provided it is a convex function of y.

14.5 ((cid:12) (cid:12)) www consider a committee in which we allow unequal weighting of the

constituent models, so that

ycom(x) =

m(cid:2)

  mym(x).

(14.55)

in order to ensure that the predictions ycom(x) remain within sensible limits, sup-
pose that we require that they be bounded at each value of x by the minimum and
maximum values given by any of the members of the committee, so that

m=1

ymin(x) (cid:1) ycom(x) (cid:1) ymax(x).

(14.56)

show that a necessary and suf   cient condition for this constraint is that the coef   -
cients   m satisfy

  m (cid:2) 0,

  m = 1.

(14.57)

m(cid:2)

m=1

exercises

675

14.6 ((cid:12)) www by differentiating the error function (14.23) with respect to   m, show
that the parameters   m in the adaboost algorithm are updated using (14.17) in
which  m is de   ned by (14.16).

14.7 ((cid:12)) by making a variational minimization of the expected exponential error function
given by (14.27) with respect to all possible functions y(x), show that the minimizing
function is given by (14.28).

14.8 ((cid:12)) show that the exponential error function (14.20), which is minimized by the
adaboost algorithm, does not correspond to the log likelihood of any well-behaved
probabilistic model. this can be done by showing that the corresponding conditional
distribution p(t|x) cannot be correctly normalized.

14.9 ((cid:12)) www show that the sequential minimization of the sum-of-squares error func-
tion for an additive model of the form (14.21) in the style of boosting simply involves
   tting each new base classi   er to the residual errors tn   fm   1(xn) from the previous
model.

14.10 ((cid:12)) verify that if we minimize the sum-of-squares error between a set of training
values {tn} and a single predictive value t, then the optimal solution for t is given
by the mean of the {tn}.

14.11 ((cid:12) (cid:12)) consider a data set comprising 400 data points from class c1 and 400 data
points from class c2. suppose that a tree model a splits these into (300, 100) at
the    rst leaf node and (100, 300) at the second leaf node, where (n, m) denotes that
n points are assigned to c1 and m points are assigned to c2. similarly, suppose
that a second tree model b splits them into (200, 400) and (200, 0). evaluate the
misclassi   cation rates for the two trees and hence show that they are equal. similarly,
evaluate the cross-id178 (14.32) and gini index (14.33) for the two trees and show
that they are both lower for tree b than for tree a.

14.12 ((cid:12) (cid:12)) extend the results of section 14.5.1 for a mixture of id75 models
to the case of multiple target values described by a vector t. to do this, make use of
the results of section 3.1.5.

14.13 ((cid:12)) www verify that the complete-data log likelihood function for the mixture of

id75 models is given by (14.36).

14.14 ((cid:12)) use the technique of lagrange multipliers (appendix e) to show that the m-step
re-estimation equation for the mixing coef   cients in the mixture of id75
models trained by maximum likelihood em is given by (14.38).

14.15 ((cid:12)) www we have already noted that if we use a squared id168 in a regres-
sion problem, the corresponding optimal prediction of the target variable for a new
input vector is given by the conditional mean of the predictive distribution. show
that the conditional mean for the mixture of id75 models discussed in
section 14.5.1 is given by a linear combination of the means of each component dis-
tribution. note that if the conditional distribution of the target data is multimodal,
the conditional mean can give poor predictions.

676

14. combining models

14.16 ((cid:12) (cid:12) (cid:12)) extend the id28 mixture model of section 14.5.2 to a mixture
of softmax classi   ers representing c (cid:2) 2 classes. write down the em algorithm for
determining the parameters of this model through maximum likelihood.

14.17 ((cid:12) (cid:12)) www consider a mixture model for a conditional distribution p(t|x) of the

k(cid:2)

form

p(t|x) =

  k  k(t|x)

k=1

(14.58)
in which each mixture component   k(t|x) is itself a mixture model. show that this
two-level hierarchical mixture is equivalent to a conventional single-level mixture
model. now suppose that the mixing coef   cients in both levels of such a hierar-
chical model are arbitrary functions of x. again, show that this hierarchical model
is again equivalent to a single-level model with x-dependent mixing coef   cients.
finally, consider the case in which the mixing coef   cients at both levels of the hi-
erarchical mixture are constrained to be linear classi   cation (logistic or softmax)
models. show that the hierarchical mixture cannot in general be represented by a
single-level mixture having linear classi   cation models for the mixing coef   cients.
hint: to do this it is suf   cient to construct a single counter-example, so consider a
mixture of two components in which one of those components is itself a mixture of
two components, with mixing coef   cients given by linear-logistic models. show that
this cannot be represented by a single-level mixture of 3 components having mixing
coef   cients determined by a linear-softmax model.

appendix a. data sets

in this appendix, we give a brief introduction to the data sets used to illustrate some
of the algorithms described in this book. detailed information on    le formats for
these data sets, as well as the data    les themselves, can be obtained from the book
web site:

http://research.microsoft.com/   cmbishop/prml

handwritten digits

the digits data used in this book is taken from the mnist data set (lecun et al.,
1998), which itself was constructed by modifying a subset of the much larger data
set produced by nist (the national institute of standards and technology). it com-
prises a training set of 60, 000 examples and a test set of 10, 000 examples. some
of the data was collected from census bureau employees and the rest was collected
from high-school children, and care was taken to ensure that the test examples were
written by different individuals to the training examples.
the original nist data had binary (black or white) pixels. to create mnist,
these images were size normalized to    t in a 20  20 pixel box while preserving their
aspect ratio. as a consequence of the anti-aliasing used to change the resolution of
the images, the resulting mnist digits are grey scale. these images were then
centred in a 28    28 box. examples of the mnist digits are shown in figure a.1.
error rates for classifying the digits range from 12% for a simple linear classi-
   er, through 0.56% for a carefully designed support vector machine, to 0.4% for a
convolutional neural network (lecun et al., 1998).

677

678

a. data sets

figure a.1 one hundred examples of the
mnist digits chosen at ran-
dom from the training set.

oil flow

this is a synthetic data set that arose out of a project aimed at measuring nonin-
vasively the proportions of oil, water, and gas in north sea oil transfer pipelines
(bishop and james, 1993). it is based on the principle of dual-energy gamma densit-
ometry. the ideas is that if a narrow beam of gamma rays is passed through the pipe,
the attenuation in the intensity of the beam provides information about the density of
material along its path. thus, for instance, the beam will be attenuated more strongly
by oil than by gas.

a single attenuation measurement alone is not suf   cient because there are two
degrees of freedom corresponding to the fraction of oil and the fraction of water (the
fraction of gas is redundant because the three fractions must add to one). to address
this, two gamma beams of different energies (in other words different frequencies or
wavelengths) are passed through the pipe along the same path, and the attenuation of
each is measured. because the absorbtion properties of different materials vary dif-
ferently as a function of energy, measurement of the attenuations at the two energies
provides two independent pieces of information. given the known absorbtion prop-
erties of oil, water, and gas at the two energies, it is then a simple matter to calculate
the average fractions of oil and water (and hence of gas) measured along the path of
the gamma beams.

there is a further complication, however, associated with the motion of the ma-
terials along the pipe. if the    ow velocity is small, then the oil    oats on top of the
water with the gas sitting above the oil. this is known as a laminar or strati   ed

figure a.2 the three geometrical con   gurations of the oil,
water, and gas phases used to generate the oil-
   ow data set. for each con   guration, the pro-
portions of the three phases can vary.

a. data sets

679

strati   ed

annular

oil

water

gas

mix

homogeneous

   ow con   guration and is illustrated in figure a.2. as the    ow velocity is increased,
more complex geometrical con   gurations of the oil, water, and gas can arise. for the
purposes of this data set, two speci   c idealizations are considered. in the annular
con   guration the oil, water, and gas form concentric cylinders with the water around
the outside and the gas in the centre, whereas in the homogeneous con   guration the
oil, water and gas are assumed to be intimately mixed as might occur at high    ow
velocities under turbulent conditions. these con   gurations are also illustrated in
figure a.2.

we have seen that a single dual-energy beam gives the oil and water fractions
measured along the path length, whereas we are interested in the volume fractions of
oil and water. this can be addressed by using multiple dual-energy gamma densit-
ometers whose beams pass through different regions of the pipe. for this particular
data set, there are six such beams, and their spatial arrangement is shown in fig-
ure a.3. a single observation is therefore represented by a 12-dimensional vector
comprising the fractions of oil and water measured along the paths of each of the
beams. we are, however, interested in obtaining the overall volume fractions of the
three phases in the pipe. this is much like the classical problem of tomographic re-
construction, used in medical imaging for example, in which a two-dimensional dis-

figure a.3 cross section of the pipe showing the arrangement of the
six beam lines, each of which comprises a single dual-
energy gamma densitometer. note that the vertical beams
are asymmetrically arranged relative to the central axis
(shown by the dotted line).

680

a. data sets

tribution is to be reconstructed from an number of one-dimensional averages. here
there are far fewer line measurements than in a typical tomography application. on
the other hand the range of geometrical con   gurations is much more limited, and so
the con   guration, as well as the phase fractions, can be predicted with reasonable
accuracy from the densitometer data.

for safety reasons, the intensity of the gamma beams is kept relatively weak and
so to obtain an accurate measurement of the attenuation, the measured beam intensity
is integrated over a speci   c time interval. for a    nite integration time, there are
random    uctuations in the measured intensity due to the fact that the gamma beams
comprise discrete packets of energy called photons. in practice, the integration time
is chosen as a compromise between reducing the noise level (which requires a long
integration time) and detecting temporal variations in the    ow (which requires a short
integration time). the oil    ow data set is generated using realistic known values for
the absorption properties of oil, water, and gas at the two gamma energies used, and
with a speci   c choice of integration time (10 seconds) chosen as characteristic of a
typical practical setup.

each point in the data set is generated independently using the following steps:

1. choose one of the three phase con   gurations at random with equal id203.

2. choose three random numbers f1, f2 and f3 from the uniform distribution over

(0, 1) and de   ne

foil =

f1

f1 + f2 + f3

,

fwater =

f2

f1 + f2 + f3

.

(a.1)

this treats the three phases on an equal footing and ensures that the volume
fractions add to one.

3. for each of the six beam lines, calculate the effective path lengths through oil

and water for the given phase con   guration.

4. perturb the path lengths using the poisson distribution based on the known
beam intensities and integration time to allow for the effect of photon statistics.

each point in the data set comprises the 12 path length measurements, together
with the fractions of oil and water and a binary label describing the phase con   gu-
ration. the data set is divided into training, validation, and test sets, each of which
comprises 1, 000 independent data points. details of the data format are available
from the book web site.

in bishop and james (1993), statistical machine learning techniques were used
to predict the volume fractions and also the geometrical con   guration of the phases
shown in figure a.2, from the 12-dimensional vector of measurements. the 12-
dimensional observation vectors can also be used to test data visualization algo-
rithms.

this data set has a rich and interesting structure, as follows. for any given
con   guration there are two degrees of freedom corresponding to the fractions of

a. data sets

681

oil and water, and so for in   nite integration time the data will locally live on a two-
dimensional manifold. for a    nite integration time, the individual data points will be
perturbed away from the manifold by the photon noise. in the homogeneous phase
con   guration, the path lengths in oil and water are linearly related to the fractions of
oil and water, and so the data points lie close to a linear manifold. for the annular
con   guration, the relationship between phase fraction and path length is nonlinear
and so the manifold will be nonlinear. in the case of the laminar con   guration the
situation is even more complex because small variations in the phase fractions can
cause one of the horizontal phase boundaries to move across one of the horizontal
beam lines leading to a discontinuous jump in the 12-dimensional observation space.
in this way, the two-dimensional nonlinear manifold for the laminar con   guration is
broken into six distinct segments. note also that some of the manifolds for different
phase con   gurations meet at speci   c points, for example if the pipe is    lled entirely
with oil, it corresponds to speci   c instances of the laminar, annular, and homoge-
neous con   gurations.

old faithful

old faithful, shown in figure a.4, is a hydrothermal geyser in yellowstone national
park in the state of wyoming, u.s.a., and is a popular tourist attraction. its name
stems from the supposed regularity of its eruptions.

the data set comprises 272 observations, each of which represents a single erup-
tion and contains two variables corresponding to the duration in minutes of the erup-
tion, and the time until the next eruption, also in minutes. figure a.5 shows a plot of
the time to the next eruption versus the duration of the eruptions. it can be seen that
the time to the next eruption varies considerably, although knowledge of the duration
of the current eruption allows it to be predicted more accurately. note that there exist
several other data sets relating to the eruptions of old faithful.

figure a.4 the old faithful geyser
national
c(cid:9)bruce t. gourley

in
park.
www.brucegourley.com.

yellowstone

682

a. data sets

figure a.5 plot of the time to the next eruption
in minutes (vertical axis) versus the
duration of the eruption in minutes
(horizontal axis) for the old faithful
data set.

100

90

80

70

60

50

40

1

2

3

4

5

6

synthetic data

throughout the book, we use two simple synthetic data sets to illustrate many of the
algorithms. the    rst of these is a regression problem, based on the sinusoidal func-
tion, shown in figure a.6. the input values {xn} are generated uniformly in range
(0, 1), and the corresponding target values {tn} are obtained by    rst computing the
corresponding values of the function sin(2  x), and then adding random noise with
a gaussian distribution having standard deviation 0.3. various forms of this data set,
having different numbers of data points, are used in the book.

the second data set is a classi   cation problem having two classes, with equal
prior probabilities, and is shown in figure a.7. the blue class is generated from a
single gaussian while the red class comes from a mixture of two gaussians. be-
cause we know the class priors and the class-conditional densities, it is straightfor-
ward to evaluate and plot the true posterior probabilities as well as the minimum
misclassi   cation-rate decision boundary, as shown in figure a.7.

a. data sets

683

t

1

0

   1

t

1

0

   1

0

x

1

0

x

1

figure a.6 the left-hand plot shows the synthetic regression data set along with the underlying sinusoidal
function from which the data points were generated. the right-hand plot shows the true conditional distribution
p(t|x) from which the labels are generated, in which the green curve denotes the mean, and the shaded region
spans one standard deviation on each side of the mean.

2

0

   2

   2

0

2

figure a.7 the left plot shows the synthetic classi   cation data set with data from the two classes shown in
red and blue. on the right is a plot of the true posterior probabilities, shown on a colour scale going from pure
red denoting id203 of the red class is 1 to pure blue denoting id203 of the red class is 0. because
these probabilities are known, the optimal decision boundary for minimizing the misclassi   cation rate (which
corresponds to the contour along which the posterior probabilities for each class equal 0.5) can be evaluated
and is shown by the green curve. this decision boundary is also plotted on the left-hand    gure.

appendix b. id203 distributions

in this appendix, we summarize the main properties of some of the most widely used
id203 distributions, and for each distribution we list some key statistics such as
the expectation e[x], the variance (or covariance), the mode, and the id178 h[x].
all of these distributions are members of the exponential family and are widely used
as building blocks for more sophisticated probabilistic models.

bernoulli
this is the distribution for a single binary variable x     {0, 1} representing, for
example, the result of    ipping a coin. it is governed by a single continuous parameter
       [0, 1] that represents the id203 of x = 1.
bern(x|  ) =   x(1       )1   x

e[x] =   
var[x] =   (1       )

(cid:12)

mode[x] =

1 if    (cid:2) 0.5,
0 otherwise

h[x] =       ln        (1       ) ln(1       ).

(b.1)
(b.2)
(b.3)

(b.4)

(b.5)

the bernoulli is a special case of the binomial distribution for the case of a single
observation. its conjugate prior for    is the beta distribution.

685

686

b. id203 distributions

beta
this is a distribution over a continuous variable        [0, 1], which is often used to
represent the id203 for some binary event. it is governed by two parameters a
and b that are constrained by a > 0 and b > 0 to ensure that the distribution can be
normalized.

beta(  |a, b) =

  (a + b)

  (a)  (b)   a   1(1       )b   1

e[  ] =

var[  ] =

mode[  ] =

a

a + b

ab

(a + b)2(a + b + 1)
a     1
a + b     2 .

(b.6)

(b.7)

(b.8)

(b.9)

the beta is the conjugate prior for the bernoulli distribution, for which a and b can
be interpreted as the effective prior number of observations of x = 1 and x = 0,
respectively. its density is    nite if a (cid:2) 1 and b (cid:2) 1, otherwise there is a singularity
at    = 0 and/or    = 1. for a = b = 1, it reduces to a uniform distribution. the beta
distribution is a special case of the k-state dirichlet distribution for k = 2.

binomial

the binomial distribution gives the id203 of observing m occurrences of x = 1
in a set of n samples from a bernoulli distribution, where the id203 of observ-
ing x = 1 is        [0, 1].

(cid:15)

(cid:16)

  m(1       )n   m

bin(m|n,   ) =

n
m
e[m] = n   
var[m] = n   (1       )
mode[m] = (cid:24)(n + 1)  (cid:25)

(b.11)
(b.12)
(b.13)
where (cid:24)(n + 1)  (cid:25) denotes the largest integer that is less than or equal to (n + 1)  ,
and the quantity

(cid:15)

(cid:16)

(b.10)

n
m

n!

=

m!(n     m)!

(b.14)

denotes the number of ways of choosing m objects out of a total of n identical
objects. here m!, pronounced    factorial m   , denotes the product m    (m     1)   
. . . ,  2    1. the particular case of the binomial distribution for n = 1 is known as
the bernoulli distribution, and for large n the binomial distribution is approximately
gaussian. the conjugate prior for    is the beta distribution.

b. id203 distributions

687

dirichlet
the dirichlet is a multivariate distribution over k random variables 0 (cid:1)   k (cid:1) 1,
where k = 1, . . . , k, subject to the constraints

0 (cid:1)   k (cid:1) 1,

  k = 1.

(b.15)

k(cid:2)

k=1

denoting    = (  1, . . . ,   k)t and    = (  1, . . . ,   k)t, we have

    k   1

k

k=1

dir(  |  ) = c(  )

e[  k] =

var[  k] =

k(cid:3)
  k(cid:4)  
  k((cid:4)         k)
(cid:4)  2((cid:4)   + 1)
(cid:4)  2((cid:4)   + 1)
(cid:4)       k
e[ln   k] =   (  k)       ((cid:4)  )
h[  ] =     k(cid:2)

  k     1

cov[  j  k] =       j  k

mode[  k] =

(  k     1){  (  k)       ((cid:4)  )}     ln c(  )

k=1

c(  ) =

  ((cid:4)  )
k(cid:2)

  k.

(cid:4)   =

  (  1)         (  k)

k=1

  (a)     d
da

ln   (a)

where

and

here

is known as the digamma function (abramowitz and stegun, 1965). the parameters
  k are subject to the constraint   k > 0 in order to ensure that the distribution can be
normalized.

the dirichlet forms the conjugate prior for the multinomial distribution and rep-
resents a generalization of the beta distribution. in this case, the parameters   k can
be interpreted as effective numbers of observations of the corresponding values of
the k-dimensional binary observation vector x. as with the beta distribution, the
dirichlet has    nite density everywhere provided   k (cid:2) 1 for all k.

(b.16)

(b.17)

(b.18)

(b.19)

(b.20)

(b.21)

(b.22)

(b.23)

(b.24)

(b.25)

688

b. id203 distributions

gamma

the gamma is a id203 distribution over a positive random variable    > 0
governed by parameters a and b that are subject to the constraints a > 0 and b > 0
to ensure that the distribution can be normalized.

gam(  |a, b) =

1

  (a) ba   a   1e

   b  

e[  ] = a
b
var[  ] = a
b2
mode[  ] = a     1
e[ln   ] =   (a)     ln b
h[  ] = ln   (a)     (a     1)  (a)     ln b + a

for    (cid:2) 1

b

(b.26)

(b.27)

(b.28)

(b.29)

(b.30)
(b.31)
where   (  ) is the digamma function de   ned by (b.25). the gamma distribution is
the conjugate prior for the precision (inverse variance) of a univariate gaussian. for
a (cid:2) 1 the density is everywhere    nite, and the special case of a = 1 is known as the
exponential distribution.

gaussian

the gaussian is the most widely used distribution for continuous variables. it is also
known as the normal distribution. in the case of a single variable x     (      ,   ) it is
governed by two parameters, the mean        (      ,   ) and the variance   2 > 0.

(cid:12)

(cid:13)

n (x|  ,   2) =

1

(2    2)1/2 exp

    1
2  2 (x       )2

e[x] =   
var[x] =   2
mode[x] =   
1
2

h[x] =

ln   2 +

1
2

(1 + ln(2  )) .

(b.32)

(b.33)
(b.34)
(b.35)

(b.36)

the inverse of the variance    = 1/  2 is called the precision, and the square root
of the variance    is called the standard deviation. the conjugate prior for    is the
gaussian, and the conjugate prior for    is the gamma distribution. if both    and   
are unknown, their joint conjugate prior is the gaussian-gamma distribution.
for a d-dimensional vector x, the gaussian is governed by a d-dimensional
mean vector    and a d    d covariance matrix    that must be symmetric and

b. id203 distributions

689

positive-de   nite.

n (x|  ,   ) =

1

(2  )d/2

1

|  |1/2 exp

(cid:12)

   1
2

(cid:13)

   1(x       )

(x       )t  

(b.37)

(b.38)
(b.39)
(b.40)

(b.42)
(b.43)

(b.44)
(b.45)

e[x] =   
cov[x] =   
mode[x] =   
1
2

h[x] =

ln|  | + d
2

(1 + ln(2  )) .

(b.41)
   1 is the precision matrix, which is also
the inverse of the covariance matrix    =   
symmetric and positive de   nite. averages of random variables tend to a gaussian, by
the central limit theorem, and the sum of two gaussian variables is again gaussian.
the gaussian is the distribution that maximizes the id178 for a given variance
(or covariance). any linear transformation of a gaussian random variable is again
gaussian. the marginal distribution of a multivariate gaussian with respect to a
subset of the variables is itself gaussian, and similarly the conditional distribution is
also gaussian. the conjugate prior for    is the gaussian, the conjugate prior for   
is the wishart, and the conjugate prior for (  ,   ) is the gaussian-wishart.

if we have a marginal gaussian distribution for x and a conditional gaussian

distribution for y given x in the form

p(x) = n (x|  ,   
p(y|x) = n (y|ax + b, l   1)

   1)

then the marginal distribution of y, and the conditional distribution of x given y, are
given by

p(y) = n (y|a   + b, l   1 + a  
   1at)
p(x|y) = n (x|  {atl(y     b) +     },   )

where

if we have a joint gaussian distribution n (x|  ,   ) with          

(b.46)
   1 and we

de   ne the following partitions

   = (   + atla)   1.

(cid:15)

(cid:16)
(cid:16)

x =

xa
xb

(cid:15)

   =

  aa   ab
  ba   bb

,

   =

,    =

(cid:16)

(cid:15)
(cid:15)

  a
  b

  aa   ab
  ba   bb

then the conditional distribution p(xa|xb) is given by
   1
aa )

p(xa|xb) = n (x|  a|b,   

  a|b =   a       

aa   ab(xb       b)
   1

(cid:16)

(b.47)

(b.48)

(b.49)
(b.50)

690

b. id203 distributions

and the marginal distribution p(xa) is given by

p(xa) = n (xa|  a,   aa).

(b.51)

gaussian-gamma
this is the conjugate prior distribution for a univariate gaussian n (x|  ,   
   1) in
which the mean    and the precision    are both unknown and is also called the
normal-gamma distribution. it comprises the product of a gaussian distribution for
  , whose precision is proportional to   , and a gamma distribution over   .

p(  ,   |  0,   , a, b) = n(cid:10)

  |  o, (    )   1

gam(  |a, b).

(b.52)

(cid:11)

gaussian-wishart
this is the conjugate prior distribution for a multivariate gaussian n (x|  ,   ) in
which both the mean    and the precision    are unknown, and is also called the
normal-wishart distribution. it comprises the product of a gaussian distribution for
  , whose precision is proportional to   , and a wishart distribution over   .

p(  ,   |  0,   , w,   ) = n(cid:10)

(cid:11) w(  |w,   ).

  |  0, (    )   1

(b.53)

for the particular case of a scalar x, this is equivalent to the gaussian-gamma distri-
bution.

multinomial

if we generalize the bernoulli distribution to an k-dimensional binary variable x
with components xk     {0, 1} such that
k xk = 1, then we obtain the following
discrete distribution

(cid:5)
k(cid:14)

p(x) =

  xk
k

k=1
e[xk] =   k
var[xk] =   k(1       k)

cov[xjxk] = ijk  k

h[x] =     m(cid:2)

  k ln   k

k=1

(b.54)

(b.55)
(b.56)
(b.57)

(b.58)

b. id203 distributions

691

where ijk is the j, k element of the identity matrix. because p(xk = 1) =   k, the
parameters must satisfy 0 (cid:1)   k (cid:1) 1 and

k   k = 1.

the multinomial distribution is a multivariate generalization of the binomial and
gives the distribution over counts mk for a k-state discrete variable to be in state k
given a total number of observations n.

(cid:15)

(cid:16) m(cid:14)

mult(m1, m2, . . . , mk|  , n) =

n

m1m2 . . . mm

k=1

e[mk] = n   k
var[mk] = n   k(1       k)

cov[mjmk] =    n   j  k

where    = (  1, . . . ,   k)t, and the quantity

(cid:15)

n

m1m2 . . . mk

=

n!

m1! . . . mk!

  mk
k

(b.59)

(b.60)
(b.61)
(b.62)

(b.63)

(cid:5)

(cid:16)

(cid:5)
gives the number of ways of taking n identical objects and assigning mk of them to
bin k for k = 1, . . . , k. the value of   k gives the id203 of the random variable
taking state k, and so these parameters are subject to the constraints 0 (cid:1)   k (cid:1) 1
k   k = 1. the conjugate prior distribution for the parameters {  k} is the
and
dirichlet.

normal

the normal distribution is simply another name for the gaussian. in this book, we
use the term gaussian throughout, although we retain the conventional use of the
symbol n to denote this distribution. for consistency, we shall refer to the normal-
gamma distribution as the gaussian-gamma distribution, and similarly the normal-
wishart is called the gaussian-wishart.

student   s t

this distribution was published by william gosset in 1908, but his employer, gui-
ness breweries, required him to publish under a pseudonym, so he chose    student   .
in the univariate form, student   s t-distribution is obtained by placing a conjugate
gamma prior over the precision of a univariate gaussian distribution and then inte-
grating out the precision variable. it can therefore be viewed as an in   nite mixture

692

b. id203 distributions

of gaussians having the same mean but different variances.

st(x|  ,   ,   ) =

e[x] =   
1
  
mode[x] =   .

var[x] =

  (  /2 + 1/2)

  (  /2)
for    > 1
  
       2

for    > 2

(cid:15)

(cid:16)1/2(cid:29)

  
    

1 +   (x       )2

  

(cid:30)     /2   1/2

(b.64)

(b.65)

(b.66)

(b.67)

here    > 0 is called the number of degrees of freedom of the distribution. the
particular case of    = 1 is called the cauchy distribution.

for a d-dimensional variable x, student   s t-distribution corresponds to marginal-
izing the precision matrix of a multivariate gaussian with respect to a conjugate
wishart prior and takes the form

(cid:29)

(cid:30)     /2   d/2

st(x|  ,   ,   ) =

  (  /2 + d/2)

  (  /2)

|  |1/2
(    )d/2

1 +

   2
  

e[x] =    for    > 1

cov[x] =

mode[x] =   

   1

  

  
       2

for    > 2

where    2 is the squared mahalanobis distance de   ned by
   2 = (x       )t  (x       ).

(b.72)
in the limit           , the t-distribution reduces to a gaussian with mean    and pre-
cision   . student   s t-distribution provides a generalization of the gaussian whose
maximum likelihood parameter values are robust to outliers.

uniform

this is a simple distribution for a continuous variable x de   ned over a    nite interval
x     [a, b] where b > a.

u(x|a, b) =

e[x] =

1
b     a
(b + a)
(b     a)2
var[x] =
h[x] = ln(b     a).

12

2

(b.76)
if x has distribution u(x|0, 1), then a + (b     a)x will have distribution u(x|a, b).

(b.68)

(b.69)

(b.70)

(b.71)

(b.73)

(b.74)

(b.75)

b. id203 distributions

693

von mises

the von mises distribution, also known as the circular normal or the circular gaus-
sian, is a univariate gaussian-like periodic distribution for a variable        [0, 2  ).

p(  |  0, m) =

1

2  i0(m)

exp{m cos(         0)}

(b.77)

where i0(m) is the zeroth-order bessel function of the    rst kind. the distribution
has period 2   so that p(   + 2  ) = p(  ) for all   . care must be taken in interpret-
ing this distribution because simple expectations will be dependent on the (arbitrary)
choice of origin for the variable   . the parameter   0 is analogous to the mean of a
univariate gaussian, and the parameter m > 0, known as the concentration param-
eter, is analogous to the precision (inverse variance). for large m, the von mises
distribution is approximately a gaussian centred on   0.

wishart

the wishart distribution is the conjugate prior for the precision matrix of a multi-
variate gaussian.

(cid:15)

(cid:16)
   1
2 tr(w   1  )
(cid:16)(cid:23)   1
(cid:15)
d(cid:14)

   + 1     i

w(  |w,   ) = b(w,   )|  |(     d   1)/2 exp

where

(cid:22)

b(w,   )     |w|     /2

2  d/2   d(d   1)/4

  

e[  ] =   w

e [ln|  |] =

(cid:15)

d(cid:2)

i=1

  

   + 1     i

2

2

(cid:16)

i=1

+ d ln 2 + ln|w|

(b.78)

(b.79)

(b.80)

(b.81)

h[  ] =     ln b(w,   )     (       d     1)

2

(b.82)
where w is a d    d symmetric, positive de   nite matrix, and   (  ) is the digamma
function de   ned by (b.25). the parameter    is called the number of degrees of
freedom of the distribution and is restricted to    > d     1 to ensure that the gamma
function in the id172 factor is well-de   ned. in one dimension, the wishart
reduces to the gamma distribution gam(  |a, b) given by (b.26) with parameters
a =   /2 and b = 1/2w .

e [ln|  |] +   d
2

appendix c. properties of matrices

in this appendix, we gather together some useful properties and identities involving
matrices and determinants. this is not intended to be an introductory tutorial, and
it is assumed that the reader is already familiar with basic id202. for some
results, we indicate how to prove them, whereas in more complex cases we leave
the interested reader to refer to standard textbooks on the subject. in all cases, we
assume that inverses exist and that matrix dimensions are such that the formulae
are correctly de   ned. a comprehensive discussion of id202 can be found in
golub and van loan (1996), and an extensive collection of matrix properties is given
by l  utkepohl (1996). matrix derivatives are discussed in magnus and neudecker
(1999).

basic matrix identities

a matrix a has elements aij where i indexes the rows, and j indexes the columns.
we use in to denote the n    n identity matrix (also called the unit matrix), and
where there is no ambiguity over dimensionality we simply use i. the transpose
matrix at has elements (at)ij = aji. from the de   nition of transpose, we have

(c.1)
which can be veri   ed by writing out the indices. the inverse of a, denoted a   1,
satis   es

(ab)t = btat

aa   1 = a   1a = i.

because abb   1a   1 = i, we have

also we have

(ab)   1 = b   1a   1.

(cid:10)

(cid:10)

(cid:11)   1 =

at

(cid:11)t

a   1

(c.2)

(c.3)

(c.4)

695

696

c. properties of matrices

which is easily proven by taking the transpose of (c.2) and applying (c.1).

a useful identity involving matrix inverses is the following

(p   1 + btr   1b)   1btr   1 = pbt(bpbt + r)   1.

(c.5)

which is easily veri   ed by right multiplying both sides by (bpbt + r). suppose
that p has dimensionality n    n while r has dimensionality m    m, so that b is
m    n. then if m (cid:5) n, it will be much cheaper to evaluate the right-hand side of
(c.5) than the left-hand side. a special case that sometimes arises is

(i + ab)   1a = a(i + ba)   1.

another useful identity involving inverses is the following:

(a + bd   1c)   1 = a   1     a   1b(d + ca   1b)   1ca   1

(c.6)

(c.7)

(cid:5)

which is known as the woodbury identity and which can be veri   ed by multiplying
both sides by (a + bd   1c). this is useful, for instance, when a is large and
diagonal, and hence easy to invert, while b has many rows but few columns (and
conversely for c) so that the right-hand side is much cheaper to evaluate than the
left-hand side.
a set of vectors {a1, . . . , an} is said to be linearly independent if the relation
n   nan = 0 holds only if all   n = 0. this implies that none of the vectors
can be expressed as a linear combination of the remainder. the rank of a matrix is
the maximum number of linearly independent rows (or equivalently the maximum
number of linearly independent columns).

traces and determinants

trace and determinant apply to square matrices. the trace tr(a) of a matrix a
is de   ned as the sum of the elements on the leading diagonal. by writing out the
indices, we see that

tr(ab) = tr(ba).

(c.8)

by applying this formula multiple times to the product of three matrices, we see that

tr(abc) = tr(cab) = tr(bca)

(c.9)

which is known as the cyclic property of the trace operator and which clearly extends
to the product of any number of matrices. the determinant |a| of an n    n matrix
a is de   ned by

(cid:2)

|a| =

(  1)a1i1a2i2        an in

(c.10)

in which the sum is taken over all products consisting of precisely one element from
each row and one element from each column, with a coef   cient +1 or    1 according

c. properties of matrices

697

to whether the permutation i1i2 . . . in is even or odd, respectively. note that |i| = 1.
thus, for a 2    2 matrix, the determinant takes the form

(cid:7)(cid:7)(cid:7)(cid:7) a11 a12

a21 a22

(cid:7)(cid:7)(cid:7)(cid:7) = a11a22     a12a21.

|a| =

the determinant of a product of two matrices is given by

as can be shown from (c.10). also, the determinant of an inverse matrix is given by

|ab| = |a||b|

1
|a|

(cid:7)(cid:7) =
(cid:7)(cid:7)a   1
(cid:7)(cid:7)im + atb
(cid:7)(cid:7) =
(cid:7)(cid:7)in + abt
(cid:7)(cid:7) = 1 + atb
(cid:7)(cid:7)in + abt

(cid:7)(cid:7) .

which can be shown by taking the determinant of (c.2) and applying (c.12).

if a and b are matrices of size n    m, then

a useful special case is

where a and b are n-dimensional column vectors.

matrix derivatives

sometimes we need to consider derivatives of vectors and matrices with respect to
scalars. the derivative of a vector a with respect to a scalar x is itself a vector whose
components are given by

   a
   x

=    ai
   x

(c.16)

with an analogous de   nition for the derivative of a matrix. derivatives with respect
to vectors and matrices can also be de   ned, for instance

(cid:15)

(cid:15)
(cid:15)

(cid:16)

i

i

(cid:16)
(cid:16)

   x
   a

=    x
   ai

and similarly

the following is easily proven by writing out the components

   a
   b

(cid:11)

=    ai
   bj

.

ij

(cid:10)

(cid:11)

(cid:10)

   
   x

xta

=    
   x

atx

= a.

(c.11)

(c.12)

(c.13)

(c.14)

(c.15)

(c.17)

(c.18)

(c.19)

698

c. properties of matrices

similarly

   
   x

(ab) =    a
   x

b + a    b
   x .

(c.20)

(cid:10)

(cid:11)

the derivative of the inverse of a matrix can be expressed as

a   1

   
   x

=    a   1    a
   x

a   1

(c.21)
as can be shown by differentiating the equation a   1a = i using (c.20) and then
right multiplying by a   1. also

(cid:15)

(cid:16)

ln|a| = tr

   
   x

a   1    a
   x

which we shall prove later. if we choose x to be one of the elements of a, we have

   

   aij

tr (ab) = bji

(c.23)

as can be seen by writing out the matrices using index notation. we can write this
result more compactly in the form

with this notation, we have the following properties

tr (ab) = bt.

   
   a

(cid:10)

(cid:11)

   
   a

tr

atb

= b

   
   a

tr(a) = i

tr(abat) = a(b + bt)

   
   a

(cid:11)t

(cid:10)

a   1

which can again be proven by writing out the matrix indices. we also have

ln|a| =

   
   a

which follows from (c.22) and (c.26).

eigenvector equation
for a square matrix a of size m    m, the eigenvector equation is de   ned by

aui =   iui

(c.29)

(c.22)

(c.24)

(c.25)

(c.26)

(c.27)

(c.28)

c. properties of matrices

699

for i = 1, . . . , m, where ui is an eigenvector and   i is the corresponding eigenvalue.
this can be viewed as a set of m simultaneous homogeneous linear equations, and
the condition for a solution is that

|a       ii| = 0

(c.30)

which is known as the characteristic equation. because this is a polynomial of order
m in   i, it must have m solutions (though these need not all be distinct). the rank
of a is equal to the number of nonzero eigenvalues.

of particular interest are symmetric matrices, which arise as covariance ma-
trices, kernel matrices, and hessians. symmetric matrices have the property that
aij = aji, or equivalently at = a. the inverse of a symmetric matrix is also sym-
metric, as can be seen by taking the transpose of a   1a = i and using aa   1 = i
together with the symmetry of i.

in general, the eigenvalues of a matrix are complex numbers, but for symmetric
matrices the eigenvalues   i are real. this can be seen by    rst left multiplying (c.29)
by (u(cid:3)

i )t, where (cid:11) denotes the complex conjugate, to give

(u(cid:3)

i )t aui =   i (u(cid:3)

i )t ui.

(c.31)

next we take the complex conjugate of (c.29) and left multiply by ut

i to give

i au(cid:3)
ut

i =   (cid:3)

i ut

i u(cid:3)
i .

(c.32)

where we have used a(cid:3) = a because we consider only real matrices a. taking
the transpose of the second of these equations, and using at = a, we see that the
left-hand sides of the two equations are equal, and hence that   (cid:3)
i =   i and so   i
must be real.

the eigenvectors ui of a real symmetric matrix can be chosen to be orthonormal

(i.e., orthogonal and of unit length) so that

ut
i uj = iij

(c.33)

where iij are the elements of the identity matrix i. to show this, we    rst left multiply
(c.29) by ut

j to give

and hence, by exchange of indices, we have

ut
j aui =   iut

j ui

ut
i auj =   jut

i uj.

(c.34)

(c.35)

we now take the transpose of the second equation and make use of the symmetry
property at = a, and then subtract the two equations to give

(  i       j) ut
hence, for   i (cid:6)=   j, we have ut
i uj = 0, and hence ui and uj are orthogonal. if the
two eigenvalues are equal, then any linear combination   ui +   uj is also an eigen-
vector with the same eigenvalue, so we can select one linear combination arbitrarily,

i uj = 0.

(c.36)

700

c. properties of matrices

and then choose the second to be orthogonal to the    rst (it can be shown that the de-
generate eigenvectors are never linearly dependent). hence the eigenvectors can be
chosen to be orthogonal, and by normalizing can be set to unit length. because there
are m eigenvalues, the corresponding m orthogonal eigenvectors form a complete
set and so any m-dimensional vector can be expressed as a linear combination of
the eigenvectors.
we can take the eigenvectors ui to be the columns of an m    m matrix u,

which from orthonormality satis   es

utu = i.

(c.37)

such a matrix is said to be orthogonal. interestingly, the rows of this matrix are also
orthogonal, so that uut = i. to show this, note that (c.37) implies utuu   1 =
u   1 = ut and so uu   1 = uut = i. using (c.12), it also follows that |u| = 1.

the eigenvector equation (c.29) can be expressed in terms of u in the form

(c.38)
where    is an m    m diagonal matrix whose diagonal elements are given by the
eigenvalues   i.

au = u  

if we consider a column vector x that is transformed by an orthogonal matrix u

to give a new vector

then the length of the vector is preserved because

(cid:4)x = ux

(cid:4)xt(cid:4)x = xtutux = xtx
(cid:4)xt(cid:4)y = xtutuy = xty.

and similarly the angle between any two such vectors is preserved because

(c.39)

(c.40)

(c.41)

thus, multiplication by u can be interpreted as a rigid rotation of the coordinate
system.

from (c.38), it follows that

utau =   

(c.42)

and because    is a diagonal matrix, we say that the matrix a is diagonalized by the
matrix u. if we left multiply by u and right multiply by ut, we obtain

(c.43)
taking the inverse of this equation, and using (c.3) together with u   1 = ut, we
have

a = u  ut

a   1 = u  

   1ut.

(c.44)

(c.45)

(c.46)

(c.47)

c. properties of matrices

701

these last two equations can also be written in the form

a =

a   1 =

  iuiut
i

1
  i

uiut
i .

|a| =

  i.

i=1

m(cid:2)
m(cid:2)

i=1

i=1

m(cid:14)

m(cid:2)

if we take the determinant of (c.43), and use (c.12), we obtain

similarly, taking the trace of (c.43), and using the cyclic property (c.8) of the trace
operator together with utu = i, we have

tr(a) =

  i.

i=1

(c.48)

we leave it as an exercise for the reader to verify (c.22) by making use of the results
(c.33), (c.45), (c.46), and (c.47).
a matrix a is said to be positive de   nite, denoted by a (cid:7) 0, if wtaw > 0 for
all values of the vector w. equivalently, a positive de   nite matrix has   i > 0 for all
of its eigenvalues (as can be seen by setting w to each of the eigenvectors in turn,
and by noting that an arbitrary vector can be expanded as a linear combination of the
eigenvectors). note that positive de   nite is not the same as all the elements being

positive. for example, the matrix (cid:15)

(cid:16)

(c.49)
has eigenvalues   1 (cid:8) 5.37 and   2 (cid:8)    0.37. a matrix is said to be positive semidef-
inite if wtaw (cid:2) 0 holds for all values of w, which is denoted a (cid:9) 0, and is
equivalent to   i (cid:2) 0.

1 2
3 4

appendix d. calculus of variations

we can think of a function y(x) as being an operator that, for any input value x,
returns an output value y. in the same way, we can de   ne a functional f [y] to be
an operator that takes a function y(x) and returns an output value f . an example of
a functional is the length of a curve drawn in a two-dimensional plane in which the
path of the curve is de   ned in terms of a function. in the context of machine learning,
a widely used functional is the id178 h[x] for a continuous variable x because, for
any choice of id203 density function p(x), it returns a scalar value representing
the id178 of x under that density. thus the id178 of p(x) could equally well have
been written as h[p].

a common problem in conventional calculus is to    nd a value of x that max-
imizes (or minimizes) a function y(x). similarly, in the calculus of variations we
seek a function y(x) that maximizes (or minimizes) a functional f [y]. that is, of all
possible functions y(x), we wish to    nd the particular function for which the func-
tional f [y] is a maximum (or minimum). the calculus of variations can be used, for
instance, to show that the shortest path between two points is a straight line or that
the maximum id178 distribution is a gaussian.

if we weren   t familiar with the rules of ordinary calculus, we could evaluate a
conventional derivative dy/ dx by making a small change   to the variable x and
then expanding in powers of  , so that

y(x +  ) = y(x) +

(d.1)
and    nally taking the limit       0. similarly, for a function of several variables
y(x1, . . . , xd), the corresponding partial derivatives are de   ned by

  + o( 2)

dy
dx

y(x1 +  1, . . . , xd +  d) = y(x1, . . . , xd) +

   y
   xi

 i + o( 2).

(d.2)

the analogous de   nition of a functional derivative arises when we consider how
much a functional f [y] changes when we make a small change    (x) to the function

703

d(cid:2)

i=1

704

d. calculus of variations

figure d.1 a functional derivative can be de   ned by
considering how the value of a functional
f [y] changes when the function y(x) is
changed to y(x) +    (x) where   (x) is an
arbitrary function of x.

y(x)

y(x) +    (x)

x

y(x), where   (x) is an arbitrary function of x, as illustrated in figure d.1. we denote
the functional derivative of e[f] with respect to f(x) by   f/  f(x), and de   ne it by
the following relation:

(cid:6)

f [y(x) +    (x)] = f [y(x)] +  

  f
  y(x)   (x) dx + o( 2).

(d.3)

this can be seen as a natural extension of (d.2) in which f [y] now depends on a
continuous set of variables, namely the values of y at all points x. requiring that the
functional be stationary with respect to small variations in the function y(x) gives

(cid:6)

  e
  y(x)   (x) dx = 0.

(d.4)

because this must hold for an arbitrary choice of   (x), it follows that the functional
derivative must vanish. to see this, imagine choosing a perturbation   (x) that is zero

everywhere except in the neighbourhood of a point(cid:1)x, in which case the functional
derivative must be zero at x = (cid:1)x. however, because this must be true for every
choice of(cid:1)x, the functional derivative must vanish for all values of x.

consider a functional that is de   ned by an integral over a function g(y, y

(cid:1)

that depends on both y(x) and its derivative y
dence on x

f [y] =

g (y(x), y

, x)
(cid:1)(x) as well as having a direct depen-
(cid:1)(x), x) dx

(d.5)

where the value of y(x) is assumed to be    xed at the boundary of the region of
integration (which might be at in   nity). if we now consider variations in the function
y(x), we obtain

f [y(x) +    (x)] = f [y(x)] +  

   g
   y

  (x) +    g
   y(cid:1)   

(cid:1)(x)

dx + o( 2).

(d.6)

(cid:13)

we now have to cast this in the form (d.3). to do so, we integrate the second term by
parts and make use of the fact that   (x) must vanish at the boundary of the integral
(because y(x) is    xed at the boundary). this gives
    d
dx

f [y(x) +    (x)] = f [y(x)] +  

  (x) dx + o( 2) (d.7)

(cid:6) (cid:12)

(cid:16)(cid:13)

   g
   y(cid:1)

   g
   y

(cid:15)

(cid:6)

(cid:6) (cid:12)

d. calculus of variations

705

from which we can read off the functional derivative by comparison with (d.3).
requiring that the functional derivative vanishes then gives

(cid:15)

(cid:16)

   g
   y

    d
dx

   g
   y(cid:1)

= 0

which are known as the euler-lagrange equations. for example, if

g = y(x)2 + (y

(cid:1)(x))2

then the euler-lagrange equations take the form

y(x)     d2y

dx2 = 0.

(d.8)

(d.9)

(d.10)

this second order differential equation can be solved for y(x) by making use of the
boundary conditions on y(x).

often, we consider functionals de   ned by integrals whose integrands take the
form g(y, x) and that do not depend on the derivatives of y(x). in this case, station-
arity simply requires that    g/   y(x) = 0 for all values of x.

if we are optimizing a functional with respect to a id203 distribution, then
we need to maintain the id172 constraint on the probabilities. this is often
most conveniently done using a lagrange multiplier, which then allows an uncon-
strained optimization to be performed.

the extension of the above results to a multidimensional variable x is straight-
forward. for a more comprehensive discussion of the calculus of variations, see
sagan (1969).

appendix e

appendix e. lagrange multipliers

lagrange multipliers, also sometimes called undetermined multipliers, are used to
   nd the stationary points of a function of several variables subject to one or more
constraints.

consider the problem of    nding the maximum of a function f(x1, x2) subject to

a constraint relating x1 and x2, which we write in the form

g(x1, x2) = 0.

(e.1)

one approach would be to solve the constraint equation (e.1) and thus express x2 as
a function of x1 in the form x2 = h(x1). this can then be substituted into f(x1, x2)
to give a function of x1 alone of the form f(x1, h(x1)). the maximum with respect
to x1 could then be found by differentiation in the usual way, to give the stationary
value x(cid:3)

1, with the corresponding value of x2 given by x(cid:3)

2 = h(x(cid:3)

1).

one problem with this approach is that it may be dif   cult to    nd an analytic
solution of the constraint equation that allows x2 to be expressed as an explicit func-
tion of x1. also, this approach treats x1 and x2 differently and so spoils the natural
symmetry between these variables.

a more elegant, and often simpler, approach is based on the introduction of a
parameter    called a lagrange multiplier. we shall motivate this technique from
a geometrical perspective. consider a d-dimensional variable x with components
x1, . . . , xd. the constraint equation g(x) = 0 then represents a (d   1)-dimensional
surface in x-space as indicated in figure e.1.
we    rst note that at any point on the constraint surface the gradient    g(x) of
the constraint function will be orthogonal to the surface. to see this, consider a point
x that lies on the constraint surface, and consider a nearby point x +   that also lies
on the surface. if we make a taylor expansion around x, we have

g(x +  ) (cid:8) g(x) +  t   g(x).

(e.2)

because both x and x+   lie on the constraint surface, we have g(x) = g(x+  ) and
hence  t   g(x) (cid:8) 0. in the limit (cid:11) (cid:11)     0 we have  t   g(x) = 0, and because   is

707

708

e. lagrange multipliers

figure e.1 a geometrical picture of

the technique of la-
grange multipliers in which we seek to maximize a
function f (x), subject to the constraint g(x) = 0.
if x is d dimensional, the constraint g(x) = 0 cor-
responds to a subspace of dimensionality d     1,
indicated by the red curve. the problem can
be solved by optimizing the lagrangian function
l(x,   ) = f (x) +   g(x).

   f(x)

xa

   g(x)

g(x) = 0

then parallel to the constraint surface g(x) = 0, we see that the vector    g is normal
to the surface.
next we seek a point x(cid:3) on the constraint surface such that f(x) is maximized.
such a point must have the property that the vector    f(x) is also orthogonal to the
constraint surface, as illustrated in figure e.1, because otherwise we could increase
the value of f(x) by moving a short distance along the constraint surface. thus    f
and    g are parallel (or anti-parallel) vectors, and so there must exist a parameter   
such that
where    (cid:6)= 0 is known as a lagrange multiplier. note that    can have either sign.
at this point, it is convenient to introduce the lagrangian function de   ned by

   f +      g = 0

(e.3)

l(x,   )     f(x) +   g(x).

(e.4)
the constrained stationarity condition (e.3) is obtained by setting    xl = 0. fur-
thermore, the condition    l/      = 0 leads to the constraint equation g(x) = 0.

thus to    nd the maximum of a function f(x) subject to the constraint g(x) = 0,
we de   ne the lagrangian function given by (e.4) and we then    nd the stationary
point of l(x,   ) with respect to both x and   . for a d-dimensional vector x, this
gives d + 1 equations that determine both the stationary point x(cid:3) and the value of   .
if we are only interested in x(cid:3), then we can eliminate    from the stationarity equa-
tions without needing to    nd its value (hence the term    undetermined multiplier   ).
as a simple example, suppose we wish to    nd the stationary point of the function
f(x1, x2) = 1     x2
2 subject to the constraint g(x1, x2) = x1 + x2     1 = 0, as
illustrated in figure e.2. the corresponding lagrangian function is given by

1     x2
l(x,   ) = 1     x2

1     x2

2 +   (x1 + x2     1).

the conditions for this lagrangian to be stationary with respect to x1, x2, and    give
the following coupled equations:

   2x1 +    = 0
   2x2 +    = 0
x1 + x2     1 = 0.

(e.5)

(e.6)
(e.7)
(e.8)

e. lagrange multipliers

709

figure e.2 a simple example of the use of lagrange multipli-
ers in which the aim is to maximize f (x1, x2) =
1     x2
2 subject to the constraint g(x1, x2) = 0
where g(x1, x2) = x1 + x2     1. the circles show
contours of the function f (x1, x2), and the diagonal
line shows the constraint surface g(x1, x2) = 0.

1     x2

x2

(x(cid:3)

1, x(cid:3)
2)

x1

g(x1, x2) = 0

solution of these equations then gives the stationary point as (x(cid:3)
the corresponding value for the lagrange multiplier is    = 1.

1, x(cid:3)

2) = ( 1

2 , 1

2), and

so far, we have considered the problem of maximizing a function subject to an
equality constraint of the form g(x) = 0. we now consider the problem of maxi-
mizing f(x) subject to an inequality constraint of the form g(x) (cid:2) 0, as illustrated
in figure e.3.

there are now two kinds of solution possible, according to whether the con-
strained stationary point lies in the region where g(x) > 0, in which case the con-
straint is inactive, or whether it lies on the boundary g(x) = 0, in which case the
constraint is said to be active. in the former case, the function g(x) plays no role
and so the stationary condition is simply    f(x) = 0. this again corresponds to
a stationary point of the lagrange function (e.4) but this time with    = 0. the
latter case, where the solution lies on the boundary, is analogous to the equality con-
straint discussed previously and corresponds to a stationary point of the lagrange
function (e.4) with    (cid:6)= 0. now, however, the sign of the lagrange multiplier is
crucial, because the function f(x) will only be at a maximum if its gradient is ori-
ented away from the region g(x) > 0, as illustrated in figure e.3. we therefore have
   f(x) =         g(x) for some value of    > 0.

for either of these two cases, the product   g(x) = 0. thus the solution to the

figure e.3 illustration of
f (x) subject
g(x) (cid:2) 0.

the problem of maximizing
to the inequality constraint

   f(x)

xa

   g(x)

xb

g(x) > 0

g(x) = 0

710

e. lagrange multipliers

problem of maximizing f(x) subject to g(x) (cid:2) 0 is obtained by optimizing the
lagrange function (e.4) with respect to x and    subject to the conditions

g(x) (cid:2) 0
   (cid:2) 0
  g(x) = 0

(e.9)
(e.10)
(e.11)

these are known as the karush-kuhn-tucker (kkt) conditions (karush, 1939; kuhn
and tucker, 1951).
note that if we wish to minimize (rather than maximize) the function f(x) sub-
ject to an inequality constraint g(x) (cid:2) 0, then we minimize the lagrangian function
l(x,   ) = f(x)       g(x) with respect to x, again subject to    (cid:2) 0.

finally, it is straightforward to extend the technique of lagrange multipliers to
the case of multiple equality and inequality constraints. suppose we wish to maxi-
mize f(x) subject to gj(x) = 0 for j = 1, . . . , j, and hk(x) (cid:2) 0 for k = 1, . . . , k.
we then introduce lagrange multipliers {  j} and {  k}, and then optimize the la-
grangian function given by

j(cid:2)

k(cid:2)

l(x,{  j},{  k}) = f(x) +

  jgj(x) +

  khk(x)

(e.12)

appendix d

subject to   k (cid:2) 0 and   khk(x) = 0 for k = 1, . . . , k. extensions to constrained
functional derivatives are similarly straightforward. for a more detailed discussion
of the technique of lagrange multipliers, see nocedal and wright (1999).

j=1

k=1

references

711

references

abramowitz, m. and i. a. stegun (1965). handbook

of mathematical functions. dover.

adler, s. l. (1981). over-relaxation method for the
monte carlo evaluation of the partition func-
tion for multiquadratic actions. physical review
d 23, 2901   2904.

ahn, j. h. and j. h. oh (2003). a constrained em
algorithm for principal component analysis. neu-
ral computation 15(1), 57   65.

aizerman, m. a., e. m. braverman, and l. i. rozo-
noer (1964). the id203 problem of pattern
recognition learning and the method of potential
functions. automation and remote control 25,
1175   1190.

akaike, h. (1974). a new look at statistical model
identi   cation. ieee transactions on automatic
control 19, 716   723.

ali, s. m. and s. d. silvey (1966). a general class
of coef   cients of divergence of one distribution
from another. journal of the royal statistical so-
ciety, b 28(1), 131   142.

allwein, e. l., r. e. schapire, and y. singer (2000).
reducing multiclass to binary: a unifying ap-
proach for margin classi   ers. journal of machine
learning research 1, 113   141.

amari, s. (1985). differential-geometrical methods

in statistics. springer.

amari, s., a. cichocki, and h. h. yang (1996). a
new learning algorithm for blind signal separa-
tion. in d. s. touretzky, m. c. mozer, and m. e.
hasselmo (eds.), advances in neural informa-
tion processing systems, volume 8, pp. 757   763.
mit press.

amari, s. i. (1998). natural gradient works ef   -
10,

ciently in learning. neural computation
251   276.

anderson, j. a. and e. rosenfeld (eds.) (1988).
neurocomputing: foundations of research. mit
press.

anderson, t. w. (1963). asymptotic theory for prin-
cipal component analysis. annals of mathemati-
cal statistics 34, 122   148.

andrieu, c., n. de freitas, a. doucet, and m. i. jor-
dan (2003). an introduction to mcmc for ma-
chine learning. machine learning 50, 5   43.

anthony, m. and n. biggs (1992). an introduction
to computational learning theory. cambridge
university press.

attias, h. (1999a). independent factor analysis. neu-

ral computation 11(4), 803   851.

attias, h. (1999b). inferring parameters and struc-
ture of latent variable models by variational
bayes. in k. b. laskey and h. prade (eds.),

712

references

uncertainty in arti   cial intelligence: proceed-
ings of the fifth conference, pp. 21   30. morgan
kaufmann.

bach, f. r. and m. i. jordan (2002). kernel inde-
pendent component analysis. journal of machine
learning research 3, 1   48.

bakir, g. h., j. weston, and b. sch  olkopf (2004).
learning to    nd pre-images. in s. thrun, l. k.
saul, and b. sch  olkopf (eds.), advances in neu-
ral information processing systems, volume 16,
pp. 449   456. mit press.

baldi, p. and s. brunak (2001). bioinformatics: the
machine learning approach (second ed.). mit
press.

baldi, p. and k. hornik (1989). neural networks
and principal component analysis: learning from
examples without
local minima. neural net-
works 2(1), 53   58.

barber, d. and c. m. bishop (1997). bayesian
model comparison by monte carlo chaining. in
m. mozer, m. jordan, and t. petsche (eds.), ad-
vances in neural information processing sys-
tems, volume 9, pp. 333   339. mit press.

barber, d. and c. m. bishop (1998a). ensemble
learning for multi-layer networks. in m. i. jor-
dan, k. j. kearns, and s. a. solla (eds.), ad-
vances in neural information processing sys-
tems, volume 10, pp. 395   401.

barber, d. and c. m. bishop (1998b). ensemble
learning in bayesian neural networks. in c. m.
bishop (ed.), generalization in neural networks
and machine learning, pp. 215   237. springer.

bartholomew, d. j. (1987). latent variable models

and factor analysis. charles grif   n.

basilevsky, a. (1994). statistical factor analysis
and related methods: theory and applications.
wiley.

baum, l. e. (1972). an inequality and associated
maximization technique in statistical estimation
of probabilistic functions of markov processes.
inequalities 3, 1   8.

becker, s. and y. le cun (1989). improving the con-
vergence of back-propagation learning with sec-
ond order methods. in d. touretzky, g. e. hin-
ton, and t. j. sejnowski (eds.), proceedings of
the 1988 connectionist models summer school,
pp. 29   37. morgan kaufmann.

bell, a. j. and t. j. sejnowski (1995). an infor-
mation maximization approach to blind separa-
tion and blind deconvolution. neural computa-
tion 7(6), 1129   1159.

bellman, r. (1961). adaptive control processes: a

guided tour. princeton university press.

bengio, y. and p. frasconi (1995). an input output
id48 architecture. in g. tesauro, d. s. touret-
zky, and t. k. leen (eds.), advances in neural
information processing systems, volume 7, pp.
427   434. mit press.

bennett, k. p. (1992). robust id135
discrimination of two linearly separable sets. op-
timization methods and software 1, 23   34.

berger, j. o. (1985). statistical decision theory and

bayesian analysis (second ed.). springer.

bernardo, j. m. and a. f. m. smith (1994). bayesian

theory. wiley.

berrou, c., a. glavieux, and p. thitimajshima
(1993). near shannon limit error-correcting cod-
ing and decoding: turbo-codes (1). in proceed-
ings icc   93, pp. 1064   1070.

besag, j. (1974). on spatio-temporal models and
markov    elds. in transactions of the 7th prague
conference on id205, statistical
decision functions and random processes, pp.
47   75. academia.

bather, j. (2000). decision theory: an introduction
to id145 and sequential deci-
sions. wiley.

besag, j. (1986). on the statistical analysis of dirty
pictures. journal of the royal statistical soci-
ety b-48, 259   302.

baudat, g. and f. anouar (2000). generalized dis-
criminant analysis using a kernel approach. neu-
ral computation 12(10), 2385   2404.

besag, j., p. j. green, d. hidgon, and k. megersen
(1995). bayesian computation and stochastic
systems. statistical science 10(1), 3   66.

references

713

bishop, c. m. (1991). a fast procedure for retraining
the multilayer id88. international journal
of neural systems 2(3), 229   236.

bishop, c. m. (1992). exact calculation of the hes-
sian matrix for the multilayer id88. neural
computation 4(4), 494   501.

bishop, c. m. (1993). curvature-driven smoothing:
a learning algorithm for feedforward networks.
ieee transactions on neural networks 4(5),
882   884.

bishop, c. m. (1994). novelty detection and neu-
ral network validation. iee proceedings: vision,
image and signal processing 141(4), 217   222.
special issue on applications of neural networks.
bishop, c. m. (1995a). neural networks for pattern

recognition. oxford university press.

bishop, c. m. (1995b). training with noise is equiv-
alent to tikhonov id173. neural compu-
tation 7(1), 108   116.

bishop, c. m. (1999a). bayesian pca. in m. s.
kearns, s. a. solla, and d. a. cohn (eds.), ad-
vances in neural information processing sys-
tems, volume 11, pp. 382   388. mit press.

(1999b). variational principal
bishop, c. m.
components.
in proceedings ninth interna-
tional conference on arti   cial neural networks,
icann   99, volume 1, pp. 509   514. iee.

bishop, c. m. and g. d. james (1993). analysis of
multiphase    ows using dual-energy gamma den-
sitometry and neural networks. nuclear instru-
ments and methods in physics research a327,
580   593.

bishop, c. m. and i. t. nabney (1996). modelling
id155 distributions for periodic
variables. neural computation 8(5), 1123   1133.
bishop, c. m. and i. t. nabney (2008). pattern
recognition and machine learning: a matlab
companion. springer. in preparation.

bishop, c. m., d. spiegelhalter, and j. winn
(2003). vibes: a variational id136 engine
for id110s. in s. becker, s. thrun,
and k. obermeyer (eds.), advances in neural

information processing systems, volume 15, pp.
793   800. mit press.

bishop, c. m. and m. svens  en (2003). bayesian hi-
erarchical mixtures of experts. in u. kjaerulff
and c. meek (eds.), proceedings nineteenth
conference on uncertainty in arti   cial intelli-
gence, pp. 57   64. morgan kaufmann.

bishop, c. m., m. svens  en, and g. e. hinton
(2004). distinguishing text from graphics in on-
line handwritten ink. in f. kimura and h. fu-
jisawa (eds.), proceedings ninth international
workshop on frontiers in handwriting recogni-
tion, iwfhr-9, tokyo, japan, pp. 142   147.

bishop, c. m., m. svens  en, and c. k. i. williams
(1996). em optimization of latent variable den-
sity models. in d. s. touretzky, m. c. mozer,
and m. e. hasselmo (eds.), advances in neural
information processing systems, volume 8, pp.
465   471. mit press.

bishop, c. m., m. svens  en, and c. k. i. williams
(1997a). gtm: a principled alternative to the
self-organizing map. in m. c. mozer, m. i. jor-
dan, and t. petche (eds.), advances in neural
information processing systems, volume 9, pp.
354   360. mit press.

bishop, c. m., m. svens  en, and c. k. i. williams
(1997b). magni   cation factors for the gtm al-
gorithm. in proceedings iee fifth international
conference on arti   cial neural networks, cam-
bridge, u.k., pp. 64   69. institute of electrical
engineers.

bishop, c. m., m. svens  en, and c. k. i. williams
(1998a). developments of the generative to-
pographic mapping. neurocomputing 21, 203   
224.

bishop, c. m., m. svens  en, and c. k. i. williams
(1998b). gtm:
the generative topographic
mapping. neural computation 10(1), 215   234.
bishop, c. m. and m. e. tipping (1998). a hier-
archical latent variable model for data visualiza-
tion. ieee transactions on pattern analysis and
machine intelligence 20(3), 281   293.

714

references

bishop, c. m. and j. winn (2000). non-linear
bayesian image modelling. in proceedings sixth
european conference on id161,
dublin, volume 1, pp. 3   17. springer.

blei, d. m., m. i. jordan, and a. y. ng (2003). hi-
erarchical bayesian models for applications in
information retrieval. in j. m. b. et al. (ed.),
bayesian statistics, 7, pp. 25   43. oxford uni-
versity press.

block, h. d. (1962). the id88:

a model
for brain functioning. reviews of modern
physics 34(1), 123   135. reprinted in anderson
and rosenfeld (1988).

blum, j. a. (1965). multidimensional stochastic ap-
proximation methods. annals of mathematical
statistics 25, 737   744.

bodlaender, h. (1993). a tourist guide through

treewidth. acta cybernetica 11, 1   21.

boser, b. e., i. m. guyon, and v. n. vapnik (1992).
a training algorithm for optimal margin classi-
   ers. in d. haussler (ed.), proceedings fifth an-
nual workshop on computational learning the-
ory (colt), pp. 144   152. acm.

bourlard, h. and y. kamp (1988). auto-association
by multilayer id88s and singular value de-
composition. biological cybernetics 59, 291   
294.

box, g. e. p., g. m. jenkins, and g. c. reinsel

(1994). time series analysis. prentice hall.

box, g. e. p. and g. c. tao (1973). bayesian infer-

ence in statistical analysis. wiley.

boyd, s. and l. vandenberghe (2004). convex opti-

mization. cambridge university press.

boyen, x. and d. koller (1998). tractable id136
for complex stochastic processes. in g. f. cooper
and s. moral (eds.), proceedings 14th annual
conference on uncertainty in arti   cial intelli-
gence (uai), pp. 33   42. morgan kaufmann.

breiman, l. (1996). id112 predictors. machine

learning 26, 123   140.

breiman, l., j. h. friedman, r. a. olshen, and
p. j. stone (1984). classi   cation and regression
trees. wadsworth.

brooks, s. p.

(1998). markov chain monte
carlo method and its application. the statisti-
cian 47(1), 69   100.

broomhead, d. s. and d. lowe (1988). multivari-
able functional interpolation and adaptive net-
works. complex systems 2, 321   355.

buntine, w. and a. weigend (1991). bayesian back-

propagation. complex systems 5, 603   643.

buntine, w. l. and a. s. weigend (1993). com-
puting second derivatives in feed-forward net-
works: a review. ieee transactions on neural
networks 5(3), 480   488.

burges, c. j. c. (1998). a tutorial on support vec-
tor machines for pattern recognition. knowledge
discovery and data mining 2(2), 121   167.

cardoso, j.-f. (1998). blind signal separation: statis-
tical principles. proceedings of the ieee 9(10),
2009   2025.

casella, g. and r. l. berger (2002). statistical in-

ference (second ed.). duxbury.

castillo, e., j. m. guti  errez, and a. s. hadi (1997).
id109 and probabilistic network mod-
els. springer.

chan, k., t. lee, and t. j. sejnowski (2003). vari-
ational bayesian learning of ica with missing
data. neural computation 15(8), 1991   2011.

chen, a. m., h. lu, and r. hecht-nielsen (1993).
on the geometry of feedforward neural network
error surfaces. neural computation 5(6), 910   
927.

chen, m. h., q. m. shao, and j. g. ibrahim (eds.)
(2001). monte carlo methods for bayesian com-
putation. springer.

boykov, y., o. veksler, and r. zabih (2001). fast
approximate energy minimization via graph cuts.
ieee transactions on pattern analysis and ma-
chine intelligence 23(11), 1222   1239.

chen, s., c. f. n. cowan, and p. m. grant (1991).
orthogonal least squares learning algorithm for
id80s. ieee transac-
tions on neural networks 2(2), 302   309.

choudrey, r. a. and s. j. roberts (2003). variational
mixture of bayesian independent component an-
alyzers. neural computation 15(1), 213   252.

clifford, p. (1990). markov random    elds in statis-
tics. in g. r. grimmett and d. j. a. welsh (eds.),
disorder in physical systems. a volume in hon-
our of john m. hammersley, pp. 19   32. oxford
university press.

collins, m., s. dasgupta, and r. e. schapire (2002).
a generalization of principal component analy-
sis to the exponential family. in t. g. dietterich,
s. becker, and z. ghahramani (eds.), advances
in neural information processing systems, vol-
ume 14, pp. 617   624. mit press.

comon, p., c. jutten, and j. herault (1991). blind
source separation, 2: problems statement. signal
processing 24(1), 11   20.

corduneanu, a. and c. m. bishop (2001). vari-
ational bayesian model selection for mixture
distributions. in t. richardson and t. jaakkola
(eds.), proceedings eighth international confer-
ence on arti   cial intelligence and statistics, pp.
27   34. morgan kaufmann.

cormen, t. h., c. e. leiserson, r. l. rivest, and
c. stein (2001). introduction to algorithms (sec-
ond ed.). mit press.

cortes, c. and v. n. vapnik (1995). support vector

networks. machine learning 20, 273   297.

cotter, n. e. (1990). the stone-weierstrass theo-
rem and its application to neural networks. ieee
transactions on neural networks 1(4), 290   295.
cover, t. and p. hart (1967). nearest neighbor pat-
tern classi   cation. ieee transactions on infor-
mation theory it-11, 21   27.

cover, t. m. and j. a. thomas (1991). elements of

id205. wiley.

cowell, r. g., a. p. dawid, s. l. lauritzen, and d. j.
spiegelhalter (1999). probabilistic networks and
id109. springer.

cox, r. t.

(1946). id203,

frequency and
reasonable expectation. american journal of
physics 14(1), 1   13.

references

715

cox, t. f. and m. a. a. cox (2000). multidimen-
sional scaling (second ed.). chapman and hall.
cressie, n. (1993). statistics for spatial data. wiley.
cristianini, n. and j. shawe-taylor (2000). support
vector machines and other kernel-based learning
methods. cambridge university press.

csat  o, l. and m. opper (2002). sparse on-line gaus-
sian processes. neural computation 14(3), 641   
668.

csisz`ar, i. and g. tusn`ady (1984). information ge-
ometry and alternating minimization procedures.
statistics and decisions 1(1), 205   237.

cybenko, g. (1989). approximation by superposi-
tions of a sigmoidal function. mathematics of
control, signals and systems 2, 304   314.

dawid, a. p. (1979). conditional independence in
statistical theory (with discussion). journal of the
royal statistical society, series b 4, 1   31.

dawid, a. p. (1980). conditional independence for
statistical operations. annals of statistics 8, 598   
617.

definetti, b. (1970). theory of id203. wiley

and sons.

dempster, a. p., n. m. laird, and d. b. rubin
(1977). maximum likelihood from incomplete
data via the em algorithm. journal of the royal
statistical society, b 39(1), 1   38.

denison, d. g. t., c. c. holmes, b. k. mallick,
and a. f. m. smith (2002). bayesian methods for
nonlinear classi   cation and regression. wiley.
diaconis, p. and l. saloff-coste (1998). what do we
know about the metropolis algorithm? journal
of computer and system sciences 57, 20   36.

dietterich, t. g. and g. bakiri (1995). solving
multiclass learning problems via error-correcting
output codes. journal of arti   cial intelligence
research 2, 263   286.

duane, s., a. d. kennedy, b. j. pendleton, and
d. roweth (1987). hybrid monte carlo. physics
letters b 195(2), 216   222.

duda, r. o. and p. e. hart (1973). pattern classi   -

cation and scene analysis. wiley.

716

references

duda, r. o., p. e. hart, and d. g. stork (2001). pat-

fletcher, r. (1987). practical methods of optimiza-

tern classi   cation (second ed.). wiley.

tion (second ed.). wiley.

durbin, r., s. eddy, a. krogh, and g. mitchi-
son (1998). biological sequence analysis. cam-
bridge university press.

dybowski, r. and s. roberts (2005). an anthology
of probabilistic models for medical informatics.
in d. husmeier, r. dybowski, and s. roberts
(eds.), probabilistic modeling in bioinformatics
and medical informatics, pp. 297   349. springer.
efron, b. (1979). bootstrap methods: another look

at the jackknife. annals of statistics 7, 1   26.

elkan, c. (2003). using the triangle inequality to ac-
celerate id116. in proceedings of the twelfth
international conference on machine learning,
pp. 147   153. aaai.

elliott, r. j., l. aggoun, and j. b. moore (1995).
id48: estimation and con-
trol. springer.

ephraim, y., d. malah, and b. h. juang (1989).
on the application of id48 for
enhancing noisy speech. ieee transactions on
acoustics, speech and signal processing 37(12),
1846   1856.

erwin, e., k. obermayer, and k. schulten (1992).
self-organizing maps: ordering, convergence
properties and energy functions. biological cy-
bernetics 67, 47   55.

everitt, b. s. (1984). an introduction to latent vari-

able models. chapman and hall.

faul, a. c. and m. e. tipping (2002). analysis of
sparse bayesian learning. in t. g. dietterich,
s. becker, and z. ghahramani (eds.), advances
in neural information processing systems, vol-
ume 14, pp. 383   389. mit press.

feller, w. (1966). an introduction to id203
theory and its applications (second ed.), vol-
ume 2. wiley.

feynman, r. p., r. b. leighton, and m. sands
(1964). the feynman lectures of physics, vol-
ume two. addison-wesley. chapter 19.

forsyth, d. a. and j. ponce (2003). computer vi-

sion: a modern approach. prentice hall.

freund, y. and r. e. schapire (1996). experiments
with a new boosting algorithm. in l. saitta (ed.),
thirteenth international conference on machine
learning, pp. 148   156. morgan kaufmann.

frey, b. j. (1998). id114 for ma-
chine learning and digital communication.
mit press.

frey, b. j. and d. j. c. mackay (1998). a revolu-
tion: belief propagation in graphs with cycles. in
m. i. jordan, m. j. kearns, and s. a. solla (eds.),
advances in neural information processing sys-
tems, volume 10. mit press.

friedman, j. h. (2001). greedy function approxi-
mation: a gradient boosting machine. annals of
statistics 29(5), 1189   1232.

friedman, j. h., t. hastie, and r. tibshirani (2000).
additive id28: a statistical view of
boosting. annals of statistics 28, 337   407.

friedman, n. and d. koller (2003). being bayesian
about network structure: a bayesian approach
to structure discovery in id110s. ma-
chine learning 50, 95   126.

frydenberg, m. (1990). the chain graph markov
property. scandinavian journal of statistics 17,
333   353.

fukunaga, k. (1990). introduction to statistical pat-
tern recognition (second ed.). academic press.
funahashi, k. (1989). on the approximate realiza-
tion of continuous mappings by neural networks.
neural networks 2(3), 183   192.

fung, r. and k. c. chang (1990). weighting and
integrating evidence for stochastic simulation in
id110s. in p. p. bonissone, m. hen-
rion, l. n. kanal, and j. f. lemmer (eds.), un-
certainty in arti   cial intelligence, volume 5, pp.
208   219. elsevier.

gallager, r. g. (1963). low-density parity-check

codes. mit press.

references

717

gamerman, d. (1997). id115:
stochastic simulation for bayesian id136.
chapman and hall.

gelman, a., j. b. carlin, h. s. stern, and d. b. ru-
bin (2004). bayesian data analysis (second ed.).
chapman and hall.

geman, s. and d. geman (1984). stochastic re-
laxation, gibbs distributions, and the bayesian
restoration of images. ieee transactions on pat-
tern analysis and machine intelligence 6(1),
721   741.

ghahramani, z. and m. j. beal (2000). variational
id136 for bayesian mixtures of factor ana-
lyzers. in s. a. solla, t. k. leen, and k. r.
m  uller (eds.), advances in neural information
processing systems, volume 12, pp. 449   455.
mit press.

ghahramani, z. and g. e. hinton (1996a). the
em algorithm for mixtures of factor analyzers.
technical report crg-tr-96-1, university of
toronto.

ghahramani, z. and g. e. hinton (1996b). param-
eter estimation for linear dynamical systems.
technical report crg-tr-96-2, university of
toronto.

ghahramani, z. and g. e. hinton (1998). variational
learning for switching state-space models. neu-
ral computation 12(4), 963   996.

ghahramani, z. and m. i. jordan (1994). super-
vised learning from incomplete data via an em
appproach. in j. d. cowan, g. t. tesauro, and
j. alspector (eds.), advances in neural informa-
tion processing systems, volume 6, pp. 120   127.
morgan kaufmann.

ghahramani, z. and m. i. jordan (1997). factorial
id48. machine learning 29,
245   275.

gibbs, m. n. (1997). bayesian gaussian processes
for regression and classi   cation. phd thesis, uni-
versity of cambridge.

gibbs, m. n. and d. j. c. mackay (2000). varia-
tional gaussian process classi   ers. ieee trans-
actions on neural networks 11, 1458   1464.

gilks, w. r.

(1992). derivative-free adaptive
rejection sampling for gibbs
in
j. bernardo, j. berger, a. p. dawid, and a. f. m.
smith (eds.), bayesian statistics, volume 4. ox-
ford university press.

sampling.

gilks, w. r., n. g. best, and k. k. c. tan (1995).
adaptive rejection metropolis sampling. applied
statistics 44, 455   472.

gilks, w. r., s. richardson, and d. j. spiegelhal-
ter (eds.) (1996). id115 in
practice. chapman and hall.

gilks, w. r. and p. wild (1992). adaptive rejection
sampling for id150. applied statis-
tics 41, 337   348.

gill, p. e., w. murray, and m. h. wright (1981).

practical optimization. academic press.

goldberg, p. w., c. k. i. williams, and c. m.
bishop (1998). regression with input-dependent
noise: a gaussian process treatment. in ad-
vances in neural information processing sys-
tems, volume 10, pp. 493   499. mit press.

golub, g. h. and c. f. van loan (1996). matrix
computations (third ed.). john hopkins univer-
sity press.

good, i. (1950). id203 and the weighing of ev-

idence. hafners.

gordon, n. j., d. j. salmond, and a. f. m. smith
approach to nonlinear/non-
iee

(1993). novel
gaussian bayesian
proceedings-f 140(2), 107   113.

estimation.

state

graepel, t. (2003). solving noisy linear operator
equations by gaussian processes: application
to ordinary and partial differential equations. in
proceedings of the twentieth international con-
ference on machine learning, pp. 234   241.

greig, d., b. porteous, and a. seheult (1989). ex-
act maximum a-posteriori estimation for binary
images. journal of the royal statistical society,
series b 51(2), 271   279.

gull, s. f. (1989). developments in maximum en-
tropy data analysis. in j. skilling (ed.), maxi-
mum id178 and bayesian methods, pp. 53   71.
kluwer.

718

references

hassibi, b. and d. g. stork (1993). second order
derivatives for network pruning: optimal brain
surgeon. in s. j. hanson, j. d. cowan, and
c. l. giles (eds.), advances in neural informa-
tion processing systems, volume 5, pp. 164   171.
morgan kaufmann.

hastie, t. and w. stuetzle (1989). principal curves.
the american statistical associa-

journal of
tion 84(106), 502   516.

hastie, t., r. tibshirani, and j. friedman (2001).

the elements of statistical learning. springer.

hastings, w. k. (1970). monte carlo sampling
methods using markov chains and their applica-
tions. biometrika 57, 97   109.

hathaway, r. j. (1986). another interpretation of the
em algorithm for mixture distributions. statistics
and id203 letters 4, 53   56.

haussler, d. (1999). convolution kernels on discrete
structures. technical report ucsc-crl-99-10,
university of california, santa cruz, computer
science department.

henrion, m. (1988). propagation of uncertainty by
logic sampling in bayes    networks. in j. f. lem-
mer and l. n. kanal (eds.), uncertainty in arti-
   cial intelligence, volume 2, pp. 149   164. north
holland.

herbrich, r. (2002). learning kernel classi   ers.

mit press.

hertz, j., a. krogh, and r. g. palmer (1991). in-
troduction to the theory of neural computation.
addison wesley.

hinton, g. e., p. dayan, and m. revow (1997).
modelling the manifolds of images of handwrit-
ten digits. ieee transactions on neural net-
works 8(1), 65   74.

hinton, g. e. and d. van camp (1993). keeping
neural networks simple by minimizing the de-
scription length of the weights. in proceedings of
the sixth annual conference on computational
learning theory, pp. 5   13. acm.

hinton, g. e., m. welling, y. w. teh, and s. osin-
dero (2001). a new view of ica. in proceedings

of the international conference on independent
component analysis and blind signal separa-
tion, volume 3.

hodgson, m. e. (1998). reducing computational re-
quirements of the minimum-distance classi   er.
remote sensing of environments 25, 117   128.

hoerl, a. e. and r. kennard (1970). ridge regres-
sion: biased estimation for nonorthogonal prob-
lems. technometrics 12, 55   67.

hofmann, t. (2000). learning the similarity of doc-
uments: an information-geometric approach to
document retrieval and classi   cation. in s. a.
solla, t. k. leen, and k. r. m  uller (eds.), ad-
vances in neural information processing sys-
tems, volume 12, pp. 914   920. mit press.

hojen-sorensen, p. a., o. winther, and l. k. hansen
(2002). mean    eld approaches to independent
component analysis. neural computation 14(4),
889   918.

hornik, k. (1991). approximation capabilities of
multilayer feedforward networks. neural net-
works 4(2), 251   257.

hornik, k., m. stinchcombe, and h. white (1989).
multilayer feedforward networks are universal
approximators. neural networks 2(5), 359   366.
hotelling, h. (1933). analysis of a complex of statis-
tical variables into principal components. jour-
nal of educational psychology 24, 417   441.

hotelling, h. (1936). relations between two sets of

variables. biometrika 28, 321   377.

hyv  arinen, a. and e. oja (1997). a fast    xed-point
algorithm for independent component analysis.
neural computation 9(7), 1483   1492.

isard, m. and a. blake (1998). condensation
    conditional density propagation for visual
tracking. international journal of computer vi-
sion 29(1), 5   18.

ito, y. (1991). representation of functions by su-
perpositions of a step or sigmoid function and
their applications to neural id177. neu-
ral networks 4(3), 385   394.

references

719

jaakkola, t. and m. i. jordan (2000). bayesian
parameter estimation via variational methods.
statistics and computing 10, 25   37.

jaakkola, t. s. (2001). tutorial on variational ap-
proximation methods. in m. opper and d. saad
(eds.), advances in mean field methods, pp.
129   159. mit press.

jaakkola, t. s. and d. haussler (1999). exploiting
generative models in discriminative classi   ers. in
m. s. kearns, s. a. solla, and d. a. cohn (eds.),
advances in neural information processing sys-
tems, volume 11. mit press.

jacobs, r. a., m. i. jordan, s. j. nowlan, and g. e.
hinton (1991). adaptive mixtures of local ex-
perts. neural computation 3(1), 79   87.

jaynes, e. t. (2003). id203 theory: the logic

of science. cambridge university press.

jebara, t. (2004). machine learning: discrimina-

tive and generative. kluwer.

jeffries, h. (1946). an invariant form for the prior
id203 in estimation problems. pro. roy.
soc. aa 186, 453   461.

jelinek, f. (1997). statistical methods for speech

recognition. mit press.

jensen, c., a. kong, and u. kjaerulff (1995). block-
ing id150 in very large probabilistic
id109. international journal of human
computer studies. special issue on real-world
applications of uncertain reasoning. 42, 647   
666.

jordan, m. i. (2007). an introduction to probabilis-

tic id114. in preparation.

jordan, m. i., z. ghahramani, t. s. jaakkola, and
l. k. saul (1999). an introduction to variational
methods for id114. in m. i. jordan
(ed.), learning in id114, pp. 105   
162. mit press.

jordan, m. i. and r. a. jacobs (1994). hierarchical
mixtures of experts and the em algorithm. neu-
ral computation 6(2), 181   214.

jutten, c. and j. herault (1991). blind separation of
sources, 1: an adaptive algorithm based on neu-
romimetic architecture. signal processing 24(1),
1   10.

kalman, r. e. (1960). a new approach to linear    l-
tering and prediction problems. transactions of
the american society for mechanical engineer-
ing, series d, journal of basic engineering 82,
35   45.

kambhatla, n. and t. k. leen (1997). dimension
reduction by local principal component analysis.
neural computation 9(7), 1493   1516.

kanazawa, k., d. koller, and s. russel (1995).
stochastic simulation algorithms for dynamic
probabilistic networks. in uncertainty in arti   -
cial intelligence, volume 11. morgan kaufmann.
kapadia, s. (1998). discriminative training of hid-
den markov models. phd thesis, university of
cambridge, u.k.

kapur, j. (1989). maximum id178 methods in sci-

jensen, f. v. (1996). an introduction to bayesian

ence and engineering. wiley.

networks. ucl press.

jerrum, m. and a. sinclair (1996). the markov
chain monte carlo method: an approach to ap-
proximate counting and integration. in d. s.
hochbaum (ed.), approximation algorithms for
np-hard problems. pws publishing.

jolliffe, i. t. (2002). principal component analysis

(second ed.). springer.

jordan, m. i. (1999). learning in id114.

mit press.

karush, w. (1939). minima of functions of several
variables with inequalities as side constraints.
master   s thesis, department of mathematics,
university of chicago.

kass, r. e. and a. e. raftery (1995). bayes fac-
tors. journal of the american statistical associ-
ation 90, 377   395.

kearns, m. j. and u. v. vazirani (1994). an intro-
duction to computational learning theory. mit
press.

720

references

kindermann, r. and j. l. snell (1980). markov ran-
dom fields and their applications. american
mathematical society.

kittler, j. and j. f  oglein (1984). contextual classi   -
cation of multispectral pixel data. image and vi-
sion computing 2, 13   29.

kohonen, t. (1982). self-organized formation of
topologically correct feature maps. biological
cybernetics 43, 59   69.

kohonen,

t.

(1995).

self-organizing maps.

springer.

kolmogorov, v. and r. zabih (2004). what en-
ergy functions can be minimized via graph cuts?
ieee transactions on pattern analysis and ma-
chine intelligence 26(2), 147   159.

kreinovich, v. y. (1991). arbitrary nonlinearity is
suf   cient to represent all functions by neural net-
works: a theorem. neural networks 4(3), 381   
383.

krogh, a., m. brown, i. s. mian, k. sj  olander, and
d. haussler (1994). id48 in
computational biology: applications to protein
modelling. journal of molecular biology 235,
1501   1531.

kschischnang, f. r., b. j. frey, and h. a. loeliger
(2001). factor graphs and the sum-product algo-
rithm. ieee transactions on information the-
ory 47(2), 498   519.

kuhn, h. w. and a. w. tucker (1951). nonlinear
programming. in proceedings of the 2nd berke-
ley symposium on mathematical statistics and
probabilities, pp. 481   492. university of cali-
fornia press.

kullback, s. and r. a. leibler (1951). on infor-
mation and suf   ciency. annals of mathematical
statistics 22(1), 79   86.

k  urkov  a, v. and p. c. kainen (1994). functionally
equivalent feed-forward neural networks. neural
computation 6(3), 543   558.

in advances in neural information processing
systems, number 18. mit press. in press.

lasserre, j., c. m. bishop, and t. minka (2006).
principled hybrids of generative and discrimina-
tive models. in proceedings 2006 ieee confer-
ence on id161 and pattern recogni-
tion, new york.

lauritzen, s. and n. wermuth (1989). graphical
models for association between variables, some
of which are qualitative some quantitative. an-
nals of statistics 17, 31   57.

lauritzen, s. l. (1992). propagation of probabilities,
means and variances in mixed graphical associa-
tion models. journal of the american statistical
association 87, 1098   1108.

lauritzen, s. l. (1996). id114. oxford

university press.

lauritzen, s. l. and d. j. spiegelhalter (1988). lo-
cal computations with probabailities on graphical
structures and their application to id109.
journal of the royal statistical society 50, 157   
224.

lawley, d. n. (1953). a modi   ed method of esti-
mation in factor analysis and some large sam-
ple results. in uppsala symposium on psycho-
logical factor analysis, number 3 in nordisk
psykologi monograph series, pp. 35   42. upp-
sala: almqvist and wiksell.

lawrence, n. d., a. i. t. rowstron, c. m. bishop,
and m. j. taylor (2002). optimising synchro-
nisation times for mobile devices. in t. g. di-
etterich, s. becker, and z. ghahramani (eds.),
advances in neural information processing sys-
tems, volume 14, pp. 1401   1408. mit press.

lazarsfeld, p. f. and n. w. henry (1968). latent

structure analysis. houghton mif   in.

le cun, y., b. boser, j. s. denker, d. henderson,
r. e. howard, w. hubbard, and l. d. jackel
(1989). id26 applied to handwritten
zip code recognition. neural computation 1(4),
541   551.

kuss, m. and c. rasmussen (2006). assessing ap-
proximations for gaussian process classi   cation.

le cun, y., j. s. denker, and s. a. solla (1990).
optimal brain damage. in d. s. touretzky (ed.),

references

721

advances in neural information processing sys-
tems, volume 2, pp. 598   605. morgan kauf-
mann.

lecun, y., l. bottou, y. bengio, and p. haffner
(1998). gradient-based learning applied to doc-
ument recognition. proceedings of the ieee 86,
2278   2324.

lee, y., y. lin, and g. wahba (2001). multicategory
support vector machines. technical report 1040,
department of statistics, university of madison,
wisconsin.

leen, t. k. (1995). from data distributions to regu-
larization in invariant learning. neural computa-
tion 7, 974   981.

lindley, d. v. (1982). scoring rules and the in-
evitability of id203. international statisti-
cal review 50, 1   26.

liu, j. s. (ed.) (2001). monte carlo strategies in

scienti   c computing. springer.

lloyd, s. p. (1982). least squares quantization in
pcm. ieee transactions on information the-
ory 28(2), 129   137.

l  utkepohl, h. (1996). handbook of matrices. wiley.
mackay, d. j. c. (1992a). bayesian interpolation.

neural computation 4(3), 415   447.

mackay, d. j. c. (1992b). the evidence framework
applied to classi   cation networks. neural com-
putation 4(5), 720   736.

mackay, d. j. c. (1992c). a practical bayesian
framework for back-propagation networks. neu-
ral computation 4(3), 448   472.

mackay, d. j. c. (1994). bayesian methods for
backprop networks. in e. domany, j. l. van
hemmen, and k. schulten (eds.), models of
neural networks, iii, chapter 6, pp. 211   254.
springer.

mackay, d. j. c. (1995). bayesian neural networks
and density networks. nuclear instruments and
methods in physics research, a 354(1), 73   80.
mackay, d. j. c. (1997). id108 for hid-
den markov models. unpublished manuscript,

department of physics, university of cam-
bridge.

mackay, d. j. c. (1998). introduction to gaus-
sian processes. in c. m. bishop (ed.), neural
networks and machine learning, pp. 133   166.
springer.

mackay, d. j. c. (1999). comparison of approx-
imate methods for handling hyperparameters.
neural computation 11(5), 1035   1068.

mackay, d. j. c. (2003). id205, infer-
ence and learning algorithms. cambridge uni-
versity press.

mackay, d. j. c. and m. n. gibbs (1999). den-
sity networks. in j. w. kay and d. m. tittering-
ton (eds.), statistics and neural networks: ad-
vances at the interface, chapter 5, pp. 129   145.
oxford university press.

mackay, d. j. c. and r. m. neal (1999). good error-
correcting codes based on very sparse matrices.
ieee transactions on id205 45,
399   431.

macqueen, j. (1967). some methods for classi   ca-
tion and analysis of multivariate observations. in
l. m. lecam and j. neyman (eds.), proceed-
ings of the fifth berkeley symposium on mathe-
matical statistics and id203, volume i, pp.
281   297. university of california press.

magnus, j. r. and h. neudecker (1999). matrix dif-
ferential calculus with applications in statistics
and econometrics. wiley.

mallat, s. (1999). a wavelet tour of signal process-

ing (second ed.). academic press.

manning, c. d. and h. sch  utze (1999). foundations
of statistical natural language processing. mit
press.

mardia, k. v. and p. e. jupp (2000). directional

statistics. wiley.

maybeck, p. s. (1982). stochastic models, estima-

tion and control. academic press.

mcallester, d. a. (2003). pac-bayesian stochastic
model selection. machine learning 51(1), 5   21.

722

references

mccullagh, p. and j. a. nelder (1989). generalized
linear models (second ed.). chapman and hall.
mcculloch, w. s. and w. pitts (1943). a logical
calculus of the ideas immanent in nervous ac-
tivity. bulletin of mathematical biophysics 5,
115   133. reprinted in anderson and rosenfeld
(1988).

mceliece, r. j., d. j. c. mackay, and j. f. cheng
(1998). turbo decoding as an instance of pearl   s
   belief ppropagation    algorithm. ieee journal
on selected areas in communications 16, 140   
152.

mclachlan, g. j. and k. e. basford (1988). mixture
models: id136 and applications to cluster-
ing. marcel dekker.

mclachlan, g. j. and t. krishnan (1997). the em

algorithm and its extensions. wiley.

mclachlan, g. j. and d. peel (2000). finite mixture

models. wiley.

meng, x. l. and d. b. rubin (1993). maximum like-
lihood estimation via the ecm algorithm: a gen-
eral framework. biometrika 80, 267   278.

metropolis, n., a. w. rosenbluth, m. n. rosen-
bluth, a. h. teller, and e. teller (1953). equa-
tion of state calculations by fast computing
machines. journal of chemical physics 21(6),
1087   1092.

metropolis, n. and s. ulam (1949). the monte
carlo method. journal of the american statisti-
cal association 44(247), 335   341.

mika, s., g. r  atsch, j. weston, and b. sch  olkopf
(1999). fisher discriminant analysis with ker-
nels. in y. h. hu, j. larsen, e. wilson, and
s. douglas (eds.), neural networks for signal
processing ix, pp. 41   48. ieee.

minka, t. (2001a). expectation propagation for ap-
proximate bayesian id136. in j. breese and
d. koller (eds.), proceedings of the seventeenth
conference on uncertainty in arti   cial intelli-
gence, pp. 362   369. morgan kaufmann.

minka, t. (2001b). a family of approximate al-
gorithms for bayesian id136. ph. d. thesis,
mit.

minka, t. (2004). power ep. technical report
msr-tr-2004-149, microsoft research cam-
bridge.

minka, t. (2005). divergence measures and mes-
sage passing. technical report msr-tr-2005-
173, microsoft research cambridge.

minka, t. p. (2001c). automatic choice of dimen-
sionality for pca. in t. k. leen, t. g. diet-
terich, and v. tresp (eds.), advances in neural
information processing systems, volume 13, pp.
598   604. mit press.

minsky, m. l. and s. a. papert (1969). id88s.

mit press. expanded edition 1990.

miskin, j. w. and d. j. c. mackay (2001). ensem-
ble learning for blind source separation. in s. j.
roberts and r. m. everson (eds.), independent
component analysis: principles and practice.
cambridge university press.

m  ller, m. (1993). ef   cient training of feed-
forward neural networks. ph. d. thesis, aarhus
university, denmark.

moody, j. and c. j. darken (1989). fast learning in
networks of locally-tuned processing units. neu-
ral computation 1(2), 281   294.

moore, a. w. (2000). the anchors hierarch: us-
ing the triangle inequality to survive high dimen-
sional data. in proceedings of the twelfth con-
ference on uncertainty in arti   cial intelligence,
pp. 397   405.

m  uller, k. r., s. mika, g. r  atsch, k. tsuda, and
b. sch  olkopf (2001). an introduction to kernel-
based learning algorithms. ieee transactions on
neural networks 12(2), 181   202.

m  uller, p. and f. a. quintana (2004). nonparametric
bayesian data analysis. statistical science 19(1),
95   110.

nabney, i. t. (2002). netlab: algorithms for pattern

recognition. springer.

nadaraya,   e. a. (1964). on estimating regression.
theory of id203 and its applications 9(1),
141   142.

references

723

nag, r., k. wong, and f. fallside (1986). script
recognition using id48. in
icassp86, pp. 2071   2074. ieee.

neal, r. m. (1993). probabilistic id136 using
id115 methods. technical
report crg-tr-93-1, department of computer
science, university of toronto, canada.

neal, r. m. (1996). bayesian learning for neural
networks. springer. lecture notes in statistics
118.

neal, r. m. (1997). monte carlo implementation of
gaussian process models for bayesian regression
and classi   cation. technical report 9702, de-
partment of computer statistics, university of
toronto.

neal, r. m. (1999). suppressing id93 in
id115 using ordered over-
relaxation. in m. i. jordan (ed.), learning in
id114, pp. 205   228. mit press.

neal, r. m. (2000). markov chain sampling for
dirichlet process mixture models. journal of
computational and graphical statistics 9, 249   
265.

neal, r. m. (2003). slice sampling. annals of statis-

tics 31, 705   767.

neal, r. m. and g. e. hinton (1999). a new view of
the em algorithm that justi   es incremental and
other variants. in m. i. jordan (ed.), learning in
id114, pp. 355   368. mit press.

nelder, j. a. and r. w. m. wedderburn (1972). gen-
eralized linear models. journal of the royal sta-
tistical society, a 135, 370   384.

nilsson, n. j. (1965). learning machines. mcgraw-
hill. reprinted as the mathematical founda-
tions of learning machines, morgan kaufmann,
(1990).

nocedal, j. and s. j. wright (1999). numerical op-

timization. springer.

nowlan, s. j. and g. e. hinton (1992). simplifying
neural networks by soft weight sharing. neural
computation 4(4), 473   493.

ogden, r. t. (1997). essential wavelets for statisti-
cal applications and data analysis. birkh  auser.
opper, m. and o. winther (1999). a bayesian ap-
proach to on-line learning. in d. saad (ed.), on-
line learning in neural networks, pp. 363   378.
cambridge university press.
opper, m. and o. winther

(2000a). gaussian
processes and id166: mean    eld theory and
leave-one-out. in a. j. smola, p. l. bartlett,
b. sch  olkopf, and d. shuurmans (eds.), ad-
vances in large margin classi   ers, pp. 311   326.
mit press.

opper, m. and o. winther

(2000b). gaussian
processes for classi   cation. neural computa-
tion 12(11), 2655   2684.

osuna, e., r. freund, and f. girosi (1996). support
vector machines: training and applications. a.i.
memo aim-1602, mit.

papoulis, a. (1984). id203, random variables,
and stochastic processes (second ed.). mcgraw-
hill.

parisi, g. (1988). statistical field theory. addison-

wesley.

pearl, j. (1988). probabilistic reasoning in intelli-

gent systems. morgan kaufmann.

pearlmutter, b. a. (1994). fast exact multiplication
by the hessian. neural computation 6(1), 147   
160.

pearlmutter, b. a. and l. c. parra (1997). maximum
likelihood source separation: a context-sensitive
generalization of ica. in m. c. mozer, m. i. jor-
dan, and t. petsche (eds.), advances in neural
information processing systems, volume 9, pp.
613   619. mit press.

pearson, k. (1901). on lines and planes of closest    t
to systems of points in space. the london, edin-
burgh and dublin philosophical magazine and
journal of science, sixth series 2, 559   572.

platt, j. c. (1999). fast training of support vector
machines using sequential minimal optimization.
in b. sch  olkopf, c. j. c. burges, and a. j. smola
(eds.), advances in kernel methods     support
vector learning, pp. 185   208. mit press.

724

references

platt, j. c. (2000). probabilities for sv machines.
in a. j. smola, p. l. bartlett, b. sch  olkopf, and
d. shuurmans (eds.), advances in large margin
classi   ers, pp. 61   73. mit press.

platt, j. c., n. cristianini, and j. shawe-taylor
(2000). large margin dags for multiclass clas-
si   cation. in s. a. solla, t. k. leen, and k. r.
m  uller (eds.), advances in neural information
processing systems, volume 12, pp. 547   553.
mit press.

poggio, t. and f. girosi (1990). networks for ap-
proximation and learning. proceedings of the
ieee 78(9), 1481   1497.

powell, m. j. d. (1987). radial basis functions for
multivariable interpolation: a review. in j. c.
mason and m. g. cox (eds.), algorithms for
approximation, pp. 143   167. oxford university
press.

press, w. h., s. a. teukolsky, w. t. vetterling, and
b. p. flannery (1992). numerical recipes in c:
the art of scienti   c computing (second ed.).
cambridge university press.

qazaz, c. s., c. k. i. williams, and c. m. bishop
(1997). an upper bound on the bayesian error
bars for generalized id75. in s. w.
ellacott, j. c. mason, and i. j. anderson (eds.),
mathematics of neural networks: models, algo-
rithms and applications, pp. 295   299. kluwer.

quinlan, j. r. (1986). induction of id90.

machine learning 1(1), 81   106.

quinlan, j. r. (1993). c4.5: programs for machine

learning. morgan kaufmann.

rabiner, l. and b. h. juang (1993). fundamentals

of id103. prentice hall.

rabiner, l. r. (1989). a tutorial on hidden markov
models and selected applications in speech
the ieee 77(2),
recognition. proceedings of
257   285.

ramasubramanian, v. and k. k. paliwal (1990). a
generalized optimization of the k-d tree for fast
nearest-neighbour search. in proceedings fourth
ieee region 10 international conference (ten-
con   89), pp. 565   568.

ramsey, f.

(1931). truth and id203.

in
r. braithwaite (ed.), the foundations of math-
ematics and other logical essays. humanities
press.

rao, c. r. and s. k. mitra (1971). generalized in-

verse of matrices and its applications. wiley.

rasmussen, c. e. (1996). evaluation of gaussian
processes and other methods for non-linear
regression. ph. d. thesis, university of toronto.

rasmussen, c. e. and j. qui  nonero-candela (2005).
healing the relevance vector machine by aug-
mentation. in l. d. raedt and s. wrobel (eds.),
proceedings of the 22nd international confer-
ence on machine learning, pp. 689   696.

rasmussen, c. e. and c. k. i. williams (2006).
gaussian processes for machine learning. mit
press.

rauch, h. e., f. tung, and c. t. striebel (1965).
maximum likelihood estimates of linear dynam-
ical systems. aiaa journal 3, 1445   1450.

ricotti, l. p., s. ragazzini, and g. martinelli (1988).
learning of word stress in a sub-optimal second
order id26 neural network. in pro-
ceedings of the ieee international conference
on neural networks, volume 1, pp. 355   361.
ieee.

ripley, b. d. (1996). pattern recognition and neu-

ral networks. cambridge university press.

robbins, h. and s. monro (1951). a stochastic
approximation method. annals of mathematical
statistics 22, 400   407.

robert, c. p. and g. casella (1999). monte carlo

statistical methods. springer.

rockafellar, r. (1972). convex analysis. princeton

university press.

rosenblatt, f. (1962). principles of neurodynam-
ics: id88s and the theory of brain mech-
anisms. spartan.

roth, v. and v. steinhage (2000). nonlinear discrim-
inant analysis using id81s. in s. a.

references

725

solla, t. k. leen, and k. r. m  uller (eds.), ad-
vances in neural information processing sys-
tems, volume 12. mit press.

roweis, s. (1998). em algorithms for pca and
spca. in m. i. jordan, m. j. kearns, and s. a.
solla (eds.), advances in neural information
processing systems, volume 10, pp. 626   632.
mit press.

roweis, s. and z. ghahramani (1999). a unifying
review of linear gaussian models. neural com-
putation 11(2), 305   345.

roweis, s. and l. saul (2000, december). nonlinear
id84 by locally linear em-
bedding. science 290, 2323   2326.

rubin, d. b. (1983). iteratively reweighted least
squares. in encyclopedia of statistical sciences,
volume 4, pp. 272   275. wiley.

rubin, d. b. and d. t. thayer (1982). em al-
gorithms for ml factor analysis. psychome-
trika 47(1), 69   76.

rumelhart, d. e., g. e. hinton, and r. j. williams
(1986). learning internal representations by er-
ror propagation. in d. e. rumelhart, j. l. mc-
clelland, and the pdp research group (eds.),
parallel distributed processing: explorations
in the microstructure of cognition, volume 1:
foundations, pp. 318   362. mit press. reprinted
in anderson and rosenfeld (1988).

rumelhart, d. e., j. l. mcclelland, and the pdp re-
search group (eds.) (1986). parallel distributed
processing: explorations in the microstruc-
ture of cognition, volume 1: foundations. mit
press.

sagan, h. (1969). introduction to the calculus of

variations. dover.

savage, l. j. (1961). the subjective basis of sta-
tistical practice. technical report, department of
statistics, university of michigan, ann arbor.

sch  olkopf, b., j. platt, j. shawe-taylor, a. smola,
and r. c. williamson (2001). estimating the sup-
port of a high-dimensional distribution. neural
computation 13(7), 1433   1471.

sch  olkopf, b., a. smola, and k.-r. m  uller (1998).
nonlinear component analysis as a kernel
eigenvalue problem. neural computation 10(5),
1299   1319.

sch  olkopf, b., a. smola, r. c. williamson, and p. l.
bartlett (2000). new support vector algorithms.
neural computation 12(5), 1207   1245.

sch  olkopf, b. and a. j. smola (2002). learning with

kernels. mit press.

schwarz, g. (1978). estimating the dimension of a

model. annals of statistics 6, 461   464.

schwarz, h. r. (1988). finite element methods. aca-

demic press.

seeger, m. (2003). bayesian gaussian process mod-
els: pac-bayesian generalization error bounds
and sparse approximations. ph. d. thesis, uni-
versity of edinburg.

seeger, m., c. k. i. williams, and n. lawrence
(2003). fast forward selection to speed up sparse
gaussian processes. in c. m. bishop and b. frey
(eds.), proceedings ninth international work-
shop on arti   cial intelligence and statistics, key
west, florida.

shachter, r. d. and m. peot (1990). simulation ap-
proaches to general probabilistic id136 on be-
lief networks. in p. p. bonissone, m. henrion,
l. n. kanal, and j. f. lemmer (eds.), uncer-
tainty in arti   cial intelligence, volume 5. else-
vier.

shannon, c. e. (1948). a mathematical theory of
communication. the bell system technical jour-
nal 27(3), 379   423 and 623   656.

shawe-taylor, j. and n. cristianini (2004). kernel
methods for pattern analysis. cambridge uni-
versity press.

sietsma, j. and r. j. f. dow (1991). creating arti   -
cial neural networks that generalize. neural net-
works 4(1), 67   79.

simard, p., y. le cun, and j. denker (1993). ef   -
cient pattern recognition using a new transforma-
tion distance. in s. j. hanson, j. d. cowan, and

726

references

c. l. giles (eds.), advances in neural informa-
tion processing systems, volume 5, pp. 50   58.
morgan kaufmann.

simard, p., b. victorri, y. le cun, and j. denker
(1992). tangent prop     a formalism for specify-
ing selected invariances in an adaptive network.
in j. e. moody, s. j. hanson, and r. p. lippmann
(eds.), advances in neural information process-
ing systems, volume 4, pp. 895   903. morgan
kaufmann.

simard, p. y., d. steinkraus, and j. platt (2003).
best practice for convolutional neural networks
applied to visual document analysis. in pro-
ceedings international conference on document
analysis and recognition (icdar), pp. 958   
962. ieee computer society.

sirovich, l. (1987). turbulence and the dynamics
of coherent structures. quarterly applied math-
ematics 45(3), 561   590.

smola, a. j. and p. bartlett (2001). sparse greedy
gaussian process regression. in t. k. leen, t. g.
dietterich, and v. tresp (eds.), advances in neu-
ral information processing systems, volume 13,
pp. 619   625. mit press.

spiegelhalter, d. and s. lauritzen (1990). sequential
updating of conditional probabilities on directed
graphical structures. networks 20, 579   605.

stinchecombe, m. and h. white (1989). universal
approximation using feed-forward networks with
non-sigmoid hidden layer id180. in
international joint conference on neural net-
works, volume 1, pp. 613   618. ieee.

stone, j. v. (2004). independent component analy-

sis: a tutorial introduction. mit press.

sung, k. k. and t. poggio (1994). example-based
learning for view-based human face detection.
a.i. memo 1521, mit.

sutton, r. s. and a. g. barto (1998). reinforcement

learning: an introduction. mit press.

tarassenko, l. (1995). novelty detection for the
identi   cation of masses in mamograms. in pro-
ceedings fourth iee international conference
on arti   cial neural networks, volume 4, pp.
442   447. iee.

tax, d. and r. duin (1999). data domain descrip-
tion by support vectors. in m. verleysen (ed.),
proceedings european symposium on arti   cial
neural networks, esann, pp. 251   256. d. facto
press.

teh, y. w., m. i. jordan, m. j. beal, and d. m. blei
(2006). hierarchical dirichlet processes. journal
of the americal statistical association. to appear.
tenenbaum, j. b., v. de silva, and j. c. langford
(2000, december). a global framework for non-
linear id84. science 290,
2319   2323.

tesauro, g. (1994). td-gammon, a self-teaching
backgammon program, achieves master-level
play. neural computation 6(2), 215   219.

thiesson, b., d. m. chickering, d. heckerman, and
c. meek (2004). arma time-series modelling
with id114. in m. chickering and
j. halpern (eds.), proceedings of the twentieth
conference on uncertainty in arti   cial intelli-
gence, banff, canada, pp. 552   560. auai press.
tibshirani, r. (1996). regression shrinkage and se-
lection via the lasso. journal of the royal statis-
tical society, b 58, 267   288.

tierney, l. (1994). markov chains for exploring pos-
terior distributions. annals of statistics 22(4),
1701   1762.

tikhonov, a. n. and v. y. arsenin (1977). solutions

of ill-posed problems. v. h. winston.

tino, p. and i. t. nabney (2002). hierarchical
gtm: constructing localized non-linear projec-
tion manifolds in a principled way. ieee trans-
actions on pattern analysis and machine intelli-
gence 24(5), 639   656.

svens  en, m. and c. m. bishop (2004). ro-
bust bayesian mixture modelling. neurocomput-
ing 64, 235   252.

tino, p., i. t. nabney, and y. sun (2001). us-
ing directional curvatures to visualize folding
patterns of the gtm projection manifolds. in

references

727

g. dorffner, h. bischof, and k. hornik (eds.),
arti   cial neural networks     icann 2001, pp.
421   428. springer.

vapnik, v. n. (1982). estimation of dependences

based on empirical data. springer.

vapnik, v. n. (1995). the nature of statistical learn-

tipping, m. e. (1999). probabilistic visualisation of
high-dimensional binary data. in m. s. kearns,
s. a. solla, and d. a. cohn (eds.), advances
in neural information processing systems, vol-
ume 11, pp. 592   598. mit press.

tipping, m. e. (2001). sparse bayesian learning and
the relevance vector machine. journal of ma-
chine learning research 1, 211   244.

tipping, m. e. and c. m. bishop (1997). probabilis-
tic principal component analysis. technical re-
port ncrg/97/010, neural computing research
group, aston university.

tipping, m. e. and c. m. bishop (1999a). mixtures
of probabilistic principal component analyzers.
neural computation 11(2), 443   482.

tipping, m. e. and c. m. bishop (1999b). prob-
abilistic principal component analysis. journal
of the royal statistical society, series b 21(3),
611   622.

tipping, m. e. and a. faul (2003). fast marginal
likelihood maximization for sparse bayesian
models. in c. m. bishop and b. frey (eds.),
proceedings ninth international workshop on
arti   cial intelligence and statistics, key west,
florida.

tong, s. and d. koller (2000). restricted bayes op-
timal classi   ers. in proceedings 17th national
conference on arti   cial intelligence, pp. 658   
664. aaai.

tresp, v. (2001). scaling kernel-based systems to
large data sets. data mining and knowledge dis-
covery 5(3), 197   211.

uhlenbeck, g. e. and l. s. ornstein (1930). on the
theory of brownian motion. phys. rev. 36, 823   
841.

ing theory. springer.

vapnik, v. n. (1998). statistical learning theory. wi-

ley.

veropoulos, k., c. campbell, and n. cristianini
(1999). controlling the sensitivity of support
vector machines. in proceedings of the interna-
tional joint conference on arti   cial intelligence
(ijcai99), workshop ml3, pp. 55   60.

vidakovic, b.

(1999). statistical modelling by

wavelets. wiley.

viola, p. and m. jones (2004). robust real-time face
detection. international journal of computer vi-
sion 57(2), 137   154.

viterbi, a. j. (1967). error bounds for convolu-
tional codes and an asymptotically optimum de-
coding algorithm. ieee transactions on infor-
mation theory it-13, 260   267.

viterbi, a. j. and j. k. omura (1979). principles of
digital communication and coding. mcgraw-
hill.

wahba, g. (1975). a comparison of gcv and gml
for choosing the smoothing parameter in the gen-
eralized spline smoothing problem. numerical
mathematics 24, 383   393.

wainwright, m. j., t. s. jaakkola, and a. s. willsky
(2005). a new class of upper bounds on the log
partition function. ieee transactions on infor-
mation theory 51, 2313   2335.

walker, a. m. (1969). on the asymptotic behaviour
of posterior distributions. journal of the royal
statistical society, b 31(1), 80   88.

walker, s. g., p. damien, p. w. laud, and a. f. m.
smith (1999). bayesian nonparametric id136
for random distributions and related functions
(with discussion). journal of the royal statisti-
cal society, b 61(3), 485   527.

valiant, l. g. (1984). a theory of the learnable.
communications of the association for comput-
ing machinery 27, 1134   1142.

watson, g. s. (1964). smooth regression analysis.
sankhy  a: the indian journal of statistics. series
a 26, 359   372.

728

references

webb, a. r. (1994). functional approximation by
feed-forward networks: a least-squares approach
to generalisation. ieee transactions on neural
networks 5(3), 363   371.

williams, o., a. blake, and r. cipolla (2005).
sparse bayesian learning for ef   cient visual
tracking. ieee transactions on pattern analysis
and machine intelligence 27(8), 1292   1304.

williams, p. m. (1996). using neural networks to
model conditional multivariate densities. neural
computation 8(4), 843   854.

winn, j. and c. m. bishop (2005). variational mes-
sage passing. journal of machine learning re-
search 6, 661   694.

zarchan, p. and h. musoff (2005). fundamentals of
kalman filtering: a practical approach (sec-
ond ed.). aiaa.

weisstein, e. w. (1999). crc concise encyclopedia

of mathematics. chapman and hall, and crc.

weston, j. and c. watkins (1999). multi-class sup-
port vector machines. in m. verlysen (ed.), pro-
ceedings esann   99, brussels. d-facto publica-
tions.

whittaker, j. (1990). id114 in applied

multivariate statistics. wiley.

widrow, b. and m. e. hoff (1960). adaptive
switching circuits. in ire wescon convention
record, volume 4, pp. 96   104. reprinted in an-
derson and rosenfeld (1988).

widrow, b. and m. a. lehr (1990). 30 years of adap-
tive neural networks: id88, madeline, and
id26. proceedings of the ieee 78(9),
1415   1442.

wiegerinck, w. and t. heskes (2003). fractional
belief propagation. in s. becker, s. thrun, and
k. obermayer (eds.), advances in neural infor-
mation processing systems, volume 15, pp. 455   
462. mit press.

williams, c. k. i. (1998). computation with in   -
nite neural networks. neural computation 10(5),
1203   1216.

williams, c. k. i. (1999). prediction with gaussian
processes: from id75 to linear pre-
diction and beyond. in m. i. jordan (ed.), learn-
ing in id114, pp. 599   621. mit
press.

williams, c. k. i. and d. barber (1998). bayesian
classi   cation with gaussian processes. ieee
transactions on pattern analysis and machine
intelligence 20, 1342   1351.

williams, c. k. i. and m. seeger (2001). using the
nystrom method to speed up kernel machines. in
t. k. leen, t. g. dietterich, and v. tresp (eds.),
advances in neural information processing sys-
tems, volume 13, pp. 682   688. mit press.

index

729

index

page numbers in bold indicate the primary source of information for the corresponding topic.

1-of-k coding scheme, 424

acceptance criterion, 538, 541, 544
activation function, 180, 213, 227
active constraint, 328, 709
adaboost, 657, 658
adaline, 196
adaptive rejection sampling, 530
adf, see assumed density    ltering
aic, see akaike information criterion
akaike information criterion, 33, 217
   family of divergences, 469
   recursion, 620
ancestral sampling, 365, 525, 613
annular    ow, 679
ar model, see autoregressive model
arc, 360
ard, see automatic relevance determination
arma, see autoregressive moving average
assumed density    ltering, 510
autoassociative networks, 592
automatic relevance determination, 259, 312, 349,

485, 582

autoregressive hidden markov model, 632
autoregressive model, 609
autoregressive moving average, 304

back-tracking, 415, 630

backgammon, 3
id26, 241
id112, 656
basis function, 138, 172, 204, 227
batch training, 240
baum-welch algorithm, 618
bayes    theorem, 15
bayes, thomas, 21
bayesian analysis, vii, 9, 21

hierarchical, 372
model averaging, 654

bayesian information criterion, 33, 216
bayesian model comparison, 161, 473, 483
id110, 360
bayesian id203, 21
belief propagation, 403
bernoulli distribution, 69, 113, 685

mixture model, 444

bernoulli, jacob, 69
beta distribution, 71, 686
beta recursion, 621
between-class covariance, 189
bias, 27, 149
bias parameter, 138, 181, 227, 346
bias-variance trade-off, 147
bic, see bayesian information criterion
binary id178, 495
binomial distribution, 70, 686

730

index

biological sequence, 610
bipartite graph, 401
bits, 49
blind source separation, 591
blocked path, 374, 378, 384
boltzmann distribution, 387
boltzmann, ludwig eduard, 53
boolean logic, 21
boosting, 657
bootstrap, 23, 656
bootstrap    lter, 646
box constraints, 333, 342
box-muller method, 527

c4.5, 663
calculus of variations, 462
canonical correlation analysis, 565
canonical link function, 212
cart, see classi   cation and regression trees
cauchy distribution, 527, 529, 692
causality, 366
cca, see canonical correlation analysis
central differences, 246
central limit theorem, 78
chain graph, 393
chaining, 555
chapman-kolmogorov equations, 397
child node, 361
cholesky decomposition, 528
chunking, 335
circular normal, see von mises distribution
classical id203, 21
classi   cation, 3
classi   cation and regression trees, 663
clique, 385
id91, 3
clutter problem, 511
co-parents, 383, 492
code-book vectors, 429
combining models, 45, 653
committee, 655
complete data set, 440
completing the square, 86
computational learning theory, 326, 344
concave function, 56

concentration parameter, 108, 693
condensation algorithm, 646
conditional id178, 55
conditional expectation, 20
conditional independence, 46, 372, 383
conditional mixture model, see mixture model
id155, 14
conjugate prior, 68, 98, 117, 490
convex duality, 494
convex function, 55, 493
convolutional neural network, 267
correlation matrix, 567
cost function, 41
covariance, 20

between-class, 189
within-class, 189

covariance matrix
diagonal, 84
isotropic, 84
partitioned, 85, 307
positive de   nite, 308

cox   s axioms, 21
credit assignment, 3
cross-id178 error function, 206, 209, 235, 631,

666

cross-validation, 32, 161
cumulative distribution function, 18
curse of dimensionality, 33, 36
curve    tting, 4

d map, see dependency map
d-separation, 373, 378, 443
dag, see directed acyclic graph
dagid166, 339
data augmentation, 537
data compression, 429
decision boundary, 39, 179
decision region, 39, 179
decision surface, see decision boundary
decision theory, 38
decision tree, 654, 663, 673
decomposition methods, 335
degrees of freedom, 559
degrees-of-freedom parameter, 102, 693
density estimation, 3, 67

index

731

density network, 597
dependency map, 392
descendant node, 376
design matrix, 142, 347
differential id178, 53
digamma function, 687
directed acyclic graph, 362
directed cycle, 362
directed factorization, 381
dirichlet distribution, 76, 687
dirichlet, lejeune, 77
discriminant function, 43, 180, 181
discriminative model, 43, 203
distortion measure, 424
distributive law of multiplication, 396
dna, 610
document retrieval, 299
dual representation, 293, 329
dual-energy gamma densitometry, 678
id145, 411
dynamical system, 548

e step, see expectation step
early stopping, 259
ecm, see expectation conditional maximization
edge, 360
effective number of observations, 72, 101
effective number of parameters, 9, 170, 281
elliptical id116, 444
em, see expectation maximization
emission id203, 611
empirical bayes, see evidence approximation
energy function, 387
id178, 49

conditional, 55
differential, 53
relative, 55

ep, see expectation propagation
 -tube, 341
 -insensitive error function, 340
equality constraint, 709
equivalent kernel, 159, 301
erf function, 211
error id26, see id26
error function, 5, 23

error-correcting output codes, 339
euler, leonhard, 465
euler-lagrange equations, 705
evidence approximation, 165, 347, 581
evidence function, 161
expectation, 19
expectation conditional maximization, 454
expectation maximization, 113, 423, 440

gaussian mixture, 435
generalized, 454
sampling methods, 536

expectation propagation, 315, 468, 505
expectation step, 437
explaining away, 378
exploitation, 3
exploration, 3
exponential distribution, 526, 688
exponential family, 68, 113, 202, 490
extensive variables, 490

face detection, 2
face tracking, 355
factor analysis, 583

mixture model, 595

factor graph, 360, 399, 625
factor loading, 584
factorial hidden markov model, 633
factorized distribution, 464, 476
feature extraction, 2
feature map, 268
feature space, 292, 586
fisher information matrix, 298
fisher kernel, 298
fisher   s linear discriminant, 186
   ooding schedule, 417
forward kinematics, 272
forward problem, 272
forward propagation, 228, 243
forward-backward algorithm, 618
fractional belief propagation, 517
frequentist id203, 21
fuel system, 376
function interpolation, 299
functional, 462, 703
derivative, 463

732

index

gamma densitometry, 678
gamma distribution, 529, 688
gamma function, 71
gating function, 672
gauss, carl friedrich, 79
gaussian, 24, 78, 688

conditional, 85, 93
marginal, 88, 93
maximum likelihood, 93
mixture, 110, 270, 273, 430
sequential estimation, 94
suf   cient statistics, 93
wrapped, 110

gaussian kernel, 296
gaussian process, 160, 303
gaussian random    eld, 305
gaussian-gamma distribution, 101, 690
gaussian-wishart distribution, 102, 475, 478, 690
gem, see expectation maximization, generalized
generalization, 2
generalized linear model, 180, 213
generalized maximum likelihood, see evidence ap-

proximation

generative model, 43, 196, 297, 365, 572, 631
generative topographic mapping, 597

directional curvature, 599
magni   cation factor, 599

geodesic distance, 596
id150, 542

blocking, 546

gibbs, josiah willard, 543
gini index, 666
global minimum, 237
id119, 240
gram matrix, 293
graph-cut algorithm, 390
graphical model, 359

bipartite, 401
directed, 360
factorization, 362, 384
fully connected, 361
id136, 393
tree, 398
treewidth, 417
triangulated, 416

undirected, 360

green   s function, 299
gtm, see generative topographic mapping

hamilton, william rowan, 549
hamiltonian dynamics, 548
hamiltonian function, 549
hammersley-clifford theorem, 387
handwriting recognition, 1, 610, 614
handwritten digit, 565, 614, 677
head-to-head path, 376
head-to-tail path, 375
heaviside step function, 206
hellinger distance, 470
hessian matrix, 167, 215, 217, 238, 249

diagonal approximation, 250
exact evaluation, 253
fast multiplication, 254
   nite differences, 252
inverse, 252
outer product approximation, 251

heteroscedastic, 273, 311
hidden markov model, 297, 610

autoregressive, 632
factorial, 633
forward-backward algorithm, 618
input-output, 633
left-to-right, 613
maximum likelihood, 615
scaling factor, 627
sum-product algorithm, 625
switching, 644
variational id136, 625

hidden unit, 227
hidden variable, 84, 364, 430, 559
hierarchical bayesian model, 372
hierarchical mixture of experts, 673
hinge error function, 337
hinton diagram, 584
histogram density estimation, 120
hme, see hierarchical mixture of experts
hold-out set, 11
homogeneous    ow, 679
homogeneous kernel, 292
homogeneous markov chain, 540, 608

hooke   s law, 580
hybrid monte carlo, 548
hyperparameter, 71, 280, 311, 346, 372, 502
hyperprior, 372

i map, see independence map
i.i.d., see independent identically distributed
ica, see independent component analysis
icm, see iterated conditional modes
 , 663
identi   ability, 435
image de-noising, 387
importance sampling, 525, 532
importance weights, 533
improper prior, 118, 259, 472
imputation step, 537
imputation-posterior algorithm, 537
inactive constraint, 328, 709
incomplete data set, 440
independence map, 392
independent component analysis, 591
independent factor analysis, 592
independent identically distributed, 26, 379
independent variables, 17
independent, identically distributed, 605
induced factorization, 485
inequality constraint, 709
id136, 38, 42
information criterion, 33
information geometry, 298
id205, 48
input-output hidden markov model, 633
intensive variables, 490
intrinsic dimensionality, 559
invariance, 261
inverse gamma distribution, 101
inverse kinematics, 272
inverse problem, 272
inverse wishart distribution, 102
ip algorithm, see imputation-posterior algorithm
irls, see iterative reweighted least squares
ising model, 389
isomap, 596
isometric feature map, 596
iterated conditional modes, 389, 415

index

733

iterative reweighted least squares, 207, 210, 316,

354, 672

jacobian matrix, 247, 264
jensen   s inequality, 56
join tree, 416
junction tree algorithm, 392, 416

k nearest neighbours, 125
id116 id91 algorithm, 424, 443
k-medoids algorithm, 428
kalman    lter, 304, 637

extended, 644

kalman gain matrix, 639
kalman smoother, 637
karhunen-lo`eve transform, 561
karush-kuhn-tucker conditions, 330, 333, 342,

710

kernel density estimator, 122, 326
id81, 123, 292, 294

fisher, 298
gaussian, 296
homogeneous, 292
nonvectorial inputs, 297
stationary, 292

kernel pca, 586
kernel regression, 300, 302
kernel substitution, 292
kernel trick, 292
kinetic energy, 549
kkt, see karush-kuhn-tucker conditions
kl divergence, see id181
kriging, see gaussian process
id181, 55, 451, 468, 505

lagrange multiplier, 707
lagrange, joseph-louis, 329
lagrangian, 328, 332, 341, 708
laminar    ow, 678
laplace approximation, 213, 217, 278, 315, 354
laplace, pierre-simon, 24
large margin, see margin
lasso, 145
latent class analysis, 444
latent trait model, 597
latent variable, 84, 364, 430, 559

734

index

lattice diagram, 414, 611, 621, 629
lds, see linear dynamical system
leapfrog discretization, 551
learning, 2
learning rate parameter, 240
least-mean-squares algorithm, 144
leave-one-out, 33
likelihood function, 22
likelihood weighted sampling, 534
linear discriminant, 181

fisher, 186

linear dynamical system, 84, 635

id136, 638

linear independence, 696
id75, 138

em, 448
mixture model, 667
variational, 486
linear smoother, 159
linear-gaussian model, 87, 370
linearly separable, 179
link, 360
link function, 180, 213
liouville   s theorem, 550
lle, see locally linear embedding
lms algorithm, see least-mean-squares algorithm
local minimum, 237
local receptive    eld, 268
locally linear embedding, 596
location parameter, 118
log odds, 197
logic sampling, 525
id28, 205, 336

bayesian, 217, 498
mixture model, 670
multiclass, 209

logistic sigmoid, 114, 139, 197, 205, 220, 227, 495
logit function, 197
loopy belief propagation, 417
id168, 41
loss matrix, 41
lossless data compression, 429
lossy data compression, 429
lower bound, 484

m step, see maximization step

machine learning, vii
macrostate, 51
mahalanobis distance, 80
manifold, 38, 590, 595, 681
map, see maximum posterior
margin, 326, 327, 502

error, 334
soft, 332

marginal likelihood, 162, 165
marginal id203, 14
markov blanket, 382, 384, 545
markov boundary, see markov blanket
markov chain, 397, 539

   rst order, 607
homogeneous, 540, 608
second order, 608

id115, 537
markov model, 607

homogeneous, 612

markov network, see markov random    eld
markov random    eld, 84, 360, 383
max-sum algorithm, 411, 629
maximal clique, 385
maximal spanning tree, 416
maximization step, 437
maximum likelihood, 9, 23, 26, 116

gaussian mixture, 432
singularities, 480
type 2, see evidence approximation

maximum margin, see margin
maximum posterior, 30, 441
mcmc, see id115
mdn, see mixture density network
mds, see multidimensional scaling
mean, 24
mean    eld theory, 465
mean value theorem, 52
measure theory, 19
memory-based methods, 292
message passing, 396

pending message, 417
schedule, 417
variational, 491

metropolis algorithm, 538
metropolis-hastings algorithm, 541

index

735

microstate, 51
minimum risk, 44
minkowski loss, 48
missing at random, 441, 579
missing data, 579
mixing coef   cient, 111
mixture component, 111
mixture density network, 272, 673
mixture distribution, see mixture model
mixture model, 162, 423
conditional, 273, 666
id75, 667
id28, 670
symmetries, 483

mixture of experts, 672
mixture of gaussians, 110, 270, 273, 430
mlp, see multilayer id88
mnist data, 677
model comparison, 6, 32, 161, 473, 483
model evidence, 161
model selection, 162
moment matching, 506, 510
momentum variable, 548
monte carlo em algorithm, 536
monte carlo sampling, 24, 523
moore-penrose pseudo-inverse, see pseudo-inverse
moralization, 391, 401
mrf, see markov random    eld
multidimensional scaling, 596
multilayer id88, 226, 229
multimodality, 272
multinomial distribution, 76, 114, 690
multiplicity, 51
mutual information, 55, 57

nadaraya-watson, see kernel regression
naive bayes model, 46, 380
nats, 50
natural language modelling, 610
natural parameters, 113
nearest-neighbour methods, 124
neural network, 225

convolutional, 267
id173, 256
relation to gaussian process, 319

newton-raphson, 207, 317
node, 360
noiseless coding theorem, 50
nonidenti   ability, 585
noninformative prior, 23, 117
nonparametric methods, 68, 120
normal distribution, see gaussian
normal equations, 142
normal-gamma distribution, 101, 691
normal-wishart distribution, 102, 475, 478, 691
normalized exponential, see softmax function
novelty detection, 44
  -id166, 334

object recognition, 366
observed variable, 364
occam factor, 217
oil    ow data, 34, 560, 568, 678
old faithful data, 110, 479, 484, 681
on-line learning, see sequential learning
one-versus-one classi   er, 183, 339
one-versus-the-rest classi   er, 182, 338
ordered over-relaxation, 545
ornstein-uhlenbeck process, 305
orthogonal least squares, 301
outlier, 44, 185, 212
outliers, 103
over-   tting, 6, 147, 434, 464
over-relaxation, 544

pac learning, see probably approximately correct
pac-bayesian framework, 345
parameter shrinkage, 144
parent node, 361
particle    lter, 645
partition function, 386, 554
parzen estimator, see kernel density estimator
parzen window, 123
pattern recognition, vii
pca, see principal component analysis
pending message, 417
id88, 192

convergence theorem, 194
hardware, 196

id88 criterion, 193
perfect map, 392

736

index

periodic variable, 105
phase space, 549
photon noise, 680
plate, 363
polynomial curve    tting, 4, 362
polytree, 399
position variable, 548
positive de   nite covariance, 81
positive de   nite matrix, 701
positive semide   nite covariance, 81
positive semide   nite matrix, 701
posterior id203, 17
posterior step, 537
potential energy, 549
potential function, 386
power ep, 517
power method, 563
precision matrix, 85
precision parameter, 24
predictive distribution, 30, 156
preprocessing, 2
principal component analysis, 561, 572, 593

bayesian, 580
em algorithm, 577
id150, 583
mixture distribution, 595
physical analogy, 580

principal curve, 595
principal subspace, 561
principal surface, 596
prior, 17

conjugate, 68, 98, 117, 490
consistent, 257
improper, 118, 259, 472
noninformative, 23, 117

probabilistic graphical model, see graphical model
probabilistic pca, 570
id203, 12

bayesian, 21
classical, 21
density, 17
frequentist, 21
mass function, 19
prior, 45
product rule, 13, 14, 359

sum rule, 13, 14, 359
theory, 12

probably approximately correct, 344
probit function, 211, 219
probit regression, 210
product rule of id203, 13, 14, 359
proposal distribution, 528, 532, 538
protected conjugate gradients, 335
protein sequence, 610
pseudo-inverse, 142, 185
pseudo-random numbers, 526

quadratic discriminant, 199
quality parameter, 351

radial basis function, 292, 299
rauch-tung-striebel equations, 637
regression, 3
regression function, 47, 95
id173, 10

tikhonov, 267

regularized least squares, 144
id23, 3
reject option, 42, 45
rejection sampling, 528
relative id178, 55
relevance vector, 348
relevance vector machine, 161, 345
responsibility, 112, 432, 477
ridge regression, 10
rms error, see root-mean-square error
robbins-monro algorithm, 95
robot arm, 272
robustness, 103, 185
root node, 399
root-mean-square error, 6
rosenblatt, frank, 193
rotation invariance, 573, 585
rts equations, see rauch-tung-striebel equations
running intersection property, 416
rvm, see relevance vector machine

sample mean, 27
sample variance, 27
sampling-importance-resampling, 534
scale invariance, 119, 261

scale parameter, 119
scaling factor, 627
schwarz criterion, see bayesian information crite-

rion

self-organizing map, 598
sequential data, 605
sequential estimation, 94
sequential id119, 144, 240
sequential learning, 73, 143
sequential minimal optimization, 335
serial message passing schedule, 417
shannon, claude, 55
shared parameters, 368
shrinkage, 10
shur complement, 87
sigmoid, see logistic sigmoid
simplex, 76
single-class support vector machine, 339
singular value decomposition, 143
sinusoidal data, 682
sir, see sampling-importance-resampling
skip-layer connection, 229
slack variable, 331
slice sampling, 546
smo, see sequential minimal optimization
smoother matrix, 159
smoothing parameter, 122
soft margin, 332
soft weight sharing, 269
softmax function, 115, 198, 236, 274, 356, 497
som, see self-organizing map
sparsity, 145, 347, 349, 582
sparsity parameter, 351
spectrogram, 606
id103, 605, 610
sphereing, 568
spline functions, 139
standard deviation, 24
standardizing, 425, 567
state space model, 609

switching, 644

stationary kernel, 292
statistical bias, see bias
statistical independence, see independent variables

index

737

statistical learning theory, see computational learn-

ing theory, 326, 344

steepest descent, 240
stirling   s approximation, 51
stochastic, 5
stochastic em, 536
stochastic id119, 144, 240
stochastic process, 305
strati   ed    ow, 678
student   s t-distribution, 102, 483, 691
subsampling, 268
suf   cient statistics, 69, 75, 116
sum rule of id203, 13, 14, 359
sum-of-squares error, 5, 29, 184, 232, 662
sum-product algorithm, 399, 402

for hidden markov model, 625

supervised learning, 3
support vector, 330
support vector machine, 225

for regression, 339
multiclass, 338

survival of the    ttest, 646
svd, see singular value decomposition
id166, see support vector machine
switching hidden markov model, 644
switching state space model, 644
synthetic data sets, 682

tail-to-tail path, 374
tangent distance, 265
tangent propagation, 262, 263
tapped delay line, 609
target vector, 2
test set, 2, 32
threshold parameter, 181
tied parameters, 368
tikhonov id173, 267
time warping, 615
tomography, 679
training, 2
training set, 2
transition id203, 540, 610
translation invariance, 118, 261
tree-reweighted message passing, 517
treewidth, 417

738

index

trellis diagram, see lattice diagram
triangulated graph, 416
type 2 maximum likelihood, see evidence approxi-

mation

undetermined multiplier, see lagrange multiplier
undirected graph, see markov random    eld
uniform distribution, 692
uniform sampling, 534
uniquenesses, 584
unobserved variable, see latent variable
unsupervised learning, 3
utility function, 41

validation set, 11, 32
vapnik-chervonenkis dimension, 344
variance, 20, 24, 149
variational id136, 315, 462, 635

for gaussian mixture, 474
for hidden markov model, 625
local, 493

vc dimension, see vapnik-chervonenkis dimen-

sion

vector quantization, 429
vertex, see node
visualization, 3
viterbi algorithm, 415, 629
von mises distribution, 108, 693

wavelets, 139
weak learner, 657
weight decay, 10, 144, 257
weight parameter, 227
weight sharing, 268

soft, 269

weight vector, 181
weight-space symmetry, 231, 281
weighted least squares, 668
well-determined parameters, 170
whitening, 299, 568
wishart distribution, 102, 693
within-class covariance, 189
woodbury identity, 696
wrapped distribution, 110

yellowstone national park, 110, 681

