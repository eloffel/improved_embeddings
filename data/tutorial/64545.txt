a course inmachine learninghal daum   iiicopyright    2013   2017 hal daum   iii

self-published

http://ciml.info/

todo. . . .

second printing, january 2017

for my students and teachers.

often the same.

table of contents

about this book

id90

6

8

limits of learning

19

geometry and nearest neighbors

29

the id88

practical issues

41

55

beyond binary classification

73

linear models

87

bias and fairness

104

probabilistic modeling

116

1

2

3

4

5

6

7

8

9

10 neural networks

129

5

11 kernel methods

12

learning theory

13

ensemble methods

14

efficient learning

141

154

164

171

15 unsupervised learning

16

expectation maximization

178

186

17

id170

195

212

222

18

imitation learning

code and datasets

bibliography

223

index

225

about this book

machine learning is a broad and fascinating field. even
today, machine learning technology runs a substantial part of your
life, often without you knowing it. any plausible approach to arti   -
cial intelligence must involve learning, at some level, if for no other
reason than it   s hard to call a system intelligent if it cannot learn.
machine learning is also fascinating in its own right for the philo-
sophical questions it raises about what it means to learn and succeed
at tasks.

machine learning is also a very broad    eld, and attempting to

cover everything would be a pedagogical disaster. it is also so quickly
moving that any book that attempts to cover the latest developments
will be outdated before it gets online. thus, this book has two goals.
first, to be a gentle introduction to what is a very deep    eld. second,
to provide readers with the skills necessary to pick up new technol-
ogy as it is developed.

0.1 how to use this book

this book is designed to be read linearly, since it   s goal is not to be
a generic reference. that said, once you get through chapter 5, you
can pretty much jump anywhere. when i teach a one-semester un-
dergraduate course, i typically cover the chapter 1-13, sometimes
skipping 7 or 9 or 10 or 12 depending on time and interest. for a
graduate course for students with no prior machine learning back-
ground, i would very quickly blaze through 1-4, then cover the rest,
augmented with some additional reading.

0.2 why another textbook?

the purpose of this book is to provide a gentle and pedagogically orga-
nized introduction to the    eld. this is in contrast to most existing ma-
chine learning texts, which tend to organize things topically, rather

7

1 mitchell 1997

than pedagogically (an exception is mitchell   s book1, but unfortu-
nately that is getting more and more outdated). this makes sense for
researchers in the    eld, but less sense for learners. a second goal of
this book is to provide a view of machine learning that focuses on
ideas and models, not on math. it is not possible (or even advisable)
to avoid math. but math should be there to aid understanding, not
hinder it. finally, this book attempts to have minimal dependencies,
so that one can fairly easily pick and choose chapters to read. when
dependencies exist, they are listed at the start of the chapter.

the audience of this book is anyone who knows differential calcu-
lus and discrete math, and can program reasonably well. (a little bit
of id202 and id203 will not hurt.) an undergraduate in
their fourth or    fth semester should be fully capable of understand-
ing this material. however, it should also be suitable for    rst year
graduate students, perhaps at a slightly faster pace.

0.3 organization and auxilary material

there is an associated web page, http://ciml.info/, which contains
an online copy of this book, as well as associated code and data. it
also contains errata. please submit bug reports on github: github.com/
hal3/ciml.

0.4 acknowledgements

acknowledgements: i am indebted to many people for this book.
my teachers, especially rami grossberg (from whom the title of this
book was borrowed) and stefan schaal. students who have taken
machine learning from me over the past ten years, including those
who suffered through the initial versions of the class before i    gured
out how to teach it. especially scott alfeld, josh de bever, cecily
heiner, jeffrey ferraro, seth juarez, john moeller, jt olds, piyush
rai. people who have helped me edit, and who have submitted bug
reports, including todo. . . , but also check github for the latest list
of contributors!

1 | id90

learning objectives:
    explain the difference between

memorization and generalization.

    implement a decision tree classi   er.
    take a concrete task and cast it as a
learning problem, with a formal no-
tion of input space, features, output
space, generating distribution and
id168.

dependencies: none.

the words printed here are concepts.
you must go through the experiences.

    carl frederick

at a basic level, machine learning is about predicting the future
based on the past. for instance, you might wish to predict how much
a user alice will like a movie that she hasn   t seen, based on her rat-
ings of movies that she has seen. this prediction could be based on
many factors of the movies: their category (drama, documentary,
etc.), the language, the director and actors, the production company,
etc. in general, this means making informed guesses about some un-
observed property of some object, based on observed properties of
that object.

the    rst question we   ll ask is: what does it mean to learn? in

order to develop learning machines, we must know what learning
actually means, and how to determine success (or failure). you   ll see
this question answered in a very limited learning setting, which will
be progressively loosened and adapted throughout the rest of this
book. for concreteness, our focus will be on a very simple model of
learning called a decision tree.

1.1 what does it mean to learn?

alice has just begun taking a course on machine learning. she knows
that at the end of the course, she will be expected to have    learned   
all about this topic. a common way of gauging whether or not she
has learned is for her teacher, bob, to give her a exam. she has done
well at learning if she does well on the exam.

but what makes a reasonable exam? if bob spends the entire

semester talking about machine learning, and then gives alice an
exam on history of pottery, then alice   s performance on this exam
will not be representative of her learning. on the other hand, if the
exam only asks questions that bob has answered exactly during lec-
tures, then this is also a bad test of alice   s learning, especially if it   s
an    open notes    exam. what is desired is that alice observes speci   c
examples from the course, and then has to answer new, but related
questions on the exam. this tests whether alice has the ability to

generalize. generalization is perhaps the most central concept in
machine learning.

as a concrete example, consider a course id126

for undergraduate computer science students. we have a collection
of students and a collection of courses. each student has taken, and
evaluated, a subset of the courses. the evaluation is simply a score
from    2 (terrible) to +2 (awesome). the job of the recommender
system is to predict how much a particular student (say, alice) will
like a particular course (say, algorithms).

given historical data from course ratings (i.e., the past) we are
trying to predict unseen ratings (i.e., the future). now, we could
be unfair to this system as well. we could ask it whether alice is
likely to enjoy the history of pottery course. this is unfair because
the system has no idea what history of pottery even is, and has no
prior experience with this course. on the other hand, we could ask
it how much alice will like arti   cial intelligence, which she took
last year and rated as +2 (awesome). we would expect the system to
predict that she would really like it, but this isn   t demonstrating that
the system has learned: it   s simply recalling its past experience. in
the former case, we   re expecting the system to generalize beyond its
experience, which is unfair. in the latter case, we   re not expecting it
to generalize at all.

this general set up of predicting the future based on the past is

at the core of most machine learning. the objects that our algorithm
will make predictions about are examples. in the recommender sys-
tem setting, an example would be some particular student/course
pair (such as alice/algorithms). the desired prediction would be the
rating that alice would give to algorithms.

to make this concrete, figure 1.1 shows the general framework of
induction. we are given training data on which our algorithm is ex-
pected to learn. this training data is the examples that alice observes
in her machine learning course, or the historical ratings data for
the recommender system. based on this training data, our learning
algorithm induces a function f that will map a new example to a cor-
responding prediction. for example, our function might guess that
f (alice/machine learning) might be high because our training data
said that alice liked arti   cial intelligence. we want our algorithm
to be able to make lots of predictions, so we refer to the collection
of examples on which we will evaluate our algorithm as the test set.
the test set is a closely guarded secret: it is the    nal exam on which
our learning algorithm is being tested. if our algorithm gets to peek
at it ahead of time, it   s going to cheat and do better than it should.
the goal of inductive machine learning is to take some training

data and use it to induce a function f . this function f will be evalu-

id90

9

figure 1.1: the general supervised ap-
proach to machine learning: a learning
algorithm reads in training data and
computes a learned function f . this
function can then automatically label
future text examples.

?

why is it bad if the learning algo-
rithm gets to peek at the test data?

known labelstrainingdatalearningalgorithmf?testexamplepredictedlabel10 a course in machine learning

ated on the test data. the machine learning algorithm has succeeded
if its performance on the test data is high.

1.2 some canonical learning problems

there are a large number of typical inductive learning problems.
the primary difference between them is in what type of thing they   re
trying to predict. here are some examples:

regression: trying to predict a real value. for instance, predict the

value of a stock tomorrow given its past performance. or predict
alice   s score on the machine learning    nal exam based on her
homework scores.

binary classi   cation: trying to predict a simple yes/no response.
for instance, predict whether alice will enjoy a course or not.
or predict whether a user review of the newest apple product is
positive or negative about the product.

multiclass classi   cation: trying to put an example into one of a num-
ber of classes. for instance, predict whether a news story is about
entertainment, sports, politics, religion, etc. or predict whether a
cs course is systems, theory, ai or other.

ranking: trying to put a set of objects in order of relevance. for in-

stance, predicting what order to put web pages in, in response to a
user query. or predict alice   s ranked preferences over courses she
hasn   t taken.

the reason that it is convenient to break machine learning prob-
lems down by the type of object that they   re trying to predict has to
do with measuring error. recall that our goal is to build a system
that can make    good predictions.    this begs the question: what does
it mean for a prediction to be    good?    the different types of learning
problems differ in how they de   ne goodness. for instance, in regres-
sion, predicting a stock price that is off by $0.05 is perhaps much
better than being off by $200.00. the same does not hold of multi-
class classi   cation. there, accidentally predicting    entertainment   
instead of    sports    is no better or worse than predicting    politics.   

1.3 the decision tree model of learning

the decision tree is a classic and natural model of learning. it is
closely related to the fundamental computer science notion of    di-
vide and conquer.    although id90 can be applied to many

?

for each of these types of canon-
ical machine learning problems,
come up with one or two concrete
examples.

learning problems, we will begin with the simplest case: binary clas-
si   cation.

suppose that your goal is to predict whether some unknown user
will enjoy some unknown course. you must simply answer    yes    or
   no.    in order to make a guess, you   re allowed to ask binary ques-
tions about the user/course under consideration. for example:

you: is the course under consideration in systems?
me: yes
you: has this student taken any other systems courses?
me: yes
you: has this student liked most previous systems courses?
me: no
you: i predict this student will not like this course.
the goal in learning is to    gure out what questions to ask, in what

order to ask them, and what answer to predict once you have asked
enough questions.

the decision tree is so-called because we can write our set of ques-

tions and guesses in a tree format, such as that in figure 1.2. in this
   gure, the questions are written in the internal tree nodes (rectangles)
and the guesses are written in the leaves (ovals). each non-terminal
node has two children: the left child speci   es what to do if the an-
swer to the question is    no    and the right child speci   es what to do if
it is    yes.   

in order to learn, i will give you training data. this data consists
of a set of user/course examples, paired with the correct answer for
these examples (did the given user enjoy the given course?). from
this, you must construct your questions. for concreteness, there is a
small data set in table 1 in the appendix of this book. this training
data consists of 20 course rating examples, with course ratings and
answers to questions that you might ask about this pair. we will
interpret ratings of 0, +1 and +2 as    liked    and ratings of    2 and    1
as    hated.   

in what follows, we will refer to the questions that you can ask as
features and the responses to these questions as feature values. the
rating is called the label. an example is just a set of feature values.
and our training data is a set of examples, paired with labels.

there are a lot of logically possible trees that you could build,
even over just this small number of features (the number is in the
millions). it is computationally infeasible to consider all of these to
try to choose the    best    one. instead, we will build our decision tree
greedily. we will begin by asking:

if i could only ask one question, what question would i ask?
you want to    nd a feature that is most useful in helping you guess

whether this student will enjoy this course. a useful way to think

id90

11

figure 1.2: a decision tree for a course
recommender system, from which the
in-text    dialog    is drawn.

figure 1.3: a histogram of labels for (a)
the entire data set; (b-e) the examples
in the data set for each value of the    rst
four features.

issystems?takenothersys?morning?likedothersys?likenahnahlikelikenonononoyesyesyesyesoverall:easy:ai:systems:theory:60%40%likenah60%40%60%40%yesno82%18%33%67%yesno20%80%100%0%yesno80%20%40%60%yesno12 a course in machine learning

about this is to look at the histogram of labels for each feature. 1
this is shown for the    rst four features in figure 1.3. each histogram
shows the frequency of    like   /   hate    labels for each possible value
of an associated feature. from this    gure, you can see that asking
the    rst feature is not useful: if the value is    no    then it   s hard to
guess the label; similarly if the answer is    yes.    on the other hand,
asking the second feature is useful: if the value is    no,    you can be
pretty con   dent that this student will hate this course; if the answer
is    yes,    you can be pretty con   dent that this student will like this
course.

more formally, you will consider each feature in turn. you might

consider the feature    is this a system   s course?    this feature has two
possible value: no and yes. some of the training examples have an
answer of    no        let   s call that the    no    set. some of the training
examples have an answer of    yes        let   s call that the    yes    set. for
each set (no and yes) we will build a histogram over the labels.
this is the second histogram in figure 1.3. now, suppose you were
to ask this question on a random example and observe a value of
   no.    further suppose that you must immediately guess the label for
this example. you will guess    like,    because that   s the more preva-
lent label in the no set (actually, it   s the only label in the no set).
alternatively, if you receive an answer of    yes,    you will guess    hate   
because that is more prevalent in the yes set.

so, for this single feature, you know what you would guess if you
had to. now you can ask yourself: if i made that guess on the train-
ing data, how well would i have done? in particular, how many ex-
amples would i classify correctly? in the no set (where you guessed
   like   ) you would classify all 10 of them correctly. in the yes set
(where you guessed    hate   ) you would classify 8 (out of 10) of them
correctly. so overall you would classify 18 (out of 20) correctly. thus,
we   ll say that the score of the    is this a system   s course?    question is
18/20.

you will then repeat this computation for each of the available

features to us, compute the scores for each of them. when you must
choose which feature consider    rst, you will want to choose the one
with the highest score.

but this only lets you choose the    rst feature to ask about. this
is the feature that goes at the root of the decision tree. how do we
choose subsequent features? this is where the notion of divide and
conquer comes in. you   ve already decided on your    rst feature:    is
this a systems course?    you can now partition the data into two parts:
the no part and the yes part. the no part is the subset of the data
on which value for this feature is    no   ; the yes half is the rest. this
is the divide step.

1 a colleague related the story of
getting his 8-year old nephew to
guess a number between 1 and 100.
his nephew   s    rst four questions
were: is it bigger than 20? (yes) is
it even? (yes) does it have a 7 in it?
(no) is it 80? (no). it took 20 more
questions to get it, even though 10
should have been suf   cient. at 8,
the nephew hadn   t quite    gured out
how to divide and conquer. http:
//blog.computationalcomplexity.
org/2007/04/
getting-8-year-old-interested-in.
html.

?

how many training examples
would you classify correctly for
each of the other three features
from figure 1.3?

id90

13

return leaf(guess)

algorithm 1 decisiontreetrain(data, remaining features)
1: guess     most frequent answer in data
2: if the labels in data are unambiguous then
3:
4: else if remaining features is empty then
5:
6: else
7:

return leaf(guess)
for all f     remaining features do

// base case: no need to split further

// base case: cannot split further
// we need to query more features

// default answer for this data

no     the subset of data on which f =no
yes     the subset of data on which f =yes
score[f ]     # of majority vote answers in no
+ # of majority vote answers in yes

// the accuracy we would get if we only queried on f

end for
f     the feature with maximal score(f )
no     the subset of data on which f =no
yes     the subset of data on which f =yes
left     decisiontreetrain(no, remaining features \ {f})
right     decisiontreetrain(yes, remaining features \ {f})
return node(f , left, right)

8:

9:

10:

11:

12:

13:

14:

15:

16:

17:

18:
19: end if

algorithm 2 decisiontreetest(tree, test point)
1: if tree is of the form leaf(guess) then
2:
3: else if tree is of the form node(f , left, right) then
4:

if f = no in test point then

return guess

5:

6:

7:

return decisiontreetest(left, test point)

else

return decisiontreetest(right, test point)

end if

8:
9: end if

the conquer step is to recurse, and run the same routine (choosing
the feature with the highest score) on the no set (to get the left half
of the tree) and then separately on the yes set (to get the right half of
the tree).

at some point it will become useless to query on additional fea-

tures. for instance, once you know that this is a systems course,
you know that everyone will hate it. so you can immediately predict
   hate    without asking any additional questions. similarly, at some
point you might have already queried every available feature and still
not whittled down to a single answer. in both cases, you will need to
create a leaf node and guess the most prevalent answer in the current
piece of the training data that you are looking at.

putting this all together, we arrive at the algorithm shown in al-
gorithm 1.3.2 this function, decisiontreetrain takes two argu-

2 there are more nuanced algorithms
for building id90, some of
which are discussed in later chapters of
this book. they primarily differ in how
they compute the score function.

14 a course in machine learning

ments: our data, and the set of as-yet unused features. it has two
base cases: either the data is unambiguous, or there are no remaining
features. in either case, it returns a leaf node containing the most
likely guess at this point. otherwise, it loops over all remaining fea-
tures to    nd the one with the highest score. it then partitions the data
into a no/yes split based on the best feature. it constructs its left
and right subtrees by recursing on itself. in each recursive call, it uses
one of the partitions of the data, and removes the just-selected feature
from consideration.

the corresponding prediction algorithm is shown in algorithm 1.3.

this function recurses down the decision tree, following the edges
speci   ed by the feature values in some test point. when it reaches a
leaf, it returns the guess associated with that leaf.

1.4 formalizing the learning problem

as you   ve seen, there are several issues that we must take into ac-
count when formalizing the notion of learning.

    the performance of the learning algorithm should be measured on

unseen    test    data.

    the way in which we measure performance should depend on the

problem we are trying to solve.

    there should be a strong relationship between the data that our
algorithm sees at training time and the data it sees at test time.

in order to accomplish this, let   s assume that someone gives us a
id168, (cid:96)(  ,  ), of two arguments. the job of (cid:96) is to tell us how
   bad    a system   s prediction is in comparison to the truth. in particu-
lar, if y is the truth and   y is the system   s prediction, then (cid:96)(y,   y) is a
measure of error.

for three of the canonical tasks discussed above, we might use the

following id168s:
regression: squared loss (cid:96)(y,   y) = (y       y)2

or absolute loss (cid:96)(y,   y) = |y       y|.

binary classi   cation: zero/one loss (cid:96)(y,   y) =

multiclass classi   cation: also zero/one loss.

(cid:40)

if y =   y

0
1 otherwise

note that the id168 is something that you must decide on

based on the goals of learning.

now that we have de   ned our id168, we need to consider

where the data (training and test) comes from. the model that we

?

is algorithm 1.3 guaranteed to
terminate?

this notation means that the loss is zero
if the prediction is correct and is one
otherwise.

?

why might it be a bad idea to use
zero/one loss to measure perfor-
mance for a regression problem?

id90

15

math review | expectated values
(x,y)   d[(cid:96)(y, f (x))] for the expected loss. expectation means    average.    this is saying    if you
we write e
drew a bunch of (x, y) pairs independently at random from d, what would your average loss be?more
formally, if d is a discrete id203 distribution, then this expectation can be expanded as:

e

(x,y)   d[(cid:96)(y, f (x))] =    
(x,y)   d

[d(x, y)(cid:96)(y, f (x))]

(1.1)

this is exactly the weighted average loss over the all (x, y) pairs in d, weighted by their id203,
d(x, y). if d is a    nite discrete distribution, for instance de   ned by a    nite data set {(x1, y1), . . . , (xn, yn)
that puts equal weight on each example (id203 1/n), then we get:

[d(x, y)(cid:96)(y, f (x))]

de   nition of expectation

e

(x,y)   d[(cid:96)(y, f (x))] =    
(x,y)   d
n   

=

d is discrete and    nite

de   nition of d

rearranging terms

(1.2)

(1.3)

(1.4)

(1.5)

[d(xn, yn)(cid:96)(yn, f (xn))]

n=1

n   

n=1
1
n

[

1
n
n   

n=1

=

=

(cid:96)(yn, f (xn))]

[(cid:96)(yn, f (xn))]

which is exactly the average loss on that dataset.

the most important thing to remember is that there are two equivalent ways to think about expections:
(1) the expectation of some function g is the weighted average value of g, where the weights are given by
the underlying id203 distribution. (2) the expectation of some function g is your best guess of the
value of g if you were to draw a single item from the underlying id203 distribution.

figure 1.4:

will use is the probabilistic model of learning. namely, there is a prob-
ability distribution d over input/output pairs. this is often called
the data generating distribution. if we write x for the input (the
user/course pair) and y for the output (the rating), then d is a distri-
bution over (x, y) pairs.

a useful way to think about d is that it gives high id203 to
reasonable (x, y) pairs, and low id203 to unreasonable (x, y)
pairs. a (x, y) pair can be unreasonable in two ways. first, x might
be an unusual input. for example, a x related to an    intro to java   
course might be highly probable; a x related to a    geometric and
solid modeling    course might be less probable. second, y might
be an unusual rating for the paired x. for instance, if alice were to
take ai 100 times (without remembering that she took it before!),
she would give the course a +2 almost every time. perhaps some

16 a course in machine learning

semesters she might give a slightly lower score, but it would be un-
likely to see x =alice/ai paired with y =    2.
it is important to remember that we are not making any assump-
tions about what the distribution d looks like. (for instance, we   re
not assuming it looks like a gaussian or some other, common distri-
bution.) we are also not assuming that we know what d is. in fact,
if you know a priori what your data generating distribution is, your
learning problem becomes signi   cantly easier. perhaps the hardest
thing about machine learning is that we don   t know what d is: all we
get is a random sample from it. this random sample is our training
data.

our learning problem, then, is de   ned by two quantities:

1. the id168 (cid:96), which captures our notion of what is important

to learn.

2. the data generating distribution d, which de   nes what sort of

data we expect to see.

?

consider the following prediction
task. given a paragraph written
about a course, we have to predict
whether the paragraph is a positive
or negative review of the course.
(this is the id31 prob-
lem.) what is a reasonable loss
function? how would you de   ne
the data generating distribution?

we are given access to training data, which is a random sample of

input/output pairs drawn from d. based on this training data, we
need to induce a function f that maps new inputs   x to corresponding
prediction   y. the key property that f should obey is that it should do
well (as measured by (cid:96)) on future examples that are also drawn from
d. formally, it   s expected loss   over d with repsect to (cid:96) should be
as small as possible:

(x,y)   d(cid:2)(cid:96)(y, f (x))(cid:3) =    

  (cid:44) e

(x,y)

d(x, y)(cid:96)(y, f (x))

(1.6)

the dif   culty in minimizing our expected loss from eq (1.6) is
that we don   t know what d is! all we have access to is some training
data sampled from it! suppose that we denote our training data
set by d. the training data consists of n-many input/output pairs,
(x1, y1), (x2, y2), . . . , (xn, yn). given a learned function f , we can
compute our training error,    :

    (cid:44) 1
n

n   

n=1

(cid:96)(yn, f (xn))

(1.7)

that is, our training error is simply our average error over the train-

ing data.

of course, we can drive     to zero by simply memorizing our train-

ing data. but as alice might    nd in memorizing past exams, this
might not generalize well to a new exam!

this is the fundamental dif   culty in machine learning: the thing

we have access to is our training error,    . but the thing we care about

?

(x,y)   d

(cid:2)(cid:96)(y, f (x))(cid:3), by thinking

verify by calculation that we
can write our training error as
e
of d as a distribution that places
id203 1/n to each example in
d and id203 0 on everything
else.

id90

17

minimizing is our expected error  . in order to get the expected error
down, our learned function needs to generalize beyond the training
data to some future data that it might not have seen yet!

so, putting it all together, we get a formal de   nition of induction
machine learning: given (i) a id168 (cid:96) and (ii) a sample d
from some unknown distribution d, you must compute a function
f that has low expected error   over d with respect to (cid:96).

a very important comment is that we should never expect a ma-
chine learning algorithm to generalize beyond the data distribution
it has seen at training time. in a famous   if posssibly apocryphal   
example from the 1970s, the us government wanted to train a clas-
si   er to distinguish between us tanks and russian tanks. they col-
lected a training and test set, and managed to build a classi   er with
nearly 100% accuracy on that data. but when this classi   er was run
in the    real world   , it failed miserably. it had not, in fact, learned
to distinguish between us tanks and russian tanks, but rather just
between clear photos and blurry photos. in this case, there was a bias
in the training data (due to how the training data was collected) that
caused the learning algorithm to learn something other than what we
were hoping for. we will return to this issue in chapter 8; for now,
simply remember that the distribution d for training data must match
the distribution d for the test data.

1.5 chapter summary and outlook

at this point, you should be able to use id90 to do machine
learning. someone will give you data. you   ll split it into training,
development and test portions. using the training and development
data, you   ll    nd a good value for maximum depth that trades off
between under   tting and over   tting. you   ll then run the resulting
decision tree model on the test data to get an estimate of how well
you are likely to do in the future.

you might think: why should i read the rest of this book? aside
from the fact that machine learning is just an awesome fun    eld to
learn about, there   s a lot left to cover. in the next two chapters, you   ll
learn about two models that have very different inductive biases than
id90. you   ll also get to see a very useful way of thinking
about learning: the geometric view of data. this will guide much of
what follows. after that, you   ll learn how to solve problems more
complicated that simple binary classi   cation. (machine learning
people like binary classi   cation a lot because it   s one of the simplest
non-trivial problems that we can work on.) after that, things will
diverge: you   ll learn about ways to think about learning as a formal
optimization problem, ways to speed up learning, ways to learn

18 a course in machine learning

without labeled data (or with very little labeled data) and all sorts of
other fun topics.

but throughout, we will focus on the view of machine learning

that you   ve seen here. you select a model (and its associated induc-
tive biases). you use data to    nd parameters of that model that work
well on the training data. you use development data to avoid under-
   tting and over   tting. and you use test data (which you   ll never look
at or touch, right?) to estimate future model performance. then you
conquer the world.

1.6 further reading

in our discussion of id90, we used misclassi   cation rate for
selecting features. while simple and intuitive, misclassi   cation rate
has problems. there has been a signi   cant amount of work that
considers more advanced splitting criteria; the most popular is  ,
based on the mutual information quantity from information the-
ory. we have also only considered a very simple mechanism for
controlling inductive bias: limiting the depth of the decision tree.
again, there are more advanced    tree pruning    techniques that typ-
ically operate by growing deep trees and then pruning back some
of the branches. these approaches have the advantage that differ-
ent branches can have different depths, accounting for the fact that
the amount of data that gets passed down each branch might differ
dramatically3.

3 quinlan 1986

2 | limits of learning

learning objectives:
    de   ne    inductive bias    and recog-

nize the role of inductive bias in
learning.

    illustrate how id173 trades
off between under   tting and over   t-
ting.

    evaluate whether a use of test data

is    cheating    or not.

dependencies: none.

our lives sometimes depend on computers performing as pre-
dicted.

    philip emeagwali

machine learning is a very general and useful framework,
but it is not    magic    and will not always work. in order to better
understand when it will and when it will not work, it is useful to
formalize the learning problem more. this will also help us develop
debugging strategies for learning algorithms.

2.1 data generating distributions

our underlying assumption for the majority of this book is that
learning problems are characterized by some unknown id203
distribution d over input/output pairs (x, y)     x  y. suppose that
someone told you what d was. in particular, they gave you a python
function computed that took two inputs, x and y, and returned the
id203 of that x, y pair under d. if you had access to such a func-
tion, classi   cation becomes simple. we can de   ne the bayes optimal
classi   er as the classi   er that, for any test input   x, simply returns the
  y that maximizes computed(   x,   y), or, more formally:

f (bo)(   x) = arg max

  y   y d(   x,   y)

(2.1)

this classi   er is optimal in one speci   c sense: of all possible classi   ers,
it achieves the smallest zero/one error.

theorem 1 (bayes optimal classi   er). the bayes optimal classi   er
f (bo) achieves minimal zero/one error of any deterministic classi   er.

this theorem assumes that you are comparing against deterministic
classi   ers. you can actually prove a stronger result that f (bo) is opti-
mal for randomized classi   ers as well, but the proof is a bit messier.
however, the intuition is the same: for a given x, f (bo) chooses the
label with highest id203, thus minimizing the id203 that it
makes an error.

proof of theorem 1. consider some other classi   er g that claims to
be better than f (bo). then, there must be some x on which g(x) (cid:54)=

20 a course in machine learning

f (bo)(x). fix such an x. now, the id203 that f (bo) makes an error
on this particular x is 1     d(x, f (bo)(x)) and the id203 that g
makes an error on this x is 1     d(x, g(x)). but f (bo) was chosen in
such a way to maximize d(x, f (bo)(x)), so this must be greater than
d(x, g(x)). thus, the id203 that f (bo) errs on this particular x is
smaller than the id203 that g errs on it. this applies to any x for
which f (bo)(x) (cid:54)= g(x) and therefore f (bo) achieves smaller zero/one
error than any g.

the bayes error rate (or bayes optimal error rate) is the error

rate of the bayes optimal classi   er. it is the best error rate you can
ever hope to achieve on this classi   cation problem (under zero/one
loss). the take-home message is that if someone gave you access to
the data distribution, forming an optimal classi   er would be trivial.
unfortunately, no one gave you this distribution, so we need to    gure
out ways of learning the mapping from x to y given only access to a
training set sampled from d, rather than d itself.

2.2

inductive bias: what we know before the data arrives

in figure 2.1 you   ll    nd training data for a binary classi   cation prob-
lem. the two labels are    a    and    b    and you can see four examples
for each label. below, in figure 2.2, you will see some test data. these
images are left unlabeled. go through quickly and, based on the
training data, label these images. (really do it before you read fur-
ther! i   ll wait!)

most likely you produced one of two labelings: either abba or

aabb. which of these solutions is right? the answer is that you can-
not tell based on the training data. if you give this same example
to 100 people, 60     70 of them come up with the abba prediction
and 30     40 come up with the aabb prediction. why? presumably
because the    rst group believes that the relevant distinction is be-
tween    bird    and    non-bird    while the second group believes that
the relevant distinction is between       y    and    no-   y.   

this preference for one distinction (bird/non-bird) over another
(   y/no-   y) is a bias that different human learners have. in the con-
text of machine learning, it is called inductive bias: in the absense of
data that narrow down the relevant concept, what type of solutions
are we more likely to prefer? two thirds of people seem to have an
inductive bias in favor of bird/non-bird, and one third seem to have
an inductive bias in favor of    y/no-   y.

throughout this book you will learn about several approaches to

machine learning. the decision tree model is the    rst such approach.
these approaches differ primarily in the sort of inductive bias that

figure 2.1: training data for a binary
classi   cation problem.

figure 2.2: test data for the same
classi   cation problem.

?

it is also possible that the correct
classi   cation on the test data is
abab. this corresponds to the bias
   is the background in focus.    some-
how no one seems to come up with
this classi   cation rule.

class aclassblimits of learning 21

they exhibit.

consider a variant of the decision tree learning algorithm. in this
variant, we will not allow the trees to grow beyond some pre-de   ned
maximum depth, d. that is, once we have queried on d-many fea-
tures, we cannot query on any more and must just make the best
guess we can at that point. this variant is called a shallow decision
tree.

the key question is: what is the inductive bias of shallow decision
trees? roughly, their bias is that decisions can be made by only look-
ing at a small number of features. for instance, a shallow decision
tree would be very good at learning a function like    students only
like ai courses.    it would be very bad at learning a function like    if
this student has liked an odd number of their past courses, they will
like the next one; otherwise they will not.    this latter is the parity
function, which requires you to inspect every feature to make a pre-
diction. the inductive bias of a decision tree is that the sorts of things
we want to learn to predict are more like the    rst example and less
like the second example.

2.3 not everything is learnable

although machine learning works well   perhaps astonishingly
well   in many cases, it is important to keep in mind that it is not
magical. there are many reasons why a machine learning algorithm
might fail on some learning task.

there could be noise in the training data. noise can occur both

at the feature level and at the label level. some features might corre-
spond to measurements taken by sensors. for instance, a robot might
use a laser range    nder to compute its distance to a wall. however,
this sensor might fail and return an incorrect value. in a sentiment
classi   cation problem, someone might have a typo in their review of
a course. these would lead to noise at the feature level. there might
also be noise at the label level. a student might write a scathingly
negative review of a course, but then accidentally click the wrong
button for the course rating.

the features available for learning might simply be insuf   cient.

for example, in a medical context, you might wish to diagnose
whether a patient has cancer or not. you may be able to collect a
large amount of data about this patient, such as gene expressions,
x-rays, family histories, etc. but, even knowing all of this information
exactly, it might still be impossible to judge for sure whether this pa-
tient has cancer or not. as a more contrived example, you might try
to classify course reviews as positive or negative. but you may have
erred when downloading the data and only gotten the    rst    ve char-

22 a course in machine learning

acters of each review. if you had the rest of the features you might
be able to do well. but with this limited feature set, there   s not much
you can do.

some examples may not have a single correct answer. you might

be building a system for    safe web search,    which removes offen-
sive web pages from search results. to build this system, you would
collect a set of web pages and ask people to classify them as    offen-
sive    or not. however, what one person considers offensive might be
completely reasonable for another person. it is common to consider
this as a form of label noise. nevertheless, since you, as the designer
of the learning system, have some control over this problem, it is
sometimes helpful to isolate it as a source of dif   culty.

finally, learning might fail because the inductive bias of the learn-
ing algorithm is too far away from the concept that is being learned.
in the bird/non-bird data, you might think that if you had gotten
a few more training examples, you might have been able to tell
whether this was intended to be a bird/non-bird classi   cation or a
   y/no-   y classi   cation. however, no one i   ve talked to has ever come
up with the    background is in focus    classi   cation. even with many
more training points, this is such an unusual distinction that it may
be hard for anyone to    gure out it. in this case, the inductive bias of
the learner is simply too misaligned with the target classi   cation to
learn.

note that the inductive bias source of error is fundamentally dif-

ferent than the other three sources of error. in the inductive bias case,
it is the particular learning algorithm that you are using that cannot
cope with the data. maybe if you switched to a different learning
algorithm, you would be able to learn well. for instance, neptunians
might have evolved to care greatly about whether backgrounds are
in focus, and for them this would be an easy classi   cation to learn.
for the other three sources of error, it is not an issue to do with the
particular learning algorithm. the error is a fundamental part of the
learning problem.

2.4 under   tting and over   tting

as with many problems, it is useful to think about the extreme cases
of learning algorithms. in particular, the extreme cases of decision
trees. in one extreme, the tree is    empty    and we do not ask any
questions at all. we simply immediately make a prediction. in the
other extreme, the tree is    full.    that is, every possible question
is asked along every branch. in the full tree, there may be leaves
with no associated training data. for these we must simply choose
arbitrarily whether to say    yes    or    no.   

consider the course recommendation data from table 1. sup-

pose we were to build an    empty    decision tree on this data. such a
decision tree will make the same prediction regardless of its input,
because it is not allowed to ask any questions about its input. since
there are more    likes    than    hates    in the training data (12 versus
8), our empty decision tree will simply always predict    likes.    the
training error,    , is 8/20 = 40%.

on the other hand, we could build a    full    decision tree. since

each row in this data is unique, we can guarantee that any leaf in a
full decision tree will have either 0 or 1 examples assigned to it (20
of the leaves will have one example; the rest will have none). for the
leaves corresponding to training points, the full decision tree will
always make the correct prediction. given this, the training error,    , is
0/20 = 0%.

of course our goal is not to build a model that gets 0% error on

the training data. this would be easy! our goal is a model that will
do well on future, unseen data. how well might we expect these two
models to do on future data? the    empty    tree is likely to do not
much better and not much worse on future data. we might expect
that it would continue to get around 40% error.

life is more complicated for the    full    decision tree. certainly
if it is given a test example that is identical to one of the training
examples, it will do the right thing (assuming no noise). but for
everything else, it will only get about 50% error. this means that
even if every other test point happens to be identical to one of the
training points, it would only get about 25% error. in practice, this is
probably optimistic, and maybe only one in every 10 examples would
match a training example, yielding a 35% error.

so, in one case (empty tree) we   ve achieved about 40% error and

in the other case (full tree) we   ve achieved 35% error. this is not
very promising! one would hope to do better! in fact, you might
notice that if you simply queried on a single feature for this data, you
would be able to get very low training error, but wouldn   t be forced
to    guess    randomly.

this example illustrates the key concepts of under   tting and

over   tting. under   tting is when you had the opportunity to learn
something but didn   t. a student who hasn   t studied much for an up-
coming exam will be under   t to the exam, and consequently will not
do well. this is also what the empty tree does. over   tting is when
you pay too much attention to idiosyncracies of the training data,
and aren   t able to generalize well. often this means that your model
is    tting noise, rather than whatever it is supposed to    t. a student
who memorizes answers to past exam questions without understand-
ing them has over   t the training data. like the full tree, this student

limits of learning 23

?

convince yourself (either by proof
or by simulation) that even in the
case of imbalanced data     for in-
stance data that is on average 80%
positive and 20% negative     a pre-
dictor that guesses randomly (50/50
positive/negative) will get about
50% error.

?

which feature is it, and what is it   s
training error?

24 a course in machine learning

math review | law of large numbers
consider some random event, like spins of a roulette wheel, cars driving through an intersection, the
outcome of an election, or pasta being appropriately al dente. we often want to make a conclusion
about the entire population (the pot of pasta) based on a much smaller sample (biting a couple pieces
of pasta). the law of large numbers tells us that under mild conditions this is an okay thing to do.

formally, suppose that v1, v2, . . . , vn are random variables (e.g., vn measures if the nth spaghetti is
al dente). assume that these random variables are independent (i.e., v2 and v3 are uncorrelated   
they weren   t both taken from the same place in the pot) and identically distributed (they were all
drawn from the same population   pot   that we wish to measure). we can compute the sample av-
n=1 vn and under the strong law of large numbers, you can prove that   v     e[v] as
erage   v = 1
n        . namely, the empirical sample average approaches the population average as the number of
samples goes do in   nity.

n    n

(cid:16)

(cid:17)

(technical note: the notion of convergence here is almost sure convergence. in particular, the formal result is
limn       1
that pr
population average.)

= 1. or, in words, with id203 one the sample average reaches the

n    n vn = e[v]

figure 2.3:

also will not do well on the exam. a model that is neither over   t nor
under   t is the one that is expected to do best in the future.

2.5 separation of training and test data

suppose that, after graduating, you get a job working for a company
that provides personalized recommendations for pottery. you go in
and implement new algorithms based on what you learned in your
machine learning class (you have learned the power of generaliza-
tion!). all you need to do now is convince your boss that you have
done a good job and deserve a raise!

how can you convince your boss that your fancy learning algo-

rithms are really working?

based on what we   ve talked about already with under   tting and
over   tting, it is not enough to just tell your boss what your training
error is. noise notwithstanding, it is easy to get a training error of
zero using a simple database query (or grep, if you prefer). your boss
will not fall for that.

the easiest approach is to set aside some of your available data as
   test data    and use this to evaluate the performance of your learning
algorithm. for instance, the pottery recommendation service that you
work for might have collected 1000 examples of pottery ratings. you
will select 800 of these as training data and set aside the    nal 200

limits of learning 25

?

if you have more data at your dis-
posal, why might a 90/10 split be
preferable to an 80/20 split?

as test data. you will run your learning algorithms only on the 800
training points. only once you   re done will you apply your learned
model to the 200 test points, and report your test error on those 200
points to your boss.

the hope in this process is that however well you do on the 200
test points will be indicative of how well you are likely to do in the
future. this is analogous to estimating support for a presidential
candidate by asking a small (random!) sample of people for their
opinions. statistics (speci   cally, concentration bounds of which the
   central limit theorem    is a famous example) tells us that if the sam-
ple is large enough, it will be a good representative. the 80/20 split
is not magic: it   s simply fairly well established. occasionally people
use a 90/10 split instead, especially if they have a lot of data.

the cardinal rule of machine learning is: never touch your test

data. ever. if that   s not clear enough:
never ever touch your test data!
if there is only one thing you learn from this book, let it be that.

do not look at your test data. even once. even a tiny peek. once
you do that, it is not test data any more. yes, perhaps your algorithm
hasn   t seen it. but you have. and you are likely a better learner than
your learning algorithm. consciously or otherwise, you might make
decisions based on whatever you might have seen. once you look at
the test data, your model   s performance on it is no longer indicative
of it   s performance on future unseen data. this is simply because
future data is unseen, but your    test    data no longer is.

2.6 models, parameters and hyperparameters

the general approach to machine learning, which captures many ex-
isting learning algorithms, is the modeling approach. the idea is that
we come up with some formal model of our data. for instance, we
might model the classi   cation decision of a student/course pair as a
decision tree. the choice of using a tree to represent this model is our
choice. we also could have used an arithmetic circuit or a polynomial
or some other function. the model tells us what sort of things we can
learn, and also tells us what our inductive bias is.

for most models, there will be associated parameters. these are

the things that we use the data to decide on. parameters in a decision
tree include: the speci   c questions we asked, the order in which we
asked them, and the classi   cation decisions at the leaves. the job of
our decision tree learning algorithm decisiontreetrain is to take
data and    gure out a good set of parameters.

26 a course in machine learning

many learning algorithms will have additional knobs that you can

adjust. in most cases, these knobs amount to tuning the inductive
bias of the algorithm. in the case of the decision tree, an obvious
knob that one can tune is the maximum depth of the decision tree.
that is, we could modify the decisiontreetrain function so that
it stops recursing once it reaches some pre-de   ned maximum depth.
by playing with this depth knob, we can adjust between under   tting
(the empty tree, depth= 0) and over   tting (the full tree, depth=    ).
such a knob is called a hyperparameter. it is so called because it

is a parameter that controls other parameters of the model. the exact
de   nition of hyperparameter is hard to pin down: it   s one of those
things that are easier to identify than de   ne. however, one of the
key identi   ers for hyperparameters (and the main reason that they
cause consternation) is that they cannot be naively adjusted using the
training data.

in decisiontreetrain, as in most machine learning, the learn-

ing algorithm is essentially trying to adjust the parameters of the
model so as to minimize training error. this suggests an idea for
choosing hyperparameters: choose them so that they minimize train-
ing error.

what is wrong with this suggestion? suppose that you were to

treat    maximum depth    as a hyperparameter and tried to tune it on
your training data. to do this, maybe you simply build a collection
of id90, tree0, tree1, tree2, . . . , tree100, where treed is a tree
of maximum depth d. we then computed the training error of each
of these trees and chose the    ideal    maximum depth as that which
minimizes training error? which one would it pick?

the answer is that it would pick d = 100. or, in general, it would

pick d as large as possible. why? because choosing a bigger d will
never hurt on the training data. by making d larger, you are simply
encouraging over   tting. but by evaluating on the training data, over-
   tting actually looks like a good idea!

an alternative idea would be to tune the maximum depth on test

data. this is promising because test data peformance is what we
really want to optimize, so tuning this knob on the test data seems
like a good idea. that is, it won   t accidentally reward over   tting. of
course, it breaks our cardinal rule about test data: that you should
never touch your test data. so that idea is immediately off the table.
however, our    test data    wasn   t magic. we simply took our 1000

examples, called 800 of them    training    data and called the other 200
   test    data. so instead, let   s do the following. let   s take our original
1000 data points, and select 700 of them as training data. from the
remainder, take 100 as development data1 and the remaining 200
as test data. the job of the development data is to allow us to tune

?

go back to the decisiontree-
train algorithm and modify it so
that it takes a maximum depth pa-
rameter. this should require adding
two lines of code and modifying
three others.

1 some people call this    validation
data    or    held-out data.   

hyperparameters. the general approach is as follows:

1. split your data into 70% training data, 10% development data and

20% test data.

2. for each possible setting of your hyperparameters:

(a) train a model using that setting of hyperparameters on the

training data.

(b) compute this model   s error rate on the development data.

3. from the above collection of models, choose the one that achieved

the lowest error rate on development data.

4. evaluate that model on the test data to estimate future test perfor-

mance.

2.7 real world applications of machine learning

figure 2.4 shows a typical sequence of decisions that must be made
to deploy a machine learning approach in the real world. in the left
column, you can see the generic decision being made. in the right
column, an example of this decision for the particular case of adver-
tising placement on a search engine we   ve built.

in this sequence, (1) we have some real world goal like increasing

revenue for our search engine, and decide to try to increase rev-
enue by (2) displaying better ads. we convert this task into a ma-
chine learning problem by (3) deciding to train a classi   er to predict
whether a user will click on an ad or not. in order to apply machine
learning, we must collect some training data; in this case, (4) we col-
lect data by logging user interactions with the current system. we
must choose what to log; (5) we choose to log the ad being displayed,
the query the user entered into our search engine, and binary value
showing if they clicked or not.

in order to make these logs consumable by a machine learning
algorithm, (6) we convert the data into input/output pairs: in this
case, pairs of words from a bag-of-words representing the query and
a bag-of-words representing the ad as input, and the click as a   
label. we then (7) select a model family (e.g., depth 20 id90),
and thereby an inductive bias, for instance depth     20 id90.

we   re now ready to (8) select a speci   c subset of data to use as
training data: in this case, data from april 2016. we split this into
training and development and (9) learn a    nal decision tree, tuning
the maximum depth on the development data. we can then use this
decision tree to (10) make predictions on some held-out test data, in

limits of learning 27

?

in step 3, you could either choose
the model (trained on the 70% train-
ing data) that did the best on the
development data. or you could
choose the hyperparameter settings
that did best and retrain the model
on the 80% union of training and
development data. is either of these
options obviously better or worse?

1

2

3

4

5

6

7

8

9

10

11

12

real world
goal
real world
mechanism
learning
problem

data collection

collected data
data
representation
select model
family
select training
data
train model &
hyperparams
predict on test
data

evaluate error

deploy!

increase
revenue
better ad
display
classify
click-through
interaction w/
current system

query, ad, click
bow2,    click
id90,
depth 20
subset from
april   16
   nal decision
tree
subset from
may   16
zero/one loss
for    click
(hope we
achieve our
goal)

figure 2.4: a typical design process for
a machine learning application.

28 a course in machine learning

this case from the following month. we can (11) measure the overall
quality of our predictor as zero/one loss (clasi   cation error) on this
test data and    nally (12) deploy our system.

the important thing about this sequence of steps is that in any

one, things can go wrong. that is, between any two rows of this table,
we are necessarily accumulating some additional error against our
original real world goal of increasing revenue. for example, in step 5,
we decided on a representation that left out many possible variables
we could have logged, like time of day or season of year. by leaving
out those variables, we set an explicit upper bound on how well our
learned system can do.

it is often an effective strategy to run an oracle experiment. in an

oracle experiment, we assume that everything below some line can be
solved perfectly, and measure how much impact that will have on a
higher line. as an extreme example, before embarking on a machine
learning approach to the ad display problem, we should measure
something like: if our classi   er were perfect, how much more money
would we make? if the number is not very high, perhaps there is
some better for our time.

finally, although this sequence is denoted linearly, the entire pro-

cess is highly interactive in practice. a large part of    debugging   
machine learning (covered more extensively in chapter 5 involves
trying to    gure out where in this sequence the biggest losses are and
   xing that step. in general, it is often useful to build the stupidest thing
that could possibly work, then look at how well it   s doing, and decide if
and where to    x it.

2.8 further reading

todo further reading

3 | geometry and nearest neighbors

learning objectives:
    describe a data set as points in a

high dimensional space.

    explain the curse of dimensionality.
    compute distances between points

in high dimensional space.

    implement a k-nearest neighbor

model of learning.

    draw decision boundaries.
    implement the id116 algorithm

for id91.

dependencies: chapter 1

our brains have evolved to get us out of the rain,    nd where the
berries are, and keep us from getting killed. our brains did not
evolve to help us grasp really large numbers or to look at things in
a hundred thousand dimensions.
    ronald graham

you can think of prediction tasks as mapping inputs (course
reviews) to outputs (course ratings). as you learned in the previ-
ous chapter, decomposing an input into a collection of features (e.g.,
words that occur in the review) forms a useful abstraction for learn-
ing. therefore, inputs are nothing more than lists of feature values.
this suggests a geometric view of data, where we have one dimen-
sion for every feature. in this view, examples are points in a high-
dimensional space.

once we think of a data set as a collection of points in high dimen-

sional space, we can start performing geometric operations on this
data. for instance, suppose you need to predict whether alice will
like algorithms. perhaps we can try to    nd another student who is
most    similar    to alice, in terms of favorite courses. say this student
is jeremy. if jeremy liked algorithms, then we might guess that alice
will as well. this is an example of a nearest neighbor model of learn-
ing. by inspecting this model, we   ll see a completely different set of
answers to the key learning questions we discovered in chapter 1.

3.1 from data to feature vectors

an example is just a collection of feature values about that example,
for instance the data in table 1 from the appendix. to a person, these
features have meaning. one feature might count how many times the
reviewer wrote    excellent    in a course review. another might count
the number of exclamation points. a third might tell us if any text is
underlined in the review.

to a machine, the features themselves have no meaning. only

the feature values, and how they vary across examples, mean some-
thing to the machine. from this perspective, you can think about an
example as being represented by a feature vector consisting of one
   dimension    for each feature, where each dimenion is simply some
real value.

consider a review that said    excellent    three times, had one excla-

30 a course in machine learning

mation point and no underlined text. this could be represented by
the feature vector (cid:104)3, 1, 0(cid:105). an almost identical review that happened
to have underlined text would have the feature vector (cid:104)3, 1, 1(cid:105).

note, here, that we have imposed the convention that for binary

features (yes/no features), the corresponding feature values are 0
and 1, respectively. this was an arbitrary choice. we could have
made them 0.92 and    16.1 if we wanted. but 0/1 is convenient and
helps us interpret the feature values. when we discuss practical
issues in chapter 5, you will see other reasons why 0/1 is a good
choice.

figure 3.1 shows the data from table 1 in three views. these three
views are constructed by considering two features at a time in differ-
ent pairs. in all cases, the plusses denote positive examples and the
minuses denote negative examples. in some cases, the points fall on
top of each other, which is why you cannot see 20 unique points in
all    gures.

the mapping from feature values to vectors is straighforward in

the case of real valued features (trivial) and binary features (mapped
to zero or one). it is less clear what to do with categorical features.
for example, if our goal is to identify whether an object in an image
is a tomato, blueberry, cucumber or cockroach, we might want to
know its color: is it red, blue, green or black?

one option would be to map red to a value of 0, blue to a value
of 1, green to a value of 2 and black to a value of 3. the problem
with this mapping is that it turns an unordered set (the set of colors)
into an ordered set (the set {0, 1, 2, 3}). in itself, this is not necessarily
a bad thing. but when we go to use these features, we will measure
examples based on their distances to each other. by doing this map-
ping, we are essentially saying that red and blue are more similar
(distance of 1) than red and black (distance of 3). this is probably
not what we want to say!

a solution is to turn a categorical feature that can take four dif-
ferent values (say: red, blue, green and black) into four binary
features (say: isitred?, isitblue?, isitgreen? and isitblack?). in gen-
eral, if we start from a categorical feature that takes v values, we can
map it to v-many binary indicator features.

with that, you should be able to take a data set and map each

example to a feature vector through the following mapping:

    real-valued features get copied directly.

    binary features become 0 (for false) or 1 (for true).

    categorical features with v possible values get mapped to v-many

binary indicator features.

figure 3.1: a    gure showing projections
of data in two dimension in three
ways     see text. top: horizontal axis
corresponds to the    rst feature (easy)
and the vertical axis corresponds to
the second feature (ai?); middle:
horizontal is second feature and vertical
is third (systems?); bottom: horizontal
is    rst and vertical is third. truly,
the data points would like exactly on
(0, 0) or (1, 0), etc., but they have been
purturbed slightly to show duplicates.
?

match the example ids from table 1
with the points in figure 3.1.

?

the computer scientist in you might
be saying: actually we could map it
to log2 v-many binary features! is
this a good idea or not?

easy?ai?ai?sys?easy?sys?geometry and nearest neighbors

31

after this mapping, you can think of a single example as a vec-
tor in a high-dimensional feature space. if you have d-many fea-
tures (after expanding categorical features), then this feature vector
will have d-many components. we will denote feature vectors as
x = (cid:104)x1, x2, . . . , xd(cid:105), so that xd denotes the value of the dth fea-
ture of x. since these are vectors with real-valued components in
d-dimensions, we say that they belong to the space rd.

for d = 2, our feature vectors are just points in the plane, like in
figure 3.1. for d = 3 this is three dimensional space. for d > 3 it
becomes quite hard to visualize. (you should resist the temptation
to think of d = 4 as    time        this will just make things confusing.)
unfortunately, for the sorts of problems you will encounter in ma-
chine learning, d     20 is considered    low dimensional,    d     1000 is
   medium dimensional    and d     100000 is    high dimensional.   

3.2 k-nearest neighbors

?

can you think of problems (per-
haps ones already mentioned in this
book!) that are low dimensional?
that are medium dimensional?
that are high dimensional?

the biggest advantage to thinking of examples as vectors in a high
dimensional space is that it allows us to apply geometric concepts
to machine learning. for instance, one of the most basic things
that one can do in a vector space is compute distances. in two-
dimensional space, the distance between (cid:104)2, 3(cid:105) and (cid:104)6, 1(cid:105) is given

by(cid:112)(2     6)2 + (3     1)2 =

18     4.24. in general, in d-dimensional

   

space, the euclidean distance between vectors a and b is given by
eq (3.1) (see figure 3.2 for geometric intuition in three dimensions):

(cid:35) 1

2

(cid:34) d   

d=1

d(a, b) =

(ad     bd)2

(3.1)

now that you have access to distances between examples, you

can start thinking about what it means to learn again. consider fig-
ure 3.3. we have a collection of training data consisting of positive
examples and negative examples. there is a test point marked by a
question mark. your job is to guess the correct label for that point.

most likely, you decided that the label of this test point is positive.

one reason why you might have thought that is that you believe
that the label for an example should be similar to the label of nearby
points. this is an example of a new form of inductive bias.

the nearest neighbor classi   er is build upon this insight. in com-

parison to id90, the algorithm is ridiculously simple. at
training time, we simply store the entire training set. at test time,
we get a test example   x. to predict its label, we    nd the training ex-
ample x that is most similar to   x. in particular, we    nd the training

figure 3.2: a    gure showing euclidean
distance in three dimensions. the
length of the green segments are 0.6, 0.6
and 0.3 respectively, in the x-, y-, and
z-axes. the total distance between the
   
red dot and the orange dot is therefore

0.62 + 0.62 + 0.32 = 0.9.

?

verify that d from eq (3.1) gives the
same result (4.24) for the previous
computation.

(0, .4, .5)(.6, 1, .8)?32 a course in machine learning

example x that minimizes d(x,   x). since x is a training example, it has
a corresponding label, y. we predict that the label of   x is also y.
despite its simplicity, this nearest neighbor classi   er is incred-
ibly effective. (some might say frustratingly effective.) however, it
is particularly prone to over   tting label noise. consider the data in
figure 3.4. you would probably want to label the test point positive.
unfortunately, it   s nearest neighbor happens to be negative. since the
nearest neighbor algorithm only looks at the single nearest neighbor,
it cannot consider the    preponderance of evidence    that this point
should probably actually be a positive example. it will make an un-
necessary error.

a solution to this problem is to consider more than just the single
nearest neighbor when making a classi   cation decision. we can con-
sider the k-nearest neighbors and let them vote on the correct class
for this test point. if you consider the 3-nearest neighbors of the test
point in figure 3.4, you will see that two of them are positive and one
is negative. through voting, positive would win.

the full algorithm for k-nearest neighbor classi   cation is given

in algorithm 3.2. note that there actually is no    training    phase for
k-nearest neighbors. in this algorithm we have introduced    ve new
conventions:

1. the training data is denoted by d.

2. we assume that there are n-many training examples.

3. these examples are pairs (x1, y1), (x2, y2), . . . , (xn, yn).

(warning: do not confuse xn, the nth training example, with xd,
the dth feature for example x.)

4. we use [ ]to denote an empty list and        to append    to that list.
5. our prediction on   x is called   y.

the    rst step in this algorithm is to compute distances from the
test point to all training points (lines 2-4). the data points are then
sorted according to distance. we then apply a clever trick of summing
the class labels for each of the k nearest neighbors (lines 6-10) and
using the sign of this sum as our prediction.

the big question, of course, is how to choose k. as we   ve seen,
with k = 1, we run the risk of over   tting. on the other hand, if
k is large (for instance, k = n), then knn-predict will always
predict the majority class. clearly that is under   tting. so, k is a
hyperparameter of the knn algorithm that allows us to trade-off
between over   tting (small value of k) and under   tting (large value of
k).

figure 3.4: a    gure showing an easy
nn classi   cation problem where the
test point is a ? and should be positive,
but its nn is actually a negative point
that   s noisy.
?

why is it a good idea to use an odd
number for k?

?

why is the sign of the sum com-
puted in lines 2-4 the same as the
majority vote of the associated
training examples?

?

why can   t you simply pick the
value of k that does best on the
training data? in other words, why
do we have to treat it like a hy-
perparameter rather than just a
parameter.

?geometry and nearest neighbors

33

// store distance to training example n

// put lowest-distance objects    rst

s     s     (cid:104)d(xn,   x), n(cid:105)

algorithm 3 knn-predict(d, k,   x)
1: s     [ ]
2: for n = 1 to n do
3:
4: end for
5: s     sort(s)
  y     0
6:
7: for k = 1 to k do
(cid:104)dist,n(cid:105)     sk
8:
  y       y + yn

9:
10: end for
11: return sign(   y)

// n this is the kth closest data point
// vote according to the label for the nth training point
// return +1 if   y > 0 and    1 if   y < 0

one aspect of inductive bias that we   ve seen for knn is that it
assumes that nearby points should have the same label. another
aspect, which is quite different from id90, is that all features
are equally important! recall that for id90, the key question
was which features are most useful for classi   cation? the whole learning
algorithm for a decision tree hinged on    nding a small set of good
features. this is all thrown away in knn classi   ers: every feature
is used, and they are all used the same amount. this means that if
you have data with only a few relevant features and lots of irrelevant
features, knn is likely to do poorly.

a related issue with knn is feature scale. suppose that we are
trying to classify whether some object is a ski or a snowboard (see
figure 3.5). we are given two features about this data: the width
and height. as is standard in skiing, width is measured in millime-
ters and height is measured in centimeters. since there are only two
features, we can actually plot the entire training set; see figure 3.6
where ski is the positive class. based on this data, you might guess
that a knn classi   er would do well.

suppose, however, that our measurement of the width was com-

puted in millimeters (instead of centimeters). this yields the data
shown in figure 3.7. since the width values are now tiny, in compar-
ison to the height values, a knn classi   er will effectively ignore the
width values and classify almost purely based on height. the pre-
dicted class for the displayed test point had changed because of this
feature scaling.

we will discuss feature scaling more in chapter 5. for now, it is

just important to keep in mind that knn does not have the power to
decide which features are important.

figure 3.5: a    gure of a ski and a
snowboard.

figure 3.6: classi   cation data for ski vs
snowboard in 2d

figure 3.7: classi   cation data for ski vs
snowboard in 2d, with width rescaled
to mm.

34 a course in machine learning

math review | vector arithmetic and vector norms
a (real-valued) vector is just an array of real values, for instance x = (cid:104)1, 2.5,   6(cid:105) is a three-dimensional
vector. in general, if x = (cid:104)x1, x2, . . . , xd(cid:105), then xd is it   s dth component. so x3 =    6 in the previous ex-
ample.
vector sums are computed pointwise, and are only de   ned when dimensions match, so (cid:104)1, 2.5,   6(cid:105) +
(cid:104)2,   2.5, 3(cid:105) = (cid:104)3, 0,   3(cid:105). in general, if c = a + b then cd = ad + bd for all d. vector addition can
be viewed geometrically as taking a vector a, then tacking on b to the end of it; the new end point is
exactly c.
vectors can be scaled by real values; for instance 2(cid:104)1, 2.5,   6(cid:105) = (cid:104)2, 5,   12(cid:105); this is called scalar multi-
plication. in general, ax = (cid:104)ax1, ax2, . . . , axd(cid:105).
the norm of a vector x, written ||x|| is its length. unless otherwise speci   ed, this is its euclidean length,
namely: ||x|| =

(cid:113)

   d x2
d.

3.3 decision boundaries

figure 3.8:

the standard way that we   ve been thinking about learning algo-
rithms up to now is in the query model. based on training data, you
learn something. i then give you a query example and you have to
guess it   s label.

an alternative, less passive, way to think about a learned model
is to ask: what sort of test examples will it classify as positive, and
what sort will it classify as negative. in figure 3.9, we have a set of
training data. the background of the image is colored blue in regions
that would be classi   ed as positive (if a query were issued there)
and colored red in regions that would be classi   ed as negative. this
coloring is based on a 1-nearest neighbor classi   er.

in figure 3.9, there is a solid line separating the positive regions
from the negative regions. this line is called the decision boundary
for this classi   er. it is the line with positive land on one side and
negative land on the other side.

decision boundaries are useful ways to visualize the complex-

ity of a learned model. intuitively, a learned model with a decision
boundary that is really jagged (like the coastline of norway) is really
complex and prone to over   tting. a learned model with a decision
boundary that is really simple (like the bounary between arizona
and utah) is potentially under   t.

now that you know about decision boundaries, it is natural to ask:

what do decision boundaries for id90 look like? in order

figure 3.9: decision boundary for 1nn.

figure 3.10: decision boundary for knn
with k=3.

geometry and nearest neighbors

35

to answer this question, we have to be a bit more formal about how
to build a decision tree on real-valued features. (remember that the
algorithm you learned in the previous chapter implicitly assumed
binary feature values.) the idea is to allow the decision tree to ask
questions of the form:    is the value of feature 5 greater than 0.2?   
that is, for real-valued features, the decision tree nodes are param-
eterized by a feature and a threshold for that feature. an example
decision tree for classifying skis versus snowboards is shown in fig-
ure 3.11.

now that a decision tree can handle feature vectors, we can talk
about decision boundaries. by example, the decision boundary for
the decision tree in figure 3.11 is shown in figure 3.12. in the    gure,
space is    rst split in half according to the    rst query along one axis.
then, depending on which half of the space you look at, it is either
split again along the other axis, or simply classi   ed.

figure 3.12 is a good visualization of decision boundaries for

id90 in general. their decision boundaries are axis-aligned
cuts. the cuts must be axis-aligned because nodes can only query on
a single feature at a time. in this case, since the decision tree was so
shallow, the decision boundary was relatively simple.

3.4 id116 id91

up through this point, you have learned all about supervised learn-
ing (in particular, binary classi   cation). as another example of the
use of geometric intuitions and data, we are going to temporarily
consider an unsupervised learning problem. in unsupervised learn-
ing, our data consists only of examples xn and does not contain corre-
sponding labels. your job is to make sense of this data, even though
no one has provided you with correct labels. the particular notion of
   making sense of    that we will talk about now is the id91 task.
consider the data shown in figure 3.13. since this is unsupervised

learning and we do not have access to labels, the data points are
simply drawn as black dots. your job is to split this data set into
three clusters. that is, you should label each data point as a, b or c
in whatever way you want.

for this data set, it   s pretty clear what you should do. you prob-

ably labeled the upper-left set of points a, the upper-right set of
points b and the bottom set of points c. or perhaps you permuted
these labels. but chances are your clusters were the same as mine.
the id116 id91 algorithm is a particularly simple and

effective approach to producing clusters on data like you see in fig-
ure 3.13. the idea is to represent each cluster by it   s cluster center.
given cluster centers, we can simply assign each point to its nearest

figure 3.11: decision tree for ski vs.
snowboard

figure 3.12: decision boundary for dt in
previous    gure

?

what sort of data might yield a
very simple decision boundary with
a decision tree and very complex
decision boundary with 1-nearest
neighbor? what about the other
way around?

figure 3.13: simple id91 data...
clusters in ul, ur and bc.

36 a course in machine learning

center. similarly, if we know the assignment of points to clusters, we
can compute the centers. this introduces a chicken-and-egg problem.
if we knew the clusters, we could compute the centers. if we knew
the centers, we could compute the clusters. but we don   t know either.
the general computer science answer to chicken-and-egg problems

is iteration. we will start with a guess of the cluster centers. based
on that guess, we will assign each data point to its closest center.
given these new assignments, we can recompute the cluster centers.
we repeat this process until clusters stop moving. the    rst few it-
erations of the id116 algorithm are shown in figure 3.14. in this
example, the clusters converge very quickly.

algorithm 3.4 spells out the id116 id91 algorithm in de-
tail. the cluster centers are initialized randomly. in line 6, data point
xn is compared against each cluster center   k. it is assigned to cluster
k if k is the center with the smallest distance. (that is the    argmin   
step.) the variable zn stores the assignment (a value from 1 to k) of
example n. in lines 8-12, the cluster centers are re-computed. first, xk
stores all examples that have been assigned to cluster k. the center of
cluster k,   k is then computed as the mean of the points assigned to
it. this process repeats until the centers converge.

an obvious question about this algorithm is: does it converge?
a second question is: how long does it take to converge. the    rst
question is actually easy to answer. yes, it does. and in practice, it
usually converges quite quickly (usually fewer than 20 iterations). in
chapter 15, we will actually prove that it converges. the question of
how long it takes to converge is actually a really interesting question.
even though the id116 algorithm dates back to the mid 1950s, the
best known convergence rates were terrible for a long time. here, ter-
rible means exponential in the number of data points. this was a sad
situation because empirically we knew that it converged very quickly.
new algorithm analysis techniques called    smoothed analysis    were
invented in 2001 and have been used to show very fast convergence
for id116 (among other algorithms). these techniques are well
beyond the scope of this book (and this author!) but suf   ce it to say
that id116 is fast in practice and is provably fast in theory.

it is important to note that although id116 is guaranteed to

converge and guaranteed to converge quickly, it is not guaranteed to
converge to the    right answer.    the key problem with unsupervised
learning is that we have no way of knowing what the    right answer   
is. convergence to a bad solution is usually due to poor initialization.

figure 3.14:    rst few iterations of
id116 running on previous data set

?

what is the difference between un-
supervised and supervised learning
that means that we know what the
   right answer    is for supervised
learning but not for unsupervised
learning?

  k     some random location

algorithm 4 id116(d, k)
1: for k = 1 to k do
2:
3: end for
4: repeat
5:

for n = 1 to n do

zn     argmink ||  k     xn||

6:

7:

8:

9:

10:

end for
for k = 1 to k do
xk     { xn : zn = k }
  k     mean(xk)

end for

11:
12: until   s stop changing
13: return z

geometry and nearest neighbors

37

// randomly initialize center for kth cluster

// assign example n to closest center

// points assigned to cluster k
// re-estimate center of cluster k

// return cluster assignments

3.5 warning: high dimensions are scary

visualizing one hundred dimensional space is incredibly dif   cult for
humans. after huge amounts of training, some people have reported
that they can visualize four dimensional space in their heads. but
beyond that seems impossible.1

in addition to being hard to visualize, there are at least two addi-
tional problems in high dimensions, both refered to as the curse of
dimensionality. one is computational, the other is mathematical.
from a computational perspective, consider the following prob-
lem. for k-nearest neighbors, the speed of prediction is slow for a
very large data set. at the very least you have to look at every train-
ing example every time you want to make a prediction. to speed
things up you might want to create an indexing data structure. you
can break the plane up into a grid like that shown in figure 3.15.
now, when the test point comes in, you can quickly identify the grid
cell in which it lies. now, instead of considering all training points,
you can limit yourself to training points in that grid cell (and perhaps
the neighboring cells). this can potentially lead to huge computa-
tional savings.
space up into a grid whose cells are 0.2  0.2, we can clearly do this
with 25 grid cells in two dimensions (assuming the range of the
features is 0 to 1 for simplicity). in three dimensions, we   ll need
125 = 5  5  5 grid cells. in four dimensions, we   ll need 625. by the
time we get to    low dimensional    data in 20 dimensions, we   ll need
95, 367, 431, 640, 625 grid cells (that   s 95 trillion, which is about 6 to
7 times the us national debt as of january 2011). so if you   re in 20
dimensions, this gridding technique will only be useful if you have at
least 95 trillion training examples.

in two dimensions, this procedure is effective. if we want to break

1 if you want to try to get an intu-
itive sense of what four dimensions
looks like, i highly recommend the
short 1884 book flatland: a romance
of many dimensions by edwin abbott
abbott. you can even read it online at
gutenberg.org/ebooks/201.

figure 3.15: 2d knn with an overlaid
grid, cell with test point highlighted

38 a course in machine learning

for    medium dimensional    data (approximately 1000) dimesions,

the number of grid cells is a 9 followed by 698 numbers before the
decimal point. for comparison, the number of atoms in the universe
is approximately 1 followed by 80 zeros. so even if each atom yielded
a googul training examples, we   d still have far fewer examples than
grid cells. for    high dimensional    data (approximately 100000) di-
mensions, we have a 1 followed by just under 70, 000 zeros. far too
big a number to even really comprehend.

suf   ce it to say that for even moderately high dimensions, the
amount of computation involved in these problems is enormous.
in addition to the computational dif   culties of working in high
dimensions, there are a large number of strange mathematical oc-
curances there. in particular, many of your intuitions that you   ve
built up from working in two and three dimensions just do not carry
over to high dimensions. we will consider two effects, but there are
countless others. the    rst is that high dimensional spheres look more
like porcupines than like balls.2 the second is that distances between
points in high dimensions are all approximately the same.

let   s start in two dimensions as in figure 3.16. we   ll start with

four green spheres, each of radius one and each touching exactly two
other green spheres. (remember that in two dimensions a    sphere   
is just a    circle.   ) we   ll place a red sphere in the middle so that it
touches all four green spheres. we can easily compute the radius of
this small sphere. the pythagorean theorem says that 12 + 12 = (1 +
2     1     0.41. thus, by calculation,
r)2, so solving for r we get r =
the blue sphere lies entirely within the cube (cube = square) that
contains the grey spheres. (yes, this is also obvious from the picture,
but perhaps you can see where this is going.)

   

now we can do the same experiment in three dimensions, as

shown in figure 3.17. again, we can use the pythagorean theorem
to compute the radius of the blue sphere. now, we get 12 + 12 + 12 =
3     1     0.73. this is still entirely enclosed in the
(1 + r)2, so r =
cube of width four that holds all eight grey spheres.

   

at this point it becomes dif   cult to produce    gures, so you   ll

have to apply your imagination. in four dimensions, we would have
16 green spheres (called hyperspheres), each of radius one. they
would still be inside a cube (called a hypercube) of width four. the
4     1 = 1. continuing
blue hypersphere would have radius r =
   
to    ve dimensions, the blue hypersphere embedded in 256 green
hyperspheres would have radius r =

5     1     1.23 and so on.

   

in general, in d-dimensional space, there will be 2d green hyper-

spheres of radius one. each green hypersphere will touch exactly
n-many other hyperspheres. the blue hyperspheres in the middle
will touch them all and will have radius r =

d     1.

   

?

how does the above analysis relate
to the number of data points you
would need to    ll out a full decision
tree with d-many features? what
does this say about the importance
of shallow trees?

2 this result was related to me by mark
reid, who heard about it from marcus
hutter.

figure 3.16: 2d spheres in spheres

figure 3.17: 3d spheres in spheres

geometry and nearest neighbors

39

think about this for a moment. as the number of dimensions

grows, the radius of the blue hypersphere grows without bound!. for
   
example, in 9-dimensions the radius of the blue hypersphere is now
9     1 = 2. but with a radius of two, the blue hypersphere is now
   squeezing    between the green hypersphere and touching the edges
of the hypercube. in 10 dimensional space, the radius is approxi-
mately 2.16 and it pokes outside the cube.

the second strange fact we will consider has to do with the dis-
tances between points in high dimensions. we start by considering
random points in one dimension. that is, we generate a fake data set
consisting of 100 random points between zero and one. we can do
the same in two dimensions and in three dimensions. see figure ??
for data distributed uniformly on the unit hypercube in different
dimensions.

now, pick two of these points at random and compute the dis-
tance between them. repeat this process for all pairs of points and
average the results. for the data shown in figure ??, the average
distance between points in one dimension is about 0.346; in two di-
mensions is about 0.518; and in three dimensions is 0.615. the fact
that these increase as the dimension increases is not surprising. the
furthest two points can be in a 1-dimensional hypercube (line) is 1;
the furthest in a 2-dimensional hypercube (square) is
2 (opposite
corners); the furthest in a 3-d hypercube is
the furthest two points in a d-dimensional hypercube will be

   
d.
you can actually compute these values analytically. write unid
for the uniform distribution in d dimensions. the quantity we are
interested in computing is:

   

   

3 and so on. in general,

(cid:104)

(cid:104) ||a     b||(cid:105)(cid:105)

avgdist(d) = ea   unid

eb   unid

(3.2)

   
we can actually compute this in closed form and arrive at avgdist(d) =

d/3. because we know that the maximum distance between two
d, this says that the ratio between average dis-

points grows like
tance and maximum distance converges to 1/3.

   

   

what is more interesting, however, is the variance of the distribu-
tion of distances. you can show that in d dimensions, the variance
18, independent of d. this means that when you look
is constant 1/
at (variance) divided-by (max distance), the variance behaves like
1/
as d grows3.

18d, which means that the effective variance continues to shrink

   

when i    rst saw and re-proved this result, i was skeptical, as i
imagine you are. so i implemented it. in figure 3.18 you can see
the results. this presents a histogram of distances between random
points in d dimensions for d     {1, 2, 3, 10, 20, 100}. as you can see,
d, even for
all of these distances begin to concentrate around 0.4

   

3 brin 1995

figure 3.18: histogram of distances in
d=2,8,32,128,512

0.00.20.40.60.81.0distance / sqrt(dimensionality)02000400060008000100001200014000# of pairs of points at that distancedimensionality versus uniform point distances2 dims8 dims32 dims128 dims512 dims40 a course in machine learning

   medium dimension    problems.

you should now be terri   ed: the only bit of information that knn

gets is distances. and you   ve just seen that in moderately high di-
mensions, all distances becomes equal. so then isn   t it the case that
knn simply cannot work?

the answer has to be no. the reason is that the data that we get

is not uniformly distributed over the unit hypercube. we can see this
by looking at two real-world data sets. the    rst is an image data set
of hand-written digits (zero through nine); see section ??. although
this data is originally in 256 dimensions (16 pixels by 16 pixels), we
can arti   cally reduce the dimensionality of this data. in figure 3.19
you can see the histogram of average distances between points in this
data at a number of dimensions.

as you can see from these histograms, distances have not con-
centrated around a single value. this is very good news: it means
that there is hope for learning algorithms to work! nevertheless, the
moral is that high dimensions are weird.

3.6 further reading

todo further reading

figure 3.19: knn:mnist: histogram of
distances in multiple d for mnist

4 | the id88

algebra is nothing more than geometry, in words; geometry is
nothing more than algebra, in pictures.

    sophie germain

so far, you   ve seen two types of learning models: in decision
trees, only a small number of features are used to make decisions; in
nearest neighbor algorithms, all features are used equally. neither of
these extremes is always desirable. in some problems, we might want
to use most of the features, but use some more than others.

in this chapter, we   ll discuss the id88 algorithm for learn-
ing weights for features. as we   ll see, learning weights for features
amounts to learning a hyperplane classi   er: that is, basically a di-
vision of space into two halves by a straight line, where one half is
   positive    and one half is    negative.    in this sense, the id88
can be seen as explicitly    nding a good linear decision boundary.

learning objectives:
    describe the biological motivation

behind the id88.

    classify learning algorithms based
on whether they are error-driven or
not.

    implement the id88 algorithm

for binary classi   cation.

    draw id88 weight vectors
and the corresponding decision
boundaries in two dimensions.

    contrast the decision boundaries
of id90, nearest neighbor
algorithms and id88s.

    compute the margin of a given

weight vector on a given data set.

4.1 bio-inspired learning

dependencies: chapter 1, chapter 3

folk biology tells us that our brains are made up of a bunch of little
units, called neurons, that send electrical signals to one another. the
rate of    ring tells us how    activated    a neuron is. a single neuron,
like that shown in figure 4.1 might have three incoming neurons.
these incoming neurons are    ring at different rates (i.e., have dif-
ferent activations). based on how much these incoming neurons are
   ring, and how    strong    the neural connections are, our main neu-
ron will    decide    how strongly it wants to    re. and so on through
the whole brain. learning in the brain happens by neurons becom-
ming connected to other neurons, and the strengths of connections
adapting over time.

the real biological world is much more complicated than this.
however, our goal isn   t to build a brain, but to simply be inspired
by how they work. we are going to think of our learning algorithm
as a single neuron. it receives input from d-many other neurons,
one for each input feature. the strength of these inputs are the fea-
ture values. this is shown schematically in figure 4.1. each incom-
ing connection has a weight and the neuron simply sums up all the
weighted inputs. based on this sum, it decides whether to       re    or

figure 4.1: a picture of a neuron

figure 4.2:    gure showing feature
vector and weight vector and products
and sum

42 a course in machine learning

not. firing is interpreted as being a positive example and not    ring is
interpreted as being a negative example. in particular, if the weighted
sum is positive, it       res    and otherwise it doesn   t    re. this is shown
diagramatically in figure 4.2.
mathematically, an input vector x = (cid:104)x1, x2, . . . , xd(cid:105) arrives. the

neuron stores d-many weights, w1, w2, . . . , wd. the neuron computes
the sum:
d   

a =

(4.1)

wdxd

d=1

to determine it   s amount of    activation.    if this activiation is posi-
tive (i.e., a > 0) it predicts that this example is a positive example.
otherwise it predicts a negative example.

the weights of this neuron are fairly easy to interpret. suppose

that a feature, for instance    is this a system   s class?    gets a zero
weight. then the activation is the same regardless of the value of
this feature. so features with zero weight are ignored. features with
positive weights are indicative of positive examples because they
cause the activation to increase. features with negative weights are
indicative of negative examples because they cause the activiation to
decrease.

it is often convenient to have a non-zero threshold. in other

words, we might want to predict positive if a >    for some value
  . the way that is most convenient to achieve this is to introduce a
bias term into the neuron, so that the activation is always increased
by some    xed value b. thus, we compute:

(cid:35)

(cid:34) d   

d=1

a =

wdxd

+ b

(4.2)

?

what would happen if we encoded
binary features like    is this a sys-
tem   s class    as no=0 and yes=   1
(rather than the standard no=0 and
yes=+1)?

this is the complete neural model of learning. the model is pa-
rameterized by d-many weights, w1, w2, . . . , wd, and a single scalar
bias value b.

?

if you wanted the activation thresh-
old to be a >    instead of a > 0,
what value would b have to be?

4.2 error-driven updating: the id88 algorithm

the id88 is a classic learning algorithm for the neural model
of learning. like k-nearest neighbors, it is one of those frustrating
algorithms that is incredibly simple and yet works amazingly well,
for some types of problems.

the algorithm is actually quite different than either the decision
tree algorithm or the knn algorithm. first, it is online. this means
that instead of considering the entire data set at the same time, it only
ever looks at one example. it processes that example and then goes

the id88 43

// initialize weights
// initialize bias

algorithm 5 id88train(d, maxiter)
1: wd     0, for all d = 1 . . . d
2: b     0
3: for iter = 1 . . . maxiter do
4:

for all (x,y)     d do

d=1 wd xd + b

a        d
if ya     0 then
wd     wd + yxd, for all d = 1 . . . d
b     b + y

5:

6:

7:

8:

9:

end if
end for

10:
11: end for
12: return w0, w1, . . . , wd, b

// compute activation for this example

// update weights
// update bias

algorithm 6 id88test(w0, w1, . . . , wd, b,   x)
1: a        d
2: return sign(a)

d=1 wd   xd + b

// compute activation for the test example

on to the next one. second, it is error driven. this means that, so
long as it is doing well, it doesn   t bother updating its parameters.
the algorithm maintains a    guess    at good parameters (weights
and bias) as it runs. it processes one example at a time. for a given
example, it makes a prediction. it checks to see if this prediction
is correct (recall that this is training data, so we have access to true
labels). if the prediction is correct, it does nothing. only when the
prediction is incorrect does it change its parameters, and it changes
them in such a way that it would do better on this example next
time around. it then goes on to the next example. once it hits the
last example in the training set, it loops back around for a speci   ed
number of iterations.

the training algorithm for the id88 is shown in algo-

rithm 4.2 and the corresponding prediction algorithm is shown in
algorithm 4.2. there is one    trick    in the training algorithm, which
probably seems silly, but will be useful later. it is in line 6, when we
check to see if we want to make an update or not. we want to make
an update if the current prediction (just sign(a)) is incorrect. the
trick is to multiply the true label y by the activation a and compare
this against zero. since the label y is either +1 or    1, you just need
to realize that ya is positive whenever a and y have the same sign.
in other words, the product ya is positive if the current prediction is
correct.

the particular form of update for the id88 is quite simple.

the weight wd is increased by yxd and the bias is increased by y. the
goal of the update is to adjust the parameters so that they are    bet-
ter    for the current example. in other words, if we saw this example

?

it is very very important to check
ya     0 rather than ya < 0. why?

44 a course in machine learning

twice in a row, we should do a better job the second time around.

to see why this particular update achieves this, consider the fol-

lowing scenario. we have some current set of parameters w1, . . . , wd, b.
we observe an example (x, y). for simplicity, suppose this is a posi-
tive example, so y = +1. we compute an activation a, and make an
error. namely, a < 0. we now update our weights and bias. let   s call
the new weights w(cid:48)
d, b(cid:48). suppose we observe the same exam-
ple again and need to compute a new activation a(cid:48). we proceed by a
little algebra:

1, . . . , w(cid:48)

a(cid:48) =

=

=

d   

d=1

d   

d=1

d   

d=1

w(cid:48)
dxd + b(cid:48)

(wd + xd)xd + (b + 1)

wdxd + b +

d   

d=1

xdxd + 1

= a +

d   

d=1

x2
d + 1 > a

(4.3)

(4.4)

(4.5)

(4.6)

d + 1. but x2

so the difference between the old activation a and the new activa-
d     0, since it   s squared. so this value is
tion a(cid:48) is    d x2
always at least one. thus, the new activation is always at least the old
activation plus one. since this was a positive example, we have suc-
cessfully moved the activation in the proper direction. (though note
that there   s no guarantee that we will correctly classify this point the
second, third or even fourth time around!)

the only hyperparameter of the id88 algorithm is maxiter,

the number of passes to make over the training data. if we make
many many passes over the training data, then the algorithm is likely
to over   t. (this would be like studying too long for an exam and just
confusing yourself.) on the other hand, going over the data only
one time might lead to under   tting. this is shown experimentally in
figure 4.3. the x-axis shows the number of passes over the data and
the y-axis shows the training error and the test error. as you can see,
there is a    sweet spot    at which test performance begins to degrade
due to over   tting.

one aspect of the id88 algorithm that is left underspeci   ed
is line 4, which says: loop over all the training examples. the natural
implementation of this would be to loop over them in a constant
order. the is actually a bad idea.

consider what the id88 algorithm would do on a data set

that consisted of 500 positive examples followed by 500 negative
examples. after seeing the    rst few positive examples (maybe    ve),
it would likely decide that every example is positive, and would stop

?

this analysis hold for the case pos-
itive examples (y = +1). it should
also hold for negative examples.
work it out.

figure 4.3: training and test error via
early stopping

learning anything. it would do well for a while (next 495 examples),
until it hit the batch of negative examples. then it would take a while
(maybe ten examples) before it would start predicting everything as
negative. by the end of one pass through the data, it would really
only have learned from a handful of examples (   fteen in this case).

so one thing you need to avoid is presenting the examples in some
   xed order. this can easily be accomplished by permuting the order
of examples once in the beginning and then cycling over the data set
in the same (permuted) order each iteration. however, it turns out
that you can actually do better if you re-permute the examples in each
iteration. figure 4.4 shows the effect of re-permuting on convergence
speed. in practice, permuting each iteration tends to yield about 20%
savings in number of iterations. in theory, you can actually prove that
it   s expected to be about twice as fast.

4.3 geometric intrepretation

the id88 45

figure 4.4: training and test error for
permuting versus not-permuting

?

if permuting the data each iteration
saves somewhere between 20% and
50% of your time, are there any
cases in which you might not want
to permute the data every iteration?

a question you should be asking yourself by now is: what does the
decision boundary of a id88 look like? you can actually answer
that question mathematically. for a id88, the decision bound-
ary is precisely where the sign of the activation, a, changes from    1
to +1. in other words, it is the set of points x that achieve zero ac-
tivation. the points that are not clearly positive nor negative. for
simplicity, we   ll    rst consider the case where there is no    bias    term
(or, equivalently, the bias is zero). formally, the decision boundary b
is:

(cid:40)

(cid:41)

b =

x :    
d

wdxd = 0

(4.7)

we can now apply some id202. recall that    d wdxd is just
the dot product between the vector w = (cid:104)w1, w2, . . . , wd(cid:105) and the
vector x. we will write this as w    x. two vectors have a zero dot
product if and only if they are perpendicular. thus, if we think of
the weights as a vector w, then the decision boundary is simply the
plane perpendicular to w.

46 a course in machine learning

math review | dot products
given two vectors u and v their dot product u    v is    d udvd. the dot product
grows large and positive when u and v point in same direction, grows large
and negative when u and v point in opposite directions, and is zero when
their are perpendicular. a useful geometric interpretation of dot products is
projection. suppose ||u|| = 1, so that u is a unit vector. we can think of any
other vector v as consisting of two components: (a) a component in the di-
rection of u and (b) a component that   s perpendicular to u. this is depicted
geometrically to the right: here, u = (cid:104)0.8, 0.6(cid:105) and v = (cid:104)0.37, 0.73(cid:105). we
can think of v as the sum of two vectors, a and b, where a is parallel to u and b is perpendicular. the
length of b is exactly u    v = 0.734, which is why you can think of dot products as projections: the dot
product between u and v is the    projection of v onto u.   

figure 4.5:

this is shown pictorially in figure 4.6. here, the weight vector is
shown, together with it   s perpendicular plane. this plane forms the
decision boundary between positive points and negative points. the
vector points in the direction of the positive examples and away from
the negative examples.

one thing to notice is that the scale of the weight vector is irrele-

vant from the perspective of classi   cation. suppose you take a weight
vector w and replace it with 2w. all activations are now doubled.
but their sign does not change. this makes complete sense geometri-
cally, since all that matters is which side of the plane a test point falls
on, now how far it is from that plane. for this reason, it is common
to work with normalized weight vectors, w, that have length one; i.e.,
||w|| = 1.
the geometric intuition can help us even more when we realize
that dot products compute projections. that is, the value w    x is
just the distance of x from the origin when projected onto the vector
w. this is shown in figure 4.7. in that    gure, all the data points are
projected onto w. below, we can think of this as a one-dimensional
version of the data, where each data point is placed according to its
projection along w. this distance along w is exactly the activiation of
that example, with no bias.

from here, you can start thinking about the role of the bias term.

previously, the threshold would be at zero. any example with a
negative projection onto w would be classi   ed negative; any exam-
ple with a positive projection, positive. the bias simply moves this
threshold. now, after the projection is computed, b is added to get
the overall activation. the projection plus b is then compared against

figure 4.6: picture of data points with
hyperplane and weight vector

?

if i give you a non-zero weight vec-
tor w, how do i compute a weight
vector w(cid:48) that points in the same
direction but has a norm of one?

figure 4.7: the same picture as before,
but with projections onto weight vector;
then, below, those points along a one-
dimensional axis with zero marked.

uv}a}bthe id88 47

figure 4.8: perc:bias: id88
picture with bias

zero.

thus, from a geometric perspective, the role of the bias is to shift
the decision boundary away from the origin, in the direction of w. it
is shifted exactly    b units. so if b is positive, the boundary is shifted
away from w and if b is negative, the boundary is shifted toward w.
this is shown in figure 4.8. this makes intuitive sense: a positive
bias means that more examples should be classi   ed positive. by
moving the decision boundary in the negative direction, more space
yields a positive classi   cation.
d dimensional space, it is always a d     1-dimensional hyperplane.
(in two dimensions, a 1-d hyperplane is simply a line. in three di-
mensions, a 2-d hyperplane is like a sheet of paper.) this hyperplane
divides space in half. in the rest of this book, we   ll refer to the weight
vector, and to hyperplane it de   nes, interchangeably.

the decision boundary for a id88 is a very magical thing. in

the id88 update can also be considered geometrically. (for

simplicity, we will consider the unbiased case.) consider the sit-
uation in figure 4.9. here, we have a current guess as to the hy-
perplane, and positive training example comes in that is currently
mis-classi   ed. the weights are updated: w     w + yx. this yields the
new weight vector, also shown in the figure. in this case, the weight
vector changed enough that this training example is now correctly
classi   ed.

4.4

interpreting id88 weights

figure 4.9: id88 picture with
update, no bias

you may    nd yourself having run the id88, learned a really
awesome classi   er, and then wondering    what the heck is this clas-
si   er doing?    you might ask this question because you   re curious to
learn something about the underlying data. you might ask this ques-
tion because you want to make sure that the id88 is learning
   the right thing.    you might ask this question because you want to
remove a bunch of features that aren   t very useful because they   re
expensive to compute or take a lot of storage.

the id88 learns a classi   er of the form x (cid:55)    sign (   d wdxd + b).

a reasonable question to ask is: how sensitive is the    nal classi   ca-
tion to small changes in some particular feature. we can answer this
question by taking a derivative. if we arbitrarily take the 7th fea-
(   d wdxd + b) = w7. this says: the rate at
ture we can compute    
   x7
which the activation changes as a function of the 7th feature is ex-
actly w7. this gives rise to a useful heuristic for interpreting percep-
tron weights: sort all the weights from largest (positive) to largest
(negative), and take the top ten and bottom ten. the top ten are the
features that the id88 is most sensitive to for making positive

48 a course in machine learning

predictions. the bottom ten are the features that the id88 is
most sensitive to for making negative predictions.

this heuristic is useful, especially when the inputs x consist en-

tirely of binary values (like a bag of words representation). the
heuristic is less useful when the range of the individual features
varies signi   cantly. the issue is that if you have one feat x5 that   s ei-
ther 0 or 1, and another feature x7 that   s either 0 or 100, but w5 = w7,
it   s reasonable to say that w7 is more important because it is likely to
have a much larger in   uence on the    nal prediction. the easiest way
to compensate for this is simply to scale your features ahead of time:
this is another reason why feature scaling is a useful preprocessing
step.

4.5 id88 convergence and linear separability

you already have an intuitive feeling for why the id88 works:
it moves the decision boundary in the direction of the training exam-
ples. a question you should be asking yourself is: does the percep-
tron converge? if so, what does it converge to? and how long does it
take?

it is easy to construct data sets on which the id88 algorithm
will never converge. in fact, consider the (very uninteresting) learn-
ing problem with no features. you have a data set consisting of one
positive example and one negative example. since there are no fea-
tures, the only thing the id88 algorithm will ever do is adjust
the bias. given this data, you can run the id88 for a bajillion
iterations and it will never settle down. as long as the bias is non-
negative, the negative example will cause it to decrease. as long as
it is non-positive, the positive example will cause it to increase. ad
in   nitum. (yes, this is a very contrived example.)

what does it mean for the id88 to converge? it means that
it can make an entire pass through the training data without making
any more updates. in other words, it has correctly classi   ed every
training example. geometrically, this means that it was found some
hyperplane that correctly segregates the data into positive and nega-
tive examples, like that shown in figure 4.10.

in this case, this data is linearly separable. this means that there

exists some hyperplane that puts all the positive examples on one side
and all the negative examples on the other side. if the training is not
linearly separable, like that shown in figure 4.11, then the id88
has no hope of converging. it could never possibly classify each point
correctly.

the somewhat surprising thing about the id88 algorithm is
that if the data is linearly separable, then it will converge to a weight

figure 4.10: separable data

figure 4.11: inseparable data

the id88 49

vector that separates the data. (and if the data is inseparable, then it
will never converge.) this is great news. it means that the id88
converges whenever it is even remotely possible to converge.

the second question is: how long does it take to converge? by

   how long,    what we really mean is    how many updates?    as is the
case for much learning theory, you will not be able to get an answer
of the form    it will converge after 5293 updates.    this is asking too
much. the sort of answer we can hope to get is of the form    it will
converge after at most 5293 updates.   

what you might expect to see is that the id88 will con-

verge more quickly for easy learning problems than for hard learning
problems. this certainly    ts intuition. the question is how to de   ne
   easy    and    hard    in a meaningful way. one way to make this def-
inition is through the notion of margin. if i give you a data set and
hyperplane that separates itthen the margin is the distance between
the hyperplane and the nearest point. intuitively, problems with large
margins should be easy (there   s lots of    wiggle room    to    nd a sepa-
rating hyperplane); and problems with small margins should be hard
(you really have to get a very speci   c well tuned weight vector).
formally, given a data set d, a weight vector w and bias b, the

margin of w, b on d is de   ned as:

(cid:40)

min(x,y)   d y(cid:0)w    x + b(cid:1)

margin(d, w, b) =

      

if w separates d
otherwise

(4.8)

in words, the margin is only de   ned if w, b actually separate the data
(otherwise it is just       ). in the case that it separates the data, we
   nd the point with the minimum activation, after the activation is
multiplied by the label.

for some historical reason (that is unknown to the author), mar-
gins are always denoted by the greek letter    (gamma). one often
talks about the margin of a data set. the margin of a data set is the
largest attainable margin on this data. formally:

margin(d) = sup
w,b

margin(d, w, b)

(4.9)

in words, to compute the margin of a data set, you    try    every possi-
ble w, b pair. for each pair, you compute its margin. we then take the
largest of these as the overall margin of the data.1 if the data is not
linearly separable, then the value of the sup, and therefore the value
of the margin, is       .

there is a famous theorem due to rosenblatt2 that shows that the
number of errors that the id88 algorithm makes is bounded by
     2. more formally:

so long as the margin is not       ,
it is always positive. geometrically
this makes sense, but why does
eq (4.8) yield this?

?

1 you can read    sup    as    max    if you
like: the only difference is a technical
difference in how the        case is
handled.
2 rosenblatt 1958

50 a course in machine learning

theorem 2 (id88 convergence theorem). suppose the id88
algorithm is run on a linearly separable data set d with margin    > 0.
assume that ||x||     1 for all x     d. then the algorithm will converge after
at most 1

  2 updates.

the proof of this theorem is elementary, in the sense that it does

not use any fancy tricks: it   s all just algebra. the idea behind the
proof is as follows. if the data is linearly separable with margin   ,
then there exists some weight vector w    that achieves this margin.
obviously we don   t know what w    is, but we know it exists. the
id88 algorithm is trying to    nd a weight vector w that points
roughly in the same direction as w   . (for large   ,    roughly    can be
very rough. for small   ,    roughly    is quite precise.) every time the
id88 makes an update, the angle between w and w    changes.
what we prove is that the angle actually decreases. we show this in
two steps. first, the dot product w    w    increases a lot. second, the
norm ||w|| does not increase very much. since the dot product is
increasing, but w isn   t getting too long, the angle between them has
to be shrinking. the rest is algebra.

proof of theorem 2. the margin    > 0 must be realized by some set
of parameters, say x   . suppose we train a id88 on this data.
denote by w(0) the initial weight vector, w(1) the weight vector after
the    rst update, and w(k) the weight vector after the kth update. (we
are essentially ignoring data points on which the id88 doesn   t
update itself.) first, we will show that w       w(k) grows quicky as

a function of k. second, we will show that(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12) does not grow

quickly.

first, suppose that the kth update happens on example (x, y). we
are trying to show that w(k) is becoming aligned with w   . because we
updated, know that this example was misclassi   ed: yw(k-1)    x < 0.
after the update, we get w(k) = w(k-1) + yx. we do a little computa-
tion:

w       w(k) = w      (cid:16)

(cid:17)
= w       w(k-1) + yw       x
    w       w(k-1) +   

w(k-1) + yx

de   nition of w(k)

(4.10)
(4.11)
(4.12)
thus, every time w(k) is updated, its projection onto w    increases by
at least   . therefore: w       w(k)     k  .
because w(k) is getting closer to w   , not just because it   s getting ex-
ceptionally long. to do this, we compute the norm of w(k):

next, we need to show that the increase of    along w    occurs

vector algebra
w    has margin   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

the id88 51

(4.13)

def. of w(k)

quadratic rule (4.14)

=

=

+ 1 + 0

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

+ y2 ||x||2 + 2yw(k-1)    x

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k-1) + yx
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k-1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
   (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k-1)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
date. therefore: (cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)2     k.
that(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)     w       w(k). putting this together, we have:
k     (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)     w       w(k)     k  

   

now we put together the two things we have learned before. by
our    rst conclusion, we know w       w(k)     k  . but our second con-
clusion,

k    (cid:12)(cid:12)(cid:12)(cid:12)w(k)(cid:12)(cid:12)(cid:12)(cid:12)2. finally, because w    is a unit vector, we know

   

thus, the squared norm of w(k) increases by at most one every up-

assumption and a < 0 (4.15)

(4.16)

taking the left-most and right-most terms, we get that
dividing both sides by k, we get 1   
k
this means that once we   ve made 1
  2 updates, we cannot make any
more!

k     k  .
       and therefore k     1
  2 .

   

it is important to keep in mind what this proof shows and what

it does not show. it shows that if i give the id88 data that
is linearly separable with margin    > 0, then the id88 will
converge to a solution that separates the data. and it will converge
quickly when    is large. it does not say anything about the solution,
other than the fact that it separates the data. in particular, the proof
makes use of the maximum margin separator. but the id88
is not guaranteed to    nd this maximum margin separator. the data
may be separable with margin 0.9 and the id88 might still
   nd a separating hyperplane with a margin of only 0.000001. later
(in chapter 7), we will see algorithms that explicitly try to    nd the
maximum margin solution.

4.6

improved generalization: voting and averaging

in the beginning of this chapter, there was a comment that the per-
ceptron works amazingly well. this was a half-truth. the    vanilla   
id88 algorithm does well, but not amazingly well. in order to
make it more competitive with other learning algorithms, you need
to modify it a bit to get better generalization. the key issue with the
vanilla id88 is that it counts later points more than it counts earlier
points.

to see why, consider a data set with 10, 000 examples. suppose

that after the    rst 100 examples, the id88 has learned a really

?

perhaps we don   t want to assume
that all x have norm at most 1. if
they have all have norm at most
r, you can achieve a very simi-
lar bound. modify the id88
convergence proof to handle this
case.

?

why does the id88 conver-
gence bound not contradict the
earlier claim that poorly ordered
data points (e.g., all positives fol-
lowed by all negatives) will cause
the id88 to take an astronom-
ically long time to learn?

52 a course in machine learning

good classi   er. it   s so good that it goes over the next 9899 exam-
ples without making any updates. it reaches the 10, 000th example
and makes an error. it updates. for all we know, the update on this
10, 000th example completely ruins the weight vector that has done so
well on 99.99% of the data!

what we would like is for weight vectors that    survive    a long

time to get more say than weight vectors that are overthrown quickly.
one way to achieve this is by voting. as the id88 learns, it
remembers how long each hyperplane survives. at test time, each
hyperplane encountered during training    votes    on the class of a test
example. if a particular hyperplane survived for 20 examples, then
it gets a vote of 20. if it only survived for one example, it only gets a
vote of 1. in particular, let (w, b)(1), . . . , (w, b)(k) be the k + 1 weight
vectors encountered during training, and c(1), . . . , c(k) be the survival
times for each of these weight vectors. (a weight vector that gets
immediately updated gets c = 1; one that survives another round
gets c = 2 and so on.) then the prediction on a test point is:

(cid:32) k   

(cid:16)

w(k)      x + b(k)(cid:17)(cid:33)

  y = sign

c(k)sign

(4.17)

k=1

this algorithm, known as the voted id88 works quite well in
practice, and there is some nice theory showing that it is guaranteed
to generalize better than the vanilla id88. unfortunately, it is
also completely impractical. if there are 1000 updates made during
id88 learning, the voted id88 requires that you store
1000 weight vectors, together with their counts. this requires an
absurd amount of storage, and makes prediction 1000 times slower
than the vanilla id88.

a much more practical alternative is the averaged id88.

the idea is similar: you maintain a collection of weight vectors and
survival times. however, at test time, you predict according to the
average weight vector, rather than the voting. in particular, the predic-
tion is:

(cid:32) k   

c(k)(cid:16)

w(k)      x + b(k)(cid:17)(cid:33)

(4.18)

  y = sign

k=1

?

the training algorithm for the voted
id88 is the same as the
vanilla id88. in particular,
in line 5 of algorithm 4.2, the ac-
tivation on a training example is
computed based on the current
weight vector, not based on the voted
prediction. why?

the only difference between the voted prediction, eq (4.17), and the
averaged prediction, eq (4.18), is the presense of the interior sign
operator. with a little bit of algebra, we can rewrite the test-time
prediction as:

(cid:33)

(cid:32)(cid:32) k   

k=1

  y = sign

c(k)w(k)

     x +

k   

k=1

c(k)b(k)

(cid:33)

(4.19)

the advantage of the averaged id88 is that we can simply
maintain a running sum of the averaged weight vector (the blue term)

the id88 53

algorithm 7 averagedid88train(d, maxiter)
1: w     (cid:104)0, 0, . . . 0(cid:105)
2: u     (cid:104)0, 0, . . . 0(cid:105)
3: c     1
4: for iter = 1 . . . maxiter do
5:

b     0
,
,        0

// initialize weights and bias
// initialize cached weights and bias
// initialize example counter to one

6:

7:

8:

9:

10:

11:

12:

if y(w    x + b)     0 then

for all (x,y)     d do
w     w + y x
b     b + y
u     u + y c x
          + y c

end if
c     c + 1

end for

13:
14: end for
15: return w - 1

c u, b - 1

c   

// update weights
// update bias
// update cached weights
// update cached bias

// increment counter regardless of update

// return averaged weights and bias

and averaged bias (the red term). test-time prediction is then just as
ef   cient as it is with the vanilla id88.

the full training algorithm for the averaged id88 is shown
in algorithm 4.6. some of the notation is changed from the original
id88: namely, vector operations are written as vector opera-
tions, and the activation computation is folded into the error check-
ing.

it is probably not immediately apparent from algorithm 4.6 that

the computation unfolding is precisely the calculation of the averaged
weights and bias. the most natural implementation would be to keep
track of an averaged weight vector u. at the end of every example,
you would increase u     u + w (and similarly for the bias). however,
such an implementation would require that you updated the aver-
aged vector on every example, rather than just on the examples that
were incorrectly classi   ed! since we hope that eventually the per-
ceptron learns to do a good job, we would hope that it will not make
updates on every example. so, ideally, you would like to only update
the averaged weight vector when the actual weight vector changes.
the slightly clever computation in algorithm 4.6 achieves this.

the averaged id88 is almost always better than the percep-
tron, in the sense that it generalizes better to test data. however, that
does not free you from having to do early stopping. it will, eventu-
ally, over   t.

4.7 limitations of the id88

although the id88 is very useful, it is fundamentally limited in
a way that neither id90 nor knn are. its limitation is that

?

by writing out the computation of
the averaged weights from eq (4.18)
as a telescoping sum, derive the
computation from algorithm 4.6.

54 a course in machine learning

its decision boundaries can only be linear. the classic way of showing
this limitation is through the xor problem (xor = exclusive or). the
xor problem is shown graphically in figure 4.12. it consists of four
data points, each at a corner of the unit square. the labels for these
points are the same, along the diagonals. you can try, but you will
not be able to    nd a linear decision boundary that perfectly separates
these data points.

one question you might ask is: do xor-like problems exist in

the real world? unfortunately for the id88, the answer is yes.
consider a sentiment classi   cation problem that has three features
that simply say whether a given word is contained in a review of
a course. these features are: excellent, terrible and not. the
excellent feature is indicative of positive reviews and the terrible
feature is indicative of negative reviews. but in the presence of the
not feature, this categorization    ips.

one way to address this problem is by adding feature combina-
tions. we could add two additional features: excellent-and-not
and terrible-and-not that indicate a conjunction of these base
features. by assigning weights as follows, you can achieve the desired
effect:

wexecellent = +1
wexecllent-and-not =    2

wterrible =    1
wterrible-and-not = +2

wnot = 0

in this particular case, we have addressed the problem. however, if
we start with d-many features, if we want to add all pairs, we   ll blow
2 ) = o(d2) features through this feature mapping. and
up to (d
there   s no guarantee that pairs of features is enough. we might need
3 ) = o(d2) features. these
triples of features, and now we   re up to (d
additional features will drastically increase computation and will
often result in a stronger propensity to over   tting.

in fact, the    xor problem    is so signi   cant that it basically killed

research in classi   ers with linear decision boundaries for a decade
or two. later in this book, we will see two alternative approaches to
taking key ideas from the id88 and generating classi   ers with
non-linear decision boundaries. one approach is to combine multi-
ple id88s in a single framework: this is the neural networks
approach (see chapter 10). the second approach is to    nd computa-
tionally ef   cient ways of doing feature mapping in a computationally
and statistically ef   cient way: this is the kernels approach (see chap-
ter 11).

4.8 further reading

todo further reading

figure 4.12: picture of xor problem

?

suppose that you took the xor
problem and added one new fea-
ture: x3 = x1     x2 (the logical and
of the two existing features). write
out feature weights and a bias that
would achieve perfect classi   cation
on this data.

5 | practical issues

a ship in port is safe, but that is not what ships are for.
grace hopper

   

at this point, you have seen three qualitatively different models
for learning: id90, nearest neighbors, and id88s. you
have also learned about id91 with the id116 algorithm. you
will shortly learn about more complex models, most of which are
variants on things you already know. however, before attempting
to understand more complex models of learning, it is important to
have a    rm grasp on how to use machine learning in practice. this
chapter is all about how to go from an abstract learning problem
to a concrete implementation. you will see some examples of    best
practices    along with justi   cations of these practices.

in many ways, going from an abstract problem to a concrete learn-

ing task is more of an art than a science. however, this art can have
a huge impact on the practical performance of learning systems. in
many cases, moving to a more complicated learning algorithm will
gain you a few percent improvement. going to a better representa-
tion will gain you an order of magnitude improvement. to this end,
we will discuss several high level ideas to help you develop a better
artistic sensibility.

5.1 the importance of good features

consider a problem of object recognition from images. if you start

machine learning is magical. you give it data and it manages to
classify that data. for many, it can actually outperform a human! but,
like so many problems in the world, there is a signi   cant    garbage
in, garbage out    aspect to machine learning. if the data you give it is
trash, the learning algorithm is unlikely to be able to overcome it.
with a 100  100 pixel image, a very easy feature representation of
this image is as a 30, 000 dimensional vector, where each dimension
corresponds to the red, green or blue component of some pixel in
the image. so perhaps feature 1 is the amount of red in pixel (1, 1);
feature 2 is the amount of green in that pixel; and so on. this is the
pixel representation of images.

learning objectives:
    translate between a problem de-
scription and a concrete learning
problem.

    perform basic feature engineering on

image and text data.

    explain how to use cross-validation

to tune hyperparameters and esti-
mate future performance.

    compare and contrast the differ-
ences between several evaluation
metrics.

    explain why feature combinations

are important for learning with
some models but not others.

    explain the relationship between the

three learning techniques you have
seen so far.

    apply several debugging techniques

to learning algorithms.

dependencies: chapter 1,chap-
ter 3,chapter 4

figure 5.1: object recognition in pixels

56 a course in machine learning

one thing to keep in mind is that the pixel representation throws

away all locality information in the image. learning algorithms don   t
care about features: they only care about feature values. so i can
permute all of the features, with no effect on the learning algorithm
(so long as i apply the same permutation to all training and test
examples). figure 5.1 shows some images whose pixels have been
randomly permuted (in this case only the pixels are permuted, not
the colors). all of these objects are things that you   ve seen plenty of
examples of; can you identify them? should you expect a machine to
be able to?

an alternative representation of images is the patch represen-

tation, where the unit of interest is a small rectangular block of an
image, rather than a single pixel. again, permuting the patches has
no effect on the classi   er. figure 5.2 shows the same images in patch
representation. can you identify them? a    nal representation is a
shape representation. here, we throw out all color and pixel infor-
mation and simply provide a bounding polygon. figure 5.3 shows
the same images in this representation. is this now enough to iden-
tify them? (if not, you can    nd the answers in figure 5.15 at the end
of the chapter.)

in the context of text categorization (for instance, the sentiment
recognition task), one standard representation is the bag of words
representation. here, we have one feature for each unique word that
appears in a document. for the feature happy, the feature value is
the number of times that the word    happy    appears in the document.
the bag of words (bow) representation throws away all position
information. table 5.1 shows a bow representation for two chapters
of this book. can you tell which is which?

5.2

irrelevant and redundant features

one big difference between learning models is how robust they are to
the addition of noisy or irrelevant features. intuitively, an irrelevant
feature is one that is completely uncorrelated with the prediction
task. a feature f whose expectation does not depend on the label
e[ f | y] = e[ f ] might be irrelevant. for instance, the presence of
the word    the    might be largely irrelevant for predicting whether a
course review is positive or negative.

a secondary issue is how well these algorithms deal with redun-

dant features. two features are redundant if they are highly cor-
related, regardless of whether they are correlated with the task or
not. for example, having a bright red pixel in an image at position
(20, 93) is probably highly redundant with having a bright red pixel
at position (21, 93). both might be useful (e.g., for identifying    re hy-

figure 5.2: object recognition in patches

figure 5.3: object recognition in shapes

data learning
training set
predict
fea-
ture function
test machine
loss alice tree
guess
features
algorithm

data knn
dimensions
points
fea-
ture    gure
decision fea-
tures point
   g training
set
space
examples

table 5.1: bag of (most frequent) words
representation for the decision tree
and knn chapters of this book, after
dropping high frequency words like
   the   .

figure 5.4: prac:bow: bow repr of one
positive and one negative review

practical issues

57

?

is it possible to have a feature f
whose expectation does not depend
on the label, but is nevertheless still
useful for prediction?

1 you might think it   s absurd to have
so many irrelevant features, but the
cases you   ve seen so far (bag of words,
bag of pixels) are both reasonable
examples of this! how many words,
out of the entire english vocabulary
(roughly 10, 000     100, 000 words), are
actually useful for predicting positive
and negative course reviews?

drants), but because of how images are structured, these two features
are likely to co-occur frequently.

when thinking about robustness to irrelevant or redundant fea-

tures, it is usually not worthwhile thinking of the case where one has
999 great features and 1 bad feature. the interesting case is when the
bad features outnumber the good features, and often outnumber by a
large degree. the question is how robust are algorithms in this case.1

for shallow id90, the model explicitly selects features

that are highly correlated with the label. in particular, by limiting the
depth of the decision tree, one can at least hope that the model will be
able to throw away irrelevant features. redundant features are almost
certainly thrown out: once you select one feature, the second feature
now looks mostly useless. the only possible issue with irrelevant
features is that even though they   re irrelevant, they happen to correlate
with the class label on the training data, but chance.

as a thought experiment, suppose that we have n training ex-

amples, and exactly half are positive examples and half are negative
examples. suppose there   s some binary feature, f , that is completely
uncorrelated with the label. this feature has a 50/50 chance of ap-
pearing in any example, regardless of the label. in principle, the deci-
sion tree should not select this feature. but, by chance, especially if n
is small, the feature might look correlated with the label. this is anal-
ogous to    ipping two coins simultaneously n times. even though the
coins are independent, it   s entirely possible that you will observe a
sequence like (h, h), (t, t), (h, h), (h, h), which makes them look
entirely correlated! the hope is that as n grows, this becomes less
and less likely. in fact, we can explicitly compute how likely this is to
happen.

to do this, let   s    x the sequence of n labels. we now    ip a coin n

times and consider how likely it is that it exactly matches the label.
this is easy: the id203 is 0.5n. now, we would also be confused
if it exactly matched not the label, which has the same id203. so
the chance that it looks perfectly correlated is 0.5n + 0.5n = 0.5n   1.
thankfully, this shrinks down very small (e.g., 10   6) after only 21
data points, meaning that even with a very small training set, the
chance that a random feature happens to correlate exactly with the
label is tiny.

this makes us happy. the problem is that we don   t have one irrel-
evant feature: we have many! if we randomly pick two irrelevant fea-
tures, each has the same id203 of perfectly correlating: 0.5n   1.
but since there are two and they   re independent coins, the chance
that either correlates perfectly is 2  0.5n   1 = 0.5n   2. in general,
if we have k irrelevant features, all of which are random indepen-
dent coins, the chance that at least one of them perfectly correlates is

58 a course in machine learning

0.5n   k. this suggests that if we have a sizeable number k of irrele-
vant features, we   d better have at least k + 21 training examples.
unfortunately, the situation is actually worse than this. in the

above analysis we only considered the case of perfect correlation. we
could also consider the case of partial correlation, which would yield
even higher probabilities. suf   ce it to say that even id90 can
become confused.

in the case of k-nearest neighbors, the situation is perhaps more

dire. since knn weighs each feature just as much as another feature,
the introduction of irrelevant features can completely mess up knn
prediction. in fact, as you saw, in high dimensional space, randomly
distributed points all look approximately the same distance apart. if
we add lots and lots of randomly distributed features to a data set,
then all distances still converge.

in the case of the id88, one can hope that it might learn to

assign zero weight to irrelevant features. for instance, consider a
binary feature is randomly one or zero independent of the label. if
the id88 makes just as many updates for positive examples
as for negative examples, there is a reasonable chance this feature
weight will be zero. at the very least, it should be small.

5.3 feature pruning and id172

in text categorization problems, some words simply do not appear
very often. perhaps the word    groovy   2 appears in exactly one train-
ing document, which is positive. is it really worth keeping this word
around as a feature? it   s a dangerous endeavor because it   s hard to
tell with just one training example if it is really correlated with the
positive class, or is it just noise. you could hope that your learning
algorithm is smart enough to    gure it out. or you could just remove
it. that means that (a) the learning algorithm won   t have to    gure it
out, and (b) you   ve reduced the number of dimensions you have, so
things should be more ef   cient, and less    scary.   

this idea of feature pruning is very useful and applied in many
applications. it is easiest in the case of binary features. if a binary
feature only appears some small number k times (in the training
data: no fair looking at the test data!), you simply remove it from
consideration. (you might also want to remove features that appear
in all-but-k many documents, for instance the word    the    appears in
pretty much every english document ever written.) typical choices
for k are 1, 2, 5, 10, 20, 50, mostly depending on the size of the data.
on a text data set with 1000 documents, a cutoff of 5 is probably
reasonable. on a text data set the size of the web, a cut of 50 or even
100 or 200 is probably reasonable3. figure 5.6 shows the effect of

figure 5.5: prac:addirrel: data from
high dimensional warning, interpolated

?

what happens with the id88
with truly redundant features (i.e.,
one is literally a copy of the other)?

2 this is typically positive indicator,
or at least it was back in the us in the
1970s.

figure 5.6: prac:pruning: effect of
pruning on text data
3 according to google, the following
words (among many others) appear
200 times on the web: moudlings, agag-
gagctg, setgravity, rogov, prosomeric,
spunlaid, piyushtwok, telelesson, nes-
mysl, brighnasa. for comparison, the
word    the    appears 19, 401, 194, 714 (19
billion) times.

practical issues

59

math review | data statistics: means and variances
we often need to discuss various statistics of a data set. most often, it is enough to consider univariate
(one-dimensional) data. suppose we have n real valued numbers z1, z2, . . . , zn. the sample mean (or
n    n zn. the sample
just mean) of these numbers is just their average value, or expected value:    = 1
   n(zn       )2,
variance (or just variance) measures how much they vary around their mean:   2 = 1
n   1
where    is the sample mean.

the mean and variance have convenient interpretations in terms of prediction. suppose we wanted
to choose a single constant value to    predict    the next z, and were minimizing squared error. call
this constant value a. then a = argmina   r
2 is for convenience and does
   n(a     zn)2
not change the answer.) to solve for a, we can take derivatives and set to zero:    
=
   a
   n(a     zn) = na        n zn; therefore na =    n zn and a =   . this means that the sample mean is
the number that minimizes squared error to the sample. moreover, the variance is propotional to the
squared error of that    predictor.   

   n(a     zn)2. (here, the 1

1
2

1
2

pruning on a id31 task. in the beginning, pruning does
not hurt (and sometimes helps!) but eventually we prune away all the
interesting words and performance suffers.

in the case of real-valued features, the question is how to extend
the idea of    does not occur much    to real values. a reasonable def-
inition is to look for features with low variance. in fact, for binary
features, ones that almost never appear or almost always appear will
also have low variance. figure 5.8 shows the result of pruning low-
variance features on the digit recognition task. again, at    rst pruning
does not hurt (and sometimes helps!) but eventually we have thrown
out all the useful features.

it is often useful to normalize the data so that it is consistent in
some way. there are two basic types of id172: feature nor-
malization and example id172. in feature id172,
you go through each feature and adjust it the same way across all
examples. in example id172, each example is adjusted indi-
vidually.

the goal of both types of id172 is to make it easier for your

learning algorithm to learn. in feature id172, there are two
standard things to do:

1. centering: moving the entire data set so that it is centered around

the origin.

2. scaling: rescaling each feature so that one of the following holds:

(a) each feature has variance 1 across the training data.

figure 5.7:

figure 5.8: prac:variance: effect of
pruning on vision

?

earlier we discussed the problem
of scale of features (e.g., millimeters
versus centimeters). does this have
an impact on variance-based feature
pruning?

figure 5.9: prac:transform: picture

60 a course in machine learning

(b) each feature has maximum absolute value 1 across the train-

ing data.

these transformations are shown geometrically in figure 5.9. the
goal of centering is to make sure that no features are arbitrarily large.
the goal of scaling is to make sure that all features have roughly the
same scale (to avoid the issue of centimeters versus millimeters).

these computations are fairly straightforward. here, xn,d refers

to the dth feature of example n. since it is very rare to apply scaling
without previously applying centering, the expressions below for
scaling assume that the data is already centered.
xn,d     xn,d       d
xn,d     xn,d/  d
xn,d     xn,d/rd
   
xn,d
  d =
n

centering:
variance scaling:
absolute scaling:

where:

(5.1)
(5.2)
(5.3)
(5.4)

1
n

(cid:115)

  d =

rd = max

n

1

n     1

(cid:12)(cid:12)xn,d

(xn,d       d)2

   
n

(cid:12)(cid:12)

(5.5)

(5.6)

?

for the three models you know
about (knn, dt, id88),
which are most sensitive to center-
ing? which are most sensitive to
scaling?

in practice, if the dynamic range of your features is already some
subset of [   2, 2] or [   3, 3], then it is probably not worth the effort of
centering and scaling. (it   s an effort because you have to keep around
your centering and scaling calculations so that you can apply them
to the test data as well!) however, if some of your features are orders
of magnitude larger than others, it might be helpful. remember that
you might know best: if the difference in scale is actually signi   cant
for your problem, then rescaling might throw away useful informa-
tion.

one thing to be wary of is centering binary data. in many cases,

binary data is very sparse: for a given example, only a few of the
features are    on.    for instance, out of a vocabulary of 10, 000 or
100, 000 words, a given document probably only contains about 100.
from a storage and computation perspective, this is very useful.
however, after centering, the data will no longer be sparse and you
will pay dearly with outrageously slow implementations.

in example id172, you view examples one at a time. the

most standard id172 is to ensure that the length of each
example vector is one: namely, each example lies somewhere on the
unit hypersphere. this is a simple transformation:

example id172:

xn     xn/ ||xn||

(5.7)

figure 5.10: prac:exnorm: example of
example id172

this transformation is depicted in figure 5.10.

the main advantage to example id172 is that it makes
comparisons more straightforward across data sets. if i hand you
two data sets that differ only in the norm of the feature vectors (i.e.,
one is just a scaled version of the other), it is dif   cult to compare the
learned models. example id172 makes this more straightfor-
ward. moreover, as you saw in the id88 convergence proof, it is
often just mathematically easier to assume normalized data.

5.4 combinatorial feature explosion

you learned in chapter 4 that linear models (like the id88)
cannot solve the xor problem. you also learned that by performing
a combinatorial feature explosion, they could. but that came at the
computational expense of gigantic feature vectors.

of the algorithms that you   ve seen so far, the id88 is the one

that has the most to gain by feature combination. and the decision
tree is the one that has the least to gain. in fact, the decision tree
construction is essentially building meta features for you. (or, at
least, it is building meta features constructed purely through    logical
ands.   )

this observation leads to a heuristic for constructing meta features

for id88s from id90. the idea is to train a decision
tree on the training data. from that decision tree, you can extract
meta features by looking at feature combinations along branches. you
can then add only those feature combinations as meta features to the
feature set for the id88. figure 5.11 shows a small decision tree
and a set of meta features that you might extract from it. there is a
hyperparameter here of what length paths to extract from the tree: in
this case, only paths of length two are extracted. for bigger trees, or
if you have more data, you might bene   t from longer paths.

in addition to combinatorial transformations, the logarithmic

transformation can be quite useful in practice. it seems like a strange
thing to be useful, since it doesn   t seem to fundamentally change
the data. however, since many learning algorithms operate by linear
operations on the features (both id88 and knn do this), the
log-transform is a way to get product-like operations. the question is
which of the following feels more applicable to your data: (1) every
time this feature increases by one, i   m equally more likely to predict
a positive label; (2) every time this feature doubles, i   m equally more
like to predict a positive label. in the    rst case, you should stick
with linear features and in the second case you should switch to
a log-transform. this is an important transformation in text data,
where the presence of the word    excellent    once is a good indicator
of a positive review; seeing    excellent    twice is a better indicator;

practical issues

61

figure 5.11: prac:dttoperc: turning a
dt into a set of meta features

figure 5.12: prac:log: performance on
text categ with word counts versus log
word counts

62 a course in machine learning

but the difference between seeing    excellent    10 times and seeing it
11 times really isn   t a big deal any more. a log-transform achieves
this. experimentally, you can see the difference in test performance
between word count data and log-word count data in figure 5.12.
here, the transformation is actually xd (cid:55)    log2(xd + 1) to ensure that
zeros remain zero and sparsity is retained. in the case that feature
values can also be negative, the slightly more complex mapping
xd (cid:55)    log2(|xd| + 1)sign(xd), where sign(xd) denotes the sign of xd.

5.5 evaluating model performance

so far, our focus has been on classi   ers that achieve high accuracy.
in some cases, this is not what you might want. for instance, if you
are trying to predict whether a patient has cancer or not, it might be
better to err on one side (saying they have cancer when they don   t)
than the other (because then they die). similarly, letting a little spam
slip through might be better than accidentally blocking one email
from your boss.

there are two major types of binary classi   cation problems. one

is    x versus y.    for instance, positive versus negative sentiment.
another is    x versus not-x.    for instance, spam versus non-spam.
(the argument being that there are lots of types of non-spam.) or
in the context of web search, relevant document versus irrelevant
document. this is a subtle and subjective decision. but    x versus not-
x    problems often have more of the feel of    x spotting    rather than
a true distinction between x and y. (can you spot the spam? can you
spot the relevant documents?)

for spotting problems (x versus not-x), there are often more ap-
propriate success metrics than accuracy. a very popular one from
information retrieval is the precision/recall metric. precision asks
the question: of all the x   s that you found, how many of them were
actually x   s? recall asks: of all the x   s that were out there, how many
of them did you    nd?4 formally, precision and recall are de   ned as:

p =

r =

i
s
i
t

s = number of xs that your system found
t = number of xs in the data
i = number of correct xs that your system found

(5.8)

(5.9)
(5.10)
(5.11)
(5.12)

here, s is mnemonic for    system,    t is mnemonic for    truth    and i
is mnemonic for    intersection.    it is generally accepted that 0/0 = 1
in these de   nitions. thus, if you system found nothing, your preci-

4 a colleague make the analogy to the
us court system   s saying    do you
promise to tell the whole truth and
nothing but the truth?    in this case, the
   whole truth    means high recall and
   nothing but the truth    means high
precision.   

sion is always perfect; and if there is nothing to    nd, your recall is
always perfect.

once you can compute precision and recall, you are often able to

produce precision/recall curves. suppose that you are attempting
to identify spam. you run a learning algorithm to make predictions
on a test set. but instead of just taking a    yes/no    answer, you allow
your algorithm to produce its con   dence. for instance, in id88,
you might use the distance from the hyperplane as a con   dence
measure. you can then sort all of your test emails according to this
ranking. you may put the most spam-like emails at the top and the
least spam-like emails at the bottom, like in figure 5.13.

once you have this sorted list, you can choose how aggressively
you want your spam    lter to be by setting a threshold anywhere on
this list. one would hope that if you set the threshold very high, you
are likely to have high precision (but low recall). if you set the thresh-
old very low, you   ll have high recall (but low precision). by consider-
ing every possible place you could put this threshold, you can trace out
a curve of precision/recall values, like the one in figure 5.14. this
allows us to ask the question: for some    xed precision, what sort of
recall can i get. obviously, the closer your curve is to the upper-right
corner, the better. and when comparing learning algorithms a and
b you can say that a dominates b if a   s precision/recall curve is
always higher than b   s.

precision/recall curves are nice because they allow us to visualize
many ways in which we could use the system. however, sometimes
we like to have a single number that informs us of the quality of the
solution. a popular way of combining precision and recall into a
single number is by taking their harmonic mean. this is known as
the balanced f-measure (or f-score):

f =

2  p  r
p + r

(5.13)

the reason that you want to use a harmonic mean rather than an
arithmetic mean (the one you   re more used to) is that it favors sys-
tems that achieve roughly equal precision and recall. in the extreme
case where p = r, then f = p = r. but in the imbalanced case, for
instance p = 0.1 and r = 0.9, the overall f-measure is a modest 0.18.
table 5.2 shows f-measures as a function of precision and recall, so
that you can see how important it is to get balanced values.

in some cases, you might believe that precision is more impor-
tant than recall. this idea leads to the weighted f-measure, which is
parameterized by a weight        [0,    ) (beta):

f   =

(1 +   2)  p  r

  2  p + r

(5.14)

practical issues

63

figure 5.13: prac:spam: show a bunch
of emails spam/nospam sorted by
model predicion, not perfect
?

how would you get a con   dence
out of a decision tree or knn?

figure 5.14: prac:prcurve: precision
recall curve
0.0
0.00
0.00
0.00
0.00
0.00
0.00

0.8
0.00
0.0
0.32
0.2
0.4
0.53
0.68
0.6
0.80
0.8
1.0
0.88
table 5.2: table of f-measures when
varying precision and recall values.

0.2
0.00
0.20
0.26
0.30
0.32
0.33

0.4
0.00
0.26
0.40
0.48
0.53
0.57

0.6
0.00
0.30
0.48
0.60
0.68
0.74

1.0
0.00
0.33
0.57
0.74
0.88
1.00

64 a course in machine learning

for    = 1, this reduces to the standard f-measure. for    = 0, it
focuses entirely on recall and for            it focuses entirely on preci-
sion. the interpretation of the weight is that f   measures the perfor-
mance for a user who cares    times as much about precision as about
recall.

one thing to keep in mind is that precision and recall (and hence
f-measure) depend crucially on which class is considered the thing
you wish to    nd. in particular, if you take a binary data set if    ip
what it means to be a positive or negative example, you will end
up with completely difference precision and recall values. it is not
the case that precision on the    ipped task is equal to recall on the
original task (nor vice versa). consequently, f-measure is also not the
same. for some tasks where people are less sure about what they
want, they will occasionally report two sets of precision/recall/f-
measure numbers, which vary based on which class is considered the
thing to spot.

there are other standard metrics that are used in different com-
munities. for instance, the medical community is fond of the sensi-
tivity/speci   city metric. a sensitive classi   er is one which almost
always    nds everything it is looking for: it has high recall. in fact,
sensitivity is exactly the same as recall. a speci   c classi   er is one
which does a good job not    nding the things that it doesn   t want to
   nd. speci   city is precision on the negation of the task at hand.

you can compute curves for sensitivity and speci   city much like
those for precision and recall. the typical plot, referred to as the re-
ceiver operating characteristic (or roc curve) plots the sensitivity
against 1     speci   city. given an roc curve, you can compute the
area under the curve (or auc) metric, which also provides a mean-
ingful single number for a system   s performance. unlike f-measures,
which tend to be low because the require agreement, auc scores
tend to be very high, even for not great systems. this is because ran-
dom chance will give you an auc of 0.5 and the best possible auc
is 1.0.

the main message for id74 is that you should choose

whichever one makes the most sense. in many cases, several might
make sense. in that case, you should do whatever is more commonly
done in your    eld. there is no reason to be an outlier without cause.

5.6 cross validation

in chapter 1, you learned about using development data (or held-out
data) to set hyperparameters. the main disadvantage to the develop-
ment data approach is that you throw out some of your training data,
just for estimating one or two hyperparameters.

practical issues

65

algorithm 8 crossvalidate(learningalgorithm, data, k)
1:            
2:          unknown
3: for all hyperparameter settings    do
4:

// store lowest error encountered so far
// store the hyperparameter setting that yielded it

// keep track of the k-many error estimates

err     [ ]
for k = 1 to k do

train     {(xn, yn)     data : n mod k (cid:54)= k     1}
test     {(xn, yn)     data : n mod k = k     1} // test every kth example
model     run learningalgorithm on train
err     err     error of model on test

// add current error to list of errors

end for
avgerr     mean of set err
if avgerr <     then

        avgerr
           

// remember these settings
// because they   re the best so far

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

end if
15:
16: end for

an alternative is the idea of cross validation. in cross validation,

you break your training data up into 10 equally-sized partitions. you
train a learning algorithm on 9 of them and test it on the remaining
1. you do this 10 times, each time holding out a different partition as
the    development    part. you can then average your performance over
all ten parts to get an estimate of how well your model will perform
in the future. you can repeat this process for every possible choice of
hyperparameters to get an estimate of which one performs best. the
general k-fold cross validation technique is shown in algorithm 5.6,
where k = 10 in the preceeding discussion.

in fact, the development data approach can be seen as an approxi-
mation to cross validation, wherein only one of the k loops (line 5 in
algorithm 5.6) is executed.
typical choices for k are 2, 5, 10 and n     1. by far the most com-

mon is k = 10: 10-fold cross validation. sometimes 5 is used for
ef   ciency reasons. and sometimes 2 is used for subtle statistical rea-
sons, but that is quite rare. in the case that k = n     1, this is known
as leave-one-out cross validation (or abbreviated as loo cross val-
idation). after running cross validation, you have two choices. you
can either select one of the k trained models as your    nal model to
make predictions with, or you can train a new model on all of the
data, using the hyperparameters selected by cross-validation. if you
have the time, the latter is probably a better options.

it may seem that loo cross validation is prohibitively expensive
to run. this is true for most learning algorithms except for k-nearest
neighbors. for knn, leave-one-out is actually very natural. we loop
through each training point and ask ourselves whether this example
would be correctly classi   ed for all different possible values of k.

66 a course in machine learning

algorithm 9 knn-train-loo(d)
1: errk     0,    1     k     n     1
2: for n = 1 to n do
3:

sm     (cid:104)||xn     xm|| , m(cid:105),    m (cid:54)= n
s     sort(s)
  y     0
for k = 1 to n     1 do

4:

5:

6:

7:

8:

9:

10:

11:

(cid:104)dist,m(cid:105)     sk
  y       y + ym
if   y (cid:54)= ym then

errk     errk + 1

end if
end for

12:
13: end for
14: return argmink errk

// errk stores how well you do with knn

// compute distances to other points
// put lowest-distance objects    rst
// current label prediction

// let kth closest point vote

// one more error for knn

// return the k that achieved lowest error

this requires only as much computation as computing the k nearest
neighbors for the highest value of k. this is such a popular and
effective approach for knn classi   cation that it is spelled out in
algorithm 5.6.

overall, the main advantage to cross validation over develop-

ment data is robustness. the main advantage of development data is
speed.

one warning to keep in mind is that the goal of both cross valida-
tion and development data is to estimate how well you will do in the
future. this is a question of statistics, and holds only if your test data
really looks like your training data. that is, it is drawn from the same
distribution. in many practical cases, this is not entirely true.

for example, in person identi   cation, we might try to classify

every pixel in an image based on whether it contains a person or not.
if we have 100 training images, each with 10, 000 pixels, then we have
a total of 1m training examples. the classi   cation for a pixel in image
5 is highly dependent on the classi   cation for a neighboring pixel in
the same image. so if one of those pixels happens to fall in training
data, and the other in development (or cross validation) data, your
model will do unreasonably well. in this case, it is important that
when you cross validate (or use development data), you do so over
images, not over pixels. the same goes for text problems where you
sometimes want to classify things at a word level, but are handed a
collection of documents. the important thing to keep in mind is that
it is the images (or documents) that are drawn independently from
your data distribution and not the pixels (or words), which are drawn
dependently.

5.7 hypothesis testing and statistical signi   cance

suppose that you   ve presented a machine learning solution to your
boss that achieves 7% error on cross validation. your nemesis, gabe,
gives a solution to your boss that achieves 6.9% error on cross vali-
dation. how impressed should your boss be? it depends. if this 0.1%
improvement was measured over 1000 examples, perhaps not too
impressed. it would mean that gabe got exactly one more example
right than you did. (in fact, they probably got 15 more right and 14
more wrong.) if this 0.1% impressed was measured over 1, 000, 000
examples, perhaps this is more impressive.

this is one of the most fundamental questions in statistics. you
have a scienti   c hypothesis of the form    gabe   s algorithm is better
than mine.    you wish to test whether this hypothesis is true. you
are testing it against the null hypothesis, which is that gabe   s algo-
rithm is no better than yours. you   ve collected data (either 1000 or
1m data points) to measure the strength of this hypothesis. you want
to ensure that the difference in performance of these two algorithms
is statistically significant: i.e., is probably not just due to random
luck. (a more common question statisticians ask is whether one drug
treatment is better than another, where    another    is either a placebo
or the competitor   s drug.)

there are about    -many ways of doing hypothesis testing. like

id74 and the number of folds of cross validation, this is
something that is very discipline speci   c. here, we will discuss two
popular tests: the paired t-test and id64. these tests, and
other statistical tests, have underlying assumptions (for instance, as-
sumptions about the distribution of observations) and strengths (for
instance, small or large samples). in most cases, the goal of hypoth-
esis testing is to compute a p-value: namely, the id203 that the
observed difference in performance was by chance. the standard way
of reporting results is to say something like    there is a 95% chance
that this difference was not by chance.    the value 95% is arbitrary,
and occasionally people use weaker (90%) test or stronger (99.5%)
tests.

the t-test is an example of a parametric test. it is applicable when
the null hypothesis states that the difference between two responses
has mean zero and unknown variance. the t-test actually assumes
that data is distributed according to a gaussian distribution, which is
probably not true of binary responses. fortunately, for large samples
(at least a few hundred), binary samples are well approximated by
a gaussian distribution. so long as your sample is suf   ciently large,
the t-test is reasonable either for regression or classi   cation problems.

suppose that you evaluate two algorithm on n-many examples.

practical issues

67

t

    1.28
    1.64
    1.96
    2.58

signi   cance

90.0%
95.0%
97.5%
99.5%

table 5.3: table of signi   cance values
for the t-test.

68 a course in machine learning

on each example, you can compute whether the algorithm made
the correct prediction. let a1, . . . , an denote the error of the    rst
algorithm on each example. let b1, . . . , bn denote the error of the
second algorithm. you can compute   a and   b as the means of a and
b, respecitively. finally, center the data as   a = a       a and   b = b       b.
the t-statistic is de   ned as:

t = (  a       b)

n(n     1)
   n(  an       bn)2

(5.15)

(cid:115)

after computing the t-value, you can compare it to a list of values
for computing con   dence intervals. assuming you have a lot of data
(n is a few hundred or more), then you can compare your t-value to
table 5.3 to determine the signi   cance level of the difference.

one disadvantage to the t-test is that it cannot easily be applied
to id74 like f-score. this is because f-score is a com-
puted over an entire test set and does not decompose into a set of
individual errors. this means that the t-test cannot be applied.

fortunately, cross validation gives you a way around this problem.

?

what does it mean for the means
  a and   b to become further apart?
how does this affect the t-value?
what happens if the variance of a
increases?

when you do k-fold cross validation, you are able to compute k
error metrics over the same data. for example, you might run 5-fold
cross validation and compute f-score for every fold. perhaps the f-
scores are 92.4, 93.9, 96.1, 92.2 and 94.4. this gives you an average
f-score of 93.8 over the 5 folds. the standard deviation of this set of
f-scores is:

(cid:115)
(cid:114) 1

=

4
= 1.595

   =

1

n     1

(ai       )2

   
n

(1.96 + 0.01 + 5.29 + 2.56 + 0.36)

(5.16)

(5.17)
(5.18)

you can now assume that the distribution of scores is approximately
gaussian. if this is true, then approximately 70% of the proba-
bility mass lies in the range [         ,    +   ]; 95% lies in the range
[       2  ,    + 2  ]; and 99.5% lies in the range [       3  ,    + 3  ]. so, if we
were comparing our algorithm against one whose average f-score was
90.6%, we could be 95% certain that our superior performance was
not due to chance.5

warning: a con   dence of 95% does not mean    there is a 95%
chance that i am better.    all it means is that if i reran the same ex-
periment 100 times, then in 95 of those experiments i would still win.
these are very different statements. if you say the    rst one, people
who know about statistics will get very mad at you!

one disadvantage to cross validation is that it is computationally
expensive. more folds typically leads to better estimates, but every

5 had we run 10-fold cross validation
we might be been able to get tighter
con   dence intervals.

practical issues

69

algorithm 10 bootstrapevaluate(y,   y, numfolds)
1: scores     [ ]
2: for k = 1 to numfolds do
truth     [ ]
3:
pred     [ ]
for n = 1 to n do

4:

5:

m     uniform random value from 1 to n
truth     truth     ym
pred     pred       ym

6:

7:

8:

9:

end for
scores     scores     f-score(truth, pred)

10:
11: end for
12: return (mean(scores), stddev(scores))

// list of values we want to predict
// list of values we actually predicted

// sample a test point
// add on the truth
// add on our prediction

// evaluate

new fold requires training a new classi   er. this can get very time
consuming. the technique of id64 (and closely related idea
of jack-knifing can address this problem.

suppose that you didn   t want to run cross validation. all you have

is a single held-out test set with 1000 data points in it. you can run
your classi   er and get predictions on these 1000 data points. you
would like to be able to compute a metric like f-score on this test set,
but also get con   dence intervals. the idea behind id64 is
that this set of 1000 is a random draw from some distribution. we
would like to get multiple random draws from this distribution on
which to evaluate. we can simulate multiple draws by repeatedly
subsampling from these 1000 examples, with replacement.

to perform a single bootstrap, you will sample 1000 random points

from your test set of 1000 random points. this sampling must be
done with replacement (so that the same example can be sampled
more than once), otherwise you   ll just end up with your original test
set. this gives you a bootstrapped sample. on this sample, you can
compute f-score (or whatever metric you want). you then do this 99
more times, to get a 100-fold bootstrap. for each bootstrapped sam-
ple, you will be a different f-score. the mean and standard deviation
of this set of f-scores can be used to estimate a con   dence interval for
your algorithm.

the bootstrap resampling procedure is sketched in algorithm 5.7.
this takes three arguments: the true labels y, the predicted labels   y
and the number of folds to run. it returns the mean and standard
deviation from which you can compute a con   dence interval.

5.8 debugging learning algorithms

learning algorithms are notoriously hard to debug, as you may have
already experienced if you have implemented any of the models

70 a course in machine learning

presented so far. the main issue is that when a learning algorithm
doesn   t learn, it   s unclear if this is because there   s a bug or because
the learning problem is too hard (or there   s too much noise, or . . . ).
moreover, sometimes bugs lead to learning algorithms performing
better than they should: these are especially hard to catch (and always
a bit disappointing when you do catch them).

in order to debug failing learning models, it is useful to revisit the

notion of: where can error enter our system? in chapter 2, we con-
sidered a typical design process for machine learning in figure 2.4.
leaving off the top steps in that are not relevant to machine learning
in particular, the basic steps that go into crafting a machine learning
system are: collect data, choose features, choose model family, choose
training data, train model, evaluate on test data. in each of these
steps, things can go wrong. below are some strategies for isolating
the cause of error.

is the problem with generalization to the test data? we have
talked a lot about training error versus test error. in general, it   s
unrealistic to expect to do better on the test data than on the training
data. can your learning system do well on    tting the training data?
if so, then the problem is in generalization (perhaps your model
family is too complicated, you have too many features or not enough
data). if not, then the problem is in representation (you probably
need better features or better data).

do you have train/test mismatch? if you can    t the training data,

but it doesn   t generalize, it could be because there   s something dif-
ferent about your test data. try shuf   ing your training data and test
data together and then randomly selecting a new test set. if you do
well in that condition, then probably the test distribution is strange
in some way. if reselecting the test data doesn   t help, you have other
generalization problems.

is your learning algorithm implemented correctly? this often

means: is it optimizing what you think it   s optimizing. instead
of measuring accuracy, try measuring whatever-quantity-your-
algorithm-is-supposedly-optimizing (like log loss or hinge loss) and
make sure that the optimizer is successfully minimizing this quantity.
it is usually useful to hand-craft some datasets on which you know
the desired behavior. for instance, you could run knn on the xor
data. or you could run id88 on some easily linearly separa-
ble data (for instance positive points along the line x2 = x1 + 1 and
negative points along the line x2 = x1     1). or a decision tree on
nice axis-aligned data. finally, can you compare against a reference
implementation?

do you have an adequate representation? if you cannot even

   t the training data, you might not have a rich enough feature set.

practical issues

71

the easiest way to try to get a learning algorithm to over   t is to add
a new feature to it. you can call this feature the cheatingisfun
feature. the feature value associated with this feature is +1 if this
is a positive example and    1 (or zero) if this is a negative example.
in other words, this feature is a perfect indicator of the class of this
example. if you add the cheatingisfun feature and your algorithm
does not get near 0% training error, this could be because there are
too many noisy features confusing it. you could either remove a lot
of the other features, or make the feature value for cheatingisfun
either +100 or    100 so that the algorithm really looks at it. if you
do this and your algorithm still cannot over   t then you likely have a
bug. (remember to remove the cheatingisfun feature from your
   nal implementation!) if the cheatingisfun technique gets you
near 0% error, then you need to work on better feature design or pick
another learning model (e.g., decision tree versus linear model). if
not, you probably don   t have enough data or have too many features;
try removing as many features as possible.

do you have enough data? try training on 80% of your training
data and look at how much this hurts performance. if it hurts a lot,
then getting more data is likely to help; if it only hurts a little, you
might be data saturated.

5.9 bias/variance trade-off

because one of the key questions in machine learning is the question
of representation, it is common to think about test error in terms of a
decomposition into two terms. let f be the learned classi   er, selected
from a set f of    all possible classi   ers using a    xed representation,   
then:

error( f ) =

(cid:123)(cid:122)
f       f error( f    )
error( f )     min

estimation error

+

f       f error( f )
min

(5.19)

approximation error

(cid:123)(cid:122)

(cid:21)
(cid:125)

(cid:20)
(cid:124)

(cid:20)
(cid:124)

(cid:21)
(cid:125)

here, the second term, the approximation error, measures the qual-
ity of the model family6. one way of thinking of approximation error
is: suppose someone gave me in   nite data to train on   how well
could i do with this representation? the    rst term, the estimation
error, measures how far the actual learned classi   er f is from the
optimal classi   er f    . you can think of this term as measuring how
much you have to pay for the fact that you don   t have in   nite training
data.

unfortunately, it is nearly impossible to compute the estima-

tion error and approxiation error, except in constructed cases. this
doesn   t make the decomposition useless. decompositions like this

6 the    model family    (such as depth
20 id90, or linear classi   ers)
is often refered to as the hypothesis
class. the hypothesis class f denotes
the set of all possible classi   ers we
consider, such as all linear classi   ers.
an classi   er f     f is sometimes called
a hypothesis, though we generally
avoid this latter terminology here.

72 a course in machine learning

are very useful for designing debugging strategies. for instance, the
cheatingisfun strategy is designed explicitly to ensure that the ap-
proximation error is zero, and therefore isolating all error into the
estimation error.

there is a fundamental trade-off between estimation error and ap-
proximation error. as you make your representation more complex,
you make f bigger. this will typically cause a decrease in approxi-
mation error, because you can now    t more functions. but you run a
risk of increasing the estimation error, because you have added more
parameters to    t, and you are likely to suffer from over   tting.

the trade-off between estimation error and approximation error

is often called the bias/variance trade-off, where    approximation
error    is    bias    and    estimation error    is    variance.    to understand
this connection, consider a very simple hypothesis class f that only
contains two functions: the always positive classi   er (that returns +1
regardless of input) and the always negative classi   er. suppose you
have a data generating distribution d that is 60% positive examples
and 40% negative examples. you draw a training set of 41 exam-
ples. there   s about a 90% chance that the majority of these training
examples will be positive, so on this impoverished hypothesis class
f, there   s a 90% chance that it will learn the    all positive    classi   er.
that is: 90% of the time, regardless of the training set, the learning
algorithm learns the same thing. this is low variance as a function of
the random draw of the training set. on the other hand, the learned
classi   er is very insensitive to the input example (in this extreme
case, it   s completely insensitive): it is strongly biased toward predicting
+1 even if everything about the input contradicts this.

5.10 further reading

todo

figure 5.15: object recognition with full
information

6 | beyond binary classification

learning objectives:
    represent complex prediction prob-

lems in a formal learning setting.

    be able to arti   cally    balance   

imbalanced data.

    understand the positive and neg-
ative aspects of several reductions
from multiclass classi   cation to
binary classi   cation.

    recognize the difference between
regression and ordinal regression.

dependencies:

different general classi   cation methods can give different, but
equally plausible, classi   cations, so you need an application
context to choose among them.

    karen sp  rck-jones

in the preceeding chapters, you have learned all about a very
simple form of prediction: predicting bits. in the real world, how-
ever, we often need to predict much more complex objects. you may
need to categorize a document into one of several categories: sports,
entertainment, news, politics, etc. you may need to rank web pages
or ads based on relevance to a query. these problems are all com-
monly encountered, yet fundamentally more complex than binary
classi   cation.

in this chapter, you will learn how to use everything you already

know about binary classi   cation to solve these more complicated
problems. you will see that it   s relatively easy to think of a binary
classi   er as a black box, which you can reuse for solving these more
complex problems. this is a very useful abstraction, since it allows us
to reuse knowledge, rather than having to build new learning models
and algorithms from scratch.

6.1 learning with imbalanced data

your boss tells you to build a classi   er that can identify fraudulent
transactions in credit card histories. fortunately, most transactions
are legitimate, so perhaps only 0.1% of the data is a positive in-
stance. the imbalanced data problem refers to the fact that for a
large number of real world problems, the number of positive exam-
ples is dwarfed by the number of negative examples (or vice versa).
this is actually something of a misnomer: it is not the data that is
imbalanced, but the distribution from which the data is drawn. (and
since the distribution is imbalanced, so must the data be.)

imbalanced data is a problem because machine learning algo-
rithms are too smart for your own good. for most learning algo-
rithms, if you give them data that is 99.9% negative and 0.1% posi-
tive, they will simply learn to always predict negative. why? because
they are trying to minimize error, and they can achieve 0.1% error by
doing nothing! if a teacher told you to study for an exam with 1000

74 a course in machine learning

true/false questions and only one of them is true, it is unlikely you
will study very long.

really, the problem is not with the data, but rather with the way

that you have de   ned the learning problem. that is to say, what you
care about is not accuracy: you care about something else. if you
want a learning algorithm to do a reasonable job, you have to tell it
what you want!

most likely, what you want is not to optimize accuracy, but rather
to optimize some other measure, like f-score or auc. you want your
algorithm to make some positive predictions, and simply prefer those
to be    good.    we will shortly discuss two heuristics for dealing with
this problem: subsampling and weighting. in subsampling, you throw
out some of your negative examples so that you are left with a bal-
anced data set (50% positive, 50% negative). this might scare you
a bit since throwing out data seems like a bad idea, but at least it
makes learning much more ef   cient. in weighting, instead of throw-
ing out positive examples, we just give them lower weight. if you
assign an importance weight of 0.00101 to each of the positive ex-
amples, then there will be as much weight associated with positive
examples as negative examples.

before formally de   ning these heuristics, we need to have a mech-

anism for formally de   ning supervised learning problems. we will
proceed by example, using binary classi   cation as the canonical
learning problem.

task: binary classification
given:
1. an input space x
2. an unknown distribution d over x  {   1, +1}
3. a training set d sampled from d
compute: a function f minimizing: e

(x,y)   d(cid:2) f (x) (cid:54)= y(cid:3)

as in all the binary classi   cation examples you   ve seen, you have
some input space (which has always been rd). there is some distri-
bution that produces labeled examples over the input space. you do
not have access to that distribution, but can obtain samples from it.
your goal is to    nd a classi   er that minimizes error on that distribu-
tion.

a small modi   cation on this de   nition gives a   -weighted classi   -
cation problem, where you believe that the positive class is   -times as

beyond binary classification 75

algorithm 11 subsamplemap(dweighted,   )
1: while true do
2:

(x, y)     dweighted
u     uniform random variable in [0, 1]
if y = +1 or u < 1
return (x, y)

   then

3:

4:

5:

// draw an example from the weighted distribution

end if

6:
7: end while

algorithm 12 subsampletest( f binary,   x)
1: return f binary(  x)

important as the negative class.

task:   -weighted binary classification
given:
1. an input space x
2. an unknown distribution d over x  {   1, +1}
3. a training set d sampled from d

compute: a function f minimizing: e

(x,y)   d

(cid:104)

  y=1(cid:2) f (x) (cid:54)= y(cid:3)(cid:105)

the objects given to you in weighted binary classi   cation are iden-

tical to standard binary classi   cation. the only difference is that the
cost of misprediction for y = +1 is   , while the cost of misprediction
for y =    1 is 1. in what follows, we assume that    > 1. if it is not,
you can simply swap the labels and use 1/  .

the question we will ask is: suppose that i have a good algorithm

for solving the binary classification problem. can i turn that into
a good algorithm for solving the   -weighted binary classification
problem?

in order to do this, you need to de   ne a transformation that maps
a concrete weighted problem into a concrete unweighted problem.
this transformation needs to happen both at training time and at test
time (though it need not be the same transformation!). algorithm 6.1
sketches a training-time sub-sampling transformation and algo-
rithm 6.1 sketches a test-time transformation (which, in this case, is
trivial). all the training algorithm is doing is retaining all positive ex-
amples and a 1/   fraction of all negative examples. the algorithm is
explicitly turning the distribution over weighted examples into a (dif-
ferent) distribution over binary examples. a vanilla binary classi   er

76 a course in machine learning

is trained on this induced distribution.

aside from the fact that this algorithm throws out a lot of data

(especially for large   ), it does seem to be doing a reasonable thing.
in fact, from a reductions perspective, it is an optimal algorithm. you
can prove the following result:

theorem 3 (subsampling optimality). suppose the binary classi   er
trained in algorithm 6.1 achieves a binary error rate of  . then the error
rate of the weighted predictor is equal to    .

this theorem states that if your binary classi   er does well (on the

induced distribution), then the learned predictor will also do well
(on the original distribution). thus, we have successfully converted
a weighted learning problem into a plain classi   cation problem! the
fact that the error rate of the weighted predictor is exactly    times
more than that of the unweighted predictor is unavoidable: the error
metric on which it is evaluated is    times bigger!

the proof of this theorem is so straightforward that we will prove

it here. it simply involves some algebra on expected values.
proof of theorem 3. let dw be the original distribution and let db be
the induced distribution. let f be the binary classi   er trained on data
from db that achieves a binary error rate of  b on that distribution.
we will compute the expected error  w of f on the weighted problem:

(cid:104)
  y=1(cid:2) f (x) (cid:54)= y(cid:3)(cid:105)
dw(x, y)  y=1(cid:2) f (x) (cid:54)= y(cid:3)
(cid:16)dw(x, +1)(cid:2) f (x) (cid:54)= +1(cid:3) + dw(x,   1)
(cid:2) f (x) (cid:54)=    1(cid:3)(cid:17)
(cid:16)db(x, +1)(cid:2) f (x) (cid:54)= +1(cid:3) + db(x,   1)(cid:2) f (x) (cid:54)=    1(cid:3)(cid:17)
(cid:2) f (x) (cid:54)= y(cid:3)

1
  

(x,y)   dw
   
y     1

 w = e
=    
x   x
=       
x   x

=       
x   x
(x,y)   db

=   e
=    b

(6.1)
(6.2)

(6.3)
(6.4)

(6.5)
(6.6)

?

why is it unreasonable to expect
   
to be able to achieve, for instance,
an error of
sublinear in   ?

   , or anything that is

and we   re done! (we implicitly assumed x is discrete. in the case
of continuous data, you need to replace all the sums over x with
integrals over x, but the result still holds.)

instead of subsampling the low-cost class, you could alternatively

oversample the high-cost class. the easiest case is when    is an in-
teger, say 5. now, whenever you get a positive point, you include 5
copies of it in the induced distribution. whenever you get a negative
point, you include a single copy.

?

how can you handle non-integral   ,
for instance 5.5?

beyond binary classification 77

this oversampling algorithm achieves exactly the same theoretical
result as the subsampling algorithm. the main advantage to the over-
sampling algorithm is that it does not throw out any data. the main
advantage to the subsampling algorithm is that it is more computa-
tionally ef   cient.

you might be asking yourself: intuitively, the oversampling algo-

rithm seems like a much better idea than the subsampling algorithm,
at least if you don   t care about computational ef   ciency. but the the-
ory tells us that they are the same! what is going on? of course the
theory isn   t wrong. it   s just that the assumptions are effectively dif-
ferent in the two cases. both theorems state that if you can get error
of   on the binary problem, you automatically get error of     on the
weighted problem. but they do not say anything about how possible
it is to get error   on the binary problem. since the oversampling al-
gorithm produces more data points than the subsampling algorithm
it is very concievable that you could get lower binary error with over-
sampling than subsampling.

the primary drawback to oversampling is computational inef   -

ciency. however, for many learning algorithms, it is straightforward
to include weighted copies of data points at no cost. the idea is to
store only the unique data points and maintain a counter saying how
many times they are replicated. this is not easy to do for the percep-
tron (it can be done, but takes work), but it is easy for both decision
trees and knn. for example, for id90 (recall algorithm 1.3),
the only changes are to: (1) ensure that line 1 computes the most fre-
quent weighted answer, and (2) change lines 10 and 11 to compute
weighted errors.

6.2 multiclass classi   cation

multiclass classi   cation is a natural extension of binary classi   cation.
the goal is still to assign a discrete label to examples (for instance,
is a document about entertainment, sports,    nance or world news?).
the difference is that you have k > 2 classes to choose from.

?

modify the proof of optimality
for the subsampling algorithm so
that it applies to the oversampling
algorithm.

?

why is it hard to change the per-
ceptron? (hint: it has to do with the
fact that id88 is online.)

?

how would you modify knn to
take into account weights?

78 a course in machine learning

algorithm 13 oneversusalltrain(dmulticlass, binarytrain)
1: for i = 1 to k do
2: dbin     relabel dmulticlass so class i is positive and   i is negative

fi     binarytrain(dbin)

3:
4: end for
5: return f1, . . . , fk

algorithm 14 oneversusalltest( f1, . . . , fk,   x)
1: score     (cid:104)0, 0, . . . , 0(cid:105)
2: for i = 1 to k do
3:

y     fi(  x)
scorei     scorei + y

4:
5: end for
6: return argmaxk scorek

// initialize k-many scores to zero

task: multiclass classification
given:
1. an input space x and number of classes k
2. an unknown distribution d over x  [k]
3. a training set d sampled from d
compute: a function f minimizing: e

(x,y)   d(cid:2) f (x) (cid:54)= y(cid:3)

note that this is identical to binary classi   cation, except for the

presence of k classes. (in the above, [k] = {1, 2, 3, . . . , k}.) in fact, if
you set k = 2 you exactly recover binary classi   cation.

the game we play is the same: someone gives you a binary classi-
   er and you have to use it to solve the multiclass classi   cation prob-
lem. a very common approach is the one versus all technique (also
called ova or one versus rest). to perform ova, you train k-many
binary classi   ers, f1, . . . , fk. each classi   er sees all of the training
data. classi   er fi receives all examples labeled class i as positives
and all other examples as negatives. at test time, whichever classi   er
predicts    positive    wins, with ties broken randomly.

the training and test algorithms for ova are sketched in algo-

rithms 6.2 and 6.2. in the testing procedure, the prediction of the ith
classi   er is added to the overall score for class i. thus, if the predic-
tion is positive, class i gets a vote; if the prdiction is negative, every-
one else (implicitly) gets a vote. (in fact, if your learning algorithm
can output a con   dence, as discussed in section ??, you can often do
better by using the con   dence as y, rather than a simple   1.)

ova is quite natural and easy to implement. it also works very

?

suppose that you have n data
points in k classes, evenly divided.
how long does it take to train an
ova classi   er, if the base binary
classi   er takes o(n) time to train?
what if the base classi   er takes
o(n2) time?

?

why would using a con   dence
help?

beyond binary classification 79

well in practice, so long as you do a good job choosing a good binary
classi   cation algorithm tuning its hyperparameters well. its weakness
is that it can be somewhat brittle. intuitively, it is not particularly
robust to errors in the underlying classi   ers. if one classi   er makes a
mistake, it is possible that the entire prediction is erroneous. in fact,
it is entirely possible that none of the k classi   ers predicts positive
(which is actually the worst-case scenario from a theoretical perspec-
tive)! this is made explicit in the ova error bound below.

theorem 4 (ova error bound). suppose the average binary error of the
k binary classi   ers is  . then the error rate of the ova multiclass predictor
is at most (k     1) .
proof of theorem 4. the key question is how erroneous predictions
from the binary classi   ers lead to multiclass errors. we break it down
into false negatives (predicting -1 when the truth is +1) and false
positives (predicting +1 when the truth is -1).

when a false negative occurs, then the testing procedure chooses
randomly between available options, which is all labels. this gives a
(k     1)/k id203 of multiclass error. since only one binary error
is necessary to make this happen, the ef   ciency of this error mode is
[(k     1)/k]/1 = (k     1)/k.

multiple false positives can occur simultaneously. suppose there
are m false positives. if there is simultaneously a false negative, the
error is 1. in order for this to happen, there have to be m + 1 errors,
so the ef   ciency is 1/(m + 1). in the case that there is not a simulta-
neous false negative, the error id203 is m/(m + 1). this requires
m errors, leading to an ef   ciency of 1/(m + 1).
the worse case, therefore, is the false negative case, which gives an
ef   ciency of (k     1)/k. since we have k-many opportunities to err,
we multiply this by k and get a bound of (k     1) .

the constants in this are relatively unimportant: the aspect that

matters is that this scales linearly in k. that is, as the number of
classes grows, so does your expected error.

to develop alternative approaches, a useful way to think about
turning multiclass classi   cation problems into binary classi   cation
problems is to think of them like tournaments (football, soccer   aka
football, cricket, tennis, or whatever appeals to you). you have k
teams entering a tournament, but unfortunately the sport they are
playing only allows two to compete at a time. you want to set up a
way of pairing the teams and having them compete so that you can
   gure out which team is best. in learning, the teams are now the
classes and you   re trying to    gure out which class is best.1

one natural approach is to have every team compete against ev-

ery other team. the team that wins the majority of its matches is

1 the sporting analogy breaks down
a bit for ova: k games are played,
wherein each team will play simultane-
ously against all other teams.

80 a course in machine learning

algorithm 15 allversusalltrain(dmulticlass, binarytrain)

fij        ,   1     i < j     k

1:
2: for i = 1 to k-1 do
3: dpos     all x     dmulticlass labeled i

for j = i+1 to k do

dneg     all x     dmulticlass labeled j
dbin     {(x, +1) : x     dpos}     {(x,   1) : x     dneg}
fij     binarytrain(dbin)

algorithm 16 allversusalltest(all fij,   x)
1: score     (cid:104)0, 0, . . . , 0(cid:105)
2: for i = 1 to k-1 do
3:

// initialize k-many scores to zero

4:

5:

6:

7:

4:

5:

6:

end for

8:
9: end for
10: return all fijs

for j = i+1 to k do
y     fij(  x)
scorei     scorei + y
scorej     scorej - y

end for

7:
8: end for
9: return argmaxk scorek

declared the winner. this is the all versus all (or ava) approach
(sometimes called all pairs). the most natural way to think about it
2) classi   ers. say fij for 1     i < j     k is the classi   er
is as training (k
that pits class i against class j. this classi   er receives all of the class i
examples as    positive    and all of the class j examples as    negative.   
when a test point arrives, it is run through all fij classi   ers. every
time fij predicts positive, class i gets a point; otherwise, class j gets a
point. after running all (k
2) classi   ers, the class with the most votes
wins.

the training and test algorithms for ava are sketched in algo-

rithms 6.2 and 6.2. in theory, the ava mapping is more complicated
than the weighted binary case. the result is stated below, but the
proof is omitted.

theorem 5 (ava error bound). suppose the average binary error of
the (k
2) binary classi   ers is  . then the error rate of the ava multiclass
predictor is at most 2(k     1) .

at this point, you might be wondering if it   s possible to do bet-
ter than something linear in k. fortunately, the answer is yes! the
solution, like so much in computer science, is divide and conquer.
the idea is to construct a binary tree of classi   ers. the leaves of this
tree correspond to the k labels. since there are only log2 k decisions
made to get from the root to a leaf, then there are only log2 k chances

?

suppose that you have n data
points in k classes, evenly divided.
how long does it take to train an
ava classi   er, if the base binary
classi   er takes o(n) time to train?
what if the base classi   er takes
o(n2) time? how does this com-
pare to ova?

the bound for ava is 2(k     1) ; the
bound for ova is (k     1) . does
this mean that ova is necessarily
better than ava? why or why not?

?

beyond binary classification 81

figure 6.2: example classi   cation tree
for k = 8

to make an error.
an example of a classi   cation tree for k = 8 classes is shown in
figure 6.2. at the root, you distinguish between classes {1, 2, 3, 4}
and classes {5, 6, 7, 8}. this means that you will train a binary clas-
si   er whose positive examples are all data points with multiclass
label {1, 2, 3, 4} and whose negative examples are all data points with
multiclass label {5, 6, 7, 8}. based on what decision is made by this
classi   er, you can walk down the appropriate path in the tree. when
k is not a power of 2, the tree will not be full. this classi   cation tree
algorithm achieves the following bound.

theorem 6 (tree error bound). suppose the average binary classi   ers
error is  . then the error rate of the tree classi   er is at most (cid:100)log2 k(cid:101)  .
proof of theorem 6. a multiclass error is made if any classi   er on
the path from the root to the correct leaf makes an error. each has
id203   of making an error and the path consists of at most
(cid:100)log2 k(cid:101) binary decisions.

one thing to keep in mind with tree classi   ers is that you have

control over how the tree is de   ned. in ova and ava you have no
say in what classi   cation problems are created. in tree classi   ers,
the only thing that matters is that, at the root, half of the classes are
considered positive and half are considered negative. you want to
split the classes in such a way that this classi   cation decision is as
easy as possible. you can use whatever you happen to know about
your classi   cation problem to try to separate the classes out in a
reasonable way.

can you do better than (cid:100)log2 k(cid:101)  ? it turns out the answer is yes,
but the algorithms to do so are relatively complicated. you can actu-
ally do as well as 2  using the idea of error-correcting tournaments.
moreover, you can prove a lower bound that states that the best you
could possible do is  /2. this means that error-correcting tourna-
ments are at most a factor of four worse than optimal.

6.3 ranking

you start a new web search company called goohooing. like other
search engines, a user inputs a query and a set of documents is re-
trieved. your goal is to rank the resulting documents based on rel-
evance to the query. the ranking problem is to take a collection of
items and sort them according to some notion of preference. one of
the trickiest parts of doing ranking through learning is to properly
de   ne the id168. toward the end of this section you will see a
very general id168, but before that let   s consider a few special
cases.

82 a course in machine learning

algorithm 17 naiveranktrain(rankingdata, binarytrain)
1: d     [ ]
2: for n = 1 to n do
3:

for all i, j = 1 to m and i (cid:54)= j do

if i is prefered to j on query n then

d     d     (xnij, +1)
d     d     (xnij,   1)

else if j is prefered to i on query n then

4:

5:

6:

7:

8:

end if
end for

9:
10: end for
11: return binarytrain(d)

continuing the web search example, you are given a collection of

queries. for each query, you are also given a collection of documents,
together with a desired ranking over those documents. in the follow-
ing, we   ll assume that you have n-many queries and for each query
you have m-many documents. (in practice, m will probably vary
by query, but for ease we   ll consider the simpli   ed case.) the goal is
to train a binary classi   er to predict a preference function. given a
query q and two documents di and dj, the classi   er should predict
whether di should be preferred to dj with respect to the query q.

as in all the previous examples, there are two things we have to
take care of: (1) how to train the classi   er that predicts preferences;
(2) how to turn the predicted preferences into a ranking. unlike the
previous examples, the second step is somewhat complicated in the
ranking case. this is because we need to predict an entire ranking of
a large number of documents, somehow assimilating the preference
function into an overall permutation.

for notationally simplicity, let xnij denote the features associated
with comparing document i to document j on query n. training is
fairly straightforward. for every n and every pair i (cid:54)= j, we will
create a binary classi   cation example based on features xnij. this
example is positive if i is preferred to j in the true ranking. it is neg-
ative if j is preferred to i. (in some cases the true ranking will not
express a preference between two objects, in which case we exclude
the i, j and j, i pair from training.)

now, you might be tempted to evaluate the classi   cation perfor-

mance of this binary classi   er on its own. the problem with this
approach is that it   s impossible to tell   just by looking at its output
on one i, j pair   how good the overall ranking is. this is because
there is the intermediate step of turning these pairwise predictions
into a coherent ranking. what you need to do is measure how well
the ranking based on your predicted preferences compares to the true
ordering. algorithms 6.3 and 6.3 show naive algorithms for training

algorithm 18 naiveranktest( f ,   x)
1: score     (cid:104)0, 0, . . . , 0(cid:105)
2: for all i, j = 1 to m and i (cid:54)= j do

3:

4:

y     f (  xij)
scorei     scorei + y
scorej     scorej - y

5:
6: end for
7: return argsort(score)

beyond binary classification 83

// initialize m-many scores to zero

// get predicted ranking of i and j

// return queries sorted by score

and testing a ranking function.

these algorithms actually work quite well in the case of bipartite
ranking problems. a bipartite ranking problem is one in which you
are only ever trying to predict a binary response, for instance    is this
document relevant or not?    but are being evaluated according to a
metric like auc. this is essentially because the only goal in bipartite
problems is to ensure that all the relevant documents are ahead of
all the irrelevant documents. there is no notion that one relevant
document is more relevant than another.

for non-bipartite ranking problems, you can do better. first, when

the preferences that you get at training time are more nuanced than
   relevant or not,    you can incorporate these preferences at training
time. effectively, you want to give a higher weight to binary prob-
lems that are very different in terms of preference than others. sec-
ond, rather than producing a list of scores and then calling an arbi-
trary sorting algorithm, you can actually use the preference function
as the sorting function inside your own implementation of quicksort.
we can now formalize the problem. de   ne a ranking as a function

   that maps the objects we are ranking (documents) to the desired
position in the list, 1, 2, . . . m. if   u <   v then u is preferred to v (i.e.,
appears earlier on the ranked document list). given data with ob-
served rankings   , our goal is to learn to predict rankings for new
objects,     . we de   ne   m as the set of all ranking functions over m
objects. we also wish to express the fact that making a mistake on
some pairs is worse than making a mistake on others. this will be
encoded in a cost function    (omega), where   (i, j) is the cost for
accidentally putting something in position j when it should have
gone in position i. to be a valid cost function,    must be (1) symmet-
ric, (2) monotonic and (3) satisfy the triangle inequality. namely: (1)
  (i, j) =   (j, i); (2) if i < j < k or i > j > k then   (i, j)       (i, k);
(3)   (i, j) +   (j, k)       (i, k). with these de   nitions, we can properly
de   ne the ranking problem.

84 a course in machine learning

task:   -ranking
given:
1. an input space x
2. an unknown distribution d over x    m
3. a training set d sampled from d
(cid:35)
compute: a function f : x       m minimizing:

(cid:34)

e

(x,  )   d

[  u <   v] [    v <     u]   (  u,   v)

   
u(cid:54)=v

where      = f (x)

(6.7)

in this de   nition, the only complex aspect is the id168 6.7.
this loss sums over all pairs of objects u and v. if the true ranking (  )
prefers u to v, but the predicted ranking (    ) prefers v to u, then you
incur a cost of   (  u,   v).
depending on the problem you care about, you can set    to many
   standard    options. if   (i, j) = 1 whenever i (cid:54)= j, then you achieve
the kemeny distance measure, which simply counts the number of
pairwise misordered items. in many applications, you may only care
about getting the top k predictions correct. for instance, your web
search algorithm may only display k = 10 results to a user. in this
case, you can de   ne:

  (i, j) =

if min{i, j}     k and i (cid:54)= j

1
0 otherwise

(6.8)

(cid:40)

in this case, only errors in the top k elements are penalized. swap-
ping items 55 and 56 is irrelevant (for k < 55).

finally, in the bipartite ranking case, you can express the area

under the curve (auc) metric as:

  (i, j) =

(m
2 )

m+(m     m+)

  

if i     m+ and j > m+
1 if j     m+ and i > m+
0 otherwise

(6.9)

                1

here, m is the total number of objects to be ranked and m+ is the
number that are actually    good.    (hence, m     m+ is the number
that are actually    bad,    since this is a bipartite problem.) you are
only penalized if you rank a good item in position greater than m+
or if you rank a bad item in a position less than or equal to m+.

in order to solve this problem, you can follow a recipe similar to

the naive approach sketched earlier. at training time, the biggest

beyond binary classification 85

algorithm 19 ranktrain(drank,   , binarytrain)
1: dbin     [ ]
2: for all (x,   )     drank do
for all u (cid:54)= v do
y     sign(  v -   u)
w       (  u,   v)
dbin     dbin     (y, w, xuv)

3:

4:

5:

6:

end for

7:
8: end for
9: return binarytrain(dbin)

// y is +1 if u is prefered to v
// w is the cost of misclassi   cation

// pick pivot
// elements that seem smaller than p
// elements that seem larger than p

  y     f (xup)
if uniform random variable <   y then

// what is the id203 that u precedes p

algorithm 20 ranktest( f ,   x, obj)
1: if obj contains 0 or 1 elements then
2:
3: else
4:

return obj
p     randomly chosen object in obj
left     [ ]
right     [ ]
for all u     obj \{p} do

left     left     u
right     right     u

else

end if
end for
left     ranktest( f ,   x, left)
right     ranktest( f ,   x, right)
return left     (cid:104)p(cid:105)     right

17:
18: end if

5:

6:

7:

8:

9:

10:

11:

12:

13:

14:

15:

16:

// sort earlier elements
// sort later elements

change is that you can weight each training example by how bad it
would be to mess it up. this change is depicted in algorithm 6.3,
where the binary classi   cation data has weights w provided for saying
how important a given example is. these weights are derived from
the cost function   .

at test time, instead of predicting scores and then sorting the list,
you essentially run the quicksort algorithm, using f as a comparison
function. at each step in algorithm 6.3, a pivot p is chosen. every
other object u is compared to p using f . if f thinks u is better, then it
is sorted on the left; otherwise it is sorted on the right. there is one
major difference between this algorithm and quicksort: the compar-
ison function is allowed to be probabilistic. if f outputs probabilities,
for instance it predicts that u has an 80% id203 of being better
than p, then it puts it on the left with 80% id203 and on the
right with 20% id203. (the pseudocode is written in such a way
that even if f just predicts    1, +1, the algorithm still works.)

86 a course in machine learning

this algorithm is better than the naive algorithm in at least two
ways. first, it only makes o(m log2 m) calls to f (in expectation),
rather than o(m2) calls in the naive case. second, it achieves a better
error bound, shown below:

theorem 7 (rank error bound). suppose the average binary error of f
is  . then the ranking algorithm achieves a test error of at most 2  in the
general case, and   in the bipartite case.

6.4 further reading

todo further reading

7 | linear models

learning objectives:
    de   ne and plot four surrogate loss

functions: squared loss, logistic loss,
exponential loss and hinge loss.

    compare and contrast the optimiza-

tion of 0/1 loss and surrogate loss
functions.

    solve the optimization problem

for squared loss with a quadratic
regularizer in closed form.

    implement and debug gradient

descent and subid119.

dependencies:

the essence of mathematics is not to make simple things compli-
cated, but to make complicated things simple.
    stanley gudder

in chapter 4, you learned about the id88 algorithm for
linear classi   cation. this was both a model (linear classi   er) and al-
gorithm (the id88 update rule) in one. in this section, we will
separate these two, and consider general ways for optimizing lin-
ear models. this will lead us into some aspects of optimization (aka
mathematical programming), but not very far. at the end of this
chapter, there are pointers to more literature on optimization for
those who are interested.

the basic idea of the id88 is to run a particular algorithm
until a linear separator is found. you might ask: are there better al-
gorithms for    nding such a linear separator? we will follow this idea
and formulate a learning problem as an explicit optimization prob-
lem:    nd me a linear separator that is not too complicated. we will
see that    nding an    optimal    separator is actually computationally
prohibitive, and so will need to    relax    the optimality requirement.
this will lead us to a convex objective that combines a loss func-
tion (how well are we doing on the training data?) and a regularizer
(how complicated is our learned model?). this learning framework
is known as both tikhonov id173 and structural risk mini-
mization.

7.1 the optimization framework for linear models

you have already seen the id88 as a way of    nding a weight
vector w and bias b that do a good job of separating positive train-
ing examples from negative training examples. the id88 is a
model and algorithm in one. here, we are interested in separating
these issues. we will focus on linear models, like the id88.
but we will think about other, more generic ways of    nding good
parameters of these models.

the goal of the id88 was to    nd a separating hyperplane
for some training data set. for simplicity, you can ignore the issue
of over   tting (but just for now!). not all data sets are linearly sepa-

you should remember the yw   
trick from the id88 discus-
sion. if not, re-convince yourself
that this is doing the right thing.

?

x

88 a course in machine learning

rable. in the case that your training data isn   t linearly separable, you
might want to    nd the hyperplane that makes the fewest errors on
the training data. we can write this down as a formal mathematics
optimization problem as follows:

min
w,b

   
n

1[yn(w    xn + b) > 0]

(7.1)

in this expression, you are optimizing over two variables, w and b.
the objective function is the thing you are trying to minimize. in
this case, the objective function is simply the error rate (or 0/1 loss) of
the linear classi   er parameterized by w, b. in this expression, 1[  ] is
the indicator function: it is one when (  ) is true and zero otherwise.

we know that the id88 algorithm is guaranteed to    nd

parameters for this model if the data is linearly separable. in other
words, if the optimum of eq (7.1) is zero, then the id88 will
ef   ciently    nd parameters for this model. the notion of    ef   ciency   
depends on the margin of the data for the id88.

you might ask: what happens if the data is not linearly separable?

is there an ef   cient algorithm for    nding an optimal setting of the
parameters? unfortunately, the answer is no. there is no polynomial
time algorithm for solving eq (7.1), unless p=np. in other words,
this problem is np-hard. sadly, the proof of this is quite complicated
and beyond the scope of this book, but it relies on a reduction from a
variant of satis   ability. the key idea is to turn a satis   ability problem
into an optimization problem where a clause is satis   ed exactly when
the hyperplane correctly separates the data.

you might then come back and say: okay, well i don   t really need

an exact solution. i   m willing to have a solution that makes one or
two more errors than it has to. unfortunately, the situation is really
bad. zero/one loss is np-hard to even appproximately minimize. in
other words, there is no ef   cient algorithm for even    nding a solution
that   s a small constant worse than optimal. (the best known constant
at this time is 418/415     1.007.)

however, before getting too disillusioned about this whole enter-
prise (remember: there   s an entire chapter about this framework, so
it must be going somewhere!), you should remember that optimizing
eq (7.1) perhaps isn   t even what you want to do! in particular, all it
says is that you will get minimal training error. it says nothing about
what your test error will be like. in order to try to    nd a solution that
will generalize well to test data, you need to ensure that you do not
over   t the data. to do this, you can introduce a regularizer over the
parameters of the model. for now, we will be vague about what this
regularizer looks like, and simply call it an arbitrary function r(w, b).

this leads to the following, regularized objective:

min
w,b

   
n

1[yn(w    xn + b) > 0] +   r(w, b)

(7.2)

in eq (7.2), we are now trying to optimize a trade-off between a so-
lution that gives low training error (the    rst term) and a solution
that is    simple    (the second term). you can think of the maximum
depth hyperparameter of a decision tree as a form of id173
for trees. here, r is a form of id173 for hyperplanes. in this
formulation,    becomes a hyperparameter for the optimization.

the key remaining questions, given this formalism, are:

    how can we adjust the optimization problem so that there are

ef   cient algorithms for solving it?

    what are good regularizers r(w, b) for hyperplanes?

    assuming we can adjust the optimization problem appropriately,
what algorithms exist for ef   ciently solving this regularized opti-
mization problem?

we will address these three questions in the next sections.

7.2 convex surrogate id168s

you might ask: why is optimizing zero/one loss so hard? intuitively,
one reason is that small changes to w, b can have a large impact on
the value of the objective function. for instance, if there is a positive
training example with w, x    +b =    0.0000001, then adjusting b up-
wards by 0.00000011 will decrease your error rate by 1. but adjusting
it upwards by 0.00000009 will have no effect. this makes it really
dif   cult to    gure out good ways to adjust the parameters.

to see this more clearly, it is useful to look at plots that relate

margin to loss. such a plot for zero/one loss is shown in figure 7.1.
in this plot, the horizontal axis measures the margin of a data point
and the vertical axis measures the loss associated with that margin.
for zero/one loss, the story is simple. if you get a positive margin
(i.e., y(w    x + b) > 0) then you get a loss of zero. otherwise you get
a loss of one. by thinking about this plot, you can see how changes
to the parameters that change the margin just a little bit can have an
enormous effect on the overall loss.

you might decide that a reasonable way to address this problem is

to replace the non-smooth zero/one loss with a smooth approxima-
tion. with a bit of effort, you could probably concoct an    s   -shaped
function like that shown in figure 7.2. the bene   t of using such an
s-function is that it is smooth, and potentially easier to optimize. the
dif   culty is that it is not convex.

linear models

89

?

assuming r does the    right thing,   
what value(s) of    will lead to over-
   tting? what value(s) will lead to
under   tting?

figure 7.1: plot of zero/one versus
margin

figure 7.2: plot of zero/one versus
margin and an s version of it

90 a course in machine learning

if you remember from calculus, a convex function is one that looks
like a happy face ((cid:94)). (on the other hand, a concave function is one
that looks like a sad face ((cid:95)); an easy mnemonic is that you can hide
under a concave function.) there are two equivalent de   nitions of
a convex function. the    rst is that it   s second derivative is always
non-negative. the second, more geometric, de   tion is that any chord
of the function lies above it. this is shown in figure 7.3. there you
can see a convex function and a non-convex function, both with two
chords drawn in. in the case of the convex function, the chords lie
above the function. in the case of the non-convex function, there are
parts of the chord that lie below the function.

convex functions are nice because they are easy to minimize. intu-
itively, if you drop a ball anywhere in a convex function, it will even-
tually get to the minimum. this is not true for non-convex functions.
for example, if you drop a ball on the very left end of the s-function
from figure 7.2, it will not go anywhere.

this leads to the idea of convex surrogate id168s. since
zero/one loss is hard to optimize, you want to optimize something
else, instead. since convex functions are easy to optimize, we want
to approximate zero/one loss with a convex function. this approxi-
mating function will be called a surrogate loss. the surrogate losses
we construct will always be upper bounds on the true id168:
this guarantees that if you minimize the surrogate loss, you are also
pushing down the real loss.

there are four common surrogate id168s, each with their

own properties: hinge loss, logistic loss, exponential loss and
squared loss. these are shown in figure 7.4 and de   ned below.
these are de   ned in terms of the true label y (which is just {   1, +1})
and the predicted value   y = w    x + b.

zero/one:
hinge:

logistic:

exponential:
squared:

(cid:96)(0/1)(y,   y) = 1[y   y     0]
(cid:96)(hin)(y,   y) = max{0, 1     y   y}
(cid:96)(log)(y,   y) =
(cid:96)(exp)(y,   y) = exp[   y   y]
(cid:96)(sqr)(y,   y) = (y       y)2

log 2

1

log (1 + exp[   y   y])

(7.3)
(7.4)
(7.5)

(7.6)
(7.7)

1
log 2 term out front is there sim-

in the de   nition of logistic loss, the
ply to ensure that (cid:96)(log)(y, 0) = 1. this ensures, like all the other
surrogate id168s, that logistic loss upper bounds the zero/one
loss. (in practice, people typically omit this constant since it does not
affect the optimization.)

there are two big differences in these id168s. the    rst

difference is how    upset    they get by erroneous predictions. in the

figure 7.3: plot of convex and non-
convex functions with two chords each

figure 7.4: surrogate loss fns

linear models

91

case of hinge loss and logistic loss, the growth of the function as   y
goes negative is linear. for squared loss and exponential loss, it is
super-linear. this means that exponential loss would rather get a few
examples a little wrong than one example really wrong. the other
difference is how they deal with very con   dent correct predictions.
once y   y > 1, hinge loss does not care any more, but logistic and
exponential still think you can do better. on the other hand, squared
loss thinks it   s just as bad to predict +3 on a positive example as it is
to predict    1 on a positive example.

7.3 weight id173

in our learning objective, eq (7.2), we had a term correspond to the
zero/one loss on the training data, plus a regularizer whose goal
was to ensure that the learned function didn   t get too    crazy.    (or,
more formally, to ensure that the function did not over   t.) if you re-
place to zero/one loss with a surrogate loss, you obtain the following
objective:

min
w,b

   
n

(cid:96)(yn, w    xn + b) +   r(w, b)

(7.8)

the question is: what should r(w, b) look like?

from the discussion of surrogate id168, we would like

to ensure that r is convex. otherwise, we will be back to the point
where optimization becomes dif   cult. beyond that, a common desire
is that the components of the weight vector (i.e., the wds) should be
small (close to zero). this is a form of inductive bias.

why are small values of wd good? or, more precisely, why do

small values of wd correspond to simple functions? suppose that we
have an example x with label +1. we might believe that other ex-
amples, x(cid:48) that are nearby x should also have label +1. for example,
if i obtain x(cid:48) by taking x and changing the    rst component by some
small value   and leaving the rest the same, you might think that the
classi   cation would be the same. if you do this, the difference be-
tween   y and   y(cid:48) will be exactly  w1. so if w1 is reasonably small, this
is unlikely to have much of an effect on the classi   cation decision. on
the other hand, if w1 is large, this could have a large effect.
another way of saying the same thing is to look at the derivative
of the predictions as a function of w1. the derivative of w    x + b with
respect to w1 is:
    [w    x + b]

    [   d wdxd + b]

=

   w1

   w1

= x1

(7.9)

interpreting the derivative as the rate of change, we can see that
the rate of change of the prediction function is proportional to the

92 a course in machine learning

(cid:113)

individual weights. so if you want the function to change slowly, you
want to ensure that the weights stay small.

one way to accomplish this is to simply use the norm of the

   d w2

weight vector. namely r(norm)(w, b) = ||w|| =
is convex and smooth, which makes it easy to minimize. in prac-
tice, it   s often easier to use the squared norm, namely r(sqr)(w, b) =
||w||2 =    d w2
remains convex. an alternative to using the sum of squared weights
is to use the sum of absolute weights: r(abs)(w, b) =    d |wd|. both of
these norms are convex.

d because it removes the ugly square root term and

d. this function

in addition to small weights being good, you could argue that zero

weights are better. if a weight wd goes to zero, then this means that
feature d is not used at all in the classi   cation decision. if there are a
large number of irrelevant features, you might want as many weights
to go to zero as possible. this suggests an alternative regularizer:
r(cnt)(w, b) =    d 1[xd (cid:54)= 0].

this line of thinking leads to the general concept of p-norms.

(technically these are called (cid:96)p (or    ell p   ) norms, but this notation
clashes with the use of (cid:96) for    loss.   ) this is a family of norms that all
have the same general    avor. we write ||w||p to denote the p-norm of
w.

(cid:32)

(cid:33) 1

p

||w||p =

|wd|p

   
d

(7.10)

you can check that the 2-norm exactly corresponds to the usual eu-
clidean norm, and that the 1-norm corresponds to the    absolute   
regularizer described above.

when p-norms are used to regularize weight vectors, the interest-

ing aspect is how they trade-off multiple features. to see the behavior
of p-norms in two dimensions, we can plot their contour (or level-
set). figure 7.5 shows the contours for the same p norms in two
dimensions. each line denotes the two-dimensional vectors to which
this norm assignes a total value of 1. by changing the value of p, you
can interpolate between a square (the so-called    max norm   ), down
to a circle (2-norm), diamond (1-norm) and pointy-star-shaped-thing
(p < 1 norm).

in general, smaller values of p    prefer    sparser vectors. you can

see this by noticing that the contours of small p-norms    stretch   
out along the axes. it is for this reason that small p-norms tend to
yield weight vectors with many zero entries (aka sparse weight vec-
tors). unfortunately, for p < 1 the norm becomes non-convex. as
you might guess, this means that the 1-norm is a popular choice for
sparsity-seeking applications.

?

why do we not regularize the bias
term b?

?

why might you not want to use
r(cnt) as a regularizer?

?

you can actually identify the r(cnt)
regularizer with a p-norm as well.
which value of p gives it to you?
(hint: you may have to take a limit.)

figure 7.5: loss:norms2d: level sets of
the same p-norms

?

the max norm corresponds to
limp      . why is this called the max
norm?

linear models

93

math review | gradients
a gradient is a multidimensional generalization of a derivative. suppose you have a function
: rd     r that takes a vector x = (cid:104)x1, x2, . . . , xd(cid:105) as input and produces a scalar value as output.
f
you can differentite this function according to any one of the inputs; for instance, you can compute     f
   x5
to get the derivative with respect to the    fth input. the gradient of f is just the vector consisting of the
derivative f with respect to each of its input coordinates independently, and is denoted     f , or, when
the input to f is ambiguous,    x f . this is de   ned as:

(cid:29)

(cid:28)     f
(cid:68)

   x1

   x f =

,

    f
   x2

,

. . .

,

    f
   xd

for example, consider the function f (x1, x2, x3) = x3
   x f =

1 + 5x2 , 5x1     3x2
3x2

3 ,     6x2x3

(cid:69)

1 + 5x1x2     3x2x2

3. the gradient is:

(7.11)

(7.12)
note that if f : rd     r, then     f : rd     rd. if you evaluate     f (x), this will give you the gradient at
x, a vector in rd. this vector can be interpreted as the direction of steepest ascent: namely, if you were
to travel an in   nitesimal amount in the direction of the gradient, you would go uphill (i.e., increase f )
the most.

7.4 optimization with id119

figure 7.6:

envision the following problem. you   re taking up a new hobby:
blindfolded mountain climbing. someone blindfolds you and drops
you on the side of a mountain. your goal is to get to the peak of the
mountain as quickly as possible. all you can do is feel the mountain
where you are standing, and take steps. how would you get to the
top of the mountain? perhaps you would feel to    nd out what direc-
tion feels the most    upward    and take a step in that direction. if you
do this repeatedly, you might hope to get the the top of the moun-
tain. (actually, if your friend promises always to drop you on purely
concave mountains, you will eventually get to the peak!)

the idea of gradient-based methods of optimization is exactly the

same. suppose you are trying to    nd the maximum of a function
f (x). the optimizer maintains a current estimate of the parameter of
interest, x. at each step, it measures the gradient of the function it is
trying to optimize. this measurement occurs at the current location,
x. call the gradient g. it then takes a step in the direction of the
gradient, where the size of the step is controlled by a parameter   
(eta). the complete step is x     x +   g. this is the basic idea of
gradient ascent.

the opposite of gradient ascent is id119. all of our

94 a course in machine learning

algorithm 21 gradientdescent(f, k,   1, . . . )
1: z(0)     (cid:104)0, 0, . . . , 0(cid:105)
2: for k = 1 . . . k do
g(k)        zf|z(k-1)
3:
z(k)     z(k-1)       (k)g(k)

// initialize variable we are optimizing

// compute gradient at current location
// take a step down the gradient

4:
5: end for
6: return z(k)

learning problems will be framed as minimization problems (trying
to reach the bottom of a ditch, rather than the top of a hill). there-
fore, descent is the primary approach you will use. one of the major
conditions for gradient ascent being able to    nd the true, global min-
imum, of its objective function is convexity. without convexity, all is
lost.
the id119 algorithm is sketched in algorithm 7.4.
the function takes as arguments the function f to be minimized,
the number of iterations k to run and a sequence of learning rates
  1, . . . ,   k. (this is to address the case that you might want to start
your mountain climbing taking large steps, but only take small steps
when you are close to the peak.)

the only real work you need to do to apply a id119

method is be able to compute derivatives. for concreteness, suppose
that you choose exponential loss as a id168 and the 2-norm as
a regularizer. then, the regularized objective function is:

exp(cid:2)     yn(w    xn + b)(cid:3) +

l(w, b) =    
n

||w||2

  
2

(7.13)

the only    strange    thing in this objective is that we have replaced
   with   
2 . the reason for this change is just to make the gradients
cleaner. we can    rst compute derivatives with respect to b:

   l
   b

   
n
   
   b

exp(cid:2)     yn(w    xn + b)(cid:3) +
exp(cid:2)     yn(w    xn + b)(cid:3) + 0
(cid:19)
(cid:18)    
yn exp(cid:2)     yn(w    xn + b)(cid:3)

    yn(w    xn + b)

   b

=

   
   b
=    
n
=    
n
=        
n

   
   b

||w||2

  
2

exp(cid:2)     yn(w    xn + b)(cid:3)

(7.14)

(7.15)

(7.16)

(7.17)

before proceeding, it is worth thinking about what this says. from a
practical perspective, the optimization will operate by updating b    
b           l
   b . consider positive examples: examples with yn = +1. we
would hope for these examples that the current prediction, w    xn + b,
is as large as possible. as this value tends toward    , the term in the
exp[] goes to zero. thus, such points will not contribute to the step.

linear models

95

?

this considered the case of posi-
tive examples. what happens with
negative examples?

however, if the current prediction is small, then the exp[] term will
be positive and non-zero. this means that the bias term b will be
increased, which is exactly what you would want. moreover, once all
points are very well classi   ed, the derivative goes to zero.

now that we have done the easy case, let   s do the gradient with

respect to w.

   wl =    w    

n

exp(cid:2)     yn(w    xn + b)(cid:3) +    w
(   w     yn(w    xn + b)) exp(cid:2)     yn(w    xn + b)(cid:3) +   w
ynxn exp(cid:2)     yn(w    xn + b)(cid:3) +   w

||w||2

  
2

(7.18)

(7.19)
(7.20)

=    
n

=        
n

now you can repeat the previous exercise. the update is of the form
w     w          wl. for well classi   ed points (ones that tend toward
yn   ), the gradient is near zero. for poorly classi   ed points, the gra-
dient points in the direction    ynxn, so the update is of the form
w     w + cynxn, where c is some constant. this is just like the per-
ceptron update! note that c is large for very poorly classi   ed points
and small for relatively well classi   ed points.
by looking at the part of the gradient related to the regularizer,
the update says: w     w       w = (1       )w. this has the effect of
shrinking the weights toward zero. this is exactly what we expect the
regulaizer to be doing!

the success of id119 hinges on appropriate choices
for the step size. figure 7.7 shows what can happen with gradient
descent with poorly chosen step sizes. if the step size is too big, you
can accidentally step over the optimum and end up oscillating. if the
step size is too small, it will take way too long to get to the optimum.
for a well-chosen step size, you can show that id119 will
approach the optimal value at a fast rate. the notion of convergence
here is that the objective value converges to the true minimum.

figure 7.7: good and bad step sizes

theorem 8 (id119 convergence). under suitable condi-
tions1, for an appropriately chosen constant step size (i.e.,   1 =   2,         =
  ), the convergence rate of id119 is o(1/k). more speci   -
cally, letting z    be the global minimum of f, we have: f (z(k))     f (z   )    
2||z(0)   z   ||2

  k

.

1 speci   cally the function to be opti-
mized needs to be strongly convex.
this is true for all our problems, pro-
vided    > 0. for    = 0 the rate could
be as bad as o(1/

   

k).

the proof of this theorem is a bit complicated because it makes

heavy use of some id202. the key is to set the learning rate
to 1/l, where l is the maximum curvature of the function that is
being optimized. the curvature is simply the    size    of the second
derivative. functions with high curvature have gradients that change

?

a naive reading of this theorem
seems to say that you should choose
huge values of   . it should be obvi-
ous that this cannot be right. what
is missing?

96 a course in machine learning

quickly, which means that you need to take small steps to avoid
overstepping the optimum.

this convergence result suggests a simple approach to decid-

ing when to stop optimizing: wait until the objective function stops
changing by much. an alternative is to wait until the parameters stop
changing by much. a    nal example is to do what you did for percep-
tron: early stopping. every iteration, you can check the performance
of the current model on some held-out data, and stop optimizing
when performance plateaus.

7.5 from gradients to subgradients

as a good exercise, you should try deriving id119 update
rules for the different id168s and different regularizers you   ve
learned about. however, if you do this, you might notice that hinge
loss and the 1-norm regularizer are not differentiable everywhere! in
particular, the 1-norm is not differentiable around wd = 0, and the
hinge loss is not differentiable around y   y = 1.

the solution to this is to use subgradient optimization. one way
to think about subgradients is just to not think about it: you essen-
tially need to just ignore the fact that you forgot that your function
wasn   t differentiable, and just try to apply id119 anyway.
z}. this function is differentiable for z > 1 and differentiable for
z < 1, but not differentiable at z = 1. you can derive this using
differentiation by parts:

to be more concrete, consider the hinge function f (z) = max{0, 1   

   
   z

0
1     z

(cid:40)
(cid:40)    
   z 0
(cid:40)
   z (1     z)
   
if z     1
0
   1 if z < 1

if z > 1
if z < 1

if z > 1
if z < 1

   
   z

f (z) =

=

=

(7.21)

(7.22)

(7.23)

thus, the derivative is zero for z < 1 and    1 for z > 1, matching

intuition from the figure. at the non-differentiable point, z = 1,
we can use a subderivative: a generalization of derivatives to non-
differentiable functions. intuitively, you can think of the derivative
of f at z as the tangent line. namely, it is the line that touches f at
z that is always below f (for convex functions). the subderivative,
denoted           f , is the set of all such lines. at differentiable positions,
this set consists just of the actual derivative. at non-differentiable
positions, this contains all slopes that de   ne lines that always lie
under the function and make contact at the operating point. this is

figure 7.8: hinge loss with sub

linear models

97

,

algorithm 22 hingeregularizedgd(d,   , maxiter)
1: w     (cid:104)0, 0, . . . 0(cid:105)
2: for iter = 1 . . . maxiter do
g     (cid:104)0, 0, . . . 0(cid:105)
,
3:
for all (x,y)     d do

b     0
g     0
if y(w    x + b)     1 then

4:

// initialize weights and bias

// initialize gradient of weights and bias

5:

6:

7:

g     g + y x
g     g + y

9:

8:

10:

end if
end for
g     g       w
11: w     w +   g
b     b +   g

12:
13: end for
14: return w, b

// update weight gradient
// update bias derivative

// add in id173 term
// update weights
// update bias

shown pictorally in figure 7.8, where example subderivatives are
shown for the hinge id168. in the particular case of hinge loss,
any value between 0 and    1 is a valid subderivative at z = 0. in fact,
the subderivative is always a closed set of the form [a, b], where a and
b can be derived by looking at limits from the left and right.

this gives you a way of computing derivative-like things for non-
differentiable functions. take hinge loss as an example. for a given
example n, the subgradient of hinge loss can be computed as:

(cid:40)

=          w

0
1     yn(w    xn + b) otherwise

         w max{0, 1     yn(w    xn + b)}
(cid:40)
(cid:40)

         w0
         w1     yn(w    xn + b) otherwise
0
   ynxn otherwise

if yn(w    xn + b) > 1

=

=

if yn(w    xn + b) > 1

if yn(w    xn + b) > 1

(7.24)

(7.25)

(7.26)

(7.27)

if you plug this subgradient form into algorithm 7.4, you obtain
algorithm 7.5. this is the subid119 for regularized hinge
loss (with a 2-norm regularizer).

7.6 closed-form optimization for squared loss

although id119 is a good, generic optimization algorithm,
there are cases when you can do better. an example is the case of a
2-norm regularizer and squared error id168. for this, you can
actually obtain a closed form solution for the optimal weights. how-
ever, to obtain this, you need to rewrite the optimization problem in
terms of matrix operations. for simplicity, we will only consider the

98 a course in machine learning

math review | id127 and inversion
if a and b are matrices, and a is n  k and b is k  m (the inner dimensions must match), then the ma-
trix product ab is a matrix c that is n   m, with cn,m =    k an,kbk,m. if v is a vector in rd, we will
treat is as a column vector, or a matrix of size d  1. thus, av is well de   ned if a is d  m, and the result-
ing product is a vector u with um =    d ad,mvd.

aside from matrix product, a fundamental matrix operation is inversion. we will often encounter a
form like ax = y, where a and y are known and we want to solve for a. if a is square of size n  n,
then the inverse of a, denoted a   1, is also a square matrix of size n  n, such that aa   1 = in = a   1a.
i.e., multiplying a matrix by its inverse (on either side) gives back the identity matrix. using this, we
can solve ax = y by multiplying both sides by a   1 on the left (recall that order matters in matrix mul-
tiplication), yielding a   1ax = a   1y from which we can conclude x = a   1y. note that not all square
matrices are invertible. for instance, the all zeros matrix does not have an inverse (in the same way
that 1/0 is not de   ned for scalars). however, there are other matrices that do not have inverses; such
matrices are called singular.

figure 7.9:

unbiased version, but the extension is exercise ??. this is precisely the
id75 setting.
you can think of the training data as a large matrix x of size n  d,

where xn,d is the value of the dth feature on the nth example. you
can think of the labels as a column (   tall   ) vector y of dimension n.
finally, you can think of the weights as a column vector w of size
d. thus, the matrix-vector product a = xw has dimension n. in
particular:

an = [xw]n =    

d

xn,dwd

(7.28)

this means, in particular, that a is actually the predictions of the
model. instead of calling this a, we will call it   y. the squared error
   n(   yn     yn)2, which can be written
says that we should minimize 1
2
in vector form as a minimization of 1
2
this can be expanded visually as:

(cid:12)(cid:12)(cid:12)(cid:12)   y     y(cid:12)(cid:12)(cid:12)(cid:12)2.
                  
                  

=

(cid:125)

(cid:124)

   d x1,dwd
   d x2,dwd

   d xn,dwd

...
(cid:123)(cid:122)

  y

                  

(cid:125)

                  

(cid:124)

w1
w2
...
(cid:123)(cid:122)
wd
w

                  

(cid:125)

   

                  

                  

y1
y2
...
(cid:124) (cid:123)(cid:122) (cid:125)
yn

  y
(7.29)

                  

(cid:124)

x1,1
x2,1
...
xn,1

. . .
. . .
...
. . .

x1,d
x2,d
...
xn,d

x1,2
x2,2
...
xn,2

(cid:123)(cid:122)

x

?

verify that the squared error can
actually be written as this vector
norm.

linear models

99

so, compactly, our optimization problem can be written as:

min
w

l(w) =

1
2

||xw     y||2 +

||w||2

  
2

(7.30)

if you recall from calculus, you can minimize a function by setting its
derivative to zero. we start with the weights w and take gradients:

we can equate this to zero and solve, yielding:

   wl(w) = x(cid:62) (xw     y) +   w
(cid:16)
= x(cid:62)xw     x(cid:62)y +   w
w     x(cid:62)y
=

(cid:17)

x(cid:62)x +   i
(cid:17)
x(cid:62)x +   i
(cid:16)
x(cid:62)x +   id

(cid:17)
w     x(cid:62)y = 0
w = x(cid:62)y
x(cid:62)x +   id

(cid:17)   1x(cid:62)y

(cid:16)
       (cid:16)

       w =

(7.31)
(7.32)
(7.33)

(7.34)
(7.35)
(7.36)

?

for those who are keen on linear
algebra, you might be worried that
the matrix you must invert might
not be invertible. is this actually a
problem?

thus, the optimal solution of the weights can be computed by a few
id127s and a matrix inversion. as a sanity check,
you can make sure that the dimensions match. the matrix x(cid:62)x has
dimension d  d, and therefore so does the inverse term. the inverse
is d  d and x(cid:62) is d  n, so that product is d  n. multiplying through
by the n  1 vector y yields a d  1 vector, which is precisely what we
want for the weights.

note that this gives an exact solution, modulo numerical innacu-

racies with computing matrix inverses. in contrast, id119
will give you progressively better solutions and will    eventually   
converge to the optimum at a rate of 1/k. this means that if you
want an answer that   s within an accuracy of   = 10   4, you will need
something on the order of one thousand steps.
the question is whether getting this exact solution is always more
ef   cient. to run id119 for one step will take o(nd) time,
with a relatively small constant. you will have to run k iterations,
yielding an overall runtime of o(knd). on the other hand, the
closed form solution requires constructing x(cid:62)x, which takes o(d2n)
time. the inversion take o(d3) time using standard matrix inver-
sion routines. the    nal multiplications take o(nd) time. thus, the
overall runtime is on the order o(d3 + d2n). in most standard cases
(though this is becoming less true over time), n > d, so this is domi-
nated by o(d2n).

thus, the overall question is whether you will need to run more
than d-many iterations of id119. if so, then the matrix
inversion will be (roughly) faster. otherwise, id119 will
be (roughly) faster. for low- and medium-dimensional problems (say,

100 a course in machine learning

d     100), it is probably faster to do the closed form solution via
matrix inversion. for high dimensional problems (d     10, 000), it is
probably faster to do id119. for things in the middle, it   s
hard to say for sure.

7.7 support vector machines

at the beginning of this chapter, you may have looked at the convex
surrogate id168s and asked yourself: where did these come
from?! they are all derived from different underlying principles,
which essentially correspond to different inductive biases.

let   s start by thinking back to the original goal of linear classi   ers:

to    nd a hyperplane that separates the positive training examples
from the negative ones. figure 7.10 shows some data and three po-
tential hyperplanes: red, green and blue. which one do you like best?

most likely you chose the green hyperplane. and most likely you
chose it because it was furthest away from the closest training points.
in other words, it had a large margin. the desire for hyperplanes
with large margins is a perfect example of an inductive bias. the data
does not tell us which of the three hyperplanes is best: we have to
choose one using some other source of information.

following this line of thinking leads us to the support vector ma-

chine (id166). this is simply a way of setting up an optimization
problem that attempts to    nd a separating hyperplane with as large
a margin as possible. it is written as a constrained optimization
problem:

figure 7.10: picture of data points with
three hyperplanes, rgb with g the best

min
w,b

1

  (w, b)

subj. to yn (w    xn + b)     1

(7.37)

(   n)

in this optimization, you are trying to    nd parameters that maximize
the margin, denoted   , (i.e., minimize the reciprocal of the margin)
subject to the constraint that all training examples are correctly classi-
   ed.

the    odd    thing about this optimization problem is that we re-

quire the classi   cation of each point to be greater than one rather than
simply greater than zero. however, the problem doesn   t fundamen-
tally change if you replace the    1    with any other positive constant
(see exercise ??). as shown in figure 7.11, the constant one can be
interpreted visually as ensuring that there is a non-trivial margin
between the positive points and negative points.

the dif   culty with the optimization problem in eq (7.37) is what
happens with data that is not linearly separable. in that case, there
is no set of parameters w, b that can simultaneously satisfy all the

figure 7.11: hyperplane with margins
on sides

linear models

101

figure 7.12: one bad point with slack

constraints. in optimization terms, you would say that the feasible
region is empty. (the feasible region is simply the set of all parame-
ters that satify the constraints.) for this reason, this is refered to as
the hard-margin id166, because enforcing the margin is a hard con-
straint. the question is: how to modify this optimization problem so
that it can handle inseparable data.

the key idea is the use of slack parameters. the intuition behind
slack parameters is the following. suppose we    nd a set of param-
eters w, b that do a really good job on 9999 data points. the points
are perfectly classifed and you achieve a large margin. but there   s
one pesky data point left that cannot be put on the proper side of the
margin: perhaps it is noisy. (see figure 7.12.) you want to be able
to pretend that you can    move    that point across the hyperplane on
to the proper side. you will have to pay a little bit to do so, but as
long as you aren   t moving a lot of points around, it should be a good
idea to do this. in this picture, the amount that you move the point is
denoted    (xi).

by introducing one slack parameter for each training example,
and penalizing yourself for having to use slack, you can create an
objective function like the following, soft-margin id166:

1

min
w,b,  

  (w, b)

+ c    
n

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)
subj. to yn (w    xn + b)     1       n

large margin

small slack

  n

  n     0

(7.38)

(   n)
(   n)

the goal of this objective function is to ensure that all points are
correctly classi   ed (the    rst constraint). but if a point n cannot be
correctly classi   ed, then you can set the slack   n to something greater
than zero to    move    it in the correct direction. however, for all non-
zero slacks, you have to pay in the objective function proportional to
the amount of slack. the hyperparameter c > 0 controls over   tting
versus under   tting. the second constraint simply says that you must
not have negative slack.

one major advantage of the soft-margin id166 over the original
hard-margin id166 is that the feasible region is never empty. that is,
there is always going to be some solution, regardless of whether your
training data is linearly separable or not.

it   s one thing to write down an optimization problem. it   s another

thing to try to solve it. there are a very large number of ways to
optimize id166s, essentially because they are such a popular learning
model. here, we will talk just about one, very simple way. more
complex methods will be discussed later in this book once you have a
bit more background.

?

what values of c will lead to over-
   tting? what values will lead to
under   tting?

?

suppose i give you a data set.
without even looking at the data,
construct for me a feasible solution
to the soft-margin id166. what is
the value of the objective for this
solution?

102 a course in machine learning

by this observation, there is some positive example that that lies

to make progress, you need to be able to measure the size of the
margin. suppose someone gives you parameters w, b that optimize
the hard-margin id166. we wish to measure the size of the margin.
the    rst observation is that the hyperplane will lie exactly halfway
between the nearest positive point and nearest negative point. if not,
the margin could be made bigger by simply sliding it one way or the
other by adjusting the bias b.
exactly 1 unit from the hyperplane. call it x+, so that w    x+ + b = 1.
similarly, there is some negative example, x   , that lies exactly on
the other side of the margin: for which w    x    + b =    1. these two
points, x+ and x    give us a way to measure the size of the margin.
as shown in figure 7.11, we can measure the size of the margin by
looking at the difference between the lengths of projections of x+
and x    onto the hyperplane. since projection requires a normalized
vector, we can measure the distances as:

figure 7.13: copy of    gure from p5 of
cs544 id166 tutorial

we can then compute the margin by algebra:

1

||w|| w    x+ + b     1
||w|| w    x        b + 1
(cid:2)d+     d   (cid:3)
(cid:20) 1
||w|| w    x   (cid:21)
(cid:20) 1
||w|| w    x        b + 1
||w|| w    x+ + b     1     1
(cid:20) 1
(cid:21)
||w|| w    x+     1
||w|| (+1)     1

||w|| (   1)

d+ =
d    =     1

   =

=

=

=

=

1
2
1
2
1
2
1
2
1
||w||

(cid:21)

(7.39)

(7.40)

(7.41)

(7.42)

(7.43)

(7.44)

(7.45)

this is a remarkable conclusion: the size of the margin is inversely
proportional to the norm of the weight vector. thus, maximizing the
margin is equivalent to minimizing ||w||! this serves as an addi-
tional justi   cation of the 2-norm regularizer: having small weights
means having large margins!

however, our goal wasn   t to justify the regularizer: it was to un-
derstand hinge loss. so let us go back to the soft-margin id166 and
plug in our new knowledge about margins:

min
w,b,  

||w||2

(cid:124) (cid:123)(cid:122) (cid:125)

1
2
large margin

+ c    
n

(cid:124) (cid:123)(cid:122) (cid:125)

  n

small slack

(7.46)

linear models

103

subj. to yn (w    xn + b)     1       n

  n     0

(   n)
(   n)

now, let   s play a thought experiment. suppose someone handed
you a solution to this optimization problem that consisted of weights
(w) and a bias (b), but they forgot to give you the slacks. could you
recover the slacks from the information you have?

in fact, the answer is yes! for simplicity, let   s consider positive

examples. suppose that you look at some positive example xn. you
need to    gure out what the slack,   n, would have been. there are two
cases. either w    xn + b is at least 1 or it is not. if it   s large enough,
then you want to set   n = 0. why? it cannot be less than zero by the
second constraint. moreover, if you set it greater than zero, you will
   pay    unnecessarily in the objective. so in this case,   n = 0. next,
suppose that w    xn + b = 0.2, so it is not big enough. in order to
satisfy the    rst constraint, you   ll need to set   n     0.8. but because
of the objective, you   ll not want to set it any larger than necessary, so
you   ll set   n = 0.8 exactly.

following this argument through for both positive and negative

(cid:40)

points, if someone gives you solutions for w, b, you can automatically
compute the optimal    variables as:

  n =

0
1     yn(w    xn + b) otherwise

if yn(w    xn + b)     1

(7.47)

in other words, the optimal value for a slack variable is exactly the
hinge loss on the corresponding example! thus, we can write the
id166 objective as an unconstrained optimization problem:

min
w,b

||w||2

(cid:124) (cid:123)(cid:122) (cid:125)

1
2
large margin

+ c    
n

(cid:124)

(cid:125)
(cid:96)(hin)(yn, w    xn + b)

(cid:123)(cid:122)

small slack

(7.48)

multiplying this objective through by   /c, we obtain exactly the reg-
ularized objective from eq (7.8) with hinge loss as the id168
and the 2-norm as the regularizer!

7.8 further reading

todo further reading

8 | bias and fairness

science and everyday life cannot
and should not be separated.

    rosalind franklin

learning objectives:
   

at the end of chapter 1, you saw the    russian tank    example of
a biased data set leading to a classi   er that seemed like it was doing
well, but was really relying on some artifact of the data collection
process. as machine learning algorithms have a greater and greater
impact on the real world, it is crucially important to ensure that they
are making decisions based on the    right    aspects of the input, rather
than exploiting arbitrary idiosyncracies of a particular training set.
for the rest of this chapter, we will consider two real world ex-
amples of bias issues that have had signi   cant impact: the effect of
gender in id103 systems and the effect of race in pre-
dicting criminal recidivism (i.e., will a convicted criminal commit
further crimes if released).1 the gender issue is that early speech
recognition systems in cars failed to recognized the voices of many
people who were not men. the race issue is that a speci   c recidivism
predictor based on standard learning algorithms was biased against
minorities.

8.1 train/test mismatch

one of the most common issues in bias is a mismatch between the
training distribution and the testing distribution. in the running
example of id103 failing to work on many non-men
speakers, a large part of this happened because most of the training
data on which the id103 system was trained was spoken
by men. the learning algorithm learned   very well   how to recog-
nize men   s speech, but its accuracy dropped signi   cantly when faced
with a different distribution of test data.

to understand why this happens, recall the bayes optimal clas-
si   er from chapter 2. this was the classi   er than magically know
the data distribution d, and then when presented with an example x
predicted argmaxy d(x, y). this was optimal, but only because d was
the correct distribution. even if one has access to the true distribu-
tion for male speakers, say d(male), the bayes optimal classi   er under

dependencies: chapter 1,chap-
ter 3,chapter 4,chapter 5
1 see autoblog and propublica for press
coverage of these two issues.

bias and fairness

105

d(male) will generally not be optimal under the distribution for any
other gender.

another example occurs in id31. it is common to

train id31 systems on data collected from reviews:
product reviews, restaurant reviews, movie reviews, etc. this data is
convenient because it includes both text (the review itself) and also
a rating (typically one to    ve stars). this yields a    free    dataset for
training a model to predict rating given text. however, one should
be wary when running such a sentiment classi   er on text other than
the types of reviews it was trained on. one can easily imagine that
a sentiment analyzer trained on movie reviews may not work so
well when trying to detect sentiment in a politician   s speech. even
moving between one type of product and another can fail wildly:
   very small    typically expresses positive sentiment for usb drives,
but not for hotel rooms.

the issue of train/test mismatch has been widely studied under

many different names: covariate shift (in statistics, the input features
are called    covariates   ), sample selection bias and domain adapta-
tion are the most common. we will refer to this problem simply as
adaptation. the adaptation challenge is: given training data from one
   old    distribution, learn a classi   er that does a good job on another
related, but different,    new    distribution.

it   s important to recognize that in general, adaptation is im-
possible. for example, even if the task remains the same (posi-
tive/negative sentiment), if the old distribution is text reviews and
the new distribution is images of text reviews, it   s hard to imagine
that doing well on the old distribution says anything about the new.
as a less extreme example, if the old distribution is movie reviews in
english and the new distribution is movie reviews in mandarin, it   s
unlikely that adaptation will be easy.

these examples give rise to the tension in adaptation problems.

1. what does it mean for two distributions to be related? we might

believe that    reviews of dvds    and    reviews of movies    are
   highly related    (though somewhat different: dvd reviews of-
ten discuss bonus features, quality, etc.); while    reviews of hotels   
and    reviews of political policies    are    less related.    but how can
we formalize this?

2. when two distributions are related, how can we build models that

effectively share information between them?

106 a course in machine learning

8.2 unsupervised adaptation

the    rst type of adaptation we will cover is unsupervised adapta-
tion. the setting is the following. there are two distributions, dold
and dnew. we have labeled training data from dold, say (x1, y1), . . . , (xn, yn)
totalling n examples. we also have m many unlabeled examples from
dnew: z1, . . . , zm. we assume that the examples live in the same
space, rd. this is called unsupervised adaptation because we do not
have access to any labels in the new distribution.2
our goal is to learn a classi   er f that achieves low expected loss
under the new distribution, dnew. the challenge is that we do not have
access to any labeled data from dnew. as a warm-up, let   s suppose
that we have a black box machine learning algorithm a that takes
in weighted examples and produces a classi   er. at the very least,
this can be achieved using either undersampling or oversampling
(see section 6.1). we   re going to attempt to reweigh the (old distri-
bution) labeled examples based on how similar they are to the new
distribution. this is justi   ed using the importance sampling trick for
switching expectations:

2 sometimes this is called semi-super-
vised adaptation in the literature.

test loss
= e
=    
(x,y)
=    
(x,y)
=    
(x,y)

(x,y)   dnew [(cid:96)(y, f (x))]
dnew(x, y)(cid:96)(y, f (x))
dold(x, y)
dold(x, y)
dnew(x, y)
dold(x, y)

dnew(x, y)

dold(x, y)

(cid:20)dnew(x, y)

= e

(x,y)   dold

dold(x, y)

de   nition

expand expectation

(8.1)
(8.2)
(8.3)

(cid:96)(y, f (x))

times one

(8.4)

(cid:96)(y, f (x))

(cid:21)

(cid:96)(y, f (x))

rearrange

(8.5)

de   nition

(8.6)

what we have achieved here is rewriting the test loss, which is an

expectation over dnew, as an expectation over dold instead.3 this
is useful because we have access to labeled examples from dold
but not dnew. the implicit suggested algorithm by this analysis
to to train a classi   er using our learning algorithm a, but where
each training example (xn, yn) is weighted according to the ratio
dnew(xn, yn)/dold(xn, yn). intuitively, this makes sense: the classi   er
is being told to pay more attention to training examples that have
high id203 under the new distribution, and less attention to
training that have low id203 under the new distribution.
the problem with this approach is that we do not have access to
dnew or dold, so we cannot compute this ratio and therefore cannot
run this algorithm. one approach to this problem is to try to explic-

3 in this example, we assumed a discrete
distribution; if the distributions are con-
tinuous, the sums are simply replaced
with integrals.

bias and fairness

107

itly estimate these distributions, a task known as density estimation.
this is an incredibly dif   cult problem; far harder than the original
adaptation problem.

a solution to this problem is to try to estimate the ratio directly,
rather than separately estimating the two id203 distributions4.
the key idea is to think of the adaptation as follows. all examples
are drawn according to some    xed base distribution dbase. some
of these are selected to go into the new distribution, and some of
them are selected to go into the old distribution. the mechanism for
deciding which ones are kept and which are thrown out is governed
by a selection variable, which we call s. the choice of selection-or-
not, s, is based only on the input example x and not on it   s label.
particular, we de   ne:

in

4 bickel et al. 2007

?

what could go wrong if s got to
look at the label, too?

dold(x, y)     dbase(x, y)p(s = 1 | x)
(8.7)
dnew(x, y)     dbase(x, y)p(s = 0 | x)
(8.8)
that is, the id203 of drawing some pair (x, y) in the old distri-
bution is proportional to the id203 of    rst drawing that example
according to the base distribution, and then the id203 of se-
lecting that particular example into the old distribution. if we can
successfully estimate p(s = 1 | x), then the ratio that we sought, then
we can compute the importance ratio as:

dnew(x, y)
dold(x, y)

=

=

1

1

1

1

znew dbase(x, y)p(s = 0 | x)
zold dbase(x, y)p(s = 1 | x)
znew p(s = 0 | x)
zold p(s = 1 | x)
p(s = 0 | x)
p(s = 1 | x)
1     p(s = 1 | x)
(cid:20)
p(s = 1 | x)
p(s = 1 | x)

    1

(cid:21)

1

= z

= z

= z

de   nition

(8.9)

cancel base (8.10)

consolidate (8.11)

binary selection (8.12)

rearrange (8.13)

this means that if we can estimate the selection id203 p(s =
1 | x), we   re done. we can therefore use 1/p(s = 1 | xn)     1 as an
example weight on example (xn, yn) when feeding these examples
into our learning algorithm a.
the remaining question is how to estimate p(s = 1 | xn). recall
that s = 1 denotes the case that x is selected into the old distribution
and s = 0 denotes the case that x is selected into the new distribution.
this means that predicting s is exactly a binary classi   cation problem,
where the    positive    class is the set of n examples from the old
distribution and the    negative    class is the set of m examples from
the new distribution.

?

as a check: make sure that these
weights are always non-negative.
furthermore, why is it okay to
ignore the z factor?

108 a course in machine learning

algorithm 23 selectionadaptation((cid:104)(xn, yn)(cid:105)n
1: ddist     (cid:104)(xn, +1)(cid:105)n
n=1

(cid:83) (cid:104)(zm,   1)(cid:105)m
m=1
(cid:69)n
  p     train id28 on ddist

3: dweighted    (cid:68)

2:

1

  p(xn)     1)

n=1

(xn, yn,
4: return a(dweighted)

n=1, (cid:104)zm(cid:105)m

m=1, a)

// assemble data for distinguishing
// between old and new distributions

// assemble weight classi   cation
// data using selector
// train classi   er

this analysis gives rise to algorithm 8.2, which consists of essen-
tially two steps. the    rst is to train a id28 classi   er5 to
distinguish between old and new distributions. the second is to use
that classi   er to produce weights on the labeled examples from the
old distribution and then train whatever learning algorithm you wish
on that.

in terms of the questions posed at the beginning of this chapter,
this approach to adaptation measures nearness of the two distribu-
tions by the degree to which the selection id203 is constant. in
particular, if the selection id203 is independent of x, then the
two distributions are identical. if the selection probabilities vary sigi-
   cantly as x changes, then the two distributions are considered very
different. more generally, if it is easy to train a classi   er to distin-
guish between the old and new distributions, then they are very
different.

in the case of id103 failing as a function of gender, a
core issue is that speech from men was massively over-represented
in the training data but not the test data. when the selection logistic
regression is trained, it is likely to say    old    on speech from men and
   new    on other speakers, thereby downweighting the signi   cance of
male speakers and upweighting the signi   cance of speakers of other
genders on the    nal learned model. this would (hopefully) address
many of the issues confounding that system.

8.3 supervised adaptation

unsupervised adaptation is very challenging because we never get to
see try labels in the new distribution. in many ways, unsupervised
adaptation attempts to guard against bad things happening. that is,
if an old distribution training example looks very unlike the new
distribution, it (and consequently it   s features) are downweighted so
much as to be ignored. in supervised adaptation, we can hope for
more: we can hope to actually do better on the new distribution than
the old because we have labeled data there.

the typical setup is similar to the unsupervised case. there are

5 the use of id28 is arbi-
trary: it need only be a classi   cation
algorithm that can produce probabili-
ties.

?

make up percentages for fraction
of speakers who are male in the
old and new distributions; estimate
(you   ll have to make some assump-
tions) what the importance weights
would look like in this case.

bias and fairness

109

6 daum   iii 2007

(cid:69)n
algorithm 24 easyadapt((cid:104)(x(old)
n

1: d    (cid:68)

, 0(cid:105), y(old)

, x(old)

)

n

n

, y(old)

n

(cid:83)(cid:68)

)(cid:105)n
n=1, (cid:104)(x(new)
((cid:104)x(new)
m , 0, x(new)

n=1

((cid:104)x(old)
n
2: return a(d)

(cid:69)m
m )(cid:105)m
m=1, a)
m , y(new)
m (cid:105), y(new)
// union
m )
// of transformed data
// train classi   er

m=1

two distributions, dold and dnew, and our goal is to learn a classi-
   er that does well in expectation on dnew. however, now we have
labeled data from both distributions: n labeled examples from dold
(cid:105)n
and m labeled examples from dnew. call them (cid:104)x(old)
n=1 from
n
dold and (cid:104)x(new)
m=1 from dnew. again, suppose that both x(old)
n
and x(new)

m (cid:105)m
m , y(new)
both live in rd.

, y(old)

n

m

one way of answer the question of    how do we share informa-

tion between the two distributions    is to say: when the distributions
agree on the value of a feature, let them share it, but when they dis-
agree, allow them to learn separately. for instance, in a sentiment
analysis task, dold might be reviews of electronics and dnew might
be reviews of hotel rooms. in both cases, if the review contains the
word    awesome    then it   s probably a positive review, regardless of
which distribution we   re in. we would want to share this information
across distributions. on the other hand,    small    might be positive
in electronics and negative in hotels, and we would like the learning
algorithm to be able to learn separate information for that feature.

a very straightforward way to accomplish this is the feature aug-
mentation approach6. this is a simple preprocessing step after which
one can apply any learning algorithm. the idea is to create three ver-
sions of every feature: one that   s shared (for words like    awesome   ),
one that   s old-distribution-speci   c and one that   s new-distribution-
speci   c. the mapping is:

(cid:55)   (cid:68)
(cid:55)   (cid:68)

x(old)
n

x(new)
m

new-only

(cid:123)(cid:122)

(cid:125)

d-many

, 0, 0, . . . , 0

(cid:124)

(cid:69)
(cid:69)

shared
x(old)
n

old-only
, x(old)

n

x(new)
m

, 0, 0, . . . , 0

, x(new)

m

(cid:124)

(cid:123)(cid:122)

d-many

(cid:125)

(8.14)

(8.15)

once you   ve applied this transformation, you can take the union
of the (transformed) old and new labeled examples and feed the en-
tire set into your favorite classi   cation algorithm. that classi   cation
algorithm can then choose to share strength between the two distri-
butions by using the    shared    features, if possible; or, if not, it can
learn distribution-speci   c properties on the old-only or new-only
parts. this is summarized in algorithm 8.3.

note that this algorithm can be combined with the instance weight-

?

why is it crucial that the separator
be trained on the untransformed
data?

110 a course in machine learning

ing (unsupervised) learning algorithm. in this case, the logistic re-
gression separator should be trained on the untransformed data, and
then weights can be used exactly as in algorithm 8.2. this is particu-
larly useful if you have access to way more old distribution data than
new distribution data, and you don   t want the old distribution data
to    wash out    the new distribution data.

although this approach is general, it is most effective when the

two distributions are    not too close but not too far   :

    if the distributions are too far, and there   s little information to

share, you   re probably better off throwing out the old distribution
data and training just on the (untransformed) new distribution
data.

    if the distributions are too close, then you might as well just take
the union of the (untransformed) old and new distribution data,
and training on that.

in general, the interplay between how far the distributions are and
how much new distribution data you have is complex, and you
should always try    old only    and    new only    and    simple union   
as baselines.

8.4 fairness and data bias

almost any data set in existence is biased in some way, in the sense
that it captures an imperfect view of the world. the degree to which
this bias is obvious can vary greatly:

    there might be obvious bias in the labeling. for instance, in crimi-
nology, learning to predict sentence lengths by predicting the sen-
tences assigned by judges will simply learn to reproduce whatever
bias already exists in judicial sentencing.

    there might be sample selection bias, as discussed early. in the
same criminology example, the only people for which we have
training data are those that have already been arrested, charged
with and convicted of a crime; these processes are inherantly bi-
ased, and so any model learned on this data may exhibit similar
biases.

    the task itself might be biased because the designers had blindspots.

an intelligent billboard that predicts the gender of the person
walking toward it so as to show    better    advertisements may be
trained as a binary classi   er between male/female and thereby
excludes anyone who does not fall in the gender binary. a similar
example holds for a classi   er that predicts political af   liation in

bias and fairness

111

7 for instance, many languages have
verbal marking that agrees with the
gender of the subject. in such cases,
   the doctor treats . . .     puts masculine
markers on the translation of    treats   
but    the nurse treats . . .     uses feminine
markers.

8 acm code of ethics

the us as democrat/republican, when in fact there are far more
possibilities.

    there might be bias in the features or model structure. machine

translation systems often exhibit incorrect gender stereotypes7
because the relevant context, which would tell them the correct
gender, is not encoded in the feature representation. alone, or
coupled with biased data, this leads to errors.

    the id168 may favor certain types of errors over others.
you   ve already seen such an example: using zero/one loss on
a highly imbalanced class problem will often lead to a learned
model that ignores the minority class entirely.

    a deployed system creates feedback loops when it begins to con-

sume it   s own output as new input. for instance, once a spam
   lter is in place, spammers will adjust their strategies, leading to
distribution shift in the inputs. or a car guidance system for pre-
dicting which roads are likely to be unoccupied may    nd those
roads now occupied by other cars that it directed there.

these are all dif   cult questions, none of which has easy answers.
nonetheless, it   s important to be aware of how our systems may
fail, especially as they take over more and more of our lives. most
computing, engineering and mathematical societies (like the acm,
ieee, bcs, etc.) have codes of ethics, all of which include a statement
about avoiding harm; the following is taken from the acm code8:

to minimize the possibility of indirectly harming others, computing
professionals must minimize malfunctions by following generally
accepted standards for system design and testing. furthermore, it is
often necessary to assess the social consequences of systems to project
the likelihood of any serious harm to others.

in addition to ethical questions, there are often related legal ques-
tions. for example, us law prohibits discrimination by race and gen-
der (among other    protected attributes   ) in processes like hiring and
housing. the current legal mechanism for measuring discimination is
disparate impact and the 80% rule. informally, the 80% rule says that
your rate of hiring women (for instance) must be at least 80% of your
rate of hiring men. formally, the rule states:

pr(y = +1 | g (cid:54)= male)     0.8   pr(y = +1 | g = male)

(8.16)

of course, gender/male can be replaced with any other protected
attribute.

one non-solution to the disparate impact problem is to simply

throw out protected attributes from your dataset. importantly, yet un-
fortunately, this is not suf   cient. many other features often correlate

112 a course in machine learning

strongly with gender, race, and other demographic information, and
just because you   ve removed explicit encodings of these factors does
not imply that a model trained as such would satisfy the 80% rule. a
natural question to ask is: can we build machine learning algorithms
that have high accuracy but simultaneously avoid disparate impact?

disparate impact is an imperfect measure of (un)fairness, and

there are alternatives, each with it   s own pros and cons9. all of these
rely on prede   ned categories of protected attributes, and a natural
question is where these come from if not governmental regulation.
regardless of the measure you choose, the most important thing to
keep in mind is that just because something comes from data, or is
algorithmically implemented, does not mean it   s fair.

8.5 how badly can it go?

9 friedler et al. 2016, hardt et al. 2016

to help better understand how badly things can go when the distri-
bution over inputs changes, it   s worth thinking about how to analyze
this situation formally. suppose we have two distributions dold and
dnew, and let   s assume that the only thing that   s different about these
is the distribution they put on the inputs x and not the outputs y.
(we will return later to the usefulness of this assumption.) that is:
dold(x, y) = dold(x)d(y | x) and dnew(x, y) = dnew(x)d(y | x),
where d(y | x) is shared between them.
dold and achieves some test error of  (old). that is:

let   s say that you   ve learned a classi   er f that does really well on

(cid:2)1[ f (x) (cid:54)= y](cid:3)

 (old) = e

x   dold
y   d(   | x)

(8.17)

the question is: how badly can f do on the new distribution?

we can calculate this directly.

(cid:2)1[ f (x) (cid:54)= y](cid:3)

 (new)
= e x   dnew
y   d(   | x)

(cid:90)
(cid:90)

(cid:90)
(cid:16)dnew(x)     dold(x) + dold(x)

dnew(x)d(y | x)1[ f (x) (cid:54)= y]dydx

(cid:17)  

y

=

x

y

x

=

(cid:90)
=  (old)+(cid:90)
(cid:90)

x

y

d(y | x)1[ f (x) (cid:54)= y]dydx

(cid:16)dnew(x)     dold(x)

(cid:17)  

d(y | x)1[ f (x) (cid:54)= y]dydx

(8.18)

def. of e (8.19)

add zero (8.20)

def.  old

(8.21)

(cid:90)

     (old) +

=  (old) + 2

(cid:12)(cid:12)(cid:12)dnew(x)     dold(x)
(cid:12)(cid:12)(cid:12)dnew     dold(cid:12)(cid:12)(cid:12)var

x

(cid:17)

(cid:12)(cid:12)(cid:12)(cid:16)

1

dx

worst case

(cid:90)
def. |  |var

y (8.22)
(8.23)

bias and fairness

113

(cid:90)

here, |  |var is the total variation distance (or variational distance)
between two id203 distributions, de   ned as:

|p     q|var = sup

e

|p(e)     q(e)| =

1
2

|p(x)     q(x)| dx

(8.24)

x

is a standard measure of dissimilarity between id203 distribu-
tions. in particular, the variational distance is the largest difference
in id203 that p and q assign to the same event (in this case, the
event is an example x).

the bound tells us that we might not be able to hope for very

good error on the new distribution, even if we have very good error
on the old distribution, when dold and dnew are very different (as-
sign very different probabilities to the same event). of course, this is
an upper bound, and possibly a very loose one.
the second observation is that we barely used the assumption that
dnew and dold share the same label distribution d(y | x) in the above
analysis. (in fact, as an exercise, repeat the analysis without this as-
sumption. it will still go through.) in general, this assumption buys
us very little. in an extreme case, dnew and dold can essentially    en-
code    which distribution a given x came from in one of the features.
once the origin distribution is completely encoded in a feature, then
d(y | x) could look at the encoding, and completely    ip the label
based on which distribution it   s from. how could dnew and dold
encode the distribution? one way would be to set the 29th decimal
digit of feature 1 to an odd value in the old distribution and an even
value in the new distribution. this tiny change will be essentially im-
perceptible if one looks at the data, but would give d(y | x) enough
power to make our lives miserable.

if we want a way out of this dilemma, we need more technology.
the core idea is that if we   re learning a function f from some hypoth-
esis class f, and this hypothesis class isn   t rich enough to peek at the
29th decimal digit of feature 1, then perhaps things are not as bad
as they could be. this motivates the idea of looking at a measure of
distance between id203 distributions that depends on the hypoth-
esis class. a popular measure is the da-distance or the discrepancy.
the discrepancy measure distances between id203 distributions
based on how much two function f and f (cid:48) in the hypothesis class can
disagree on their labels. let:

(cid:104)

(cid:105)

 p( f , f (cid:48)) = ex   p

1[ f (x) (cid:54)= f (cid:48)(x)]

(8.25)

114 a course in machine learning

you can think of  p( f , f (cid:48)) as the error of f (cid:48) when the ground truth is
given by f , where the error is taken with repsect to examples drawn
from p. given a hypothesis class f, the discrepancy between p and
q is de   ned as:

(cid:12)(cid:12) p( f , f (cid:48))      q( f , f (cid:48))(cid:12)(cid:12)

da(p, q) = max
f , f (cid:48)   f

(8.26)

the discrepancy very much has the    avor of a classi   er: if you think
of f as providing the ground truth labeling, then the discrepancy is
large whenever there exists a function f (cid:48) that agrees strongly with f
on distribution p but disagrees strongly with f on q. this feels natu-
ral: if all functions behave similarly on p and q, then the discrepancy
will be small, and we also expect to be able to generalize across these
two distributions easily.
one very attractive property of the discrepancy is that you can
estimate it from    nite unlabeled samples from dold and dnew. al-
though not obvious at    rst, the discrepancy is very closely related
to a quantity we saw earlier in unsupervised adaptation: a classi   er
that distinguishes between dold and dnew. in fact, the discrepancy is
precisely twice the accuracy of the best classi   er from h at separating
dold from dnew.

how does this work in practice? exactly as in the section on un-
supervised adaptation, we train a classi   er to distinguish between
dold and dnew. it needn   t be a probabilistic classi   er; any binary
classi   er is    ne. this classi   er, the    domain separator,    will have
some (heldout) accuracy, call it acc. the discrepancy is then exactly
da = 2(acc     0.5).

intuitively the accuracy of the domain separator is a natural mea-

one can, in fact, prove a generalization bound   generalizing from

sure of how different the two distributions are. if the two distribu-
tions are identical, you shouldn   t expect to get very good accuracy at
separating them. in particular, you expect the accuracy to be around
0.5, which puts the discrepancy at zero. on the other hand, if the two
distributions are really far apart, separation is easy, and you expect
an accuracy of about 1, yielding a discrepancy also of about 1.
   nite samples from dold to expected loss on dnew   based on the
discrepancy10.
theorem 9 (unsupervised adaptation bound). given a    xed rep-
resentation and a    xed hypothesis space f, let f     f and let  (best) =
min f       f 1
(cid:124)
2
 (new)( f )
error on dnew
in this bound,  (best) denotes the error rate of the best possible
classi   er from f, where the error rate is measured as the average

(cid:2) (old)( f    ) +  (new)( f    )(cid:3), then, for all f     f:
(cid:123)(cid:122)
(cid:125)

(cid:125)
(cid:124)
+ da(dold,dnew)

(cid:124) (cid:123)(cid:122) (cid:125)
     (old)( f )
error on dold

(cid:124)(cid:123)(cid:122)(cid:125)

minimal avg error

(cid:123)(cid:122)

 (best)

distance

+

(8.27)

10 ben-david et al. 2007

bias and fairness

115

error rate on dnew and dold; this term ensures that at least some
good classi   er exists that does well on both distributions.

the main practical use of this result is that it suggests a way to
look for representations that are good for adaptation: on the one
hand, we should try to get our training error (on dold) as low as
possible; on the other hand, we want to make sure that it is hard to
distinguish between dold and dnew in this representation.

8.6 further reading

todo further reading

9 | probabilistic modeling

learning objectives:
    de   ne the generative story for a

naive bayes classi   er.

    derive relative frequency as the so-
lution to a constrained optimization
problem.

    compare and contrast generative,

conditional and discriminative
learning.

    explain when generative models are

likely to fail.

    derive logistic loss with an (cid:96)2

regularizer from a probabilistic
perspective.

dependencies:

the world is noisy and messy. you need to deal with the noise
and uncertainty.

    daphne koller

many of the models and algorithms you have learned about
thus far are relatively disconnected. there is an alternative view of
machine learning that unites and generalizes much of what you have
already learned. this is the probabilistic modeling framework, in
which you will explicitly think of learning as a problem of statistical
id136.

in this chapter, you will learn about two    avors of probabilistic

models: generative and conditional. you will see that many of the ap-
proaches (both supervised and unsupervised) we have seen already
can be cast as probabilistic models. through this new view, you will
be able to develop learning algorithms that have inductive biases
closer to what you, as a designer, believe. moreover, the two chap-
ters that follow will make heavy use of the probabilistic modeling
approach to open doors to other learning problems.

9.1 classi   cation by density estimation

recall from chapter 2 that if we had access to the underlying prob-
ability distribution d, then we could form a bayes optimal classi   er
as:

f (bo)(   x) = arg max

  y   y d(   x,   y)

(9.1)

unfortunately, no one gave you this distribution, but the optimality
of this approach suggests that good way to build a classi   er is to
try to estimate d. in other words, you try to learn a distribution   d,
which you hope to very similar to d, and then use this distribution
for classi   cation. just as in the preceding chapters, you can try to
form your estimate of d based on a    nite training set.

the most direct way that you can attempt to construct such a

id203 distribution is to select a family of parametric distribu-
tions. for instance, a gaussian (or normal) distribution is parametric:
it   s parameters are its mean and covariance. the job of learning is

probabilistic modeling 117

math review | rules of id203
a id203 distribution p speci   es the likelihood of an event e, where p(e)     [0, 1]. it   s often con-
venient to think of events as    con   gurations of the world   , so p(e) says    how likely is it that the
world is in con   guration e.    often world con   gurations are built up of smaller pieces, for instance you
might say    e = the con   guration in which it is rainy, windy and cold.    formally, we might write this as
   e = {weather = rainy, wind = windy, temperature = cold}   , where we   ve used a convention that
random variables (like temperature) are capitalized and their instantiations (like cold) are lower case.
considering this event, we want to evaluate p(weather = rainy, wind = windy, temperature = cold),
or more generally p(a = a, b = b, c = c) for some random variables a, b and c, and some instantia-
tions of those random variables a, b and c respectively.

there are a few standard rules of id203 that we will use regularly:

sum-to-one: if you sum over all possible con   gurations of the world, p sums to one:    e p(e = e) = 1.
marginalization: you can sum out one random variable to remove it from the world:    a p(a = a, b =
b) = p(b = b).
chain rule: if a world con   guration consists of two or more random variables, you can evaluate the
likelihood of the world one step at a time: p(a = a, b = b) = p(a = a)p(b = b | a = a). events are
unordered, so you can also get p(a = a, b = b) = p(b = b)p(a = a | b = b).
bayes rule: combining the two chain rule equalities and dividing, we can relate a conditional proba-
bility in one direction with that in the other direction: p(a = a | b = b) = p(a = a)p(b = b | a =
a)/p(b = b).

figure 9.1:

then to infer which parameters are    best    as far as the observed train-
ing data is concerned, as well as whatever inductive bias you bring.
a key assumption that you will need to make is that the training data
you have access to is drawn independently from d. in particular, as
you draw examples (x1, y1)     d then (x2, y2)     d and so on, the
nth draw (xn, yn) is drawn from d and does not otherwise depend
on the previous n     1 samples. this assumption is usually false, but
is also usually suf   ciently close to being true to be useful. together
with the assumption that all the training data is drawn from the same
distribution d leads to the i.i.d. assumption or independently and
identically distributed assumption. this is a key assumption in al-
most all of machine learning.

9.2 statistical estimation

suppose you need to model a coin that is possibly biased (you can
think of this as modeling the label in a binary classi   cation problem),
and that you observe data hhth (where h means a    ip came up heads

118 a course in machine learning

and t means it came up tails). you can assume that all the    ips came
from the same coin, and that each    ip was independent (hence, the
data was i.i.d.). further, you may choose to believe that the coin has
a    xed id203    of coming up heads (and hence 1        of coming
up tails). thus, the parameter of your model is simply the scalar   .

the most basic computation you might perform is maximum like-

lihood estimation: namely, select the paramter    the maximizes the
id203 of the data under that parameter. in order to do so, you
need to compute the id203 of the data:

p  (d) = p  (hhth)

= p  (h)p  (h)p  (t)p  (h)
=     (1       )  
=   3(1       )
=   3       4

de   nition of d
data is independent

(9.2)
(9.3)
(9.4)
(9.5)
(9.6)

?

describe a case in which at least
one of the assumptions we are
making about the coin    ip is false.

thus, if you want the parameter    that maximizes the id203 of
the data, you can take the derivative of   3       4 with respect to   , set
it equal to zero and solve for   :

(cid:104)

  3       4(cid:105)

   
     

= 3  2     4  3
4  3 = 3  2

      4   = 3
3
         =
4

(9.7)

(9.8)
(9.9)
(9.10)

thus, the maximum likelihood    is 0.75, which is probably what
you would have selected by intuition. you can solve this problem
more generally as follows. if you have h-many heads and t-many
tails, the id203 of your data sequence is   h(1       )t. you can
try to take the derivative of this with respect to    and follow the
same recipe, but all of the products make things dif   cult. a more
friendly solution is to work with the log likelihood or log proba-
bility instead. the log likelihood of this data sequence is h log    +
t log(1       ). differentiating with respect to   , you get h/       t/(1    
  ). to solve, you obtain h/   = t/(1       ) so h(1       ) = t  .
thus h     h   = t   and so h = (h + t)  ,    nally yeilding that
   = h/(h + t) or, simply, the fraction of observed data that came up
heads. in this case, the maximum likelihood estimate is nothing but
the relative frequency of observing heads!

now, suppose that instead of    ipping a coin, you   re rolling a k-

sided die (for instance, to pick the label for a multiclass classi   cation
problem). you might model this by saying that there are parameters
  1,   2, . . . ,   k specifying, respectively, the probabilities that any given

?

how do you know that the solution
of    = h/(h + t) is actually a
maximum?

probabilistic modeling 119

side comes up on a role. since these are themselves probabilities,
each   k should be at least zero, and the sum of the   ks should be one.
given a data set that consists of x1 rolls of 1, x2 rolls of 2 and so on,
the id203 of this data is    k   
   k xk log   k. if you pick some particular parameter, say   3, the deriva-
tive of this with respect to   3 is x3/  3, which you want to equate to
zero. this leads to. . .   3        .

xk
k , yielding a log id203 of

this is obviously    wrong.    from the mathematical formulation,
xk
k

it   s correct: in fact, setting all of the   ks to     does maximize    k   
for
any (non-negative) xks. the problem is that you need to constrain the
  s to sum to one. in particular, you have a constraint that    k   k = 1
that you forgot to enforce. a convenient way to enforce such con-
straints is through the technique of lagrange multipliers. to make
this problem consistent with standard minimization problems, it is
convenient to minimize negative log probabilities, instead of maxi-
mizing log probabilities. thus, the constrainted optimization problem
is:

       
k

xk log   k
  k     1 = 0

min

  

subj. to    
k

(9.11)

the lagrange multiplier approach involves adding a new variable   
to the problem (called the lagrange variable) corresponding to the
constraint, and to use that to move the constraint into the objective.
the result, in this case, is:

(cid:32)

(cid:33)

max

  

min

  

xk log   k       

       
k

  k     1

   
k

(9.12)

turning a constrained optimization problem into it   s corresponding
lagrangian is straightforward. the mystical aspect is why it works.
in this case, the idea is as follows. think of    as an adversary:    is
trying to maximize this function (you   re trying to minimize it). if
you pick some parameters    that actually satisfy the constraint, then
the green term in eq (9.12) goes to zero, and therefore    does not
matter: the adversary cannot do anything. on the other hand, if the
constraint is even slightly unsatis   ed, then    can tend toward +   
or        to blow up the objective. so, in order to have a non-in   nite
objective value, the optimizer must    nd values of    that satisfy the
constraint.

if we solve the inner optimization of eq (9.12) by differentiating

with respect to   1, we get x1/  1 =   , yielding   1 = x1/  . in general,
the solution is   k = xk/  . remembering that the goal of    is to
enforce the sums-to-one constraint, we can set    =    k xk and verify

120 a course in machine learning

that this is a solution. thus, our optimal   k = xk/    k xk, which again
completely corresponds to intuition.

9.3 naive bayes models

now, consider the binary classi   cation problem. you are looking for
a parameterized id203 distribution that can describe the training
data you have. to be concrete, your task might be to predict whether
a movie review is positive or negative (label) based on what words
(features) appear in that review. thus, the id203 for a single data
point can be written as:

p  ((y, x)) = p  (y, x1, x2, . . . , xd)

(9.13)

the challenge in working with a id203 distribution like eq (9.13)
is that it   s a distribution over a lot of variables. you can try to sim-
plify it by applying the chain rule of probabilities:

p  (x1, x2, . . . , xd, y) = p  (y)p  (x1 | y)p  (x2 | y, x1)p  (x3 | y, x1, x2)
(9.14)
(9.15)

         p  (xd | y, x1, x2, . . . , xd   1)
p  (xd | y, x1, . . . , xd   1)

= p  (y)    

d

at this point, this equality is exact for any id203 distribution.
however, it might be dif   cult to craft a id203 distribution for
the 10000th feature, given the previous 9999. even if you could, it
might be dif   cult to accurately estimate it. at this point, you can
make assumptions. a classic assumption, called the naive bayes as-
sumption, is that the features are independent, conditioned on the label.
in the movie review example, this is saying that once you know that
it   s a positive review, the id203 that the word    excellent    appears
is independent of whether    amazing    also appeared. (note that
this does not imply that these words are independent when you
don   t know the label   they most certainly are not.) formally this
assumption states that:

assumption:

p(xd | y, xd(cid:48) ) = p(xd | y)

,

   d (cid:54)= d(cid:48)

(9.16)

under this assumption, you can simplify eq (9.15) to:

p  ((y, x)) = p  (y)    

d

p  (xd | y)

naive bayes assumption

(9.17)

at this point, you can start parameterizing p. suppose, for now,
that your labels are binary and your features are also binary. in this
case, you could model the label as a biased coin, with id203 of
heads (e.g., positive review) given by   0. then, for each label, you

probabilistic modeling 121

can imagine having one (biased) coin for each feature. so if there are
d-many features, you   ll have 1 + 2d total coins: one for the label
(call it   0) and one for each label/feature combination (call these   +1
and as      1). in the movie review example, we might expect   0     0.4
(forty percent of movie reviews are positive) and also that   +1 might
give high id203 to words like    excellent    and    amazing    and
   good    and      1 might give high id203 to words like    terrible   
and    boring    and    hate   . you can rewrite the id203 of a single
example as follows, eventually leading to the log id203 of the
entire data set:

p  ((y, x)) = p  (y)    

p  (xd | y)

(cid:16)

=

[y=+1]
0

  

d

(1       0)[y=   1](cid:17)    

naive bayes assumption

[xd=1]

(y),d (1       (y),d)[xd=0]

  

(9.18)

(9.19)

model assumptions

d

solving for   0 is identical to solving for the biased coin case from
before: it is just the relative frequency of positive labels in your data
(because   0 doesn   t depend on x at all). for the other parameters,
you can repeat the same exercise as before for each of the 2d coins
independently. this yields:

    0 =

    (+1),d =

    (   1),d =

   
n

[yn = +1]

1
n
   n[yn = +1     xn,d = 1]
   n[yn =    1     xn,d = 1]

   n[yn = +1]
   n[yn =    1]

(9.20)

(9.21)

(9.22)

in the case that the features are not binary, you need to choose a dif-
ferent model for p(xd | y). the model we chose here is the bernouilli
distribution, which is effectively a distribution over independent
coin    ips. for other types of data, other distributions become more
appropriate. the die example from before corresponds to a discrete
distribution. if the data is continuous, you might choose to use a
gaussian distribution (aka normal distribution). the choice of dis-
tribution is a form of inductive bias by which you can inject your
knowledge of the problem into the learning algorithm.

9.4 prediction

consider the predictions made by the naive bayes model with bernoulli
features in eq (9.18). you can better understand this model by con-
sidering its decision boundary. in the case of probabilistic models,

122 a course in machine learning

math review | common id203 distributions
there are a few common id203 distributions that we use in this book. the    rst is the bernouilli
distribution, which models binary outcomes (like coin    ips). a bernouilli distribution, ber(  ) is pa-
rameterized by a single scalar value        [0, 1] that represents the id203 of heads. the likelihood
function is ber(x |   ) =   x(1       )1   x. the generalization of the bernouilli to more than two possible
outcomes (like rolls of a die) is the discrete distribution, disc(th). if the die has k sides, then        rk
with all entries non-negative and    k   k = 1.   k is the probabability that the die comes up on side k.
the likelihood function is disc(x |   ) =    k   
. the binomial distribution is just like the bernouilli
bin(k | n,  )=n
k  k(1     )n   k ), where n
distribution but for multiple    ips of the rather than a single    ip; it   s likelihood is (
is the number of    ips and k is the number of heads. the multinomial distribution extends the discrete
distribution also to multiple rolls; it   s likelihood is mult(x | n,   ) =
xk
k , where n is the total
number of rolls and xk is the number of times the die came up on side k (so    k xk = n). the preceding
distributions are all discrete.

1[x=k]
k

   k   

   k xk!

n!

there are two common continuous distributions we need. the    rst is the uniform distribution,
b   a 1[x    
uni(a, b) which is uniform over the closed range [a, b]. it   s density function is uni(x | a, b) = 1
[a, b]]. finally, the gaussian distribution is parameterized by a mean    and variance   2 and has density
nor(x |   ,   2) = (2    2)    1

(cid:104)    1
2  2 (x       )2(cid:105)

2 exp

.

(cid:34)

  0    
(cid:34)

d

the decision boundary is the set of inputs for which the likelihood of
y = +1 is precisely 50%. or, in other words, the set of inputs x for
which p(y = +1 | x)/p(y =    1 | x) = 1. in order to do this, the
   rst thing to notice is that p(y | x) = p(y, x)/p(x). in the ratio, the
p(x) terms cancel, leaving p(y = +1, x)/p(y =    1, x). instead of
computing this ratio, it is easier to compute the log-likelihood ratio
(or llr), log p(y = +1, x)     log p(y =    1, x), computed below:

llr = log

[xd=1]

(+1),d (1       (+1),d)[xd=0]

  

    log

(1       0)    

[xd=1]

(   1),d (1       (   1),d)[xd=0]

  

(cid:35)

d

(cid:16)

= log   0     log(1       0) +    

[xd = 1]

log   (+1),d     log   (   1),d

+    
d

[xd = 0]

d

log(1       (+1),d)     log(1       (   1),d)

(9.23)

(cid:17)

(cid:17)

=    
d

xd log

  (+1),d
  (   1),d

+    
d

(1     xd) log

1       (+1),d
1       (   1),d

(9.24)

+ log

  0
1       0

(9.25)

(cid:35)

(cid:16)

figure 9.2:

model assumptions

take logs and rearrange

simplify log terms

(cid:34)

xd

log

=    
d

  (+1),d
  (   1),d

    log

1       (+1),d
1       (   1),d

(cid:35)

+    
d

log

1       (+1),d
1       (   1),d

probabilistic modeling 123

+ log

  0
1       0

group x-terms

= x    w + b

wd = log

  (+1),d(1       (   1),d)
  (   1),d(1       (+1),d)

,

b =    
d

log

1       (+1),d
1       (   1),d

(9.26)
(9.27)

+ log

  0
1       0
(9.28)

the result of the algebra is that the naive bayes model has precisely
the form of a linear model! thus, like id88 and many of the
other models you   ve previous studied, the decision boundary is
linear.

9.5 generative stories

a useful way to develop probabilistic models is to tell a generative
story. this is a    ctional story that explains how you believe your
training data came into existence. to make things interesting, con-
sider a multiclass classi   cation problem, with continuous features
modeled by independent gaussians. since the label can take values
1 . . . k, you can use a discrete distribution (die roll) to model it (as
opposed to the bernoilli distribution from before):
1. for each example n = 1 . . . n:
(a) choose a label yn     disc(  )
(b) for each feature d = 1 . . . d:

i. choose feature value xn,d     nor(  yn,d,   2

yn,d)

this generative story can be directly translated into a likelihood
function by replacing the    for each   s with products:

(cid:122)

   
n

p(d) =

for each example

(cid:34)

(cid:125)(cid:124)

exp

1(cid:113)
(cid:124)

2    2

yn,d

yn,d

    1
2  2

(cid:123)(cid:122)
(cid:123)(cid:122)

choose feature value

for each feature

  yn(cid:124)(cid:123)(cid:122)(cid:125)

choose label

   
d

(cid:124)

(xn,d       yn,d)2

(cid:123)
(cid:35)
(cid:125)
(cid:125)

(cid:34)

you can take logs to arrive at the log-likelihood:

log p(d) =    
n

log   yn +    

d

    1
2

log(  2

yn,d)     1
2  2

yn,d

(9.29)

(cid:35)

+ const

(xn,d       yn,d)2

(9.30)

124 a course in machine learning

to optimize for   , you need to add a    sums to one    constraint as
before. this leads to the previous solution where the   ks are propor-
tional to the number of examples with label k. in the case of the   s
you can take a derivative with respect to, say   k,i and obtain:

    log p(d)

     k,i

=

   

     k,i

       
n

   
d

1
2  2

yn,d

(xn,d       yn,d)2

=

   

     k,i

       
n:yn=k

1
2  2
k,d

(xn,i       k,i)2

=    
n:yn=k

1
  2
k,d

(xn,i       k,i)

setting this equal to zero and solving yields:

  k,i =

   n:yn=k xn,i
   n:yn=k 1

ignore irrelevant terms

(9.31)

ignore irrelevant terms

(9.32)

take derivative

(9.33)

(9.34)

namely, the sample mean of the ith feature of the data points that fall
in class k. a similar analysis for   2

k,i yields:

(cid:35)

    log p(d)

     2
k,i

=

   
     2
k,i

       
y:yn=k

1
2

log(  2

k,i) +

1
2  2
k,i

(xn,i       k,i)2

(cid:34)

(cid:34)

(cid:104)

(cid:35)

(cid:105)

=        
y:yn=k

1
2  2
k,i

   

1
2(  2
k,i)2

(xn,i       k,i)2

=

1
2  4
k,i

   
y:yn=k

(xn,i       k, i)2       2

k,i

ignore irrelevant terms

take derivative

simplify

(9.35)

(9.36)

(9.37)

(9.38)

you can now set this equal to zero and solve, yielding:

  2
k,i =

   n:yn=k(xn,i       k,i)2

   n:yn=k 1

which is just the sample variance of feature i for class k.

9.6 conditional models

in the foregoing examples, the task was formulated as attempting to
model the joint distribution of (x, y) pairs. this may seem wasteful:
at prediction time, all you care about is p(y | x), so why not model it
directly?

?

what would the estimate be if you
decided that, for a given class k, all
features had equal variance? what
if you assumed feature i had equal
variance for each class? under what
circumstances might it be a good
idea to make such assumptions?

probabilistic modeling 125

starting with the case of regression is actually somewhat simpler
than starting with classi   cation in this case. suppose you    believe   
that the relationship between the real value y and the vector x should
be linear. that is, you expect that y = w    x + b should hold for some
parameters (w, b). of course, the data that you get does not exactly
obey this: that   s    ne, you can think of deviations from y = w    x +
b as noise. to form a probabilistic model, you must assume some
distribution over noise; a convenient choice is zero-mean gaussian
noise. this leads to the following generative story:
1. for each example n = 1 . . . n:
(a) compute tn = w    xn + b
(b) choose noise en     nor(0,   2)
(c) return yn = tn + en

in this story, the variable tn stands for    target.    it is the noiseless
variable that you do not get to observe. similarly en is the error
(noise) on example n. the value that you actually get to observe is
yn = tn + en. see figure 9.3.
a basic property of the gaussian distribution is additivity. namely,
that if a     nor(  ,   2) and b = a + c, then b     nor(   + c,   2). given
this, from the generative story above, you can derive a shorter gener-
ative story:
1. for each example n = 1 . . . n:

(a) choose yn     nor(w    xn + b,   2)

figure 9.3: pictorial view of targets
versus labels

reading off the log likelihood of a dataset from this generative story,
you obtain:

log p(d) =    
n

    1
2

log(  2)     1

2  2 (w    xn + b     yn)2

(cid:20)

(cid:21)

=     1

2  2    

n

(w    xn + b     yn)2 + const

model assumptions

(9.39)

remove constants

(9.40)

this is precisely the id75 model you encountered in
section 7.6! to minimizing the negative log id203, you need only
solve for the regression coef   cients w, b as before.

in the case of binary classi   cation, using a gaussian noise model

does not make sense. switching to a bernoulli model, which de-
scribes binary outcomes, makes more sense. the only remaining
dif   culty is that the parameter of a bernoulli is a value between zero
and one (the id203 of    heads   ) so your model must produce

1 also called the sigmoid function
because of it   s    s   -shape.

figure 9.4: sketch of logistic function

126 a course in machine learning

such values. a classic approach is to produce a real-valued target, as
before, and then transform this target into a value between zero and
one, so that        maps to 0 and +    maps to 1. a function that does
this is the logistic function1, de   ned below and plotted in figure 9.4:

logistic function:   (z) =

1

1 + exp[   z]

=

exp z

1 + exp z

(9.41)

the logistic function has several nice properties that you can verify
for yourself:   (   z) = 1       (z) and      /   z = z  2(z).

using the logistic function, you can write down a generative story

for binary classi   cation:

1. for each example n = 1 . . . n:

(a) compute tn =    (w    xn + b)
(b) compute zn     ber(tn)
(c) return yn = 2zn     1 (to make it   1)

the log-likelihood for this model is:
log p(d) =    
n

(cid:104)
[yn = +1] log    (w    xn + b)
+ [yn =    1] log    (   w    xn + b)

(cid:105)

log    (yn (w    xn + b))

=    
n

=        
n

=        
n

log [1 + exp (   yn (w    xn + b))]

(cid:96)(log)(yn, w    xn + b)

model and properties of   

(9.42)

join terms

(9.43)
de   nition of   
(9.44)

de   nition of (cid:96)(log)

(9.45)

as you can see, the log-likelihood is precisely the negative of (a

scaled version of) the logistic loss from chapter 7. this model is the
id28 model, and this is where logisitic loss originally
derived from.

todo: conditional versus joint

9.7 id173 via priors

in the foregoing discussion, parameters of the model were selected
according to the maximum likelihood criteria:    nd the parameters
   that maximize p  (d). the trouble with this approach is easy to

probabilistic modeling 127

see even in a simple coin    ipping example. if you    ip a coin twice
and it comes up heads both times, the maximum likelihood estimate
for the bias of the coin is 100%: it will always come up heads. this is
true even if you had only    ipped it once! if course if you had    ipped
it one million times and it had come up heads every time, then you
might    nd this to be a reasonable solution.

this is clearly undesirable behavior, especially since data is expen-
sive in a machine learning setting. one solution (there are others!) is
to seek parameters that balance a tradeoff between the likelihood of
the data and some prior belief you have about what values of those
parameters are likely. taking the case of the id28, you
might a priori believe that small values of w are more likely than
large values, and choose to represent this as a gaussian prior on each
component of w.

the maximum a posteriori principle is a method for incoporat-

ing both data and prior beliefs to obtain a more balanced parameter
estimate. in abstract terms, consider a probabilistic model over data
d that is parameterized by parameters   . if you think of the pa-
rameters as just another random variable, then you can write this
model as p(d |   ), and maximum likelihood amounts to choosing   
to maximize p(d |   ). however, you might instead with to maximize
the id203 of the parameters, given the data. namely, maximize
p(   | d). this term is known as the posterior distribution on   , and
can be computed by bayes    rule:

likelihood

prior
p(  )

(cid:122)
(cid:122)(cid:125)(cid:124)(cid:123)
(cid:125)(cid:124)
(cid:123)
p(d |   )
(cid:124)(cid:123)(cid:122)(cid:125)
p(d)

evidence

(cid:124)
(cid:123)(cid:122)
(cid:125)
p(   | d)

posterior

=

(cid:90)

, where

p(d) =

d  p(  )p(d |   )

(9.46)

this reads: the posterior is equal to the prior times the likelihood di-
vided by the evidence.2 the evidence is a scary-looking term (it has
an integral!) but note that from the perspective of seeking parameters
   than maximize the posterior, the evidence is just a constant (it does
not depend on   ) and therefore can be ignored.

returning to the id28 example with gaussian priors

on the weights, the log posterior looks like:

2 the evidence is sometimes called the
marginal likelihood.

log p(   | d) =        
n

(cid:96)(log)(yn, w    xn + b)        

d

1
2  2 w2

d + const

=        
n

(cid:96)(log)(yn, w    xn + b)     1

2  2 ||w||2

and therefore reduces to a regularized logistic function, with a

model de   nition

(9.47)
(9.48)

128 a course in machine learning

squared 2-norm regularizer on the weights. (a 1-norm regularizer
can be obtained by using a laplace prior on w rather than a gaussian
prior on w.)

9.8 further reading

todo

10 | neural networks

todo

   

the first learning models you learned about (id90
and nearest neighbor models) created complex, non-linear decision
boundaries. we moved from there to the id88, perhaps the
most classic linear model. at this point, we will move back to non-
linear learning models, but using all that we have learned about
linear learning thus far.

this chapter presents an extension of id88 learning to non-
linear decision boundaries, taking the biological inspiration of neu-
rons even further. in the id88, we thought of the input data
point (e.g., an image) as being directly connected to an output (e.g.,
label). this is often called a single-layer network because there is one
layer of weights. now, instead of directly connecting the inputs to
the outputs, we will insert a layer of    hidden    nodes, moving from
a single-layer network to a multi-layer network. but introducing
a non-linearity at inner layers, this will give us non-linear decision
boundaires. in fact, such networks are able to express almost any
function we want, not just linear functions. the trade-off for this    ex-
ibility is increased complexity in parameter tuning and model design.

10.1 bio-inspired multi-layer networks

one of the major weaknesses of linear models, like id88 and
the regularized linear models from the previous chapter, is that they
are linear! namely, they are unable to learn arbitrary decision bound-
aries. in contrast, id90 and knn could learn arbitrarily
complicated decision boundaries.

one approach to doing this is to chain together a collection of

id88s to build more complex neural networks. an example of
a two-layer network is shown in figure 10.1. here, you can see    ve
inputs (features) that are fed into two hidden units. these hidden
units are then fed in to a single output unit. each edge in this    gure
corresponds to a different weight. (even though it looks like there are
three layers, this is called a two-layer network because we don   t count

learning objectives:
    explain the biological inspiration for

multi-layer neural networks.

    construct a two-layer network that

can solve the xor problem.

    implement the back-propogation
algorithm for training multi-layer
networks.

    explain the trade-off between depth

and breadth in network structure.
    contrast neural networks with ra-
dial basis functions with k-nearest
neighbor learning.

dependencies:

figure 10.1: picture of a two-layer
network with 5 inputs and two hidden
units

130 a course in machine learning

the inputs as a real layer. that is, it   s two layers of trained weights.)
prediction with a neural network is a straightforward generaliza-
tion of prediction with a id88. first you compute activations
of the nodes in the hidden unit based on the inputs and the input
weights. then you compute activations of the output unit given the
hidden unit activations and the second layer of weights.

the only major difference between this computation and the per-

ceptron computation is that the hidden units compute a non-linear
function of their inputs. this is usually called the activation function
or link function. more formally, if wi,d is the weights on the edge
connecting input d to hidden unit i, then the activation of hidden unit
i is computed as:
hi = f (wi    x)

(10.1)

where f is the link function and wi refers to the vector of weights
feeding in to node i.
one example link function is the sign function. that is, if the
incoming signal is negative, the activation is    1. otherwise the
activation is +1. this is a potentially useful activiation function,
but you might already have guessed the problem with it: it is non-
differentiable.

explain bias!!!
a more popular link function is the hyperbolic tangent function,
tanh. a comparison between the sign function and the tanh function
is in figure 10.2. as you can see, it is a reasonable approximation
to the sign function, but is convenient in that it is differentiable.1
because it looks like an    s    and because the greek character for    s   
is    sigma,    such functions are usually called sigmoid functions.

assuming for now that we are using tanh as the link function, the

overall prediction made by a two-layer network can be computed
using algorithm 10.1. this function takes a matrix of weights w
corresponding to the    rst layer weights and a vector of weights v cor-
responding to the second layer. you can write this entire computation
out in one line as:

vi tanh(wi      x)

  y =    
i
= v    tanh(w  x)

(10.2)

(10.3)

where the second line is short hand assuming that tanh can take a
vector as input and product a vector as output.

figure 10.2: picture of sign versus tanh
1 it   s derivative is just 1     tanh2(x).

?

is it necessary to use a link function
at all? what would happen if you
just used the identify function as a
link?

neural networks

131

x0

x1

x2
y
+1 +1 +1 +1
+1 +1
-1
-1
-1 +1 +1
-1
-1 +1
-1 +1
table 10.1: small xor data set.

?

verify that these output weights
will actually give you xor.

?

this shows how to create an    or   
function. how can you create an
   and    function?

algorithm 25 twolayernetworkpredict(w, v,   x)
1: for i = 1 to number of hidden units do
2:
3: end for
4: return v    h

hi     tanh(wi      x)

// compute activation of hidden unit i

// compute output unit

the claim is that two-layer neural networks are more expressive

than single layer networks (i.e., id88s). to see this, you can
construct a very small two-layer network for solving the xor prob-
lem. for simplicity, suppose that the data set consists of four data
points, given in table 10.1. the classi   cation rule is that y = +1 if an
only if x1 = x2, where the features are just   1.

to achieve the    or    behavior, you can start by setting the bias to

you can solve this problem using a two layer network with two
hidden units. the key idea is to make the    rst hidden unit compute
an    or    function: x1     x2. the second hidden unit can compute an
   and    function: x1     x2. the the output can combine these into a
single prediction that mimics xor. once you have the    rst hidden
unit activate for    or    and the second for    and,    you need only set the
output weights as    2 and +1, respectively.
   0.5 and the weights for the two    real    features as both being 1. you
can check for yourself that this will do the    right thing    if the link
function were the sign function. of course it   s not, it   s tanh. to get
tanh to mimic sign, you need to make the dot product either really
really large or really really small. you can accomplish this by set-
ting the bias to    500, 000 and both of the two weights to 1, 000, 000.
now, the activation of this unit will be just slightly above    1 for
x = (cid:104)   1,   1(cid:105) and just slightly below +1 for the other three examples.

at this point you   ve seen that one-layer networks (aka percep-
trons) can represent any linear function and only linear functions.
you   ve also seen that two-layer networks can represent non-linear
functions like xor. a natural question is: do you get additional
representational power by moving beyond two layers? the answer
is partially provided in the following theorem, due originally to
george cybenko for one particular type of link function, and ex-
tended later by kurt hornik to arbitrary link functions.

theorem 10 (two-layer networks are universal function approx-
imators). let f be a continuous function on a bounded subset of d-
dimensional space. then there exists a two-layer neural network   f with a
   nite number of hidden units that approximate f arbitrarily well. namely,

for all x in the domain of f,(cid:12)(cid:12)f(x)       f(x)(cid:12)(cid:12) <  .

or, in colloquial terms    two-layer networks can approximate any

132 a course in machine learning

function.   

this is a remarkable theorem. practically, it says that if you give

me a function f and some error tolerance parameter  , i can construct
a two layer network that computes f. in a sense, it says that going
from one layer to two layers completely changes the representational
capacity of your model.

when working with two-layer networks, the key question is: how

many hidden units should i have? if your data is d dimensional
and you have k hidden units, then the total number of parameters
is (d + 2)k. (the    rst +1 is from the bias, the second is from the
second layer of weights.) following on from the heuristic that you
should have one to two examples for each parameter you are trying
to estimate, this suggests a method for choosing the number of hid-
den units as roughly (cid:98) n
d(cid:99). in other words, if you have tons and tons
of examples, you can safely have lots of hidden units. if you only
have a few examples, you should probably restrict the number of
hidden units in your network.

the number of units is both a form of inductive bias and a form
of id173. in both view, the number of hidden units controls
how complex your function will be. lots of hidden units     very
complicated function. as the number increases, training performance
continues to get better. but at some point, test performance gets
worse because the network has over   t the data.

10.2 the back-propagation algorithm

the back-propagation algorithm is a classic approach to training
neural networks. although it was not originally seen this way, based
on what you know from the last chapter, you can summarize back-
propagation as:

back-propagation = id119 + chain rule

(10.4)

more speci   cally, the set up is exactly the same as before. you are
going to optimize the weights in the network to minimize some ob-
jective function. the only difference is that the predictor is no longer
linear (i.e.,   y = w    x + b) but now non-linear (i.e., v    tanh(w  x)).
the only question is how to do id119 on this more compli-
cated objective.

for now, we will ignore the idea of id173. this is for two
reasons. the    rst is that you already know how to deal with regular-
ization, so everything you   ve learned before applies. the second is
that historically, neural networks have not been regularized. instead,
people have used early stopping as a method for controlling over   t-
ting. presently, it   s not obvious which is a better solution: both are

valid options.

neural networks

133

to be completely explicit, we will focus on optimizing squared
error. again, this is mostly for historic reasons. you could easily
replace squared error with your id168 of choice. our overall
objective is:

(cid:32)

(cid:33)2

min
w,v

   
n

1
2

yn        

i

vi f (wi    xn)

(10.5)

here, f is some link function like tanh.

the easy case is to differentiate this with respect to v: the weights

for the output unit. without even doing any math, you should be
able to guess what this looks like. the way to think about it is that
from vs perspective, it is just a linear model, attempting to minimize
squared error. the only    funny    thing is that its inputs are the activa-
tions h rather than the examples x. so the gradient with respect to v
is just as for the linear case.

to make things notationally more convenient, let en denote the

error on the nth example (i.e., the blue term above), and let hn denote
the vector of hidden unit activations on that example. then:

   v =        

n

enhn

(10.6)

this is exactly like the linear case. one way of interpreting this is:
how would the output weights have to change to make the prediction
better? this is an easy question to answer because they can easily
measure how their changes affect the output.

the more complicated aspect to deal with is the weights corre-

sponding to the    rst layer. the reason this is dif   cult is because the
weights in the    rst layer aren   t necessarily trying to produce speci   c
values, say 0 or 5 or    2.1. they are simply trying to produce acti-
vations that get fed to the output layer. so the change they want to
make depends crucially on how the output layer interprets them.

thankfully, the chain rule of calculus saves us. ignoring the sum

over data points, we can compute:

(cid:33)2

l(w) =

vi f (wi    x)

(cid:32)

=

1
y        
2
i
   l
(cid:32)
    fi
=    

    fi
   wi
y        
i

= f (cid:48)(wi    x)x

   l
   wi
   l
    fi
    fi
   wi

(cid:33)

vi =    evi

vi f (wi    x)

(10.7)

(10.8)

(10.9)

(10.10)

134 a course in machine learning

algorithm 26 twolayernetworktrain(d,   , k, maxiter)
1: w     d  k matrix of small random values
2: v     k-vector of small random values
3: for iter = 1 . . . maxiter do
4: g     d  k matrix of zeros
g     k-vector of zeros
for all (x,y)     d do
for i = 1 to k do
ai     wi      x
hi     tanh(ai)

5:

6:

7:

8:

9:

// initialize input layer weights
// initialize output layer weights

// initialize input layer gradient
// initialize output layer gradient

// compute activation of hidden unit i

end for
  y     v    h
e     y       y
g     g     eh
for i = 1 to k do

gi     gi     evi(1     tanh2(ai))x

10:

11:

12:

13:

14:

15:

16:

end for

17:

18: w     w       g

end for
v     v       g

19:
20: end for
21: return w, v

// compute output unit
// compute error
// update gradient for output layer

// update gradient for input layer

// update input layer weights
// update output layer weights

putting this together, we get that the gradient with respect to wi is:

   wi =    evi f (cid:48)(wi    x)x

(10.11)

intuitively you can make sense of this. if the overall error of the
predictor (e) is small, you want to make small steps. if vi is small
for hidden unit i, then this means that the output is not particularly
sensitive to the activation of the ith hidden unit. thus, its gradient
should be small. if vi    ips sign, the gradient at wi should also    ip
signs. the name back-propagation comes from the fact that you
propagate gradients backward through the network, starting at the
end.

the complete instantiation of id119 for a two layer

network with k hidden units is sketched in algorithm 10.2. note that
this really is exactly a id119 algorithm; the only different is
that the computation of the gradients of the input layer is moderately
complicated.

as a bit of practical advice, implementing the back-propagation

algorithm can be a bit tricky. sign errors often abound. a useful trick
is    rst to keep w    xed and work on just training v. then keep v
   xed and work on training w. then put them together.

?

what would happen to this algo-
rithm if you wanted to optimize
exponential loss instead of squared
error? what if you wanted to add in
weight id173?

?

if you like matrix calculus, derive
the same algorithm starting from
eq (10.3).

10.3

initialization and convergence of neural networks

neural networks

135

based on what you know about linear models, you might be tempted
to initialize all the weights in a neural network to zero. you might
also have noticed that in algorithm 10.2, this is not what   s done:
they   re initialized to small random values. the question is why?

the answer is because an initialization of w = 0 and v = 0 will

lead to    uninteresting    solutions. in other words, if you initialize the
model in this way, it will eventually get stuck in a bad local optimum.
to see this,    rst realize that on any example x, the activation hi of the
hidden units will all be zero since w = 0. this means that on the    rst
iteration, the gradient on the output weights (v) will be zero, so they
will stay put. furthermore, the gradient w1,d for the dth feature on
the ith unit will be exactly the same as the gradient w2,d for the same
feature on the second unit. this means that the weight matrix, after
a gradient step, will change in exactly the same way for every hidden
unit. thinking through this example for iterations 2 . . . , the values of
the hidden units will always be exactly the same, which means that
the weights feeding in to any of the hidden units will be exactly the
same. eventually the model will converge, but it will converge to a
solution that does not take advantage of having access to the hidden
units.

this shows that neural networks are sensitive to their initialization.
in particular, the function that they optimize is non-convex, meaning
that it might have plentiful local optima. (one of which is the trivial
local optimum described in the preceding paragraph.) in a sense,
neural networks must have local optima. suppose you have a two
layer network with two hidden units that   s been optimized. you have
weights w1 from inputs to the    rst hidden unit, weights w2 from in-
puts to the second hidden unit and weights (v1, v2) from the hidden
units to the output. if i give you back another network with w1 and
w2 swapped, and v1 and v2 swapped, the network computes exactly
the same thing, but with a markedly different weight structure. this
phenomena is known as symmetric modes (   mode    referring to an
optima) meaning that there are symmetries in the weight space. it
would be one thing if there were lots of modes and they were all
symmetric: then    nding one of them would be as good as    nding
any other. unfortunately there are additional local optima that are
not global optima.

random initialization of the weights of a network is a way to

address both of these problems. by initializing a network with small
random weights (say, uniform between    0.1 and 0.1), the network is
unlikely to fall into the trivial, symmetric local optimum. moreover,
by training a collection of networks, each with a different random

figure 10.3: convergence of randomly
initialized networks

136 a course in machine learning

initialization, you can often obtain better solutions that with just
one initialization. in other words, you can train ten networks with
different random seeds, and then pick the one that does best on held-
out data. figure 10.3 shows prototypical test-set performance for ten
networks with different random initialization, plus an eleventh plot
for the trivial symmetric network initialized with zeros.

one of the typical complaints about neural networks is that they

are    nicky. in particular, they have a rather large number of knobs to
tune:
1. the number of layers
2. the number of hidden units per layer
3. the id119 learning rate   
4. the initialization
5. the stopping iteration or weight id173

the last of these is minor (early stopping is an easy id173
method that does not require much effort to tune), but the others
are somewhat signi   cant. even for two layer networks, having to
choose the number of hidden units, and then get the learning rate
and initialization    right    can take a bit of work. clearly it can be
automated, but nonetheless it takes time.

another dif   culty of neural networks is that their weights can
be dif   cult to interpret. you   ve seen that, for linear networks, you
can often interpret high weights as indicative of positive examples
and low weights as indicative of negative examples. in multilayer
networks, it becomes very dif   cult to try to understand what the
different hidden units are doing.

10.4 beyond two layers

the de   nition of neural networks and the back-propagation algo-
rithm can be generalized beyond two layers to any arbitrary directed
acyclic graph. in practice, it is most common to use a layered net-
work like that shown in figure 10.4 unless one has a very strong
reason (aka inductive bias) to do something different. however, the
view as a directed graph sheds a different sort of insight on the back-
propagation algorithm.

suppose that your network structure is stored in some directed
acyclic graph, like that in figure 10.5. we index nodes in this graph
as u, v. the activation before applying non-linearity at a node is au
and after non-linearity is hu. the graph has a single sink, which is
the output node y with activation ay (no non-linearity is performed

figure 10.4: multi-layer network

figure 10.5: dag network

neural networks

137

hu     corresponding feature of x

algorithm 27 forwardpropagation(x)
1: for all input nodes u do
2:
3: end for
4: for all nodes v in the network whose parent   s are computed do
5:

av        u   par(v) w(u,v)hu
hv     tanh(av)

6:
7: end for
8: return ay

algorithm 28 id26(x, y)
1: run forwardpropagation(x) to compute activations
2: ey     y     ay
3: for all nodes v in the network whose error ev is computed do
4:

for all u     par(v) do

// compute overall network error

5:

6:

gu,v        evhu
// compute gradient of this edge
eu     eu + evwu,v(1     tanh2(au)) // compute the    error    of the parent node

end for

7:
8: end for
9: return all gradients ge

on the output unit). the graph has d-many inputs (i.e., nodes with
no parent), whose activations hu are given by an input example. an
edge (u, v) is from a parent to a child (i.e., from an input to a hidden
unit, or from a hidden unit to the sink). each edge has a weight wu,v.
we say that par(u) is the set of parents of u.

there are two relevant algorithms: forward-propagation and back-

propagation. forward-propagation tells you how to compute the
activation of the sink y given the inputs. back-propagation computes
derivatives of the edge weights for a given input.

the key aspect of the forward-propagation algorithm is to iter-
atively compute activations, going deeper and deeper in the dag.
once the activations of all the parents of a node u have been com-
puted, you can compute the activation of node u. this is spelled out
in algorithm 10.4. this is also explained pictorially in figure 10.6.
back-propagation (see algorithm 10.4) does the opposite: it com-

putes gradients top-down in the network. the key idea is to compute
an error for each node in the network. the error at the output unit is
the    true error.    for any input unit, the error is the amount of gradi-
ent that we see coming from our children (i.e., higher in the network).
these errors are computed backwards in the network (hence the
name back-propagation) along with the gradients themselves. this is
also explained pictorially in figure 10.7.

given the back-propagation algorithm, you can directly run gradi-

ent descent, using it as a subroutine for computing the gradients.

figure 10.6: picture of forward prop

figure 10.7: picture of back prop

138 a course in machine learning

10.5 breadth versus depth

at this point, you   ve seen how to train two-layer networks and how
to train arbitrary networks. you   ve also seen a theorem that says
that two-layer networks are universal function approximators. this
begs the question: if two-layer networks are so great, why do we care
about deeper networks?

to understand the answer, we can borrow some ideas from cs
theory, namely the idea of circuit complexity. the goal is to show
that there are functions for which it might be a    good idea    to use a
deep network. in other words, there are functions that will require a
huge number of hidden units if you force the network to be shallow,
but can be done in a small number of units if you allow it to be deep.
the example that we   ll use is the parity function which, ironically
enough, is just a generalization of the xor problem. the function is
de   ned over binary inputs as:

xd mod 2

(10.12)

parity(x) =    
d

(cid:40)

=

1 if the number of 1s in x is odd
0 if the number of 1s in x is even

(10.13)
it is easy to de   ne a circuit of depth o(log2 d) with o(d)-many
gates for computing the parity function. each gate is an xor, ar-
ranged in a complete binary tree, as shown in figure 10.8. (if you
want to disallow xor as a gate, you can    x this by allowing the
depth to be doubled and replacing each xor with an and, or and
not combination, like you did at the beginning of this chapter.)

this shows that if you are allowed to be deep, you can construct a
circuit with that computes parity using a number of hidden units that
is linear in the dimensionality. so can you do the same with shallow
circuits? the answer is no. it   s a famous result of circuit complexity
that parity requires exponentially many gates to compute in constant
depth. the formal theorem is below:
theorem 11 (parity function complexity). any circuit of depth k <
log2 d that computes the parity function of d input bits must contain oed
gates.

this is a very famous result because it shows that constant-depth
circuits are less powerful that deep circuits. although a neural net-
work isn   t exactly the same as a circuit, the is generally believed that
the same result holds for neural networks. at the very least, this
gives a strong indication that depth might be an important considera-
tion in neural networks.

one way of thinking about the issue of breadth versus depth has

to do with the number of parameters that need to be estimated. by

figure 10.8: nnet:paritydeep: deep
function for computing parity

?

what is it about neural networks
that makes it so that the theorem
about circuits does not apply di-
rectly?

neural networks

139

?

while these small derivatives might
make training dif   cult, they might
be good for other reasons: what
reasons?

the heuristic that you need roughly one or two examples for every
parameter, a deep model could potentially require exponentially
fewer examples to train than a shallow model!

this now    ips the question: if deep is potentially so much better,

why doesn   t everyone use deep networks? there are at least two
answers. first, it makes the architecture selection problem more
signi   cant. namely, when you use a two-layer network, the only
hyperparameter to choose is how many hidden units should go in
the middle layer. when you choose a deep network, you need to
choose how many layers, and what is the width of all those layers.
this can be somewhat daunting.

a second issue has to do with training deep models with back-
propagation. in general, as back-propagation works its way down
through the model, the sizes of the gradients shrink. you can work
this out mathematically, but the intuition is simpler. if you are the
beginning of a very deep network, changing one single weight is
unlikely to have a signi   cant effect on the output, since it has to
go through so many other units before getting there. this directly
implies that the derivatives are small. this, in turn, means that back-
propagation essentially never moves far from its initialization when
run on very deep networks.

finding good ways to train deep networks is an active research
area. there are two general strategies. the    rst is to attempt to ini-
tialize the weights better, often by a layer-wise initialization strategy.
this can be often done using unlabeled data. after this initializa-
tion, back-propagation can be run to tweak the weights for whatever
classi   cation problem you care about. a second approach is to use a
more complex optimization procedure, rather than id119.
you will learn about some such procedures later in this book.

10.6 basis functions

at this point, we   ve seen that: (a) neural networks can mimic linear
functions and (b) they can learn more complex functions. a rea-
sonable question is whether they can mimic a knn classi   er, and
whether they can do it ef   ciently (i.e., with not-too-many hidden
units).

a natural way to train a neural network to mimic a knn classi   er

is to replace the sigmoid link function with a radial basis function
(rbf). in a sigmoid network (i.e., a network with sigmoid links),
the hidden units were computed as hi = tanh(wi, x  ). in an rbf
network, the hidden units are computed as:

(cid:104)     i ||wi     x||2(cid:105)

hi = exp

(10.14)

140 a course in machine learning

in other words, the hidden units behave like little gaussian    bumps   

centered around locations speci   ed by the vectors wi. a one-dimensional
example is shown in figure 10.9. the parameter   i speci   es the width
of the gaussian bump. if   i is large, then only data points that are
really close to wi have non-zero activations. to distinguish sigmoid
networks from rbf networks, the hidden units are typically drawn
with sigmoids or with gaussian bumps, as in figure 10.10.

training rbf networks involves    nding good values for the gas-
sian widths,   i, the centers of the gaussian bumps, wi and the con-
nections between the gaussian bumps and the output unit, v. this
can all be done using back-propagation. the gradient terms for v re-
main unchanged from before, the the derivates for the other variables
differ (see exercise ??).

one of the big questions with rbf networks is: where should

the gaussian bumps be centered? one can, of course, apply back-
propagation to attempt to    nd the centers. another option is to spec-
ify them ahead of time. for instance, one potential approach is to
have one rbf unit per data point, centered on that data point. if you
carefully choose the   s and vs, you can obtain something that looks
nearly identical to distance-weighted knn by doing so. this has the
added advantage that you can go futher, and use back-propagation
to learn good gaussian widths (  ) and    voting    factors (v) for the
nearest neighbor algorithm.

10.7 further reading

todo further reading

figure 10.9: nnet:rbfpicture: a one-d
picture of rbf bumps

figure 10.10: nnet:unitsymbols: picture
of nnet with sigmoid/rbf units

?

consider an rbf network with
one hidden unit per training point,
centered at that point. what bad
thing might happen if you use back-
propagation to estimate the   s and
v on this data if you   re not careful?
how could you be careful?

11 | kernel methods

learning objectives:
    explain how kernels generalize

both feature combinations and basis
functions.

    contrast dot products with kernel

products.

    implement kernelized id88.
    derive a kernelized version of

regularized least squares regression.

    implement a kernelized version of

the id88.

    derive the dual formulation of the

support vector machine.

dependencies:

many who have had an opportunity of knowing any more about
mathematics confuse it with arithmetic, and consider it an arid
science. in reality, however, it is a science which requires a great
amount of imagination.

    so   a kovalevskaya

linear models are great because they are easy to understand
and easy to optimize. they suffer because they can only learn very
simple decision boundaries. neural networks can learn more com-
plex decision boundaries, but lose the nice convexity properties of
many linear models.

one way of getting a linear model to behave non-linearly is to

transform the input. for instance, by adding feature pairs as addi-
tional inputs. learning a linear model on such a representation is
convex, but is computationally prohibitive in all but very low dimen-
sional spaces. you might ask: instead of explicitly expanding the fea-
ture space, is it possible to stay with our original data representation
and do all the feature blow up implicitly? surprisingly, the answer is
often    yes    and the family of techniques that makes this possible are
known as kernel approaches.

11.1 from feature combinations to kernels

in section 5.4, you learned one method for increasing the expressive
power of linear models: explode the feature space. for instance,
a    quadratic    feature explosion might map a feature vector x =
(cid:104)x1, x2, x3, . . . , xd(cid:105) to an expanded version denoted   (x):

  (x) = (cid:104)1, 2x1, 2x2, 2x3, . . . , 2xd,
x2
1, x1x2, x1x3, . . . , x1xd,
x2x1, x2
2, x2x3, . . . , x2xd,
x3x1, x3x2, x2
3, . . . , x2xd,
. . . ,
d(cid:105)
xdx1, xdx2, xdx3, . . . , x2

(11.1)

(note that there are repetitions here, but hopefully most learning
algorithms can deal well with redundant features; in particular, the
2x1 terms are due to collapsing some repetitions.)

142 a course in machine learning

you could then train a classi   er on this expanded feature space.
there are two primary concerns in doing so. the    rst is computa-
tional: if your learning algorithm scales linearly in the number of fea-
tures, then you   ve just squared the amount of computation you need
to perform; you   ve also squared the amount of memory you   ll need.
the second is statistical: if you go by the heuristic that you should
have about two examples for every feature, then you will now need
quadratically many training examples in order to avoid over   tting.
this chapter is all about dealing with the computational issue. it

will turn out in chapter 12 that you can also deal with the statistical
issue: for now, you can just hope that id173 will be suf   cient
to attenuate over   tting.

the key insight in kernel-based learning is that you can rewrite
many linear models in a way that doesn   t require you to ever ex-
plicitly compute   (x). to start with, you can think of this purely
as a computational    trick    that enables you to use the power of a
quadratic feature mapping without actually having to compute and
store the mapped vectors. later, you will see that it   s actually quite a
bit deeper. most algorithms we discuss involve a product of the form
w      (x), after performing the feature mapping. the goal is to rewrite
these algorithms so that they only ever depend on dot products be-
tween two examples, say x and z; namely, they depend on   (x)      (z).
to understand why this is helpful, consider the quadratic expansion
from above, and the dot-product between two vectors. you get:

  (x)      (z) = 1 + x1z1 + x2z2 +          + xdzd + x2
1 +          + x1xdz1zd+
1z2
         + xdx1zdz1 + xdx2zdz2 +          + x2
(11.2)
dz2
d
= 1 + 2    
xdzd +    
   
(11.3)
e
d
d
= 1 + 2x    z + (x    z)2
= (1 + x    z)2

(11.4)
(11.5)

xdxezdze

thus, you can compute   (x)      (z) in exactly the same amount of
time as you can compute x    z (plus the time it takes to perform an
addition and a multiply, about 0.02 nanoseconds on a circa 2011
processor).

the rest of the practical challenge is to rewrite your algorithms so
that they only depend on dot products between examples and not on
any explicit weight vectors.

11.2 kernelized id88

consider the original id88 algorithm from chapter 4, re-

peated in algorithm 11.2 using id202 notation and using fea-
ture expansion notation   (x). in this algorithm, there are two places

kernel methods

143

// initialize weights and bias

algorithm 29 id88train(d, maxiter)
1: w     0, b     0
2: for iter = 1 . . . maxiter do
3:

for all (x,y)     d do
a     w      (x) + b
if ya     0 then
w     w + y   (x)
b     b + y

4:

5:

6:

7:

8:

end if
end for

9:
10: end for
11: return w, b

// compute activation for this example

// update weights
// update bias

math review | spans
if u = {ui}i
linear combinations of uis; namely: span(u ) = {   i aiui
linearly independent, then the dimension of span(u ) is i; in particular, if there are d-many linearly
independent vectors then they span rd.

i=1 is a set of vectors in rd, then the span of u is the set of vectors that can be written as
: a1     r, . . . , ai     r}. if all of the uis are

figure 11.1:

where   (x) is used explicitly. the    rst is in computing the activation
(line 4) and the second is in updating the weights (line 6). the goal is
to remove the explicit dependence of this algorithm on    and on the
weight vector.

to do so, you can observe that at any point in the algorithm, the
weight vector w can be written as a linear combination of expanded
training data. in particular, at any point, w =    n   n  (xn) for some
parameters   . initially, w = 0 so choosing    = 0 yields this. if the
   rst update occurs on the nth training example, then the resolution
weight vector is simply yn  (xn), which is equivalent to setting   n =
yn. if the second update occurs on the mth training example, then all
you need to do is update   m       m + ym. this is true, even if you
make multiple passes over the data. this observation leads to the
following representer theorem, which states that the weight vector of
the id88 lies in the span of the training data.

theorem 12 (id88 representer theorem). during a run of
the id88 algorithm, the weight vector w is always in the span of the
(assumed non-empty) training data,   (x1), . . . ,   (xn).
proof of theorem 12. by induction. base case: the span of any non-
empty set contains the zero vector, which is the initial weight vec-
tor. inductive case: suppose that the theorem is true before the kth
update, and suppose that the kth update happens on example n.
by the inductive hypothesis, you can write w =    i   i  (xi) before

144 a course in machine learning

algorithm 30 kernelizedid88train(d, maxiter)
1:        0, b     0
2: for iter = 1 . . . maxiter do
for all (xn,yn)     d do
3:

// initialize coef   cients and bias

a        m   m  (xm)      (xn) + b
if yna     0 then
  n       n + yn
b     b + y

// compute activation for this example

// update coef   cients
// update bias

4:

5:

6:

7:

8:

end if
end for

9:
10: end for
11: return   , b

the update. the new weight vector is [   i   i  (xi)] + yn  (xn) =
   i(  i + yn[i = n])  (xi), which is still in the span of the training
data.

now that you know that you can always write w =    n   n  (xn) for

some   is, you can additionall compute the activations (line 4) as:

(cid:32)

(cid:33)

w      (x) + b =

   
n

  n  (xn)

     (x) + b
(cid:105)

+ b

(cid:104)

=    
n

  n

  (xn)      (x)

de   nition of w

(11.6)

dot products are linear

(11.7)

this now depends only on dot-products between data points, and
never explicitly requires a weight vector. you can now rewrite the
entire id88 algorithm so that it never refers explicitly to the
weights and only ever depends on pairwise dot products between
examples. this is shown in algorithm 11.2.

the advantage to this    kernelized    algorithm is that you can per-

form feature expansions like the quadratic feature expansion from
the introduction for    free.    for example, for exactly the same cost as
the quadratic features, you can use a cubic feature map, computed
    (x)  (z) = (1 + x    z)3, which corresponds to three-way inter-
as
actions between variables. (and, in general, you can do so for any
polynomial degree p at the same computational complexity.)

11.3 kernelized id116

for a complete change of pace, consider the id116 algorithm from
section 3. this algorithm is for id91 where there is no notion of
   training labels.    instead, you want to partition the data into coher-
ent clusters. for data in rd, it involves randomly initializing k-many

kernel methods

145

cluster means   (1), . . . ,   (k). the algorithm then alternates between the
following two steps until convergence, with x replaced by   (x) since
that is the eventual goal:

1. for each example n, set cluster label zn = arg mink
2. for each cluster k, update   (k) = 1
nk

   n:zn=k   (xn), where nk is the

number of n with zn = k.

(cid:12)(cid:12)(cid:12)(cid:12)  (xn)       (k)(cid:12)(cid:12)(cid:12)(cid:12)2.

the question is whether you can perform these steps without ex-
plicitly computing   (xn). the representer theorem is more straight-
forward here than in the id88. the mean of a set of data is,
almost by de   nition, in the span of that data (choose the ais all to be
equal to 1/n). thus, so long as you initialize the means in the span
of the data, you are guaranteed always to have the means in the span
of the data. given this, you know that you can write each mean as an
(k)
expansion of the data; say that   (k) =    n   
n   (xn) for some parame-
ters   

n (there are n  k-many such parameters).

given this expansion, in order to execute step (1), you need to

(k)

compute norms. this can be done as follows:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (xn)       (k)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)  (xn)        
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

||  (xn)||2 +

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   

m

m

  

zn = arg min

k

= arg min

k

= arg min

k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(k)
m   (xm)

(cid:34)

+   (xn)   

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

   
m

(11.9)

(cid:35)

(k)
m   (xm)

  

(k)
m   (xm)

  

k

  

(k)

(k)
m   

   
m

   
m(cid:48)

= arg min

m(cid:48)   (xm)      (xm(cid:48) ) +    

(11.10)
m   (xm)      (xn) + const
(11.11)
this computation can replace the assignments in step (1) of id116.
the mean updates are more direct in step (2):

(k)

m

  

(cid:40) 1

nk
0

if zn = k
otherwise

(11.12)

  (k) =

1
nk

   
n:zn=k

  (xn)          

(k)
n =

11.4 what makes a kernel

a kernel is just a form of generalized dot product. you can also
think of it as simply shorthand for   (x)      (z), which is commonly
written k  (x, z). or, when    is clear from context, simply k(x, z).

(11.8)

de   nition of zn

de   nition of   (k)

expand quadratic term

linearity and constant

146 a course in machine learning

this is often refered to as the kernel product between x and z (under
the mapping   ).

in this view, what you   ve seen in the preceding two sections is

that you can rewrite both the id88 algorithm and the id116
algorithm so that they only ever depend on kernel products between data
points, and never on the actual datapoints themselves. this is a very pow-
erful notion, as it has enabled the development of a large number of
non-linear algorithms essentially    for free    (by applying the so-called
kernel trick, that you   ve just seen twice).

this raises an interesting question. if you have rewritten these
algorithms so that they only depend on the data through a function
k : x  x     r, can you stick any function k in these algorithms,
or are there some k that are    forbidden?    in one sense, you    could   
use any k, but the real question is: for what types of functions k do
these algorithms retain the properties that we expect them to have
(like convergence, optimality, etc.)?
one way to answer this question is to say that k(  ,  ) is a valid
kernel if it corresponds to the inner product between two vectors.
that is, k is valid if there exists a function    such that k(x, z) =
  (x)      (z). this is a direct de   nition and it should be clear that if k
satis   es this, then the algorithms go through as expected (because
this is how we derived them).

you   ve already seen the general class of polynomial kernels,

which have the form:

k(poly)
d

(x, z) =

1 + x    z

(cid:16)

(cid:17)d

(11.13)

where d is a hyperparameter of the kernel. these kernels correspond
to polynomial feature expansions.
there is an alternative characterization of a valid id81
that is more mathematical. it states that k : x  x     r is a kernel if
k is positive semi-definite (or, in shorthand, psd). this property is
also sometimes called mercer   s condition. in this context, this means

the for all functions f that are square integrable (i.e.,(cid:82) f (x)2dx <    ),
(cid:90)(cid:90)

other than the zero function, the following property holds:

f (x)k(x, z) f (z)dxdz > 0

(11.14)

this likely seems like it came out of nowhere. unfortunately, the
connection is well beyond the scope of this book, but is covered well
is external sources. for now, simply take it as a given that this is an
equivalent requirement. (for those so inclined, the appendix of this
book gives a proof, but it requires a bit of knowledge of function
spaces to understand.)

the question is: why is this alternative characterization useful? it
is useful because it gives you an alternative way to construct kernel

kernel methods

147

de   nition of k

functions. for instance, using it you can easily prove the following,
which would be dif   cult from the de   nition of kernels as inner prod-
ucts after feature mappings.

theorem 13 (kernel addition). if k1 and k2 are kernels, the k de   ned
by k(x, z) = k1(x, z) + k2(x, z) is also a kernel.
proof of theorem 13. you need to verify the positive semi-de   nite
property on k. you can do this as follows:

(cid:90)(cid:90)

f (x)k(x, z) f (z)dxdz =

=

f (x) [k1(x, z) + k2(x, z)] f (z)dxdz
(11.15)

f (x)k1(x, z) f (z)dxdz

(cid:90)(cid:90)
(cid:90)(cid:90)
(cid:90)(cid:90)

+

f (x)k2(x, z) f (z)dxdz

distributive rule

(11.16)

(11.17)

k1 and k2 are psd

> 0 + 0

more generally, any positive linear combination of kernels is still a
kernel. speci   cally, if k1, . . . , km are all kernels, and   1, . . . ,   m     0,
then k(x, z) =    m   mkm(x, z) is also a kernel.

you can also use this property to show that the following gaus-

sian kernel (also called the rbf kernel) is also psd:

(cid:104)      ||x     z||2(cid:105)

k(rbf)
  

(x, z) = exp

(11.18)

here    is a hyperparameter that controls the width of this gaussian-
like bumps. to gain an intuition for what the rbf kernel is doing,
consider what prediction looks like in the id88:

f (x) =    
n
=    
n

  nk(xn, x) + b

(cid:104)      ||xn     z||2(cid:105)

  n exp

(11.19)

(11.20)

in this computation, each training example is getting to    vote    on the
label of the test point x. the amount of    vote    that the nth training
example gets is proportional to the negative exponential of the dis-
tance between the test point and itself. this is very much like an rbf
neural network, in which there is a gaussian    bump    at each training
example, with variance 1/(2  ), and where the   ns act as the weights
connecting these rbf bumps to the output.

showing that this kernel is positive de   nite is a bit of an exercise

in analysis (particularly, integration by parts), but otherwise not
dif   cult. again, the proof is provided in the appendix.

148 a course in machine learning

so far, you have seen two bsaic classes of kernels: polynomial
kernels (k(x, z) = (1 + x    z)d), which includes the linear kernel
(k(x, z) = x    z) and rbf kernels (k(x, z) = exp[      ||x     z||2]). the
former have a direct connection to feature expansion; the latter to
rbf networks. you also know how to combine kernels to get new
kernels by addition. in fact, you can do more than that: the product
of two kernels is also a kernel.

as far as a    library of kernels    goes, there are many. polynomial
and rbf are by far the most popular. a commonly used, but techni-
cally invalid kernel, is the hyperbolic-tangent kernel, which mimics
the behavior of a two-layer neural network. it is de   ned as:

k(tanh) = tanh(1 + x    z)

warning: not psd

(11.21)

a    nal example, which is not very common, but is nonetheless
interesting, is the all-subsets kernel. suppose that your d features
are all binary: all take values 0 or 1. let a     {1, 2, . . . d} be a subset

of features, and let fa(x) = (cid:86)

d   a xd be the conjunction of all the
features in a. let   (x) be a feature vector over all such as, so that
there are 2d features in the vector   . you can compute the kernel
associated with this feature mapping as:

(cid:16)

(cid:17)

k(subs)(x, z) =    
d

1 + xdzd

(11.22)

verifying the relationship between this kernel and the all-subsets
feature mapping is left as an exercise (but closely resembles the ex-
pansion for the quadratic kernel).

11.5 support vector machines

kernelization predated support vector machines, but id166s are def-
initely the model that popularized the idea. recall the de   nition of
the soft-margin id166 from chapter 7.7 and in particular the opti-
mization problem (7.38), which attempts to balance a large margin
(small ||w||2) with a small loss (small   ns, where   n is the slack on
the nth training example). this problem is repeated below:

  n

(11.23)

min
w,b,  

1
2

||w||2 + c    
n

subj. to yn (w    xn + b)     1       n

  n     0

(   n)
(   n)

previously, you optimized this by explicitly computing the slack
variables   n, given a solution to the decision boundary, w and b.
however, you are now an expert with using lagrange multipliers

kernel methods

149

to optimize constrained problems! the overall goal is going to be to
rewrite the id166 optimization problem in a way that it no longer ex-
plicitly depends on the weights w and only depends on the examples
xn through kernel products.

there are 2n constraints in this optimization, one for each slack

constraint and one for the requirement that the slacks are non-
negative. unlike the last time, these constraints are now inequalities,
which require a slightly different solution. first, you rewrite all the
inequalities so that they read as something     0 and then add cor-
responding lagrange multipliers. the main difference is that the
lagrange multipliers are now constrained to be non-negative, and
their sign in the augmented objective function matters.
the second set of constraints is already in the proper form; the
   rst set can be rewritten as yn (w    xn + b)     1 +   n     0. you   re now
ready to construct the lagrangian, using multipliers   n for the    rst
set of constraints and   n for the second set.

l(w, b,   ,   ,   ) =

1
2

  n        

  n  n

n

  n [yn (w    xn + b)     1 +   n]

||w||2 + c    
n
       
n

the new optimization problem is:

min
w,b,  

max
     0

max
     0

l(w, b,   ,   ,   )

(11.24)

(11.25)

(11.26)

the intuition is exactly the same as before. if you are able to    nd a
solution that satis   es the constraints (e.g., the purple term is prop-
erly non-negative), then the   ns cannot do anything to    hurt    the
solution. on the other hand, if the purple term is negative, then the
corresponding   n can go to +   , breaking the solution.

you can solve this problem by taking gradients. this is a bit te-

dious, but and important step to realize how everything    ts together.
since your goal is to remove the dependence on w, the    rst step is to
take a gradient with respect to w, set it equal to zero, and solve for w
in terms of the other variables.

   wl = w        

n

  nynxn = 0        w =    

n

  nynxn

(11.27)

at this point, you should immediately recognize a similarity to the
kernelized id88: the optimal weight vector takes exactly the
same form in both algorithms.
you can now take this new expression for w and plug it back in to
the expression for l, thus removing w from consideration. to avoid
subscript overloading, you should replace the n in the expression for

(11.28)

(cid:35)

(11.29)

(11.31)
(11.32)

(11.33)

150 a course in machine learning

w with, say, m. this yields:

l(b,   ,   ,   ) =

1
2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)   

m
       
n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:32)(cid:34)

   
m

  mymxm

(cid:34)

  n

yn

+ c    
n

  n        
(cid:35)

n

  n  n

(cid:33)

  mymxm

   xn + b

    1 +   n

at this point, it   s convenient to rewrite these terms; be sure you un-
derstand where the following comes from:

l(b,   ,   ,   ) =

1
2

   
   
n
m
       
n

  n  mynymxn    xm +    
  n  mynymxn    xm        
   
m

n

n

(c       n)  n

(11.30)

  n (ynb     1 +   n)

=     1
  n  mynymxn    xm +    
   
   
2
n
m
   b    
  nyn        
n

  n(  n     1)

n

n

(c       n)  n

things are starting to look good: you   ve successfully removed the de-
pendence on w, and everything is now written in terms of dot prod-
ucts between input vectors! this might still be a dif   cult problem to
solve, so you need to continue and attempt to remove the remaining
variables b and   .

the derivative with respect to b is:
   l
   b

=        
n

  nyn = 0

(11.34)

this doesn   t allow you to substitute b with something (as you did
with w), but it does mean that the fourth term (b    n   nyn) goes to
zero at the optimum.

the last of the original variables is   n; the derivatives in this case

look like:

   l
     n

= c       n       n        c       n =   n

(11.35)

again, this doesn   t allow you to substitute, but it does mean that you
can rewrite the second term, which as    n(c       n)  n as    n   n  n. this
then cancels with (most of) the    nal term. however, you need to be
careful to remember something. when we optimize, both   n and   n
are constrained to be non-negative. what this means is that since we
are dropping    from the optimization, we need to ensure that   n     c,
otherwise the corresponding    will need to be negative, which is not

kernel methods

151

allowed. you    nally wind up with the following, where xn    xm has
been replaced by k(xn, xm):

l(  ) =    
n

  n     1
2

   
n

   
m

  n  mynymk(xn, xm)

(11.36)

if you are comfortable with matrix notation, this has a very compact
form. let 1 denote the n-dimensional vector of all 1s, let y denote
the vector of labels and let g be the n  n matrix, where gn,m =
ynymk(xn, xm), then this has the following form:

l(  ) =   

(cid:62)1     1
2   

(cid:62)g  

(11.37)
the resulting optimization problem is to maximize l(  ) as a function
of   , subject to the constraint that the   ns are all non-negative and
less than c (because of the constraint added when removing the   
variables). thus, your problem is:

min

1
    l(  ) =
2
subj. to 0       n     c

  

   
n

   
m

  n  mynymk(xn, xm)        

n

  n

(11.38)
(   n)

one way to solve this problem is id119 on   . the only
complication is making sure that the   s satisfy the constraints. in
this case, you can use a projected gradient algorithm: after each
gradient update, you adjust your parameters to satisfy the constraints
by projecting them into the feasible region. in this case, the projection
is trivial: if, after a gradient step, any   n < 0, simply set it to 0; if any
  n > c, set it to c.

11.6 understanding support vector machines

the prior discussion involved quite a bit of math to derive a repre-
sentation of the support vector machine in terms of the lagrange
variables. this mapping is actually suf   ciently standard that every-
thing in it has a name. the original problem variables (w, b,   ) are
called the primal variables; the lagrange variables are called the
dual variables. the optimization problem that results after removing
all of the primal variables is called the dual problem.

a succinct way of saying what you   ve done is: you found that after

converting the id166 into its dual, it is possible to kernelize.

to understand id166s, a    rst step is to peek into the dual formula-
tion, eq (11.38). the objective has two terms: the    rst depends on the
data, and the second depends only on the dual variables. the    rst
thing to notice is that, because of the second term, the   s    want    to

152 a course in machine learning

get as large as possible. the constraint ensures that they cannot ex-
ceed c, which means that the general tendency is for the   s to grow
as close to c as possible.

to further understand the dual optimization problem, it is useful

to think of the kernel as being a measure of similarity between two
data points. this analogy is most clear in the case of rbf kernels,
but even in the case of linear kernels, if your examples all have unit
norm, then their dot product is still a measure of similarity. since you
can write the prediction function as f (  x) = sign(   n   nynk(xn,   x)), it
is natural to think of   n as the    importance    of training example n,
where   n = 0 means that it is not used at all at test time.

consider two data points that have the same label; namely, yn =

ym. this means that ynym = +1 and the objective function has a term
that looks like   n  mk(xn, xm). since the goal is to make this term
small, then one of two things has to happen: either k has to be small,
or   n  m has to be small. if k is already small, then this doesn   t affect
the setting of the corresponding   s. but if k is large, then this strongly
encourages at least one of   n or   m to go to zero. so if you have two
data points that are very similar and have the same label, at least one
of the corresponding   s will be small. this makes intuitive sense: if
you have two data points that are basically the same (both in the x
and y sense) then you only need to    keep    one of them around.
suppose that you have two data points with different labels:
ynym =    1. again, if k(xn, xm) is small, nothing happens. but if
it is large, then the corresponding   s are encouraged to be as large as
possible. in other words, if you have two similar examples with dif-
ferent labels, you are strongly encouraged to keep the corresponding
  s as large as c.

an alternative way of understanding the id166 dual problem is
geometrically. remember that the whole point of introducing the
variable   n was to ensure that the nth training example was correctly
classi   ed, modulo slack. more formally, the goal of   n is to ensure
that yn(w    xn + b)     1 +   n     0. suppose that this constraint it
not satis   ed. there is an important result in optimization theory,
called the karush-kuhn-tucker conditions (or kkt conditions, for
short) that states that at the optimum, the product of the lagrange
multiplier for a constraint, and the value of that constraint, will equal
zero. in this case, this says that at the optimum, you have:

yn (w    xn + b)     1 +   n

  n

= 0

(11.39)

(cid:104)

(cid:105)

in order for this to be true, it means that (at least) one of the follow-
ing must be true:

  n = 0

or

yn (w    xn + b)     1 +   n = 0

(11.40)

kernel methods

153

a reasonable question to ask is: under what circumstances will   n
be non-zero? from the kkt conditions, you can discern that   n can
be non-zero only when the constraint holds exactly; namely, that
yn (w    xn + b)     1 +   n = 0. when does that constraint hold ex-
actly? it holds exactly only for those points precisely on the margin of
the hyperplane.

in other words, the only training examples for which   n (cid:54)= 0

are those that lie precisely 1 unit away from the maximum margin
decision boundary! (or those that are    moved    there by the corre-
sponding slack.) these points are called the support vectors because
they    support    the decision boundary. in general, the number of sup-
port vectors is far smaller than the number of training examples, and
therefore you naturally end up with a solution that only uses a subset
of the training data.

from the    rst discussion, you know that the points that wind up
being support vectors are exactly those that are    confusable    in the
sense that you have to examples that are nearby, but have different la-
bels. this is a completely in line with the previous discussion. if you
have a decision boundary, it will pass between these    confusable   
points, and therefore they will end up being part of the set of support
vectors.

11.7 further reading

todo further reading

12 | learning theory

learning objectives:
    explain why inductive bias is

necessary.

    de   ne the pac model and explain

why both the    p    and    a    are
necessary.

    explain the relationship between

complexity measures and regulariz-
ers.

    identify the role of complexity in

generalization.

    formalize the relationship between

margins and complexity.

dependencies:

the universe is under no obligation to make sense to you.
neil degrasse tyson

   

by now, you are an expert at building learning algorithms. you
probably understand how they work, intuitively. and you under-
stand why they should generalize. however, there are several basic
questions you might want to know the answer to. is learning always
possible? how many training examples will i need to do a good job
learning? is my test performance going to be much worse than my
training performance? the key idea that underlies all these answer is
that simple functions generalize well.

the amazing thing is that you can actually prove strong results

that address the above questions. in this chapter, you will learn
some of the most important results in learning theory that attempt
to answer these questions. the goal of this chapter is not theory for
theory   s sake, but rather as a way to better understand why learning
models work, and how to use this theory to build better algorithms.
as a concrete example, we will see how 2-norm id173 prov-
ably leads to better generalization performance, thus justifying our
common practice!

12.1 the role of theory

in contrast to the quote at the start of this chapter, a practitioner
friend once said    i would happily give up a few percent perfor-
mance for an algorithm that i can understand.    both perspectives
are completely valid, and are actually not contradictory. the second
statement is presupposing that theory helps you understand, which
hopefully you   ll    nd to be the case in this chapter.

theory can serve two roles. it can justify and help understand

why common practice works. this is the    theory after    view. it can
also serve to suggest new algorithms and approaches that turn out to
work well in practice. this is the    theory before    view. often, it turns
out to be a mix. practitioners discover something that works surpris-
ingly well. theorists    gure out why it works and prove something
about it. and in the process, they make it better or    nd new algo-

learning theory

155

1 in 2008, corinna cortes and vladimir
vapnik won it for support vector
machines.

rithms that more directly exploit whatever property it is that made
the theory go through.

theory can also help you understand what   s possible and what   s
not possible. one of the    rst things we   ll see is that, in general, ma-
chine learning can not work. of course it does work, so this means
that we need to think harder about what it means for learning algo-
rithms to work. by understanding what   s not possible, you can focus
our energy on things that are.

probably the biggest practical success story for theoretical machine

learning is the theory of boosting, which you won   t actually see in
this chapter. (you   ll have to wait for chapter 13.) boosting is a very
simple style of algorithm that came out of theoretical machine learn-
ing, and has proven to be incredibly successful in practice. so much
so that it is one of the de facto algorithms to run when someone gives
you a new data set. in fact, in 2004, yoav freund and rob schapire
won the acm   s paris kanellakis award for their boosting algorithm
adaboost. this award is given for theoretical accomplishments that
have had a signi   cant and demonstrable effect on the practice of
computing.1

12.2

induction is impossible

one nice thing about theory is that it forces you to be precise about
what you are trying to do. you   ve already seen a formal de   nition
of binary classi   cation in chapter 6. but let   s take a step back and
re-analyze what it means to learn to do binary classi   cation.
from an algorithmic perspective, a natural question is whether
there is an    ultimate    learning algorithm, aawesome, that solves the
binary classi   cation problem above. in other words, have you been
wasting your time learning about knn and id88 and decision
trees, when aawesome is out there.

what would such an ultimate learning algorithm do? you would

like it to take in a data set d and produce a function f . no matter
what d looks like, this function f should get perfect classi   cation on
all future examples drawn from the same distribution that produced
d.

a little bit of introspection should demonstrate that this is impos-
sible. for instance, there might be label noise in our distribution. as
a very simple example, let x = {   1, +1} (i.e., a one-dimensional,
binary distribution. de   ne the data distribution as:

d((cid:104)+1(cid:105), +1) = 0.4
d((cid:104)+1(cid:105),   1) = 0.1

d((cid:104)   1(cid:105),   1) = 0.4
d((cid:104)   1(cid:105), +1) = 0.1

(12.1)
(12.2)

in other words, 80% of data points in this distrubtion have x = y

156 a course in machine learning

and 20% don   t. no matter what function your learning algorithm
produces, there   s no way that it can do better than 20% error on this
data.
given this, it seems hopeless to have an algorithm aawesome that
always achieves an error rate of zero. the best that we can hope is
that the error rate is not    too large.   

unfortunately, simply weakening our requirement on the error
rate is not enough to make learning possible. the second source of
dif   culty comes from the fact that the only access we have to the
data distribution is through sampling. in particular, when trying to
learn about a distribution like that in 12.1, you only get to see data
points drawn from that distribution. you know that    eventually    you
will see enough data points that your sample is representative of the
distribution, but it might not happen immediately. for instance, even
though a fair coin will come up heads only with id203 1/2, it   s
completely plausible that in a sequence of four coin    ips you never
see a tails, or perhaps only see one tails.
aawesome will always work. in particular, if we happen to get a lousy
sample of data from d, we need to allow aawesome to do something
completely unreasonable.
thus, we cannot hope that aawesome will do perfectly, every time.
we cannot even hope that it will do pretty well, all of the time. nor
can we hope that it will do perfectly, most of the time. the best best
we can reasonably hope of aawesome is that it it will do pretty well,
most of the time.

so the second thing that we have to give up is the hope that

12.3 probably approximately correct learning

probably approximately correct (pac) learning is a formalism
of inductive learning based on the realization that the best we can
hope of an algorithm is that it does a good job (i.e., is approximately
correct), most of the time (i.e., it is probably appoximately correct).2

consider a hypothetical learning algorithm. you run it on ten dif-
ferent binary classi   cation data sets. for each one, it comes back with
functions f1, f2, . . . , f10. for some reason, whenever you run f4 on a
test point, it crashes your computer. for the other learned functions,
their performance on test data is always at most 5% error. if this
situtation is guaranteed to happen, then this hypothetical learning
algorithm is a pac learning algorithm. it satis   es    probably    because
it only failed in one out of ten cases, and it   s    approximate    because
it achieved low, but non-zero, error on the remainder of the cases.

this leads to the formal de   nition of an ( ,   ) pac-learning algo-

rithm. in this de   nition,   plays the role of measuring accuracy (in

?

it   s clear that if your algorithm pro-
duces a deterministic function that
it cannot do better than 20% error.
what if it produces a stochastic (aka
randomized) function?

2 leslie valiant invented the notion
of pac learning in 1984. in 2011,
he received the turing award, the
highest honor in computing for his
work in learning theory, computational
complexity and parallel systems.

learning theory

157

the previous example,   = 0.05) and    plays the role of measuring
failure (in the previous,    = 0.1).
de   nitions 1. an algorithm a is an ( ,   )-pac learning algorithm if, for
all distributions d: given samples from d, the id203 that it returns a
   bad function    is at most   ; where a    bad    function is one with test error
rate more than   on d.

there are two notions of ef   ciency that matter in pac learning. the
   rst is the usual notion of computational complexity. you would prefer
an algorithm that runs quickly to one that takes forever. the second
is the notion of sample complexity: the number of examples required
for your algorithm to achieve its goals. note that the goal of both
of these measure of complexity is to bound how much of a scarse
resource your algorithm uses. in the computational case, the resource
is cpu cycles. in the sample case, the resource is labeled examples.
de   nition: an algorithm a is an ef   cient ( ,   )-pac learning al-
gorithm if it is an ( ,   )-pac learning algorithm whose runtime is
polynomial in 1

  and 1
   .

in other words, suppose that you want your algorithm to achieve
4% error rate rather than 5%. the runtime required to do so should
no go up by an exponential factor.

12.4 pac learning of conjunctions

to get a better sense of pac learning, we will start with a completely
irrelevant and uninteresting example. the purpose of this example is
only to help understand how pac learning works.
the setting is learning conjunctions. your data points are binary
vectors, for instance x = (cid:104)0, 1, 1, 0, 1(cid:105). someone guarantees for you
that there is some boolean conjunction that de   nes the true labeling
of this data. for instance, x1       x2     x5 (   or    is not allowed). in
formal terms, we often call the true underlying classi   cation function
the concept. so this is saying that the concept you are trying to learn
is a conjunction. in this case, the boolean function would assign a
negative label to the example above.

since you know that the concept you are trying to learn is a con-
junction, it makes sense that you would represent your function as
a conjunction as well. for historical reasons, the function that you
learn is often called a hypothesis and is often denoted h. however,
in keeping with the other notation in this book, we will continue to
denote it f .
formally, the set up is as follows. there is some distribution dx
over binary data points (vectors) x = (cid:104)x1, x2, . . . , xd(cid:105). there is a    xed

158 a course in machine learning

concept conjunction c that we are trying to learn. there is no noise,
so for any example x, its true label is simply y = c(x).

what is a reasonable algorithm in this case? suppose that you

observe the example in table 12.1. from the    rst example, we know
that the true formula cannot include the term x1. if it did, this exam-
ple would have to be negative, which it is not. by the same reason-
ing, it cannot include x2. by analogous reasoning, it also can neither
include the term   x3 nor the term   x4.

this suggests the algorithm in algorithm 12.4, colloquially the

y
+1
+1
-1

x1
0
0
1

x2
0
1
1

x3
1
1
0

x4
1
1
1

table 12.1: data set for learning con-
junctions.

?

verify that algorithm 12.4 main-
tains an invariant that it always errs
on the side of classifying examples
negative and never errs the other
way.

   throw out bad terms    algorithm. in this algorithm, you begin with
a function that includes all possible 2d terms. note that this function
will initially classify everything as negative. you then process each
example in sequence. on a negative example, you do nothing. on
a positive example, you throw out terms from f that contradict the
given positive example.

if you run this algorithm on the data in table 12.1, the sequence of

f s that you cycle through are:

f 0(x) = x1       x1     x2       x2     x3       x3     x4       x4
f 1(x) =   x1       x2     x3     x4
f 2(x) =   x1     x3     x4
f 3(x) =   x1     x3     x4

(12.3)
(12.4)
(12.5)
(12.6)

the    rst thing to notice about this algorithm is that after processing
an example, it is guaranteed to classify that example correctly. this
observation requires that there is no noise in the data.

the second thing to notice is that it   s very computationally ef-
   cient. given a data set of n examples in d dimensions, it takes
o(nd) time to process the data. this is linear in the size of the data
set.

however, in order to be an ef   cient ( ,   )-pac learning algorithm,
you need to be able to get a bound on the sample complexity of this
algorithm. sure, you know that its run time is linear in the number
of example n. but how many examples n do you need to see in order
to guarantee that it achieves an error rate of at most   (in all but   -
many cases)? perhaps n has to be gigantic (like 22d/ ) to (probably)
guarantee a small error.

the goal is to prove that the number of samples n required to
(probably) achieve a small error is not-too-big. the general proof
technique for this has essentially the same    avor as almost every pac
learning proof around. first, you de   ne a    bad thing.    in this case,
a    bad thing    is that there is some term (say   x8) that should have
been thrown out, but wasn   t. then you say: well, bad things happen.
then you notice that if this bad thing happened, you must not have

learning theory

159

// initialize function

algorithm 31 binaryconjunctiontrain(d)

f     x1       x1     x2       x2                  xd       xd
1:
2: for all positive examples (x,+1) in d do
3:

for d = 1 . . . d do
if xd = 0 then

else

f     f without term    xd   
f     f without term      xd   

4:

5:

6:

7:

8:

end if
end for

9:
10: end for
11: return f

seen any positive training examples with x8 = 0. so example with
x8 = 0 must have low id203 (otherwise you would have seen
them). so bad things must not be that common.
theorem 14. with id203 at least (1       ): algorithm 12.4 requires at
most n = . . . examples to achieve an error rate      .

proof of theorem 14. let c be the concept you are trying to learn and
let d be the distribution that generates the data.

a learned function f can make a mistake if it contains any term t
that is not in c. there are initially 2d many terms in f , and any (or
all!) of them might not be in c. we want to ensure that the id203
that f makes an error is at most  . it is suf   cient to ensure that

for a term t (e.g.,   x5), we say that t    negates    an example x if

t(x) = 0. call a term t    bad    if (a) it does not appear in c and (b) has
id203 at least  /2d of appearing (with respect to the unknown
distribution d over data points).

first, we show that if we have no bad terms left in f , then f has an

error rate at most  .

we know that f contains at most 2d terms, since is begins with 2d

terms and throws them out.

the algorithm begins with 2d terms (one for each variable and

one for each negated variable). note that f will only make one type
of error: it can call positive examples negative, but can never call a
negative example positive. let c be the true concept (true boolean
formula) and call a term    bad    if it does not appear in c. a speci   c
bad term (e.g.,   x5) will cause f to err only on positive examples
that contain a corresponding bad value (e.g., x5 = 1). todo...    nish
this

what we   ve shown in this theorem is that: if the true underly-

ing concept is a boolean conjunction, and there is no noise, then the
   throw out bad terms    algorithm needs n     . . . examples in order

160 a course in machine learning

to learn a boolean conjunction that is (1       )-likely to achieve an er-
ror of at most  . that is to say, that the sample complexity of    throw
out bad terms    is . . . . moreover, since the algorithm   s runtime is
linear in n, it is an ef   cient pac learning algorithm.

12.5 occam   s razor: simple solutions generalize

the previous example of boolean conjunctions is mostly just a warm-
up exercise to understand pac-style proofs in a concrete setting.
in this section, you get to generalize the above argument to a much
larger range of learning problems. we will still assume that there is
no noise, because it makes the analysis much simpler. (don   t worry:
noise will be added eventually.)

william of occam (c. 1288     c. 1348) was an english friar and

philosopher is is most famous for what later became known as oc-
cam   s razor and popularized by bertrand russell. the principle ba-
sically states that you should only assume as much as you need. or,
more verbosely,    if one can explain a phenomenon without assuming
this or that hypothetical entity, then there is no ground for assuming
it i.e. that one should always opt for an explanation in terms of the
fewest possible number of causes, factors, or variables.    what occam
actually wrote is the quote that began this chapter.

in a machine learning context, a reasonable paraphrase is    simple
solutions generalize well.    in other words, you have 10, 000 features
you could be looking at. if you   re able to explain your predictions
using just 5 of them, or using all 10, 000 of them, then you should just
use the 5.

the occam   s razor theorem states that this is a good idea, theo-
retically. it essentially states that if you are learning some unknown
concept, and if you are able to    t your training data perfectly, but you
don   t need to resort to a huge class of possible functions to do so,
then your learned function will generalize well. it   s an amazing theo-
rem, due partly to the simplicity of its proof. in some ways, the proof
is actually easier than the proof of the boolean conjunctions, though it
follows the same basic argument.

in order to state the theorem explicitly, you need to be able to

think about a hypothesis class. this is the set of possible hypotheses
that your algorithm searches through to    nd the    best    one. in the
case of the boolean conjunctions example, the hypothesis class, h,
is the set of all boolean formulae over d-many variables. in the case
of a id88, your hypothesis class is the set of all possible linear
classi   ers. the hypothesis class for boolean conjunctions is    nite; the
hypothesis class for linear classi   ers is in   nite. for occam   s razor, we
can only work with    nite hypothesis classes.

learning theory

161

figure 12.1: thy:dt: picture of full
decision tree

theorem 15 (occam   s bound). suppose a is an algorithm that learns
a function f from some    nite hypothesis class h. suppose the learned
function always gets zero error on the training data. then, the sample com-
plexity of f is at most log|h|.

todo comments

proof of theorem 15. todo

this theorem applies directly to the    throw out bad terms    algo-
rithm, since (a) the hypothesis class is    nite and (b) the learned func-
tion always achieves zero error on the training data. to apply oc-
cam   s bound, you need only compute the size of the hypothesis class
h of boolean conjunctions. you can compute this by noticing that
there are a total of 2d possible terms in any formula in h. moreover,
each term may or may not be in a formula. so there are 22d = 4d
possible formulae; thus, |h| = 4d. applying occam   s bound, we see
that the sample complexity of this algorithm is n     . . . .

of course, occam   s bound is general enough to capture other
learning algorithms as well. in particular, it can capture decision
trees! in the no-noise setting, a decision tree will always    t the train-
ing data perfectly. the only remaining dif   culty is to compute the
size of the hypothesis class of a decision tree learner.

for simplicity   s sake, suppose that our decision tree algorithm
always learns complete trees: i.e., every branch from root to leaf
is length d. so the number of split points in the tree (i.e., places
where a feature is queried) is 2d   1. (see figure 12.1.) each split
point needs to be assigned a feature: there d-many choices here.
this gives d2d   1 trees. the last thing is that there are 2d leaves
of the tree, each of which can take two possible values, depending
on whether this leaf is classi   ed as +1 or    1: this is 2  2d = 2d+1
possibilities. putting this all togeter gives a total number of trees
|h| = d2d   12d+1 = d22d = d4d. applying occam   s bound, we see
that todo examples is enough to learn a decision tree!

12.6 complexity of in   nite hypothesis spaces

occam   s bound is a fantastic result for learning over    nite hypothesis
spaces. unfortunately, it is completely useless when |h| =    . this is
because the proof works by using each of the n training examples to
   throw out    bad hypotheses until only a small number are left. but if
|h| =    , and you   re throwing out a    nite number at each step, there
will always be an in   nite number remaining.

this means that, if you want to establish sample complexity results
for in   nite hypothesis spaces, you need some new way of measuring

162 a course in machine learning

their    size    or    complexity.    a prototypical way of doing this is to
measure the complexity of a hypothesis class as the number of different
things it can do.

as a silly example, consider boolean conjunctions again. your

input is a vector of binary features. however, instead of representing
your hypothesis as a boolean conjunction, you choose to represent
it as a conjunction of inequalities. that is, instead of writing x1    
  x2     x5, you write [x1 > 0.2]     [x2 < 0.77]     [x5 <   /4]. in this
representation, for each feature, you need to choose an inequality
(< or >) and a threshold. since the thresholds can be arbitrary real
values, there are now in   nitely many possibilities: |h| = 2d      =    .
however, you can immediately recognize that on binary features,
there really is no difference between [x2 < 0.77] and [x2 < 0.12] and
any other number of in   nitely many possibilities. in other words,
even though there are in   nitely many hypotheses, there are only    nitely
many behaviors.

the vapnik-chernovenkis dimension (or vc dimension) is a

classic measure of complexity of in   nite hypothesis classes based on
this intuition3. the vc dimension is a very classi   cation-oriented no-
tion of complexity. the idea is to look at a    nite set of unlabeled ex-
amples, such as those in figure 12.2. the question is: no matter how
these points were labeled, would we be able to    nd a hypothesis that
correctly classi   es them. the idea is that as you add more points,
being able to represent an arbitrary labeling becomes harder and
harder. for instance, regardless of how the three points are labeled,
you can    nd a linear classi   er that agrees with that classi   cation.
however, for the four points, there exists a labeling for which you
cannot    nd a perfect classi   er. the vc dimension is the maximum
number of points for which you can always    nd such a classi   er.
you can think of vc dimension as a game between you and an

adversary. to play this game, you choose k unlabeled points however
you want. then your adversary looks at those k points and assigns
binary labels to them them however they want. you must then    nd
a hypothesis (classi   er) that agrees with their labeling. you win if
you can    nd such a hypothesis; they win if you cannot. the vc
dimension of your hypothesis class is the maximum number of points
k so that you can always win this game. this leads to the following
formal de   nition, where you can interpret there exists as your move
and for all as adversary   s move.

de   nitions 2. for data drawn from some space x , the vc dimension of
a hypothesis space h over x is the maximal k such that: there exists a set
x     x of size |x| = k, such that for all binary labelings of x, there exists
a function f     h that matches this labeling.

figure 12.2: thy:vcex:    gure with three
and four examples
3 yes, this is the same vapnik who
is credited with the creation of the
support vector machine.

?

what is that labeling? what is it   s
name?

learning theory

163

in general, it is much easier to show that the vc dimension is at
least some value; it is much harder to show that it is at most some
value. for example, following on the example from figure 12.2, the
image of three points (plus a little argumentation) is enough to show
that the vc dimension of linear classi   ers in two dimension is at least
three.

to show that the vc dimension is exactly three it suf   ces to show
that you cannot    nd a set of four points such that you win this game
against the adversary. this is much more dif   cult. in the proof that
the vc dimension is at least three, you simply need to provide an
example of three points, and then work through the small number of
possible labelings of that data. to show that it is at most three, you
need to argue that no matter what set of four point you pick, you
cannot win the game.

12.7 further reading

todo

13 | ensemble methods

learning objectives:
    implement id112 and explain how

it reduces variance in a predictor.
    explain the difference between a

weak learner and a strong learner.

    derive the adaboost algorithm.
    understand the relationship between
boosting decision stumps and linear
classi   cation.

dependencies:

this is the central illusion in life: that randomness is a risk, that it
is a bad thing. . .

    nassim nicholas taleb

groups of people can often make better decisions than
individuals, especially when group members each come in with
their own biases. the same is true in machine learning. ensemble
methods are learning models that achieve performance by combining
the opinions of multiple learners. in doing so, you can often get away
with using much simpler learners and still achieve great performance.
moreover, ensembles are inherantly parallel, which can make them
much more ef   cient at training and test time, if you have access to
multiple processors.

in this chapter, you will learn about various ways of combining
base learners into ensembles. one of the shocking results we will
see is that you can take a learning model that only ever does slightly
better than chance, and turn it into an arbitrarily good learning
model, though a technique known as boosting. you will also learn
how ensembles can decrease the variance of predictors as well as
perform id173.

13.1 voting multiple classi   ers

all of the learning algorithms you have seen so far are deterministic.
if you train a decision tree multiple times on the same data set, you
will always get the same tree back. in order to get an effect out of
voting multiple classi   ers, they need to differ. there are two primary
ways to get variability. you can either change the learning algorithm
or change the data set.

building an emsemble by training different classi   ers is the most

straightforward approach. as in single-model learning, you are given
a data set (say, for classi   cation). instead of learning a single classi   er
(e.g., a decision tree) on this data set, you learn multiple different
classi   ers. for instance, you might train a decision tree, a id88,
a knn, and multiple neural networks with different architectures.
call these classi   ers f1, . . . , fm. at test time, you can make a predic-
tion by voting. on a test example   x, you compute   y1 = f1(   x), . . . ,

ensemble methods

165

?

which of the classi   ers you   ve
learned about so far have high
variance?

figure 13.1: picture of sampling with
replacement
1 to sample with replacement, imagine
putting all items from d in a hat. to
draw a single sample, pick an element
at random from that hat, write it down,
and then put it back.

  ym = fm(   x). if there are more +1s in the list (cid:104)y1, . . . , ym then you
predict +1; otherwise you predict    1.

the main advantage of ensembles of different classi   ers is that it
is unlikely that all classi   ers will make the same mistake. in fact, as
long as every error is made by a minority of the classi   ers, you will
achieve optimal classi   cation! unfortunately, the inductive biases of
different learning algorithms are highly correlated. this means that
different algorithms are prone to similar types of errors. in particular,
ensembles tend to reduce the variance of classi   ers. so if you have
a classi   cation algorithm that tends to be very sensitive to small
changes in the training data, ensembles are likely to be useful.

note that the voting scheme naturally extends to multiclass clas-
si   cation. however, it does not make sense in the contexts of regres-
sion, ranking or collective classi   cation. this is because you will
rarely see the same exact output predicted twice by two different
regression models (or ranking models or collective classi   cation mod-
els). for regression, a simple solution is to take the mean or median
prediction from the different models. for ranking and collective clas-
si   cation, different approaches are required.

instead of training different types of classi   ers on the same data

set, you can train a single type of classi   er (e.g., decision tree) on
multiple data sets. the question is: where do these multiple data sets
come from, since you   re only given one at training time?

one option is to fragment your original data set. for instance, you
could break it into 10 pieces and build id90 on each of these
pieces individually. unfortunately, this means that each decision tree
is trained on only a very small part of the entire data set and is likely
to perform poorly.

a better solution is to use bootstrap resampling. this is a tech-
nique from the statistics literature based on the following observa-
tion. the data set we are given, d, is a sample drawn i.i.d. from an
unknown distribution d. if we draw a new data set   d by random
sampling from d with replacement1, then   d is also a sample from d.
figure 13.1 shows the process of bootstrap resampling of ten objects.
applying this idea to ensemble methods yields a technique known

as id112. you start with a single data set d that contains n train-
ing examples. from this single data set, you create m-many    boot-
strapped training sets      d1, . . .   dm. each of these bootstrapped sets
also contains n training examples, drawn randomly from d with
replacement. you can then train a decision tree (or other model)
seperately on each of these data sets to obtain classi   ers f1, . . . , fm.
as before, you can use these classi   ers to vote on new test points.

note that the bootstrapped data sets will be similar. however, they
will not be too similar. for example, if n is large then the number of

figure 13.2: graph depicting over   tting
using id173 versus id112

166 a course in machine learning

examples that are not present in any particular bootstrapped sample
is relatively large. the id203 that the    rst training example is
not selected once is (1     1/n). the id203 that it is not selected
at all is (1     1/n)n. as n        , this tends to 1/e     0.3679. (already
for n = 1000 this is correct to four decimal points.) so only about
63% of the original training examples will be represented in any
given bootstrapped set.

since id112 tends to reduce variance, it provides an alternative
approach to id173. that is, even if each of the learned clas-
si   ers f1, . . . , fm are individually over   t, they are likely to be over   t
to different things. through voting, you are able to overcome a sig-
ni   cant portion of this over   tting. figure 13.2 shows this effect by
comparing id173 via hyperparameters to id173 via
id112.

13.2 boosting weak learners

boosting is the process of taking a crummy learning algorithm (tech-
nically called a weak learner) and turning it into a great learning
algorithm (technically, a strong learner). of all the ideas that origi-
nated in the theoretical machine learning community, boosting has
had   perhaps   the greatest practical impact. the idea of boosting
is reminiscent of what you (like me!) might have thought when you
   rst learned about    le compression. if i compress a    le, and then
re-compress it, and then re-compress it, eventually i   ll end up with a
   nal that   s only one byte in size!
to be more formal, let   s de   ne a strong learning algorithm l as
follows. when given a desired error rate  , a failure id203   
and access to    enough    labeled examples from some distribution d,
then, with high id203 (at least 1       ), l learns a classi   er f that
has error at most  . this is precisely the de   nition of pac learning
that you learned about in chapter 12. building a strong learning
algorithm might be dif   cult. we can as if, instead, it is possible to
build a weak learning algorithm w that only has to achieve an error
rate of 49%, rather than some arbitrary user-de   ned parameter  .
(49% is arbitrary: anything strictly less than 50% would be    ne.)
work for taking a weak learning algorithm w and turning it into a
strong learning algorithm. the particular boosting algorithm dis-
cussed here is adaboost, short for    adaptive boosting algorithm.   
adaboost is famous because it was one of the    rst practical boosting
algorithms: it runs in polynomial time and does not require you to
de   ne a large number of hyperparameters. it gets its name from the
latter bene   t: it automatically adapts to the data that you give it.

boosting is more of a    framework    than an algorithm. it   s a frame-

ensemble methods

167

?

what happens if the weak learn-
ing assumption is violated and     is
equal to 50%? what if it is worse
than 50%? what does this mean, in
practice?

n , 1

algorithm 32 adaboost(w, d, k)
1: d(0)     (cid:104) 1
2: for k = 1 . . . k do
3:

n(cid:105)
n , . . . , 1
f (k)     w (d, d(k-1))
  yn     f (k)(xn),    n
(cid:17)
(cid:16) 1       (k)
   (k)        n d(k-1)
[yn (cid:54)=   yn]
  (k)     1
exp[     (k)yn   yn],    n
n     1
d(k)

9: return f (  x) = sgn(cid:2)   k   (k) f (k)(  x)(cid:3)

n
2 log
z d(k-1)
n

7:
8: end for

4:

5:

6:

   (k)

// initialize uniform importance to each example

// train kth classi   er on weighted data
// make predictions on training data
// compute weighted training error
// compute    adaptive    parameter

// re-weight examples and normalize

// return (weighted) voted classi   er

the intuition behind adaboost is like studying for an exam by

using a past exam. you take the past exam and grade yourself. the
questions that you got right, you pay less attention to. those that you
got wrong, you study more. then you take the exam again and repeat
this process. you continually down-weight the importance of questions
you routinely answer correctly and up-weight the importance of ques-
tions you routinely answer incorrectly. after going over the exam
multiple times, you hope to have mastered everything.

the precise adaboost training algorithm is shown in algorithm 13.2.

the basic functioning of the algorithm is to maintain a weight dis-
tribution d, over data points. a weak learner, f (k) is trained on this
weighted data. (note that we implicitly assume that our weak learner
can accept weighted training data, a relatively mild assumption that
is nearly always true.) the (weighted) error rate of f (k) is used to de-
termine the adaptive parameter   , which controls how    important    f (k)
is. as long as the weak learner does, indeed, achieve < 50% error,
then    will be greater than zero. as the error drops to zero,    grows
without bound.

after the adaptive parameter is computed, the weight distibution
is updated for the next iteration. as desired, examples that are cor-
rectly classi   ed (for which yn   yn = +1) have their weight decreased
multiplicatively. examples that are incorrectly classi   ed (yn   yn =    1)
have their weight increased multiplicatively. the z term is a nom-
ralization constant to ensure that the sum of d is one (i.e., d can be
interpreted as a distribution). the    nal classi   er returned by ad-
aboost is a weighted vote of the individual classi   ers, with weights
given by the adaptive parameters.

to better understand why    is de   ned as it is, suppose that our

weak learner simply returns a constant function that returns the
(weighted) majority class. so if the total weight of positive exam-
ples exceeds that of negative examples, f (x) = +1 for all x; otherwise
f (x) =    1 for all x. to make the problem moderately interesting,
suppose that in the original training set, there are 80 positive ex-

168 a course in machine learning

2 log 4] = 1

2 log 4] = 2. we can compute z = 80  1

amples and 20 negative examples. in this case, f (1)(x) = +1. it   s
weighted error rate will be    (1) = 0.2 because it gets every negative
example wrong. computing, we get   (1) = 1
2 log 4. before normaliza-
tion, we get the new weight for each positive (correct) example to be
1 exp[    1
2. the weight for each negative (incorrect) example
2 + 20  2 = 80.
becomes 1 exp[ 1
therefore, after id172, the weight distribution on any single
positive example is 1
160 and the weight on any negative example is 1
40.
however, since there are 80 positive examples and 20 negative exam-
ples, the cumulative weight on all positive examples is 80   1
160 = 1
2;
the cumulative weight on all negative examples is 20   1
40 = 1
2. thus,
after a single boosting iteration, the data has become precisely evenly
weighted. this guarantees that in the next iteration, our weak learner
must do something more interesting than majority voting if it is to
achieve an error rate less than 50%, as required.

one of the major attractions of boosting is that it is perhaps easy
to design computationally ef   cient weak learners. a very popular
type of weak learner is a shallow decision tree: a decision tree with a
small depth limit. figure 13.3 shows test error rates for id90
of different maximum depths (the different curves) run for differing
numbers of boosting iterations (the x-axis). as you can see, if you
are willing to boost for many iterations, very shallow trees are quite
effective.

in fact, a very popular weak learner is a decision decision stump:
a decision tree that can only ask one question. this may seem like a
silly model (and, in fact, it is on it   s own), but when combined with
boosting, it becomes very effective. to understand why, suppose for
a moment that our data consists only of binary features, so that any
question that a decision tree might ask is of the form    is feature 5
on?    by concentrating on decision stumps, all weak functions must
have the form f (x) = s(2xd     1), where s     {  1} and d indexes some
feature.

now, consider the    nal form of a function learned by adaboost.

?

this example uses concrete num-
bers, but the same result holds no
matter what the data distribution
looks like nor how many examples
there are. write out the general case
to see that you will still arrive at an
even weighting after one iteration.

we can expand it as follow, where we let fk denote the single feature
selected by the kth decision stump and let sk denote its sign:

(cid:35)

(cid:34)
(cid:34)
(cid:34)

   
k
   
k
   
k

f (x) = sgn

= sgn

  k f (k)(x)

  ksk(2x fk     1)

(cid:35)

(cid:35)

2  kskx fk        

  ksk

k

= sgn
= sgn [w    x + b]

figure 13.3: perf comparison of depth
vs # boost
?

why do the functions have this
form?

(13.1)

(13.2)

(13.3)

(13.4)

ensemble methods

169

algorithm 33 randomforesttrain(d, depth, k)
1: for k = 1 . . . k do
2:

t(k)     complete binary tree of depth depth with random feature splits
f (k)     the function computed by t(k), with leaves    lled in by d

3:
4: end for

5: return f (  x) = sgn(cid:2)   k f (k)(  x)(cid:3)

// return voted classi   er

where wd =    
k: fk=d

2  ksk

and b =        
k

  ksk

(13.5)

thus, when working with decision stumps, adaboost actually pro-
vides an algorithm for learning linear classi   ers! in fact, this con-
nection has recently been strengthened: you can show that adaboost
provides an algorithm for optimizing exponential loss. (however,
this connection is beyond the scope of this book.)

as a further example, consider the case of boosting a linear classi-
   er. in this case, if we let the kth weak classi   er be parameterized by
w(k) and b(k), the overall predictor will have the form:

(cid:34)

(cid:16)

w(k)    x + b(k)(cid:17)(cid:35)

f (x) = sgn

   
k

  ksgn

(13.6)

you can notice that this is nothing but a two-layer neural network,
with k-many hidden units! of course it   s not a classi   cally trained
neural network (once you learn w(k) you never go back and update
it), but the structure is identical.

13.3 random ensembles

one of the most computationally expensive aspects of ensembles of
id90 is training the id90. this is very fast for de-
cision stumps, but for deeper trees it can be prohibitively expensive.
the expensive part is choosing the tree structure. once the tree struc-
ture is chosen, it is very cheap to    ll in the leaves (i.e., the predictions
of the trees) using the training data.

an ef   cient and surprisingly effective alternative is to use trees
with    xed structures and random features. collections of trees are
called forests, and so classi   ers built like this are called random
forests. the id79 training algorithm, shown in algo-
rithm 13.3 is quite short. it takes three arguments: the data, a desired
depth of the id90, and a number k of total id90 to
build.

the algorithm generates each of the k trees independently, which

makes it very easy to parallelize. for each trees, it constructs a full
binary tree of depth depth. the features used at the branches of this

170 a course in machine learning

tree are selected randomly, typically with replacement, meaning that
the same feature can appear multiple times, even in one branch. the
leaves of this tree, where predictions are made, are    lled in based on
the training data. this last step is the only point at which the training
data is used. the resulting classi   er is then just a voting of the k-
many random trees.

the most amazing thing about this approach is that it actually

works remarkably well. it tends to work best when all of the features
are at least marginally relevant, since the number of features selected
for any given tree is small. an intuitive reason that it works well
is the following. some of the trees will query on useless features.
these trees will essentially make random predictions. but some
of the trees will happen to query on good features and will make
good predictions (because the leaves are estimated based on the
training data). if you have enough trees, the random ones will wash
out as noise, and only the good trees will have an effect on the    nal
classi   cation.

13.4 further reading

todo further reading

14 | efficient learning

learning objectives:
    understand and be able to imple-
ment stochastic id119
algorithms.

    compare and contrast small ver-
sus large batch sizes in stochastic
optimization.

    derive subgradients for sparse

regularizers.

    implement feature hashing.

dependencies:

one essential object is to choose that arrangement which shall
tend to reduce to a minimum the time necessary for completing
the calculation.

    ada lovelace

so far, our focus has been on models of learning and basic al-
gorithms for those models. we have not placed much emphasis on
how to learn quickly. the basic techniques you learned about so far
are enough to get learning algorithms running on tens or hundreds
of thousands of examples. but if you want to build an algorithm for
web page ranking, you will need to deal with millions or billions
of examples, in hundreds of thousands of dimensions. the basic
approaches you have seen so far are insuf   cient to achieve such a
massive scale.

in this chapter, you will learn some techniques for scaling learning

algorithms. this are useful even when you do not have billions of
training examples, because it   s always nice to have a program that
runs quickly. you will see techniques for speeding up both model
training and model prediction. the focus in this chapter is on linear
models (for simplicity), but most of what you will learn applies more
generally.

14.1 what does it mean to be fast?

everyone always wants fast algorithms. in the context of machine
learning, this can mean many things. you might want fast training
algorithms, or perhaps training algorithms that scale to very large
data sets (for instance, ones that will not    t in main memory). you
might want training algorithms that can be easily parallelized. or,
you might not care about training ef   ciency, since it is an of   ine
process, and only care about how quickly your learned functions can
make classi   cation decisions.

it is important to separate out these desires. if you care about

ef   ciency at training time, then what you are really asking for are
more ef   cient learning algorithms. on the other hand, if you care
about ef   ciency at test time, then you are asking for models that can
be quickly evaluated.

one issue that is not covered in this chapter is parallel learning.

172 a course in machine learning

this is largely because it is currently not a well-understood area in
machine learning. there are many aspects of parallelism that come
into play, such as the speed of communication across the network,
whether you have shared memory, etc. right now, this the general,
poor-man   s approach to parallelization, is to employ ensembles.

14.2 stochastic optimization

during training of most learning algorithms, you consider the entire
data set simultaneously. this is certainly true of id119
algorithms for regularized linear classi   ers (recall algorithm 7.4), in
which you    rst compute a gradient over the entire training data (for
simplicity, consider the unbiased case):

g =    
n

   w(cid:96)(yn, w    xn) +   w

(14.1)

where (cid:96)(y,   y) is some id168. then you update the weights by
w     w       g. in this algorithm, in order to make a single update, you
have to look at every training example.

when there are billions of training examples, it is a bit silly to look

at every one before doing anything. perhaps just on the basis of the
   rst few examples, you can already start learning something!

stochastic optimization involves thinking of your training data
as a big distribution over examples. a draw from this distribution
corresponds to picking some example (uniformly at random) from
your data set. viewed this way, the optimization problem becomes a
stochastic optimization problem, because you are trying to optimize
some function (say, a regularized linear classi   er) over a id203
distribution. you can derive this intepretation directly as follows:

w    = arg max

w

= arg max

w

= arg max

w

   
n

   
n

   
n

(cid:96)(yn, w    xn) + r(w)
(cid:20)
(cid:20) 1

(cid:96)(yn, w    xn) +

1
n

(cid:96)(yn, w    xn) +
(cid:20)

(cid:96)(y, w    x) +

n

(cid:21)

r(w)

(cid:21)
(cid:21)

1
n2 r(w)

1
n

r(w)

= arg max

w

e

(y,x)   d

where d is the training data distribution

de   nition

(14.2)

move r inside sum
(14.3)

divide through by n
(14.4)

write as expectation
(14.5)
(14.6)

given this framework, you have the following general form of an

efficient learning 173

algorithm 34 stochasticgradientdescent(f, d, s, k,   1, . . . )
1: z(0)     (cid:104)0, 0, . . . , 0(cid:105)
2: for k = 1 . . . k do
3:

g(k)        zf (d(k))(cid:12)(cid:12)z(k-1)

d(k)     s-many random data points from d
z(k)     z(k-1)       (k)g(k)

// compute gradient on sample
// take a step down the gradient

// initialize variable we are optimizing

4:

5:
6: end for
7: return z(k)

optimization problem:
   [f (z,   )]

min

e

z

(14.7)

in the example,    denotes the random choice of examples over the
dataset, z denotes the weight vector and f (w,   ) denotes the loss on
that example plus a fraction of the regularizer.

stochastic optimization problems are formally harder than regu-
lar (deterministic) optimization problems because you do not even
get access to exact function values and gradients. the only access
you have to the function f that you wish to optimize are noisy mea-
surements, governed by the distribution over   . despite this lack of
information, you can still run a gradient-based algorithm, where you
simply compute local gradients on a current sample of data.

more precisely, you can draw a data point at random from your
data set. this is analogous to drawing a single value    from its
distribution. you can compute the gradient of f just at that point.
in this case of a 2-norm regularized linear model, this is simply
g =    w(cid:96)(y, w    x) + 1
n w, where (y, x) is the random point you
selected. given this estimate of the gradient (it   s an estimate because
it   s based on a single random draw), you can take a small gradient
step w     w       g.

this is the stochastic id119 algorithm (sgd). in prac-

tice, taking gradients with respect to a single data point might be
too myopic. in such cases, it is useful to use a small batch of data.
here, you can draw 10 random examples from the training data
and compute a small gradient (estimate) based on those examples:
g =    10
n w, where you need to include 10
counts of the regularizer. popular batch sizes are 1 (single points)
and 10. the generic sgd algorithm is depicted in algorithm 14.2,
which takes k-many steps over batches of s-many examples.

m=1    w(cid:96)(ym, w    xm) + 10

in stochastic id119, it is imperative to choose good step
sizes. it is also very important that the steps get smaller over time at
a reasonable slow rate. in particular, convergence can be guaranteed
for learning rates of the form:   (k) =   0   
k , where   0 is a    xed, initial
step size, typically 0.01, 0.1 or 1 depending on how quickly you ex-

174 a course in machine learning

pect the algorithm to converge. unfortunately, in comparisong to
id119, stochastic gradient is quite sensitive to the selection
of a good learning rate.

there is one more practical issues related to the use of sgd as a
learning algorithm: do you really select a random point (or subset
of random points) at each step, or do you stream through the data
in order. the answer is akin to the answer of the same question for
the id88 algorithm (chapter 4). if you do not permute your
data at all, very bad things can happen. if you do permute your data
once and then do multiple passes over that same permutation, it
will converge, but more slowly. in theory, you really should permute
every iteration. if your data is small enough to    t in memory, this
is not a big deal: you will only pay for cache misses. however, if
your data is too large for memory and resides on a magnetic disk
that has a slow seek time, randomly seeking to new data points for
each example is prohibitivly slow, and you will likely need to forgo
permuting the data. the speed hit in convergence speed will almost
certainly be recovered by the speed gain in not having to seek on disk
routinely. (note that the story is very different for solid state disks,
on which random accesses really are quite ef   cient.)

14.3 sparse id173

for many learning algorithms, the test-time ef   ciency is governed
by how many features are used for prediction. this is one reason de-
cision trees tend to be among the fastest predictors: they only use a
small number of features. especially in cases where the actual com-
putation of these features is expensive, cutting down on the number
that are used at test time can yield huge gains in ef   ciency. moreover,
the amount of memory used to make predictions is also typically
governed by the number of features. (note: this is not true of kernel
methods like support vector machines, in which the dominant cost is
the number of support vectors.) furthermore, you may simply believe
that your learning problem can be solved with a very small number
of features: this is a very reasonable form of inductive bias.

this is the idea behind sparse models, and in particular, sparse
regularizers. one of the disadvantages of a 2-norm regularizer for
linear models is that they tend to never produce weights that are
exactly zero. they get close to zero, but never hit it. to understand
why, as a weight wd approaches zero, its gradient also approaches
zero. thus, even if the weight should be zero, it will essentially never
get there because of the constantly shrinking gradient.

this suggests that an alternative regularizer is required to yield a
sparse inductive bias. an ideal case would be the zero-norm regular-

efficient learning 175

izer, which simply counts the number of non-zero values in a vector:
||w||0 =    d[wd (cid:54)= 0]. if you could minimize this regularizer, you
would be explicitly minimizing the number of non-zero features. un-
fortunately, not only is the zero-norm non-convex, it   s also discrete.
optimizing it is np-hard.
a reasonable middle-ground is the one-norm: ||w||1 =    d |wd|.
it is indeed convex: in fact, it is the tighest (cid:96)p norm that is convex.
moreover, its gradients do not go to zero as in the two-norm. just as
hinge-loss is the tightest convex upper bound on zero-one error, the
one-norm is the tighest convex upper bound on the zero-norm.

at this point, you should be content. you can take your subgradi-
ent optimizer for arbitrary functions and plug in the one-norm as a
regularizer. the one-norm is surely non-differentiable at wd = 0, but
you can simply choose any value in the range [   1, +1] as a subgradi-
ent at that point. (you should choose zero.)

unfortunately, this does not quite work the way you might expect.

the issue is that the gradient might    overstep    zero and you will
never end up with a solution that is particularly sparse. for example,
at the end of one gradient step, you might have w3 = 0.6. your
gradient might have g6 = 0.8 and your gradient step (assuming
   = 1) will update so that the new w3 =    0.2. in the subsequent
iteration, you might have g6 =    0.3 and step to w3 = 0.1.

this observation leads to the idea of trucated gradients. the idea

is simple: if you have a gradient that would step you over wd = 0,
then just set wd = 0. in the easy case when the learning rate is 1, this
means that if the sign of wd     gd is different than the sign of wd then
you truncate the gradient step and simply set wd = 0. in other words,
gd should never be larger than wd once you incorporate learning
rates, you can express this as:

                gd

gd
0

gd    

if wd > 0 and gd     1
if wd < 0 and gd     1
otherwise

  (k) wd
  (k) wd

(14.8)

this works quite well in the case of subid119. it works
somewhat less well in the case of stochastic subid119. the
problem that arises in the stochastic case is that wherever you choose
to stop optimizing, you will have just touched a single example (or
small batch of examples), which will increase the weights for a lot of
features, before the regularizer    has time    to shrink them back down
to zero. you will still end up with somewhat sparse solutions, but not
as sparse as they could be. there are algorithms for dealing with this
situation, but they all have a heuristic    avor to them and are beyond
the scope of this book.

176 a course in machine learning

14.4 feature hashing

as much as speed is a bottleneck in prediction, so often is memory
usage. if you have a very large number of features, the amount of
memory that it takes to store weights for all of them can become
prohibitive, especially if you wish to run your algorithm on small de-
vices. feature hashing is an incredibly simple technique for reducing
the memory footprint of linear models, with very small sacri   ces in
accuracy.

the basic idea is to replace all of your features with hashed ver-
sions of those features, thus reducing your space from d-many fea-
ture weights to p-many feature weights, where p is the range of
the hash function. you can actually think of hashing as a (random-
ized) feature mapping    : rd     rp, for some p (cid:28) d. the idea
is as follows. first, you choose a hash function h whose domain is
[d] = {1, 2, . . . , d} and whose range is [p]. then, when you receive a
feature vector x     rd, you map it to a shorter feature vector   x     rp.
algorithmically, you can think of this mapping as follows:

1. initialize   x = (cid:104)0, 0, . . . , 0(cid:105)

2. for each d = 1 . . . d:

(a) hash d to position p = h(d)
(b) update the pth position by adding xd:   xp       xp + xd

3. return   x

mathematically, the mapping looks like:

  (x)p =    
d

[h(d) = p]xd =    

d   h   1(p)

xd

where h   1(p) = {d : h(d) = p}.

(14.9)

in the (unrealistic) case where p = d and h simply encodes a per-

mutation, then this mapping does not change the learning problem
at all. all it does is rename all of the features. in practice, p (cid:28) d
and there will be collisions. in this context, a collision means that
two features, which are really different, end up looking the same to
the learning algorithm. for instance,    is it sunny today?    and    did
my favorite sports team win last night?    might get mapped to the
same location after hashing. the hope is that the learning algorithm
is suf   ciently robust to noise that it can handle this case well.

consider the kernel de   ned by this hash mapping. namely:
k(hash)(x, z) =   (x)      (z)

(cid:33)(cid:32)

(cid:33)

[h(d) = p]zd

(cid:32)

   
d

[h(d) = p]xd

   
d
[h(d) = p][h(e) = p]xdze
   

xdze

   
d,e

=    
p
=    
p
=    
d

e   h   1(h(d))

= x    z +    
d

   
e(cid:54)=d,

e   h   1(h(d))

xdze

efficient learning 177

(14.10)

(14.11)

(14.12)

(14.13)

(14.14)

this hash kernel has the form of a linear kernel plus a small number
of quadratic terms. the particular quadratic terms are exactly those
given by collisions of the hash function.

there are two things to notice about this. the    rst is that collisions

might not actually be bad things! in a sense, they   re giving you a
little extra representational power. in particular, if the hash function
happens to select out feature pairs that bene   t from being paired,
then you now have a better representation. the second is that even if
this doesn   t happen, the quadratic term in the kernel has only a small
effect on the overall prediction. in particular, if you assume that your
hash function is pairwise independent (a common assumption of
hash functions), then the expected value of this quadratic term is zero,
and its variance decreases at a rate of o(p   2). in other words, if you
choose p     100, then the variance is on the order of 0.0001.

14.5 further reading

todo further reading

15 | unsupervised learning

learning objectives:
    explain the difference between

linear and non-linear dimensionality
reduction.

    relate the view of pca as maximiz-

ing variance with the view of it as
minimizing reconstruction error.

    implement latent semantic analysis

for text data.

    motivate manifold learning from the

perspective of reconstruction error.
    understand id116 id91 as

distance minimization.

    explain the importance of initial-

ization in id116 and furthest-   rst
heuristic.

    implement agglomerative id91.
    argue whether spectral cluster-
ing is a id91 algorithm or a
id84 algorithm.

dependencies:

[my father] advised me to sit every few months in my reading
chair for an entire evening, close my eyes and try to think of new
problems to solve. i took his advice very seriously and have been
glad ever since that he did.
    luis walter alvarez

if you have access to labeled training data, you know what
to do. this is the    supervised    setting, in which you have a teacher
telling you the right answers. unfortunately,    nding such a teacher
is often dif   cult, expensive, or down right impossible. in those cases,
you might still want to be able to analyze your data, even though you
do not have labels.

unsupervised learning is learning without a teacher. one basic

thing that you might want to do with data is to visualize it. sadly, it
is dif   cult to visualize things in more than two (or three) dimensions,
and most data is in hundreds of dimensions (or more). dimension-
ality reduction is the problem of taking high dimensional data and
embedding it in a lower dimension space. another thing you might
want to do is automatically derive a partitioning of the data into
clusters. you   ve already learned a basic approach for doing this: the
id116 algorithm (chapter 3). here you will analyze this algorithm
to see why it works. you will also learn more advanced id91
approaches.

15.1 id116 id91, revisited

the id116 id91 algorithm is re-presented in algorithm 15.1.

there are two very basic questions about this algorithm: (1) does it
converge (and if so, how quickly); (2) how sensitive it is to initializa-
tion? the answers to these questions, detailed below, are: (1) yes it
converges, and it converges very quickly in practice (though slowly
in theory); (2) yes it is sensitive to initialization, but there are good
ways to initialize it.

consider the question of convergence. the following theorem

states that the id116 algorithm converges, though it does not say
how quickly it happens. the method of proving the convergence is
to specify a id91 quality objective function, and then to show
that the id116 algorithm converges to a (local) optimum of that
objective function. the particular objective function that id116

algorithm 35 id116(d, k)
1: for k = 1 to k do
  k     some random location
2:
3: end for
4: repeat
5:

for n = 1 to n do

zn     argmink ||  k     xn||

end for
for k = 1 to k do

  k     mean({ xn : zn = k })

6:

7:

8:

9:

end for

10:
11: until converged
12: return z

unsupervised learning 179

// randomly initialize mean for kth cluster

// assign example n to closest center

// re-estimate mean of cluster k

// return cluster assignments

is optimizing is the sum of squared distances from any data point to its
assigned center. this is a natural generalization of the de   nition of a
mean: the mean of a set of points is the single point that minimizes
the sum of squared distances from the mean to every point in the
data. formally, the id116 objective is:
=    
k

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xn       zn

l(z,   ; d) =    
n

||xn       k||2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

   
n:zn=k

(15.1)

theorem 16 (id116 convergence theorem). for any dataset d and
any number of clusters k, the id116 algorithm converges in a    nite num-
ber of iterations, where convergence is measured by l ceasing the change.
proof of theorem 16. the proof works as follows. there are only two
points in which the id116 algorithm changes the values of    or z:
lines 6 and 9. we will show that both of these operations can never
increase the value of l. assuming this is true, the rest of the argu-
ment is as follows. after the    rst pass through the data, there are
are only    nitely many possible assignments to z and   , because z is
discrete and because    can only take on a    nite number of values:
means of some subset of the data. furthermore, l is lower-bounded
by zero. together, this means that l cannot decrease more than a
   nite number of times. thus, it must stop decreasing at some point,
and at that point the algorithm has converged.
it remains to show that lines 6 and 9 decrease l. for line 6, when
looking at example n, suppose that the previous value of zn is a and
the new value is b. it must be the case that ||xn       b||     ||xn       b||.
thus, changing from a to b can only decrease l. for line 9, consider
the second form of l. line 9 computes   k as the mean of the data
points for which zn = k, which is precisely the point that minimizes
squared sitances. thus, this update to   k can only decrease l.

there are several aspects of id116 that are unfortunate. first,
the convergence is only to a local optimum of l. in practice, this

180 a course in machine learning

means that you should usually run it 10 times with different initial-
izations and pick the one with minimal resulting l. second, one
can show that there are input datasets and initializations on which
it might take an exponential amount of time to converge. fortu-
nately, these cases almost never happen in practice, and in fact it has
recently been shown that (roughly) if you limit the    oating point pre-
cision of your machine, id116 will converge in polynomial time
(though still only to a local optimum), using techniques of smoothed
analysis.

the biggest practical issue in id116 is initialization. if the clus-
ter means are initialized poorly, you often get convergence to uninter-
esting solutions. a useful heuristic is the furthest-   rst heuristic. this
gives a way to perform a semi-random initialization that attempts to
pick initial means as far from each other as possible. the heuristic is
sketched below:
1. pick a random example m and set   1 = xm.
2. for k = 2 . . . k:

(a) find the example m that is as far as possible from all previ-

ously selected means; namely: m = arg maxm mink(cid:48)<k ||xm       k(cid:48)||2
and set   k = xm

in this heuristic, the only bit of randomness is the selection of the
   rst data point. after that, it is completely deterministic (except in
the rare case that there are multiple equidistant points in step 2a). it
is extremely important that when selecting the 3rd mean, you select
that point that maximizes the minimum distance to the closest other
mean. you want the point that   s as far away from all previous means
as possible.

the furthest-   rst heuristic is just that: a heuristic. it works very

well in practice, though can be somewhat sensitive to outliers (which
will often get selected as some of the initial means). however, this
outlier sensitivity is usually reduced after one iteration through the
id116 algorithm. despite being just a heuristic, it is quite useful in
practice.

you can turn the heuristic into an algorithm by adding a bit more

randomness. this is the idea of the id116++ algorithm, which
is a simple randomized tweak on the furthest-   rst heuristic. the
idea is that when you select the kth mean, instead of choosing the
absolute furthest data point, you choose a data point at random, with
id203 proportional to its distance squared. this is made formal
in algorithm 15.1.

if you use id116++ as an initialization for id116, then you
are able to achieve an approximation guarantee on the    nal value

unsupervised learning 181

algorithm 36 id116++(d, k)
1:   1     xm for m chosen uniformly at random // randomly initialize    rst point
2: for k = 2 to k do
3:

// compute distances
// normalize to id203 distribution
// pick an example at random

4:

5:

dn     mink(cid:48)<k ||xn       k(cid:48)||2,    n
p     1
   n nd
m     random sample from p
  k     xm

d

6:
7: end for
8: run id116 using    as initial centers

of the objective. this doesn   t tell you that you will reach the global
optimum, but it does tell you that you will get reasonably close. in
particular, if   l is the value obtained by running id116++, then this
will not be    too far    from l(opt), the true global minimum.
theorem 17 (id116++ approximation guarantee). the expected
value of the objective returned by id116++ is never more than o(log k)
from optimal and can be as close as o(1) from optimal. even in the former
case, with 2k random restarts, one restart will be o(1) from optimal (with

high id203). formally: e(cid:2)   l(cid:3)     8(log k + 2)l(opt). moreover, if the
data is    well suited    for id91, then e(cid:2)   l(cid:3)     o(1)l(opt).

the notion of    well suited    for id91 informally states that
the advantage of going from k     1 clusters to k clusters is    large.   
formally, it means that lk
(opt) is the
optimal value for id91 with k clusters, and   is the desired
degree of approximation. the idea is that if this condition does not
hold, then you shouldn   t bother id91 the data.

(opt)      2lk   1

(opt), where lk

one of the biggest practical issues with id116 id91 is
   choosing k.    namely, if someone just hands you a dataset and
asks you to cluster it, how many clusters should you produce? this
is dif   cult, because increasing k will always decrease lk
(opt) (until
k > n), and so simply using l as a notion of goodness is insuf   -
cient (analogous to over   tting in a supervised setting). a number
of    information criteria    have been proposed to try to address this
problem. they all effectively boil down to    regularizing    k so that
the model cannot grow to be too complicated. the two most popular
are the bayes information criteria (bic) and the akaike information
criteria (aic), de   ned below in the context of id116:

bic:

aic:

arg min

k

arg min

k

  lk + k log d
  lk + 2kd

(15.2)
(15.3)

the informal intuition behind these criteria is that increasing k is
going to make lk go down. however, if it doesn   t go down    by
enough    then it   s not worth doing. in the case of bic,    by enough   

182 a course in machine learning

means by an amount proportional to log d; in the case of aic, it   s
proportional to 2d. thus, aic provides a much stronger penalty for
many clusters than does bic, especially in high dimensions.

a more formal intuition for bic is the following. you ask yourself

the question    if i wanted to send this data across a network, how
many bits would i need to send?    clearly you could simply send
all of the n examples, each of which would take roughly log d bits
to send. this gives n log d to send all the data. alternatively, you
could    rst cluster the data and send the cluster centers. this will take
k log d bits. then, for each data point, you send its center as well as
its deviation from that center. it turns out this will cost exactly   lk
bits. therefore, the bic is precisely measuring how many bits it will
take to send your data using k clusters. the k that minimizes this
number of bits is the optimal value.

15.2 linear id84

id84 is the task of taking a dataset in high di-
mensions (say 10000) and reducing it to low dimensions (say 2) while
retaining the    important    characteristics of the data. since this is
an unsupervised setting, the notion of important characteristics is
dif   cult to de   ne.

consider the dataset in figure 15.1, which lives in high dimensions

(two) and you want to reduce to low dimensions (one). in the case
of linear id84, the only thing you can do is to
project the data onto a vector and use the projected distances as the
embeddings. figure 15.2 shows a projection of this data onto the
vector that points in the direction of maximal variance of the original
dataset. intuitively, this is a reasonable notion of importance, since
this is the direction in which most information is encoded in the data.

for the rest of this section, assume that the data is centered:
namely, the mean of all the data is at the origin. (this will sim-
ply make the math easier.) suppose the two dimensional data is
x1, . . . , xn and you   re looking for a vector u that points in the direc-
tion of maximal variance. you can compute this by projecting each
point onto u and looking at the variance of the result. in order for
the projection to make sense, you need to constrain ||u||2 = 1. in
this case, the projections are x1    u, x2    u, . . . , xn    u. call these values
p1, . . . , pn.
the goal is to compute the variance of the {pn}s and then choose
u to maximize this variance. to compute the variance, you    rst need
to compute the mean. because the mean of the xns was zero, the

figure 15.1: unsup:pcadata: data in
two dimensions

figure 15.2: unsup:pcadata2: projection
of that data down to one dimension by
pca

math review | eigenvalues and eigenvectors
todo the usual...

unsupervised learning 183

figure 15.3:

mean of the ps is also zero. this can be seen as follows:

(cid:32)

(cid:33)

   
n

pn =    
n

xn    u =

   
n

xn

   u = 0    u = 0

(15.4)

the variance of the {pn} is then just    n p2
n. finding the optimal
u (from the perspective of variance maximization) reduces to the
following optimization problem:

max

u

(xn    u)2

   
n

subj. to

||u||2 = 1

(15.5)

in this problem it becomes apparent why keeping u unit length is
important: if not, u would simply stretch to have in   nite length to
maximize the objective.
it is now helpful to write the collection of datapoints xn as a n  
d matrix x. if you take this matrix x and multiply it by u, which
has dimensions d  1, you end up with a n  1 vector whose values
are exactly the values p. the objective in eq (15.5) is then just the
squared norm of p. this simpli   es eq (15.5) to:

||xu||2

subj. to

||u||2     1 = 0

max

u

(15.6)

where the constraint has been rewritten to make it amenable to con-
structing the lagrangian. doing so and taking gradients yields:

(cid:17)

l(u,   ) = ||xu||2       

(cid:16)||u||2     1
(cid:17)
   ul = 2x(cid:62)xu     2  u
=      u =
u

x(cid:62)x

(cid:16)

(15.7)
(15.8)
(15.9)

you can solve this expression (  u = x(cid:62)xu) by computing the    rst
eigenvector and eigenvalue of the matrix x(cid:62)x.

this gives you the solution to a projection into a one-dimensional

space. to get a second dimension, you want to    nd a new vector v on
which the data has maximal variance. however, to avoid redundancy,
you want v to be orthogonal to u; namely u    v = 0. this gives:

||xv||2

subj. to

||v||2 = 1, and u    v = 0

(15.10)

max

v

following the same procedure as before, you can construct a la-

184 a course in machine learning

algorithm 37 pca(d, k)
1:        mean(x)

x       1(cid:62)(cid:17) (cid:62)(cid:16)

2: d    (cid:16)

x       1(cid:62)(cid:17)

3: {  k, uk}     top k eigenvalues/eigenvectors of d
4: return (x       1) u

// compute data mean for centering
// compute covariance, 1 is a vector of ones

grangian and differentiate:

l(v,   1,   2) = ||xv||2       1

(cid:17)       2u    v

(cid:16)||v||2     1
(cid:17)

   ul = 2x(cid:62)xv     2  1v       2u
v       2
=      1v =
u
2

x(cid:62)x

(cid:16)

// project data using u

(15.11)
(15.12)
(15.13)

however, you know that u is the    rst eigenvector of x(cid:62)x, so the
solution to this problem for   1 and v is given by the second eigen-
value/eigenvector pair of x(cid:62)x.

repeating this analysis inductively tells you that if you want to

project onto k mutually orthogonal dimensions, you simply need to
take the    rst k eigenvectors of the matrix x(cid:62)x. this matrix is often
called the data covariance matrix because [x(cid:62)x]i,j =    n    m xn,ixm,j,
which is the sample covariance between features i and j.

this leads to the technique of principle components analysis,
or pca. for completeness, the is depicted in algorithm 15.2. the
important thing to note is that the eigenanalysis only gives you the
projection directions. it does not give you the embedded data. to
embed a data point x you need to compute its embedding as (cid:104)x   
u1, x    u2, . . . , x    uk(cid:105). if you write u for the d  k matrix of us, then this
is just xu.

there is an alternative derivation of pca that can be informative,

based on reconstruction error. consider the one-dimensional case
again, where you are looking for a single projection direction u. if
you were to use this direction, your projected data would be z = xu.
each zn gives the position of the nth datapoint along u. you can
project this one-dimensional data back into the original space by
multiplying it by u(cid:62). this gives you reconstructed values zu(cid:62). instead
of maximizing variance, you might instead want to minimize the
reconstruction error, de   ned by:

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x     zu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)x     xuu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

=

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xuu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2     2x(cid:62)xuu(cid:62)

= ||x||2 +

de   nition of z

(15.14)

quadratic rule

(15.15)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xuu(cid:62)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2     2u(cid:62)x(cid:62)xu

= ||x||2 +

= ||x||2 + ||x||2     2u(cid:62)x(cid:62)xu

unsupervised learning 185

quadratic rule

(15.16)

u is a unit vector

(15.17)

= c     2||xu||2

join constants, rewrite last term

(15.18)
minimizing this    nal term is equivalent to maximizing ||xu||2, which
is exactly the form of the maximum variance derivation of pca.
thus, you can see that maximizing variance is identical to minimiz-
ing reconstruction error.

the same question of    what should k be    arises in dimension-
ality reduction as in id91. if the purpose of dimensionality
reduction is to visualize, then k should be 2 or 3. however, an alter-
native purpose of id84 is to avoid the curse of
dimensionality. for instance, even if you have labeled data, it might
be worthwhile to reduce the dimensionality before applying super-
vised learning, essentially as a form of id173. in this case,
the question of an optimal k comes up again. in this case, the same
criteria (aic and bic) that can be used for id91 can be used for
pca. the only difference is the quality measure changes from a sum
of squared distances to means (for id91) to a sum of squared
distances to original data points (for pca). in particular, for bic you
get the reconstruction error plus k log d; for aic, you get the recon-
struction error plus 2kd.

15.3 autoencoders

todo

15.4 further reading

todo

16 | expectation maximization

learning objectives:
    explain the relationship between
parameters and hidden variables.

    construct generative stories for

id91 and dimensionality
reduction.

    draw a graph explaining how em

works by constructing convex lower
bounds.

    implement em for id91 with

mixtures of gaussians, and contrast-
ing it with id116.

    evaluate the differences betweem

em and id119 for hidden
variable models.

dependencies:

a hen is only an egg   s way of making another egg.     samuel butler

suppose you were building a naive bayes model for a text cate-
gorization problem. after you were done, your boss told you that it
became prohibitively expensive to obtain labeled data. you now have
a probabilistic model that assumes access to labels, but you don   t
have any labels! can you still do something?

amazingly, you can. you can treat the labels as hidden variables,
and attempt to learn them at the same time as you learn the param-
eters of your model. a very broad family of algorithms for solving
problems just like this is the expectation maximization family. in this
chapter, you will derive expectation maximization (em) algorithms
for id91 and id84, and then see why em
works.

16.1 grading an exam without an answer key

alice   s machine learning professor carlos gives out an exam that
consists of 50 true/false questions. alice   s class of 100 students takes
the exam and carlos goes to grade their solutions. if carlos made
an answer key, this would be easy: he would just count the fraction
of correctly answered questions each student got, and that would be
their score. but, like many professors, carlos was really busy and
didn   t have time to make an answer key. can he still grade the exam?

there are two insights that suggest that he might be able to. sup-
pose he know ahead of time that alice was an awesome student, and
is basically guaranteed to get 100% on the exam. in that case, carlos
can simply use alice   s answers as the ground truth. more generally,
if carlos assumes that on average students are better than random
guessing, he can hope that the majority answer for each question is
likely to be correct. combining this with the previous insight, when
doing the    voting   , he might want to pay more attention to the an-
swers of the better students.

to be a bit more pedantic, suppose there are n = 100 students

and m = 50 questions. each student n has a score sn, between 0 and

expectation maximization 187

1 that denotes how well they do on the exam. the score is what we
really want to compute. for each question m and each student n, the
student has provided an answer an,m, which is either zero or one.
there is also an unknown ground truth answer for each question m,
which we   ll call tm, which is also either zero or one.

as a starting point, let   s consider a simple heuristic and then com-
plexify it. the heuristic is the    majority vote    heuristic and works as
follows. first, we estimate tm as the most common answer for ques-
tion m: tm = argmaxt    n 1[an,m = t]. once we have a guess for each
true answer, we estimate each students    score as how many answers
m    m 1[an,m = tm].
they produced that match this guessed key: sn = 1
once we have these scores, however, we might want to trust some

of the students more than others. in particular, answers from stu-
dents with high scores are perhaps more likely to be correct, so we
can recompute the ground truth, according to weighted votes. the
weight of the votes will be precisely the score the corresponding each
student:

tm = argmax

t

   
n

sn1[an,m = t]

(16.1)

you can recognize this as a chicken and egg problem. if you knew the
student   s scores, you could estimate an answer key. if you had an
answer key, you could compute student scores. a very common
strategy in computer science for dealing with such chicken and egg
problems is to iterate. take a guess at the    rst, compute the second,
recompute the    rst, and so on.

in order to develop this idea formally, we have to case the prob-
lem in terms of a probabilistic model with a generative story. the
generative story we   ll use is:
1. for each question m, choose a true answer tm     ber(0.5)
2. for each student n, choose a score sn     uni(0, 1)
3. for each question m and each student n, choose an answer

an,m     ber(sn)tmber(1     sn)1   tm
in the    rst step, we generate the true answers independently by
   ipping a fair coin. in the second step, each students    overall score
is determined to be a uniform random number between zero and
one. the tricky step is step three, where each students    answer is
generated for each question. consider student n answering question
m, and suppose that sn = 0.9. if tm = 1, then an,m should be 1 (i.e.,
correct) 90% of the time; this can be accomplished by drawing the an-
swer from ber(0.9). on the other hand, if tm = 0, then an,m should 1
(i.e., incorrect) 10% of the time; this can be accomplished by drawing

188 a course in machine learning

the answer from ber(0.1). the exponent in step 3 selects which of two
bernoulli distributions to draw from, and then implements this rule.

this can be translated into the following likelihood:

(cid:35)

(cid:34)

(cid:35)

   
n

1

0.5tm0.51   tm

  

(cid:34)

=

p(a, t, s)
   
m
  

(cid:34)

(1     sn)(1   an,m)tm

(1     sn)an,m(1   tm)(cid:105)

san,mtm
n

   
   
n
m
s(1   an,m)(1   tm)
n
   
= 0.5m   
m
n

san,mtm
n

(1     sn)(1   an,m)tms(1   an,m)(1   tm)

n

(16.2)
(1     sn)an,m(1   tm)
(16.3)

suppose we knew the true lables t. we can take the log of this
likelihood and differentiate it with respect to the score sn of some
student (note: we can drop the 0.5m term because it is just a con-
stant):

(cid:104)

log p(a, t, s) =    
n

   
m

an,mtm log sn + (1     an,m)(1     tm) log(sn)
+ (1     an,m)tm log(1     sn) + an,m(1     tm) log(1     sn)
(16.4)

(cid:104) an,mtm + (1     an,m)(1     tm)

(cid:105)

    (1     an,m)tm + an,m(1     tm)

(cid:105)

    log p(a, t, s)

   sn

=    
m

sn

the derivative has the form a
sn
solve for sn, we get an optimum of sn = a

    b

1   sn . if we set this equal to zero and

a+b . in this case:

(cid:2)an,mtm + (1     an,m)(1     tm)(cid:3)
(cid:2)(1     an,m)tm + an,m(1     tm)(cid:3)
(cid:2)1(cid:3) = m
(cid:2)an,mtm + (1     an,m)(1     tm)(cid:3)

a =    
m
b =    
m
a + b =    
m

sn =

1
m

   
m

putting this together, we get:

in the case of known ts, this matches exactly what we had in the
heuristic.

however, we do not know t, so instead of using the    true    val-
ues of t, we   re going to use their expectations. in particular, we will
compute sn by maximizing its likelihood under the expected values

1     sn
(16.5)

(16.6)

(16.7)

(16.8)

(16.9)

expectation maximization 189

of t, hence the name expectation maximization. if we are going
to compute expectations of t, we have to say: expectations accord-
ing to which id203 distribution? we will use the distribution
p(tm | a, s). let   tm denote etm   p(tm | a,s)[tm]. because tm is a bi-
nary variable, its expectation is equal to it   s id203; namely:
  tm = p(tm | a, s).

how can we compute this? we will compute c = p(tm = 1, a, s)
and d = p(tm = 0, a, s) and then compute   tm = c/(c + d). the
computation is straightforward:

c = 0.5    
n
d = 0.5    
n

san,m
n

s1   an,m

n

(1     sn)1   an,m = 0.5    
an,m=1
(1     sn)an,m = 0.5    
an,m=1

n:

n:

sn    
n:
an,m=0

(1     sn)    
an,m=0

n:

(1     sn)

(16.10)

sn

(16.11)

if you inspect the value of c, it is basically    voting    (in a product
form, not a sum form) the scores of those students who agree that the
answer is 1 with one-minus-the-score of those students who do not.
the value of d is doing the reverse. this is a form of multiplicative
voting, which has the effect that if a given student has a perfect score
of 1.0, their results will carry the vote completely.

we now have a way to:

1. compute expected ground truth values   tm, given scores.
2. optimize scores sn given expected ground truth values.

the full solution is then to alternate between these two. you can
start by initializing the ground truth values at the majority vote (this
seems like a safe initialization). given those, compute new scores.
given those new scores, compute new ground truth values. and
repeat until tired.

in the next two sections, we will consider a more complex unsu-
pervised learning model for id91, and then a generic mathe-
matical framework for expectation maximization, which will answer
questions like: will this process converge, and, if so, to what?

16.2 id91 with a mixture of gaussians

in chapter 9, you learned about probabilitic models for classi   cation
based on density estimation. let   s start with a fairly simple classi   ca-
tion model that assumes we have labeled data. we will shortly remove
this assumption. our model will state that we have k classes, and
data from class k is drawn from a gaussian with mean   k and vari-
ance   2
k . the choice of classes is parameterized by   . the generative
story for this model is:

190 a course in machine learning

1. for each example n = 1 . . . n:
(a) choose a label yn     disc(  )
(b) choose example xn     nor(  yn,   2
yn )

this generative story can be directly translated into a likelihood as
before:

p(d) =    
n

(cid:122)

=

   
n

mult(yn |   )nor(xn |   yn,   2
yn )
(cid:34)

for each example

(cid:125)(cid:124)

(cid:105)    d

2    2
yn

2 exp

  yn(cid:124)(cid:123)(cid:122)(cid:125)

choose label

(cid:104)
(cid:124)

    1
2  2
yn

(cid:123)(cid:122)

choose feature values

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)xn       yn

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)2

(16.12)

(16.13)

(cid:123)
(cid:35)
(cid:125)

if you had access to labels, this would be all well and good, and
you could obtain closed form solutions for the maximum likelihood
estimates of all parameters by taking a log and then taking gradients
of the log likelihood:

  k = fraction of training examples in class k

=

1
n

   
n

[yn = k]

  k = mean of training examples in class k

=

   n[yn = k]xn
   n[yn = k]

  2
k = variance of training examples in class k

   n[yn = k] ||xn       k||

   n[yn = k]

=

(16.14)

(16.15)

(16.16)

?

you should be able to derive the
maximum likelihood solution re-
sults formally by now.

suppose that you don   t have labels. analogously to the id116
algorithm, one potential solution is to iterate. you can start off with
guesses for the values of the unknown variables, and then iteratively
improve them over time. in id116, the approach was the assign
examples to labels (or clusters). this time, instead of making hard
assignments (   example 10 belongs to cluster 4   ), we   ll make soft as-
signments (   example 10 belongs half to cluster 4, a quarter to cluster
2 and a quarter to cluster 5   ). so as not to confuse ourselves too
much, we   ll introduce a new variable, zn = (cid:104)zn,1, . . . , zn,k (that sums
to one), to denote a fractional assignment of examples to clusters.

this notion of soft-assignments is visualized in figure 16.1. here,
we   ve depicted each example as a pie chart, and it   s coloring denotes
the degree to which it   s been assigned to each (of three) clusters. the
size of the pie pieces correspond to the zn values.

figure 16.1: em:piecharts: a    gure
showing pie charts

formally, zn,k denotes the id203 that example n is assigned to

cluster k:

expectation maximization 191

zn,k = p(yn = k | xn)
p(yn = k, xn)

=

=

1
zn

p(xn)
mult(k |   )nor(xn |   k,   2
k )

(16.17)

(16.18)

(16.19)

here, the normalizer zn is to ensure that zn sums to one.

given a set of parameters (the   s,   s and   2s), the fractional as-
signments zn,k are easy to compute. now, akin to id116, given
fractional assignments, you need to recompute estimates of the
model parameters. in analogy to the maximum likelihood solution
(eqs (??)-(??)), you can do this by counting fractional points rather
than full points. this gives the following re-estimation updates:

  k = fraction of training examples in class k

=

1
n

   
n

zn,k

  k = mean of fractional examples in class k

=

   n zn,kxn
   n zn,k

  2
k = variance of fractional examples in class k

   n zn,k ||xn       k||

=

   n zn,k

(16.20)

(16.21)

(16.22)

all that has happened here is that the hard assignments    [yn = k]   
have been replaced with soft assignments    zn,k   . as a bit of fore-
shadowing of what is to come, what we   ve done is essentially replace
known labels with expected labels, hence the name    expectation maxi-
mization.   

putting this together yields algorithm 16.2. this is the gmm
(   gaussian mixture models   ) algorithm, because the probabilitic
model being learned describes a dataset as being drawn from a mix-
ture distribution, where each component of this distribution is a
gaussian.

just as in the id116 algorithm, this approach is succeptible to
local optima and quality of initialization. the heuristics for comput-
ing better initializers for id116 are also useful here.

16.3 the expectation maximization framework

at this point, you   ve seen a method for learning in a particular prob-
abilistic model with hidden variables. two questions remain: (1) can

?

aside from the fact that gmms
use soft assignments and id116
uses hard assignments, there are
other differences between the two
approaches. what are they?

192 a course in machine learning

algorithm 38 gmm(x, k)

3:

1: for k = 1 to k do
2:

  k     some random location
k     1
  2
  k     1/k
4:
5: end for
6: repeat
7:

for n = 1 to n do

(cid:2)2    2

(cid:3)    d

for k = 1 to k do

zn,k       k
(unnormalized) fractional assignments

2 exp

2  2
k

k

(cid:104)    1

||xn       k||2(cid:105)

8:

9:

10:

11:

12:

13:

14:

15:

16:

end for
zn     1
   k zn,k

zn

end for
for k = 1 to k do
  k     1
n    n zn,k
  k        n zn,kxn
   n zn,k
k        n zn,k||xn     k||
  2
   n zn,k

end for

17:
18: until converged
19: return z

// randomly initialize mean for kth cluster
// initialize variances
// each cluster equally likely a priori

// compute

// normalize fractional assignments

// re-estimate prior id203 of cluster k
// re-estimate mean of cluster k
// re-estimate variance of cluster k

// return cluster assignments

you apply this idea more generally and (2) why is it even a reason-
able thing to do? expectation maximization is a family of algorithms
for performing id113 in probabilistic mod-
els with hidden variables.
the general    avor of how we will proceed is as follows. we want
to maximize the log likelihood l, but this will turn out to be dif   -
cult to do directly. instead, we   ll pick a surrogate function   l that   s a
lower bound on l (i.e.,   l     l everywhere) that   s (hopefully) easier
to maximize. we   ll construct the surrogate in such a way that increas-
ing it will force the true likelihood to also go up. after maximizing
  l, we   ll construct a new lower bound and optimize that. this process
is shown pictorially in figure 16.2.

to proceed, consider an arbitrary probabilistic model p(x, y |   ),
where x denotes the observed data, y denotes the hidden data and
   denotes the parameters. in the case of gaussian mixture models,
x was the data points, y was the (unknown) labels and    included
the cluster prior probabilities, the cluster means and the cluster vari-
ances. now, given access only to a number of examples x1, . . . , xn,
you would like to estimate the parameters (  ) of the model.

probabilistically, this means that some of the variables are un-
known and therefore you need to marginalize (or sum) over their
possible values. now, your data consists only of x = (cid:104)x1, x2, . . . , xn(cid:105),

figure 16.2: em:lowerbound: a    gure
showing successive lower bounds

expectation maximization 193

not the (x, y) pairs in d. you can then write the likelihood as:

p(x |   ) =    
y1

   
y2

            
yn

p(x, y1, y2, . . . yn |   )

=    
y1

   
y2

            
yn

   
n

p(xn, yn |   )

=    
n

   
yn

p(xn, yn |   )

marginalization

(16.23)

examples are independent

(16.24)

algebra

(16.25)

at this point, the natural thing to do is to take logs and then start
taking gradients. however, once you start taking logs, you run into a
problem: the log cannot eat the sum!
p(xn, yn |   )

(16.26)

l(x |   ) =    
n

log    
yn

the next step is to apply the somewhat strange, but strangely

namely, the log gets    stuck    outside the sum and cannot move in to
decompose the rest of the likelihood term!
useful, trick of multiplying by 1. in particular, let q(  ) be an arbitrary
id203 distribution. we will multiply the p(. . . ) term above by
q(yn)/q(yn), a valid step so long as q is never zero. this leads to:

l(x |   ) =    
n

log    
yn

q(yn)

p(xn, yn |   )

q(yn)

(16.27)

we will now construct a lower bound using jensen   s inequality.
this is a very useful (and easy to prove!) result that states that
f (   i   ixi)        i   i f (xi), so long as (a)   i     0 for all i, (b)    i   i = 1,
and (c) f is concave. if this looks familiar, that   s just because it   s a
direct result of the de   nition of concavity. recall that f is concave if
f (ax + by)     a f (x) + b f (x) whenever a + b = 1.

you can now apply jensen   s inequality to the log likelihood by
identifying the list of q(yn)s as the   s, log as f (which is, indeed,
concave) and each    x    as the p/q term. this yields:

?

prove jensen   s inequality using the
de   nition of concavity and induc-
tion.

p(xn, yn |   )

q(yn) log
q(yn) log p(xn, yn |   )     q(yn) log q(yn)

q(yn)

l(x |   )        
(cid:104)
n
=    
n
(cid:44)   l(x |   )

   
yn
   
yn

(cid:105)

(16.28)

(16.29)

(16.30)

note that this inequality holds for any choice of function q, so long as
its non-negative and sums to one. in particular, it needn   t even by the

194 a course in machine learning

same function q for each n. we will need to take advantage of both of
these properties.
we have succeeded in our    rst goal: constructing a lower bound
on l. when you go to optimize this lower bound for   , the only part
that matters is the    rst term. the second term, q log q, drops out as a
function of   . this means that the the maximization you need to be
able to compute, for    xed qns, is:

  (new)     arg max

  

   
n

   
yn

qn(yn) log p(xn, yn |   )

(16.31)

this is exactly the sort of maximization done for gaussian mixture
models when we recomputed new means, variances and cluster prior
probabilities.

the second question is: what should qn(  ) actually be? any rea-

sonable q will lead to a lower bound, so in order to choose one q over
another, we need another criterion. recall that we are hoping to max-
imize l by instead maximizing a lower bound. in order to ensure
that an increase in the lower bound implies an increase in l, we need
to ensure that l(x |   ) =   l(x |   ). in words:
  l should be a lower
bound on l that makes contact at the current point,   .

16.4 further reading

todo further reading

17 | id170

learning objectives:
    recognize when a problem should
be solved using a structured predic-
tion technique.

    implement the structured id88

algorithm for sequence labeling.

    map    argmax    problems to integer

linear programs.

    augment the structured id88

with losses to derive structured
id166s.

dependencies:

it is often the case that instead of predicting a single output, you
need to predict multiple, correlated outputs simultaneously. in nat-
ural language processing, you might want to assign a syntactic label
(like noun, verb, adjective, etc.) to words in a sentence: there is clear
correlation among these labels. in id161, you might want
to label regions in an image with object categories; again, there is
correlation among these regions. the branch of machine learning that
studies such questions is id170.

in this chapter, we will cover two of the most common algorithms

for id170: the structured id88 and the struc-
tured support vector machine. we will consider two types of struc-
ture. the    rst is the    sequence labeling    problem, typi   ed by the
natural language processing example above, but also common in
computational biology (labeling amino acids in dna) and robotics
(labeling actions in a sequence). for this, we will develop specialized
prediction algorithms that take advantage of the sequential nature
of the task. we will also consider more general structures beyond
sequences, and discuss how to cast them in a generic optimization
framework: integer id135 (or ilp).

the general framework we will explore is that of jointly scoring

input/output con   gurations. we will construct algorithms that learn a
function s(x,   y) (s for    score   ), where x is an input (like an image)
and   y is some predicted output (like a segmentation of that image).
for any given image, there are a lot of possible segmentations (i.e.,
a lot of possible   ys), and the goal of s is to rank them in order of
   how good    they are: how compatible they are with the input x. the
most important thing is that the scoring function s ranks the true
segmentation y higher than any other imposter segmentation   y. that
is, we want to ensure that s(x, y) > s(x,   y) for all   y (cid:54)= y. the main
challenge we will face is how to do this ef   ciently, given that there are
so many imposter   ys.

196 a course in machine learning

17.1 multiclass id88

in order to build up to structured problems, let   s begin with a simpli-
   ed by pedagogically useful stepping stone: multiclass classi   cation
with a id88. as discussed earlier, in multiclass classi   cation we
have inputs x     rd and output labels y     {1, 2, . . . , k}. our goal
is to learn a scoring function s so that s(x, y) > s(x,   y) for all   y (cid:54)= y,
where y is the true label and   y is an imposter label. the general form
of scoring function we consider is a linear function of a joint feature
vector   (x, y):

s(x, y) = w      (x, y)

(17.1)

here, the features   (x, y) should denote how    compatible    the input
x is with the label y. we keep track of a single weight vector w that
learns how to weigh these different    compatibility    features.

a natural way to represent   , if you know nothing else about

the problem, is an outer product between x and the label space. this
yields the following representation:

(cid:68)

  (x, k) =

(cid:124)

(cid:123)(cid:122)

0, 0, . . . , 0
d(k   1) zeros

(cid:125)

, x(cid:124)(cid:123)(cid:122)(cid:125)   rd

(cid:124)

, 0, 0, . . . , 0
d(k   k) zeros

(cid:123)(cid:122)

(cid:125)

(cid:69)     rdk

(17.2)

in this representation, w effectively encodes a separate weight for
every feature/label pair.

how are we going to learn w? we will start with w = 0 and then

process each input one at a time. suppose we get an input x with
gold standard label y. we will use the current scoring function to
predict a label. in particular, we will predict the label   y that maxi-
mizes the score:

  y = argmax
  y   [1,k]
= argmax
  y   [1,k]

s(x,   y)
w      (x,   y)

(17.3)

(17.4)

if this predicted output is correct (i.e.,   y = y), then, per the normal
id88, we will do nothing. suppose that   y (cid:54)= y. this means that
the score of   y is greater than the score of y, so we want to update w
so that the score of   y is decreased and the score of y is increased. we
do this by:

w     w +   (x, y)       (x,   y)

(17.5)

to make sure this is doing what we expect, let   s consider what would
happen if we computed scores under the updated value of w. to make
the notation clear, let   s say w(old) are the weights before update, and

id170 197

algorithm 39 multiclassid88train(d, maxiter)
1: w     0
2: for iter = 1 . . . maxiter do
3:

for all (x,y)     d do

// initialize weights

  y     argmaxk w      (x, k)
if   y (cid:54)= y then

w     w +   (x, y)       (x,   y)

4:

5:

6:

7:

end if
end for

8:
9: end for
10: return w

// compute prediction

// update weights

// return learned weights

w(new) are the weights after update. then:

(cid:16)
w(new)      (x, y)
=
(cid:123)(cid:122)
(cid:124)
(cid:125)
= w(old)      (x, y)

w(old) +   (x, y)       (x,   y)
(cid:123)(cid:122)

(cid:17)      (x, y)
(cid:124)
(cid:125)
(cid:124)
(cid:125)
+   (x, y)      (x, y)
      (x,   y)      (x, y)

(cid:123)(cid:122)

old prediction

   0

=0

(17.6)
(17.7)
(17.8)

here, the    rst term is the old prediction. the second term is of the
form a    a which is non-negative (and, unless   (x, y) is the zero vec-
tor, positive). the third term is the dot product between    for two
different labels, which by de   nition of    is zero (see eq (17.2)).

this gives rise to the updated multiclass id88 speci   ed in
algorithm 17.1. as with the normal id88, the generalization
of the multiclass id88 increases dramatically if you do weight
averaging.

an important note is that multiclassid88train is actually

more powerful than suggested so far. for instance, suppose that you
have three categories, but believe that two of them are tightly related,
while the third is very different. for instance, the categories might
be {music, movies, oncology}. you can encode this relatedness by
de   ning a feature expansion    that re   ects this:

  (x, music) = (cid:104)x, 0, 0, x(cid:105)
  (x, movies) = (cid:104)0, x, 0, x(cid:105)
  (x, oncology) = (cid:104)0, 0, x, 0(cid:105)

(17.9)
(17.10)
(17.11)

this encoding is identical to the normal encoding in the    rst three
positions, but includes an extra copy of the features at the end,
shared between music and movies. by doing so, if the id88
wants to learn something common to music and movies, it can use
this    nal shared position.

?

verify the score of   y, w(new)      (x,   y),
decreases after an update, as we
would want.

?

suppose you have a hierarchy
of classes arranged in a tree.
how could you use that to
construct a feature representa-
tion. you can think of the mu-
sic/movies/oncology example as
a binary tree: the left branch of the
root splits into music and movies;
the right branch of the root is just
oncology.

198 a course in machine learning

17.2 structured id88

let us now consider the sequence labeling task. in sequence labeling,
the outputs are themselves variable-length vectors. an input/output
pair (which must have the same length) might look like:

x =     monsters eat tasty bunnies    
y =

noun verb adj

noun

(17.12)
(17.13)

to set terminology, we will refer to the entire sequence y as the    out-
put    and a single label within y as a    label   . as before, our goal is to
learn a scoring function that scores the true output sequence y higher
than any imposter output sequence.

as before, despite the fact that y is now a vector, we can still de-

   ne feature functions over the entire input/output pair. for instance,
we might want to count the number of times    monsters    has been
tagged as    noun    in a given output. or the number of times    verb   
is followed by    noun    in an output. both of these are features that
are likely indicative of a correct output. we might also count the num-
ber of times    tasty    has been tagged as a verb (probably a negative
feature) and the number of times two verbs are adjacent (again, prob-
ably a negative feature).

more generally, a very standard set of features would be:

    the number of times word w has been labeled with tag l, for all

words w and all syntactic tags l

    the number of times tag l is adjacent to tag l(cid:48) in the output, for all

tags l and l(cid:48)

the    rst set of features are often called unary features, because they
talk only about the relationship between the input (sentence) and a
single (unit) label in the output sequence. the second set of features
are often called markov features, because they talk about adjacent la-
bels in the output sequence, which is reminiscent of markov models
which only have short term memory.

note that for a given input x of length l (in the example, l =

4), the number of possible outputs is kl, where k is the number of
syntactic tags. this means that the number of possible outputs grows
exponentially in the length of the input. in general, we write y (x) to
mean    the set of all possible structured outputs for the input x   . we
have just seen that |y (x)| = klen(x).

despite the fact that the inputs and outputs have variable length,
the size of the feature representation is constant. if there are v words
in your vocabulary and k labels for a given word, the the number of
unary features is vk and the number of markov features is k2, so

id170 199

algorithm 40 structuredid88train(d, maxiter)
1: w     0
2: for iter = 1 . . . maxiter do
3:

for all (x,y)     d do

// initialize weights

  y     argmax   y   y (x) w      (x,   y)
if   y (cid:54)= y then
w     w +   (x, y)       (x,   y)

4:

5:

6:

7:

end if
end for

8:
9: end for
10: return w

// compute prediction

// update weights

// return learned weights

the total number of features is k(v + k). of course, more complex
feature representations are possible and, in general, are a good idea.
for example, it is often useful to have unary features of neighboring
words like    the number of times the word immediately preceding a
verb was    monsters   .   

now that we have a    xed size feature representation, we can de-
velop a id88-style algorithm for sequence labeling. the core
idea is the same as before. we will maintain a single weight vector w.
we will make predictions by choosing the (entire) output sequence
  y that maximizes a score given by w      (x,   y). and if this output se-
quence is incorrect, we will adjust the weights word the correct output
sequence y and away from the incorrect output sequence   y. this is
summarized in algorithm 17.2

you may have noticed that algorithm 17.2 for the structured per-
ceptron is identical to algorithm 17.1, aside from the fact that in the
multiclass id88 the argmax is over the k possible classes, while
in the structured id88, the argmax is over the kl possible out-
put sequences!

the only dif   culty in this algorithm is in line 4:
  y     argmax
  y   y (x)

w      (x,   y)

(17.14)

in principle, this requires you to search over kl possible output se-
quences   y to    nd the one that maximizes the dot product. except for
very small k or very small l, this is computationally infeasible. be-
cause of its dif   culty, this is often refered to as the argmax problem
in id170. below, we consider how to solve the argmax
problem for sequences.

17.3 argmax for sequences

we now face an algorithmic question, not a machine learning ques-
tion: how to compute the argmax in eq 17.14 ef   ciently. in general,

200 a course in machine learning

this is not possible. however, under somewhat restrictive assump-
tions about the form of our features   , we can solve this problem ef   -
ciently, by casting it as the problem of computing a maximum weight
path through a speci   cally constructed lattice. this is a variant of the
viterbi algorithm for id48, a classic example of dy-
namic programming. (later, in section 17.6, we will consider argmax
for more general problems.)

the key observation for sequences is that   so long as we restrict

our attention to unary features and markov features   the feature
function    decomposes over the input. this is easiest to see with
an example. consider the input/output sequence from before: x =
   monsters eat tasty bunnies    and y = [noun verb adj noun]. if we
want to compute the number of times    bunnies    is tagged as    noun   
in this pair, we can do this by:
1. count the number of times    bunnies    is tagged as    noun    in the

   rst three words of the sentence

2. add to that the number of times    bunnies    is tagged as    noun    in

the    nal word

we can do a similar exercise for markov features, like the number of
times    adj    is followed by    noun   .

however, we don   t actually need these counts. all we need for
computing the argmax sequence is the dot product between the
weights w and these counts. in particular, we can compute w      (x, y)
as the dot product on all-but-the-last word plus the dot product on
the last word: w      1:3(x, y) + w      4(x, y). here,   1:3 means    fea-
tures for everything up to and including position 3    and   4 means
   features for position 4.   

more generally, we can write   (x, y) =    l

l=1   l(x, y), where

  l(x, y) only includes features about position l.1 in particular, we   re
taking advantage of the associative law for addition:

w      (x, y) = w    l   

l=1

  l(x, y)

decomposition of structure

(17.15)

=

l   

l=1

w      l(x, y)

associative law

(17.16)

what this means is that we can build a graph like that in figure ??,
with one verticle slice per time step (l 1 . . . l).2 each edge in this
graph will receive a weight, constructed in such a way that if you
take a complete path through the lattice, and add up all the weights,
this will correspond exactly to w      (x, y).
to complete the construction, let   l(x,             y     y(cid:48)) denote the unary

features at position l together with the markov features that end at

1 in the case of markov features, we
think of them as pairs that end at
position l, so    verb adj    would be the
active feature for   3.

2 a graph of this sort is called a trel-
lis, and sometimes a lattice in the
literature.

id170 201

figure 17.1: a picture of a trellis se-
quence labeling. at each time step l
the corresponding word can have any
of the three possible labels. any path
through this trellis corresponds to a
unique labeling of this sentence. the
gold standard path is drawn with bold
red arrows. the highlighted edge cor-
responds to the edge between l = 2
and l = 3 for verb/adj as described
in the text. that edge has weight
w      3(x,             verb     adj).

position l. these features depend only on x, y and y(cid:48), and not any of
the previous parts of the output.

for example, in the running example    monsters/noun eat/verb

tasty/adj bunnies/noun   , consider the edge between l = 2 and
l = 3 going from    verb    to    adj   . (note: this is a    correct    edge, in
the sense that it belongs to the ground truth output.) the features
associated with this edge will be unary features about    tasty/adj   
as well as markov features about    verb/adj   . the weight of this edge
will be exactly the total score (according to w) of those features.

formally, consider an edge in the trellis that goes from time l    
1 to l, and transitions from y to y(cid:48). set the weight of this edge to
exactly w      l(x,             y     y(cid:48)). by doing so, we guarantee that the
sum of weights along any path through this lattice is exactly equal
to the score of that path. once we have constructed the graph as
such, we can run any max-weight path algorithm to compute the
highest scoring output. for trellises, this can be computed by the
viterbi algorithm, or by applying any of a number of path    nding
algorithms for more general graphs. a complete derivation of the
dynamic program in this case is given in section 17.7 for those who
want to implement it directly.

the main bene   t of this construction is that it is guaranteed to
exactly compute the argmax output for sequences required in the
structured id88 algorithm, ef   ciently. in particular, it   s run-
time is o(lk2), which is an exponential improvement on the naive
o(kl) runtime if one were to enumerate every possible output se-
quence. the algorithm can be naturally extended to handle    higher
order    markov assumptions, where features depend on triples or
quadruples of the output. the trellis becomes larger, but the algo-
rithm remains essentially the same. in order to handle a length m
markov features, the resulting algorithm will take o(lkm) time. in
practice, it   s rare that m > 3 is necessary or useful.

tastynvabunniesnvaeatnvamonstersnva202 a course in machine learning

17.4 structured support vector machines

in section 7.7 we saw the support vector machine as a very useful
general framework for binary classi   cation. in this section, we will
develop a related framework for structured support vector machines.
the two main advantages of structured id166s over the structured
id88 are (1) it is regularized (though averaging in structured
id88 achieves a similar effect) and (2) we can incorporate more
complex id168s.

in particular, one suboptimal thing about the structured percep-
tron is that all errors are consider equally bad. for structured prob-
lems, we often have much more nuanced and elaborate id168s
that we want to optimize. even for sequence labeling, it is typically
far worse to label every word incorrectly than to just label one word
incorrectly. it is very common to use hamming loss as a general loss
function for id170. hamming loss simply counts:
of all the predictions you made, how many were incorrect? for se-
quence labeling, it is:
l   

(cid:96)(ham)(y,   y) =

1[yl (cid:54)=   yl]

(17.17)

l=1

in order to build up to structured id166s, recall that id166s began with
the following optimization problem:

min
w,  

||w||2

(cid:124) (cid:123)(cid:122) (cid:125)

+ c    
n

1
2
large margin

(cid:124) (cid:123)(cid:122) (cid:125)
subj. to yn (w    xn + b)     1       n

small slack

  n

  n     0

(17.18)

(   n)
(   n)

after a bit of work, we were able to reformulate this in terms of a
standard loss optimization algorithm with hinge loss:

min
w

||w||2

(cid:124) (cid:123)(cid:122) (cid:125)

1
2
large margin

+ c    
n

(cid:124)

(cid:125)
(cid:96)(hin)(yn, w    xn + b)

(cid:123)(cid:122)

small slack

(17.19)

we can do a similar derivation in the structured case. the question
is: exactly what should the slack be measuring? our goal is for the
score of the true output y to beat the score of any imposter output
  y. to incorporate loss, we will say that we want the score of the true
output to beat the score of any imposter output by at least the loss
that would be suffered if we were to predict that imposter output. an
alternative view is the ranking view: we want the true output to be
ranked above any imposter by an amount at least equal to the loss.

id170 203

to keep notation simple, we will write sw(x, y) to denote the score
of the pair x, y, namely w      (x, y). this suggests a set of constraints
of the form:

sw(x, y)     sw(x,   y)     (cid:96)(ham)(y,   y)          y

(   n,      y     y (x))

(17.20)

the rest of the optimization problem remains the same, yielding:

1
2

min
w,  

||w||2 + c    
n

   
  y   y xn
subj. to sw(x, y)     sw(x,   y)

  n,   y

(17.21)

  n,   y     0

    (cid:96)(ham)(yn,   y)       n,   y

(   n,      y     y (xn))
(   n,      y     y (xn))
this optimization problem asks for a large margin and small slack,
where there is a slack very for every training example and every
possible incorrect output associated with that training example. in
general, this is way too many slack variables and way too many con-
straints!
there is a very useful, general trick we can apply. if you focus on
the    rst constraint, it roughly says (letting s() denote score): s(y)    

(cid:2)s(   y) + (cid:96)(y,   y)(cid:3) for all   y, modulo slack. we   ll refer to the thing in

(cid:2)s(   y) + (cid:96)(y,   y)(cid:3), modulo slack. expanding out the de   nition

brackets as the    loss-augmented score.    but if we want to guarantee
that the score of the true y beats the loss-augmented score of all   y, it   s
enough to ensure that it beats the loss-augmented score of the most
confusing imposter. namely, it is suf   cient to require that s(y)    
max   y
of s() and adding slack back in, we can replace the exponentially
large number of constraints in eq (17.21) with the simpler set of
constraints:

sw(xn, yn)     max
  y   y (xn)

sw(xn,   y) + (cid:96)(ham)(yn,   y)

(   n)

(cid:105)       n

we can now apply the same trick as before to remove   n from the
analysis. in particular, because   n is constrained to be     0 and be-
cause we are trying to minimize it   s sum, we can    gure out that out
(cid:41)
the optimum, it will be the case that:

(cid:40)

(cid:105)     sw(xn, yn)

sw(xn,   y) + (cid:96)(ham)(yn,   y)

(cid:104)

(cid:104)

  n = max

0, max
  y   y (xn)

= (cid:96)(s-h)(yn, xn, w)

(17.22)
(17.23)

this value is referred to as the structured hinge loss, which we have
denoted as (cid:96)(s-h)(yn, xn, w). this is because, although it is more com-
plex, it bears a striking resemlance to the hinge loss from chapter 7.

204 a course in machine learning

in particular, if the score of the true output beats the score of every
the best imposter by at least its loss, then   n will be zero. on the
other hand, if some imposter (plus its loss) beats the true output, the
loss scales linearly as a function of the difference. at this point, there
is nothing special about hamming loss, so we will replace it with
some arbitrary structured loss (cid:96).

plugging this back into the objective function of eq (17.21), we can
write the structured id166 as an unconstrained optimization problem,
akin to eq (17.19), as:

min
w

1
2

||w||2 + c    
n

(cid:96)(s-h)(yn, xn, w)

(17.24)

this is now in a form that we can optimize using subid119
(chapter 7) or stochastic subid119 (chapter 14).

in order to compute subgradients of eq (17.24), we need to be able
to compute subgradients of the structured hinge loss. mathematically
this is straightforward. if the structured hinge loss on an example
(x, vy) is zero, then the gradient with respect to w is also zero. if the
structured hinge loss is positive, then the gradient is:

if the loss is > 0

   w(cid:96)(s-h)(y, x, w)
expand de   nition using arbitrary structured loss (cid:96)
=    w

w      (xn,   y) + (cid:96)(yn,   y)

(cid:105)     w      (xn, yn)

(cid:104)

(cid:41)

(17.25)

(17.26)

max
  y   y (xn)

w      (xn,   y)     w      (xn, yn) + (cid:96)(yn,   y)

de   ne   yn to be the output that attains the maximum above, rearrange
=    w
take gradient
=   (xn,   y)       (xn, yn)
(cid:40)

putting this together, we get the full gradient as:

(17.27)

(17.28)

(cid:40)
(cid:110)

(cid:111)

   w(cid:96)(s-h)(yn, xn, w) =

if (cid:96)(s-h)(yn, xn, w) = 0

0
(cid:105)
  (xn,   yn)       (xn, yn) otherwise
w      (xn,   yn) + (cid:96)(yn,   yn)

(cid:104)

(17.29)

where   yn = argmax
  yn   y (xn)

the form of this gradient is very simple: it is equal to the features
of the worst imposter minus the features of the truth, unless the
truth beats all imposters, in which case it   s zero. when plugged into
stochastic subid119, you end up with an update that looks
very much like the structured id88: if the current prediction
(   yn) is correct, there is no gradient step. but if the current prediction
is incorrect, you step w toward the truth and away from the imposter.

id170 205

algorithm 41 stochsubgradstructid166(d, maxiter,   , (cid:96))
1: w     0
2: for iter = 1 . . . maxiter do
3:

for all (x,y)     d do

// initialize weights

  y     argmax   y   y (x) w      (x,   y) + (cid:96)(y,   y)
if   y (cid:54)= y then

w     w +   (x, y)       (x,   y)

// loss-augmented prediction

// update weights

4:

5:

6:

7:

8:

end if
w     w       

n w

end for

9:
10: end for
11: return w

// shrink weights due to regularizer

// return learned weights

we will consider how to compute the loss-augmented argmax in

the next section, but before that we summarize an algorithm for opti-
mizing structured id166s using stochastic subid119: algo-
rithm 17.4. of course there are other possible optimization strategies;
we are highlighting this one because it is nearly identical to the struc-
tured id88. the only differences are: (1) on line 4 you use loss-
augmented argmax instead of argmax; and (2) on line 8 the weights
are shrunk slightly corresponding to the (cid:96)2 regularizer on w. (note:
we have used    = 1/(2c) to make the connection to linear models
clearer.)

17.5 loss-augmented argmax

the challenge that arises is that we now have a more complicated
argmax problem that before. in structured id88, we only
needed to compute   yn as the output that maximized its score (see
eq 17.14). here, we need to    nd the output that maximizes it score
plus it   s loss (eq (17.29)). this optimization problem is refered to as
loss-augmented search or loss-augmented id136.

before solving the loss-augmented id136 problem, it   s worth
thinking about why it makes sense. what is   yn? it   s the output that
has the highest score among all outputs, after adding the output   s
corresponding loss to that score. in other words, every incorrect
output gets an arti   cial boost to its score, equal to its loss. the loss is
serving to make imposters look even better than they really are, so if
the truth is to beat an imposter, it has to beat it by a lot. in fact, this
loss augmentation is essentially playing the role of a margin, where
the required margin scales according to the loss.

the algorithmic question, then, is how to compute   yn. in the fully
general case, this is at least as hard as the normal argmax problem, so
we cannot expect a general solution. moreover, even in cases where
the argmax problem is easy (like for sequences), the loss-augmented

206 a course in machine learning

argmax problem can still be dif   cult. in order to make it easier, we
need to assume that the loss decomposes of the input in a way that   s
consistent with the features. in particular, if the structured loss func-
tion is hamming loss, this is often straightforward.

as a concrete example, let   s consider loss-augmented argmax for

sequences under hamming loss. in comparison to the trellis problem
solved in section 17.7, the only difference is that we want to reward
paths that go through incorrect nodes in the trellis! in particular, in
figure 17.1, all of the edges that are not part of the gold standard
path   those that are thinner and grey   get a free    +1    added to their
weights. since hamming loss adds one to the score for any word
that   s predicted incorrectly, this means that every edge in the trellis
that leads to an incorrect node (i.e., one that does not match the gold
truth label) gets a    +1    added to its weight.
again, consider an edge in the trellis that goes from time l     1 to
l, and transitions from y to y(cid:48). in the non-loss-augmented, the weight
of this edge was exactly w      l(x,             y     y(cid:48)). in the loss-augmented
cases, the weight of this edge becomes:
(cid:125)
(cid:124)
(cid:124)
(cid:123)(cid:122)
(cid:125)
w      l(x,             y     y(cid:48))
1[y(cid:48) (cid:54)= yl]

(17.30)

(cid:123)(cid:122)

+

edge score, as before

+1 for mispredictions

once this loss-augmented graph has been constructed, the same max-
weight path algorithm can be run to    nd the loss-augmented argmax
sequence.

17.6 argmax in general

the general argmax problem for structured id88 is the algo-
rithmic question of whether the following can be ef   ciently com-
puted:

  y     argmax
  y   y (x)

w      (x,   y)

(17.31)

we have seen that if the output space y (x) is sequences and the
only types of features are unary features and markov features, then
this can be computed ef   ciently. there are a small number of other
structured output spaces and feature restrictions for which ef   cient
problem-speci   c algorithms exist:

    binary trees, with context-free features: use the cky algorithm
    2d image segmentation, with adjacent-pixel features: use a form of

graph cuts

    spanning trees, with edge-based features: use kruskal   s algorithm
(or for directed spanning trees, use chu-liu/edmonds algorithm)

id170 207

3 i like gurobi best, and it   s free for
academic use. it also has a really nice
python interface.

these special cases are often very useful, and many problems can be
cast in one of these frameworks. however, it is often the case that you
need a more general solution.

one of the most generally useful solutions is to cast the argmax
problem as an integer linear program, or ilp. ilps are a speci   c
type of mathematical program/optimization problem, in which the
objective function being optimized is linear and the constraints are
linear. however, unlike    normal    linear programs, in an ilp you are
allowed to have integer constraints and disallow fractional values.
the general form of an ilp is, for a    xed vector a:

a    z

max

z

subj. to linear constraints on z

(17.32)

the main point is that the constraints on z are allowed to include
constraints like z3     {0, 1}, which is considered an integer constraint.
being able to cast your argmax problem as an ilp has the advan-
tage that there are very good, ef   ciently, well-engineered ilp solvers
out there in the world.3 ilps are not a panacea though: in the worst
case, the ilp solver will be horribly inef   cient. but for prototyping,
or if there are no better options, it   s a very handy technique.

figuring out how exactly to cast your argmax problem as an ilp

can be a bit challenging. let   s start with an example of encoding
sequence labeling with markov features as an ilp. we    rst need
to decide what the variables will be. because we need to encode
pairwise features, we will let our variables be of the form:

zl,k(cid:48),k = 1[label l is k and label l     1 is k(cid:48)]

(17.33)

these zs will all be binary indicator variables.
our next task is to construct the linear objective function. to do
so, we need to assign a value to al,k(cid:48),k in such a way that a    z will be
exactly equal to w      (x, y(z)), where y(z) denotes the sequence that
we can read off of the variables z. with a little thought, we arrive at:

al,k(cid:48),k = w      l(x,(cid:104). . . , k(cid:48), k(cid:105))

(17.34)

finally, we need to construct constaints. there are a few things that
these constraints need to enforce:
1. that all the zs are binary. that   s easy: just say zl,k(cid:48),k     {0, 1}, for

all l, k(cid:48), k.

2. that for a given position l, there is exactly one active z. we can do

this with an equality constraint:    k    k(cid:48) zl,k(cid:48),k = 1 for all l.

3. that the zs are internally consistent: if the label at position 5 is
supposed to be    noun    then both z5,.,. and z6,.,. need to agree on

208 a course in machine learning

this. we can do this as:    k(cid:48) zl,k(cid:48),k =    k(cid:48)(cid:48) zl+1,k,k(cid:48)(cid:48) for all l, k. effec-
tively what this is saying is that z5,?,verb = z6,verb,? where the    ?   
means    sum over all possibilities.   

this fully speci   es an ilp that you can relatively easily implement
(arguably more easily than the dynamic program in algorithm 17.7)
and which will solve the argmax problem for you. will it be ef   cient?
in this case, probably yes. will it be as ef   cient as the dynamic pro-
gram? probably not.

it takes a bit of effort and time to get used to casting optimization
problems as ilps, and certainly not all can be, but most can and it   s a
very nice alternative.

in the case of loss-augmented search for structured id166s (as

opposed to structured id88), the objective function of the ilp
will need to be modi   ed to include terms corresponding to the loss.

17.7 id145 for sequences

recall the decomposition we derived earlier:

w      (x, y) = w    l   

l=1

  l(x, y)

decomposition of structure

(17.35)

=

l   

l=1

w      l(x, y)

associative law

(17.36)

this decomposition allows us to construct the following dynamic
program. we will compute   l,k as the score of the best possible output
pre   x up to and including position l that labels the lth word with
label k. more formally:

  l,k = max
  y1:l   1

w      1:l(x,   y     k)

(17.37)

here,   y is a sequence of length l     1, and   y     k denotes the sequence
of length l obtained by adding k onto the end. the max denotes the
fact that we are seeking the best possible pre   x up to position l     1,
and the forcing the label for position l to be k.

before working through the details, let   s consider an example.

suppose that we   ve computing the   s up to l = 2, and have:   2,noun =
2,   2,verb = 9,   2,adj =    1 (recall: position l = 2 is    eat   ). we want
to extend this to position 3; for example, we want to compute   3,adj.
let   s assume there   s a single unary feature here,    tasty/adj    and
three possible markov features of the form    ?:adj   . assume these
weights are as given to the right. 4 now, the question for   3,adj is:
what   s the score of the best pre   x that labels    tasty    as    adj   ? we can
obtain this by taking the best pre   x up to    eat    and then appending

4 w   tasty/adj    = 1.2
w   noun:adj    =    5
w   verb:adj    = 2.5
w   adj:adj    = 2.2

id170 209

each possible label. whichever combination is best is the winner. the
relevant computation is:

(cid:110)

  2,noun + w   tasty/adj    + w   noun:adj   
  2,verb + w   tasty/adj    + w   verb:adj   
  2,adj + w   tasty/adj    + w   adj:adj   
2 + 1.2     5,

(cid:111)

9 + 1.2 + 2.5,    1 + 1.2 + 2.2

  3,adj = max

= max

= max

(cid:110)
(cid:110)     1.8,

12.7,

2.4

= 12.7

(cid:111)

(cid:111)

(17.38)

(17.39)

(17.40)

this means that (a) the score for the pre   x ending at position 3 la-
beled as adjective is 12.7, and (b) the    winning    previous label was
   verb   . we will need to record these winning previous labels so that
we can extract the best path at the end. let   s denote by   l,k the label
at position l     1 that achieves the max.

from here, we can formally compute the   s recursively. the

main observation that will be necessary is that, because we have
limited ourselves to markov features,   l+1(x,(cid:104)y1, y2, . . . , yl, yl+1(cid:105))
depends only on the last two terms of y, and does not depend on
y1, y2, . . . , yl   1. the full recursion is derived as:

  0,k = 0    k
  0,k =        k

the score for any empty sequence is zero

  l+1,k = max
  y1:l

w      1:l+1(x,   y     k)

w   (cid:16)
(cid:104)

separate score of pre   x from score of position l+1

= max
  y1:l

  1:l(x,   y) +   l+1(x,   y     k)

distributive law over dot products

= max
  y1:l

w      1:l(x,   y) + w      l+1(x,   y     k)
(cid:104)

separate out    nal label from pre   x, call it k   

(cid:17)

(cid:105)

= max
  y1:l   1

max
k(cid:48)

w      1:l(x,   y     k(cid:48)) + w      l+1(x,   y     k(cid:48)     k)

swap order of maxes, and last term doesn   t depend on pre   x

(cid:105)
+ w      l+1(x,(cid:104). . . , k(cid:48), k(cid:105))

(cid:105)

(cid:20)(cid:104)

(cid:104)

= max

k(cid:48)

max
  y1:l   1

w      1:l(x,   y     k(cid:48))

apply recursive de   nition

= max

k(cid:48)

  l,k(cid:48) + w      l+1(x,(cid:104). . . , k(cid:48), k(cid:105))

(cid:105)

(17.41)
(17.42)

(17.43)

(17.44)

(17.45)

(17.46)

(17.47)

(17.48)

(cid:105)

210 a course in machine learning

algorithm 42 argmaxforsequences(x, w)
1: l     len(x)
2:   l,k     0,
  k,l     0,
3: for l = 0 . . . l-1 do
for k = 1 . . . k do
4:

  l+1,k     maxk(cid:48)(cid:2)  l,k(cid:48) + w      l+1(x,(cid:104). . . , k(cid:48), k(cid:105))(cid:3)

    k = 1 . . . k,

   l = 0 . . . l

5:

6:

// recursion:
// here,   l+1(. . . k(cid:48), k . . . ) is the set of features associated with
// output position l + 1 and two adjacent labels k(cid:48) and k at that position
// store backpointer

  l+1,k     the k    that achieves the maximum above

// initialize variables

end for

7:
8: end for
9: y     (cid:104)0, 0, . . . , 0(cid:105)
10: yl     argmaxk   l,k
11: for l = l-1 . . . 1 do
12:
13: end for
14: return y

yl       l,yl+1

// initialize predicted output to l-many zeros
// extract highest scoring    nal label

// traceback    based on yl+1

// return predicted output

(cid:104)

and record a backpointer to the k    that achieves the max

  l,k(cid:48) + w      l+1(x,(cid:104). . . , k(cid:48), k(cid:105))

(17.49)

  l+1,k = argmax

k(cid:48)

(cid:105)

at the end, we can take maxk   l,k as the score of the best output
sequence. to extract the    nal sequence, we know that the best label
for the last word is argmax   l,k. let   s call this   yl once we know that,
the best previous label is   l   1,   yl. we can then follow a path through   
back to the beginning. putting this all together gives algorithm 17.7.
the main bene   t of algorithm 17.7 is that it is guaranteed to ex-

actly compute the argmax output for sequences required in the struc-
tured id88 algorithm, ef   ciently. in particular, it   s runtime is
o(lk2), which is an exponential improvement on the naive o(kl)
runtime if one were to enumerate every possible output sequence.
the algorithm can be naturally extended to handle    higher order   
markov assumptions, where features depend on triples or quadru-
ples of the output. the memoization becomes notationally cumber-
some, but the algorithm remains essentially the same. in order to
handle a length m markov features, the resulting algorithm will take
o(lkm) time. in practice, it   s rare that m > 3 is necessary or useful.

in the case of loss-augmented search for structured id166s (as
opposed to structured id88), we need to include the scores
coming from the loss augmentation in the dynamic program. the
only thing that changes between the standard argmax solution (al-
gorithm 17.7, and derivation in eq (17.48)) is that the any time an
incorrect label is used, the (loss-augmented) score increases by one.
recall that in the non-loss-augmented case, we have the    recursion

id170 211

as:

  l+1,k = max
  y1:l
= max

k(cid:48)

(cid:104)
w      1:l+1(x,   y     k)
  l,k(cid:48) + w      l+1(x,(cid:104). . . , k(cid:48), k(cid:105))

(cid:105)

(17.50)

(17.51)

if we de   ne      to be the loss-augmented score, the corresponding
recursion is (differences highlighted in blue):

    l+1,k = max
  y1:l
= max

k(cid:48)

(cid:104)
w      1:l+1(x,   y     k)+(cid:96)(ham)
    l,k(cid:48) + w      l+1(x,(cid:104). . . , k(cid:48), k(cid:105))

(cid:105)
1:l+1 (y,   y     k)

+1[k (cid:54)= yl+1]

(17.52)

(17.53)

in other words, when computing      in the loss-augmented case,
whenever the output prediction is forced to pass through an incorrect
label, the score for that cell in the dynamic program gets increased
by one. the resulting algorithm is identical to algorithm 17.7, except
that eq (17.53) is used for computing   s.

17.8 further reading

todo

18 | imitation learning

learning objectives:
    be able to formulate imitation

learning problems.

    understand the failure cases of

simple classi   cation approaches to
imitation learning.

    implement solutions to those prob-

lems based on either classi   cation or
dataset aggregation.

    relate id170 and

imitation learning.

dependencies:

programming is a skill best acquired by practice and example
rather than from books.

    alan turing

so far, we have largely considered machine learning
problems in which the goal of the learning algorithm is to make
a single prediction. in many real world problems, however, an algo-
rithm must make a sequence of decisions, with the world possibly
changing during that sequence. such problems are often called se-
quential decision making problems. a straightforward example   
which will be the running example for this chapter   is that of self-
driving cars. we want to train a machine learning algorithm to drive
a car. but driving a car is not a single prediction: it   s a sequence of
predictions over time. and as the machine is driving the car, the
world around it is changing, often based on its own behavior. this
creates complicated feedback loops, and one of the major challenges
we will face is how to deal with these feedback loops.

to make this discussion more concrete, let   s consider the case of a
self-driving car. and let   s consider a very simplistic car, in which the
only decision that has to be made is how to steer, and that   s between
one of three options: {left, right, none}. in the imitation learning
setting, we assume that we have access to an expert or oracle that al-
ready knows how to drive. we want to watch the expert driving, and
learn to imitate their behavior. hence: imitation learning (sometimes
called learning by demonstration or programming by example, in
the sense that programs are learned, and not implemented).

at each point in time t = 1 . . . t, the car recieves sensor informa-
tion xt (for instance, a camera photo ahead of the car, or radar read-
ings). it then has to take an action, at; in the case of the car, this is
one of the three available steering actions. the car then suffers some
loss (cid:96)t; this might be zero in the case that it   s driving well, or large in
the case that it crashes. the world then changes, moves to time step
t + 1, sensor readings xt+1 are observed, action at+1 is taken, loss (cid:96)t+1
is suffered, and the process continues.

the goal is to learn a function f that maps from sensor readings xt
to actions. because of close connections to the    eld of reinforcement
learning, this function is typically called a policy. the measure of

imitation learning 213

1 it   s completely okay for f to look
at more than just xt when making
predictions; for instance, it might want
to look at xt   1, or at   1 and at   2. as
long as it only references information
from the past, this is    ne. for notational
simplicity, we will assume that all of
this relevant information is summarized
in xt.

figure 18.1: a single expert trajectory in
a self-driving car.

success of a policy is: if we were to run this policy, how much total
loss would be suffered. in particular, suppose that the trajectory
(denoted   ) of observation/action/reward triples encountered by
your policy is:

   = x1 ,

, (cid:96)1 , x2 ,

, (cid:96)2 ,

. . . , xt ,

, (cid:96)t

(18.1)

a2(cid:124)(cid:123)(cid:122)(cid:125)

= f (x2)

at(cid:124)(cid:123)(cid:122)(cid:125)

= f (xt )

(cid:104)   t

the losses (cid:96)t depend implicitly on the state of the world and the
actions of the policy. the goal of f is to minimize the expected loss
      f
e
the world, and the sequence of actions taken is according to f .1

, where the expectation is taken over all randomness in

t=1 (cid:96)t

= f (x1)

a1(cid:124)(cid:123)(cid:122)(cid:125)
(cid:105)

18.1

imitation learning by classi   cation

we will begin with a straightforward, but brittle, approach to imita-
tion learning. we assume access to a set of training trajectories taken
by an expert. for example, consider a self-driving car, like that in fig-
ure 18.1. a single trajectory    consists of a sequence of observations
(what is seen from the car   s sensors) and a sequence of actions (what
action did the expect take at that point in time). the idea in imitation
learning by classi   cation is to learn a classi   er that attempts to mimic
the expert   s action based on the observations at that time.

in particular, we have   1,   2, . . . ,   n. each of the n trajectories
comprises a sequence of t-many observation/action/loss triples,
where the action is the action taken by the expert. t, the length of
the trajectory is typically called the time horizon (or just horizon).
for instance, we may ask an expert human driver to drive n = 20
different routes, and record the observations and actions that driver
saw and took during those routes. these are our training trajectories.
we assume for simplicity that each of these trajectories is of    xed
length t, though this is mostly for notational convenience.

the most straightforward thing we can do is convert this expert

data into a big multiclass classi   cation problem. we take our favorite
multiclass classi   cation algorithm, and use it to learn a mapping
from x to a. the data on which it is trained is the set of all observa-
tion/action pairs visited during any of the n trajectories. in total,
this would be nt examples. this approach is summarized in algo-
rithm 18.1 for training and algorithm 18.1 for prediction.

how well does this approach work?
the    rst question is: how good is the expert? if we learn to mimic
an expert, but the expert is no good, we lose. in general, it also seems
unrealistic to believe this algorithm should be able to improve on
the expert. similarly, if our multiclass classi   cation algorithm a
is crummy, we also lose. so part of the question    how well does

expertexpert214 a course in machine learning

1: d    (cid:10)(x, a) :    n ,    (x, a, (cid:96))       n
algorithm 43 supervisedimitationtrain(a,   1,   2, . . . ,   n)
2: return a(d)

// collect all observation/action pairs
// train multiclass classi   er on d

(cid:11)

algorithm 44 supervisedimitationtest( f )
1: for t = 1 . . . t do
2:

xt     current observation
at     f (xt)
take action at
(cid:96)t     observe instantaneous loss

3:

4:

5:
6: end for
7: return    t

t=1 (cid:96)t

// ask policy to choose an action

// return total loss

this work    is the more basic question of: what are we even trying to
measure?

there is a nice theorem2 that gives an upper bound on the loss
suffered by the supervisedil algorithm (algorithm 18.1) as a func-
tion of (a) the quality of the expert, and (b) the error rate of the
learned classi   er. to be clear, we need to distinguish between the
loss of the policy when run for t steps to form a full trajectory, and
the error rate of the learned classi   er, which is just it   s average mul-
ticlass classi   cation error. the theorem states, roughly, that the loss
of the learned policy is at most the loss of the expert plus t2 times the
error rate of the classi   er.

2 ross et al. 2011

theorem 18 (loss of supervisedil). suppose that one runs algo-
rithm 18.1 using a multiclass classi   er that optimizes the 0-1 loss (or an
upperbound thereof). let   be the error rate of the underlying classi   er
(in expectation) and assume that all instantaneous losses are in the range
[0, (cid:96)(max)]. let f be the learned policy; then:

(cid:34)
(cid:123)(cid:122)

   
t

(cid:96)t

e

      f

(cid:124)

(cid:35)
(cid:125)

(cid:34)
    e     expert
(cid:124)
(cid:123)(cid:122)

   
t

(cid:96)t

(cid:35)
(cid:125)

loss of learned policy

loss of expert

+(cid:96)(max)t2 

(18.2)

intuitively, this bound on the loss is about a factor of t away from
what we might hope for. in particular, the multiclass classi   er makes
errors on an   fraction of it   s actions, measured by zero/one loss.
in the worst case, this will lead to a loss of (cid:96)(max)  for a single step.
summing all these errors over the entire trajectory would lead to
a loss on the order of (cid:96)(max)t , which is a factor t better than this
theorem provides. a natural question (addressed in the next section)
is whether this is analysis is tight. a related question (addressed in
the section after that) is whether we can do better. before getting
there, though, it   s worth highlighting that an extra factor of t is really

imitation learning 215

bad. it can cause even very small multiclass error rates to blow up; in
particular, if       1/t, we lose, and t can be in the hundreds or more.

18.2 failure analysis

the biggest single issue with the supervised learning approach to
imitation learning is that it cannot learn to recover from failures. that
is: it has only been trained based on expert trajectories. this means
that the only training data it has seen is that of an expert driver. if
it ever veers from that state distribution, it may have no idea how
to recover. as a concrete example, perhaps the expert driver never
ever gets themselves into a state where they are directly facing a
wall. moreover, the expert driver probably tends to drive forward
more than backward. if the imperfect learner manages to make a few
errors and get stuck next to a wall, it   s likely to resort to the general
   drive forward    rule and stay there forever. this is the problem of
compounding error; and yes, it does happen in practice.

it turns out that it   s possible to construct an imitation learning

problem on which the t2 compounding error is unavoidable. con-
sider the following somewhat arti   cial problem. at time t = 1 you   re
shown a picture of either a zero or a one. you have two possible ac-
tions: press a button marked    zero    or press a button marked    one.   
the    correct    thing to do at t = 1 is to press the button that corre-
sponds to the image you   ve been shown. pressing the correct button
leads to (cid:96)1 = 0; the incorrect leads to (cid:96)1 = 1. now, at time t = 2 you
are shown another image, again of a zero or one. the correct thing to
do in this time step is the xor of (a) the number written on the picture
you see right now, and (b) the correct answer from the previous time
step. this holds in general for t > 1.

there are two important things about this construction. the    rst

is that the expert can easily get zero loss. the second is that once the
learned policy makes a single mistake, this can cause it to make all
future decisions incorrectly. (at least until it    luckily    makes another
   mistake    to get it back on track.)

based on this construction, you can show the following theorem3.

3 k    ri  inen 2006

theorem 19 (lower bound for supervisedil). there exist imitation
learning problems on which algorithm 18.1 is able to achieve small classi   -
cation error       [0, 1/t] under an optimal expert, but for which the test loss
is lower bounded as:

(cid:34)
(cid:123)(cid:122)

e

      f

(cid:124)

(cid:35)
(cid:125)

loss of learned policy

   
t

(cid:96)t

    t + 1

2

    1
4 

(cid:104)

1     (1     2 )t+1(cid:105)

(18.3)

which is bounded by t2  and, for small  , grows like t2 .

216 a course in machine learning

up to constants, this gives matching upper and lower bounds for
the loss of a policy learned by supervised imitation learning that is
pretty far (a factor of t) from what we might hope for.

18.3 dataset aggregation

supervised imitation learning fails because once it gets    off the ex-
pert path,    things can go really badly. ideally, we might want to train
our policy to deal with any possible situation it could encounter.
unfortunately, this is unrealistic: we cannot hope to be able to train
on every possible con   guration of the world; and if we could, we
wouldn   t really need to learn anyway, we could just memorize. so
we want to train f on a subset of world con   gurations, but using
   con   gurations visited by the expert    fails because f cannot learn to
recover from its own errors. somehow what we   d like to do is train f
to do well on the con   gurations that it, itself, encounters!

this is a classic chicken-and-egg problem. we want a policy f that
does well in a bunch of world con   gurations. what set of con   gura-
tions? the con   gurations that f encounters! a very classic approach
to solving chicken-and-egg problems is iteration. start with some
policy f . run f and see what con   gurations is visits. train a new f
to do well there. repeat.

this is exactly what the dataset aggregation algorithm (   dagger   )

does. continuing with the self-driving car analogy, we    rst let a
human expert drive a car for a while, and learn an initial policy f0 by
running standard supervised imitation learning (algorithm 18.1) on
the trajectories visited by the human. we then do something unusual.
we put the human expert in the car, and record their actions, but the
car behaves not according to the expert   s behavior, but according to
f0. that is, f0 is in control of the car, and the expert is trying to steer,
but the car is ignoring them4 and simply recording their actions as
training data. this is shown in figure 18.2.

based on trajectories generated by f0 but actions given by the

expert, we generate a new dataset that contains information about
how to recover from the errors of f0. we now will train a new policy,
f1. because we don   t want f1 to    forget    what f0 already knows, f1
is trained on the union of the initial expert-only trajectories together
with the new trajectories generated by f0. we repeat this process a
number of times maxiter, yielding algorithm 18.3.

this algorithm returns the list of all policies generated during its
run. a very practical question is: which one should you use? there
are essentially two choices. the    rst choice is just to use the    nal
policy learned. the problem with this approach is that dagger can
be somewhat unstable in practice, and policies do not monotonically

4 this is possibly terrifying for the
expert!

figure 18.2: in dagger, the trajectory
(red) is generated according to the
previously learned policy, f0, but the
gold standard actions are given by the
expert.

expertexpertff00imitation learning 217

(cid:11) // collect all pairs (same as supervised)

// train initial policy (multiclass classi   er) on d0

(0)

(0)
n

algorithm 45 daggertrain(a, maxiter, n, expert)
1: (cid:104)  

n=1     run the expert n many times

3:
4: for i = 1 . . . maxiter do
5:

n (cid:105)n
f0     a(d0)
(cid:104)  

2: d0    (cid:10)(x, a) :    n ,    (x, a, (cid:96))       
di    (cid:10)(x, expert(x)) :    n ,    (x, a, (cid:96))       
fi     a(cid:16)(cid:83)i

n=1     run policy fi   1 n-many times

n (cid:105)n

(cid:17)

(cid:11)

j=0 dj

(i)

6:

7:
8: end for
9: return (cid:104) f0, f1, . . . , fmaxiter(cid:105)

(i)
n

// trajectories by fi   1
// collect data set
// observations x visited by fi   1
// but actions according to the expert
// train policy fi on union of all data so far

// return collection of all learned policies

improve. a safer alternative (as we   ll see by theory below) is to test
all of them on some held-out    development    tasks, and pick the one
that does best there. this requires a bit more computation, but is a
much better approach in general.

one major difference in requirements between dagger (algo-

rithm 18.3) and supervisedil (algorithm 18.1) is the requirement
of interaction with the expert. in supervisedil, you only need access
to a bunch of trajectories taken by the expert, passively. in dagger,
you need access to them expert themselves, so you can ask questions
like    if you saw con   guration x, what would you do?    this puts
much more demand on the expert.

another question that arises is: what should n, the number of
trajectories generated in each round, be? in practice, the initial n
should probably be reasonably large, so that the initial policy f0
is pretty good. the number of trajectories generated by iteration
subsequently can be much smaller, perhaps even just one.

intuitively, dagger should be less sensitive to compounding error
than supervisedil, precisely because it gets trained on observations
that it is likely to see at test time. this is formalized in the following
theorem:

theorem 20 (loss of dagger). suppose that one runs algorithm 18.3
using a multiclass classi   er that optimizes the 0-1 loss (or an upperbound
thereof). let   be the error rate of the underlying classi   er (in expectation)
and assume that all instantaneous losses are in the range [0, (cid:96)(max)]. let f be
the learned policy; then:

(cid:34)
(cid:123)(cid:122)

   
t

(cid:96)t

e

      f

(cid:124)

(cid:35)
(cid:125)

(cid:34)
    e     expert
(cid:123)(cid:122)
(cid:124)

   
t

(cid:96)t

(cid:35)
(cid:125)

loss of learned policy

loss of expert

+(cid:96)(max)t  + o

(cid:18) (cid:96)(max)t log t

(cid:19)

maxiter

(18.4)

furthermore, if the id168 is strongly convex in f , and maxiter is

218 a course in machine learning

  o(t/ ), then:

(cid:34)
(cid:123)(cid:122)

   
t

(cid:96)t

e

      f

(cid:124)

(cid:35)
(cid:125)

(cid:34)
    e     expert
(cid:123)(cid:122)
(cid:124)

   
t

(cid:96)t

(cid:35)
(cid:125)

+(cid:96)(max)t  + o( )

(18.5)

loss of learned policy

loss of expert

both of these results show that, assuming maxiter is large enough,

the loss of the learned policy f (here, taken to be the best on of all
the maxiter policies learned) grows like t , which is what we hope
for. note that the    nal term in the    rst bound gets small so long as
maxiter is at least t log t.

18.4 expensive algorithms as experts

because of the strong requirement on the expert in dagger (i.e., that
you need to be able to query it many times during training), one of
the most substantial use cases for dagger is to learn to (quickly) imi-
tate otherwise slow algorithms. here are two prototypical examples:
1. game playing. when a game (like chess or minecraft) can be run

in simulation, you can often explicitly compute a semi-optimal
expert behavior with brute-force search. but this search might be
too computationally expensive to play in real time, so you can
use it during training time, learning a fast policy that attempts
to mimic the expensive search. this learned policy can then be
applied at test time.

2. discrete optimizers. many discrete optimization problems can be
computationally expensive to run in real time; for instance, even
shortest path search on a large graph can be too slow for real time
use. we can compute shortest paths of   ine as    training data    and
then use imitation learning to try to build shortest path optimizers
that will run suf   ciently ef   ciently in real time.

consider the game playing example, and for concreteness, sup-
pose you are trying to learn to play solitaire (this is an easier exam-
ple because it   s a single player game). when running daggertrain
(algorithm 18.3 to learn a chess-playing policy, the algorithm will
repeatedly ask for expert(x), where x is the current state of the game.
what should this function return? ideally, it should return the/an ac-
tion a such that, if a is taken, and then the rest of the game is played
optimally, the player wins. computing this exactly is going to be very
dif   cult for anything except the simplest games, so we need to restort
to an approxiamtion.

imitation learning 219

algorithm 46 depthlimiteddfs(x, h, maxdepth)
1: if x is a terminal state or maxdepth     0 then

return (   , h(x))

// if we cannot search deeper
// return    no action    (   ) and the current heuristic score

3: else
4:

bestaction, bestscore        ,       
for all actions a from x do

// keep track of best action & its score

(_, score)     depthlimiteddfs(x     a, h, maxdepth     1)

// get score
// for action a, depth reduced by one by appending a to x

if score > bestscore then

bestaction, bestscore     a, score

// update tracked best action & score

2:

5:

6:

7:

8:

9:

end if
end for

10:
11: end if
12: return (bestaction, bestscore)

// return best found action and its score

a common strategy is to run a depth-limited depth    rst search,

starting at state x, and terminating after at most three of four moves
(see figure 18.3). this will generate a search tree. unless you are
very near the end of the game, none of the leaves of this tree will
correspond to the end of the game. so you   ll need some heuristic, h,
for evaluating states that are non-terminals. you can propagate this
heuristic score up to the root, and choose the action that looks best
with this depth four search. this is not necessarily going to be the
optimal action, and there   s a speed/accuracy trade-off for searching
deeper, but this is typically effective. this approach summarized in
algorithm 18.4.

18.5 id170 via imitation learning

a    nal case where an expert can often be computed algorithmically
arises when one solves id170 (see chapter 17) via
imitation learning. it is clearest how this can work in the case of
sequence labeling. recall there that predicted outputs should be
sequences of labels. the running example from the earlier chapter
was:

figure 18.3: imit:dldfs: depth limited
depth-   rst search

x =     monsters eat tasty bunnies    
y =

noun verb adj

noun

(18.6)
(18.7)

one can easily cast the prediction of y as a sequential decision mak-
ing problem, by treating the production of y in a left-to-right manner.
in this case, we have a time horizon t = 4. we want to learn a policy
f that    rst predicts    noun    then    verb    then    adj    then    noun    on
this input.

220 a course in machine learning

let   s suppose that the input to f consists of features extracted both
from the input (x) and the current predicted output pre   x   y, denoted
  (x,   y). for instance,   (x,   y) might represent a similar set of features
to those use in chapter 17. it is perhaps easiest to think of f as just
a classi   er: given some features of the input sentence x (   monsters
eat tasty bunnies   ), and some features about previous predictions in
the output pre   x (so far, produced    noun verb   ), the goal of f is to
predict the tag for the next word (   tasty   ) in this context.

an important question is: what is the    expert    in this case? in-
tuitively, the expert should provide the correct next label, but what
does this mean? that depends on the id168 being optimized.
under hamming loss (sum zero/one loss over each individual pre-
diction), the expert is straightforward. when the expert is asked to
produce an action for the third word, the expert   s response is always
   adj    (or whatever happens to be the correct label for the third word
in the sentence it is currently training on).

more generally, the expert gets to look at x, y and a pre   x   y of the
output. note, importantly, that the pre   x might be wrong! in particular,
after the    rst iteration of dagger, the pre   x will be predicted by
the learned policy, which may make mistakes! the expert also has
some structured id168 (cid:96) that it is trying to minimize. like
in the previous section, the expert   s goal is to choose the action that
minimizes the long-term loss according to (cid:96) on this example.

to be more formal, we need a bit of notation. let best((cid:96), y,   y)

denote the loss (according to (cid:96) and the ground truth y) of the best
reachable output starting at   y. for instance, if y is    noun verb adj
noun    and   y is    noun noun   , and the loss is hamming loss, then the
best achievable output (predicting left-to-right) is    noun noun adj
noun    which has a loss of 1. thus, best for this situation is 1.

given that notion of best, the expert is easy to de   ne:

expert((cid:96), y,   y) = argmin

a

best((cid:96), y,   y     a)

(18.8)

namely, it is the action that leads to the best possible completion
after taking that action. so in the example above, the expert action
is    adj   . for some problems and some id168s, computing
the expert is easy. in particular, for sequence labeling under ham-
ming loss, it   s trivial. in the case that you can compute the expert
exactly, it is often called an oracle.5 for some other problems, exactly
computing an oracle is computationally expensive or intractable. in
those cases, one can often resort to depth limited depth-   rst-search
(algorithm 18.4) to compute an approximate oracle as an expert.

to be very concrete, a typical implementation of dagger applied
to sequence labeling would go as follows. each structured training
example (a pair of sentence and tag-sequence) gives rise to one trajec-

5 some literature calls it a    dynamic
oracle   , though the extra word is
unnecessary.

imitation learning 221

tory. at training time, a predict tag seqence is generated left-to-right,
starting with the empty sequence. at any given time step, you are
attempting to predict the label of the tth word in the input. you de-
   ne a feature vector   (x,   y), which will typically consist of: (a) the tth
word, (b) left and right neighbors of the tth word, (c) the last few pre-
dictions in   y, and (d) anything else you can think of. in particular, the
features are not limited to markov style features, because we   re not
longer trying to do id145. the expert label for the
tth word is just the corresponding label in the ground truth y. given
all this, one can run dagger (algorithm 18.4) exactly as speci   ed.
moving to id170 problems other than sequence

labeling problems is beyond the scope of this book. the general
framework is to cast your id170 problem as a sequen-
tial decision making problem. once you   ve done that, you need to
decide on features (this is the easy part) and an expert (this is often
the harder part). however, once you   ve done so, there are generic
libraries for    compiling    your speci   cation down to code.

18.6 further reading

todo further reading

code and datasets

rating

+2
+2
+2
+2
+2
+1
+1
+1
0
0
0
0
-1
-1
-1
-1
-2
-2
-2
-2

easy? ai? sys? thy? morning?

y
y
n
n
n
y
y
n
n
y
n
y
y
n
n
y
n
n
y
y

y
y
y
n
y
y
y
y
n
n
y
y
y
n
n
n
n
y
n
n

n
n
n
n
y
n
n
n
n
n
n
y
y
y
y
y
y
y
y
y

y
y
n
y
n
n
y
y
n
y
y
y
n
y
n
n
y
n
n
n

n
n
n
n
y
n
n
n
y
y
n
y
y
n
y
y
n
y
n
y

table 1: course rating data set

bibliography

shai ben-david, john blitzer, koby crammer, and fernando pereira.
analysis of representations for id20. advances in
neural information processing systems, 19:137, 2007.

steffen bickel, michael bruckner, and tobias scheffer. discriminative
learning for differing training and test distributions. in proceedings
of the international conference on machine learning (icml), 2007.

sergey brin. near neighbor search in large metric spaces. in confer-
ence on very large databases (vldb), 1995.

hal daum   iii. frustratingly easy id20. in conference
of the association for computational linguistics (acl), prague, czech
republic, 2007.

sorelle a friedler, carlos scheidegger, and suresh venkatasub-
ramanian. on the (im)possibility of fairness. arxiv preprint
arxiv:1609.07236, 2016.

moritz hardt, eric price, and nathan srebro. equality of oppor-
tunity in supervised learning. in advances in neural information
processing systems, pages 3315   3323, 2016.

matti k    ri  inen. lower bounds for reductions. talk at the atomic
learning workshop (tti-c), march 2006.
tom m. mitchell. machine learning. mcgraw hill, 1997.
j. ross quinlan. induction of id90. machine learning, 1(1):
81   106, 1986.

frank rosenblatt. the id88: a probabilistic model for infor-
mation storage and organization in the brain. psychological review,
65:386   408, 1958. reprinted in neurocomputing (mit press, 1998).

st  phane ross, geoff j. gordon, and j. andrew bagnell. a reduction
of imitation learning and id170 to no-regret online
learning. in proceedings of the workshop on arti   cial intelligence and
statistics (aistats), 2011.

224 a course in machine learning

index

k-nearest neighbors, 58
da-distance, 113
p-norms, 92
0/1 loss, 88
80% rule, 111

absolute loss, 14
activation function, 130
activations, 41
adaboost, 166
adaptation, 105
algorithm, 87
all pairs, 80
all versus all, 80
approximation error, 71
architecture selection, 139
area under the curve, 64, 84
argmax problem, 199
auc, 64, 83, 84
ava, 80
averaged id88, 52

back-propagation, 134, 137
bag of words, 56
id112, 165
base learner, 164
batch, 173
bayes error rate, 20
bayes optimal classi   er, 19
bayes optimal error rate, 20
bayes rule, 117
bernouilli distribution, 121
bias, 42
bias/variance trade-off, 72
binary features, 30
bipartite ranking problems, 83
boosting, 155, 164
bootstrap resampling, 165
id64, 67, 69

categorical features, 30
chain rule, 117, 120
chord, 90
circuit complexity, 138
id91, 35, 178
id91 quality, 178
complexity, 34
compounding error, 215
concave, 90
concavity, 193
concept, 157
con   dence intervals, 68
constrained optimization problem,

100

contour, 92
convergence rate, 95
convex, 87, 89
covariate shift, 105
cross validation, 65, 68
cubic feature map, 144
curvature, 95

data covariance matrix, 184
data generating distribution, 15
decision boundary, 34
decision stump, 168
decision tree, 8, 10
id90, 57
density estimation, 107
development data, 26
id84, 178
discrepancy, 113
discrete distribution, 121
disparate impact, 111
distance, 31
id20, 105
dominates, 63
dot product, 45
dual problem, 151

dual variables, 151

early stopping, 53, 132
embedding, 178
ensemble, 164
error driven, 43
error rate, 88
estimation error, 71
euclidean distance, 31
evidence, 127
example id172, 59, 60
examples, 9
expectation maximization, 186, 189
expected loss, 16
expert, 212
exponential loss, 90, 169

feasible region, 101
feature augmentation, 109
feature combinations, 54
feature mapping, 54
feature id172, 59
feature scale, 33
feature space, 31
feature values, 11, 29
feature vector, 29, 31
features, 11, 29
forward-propagation, 137
fractional assignments, 191
furthest-   rst heuristic, 180

gaussian distribution, 121
gaussian kernel, 147
gaussian mixture models, 191
generalize, 9, 17
generative story, 123
geometric view, 29
global minimum, 94
gmm, 191

226 a course in machine learning

gradient, 93
gradient ascent, 93
id119, 93

hamming loss, 202
hard-margin id166, 101
hash kernel, 177
held-out data, 26
hidden units, 129
hidden variables, 186
hinge loss, 90, 203
histogram, 12
horizon, 213
hyperbolic tangent, 130
hypercube, 38
hyperparameter, 26, 44, 89
hyperplane, 41
hyperspheres, 38
hypothesis, 71, 157
hypothesis class, 71, 160
hypothesis testing, 67

i.i.d. assumption, 117
identically distributed, 24
ilp, 195, 207
imbalanced data, 73
imitation learning, 212
importance sampling, 106
importance weight, 74
independent, 24
independently, 117
independently and identically dis-

tributed, 117

indicator function, 88
induce, 16
induced distribution, 76
induction, 9
inductive bias, 20, 31, 33, 91, 121
integer linear program, 207
integer id135, 195
iteration, 36

jack-kni   ng, 69
jensen   s inequality, 193
joint, 124

kkt conditions, 152

label, 11
lagrange multipliers, 119
lagrange variable, 119
lagrangian, 119
lattice, 200
layer-wise, 139
learning by demonstration, 212
leave-one-out cross validation, 65
level-set, 92
license, 2
likelihood, 127
linear classi   er, 169
linear classi   ers, 169
linear decision boundary, 41
id75, 98
linearly separable, 48
link function, 130
log likelihood, 118
log posterior, 127
log id203, 118
log-likelihood ratio, 122
logarithmic transformation, 61
logistic loss, 90
id28, 126
loo cross validation, 65
id168, 14
loss-augmented id136, 205
loss-augmented search, 205

margin, 49, 100
margin of a data set, 49
marginal likelihood, 127
marginalization, 117
markov features, 198
maximum a posteriori, 127
maximum depth, 26
id113, 118
mean, 59
mercer   s condition, 146
model, 87
modeling, 25
multi-layer network, 129

k-nearest neighbors, 32
karush-kuhn-tucker conditions, 152
kernel, 141, 145
kernel trick, 146
kernels, 54

naive bayes assumption, 120
nearest neighbor, 29, 31
neural network, 169
neural networks, 54, 129
neurons, 41
noise, 21

non-convex, 135
non-linear, 129
normal distribution, 121
normalize, 46, 59
null hypothesis, 67

objective function, 88
one versus all, 78
one versus rest, 78
online, 42
optimization problem, 88
oracle, 212, 220
oracle experiment, 28
output unit, 129
ova, 78
over   tting, 23
oversample, 76

p-value, 67
pac, 156, 166
paired t-test, 67
parametric test, 67
parity, 21
parity function, 138
patch representation, 56
pca, 184
id88, 41, 42, 58
perpendicular, 45
pixel representation, 55
policy, 212
polynomial kernels, 146
positive semi-de   nite, 146
posterior, 127
precision, 62
precision/recall curves, 63
predict, 9
preference function, 82
primal variables, 151
principle components analysis, 184
prior, 127
probabilistic modeling, 116
probably approximately correct, 156
programming by example, 212
projected gradient, 151
projection, 46
psd, 146

radial basis function, 139
id79s, 169
random variable, 117
rbf kernel, 147

rbf network, 139
recall, 62
receiver operating characteristic, 64
reconstruction error, 184
reductions, 76
redundant features, 56
regularized objective, 89
regularizer, 88, 91
id23, 212
representer theorem, 143, 145
roc curve, 64

sample complexity, 157, 158, 160
sample mean, 59
sample selection bias, 105
sample variance, 59
semi-supervised adaptation, 106
sensitivity, 64
separating hyperplane, 87
sequential decision making, 212
sgd, 173
shallow decision tree, 21, 168
shape representation, 56
sigmoid, 130
sigmoid function, 126
sigmoid network, 139
sign, 130
single-layer network, 129
singular, 98
slack, 148
slack parameters, 101
smoothed analysis, 180
soft assignments, 190
soft-margin id166, 101

span, 143
sparse, 92
speci   city, 64
squared loss, 14, 90
statistical id136, 116
statistically signi   cant, 67
steepest ascent, 93
stochastic id119, 173
stochastic optimization, 172
strong law of large numbers, 24
strong learner, 166
strong learning algorithm, 166
strongly convex, 95
structural risk minimization, 87
structured hinge loss, 203
id170, 195
sub-sampling, 75
subderivative, 96
subgradient, 96
subid119, 97
sum-to-one, 117
support vector machine, 100
support vectors, 153
surrogate loss, 90
symmetric modes, 135

t-test, 67
test data, 25
test error, 25
test set, 9
text categorization, 56
the curse of dimensionality, 37
threshold, 42
tikhonov id173, 87

index

227

time horizon, 213
total variation distance, 113
train/test mismatch, 105
training data, 9, 16, 24
training error, 16
trajectory, 213
trellis, 200
trucated gradients, 175
two-layer network, 129

unary features, 198
unbiased, 47
under   tting, 23
unit hypercube, 39
unit vector, 46
unsupervised adaptation, 106
unsupervised learning, 35

validation data, 26
vapnik-chernovenkis dimension, 162
variance, 59, 165
variational distance, 113
vc dimension, 162
vector, 31
visualize, 178
vote, 32
voted id88, 52
voting, 52

weak learner, 166
weak learning algorithm, 166
weights, 41

zero/one loss, 14

