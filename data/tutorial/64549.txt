springer series in statistics

trevor hastie     robert tibshirani     jerome friedman
the elements of statictical learning

during the past decade there has been an explosion in computation and information tech-
nology. with it have come vast amounts of data in a variety of fields such as medicine, biolo-
gy, finance, and marketing. the challenge of understanding these data has led to the devel-
opment of new tools in the field of statistics, and spawned new areas such as data mining,
machine learning, and bioinformatics. many of these tools have common underpinnings but
are often expressed with different terminology. this book describes the important ideas in
these areas in a common conceptual framework. while the approach is statistical, the
emphasis is on concepts rather than mathematics. many examples are given, with a liberal
use of color graphics. it should be a valuable resource for statisticians and anyone interested
in data mining in science or industry. the book   s coverage is broad, from supervised learning
(prediction) to unsupervised learning. the many topics include neural networks, support
vector machines, classification trees and boosting   the first comprehensive treatment of this
topic in any book.

this major new edition features many topics not covered in the original, including graphical
models, id79s, ensemble methods, least angle regression & path algorithms for the
lasso, non-negative id105, and spectral id91. there is also a chapter on
methods for    wide    data (p bigger than n), including multiple testing and false discovery rates.

trevor hastie, robert tibshirani, and jerome friedman are professors of statistics at
stanford university. they are prominent researchers in this area: hastie and tibshirani
developed generalized additive models and wrote a popular book of that title. hastie co-
developed much of the statistical modeling software and environment in r/s-plus and
invented principal curves and surfaces. tibshirani proposed the lasso and is co-author of the
very successful an introduction to the bootstrap. friedman is the co-inventor of many data-
mining tools including cart, mars, projection pursuit and gradient boosting.

s t at i s t i c s

isbn 978-0-387-84857-0

    springer.com

h
a
s
t
i
e
 
   
 
t
i
b
s
h
i
r
a
n

i
 
   
 
f
r
i
e
d
m
a
n

 

 

t
h
e
e
l
e
m
e
n
t
s
o
f
 
s
t
a
t
i
s
t
i
c
a
l
 
l
e
a
r
n
n
g

i

springer series in statistics

trevor hastie
robert tibshirani
jerome friedman

the elements of
statistical learning
data mining, id136, and prediction

second edition

this is page v
printer: opaque this

to our parents:

valerie and patrick hastie

vera and sami tibshirani

florence and harry friedman

and to our families:

samantha, timothy, and lynda

charlie, ryan, julie, and cheryl

melanie, dora, monika, and ildiko

vi

preface to the second edition

this is page vii
printer: opaque this

in god we trust, all others bring data.

   william edwards deming (1900-1993)1

we have been grati   ed by the popularity of the    rst edition of the
elements of statistical learning. this, along with the fast pace of research
in the statistical learning    eld, motivated us to update our book with a
second edition.

we have added four new chapters and updated some of the existing
chapters. because many readers are familiar with the layout of the    rst
edition, we have tried to change it as little as possible. here is a summary
of the main changes:

1on the web, this quote has been widely attributed to both deming and robert w.
hayden; however professor hayden told us that he can claim no credit for this quote,
and ironically we could    nd no    data    con   rming that deming actually said this.

viii

preface to the second edition

chapter
1. introduction
2. overview of supervised learning
3. linear methods for regression

4. linear methods for classi   cation
5. basis expansions and regulariza-
tion
6. kernel smoothing methods
7. model assessment and selection

8. model id136 and averaging
9. additive models, trees, and
related methods
10. boosting and additive trees

11. neural networks

12. support vector machines and
flexible discriminants
13.
nearest-neighbors
14. unsupervised learning

prototype methods

and

15. id79s
16. id108
17. undirected id114
18. high-dimensional problems

some further notes:

what   s new

lar algorithm and generalizations
of the lasso
lasso path for id28
additional illustrations of rkhs

strengths and pitfalls of
validation

cross-

new example from ecology; some
material split o    to chapter 16.
bayesian neural nets and the nips
2003 challenge
path algorithm for id166 classi   er

dimension

spectral id91, kernel pca,
sparse pca, non-negative matrix
factorization archetypal
analysis,
nonlinear
reduction,
google page
rank algorithm, a
direct approach to ica
new
new
new
new

    our    rst edition was unfriendly to colorblind readers; in particular,
we tended to favor red/green contrasts which are particularly trou-
blesome. we have changed the color palette in this edition to a large
extent, replacing the above with an orange/blue contrast.

    we have changed the name of chapter 6 from    kernel methods    to
   kernel smoothing methods   , to avoid confusion with the machine-
learning kernel method that is discussed in the context of support vec-
tor machines (chapter 12) and more generally in chapters 5 and 14.

    in the    rst edition, the discussion of error-rate estimation in chap-
ter 7 was sloppy, as we did not clearly di   erentiate the notions of
conditional error rates (conditional on the training set) and uncondi-
tional rates. we have    xed this in the new edition.

preface to the second edition

ix

    chapters 15 and 16 follow naturally from chapter 10, and the chap-

ters are probably best read in that order.

    in chapter 17, we have not attempted a comprehensive treatment
of id114, and discuss only undirected models and some
new methods for their estimation. due to a lack of space, we have
speci   cally omitted coverage of directed id114.

    chapter 18 explores the    p     n     problem, which is learning in high-
dimensional feature spaces. these problems arise in many areas, in-
cluding genomic and proteomic studies, and document classi   cation.

we thank the many readers who have found the (too numerous) errors in
the    rst edition. we apologize for those and have done our best to avoid er-
rors in this new edition. we thank mark segal, bala rajaratnam, and larry
wasserman for comments on some of the new chapters, and many stanford
graduate and post-doctoral students who o   ered comments, in particular
mohammed alquraishi, john boik, holger hoe   ing, arian maleki, donal
mcmahon, saharon rosset, babak shababa, daniela witten, ji zhu and
hui zou. we thank john kimmel for his patience in guiding us through this
new edition. rt dedicates this edition to the memory of anna mcphee.

trevor hastie
robert tibshirani
jerome friedman

stanford, california
august 2008

x

preface to the second edition

preface to the first edition

this is page xi
printer: opaque this

we are drowning in information and starving for knowledge.

   rutherford d. roger

the    eld of statistics is constantly challenged by the problems that science
and industry brings to its door. in the early days, these problems often came
from agricultural and industrial experiments and were relatively small in
scope. with the advent of computers and the information age, statistical
problems have exploded both in size and complexity. challenges in the
areas of data storage, organization and searching have led to the new    eld
of    data mining   ; statistical and computational problems in biology and
medicine have created    bioinformatics.    vast amounts of data are being
generated in many    elds, and the statistician   s job is to make sense of it
all: to extract important patterns and trends, and understand    what the
data says.    we call this learning from data.

the challenges in learning from data have led to a revolution in the sta-
tistical sciences. since computation plays such a key role, it is not surprising
that much of this new development has been done by researchers in other
   elds such as computer science and engineering.

the learning problems that we consider can be roughly categorized as
either supervised or unsupervised. in supervised learning, the goal is to pre-
dict the value of an outcome measure based on a number of input measures;
in unsupervised learning, there is no outcome measure, and the goal is to
describe the associations and patterns among a set of input measures.

xii

preface to the first edition

this book is our attempt to bring together many of the important new
ideas in learning, and explain them in a statistical framework. while some
mathematical details are needed, we emphasize the methods and their con-
ceptual underpinnings rather than their theoretical properties. as a result,
we hope that this book will appeal not just to statisticians but also to
researchers and practitioners in a wide variety of    elds.

just as we have learned a great deal from researchers outside of the    eld
of statistics, our statistical viewpoint may help others to better understand
di   erent aspects of learning:

there is no true interpretation of anything; interpretation is a
vehicle in the service of human comprehension. the value of
interpretation is in enabling others to fruitfully think about an
idea.

   andreas buja

we would like to acknowledge the contribution of many people to the
conception and completion of this book. david andrews, leo breiman,
andreas buja, john chambers, bradley efron, geo   rey hinton, werner
stuetzle, and john tukey have greatly in   uenced our careers. balasub-
ramanian narasimhan gave us advice and help on many computational
problems, and maintained an excellent computing environment. shin-ho
bang helped in the production of a number of the    gures. lee wilkinson
gave valuable tips on color production. ilana belitskaya, eva cantoni, maya
gupta, michael jordan, shanti gopatam, radford neal, jorge picazo, bog-
dan popescu, olivier renaud, saharon rosset, john storey, ji zhu, mu
zhu, two reviewers and many students read parts of the manuscript and
o   ered helpful suggestions. john kimmel was supportive, patient and help-
ful at every phase; maryann brickner and frank ganz headed a superb
production team at springer. trevor hastie would like to thank the statis-
tics department at the university of cape town for their hospitality during
the    nal stages of this book. we gratefully acknowledge nsf and nih for
their support of this work. finally, we would like to thank our families and
our parents for their love and support.

trevor hastie
robert tibshirani
jerome friedman

stanford, california
may 2001

the quiet statisticians have changed our world; not by discov-
ering new facts or technical developments, but by changing the
ways that we reason, experiment and form our opinions ....

   ian hacking

contents

this is page xiii
printer: opaque this

preface to the second edition

preface to the first edition

1 introduction

2 overview of supervised learning

2.1
2.2
2.3

2.4
2.5
2.6

2.7

introduction . . . . . . . . . . . . . . . . . . . . . . . . .
variable types and terminology . . . . . . . . . . . . . .
two simple approaches to prediction:
. . . . . . . . . . .
least squares and nearest neighbors
linear models and least squares
. . . . . . . .
2.3.1
nearest-neighbor methods . . . . . . . . . . . .
2.3.2
2.3.3
from least squares to nearest neighbors . . . .
statistical decision theory . . . . . . . . . . . . . . . . .
local methods in high dimensions . . . . . . . . . . . . .
statistical models, supervised learning
and function approximation . . . . . . . . . . . . . . . .
2.6.1

a statistical model
for the joint distribution pr(x, y )
. . . . . . .
supervised learning . . . . . . . . . . . . . . . .
function approximation . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
di   culty of the problem . . . . . . . . . . . . .

2.6.2
2.6.3
structured regression models
2.7.1

vii

xi

1

9
9
9

11
11
14
16
18
22

28

28
29
29
32
32

xiv

contents

2.8

classes of restricted estimators . . . . . . . . . . . . . .
2.8.1
. . .
roughness penalty and bayesian methods
kernel methods and local regression . . . . . .
2.8.2
2.8.3
basis functions and dictionary methods
. . . .
2.9 model selection and the bias   variance tradeo    . . . . .
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . .
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 linear methods for regression

3.1
3.2

3.3

3.4

introduction . . . . . . . . . . . . . . . . . . . . . . . . .
id75 models and least squares
. . . . . . .
example: prostate cancer
3.2.1
. . . . . . . . . . . .
the gauss   markov theorem . . . . . . . . . . .
3.2.2
multiple regression
3.2.3
from simple univariate regression . . . . . . . .
3.2.4
. . . . . . . . . . . . . . . . .
multiple outputs
subset selection . . . . . . . . . . . . . . . . . . . . . . .
best-subset selection . . . . . . . . . . . . . . .
3.3.1
3.3.2
forward- and backward-stepwise selection . . .
forward-stagewise regression . . . . . . . . . .
3.3.3
3.3.4
prostate cancer data example (continued)
. .
shrinkage methods . . . . . . . . . . . . . . . . . . . . . .
3.4.1
ridge regression . . . . . . . . . . . . . . . . .
the lasso . . . . . . . . . . . . . . . . . . . . .
3.4.2
discussion: subset selection, ridge regression
3.4.3
and the lasso . . . . . . . . . . . . . . . . . . .
least angle regression . . . . . . . . . . . . . .
. . . . . . . . .
principal components regression . . . . . . . .
partial least squares . . . . . . . . . . . . . . .

3.5.1
3.5.2

3.4.4

3.5 methods using derived input directions

3.6 discussion: a comparison of the selection

and shrinkage methods . . . . . . . . . . . . . . . . . . .
3.7 multiple outcome shrinkage and selection . . . . . . . .
3.8 more on the lasso and related path algorithms . . . . .
3.8.1
incremental forward stagewise regression . . .
3.8.2
piecewise-linear path algorithms . . . . . . . .
3.8.3
the dantzig selector
. . . . . . . . . . . . . . .
3.8.4
the grouped lasso . . . . . . . . . . . . . . . .
3.8.5
further properties of the lasso . . . . . . . . . .
3.8.6
pathwise coordinate optimization . . . . . . . .
3.9
computational considerations . . . . . . . . . . . . . . .
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . .
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33
34
34
35
37
39
39

43
43
44
49
51

52
56
57
57
58
60
61
61
61
68

69
73
79
79
80

82
84
86
86
89
89
90
91
92
93
94
94

contents

xv

4 linear methods for classi   cation

4.4

4.1
4.2
4.3

101
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 101
id75 of an indicator matrix . . . . . . . . . 103
id156 . . . . . . . . . . . . . . . . 106
regularized discriminant analysis . . . . . . . . 112
4.3.1
4.3.2
computations for lda . . . . . . . . . . . . . . 113
4.3.3
reduced-rank id156 . . 113
id28 . . . . . . . . . . . . . . . . . . . . . . 119
fitting id28 models . . . . . . . . 120
4.4.1
4.4.2
example: south african heart disease
. . . . . 122
quadratic approximations and id136 . . . . 124
4.4.3
l1 regularized id28 . . . . . . . . 125
4.4.4
4.4.5
id28 or lda? . . . . . . . . . . . 127
separating hyperplanes . . . . . . . . . . . . . . . . . . . 129
rosenblatt   s id88 learning algorithm . . 130
4.5.1
4.5.2
optimal separating hyperplanes . . . . . . . . . 132
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 135
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135

4.5

5 basis expansions and id173

5.5

5.3
5.4

5.1
5.2

139
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 139
piecewise polynomials and splines . . . . . . . . . . . . . 141
natural cubic splines . . . . . . . . . . . . . . . 144
5.2.1
example: south african heart disease (continued)146
5.2.2
5.2.3
example: phoneme recognition . . . . . . . . . 148
filtering and feature extraction . . . . . . . . . . . . . . 150
smoothing splines . . . . . . . . . . . . . . . . . . . . . . 151
5.4.1
degrees of freedom and smoother matrices . . . 153
automatic selection of the smoothing parameters . . . . 156
5.5.1
fixing the degrees of freedom . . . . . . . . . . 158
5.5.2
the bias   variance tradeo    . . . . . . . . . . . . 158
nonparametric id28 . . . . . . . . . . . . . 161
. . . . . . . . . . . . . . . . . . 162
. 167
spaces of functions generated by kernels
. . . 168
examples of rkhs . . . . . . . . . . . . . . . . 170
5.9 wavelet smoothing . . . . . . . . . . . . . . . . . . . . . 174
5.9.1 wavelet bases and the wavelet transform . . . 176
5.9.2
adaptive wavelet filtering . . . . . . . . . . . . 179
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 181
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
appendix: computational considerations for splines . . . . . . 186
appendix: b-splines . . . . . . . . . . . . . . . . . . . . . 186
. . . . . 189
appendix: computations for smoothing splines

id173 and reproducing kernel hilbert spaces
5.8.1
5.8.2

5.6
5.7 multidimensional splines
5.8

xvi

contents

6 kernel smoothing methods

6.2
6.3
6.4

6.1 one-dimensional kernel smoothers

191
. . . . . . . . . . . . 192
local id75 . . . . . . . . . . . . . . 194
6.1.1
local polynomial regression . . . . . . . . . . . 197
6.1.2
selecting the width of the kernel
. . . . . . . . . . . . . 198
local regression in irp . . . . . . . . . . . . . . . . . . . 200
structured local regression models in irp
. . . . . . . . 201
structured kernels . . . . . . . . . . . . . . . . . 203
6.4.1
structured regression functions . . . . . . . . . 203
6.4.2
local likelihood and other models
. . . . . . . . . . . . 205
6.5
6.6 kernel density estimation and classi   cation . . . . . . . 208
kernel density estimation . . . . . . . . . . . . 208
kernel density classi   cation . . . . . . . . . . . 210
the naive bayes classi   er . . . . . . . . . . . . 210
6.7
. . . . . . . . . . . . 212
6.8 mixture models for density estimation and classi   cation 214
6.9
computational considerations . . . . . . . . . . . . . . . 216
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 216
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216

6.6.1
6.6.2
6.6.3
radial basis functions and kernels

7 model assessment and selection

7.1
7.2
7.3

219
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 219
bias, variance and model complexity . . . . . . . . . . . 219
the bias   variance decomposition . . . . . . . . . . . . . 223
example: bias   variance tradeo    . . . . . . . . 226
7.3.1
. . . . . . . . . . . 228
7.4 optimism of the training error rate
estimates of in-sample prediction error . . . . . . . . . . 230
7.5
the e   ective number of parameters . . . . . . . . . . . . 232
7.6
7.7
the bayesian approach and bic . . . . . . . . . . . . . . 233
7.8 minimum description length . . . . . . . . . . . . . . . . 235
7.9
vapnik   chervonenkis dimension . . . . . . . . . . . . . . 237
example (continued) . . . . . . . . . . . . . . . 239
7.9.1
7.10 cross-validation . . . . . . . . . . . . . . . . . . . . . . . 241
7.10.1 k-fold cross-validation . . . . . . . . . . . . . 241
7.10.2

the wrong and right way
to do cross-validation . . . . . . . . . . . . . . . 245
7.10.3 does cross-validation really work? . . . . . . . 247
. . . . . . . . . . . . . . . . . . . . . 249
example (continued) . . . . . . . . . . . . . . . 252
7.12 conditional or expected test error? . . . . . . . . . . . . 254
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 257
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257

7.11 bootstrap methods

7.11.1

8 model id136 and averaging

261
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 261

8.1

contents

xvii

8.5

8.2

8.3
8.4

the bootstrap and maximum likelihood methods . . . . 261
8.2.1
. . . . . . . . . . . . . . 261
a smoothing example
maximum likelihood id136 . . . . . . . . . . 265
8.2.2
8.2.3
bootstrap versus maximum likelihood . . . . . 267
bayesian methods . . . . . . . . . . . . . . . . . . . . . . 267
relationship between the bootstrap
and bayesian id136
. . . . . . . . . . . . . . . . . . . 271
the em algorithm . . . . . . . . . . . . . . . . . . . . . 272
. . . . . . . . . 272
8.5.1
two-component mixture model
8.5.2
. . . . . . . . . . 276
the em algorithm in general
em as a maximization   maximization procedure 277
8.5.3
8.6 mcmc for sampling from the posterior . . . . . . . . . . 279
id112 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
8.7
8.7.1
example: trees with simulated data . . . . . . 283
8.8 model averaging and stacking . . . . . . . . . . . . . . . 288
8.9
stochastic search: bumping . . . . . . . . . . . . . . . . . 290
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 292
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293

9 additive models, trees, and related methods

9.2

295
9.1 generalized additive models . . . . . . . . . . . . . . . . 295
fitting additive models . . . . . . . . . . . . . . 297
9.1.1
example: additive id28 . . . . . 299
9.1.2
9.1.3
summary . . . . . . . . . . . . . . . . . . . . . . 304
tree-based methods . . . . . . . . . . . . . . . . . . . . . 305
background . . . . . . . . . . . . . . . . . . . . 305
9.2.1
9.2.2
regression trees . . . . . . . . . . . . . . . . . . 307
classi   cation trees
9.2.3
. . . . . . . . . . . . . . . . 308
. . . . . . . . . . . . . . . . . . . . 310
other issues
9.2.4
9.2.5
spam example (continued)
. . . . . . . . . . . 313
prim: bump hunting . . . . . . . . . . . . . . . . . . . . 317
. . . . . . . . . . . 320
9.3.1
9.4 mars: multivariate adaptive regression splines . . . . . 321
spam example (continued)
. . . . . . . . . . . 326
9.4.1
example (simulated data) . . . . . . . . . . . . 327
9.4.2
9.4.3
. . . . . . . . . . . . . . . . . . . . 328
other issues
9.5
hierarchical mixtures of experts . . . . . . . . . . . . . . 329
9.6 missing data . . . . . . . . . . . . . . . . . . . . . . . . . 332
9.7
computational considerations . . . . . . . . . . . . . . . 334
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 334
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335

spam example (continued)

9.3

10 boosting and additive trees

337
10.1 boosting methods . . . . . . . . . . . . . . . . . . . . . . 337
10.1.1 outline of this chapter . . . . . . . . . . . . . . 340

xviii

contents

10.2 boosting fits an additive model . . . . . . . . . . . . . . 341
10.3 forward stagewise additive modeling . . . . . . . . . . . 342
10.4 exponential loss and adaboost
. . . . . . . . . . . . . . 343
10.5 why exponential loss? . . . . . . . . . . . . . . . . . . . 345
10.6 id168s and robustness . . . . . . . . . . . . . . . 346
10.7
   o   -the-shelf    procedures for data mining . . . . . . . . 350
10.8 example: spam data . . . . . . . . . . . . . . . . . . . . 352
10.9 boosting trees . . . . . . . . . . . . . . . . . . . . . . . . 353
10.10 numerical optimization via gradient boosting . . . . . . 358
10.10.1 steepest descent . . . . . . . . . . . . . . . . . . 358
10.10.2 gradient boosting . . . . . . . . . . . . . . . . . 359
10.10.3 implementations of gradient boosting . . . . . . 360
10.11 right-sized trees for boosting . . . . . . . . . . . . . . . 361
10.12 id173 . . . . . . . . . . . . . . . . . . . . . . . . 364
10.12.1 shrinkage . . . . . . . . . . . . . . . . . . . . . . 364
10.12.2 subsampling . . . . . . . . . . . . . . . . . . . . 365
10.13 interpretation . . . . . . . . . . . . . . . . . . . . . . . . 367
10.13.1 relative importance of predictor variables . . . 367
10.13.2 partial dependence plots . . . . . . . . . . . . . 369
10.14 illustrations . . . . . . . . . . . . . . . . . . . . . . . . . . 371
10.14.1 california housing . . . . . . . . . . . . . . . . . 371
10.14.2 new zealand fish . . . . . . . . . . . . . . . . . 375
10.14.3 demographics data . . . . . . . . . . . . . . . . 379
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 380
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384

11 neural networks

scaling of the inputs

389
11.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 389
11.2 projection pursuit regression . . . . . . . . . . . . . . . 389
11.3 neural networks . . . . . . . . . . . . . . . . . . . . . . . 392
11.4 fitting neural networks . . . . . . . . . . . . . . . . . . . 395
11.5 some issues in training neural networks . . . . . . . . . 397
11.5.1
starting values . . . . . . . . . . . . . . . . . . . 397
11.5.2 over   tting . . . . . . . . . . . . . . . . . . . . . 398
11.5.3
. . . . . . . . . . . . . . . 398
11.5.4 number of hidden units and layers . . . . . . . 400
11.5.5 multiple minima . . . . . . . . . . . . . . . . . . 400
11.6 example: simulated data . . . . . . . . . . . . . . . . . . 401
11.7 example: zip code data . . . . . . . . . . . . . . . . . . 404
11.8 discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 408
11.9 bayesian neural nets and the nips 2003 challenge . . . 409
bayes, boosting and id112 . . . . . . . . . . . 410
performance comparisons
. . . . . . . . . . . . 412
11.10 computational considerations . . . . . . . . . . . . . . . 414
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 415

11.9.1
11.9.2

exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 415

12 support vector machines and

contents

xix

417
flexible discriminants
12.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 417
12.2 the support vector classi   er . . . . . . . . . . . . . . . . 417
12.2.1
computing the support vector classi   er . . . . 420
12.2.2 mixture example (continued) . . . . . . . . . . 421
12.3 support vector machines and kernels . . . . . . . . . . . 423
computing the id166 for classi   cation . . . . . . 423
12.3.1
the id166 as a penalization method . . . . . . . 426
12.3.2
function estimation and reproducing kernels . 428
12.3.3
12.3.4
id166s and the curse of dimensionality . . . . . 431
12.3.5 a path algorithm for the id166 classi   er . . . . 432
12.3.6
support vector machines for regression . . . . . 434
12.3.7 regression and kernels . . . . . . . . . . . . . . 436
12.3.8 discussion . . . . . . . . . . . . . . . . . . . . . 438
. . . . . . . . 438
12.4 generalizing id156
12.5 flexible discriminant analysis . . . . . . . . . . . . . . . 440
computing the fda estimates . . . . . . . . . . 444
12.6 penalized discriminant analysis . . . . . . . . . . . . . . 446
12.7 mixture discriminant analysis . . . . . . . . . . . . . . . 449
example: waveform data . . . . . . . . . . . . . 451
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 455
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455

12.5.1

12.7.1

13 prototype methods and nearest-neighbors

13.3 k-nearest-neighbor classi   ers

13.1
13.2 prototype methods

459
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 459
. . . . . . . . . . . . . . . . . . . . . 459
13.2.1 id116 id91 . . . . . . . . . . . . . . . . 460
13.2.2
learning vector quantization . . . . . . . . . . 462
13.2.3 gaussian mixtures . . . . . . . . . . . . . . . . . 463
. . . . . . . . . . . . . . . 463
example: a comparative study . . . . . . . . . 468
example: k-nearest-neighbors
and image scene classi   cation . . . . . . . . . . 470
invariant metrics and tangent distance . . . . . 471
13.4 adaptive nearest-neighbor methods . . . . . . . . . . . . 475
example . . . . . . . . . . . . . . . . . . . . . . 478

13.3.1
13.3.2

13.3.3

13.4.1
13.4.2 global dimension reduction

for nearest-neighbors . . . . . . . . . . . . . . . 479
13.5 computational considerations . . . . . . . . . . . . . . . 480
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 481
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 481

xx

contents

14 unsupervised learning

proximity matrices

14.1
14.2 association rules

485
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 485
. . . . . . . . . . . . . . . . . . . . . . 487
14.2.1 market basket analysis . . . . . . . . . . . . . . 488
the apriori algorithm . . . . . . . . . . . . . . 489
14.2.2
14.2.3
example: market basket analysis . . . . . . . . 492
14.2.4 unsupervised as supervised learning . . . . . . 495
14.2.5 generalized association rules
. . . . . . . . . . 497
choice of supervised learning method . . . . . 499
14.2.6
14.2.7
example: market basket analysis (continued) . 499
14.3 cluster analysis . . . . . . . . . . . . . . . . . . . . . . . 501
. . . . . . . . . . . . . . . . 503
14.3.1
14.3.2 dissimilarities based on attributes
. . . . . . . 503
14.3.3 object dissimilarity . . . . . . . . . . . . . . . . 505
id91 algorithms . . . . . . . . . . . . . . . 507
14.3.4
14.3.5
combinatorial algorithms
. . . . . . . . . . . . 507
14.3.6 id116 . . . . . . . . . . . . . . . . . . . . . . 509
14.3.7 gaussian mixtures as soft id116 id91 . 510
14.3.8
example: human tumor microarray data . . . 512
14.3.9 vector quantization . . . . . . . . . . . . . . . . 514
14.3.10 k-medoids . . . . . . . . . . . . . . . . . . . . . 515
14.3.11 practical issues
. . . . . . . . . . . . . . . . . . 518
14.3.12 hierarchical id91 . . . . . . . . . . . . . . 520
14.4 self-organizing maps
. . . . . . . . . . . . . . . . . . . . 528
14.5 principal components, curves and surfaces . . . . . . . . 534
principal components . . . . . . . . . . . . . . . 534
14.5.1
principal curves and surfaces
. . . . . . . . . . 541
14.5.2
14.5.3
spectral id91 . . . . . . . . . . . . . . . . 544
14.5.4 kernel principal components . . . . . . . . . . . 547
14.5.5
sparse principal components . . . . . . . . . . . 550
14.6 non-negative id105 . . . . . . . . . . . . . 553
14.6.1 archetypal analysis . . . . . . . . . . . . . . . . 554
independent component analysis
. . . . . . . . . . . . 557
and exploratory projection pursuit
latent variables and factor analysis
. . . . . . 558
14.7.1
independent component analysis . . . . . . . . 560
14.7.2
14.7.3
exploratory projection pursuit . . . . . . . . . . 565
14.7.4 a direct approach to ica . . . . . . . . . . . . 565
14.8 multidimensional scaling . . . . . . . . . . . . . . . . . . 570
14.9 nonlinear dimension reduction

14.7

and local multidimensional scaling . . . . . . . . . . . . 572
14.10 the google id95 algorithm . . . . . . . . . . . . . 576
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 578
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579

contents

xxi

15 id79s

587
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 587
15.1
15.2 de   nition of id79s . . . . . . . . . . . . . . . . 587
15.3 details of id79s
. . . . . . . . . . . . . . . . . 592
15.3.1 out of bag samples . . . . . . . . . . . . . . . . 592
15.3.2 variable importance . . . . . . . . . . . . . . . . 593
15.3.3
. . . . . . . . . . . . . . . . . . 595
15.3.4 id79s and over   tting . . . . . . . . . 596
15.4 analysis of id79s . . . . . . . . . . . . . . . . . 597
15.4.1 variance and the de-correlation e   ect . . . . . 597
bias . . . . . . . . . . . . . . . . . . . . . . . . . 600
15.4.2
15.4.3 adaptive nearest neighbors
. . . . . . . . . . . 601
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 602
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603

proximity plots

16 id108

605
16.1
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 605
16.2 boosting and id173 paths . . . . . . . . . . . . . 607
penalized regression . . . . . . . . . . . . . . . 607
16.2.1
16.2.2
the    bet on sparsity    principle . . . . . . . . . 610
16.2.3 id173 paths, over-   tting and margins . 613
16.3 learning ensembles . . . . . . . . . . . . . . . . . . . . . 616
learning a good ensemble . . . . . . . . . . . . 617
. . . . . . . . . . . . . . . . . . 622
bibliographic notes . . . . . . . . . . . . . . . . . . . . . . . . . 623
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624

16.3.1
16.3.2 rule ensembles

17 undirected id114

625
introduction . . . . . . . . . . . . . . . . . . . . . . . . . 625
. . . . . . . . . . . 627
. 630

17.1
17.2 markov graphs and their properties
17.3 undirected id114 for continuous variables

17.3.1

17.3.2

estimation of the parameters
when the graph structure is known . . . . . . . 631
estimation of the graph structure . . . . . . . . 635
. . . 638

17.4 undirected id114 for discrete variables

17.4.1

estimation of the parameters
when the graph structure is known . . . . . . . 639
. . . . . . . . . . . . . . . . . . . 641
estimation of the graph structure . . . . . . . . 642
. . . . . . . . . 643
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 645

17.4.2 hidden nodes
17.4.3
17.4.4 restricted id82s

18 high-dimensional problems: p     n

649
18.1 when p is much bigger than n . . . . . . . . . . . . . . 649

xxii

contents

18.2 diagonal id156

and nearest shrunken centroids . . . . . . . . . . . . . . 651
18.3 linear classi   ers with quadratic id173 . . . . . 654
18.3.1 regularized discriminant analysis . . . . . . . . 656
18.3.2

id28
with quadratic id173 . . . . . . . . . . 657
the support vector classi   er
. . . . . . . . . . 657
feature selection . . . . . . . . . . . . . . . . . . 658
computational shortcuts when p     n . . . . . 659
18.4 linear classi   ers with l1 id173 . . . . . . . . . 661

18.3.3
18.3.4
18.3.5

18.4.1 application of lasso

to protein mass spectroscopy . . . . . . . . . . 664
the fused lasso for functional data . . . . . . 666
18.5 classi   cation when features are unavailable . . . . . . . 668

18.4.2

18.5.1

18.5.2

18.5.3

example: string kernels
and protein classi   cation . . . . . . . . . . . . . 668
classi   cation and other models using
inner-product kernels and pairwise distances . 670
example: abstracts classi   cation . . . . . . . . 672

18.6 high-dimensional regression:

. . . . . . . . . . . . . 674
supervised principal components
18.6.1
connection to latent-variable modeling . . . . 678
18.6.2 relationship with partial least squares . . . . . 680
pre-conditioning for feature selection . . . . . 681
18.6.3
18.7 feature assessment and the multiple-testing problem . . 683
the false discovery rate . . . . . . . . . . . . . 687
18.7.1
690
18.7.2 asymmetric cutpoints and the sam procedure
18.7.3 a bayesian interpretation of the fdr . . . . . . 692
18.8 bibliographic notes
. . . . . . . . . . . . . . . . . . . . . 693
exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 694

references

author index

index

699

729

737

1
introduction

this is page 1
printer: opaque this

statistical learning plays a key role in many areas of science,    nance and
industry. here are some examples of learning problems:

    predict whether a patient, hospitalized due to a heart attack, will
have a second heart attack. the prediction is to be based on demo-
graphic, diet and clinical measurements for that patient.

    predict the price of a stock in 6 months from now, on the basis of

company performance measures and economic data.

    identify the numbers in a handwritten zip code, from a digitized

image.

    estimate the amount of glucose in the blood of a diabetic person,

from the infrared absorption spectrum of that person   s blood.

    identify the risk factors for prostate cancer, based on clinical and

demographic variables.

the science of learning plays a key role in the    elds of statistics, data
mining and arti   cial intelligence, intersecting with areas of engineering and
other disciplines.

this book is about learning from data. in a typical scenario, we have
an outcome measurement, usually quantitative (such as a stock price) or
categorical (such as heart attack/no heart attack), that we wish to predict
based on a set of features (such as diet and clinical measurements). we
have a training set of data, in which we observe the outcome and feature

2

1. introduction

table 1.1. average percentage of words or characters in an email message
equal to the indicated word or character. we have chosen the words and characters
showing the largest di   erence between spam and email.

george

you your

hp free

hpl

!

our

re

spam
email

0.00 2.26 1.38 0.02 0.52 0.01 0.51 0.51 0.13 0.01
1.27 1.27 0.44 0.90 0.07 0.43 0.11 0.18 0.42 0.29

edu remove
0.28
0.01

measurements for a set of objects (such as people). using this data we build
a prediction model, or learner, which will enable us to predict the outcome
for new unseen objects. a good learner is one that accurately predicts such
an outcome.

the examples above describe what is called the supervised learning prob-
lem. it is called    supervised    because of the presence of the outcome vari-
able to guide the learning process. in the unsupervised learning problem,
we observe only the features and have no measurements of the outcome.
our task is rather to describe how the data are organized or clustered. we
devote most of this book to supervised learning; the unsupervised problem
is less developed in the literature, and is the focus of chapter 14.

here are some examples of real learning problems that are discussed in

this book.

example 1: email spam

the data for this example consists of information from 4601 email mes-
sages, in a study to try to predict whether the email was junk email, or
   spam.    the objective was to design an automatic spam detector that
could    lter out spam before clogging the users    mailboxes. for all 4601
email messages, the true outcome (email type) email or spam is available,
along with the relative frequencies of 57 of the most commonly occurring
words and punctuation marks in the email message. this is a supervised
learning problem, with the outcome the class variable email/spam. it is also
called a classi   cation problem.

table 1.1 lists the words and characters showing the largest average

di   erence between spam and email.

our learning method has to decide which features to use and how: for

example, we might use a rule such as

if (%george < 0.6) & (%you > 1.5)

then spam
else email.

another form of a rule might be:

if (0.2    %you     0.3    %george) > 0

then spam
else email.

1. introduction

3

40 50 60 70 80

0.0

0.4

0.8

6.0

7.0

8.0

9.0

4

3

2

1

1
   

0
8

0
7

0
6

0
5

0
4

8
.
0

4
.
0

0
.
0

0
.
9

0
.
8

0
.
7

0
.
6

   1

1 2 3 4

oo o
ooo o
o
o
oo
o
o
ooo
o
o ooo
oo
o ooo o
o oooo oo
o
oooo oo
o
o
o
o ooo o
o
ooo
oo oo
o
ooo
oo
oo
o
o
o
ooo
o
o
o
o o o o
o
o
oooo
o oo
o
oo
o
o
oo oo

lcavol

o

o

lpsa

o
oo

o
o
o
o
oo
oo
o
ooo
o
o
ooo
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
ooo
o
o
oooo
o
o
o
o
o
o
oo
o
oo
oo
oo
o
o
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o

o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
oo
ooo
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
oo
ooo
o
o
o
oooo
o
o
o o
oo
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o

o

o

o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
oooooo
o
o
o
oo
o
o
o
o
oooo
o
oo
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o

o
o

o

oo
o

o

o

o

o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o

o
o
o

o

o
o
o

oo

o

o

o

o

o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oo
o o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
o

o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo

o

o

o

o
o
o

o
o
o
o

o
o

o

o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
ooo
o
oooo
ooo
o
oo
o
o
oo

oooo oo

o

o

o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o oo
o
o
o
oooo
o oo
o
o
o
o
o
oo oo
o
o
oo

o o o
o oo oo
o
o o oooo oo
o o
oo
o
o
o
o
oo
oo oo ooo o o
o
o oo
o
o
o
o
o
oo o o
oo
oo
ooo
oo
o
oo
o oo
oo
o o
oo
o
ooooo o
oo oo ooo
oo oooo oo

o
o

o
o

o
o
o
o
o
o
o
o o
o o
o o
o
o
o oo
o
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
oo
o
oo oo
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
oo
o
oo
o
o o
o
o
o
oo
o o
o
oo
o
o
o
o
o
o
o
o
o

o
o

o
o

lweight

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
oo
o
o
o oo
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o

o
o

oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo o
oo oo
oo
o
o
o
o
ooo
o
oo
o

o

o

o

o

o

o

o

o
o

o

o

o

o
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
oo
oooo
ooo
oo
oooo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o

o

o
o

o
o
o
o o
oo
o
o
o o o
o
oo
o
o
o
ooo
o
oo
o
o
oo
o
ooo
o
o
o
o
o
o
o

oo
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o

o

o

o

o
o
o

o
o

o

o
o
o
o
o
o
o
o

o
o

o

o

o
o
o
o
o
o
o
o
o
o

o
o

o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o

o
o
oo
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o

o

o

lbph

o

o
o

o

o

o

oo
oo o
o
oo oo oo
o
o
ooo
oo o o
o
o
o
o
oooo o ooo
o
o
o
o
o o
oooo o
o
o
ooo oo
o o
o o
o
oo
o
ooo o oooooooo o
o
o oo
o
oooo o
oo
o
o
o

o

oo

o

o
o

o
o
o
o
o

o

o
o
o
o
oo
o
o o
o
oo
o
o
o
o
o
o
o o
o
oo
o
o o
o
o
o
o
o
o oo
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
oo
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o

o
o
o

o

o
o
o
o

o

o

o
o
o

o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
oo
o
o
o
ooo o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o o
o

o

o
o

o

o

age

o

o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o o
oo
o
o
o
o
o
o
o o
o
o

o

o
o

o
o
o

o
o

o
o
o
o

o

o
oooo
o
o
o
o
o
o
o
ooo

oooooo

o
o
o
o o
o
o

oo ooo o
o
oo
o
o
o
o
o

o
o
o
o
o
o
o
o o o
o
o
o
o
o oo
o
o
o

o

o

o
o
o
o
oo
o
o o
o
o
oo
o
oo

o
o
o

o
o
o
o
o
oo
ooo

o

o

o
o

o

o
o
o
o

o
o

o
oo
o
oooooooooooooo
ooooooo
oooo oooooooooooooooooooooooooooooooooo
o
oooooo
ooo
oo
o

o
oooo
o
o
oo oo
o o
o
o oo
oo
o o o o
o
ooo
o
o
o
o
o
o
o
ooo
oo oo
o ooo o
o
oooo oo
oo
oo
o
ooo
oo oo
o
o
o
o
o
o
o
o
o

o
o

o

oo
ooo oooo oo
ooooo o
o
o
oo
o
oo
o
oo o o
oo oo ooo
o o
o oo
oo
o
o o
o
o
o
o
o
oo
o
o
ooo
o
oo
oo
o oo
o ooo o o
o
o
o

o
o

o
o
oo
o

oooo o
o
oo
o
o
o o
o o
o oo
o
o o
o
ooo o oooooooo o
oo
o
o o
o
oo
o
o
o
oo
ooo oo
o
oooo o
oo o ooo
o
o
o
o
o

o

o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oooo oooooooo
o
ooo
oo
o
oo
oooo
o
ooo
o
oo
o

o
o
oo

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
oo oo
o
o oo
oo
o
o
o
o
o
o
o o o
o
oo oo
o
o
o
o
o
o
o
o
o o

o
o
oo

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo oooo oo
o
o
o
o
o
oo
o
o
o
o
o
o
o oo
o
o o
o
o
oo
o o
o

o

o
o

o

o

o

o

o

o
o

o
o
o
o

o

o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o o
oo
o
o
oo
o
o
o
o o
o
o
o
o
o
o o
o
oo
oo

o

o
o

o
o

o
o

o
o
o
o

o

o

o

o

o
o
o

o
o

o

o
o

o
o

o

o

o

o

o

o

o

oo
ooooooooo
o
o
ooo
o
ooo
o
ooo
o
o
ooo
o
o
ooooo
oo
o
ooo
oooo
ooo

oooooo

oo
ooo
o
o
o ooo
o
o
oo
oo
o
o
o
ooo
o
o
oo
o oo
o ooo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oo ooo o
o

o

o

o
oo
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o o o
o
o o
oo
o o o
oo
o
o
ooo
o
o
o
o oo
o
oo

o
o

o

o

o

o o
o
o
o
o
o
oo o o
o oo
o o
o
o o
o
ooo
o
o
o
o
o o
o
o
o
o o o
o
o
o
ooo
oo
o o
oo
o
o
o
o o
o

oo
o
oooooo
oooo
o
o oooooooo
oo
o
oo
o
o
o
o
o

o
o

o
oo
o
o
o oo
oo
o
o
o
o
o
o o o
o
o
o
o
oo
oo
o
o
oo
o
o
o
o

o

o

oo
ooo oooo oo
oo
oo
o
o
o
o
o
o
o oo
oo
o
o
o
o
o
o

o
o

o
o

o
o
o
o

o

o
o
o
o o
ooo o
ooo
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
oo
o
o
ooooooo
ooo
oo
o
o
oo
oooo
o oooooooo
oo
o
o
o
o
oo

o

o

o

0 1 2 3 4 5

o

o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
ooo
oo
o
o
o
o
o
o
oo
oo
o
o
o
oo
o
o oo
oo
o o o
o
o
o
o
o
oo
o
o

o

o

o

o

o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
o
o oo
o
o
o
o
oo
o
o
o oo
o
oo
o
ooo oooo oo
oo
o
o
o
o
oo
o

o
o

o

o
o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
oo
o
o
oo
o
o
o
oo
ooo o
o
o o
o
oo
o
ooo
o
o
o
o
o
o
o
o
o
o

o
o
o

oooooo
ooo
oooo
oo
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o

o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oooooo
ooo
o
o
o
o
o
o
o
o
o
o

o
o

o
ooo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
ooo

oo
ooo
ooo
o
oo
o
o
oo
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
oo
oo
ooo
ooo
oo
o
o
o

o
o

o
o
o
o
oo
o
o

o
o
o
ooo
o
o o
o
o
o
o
o
o
o
o
ooo
o
o
o o o
o
o
o
o
o
o
oo

o

o
o

o
o

o

o

o

o

o

o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o

o

o

o

o

o

o

o

o

o

o
o
o
o
oo
o

o
o
oo
o
o
o
o
o
o
o
o o o
o
oo
o
o
o
o
o
oo
o

o

o

o

o
ooo
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o

o
o
o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o

o

o
o
oo
ooo
oo
o
oooooo
o
oooooooooooooo
ooooooo

oooooooooooooooooooooooooooooooooo

oooo

o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
ooo
o
o
oooo
o
o
o
o
o
oo
o
oo
oo
o
o
o
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
oo
o
o
o
o
o
o
o
o
oo
o
o
oo
oo
ooo
o
o
oooo
o
o
oo
oo
o
o
o
oo
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
oooooo
o
o
o
o
o
o
oooo
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o

o
o
o
o
o
o
o
o
o
o
oo
o
o
o
ooo
oooo
oo
o
o
o
oooooo
oo

o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oooooooooooo
o
o
ooo
o
oo
o
oo
oooo
ooo
o
o
o
o
o
oo

o
o
o

o

o
o
o
ooo
o
o
ooooo
oo
ooo
o
o
oo
ooo
o
o
ooo
o
o
o
ooo
o

o
oo
o
o
o
o
o
o
o
oo
ooooooooo
o
oooo
o
oo
oooooo

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
oo
oo
o
ooooooo
o
ooo
oo
o
o
oo
oo
ooooooooo
oooo
o
o
oo
o
o

ooo
ooo
ooo
o
o
o
oooo
o
o
o
o
o

o
o
o
oo
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
oo
ooo

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o

svi

oo
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
oooo
o
oo
o
o ooo
o
o
o
o
o
oo
o
o
oooo oo
o
oo oo
o
ooo
o o
o
oooooooo oo
oooo

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
oo
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o o
ooo
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o

o
o

o

o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
oo
o
o
o
ooo o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o o
o
o
o
o
o
o o o
o
o
oo o
o
o
o
o
o
o
o

o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o

o
o
o
o
oo
o
o
o
o
oo
ooo
o
o
o
o
o
oooooo
o o

o

o

o
o
o
o
o

o

o
o
o
o
o
o oo
o

o

o

o
o
o
o

o
o
o
o
o o o
o
o
o oo
o
o
o
o

oooooooooooo oo
o
o
o
o
o o
ooo
o
oo oo
oooo oo
o
oo
o
o ooo
oo
oooo
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o

o
o

o
o
o
o
o
o
o
o

o

lcp

o

o

o
o

o

o

ooo
ooo
ooo
oo
ooooooooo
oooo
ooo
o
o
oo
ooooo
o
o
ooo
o
ooo
o
ooo
o
o
o
ooo

o

o
o
o
o
oo
oo
oo
o
ooo
o
oo
oo
o
o
o
o
o
o
o
ooo
o
ooo
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o

o
o
o
oo
o
o
o
o
o
o
oo
ooo
o
ooo
o
o

o
o
o
o
o
oo
o
o
o
ooo
oooooo

o
ooo
o
o
o
ooo
o
o
o
o
ooo
o
o
ooooo
oo
ooo
o
o
oo
ooo
o

o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o

o
o
o
o
o
o
o
oo
o
oooooo
oo
o
oooo
o
oooooooo
oo
o

o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
ooo
oo
o
o
o
o
oo
o
o
o
o
o
o
o

o
o
o
o
oo
o
o
o
o
ooo
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
ooo
o
oo
o
o
oo
ooo
oo

oo
ooooooooo
o
oooo
o
oo
oooooo
o
oo
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
oo
ooooooooo
o
ooo
o
oo
o
o
ooo
o
o
o
o

o

o

o

o

o

o

o

gleason

o

o
o
o o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o

o
o o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o ooo
o
oo
ooo oo
o
ooooooo oo
o
oo
o
o
oooo
ooo
oooooooo
oo
o

o

o
o
o

o
o
o

o
o
o
o
o
o
o
o

o
o

o

o

o
o
o
o
o
o
o
o

o
o

o
o
o
o
o
o

o
o

o

o
o
o

o
o
o
o
o
o
o

o
o

o
o

o

o
o
o
o
o
o

o
o
o

o
o
o

o
o
o
o
o
o
o
o

o

o
o

o
o

o
o
o
o
o
o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
ooo
o
o
o
o
o o
o
oo
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
ooo
o
o
o
o
oo
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
ooo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
ooo
oo
o
o
oo
o
ooo
oo

o
o

o

o
o

o
o
o
o

o
o

o

o

o
o
o
o
o

o
o
o
o
o

o

o o
o
o
o
o
o
o

o

o

oo
o
ooo
o
o
o
o
o
ooooooo oo
ooooooooo
oooo
o
oo
o
ooo oo
oo
o
o ooo
o
o
ooo
o
o
o
o
o
o

o
o
oo
o
o

o
o
o

o
o
o
o

o
o

o

o

o
o
o
o

o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
ooooooooo
o
o
ooo
o
oo
o o
o
o
o
o
ooo
o
o
o o
o
o

o
o
o

o

o
o
o
o

oo
o

o

o

o

o
o

o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o

o

o

o

o
o
o

o

o
o
oo
o
o
o
o
o
o o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o oo
o
oo
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

oo
ooooooooo
o
oooo
o
oo
oooooo
o
oo
o
o
o
o
o
o
o

pgg45

o
o
o
o
o

o
o
o
o

o

o
o
o
o
o

o
o
o
o
o

o
o
o

o
o

o
o

o
o
o

o

o
o
o

o

o
o
o
o
o

5

4

3

2

1

0

5
4

.

5
3

.

5
2

.

2

1

0

1
   

3

2

1

0

1
   

0
0
1

0
6

0
2

0

o
o
o
o
o
oo
o
o
o
ooo
oooooo

o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o

o
o o
o
o
o
o
o
o
o
o
o oo
o
o
o
o

o

oo
ooooooooo
o
ooo
o
oo
oooo o
o o
o
o
o
o
o
o
o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o

o

o

o
o
o
o
o
oo
oooooo
o
oo
ooooooooo
o
oooo
o
o
oo
o

o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oooo oo
o
o
o
o
oo
oo
ooooooooo
o
ooo
o
o
o o
o
o

o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o

o

2.5

3.5

4.5

   1

0

1

2

   1 0

1

2

3

0 20

60

100

figure 1.1. scatterplot matrix of the prostate cancer data. the    rst row shows
the response against each of the predictors in turn. two of the predictors, svi and
gleason, are categorical.

for this problem not all errors are equal; we want to avoid    ltering out
good email, while letting spam get through is not desirable but less serious
in its consequences. we discuss a number of di   erent methods for tackling
this learning problem in the book.

example 2: prostate cancer

the data for this example, displayed in figure 1.11, come from a study
by stamey et al. (1989) that examined the correlation between the level of

1there was an error in these data in the    rst edition of this book. subject 32 had
a value of 6.1 for lweight, which translates to a 449 gm prostate! the correct value is
44.9 gm. we are grateful to prof. stephen w. link for alerting us to this error.

4

1. introduction

figure 1.2. examples of handwritten digits from u.s. postal envelopes.

prostate speci   c antigen (psa) and a number of clinical measures, in 97
men who were about to receive a radical prostatectomy.

the goal is to predict the log of psa (lpsa) from a number of measure-
ments including log cancer volume (lcavol), log prostate weight lweight,
age, log of benign prostatic hyperplasia amount lbph, seminal vesicle in-
vasion svi, log of capsular penetration lcp, gleason score gleason, and
percent of gleason scores 4 or 5 pgg45. figure 1.1 is a scatterplot matrix
of the variables. some correlations with lpsa are evident, but a good pre-
dictive model is di   cult to construct by eye.

this is a supervised learning problem, known as a regression problem,

because the outcome measurement is quantitative.

example 3: handwritten digit recognition

the data from this example come from the handwritten zip codes on
envelopes from u.s. postal mail. each image is a segment from a    ve digit
zip code, isolating a single digit. the images are 16  16 eight-bit grayscale
maps, with each pixel ranging in intensity from 0 to 255. some sample
images are shown in figure 1.2.

the images have been normalized to have approximately the same size
and orientation. the task is to predict, from the 16    16 matrix of pixel
intensities, the identity of each image (0, 1, . . . , 9) quickly and accurately. if
it is accurate enough, the resulting algorithm would be used as part of an
automatic sorting procedure for envelopes. this is a classi   cation problem
for which the error rate needs to be kept very low to avoid misdirection of

1. introduction

5

mail. in order to achieve this low error rate, some objects can be assigned
to a    don   t know    category, and sorted instead by hand.

example 4: dna expression microarrays

dna stands for deoxyribonucleic acid, and is the basic material that makes
up human chromosomes. dna microarrays measure the expression of a
gene in a cell by measuring the amount of mrna (messenger ribonucleic
acid) present for that gene. microarrays are considered a breakthrough
technology in biology, facilitating the quantitative study of thousands of
genes simultaneously from a single sample of cells.

here is how a dna microarray works. the nucleotide sequences for a few
thousand genes are printed on a glass slide. a target sample and a reference
sample are labeled with red and green dyes, and each are hybridized with
the dna on the slide. through    uoroscopy, the log (red/green) intensities
of rna hybridizing at each site is measured. the result is a few thousand
numbers, typically ranging from say    6 to 6, measuring the expression level
of each gene in the target relative to the reference sample. positive values
indicate higher expression in the target versus the reference, and vice versa
for negative values.

a gene expression dataset collects together the expression values from a
series of dna microarray experiments, with each column representing an
experiment. there are therefore several thousand rows representing individ-
ual genes, and tens of columns representing samples: in the particular ex-
ample of figure 1.3 there are 6830 genes (rows) and 64 samples (columns),
although for clarity only a random sample of 100 rows are shown. the    g-
ure displays the data set as a heat map, ranging from green (negative) to
red (positive). the samples are 64 cancer tumors from di   erent patients.

the challenge here is to understand how the genes and samples are or-

ganized. typical questions include the following:

(a) which samples are most similar to each other, in terms of their expres-

sion pro   les across genes?

(b) which genes are most similar to each other, in terms of their expression

pro   les across samples?

(c) do certain genes show very high (or low) expression for certain cancer

samples?

we could view this task as a regression problem, with two categorical
predictor variables   genes and samples   with the response variable being
the level of expression. however, it is probably more useful to view it as
unsupervised learning problem. for example, for question (a) above, we
think of the samples as points in 6830   dimensional space, which we want
to cluster together in some way.

6

1. introduction

sidw299104
sidw380102
s 3161
gnal
h.sapiensmrna
s 25394
rasgtpase
s 07172
ests
sidw377402
humanmrna
sidw469884
ests
s 71915
mybproto
estschr.1
s 77451
dnapolymer
s 75812
sidw31489
s 7117
sidw470459
sidw487261
homosapiens
sidw376586
chr
mitochondrial60
s 7116
estschr.6
sidw296310
s 88017
s 05167
estschr.3
s 7504
s 89414
ptprc
sidw298203
sidw310141
sidw376928
estsch31
s 4241
s 77419
s 97117
sidw201620
sidw279664
sidw510534
hlaclassi
sidw203464
s 39012
sidw205716
sidw376776
hypothetical
waswiskott
sidw321854
estschr.15
sidw376394
s 80066
estschr.5
sidw488221
s 6536
sidw257915
estschr.2
sidw322806
s 00394
estschr.15
s 84853
s 85148
s 97905
ests
sidw486740
smallnuc
ests
sidw366311
sidw357197
s 2979
ests
s 3609
sidw416621
erlumen
tuple1tup1
sidw428642
s 81079
sidw298052
sidw417270
sidw362471
estschr.15
sidw321925
s 80265
sidw308182
s 81508
s 77133
sidw365099
estschr.10
sidw325120
s 60097
s 75990
sidw128368
s 01902
s 1984
s 2354

l
a
n
e
r

t
s
a
e
r
b

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

o
r
p
e
r
-

d
7
f
c
m

n
o
l
o
c

n
o
l
o
c

o
r
p
e
r
-

b
2
6
5
k

n
o
l
o
c

c
l
c
s
n

t
s
a
e
r
b

l
a
n
e
r

i

a
m
e
k
u
e
l

a
m
o
n
a
l
e
m

s
n
c

s
n
c

l
a
n
e
r

o
r
p
e
r
-

a
7
f
c
m

c
l
c
s
n

o
r
p
e
r
-

a
2
6
5
k

n
o
l
o
c

s
n
c

c
l
c
s
n

c
l
c
s
n

s
n
c

t
s
a
e
r
b

i

n
a
r
a
v
o

i

a
m
e
k
u
e
l

i

a
m
e
k
u
e
l

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

i

n
a
r
a
v
o

i

n
a
r
a
v
o

c
l
c
s
n

l
a
n
e
r

t
s
a
e
r
b

a
m
o
n
a
l
e
m

i

n
a
r
a
v
o

i

n
a
r
a
v
o

c
l
c
s
n

l
a
n
e
r

t
s
a
e
r
b

s
n
c

n
o
l
o
c

i

a
m
e
k
u
e
l

a
m
o
n
a
l
e
m

n
o
l
o
c

t
s
a
e
r
b

i

a
m
e
k
u
e
l

a
m
o
n
a
l
e
m

c
l
c
s
n

e
t
a
t
s
o
r
p

c
l
c
s
n

l
a
n
e
r

l
a
n
e
r

c
l
c
s
n

l
a
n
e
r

i

n
a
r
a
v
o

i

a
m
e
k
u
e
l

e
t
a
t
s
o
r
p

n
o
l
o
c

t
s
a
e
r
b

l
a
n
e
r

n
w
o
n
k
n
u

figure 1.3. dna microarray data: expression matrix of 6830 genes (rows)
and 64 samples (columns), for the human tumor data. only a random sample
of 100 rows are shown. the display is a heat map, ranging from bright green
(negative, under expressed) to bright red (positive, over expressed). missing values
are gray. the rows and columns are displayed in a randomly chosen order.

1. introduction

7

who should read this book

this book is designed for researchers and students in a broad variety of
   elds: statistics, arti   cial intelligence, engineering,    nance and others. we
expect that the reader will have had at least one elementary course in
statistics, covering basic topics including id75.

we have not attempted to write a comprehensive catalog of learning
methods, but rather to describe some of the most important techniques.
equally notable, we describe the underlying concepts and considerations
by which a researcher can judge a learning method. we have tried to write
this book in an intuitive fashion, emphasizing concepts rather than math-
ematical details.

as statisticians, our exposition will naturally re   ect our backgrounds and
areas of expertise. however in the past eight years we have been attending
conferences in neural networks, data mining and machine learning, and our
thinking has been heavily in   uenced by these exciting    elds. this in   uence
is evident in our current research, and in this book.

how this book is organized

our view is that one must understand simple methods before trying to
grasp more complex ones. hence, after giving an overview of the supervis-
ing learning problem in chapter 2, we discuss linear methods for regression
and classi   cation in chapters 3 and 4. in chapter 5 we describe splines,
wavelets and id173/penalization methods for a single predictor,
while chapter 6 covers kernel methods and local regression. both of these
sets of methods are important building blocks for high-dimensional learn-
ing techniques. model assessment and selection is the topic of chapter 7,
covering the concepts of bias and variance, over   tting and methods such as
cross-validation for choosing models. chapter 8 discusses model id136
and averaging, including an overview of maximum likelihood, bayesian in-
ference and the bootstrap, the em algorithm, id150 and id112,
a related procedure called boosting is the focus of chapter 10.

in chapters 9   13 we describe a series of structured methods for su-
pervised learning, with chapters 9 and 11 covering regression and chap-
ters 12 and 13 focusing on classi   cation. chapter 14 describes methods for
unsupervised learning. two recently proposed techniques, id79s
and id108, are discussed in chapters 15 and 16. we describe
undirected id114 in chapter 17 and    nally we study high-
dimensional problems in chapter 18.

at the end of each chapter we discuss computational considerations im-
portant for data mining applications, including how the computations scale
with the number of observations and predictors. each chapter ends with
bibliographic notes giving background references for the material.

8

1. introduction

we recommend that chapters 1   4 be    rst read in sequence. chapter 7
should also be considered mandatory, as it covers central concepts that
pertain to all learning methods. with this in mind, the rest of the book
can be read sequentially, or sampled, depending on the reader   s interest.

the symbol

indicates a technically di   cult section, one that can

be skipped without interrupting the    ow of the discussion.

book website

the website for this book is located at

http://www-stat.stanford.edu/elemstatlearn

it contains a number of resources, including many of the datasets used in
this book.

note for instructors

we have successively used the    rst edition of this book as the basis for a
two-quarter course, and with the additional materials in this second edition,
it could even be used for a three-quarter sequence. exercises are provided at
the end of each chapter. it is important for students to have access to good
software tools for these topics. we used the r and s-plus programming
languages in our courses.

2
overview of supervised learning

this is page 9
printer: opaque this

2.1

introduction

the    rst three examples described in chapter 1 have several components
in common. for each there is a set of variables that might be denoted as
inputs, which are measured or preset. these have some in   uence on one or
more outputs. for each example the goal is to use the inputs to predict the
values of the outputs. this exercise is called supervised learning.

we have used the more modern language of machine learning. in the
statistical literature the inputs are often called the predictors, a term we
will use interchangeably with inputs, and more classically the independent
variables. in the pattern recognition literature the term features is preferred,
which we use as well. the outputs are called the responses, or classically
the dependent variables.

2.2 variable types and terminology

the outputs vary in nature among the examples. in the glucose prediction
example, the output is a quantitative measurement, where some measure-
ments are bigger than others, and measurements close in value are close
in nature. in the famous iris discrimination example due to r. a. fisher,
the output is qualitative (species of iris) and assumes values in a    nite set
g = {virginica, setosa and versicolor}. in the handwritten digit example
the output is one of 10 di   erent digit classes: g = {0, 1, . . . , 9}. in both of

10

2. overview of supervised learning

these there is no explicit ordering in the classes, and in fact often descrip-
tive labels rather than numbers are used to denote the classes. qualitative
variables are also referred to as categorical or discrete variables as well as
factors.

for both types of outputs it makes sense to think of using the inputs to
predict the output. given some speci   c atmospheric measurements today
and yesterday, we want to predict the ozone level tomorrow. given the
grayscale values for the pixels of the digitized image of the handwritten
digit, we want to predict its class label.

this distinction in output type has led to a naming convention for the
prediction tasks: regression when we predict quantitative outputs, and clas-
si   cation when we predict qualitative outputs. we will see that these two
tasks have a lot in common, and in particular both can be viewed as a task
in function approximation.

inputs also vary in measurement type; we can have some of each of qual-
itative and quantitative input variables. these have also led to distinctions
in the types of methods that are used for prediction: some methods are
de   ned most naturally for quantitative inputs, some most naturally for
qualitative and some for both.

a third variable type is ordered categorical, such as small, medium and
large, where there is an ordering between the values, but no metric notion
is appropriate (the di   erence between medium and small need not be the
same as that between large and medium). these are discussed further in
chapter 4.

qualitative variables are typically represented numerically by codes. the
easiest case is when there are only two classes or categories, such as    suc-
cess    or    failure,       survived    or    died.    these are often represented by a
single binary digit or bit as 0 or 1, or else by    1 and 1. for reasons that will
become apparent, such numeric codes are sometimes referred to as targets.
when there are more than two categories, several alternatives are available.
the most useful and commonly used coding is via dummy variables. here a
k-level qualitative variable is represented by a vector of k binary variables
or bits, only one of which is    on    at a time. although more compact coding
schemes are possible, dummy variables are symmetric in the levels of the
factor.

we will typically denote an input variable by the symbol x. if x is
a vector, its components can be accessed by subscripts xj. quantitative
outputs will be denoted by y , and qualitative outputs by g (for group).
we use uppercase letters such as x, y or g when referring to the generic
aspects of a variable. observed values are written in lowercase; hence the
ith observed value of x is written as xi (where xi is again a scalar or
vector). matrices are represented by bold uppercase letters; for example, a
set of n input p-vectors xi, i = 1, . . . , n would be represented by the n   p
matrix x. in general, vectors will not be bold, except when they have n
components; this convention distinguishes a p-vector of inputs xi for the

2.3 least squares and nearest neighbors

11

ith observation from the n -vector xj consisting of all the observations on
variable xj. since all vectors are assumed to be column vectors, the ith
row of x is xt

i , the vector transpose of xi.

for the moment we can loosely state the learning task as follows: given
the value of an input vector x, make a good prediction of the output y,
denoted by   y (pronounced    y-hat   ). if y takes values in ir then so should
  y ; likewise for categorical outputs,   g should take values in the same set g
associated with g.
for a two-class g, one approach is to denote the binary coded target
as y , and then treat it as a quantitative output. the predictions   y will
typically lie in [0, 1], and we can assign to   g the class label according to
whether   y > 0.5. this approach generalizes to k-level qualitative outputs
as well.

we need data to construct prediction rules, often a lot of it. we thus
suppose we have available a set of measurements (xi, yi) or (xi, gi), i =
1, . . . , n , known as the training data, with which to construct our prediction
rule.

2.3 two simple approaches to prediction: least

squares and nearest neighbors

in this section we develop two simple but powerful prediction methods: the
linear model    t by least squares and the k-nearest-neighbor prediction rule.
the linear model makes huge assumptions about structure and yields stable
but possibly inaccurate predictions. the method of k-nearest neighbors
makes very mild structural assumptions: its predictions are often accurate
but can be unstable.

2.3.1 linear models and least squares

the linear model has been a mainstay of statistics for the past 30 years
and remains one of our most important tools. given a vector of inputs
x t = (x1, x2, . . . , xp), we predict the output y via the model

  y =     0 +

xj     j.

pxj=1

(2.1)

the term     0 is the intercept, also known as the bias in machine learning.
often it is convenient to include the constant variable 1 in x, include     0 in
the vector of coe   cients     , and then write the linear model in vector form
as an inner product

  y = x t     ,

(2.2)

12

2. overview of supervised learning

where x t denotes vector or matrix transpose (x being a column vector).
here we are modeling a single output, so   y is a scalar; in general   y can be
a k   vector, in which case    would be a p   k matrix of coe   cients. in the
(p + 1)-dimensional input   output space, (x,   y ) represents a hyperplane.
if the constant is included in x, then the hyperplane includes the origin
and is a subspace; if not, it is an a   ne set cutting the y -axis at the point
(0,     0). from now on we assume that the intercept is included in     .

viewed as a function over the p-dimensional input space, f (x) = x t   
is linear, and the gradient f    (x) =    is a vector in input space that points
in the steepest uphill direction.

how do we    t the linear model to a set of training data? there are
many di   erent methods, but by far the most popular is the method of
least squares. in this approach, we pick the coe   cients    to minimize the
residual sum of squares

rss(  ) =

(yi     xt

i   )2.

(2.3)

nxi=1

rss(  ) is a quadratic function of the parameters, and hence its minimum
always exists, but may not be unique. the solution is easiest to characterize
in matrix notation. we can write

rss(  ) = (y     x  )t (y     x  ),

(2.4)
where x is an n    p matrix with each row an input vector, and y is an
n -vector of the outputs in the training set. di   erentiating w.r.t.    we get
the normal equations

if xt x is nonsingular, then the unique solution is given by

xt (y     x  ) = 0.

     = (xt x)   1xt y,

(2.5)

(2.6)

    . at an arbi-
and the    tted value at the ith input xi is   yi =   y(xi) = xt
i
    . the entire    tted surface is
trary input x0 the prediction is   y(x0) = xt
0
characterized by the p parameters     . intuitively, it seems that we do not
need a very large data set to    t such a model.

let   s look at an example of the linear model in a classi   cation context.
figure 2.1 shows a scatterplot of training data on a pair of inputs x1 and
x2. the data are simulated, and for the present the simulation model is
not important. the output class variable g has the values blue or orange,
and is represented as such in the scatterplot. there are 100 points in each
of the two classes. the id75 model was    t to these data, with
the response y coded as 0 for blue and 1 for orange. the    tted values   y
are converted to a    tted class variable   g according to the rule

  g =(orange

blue

if   y > 0.5,
if   y     0.5.

(2.7)

2.3 least squares and nearest neighbors

13

id75 of 0/1 response

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . .. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
oo
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
o
o
o
o
o
o

o
o
o
o

o
o

o
o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

figure 2.1. a classi   cation example in two dimensions. the classes are coded
as a binary variable (blue = 0, orange = 1), and then    t by id75.
the line is the decision boundary de   ned by xt      = 0.5. the orange shaded region
denotes that part of input space classi   ed as orange, while the blue region is
classi   ed as blue.

the set of points in ir2 classi   ed as orange corresponds to {x : xt      > 0.5},
indicated in figure 2.1, and the two predicted classes are separated by the
decision boundary {x : xt      = 0.5}, which is linear in this case. we see
that for these data there are several misclassi   cations on both sides of the
decision boundary. perhaps our linear model is too rigid    or are such errors
unavoidable? remember that these are errors on the training data itself,
and we have not said where the constructed data came from. consider the
two possible scenarios:

scenario 1: the training data in each class were generated from bivariate
gaussian distributions with uncorrelated components and di   erent
means.

scenario 2: the training data in each class came from a mixture of 10 low-
variance gaussian distributions, with individual means themselves
distributed as gaussian.

a mixture of gaussians is best described in terms of the generative
model. one    rst generates a discrete variable that determines which of

14

2. overview of supervised learning

the component gaussians to use, and then generates an observation from
the chosen density. in the case of one gaussian per class, we will see in
chapter 4 that a linear decision boundary is the best one can do, and that
our estimate is almost optimal. the region of overlap is inevitable, and
future data to be predicted will be plagued by this overlap as well.

in the case of mixtures of tightly clustered gaussians the story is dif-
ferent. a linear decision boundary is unlikely to be optimal, and in fact is
not. the optimal decision boundary is nonlinear and disjoint, and as such
will be much more di   cult to obtain.

we now look at another classi   cation and regression procedure that is
in some sense at the opposite end of the spectrum to the linear model, and
far better suited to the second scenario.

2.3.2 nearest-neighbor methods
nearest-neighbor methods use those observations in the training set t clos-
est in input space to x to form   y . speci   cally, the k-nearest neighbor    t
for   y is de   ned as follows:

  y (x) =

1

k xxi   nk(x)

yi,

(2.8)

where nk(x) is the neighborhood of x de   ned by the k closest points xi in
the training sample. closeness implies a metric, which for the moment we
assume is euclidean distance. so, in words, we    nd the k observations with
xi closest to x in input space, and average their responses.

in figure 2.2 we use the same training data as in figure 2.1, and use
15-nearest-neighbor averaging of the binary coded response as the method
of    tting. thus   y is the proportion of orange   s in the neighborhood, and
so assigning class orange to   g if   y > 0.5 amounts to a majority vote in
the neighborhood. the colored regions indicate all those points in input
space classi   ed as blue or orange by such a rule, in this case found by
evaluating the procedure on a    ne grid in input space. we see that the
decision boundaries that separate the blue from the orange regions are far
more irregular, and respond to local clusters where one class dominates.

figure 2.3 shows the results for 1-nearest-neighbor classi   cation:   y is
assigned the value y    of the closest point x    to x in the training data. in
this case the regions of classi   cation can be computed relatively easily, and
correspond to a voronoi tessellation of the training data. each point xi
has an associated tile bounding the region for which it is the closest input
point. for all points x in the tile,   g(x) = gi. the decision boundary is even
more irregular than before.

the method of k-nearest-neighbor averaging is de   ned in exactly the
same way for regression of a quantitative output y , although k = 1 would
be an unlikely choice.

2.3 least squares and nearest neighbors

15

15-nearest neighbor classifier

o

o

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
.
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . .
. . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
o
o

o
o
oo

o
o

o
o
o

o
o

o
o

... .. .. .. .. . .

o
o

o
o

o

o

o

o

o

o

o

.
.

o

figure 2.2. the same classi   cation example in two dimensions as in fig-
ure 2.1. the classes are coded as a binary variable (blue = 0, orange = 1) and
then    t by 15-nearest-neighbor averaging as in (2.8). the predicted class is hence
chosen by majority vote amongst the 15-nearest neighbors.

in figure 2.2 we see that far fewer training observations are misclassi   ed
than in figure 2.1. this should not give us too much comfort, though, since
in figure 2.3 none of the training data are misclassi   ed. a little thought
suggests that for k-nearest-neighbor    ts, the error on the training data
should be approximately an increasing function of k, and will always be 0
for k = 1. an independent test set would give us a more satisfactory means
for comparing the di   erent methods.

it appears that k-nearest-neighbor    ts have a single parameter, the num-
ber of neighbors k, compared to the p parameters in least-squares    ts. al-
though this is the case, we will see that the e   ective number of parameters
of k-nearest neighbors is n/k and is generally bigger than p, and decreases
with increasing k. to get an idea of why, note that if the neighborhoods
were nonoverlapping, there would be n/k neighborhoods and we would    t
one parameter (a mean) in each neighborhood.

it is also clear that we cannot use sum-of-squared errors on the training
set as a criterion for picking k, since we would always pick k = 1! it would
seem that k-nearest-neighbor methods would be more appropriate for the
mixture scenario 2 described above, while for gaussian data the decision
boundaries of k-nearest neighbors would be unnecessarily noisy.

16

2. overview of supervised learning

1   nearest neighbor classifier

o
o

o

o

o

o
o
o
o

o
o
o

o
o

o

o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o

o
o

o
o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o

o

o

o
o

o
o

o

o

o
o
oo

o

o

o
o
o

o
o

o
o
o
o
o

figure 2.3. the same classi   cation example in two dimensions as in fig-
ure 2.1. the classes are coded as a binary variable (blue = 0, orange = 1), and
then predicted by 1-nearest-neighbor classi   cation.

2.3.3 from least squares to nearest neighbors

the linear decision boundary from least squares is very smooth, and ap-
parently stable to    t. it does appear to rely heavily on the assumption
that a linear decision boundary is appropriate. in language we will develop
shortly, it has low variance and potentially high bias.

on the other hand, the k-nearest-neighbor procedures do not appear to
rely on any stringent assumptions about the underlying data, and can adapt
to any situation. however, any particular subregion of the decision bound-
ary depends on a handful of input points and their particular positions,
and is thus wiggly and unstable   high variance and low bias.

each method has its own situations for which it works best; in particular
id75 is more appropriate for scenario 1 above, while nearest
neighbors are more suitable for scenario 2. the time has come to expose
the oracle! the data in fact were simulated from a model somewhere be-
tween the two, but closer to scenario 2. first we generated 10 means mk
from a bivariate gaussian distribution n ((1, 0)t , i) and labeled this class
blue. similarly, 10 more were drawn from n ((0, 1)t , i) and labeled class
orange. then for each class we generated 100 observations as follows: for
each observation, we picked an mk at random with id203 1/10, and

2.3 least squares and nearest neighbors

17

k      number of nearest neighbors

151 101  69

 45  31  21

 11

  7   5

  3

  1

r
o
r
r

e

 
t
s
e
t

0
3

.

0

5
2
0

.

0
2
0

.

5
1
0

.

0
1
.
0

linear

train
test
bayes

  2

  3

  5

  8

 12

 18

 29

 67

200

degrees of freedom     n/k

figure 2.4. misclassi   cation curves for the simulation example used in fig-
ures 2.1, 2.2 and 2.3. a single training sample of size 200 was used, and a test
sample of size 10, 000. the orange curves are test and the blue are training er-
ror for k-nearest-neighbor classi   cation. the results for id75 are the
bigger orange and blue squares at three degrees of freedom. the purple line is the
optimal bayes error rate.

then generated a n (mk, i/5), thus leading to a mixture of gaussian clus-
ters for each class. figure 2.4 shows the results of classifying 10,000 new
observations generated from the model. we compare the results for least
squares and those for k-nearest neighbors for a range of values of k.

a large subset of the most popular techniques in use today are variants of
these two simple procedures. in fact 1-nearest-neighbor, the simplest of all,
captures a large percentage of the market for low-dimensional problems.
the following list describes some ways in which these simple procedures
have been enhanced:

    kernel methods use weights that decrease smoothly to zero with dis-
tance from the target point, rather than the e   ective 0/1 weights used
by k-nearest neighbors.

    in high-dimensional spaces the distance kernels are modi   ed to em-

phasize some variable more than others.

18

2. overview of supervised learning

    local regression    ts linear models by locally weighted least squares,

rather than    tting constants locally.

    linear models    t to a basis expansion of the original inputs allow

arbitrarily complex models.

    projection pursuit and neural network models consist of sums of non-

linearly transformed linear models.

2.4 statistical decision theory

in this section we develop a small amount of theory that provides a frame-
work for developing models such as those discussed informally so far. we
   rst consider the case of a quantitative output, and place ourselves in the
world of random variables and id203 spaces. let x     irp denote a
real valued random input vector, and y     ir a real valued random out-
put variable, with joint distribution pr(x, y ). we seek a function f (x)
for predicting y given values of the input x. this theory requires a loss
function l(y, f (x)) for penalizing errors in prediction, and by far the most
common and convenient is squared error loss: l(y, f (x)) = (y     f (x))2.
this leads us to a criterion for choosing f ,

epe(f ) = e(y     f (x))2

= z [y     f (x)]2 pr(dx, dy),

(2.9)

(2.10)

the expected (squared) prediction error . by conditioning1 on x, we can
write epe as

and we see that it su   ces to minimize epe pointwise:

epe(f ) = ex ey |x(cid:0)[y     f (x)]2|x(cid:1)
f (x) = argmincey |x(cid:0)[y     c]2|x = x(cid:1) .

f (x) = e(y |x = x),

the solution is

(2.11)

(2.12)

(2.13)

the conditional expectation, also known as the regression function. thus
the best prediction of y at any point x = x is the conditional mean, when
best is measured by average squared error.

the nearest-neighbor methods attempt to directly implement this recipe
using the training data. at each point x, we might ask for the average of all

1conditioning here amounts to factoring the joint density pr(x, y ) = pr(y |x)pr(x)
where pr(y |x) = pr(y, x)/pr(x), and splitting up the bivariate integral accordingly.

2.4 statistical decision theory

19

those yis with input xi = x. since there is typically at most one observation
at any point x, we settle for

  f (x) = ave(yi|xi     nk(x)),

(2.14)

where    ave    denotes average, and nk(x) is the neighborhood containing
the k points in t closest to x. two approximations are happening here:

    expectation is approximated by averaging over sample data;
    conditioning at a point is relaxed to conditioning on some region

   close    to the target point.

for large training sample size n , the points in the neighborhood are likely
to be close to x, and as k gets large the average will get more stable.
in fact, under mild regularity conditions on the joint id203 distri-
bution pr(x, y ), one can show that as n, k         such that k/n     0,
  f (x)     e(y |x = x). in light of this, why look further, since it seems
we have a universal approximator? we often do not have very large sam-
ples. if the linear or some more structured model is appropriate, then we
can usually get a more stable estimate than k-nearest neighbors, although
such knowledge has to be learned from the data as well. there are other
problems though, sometimes disastrous. in section 2.5 we see that as the
dimension p gets large, so does the metric size of the k-nearest neighbor-
hood. so settling for nearest neighborhood as a surrogate for conditioning
will fail us miserably. the convergence above still holds, but the rate of
convergence decreases as the dimension increases.

how does id75    t into this framework? the simplest explana-
tion is that one assumes that the regression function f (x) is approximately
linear in its arguments:

f (x)     xt   .

(2.15)

this is a model-based approach   we specify a model for the regression func-
tion. plugging this linear model for f (x) into epe (2.9) and di   erentiating
we can solve for    theoretically:

   = [e(xx t )]   1e(xy ).

(2.16)

note we have not conditioned on x; rather we have used our knowledge
of the functional relationship to pool over values of x. the least squares
solution (2.6) amounts to replacing the expectation in (2.16) by averages
over the training data.

so both k-nearest neighbors and least squares end up approximating
conditional expectations by averages. but they di   er dramatically in terms
of model assumptions:

    least squares assumes f (x) is well approximated by a globally linear

function.

20

2. overview of supervised learning

    k-nearest neighbors assumes f (x) is well approximated by a locally

constant function.

although the latter seems more palatable, we have already seen that we
may pay a price for this    exibility.

many of the more modern techniques described in this book are model
based, although far more    exible than the rigid linear model. for example,
additive models assume that

f (x) =

fj(xj).

pxj=1

(2.17)

this retains the additivity of the linear model, but each coordinate function
fj is arbitrary. it turns out that the optimal estimate for the additive model
uses techniques such as k-nearest neighbors to approximate univariate con-
ditional expectations simultaneously for each of the coordinate functions.
thus the problems of estimating a conditional expectation in high dimen-
sions are swept away in this case by imposing some (often unrealistic) model
assumptions, in this case additivity.

are we happy with the criterion (2.11)? what happens if we replace the
l2 id168 with the l1: e|y     f (x)|? the solution in this case is the
conditional median,

  f (x) = median(y |x = x),

(2.18)

which is a di   erent measure of location, and its estimates are more robust
than those for the conditional mean. l1 criteria have discontinuities in
their derivatives, which have hindered their widespread use. other more
resistant id168s will be mentioned in later chapters, but squared
error is analytically convenient and the most popular.

what do we do when the output is a categorical variable g? the same
paradigm works here, except we need a di   erent id168 for penalizing
prediction errors. an estimate   g will assume values in g, the set of possible
classes. our id168 can be represented by a k    k matrix l, where
k = card(g). l will be zero on the diagonal and nonnegative elsewhere,
where l(k,    ) is the price paid for classifying an observation belonging to
class gk as g   . most often we use the zero   one id168, where all
misclassi   cations are charged a single unit. the expected prediction error
is

epe = e[l(g,   g(x))],

(2.19)

where again the expectation is taken with respect to the joint distribution
pr(g, x). again we condition, and can write epe as

epe = ex

kxk=1

l[gk,   g(x)]pr(gk|x)

(2.20)

2.4 statistical decision theory

21

bayes optimal classifier

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . .
o
o
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . .
. . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
. .
. . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
. . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
... .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
o
o

o
o
oo

o
o
o

o
o

o
o

o

o

o

o

o

o

o

figure 2.5. the optimal bayes decision boundary for the simulation example
of figures 2.1, 2.2 and 2.3. since the generating density is known for each class,
this boundary can be calculated exactly (exercise 2.2).

and again it su   ces to minimize epe pointwise:

  g(x) = argming   g

kxk=1

l(gk, g)pr(gk|x = x).

(2.21)

with the 0   1 id168 this simpli   es to

  g(x) = argming   g[1     pr(g|x = x)]

(2.22)

or simply

  g(x) = gk if pr(gk|x = x) = max

g   g

pr(g|x = x).

(2.23)

this reasonable solution is known as the bayes classi   er, and says that
we classify to the most probable class, using the conditional (discrete) dis-
tribution pr(g|x). figure 2.5 shows the bayes-optimal decision boundary
for our simulation example. the error rate of the bayes classi   er is called
the bayes rate.

22

2. overview of supervised learning

again we see that the k-nearest neighbor classi   er directly approximates
this solution   a majority vote in a nearest neighborhood amounts to ex-
actly this, except that id155 at a point is relaxed to con-
ditional id203 within a neighborhood of a point, and probabilities are
estimated by training-sample proportions.

suppose for a two-class problem we had taken the dummy-variable ap-
proach and coded g via a binary y , followed by squared error loss estima-
tion. then   f (x) = e(y |x) = pr(g = g1|x) if g1 corresponded to y = 1.
likewise for a k-class problem, e(yk|x) = pr(g = gk|x). this shows
that our dummy-variable regression procedure, followed by classi   cation to
the largest    tted value, is another way of representing the bayes classi   er.
although this theory is exact, in practice problems can occur, depending
on the regression model used. for example, when id75 is used,
  f (x) need not be positive, and we might be suspicious about using it as
an estimate of a id203. we will discuss a variety of approaches to
modeling pr(g|x) in chapter 4.

2.5 local methods in high dimensions

we have examined two learning techniques for prediction so far: the stable
but biased linear model and the less stable but apparently less biased class
of k-nearest-neighbor estimates. it would seem that with a reasonably large
set of training data, we could always approximate the theoretically optimal
conditional expectation by k-nearest-neighbor averaging, since we should
be able to    nd a fairly large neighborhood of observations close to any x
and average them. this approach and our intuition breaks down in high
dimensions, and the phenomenon is commonly referred to as the curse
of dimensionality (bellman, 1961). there are many manifestations of this
problem, and we will examine a few here.

consider the nearest-neighbor procedure for inputs uniformly distributed
in a p-dimensional unit hypercube, as in figure 2.6. suppose we send out a
hypercubical neighborhood about a target point to capture a fraction r of
the observations. since this corresponds to a fraction r of the unit volume,
the expected edge length will be ep(r) = r1/p. in ten dimensions e10(0.01) =
0.63 and e10(0.1) = 0.80, while the entire range for each input is only 1.0.
so to capture 1% or 10% of the data to form a local average, we must cover
63% or 80% of the range of each input variable. such neighborhoods are no
longer    local.    reducing r dramatically does not help much either, since
the fewer observations we average, the higher is the variance of our    t.

another consequence of the sparse sampling in high dimensions is that
all sample points are close to an edge of the sample. consider n data points
uniformly distributed in a p-dimensional unit ball centered at the origin.
suppose we consider a nearest-neighbor estimate at the origin. the median

2.5 local methods in high dimensions

23

unit cube

1

0

1

neighborhood

e
c
n
a

t
s
d

i

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

p=10

p=3
p=2

p=1

0.0

0.2

0.4

0.6

fraction of volume

figure 2.6. the curse of dimensionality is well illustrated by a subcubical
neighborhood for uniform data in a unit cube. the    gure on the right shows the
side-length of the subcube needed to capture a fraction r of the volume of the data,
for di   erent dimensions p. in ten dimensions we need to cover 80% of the range
of each coordinate to capture 10% of the data.

distance from the origin to the closest data point is given by the expression

d(p, n ) =(cid:16)1    

1
2

1/n(cid:17)1/p

(2.24)

(exercise 2.3). a more complicated expression exists for the mean distance
to the closest point. for n = 500, p = 10 , d(p, n )     0.52, more than
halfway to the boundary. hence most data points are closer to the boundary
of the sample space than to any other data point. the reason that this
presents a problem is that prediction is much more di   cult near the edges
of the training sample. one must extrapolate from neighboring sample
points rather than interpolate between them.

another manifestation of the curse is that the sampling density is pro-
portional to n 1/p, where p is the dimension of the input space and n is the
sample size. thus, if n1 = 100 represents a dense sample for a single input
problem, then n10 = 10010 is the sample size required for the same sam-
pling density with 10 inputs. thus in high dimensions all feasible training
samples sparsely populate the input space.

let us construct another uniform example. suppose we have 1000 train-
ing examples xi generated uniformly on [   1, 1]p. assume that the true
relationship between x and y is

y = f (x) = e   8||x||2

,

without any measurement error. we use the 1-nearest-neighbor rule to
predict y0 at the test-point x0 = 0. denote the training set by t . we can

24

2. overview of supervised learning

compute the expected prediction error at x0 for our procedure, averaging
over all such samples of size 1000. since the problem is deterministic, this
is the mean squared error (mse) for estimating f (0):

mse(x0) = et [f (x0)       y0]2

= et [  y0     et (  y0)]2 + [et (  y0)     f (x0)]2
= vart (  y0) + bias2(  y0).

(2.25)

figure 2.7 illustrates the setup. we have broken down the mse into two
components that will become familiar as we proceed: variance and squared
bias. such a decomposition is always possible and often useful, and is known
as the bias   variance decomposition. unless the nearest neighbor is at 0,
  y0 will be smaller than f (0) in this example, and so the average estimate
will be biased downward. the variance is due to the sampling variance of
the 1-nearest neighbor. in low dimensions and with n = 1000, the nearest
neighbor is very close to 0, and so both the bias and variance are small. as
the dimension increases, the nearest neighbor tends to stray further from
the target point, and both bias and variance are incurred. by p = 10, for
more than 99% of the samples the nearest neighbor is a distance greater
than 0.5 from the origin. thus as p increases, the estimate tends to be 0
more often than not, and hence the mse levels o    at 1.0, as does the bias,
and the variance starts dropping (an artifact of this example).

although this is a highly contrived example, similar phenomena occur
more generally. the complexity of functions of many variables can grow
exponentially with the dimension, and if we wish to be able to estimate
such functions with the same accuracy as function in low dimensions, then
we need the size of our training set to grow exponentially as well. in this
example, the function is a complex interaction of all p variables involved.
the dependence of the bias term on distance depends on the truth, and
it need not always dominate with 1-nearest neighbor. for example, if the
function always involves only a few dimensions as in figure 2.8, then the
variance can dominate instead.

suppose, on the other hand, that we know that the relationship between

y and x is linear,

y = x t    +   ,

(2.26)

where        n (0,   2) and we    t the model by least squares to the train-
    , which can
ing data. for an arbitrary test point x0, we have   y0 = xt
0
be written as   y0 = xt
i=1    i(x0)  i, where    i(x0) is the ith element
of x(xt x)   1x0. since under this model the least squares estimates are

0    +pn

2.5 local methods in high dimensions

25

1-nn in one dimension

1-nn in one vs. two dimensions

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

8
0

.

6
0

.

4

.

0

2
0

.

0
0

.

)

x

(
f

r
o
b
h
g
e
n

i

 
t
s
e
r
a
e
n
o

 

t
 

i

e
c
n
a
t
s
d
e
g
a
r
e
v
a

 

   

   

   

   

   
   
   

   

   

0

.

1

5

.

0

2
x

0

.

0

5

.

0
-

.

0
1
-

-1.0

-0.5

0.0

x

0.5

1.0

-1.0

-0.5

   

   

   

   
   
   

   

   

   

   

   

   

   

   

   

   

   

   

0.0

x1

0.5

1.0

distance to 1-nn vs. dimension

mse vs. dimension

   

   

   

   

   

   

6

8

10

e
s
m

0

.

1

8

.

0

6
0

.

4

.

0

2

.

0

0
0

.

   
   
   

mse
variance
sq. bias

   
   

   
   

   
   

   
   

   
   

   

6

   
   

   

   
   
   

   
   
   

   
   
   
2

   
   
   

4

   

   
8

   

   
10

dimension

dimension

   

   

4

   

   

2

figure 2.7. a simulation example, demonstrating the curse of dimensional-
ity and its e   ect on mse, bias and variance. the input features are uniformly
distributed in [   1, 1]p for p = 1, . . . , 10 the top left panel shows the target func-
tion (no noise) in ir: f (x) = e   8||x||2
, and demonstrates the error that 1-nearest
neighbor makes in estimating f (0). the training point is indicated by the blue tick
mark. the top right panel illustrates why the radius of the 1-nearest neighborhood
increases with dimension p. the lower left panel shows the average radius of the
1-nearest neighborhoods. the lower-right panel shows the mse, squared bias and
variance curves as a function of dimension p.

26

2. overview of supervised learning

1-nn in one dimension

mse  vs. dimension

4

3

)

x

(
f

2

1

0

   

0.0

x

0.5

1.0

-1.0

-0.5

e
s
m

5
2
.
0

0
2
.
0

5
1
.
0

0
1
.
0

5
0
.
0

0
.
0

   
   
   

mse
variance
sq. bias

   
   

   

   
   
   
4

   
   
   

   
   
   

   
   
   
2

   
   

   

   
   

   

10

   
   

   
   

   

   
8

   
   

   
6

dimension

figure 2.8. a simulation example with the same setup as in figure 2.7. here
2 (x1 + 1)3. the
the function is constant in all but one dimension: f (x) = 1
variance dominates.

unbiased, we    nd that

epe(x0) = ey0|x0 et (y0       y0)2

= var(y0|x0) + et [  y0     et   y0]2 + [et   y0     xt
= var(y0|x0) + vart (  y0) + bias2(  y0)
=   2 + et xt

0 (xt x)   1x0  2 + 02.

0   ]2

(2.27)

here we have incurred an additional variance   2 in the prediction error,
since our target is not deterministic. there is no bias, and the variance
depends on x0. if n is large and t were selected at random, and assuming
e(x) = 0, then xt x     n cov(x) and

ex0epe(x0)     ex0xt

0 cov(x)   1x0  2/n +   2

= trace[cov(x)   1cov(x0)]  2/n +   2
=   2(p/n ) +   2.

(2.28)

here we see that the expected epe increases linearly as a function of p,
with slope   2/n . if n is large and/or   2 is small, this growth in vari-
ance is negligible (0 in the deterministic case). by imposing some heavy
restrictions on the class of models being    tted, we have avoided the curse
of dimensionality. some of the technical details in (2.27) and (2.28) are
derived in exercise 2.5.

figure 2.9 compares 1-nearest neighbor vs. least squares in two situa-
tions, both of which have the form y = f (x) +   , x uniform as before,
and        n (0, 1). the sample size is n = 500. for the orange curve, f (x)

2.5 local methods in high dimensions

27

expected prediction error of 1nn vs. ols

o

i
t

 

a
r
e
p
e

1

.

2

0

.

2

9
1

.

8
1

.

7

.

1

6
1

.

   

   

   

   

   
   

linear
cubic

   

   

   

2

   

4

   

   

   

   

8

   

   

   

   

10

   

   

   

   

6

dimension

figure 2.9. the curves show the expected prediction error (at x0 = 0) for
1-nearest neighbor relative to least squares for the model y = f (x) +   . for the
orange curve, f (x) = x1, while for the blue curve f (x) = 1

2 (x1 + 1)3.

is linear in the    rst coordinate, for the blue curve, cubic as in figure 2.8.
shown is the relative epe of 1-nearest neighbor to least squares, which
appears to start at around 2 for the linear case. least squares is unbiased
in this case, and as discussed above the epe is slightly above   2 = 1.
the epe for 1-nearest neighbor is always above 2, since the variance of
  f (x0) in this case is at least   2, and the ratio increases with dimension as
the nearest neighbor strays from the target point. for the cubic case, least
squares is biased, which moderates the ratio. clearly we could manufacture
examples where the bias of least squares would dominate the variance, and
the 1-nearest neighbor would come out the winner.

by relying on rigid assumptions, the linear model has no bias at all and
negligible variance, while the error in 1-nearest neighbor is substantially
larger. however, if the assumptions are wrong, all bets are o    and the
1-nearest neighbor may dominate. we will see that there is a whole spec-
trum of models between the rigid linear models and the extremely    exible
1-nearest-neighbor models, each with their own assumptions and biases,
which have been proposed speci   cally to avoid the exponential growth in
complexity of functions in high dimensions by drawing heavily on these
assumptions.

before we delve more deeply, let us elaborate a bit on the concept of

statistical models and see how they    t into the prediction framework.

28

2. overview of supervised learning

2.6 statistical models, supervised learning and

function approximation

our goal is to    nd a useful approximation   f (x) to the function f (x) that
underlies the predictive relationship between the inputs and outputs. in the
theoretical setting of section 2.4, we saw that squared error loss lead us
to the regression function f (x) = e(y |x = x) for a quantitative response.
the class of nearest-neighbor methods can be viewed as direct estimates
of this conditional expectation, but we have seen that they can fail in at
least two ways:

    if the dimension of the input space is high, the nearest neighbors need

not be close to the target point, and can result in large errors;

    if special structure is known to exist, this can be used to reduce both

the bias and the variance of the estimates.

we anticipate using other classes of models for f (x), in many cases specif-
ically designed to overcome the dimensionality problems, and here we dis-
cuss a framework for incorporating them into the prediction problem.

2.6.1 a statistical model for the joint distribution pr(x, y )

suppose in fact that our data arose from a statistical model

y = f (x) +   ,

(2.29)

where the random error    has e(  ) = 0 and is independent of x. note that
for this model, f (x) = e(y |x = x), and in fact the conditional distribution
pr(y |x) depends on x only through the conditional mean f (x).
the additive error model is a useful approximation to the truth. for
most systems the input   output pairs (x, y ) will not have a deterministic
relationship y = f (x). generally there will be other unmeasured variables
that also contribute to y , including measurement error. the additive model
assumes that we can capture all these departures from a deterministic re-
lationship via the error   .

for some problems a deterministic relationship does hold. many of the
classi   cation problems studied in machine learning are of this form, where
the response surface can be thought of as a colored map de   ned in irp.
the training data consist of colored examples from the map {xi, gi}, and
the goal is to be able to color any point. here the function is deterministic,
and the randomness enters through the x location of the training points.
for the moment we will not pursue such problems, but will see that they
can be handled by techniques appropriate for the error-based models.

the assumption in (2.29) that the errors are independent and identically
distributed is not strictly necessary, but seems to be at the back of our mind

2.6 statistical models, supervised learning and function approximation

29

when we average squared errors uniformly in our epe criterion. with such
a model it becomes natural to use least squares as a data criterion for
model estimation as in (2.1). simple modi   cations can be made to avoid
the independence assumption; for example, we can have var(y |x = x) =
  (x), and now both the mean and variance depend on x. in general the
conditional distribution pr(y |x) can depend on x in complicated ways,
but the additive error model precludes these.
so far we have concentrated on the quantitative response. additive error
models are typically not used for qualitative outputs g; in this case the tar-
get function p(x) is the conditional density pr(g|x), and this is modeled
directly. for example, for two-class data, it is often reasonable to assume
that the data arise from independent binary trials, with the id203 of
one particular outcome being p(x), and the other 1     p(x). thus if y is
the 0   1 coded version of g, then e(y |x = x) = p(x), but the variance
depends on x as well: var(y |x = x) = p(x)[1     p(x)].

2.6.2 supervised learning

before we launch into more statistically oriented jargon, we present the
function-   tting paradigm from a machine learning point of view. suppose
for simplicity that the errors are additive and that the model y = f (x) +   
is a reasonable assumption. supervised learning attempts to learn f by
example through a teacher. one observes the system under study, both
the inputs and outputs, and assembles a training set of observations t =
(xi, yi), i = 1, . . . , n . the observed input values to the system xi are also
fed into an arti   cial system, known as a learning algorithm (usually a com-
puter program), which also produces outputs   f (xi) in response to the in-
puts. the learning algorithm has the property that it can modify its in-
put/output relationship   f in response to di   erences yi       f (xi) between the
original and generated outputs. this process is known as learning by exam-
ple. upon completion of the learning process the hope is that the arti   cial
and real outputs will be close enough to be useful for all sets of inputs likely
to be encountered in practice.

2.6.3 function approximation

the learning paradigm of the previous section has been the motivation
for research into the supervised learning problem in the    elds of machine
learning (with analogies to human reasoning) and neural networks (with
biological analogies to the brain). the approach taken in applied mathe-
matics and statistics has been from the perspective of function approxima-
tion and estimation. here the data pairs {xi, yi} are viewed as points in a
(p + 1)-dimensional euclidean space. the function f (x) has domain equal
to the p-dimensional input subspace, and is related to the data via a model

30

2. overview of supervised learning

such as yi = f (xi) +   i. for convenience in this chapter we will assume the
domain is irp, a p-dimensional euclidean space, although in general the
inputs can be of mixed type. the goal is to obtain a useful approximation
to f (x) for all x in some region of irp, given the representations in t .
although somewhat less glamorous than the learning paradigm, treating
supervised learning as a problem in function approximation encourages the
geometrical concepts of euclidean spaces and mathematical concepts of
probabilistic id136 to be applied to the problem. this is the approach
taken in this book.

many of the approximations we will encounter have associated a set of
parameters    that can be modi   ed to suit the data at hand. for example,
the linear model f (x) = xt    has    =   . another class of useful approxi-
mators can be expressed as linear basis expansions

f  (x) =

kxk=1

hk(x)  k,

(2.30)

where the hk are a suitable set of functions or transformations of the input
vector x. traditional examples are polynomial and trigonometric expan-
sions, where for example hk might be x2
2, cos(x1) and so on. we
also encounter nonlinear expansions, such as the sigmoid transformation
common to neural network models,

1, x1x2

hk(x) =

1

1 + exp(   xt   k)

.

(2.31)

we can use least squares to estimate the parameters    in f   as we did

for the linear model, by minimizing the residual sum-of-squares

rss(  ) =

nxi=1

(yi     f  (xi))2

(2.32)

as a function of   . this seems a reasonable criterion for an additive error
model. in terms of function approximation, we imagine our parameterized
function as a surface in p + 1 space, and what we observe are noisy re-
alizations from it. this is easy to visualize when p = 2 and the vertical
coordinate is the output y, as in figure 2.10. the noise is in the output
coordinate, so we    nd the set of parameters such that the    tted surface
gets as close to the observed points as possible, where close is measured by
the sum of squared vertical errors in rss(  ).

for the linear model we get a simple closed form solution to the mini-
mization problem. this is also true for the basis function methods, if the
basis functions themselves do not have any hidden parameters. otherwise
the solution requires either iterative methods or numerical optimization.

while least squares is generally very convenient, it is not the only crite-
rion used and in some cases would not make much sense. a more general

2.6 statistical models, supervised learning and function approximation

31

   
   

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   

   
   
   

   
   
   

          
   
   
   
   
   

   

   
   
   

   
   

   
   

   
   

   

   
   
   

   
   
   

   
   
   
   
   
      
   
   
   
   
   
   

   
   
   

   

   

   
      
   
   
   

   

figure 2.10. least squares    tting of a function of two inputs. the parameters
of f  (x) are chosen so as to minimize the sum-of-squared vertical errors.

principle for estimation is id113. suppose we have
a random sample yi, i = 1, . . . , n from a density pr  (y) indexed by some
parameters   . the log-id203 of the observed sample is

l(  ) =

nxi=1

log pr  (yi).

(2.33)

the principle of maximum likelihood assumes that the most reasonable
values for    are those for which the id203 of the observed sample is
largest. least squares for the additive error model y = f  (x) +   , with
       n (0,   2), is equivalent to maximum likelihood using the conditional
likelihood
(2.34)

pr(y |x,   ) = n (f  (x),   2).

so although the additional assumption of normality seems more restrictive,
the results are the same. the log-likelihood of the data is

l(  ) =    

n
2

log(2  )     n log       

1
2  2

nxi=1

(yi     f  (xi))2,

(2.35)

and the only term involving    is the last, which is rss(  ) up to a scalar
negative multiplier.

a more interesting example is the multinomial likelihood for the regres-
sion function pr(g|x) for a qualitative output g. suppose we have a model
pr(g = gk|x = x) = pk,  (x), k = 1, . . . , k for the conditional probabil-
ity of each class given x, indexed by the parameter vector   . then the

32

2. overview of supervised learning

log-likelihood (also referred to as the cross-id178) is

l(  ) =

nxi=1

log pgi,  (xi),

(2.36)

and when maximized it delivers values of    that best conform with the data
in this likelihood sense.

2.7 structured regression models

we have seen that although nearest-neighbor and other local methods focus
directly on estimating the function at a point, they face problems in high
dimensions. they may also be inappropriate even in low dimensions in
cases where more structured approaches can make more e   cient use of the
data. this section introduces classes of such structured approaches. before
we proceed, though, we discuss further the need for such classes.

2.7.1 di   culty of the problem

consider the rss criterion for an arbitrary function f ,

rss(f ) =

nxi=1

(yi     f (xi))2.

(2.37)

minimizing (2.37) leads to in   nitely many solutions: any function   f passing
through the training points (xi, yi) is a solution. any particular solution
chosen might be a poor predictor at test points di   erent from the training
points. if there are multiple observation pairs xi, yi   ,     = 1, . . . , ni at each
value of xi, the risk is limited. in this case, the solutions pass through
the average values of the yi    at each xi; see exercise 2.6. the situation is
similar to the one we have already visited in section 2.4; indeed, (2.37) is
the    nite sample version of (2.11) on page 18. if the sample size n were
su   ciently large such that repeats were guaranteed and densely arranged,
it would seem that these solutions might all tend to the limiting conditional
expectation.

in order to obtain useful results for    nite n , we must restrict the eligible
solutions to (2.37) to a smaller set of functions. how to decide on the
nature of the restrictions is based on considerations outside of the data.
these restrictions are sometimes encoded via the parametric representation
of f  , or may be built into the learning method itself, either implicitly or
explicitly. these restricted classes of solutions are the major topic of this
book. one thing should be clear, though. any restrictions imposed on f
that lead to a unique solution to (2.37) do not really remove the ambiguity

2.8 classes of restricted estimators

33

caused by the multiplicity of solutions. there are in   nitely many possible
restrictions, each leading to a unique solution, so the ambiguity has simply
been transferred to the choice of constraint.

in general the constraints imposed by most learning methods can be
described as complexity restrictions of one kind or another. this usually
means some kind of regular behavior in small neighborhoods of the input
space. that is, for all input points x su   ciently close to each other in
some metric,   f exhibits some special structure such as nearly constant,
linear or low-order polynomial behavior. the estimator is then obtained by
averaging or polynomial    tting in that neighborhood.

the strength of the constraint is dictated by the neighborhood size. the
larger the size of the neighborhood, the stronger the constraint, and the
more sensitive the solution is to the particular choice of constraint. for
example, local constant    ts in in   nitesimally small neighborhoods is no
constraint at all; local linear    ts in very large neighborhoods is almost a
globally linear model, and is very restrictive.

the nature of the constraint depends on the metric used. some methods,
such as kernel and local regression and tree-based methods, directly specify
the metric and size of the neighborhood. the nearest-neighbor methods
discussed so far are based on the assumption that locally the function is
constant; close to a target input x0, the function does not change much, and
so close outputs can be averaged to produce   f (x0). other methods such
as splines, neural networks and basis-function methods implicitly de   ne
neighborhoods of local behavior. in section 5.4.1 we discuss the concept
of an equivalent kernel (see figure 5.8 on page 157), which describes this
local dependence for any method linear in the outputs. these equivalent
kernels in many cases look just like the explicitly de   ned weighting kernels
discussed above   peaked at the target point and falling away smoothly
away from it.

one fact should be clear by now. any method that attempts to pro-
duce locally varying functions in small isotropic neighborhoods will run
into problems in high dimensions   again the curse of dimensionality. and
conversely, all methods that overcome the dimensionality problems have an
associated   and often implicit or adaptive   metric for measuring neighbor-
hoods, which basically does not allow the neighborhood to be simultane-
ously small in all directions.

2.8 classes of restricted estimators

the variety of nonparametric regression techniques or learning methods fall
into a number of di   erent classes depending on the nature of the restrictions
imposed. these classes are not distinct, and indeed some methods fall in
several classes. here we give a brief summary, since detailed descriptions

34

2. overview of supervised learning

are given in later chapters. each of the classes has associated with it one
or more parameters, sometimes appropriately called smoothing parameters,
that control the e   ective size of the local neighborhood. here we describe
three broad classes.

2.8.1 roughness penalty and bayesian methods

here the class of functions is controlled by explicitly penalizing rss(f )
with a roughness penalty

prss(f ;   ) = rss(f ) +   j(f ).

(2.38)

the user-selected functional j(f ) will be large for functions f that vary too
rapidly over small regions of input space. for example, the popular cubic
smoothing spline for one-dimensional inputs is the solution to the penalized
least-squares criterion

prss(f ;   ) =

(yi     f (xi))2 +   z [f       (x)]2dx.

nxi=1

(2.39)

the roughness penalty here controls large values of the second derivative
of f , and the amount of penalty is dictated by        0. for    = 0 no penalty
is imposed, and any interpolating function will do, while for    =     only
functions linear in x are permitted.
penalty functionals j can be constructed for functions in any dimension,
and special versions can be created to impose special structure. for ex-
j=1 j(fj) are used in conjunction with
j=1 fj(xj) to create additive models with
smooth coordinate functions. similarly, projection pursuit regression mod-
mx) for adaptively chosen directions   m, and

ample, additive penalties j(f ) =pp
additive functions f (x) = pp
els have f (x) =pm

the functions gm can each have an associated roughness penalty.

penalty function, or id173 methods, express our prior belief that
the type of functions we seek exhibit a certain type of smooth behavior, and
indeed can usually be cast in a bayesian framework. the penalty j corre-
sponds to a log-prior, and prss(f ;   ) the log-posterior distribution, and
minimizing prss(f ;   ) amounts to    nding the posterior mode. we discuss
roughness-penalty approaches in chapter 5 and the bayesian paradigm in
chapter 8.

m=1 gm(  t

2.8.2 kernel methods and local regression

these methods can be thought of as explicitly providing estimates of the re-
gression function or conditional expectation by specifying the nature of the
local neighborhood, and of the class of regular functions    tted locally. the
local neighborhood is speci   ed by a id81 k  (x0, x) which assigns

2.8 classes of restricted estimators

35

weights to points x in a region around x0 (see figure 6.1 on page 192). for
example, the gaussian kernel has a weight function based on the gaussian
density function

k  (x0, x) =

1
  

exp(cid:20)   ||x     x0||2

2  

(cid:21)

(2.40)

and assigns weights to points that die exponentially with their squared
euclidean distance from x0. the parameter    corresponds to the variance
of the gaussian density, and controls the width of the neighborhood. the
simplest form of kernel estimate is the nadaraya   watson weighted average

  f (x0) = pn
pn

i=1 k  (x0, xi)yi
i=1 k  (x0, xi)

.

(2.41)

in general we can de   ne a local regression estimate of f (x0) as f    (x0),
where      minimizes

rss(f  , x0) =

nxi=1

k  (x0, xi)(yi     f  (xi))2,

(2.42)

and f   is some parameterized function, such as a low-order polynomial.
some examples are:

    f  (x) =   0, the constant function; this results in the nadaraya   

watson estimate in (2.41) above.

    f  (x) =   0 +   1x gives the popular local id75 model.
nearest-neighbor methods can be thought of as kernel methods having a
more data-dependent metric. indeed, the metric for k-nearest neighbors is

kk(x, x0) = i(||x     x0||     ||x(k)     x0||),

where x(k) is the training observation ranked kth in distance from x0, and
i(s) is the indicator of the set s.

these methods of course need to be modi   ed in high dimensions, to avoid
the curse of dimensionality. various adaptations are discussed in chapter 6.

2.8.3 basis functions and dictionary methods

this class of methods includes the familiar linear and polynomial expan-
sions, but more importantly a wide variety of more    exible models. the
model for f is a linear expansion of basis functions

f  (x) =

mxm=1

  mhm(x),

(2.43)

36

2. overview of supervised learning

where each of the hm is a function of the input x, and the term linear here
refers to the action of the parameters   . this class covers a wide variety of
methods. in some cases the sequence of basis functions is prescribed, such
as a basis for polynomials in x of total degree m .

for one-dimensional x, polynomial splines of degree k can be represented
by an appropriate sequence of m spline basis functions, determined in turn
by m   k   1 knots. these produce functions that are piecewise polynomials
of degree k between the knots, and joined up with continuity of degree
k     1 at the knots. as an example consider linear splines, or piecewise
linear functions. one intuitively satisfying basis consists of the functions
b1(x) = 1, b2(x) = x, and bm+2(x) = (x     tm)+, m = 1, . . . , m     2,
where tm is the mth knot, and z+ denotes positive part. tensor products
of spline bases can be used for inputs with dimensions larger than one
(see section 5.2, and the cart and mars models in chapter 9.) the
parameter m controls the degree of the polynomial or the number of knots
in the case of splines.

radial basis functions are symmetric p-dimensional kernels located at

particular centroids,

f  (x) =

mxm=1

k  m (  m, x)  m;

(2.44)

for example, the gaussian kernel k  (  , x) = e   ||x     ||2/2   is popular.

radial basis functions have centroids   m and scales   m that have to
be determined. the spline basis functions have knots. in general we would
like the data to dictate them as well. including these as parameters changes
the regression problem from a straightforward linear problem to a combi-
natorially hard nonlinear problem. in practice, shortcuts such as greedy
algorithms or two stage processes are used. section 6.7 describes some such
approaches.

a single-layer feed-forward neural network model with linear output
weights can be thought of as an adaptive basis function method. the model
has the form

f  (x) =

mxm=1

  m  (  t

mx + bm),

(2.45)

where   (x) = 1/(1 + e   x) is known as the activation function. here, as
in the projection pursuit model, the directions   m and the bias terms bm
have to be determined, and their estimation is the meat of the computation.
details are given in chapter 11.

these adaptively chosen basis function methods are also known as dictio-
nary methods, where one has available a possibly in   nite set or dictionary
d of candidate basis functions from which to choose, and models are built
up by employing some kind of search mechanism.

2.9 model selection and the bias   variance tradeo   

37

2.9 model selection and the bias   variance

tradeo   

all the models described above and many others discussed in later chapters
have a smoothing or complexity parameter that has to be determined:

    the multiplier of the penalty term;
    the width of the kernel;
    or the number of basis functions.

in the case of the smoothing spline, the parameter    indexes models ranging
from a straight line    t to the interpolating model. similarly a local degree-
m polynomial model ranges between a degree-m global polynomial when
the window size is in   nitely large, to an interpolating    t when the window
size shrinks to zero. this means that we cannot use residual sum-of-squares
on the training data to determine these parameters as well, since we would
always pick those that gave interpolating    ts and hence zero residuals. such
a model is unlikely to predict future data well at all.

the k-nearest-neighbor regression    t   fk(x0) usefully illustrates the com-
peting forces that a   ect the predictive ability of such approximations. sup-
pose the data arise from a model y = f (x) +   , with e(  ) = 0 and
var(  ) =   2. for simplicity here we assume that the values of xi in the
sample are    xed in advance (nonrandom). the expected prediction error
at x0, also known as test or generalization error, can be decomposed:

epek(x0) = e[(y       fk(x0))2|x = x0]

=   2 + [bias2(   fk(x0)) + vart (   fk(x0))]
  2
k

=   2 +hf (x0)    

f (x(   ))i2

kx   =1

1
k

+

.

(2.46)

(2.47)

the subscripts in parentheses (   ) indicate the sequence of nearest neighbors
to x0.

there are three terms in this expression. the    rst term   2 is the ir-
reducible error   the variance of the new test target   and is beyond our
control, even if we know the true f (x0).

the second and third terms are under our control, and make up the
mean squared error of   fk(x0) in estimating f (x0), which is broken down
into a bias component and a variance component. the bias term is the
squared di   erence between the true mean f (x0) and the expected value of
the estimate   [et (   fk(x0))     f (x0)]2   where the expectation averages the
randomness in the training data. this term will most likely increase with
k, if the true function is reasonably smooth. for small k the few closest
neighbors will have values f (x(   )) close to f (x0), so their average should

38

2. overview of supervised learning

high bias
low variance

low bias
high variance

r
o
r
r
e
n
o
i
t
c
i
d
e
r
p

test sample

training sample

low

high

model complexity

figure 2.11. test and training error as a function of model complexity.

be close to f (x0). as k grows, the neighbors are further away, and then
anything can happen.

the variance term is simply the variance of an average here, and de-
creases as the inverse of k. so as k varies, there is a bias   variance tradeo   .
more generally, as the model complexity of our procedure is increased, the
variance tends to increase and the squared bias tends to decrease. the op-
posite behavior occurs as the model complexity is decreased. for k-nearest
neighbors, the model complexity is controlled by k.

typically we would like to choose our model complexity to trade bias
o    with variance in such a way as to minimize the test error. an obvious
estimate of test error is the training error 1
training error is not a good estimate of test error, as it does not properly
account for model complexity.

npi(yi       yi)2. unfortunately

figure 2.11 shows the typical behavior of the test and training error, as
model complexity is varied. the training error tends to decrease whenever
we increase the model complexity, that is, whenever we    t the data harder.
however with too much    tting, the model adapts itself too closely to the
training data, and will not generalize well (i.e., have large test error). in
that case the predictions   f (x0) will have large variance, as re   ected in the
last term of expression (2.46). in contrast, if the model is not complex
enough, it will under   t and may have large bias, again resulting in poor
generalization. in chapter 7 we discuss methods for estimating the test
error of a prediction method, and hence estimating the optimal amount of
model complexity for a given prediction method and training set.

exercises

39

bibliographic notes

some good general books on the learning problem are duda et al. (2000),
bishop (1995),(bishop, 2006), ripley (1996), cherkassky and mulier (2007)
and vapnik (1996). parts of this chapter are based on friedman (1994b).

exercises

ex. 2.1 suppose each of k-classes has an associated target tk, which is a
vector of all zeros, except a one in the kth position. show that classifying to
the largest element of   y amounts to choosing the closest target, mink ||tk    
  y||, if the elements of   y sum to one.
ex. 2.2 show how to compute the bayes decision boundary for the simula-
tion example in figure 2.5.

ex. 2.3 derive equation (2.24).

ex. 2.4 the edge e   ect problem discussed on page 23 is not peculiar to
uniform sampling from bounded domains. consider inputs drawn from a
spherical multinormal distribution x     n (0, ip). the squared distance
from any sample point to the origin has a   2
p distribution with mean p.
consider a prediction point x0 drawn from this distribution, and let a =
x0/||x0|| be an associated unit vector. let zi = at xi be the projection of
each of the training points on this direction.
show that the zi are distributed n (0, 1) with expected squared distance
from the origin 1, while the target point has expected squared distance p
from the origin.

hence for p = 10, a randomly drawn test point is about 3.1 standard
deviations from the origin, while all the training points are on average
one standard deviation along direction a. so most prediction points see
themselves as lying on the edge of the training set.

ex. 2.5

(a) derive equation (2.27). the last line makes use of (3.8) through a

conditioning argument.

(b) derive equation (2.28), making use of the cyclic property of the trace
operator [trace(ab) = trace(ba)], and its linearity (which allows us
to interchange the order of trace and expectation).

ex. 2.6 consider a regression problem with inputs xi and outputs yi, and a
parameterized model f  (x) to be    t by least squares. show that if there are
observations with tied or identical values of x, then the    t can be obtained
from a reduced weighted least squares problem.

40

2. overview of supervised learning

ex. 2.7 suppose we have a sample of n pairs xi, yi drawn i.i.d. from the
distribution characterized as follows:

xi     h(x), the design density
yi = f (xi) +   i, f is the regression function
  i     (0,   2) (mean zero, variance   2)

we construct an estimator for f linear in the yi,

  f (x0) =

nxi=1

   i(x0;x )yi,

where the weights    i(x0;x ) do not depend on the yi, but do depend on the
entire training sequence of xi, denoted here by x .
(a) show that id75 and k-nearest-neighbor regression are mem-
bers of this class of estimators. describe explicitly the weights    i(x0;x )
in each of these cases.

(b) decompose the conditional mean-squared error
ey|x (f (x0)       f (x0))2

into a conditional squared bias and a conditional variance component.
like x , y represents the entire training sequence of yi.

(c) decompose the (unconditional) mean-squared error

ey,x (f (x0)       f (x0))2
into a squared bias and a variance component.

(d) establish a relationship between the squared biases and variances in

the above two cases.

ex. 2.8 compare the classi   cation performance of id75 and k   
nearest neighbor classi   cation on the zipcode data. in particular, consider
only the 2   s and 3   s, and k = 1, 3, 5, 7 and 15. show both the training and
test error for each choice. the zipcode data are available from the book
website www-stat.stanford.edu/elemstatlearn.

ex. 2.9 consider a id75 model with p parameters,    t by least
squares to a set of training data (x1, y1), . . . , (xn , yn ) drawn at random
from a population. let      be the least squares estimate. suppose we have
some test data (  x1,   y1), . . . , (  xm ,   ym ) drawn at random from the same pop-
ulation as the training data. if rtr(  ) = 1
1 (yi       t xi)2 and rte(  ) =
1

1 (  yi       t   xi)2, prove that

npn
e[rtr(     )]     e[rte(     )],

mpm

where the expectations are over all that is random in each expression. [this
exercise was brought to our attention by ryan tibshirani, from a homework
assignment given by andrew ng.]

exercises

41

42

2. overview of supervised learning

3
linear methods for regression

this is page 43
printer: opaque this

3.1

introduction

a id75 model assumes that the regression function e(y |x) is
linear in the inputs x1, . . . , xp. linear models were largely developed in
the precomputer age of statistics, but even in today   s computer era there
are still good reasons to study and use them. they are simple and often
provide an adequate and interpretable description of how the inputs a   ect
the output. for prediction purposes they can sometimes outperform fancier
nonlinear models, especially in situations with small numbers of training
cases, low signal-to-noise ratio or sparse data. finally, linear methods can be
applied to transformations of the inputs and this considerably expands their
scope. these generalizations are sometimes called basis-function methods,
and are discussed in chapter 5.

in this chapter we describe linear methods for regression, while in the
next chapter we discuss linear methods for classi   cation. on some topics we
go into considerable detail, as it is our    rm belief that an understanding
of linear methods is essential for understanding nonlinear ones. in fact,
many nonlinear techniques are direct generalizations of the linear methods
discussed here.

44

3. linear methods for regression

3.2 id75 models and least squares

as introduced in chapter 2, we have an input vector x t = (x1, x2, . . . , xp),
and want to predict a real-valued output y . the id75 model
has the form

f (x) =   0 +

xj  j.

(3.1)

pxj=1

the linear model either assumes that the regression function e(y |x) is
linear, or that the linear model is a reasonable approximation. here the
  j   s are unknown parameters or coe   cients, and the variables xj can come
from di   erent sources:

    quantitative inputs;
    transformations of quantitative inputs, such as log, square-root or

square;

    basis expansions, such as x2 = x 2

representation;

1 , x3 = x 3

1 , leading to a polynomial

    numeric or    dummy    coding of the levels of qualitative inputs. for
example, if g is a    ve-level factor input, we might create xj, j =
1, . . . , 5, such that xj = i(g = j). together this group of xj repre-
sents the e   ect of g by a set of level-dependent constants, since in

p5

j=1 xj  j, one of the xjs is one, and the others are zero.
    interactions between variables, for example, x3 = x1    x2.

no matter the source of the xj, the model is linear in the parameters.

typically we have a set of training data (x1, y1) . . . (xn , yn ) from which
to estimate the parameters   . each xi = (xi1, xi2, . . . , xip)t is a vector
of feature measurements for the ith case. the most popular estimation
method is least squares, in which we pick the coe   cients    = (  0,   1, . . . ,   p)t
to minimize the residual sum of squares

rss(  ) =

=

(yi     f (xi))2

nxi=1
nxi=1(cid:16)yi       0    

pxj=1

xij  j(cid:17)2

.

(3.2)

from a statistical point of view, this criterion is reasonable if the training
observations (xi, yi) represent independent random draws from their popu-
lation. even if the xi   s were not drawn randomly, the criterion is still valid
if the yi   s are conditionally independent given the inputs xi. figure 3.1
illustrates the geometry of least-squares    tting in the irp+1-dimensional

3.2 id75 models and least squares

45

y

   
   
   
   
   
   

   

   
   

   
   

       
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   
   
   
   
   
   
   
   

       
   
   
   
   

   
   

   
   

   
   

   

   

   

   
   

   

   
   
   
   
   
   
   

   
   
   
   

   
   
   

   
   

   

x2

x1

figure 3.1. linear least squares    tting with x     ir2. we seek the linear
function of x that minimizes the sum of squared residuals from y .

space occupied by the pairs (x, y ). note that (3.2) makes no assumptions
about the validity of model (3.1); it simply    nds the best linear    t to the
data. least squares    tting is intuitively satisfying no matter how the data
arise; the criterion measures the average lack of    t.

how do we minimize (3.2)? denote by x the n    (p + 1) matrix with
each row an input vector (with a 1 in the    rst position), and similarly let
y be the n -vector of outputs in the training set. then we can write the
residual sum-of-squares as

rss(  ) = (y     x  )t (y     x  ).

(3.3)

this is a quadratic function in the p + 1 parameters. di   erentiating with
respect to    we obtain

   rss

     

=    2xt (y     x  )

   2rss
          t = 2xt x.

(3.4)

assuming (for the moment) that x has full column rank, and hence xt x
is positive de   nite, we set the    rst derivative to zero

to obtain the unique solution

xt (y     x  ) = 0

     = (xt x)   1xt y.

(3.5)

(3.6)

46

3. linear methods for regression

y

  y

x2

x1

figure 3.2. the n -dimensional geometry of least squares regression with two
predictors. the outcome vector y is orthogonally projected onto the hyperplane
spanned by the input vectors x1 and x2. the projection   y represents the vector
of the least squares predictions

the predicted values at an input vector x0 are given by   f (x0) = (1 : x0)t     ;
the    tted values at the training inputs are

  y = x      = x(xt x)   1xt y,

(3.7)

where   yi =   f (xi). the matrix h = x(xt x)   1xt appearing in equation
(3.7) is sometimes called the    hat    matrix because it puts the hat on y.

figure 3.2 shows a di   erent geometrical representation of the least squares

estimate, this time in irn . we denote the column vectors of x by x0, x1, . . . , xp,
with x0     1. for much of what follows, this    rst column is treated like any
other. these vectors span a subspace of irn , also referred to as the column
space of x. we minimize rss(  ) = ky     x  k2 by choosing      so that the
residual vector y       y is orthogonal to this subspace. this orthogonality is
expressed in (3.5), and the resulting estimate   y is hence the orthogonal pro-
jection of y onto this subspace. the hat matrix h computes the orthogonal
projection, and hence it is also known as a projection matrix.

it might happen that the columns of x are not linearly independent, so
that x is not of full rank. this would occur, for example, if two of the
inputs were perfectly correlated, (e.g., x2 = 3x1). then xt x is singular
and the least squares coe   cients      are not uniquely de   ned. however,
the    tted values   y = x      are still the projection of y onto the column
space of x; there is just more than one way to express that projection
in terms of the column vectors of x. the non-full-rank case occurs most
often when one or more qualitative inputs are coded in a redundant fashion.
there is usually a natural way to resolve the non-unique representation,
by recoding and/or dropping redundant columns in x. most regression
software packages detect these redundancies and automatically implement

3.2 id75 models and least squares

47

some strategy for removing them. rank de   ciencies can also occur in signal
and image analysis, where the number of inputs p can exceed the number
of training cases n . in this case, the features are typically reduced by
   ltering or else the    tting is controlled by id173 (section 5.2.3 and
chapter 18).

up to now we have made minimal assumptions about the true distribu-
tion of the data. in order to pin down the sampling properties of     , we now
assume that the observations yi are uncorrelated and have constant vari-
ance   2, and that the xi are    xed (non random). the variance   covariance
matrix of the least squares parameter estimates is easily derived from (3.6)
and is given by

var(     ) = (xt x)   1  2.

(3.8)

typically one estimates the variance   2 by

    2 =

1

n     p     1

nxi=1

(yi       yi)2.

the n     p     1 rather than n in the denominator makes     2 an unbiased
estimate of   2: e(    2) =   2.
to draw id136s about the parameters and the model, additional as-
sumptions are needed. we now assume that (3.1) is the correct model for
the mean; that is, the conditional expectation of y is linear in x1, . . . , xp.
we also assume that the deviations of y around its expectation are additive
and gaussian. hence

y = e(y |x1, . . . , xp) +   

=   0 +

xj  j +   ,

(3.9)

pxj=1

where the error    is a gaussian random variable with expectation zero and
variance   2, written        n (0,   2).

under (3.9), it is easy to show that

         n (  , (xt x)   1  2).

(3.10)

this is a multivariate normal distribution with mean vector and variance   
covariance matrix as shown. also

(n     p     1)    2       2  2

n    p   1,

(3.11)

a chi-squared distribution with n     p    1 degrees of freedom. in addition     
and     2 are statistically independent. we use these distributional properties
to form tests of hypothesis and con   dence intervals for the parameters   j.

48

3. linear methods for regression

s
e
i
t
i
l
i

b
a
b
o
r
p

 
l
i

a
t

6
0
.
0

5
0
.
0

4
0
.
0

3
0
.
0

2
0
.
0

1
0
.
0

t30
t100
normal

2.0

2.2

2.4

z

2.6

2.8

3.0

figure 3.3. the tail probabilities pr(|z| > z) for three distributions, t30, t100
and standard normal. shown are the appropriate quantiles for testing signi   cance
at the p = 0.05 and 0.01 levels. the di   erence between t and the standard normal
becomes negligible for n bigger than about 100.

to test the hypothesis that a particular coe   cient   j = 0, we form the

standardized coe   cient or z-score

zj =

    j
       vj

,

(3.12)

where vj is the jth diagonal element of (xt x)   1. under the null hypothesis
that   j = 0, zj is distributed as tn    p   1 (a t distribution with n     p     1
degrees of freedom), and hence a large (absolute) value of zj will lead to
rejection of this null hypothesis. if      is replaced by a known value   , then
zj would have a standard normal distribution. the di   erence between the
tail quantiles of a t-distribution and a standard normal become negligible
as the sample size increases, and so we typically use the normal quantiles
(see figure 3.3).

often we need to test for the signi   cance of groups of coe   cients simul-
taneously. for example, to test if a categorical variable with k levels can
be excluded from a model, we need to test whether the coe   cients of the
dummy variables used to represent the levels can all be set to zero. here
we use the f statistic,

f =

(rss0     rss1)/(p1     p0)

rss1/(n     p1     1)

,

(3.13)

where rss1 is the residual sum-of-squares for the least squares    t of the big-
ger model with p1 +1 parameters, and rss0 the same for the nested smaller
model with p0 + 1 parameters, having p1     p0 parameters constrained to be

3.2 id75 models and least squares

49

zero. the f statistic measures the change in residual sum-of-squares per
additional parameter in the bigger model, and it is normalized by an esti-
mate of   2. under the gaussian assumptions, and the null hypothesis that
the smaller model is correct, the f statistic will have a fp1   p0,n    p1   1 dis-
tribution. it can be shown (exercise 3.1) that the zj in (3.12) are equivalent
to the f statistic for dropping the single coe   cient   j from the model. for
large n , the quantiles of fp1   p0,n    p1   1 approach those of   2
p1   p0/(p1   p0).
similarly, we can isolate   j in (3.10) to obtain a 1   2   con   dence interval

for   j:

(     j     z(1     )v

1
2

j     ,     j + z(1     )v

1
2

j     ).

(3.14)

here z(1     ) is the 1        percentile of the normal distribution:

z(1   0.025) = 1.96,
z(1   .05)

= 1.645, etc.

hence the standard practice of reporting         2    se(     ) amounts to an ap-
proximate 95% con   dence interval. even if the gaussian error assumption
does not hold, this interval will be approximately correct, with its coverage
approaching 1     2   as the sample size n        .
entire parameter vector   , namely

in a similar fashion we can obtain an approximate con   dence set for the

(1     )

c   = {  |(            )t xt x(            )         2  2
(1     ) is the 1        percentile of the chi-squared distribution on    
where   2
   
(1   0.1) = 9.2. this
degrees of freedom: for example,   2
5
con   dence set for    generates a corresponding con   dence set for the true
function f (x) = xt   , namely {xt   |       c  } (exercise 3.2; see also fig-
ure 5.4 in section 5.2.2 for examples of con   dence bands for functions).

(1   0.05) = 11.1,   2
5

(3.15)

},

p+1

3.2.1 example: prostate cancer

the data for this example come from a study by stamey et al. (1989). they
examined the correlation between the level of prostate-speci   c antigen and
a number of clinical measures in men who were about to receive a radical
prostatectomy. the variables are log cancer volume (lcavol), log prostate
weight (lweight), age, log of the amount of benign prostatic hyperplasia
(lbph), seminal vesicle invasion (svi), log of capsular penetration (lcp),
gleason score (gleason), and percent of gleason scores 4 or 5 (pgg45).
the correlation matrix of the predictors given in table 3.1 shows many
strong correlations. figure 1.1 (page 3) of chapter 1 is a scatterplot matrix
showing every pairwise plot between the variables. we see that svi is a
binary variable, and gleason is an ordered categorical variable. we see, for

50

3. linear methods for regression

table 3.1. correlations of predictors in the prostate cancer data.

lweight
age
lbph
svi
lcp
gleason
pgg45

lcavol
0.300
0.286
0.063
0.593
0.692
0.426
0.483

lweight

age

lbph

svi

lcp

gleason

0.317
0.437
0.181
0.157
0.024
0.074

0.287
0.129    0.139
0.173    0.089
0.033
0.366
0.276    0.030

0.671
0.307
0.481

0.476
0.663

0.757

table 3.2. linear model    t to the prostate cancer data. the z score is the
coe   cient divided by its standard error (3.12). roughly a z score larger than two
in absolute value is signi   cantly nonzero at the p = 0.05 level.

term coe   cient
2.46
0.68
0.26
   0.14
0.21
0.31
   0.29
   0.02
0.27

intercept
lcavol
lweight
age
lbph
svi
lcp
gleason
pgg45

std. error z score
27.60
5.37
2.75
   1.40
2.06
2.47
   1.87
   0.15
1.74

0.09
0.13
0.10
0.10
0.10
0.12
0.15
0.15
0.15

example, that both lcavol and lcp show a strong relationship with the
response lpsa, and with each other. we need to    t the e   ects jointly to
untangle the relationships between the predictors and the response.

we    t a linear model to the log of prostate-speci   c antigen, lpsa, after
   rst standardizing the predictors to have unit variance. we randomly split
the dataset into a training set of size 67 and a test set of size 30. we ap-
plied least squares estimation to the training set, producing the estimates,
standard errors and z-scores shown in table 3.2. the z-scores are de   ned
in (3.12), and measure the e   ect of dropping that variable from the model.
a z-score greater than 2 in absolute value is approximately signi   cant at
the 5% level. (for our example, we have nine parameters, and the 0.025 tail
quantiles of the t67   9 distribution are   2.002!) the predictor lcavol shows
the strongest e   ect, with lweight and svi also strong. notice that lcp is
not signi   cant, once lcavol is in the model (when used in a model without
lcavol, lcp is strongly signi   cant). we can also test for the exclusion of
a number of terms at once, using the f -statistic (3.13). for example, we
consider dropping all the non-signi   cant terms in table 3.2, namely age,

3.2 id75 models and least squares

51

lcp, gleason, and pgg45. we get

f =

(32.81     29.43)/(9     5)

29.43/(67     9)

= 1.67,

(3.16)

which has a p-value of 0.17 (pr(f4,58 > 1.67) = 0.17), and hence is not
signi   cant.

the mean prediction error on the test data is 0.521. in contrast, predic-
tion using the mean training value of lpsa has a test error of 1.057, which
is called the    base error rate.    hence the linear model reduces the base
error rate by about 50%. we will return to this example later to compare
various selection and shrinkage methods.

3.2.2 the gauss   markov theorem

one of the most famous results in statistics asserts that the least squares
estimates of the parameters    have the smallest variance among all linear
unbiased estimates. we will make this precise here, and also make clear
that the restriction to unbiased estimates is not necessarily a wise one. this
observation will lead us to consider biased estimates such as ridge regression
later in the chapter. we focus on estimation of any linear combination of
the parameters    = at   ; for example, predictions f (x0) = xt
0    are of this
form. the least squares estimate of at    is

     = at      = at (xt x)   1xt y.

(3.17)

considering x to be    xed, this is a linear function ct
0 y of the response
vector y. if we assume that the linear model is correct, at      is unbiased
since

e(at     ) = e(at (xt x)   1xt y)

= at (xt x)   1xt x  
= at   .

(3.18)

the gauss   markov theorem states that if we have any other linear estima-
tor      = ct y that is unbiased for at   , that is, e(ct y) = at   , then

var(at     )     var(ct y).

(3.19)

the proof (exercise 3.3) uses the triangle inequality. for simplicity we have
stated the result in terms of estimation of a single parameter at   , but with
a few more de   nitions one can state it in terms of the entire parameter
vector    (exercise 3.3).

consider the mean squared error of an estimator      in estimating   :

mse(    ) = e(           )2

= var(    ) + [e(    )       ]2.

(3.20)

52

3. linear methods for regression

the    rst term is the variance, while the second term is the squared bias.
the gauss-markov theorem implies that the least squares estimator has the
smallest mean squared error of all linear estimators with no bias. however,
there may well exist a biased estimator with smaller mean squared error.
such an estimator would trade a little bias for a larger reduction in variance.
biased estimates are commonly used. any method that shrinks or sets to
zero some of the least squares coe   cients may result in a biased estimate.
we discuss many examples, including variable subset selection and ridge
regression, later in this chapter. from a more pragmatic point of view, most
models are distortions of the truth, and hence are biased; picking the right
model amounts to creating the right balance between bias and variance.
we go into these issues in more detail in chapter 7.

mean squared error is intimately related to prediction accuracy, as dis-
cussed in chapter 2. consider the prediction of the new response at input
x0,

then the expected prediction error of an estimate   f (x0) = xt
0

     is

y0 = f (x0) +   0.

e(y0       f (x0))2 =   2 + e(xt

0

         f (x0))2

=   2 + mse(   f (x0)).

(3.21)

(3.22)

therefore, expected prediction error and mean squared error di   er only by
the constant   2, representing the variance of the new observation y0.

3.2.3 multiple regression from simple univariate regression

the linear model (3.1) with p > 1 inputs is called the multiple linear
regression model. the least squares estimates (3.6) for this model are best
understood in terms of the estimates for the univariate (p = 1) linear
model, as we indicate in this section.

suppose    rst that we have a univariate model with no intercept, that is,

the least squares estimate and residuals are

y = x   +   .

     = pn
1 xiyi
pn
1 x2
i
ri = yi     xi     .

,

(3.23)

(3.24)

in convenient vector notation, we let y = (y1, . . . , yn )t , x = (x1, . . . , xn )t
and de   ne

hx, yi =

xiyi,

nxi=1

= xt y,

(3.25)

3.2 id75 models and least squares

53

the inner product between x and y1. then we can write

,

     = hx, yi
hx, xi
r = y     x     .

(3.26)

as we will see, this simple univariate regression provides the building block
for multiple id75. suppose next that the inputs x1, x2, . . . , xp
(the columns of the data matrix x) are orthogonal; that is hxj, xki = 0
for all j 6= k. then it is easy to check that the multiple least squares esti-
mates     j are equal to hxj, yi/hxj, xji   the univariate estimates. in other
words, when the inputs are orthogonal, they have no e   ect on each other   s
parameter estimates in the model.

orthogonal inputs occur most often with balanced, designed experiments
(where orthogonality is enforced), but almost never with observational
data. hence we will have to orthogonalize them in order to carry this idea
further. suppose next that we have an intercept and a single input x. then
the least squares coe   cient of x has the form

    1 =

hx       x1, yi

hx       x1, x       x1i

,

(3.27)

where   x =pi xi/n , and 1 = x0, the vector of n ones. we can view the

estimate (3.27) as the result of two applications of the simple regression
(3.26). the steps are:

1. regress x on 1 to produce the residual z = x       x1;
2. regress y on the residual z to give the coe   cient     1.

in this procedure,    regress b on a    means a simple univariate regression of b
on a with no intercept, producing coe   cient      = ha, bi/ha, ai and residual
vector b        a. we say that b is adjusted for a, or is    orthogonalized    with
respect to a.
step 1 orthogonalizes x with respect to x0 = 1. step 2 is just a simple
univariate regression, using the orthogonal predictors 1 and z. figure 3.4
shows this process for two general inputs x1 and x2. the orthogonalization
does not change the subspace spanned by x1 and x2, it simply produces an
orthogonal basis for representing it.

this recipe generalizes to the case of p inputs, as shown in algorithm 3.1.
note that the inputs z0, . . . , zj   1 in step 2 are orthogonal, hence the simple
regression coe   cients computed there are in fact also the multiple regres-
sion coe   cients.

1the inner-product notation is suggestive of generalizations of id75 to

di   erent metric spaces, as well as to id203 spaces.

54

3. linear methods for regression

y

  y

x2

zzzzz

x1

figure 3.4. least squares regression by orthogonalization of the inputs. the
vector x2 is regressed on the vector x1, leaving the residual vector z. the regres-
sion of y on z gives the multiple regression coe   cient of x2. adding together the
projections of y on each of x1 and z gives the least squares    t   y.

algorithm 3.1 regression by successive orthogonalization.

1. initialize z0 = x0 = 1.

2. for j = 1, 2, . . . , p

regress xj on z0, z1, . . . , , zj   1 to produce coe   cients        j =
hz   , xji/hz   , z   i,     = 0, . . . , j     1 and residual vector zj =

xj    pj   1

k=0     kj zk.

3. regress y on the residual zp to give the estimate     p.

the result of this algorithm is

    p = hzp, yi
hzp, zpi

.

(3.28)

re-arranging the residual in step 2, we can see that each of the xj is a linear
combination of the zk, k     j. since the zj are all orthogonal, they form
a basis for the column space of x, and hence the least squares projection
onto this subspace is   y. since zp alone involves xp (with coe   cient 1), we
see that the coe   cient (3.28) is indeed the multiple regression coe   cient of
y on xp. this key result exposes the e   ect of correlated inputs in multiple
regression. note also that by rearranging the xj, any one of them could
be in the last position, and a similar results holds. hence stated more
generally, we have shown that the jth multiple regression coe   cient is the
univariate regression coe   cient of y on xj  012...(j   1)(j+1)...,p, the residual
after regressing xj on x0, x1, . . . , xj   1, xj+1, . . . , xp:

3.2 id75 models and least squares

55

the multiple regression coe   cient     j represents the additional
contribution of xj on y, after xj has been adjusted for x0, x1, . . . , xj   1,
xj+1, . . . , xp.

if xp is highly correlated with some of the other xk   s, the residual vector
zp will be close to zero, and from (3.28) the coe   cient     p will be very
unstable. this will be true for all the variables in the correlated set. in
such situations, we might have all the z-scores (as in table 3.2) be small   
any one of the set can be deleted   yet we cannot delete them all. from
(3.28) we also obtain an alternate formula for the variance estimates (3.8),

var(     p) =

  2

hzp, zpi

=

  2

kzpk2 .

(3.29)

in other words, the precision with which we can estimate     p depends on
the length of the residual vector zp; this represents how much of xp is
unexplained by the other xk   s.

algorithm 3.1 is known as the gram   schmidt procedure for multiple
regression, and is also a useful numerical strategy for computing the esti-
mates. we can obtain from it not just     p, but also the entire multiple least
squares    t, as shown in exercise 3.4.

we can represent step 2 of algorithm 3.1 in matrix form:

x = z  ,

(3.30)

where z has as columns the zj (in order), and    is the upper triangular ma-
trix with entries     kj. introducing the diagonal matrix d with jth diagonal
entry djj = kzjk, we get

x = zd   1d  

= qr,

(3.31)

the so-called qr decomposition of x. here q is an n    (p + 1) orthogonal
matrix, qt q = i, and r is a (p + 1)    (p + 1) upper triangular matrix.
the qr decomposition represents a convenient orthogonal basis for the
column space of x. it is easy to see, for example, that the least squares
solution is given by

     = r   1qt y,
  y = qqt y.

(3.32)

(3.33)

equation (3.32) is easy to solve because r is upper triangular
(exercise 3.4).

56

3. linear methods for regression

3.2.4 multiple outputs

suppose we have multiple outputs y1, y2, . . . , yk that we wish to predict
from our inputs x0, x1, x2, . . . , xp. we assume a linear model for each
output

yk =   0k +

xj  jk +   k

pxj=1

= fk(x) +   k.

with n training cases we can write the model in matrix notation

y = xb + e.

(3.34)

(3.35)

(3.36)

here y is the n   k response matrix, with ik entry yik, x is the n   (p+1)
input matrix, b is the (p + 1)    k matrix of parameters and e is the
n    k matrix of errors. a straightforward generalization of the univariate
id168 (3.2) is

rss(b) =

nxi=1
(yik     fk(xi))2

kxk=1
= tr[(y     xb)t (y     xb)].

the least squares estimates have exactly the same form as before

  b = (xt x)   1xt y.

(3.37)

(3.38)

(3.39)

hence the coe   cients for the kth outcome are just the least squares es-
timates in the regression of yk on x0, x1, . . . , xp. multiple outputs do not
a   ect one another   s least squares estimates.

if the errors    = (  1, . . . ,   k ) in (3.34) are correlated, then it might seem
appropriate to modify (3.37) in favor of a multivariate version. speci   cally,
suppose cov(  ) =   , then the multivariate weighted criterion

rss(b;   ) =

nxi=1

(yi     f (xi))t      1(yi     f (xi))

(3.40)

arises naturally from multivariate gaussian theory. here f (x) is the vector
function (f1(x), . . . , fk (x))t , and yi the vector of k responses for obser-
vation i. however, it can be shown that again the solution is given by
(3.39); k separate regressions that ignore the correlations (exercise 3.11).
if the   i vary among observations, then this is no longer the case, and the
solution for b no longer decouples.

in section 3.7 we pursue the multiple outcome problem, and consider

situations where it does pay to combine the regressions.

3.3 subset selection

57

3.3 subset selection

there are two reasons why we are often not satis   ed with the least squares
estimates (3.6).

    the    rst is prediction accuracy: the least squares estimates often have
low bias but large variance. prediction accuracy can sometimes be
improved by shrinking or setting some coe   cients to zero. by doing
so we sacri   ce a little bit of bias to reduce the variance of the predicted
values, and hence may improve the overall prediction accuracy.

    the second reason is interpretation. with a large number of predic-
tors, we often would like to determine a smaller subset that exhibit
the strongest e   ects. in order to get the    big picture,    we are willing
to sacri   ce some of the small details.

in this section we describe a number of approaches to variable subset selec-
tion with id75. in later sections we discuss shrinkage and hybrid
approaches for controlling variance, as well as other dimension-reduction
strategies. these all fall under the general heading model selection. model
selection is not restricted to linear models; chapter 7 covers this topic in
some detail.

with subset selection we retain only a subset of the variables, and elim-
inate the rest from the model. least squares regression is used to estimate
the coe   cients of the inputs that are retained. there are a number of dif-
ferent strategies for choosing the subset.

3.3.1 best-subset selection
best subset regression    nds for each k     {0, 1, 2, . . . , p} the subset of size k
that gives smallest residual sum of squares (3.2). an e   cient algorithm   
the leaps and bounds procedure (furnival and wilson, 1974)   makes this
feasible for p as large as 30 or 40. figure 3.5 shows all the subset models
for the prostate cancer example. the lower boundary represents the models
that are eligible for selection by the best-subsets approach. note that the
best subset of size 2, for example, need not include the variable that was
in the best subset of size 1 (for this example all the subsets are nested).
the best-subset curve (red lower boundary in figure 3.5) is necessarily
decreasing, so cannot be used to select the subset size k. the question of
how to choose k involves the tradeo    between bias and variance, along with
the more subjective desire for parsimony. there are a number of criteria
that one may use; typically we choose the smallest model that minimizes
an estimate of the expected prediction error.

many of the other approaches that we discuss in this chapter are similar,
in that they use the training data to produce a sequence of models varying
in complexity and indexed by a single parameter. in the next section we use

58

3. linear methods for regression

      
   
         
   

   
   

      
                           
               
            
                  
   

                  
                           
                                                                                                      

   

   
   

s
e
r
a
u
q
s
   
o
   
m
u
s

f

 
l

i

a
u
d
s
e
r

0
0
1

0
8

0
6

0
4

0
2

0

      
                     

                                                                                                                                                                           

   

      

                                                                                                                                 

   

      
               
                                                         
   

   
                     
   

   
   

0

1

2

3

4

5

6

7

8

subset size k

figure 3.5. all possible subset models for the prostate cancer example. at
each subset size is shown the residual sum-of-squares for each model of that size.

cross-validation to estimate prediction error and select k; the aic criterion
is a popular alternative. we defer more detailed discussion of these and
other approaches to chapter 7.

3.3.2 forward- and backward-stepwise selection

rather than search through all possible subsets (which becomes infeasible
for p much larger than 40), we can seek a good path through them. forward-
stepwise selection starts with the intercept, and then sequentially adds into
the model the predictor that most improves the    t. with many candidate
predictors, this might seem like a lot of computation; however, clever up-
dating algorithms can exploit the qr decomposition for the current    t to
rapidly establish the next candidate (exercise 3.9). like best-subset re-
gression, forward stepwise produces a sequence of models indexed by k, the
subset size, which must be determined.

forward-stepwise selection is a greedy algorithm, producing a nested se-
quence of models. in this sense it might seem sub-optimal compared to
best-subset selection. however, there are several reasons why it might be
preferred:

3.3 subset selection

59

    computational; for large p we cannot compute the best subset se-
quence, but we can always compute the forward stepwise sequence
(even when p     n ).

    statistical; a price is paid in variance for selecting the best subset
of each size; forward stepwise is a more constrained search, and will
have lower variance, but perhaps more bias.

best subset
forward stepwise
backward stepwise
forward stagewise

2
|
|

  
   

)
k
(
    

|
|

e

5
9

.

0

0
9

.

0

5
8

.

0

0
8

.

0

5
7

.

0

0
7

.

0

5
6

.

0

0

5

10

15

20

25

30

subset size k

figure 3.6. comparison of four subset-selection techniques on a simulated lin-
ear regression problem y = x t    +   . there are n = 300 observations on p = 31
standard gaussian variables, with pairwise correlations all equal to 0.85. for 10 of
the variables, the coe   cients are drawn at random from a n (0, 0.4) distribution;
the rest are zero. the noise        n (0, 6.25), resulting in a signal-to-noise ratio of
0.64. results are averaged over 50 simulations. shown is the mean-squared error
of the estimated coe   cient     (k) at each step from the true   .

backward-stepwise selection starts with the full model, and sequentially
deletes the predictor that has the least impact on the    t. the candidate for
dropping is the variable with the smallest z-score (exercise 3.10). backward
selection can only be used when n > p, while forward stepwise can always
be used.

figure 3.6 shows the results of a small simulation study to compare
best-subset regression with the simpler alternatives forward and backward
selection. their performance is very similar, as is often the case. included in
the    gure is forward stagewise regression (next section), which takes longer
to reach minimum error.

60

3. linear methods for regression

on the prostate cancer example, best-subset, forward and backward se-

lection all gave exactly the same sequence of terms.

some software packages implement hybrid stepwise-selection strategies
that consider both forward and backward moves at each step, and select
the    best    of the two. for example in the r package the step function uses
the aic criterion for weighing the choices, which takes proper account of
the number of parameters    t; at each step an add or drop will be performed
that minimizes the aic score.

other more traditional packages base the selection on f -statistics, adding
   signi   cant    terms, and dropping    non-signi   cant    terms. these are out
of fashion, since they do not take proper account of the multiple testing
issues. it is also tempting after a model search to print out a summary of
the chosen model, such as in table 3.2; however, the standard errors are
not valid, since they do not account for the search process. the bootstrap
(section 8.2) can be useful in such settings.

finally, we note that often variables come in groups (such as the dummy
variables that code a multi-level categorical predictor). smart stepwise pro-
cedures (such as step in r) will add or drop whole groups at a time, taking
proper account of their degrees-of-freedom.

3.3.3 forward-stagewise regression

forward-stagewise regression (fs) is even more constrained than forward-
stepwise regression. it starts like forward-stepwise regression, with an in-
tercept equal to   y, and centered predictors with coe   cients initially all 0.
at each step the algorithm identi   es the variable most correlated with the
current residual. it then computes the simple id75 coe   cient
of the residual on this chosen variable, and then adds it to the current co-
e   cient for that variable. this is continued till none of the variables have
correlation with the residuals   i.e. the least-squares    t when n > p.

unlike forward-stepwise regression, none of the other variables are ad-
justed when a term is added to the model. as a consequence, forward
stagewise can take many more than p steps to reach the least squares    t,
and historically has been dismissed as being ine   cient. it turns out that
this    slow    tting    can pay dividends in high-dimensional problems. we
see in section 3.8.1 that both forward stagewise and a variant which is
slowed down even further are quite competitive, especially in very high-
dimensional problems.

forward-stagewise regression is included in figure 3.6. in this example it
takes over 1000 steps to get all the correlations below 10   4. for subset size
k, we plotted the error for the last step for which there where k nonzero
coe   cients. although it catches up with the best    t, it takes longer to
do so.

3.4 shrinkage methods

61

3.3.4 prostate cancer data example (continued)

table 3.3 shows the coe   cients from a number of di   erent selection and
shrinkage methods. they are best-subset selection using an all-subsets search,
ridge regression, the lasso, principal components regression and partial least
squares. each method has a complexity parameter, and this was chosen to
minimize an estimate of prediction error based on tenfold cross-validation;
full details are given in section 7.10. brie   y, cross-validation works by divid-
ing the training data randomly into ten equal parts. the learning method
is    t   for a range of values of the complexity parameter   to nine-tenths of
the data, and the prediction error is computed on the remaining one-tenth.
this is done in turn for each one-tenth of the data, and the ten prediction
error estimates are averaged. from this we obtain an estimated prediction
error curve as a function of the complexity parameter.

note that we have already divided these data into a training set of size
67 and a test set of size 30. cross-validation is applied to the training set,
since selecting the shrinkage parameter is part of the training process. the
test set is there to judge the performance of the selected model.

the estimated prediction error curves are shown in figure 3.7. many of
the curves are very    at over large ranges near their minimum. included
are estimated standard error bands for each estimated error rate, based on
the ten error estimates computed by cross-validation. we have used the
   one-standard-error    rule   we pick the most parsimonious model within
one standard error of the minimum (section 7.10, page 244). such a rule
acknowledges the fact that the tradeo    curve is estimated with error, and
hence takes a conservative approach.

best-subset selection chose to use the two predictors lcvol and lweight.
the last two lines of the table give the average prediction error (and its
estimated standard error) over the test set.

3.4 shrinkage methods

by retaining a subset of the predictors and discarding the rest, subset selec-
tion produces a model that is interpretable and has possibly lower predic-
tion error than the full model. however, because it is a discrete process   
variables are either retained or discarded   it often exhibits high variance,
and so doesn   t reduce the prediction error of the full model. shrinkage
methods are more continuous, and don   t su   er as much from high
variability.

3.4.1 ridge regression

ridge regression shrinks the regression coe   cients by imposing a penalty
on their size. the ridge coe   cients minimize a penalized residual sum of

62

3. linear methods for regression

all subsets

ridge regression

r
o
r
r

 

e
v
c

r
o
r
r

 

e
v
c

r
o
r
r

 

e
v
c

   

   

   

2

   

   

4

   

6

   

   

8

degrees of freedom

principal components regression

   

   

2

   

   

   

4

   

6

number of directions

   

   

8

8
.
1

6
.
1

4
.
1

2
.
1

0
.
1

8
.
0

6
.
0

8

.

1

6

.

1

4
1

.

2

.

1

0

.

1

8

.

0

6
0

.

8
1

.

6

.

1

4
1

.

2

.

1

0

.

1

8

.

0

6
0

.

   

0

   

   

   

r
o
r
r

 

e
v
c

   

   

   

2

   

4

   

6

   

   

8

subset size

lasso

r
o
r
r

 

e
v
c

   

   

   

   

   

   

   

   

0

   

8
.
1

6
.
1

4
.
1

2
.
1

0
.
1

8
.
0

6
.
0

8

.

1

6

.

1

4
1

.

2

.

1

0

.

1

8

.

0

6
0

.

0.0

0.2

0.4

0.6

0.8

1.0

0

shrinkage factor s

partial least squares

   

0

   

   

   

2

   

4

   

   

6

   

   

8

number of  directions

figure 3.7. estimated prediction error curves and their standard errors for
the various selection and shrinkage methods. each curve is plotted as a function
of the corresponding complexity parameter for that method. the horizontal axis
has been chosen so that the model complexity increases as we move from left to
right. the estimates of prediction error and their standard errors were obtained by
tenfold cross-validation; full details are given in section 7.10. the least complex
model within one standard error of the best is chosen, indicated by the purple
vertical broken lines.

3.4 shrinkage methods

63

table 3.3. estimated coe   cients and test error results, for di   erent subset
and shrinkage methods applied to the prostate data. the blank entries correspond
to variables omitted.

term
intercept
lcavol
lweight

2.465
0.680
0.263
age    0.141
0.210
lbph
0.305
svi
lcp    0.288
gleason    0.021
0.267
0.521
0.179

pgg45
test error
std error

ls best subset
2.477
0.740
0.316

ridge
2.452
0.420
0.238
   0.046
0.162
0.227
0.000
0.040
0.133
0.492
0.165

lasso
2.468
0.533
0.169

0.002
0.094

0.479
0.164

pcr
pls
2.497
2.452
0.543
0.419
0.289
0.344
   0.152    0.026
0.220
0.214
0.315
0.243
   0.051
0.079
0.011
0.232
0.084
   0.056
0.449
0.528
0.152
0.105

0.492
0.143

squares,

    ridge = argmin

   (cid:26) nxi=1(cid:0)yi       0    

pxj=1

xij  j(cid:1)2

+   

  2

j(cid:27).

pxj=1

(3.41)

here        0 is a complexity parameter that controls the amount of shrink-
age: the larger the value of   , the greater the amount of shrinkage. the
coe   cients are shrunk toward zero (and each other). the idea of penaliz-
ing by the sum-of-squares of the parameters is also used in neural networks,
where it is known as weight decay (chapter 11).

an equivalent way to write the ridge problem is

    ridge = argmin

  

subject to

nxi=1(cid:16)yi       0    
pxj=1

  2
j     t,

pxj=1

xij  j(cid:17)2

,

(3.42)

which makes explicit the size constraint on the parameters. there is a one-
to-one correspondence between the parameters    in (3.41) and t in (3.42).
when there are many correlated variables in a id75 model,
their coe   cients can become poorly determined and exhibit high variance.
a wildly large positive coe   cient on one variable can be canceled by a
similarly large negative coe   cient on its correlated cousin. by imposing a
size constraint on the coe   cients, as in (3.42), this problem is alleviated.
the ridge solutions are not equivariant under scaling of the inputs, and
so one normally standardizes the inputs before solving (3.41). in addition,

64

3. linear methods for regression

notice that the intercept   0 has been left out of the penalty term. penal-
ization of the intercept would make the procedure depend on the origin
chosen for y ; that is, adding a constant c to each of the targets yi would
not simply result in a shift of the predictions by the same amount c. it
can be shown (exercise 3.5) that the solution to (3.41) can be separated
into two parts, after reparametrization using centered inputs: each xij gets
replaced by xij       xj. we estimate   0 by   y = 1
1 yi. the remaining co-
e   cients get estimated by a ridge regression without intercept, using the
centered xij. henceforth we assume that this centering has been done, so
that the input matrix x has p (rather than p + 1) columns.

npn

writing the criterion in (3.41) in matrix form,

rss(  ) = (y     x  )t (y     x  ) +     t   ,

the ridge regression solutions are easily seen to be

    ridge = (xt x +   i)   1xt y,

(3.43)

(3.44)

where i is the p  p identity matrix. notice that with the choice of quadratic
penalty   t   , the ridge regression solution is again a linear function of
y. the solution adds a positive constant to the diagonal of xt x before
inversion. this makes the problem nonsingular, even if xt x is not of full
rank, and was the main motivation for ridge regression when it was    rst
introduced in statistics (hoerl and kennard, 1970). traditional descriptions
of ridge regression start with de   nition (3.44). we choose to motivate it via
(3.41) and (3.42), as these provide insight into how it works.

figure 3.8 shows the ridge coe   cient estimates for the prostate can-
cer example, plotted as functions of df(  ), the e   ective degrees of freedom
implied by the penalty    (de   ned in (3.50) on page 68). in the case of or-
thonormal inputs, the ridge estimates are just a scaled version of the least
squares estimates, that is,     ridge =     /(1 +   ).

ridge regression can also be derived as the mean or mode of a poste-
rior distribution, with a suitably chosen prior distribution. in detail, sup-
pose yi     n (  0 + xt
i   ,   2), and the parameters   j are each distributed as
n (0,    2), independently of one another. then the (negative) log-posterior
density of   , with    2 and   2 assumed known, is equal to the expression
in curly braces in (3.41), with    =   2/   2 (exercise 3.6). thus the ridge
estimate is the mode of the posterior distribution; since the distribution is
gaussian, it is also the posterior mean.

the singular value decomposition (svd) of the centered input matrix x
gives us some additional insight into the nature of ridge regression. this de-
composition is extremely useful in the analysis of many statistical methods.
the svd of the n    p matrix x has the form

x = udvt .

(3.45)

3.4 shrinkage methods

65

6

.

0

4
0

.

2
0

.

s
t
n
e
c
i
f
f

i

e
o
c

0

.

0

   
   
   
   
   
   
   
   

2

.

0
   

   

   
   

   
   

   
   

   

   

   
   

   
   

   
   

   

   

   
   

   
   

   
   
   

   

   
   

   
   

   

   
   

   

   
   

   
   

   

   
   

   

   
   

   
   

   
   

   

   

   
   

   
   
   
   

   

   

   
   

   
   
   
   
   

   

   
   

   
   
   
   
   

   

   
   

   
   
   
   
   

         

   

   

   

   

                     
                     
                     
                     

                     

   

   

   

                     
      

   

   

   

   

   

   

                  
                  
      
                  
                  
                  
                  
                  

0

2

4

df(  )

6

   

lcavol

   
   
   

svi
lweight
pgg45

   

lbph

   

gleason

   

age

   

lcp

8

figure 3.8. pro   les of ridge coe   cients for the prostate cancer example, as
the tuning parameter    is varied. coe   cients are plotted versus df(  ), the e   ective
degrees of freedom. a vertical line is drawn at df = 5.0, the value chosen by
cross-validation.

66

3. linear methods for regression

here u and v are n    p and p    p orthogonal matrices, with the columns
of u spanning the column space of x, and the columns of v spanning the
row space. d is a p    p diagonal matrix, with diagonal entries d1     d2    
           dp     0 called the singular values of x. if one or more values dj = 0,
x is singular.
using the singular value decomposition we can write the least squares

   tted vector as

x     ls = x(xt x)   1xt y

= uut y,

(3.46)

after some simpli   cation. note that ut y are the coordinates of y with
respect to the orthonormal basis u. note also the similarity with (3.33);
q and u are generally di   erent orthogonal bases for the column space of
x (exercise 3.8).

now the ridge solutions are

x     ridge = x(xt x +   i)   1xt y

= u d(d2 +   i)   1d ut y

=

pxj=1

uj

d2
j
d2
j +   

ut

j y,

(3.47)

where the uj are the columns of u. note that since        0, we have d2
j +
  )     1. like id75, ridge regression computes the coordinates of
y with respect to the orthonormal basis u. it then shrinks these coordinates
by the factors d2
j +   ). this means that a greater amount of shrinkage
is applied to the coordinates of basis vectors with smaller d2
j .

j /(d2

j /(d2

what does a small value of d2

j mean? the svd of the centered matrix
x is another way of expressing the principal components of the variables
in x. the sample covariance matrix is given by s = xt x/n , and from
(3.45) we have

xt x = vd2vt ,

(3.48)

which is the eigen decomposition of xt x (and of s, up to a factor n ).
the eigenvectors vj (columns of v) are also called the principal compo-
nents (or karhunen   loeve) directions of x. the    rst principal component
direction v1 has the property that z1 = xv1 has the largest sample vari-
ance amongst all normalized linear combinations of the columns of x. this
sample variance is easily seen to be

var(z1) = var(xv1) =

d2
1
n

,

(3.49)

and in fact z1 = xv1 = u1d1. the derived variable z1 is called the    rst
principal component of x, and hence u1 is the normalized    rst principal

4

2

2
x

0

2
-

4
-

3.4 shrinkage methods

67

largest principal

component

o

o

o

o
o
o

o
o

o

o

o

o

o

o

o

o
o

o

o
o
o

o

o
oo

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o

o
o

o

o

o

o
o

o
o
o

o

o
o

smallest principal

component

o

o

-4

-2

0

x1

2

4

figure 3.9. principal components of some input data points. the largest prin-
cipal component is the direction that maximizes the variance of the projected data,
and the smallest principal component minimizes that variance. ridge regression
projects y onto these components, and then shrinks the coe   cients of the low   
variance components more than the high-variance components.

component. subsequent principal components zj have maximum variance
d2
j /n , subject to being orthogonal to the earlier ones. conversely the last
principal component has minimum variance. hence the small singular val-
ues dj correspond to directions in the column space of x having small
variance, and ridge regression shrinks these directions the most.

figure 3.9 illustrates the principal components of some data points in
two dimensions. if we consider    tting a linear surface over this domain
(the y -axis is sticking out of the page), the con   guration of the data allow
us to determine its gradient more accurately in the long direction than
the short. ridge regression protects against the potentially high variance
of gradients estimated in the short directions. the implicit assumption is
that the response will tend to vary most in the directions of high variance
of the inputs. this is often a reasonable assumption, since predictors are
often chosen for study because they vary with the response variable, but
need not hold in general.

68

3. linear methods for regression

in figure 3.7 we have plotted the estimated prediction error versus the

quantity

df(  ) = tr[x(xt x +   i)   1xt ],

= tr(h  )

=

d2
j
d2
j +   

.

pxj=1

(3.50)

this monotone decreasing function of    is the e   ective degrees of freedom
of the ridge regression    t. usually in a linear-regression    t with p variables,
the degrees-of-freedom of the    t is p, the number of free parameters. the
idea is that although all p coe   cients in a ridge    t will be non-zero, they
are    t in a restricted fashion controlled by   . note that df(  ) = p when
   = 0 (no id173) and df(  )     0 as           . of course there
is always an additional one degree of freedom for the intercept, which was
removed apriori. this de   nition is motivated in more detail in section 3.4.4
and sections 7.4   7.6. in figure 3.7 the minimum occurs at df(  ) = 5.0.
table 3.3 shows that ridge regression reduces the test error of the full least
squares estimates by a small amount.

3.4.2 the lasso

the lasso is a shrinkage method like ridge, with subtle but important dif-
ferences. the lasso estimate is de   ned by

    lasso = argmin

  

pxj=1

xij  j(cid:17)2

nxi=1(cid:16)yi       0    
pxj=1

|  j|     t.

subject to

(3.51)

just as in ridge regression, we can re-parametrize the constant   0 by stan-
dardizing the predictors; the solution for     0 is   y, and thereafter we    t a
model without an intercept (exercise 3.5). in the signal processing litera-
ture, the lasso is also known as basis pursuit (chen et al., 1998).

we can also write the lasso problem in the equivalent lagrangian form

2

    lasso = argmin

   (cid:26) 1
l2 ridge penalty pp

xij  j(cid:1)2

|  j|(cid:27).
nxi=1(cid:0)yi       0    
j is replaced by the l1 lasso penalty pp

notice the similarity to the ridge regression problem (3.42) or (3.41): the
1 |  j|. this
latter constraint makes the solutions nonlinear in the yi, and there is no
closed form expression as in ridge regression. computing the lasso solution

pxj=1

pxj=1

(3.52)

1   2

+   

3.4 shrinkage methods

69

of continuous subset selection. if t is chosen larger than t0 =pp

is a quadratic programming problem, although we see in section 3.4.4 that
e   cient algorithms are available for computing the entire path of solutions
as    is varied, with the same computational cost as for ridge regression.
because of the nature of the constraint, making t su   ciently small will
cause some of the coe   cients to be exactly zero. thus the lasso does a kind
1 |     j| (where
    j =     ls
j , the least squares estimates), then the lasso estimates are the     j   s.
on the other hand, for t = t0/2 say, then the least squares coe   cients are
shrunk by about 50% on average. however, the nature of the shrinkage
is not obvious, and we investigate it further in section 3.4.4 below. like
the subset size in variable subset selection, or the penalty parameter in
ridge regression, t should be adaptively chosen to minimize an estimate of
expected prediction error.

diction error estimates versus the standardized parameter s = t/pp

in figure 3.7, for ease of interpretation, we have plotted the lasso pre-
1 |     j|.
a value   s     0.36 was chosen by 10-fold cross-validation; this caused four
coe   cients to be set to zero (   fth column of table 3.3). the resulting
model has the second lowest test error, slightly lower than the full least
squares model, but the standard errors of the test error estimates (last line
of table 3.3) are fairly large.

rameter s = t/pp

figure 3.10 shows the lasso coe   cients as the standardized tuning pa-
1 |     j| is varied. at s = 1.0 these are the least squares
estimates; they decrease to 0 as s     0. this decrease is not always strictly
monotonic, although it is in this example. a vertical line is drawn at
s = 0.36, the value chosen by cross-validation.

3.4.3 discussion: subset selection, ridge regression and the

lasso

in this section we discuss and compare the three approaches discussed so far
for restricting the id75 model: subset selection, ridge regression
and the lasso.

in the case of an orthonormal input matrix x the three procedures have
explicit solutions. each method applies a simple transformation to the least
squares estimate     j, as detailed in table 3.4.

ridge regression does a proportional shrinkage. lasso translates each
coe   cient by a constant factor   , truncating at zero. this is called    soft
thresholding,    and is used in the context of wavelet-based smoothing in sec-
tion 5.9. best-subset selection drops all variables with coe   cients smaller
than the m th largest; this is a form of    hard-thresholding.   

back to the nonorthogonal case; some pictures help understand their re-
lationship. figure 3.11 depicts the lasso (left) and ridge regression (right)
when there are only two parameters. the residual sum of squares has ellip-
tical contours, centered at the full least squares estimate. the constraint

70

3. linear methods for regression

s
t

i

n
e
c
i
f
f

e
o
c

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
   

lcavol

svi
lweight
pgg45

lbph

gleason

age

lcp

0.0

0.2

0.4

0.6

0.8

1.0

shrinkage factor s

figure 3.10. pro   les of lasso coe   cients, as the tuning parameter t is varied.
1 |     j|. a vertical line is drawn at s = 0.36,
coe   cients are plotted versus s = t/pp
the value chosen by cross-validation. compare figure 3.8 on page 65; the lasso
pro   les hit zero, while those for ridge do not. the pro   les are piece-wise linear,
and so are computed only at the points displayed; see section 3.4.4 for details.

3.4 shrinkage methods

71

table 3.4. estimators of   j in the case of orthonormal columns of x. m and   
are constants chosen by the corresponding techniques; sign denotes the sign of its
argument (  1), and x+ denotes    positive part    of x. below the table, estimators
are shown by broken red lines. the 45    line in gray shows the unrestricted estimate
for reference.

estimator

best subset (size m )

ridge

lasso

formula
    j    i(|     j|     |     (m )|)
    j/(1 +   )
sign(     j)(|     j|       )+

best subset

ridge

lasso

|     (m )|

(0,0)

(0,0)

(0,0)

  

2

.

b^

2

.

b^

b 1

1

figure 3.11. estimation picture for the lasso (left) and ridge regression
(right). shown are contours of the error and constraint functions. the solid blue
areas are the constraint regions |  1| + |  2|     t and   2
2     t2, respectively,
while the red ellipses are the contours of the least squares error function.

1 +   2

b
b
b
72

3. linear methods for regression

1 +   2

region for ridge regression is the disk   2
2     t, while that for lasso is
the diamond |  1| + |  2|     t. both methods    nd the    rst point where the
elliptical contours hit the constraint region. unlike the disk, the diamond
has corners; if the solution occurs at a corner, then it has one parameter
  j equal to zero. when p > 2, the diamond becomes a rhomboid, and has
many corners,    at edges and faces; there are many more opportunities for
the estimated parameters to be zero.

we can generalize ridge regression and the lasso, and view them as bayes

estimates. consider the criterion

     = argmin

   ( nxi=1(cid:0)yi       0    

pxj=1

xij  j(cid:1)2

+   

pxj=1

|  j|q)

(3.53)

ure 3.12, for the case of two inputs.

for q     0. the contours of constant value of pj |  j|q are shown in fig-

thinking of |  j|q as the log-prior density for   j, these are also the equi-
contours of the prior distribution of the parameters. the value q = 0 corre-
sponds to variable subset selection, as the penalty simply counts the number
of nonzero parameters; q = 1 corresponds to the lasso, while q = 2 to ridge
regression. notice that for q     1, the prior is not uniform in direction, but
concentrates more mass in the coordinate directions. the prior correspond-
ing to the q = 1 case is an independent double exponential (or laplace)
distribution for each input, with density (1/2   ) exp(   |  |/   ) and    = 1/  .
the case q = 1 (lasso) is the smallest q such that the constraint region
is convex; non-convex constraint regions make the optimization problem
more di   cult.

in this view, the lasso, ridge regression and best subset selection are
bayes estimates with di   erent priors. note, however, that they are derived
as posterior modes, that is, maximizers of the posterior. it is more common
to use the mean of the posterior as the bayes estimate. ridge regression is
also the posterior mean, but the lasso and best subset selection are not.

looking again at the criterion (3.53), we might try using other values
of q besides 0, 1, or 2. although one might consider estimating q from
the data, our experience is that it is not worth the e   ort for the extra
variance incurred. values of q     (1, 2) suggest a compromise between the
lasso and ridge regression. although this is the case, with q > 1, |  j|q is
di   erentiable at 0, and so does not share the ability of lasso (q = 1) for

q = 4

q = 2

q = 1

q = 0.5

q = 0.1

figure 3.12. contours of constant value of pj |  j|q for given values of q.

3.4 shrinkage methods

73

q = 1.2

   = 0.2

lq

elastic net

figure 3.13. contours of constant value of pj |  j|q for q = 1.2 (left plot),
and the elastic-net penalty pj(    2
j +(1     )|  j|) for    = 0.2 (right plot). although
visually very similar, the elastic-net has sharp (non-di   erentiable) corners, while
the q = 1.2 penalty does not.

setting coe   cients exactly to zero. partly for this reason as well as for
computational tractability, zou and hastie (2005) introduced the elastic-
net penalty

  

pxj=1(cid:0)    2

j + (1       )|  j|(cid:1),

(3.54)

a di   erent compromise between ridge and lasso. figure 3.13 compares the
lq penalty with q = 1.2 and the elastic-net penalty with    = 0.2; it is
hard to detect the di   erence by eye. the elastic-net selects variables like
the lasso, and shrinks together the coe   cients of correlated predictors like
ridge. it also has considerable computational advantages over the lq penal-
ties. we discuss the elastic-net further in section 18.4.

3.4.4 least angle regression

least angle regression (lar) is a relative newcomer (efron et al., 2004),
and can be viewed as a kind of    democratic    version of forward stepwise
regression (section 3.3.2). as we will see, lar is intimately connected
with the lasso, and in fact provides an extremely e   cient algorithm for
computing the entire lasso path as in figure 3.10.

forward stepwise regression builds a model sequentially, adding one vari-
able at a time. at each step, it identi   es the best variable to include in the
active set, and then updates the least squares    t to include all the active
variables.

least angle regression uses a similar strategy, but only enters    as much   
of a predictor as it deserves. at the    rst step it identi   es the variable
most correlated with the response. rather than    t this variable completely,
lar moves the coe   cient of this variable continuously toward its least-
squares value (causing its correlation with the evolving residual to decrease
in absolute value). as soon as another variable    catches up    in terms of
correlation with the residual, the process is paused. the second variable
then joins the active set, and their coe   cients are moved together in a way
that keeps their correlations tied and decreasing. this process is continued

74

3. linear methods for regression

until all the variables are in the model, and ends at the full least-squares
   t. algorithm 3.2 provides the details. the termination condition in step 5
requires some explanation. if p > n     1, the lar algorithm reaches a zero
residual solution after n     1 steps (the    1 is because we have centered the
data).

algorithm 3.2 least angle regression.

1. standardize the predictors to have mean zero and unit norm. start

with the residual r = y       y,   1,   2, . . . ,   p = 0.
2. find the predictor xj most correlated with r.

3. move   j from 0 towards its least-squares coe   cient hxj, ri, until some
other competitor xk has as much correlation with the current residual
as does xj.

4. move   j and   k in the direction de   ned by their joint least squares
coe   cient of the current residual on (xj, xk), until some other com-
petitor xl has as much correlation with the current residual.

5. continue in this way until all p predictors have been entered. after

min(n     1, p) steps, we arrive at the full least-squares solution.

  k = (xt
ak

xak )   1xt
ak

suppose ak is the active set of variables at the beginning of the kth
step, and let   ak be the coe   cient vector for these variables at this step;
there will be k     1 nonzero values, and the one just entered will be zero. if
rk = y     xak   ak is the current residual, then the direction for this step is
(3.55)
the coe   cient pro   le then evolves as   ak (  ) =   ak +         k. exercise 3.23
veri   es that the directions chosen in this fashion do what is claimed: keep
the correlations tied and decreasing. if the    t vector at the beginning of
this step is   fk, then it evolves as   fk(  ) =   fk +       uk, where uk = xak   k
is the new    t direction. the name    least angle    arises from a geometrical
interpretation of this process; uk makes the smallest (and equal) angle
with each of the predictors in ak (exercise 3.24). figure 3.14 shows the
absolute correlations decreasing and joining ranks with each step of the
lar algorithm, using simulated data.

rk.

by construction the coe   cients in lar change in a piecewise linear fash-
ion. figure 3.15 [left panel] shows the lar coe   cient pro   le evolving as a
function of their l1 arc length 2. note that we do not need to take small

2the l1 arc-length of a di   erentiable curve   (s) for s     [0, s] is given by tv(  , s) =
r s
0 ||     (s)||1ds, where     (s) =      (s)/   s. for the piecewise-linear lar coe   cient pro   le,
this amounts to summing the l1 norms of the changes in coe   cients from step to step.

3.4 shrinkage methods

75

v2

v6

v4

v5

v3

v1

s
n
o
i
t
a
l
e
r
r
o
c

l

e
t
u
o
s
b
a

4

.

0

3

.

0

2
0

.

.

1
0

0

.
0

0

5

10

15

l1 arc length

figure 3.14. progression of the absolute correlations during each step of the
lar procedure, using a simulated data set with six predictors. the labels at the
top of the plot indicate which variables enter the active set at each step. the step
length are measured in units of l1 arc length.

least angle regression

lasso

s
t
n
e
i
c
   
e
o
c

5
0

.

0

.

0

.

5
0
   

.

0
1
   

5

.
1
   

s
t
n
e
i
c
   
e
o
c

5
0

.

0

.

0

.

5
0
   

.

0
1
   

5

.
1
   

0

5

10

15

0

5

10

15

l1 arc length

l1 arc length

figure 3.15. left panel shows the lar coe   cient pro   les on the simulated
data, as a function of the l1 arc length. the right panel shows the lasso pro   le.
they are identical until the dark-blue coe   cient crosses zero at an arc length of
about 18.

76

3. linear methods for regression

steps and recheck the correlations in step 3; using knowledge of the covari-
ance of the predictors and the piecewise linearity of the algorithm, we can
work out the exact step length at the beginning of each step (exercise 3.25).
the right panel of figure 3.15 shows the lasso coe   cient pro   les on the
same data. they are almost identical to those in the left panel, and di   er
for the    rst time when the blue coe   cient passes back through zero. for the
prostate data, the lar coe   cient pro   le turns out to be identical to the
lasso pro   le in figure 3.10, which never crosses zero. these observations
lead to a simple modi   cation of the lar algorithm that gives the entire
lasso path, which is also piecewise-linear.

algorithm 3.2a least angle regression: lasso modi   cation.

4a. if a non-zero coe   cient hits zero, drop its variable from the active set
of variables and recompute the current joint least squares direction.

the lar(lasso) algorithm is extremely e   cient, requiring the same order
of computation as that of a single least squares    t using the p predictors.
least angle regression always takes p steps to get to the full least squares
estimates. the lasso path can have more than p steps, although the two
are often quite similar. algorithm 3.2 with the lasso modi   cation 3.2a is
an e   cient way of computing the solution to any lasso problem, especially
when p     n . osborne et al. (2000a) also discovered a piecewise-linear path
for computing the lasso, which they called a homotopy algorithm.
we now give a heuristic argument for why these procedures are so similar.
although the lar algorithm is stated in terms of correlations, if the input
features are standardized, it is equivalent and easier to work with inner-
products. suppose a is the active set of variables at some stage in the
algorithm, tied in their absolute inner-product with the current residuals
y     x  . we can express this as

xt
j (y     x  ) =       sj,    j     a

(3.56)

where sj     {   1, 1} indicates the sign of the inner-product, and    is the
common value. also |xt
k (y     x  )|           k 6    a. now consider the lasso
criterion (3.52), which we write in vector form

r(  ) = 1

2||y     x  ||2

2 +   ||  ||1.

(3.57)

let b be the active set of variables in the solution for a given value of   .
for these variables r(  ) is di   erentiable, and the stationarity conditions
give

xt
j (y     x  ) =       sign(  j),    j     b

(3.58)

comparing (3.58) with (3.56), we see that they are identical only if the
sign of   j matches the sign of the inner product. that is why the lar

3.4 shrinkage methods

77

algorithm and lasso start to di   er when an active coe   cient passes through
zero; condition (3.58) is violated for that variable, and it is kicked out of the
active set b. exercise 3.23 shows that these equations imply a piecewise-
linear coe   cient pro   le as    decreases. the stationarity conditions for the
non-active variables require that

|xt
k (y     x  )|       ,    k 6    b,

(3.59)

which again agrees with the lar algorithm.

figure 3.16 compares lar and lasso to forward stepwise and stagewise
regression. the setup is the same as in figure 3.6 on page 59, except here
n = 100 here rather than 300, so the problem is more di   cult. we see
that the more aggressive forward stepwise starts to over   t quite early (well
before the 10 true variables can enter the model), and ultimately performs
worse than the slower forward stagewise regression. the behavior of lar
and lasso is similar to that of forward stagewise regression. incremental
forward stagewise is similar to lar and lasso, and is described in sec-
tion 3.8.1.

degrees-of-freedom formula for lar and lasso

suppose that we    t a linear model via the least angle regression procedure,
stopping at some number of steps k < p, or equivalently using a lasso bound
t that produces a constrained version of the full least squares    t. how many
parameters, or    degrees of freedom    have we used?

consider    rst a id75 using a subset of k features. if this subset
is prespeci   ed in advance without reference to the training data, then the
degrees of freedom used in the    tted model is de   ned to be k. indeed, in
classical statistics, the number of linearly independent parameters is what
is meant by    degrees of freedom.    alternatively, suppose that we carry out
a best subset selection to determine the    optimal    set of k predictors. then
the resulting model has k parameters, but in some sense we have used up
more than k degrees of freedom.

we need a more general de   nition for the e   ective degrees of freedom of
an adaptively    tted model. we de   ne the degrees of freedom of the    tted
vector   y = (  y1,   y2, . . . ,   yn ) as

df(  y) =

1
  2

nxi=1

cov(  yi, yi).

(3.60)

here cov(  yi, yi) refers to the sampling covariance between the predicted
value   yi and its corresponding outcome value yi. this makes intuitive sense:
the harder that we    t to the data, the larger this covariance and hence
df(  y). expression (3.60) is a useful notion of degrees of freedom, one that
can be applied to any model prediction   y. this includes models that are

78

3. linear methods for regression

2
|
|

  
   

)
k
(
    

|
|

e

5
6

.

0

0
6

.

0

5
5

.

0

forward stepwise
lar
lasso
forward stagewise
incremental forward stagewise

0.0

0.2

0.4

0.6

0.8

1.0

fraction of l1 arc-length

figure 3.16. comparison of lar and lasso with forward stepwise, forward
stagewise (fs) and incremental forward stagewise (fs0) regression. the setup
is the same as in figure 3.6, except n = 100 here rather than 300. here the
slower fs regression ultimately outperforms forward stepwise. lar and lasso
show similar behavior to fs and fs0. since the procedures take di   erent numbers
of steps (across simulation replicates and methods), we plot the mse as a function
of the fraction of total l1 arc-length toward the least-squares    t.

adaptively    tted to the training data. this de   nition is motivated and
discussed further in sections 7.4   7.6.

now for a id75 with k    xed predictors, it is easy to show
that df(  y) = k. likewise for ridge regression, this de   nition leads to the
closed-form expression (3.50) on page 68: df(  y) = tr(s  ). in both these
cases, (3.60) is simple to evaluate because the    t   y = h  y is linear in y.
if we think about de   nition (3.60) in the context of a best subset selection
of size k, it seems clear that df(  y) will be larger than k, and this can be
veri   ed by estimating cov(  yi, yi)/  2 directly by simulation. however there
is no closed form method for estimating df(  y) for best subset selection.

for lar and lasso, something magical happens. these techniques are
adaptive in a smoother way than best subset selection, and hence estimation
of degrees of freedom is more tractable. speci   cally it can be shown that
after the kth step of the lar procedure, the e   ective degrees of freedom of
the    t vector is exactly k. now for the lasso, the (modi   ed) lar procedure

3.5 methods using derived input directions

79

often takes more than p steps, since predictors can drop out. hence the
de   nition is a little di   erent; for the lasso, at any stage df(  y) approximately
equals the number of predictors in the model. while this approximation
works reasonably well anywhere in the lasso path, for each k it works best
at the last model in the sequence that contains k predictors. a detailed
study of the degrees of freedom for the lasso may be found in zou et al.
(2007).

3.5 methods using derived input directions

in many situations we have a large number of inputs, often very correlated.
the methods in this section produce a small number of linear combinations
zm, m = 1, . . . , m of the original inputs xj, and the zm are then used in
place of the xj as inputs in the regression. the methods di   er in how the
linear combinations are constructed.

3.5.1 principal components regression

in this approach the linear combinations zm used are the principal com-
ponents as de   ned in section 3.4.1 above.

principal component regression forms the derived input columns zm =
xvm, and then regresses y on z1, z2, . . . , zm for some m     p. since the zm
are orthogonal, this regression is just a sum of univariate regressions:

  ypcr
(m ) =   y1 +

    mzm,

mxm=1

(3.61)

where     m = hzm, yi/hzm, zmi. since the zm are each linear combinations
of the original xj, we can express the solution (3.61) in terms of coe   cients
of the xj (exercise 3.13):

    pcr(m ) =

    mvm.

mxm=1

(3.62)

as with ridge regression, principal components depend on the scaling of
the inputs, so typically we    rst standardize them. note that if m = p, we
would just get back the usual least squares estimates, since the columns of
z = ud span the column space of x. for m < p we get a reduced regres-
sion. we see that principal components regression is very similar to ridge
regression: both operate via the principal components of the input ma-
trix. ridge regression shrinks the coe   cients of the principal components
(figure 3.17), shrinking more depending on the size of the corresponding
eigenvalue; principal components regression discards the p     m smallest
eigenvalue components. figure 3.17 illustrates this.

80

3. linear methods for regression

   
   

 

r
o
t
c
a
f
e
g
a
k
n
i
r
h
s

0

.

1

8

.

0

6

.

0

4

.

0

2
0

.

0
0

.

   

   

ridge
pcr

   

   

2

   

   

   

   

4

index

   

   

   

   

   

6

   

   

8

figure 3.17. ridge regression shrinks the regression coe   cients of the prin-
cipal components, using shrinkage factors d2
j +   ) as in (3.47). principal
component regression truncates them. shown are the shrinkage and truncation
patterns corresponding to figure 3.7, as a function of the principal component
index.

j /(d2

in figure 3.7 we see that cross-validation suggests seven terms; the re-

sulting model has the lowest test error in table 3.3.

3.5.2 partial least squares

z1 = pj     1j xj, which is the    rst partial least squares direction. hence

this technique also constructs a set of linear combinations of the inputs
for regression, but unlike principal components regression it uses y (in ad-
dition to x) for this construction. like principal component regression,
partial least squares (pls) is not scale invariant, so we assume that each
xj is standardized to have mean 0 and variance 1. pls begins by com-
puting     1j = hxj, yi for each j. from this we construct the derived input
in the construction of each zm, the inputs are weighted by the strength
of their univariate e   ect on y3. the outcome y is regressed on z1 giving
coe   cient     1, and then we orthogonalize x1, . . . , xp with respect to z1. we
continue this process, until m     p directions have been obtained. in this
manner, partial least squares produces a sequence of derived, orthogonal
inputs or directions z1, z2, . . . , zm . as with principal-component regres-
sion, if we were to construct all m = p directions, we would get back a
solution equivalent to the usual least squares estimates; using m < p di-
rections produces a reduced regression. the procedure is described fully in
algorithm 3.3.

3since the xj are standardized, the    rst directions     1j are the univariate regression
coe   cients (up to an irrelevant constant); this is not the case for subsequent directions.

3.5 methods using derived input directions

81

algorithm 3.3 partial least squares.

1. standardize each xj to have mean zero and variance one. set   y(0) =

  y1, and x(0)

j = xj, j = 1, . . . , p.

2. for m = 1, 2, . . . , p

, where     mj = hx(m   1)

j

, yi.

j

(a) zm =pp
j=1     mj x(m   1)
(b)     m = hzm, yi/hzm, zmi.
(c)   y(m) =   y(m   1) +     mzm.
(d) orthogonalize each x(m   1)

j

with respect to zm: x(m)

j = x(m   1)

j

   

j

[hzm, x(m   1)

i/hzm, zmi]zm, j = 1, 2, . . . , p.
3. output the sequence of    tted vectors {  y(m)}p

1 are
linear in the original xj, so is   y(m) = x     pls(m). these linear coe   -
cients can be recovered from the sequence of pls transformations.

1. since the {z   }m

in the prostate cancer example, cross-validation chose m = 2 pls direc-
tions in figure 3.7. this produced the model given in the rightmost column
of table 3.3.

what optimization problem is partial least squares solving? since it uses
the response y to construct its directions, its solution path is a nonlinear
function of y. it can be shown (exercise 3.15) that partial least squares
seeks directions that have high variance and have high correlation with the
response, in contrast to principal components regression which keys only
on high variance (stone and brooks, 1990; frank and friedman, 1993). in
particular, the mth principal component direction vm solves:

max   var(x  )

(3.63)

subject to ||  || = 1,   t sv    = 0,     = 1, . . . , m     1,

where s is the sample covariance matrix of the xj. the conditions   t sv    =
0 ensures that zm = x   is uncorrelated with all the previous linear com-
binations z    = xv   . the mth pls direction     m solves:

max   corr2(y, x  )var(x  )

(3.64)

subject to ||  || = 1,   t s         = 0,     = 1, . . . , m     1.

further analysis reveals that the variance aspect tends to dominate, and
so partial least squares behaves much like ridge regression and principal
components regression. we discuss this further in the next section.

if the input matrix x is orthogonal, then partial least squares    nds the
least squares estimates after m = 1 steps. subsequent steps have no e   ect

82

3. linear methods for regression

since the     mj are zero for m > 1 (exercise 3.14). it can also be shown that
the sequence of pls coe   cients for m = 1, 2, . . . , p represents the conjugate
gradient sequence for computing the least squares solutions (exercise 3.18).

3.6 discussion: a comparison of the selection and

shrinkage methods

there are some simple settings where we can understand better the rela-
tionship between the di   erent methods described above. consider an exam-
ple with two correlated inputs x1 and x2, with correlation   . we assume
that the true regression coe   cients are   1 = 4 and   2 = 2. figure 3.18
shows the coe   cient pro   les for the di   erent methods, as their tuning pa-
rameters are varied. the top panel has    = 0.5, the bottom panel    =    0.5.
the tuning parameters for ridge and lasso vary over a continuous range,
while best subset, pls and pcr take just two discrete steps to the least
squares solution. in the top panel, starting at the origin, ridge regression
shrinks the coe   cients together until it    nally converges to least squares.
pls and pcr show similar behavior to ridge, although are discrete and
more extreme. best subset overshoots the solution and then backtracks.
the behavior of the lasso is intermediate to the other methods. when the
correlation is negative (lower panel), again pls and pcr roughly track
the ridge path, while all of the methods are more similar to one another.

it is interesting to compare the shrinkage behavior of these di   erent
methods. recall that ridge regression shrinks all directions, but shrinks
low-variance directions more. principal components regression leaves m
high-variance directions alone, and discards the rest. interestingly, it can
be shown that partial least squares also tends to shrink the low-variance
directions, but can actually in   ate some of the higher variance directions.
this can make pls a little unstable, and cause it to have slightly higher
prediction error compared to ridge regression. a full study is given in frank
and friedman (1993). these authors conclude that for minimizing predic-
tion error, ridge regression is generally preferable to variable subset selec-
tion, principal components regression and partial least squares. however
the improvement over the latter two methods was only slight.

to summarize, pls, pcr and ridge regression tend to behave similarly.
ridge regression may be preferred because it shrinks smoothly, rather than
in discrete steps. lasso falls somewhere between ridge regression and best
subset regression, and enjoys some of the properties of each.

3.6 discussion: a comparison of the selection and shrinkage methods

83

3

2

1

2
  

0

1
-

3

2

1

2
  

0

1
-

0

0

0

0

   = 0.5

pcr

pls
ridge

   

least squares

lasso

best subset

4

5

6

1

2

3

  1

   =    0.5

   

least squares

ridge

lasso

best subset

pcr

1

2

pls

3

  1

4

5

6

figure 3.18. coe   cient pro   les from di   erent methods for a simple problem:
two inputs with correlation   0.5, and the true regression coe   cients    = (4, 2).

84

3. linear methods for regression

3.7 multiple outcome shrinkage and selection

as noted in section 3.2.4, the least squares estimates in a multiple-output
linear model are simply the individual least squares estimates for each of
the outputs.

to apply selection and shrinkage methods in the multiple output case,
one could apply a univariate technique individually to each outcome or si-
multaneously to all outcomes. with ridge regression, for example, we could
apply formula (3.44) to each of the k columns of the outcome matrix y ,
using possibly di   erent parameters   , or apply it to all columns using the
same value of   . the former strategy would allow di   erent amounts of
id173 to be applied to di   erent outcomes but require estimation
of k separate id173 parameters   1, . . . ,   k, while the latter would
permit all k outputs to be used in estimating the sole id173 pa-
rameter   .

other more sophisticated shrinkage and selection strategies that exploit
correlations in the di   erent responses can be helpful in the multiple output
case. suppose for example that among the outputs we have

yk = f (x) +   k
y    = f (x) +      ;

(3.65)

(3.66)

i.e., (3.65) and (3.66) share the same structural part f (x) in their models.
it is clear in this case that we should pool our observations on yk and yl
to estimate the common f .

combining responses is at the heart of canonical correlation analysis
(cca), a data reduction technique developed for the multiple output case.
similar to pca, cca    nds a sequence of uncorrelated linear combina-
tions xvm, m = 1, . . . , m of the xj, and a corresponding sequence of
uncorrelated linear combinations yum of the responses yk, such that the
correlations

corr2(yum, xvm)

(3.67)

are successively maximized. note that at most m = min(k, p) directions
can be found. the leading canonical response variates are those linear com-
binations (derived responses) best predicted by the xj; in contrast, the
trailing canonical variates can be poorly predicted by the xj, and are can-
didates for being dropped. the cca solution is computed using a general-
ized svd of the sample cross-covariance matrix yt x/n (assuming y and
x are centered; exercise 3.20).

reduced-rank regression (izenman, 1975; van der merwe and zidek, 1980)
formalizes this approach in terms of a regression model that explicitly pools
information. given an error covariance cov(  ) =   , we solve the following

3.7 multiple outcome shrinkage and selection

85

restricted multivariate regression problem:

  brr(m) = argmin
rank(b)=m

nxi=1

(yi     bt xi)t      1(yi     bt xi).

(3.68)

with    replaced by the estimate yt y/n , one can show (exercise 3.21)
that the solution is given by a cca of y and x:

  brr(m) =   bumu   
m,

(3.69)

where um is the k    m sub-matrix of u consisting of the    rst m columns,
and u is the k    m matrix of left canonical vectors u1, u2, . . . , um . u   
is its generalized inverse. writing the solution as

m

  brr(m ) = (xt x)   1xt (yum)u   
m,

(3.70)

we see that reduced-rank regression performs a id75 on the
pooled response matrix yum, and then maps the coe   cients (and hence
the    ts as well) back to the original response space. the reduced-rank    ts
are given by

  yrr(m) = x(xt x)   1xt yumu   
m

= hypm,

(3.71)

where h is the usual id75 projection operator, and pm is the
rank-m cca response projection operator. although a better estimate of
   would be (y   x   b)t (y   x   b)/(n   pk), one can show that the solution
remains the same (exercise 3.22).
reduced-rank regression borrows strength among responses by truncat-
ing the cca. breiman and friedman (1997) explored with some success
shrinkage of the canonical variates between x and y, a smooth version of
reduced rank regression. their proposal has the form (compare (3.69))

  bc+w =   bu  u   1,

(3.72)

where    is a diagonal shrinkage matrix (the    c+w    stands for    curds
and whey,    the name they gave to their procedure). based on optimal
prediction in the population setting, they show that    has diagonal entries

  m =

c2
m
m + p
n (1     c2
c2
m)

, m = 1, . . . , m,

(3.73)

where cm is the mth canonical correlation coe   cient. note that as the ratio
of the number of input variables to sample size p/n gets small, the shrink-
age factors approach 1. breiman and friedman (1997) proposed modi   ed
versions of    based on training data and cross-validation, but the general
form is the same. here the    tted response has the form

  yc+w = hysc+w,

(3.74)

86

3. linear methods for regression

where sc+w = u  u   1 is the response shrinkage operator.

breiman and friedman (1997) also suggested shrinking in both the y

space and x space. this leads to hybrid shrinkage models of the form

  yridge,c+w = a  ysc+w,

(3.75)

where a   = x(xt x +   i)   1xt is the ridge regression shrinkage operator,
as in (3.46) on page 66. their paper and the discussions thereof contain
many more details.

3.8 more on the lasso and related path

algorithms

since the publication of the lar algorithm (efron et al., 2004) there has
been a lot of activity in developing algorithms for    tting id173
paths for a variety of di   erent problems. in addition, l1 id173 has
taken on a life of its own, leading to the development of the    eld compressed
sensing in the signal-processing literature. (donoho, 2006a; candes, 2006).
in this section we discuss some related proposals and other path algorithms,
starting o    with a precursor to the lar algorithm.

3.8.1 incremental forward stagewise regression

here we present another lar-like algorithm, this time focused on forward
stagewise regression. interestingly, e   orts to understand a    exible nonlinear
regression procedure (boosting) led to a new algorithm for linear models
(lar). in reading the    rst edition of this book and the forward stagewise

algorithm 3.4 incremental forward stagewise regression   fs  .

1. start with the residual r equal to y and   1,   2, . . . ,   p = 0. all the

predictors are standardized to have mean zero and unit norm.

2. find the predictor xj most correlated with r

3. update   j       j +   j, where   j =       sign[hxj, ri] and    > 0 is a small

step size, and set r     r       j xj.

4. repeat steps 2 and 3 many times, until the residuals are uncorrelated

with all the predictors.

algorithm 16.1 of chapter 164, our colleague brad efron realized that with

4in the    rst edition, this was algorithm 10.4 in chapter 10.

3.8 more on the lasso and related path algorithms

87

fs  

fs0

lcavol

svi

lweight
pgg45
lbph

gleason

age

lcp

s
t
n
e
i
c
   
e
o
c

6
0

.

4

.

0

2

.

0

0

.

0

2

.

0
   

lcavol

svi

lweight
pgg45
lbph

gleason

age

lcp

s
t
n
e
i
c
   
e
o
c

6
0

.

4

.

0

2

.

0

0

.

0

2

.

0
   

0

50

100

150

200

0.0

0.5

1.0

1.5

2.0

iteration

l1 arc-length of coe   cients

figure 3.19. coe   cient pro   les for the prostate data. the left panel shows
incremental forward stagewise regression with step size    = 0.01. the right panel
shows the in   nitesimal version fs0 obtained letting        0. this pro   le was    t by
the modi   cation 3.2b to the lar algorithm 3.2. in this example the fs0 pro   les
are monotone, and hence identical to those of lasso and lar.

linear models, one could explicitly construct the piecewise-linear lasso paths
of figure 3.10. this led him to propose the lar procedure of section 3.4.4,
as well as the incremental version of forward-stagewise regression presented
here.

consider the linear-regression version of the forward-stagewise boosting
algorithm 16.1 proposed in section 16.1 (page 608). it generates a coe   cient
pro   le by repeatedly updating (by a small amount   ) the coe   cient of the
variable most correlated with the current residuals. algorithm 3.4 gives
the details. figure 3.19 (left panel) shows the progress of the algorithm on
the prostate data with step size    = 0.01. if   j = hxj, ri (the least-squares
coe   cient of the residual on jth predictor), then this is exactly the usual
forward stagewise procedure (fs) outlined in section 3.3.3.

here we are mainly interested in small values of   . letting        0 gives
the right panel of figure 3.19, which in this case is identical to the lasso
path in figure 3.10. we call this limiting procedure in   nitesimal forward
stagewise regression or fs0. this procedure plays an important role in
non-linear, adaptive methods like boosting (chapters 10 and 16) and is the
version of incremental forward stagewise regression that is most amenable
to theoretical analysis. b  uhlmann and hothorn (2007) refer to the same
procedure as    l2boost   , because of its connections to boosting.

88

3. linear methods for regression

efron originally thought that the lar algorithm 3.2 was an implemen-
tation of fs0, allowing each tied predictor a chance to update their coe   -
cients in a balanced way, while remaining tied in correlation. however, he
then realized that the lar least-squares    t amongst the tied predictors
can result in coe   cients moving in the opposite direction to their correla-
tion, which cannot happen in algorithm 3.4. the following modi   cation of
the lar algorithm implements fs0:

algorithm 3.2b least angle regression: fs0 modi   cation.

4. find the new direction by solving the constrained least squares prob-

lem

min

||r     xab||2
where sj is the sign of hxj, ri.

b

2 subject to bjsj     0, j     a,

the modi   cation amounts to a non-negative least squares    t, keeping the
signs of the coe   cients the same as those of the correlations. one can show
that this achieves the optimal balancing of in   nitesimal    update turns   
for the variables tied for maximal correlation (hastie et al., 2007). like
lasso, the entire fs0 path can be computed very e   ciently via the lar
algorithm.

as a consequence of these results, if the lar pro   les are monotone non-
increasing or non-decreasing, as they are in figure 3.19, then all three
methods   lar, lasso, and fs0   give identical pro   les. if the pro   les are
not monotone but do not cross the zero axis, then lar and lasso are
identical.

since fs0 is di   erent from the lasso, it is natural to ask if it optimizes
a criterion. the answer is more complex than for lasso; the fs0 coe   cient
pro   le is the solution to a di   erential equation. while the lasso makes op-
timal progress in terms of reducing the residual sum-of-squares per unit
increase in l1-norm of the coe   cient vector   , fs0 is optimal per unit
increase in l1 arc-length traveled along the coe   cient path. hence its co-
e   cient path is discouraged from changing directions too often.

fs0 is more constrained than lasso, and in fact can be viewed as a mono-
tone version of the lasso; see figure 16.3 on page 614 for a dramatic exam-
ple. fs0 may be useful in p     n situations, where its coe   cient pro   les
are much smoother and hence have less variance than those of lasso. more
details on fs0 are given in section 16.2.3 and hastie et al. (2007). fig-
ure 3.16 includes fs0 where its performance is very similar to that of the
lasso.

3.8 more on the lasso and related path algorithms

89

3.8.2 piecewise-linear path algorithms

the least angle regression procedure exploits the piecewise linear nature of
the lasso solution paths. it has led to similar    path algorithms    for other
regularized problems. suppose we solve

    (  ) = argmin   [r(  ) +   j(  )] ,

(3.76)

with

r(  ) =

nxi=1

l(yi,   0 +

pxj=1

xij  j),

(3.77)

where both the id168 l and the penalty function j are convex.
then the following are su   cient conditions for the solution path     (  ) to
be piecewise linear (rosset and zhu, 2007):

1. r is quadratic or piecewise-quadratic as a function of   , and

2. j is piecewise linear in   .

this also implies (in principle) that the solution path can be e   ciently
computed. examples include squared- and absolute-error loss,    huberized   
losses, and the l1, l    penalties on   . another example is the    hinge loss   
function used in the support vector machine. there the loss is piecewise
linear, and the penalty is quadratic. interestingly, this leads to a piecewise-
linear path algorithm in the dual space; more details are given in sec-
tion 12.3.5.

3.8.3 the dantzig selector

candes and tao (2007) proposed the following criterion:
min  ||  ||1 subject to ||xt (y     x  )||        s.

(3.78)

they call the solution the dantzig selector (ds). it can be written equiva-
lently as

min  ||xt (y     x  )||    subject to ||  ||1     t.

(3.79)

here ||    ||    denotes the l    norm, the maximum absolute value of the
components of the vector. in this form it resembles the lasso, replacing
squared error loss by the maximum absolute value of its gradient. note
that as t gets large, both procedures yield the least squares solution if
n < p. if p     n , they both yield the least squares solution with minimum
l1 norm. however for smaller values of t, the ds procedure produces a
di   erent path of solutions than the lasso.

candes and tao (2007) show that the solution to ds is a linear pro-
gramming problem; hence the name dantzig selector, in honor of the late

90

3. linear methods for regression

george dantzig, the inventor of the simplex method for linear program-
ming. they also prove a number of interesting mathematical properties for
the method, related to its ability to recover an underlying sparse coe   -
cient vector. these same properties also hold for the lasso, as shown later
by bickel et al. (2008).

unfortunately the operating properties of the ds method are somewhat
unsatisfactory. the method seems similar in spirit to the lasso, especially
when we look at the lasso   s stationary conditions (3.58). like the lar al-
gorithm, the lasso maintains the same inner product (and correlation) with
the current residual for all variables in the active set, and moves their co-
e   cients to optimally decrease the residual sum of squares. in the process,
this common correlation is decreased monotonically (exercise 3.23), and at
all times this correlation is larger than that for non-active variables. the
dantzig selector instead tries to minimize the maximum inner product of
the current residual with all the predictors. hence it can achieve a smaller
maximum than the lasso, but in the process a curious phenomenon can
occur. if the size of the active set is m, there will be m variables tied with
maximum correlation. however, these need not coincide with the active set!
hence it can include a variable in the model that has smaller correlation
with the current residual than some of the excluded variables (efron et
al., 2007). this seems unreasonable and may be responsible for its some-
times inferior prediction accuracy. efron et al. (2007) also show that ds
can yield extremely erratic coe   cient paths as the id173 parameter
s is varied.

3.8.4 the grouped lasso

in some problems, the predictors belong to pre-de   ned groups; for example
genes that belong to the same biological pathway, or collections of indicator
(dummy) variables for representing the levels of a categorical predictor. in
this situation it may be desirable to shrink and select the members of a
group together. the grouped lasso is one way to achieve this. suppose that
the p predictors are divided into l groups, with p    the number in group
   . for ease of notation, we use a matrix x    to represent the predictors
corresponding to the    th group, with corresponding coe   cient vector      .
the grouped-lasso minimizes the convex criterion

     irp ||y       01    

min

lx   =1

x        ||2

2 +   

lx   =1

   p   ||     ||2! ,

(3.80)

where the    p    terms accounts for the varying group sizes, and ||    ||2 is

the euclidean norm (not squared). since the euclidean norm of a vector
      is zero only if all of its components are zero, this procedure encourages
sparsity at both the group and individual levels. that is, for some values of
  , an entire group of predictors may drop out of the model. this procedure

3.8 more on the lasso and related path algorithms

91

was proposed by bakin (1999) and lin and zhang (2006), and studied and
generalized by yuan and lin (2007). generalizations include more general
l2 norms ||  ||k = (  t k  )1/2, as well as allowing overlapping groups of
predictors (zhao et al., 2008). there are also connections to methods for
   tting sparse additive models (lin and zhang, 2006; ravikumar et al.,
2008).

3.8.5 further properties of the lasso

a number of authors have studied the ability of the lasso and related pro-
cedures to recover the correct model, as n and p grow. examples of this
work include knight and fu (2000), greenshtein and ritov (2004), tropp
(2004), donoho (2006b), meinshausen (2007), meinshausen and b  uhlmann
(2006), tropp (2006), zhao and yu (2006), wainwright (2006), and bunea
et al. (2007). for example donoho (2006b) focuses on the p > n case and
considers the lasso solution as the bound t gets large. in the limit this gives
the solution with minimum l1 norm among all models with zero training
error. he shows that under certain assumptions on the model matrix x, if
the true model is sparse, this solution identi   es the correct predictors with
high id203.

many of the results in this area assume a condition on the model matrix

of the form

j   s c ||xt
max

j xs (xs

t xs )   1||1     (1       ) for some        (0, 1].

(3.81)

here s indexes the subset of features with non-zero coe   cients in the true
underlying model, and xs are the columns of x corresponding to those
features. similarly s c are the features with true coe   cients equal to zero,
and xs c the corresponding columns. this says that the least squares coef-
   cients for the columns of xs c on xs are not too large, that is, the    good   
variables s are not too highly correlated with the nuisance variables s c.
regarding the coe   cients themselves, the lasso shrinkage causes the esti-
mates of the non-zero coe   cients to be biased towards zero, and in general
they are not consistent5. one approach for reducing this bias is to run
the lasso to identify the set of non-zero coe   cients, and then    t an un-
restricted linear model to the selected set of features. this is not always
feasible, if the selected set is large. alternatively, one can use the lasso to
select the set of non-zero predictors, and then apply the lasso again, but
using only the selected predictors from the    rst step. this is known as the
relaxed lasso (meinshausen, 2007). the idea is to use cross-validation to
estimate the initial penalty parameter for the lasso, and then again for a
second penalty parameter applied to the selected set of predictors. since

5statistical consistency means as the sample size grows, the estimates converge to

the true values.

92

3. linear methods for regression

the variables in the second step have less    competition    from noise vari-
ables, cross-validation will tend to pick a smaller value for   , and hence
their coe   cients will be shrunken less than those in the initial estimate.

alternatively, one can modify the lasso penalty function so that larger co-
e   cients are shrunken less severely; the smoothly clipped absolute deviation
(scad) penalty of fan and li (2005) replaces   |  | by ja(  ,   ), where

dja(  ,   )

d  

=       sign(  )hi(|  |       ) +

(a       |  |)+
(a     1)  

i(|  | >   )i

for some a     2. the second term in square-braces reduces the amount of
shrinkage in the lasso for larger values of   , with ultimately no shrinkage
as a        . figure 3.20 shows the scad penalty, along with the lasso and

(3.82)

|  |

scad

|  |1     

5

4

3

2

1

0

5
2

.

0
2

.

5
1

.

0
1

.

5
0

.

0
0

.

0
2

.

5
1

.

0
1

.

5

.

0

   4

   2

0

  

2

4

   4

   2

0

  

2

4

   4

   2

2

4

0

  

figure 3.20. the lasso and two alternative non-convex penalties designed to
penalize large coe   cients less. for scad we use    = 1 and a = 4, and    = 1
2 in
the last panel.

|  |1     . however this criterion is non-convex, which is a drawback since it
makes the computation much more di   cult. the adaptive lasso (zou, 2006)
uses a weighted penalty of the formpp
j=1 wj|  j| where wj = 1/|     j|  ,     j is
the ordinary least squares estimate and    > 0. this is a practical approxi-
mation to the |  |q penalties (q = 1       here) discussed in section 3.4.3. the
adaptive lasso yields consistent estimates of the parameters while retaining
the attractive convexity property of the lasso.

3.8.6 pathwise coordinate optimization

an alternate approach to the lars algorithm for computing the lasso
solution is simple coordinate descent. this idea was proposed by fu (1998)
and daubechies et al. (2004), and later studied and generalized by friedman
et al. (2007), wu and lange (2008) and others. the idea is to    x the penalty
parameter    in the lagrangian form (3.52) and optimize successively over
each parameter, holding the other parameters    xed at their current values.
suppose the predictors are all standardized to have mean zero and unit
norm. denote by     k(  ) the current estimate for   k at penalty parameter

  . we can rearrange (3.52) to isolate   j,

3.9 computational considerations

93

1
2

nxi=1 yi    xk6=j

xik     k(  )     xij  j!2

r(     (  ),   j) =

|     k(  )| +   |  j|,
(3.83)
where we have suppressed the intercept and introduced a factor 1
2 for con-
venience. this can be viewed as a univariate lasso problem with response

+   xk6=j

variable the partial residual yi       y(j)

explicit solution, resulting in the update

i = yi    pk6=j xik     k(  ). this has an
xij(yi       y(j)

),   !.

(3.84)

i

    j(  )     s  nxi=1

here s(t,   ) = sign(t)(|t|     )+ is the soft-thresholding operator in table 3.4
on page 71. the    rst argument to s(  ) is the simple least-squares coe   cient
of the partial residual on the standardized variable xij. repeated iteration
of (3.84)   cycling through each variable in turn until convergence   yields
the lasso estimate     (  ).

we can also use this simple algorithm to e   ciently compute the lasso
solutions at a grid of values of   . we start with the smallest value   max
for which     (  max) = 0, decrease it a little and cycle through the variables
until convergence. then    is decreased again and the process is repeated,
using the previous solution as a    warm start    for the new value of   . this
can be faster than the lars algorithm, especially in large problems. a
key to its speed is the fact that the quantities in (3.84) can be updated
quickly as j varies, and often the update is to leave     j = 0. on the other
hand, it delivers solutions over a grid of    values, rather than the entire
solution path. the same kind of algorithm can be applied to the elastic
net, the grouped lasso and many other models in which the penalty is a
sum of functions of the individual parameters (friedman et al., 2010). it
can also be applied, with some substantial modi   cations, to the fused lasso
(section 18.4.2); details are in friedman et al. (2007).

3.9 computational considerations

least squares    tting is usually done via the cholesky decomposition of
the matrix xt x or a qr decomposition of x. with n observations and p
features, the cholesky decomposition requires p3 +n p2/2 operations, while
the qr decomposition requires n p2 operations. depending on the relative
size of n and p, the cholesky can sometimes be faster; on the other hand,
it can be less numerically stable (lawson and hansen, 1974). computation
of the lasso via the lar algorithm has the same order of computation as
a least squares    t.

94

3. linear methods for regression

bibliographic notes

id75 is discussed in many statistics books, for example, seber
(1984), weisberg (1980) and mardia et al. (1979). ridge regression was
introduced by hoerl and kennard (1970), while the lasso was proposed by
tibshirani (1996). around the same time, lasso-type penalties were pro-
posed in the basis pursuit method for signal processing (chen et al., 1998).
the least angle regression procedure was proposed in efron et al. (2004);
related to this is the earlier homotopy procedure of osborne et al. (2000a)
and osborne et al. (2000b). their algorithm also exploits the piecewise
linearity used in the lar/lasso algorithm, but lacks its transparency. the
criterion for the forward stagewise criterion is discussed in hastie et al.
(2007). park and hastie (2007) develop a path algorithm similar to least
angle regression for generalized regression models. partial least squares
was introduced by wold (1975). comparisons of shrinkage methods may
be found in copas (1983) and frank and friedman (1993).

exercises

ex. 3.1 show that the f statistic (3.13) for dropping a single coe   cient
from a model is equal to the square of the corresponding z-score (3.12).

polynomial regression model f (x) =p3

ex. 3.2 given data on two variables x and y , consider    tting a cubic
j=0   jx j. in addition to plotting
the    tted curve, you would like a 95% con   dence band about the curve.
consider the following two approaches:

1. at each point x0, form a 95% con   dence interval for the linear func-

tion at    =p3

j=0   jxj
0.

2. form a 95% con   dence set for    as in (3.15), which in turn generates

con   dence intervals for f (x0).

how do these approaches di   er? which band is likely to be wider? conduct
a small simulation experiment to compare the two methods.

ex. 3.3 gauss   markov theorem:

(a) prove the gauss   markov theorem: the least squares estimate of a
parameter at    has variance no bigger than that of any other linear
unbiased estimate of at    (section 3.2.2).

(b) the matrix inequality b (cid:22) a holds if a     b is positive semide   nite.
show that if   v is the variance-covariance matrix of the least squares
estimate of    and   v is the variance-covariance matrix of any other
linear unbiased estimate, then   v (cid:22)   v.

exercises

95

ex. 3.4 show how the vector of least squares coe   cients can be obtained
from a single pass of the gram   schmidt procedure (algorithm 3.1). rep-
resent your solution in terms of the qr decomposition of x.

ex. 3.5 consider the ridge regression problem (3.41). show that this prob-
lem is equivalent to the problem

    c = argmin

  c ( nxi=1(cid:2)yi       c

0    

pxj=1

j(cid:3)2
(xij       xj)  c

+   

2).

  c
j

pxj=1

(3.85)

give the correspondence between   c and the original    in (3.41). char-
acterize the solution to this modi   ed criterion. show that a similar result
holds for the lasso.

ex. 3.6 show that the ridge regression estimate is the mean (and mode)
of the posterior distribution, under a gaussian prior        n (0,    i), and
gaussian sampling model y     n (x  ,   2i). find the relationship between
the id173 parameter    in the ridge formula, and the variances   
and   2.
ex. 3.7 assume yi     n (  0 + xt
i   ,   2), i = 1, 2, . . . , n , and the parameters
  j, j = 1, . . . , p are each distributed as n (0,    2), independently of one
another. assuming   2 and    2 are known, and   0 is not governed by a
prior (or has a    at improper prior), show that the (minus) log-posterior
j=1   2
j

density of    is proportional to pn

where    =   2/   2.
ex. 3.8 consider the qr decomposition of the uncentered n    (p + 1)
matrix x (whose    rst column is all ones), and the svd of the n    p
centered matrix   x. show that q2 and u span the same subspace, where
q2 is the sub-matrix of q with the    rst column removed. under what
circumstances will they be the same, up to sign    ips?

i=1(yi       0    pj xij  j)2 +   pp

ex. 3.9 forward stepwise regression. suppose we have the qr decomposi-
tion for the n   q matrix x1 in a multiple regression problem with response
y, and we have an additional p    q predictors in the matrix x2. denote the
current residual by r. we wish to establish which one of these additional
variables will reduce the residual-sum-of squares the most when included
with those in x1. describe an e   cient procedure for doing this.

ex. 3.10 backward stepwise regression. suppose we have the multiple re-
gression    t of y on x, along with the standard errors and z-scores as in
table 3.2. we wish to establish which variable, when dropped, will increase
the residual sum-of-squares the least. how would you do this?

ex. 3.11 show that the solution to the multivariate id75 prob-
lem (3.40) is given by (3.39). what happens if the covariance matrices   i
are di   erent for each observation?

96

3. linear methods for regression

ex. 3.12 show that the ridge regression estimates can be obtained by
ordinary least squares regression on an augmented data set. we augment
the centered matrix x with p additional rows      i, and augment y with p
zeros. by introducing arti   cial data having response value zero, the    tting
procedure is forced to shrink the coe   cients toward zero. this is related to
the idea of hints due to abu-mostafa (1995), where model constraints are
implemented by adding arti   cial data examples that satisfy them.

ex. 3.13 derive the expression (3.62), and show that     pcr(p) =     ls.

ex. 3.14 show that in the orthogonal case, pls stops after m = 1 steps,
because subsequent     mj in step 2 in algorithm 3.3 are zero.

ex. 3.15 verify expression (3.64), and hence show that the partial least
squares directions are a compromise between the ordinary regression coef-
   cient and the principal component directions.

ex. 3.16 derive the entries in table 3.4, the explicit forms for estimators
in the orthogonal case.

ex. 3.17 repeat the analysis of table 3.3 on the spam data discussed in
chapter 1.

ex. 3.18 read about conjugate gradient algorithms (murray et al., 1981, for
example), and establish a connection between these algorithms and partial
least squares.
ex. 3.19 show that k     ridgek increases as its tuning parameter        0. does
the same property hold for the lasso and partial least squares estimates?
for the latter, consider the    tuning parameter    to be the successive steps
in the algorithm.

ex. 3.20 consider the canonical-correlation problem (3.67). show that the
leading pair of canonical variates u1 and v1 solve the problem

max

ut (yt y)u=1
vt (xt x)v=1

ut (yt x)v,

(3.86)

a generalized svd problem. show that the solution is given by u1 =
(yt y)    1
1 are the leading left
and right singular vectors in

1, and v1 = (xt x)    1

1, where u   

1 and v   

2 u   

2 v   

(yt y)    1

2 (yt x)(xt x)    1

2 = u   d   v   t .

(3.87)

show that the entire sequence um, vm, m = 1, . . . , min(k, p) is also given
by (3.87).

ex. 3.21 show that the solution to the reduced-rank regression problem
(3.68), with    estimated by yt y/n , is given by (3.69). hint: transform

exercises

97

y to y    = y      1
that um =       1
2 u   

2 , and solved in terms of the canonical vectors u   
m, and a generalized inverse is u   
2 .

m = u   
m

t   

1

m. show

ex. 3.22 show that the solution in exercise 3.21 does not change if    is
estimated by the more natural quantity (y     x   b)t (y     x   b)/(n     pk).
ex. 3.23 consider a regression problem with all variables and response hav-
ing mean zero and standard deviation one. suppose also that each variable
has identical absolute correlation with the response:

1
n |hxj, yi| =   , j = 1, . . . , p.

let      be the least-squares coe   cient of y on x, and let u(  ) =   x      for
       [0, 1] be the vector that moves a fraction    toward the least squares    t
u. let rss be the residual sum-of-squares from the full least squares    t.

(a) show that

1
n |hxj, y     u(  )i| = (1       )  , j = 1, . . . , p,

and hence the correlations of each xj with the residuals remain equal
in magnitude as we progress toward u.

(b) show that these correlations are all equal to

  (  ) =

(1       )

q(1       )2 +   (2     )

n

   rss      ,

and hence they decrease monotonically to zero.

(c) use these results to show that the lar algorithm in section 3.4.4
keeps the correlations tied and monotonically decreasing, as claimed
in (3.55).

ex. 3.24 lar directions. using the notation around equation (3.55) on
page 74, show that the lar direction makes an equal angle with each of
the predictors in ak.
ex. 3.25 lar look-ahead (efron et al., 2004, sec. 2). starting at the be-
ginning of the kth step of the lar algorithm, derive expressions to identify
the next variable to enter the active set at step k + 1, and the value of    at
which this occurs (using the notation around equation (3.55) on page 74).

ex. 3.26 forward stepwise regression enters the variable at each step that
most reduces the residual sum-of-squares. lar adjusts variables that have
the most (absolute) correlation with the current residuals. show that these
two entry criteria are not necessarily the same. [hint: let xj.a be the jth

98

3. linear methods for regression

variable, linearly adjusted for all the variables currently in the model. show
that the    rst criterion amounts to identifying the j for which cor(xj.a, r)
is largest in magnitude.

ex. 3.27 lasso and lar: consider the lasso problem in lagrange multiplier
form: with l(  ) = 1

2pi(yi    pj xij  j)2, we minimize

(3.88)

l(  ) +   xj

|  j|

for    xed    > 0.

(a) setting   j =   +

l(  ) +   pj(  +

j          
j +      
l(  ) +   xj

j ,      

j with   +
j ). show that the lagrange dual function is

j     0, expression (3.88) becomes

(  +

j +      

j )    xj

j   +
  +

j    xj

j      
     

j

(3.89)

and the karush   kuhn   tucker optimality conditions are

   l(  )j +          +
      l(  )j +             
j   +
  +
     
j      

j = 0
j = 0
j = 0
j = 0,

along with the non-negativity constraints on the parameters and all
the lagrange multipliers.

(b) show that |   l(  )j|           j, and that the kkt conditions imply one

of the following three scenarios:

   = 0        l(  )j = 0    j

j > 0,    > 0       +
  +
j > 0,    > 0          
     

j = 0,    l(  )j =       < 0,      
j = 0,    l(  )j =    > 0,   +

j = 0
j = 0.

hence show that for any    active    predictor having   j 6= 0, we must
have    l(  )j =       if   j > 0, and    l(  )j =    if   j < 0. assuming
the predictors are standardized, relate    to the correlation between
the jth predictor and the current residuals.

(c) suppose that the set of active predictors is unchanged for   0              1.

show that there is a vector   0 such that

    (  ) =     (  0)     (         0)  0

(3.90)

thus the lasso solution path is linear as    ranges from   0 to   1(efron
et al., 2004; rosset and zhu, 2007).

exercises

99

ex. 3.28 suppose for a given t in (3.51), the    tted lasso coe   cient for
variable xj is     j = a. suppose we augment our set of variables with an
identical copy x    
j = xj. characterize the e   ect of this exact collinearity
by describing the set of solutions for     j and        
j , using the same value of t.

ex. 3.29 suppose we run a ridge regression with parameter    on a single
variable x, and get coe   cient a. we now include an exact copy x     = x,
and re   t our ridge regression. show that both coe   cients are identical, and
derive their value. show in general that if m copies of a variable xj are
included in a ridge regression, their coe   cients are all the same.

ex. 3.30 consider the elastic-net optimization problem:

min

   ||y     x  ||2 +   (cid:2)  ||  ||2

2 + (1       )||  ||1(cid:3).

(3.91)

show how one can turn this into a lasso problem, using an augmented
version of x and y.

100

3. linear methods for regression

4
linear methods for classi   cation

this is page 101
printer: opaque this

4.1

introduction

in this chapter we revisit the classi   cation problem and focus on linear
methods for classi   cation. since our predictor g(x) takes values in a dis-
crete set g, we can always divide the input space into a collection of regions
labeled according to the classi   cation. we saw in chapter 2 that the bound-
aries of these regions can be rough or smooth, depending on the prediction
function. for an important class of procedures, these decision boundaries
are linear; this is what we will mean by linear methods for classi   cation.

there are several di   erent ways in which linear decision boundaries can
be found. in chapter 2 we    t id75 models to the class indicator
variables, and classify to the largest    t. suppose there are k classes, for
convenience labeled 1, 2, . . . , k, and the    tted linear model for the kth
indicator response variable is   fk(x) =     k0 +     t
k x. the decision boundary
between class k and     is that set of points for which   fk(x) =   f   (x), that is,
the set {x : (     k0            0) + (     k            )t x = 0}, an a   ne set or hyperplane.1
since the same is true for any pair of classes, the input space is divided
into regions of constant classi   cation, with piecewise hyperplanar decision
boundaries. this regression approach is a member of a class of methods
that model discriminant functions   k(x) for each class, and then classify x
to the class with the largest value for its discriminant function. methods

1strictly speaking, a hyperplane passes through the origin, while an a   ne set need

not. we sometimes ignore the distinction and refer in general to hyperplanes.

102

4. linear methods for classi   cation

that model the posterior probabilities pr(g = k|x = x) are also in this
class. clearly, if either the   k(x) or pr(g = k|x = x) are linear in x, then
the decision boundaries will be linear.
actually, all we require is that some monotone transformation of   k or
pr(g = k|x = x) be linear for the decision boundaries to be linear. for
example, if there are two classes, a popular model for the posterior proba-
bilities is

pr(g = 1|x = x) =

pr(g = 2|x = x) =

exp(  0 +   t x)

1 + exp(  0 +   t x)

1

1 + exp(  0 +   t x)

,

.

(4.1)

here the monotone transformation is the logit transformation: log[p/(1   p)],
and in fact we see that

log

pr(g = 1|x = x)
pr(g = 2|x = x)

=   0 +   t x.

(4.2)

the decision boundary is the set of points for which the log-odds are zero,

and this is a hyperplane de   ned by(cid:8)x|  0 +   t x = 0(cid:9). we discuss two very

popular but di   erent methods that result in linear log-odds or logits: linear
discriminant analysis and linear id28. although they di   er in
their derivation, the essential di   erence between them is in the way the
linear function is    t to the training data.

a more direct approach is to explicitly model the boundaries between
the classes as linear. for a two-class problem in a p-dimensional input
space, this amounts to modeling the decision boundary as a hyperplane   in
other words, a normal vector and a cut-point. we will look at two methods
that explicitly look for    separating hyperplanes.    the    rst is the well-
known id88 model of rosenblatt (1958), with an algorithm that    nds
a separating hyperplane in the training data, if one exists. the second
method, due to vapnik (1996),    nds an optimally separating hyperplane if
one exists, else    nds a hyperplane that minimizes some measure of overlap
in the training data. we treat the separable case here, and defer treatment
of the nonseparable case to chapter 12.

while this entire chapter is devoted to linear decision boundaries, there is
considerable scope for generalization. for example, we can expand our vari-
able set x1, . . . , xp by including their squares and cross-products x 2
x1x2, . . ., thereby adding p(p + 1)/2 additional variables. linear functions
in the augmented space map down to quadratic functions in the original
space   hence linear decision boundaries to quadratic decision boundaries.
figure 4.1 illustrates the idea. the data are the same: the left plot uses
linear decision boundaries in the two-dimensional space shown, while the
right plot uses linear decision boundaries in the augmented    ve-dimensional
space described above. this approach can be used with any basis transfor-

1 , x 2

2 , . . . ,

4.2 id75 of an indicator matrix

103

1

1

2

1
1

1

2
2

2

1

1

2
2

1
2

2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
22
2
2
2
2
1
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
22
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
2
1
2
1
2
2
2
1
2
2
2
2
2
2
1
2
2
2
2
1
2
1
2
2
2
2
2
2
1
2
2
2
1
1
2
1
2
1
2
2
2
1
2
1
1
2
1
2
2
2
1
1
1
1
2
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1

2
2

1

1

1

1

1

2

2

1
1

2

2
2
2
2
2
2
2
2
2

2
2

1
1

1

1

1

3

3
3
3
3
3

3

1

3
1
3
3
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
3

1

1
1

1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

3
3
3
3

3

3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
1
3
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
1
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
3
3 3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
1
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

2

1
1

1

2
2

2

1

1

2
2

1
2

2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
22
2
2
2
2
1
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
22
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
2
1
2
1
2
2
2
1
2
2
2
2
2
2
1
2
2
2
2
1
2
1
2
2
2
2
2
2
1
2
2
2
1
1
2
1
2
1
2
2
2
1
2
1
1
2
1
2
2
2
1
1
1
1
2
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1

2
2

1

1

1

1

1

2

2

1
1

2

2
2
2
2
2
2
2
2
2

2
2

1
1

1

1

1

3

3
3
3
3
3

3

1

3
1
3
3
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
3

1

1
1

1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

3
3
3
3

3

3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
1
3
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
1
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
3
3 3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
1
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

figure 4.1. the left plot shows some data from three classes, with linear
decision boundaries found by id156. the right plot shows
quadratic decision boundaries. these were obtained by    nding linear boundaries
in the    ve-dimensional space x1, x2, x1x2, x 2
1 , x 2
2 . linear inequalities in this
space are quadratic inequalities in the original space.

mation h(x) where h : irp 7    irq with q > p, and will be explored in later
chapters.

4.2 id75 of an indicator matrix

here each of the response categories are coded via an indicator variable.
thus if g has k classes, there will be k such indicators yk, k = 1, . . . , k,
with yk = 1 if g = k else 0. these are collected together in a vector
y = (y1, . . . , yk ), and the n training instances of these form an n    k
indicator response matrix y. y is a matrix of 0   s and 1   s, with each row
having a single 1. we    t a id75 model to each of the columns
of y simultaneously, and the    t is given by

  y = x(xt x)   1xt y.

(4.3)

chapter 3 has more details on id75. note that we have a coe   -
cient vector for each response column yk, and hence a (p+1)  k coe   cient
matrix   b = (xt x)   1xt y. here x is the model matrix with p+1 columns
corresponding to the p inputs, and a leading column of 1   s for the intercept.

a new observation with input x is classi   ed as follows:
    compute the    tted output   f (x)t = (1, xt )   b, a k vector;
    identify the largest component and classify accordingly:

  g(x) = argmaxk   g

  fk(x).

(4.4)

104

4. linear methods for classi   cation

it is quite straightforward to verify that pk   g

what is the rationale for this approach? one rather formal justi   cation
is to view the regression as an estimate of conditional expectation. for the
random variable yk, e(yk|x = x) = pr(g = k|x = x), so conditional
expectation of each of the yk seems a sensible goal. the real issue is: how
good an approximation to conditional expectation is the rather rigid linear
regression model? alternatively, are the   fk(x) reasonable estimates of the
posterior probabilities pr(g = k|x = x), and more importantly, does this
matter?
  fk(x) = 1 for any x, as
long as there is an intercept in the model (column of 1   s in x). however,
the   fk(x) can be negative or greater than 1, and typically some are. this
is a consequence of the rigid nature of id75, especially if we
make predictions outside the hull of the training data. these violations in
themselves do not guarantee that this approach will not work, and in fact
on many problems it gives similar results to more standard linear meth-
ods for classi   cation. if we allow id75 onto basis expansions
h(x) of the inputs, this approach can lead to consistent estimates of the
probabilities. as the size of the training set n grows bigger, we adaptively
include more basis elements so that id75 onto these basis func-
tions approaches conditional expectation. we discuss such approaches in
chapter 5.

a more simplistic viewpoint is to construct targets tk for each class,
where tk is the kth column of the k    k identity matrix. our prediction
problem is to try and reproduce the appropriate target for an observation.
with the same coding as before, the response vector yi (ith row of y) for
observation i has the value yi = tk if gi = k. we might then    t the linear
model by least squares:

min

b

nxi=1

||yi     [(1, xt

i )b]t||2.

(4.5)

the criterion is a sum-of-squared euclidean distances of the    tted vectors
from their targets. a new observation is classi   ed by computing its    tted
vector   f (x) and classifying to the closest target:

  g(x) = argmin

k

||   f (x)     tk||2.

(4.6)

this is exactly the same as the previous approach:

    the sum-of-squared-norm criterion is exactly the criterion for multi-
ple response id75, just viewed slightly di   erently. since
a squared norm is itself a sum of squares, the components decouple
and can be rearranged as a separate linear model for each element.
note that this is only possible because there is nothing in the model
that binds the di   erent responses together.

4.2 id75 of an indicator matrix

105

id75

id156

3

3

3

3

3

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
333
3
3
3
3
33
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
33
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3
3

3
3

3

3

3

3

3

2

3

2

2

2

2
2

3
2
2

2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2 2
2
2
2
2
22
2
2
2
2 2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2 2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

1

1

2

2

2
2
2

2
2
1

3

3

2
2
2
2
2

2

2

2
2

2

x

1

1

1

1

1

1

1

1

1

1

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
111
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1

1

1

1

1

1
1
1
1
1
1

1

1

1

1

3

3

3

3

3

2

x

1

1

1

x1

1

1

1

1

1

1

1

1

1

1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
111
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1

1

1

1

1
1
1
1
1
1

1

1

1

1

1

x1

3

3

3

3

3

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
333
3
3
3
3
33
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
33
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3
3

3
3

3

3

3

3

3

3

3

3

3

3

2

3

2

2

2

2
2

3
2
2

2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2 2
2
2
2
2
22
2
2
2
2 2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2 2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2

1

1

2

2

2
2
2

2
2
1

3

3

2
2
2
2
2

2

2

2
2

figure 4.2. the data come from three classes in ir2 and are easily separated
by linear decision boundaries. the right plot shows the boundaries found by linear
discriminant analysis. the left plot shows the boundaries found by linear regres-
sion of the indicator response variables. the middle class is completely masked
(never dominates).

    the closest target classi   cation rule (4.6) is easily seen to be exactly

the same as the maximum    tted component criterion (4.4).

there is a serious problem with the regression approach when the number
of classes k     3, especially prevalent when k is large. because of the rigid
nature of the regression model, classes can be masked by others. figure 4.2
illustrates an extreme situation when k = 3. the three classes are perfectly
separated by linear decision boundaries, yet id75 misses the
middle class completely.

in figure 4.3 we have projected the data onto the line joining the three
centroids (there is no information in the orthogonal direction in this case),
and we have included and coded the three response variables y1, y2 and
y3. the three regression lines (left panel) are included, and we see that
the line corresponding to the middle class is horizontal and its    tted values
are never dominant! thus, observations from class 2 are classi   ed either
as class 1 or class 3. the right panel uses quadratic regression rather than
id75. for this simple example a quadratic rather than linear
   t (for the middle class at least) would solve the problem. however, it
can be seen that if there were four rather than three classes lined up like
this, a quadratic would not come down fast enough, and a cubic would
be needed as well. a loose but general rule is that if k     3 classes are
lined up, polynomial terms up to degree k     1 might be needed to resolve
them. note also that these are polynomials along the derived direction
passing through the centroids, which can have arbitrary orientation. so in

106

4. linear methods for classi   cation

degree = 1; error = 0.33

degree = 2; error = 0.04

1

1.0

1
1
1
1
1
1
1
1
1

1
11
1

1

1
11
1
11
1
1
1
1
1
1
1
1
1
11
1
1
11
1
1
1
1
1
1
1
11
1
1

1
1

0.5

0.0

2

22
2
2
2
2
2
2
2

2
2
22

2
222
2
22
2 22 2
2
222
22 222
2 2
2
22 2 22
2
2
2 2
2
2
2

3
3

33
3
3
3
3
3
3
33
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
33
3
33
3

3

3
33
3

3
3
3
3
3
3
3
3
3

3

1

2

3

1
1
1
1
1
33
11
1
1
3
3
3
3
1
33
11
3
333
1
1
3
3
1
11
3
3
1
33
333
11
33
111
1
22
1
2
2
22 2
2222 2
1
2222 22
22
222
2
2
2
222
22
22
2
22
3
2
22
3
2
2
2
2
2
3
11
3
33
1
1
3
111
33
1
3
11
1
3
1
3
1
1
1
1
3
33
11
3
3
3
3
3
3

3
3

3
3
3
3
3
33
3
33
3
3
3
3
3
3
3
333
3
3
33
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3
3
3

3
3

3

3
3
3
3
3
3

2 2
2
2
2
2
2

2
2

2
2
2

2
2
2 2
2
2
2
22
2
2 2
2 2
2
2
22
22
222
2
22
2
2
2 22
22
22
2
2
2 2
2
2

2
2

1
1
1
1
1
1

1

1
1

1
1
1

1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
11
111
1
1
1
1
1
1
1
11
1
1
1
1
11
1
1
1
1
1

1
1

1.0

0.5

0.0

1

3

2

1
1
1
1
1
1
1
1
1

1
11
1

1

1
11
1
11
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
11
1
1
1
1
2
2
22
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
22
2

2
3 3
3
3
33 333
33 3 33
3
3 3
3 33 3
3
333
3
3
333
33
3
3
3

3
33
3
2
22
2

3
3
3
3
3
3
3
3
3

2
2
2
2
2
2
2
2
2

22
222
22
2222 22
222
2222 2
22 2
22
2
2
22
2
2
2
2
22
22
2
2
2
2
2
2

1
1
1
1
1
11
1
33
1
3
1
3
3
3
11
1
1 1
33
3
1
11
333
3
1
3
3
11
333
111
11
1
333
33
33
111
3 3
3
111
1
1
11
1
3
33
1
3
33
1
3
1
1
11

3
33
3
3
3
3
3
3

2

1

3

2
2
2
2
2
2

3
3
3
3
3
3

2
2
3
3

3
3
3
2
2
2

2

3

1 1
1
1
1
1
1

1
1

1
1
1

3
3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
1
1
1 1
2
1
2
1 1
1
11
1
1
1 1
1
2
11
111
11
1 11
1
11
1 1
1
11
1
1
11
1
1
1
2
2
2
2
2
2
2

1
1

2
2

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

figure 4.3. the e   ects of masking on id75 in ir for a three-class
problem. the rug plot at the base indicates the positions and class membership of
each observation. the three curves in each panel are the    tted regressions to the
three-class indicator variables; for example, for the blue class, yblue is 1 for the
blue observations, and 0 for the green and orange. the    ts are linear and quadratic
polynomials. above each plot is the training error rate. the bayes error rate is
0.025 for this problem, as is the lda error rate.

p-dimensional input space, one would need general polynomial terms and
cross-products of total degree k     1, o(pk   1) terms in all, to resolve such
worst-case scenarios.
the example is extreme, but for large k and small p such maskings
naturally occur. as a more realistic illustration, figure 4.4 is a projection
of the training data for a vowel recognition problem onto an informative
two-dimensional subspace. there are k = 11 classes in p = 10 dimensions.
this is a di   cult classi   cation problem, and the best methods achieve
around 40% errors on the test data. the main point here is summarized in
table 4.1; id75 has an error rate of 67%, while a close relative,
id156, has an error rate of 56%. it seems that masking
has hurt in this case. while all the other methods in this chapter are based
on linear functions of x as well, they use them in such a way that avoids
this masking problem.

4.3 id156

decision theory for classi   cation (section 2.4) tells us that we need to know
the class posteriors pr(g|x) for optimal classi   cation. suppose fk(x) is
the class-conditional density of x in class g = k, and let   k be the prior
k=1   k = 1. a simple application of bayes

id203 of class k, with pk

4.3 id156

107

id156

oooooo

oooooo

o

ooo
o

      

ooo
o
o
ooo
oooooo
o oooo
ooooo o
oo
o o
o
o
o
o
o
o
o
ooo
o
o
o o
o o
ooo
o
o
o
o
o
oo
o
oooo
o
ooo
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
oo
o
o
oo
oo
o
o
o
oooo
o
o
o
o
o
o oo
o
o
o
o o
o
o
ooo
ooo
o
o
o
o
o
oo
o
oo
o
o oo
ooo ooo
o
o
o
o
o
ooo
ooo
oooo
o
o
o
o
o
o
o
ooo
o
o

      
      

      

o

o

o

o

o

o

o

o

o oooo
o
o
oooooo
o
o
o
o
o o
o
o
o
o
oo

o

o

o

o
oo
o
o

oo
o

o

o
o
o

o
o

ooooo o

o

o

      
o
o
o
oo
o
o
o
oooo
o
o
o
oo
o o
ooo
o
o
oo
oo
o
o
o
o
o
o
o

      

o

o

o
o
o
o
oo

o

ooo
o o
o oo
o
o

o

o
o
o

o

      

o oo
o o
o
oo
o
o
o
o
o o
o
oo o oo
o
oo
o
oo
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
ooo

o o o

oo
oo oo
o
ooooo

o

ooooo

o
ooooo
o

      

      

ooooo
o
oooooo
o
oooo
o
oo
oooo
o
o
o
o
ooo
o
o
oo
ooo
o
o ooo
oo
oo
o
ooooo
o
o
o
o
o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o

oo
o
o
o
o

      
      

o

o
oooo
o

4

2

0

2
-

4
-

6
-

i

i

a
t
a
d
 
g
n
n
a
r
t
 
r
o
f
 
2
 
e
t
a
n
d
r
o
o
c

i

oo
ooo o

o

o o
o

oo
ooo

o

o

o
o

o o

o
o

o
o
oo o
oo
o
o
oo
oo
oo o

o
o oo
o

oo

o

o

o o o
o
o

oo

o

o

o

o

o
o

o

o

0

coordinate 1 for training data

2

4

-4

-2

figure 4.4. a two-dimensional plot of the vowel training data. there are
eleven classes with x     ir10, and this is the best view in terms of a lda model
(section 4.3.3). the heavy circles are the projected mean vectors for each class.
the class overlap is considerable.

table 4.1. training and test error rates using a variety of linear techniques
on the vowel data. there are eleven classes in ten dimensions, of which three
account for 90% of the variance (via a principal components analysis). we see
that id75 is hurt by masking, increasing the test and training error
by over 10%.

technique

error rates

id75
id156
quadratic discriminant analysis
id28

training test
0.67
0.48
0.56
0.32
0.53
0.01
0.22
0.51

108

4. linear methods for classi   cation

theorem gives us

pr(g = k|x = x) =

.

(4.7)

fk(x)  k
   =1 f   (x)     

pk

we see that in terms of ability to classify, having the fk(x) is almost equiv-
alent to having the quantity pr(g = k|x = x).

many techniques are based on models for the class densities:

    linear and quadratic discriminant analysis use gaussian densities;
    more    exible mixtures of gaussians allow for nonlinear decision bound-

aries (section 6.8);

    general nonparametric density estimates for each class density allow

the most    exibility (section 6.6.2);

    naive bayes models are a variant of the previous case, and assume
that each of the class densities are products of marginal densities;
that is, they assume that the inputs are conditionally independent in
each class (section 6.6.3).

suppose that we model each class density as multivariate gaussian

fk(x) =

1

(2  )p/2|  k|1/2 e    1

2 (x     k)t      1

k (x     k).

(4.8)

id156 (lda) arises in the special case when we
assume that the classes have a common covariance matrix   k =       k. in
comparing two classes k and    , it is su   cient to look at the log-ratio, and
we see that

log

pr(g = k|x = x)
pr(g =    |x = x)

= log

= log

+ log

  k
     

fk(x)
f   (x)
  k
         
+ xt      1(  k          ),

1
2

(  k +      )t      1(  k          )

(4.9)

an equation linear in x. the equal covariance matrices cause the normal-
ization factors to cancel, as well as the quadratic part in the exponents.
this linear log-odds function implies that the decision boundary between
classes k and       the set where pr(g = k|x = x) = pr(g =    |x = x)   is
linear in x; in p dimensions a hyperplane. this is of course true for any pair
of classes, so all the decision boundaries are linear. if we divide irp into
regions that are classi   ed as class 1, class 2, etc., these regions will be sep-
arated by hyperplanes. figure 4.5 (left panel) shows an idealized example
with three classes and p = 2. here the data do arise from three gaus-
sian distributions with a common covariance matrix. we have included in

4.3 id156

109

+

+

+

3
1

1

3
3

1
1
1

1
3
3
3
3
3
1
3
3
3
1
2
2
1 2
2
3
2
1

2

2

3
3
2

3

2
2

2
2
2
2
2

3

2

2

2

2 2

2

2

2

3
1
3

3

1

3
3
3
1

3

1
3
3
3
3
1
1
2
1
3
1
1
3
1
1
3
1
1
1
1
1
1
1

2
1

1

2
2

2

2
2

figure 4.5. the left panel shows three gaussian distributions, with the same
covariance and di   erent means. included are the contours of constant density
enclosing 95% of the id203 in each case. the bayes decision boundaries
between each pair of classes are shown (broken straight lines), and the bayes
decision boundaries separating all three classes are the thicker solid lines (a subset
of the former). on the right we see a sample of 30 drawn from each gaussian
distribution, and the    tted lda decision boundaries.

the    gure the contours corresponding to 95% highest id203 density,
as well as the class centroids. notice that the decision boundaries are not
the perpendicular bisectors of the line segments joining the centroids. this
would be the case if the covariance    were spherical   2i, and the class
priors were equal. from (4.9) we see that the linear discriminant functions

  k(x) = xt      1  k    

1
2

k      1  k + log   k
  t

(4.10)

are an equivalent description of the decision rule, with g(x) = argmaxk  k(x).
in practice we do not know the parameters of the gaussian distributions,

and will need to estimate them using our training data:

        k = nk/n , where nk is the number of class-k observations;

        k =pgi=k xi/nk;
         =pk

k=1pgi=k(xi         k)(xi         k)t /(n     k).

figure 4.5 (right panel) shows the estimated decision boundaries based on
a sample of size 30 each from three gaussian distributions. figure 4.1 on
page 103 is another example, but here the classes are not gaussian.

with two classes there is a simple correspondence between linear dis-
criminant analysis and classi   cation by id75, as in (4.5). the
lda rule classi   es to class 2 if

   1

xt     

(    2         1) >

1
2

(    2 +     1)t     

   1

(    2         1)     log(n2/n1),

(4.11)

110

4. linear methods for classi   cation

and class 1 otherwise. suppose we code the targets in the two classes as +1
and    1, respectively. it is easy to show that the coe   cient vector from least
squares is proportional to the lda direction given in (4.11) (exercise 4.2).
[in fact, this correspondence occurs for any (distinct) coding of the targets;
see exercise 4.2]. however unless n1 = n2 the intercepts are di   erent and
hence the resulting decision rules are di   erent.

since this derivation of the lda direction via least squares does not use a
gaussian assumption for the features, its applicability extends beyond the
realm of gaussian data. however the derivation of the particular intercept
or cut-point given in (4.11) does require gaussian data. thus it makes
sense to instead choose the cut-point that empirically minimizes training
error for a given dataset. this is something we have found to work well in
practice, but have not seen it mentioned in the literature.

with more than two classes, lda is not the same as id75 of
the class indicator matrix, and it avoids the masking problems associated
with that approach (hastie et al., 1994). a correspondence between regres-
sion and lda can be established through the notion of optimal scoring,
discussed in section 12.5.

getting back to the general discriminant problem (4.8), if the   k are
not assumed to be equal, then the convenient cancellations in (4.9) do not
occur; in particular the pieces quadratic in x remain. we then get quadratic
discriminant functions (qda),

  k(x) =    

1
2

log |  k|    

1
2

(x       k)t      1

k (x       k) + log   k.

(4.12)

the decision boundary between each pair of classes k and     is described by
a quadratic equation {x :   k(x) =      (x)}.
figure 4.6 shows an example (from figure 4.1 on page 103) where the
three classes are gaussian mixtures (section 6.8) and the decision bound-
aries are approximated by quadratic equations in x. here we illustrate
two popular ways of    tting these quadratic boundaries. the right plot
uses qda as described here, while the left plot uses lda in the enlarged
   ve-dimensional quadratic polynomial space. the di   erences are generally
small; qda is the preferred approach, with the lda method a convenient
substitute 2.

the estimates for qda are similar to those for lda, except that separate
covariance matrices must be estimated for each class. when p is large this
can mean a dramatic increase in parameters. since the decision boundaries
are functions of the parameters of the densities, counting the number of
parameters must be done with care. for lda, it seems there are (k    
1)    (p + 1) parameters, since we only need the di   erences   k(x)       k(x)

2for this    gure and many similar    gures in the book we compute the decision bound-
aries by an exhaustive contouring method. we compute the decision rule on a    ne lattice
of points, and then use contouring algorithms to compute the boundaries.

4.3 id156

111

1

1

2

1
1

1

2
2

2

1

1

2
2

1
2

2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
22
2
2
2
2
1
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
22
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
2
1
2
1
2
2
2
1
2
2
2
2
2
2
1
2
2
2
2
1
2
1
2
2
2
2
2
2
1
2
2
2
1
1
2
1
2
1
2
2
2
1
2
1
1
2
1
2
2
2
1
1
1
1
2
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1

2
2

1

1

1

1

1

2

2

1
1

2

2
2
2
2
2
2
2
2
2

2
2

1
1

1

1

1

3

3
3
3
3
3

3

1

3
1
3
3
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
3

1

1
1

1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

3
3
3
3

3

3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
1
3
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
1
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
3
3 3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
1
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

2

1
1

1

2
2

2

1

1

2
2

1
2

2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
2
2
2
22
2
2
2
2
1
2
2
2
2
2
2
2
2
2
2
2
2
2 2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2
2
2
22
2
2
2
2
22
2
2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2 2
1
2
2
2
2
2
2
2
2
2
2
2
2
2
2
1
2
1
2
1
2
2
2
1
2
2
2
2
2
2
1
2
2
2
2
1
2
1
2
2
2
2
2
2
1
2
2
2
1
1
2
1
2
1
2
2
2
1
2
1
1
2
1
2
2
2
1
1
1
1
2
1
1
1
1
1
2
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

1
1

2
2

1

1

1

1

1

2

2

1
1

2

2
2
2
2
2
2
2
2
2

2
2

1
1

1

1

1

3

3
3
3
3
3

3

1

3
1
3
3
1
1
1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
3
1
1
3

1

1
1

1

1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1

3
3
3
3

3

3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
1
3
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
1
3
3
3
3
3
3
1
3
3
3
3
3
3
3
3
3
3 3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
1
1
3
3
3
3
3
3
3
3
3
3
3
3
3
3 3
3
3
3
3
3
3
1
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
33
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

figure 4.6. two methods for    tting quadratic boundaries. the left plot shows
the quadratic decision boundaries for the data in figure 4.1 (obtained using lda
in the    ve-dimensional space x1, x2, x1x2, x 2
2 ). the right plot shows the
quadratic decision boundaries found by qda. the di   erences are small, as is
usually the case.

1 , x 2

between the discriminant functions where k is some pre-chosen class (here
we have chosen the last), and each di   erence requires p + 1 parameters3.
likewise for qda there will be (k     1)    {p(p + 3)/2 + 1} parameters.
both lda and qda perform well on an amazingly large and diverse set
of classi   cation tasks. for example, in the statlog project (michie et
al., 1994) lda was among the top three classi   ers for 7 of the 22 datasets,
qda among the top three for four datasets, and one of the pair were in the
top three for 10 datasets. both techniques are widely used, and entire books
are devoted to lda. it seems that whatever exotic tools are the rage of the
day, we should always have available these two simple tools. the question
arises why lda and qda have such a good track record. the reason is not
likely to be that the data are approximately gaussian, and in addition for
lda that the covariances are approximately equal. more likely a reason is
that the data can only support simple decision boundaries such as linear or
quadratic, and the estimates provided via the gaussian models are stable.
this is a bias variance tradeo      we can put up with the bias of a linear
decision boundary because it can be estimated with much lower variance
than more exotic alternatives. this argument is less believable for qda,
since it can have many parameters itself, although perhaps fewer than the
non-parametric alternatives.

3although we    t the covariance matrix      to compute the lda discriminant functions,
a much reduced function of it is all that is required to estimate the o(p) parameters
needed to compute the decision boundaries.

112

4. linear methods for classi   cation

e
t
a
r
 
n
o
i
t
a
c
i
f
i
s
s
a
c
s
m

i

l

regularized discriminant analysis on the vowel data

                                                                                                                                                                                                   

   

   

                                                                                                                                                                                                   

test data
train data

0.0

0.2

0.4

0.6

0.8

1.0

  

5
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

figure 4.7. test and training errors for the vowel data, using regularized
discriminant analysis with a series of values of        [0, 1]. the optimum for the
test data occurs around    = 0.9, close to quadratic discriminant analysis.

4.3.1 regularized discriminant analysis

friedman (1989) proposed a compromise between lda and qda, which
allows one to shrink the separate covariances of qda toward a common
covariance as in lda. these methods are very similar in    avor to ridge
regression. the regularized covariance matrices have the form

    k(  ) =        k + (1       )     ,

(4.13)

where      is the pooled covariance matrix as used in lda. here        [0, 1]
allows a continuum of models between lda and qda, and needs to be
speci   ed. in practice    can be chosen based on the performance of the
model on validation data, or by cross-validation.

figure 4.7 shows the results of rda applied to the vowel data. both
the training and test error improve with increasing   , although the test
error increases sharply after    = 0.9. the large discrepancy between the
training and test error is partly due to the fact that there are many repeat
measurements on a small number of individuals, di   erent in the training
and test set.

similar modi   cations allow      itself to be shrunk toward the scalar

covariance,

    (  ) =         + (1       )    2i

(4.14)
for        [0, 1]. replacing      in (4.13) by     (  ) leads to a more general family
of covariances     (  ,   ) indexed by a pair of parameters.
in chapter 12, we discuss other regularized versions of lda, which are
more suitable when the data arise from digitized analog signals and images.

4.3 id156

113

in these situations the features are high-dimensional and correlated, and the
lda coe   cients can be regularized to be smooth or sparse in the original
domain of the signal. this leads to better generalization and allows for
easier interpretation of the coe   cients. in chapter 18 we also deal with
very high-dimensional problems, where for example the features are gene-
expression measurements in microarray studies. there the methods focus
on the case    = 0 in (4.14), and other severely regularized versions of lda.

4.3.2 computations for lda

as a lead-in to the next topic, we brie   y digress on the computations
required for lda and especially qda. their computations are simpli   ed
by diagonalizing      or     k. for the latter, suppose we compute the eigen-
decomposition for each     k = ukdkut
k , where uk is p    p orthonormal,
and dk a diagonal matrix of positive eigenvalues dk   . then the ingredients
for   k(x) (4.12) are

k (x         k) = [ut

k (x         k)]t d   1

k [ut

k (x         k)];

   1

    (x         k)t     
    log |     k| =p    log dk   .

in light of the computational steps outlined above, the lda classi   er

can be implemented by the following pair of steps:

    sphere the data with respect to the common covariance estimate     :
2 ut x, where      = udut . the common covariance esti-

x         d    1
mate of x     will now be the identity.

    classify to the closest class centroid in the transformed space, modulo

the e   ect of the class prior probabilities   k.

4.3.3 reduced-rank id156

so far we have discussed lda as a restricted gaussian classi   er. part of
its popularity is due to an additional restriction that allows us to view
informative low-dimensional projections of the data.

the k centroids in p-dimensional input space lie in an a   ne subspace
of dimension     k     1, and if p is much larger than k, this will be a con-
siderable drop in dimension. moreover, in locating the closest centroid, we
can ignore distances orthogonal to this subspace, since they will contribute
equally to each class. thus we might just as well project the x     onto this
centroid-spanning subspace hk   1, and make distance comparisons there.
thus there is a fundamental dimension reduction in lda, namely, that we
need only consider the data in a subspace of dimension at most k     1.

114

4. linear methods for classi   cation

if k = 3, for instance, this could allow us to view the data in a two-
dimensional plot, color-coding the classes. in doing so we would not have
relinquished any of the information needed for lda classi   cation.

what if k > 3? we might then ask for a l < k   1 dimensional subspace
hl     hk   1 optimal for lda in some sense. fisher de   ned optimal to
mean that the projected centroids were spread out as much as possible in
terms of variance. this amounts to    nding principal component subspaces
of the centroids themselves (principal components are described brie   y in
section 3.5.1, and in more detail in section 14.5.1). figure 4.4 shows such an
optimal two-dimensional subspace for the vowel data. here there are eleven
classes, each a di   erent vowel sound, in a ten-dimensional input space. the
centroids require the full space in this case, since k     1 = p, but we have
shown an optimal two-dimensional subspace. the dimensions are ordered,
so we can compute additional dimensions in sequence. figure 4.8 shows four
additional pairs of coordinates, also known as canonical or discriminant
variables.
in summary then,    nding the sequences of optimal subspaces
for lda involves the following steps:

    compute the k    p matrix of class centroids m and the common

covariance matrix w (for within-class covariance);

2 using the eigen-decomposition of w;

    compute m    = mw    1
    compute b   , the covariance matrix of m    (b for between-class covari-
ance), and its eigen-decomposition b    = v   dbv   t . the columns
v   
    of v    in sequence from    rst to last de   ne the coordinates of the
optimal subspaces.

combining all these operations the    th discriminant variable is given by
z    = vt

    x with v    = w    1

2 v   
    .

fisher arrived at this decomposition via a di   erent route, without refer-

ring to gaussian distributions at all. he posed the problem:

find the linear combination z = at x such that the between-
class variance is maximized relative to the within-class variance.

again, the between class variance is the variance of the class means of
z, and the within class variance is the pooled variance about the means.
figure 4.9 shows why this criterion makes sense. although the direction
joining the centroids separates the means as much as possible (i.e., max-
imizes the between-class variance), there is considerable overlap between
the projected classes due to the nature of the covariances. by taking the
covariance into account as well, a direction with minimum overlap can be
found.

the between-class variance of z is at ba and the within-class variance
at wa, where w is de   ned earlier, and b is the covariance matrix of the
class centroid matrix m. note that b + w = t, where t is the total
covariance matrix of x, ignoring class information.

4.3 id156

115

id156

o

o

o

o o
o

o

o

o

oo o

o

o

o
o

o

o o
o
o

o

o
o

o

o

o

o

o

o

o

o

o

oo
o
o
o
o
o
o
o
oo
o
oo
o
oo
o
o
o
oo
o
o
o
o o o
o
ooooo
oo oo
o
o
o
o
o
o
o
o
o
oooo
o
o
o
oo
o oooo
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
ooo
o
o
oo
o
o oo
oo
o
oo
o
ooo
o
oo
o
oo
o
o
o
ooo
o
ooo
o
o
o
o
oo
oo o
o
o
o
oo
oo
oooo
o
oo
o
oo o
o
o
oo
o o o
ooo
oo
o
o
o
o
o
o
ooo
o
o
oo
o
o
o
o
o
o
o
o
oo
o o
o
o
oo
o
o
o
ooo
o
o
o
o
oooo
o
ooo
o o
o
ooo
o
o
o
o
oooo
ooo
o o
o
oo
oo
o
o
o o
oo
o
oo
o
oo
o
o o
o
o
oo
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o o
o
o
o
oo
o
o
oo
o
o
ooo
o o
ooo
o
o
o
o
o
o
o
o
oo
o
o
oo
ooo
oo
oo
o o
o
o
o
o
o
o
oo
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
o
o
oooo
o o
oo
o
oo
o
o
ooo
o
oooo
o
oo
o
oo
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
oo oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
oo
oo
oooo
o
o
o
oo
o
o
o o o
o
o
o
oo
o
o
o
ooooo
o
oo
o
oo
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o

                   
      

                  
      

      
            

o
o
o
o
o
o

o
ooo

o

o

o

o

o

o

o

o

o

o

o

o

o
ooo
o
o

o

ooo

o

o

o

o

-4

-2

0

2

4

coordinate 1 

oo
o
o o
o

o
o

o

o

oo
oo
o
o
ooooo o

o
o
o

o

o

o
o

o
o
o
o
o

ooo
o
o
o
o
ooo
o
o
o
o
o
o
o
ooo
ooo
o
o
o o
o
oo
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
oo
o
ooo
o
o
o
oo
o
o
o
oo
o oo
o
ooo
o
oooo
oo
o
o
o
ooo o o
ooo
o o
oo
oooooo
o
o
o
o
o
ooo
o
o
oo
oooo
oo
o
o
o
oo
o
o
oo
o
o
o
o
o
oo
o
oooo
o
oo oo
o
o
o
ooo
o
o
oo
oo
oo
o
o o oo
o
o
ooo
o o
o
o
o
o o
o
oo
o
oo o
oo
o
o
o
o
o
oo
ooo
o
ooo
o
o
o
ooo
oo
o
o
oo
o
o
o
o
o
o
oo
o
oo
o
ooo
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oooooo
o
oo
o ooo
ooo
o
oooo
o
o
o
o
oo
oooo
o
o o oo
o
oo
o o
o
oo
ooo
o
o
o
o
o
oo
o
ooooo
o
o
o
oo
o
o o o
oooo
o
oo
o
o o
oo
o
o
o
o
o
o
oo
oo
o
o
oooo
oo
oo
o
o o
o oo
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
oo oo
o
o
o o
o
o oo
oo
o
o
o
oo
ooo
ooo
o
o
o
oo
o
ooo
o
oo
oo
o
o
ooo
o
o
ooo
o o o oooo
ooo
oo
o
o
o
oo
o
oo
o
o
oooo
o
o
o
oo
o
o
o
o
o
oo
o

                   
      

      
            

                        

o
o
oo

o o

o
o

o

o

o

o

oo
oo

o o
o
o

o

o

 
3
 
e
t
a
n
d
r
o
o
c

i

 

 

t

7
e
a
n
d
r
o
o
c

i

2

0

2
-

3

2

1

0

1
-

2
-

3
-

o
o

o

o

o
oo
o

o

oo

oo
oo
o
o

o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
oo o
ooooo
ooo o
o
o
ooo
o
o
o
o
o
o
o
o
oooo
o
o
o
o
oo
o
ooooo
o
o
o
o
o
o
o
o
o
oo
ooo
o
o
ooo
o
o
oo
o
oo
oo
o
o
oo
o
ooo
o
o
oo
o
o
oo
o
o
o
ooo
o
ooo
o
o
o
o
o
oo
oo o
o
o
o
oo
oo
oooo
o
o
oo
o
ooo
o
o
oo
ooo
o oo
oo
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
o
o
o
o
o oo
o
o
o
o
oooo
o
ooo
oo
o
ooo
o
o
o
oooo
o
o
oo
o
o
oo
oo
o
o o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o o
o
oo
o
oo
o
o
o oo
o
ooo
o
o
o
o
o
o
o
o
oo
o
o
oo
ooo
oo
oo
o
o
oo o
o
o
o
o
oo
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
oo
o
o
ooo
ooo
o
oo oo
o
oo
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
oooo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
o
o
oo
o
o
ooo
o
o
oo
o
o
oo
o
oo
o
oo
o
o
o
oo
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
oo
oo

             
      
                                    
      

      

o
o
oooo
o

o
o
o
o

o
ooo
o
o

o
o

o
o
o

o
o

o
o

o

o

oo

o
o
oo
oo

o
ooooo

2

0

o

o

o

oo

o

 
3
 
e
t
a
n
d
r
o
o
c

i

2
-

o

o

o

o

o
ooo
o
o
o
o
o
o

o

o

-4

2

1

 

 

t

0
1
e
a
n
d
r
o
o
c

i

-6

o
o

oo

o

o

o o
o
o
o
o

-2

0

2

4

coordinate 2 

o
o

o

o

o

o
o

o

o o
o
o

o

o

o

o

o

o

o

o

o
o

o
o

o
o

o
o
o

o o o

oo
o
o
o
oo
o
o

o
o
o o
o
o

oo
o
o
oooo
o o
o
o
o
o
o
o
o
o
o
o
oo o
oo
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo o
o
o
o
o o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
ooooo
o oo
o
o
o
oo
oo
o
o o
o
o
o
o
o o
o o
o
o
o
o o
o
o
oo ooo
o
oo
oo
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
ooo
o
o
o
o
o
ooo
oo
o
o o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oooo
oooo
o
o
o
o
o
oo
oo
o
o o
o o o
o
oo
ooo
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
oo
oo o
o
o
o
o
oo
o
oo
o
o
oo
o
o
o o
o
o
oo
oo
o
o
oooo
o
o
o
o
o
o
o
o
o o
oo
ooo
o o
o
oooo
o
o
o
o
o
o
o
o
o
o
o
oo
oo
ooo
oo
o
o
o
oo
o
o
o oo
o
o
o o
o
o
o
o
oo
o o
ooo
o
o
o o
o
oooo
ooo
o
o
o
o
o
o
oo
o
o
o
oo
oo
o
o
o
o
oo
o
oo
o
o
o
oo
o
o
ooo
o
o o
o
o
o
o
o
o
o
o
o
o
oooooo
oo
oo o
o
o
o
o
o

o                                                                  

o
o

o
o

oo
o

o

o

o

o

o

o

o

o

o

o

o

o

ooo
o
oo

o
oo
o

oooo
oo
o o
o

o

o

0

1
-

2
-

-4

-2

0

2

4

-2

-1

0

1

2

3

coordinate 1 

coordinate 9 

figure 4.8. four projections onto pairs of canonical variates. notice that as
the rank of the canonical variates increases, the centroids become less spread out.
in the lower right panel they appear to be superimposed, and the classes most
confused.

116

4. linear methods for classi   cation

+

+

+

+

figure 4.9. although the line joining the centroids de   nes the direction of
greatest centroid spread, the projected data overlap because of the covariance
(left panel). the discriminant direction minimizes this overlap for gaussian data
(right panel).

fisher   s problem therefore amounts to maximizing the rayleigh quotient,

max

a

at ba
at wa

,

or equivalently

at ba subject to at wa = 1.

max

a

(4.15)

(4.16)

this is a generalized eigenvalue problem, with a given by the largest
eigenvalue of w   1b. it is not hard to show (exercise 4.1) that the optimal
a1 is identical to v1 de   ned above. similarly one can    nd the next direction
a2, orthogonal in w to a1, such that at
2 wa2 is maximized; the
solution is a2 = v2, and so on. the a    are referred to as discriminant
coordinates, not to be confused with discriminant functions. they are also
referred to as canonical variates, since an alternative derivation of these
results is through a canonical correlation analysis of the indicator response
matrix y on the predictor matrix x. this line is pursued in section 12.5.

2 ba2/at

to summarize the developments so far:

    gaussian classi   cation with common covariances leads to linear deci-
sion boundaries. classi   cation can be achieved by sphering the data
with respect to w, and classifying to the closest centroid (modulo
log   k) in the sphered space.

    since only the relative distances to the centroids count, one can con-
   ne the data to the subspace spanned by the centroids in the sphered
space.

    this subspace can be further decomposed into successively optimal
subspaces in term of centroid separation. this decomposition is iden-
tical to the decomposition due to fisher.

4.3 id156

117

lda and dimension reduction on the vowel data

   

   

7
.
0

6
.
0

5
.
0

4
.
0

3
.
0

   

   

   

   

2

   

   

   

   

4

   

   

   

test data
train data

   

   

6

   

8

   

   

   

   

10

dimension

e
t
a
r
 
n
o
i
t
a
c
i
f
i
s
s
a
c
s
m

i

l

figure 4.10. training and test error rates for the vowel data, as a function
of the dimension of the discriminant subspace. in this case the best error rate is
for dimension 2. figure 4.11 shows the decision boundaries in this space.

the reduced subspaces have been motivated as a data reduction (for
viewing) tool. can they also be used for classi   cation, and what is the
rationale? clearly they can, as in our original derivation; we simply limit
the distance-to-centroid calculations to the chosen subspace. one can show
that this is a gaussian classi   cation rule with the additional restriction
that the centroids of the gaussians lie in a l-dimensional subspace of irp.
fitting such a model by maximum likelihood, and then constructing the
posterior probabilities using bayes    theorem amounts to the classi   cation
rule described above (exercise 4.8).

gaussian classi   cation dictates the log   k correction factor in the dis-
tance calculation. the reason for this correction can be seen in figure 4.9.
the misclassi   cation rate is based on the area of overlap between the two
densities. if the   k are equal (implicit in that    gure), then the optimal
cut-point is midway between the projected means. if the   k are not equal,
moving the cut-point toward the smaller class will improve the error rate.
as mentioned earlier for two classes, one can derive the linear rule using
lda (or any other method), and then choose the cut-point to minimize
misclassi   cation error over the training data.

as an example of the bene   t of the reduced-rank restriction, we return
to the vowel data. there are 11 classes and 10 variables, and hence 10
possible dimensions for the classi   er. we can compute the training and
test error in each of these hierarchical subspaces; figure 4.10 shows the
results. figure 4.11 shows the decision boundaries for the classi   er based
on the two-dimensional lda solution.

there is a close connection between fisher   s reduced rank discriminant
analysis and regression of an indicator response matrix. it turns out that

118

4. linear methods for classi   cation

classification in reduced subspace

o
o
o
o
o
o

o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o

o

o

o

o

o

o
o
o
o
o
o
o

            
      

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o

o

o

o

o

o

o

o

o

o
o
o
o
o
o
o

      

      

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o

o
o
o

o
o
o
o
o
o
o
o
o
o

o
o
o
o
o

o

o

o
o
o

o

o
o

o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o

      
      

o

o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o

      

o
o
o
o
o
o
oo
o
o

o
o
o

o

o
o

o

o

o

o

o

o
o
o
o
o

      

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

      

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o

o

o
o
o
o
o
o

o
o
o

o

o
o
o
o
o
o
o
o
o

      

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o

o

o

o
o
o

o

o

o
o
o
o
o
o

o

o

o
o

o
o
o
o
o

o

 

t

2
e
a
n
d
r
o
o
c

i

 
l

o

o
o

o

o

o
o

o
o
o

o
o
o
o
o
o
o

i

a
c
n
o
n
a
c

o
o
o
o

o
o
o
o
o
o
o
o

o

o
o

o

o

o

o

o

o

o

o

o

o

o

o

o
o

o

canonical coordinate 1

figure 4.11. decision boundaries for the vowel training data, in the two-di-
mensional subspace spanned by the    rst two canonical variates. note that in
any higher-dimensional subspace, the decision boundaries are higher-dimensional
a   ne planes, and could not be represented as lines.

4.4 id28

119

lda amounts to the regression followed by an eigen-decomposition of
  yt y. in the case of two classes, there is a single discriminant variable
that is identical up to a scalar multiplication to either of the columns of   y.
these connections are developed in chapter 12. a related fact is that if one
transforms the original predictors x to   y, then lda using   y is identical
to lda in the original space (exercise 4.3).

4.4 id28

the id28 model arises from the desire to model the posterior
probabilities of the k classes via linear functions in x, while at the same
time ensuring that they sum to one and remain in [0, 1]. the model has
the form

log

log

pr(g = 1|x = x)
pr(g = k|x = x)
pr(g = 2|x = x)
pr(g = k|x = x)

=   10 +   t

1 x

=   20 +   t

2 x

...

log

pr(g = k     1|x = x)
pr(g = k|x = x)

=   (k   1)0 +   t

k   1x.

(4.17)

the model is speci   ed in terms of k     1 log-odds or logit transformations
(re   ecting the constraint that the probabilities sum to one). although the
model uses the last class as the denominator in the odds-ratios, the choice
of denominator is arbitrary in that the estimates are equivariant under this
choice. a simple calculation shows that

pr(g = k|x = x) =

pr(g = k|x = x) =

exp(  k0 +   t
   =1 exp(     0 +   t

k x)

    x)

   =1 exp(     0 +   t

    x)

1 +pk   1
1 +pk   1

1

, k = 1, . . . , k     1,

,

(4.18)

1 , . . . ,   (k   1)0,   t

and they clearly sum to one. to emphasize the dependence on the entire pa-
rameter set    = {  10,   t
k   1}, we denote the probabilities
pr(g = k|x = x) = pk(x;   ).
when k = 2, this model is especially simple, since there is only a single
linear function. it is widely used in biostatistical applications where binary
responses (two classes) occur quite frequently. for example, patients survive
or die, have heart disease or not, or a condition is present or absent.

120

4. linear methods for classi   cation

4.4.1 fitting id28 models

id28 models are usually    t by maximum likelihood, using the
conditional likelihood of g given x. since pr(g|x) completely speci   es the
conditional distribution, the multinomial distribution is appropriate. the
log-likelihood for n observations is

   (  ) =

nxi=1

log pgi(xi;   ),

(4.19)

where pk(xi;   ) = pr(g = k|x = xi;   ).
we discuss in detail the two-class case, since the algorithms simplify
considerably. it is convenient to code the two-class gi via a 0/1 response yi,
where yi = 1 when gi = 1, and yi = 0 when gi = 2. let p1(x;   ) = p(x;   ),
and p2(x;   ) = 1     p(x;   ). the log-likelihood can be written

   (  ) =

=

nxi=1nyi log p(xi;   ) + (1     yi) log(1     p(xi;   ))o
nxi=1nyi  t xi     log(1 + e  t xi )o .

(4.20)

here    = {  10,   1}, and we assume that the vector of inputs xi includes
the constant term 1 to accommodate the intercept.
to maximize the log-likelihood, we set its derivatives to zero. these score

equations are

      (  )

     

=

nxi=1

xi(yi     p(xi;   )) = 0,

(4.21)

nent of xi is 1, the    rst score equation speci   es thatpn

which are p + 1 equations nonlinear in   . notice that since the    rst compo-
i=1 p(xi;   );
the expected number of class ones matches the observed number (and hence
also class twos.)

i=1 yi =pn

to solve the score equations (4.21), we use the newton   raphson algo-

rithm, which requires the second-derivative or hessian matrix

   2   (  )
          t =    

nxi=1

xixi

t p(xi;   )(1     p(xi;   )).

(4.22)

starting with   old, a single newton update is

  new =   old    (cid:18)    2   (  )

          t(cid:19)   1       (  )

     

,

(4.23)

where the derivatives are evaluated at   old.

4.4 id28

121

it is convenient to write the score and hessian in matrix notation. let
y denote the vector of yi values, x the n    (p + 1) matrix of xi values,
p the vector of    tted probabilities with ith element p(xi;   old) and w a
n    n diagonal matrix of weights with ith diagonal element p(xi;   old)(1   
p(xi;   old)). then we have

      (  )

= xt (y     p)
     
   2   (  )
          t =    xt wx

the newton step is thus

  new =   old + (xt wx)   1xt (y     p)

= (xt wx)   1xt w(cid:0)x  old + w   1(y     p)(cid:1)

= (xt wx)   1xt wz.

(4.24)

(4.25)

(4.26)

in the second and third line we have re-expressed the newton step as a
weighted least squares step, with the response
z = x  old + w   1(y     p),

(4.27)

sometimes known as the adjusted response. these equations get solved re-
peatedly, since at each iteration p changes, and hence so does w and z.
this algorithm is referred to as iteratively reweighted least squares or irls,
since each iteration solves the weighted least squares problem:

  new     arg min

  

(z     x  )t w(z     x  ).

(4.28)

it seems that    = 0 is a good starting value for the iterative procedure,
although convergence is never guaranteed. typically the algorithm does
converge, since the log-likelihood is concave, but overshooting can occur.
in the rare cases that the log-likelihood decreases, step size halving will
guarantee convergence.

for the multiclass case (k     3) the newton algorithm can also be ex-
pressed as an iteratively reweighted least squares algorithm, but with a
vector of k   1 responses and a nondiagonal weight matrix per observation.
the latter precludes any simpli   ed algorithms, and in this case it is numer-
ically more convenient to work with the expanded vector    directly (ex-
ercise 4.4). alternatively coordinate-descent methods (section 3.8.6) can
be used to maximize the log-likelihood e   ciently. the r package glmnet
(friedman et al., 2010) can    t very large id28 problems ef-
   ciently, both in n and p. although designed to    t regularized models,
options allow for unregularized    ts.

id28 models are used mostly as a data analysis and infer-
ence tool, where the goal is to understand the role of the input variables

122

4. linear methods for classi   cation

table 4.2. results from a id28    t to the south african heart
disease data.

(intercept)
sbp
tobacco
ldl
famhist
obesity
alcohol
age

coe   cient
   4.130
0.006
0.080
0.185
0.939
-0.035
0.001
0.043

std. error z score
   4.285
1.023
3.034
3.219
4.178
   1.187
0.136
4.184

0.964
0.006
0.026
0.057
0.225
0.029
0.004
0.010

in explaining the outcome. typically many models are    t in a search for a
parsimonious model involving a subset of the variables, possibly with some
interactions terms. the following example illustrates some of the issues
involved.

4.4.2 example: south african heart disease

here we present an analysis of binary data to illustrate the traditional
statistical use of the id28 model. the data in figure 4.12 are a
subset of the coronary risk-factor study (coris) baseline survey, carried
out in three rural areas of the western cape, south africa (rousseauw et
al., 1983). the aim of the study was to establish the intensity of ischemic
heart disease risk factors in that high-incidence region. the data represent
white males between 15 and 64, and the response variable is the presence or
absence of myocardial infarction (mi) at the time of the survey (the overall
prevalence of mi was 5.1% in this region). there are 160 cases in our data
set, and a sample of 302 controls. these data are described in more detail
in hastie and tibshirani (1987).

we    t a logistic-regression model by maximum likelihood, giving the
results shown in table 4.2. this summary includes z scores for each of the
coe   cients in the model (coe   cients divided by their standard errors); a
nonsigni   cant z score suggests a coe   cient can be dropped from the model.
each of these correspond formally to a test of the null hypothesis that the
coe   cient in question is zero, while all the others are not (also known as
the wald test). a z score greater than approximately 2 in absolute value
is signi   cant at the 5% level.

there are some surprises in this table of coe   cients, which must be in-
terpreted with caution. systolic blood pressure (sbp) is not signi   cant! nor
is obesity, and its sign is negative. this confusion is a result of the corre-
lation between the set of predictors. on their own, both sbp and obesity
are signi   cant, and with positive sign. however, in the presence of many

4.4 id28

123

20

30

0.0

0.4

0.8

0

50 100

0
3

0
2

0
1

0

8

.

0

4

.

0

0
0

.

0
0
1

0
5

0

0

10
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
o oo
o
ooo
oooo
o
o
o
oo
o
o
oo
oo
o ooo
oo oo
oo
o
o
o
o oooooo
o
o
oo
oo
o
o
o
oo
o
o
ooooooo
o
ooo
o
o
ooo
o
o
o
ooo
o
oo
oo
ooo
o
oo oo
oo
o
oo
o
o
o
o
oo
oo
o oo
o o
o o
o
o o
o
ooo
o
o
o oooo
oo
oo
oo
o
o
o
o
o
o
oo o
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
ooo
o o
o
o
oo
o
oooo
o
o
o
o
o
oo o
oo
oo
oo
o
oo
o
o
oooo
ooo
o o
o
o
oo
o
o
o
o
o
ooo
o
o
o
o
o
oo
o
o
o
oo
o
oo
o
oo
o
oo
ooo
o
oo
ooo
o o
o
oo
o
o
o
oo
o
oo o
o
o
oo
o ooo
o
o
o
o
o
o
oo
oo
o
o o
o
o
oo
o
oo
oo
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o

tobacco

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o oo
o
o
ooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
oo
o
oo oo
oo
o
oo
o
o
o o
o
o
o
o
ooo oo o
o
o
oo
o
o
ooo
ooo
o
o
ooo
o oo
o
oo
ooo ooo
o o
oo
o
ooo
o
ooo
o
o
oo
o
o o
o
o
o ooo
oo
o
oo
o
oo
ooo
oo
o o
oo
o
o
oo
o
o
oo
o
o
oo o
o
o
o
o
o
o o
o
o
ooo
o
ooo
o
o
o
o
o
o
o
o
o o
o
oo oo
ooo
o
o
o
o
o
o oo
o
ooo
o
o
o
o
o o
o o
o
o
o
o
o
o
o
ooo
oo
o
o
o
o
o
oo
oooo
oo
o o
ooo
o
oo
o
oo
ooo
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
o o
o
oo
o
oo
oo
oo
o
oo
o
o
o o
o
oo
oo
o
oo
o
o
o
o o
o
oo
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
oo
oo
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
oo
o
oo
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o o o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
oo
o
o o
o
oo
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
ooo
o
oo oo
oo
o
o
o
o
oo
o
o
oo
o
o
o
o
ooo
o
o
oo
o
o
o
o
o
o
oo
o
o
o oooo
o
o
o o
o
o o
oo
o
o ooo
o
ooo ooooo ooo
o
ooo
o
o
o
o
o
o
o
o
o oooo
o
ooo oo
o
oo
o
o
oooo o
ooo
o
oo o
o
o
oooo
oo o
o
oooo o
o
oo
ooo
o
o
o o
oo
oo ooooo
ooo
o
ooo
o
oo
o ooo
oo
oo o
o
o
ooo
o
o
oo
o
oo
oo
o
o
oo o
o
o
o oo
oo
o
o
oo
oo
ooo oo
o
o
ooo
oo o
o
ooo
oo
o o
oooo
o
o
o
oooo
ooo
oooo oooo
o ooo
o o
o
o o
ooo
oo
oo
o
o o
o
o o
oo
oo
o
o
oo
o
o
o
o
ooo oo
o
oo
o oo
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o ooo
o
oo
oo
o
o
ooo
o
o
oo
o
oo
o
o
oo
oo
o
o
oooo
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
oo o
o
o
oo
o
o
o
o
o
o
o
o oo
oo
o
o
o
oooo
o
oo
oo
o
o
o
ooo
o ooo
o
o o
ooo
o
o
o
oooo
o
ooo
oo
o
o
o ooo o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o oo o
o
o
oo
o
o
o
o
o
o
ooo o o
o
o
o
o
o
oo oo
oo o
ooooo o
o
o
o
o o
oo
oo
oo
o
ooo
o
o
o
o
o
oo
o
oo
o
o
o
oo
o
o
o
o
o
oooo
ooo
o
o
o
o
oo
oo
o
ooo
o o
o
oo
oo
ooo
o
o
o
o
ooo
o
o
o
o
oo
o
o
oo
o o
o
o oo
ooo o
o
o
oo
oo
o
o
oo
o
o
o
o
oo
o
oo
o
ooo
oo
oo o
oo
o
o
o
o
o
o
o
oo o
o
o
o
o o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o
oo
o
o
oo
o o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
ooo
o oo
oo
oo
o
o
o
o o
o
o
oo
o
o
o
oo
o o
ooo
o
o
o
o o
oo
o
oo o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
o oo
o oo oo
o
o o
oo o
o
oo
o
oo
oo
o
oo o o
ooo
o o
oo
o
o
oo
o
o
o o
o
o
o
oo oo
o
oo
oo
o
o
o o
o
o oo
o
o
ooo
o
o
o
o
oo
o
oo
o
o
oo
o
o o
o
o
oo
o
ooo
o o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
oo
o
o

o

o

ldl

oo

o oo o
o o
o oo
o
oooo
o
o o o
o
o
o
o o
ooo
o o
o
o
ooo
o
oo
oo
oo o
ooo
o
oo o
oo
oooo
oo
o
oo
oo
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o oo
o
o
o oo
o
o
ooo
o
oo
o o
o oo
o
oo
o
o
o o
o
o
oo
oo
o
oo
o
o
o o
oo o
o
oo
ooo
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
o
o
o o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o

sbp

o

o

o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
o
ooo
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
oo
o
o
o
o o
o o
oo o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
ooo
oo
o
oo oo
o
o
o
oooooo ooo
o
o
o ooo
o
oo
o o
o
o
o
o
ooooo o
oo
o
o o oo
o o
o o
o
o
o
oooo
oo oo
o
oo o
o
ooo oo
o ooo
ooo
o oo
o oo
ooo oo
o
o
oooo
oo
o
o
ooo
o
oo
o
o
o
o
oo o
o oo
oo oo
oo
o
o o
o oo
oo
oo
o
o
o
ooo
o
o
o
o o oo
o
o
o
ooo
o
o
o
oo
oo
o
o
ooo
o
o
ooo
o oo
o
oo ooooo
o
o
ooo
o
ooooo
ooo
oo
ooooooo
o
o
oo
o
o
oo
o o ooo
oo
oo
o
oooo
o
o
oo
o
ooo
o
oo
oo
o
o o
oo
o
oo
o
o
oo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo o
oo
o
o
o
o
o
oo
o
o
o
o oo
o
o
o
o
o
ooo
o
oo
o
oooo
o
oo
o
o
o
o
o
oo
o
o
o
oo
o
o
oo
oo
o
oo
o
ooo
o oo
oooo
o
o
o
o
o
o
o
o
o
o
o
oo
oo o
o
o
o
o
ooo
o
o
o
o
o oo
oooo
o o
oo
o
o
o
o o
o
o
o oo
o
oo
o
o
o
ooo
o o
oo o
o
o
oo oo
o
oo
oo
o
o
o
o
o
o
o
oooo
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
ooo
o
o
o
o
o
o
o
o
oo
o
ooo oo
o o
o
o
o
oo
oo o
o
o
o
o
oo
o
oo
o
oo
o
o oo
ooo
o ooo
o
o
o
o
o
o
ooo
o
o
o
o
oo
o
o
ooo
ooo
o
o
o
o
o
oo
o
o ooo
o
oo o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o oo
oo
o o
o
o
o
o o
oo
o
o
o
o
o
o o
o
oo
o
oo
o
oo o
oo
oo
o
o
o
o o
oo
o
o
o
ooo
o
o
o
o
o o
o
oo
o
oo
ooo
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o ooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
oo
o o
ooo
o
o
o
oo
o
o
o
o
o
oo
o
o
o o
o o
o
o
o
o
o
o
o
o
o o
ooo
o oo
o
oo o
o o
o o
o oooo
o oo
o
o
o
oo o
o
o
o
o o
oo
o
o
oooo
oo
oo
o o
o
o
o o
o
o
o
o
o
o
o
o
oo
o o
oo
o
o
o o
o
ooo
o
o
oo
o
o
o
oo
o
o
o
o
oo
o o
ooo
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o oo
ooo
oo oooo
oo o
o
oo
ooooooo
o
o
oo
o
oo
o oo o o
o
oo o
o o
o oo o
o o
oo
ooooo
o oo
oooo
o
oo
o
o
oo
o
o oo
o
o
ooo
o
o
o
ooooo
o
ooo
o
o
ooo
o
ooo ooo
oo
o
oo
o
o
oooo o o
oo
o
ooo
o
o
o
oo
o
o
o
o
o
o
oo oo
oo
o
oo
oo o
o o
o
oo
o
oo
o o
oo
o
o o
oo
oo
ooo
oo
o
oo
oo
o
oo
o
o o
oo
o
o
o
o
o o
ooo
oo
o
o
o
o
oo
oo
o
o o
o
o
oooo
o
o
o
o
o
oo
oo
o
o
o
o
oo
o
o
o
o
oo
ooo oo
o
o
o
oo
o
o
oo
oo
o
o
oo
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
oo
o
o
o
o
o
o
o
o
oo o o
o
o
oo
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
oo o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
oo
oo
o ooo
o
o
o
o
oo
o
oo
oo
ooo
o o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o oo
o
oo
o
o
o
o
o
o
o
o
o
ooo
oo
o
o
o
o
o
ooo o oo oo
oo
o
o
o
o
o
o o
ooo
o
o o
o
oo
o
o
o
o
o
o
oo
o
o
ooo
o
o
oo oo o
o
o
o
o
o
o
o
o
oo
o
o oo
o
o
o
oo o o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
oo o
o
o
o
o o
o
o
o
o ooo
o
oooooo
o o
oo o
o
ooo o
o
o
o
o
oo
o o
o
o
o
o
o
o
o
oo
oo
oo oo
o
o
o
o
o o
o
o
o
o
o
oo
oo
oo
oo o
oo
o
o
oo
o
o
o
o
o o
oo oo
o
o
o
o
oo oo
o
o
oo
o
oo
oo
o
o
o
o
o o oo
o
o
oo
oo
ooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
oo
o o
o
oo
o
o
ooo
oo
o
oo
oo
oo
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
oo
o o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o o
o
oo
o
ooo
o
oo
oo
o
o
ooo oo o
o
o
o
oo
oo oo
oooo
o
o
o
o
o
o
o
o
o
o
ooo
oo
o
o
oo
o
o
o
ooo oo
o
o
o
o
o o
o
o
o
oo
o
o
ooo
o
ooo
o
o
oo
oooo
o
o
ooo o oo
oo
oo
o
oo
o
o
o
oo o
o
o
o o
oo
oo
oo
ooo
o
o
o
oo
ooo
oo oooooooo
o
o
o
oo
o
o
o
o
o
o
o
oo o
o
oooo
o oo
o
oo
ooo
oooo oo
o
o
o
oo o
ooo
oo
oo
ooo o
oo
oo
oo
o
oo
o
oooo
oo
o
oo
ooo
o
o
o
ooooo
o
o
o ooooo
o
oooo
o
o
o
o
o oooo
o
oo
o
ooo
oo
o o
o
oooo
o
o oo o
o oo
o
oo
oo
o
o
o
oo
o
o
oo
oo
o
o
oo oo
o
oo
o
ooo
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
ooo oo
o
oo
o
o
o
o
o
oo
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o ooo
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
ooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
o
oo
o
oo
o
oooo
o
o
o
o o
oo
o
o
oo
o
oo
o
oo oooo
o
o
oo
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
100

160

o

220

o

o

o

o

o

o

o
o

oo oo
oo o
oooo ooo
oooo
o
ooo o o
oo
o
o
oo
o
o oo
o oo
ooo
oooooooo
o
oo
oo
o oo
ooo o
o
oo oo
o
oo
oo
o
oooo
o ooo
oo
o ooo
o
o
o
o
o oo
o oo oo
oooo
oo
ooo
o o
oo
oo
o
o
o
o o
o
ooo
ooooo
o
o
oo o
ooo
o
o
o
o
o o
ooooo
oo oo ooo
o
ooo o
o
o
o
o o
oooo o o
o
o oo
oo
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
ooo
oo
oo
o
o
oooo
oo
ooo
o o
o
o
o
oo
o
oo
oo
oo
o
o
o
oo
o
o
o
o
o
oo
oo
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
oo
o
o
o
o
oooo
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
oo
o
oooo
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
oo
o o
o
o
o
oo
oo
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
oo
o o
oo
o
o
o
o
o o
o
o o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
oo o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o o
ooo
o
oo
o
o
o
o
o
o
oo
o
oo
o o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
oo oo
o
oo
oo oo
o
oo
o oo
o
oo
o
o
ooo
ooooo
o
oo
oooo
ooo
o o
o
o
oo
oooo
o
o
o
o
oo oo
o
o
o
o
ooo
o
o o
ooo
o
o o
o
o
o o oo
o
o
o
o o
oo
o
o
ooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
oo
o
o
o
oo
oo
oo
oo oo o
oo
oo
oo
o
o
ooo
o
o o o
o
o
ooo
o
o
o
o
oo o
oo
o
o
o
o
ooo
o
o
oo
o
o
o
o
o
oooo
o
oo
oo
oo
o
oo
o
o
oo
ooo
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
oooo o o
ooo o oo
o
o
ooo
o
o
o
o
ooo
o
o
o
o
o
o
oooo
o
o
o
o o
o
o
o
o
o
o o
o
o
o o
o
o o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
ooo
o
oo
o ooo
o
oo oo
o
ooo
o
o
oo
o
ooo
o
ooooo
o
oo ooooo
o
ooooo
o o
oo
ooo oo
o
o
o
o
o oo o
ooooo
ooo
oooo
oo
oo
ooo ooo
o
oooo
o
o
o
oo oooooo o
o
oooo
o
o o
o o
oo
o
oooo
o
oo
oo
ooo
o ooo oo o
o oooo
o o
o
ooo
ooooo
oo
o
oooooooooo ooo
o ooo
oo
oo
o
o
ooo
o
ooo
o
ooo
oo
oo
o o
oo
oo
o
o
o
o
o
oo
ooooo
o
oo
o o o
oo
o
oo
ooo
o
o
oooo
oo
oo o
oo
oo
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
oooo
oo
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo o
o
o
o
o
o
o
oo
o
o
o
o
ooo
o
oo
o
ooo
o
o
o
o
o
o
o
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
oo o
oo
o
o
o
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
oo
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o
o
o
oooo
oo
o
o
o
oo
oooooooo
oo
o
o
o
oo
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o

o
o

o

o

oo
ooo
oo
oo o
oooo
o
oo o
o
o
o
oo
o
oo oo
oo
o
o oo ooooooo o
ooo
oo
o
o oo
ooo oo
o ooooo
oooo o
oo oooo
ooo oo
o
o
o o
o
o oo
o
o
o
o
o
o
oo
o
o
ooo oo
oo
oo
oooooo
oo
ooo
o
oo
ooo o
o
o o
o o
o oo o
o
oo
oo oo o oo
oo
o
o
ooo oo
oo
oo o
oooo
o
o
ooo
o
o
o
ooo
oo
o
o o
ooo
o
o
ooo
oo
oo
o oo
o
oooo
oo
o
o
o o
oo
o
o
oo
oooooo
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
oo
o
o
o
oo
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oo
o
o
o
o
o
o
o
oo
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o oo
o
o
o
o
o
o
o
o
ooo
oo
o
o
oo
o
oo
o
o
oo oo
o
oo
o
o
o
o
oo
oo
o
o oooo
oo
o
o o
oo o
o
o
oo
oo
o
ooo
o
o
o
oo
o
o
oo
o
o
o
o o o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
oo o
o
o o
o
oo
o
o
ooo
o
o
o
o
o
ooo
ooo
o
o
o
oo
o
o
o ooo
o
oo
o
o
o
o
oo
oo
o
oooo o o
o
o
o
ooooo
o
o
oo
oo oo
ooo
o o
o
o
o
o
o
o oo
o
o
o
o
o
o o
oo
oooo
o o
o
ooooo
o o
o
o
oo
o
o
o
o
o
o
o
ooo
o
oo
o
oo
oooo
oo o
oo
o
o
oo
oo
oo oo
o o
o
o
o
o
oo
o
o
o
o
ooo
oo
o
o
oo
o
o o
o
o
o
oo
o
o ooo
oo oo o
o
oooo
ooo
oo o
o
oo
o
o
o
o
o
o
oo
o oo
o
o
o
o
oo
o
o
o oo
o
o
o
oo
o
oo
o
o o
oo
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
ooo
o
o
o
o
o
o
o
o o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o

o

o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
oo
o o
o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o o
o ooo
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
ooooo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
ooo
o
o
o o oo
oo
oo ooo
o
oo ooo
oo
o
oo
o
oooo o
oo
o
o
o
oooo
o
o
o
ooo
o
oo
o o
ooo ooo o
o
oooo
oo o
o
o o
oo
o
ooo ooo
oooo
oo
ooooo o
o
o
ooo oo
ooo oo o o o
o
o
o oo o
oo
oo
ooo
o
oo
ooo
o
oo
ooo oo
o
oo
ooo
o oo
oooo ooooo ooo
oo
o
o
o
ooo o
oo
oo oo
oo
oo
oo
o
o
oo
o
o
oo
o
o
oooooo
o
oooo
oo
ooo o
oo
oo
o
oo
oo o
o
ooo
o o
o
oo
o
oo
o
o
o
oo
o
o
o
o o
o
o
o
oo
ooo
o
oo
o o
o
o
o o
o
oo
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
o oooo
o oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo
oo
o
o
o
o o
o
o
o
o o
o
o
o
o
oo o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
oo
o
ooo
o
o
o
o
o
o
o o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo o
o
ooo
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
oo
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
oo
o
o
o o
o
o
ooo o
o
o
o o
oo
oo ooooo o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
2

6

o

o

o

o
o

o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
oo
o
o
oo
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
oo
o
o
oo
o
oo
o
oo
o
oo
ooo
oo
o
oooo
o
o
ooo
o
o
oo
oo
o
o
ooo
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o
o
o
oo
o
oo
o
o
oo
oooo
o
o
oo
o
o
oo
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
ooo
oo
o
oo
o
o
oo
oo
o
ooo
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
oo
o
o
o
oooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
oo
o
o
o
oo
ooo
o
ooo
oooo
o
o
o
o
o
o
oo
oo
oo
oo
o
o
oo
ooo
oo
o
o
oo
o
o
o
o
o
oooo
o
o
oo
ooo
o
o
o
oo
oo
ooo
o
oooooooo
oo
o
oo
oo
ooo
oo
o
o
ooo
o
oo
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
oo
o
o
oooo
o
ooooo
o
oo
o
oo
ooo
o
o
o
o
o
o
o
oo
o
oo
ooo
o
o
oo
oo
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
o
o
oo
oo
o
o
o
o
o
oo
o
o
o
o
oo
o
o
oooo
ooo
o
o
oo
o
o
o
o
oo
o
o
o
o
ooooo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oooo
o
oo
o
oo
o
o
o
ooo
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
oo
o
o
oooo
o
o
ooooo
o
o
oo
o
oo
o
oo
o
o
o
o
oo
oo
o
oo
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
oooo
o
oo
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
o
o
o
o
o
oo
o
ooo
o
ooo
o
o
o
o
o
o
o
oooo
oo
o
o
o
o
o
o
o
oo
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
ooooo
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
ooo
oo
o
o
ooo
o
oo
ooo
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
oo
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
o
oo
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oooooo
o
o
o
ooo
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
ooo
ooo
o
oooo
o
o
o
o
o
ooo
oo
oo
o
oo
ooo
o
oo
ooo
o
o
o
ooo
o
o
oo
oo
o
ooo
ooo
o
o
o
ooo
oo
oooo
oo
o
o
o
o
o
oo
o
o
o
o
o
o
oo
oooo
oooo
ooo
o
o
o
oo
o
oo
oo
oo
ooooo
ooooo
o
o
o
oo
ooo
o
ooo
o
oo
o
o
o
ooo
o
oo
ooo
o
o
o
oo
o
o
o
ooo
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oooo
oo
oo
o
o
oooo
o
o
o
oo
o
oo
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o

famhist

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
o
oo
o
o
o
o
o
o
oo
oo
o
o
o
oo
oo
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
o
o
o
oo
o
o
oo
o
oo
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
ooo
o
o
o
o
oo
o
o
o
o
o
o
oo
ooo
oo
o
oo
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
oo
o
o
oo
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
ooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o
o
o
o
o
o
oooo
o
o
ooo
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
o
o
ooo
o
o
oo
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
oo
o
oo
o
o
o
o
oo
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
oo
o
oo
o
oo
o
o
o oo
o
o
o
o
oo
o
ooo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
ooo o
oo
o
o
oo
o
oo
o
o
oo
o
o
o o oo
o
o
o o
oo
ooo
o
ooo
oo
o
o
o
o
o
o
ooo
oo
o oo
oo
o
oo
o
o oo
o o
o oo
o
o o
oo
o o oo
o
oo
o
o ooo
o
oo
oo
o
o
oo
oo
oo
o
oo
oo o
o
o
o o o
o
o
o
o
o
ooo
oo
oo
o o
o
o
o
o
o
oo
o
o
o
o
o
o oo
o
oo
o
o
o o
o oo
o o
o
oo
o
o
oo
o
o o
oooo
o
o
o o o
o o
o
o
o o
o
o
o
o o
o
oo
o
o
oo
ooo
oo
oo
o
o
o
o
o
oo
o
o
o
o oo
o
o
o
o
oo
o
o o o
o
o
oo
oo
o
oo
ooo
o
o
oo
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
oo
o
o
oo
o
ooo
o o
o
o
o
o
oo
o
o
o
o
oo
o oo
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o o
oo oo
o
o
ooo
oo
o
o
o
o
o
oo
oo
o
oo
o
o
oo
o
oo
o
o
o
o
o
o
ooo
o
ooo
o
o
oo
o
o
o
o
o
o
o
o o
o
oo
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o o
oo
o
oo
o
o
o
o
o
oooo
o
o oooo
o
oo o
o o
o
oo
oo
o oo
o o
o
o
o
o
o
o o
oo
o o
o
o
o oo
o
ooo ooo o
ooo oo
oo
o
o
o
ooo oo
oo
oo
o o
o
ooo
o
o
o
o
oo oo
ooo o
o
ooo o
o
o
o
o o o
oo oo
ooo
ooo
o
o o
o
oo
o
oooooo o ooo
ooo
oo
o
o
o o
ooo
oo o
o o
o
o
ooo
oo
oo o
oooo
o
o
o oo
o
o
o
o
o
oo
o
oo oo
o
o o
oo oo
oo o
oo o
o
oo
o
oo
o
o oo
o
oo
oo
o
o
oo
o o
o
o
ooo
o
o ooo ooo o
oo o
o
o
oo
oo
oo
o
oo
oo
o
o
o
o
o
ooo
oo
o
o
o
o
o
oo
o
o
oo
o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
oo
ooooo
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o o
o
o
o
o
ooo
o
o
o
oo
o
o
o
o
o o
o o oo
oo
o
o oo
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o ooo
o
o
ooo
o
o
oo
oooooo
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo oo
o
o oo
oooo
o o
o
o
o
oooo
o
o o
o
o
o
o
o
o
o o
o
o
o
o ooo
oo
o o
ooo
o ooo
oo
oo oo
o o
o
o
o
o
o
o
o
o
o
o
o o
o
oo
o
o
o
ooo
o
oo o
o
oo
o
o
o
oo
o
oo
o
o
o
o
oo
oo
oo
o oo
o
o
o
oooo
o
o
o
o
o
o ooo oo
o
o
o
o
ooo
o
o
o
o
o
o
oo
o
o
oo
o
oo o
o oo
ooo
o
o oo
o
o
oo
ooo
o
o
o
o oo
o
o o
o
o
o
o
o
o
o
o
o
oo
o
ooo
o
o o
oo
ooo
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o
oo
o
o
o
oo
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oooo
o
o o
oo
oo
o
o
o o
o o
oo
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o o
oo
oo
oo oooo
o oo
o
oo
o
oo
oo
ooo
oo
o
o
o o o
oo
ooo
o
o
oo oo
o
o
o
oo
oo
o
ooo
o
o
oo o
o
o
o
oo oo
oo
oo
o
o
oo o
o
oo
o
oo
o
o
o
o
oo
o oo o
ooo
oo
o
o
o
o
o
oo
o
o
o
o o
o
o
o
oo
o
oo
ooo
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo o
o
o
o o
o o
o oo
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o

o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
ooo
o
oo
o
o
o
o
o
o
o
o
o
oo o
o
o
oo
ooo
oo
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o ooo
o
oo
o
ooo
oo
oo
o
o
o
oo
o
o
o
o
o oo
o o
ooo
oooooo
oo
oo
o
o
ooooo
o
oo
o
oo
o
o
oo
oo
oo
o
o
oo
oo
oo oo
oooo
o
o
o o
o
oo
o
oo
o
o
o
oo
oo
o
o
o
o
ooo
ooo
o o
o
oo
oo
oo
ooo
o
o
o
o
ooo
o
o
o
o
o
o
o
o
oo
oo o
o
o o
o
o
o
oo
o o
oo
oo
ooo
oo
oo
o
o
o
oo o
o
o
oooo
o
o
o oo
o o
o
o
ooo
o
o
o
oo
o
o
oo
o
o
o
oo
o
o
o
o
o
o
oo
o
oo
oo
o
o
o
oo
o
oo
oooo
o
o
o
o
o
oo
ooooo
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o oo
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
oo o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
oo o
o
o o
o
oooo
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
oo
o
o o
o o
o
oo
o
oo
o
o
o
o
oo
o
o
ooo
o
o
ooo
o
oo o
o
o
o o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oooo
o
oo
o
o
o
ooo oo
ooooo
ooo
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
ooo
o
o
oooo
o
oo
o
o oo
o o
o o
o
o
o
o
ooo
oo o
ooooo
ooo
oo
o o
oo
o
o ooo
o o
oo
o oo
oo
ooooo o
o
o
o
o
ooo
oo
o
o
oooo
o
o
o o
o
ooo
oo
o o o
oo
oooooooooo
o
oo o
o
oooo
oo
o
o
oo o
o
o
o
o
o
o
oooo
ooo
ooo
o
o
oo
oooo oooo
ooo
o
o
o o
o
oo
ooo
ooooo
o
oo
oo
o
oo
o
o
oo
oo
o
o
o
o
oo
oo
o
oo
o
o
oooo oooo
o
o
o
ooo
o
o
oo
o
oo
o o
o
o
o ooo
o
oo
oo
oo
ooo
o
o
ooo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
oo o
o
oo
o o
o
o
ooo
o
o
o
oo
o
o
o
o
o
o
o
oo
oo oo
o
ooo
o
ooo
oo
o
o
o
oo
o
o
o
o
o
o
o o
o
o
o
o
o
ooo
oo
o
o
o
ooo
oo
o
o
o o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
o
oo
o oo
o o
o
oooo
o
o
o oo
oo
o
oo
o
o
o
o
o
o oo
o
o o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o o
o
o
o o
o
oo
o
o
o
o
o
oo
o
o
oo
o
o
o oo
ooo
o
ooo
o
o
o
o
o
o oooo
o
oo
o
ooo
oooo
o
o
o
o
oo
oo
o
ooo
ooo
o
oo
oo
o
o
o
o
o
o
o
o oo
oo
o
o
ooo
o
o
o o
oo o
oo
o
o
oooo o
o
o
o
o o
o
oo
o
o
o
o
o
o
o
o
ooo
o o
oo
o
o
o
oo
ooo
o
oo
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o o
oo
o o
o
o o
o
o
o
oo
ooo
oo
oo
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
ooo o
o
oo
o
o o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
ooo
o o
o
o
ooo o
o o
o
o
o
oo
o
oo
o
o
o
o oo
oooo
o o
oo
ooo
o
o
o o
o
oooo
o
o
oo oo
ooo
o
oo
o
o
o o
oo
o
o
o
o
o
o o o
o
o
oo
o
o
o
o
o
ooo
o
oo o o
o
o
oo
o
oo
oo
o
o
o
o
oo
oo
ooo
o
o
o
o o
o
oo
o o
oo
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o

o
o
o

o
o
o

oo
o
o
o

o oooo
o o
oo
o
oo
ooo
ooo o
ooooo
o
ooo
oooooo
o
o
o
oo
ooo
oo o
ooo o
oo
oo
oo
o oo
o o
o
o o
o o
o
o
o ooo
oo oo
o
o
o o
o
o
oooooo
o ooo ooo o
o o
o
o
o o
o
o
oo
ooooooooo o
o
o ooo oo
o
oo
o
oooo
oo o
oo
o ooo
oo
oo
o
oo
oo o
o
oooooo
o
oo
o
o
o
o o
o
o
ooo
o
o
o oo oo
o
o
o
o o
oo
o
o
oo
ooo o
oo o
o
o o
o
o o
oo
o
o
o
ooo
o
o
o
oo o
o
o
o
o
o
o
oo
o
oo
o
oo
o
o
oo
o
ooo
o
o
o
o
o
oo o
o
o
o
o
o
o
o
o o
o
oo
o
o
o
oo
o
o
o
o
o
o
o

o
o

obesity

oo
o oo oooo oo
o
oo
oooo
o
oo
ooo
oo
oo
ooo
ooo
ooo
oooo oo
oo
oo
ooo o o
o
ooo oo
o
oo
o ooo
oooo oooo
o
o
o
o
o
ooooooooo
o
oo o
o
oo
o
o
ooo
o
oo
o
o
oo
oo
ooo oooo
o
o
oooo
o
ooooo
o
oo
o
o
oooooo
o
o
o
oo o
o o
o
ooo
oo
ooo
oo
oo
o
o
oo
o
o
o
o
o
o o
o o
o
o
o
o
o
oo o
o
oo
o
oooo
oo
o ooo
oo o
o
o
o
oo
oo
o
o
o
o
o
o
o ooo
o
o
o
oo
o
o o
o
oo
o
o
o
ooo
o
o
o
o
o
o
o
oo
oooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
ooo
o
oo
o
o
oo o
oo
o
o
o
o
o
oo
o
oo
oo
oo
oo
oo
o
o
o
o
o ooo
o
o
o
o
oo
oo o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
oo
o
o
o
o
o
o
o
o
o
o o o
oo
ooo o
o
o
oo
oooo oo
o
ooo
oo
oo
o
o
o o
oo
o
oo
o
ooo
o
o
oo
oooo
o o
oooo
oo
oo
o oo
o
oo
o
o
o
o o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o o
o ooo
o
o
o
oooo
o
oo
o
o
oo o
ooo
o
o
ooo o
o
o
o
o
o
o o
o
o
o
ooo
o
o o
o
oo
o
oo
o o
o
o
o
o
o
o
oo
o
oo
o
oooo
oo
o
oo
o o o
oo
oo
o
o
o
o
ooo
oo
oo
ooo o
o
o
oo o
o
o
o
o
ooo
o
o
o
o
oo
o
o
ooo
o
o
oo
oo
o
o
o
o
o
o
o o
o
ooo
o
o
oooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
ooo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o oooo
o
o
o
o
o o
o
o
o
o
o
ooo
o
o
o
o oo
o
oo o
oo
o
ooo
o
o
o oo
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
oo
ooo
oo o
o
o
o
o
o
oo o
o
o
o
o
o oo o
o
oo
o
ooo o
o
ooo o
o
oo oo
ooo
o
o
oo oo
oo
ooo oo
o
oooooo
o
o
oooo oo oo
o
o o ooo
o
o
o
oo oo
o
o
o
oo
o
oo o
oo
oo
o
o oo
oo o
o
ooo
ooo
ooo o
oo
oo o o
o o
ooo ooo
oo
o
o ooo oooo oooo oo
oo
oooo oo o
oo
oo
oo
o
o
o
o o
oo
o
oo ooo o
o
oo
o
o
o
oo o
oo
o ooo
oo
ooo
o
o
o
ooo
oooo
ooo
o ooo
oo
ooo
oo oo
o
o
o
ooo
o
ooo
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
ooooo
oo
o o
oo
o
ooo
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
oo
o
o o
o
o
o o
oo
o
o
o
o
o
o
oo
o
o
o
o
oo
o o
o
o o
o
o
o
o
o
oo
oo
o
o
o
oo
o
o
o
o
oo
ooo
o
oo oo
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
oo
o
o
o
oo
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o o
o
o o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o o
o
o
o
o
o o
o
o ooo
o
o
oo
o
oo ooo o oo
o
oo
o
o
o
o
o
o
o
oo
oo
o
o
o o
o
o
o
o
o
25

35

alcohol

o
o

o

o

o
oo
o
o
oo
oo
o
o
o
o
o
o
oo
o
o
o
oo o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o o
o o
o
o
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
oo
o o
o
o
o
o
o
oo o
oo o
oo
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
oo
o
oo
o
oooo
o
o
oooooooo
o
o
oo
oo
o
o
o
o
o
oo
o
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o

o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
ooo
o
o
o o o
o
o
o
o
o
oo o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
oo o
o
o
o
o o
o
o
o
o
oo
o
o
o
oo
o o
o
oo
o
o
o
o
o
oooo
o
o
o o
o o
o
o
o
o
o
o
ooooo
o
o
oo
o
o
o
ooo
o
o
o
o
o
o
ooo
o
o
o
o o
o o
oo
oo
ooo
oo
o
o
o o
oo
o
o
o o
o
o
o
o
oo
o
o
ooo
o
o
o
o
oooo
o
o
o
o
o
o
oo
o o
o
o
o
o
oo
o
o
o
oo
o
o
o o
o
o o
o o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o o
o
o
o o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o
ooo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
oo
o
o
oo
ooo
o
oo
o o
o
o o
o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
ooo oo
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o
oo oo
o
o
o o
o
oo o o
o
o
o
ooo o
o
o
oo
o
o oo
o
o
oo
o
oo
o o
o
o
o o
o o
o
o
o
oo
o
o
oo
oo
o
o
oo
oo
o
o
oo
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
oo
o
o
o
o
o
o oo
o
o
o
oo
o
o
o
o
o
o
o oo
o
o
o o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
oooooooo
o
o
o
oo
o
o
o
oo
o
o
o
o
o oo
oooo
o
o
o
o
o
o o
o
oo
oo
o
o o
o
o
o
oo
o
oo
oo
o
o
oo
oo
o
o
o
o
o
oooo
o
o
o o
oo
oo
o
o
o
o
o
o
o
o
oo
o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o o
o
o
o
o
o
o
o
o o
oo
o
oo o
o
o
o
o
oo
o
o
oo
oo
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
oo
o
oo
o o
o o
oo
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
ooo
o
oo o
o
o
ooo
o
oo o
o
o o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
oo
o
o
o
oo
o
o
o
o
o oo
o
o
o
o
o o
o
o
o
o
o
o
o o
o
o
oo
o
o
ooo
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
oo
o
o oo
o
o
oo
o
o
o
o
ooo
o
oo
o
o
o
o
o
o
o
oo
o
o o
o
o
o
oo
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
ooo
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o o
o
o o
o
oo
o
o
oo
o
o
o oo
oooo
oo
o
o
o
o
o o
o
oo
o
o
o
o
o
o
o o
o
o o
oo o
o o
o
oo
o
o
o o
oo
o o
o o
o
oo
oo
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o o
o
o o
o
o
o
o oo
o
o
oo
o
ooo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o

o
o
o
o oo
oooo
o
o
oo
o
o o o
oo
o
o
o
oo
o
ooo
o
o
o
o o
o
o
o
o
oo
o
o
o
oo
o
o
oo
o
o
o
o
oo
ooo
oo
o
oo
o o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
ooo
o
o
o o
o
oo
o
oooo
o
oo
o
o
o
o o o
o
oo
o
o
o
oo
oo
o o
oo
oo
o
oo
o
oo
oo
oo
o
oo
o
o
ooo
ooo
o
o
o o
o o
o
o
o o
o
o
o
o
oo
o o
o
oo
o o
o
o
oo
oo
o
oo
o o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o o
o
o o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
oo
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
oo
o
oo
oo
o
o
o
o o
oo
o
oo
o
o
o o
o
o
o oo
o oo
o o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o o o
o
o
o
o
oo
o
o
o
o
o
oo o
o
o
o
o
o
oo
o
o
o
o ooo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
o
o
o
o o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
oo
oo
o
o
o
o o
oo
o
o
o
o
o o
o
oo
oo o
o
o
o
o
o
o
o
o
o
ooo
oo
o
o
o
o
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o oo
o
o
o
o o
o
o
o
oo
o
o
o
o
oo
oo
o
oooo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
ooo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
ooo
oo
oo
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o o oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
oo
o ooo
o
oo o
o
o
o
o o
oo
oo
o o
o o
oo
o
o
o
oo
o
o oo
o
o
o
o
o
o
o
oo
o o
o o o
o o
o oo
o
oo
ooo
o oooooo
o
oo
o
o
o
o o
o
oo
o
oo
ooo
o
o
o
oo
o
o oo
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o o
o o
oo
o
oo
o o
o oo
o
o
o
o
o
o
o
o o
oooo
o
o
o
oo
o
o
o
ooo
oooooooo
o
o
o
o
oo
o
o
oo
o o
o
o o
o
oo
o
o
o
oo
o
o
o ooo o
o
o
o
o
o
oo
o
o
o
o
o
o oo
o
oo
o o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
ooo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

age

20

40

60

0
2
2

0
6
1

0
0
1

4
1

0
1

6

2

5
4

5
3

5
2

5
1

0
6

0
4

0
2

10 14

15

45

figure 4.12. a scatterplot matrix of the south african heart disease data.
each plot shows a pair of risk factors, and the cases and controls are color coded
(red is a case). the variable family history of heart disease (famhist) is binary
(yes or no).

124

4. linear methods for classi   cation

table 4.3. results from stepwise id28    t to south african heart
disease data.

(intercept)
tobacco
ldl
famhist
age

coe   cient
   4.204
0.081
0.168
0.924
0.044

std. error z score
   8.45
3.16
3.09
4.14
4.52

0.498
0.026
0.054
0.223
0.010

other correlated variables, they are no longer needed (and can even get a
negative sign).

at this stage the analyst might do some model selection;    nd a subset
of the variables that are su   cient for explaining their joint e   ect on the
prevalence of chd. one way to proceed by is to drop the least signi   cant co-
e   cient, and re   t the model. this is done repeatedly until no further terms
can be dropped from the model. this gave the model shown in table 4.3.
a better but more time-consuming strategy is to re   t each of the models
with one variable removed, and then perform an analysis of deviance to
decide which variable to exclude. the residual deviance of a    tted model
is minus twice its log-likelihood, and the deviance between two models is
the di   erence of their individual residual deviances (in analogy to sums-of-
squares). this strategy gave the same    nal model as above.

how does one interpret a coe   cient of 0.081 (std. error = 0.026) for
tobacco, for example? tobacco is measured in total lifetime usage in kilo-
grams, with a median of 1.0kg for the controls and 4.1kg for the cases. thus
an increase of 1kg in lifetime tobacco usage accounts for an increase in the
odds of coronary heart disease of exp(0.081) = 1.084 or 8.4%. incorporat-
ing the standard error we get an approximate 95% con   dence interval of
exp(0.081    2    0.026) = (1.03, 1.14).
we return to these data in chapter 5, where we see that some of the
variables have nonlinear e   ects, and when modeled appropriately, are not
excluded from the model.

4.4.3 quadratic approximations and id136
the maximum-likelihood parameter estimates      satisfy a self-consistency
relationship: they are the coe   cients of a weighted least squares    t, where
the responses are

zi = xt
i

     +

(yi       pi)
  pi(1       pi)

,

(4.29)

4.4 id28

125

and the weights are wi =   pi(1      pi), both depending on      itself. apart from
providing a convenient algorithm, this connection with least squares has
more to o   er:

    the weighted residual sum-of-squares is the familiar pearson chi-

square statistic

nxi=1

(yi       pi)2
  pi(1       pi)

,

(4.30)

a quadratic approximation to the deviance.

    asymptotic likelihood theory says that if the model is correct, then

     is consistent (i.e., converges to the true   ).

    a central limit theorem then shows that the distribution of      con-
verges to n (  , (xt wx)   1). this and other asymptotics can be de-
rived directly from the weighted least squares    t by mimicking normal
theory id136.

    model building can be costly for id28 models, because
each model    tted requires iteration. popular shortcuts are the rao
score test which tests for inclusion of a term, and the wald test which
can be used to test for exclusion of a term. neither of these require
iterative    tting, and are based on the maximum-likelihood    t of the
current model. it turns out that both of these amount to adding
or dropping a term from the weighted least squares    t, using the
same weights. such computations can be done e   ciently, without
recomputing the entire weighted least squares    t.

software implementations can take advantage of these connections. for
example, the generalized linear modeling software in r (which includes lo-
gistic regression as part of the binomial family of models) exploits them
fully. glm (generalized linear model) objects can be treated as linear model
objects, and all the tools available for linear models can be applied auto-
matically.

4.4.4 l1 regularized id28

the l1 penalty used in the lasso (section 3.4.2) can be used for variable
selection and shrinkage with any id75 model. for logistic re-
gression, we would maximize a penalized version of (4.20):

max

  0,           

nxi=1hyi(  0 +   t xi)     log(1 + e  0+  t xi )i       

pxj=1

.

(4.31)

as with the lasso, we typically do not penalize the intercept term, and stan-
dardize the predictors for the penalty to be meaningful. criterion (4.31) is

|  j|         

126

4. linear methods for classi   cation

concave, and a solution can be found using nonid135 meth-
ods (koh et al., 2007, for example). alternatively, using the same quadratic
approximations that were used in the newton algorithm in section 4.4.1,
we can solve (4.31) by repeated application of a weighted lasso algorithm.
interestingly, the score equations [see (4.24)] for the variables with non-zero
coe   cients have the form

xt
j (y     p) =       sign(  j),

(4.32)

which generalizes (3.58) in section 3.4.4; the active variables are tied in
their generalized correlation with the residuals.

path algorithms such as lar for lasso are more di   cult, because the
coe   cient pro   les are piecewise smooth rather than linear. nevertheless,
progress can be made using quadratic approximations.

1

2

4

5

6

7

6
0

.

4
0

.

2

.

0

0

.

0

)
  
(
j
  

s
t
n
e
i
c
   
e
o
c

*********************************************************************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************************************************************

*********************************************************************************************************************************************************************************************************************************************
*********************************************************************************************************************************************************************************************************************************************

*********************************************************************************************************************************************************************************************************************************************

age

famhist

ldl
tobacco

sbp

alcohol

obesity

0.0

0.5

1.0

1.5

2.0

||  (  )||1

figure 4.13. l1 regularized id28 coe   cients for the south
african heart disease data, plotted as a function of the l1 norm. the variables
were all standardized to have unit variance. the pro   les are computed exactly at
each of the plotted points.

figure 4.13 shows the l1 id173 path for the south african
heart disease data of section 4.4.2. this was produced using the r package
glmpath (park and hastie, 2007), which uses predictor   corrector methods
of id76 to identify the exact values of    at which the active
set of non-zero coe   cients changes (vertical lines in the    gure). here the
pro   les look almost linear; in other examples the curvature will be more
visible.

coordinate descent methods (section 3.8.6) are very e   cient for comput-
ing the coe   cient pro   les on a grid of values for   . the r package glmnet

4.4 id28

127

(friedman et al., 2010) can    t coe   cient paths for very large logistic re-
gression problems e   ciently (large in n or p). their algorithms can exploit
sparsity in the predictor matrix x, which allows for even larger problems.
see section 18.4 for more details, and a discussion of l1-regularized multi-
nomial models.

4.4.5 id28 or lda?

in section 4.3 we    nd that the log-posterior odds between class k and k
are linear functions of x (4.9):

log

pr(g = k|x = x)
pr(g = k|x = x)

(  k +   k)t      1(  k       k)

= log

1
2

  k
  k    
+xt      1(  k       k )

=   k0 +   t

k x.

(4.33)

this linearity is a consequence of the gaussian assumption for the class
densities, as well as the assumption of a common covariance matrix. the
linear logistic model (4.17) by construction has linear logits:

log

pr(g = k|x = x)
pr(g = k|x = x)

=   k0 +   t

k x.

(4.34)

it seems that the models are the same. although they have exactly the same
form, the di   erence lies in the way the linear coe   cients are estimated. the
id28 model is more general, in that it makes less assumptions.
we can write the joint density of x and g as

pr(x, g = k) = pr(x)pr(g = k|x),

(4.35)

where pr(x) denotes the marginal density of the inputs x. for both lda
and id28, the second term on the right has the logit-linear
form

e  k0+  t
k x
   =1 e     0+  t
    x

1 +pk   1

pr(g = k|x = x) =

,

(4.36)

where we have again arbitrarily chosen the last class as the reference.

the id28 model leaves the marginal density of x as an arbi-
trary density function pr(x), and    ts the parameters of pr(g|x) by max-
imizing the conditional likelihood   the multinomial likelihood with proba-
bilities the pr(g = k|x). although pr(x) is totally ignored, we can think
of this marginal density as being estimated in a fully nonparametric and
unrestricted fashion, using the empirical distribution function which places
mass 1/n at each observation.

with lda we    t the parameters by maximizing the full log-likelihood,

based on the joint density

pr(x, g = k) =   (x;   k,   )  k,

(4.37)

128

4. linear methods for classi   cation

where    is the gaussian density function. standard normal theory leads
easily to the estimates     k,     , and     k given in section 4.3. since the linear
parameters of the logistic form (4.33) are functions of the gaussian param-
eters, we get their maximum-likelihood estimates by plugging in the corre-
sponding estimates. however, unlike in the conditional case, the marginal
density pr(x) does play a role here. it is a mixture density

pr(x) =

kxk=1

  k  (x;   k,   ),

(4.38)

which also involves the parameters.

what role can this additional component/restriction play? by relying
on the additional model assumptions, we have more information about the
parameters, and hence can estimate them more e   ciently (lower variance).
if in fact the true fk(x) are gaussian, then in the worst case ignoring this
marginal part of the likelihood constitutes a loss of e   ciency of about 30%
asymptotically in the error rate (efron, 1975). id141: with 30%
more data, the conditional likelihood will do as well.

for example, observations far from the decision boundary (which are
down-weighted by id28) play a role in estimating the common
covariance matrix. this is not all good news, because it also means that
lda is not robust to gross outliers.

from the mixture formulation, it is clear that even observations without
class labels have information about the parameters. often it is expensive
to generate class labels, but unclassi   ed observations come cheaply. by
relying on strong model assumptions, such as here, we can use both types
of information.

the marginal likelihood can be thought of as a regularizer, requiring
in some sense that class densities be visible from this marginal view. for
example, if the data in a two-class id28 model can be per-
fectly separated by a hyperplane, the maximum likelihood estimates of the
parameters are unde   ned (i.e., in   nite; see exercise 4.5). the lda coe   -
cients for the same data will be well de   ned, since the marginal likelihood
will not permit these degeneracies.

in practice these assumptions are never correct, and often some of the
components of x are qualitative variables. it is generally felt that logistic
regression is a safer, more robust bet than the lda model, relying on fewer
assumptions. it is our experience that the models give very similar results,
even when lda is used inappropriately, such as with qualitative predictors.

4.5 separating hyperplanes

129

figure 4.14. a toy example with two classes separable by a hyperplane. the
orange line is the least squares solution, which misclassi   es one of the training
points. also shown are two blue separating hyperplanes found by the id88
learning algorithm with di   erent random starts.

4.5 separating hyperplanes

we have seen that id156 and id28 both
estimate linear decision boundaries in similar but slightly di   erent ways.
for the rest of this chapter we describe separating hyperplane classi   ers.
these procedures construct linear decision boundaries that explicitly try
to separate the data into di   erent classes as well as possible. they provide
the basis for support vector classi   ers, discussed in chapter 12. the math-
ematical level of this section is somewhat higher than that of the previous
sections.

figure 4.14 shows 20 data points in two classes in ir2. these data can be
separated by a linear boundary. included in the    gure (blue lines) are two
of the in   nitely many possible separating hyperplanes. the orange line is
the least squares solution to the problem, obtained by regressing the    1/1
response y on x (with intercept); the line is given by

{x :     0 +     1x1 +     2x2 = 0}.

(4.39)

this least squares solution does not do a perfect job in separating the
points, and makes one error. this is the same boundary found by lda,
in light of its equivalence with id75 in the two-class case (sec-
tion 4.3 and exercise 4.2).

classi   ers such as (4.39), that compute a linear combination of the input
features and return the sign, were called id88s in the engineering liter-

130

4. linear methods for classi   cation

x0

x

  0 +   t x = 0

     

figure 4.15. the id202 of a hyperplane (a   ne set).

ature in the late 1950s (rosenblatt, 1958). id88s set the foundations
for the neural network models of the 1980s and 1990s.

before we continue, let us digress slightly and review some vector algebra.
figure 4.15 depicts a hyperplane or a   ne set l de   ned by the equation
f (x) =   0 +   t x = 0; since we are in ir2 this is a line.

here we list some properties:
1. for any two points x1 and x2 lying in l,   t (x1     x2) = 0, and hence

      =   /||  || is the vector normal to the surface of l.

2. for any point x0 in l,   t x0 =      0.
3. the signed distance of any point x to l is given by

     t (x     x0) =
=

(  t x +   0)

1
k  k
1
||f    (x)||

f (x).

(4.40)

hence f (x) is proportional to the signed distance from x to the hyperplane
de   ned by f (x) = 0.

4.5.1 rosenblatt   s id88 learning algorithm

the id88 learning algorithm tries to    nd a separating hyperplane by
minimizing the distance of misclassi   ed points to the decision boundary. if

4.5 separating hyperplanes

131

a response yi = 1 is misclassi   ed, then xt
a misclassi   ed response with yi =    1. the goal is to minimize

i    +   0 < 0, and the opposite for

d(  ,   0) =    xi   m

yi(xt

i    +   0),

(4.41)

where m indexes the set of misclassi   ed points. the quantity is non-
negative and proportional to the distance of the misclassi   ed points to
the decision boundary de   ned by   t x +   0 = 0. the gradient (assuming
m is    xed) is given by

   

   

d(  ,   0)

     

d(  ,   0)

     0

=    xi   m
=    xi   m

yixi,

yi.

(4.42)

(4.43)

the algorithm in fact uses stochastic id119 to minimize this
piecewise linear criterion. this means that rather than computing the sum
of the gradient contributions of each observation followed by a step in the
negative gradient direction, a step is taken after each observation is visited.
hence the misclassi   ed observations are visited in some sequence, and the
parameters    are updated via

(cid:18)   
  0(cid:19)    (cid:18)   

  0(cid:19) +   (cid:18)yixi
yi (cid:19) .

(4.44)

here    is the learning rate, which in this case can be taken to be 1 without
loss in generality. if the classes are linearly separable, it can be shown that
the algorithm converges to a separating hyperplane in a    nite number of
steps (exercise 4.6). figure 4.14 shows two solutions to a toy problem, each
started at a di   erent random guess.

there are a number of problems with this algorithm, summarized in

ripley (1996):

    when the data are separable, there are many solutions, and which

one is found depends on the starting values.

    the       nite    number of steps can be very large. the smaller the gap,

the longer the time to    nd it.

    when the data are not separable, the algorithm will not converge,
and cycles develop. the cycles can be long and therefore hard to
detect.

the second problem can often be eliminated by seeking a hyperplane not
in the original space, but in a much enlarged space obtained by creating

132

4. linear methods for classi   cation

many basis-function transformations of the original variables. this is anal-
ogous to driving the residuals in a polynomial regression problem down
to zero by making the degree su   ciently large. perfect separation cannot
always be achieved: for example, if observations from two di   erent classes
share the same input. it may not be desirable either, since the resulting
model is likely to be over   t and will not generalize well. we return to this
point at the end of the next section.

a rather elegant solution to the    rst problem is to add additional con-

straints to the separating hyperplane.

4.5.2 optimal separating hyperplanes

the optimal separating hyperplane separates the two classes and maximizes
the distance to the closest point from either class (vapnik, 1996). not only
does this provide a unique solution to the separating hyperplane problem,
but by maximizing the margin between the two classes on the training data,
this leads to better classi   cation performance on test data.

we need to generalize criterion (4.41). consider the optimization problem

max

  ,  0,||  ||=1

m

subject to yi(xt

i    +   0)     m, i = 1, . . . , n.

(4.45)

the set of conditions ensure that all the points are at least a signed
distance m from the decision boundary de   ned by    and   0, and we seek
the largest such m and associated parameters. we can get rid of the ||  || =
1 constraint by replacing the conditions with

1
||  ||

yi(xt

i    +   0)     m,

(which rede   nes   0) or equivalently

yi(xt

i    +   0)     m||  ||.

(4.46)

(4.47)

since for any    and   0 satisfying these inequalities, any positively scaled
multiple satis   es them too, we can arbitrarily set ||  || = 1/m . thus (4.45)
is equivalent to

min
  ,  0
subject to yi(xt

1
2||  ||2
i    +   0)     1, i = 1, . . . , n.

(4.48)

in light of (4.40), the constraints de   ne an empty slab or margin around the
linear decision boundary of thickness 1/||  ||. hence we choose    and   0 to
maximize its thickness. this is a id76 problem (quadratic

4.5 separating hyperplanes

133

criterion with linear inequality constraints). the lagrange (primal) func-
tion, to be minimized w.r.t.    and   0, is

lp =

1
2||  ||2    

nxi=1

  i[yi(xt

i    +   0)     1].

(4.49)

setting the derivatives to zero, we obtain:

   =

0 =

  iyixi,

  iyi,

nxi=1
nxi=1

(4.50)

(4.51)

and substituting these in (4.49) we obtain the so-called wolfe dual

ld =

nxi=1

  i    

1
2

nxi=1

nxk=1

  i  kyiykxt

i xk

subject to   i     0 and

  iyi = 0.

(4.52)

nxi=1

the solution is obtained by maximizing ld in the positive orthant, a sim-
pler id76 problem, for which standard software can be used.
in addition the solution must satisfy the karush   kuhn   tucker conditions,
which include (4.50), (4.51), (4.52) and

  i[yi(xt

i    +   0)     1] = 0    i.

(4.53)

from these we can see that
    if   i > 0, then yi(xt
boundary of the slab;

i    +   0) = 1, or in other words, xi is on the

    if yi(xt

i    +   0) > 1, xi is not on the boundary of the slab, and   i = 0.

from (4.50) we see that the solution vector    is de   ned in terms of a linear
combination of the support points xi   those points de   ned to be on the
boundary of the slab via   i > 0. figure 4.16 shows the optimal separating
hyperplane for our toy example; there are three support points. likewise,
  0 is obtained by solving (4.53) for any of the support points.

the optimal separating hyperplane produces a function   f (x) = xt      +     0

for classifying new observations:

  g(x) = sign   f (x).

(4.54)

although none of the training observations fall in the margin (by con-
struction), this will not necessarily be the case for test observations. the

134

4. linear methods for classi   cation

figure 4.16. the same data as in figure 4.14. the shaded region delineates
the maximum margin separating the two classes. there are three support points
indicated, which lie on the boundary of the margin, and the optimal separating
hyperplane (blue line) bisects the slab. included in the    gure is the boundary found
using id28 (red line), which is very close to the optimal separating
hyperplane (see section 12.3.3).

intuition is that a large margin on the training data will lead to good
separation on the test data.

the description of the solution in terms of support points seems to sug-
gest that the optimal hyperplane focuses more on the points that count,
and is more robust to model misspeci   cation. the lda solution, on the
other hand, depends on all of the data, even points far away from the de-
cision boundary. note, however, that the identi   cation of these support
points required the use of all the data. of course, if the classes are really
gaussian, then lda is optimal, and separating hyperplanes will pay a price
for focusing on the (noisier) data at the boundaries of the classes.

included in figure 4.16 is the id28 solution to this prob-
lem,    t by maximum likelihood. both solutions are similar in this case.
when a separating hyperplane exists, id28 will always    nd
it, since the log-likelihood can be driven to 0 in this case (exercise 4.5).
the id28 solution shares some other qualitative features with
the separating hyperplane solution. the coe   cient vector is de   ned by a
weighted least squares    t of a zero-mean linearized response on the input
features, and the weights are larger for points near the decision boundary
than for those further away.

when the data are not separable, there will be no feasible solution to
this problem, and an alternative formulation is needed. again one can en-
large the space using basis transformations, but this can lead to arti   cial

separation through over-   tting. in chapter 12 we discuss a more attractive
alternative known as the support vector machine, which allows for overlap,
but minimizes a measure of the extent of this overlap.

exercises

135

bibliographic notes

good general texts on classi   cation include duda et al. (2000), hand
(1981), mclachlan (1992) and ripley (1996). mardia et al. (1979) have
a concise discussion of id156. michie et al. (1994)
compare a large number of popular classi   ers on benchmark datasets. lin-
ear separating hyperplanes are discussed in vapnik (1996). our account of
the id88 learning algorithm follows ripley (1996).

exercises

ex. 4.1 show how to solve the generalized eigenvalue problem max at ba
subject to at wa = 1 by transforming to a standard eigenvalue problem.
ex. 4.2 suppose we have features x     irp, a two-class response, with class
sizes n1, n2, and the target coded as    n/n1, n/n2.
(a) show that the lda rule classi   es to class 2 if

   1

xt     

(    2         1) >
and class 1 otherwise.

1
2

(    2 +     1)t     

   1

(    2         1)     log(n2/n1),

(b) consider minimization of the least squares criterion

nxi=1

(yi       0     xt

i   )2.

show that the solution      satis   es

h(n     2)      + n     bi    = n (    2         1)

(after simpli   cation), where     b = n1n2

n 2 (    2         1)(    2         1)t .
(c) hence show that     b   is in the direction (    2         1) and thus

   1

             

(    2         1).

(4.55)

(4.56)

(4.57)

therefore the least-squares regression coe   cient is identical to the
lda coe   cient, up to a scalar multiple.

136

4. linear methods for classi   cation

(d) show that this result holds for any (distinct) coding of the two classes.

(e) find the solution     0 (up to the same scalar multiple as in (c), and
hence the predicted value   f (x) =     0 + xt     . consider the following
rule: classify to class 2 if   f (x) > 0 and class 1 otherwise. show this is
not the same as the lda rule unless the classes have equal numbers
of observations.

(fisher, 1936; ripley, 1996)

ex. 4.3 suppose we transform the original predictors x to   y via linear
regression. in detail, let   y = x(xt x)   1xt y = x   b, where y is the
indicator response matrix. similarly for any input x     irp, we get a trans-
formed vector   y =   bt x     irk . show that lda using   y is identical to
lda in the original space.

ex. 4.4 consider the multilogit model with k classes (4.17). let    be the
(p + 1)(k     1)-vector consisting of all the coe   cients. de   ne a suitably
enlarged version of the input vector x to accommodate this vectorized co-
e   cient matrix. derive the newton-raphson algorithm for maximizing the
multinomial log-likelihood, and describe how you would implement this
algorithm.

ex. 4.5 consider a two-class id28 problem with x     ir. char-
acterize the maximum-likelihood estimates of the slope and intercept pa-
rameter if the sample xi for the two classes are separated by a point x0     ir.
generalize this result to (a) x     irp (see figure 4.16), and (b) more than
two classes.

ex. 4.6 suppose we have n points xi in irp in general position, with class
labels yi     {   1, 1}. prove that the id88 learning algorithm converges
to a separating hyperplane in a    nite number of steps:

(a) denote a hyperplane by f (x) =   t

1 x +   0 = 0, or in more compact
notation   t x    = 0, where x    = (x, 1) and    = (  1,   0). let zi =
i /||x   
x   
i ||. show that separability implies the existence of a   sep such
that yi  t

sepzi     1    i

(b) given a current   old, the id88 algorithm identi   es a point zi that
is misclassi   ed, and produces the update   new       old + yizi. show
that ||  new     sep||2     ||  old     sep||2   1, and hence that the algorithm
converges to a separating hyperplane in no more than ||  start      sep||2
steps (ripley, 1996).

ex. 4.7 consider the criterion

d   (  ,   0) =    

nxi=1

yi(xt

i    +   0),

(4.58)

exercises

137

a generalization of (4.41) where we sum over all the observations. consider
minimizing d    subject to ||  || = 1. describe this criterion in words. does
it solve the optimal separating hyperplane problem?

ex. 4.8 consider the multivariate gaussian model x|g = k     n (  k,   ),
with the additional restriction that rank{  k}k
1 = l < max(k     1, p).
derive the constrained id113s for the   k and   . show that the bayes clas-
si   cation rule is equivalent to classifying in the reduced subspace computed
by lda (hastie and tibshirani, 1996b).

ex. 4.9 write a computer program to perform a quadratic discriminant
analysis by    tting a separate gaussian model per class. try it out on the
vowel data, and compute the misclassi   cation error for the test data. the
data can be found in the book website www-stat.stanford.edu/elemstatlearn.

138

4. linear methods for classi   cation

5
basis expansions and id173

this is page 139
printer: opaque this

5.1

introduction

we have already made use of models linear in the input features, both for
regression and classi   cation. id75, id156,
id28 and separating hyperplanes all rely on a linear model.
it is extremely unlikely that the true function f (x) is actually linear in
x. in regression problems, f (x) = e(y |x) will typically be nonlinear and
nonadditive in x, and representing f (x) by a linear model is usually a con-
venient, and sometimes a necessary, approximation. convenient because a
linear model is easy to interpret, and is the    rst-order taylor approxima-
tion to f (x). sometimes necessary, because with n small and/or p large,
a linear model might be all we are able to    t to the data without over   t-
ting. likewise in classi   cation, a linear, bayes-optimal decision boundary
implies that some monotone transformation of pr(y = 1|x) is linear in x.
this is inevitably an approximation.
in this chapter and the next we discuss popular methods for moving
beyond linearity. the core idea in this chapter is to augment/replace the
vector of inputs x with additional variables, which are transformations of
x, and then use linear models in this new space of derived input features.
denote by hm(x) : irp 7    ir the mth transformation of x, m =

1, . . . , m . we then model

f (x) =

mxm=1

  mhm(x),

(5.1)

140

5. basis expansions and id173

a linear basis expansion in x. the beauty of this approach is that once the
basis functions hm have been determined, the models are linear in these
new variables, and the    tting proceeds as before.

some simple and widely used examples of the hm are the following:

    hm(x) = xm, m = 1, . . . , p recovers the original linear model.
    hm(x) = x 2

j or hm(x) = xjxk allows us to augment the inputs with
polynomial terms to achieve higher-order taylor expansions. note,
however, that the number of variables grows exponentially in the de-
gree of the polynomial. a full quadratic model in p variables requires
o(p2) square and cross-product terms, or more generally o(pd) for a
degree-d polynomial.

    hm(x) = log(xj), pxj, . . . permits other nonlinear transformations

of single inputs. more generally one can use similar functions involv-
ing several inputs, such as hm(x) = ||x||.

    hm(x) = i(lm     xk < um), an indicator for a region of xk. by
breaking the range of xk up into mk such nonoverlapping regions
results in a model with a piecewise constant contribution for xk.

sometimes the problem at hand will call for particular basis functions hm,
such as logarithms or power functions. more often, however, we use the basis
expansions as a device to achieve more    exible representations for f (x).
polynomials are an example of the latter, although they are limited by
their global nature   tweaking the coe   cients to achieve a functional form
in one region can cause the function to    ap about madly in remote regions.
in this chapter we consider more useful families of piecewise-polynomials
and splines that allow for local polynomial representations. we also discuss
the wavelet bases, especially useful for modeling signals and images. these
methods produce a dictionary d consisting of typically a very large number
|d| of basis functions, far more than we can a   ord to    t to our data. along
with the dictionary we require a method for controlling the complexity
of our model, using basis functions from the dictionary. there are three
common approaches:

    restriction methods, where we decide before-hand to limit the class
of functions. additivity is an example, where we assume that our
model has the form

f (x) =

=

pxj=1
pxj=1

fj(xj)

mjxm=1

  jmhjm(xj).

(5.2)

5.2 piecewise polynomials and splines

141

the size of the model is limited by the number of basis functions mj
used for each component function fj.

    selection methods, which adaptively scan the dictionary and include
only those basis functions hm that contribute signi   cantly to the    t of
the model. here the variable selection techniques discussed in chap-
ter 3 are useful. the stagewise greedy approaches such as cart,
mars and boosting fall into this category as well.

    id173 methods where we use the entire dictionary but re-
strict the coe   cients. ridge regression is a simple example of a regu-
larization approach, while the lasso is both a id173 and selec-
tion method. here we discuss these and more sophisticated methods
for id173.

5.2 piecewise polynomials and splines

we assume until section 5.7 that x is one-dimensional. a piecewise poly-
nomial function f (x) is obtained by dividing the domain of x into contigu-
ous intervals, and representing f by a separate polynomial in each interval.
figure 5.1 shows two simple piecewise polynomials. the    rst is piecewise
constant, with three basis functions:

h1(x) = i(x <   1),

h3(x) = i(  2     x).
since these are positive over disjoint regions, the least squares estimate of
m=1   mhm(x) amounts to     m =   ym, the mean of y

h2(x) = i(  1     x <   2),

the model f (x) =p3

in the mth region.

the top right panel shows a piecewise linear    t. three additional basis
functions are needed: hm+3 = hm(x)x, m = 1, . . . , 3. except in special
cases, we would typically prefer the third panel, which is also piecewise
linear, but restricted to be continuous at the two knots. these continu-
ity restrictions lead to linear constraints on the parameters; for example,
f (     
1 ) implies that   1 +   1  4 =   2 +   1  5. in this case, since there
are two restrictions, we expect to get back two parameters, leaving four free
parameters.

1 ) = f (  +

a more direct way to proceed in this case is to use a basis that incorpo-

rates the constraints:

h1(x) = 1,

h2(x) = x,

h3(x) = (x       1)+,

h4(x) = (x       2)+,
where t+ denotes the positive part. the function h3 is shown in the lower
right panel of figure 5.1. we often prefer smoother functions, and these
can be achieved by increasing the order of the local polynomial. figure 5.2
shows a series of piecewise-cubic polynomials    t to the same data, with

142

5. basis expansions and id173

piecewise constant

piecewise linear

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

o

o

o
o

o

o
o

o
o

o

o
o
o

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o
o

o

o
o

o
o

o

o
o
o

o

o

o

o

o

o

o

o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

o

o

o

o

  1

  2

  1

  2

continuous piecewise linear

piecewise-linear basis function

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o
o

o

o
o

o
o

o

o
o
o

o

o

o

o

o

o

o

o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

o

o

o

   
   
   

   
   
   
      
   
   
   

(x       1)+

   
   

   

   

   

   
   
   

   
   

   
   
   

   

   

      
   
   

   
   
   
   
   
      
   

   
   

   
   

   
   

   

   
   
   

   
   

  1

  2

  1

  2

figure 5.1. the top left panel shows a piecewise constant function    t to some
arti   cial data. the broken vertical lines indicate the positions of the two knots
  1 and   2. the blue curve represents the true function, from which the data were
generated with gaussian noise. the remaining two panels show piecewise lin-
ear functions    t to the same data   the top right unrestricted, and the lower left
restricted to be continuous at the knots. the lower right panel shows a piecewise   
linear basis function, h3(x) = (x       1)+, continuous at   1. the black points
indicate the sample evaluations h3(xi), i = 1, . . . , n .

5.2 piecewise polynomials and splines

143

piecewise cubic polynomials

discontinuous

continuous

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o

o
o

o

o

o

o
o

o
o
o

o

o

o
o

o
o

o

o
o

o

o

o

o

o

o

o
o
o

o

o

o

o

  1

  2

continuous first derivative

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o

o
o

o

o

o

o
o

o
o
o

o

o

o
o

o
o

o

o
o

o

o

o

o

o

o

o
o
o

o

o

o

o
o

o

o
o

o

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o

o
o

o

o

o

o
o

o
o
o

o

o

o
o

o
o

o

o
o

o

o

o

o

o

o

o
o
o

o

o

o

o

  1

  2

continuous second derivative

o
o

o

o

o
o
o
o

o

o

o

o

o

o

o

o
o

o

o

o

o
o

o
o
o

o

o

o
o

o
o

o

o
o

o

o

o

o

o

o

o
o
o

o

o

o

o
o

o

o
o

o

o

o

  1

  2

  1

  2

figure 5.2. a series of piecewise-cubic polynomials, with increasing orders of
continuity.

increasing orders of continuity at the knots. the function in the lower
right panel is continuous, and has continuous    rst and second derivatives
at the knots. it is known as a cubic spline. enforcing one more order of
continuity would lead to a global cubic polynomial. it is not hard to show
(exercise 5.1) that the following basis represents a cubic spline with knots
at   1 and   2:

h3(x) = x 2, h5(x) = (x       1)3
+,
h1(x) = 1,
h2(x) = x, h4(x) = x 3, h6(x) = (x       2)3
+.

(5.3)

there are six basis functions corresponding to a six-dimensional linear space
of functions. a quick check con   rms the parameter count: (3 regions)  (4
parameters per region)    (2 knots)  (3 constraints per knot)= 6.

144

5. basis expansions and id173

more generally, an order-m spline with knots   j, j = 1, . . . , k is a
piecewise-polynomial of order m , and has continuous derivatives up to
order m     2. a cubic spline has m = 4. in fact the piecewise-constant
function in figure 5.1 is an order-1 spline, while the continuous piece-
wise linear function is an order-2 spline. likewise the general form for the
truncated-power basis set would be

hj(x) = x j   1, j = 1, . . . , m,

hm +   (x) = (x          )m    1

+

,     = 1, . . . , k.

it is claimed that cubic splines are the lowest-order spline for which the
knot-discontinuity is not visible to the human eye. there is seldom any
good reason to go beyond cubic-splines, unless one is interested in smooth
derivatives. in practice the most widely used orders are m = 1, 2 and 4.

these    xed-knot splines are also known as regression splines. one needs
to select the order of the spline, the number of knots and their placement.
one simple approach is to parameterize a family of splines by the number
of basis functions or degrees of freedom, and have the observations xi de-
termine the positions of the knots. for example, the expression bs(x,df=7)
in r generates a basis matrix of cubic-spline functions evaluated at the n
observations in x, with the 7    3 = 41 interior knots at the appropriate per-
centiles of x (20, 40, 60 and 80th.) one can be more explicit, however; bs(x,
degree=1, knots = c(0.2, 0.4, 0.6)) generates a basis for linear splines,
with three interior knots, and returns an n    4 matrix.
since the space of spline functions of a particular order and knot sequence
is a vector space, there are many equivalent bases for representing them
(just as there are for ordinary polynomials.) while the truncated power
basis is conceptually simple, it is not too attractive numerically: powers of
large numbers can lead to severe rounding problems. the b-spline basis,
described in the appendix to this chapter, allows for e   cient computations
even when the number of knots k is large.

5.2.1 natural cubic splines

we know that the behavior of polynomials    t to data tends to be erratic
near the boundaries, and extrapolation can be dangerous. these problems
are exacerbated with splines. the polynomials    t beyond the boundary
knots behave even more wildly than the corresponding global polynomials
in that region. this can be conveniently summarized in terms of the point-
wise variance of spline functions    t by least squares (see the example in the
next section for details on these variance calculations). figure 5.3 compares

1a cubic spline with four knots is eight-dimensional. the bs() function omits by
default the constant term in the basis, since terms like this are typically included with
other terms in the model.

5.2 piecewise polynomials and splines

145

global linear
global cubic polynomial
cubic spline - 2 knots
natural cubic spline - 6 knots

      
      
      

          
   
   
      
      
                

               
               
      
      
                      
                      

                        
                  
                                           
                                           
                                           

   

            
                                                       
                               
      
                                                       
                                                       

   

   
   

      
      
      

      
             
      
      
      
                       

   

   
   

   

6

.

0

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

s
e
c
n
a
i
r
a
v
e
s
w
n
o
p

t

 

i

i

0.0

0.2

0.4

0.6

0.8

1.0

x

figure 5.3. pointwise variance curves for four di   erent models, with x con-
sisting of 50 points drawn at random from u [0, 1], and an assumed error model
with constant variance. the linear and cubic polynomial    ts have two and four
degrees of freedom, respectively, while the cubic spline and natural cubic spline
each have six degrees of freedom. the cubic spline has two knots at 0.33 and 0.66,
while the natural spline has boundary knots at 0.1 and 0.9, and four interior knots
uniformly spaced between them.

the pointwise variances for a variety of di   erent models. the explosion of
the variance near the boundaries is clear, and inevitably is worst for cubic
splines.

a natural cubic spline adds additional constraints, namely that the func-
tion is linear beyond the boundary knots. this frees up four degrees of
freedom (two constraints each in both boundary regions), which can be
spent more pro   tably by sprinkling more knots in the interior region. this
tradeo    is illustrated in terms of variance in figure 5.3. there will be a
price paid in bias near the boundaries, but assuming the function is lin-
ear near the boundaries (where we have less information anyway) is often
considered reasonable.

a natural cubic spline with k knots is represented by k basis functions.
one can start from a basis for cubic splines, and derive the reduced ba-
sis by imposing the boundary constraints. for example, starting from the
truncated power series basis described in section 5.2, we arrive at (exer-
cise 5.4):

n1(x) = 1, n2(x) = x, nk+2(x) = dk(x)     dk   1(x),

(5.4)

146

5. basis expansions and id173

where

dk(x) =

(x       k)3

+     (x       k )3
  k       k

+

.

(5.5)

each of these basis functions can be seen to have zero second and third
derivative for x       k .

5.2.2 example: south african heart disease (continued)

in section 4.4.2 we    t linear id28 models to the south african
heart disease data. here we explore nonlinearities in the functions using
natural splines. the functional form of the model is
logit[pr(chd|x)] =   0 + h1(x1)t   1 + h2(x2)t   2 +        + hp(xp)t   p, (5.6)
where each of the   j are vectors of coe   cients multiplying their associated
vector of natural spline basis functions hj.

we use four natural spline bases for each term in the model. for example,
with x1 representing sbp, h1(x1) is a basis consisting of four basis func-
tions. this actually implies three rather than two interior knots (chosen at
uniform quantiles of sbp), plus two boundary knots at the extremes of the
data, since we exclude the constant term from each of the hj.

since famhist is a two-level factor, it is coded by a simple binary or
dummy variable, and is associated with a single coe   cient in the    t of the
model.

h(x)t   , with total number of parameters df = 1 +pp

more compactly we can combine all p vectors of basis functions (and
the constant term) into one big vector h(x), and then the model is simply
j=1 dfj, the sum of
the parameters in each component term. each basis function is evaluated
at each of the n samples, resulting in a n    df basis matrix h. at this
point the model is like any other linear logistic model, and the algorithms
described in section 4.4.1 apply.

we carried out a backward stepwise deletion process, dropping terms
from this model while preserving the group structure of each term, rather
than dropping one coe   cient at a time. the aic statistic (section 7.5) was
used to drop terms, and all the terms remaining in the    nal model would
cause aic to increase if deleted from the model (see table 5.1). figure 5.4
shows a plot of the    nal model selected by the stepwise regression. the
functions displayed are   fj(xj) = hj(xj)t     j for each variable xj. the
covariance matrix cov(    ) =    is estimated by      = (ht wh)   1, where w
is the diagonal weight matrix from the id28. hence vj(xj) =
var[   fj(xj)] = hj(xj)t     jjhj(xj) is the pointwise variance function of   fj,
where cov(    j) =     jj is the appropriate sub-matrix of     . the shaded region

in each panel is de   ned by   fj(xj)    2pvj(xj).

the aic statistic is slightly more generous than the likelihood-ratio test
(deviance test). both sbp and obesity are included in this model, while

5.2 piecewise polynomials and splines

147

)
o
c
c
a
b
o
t
(
  f

8

6

4

2

0

100

120

140

160
sbp

180

200

220

0

5

10

15

20
tobacco

25

30

2

4

6

10

12

14

8
ldl

absent

famhist

present

)
t
s
i
h
m
a
f
(
  f

)
e
g
a
(
  f

4

2

0

2
-

4
-

2

0

2
-

4
-

6
-

)
p
b
s
(
  f

)
l
d
l
(
  f

)
y
t
i
s
e
b
o
(
  f

4

2

0

2
-

4

2

0

2
-

4
-

6

4

2

0

2
-

15

20

25

35

30
obesity

40

45

20

30

50

60

40
age

figure 5.4. fitted natural-spline functions for each of the terms in the    nal
model selected by the stepwise procedure. included are pointwise standard-error
bands. the rug plot at the base of each    gure indicates the location of each of the
sample values for that variable (jittered to break ties).

148

5. basis expansions and id173

table 5.1. final id28 model, after stepwise deletion of natural
splines terms. the column labeled    lrt    is the likelihood-ratio test statistic when
that term is deleted from the model, and is the change in deviance from the full
model (labeled    none   ).

terms df deviance
458.09
467.16
470.48
472.39
479.44
466.24
481.86

none
sbp
tobacco
ldl
famhist
obesity
age

4
4
4
1
4
4

aic
502.09
503.16
506.48
508.39
521.44
502.24
517.86

lrt p-value

9.076
12.387
14.307
21.356
8.147
23.768

0.059
0.015
0.006
0.000
0.086
0.000

they were not in the linear model. the    gure explains why, since their
contributions are inherently nonlinear. these e   ects at    rst may come as
a surprise, but an explanation lies in the nature of the retrospective data.
these measurements were made sometime after the patients su   ered a
heart attack, and in many cases they had already bene   ted from a healthier
diet and lifestyle, hence the apparent increase in risk at low values for
obesity and sbp. table 5.1 shows a summary of the selected model.

5.2.3 example: phoneme recognition

in this example we use splines to reduce    exibility rather than increase it;
the application comes under the general heading of functional modeling. in
the top panel of figure 5.5 are displayed a sample of 15 log-periodograms
for each of the two phonemes    aa    and    ao    measured at 256 frequencies.
the goal is to use such data to classify a spoken phoneme. these two
phonemes were chosen because they are di   cult to separate.

the input feature is a vector x of length 256, which we can think of as
a vector of evaluations of a function x(f ) over a grid of frequencies f . in
reality there is a continuous analog signal which is a function of frequency,
and we have a sampled version of it.

the gray lines in the lower panel of figure 5.5 show the coe   cients of
a linear id28 model    t by maximum likelihood to a training
sample of 1000 drawn from the total of 695    aa   s and 1022    ao   s. the
coe   cients are also plotted as a function of frequency, and in fact we can
think of the model in terms of its continuous counterpart

log

pr(aa|x)
pr(ao|x)

=z x(f )  (f )df,

(5.7)

5.2 piecewise polynomials and splines

149

phoneme examples

aa
ao

0

50

100

150

200

250

frequency

phoneme classification: raw and restricted id28

5
2

0
2

5
1

0
1

5

0

4
.
0

2
.
0

0
.
0

2
.
0
-

4
.
0
-

m
a
r
g
o
d
o
i
r
e
p
-
g
o
l

i

i

s
t
n
e
c
i
f
f
e
o
c
 
n
o
s
s
e
r
g
e
r
 
c
i
t
s
g
o
l

i

0

50

100

150

200

250

frequency

figure 5.5. the top panel displays the log-periodogram as a function of fre-
quency for 15 examples each of the phonemes    aa    and    ao    sampled from a total
of 695    aa   s and 1022    ao   s. each log-periodogram is measured at 256 uniformly
spaced frequencies. the lower panel shows the coe   cients (as a function of fre-
quency) of a id28    t to the data by maximum likelihood, using the
256 log-periodogram values as inputs. the coe   cients are restricted to be smooth
in the red curve, and are unrestricted in the jagged gray curve.

150

5. basis expansions and id173

which we approximate by

x(fj)  (fj) =

256xj=1

xj  j.

256xj=1

(5.8)

the coe   cients compute a contrast functional, and will have appreciable
values in regions of frequency where the log-periodograms di   er between
the two classes.

the gray curves are very rough. since the input signals have fairly strong
positive autocorrelation, this results in negative autocorrelation in the co-
e   cients. in addition the sample size e   ectively provides only four obser-
vations per coe   cient.

applications such as this permit a natural id173. we force the
coe   cients to vary smoothly as a function of frequency. the red curve in the
lower panel of figure 5.5 shows such a smooth coe   cient curve    t to these
data. we see that the lower frequencies o   er the most discriminatory power.
not only does the smoothing allow easier interpretation of the contrast, it
also produces a more accurate classi   er:

training error
test error

raw regularized
0.080
0.255

0.185
0.158

splines   (f ) =pm

the smooth red curve was obtained through a very simple use of natural
cubic splines. we can represent the coe   cient function as an expansion of
m=1 hm(f )  m. in practice this means that    = h   where,
h is a p    m basis matrix of natural cubic splines, de   ned on the set of
frequencies. here we used m = 12 basis functions, with knots uniformly
placed over the integers 1, 2, . . . , 256 representing the frequencies. since
xt    = xt h  , we can simply replace the input features x by their    ltered
versions x    = ht x, and    t    by linear id28 on the x   . the
red curve is thus     (f ) = h(f )t     .

5.3 filtering and feature extraction

in the previous example, we constructed a p   m basis matrix h, and then
transformed our features x into new features x    = ht x. these    ltered
versions of the features were then used as inputs into a learning procedure:
in the previous example, this was linear id28.

preprocessing of high-dimensional features is a very general and pow-
erful method for improving the performance of a learning algorithm. the
preprocessing need not be linear as it was above, but can be a general

5.4 smoothing splines

151

(nonlinear) function of the form x    = g(x). the derived features x    can
then be used as inputs into any (linear or nonlinear) learning procedure.

for example, for signal or image recognition a popular approach is to    rst
transform the raw features via a wavelet transform x    = ht x (section 5.9)
and then use the features x    as inputs into a neural network (chapter 11).
wavelets are e   ective in capturing discrete jumps or edges, and the neural
network is a powerful tool for constructing nonlinear functions of these
features for predicting the target variable. by using domain knowledge
to construct appropriate features, one can often improve upon a learning
method that has only the raw features x at its disposal.

5.4 smoothing splines

here we discuss a spline basis method that avoids the knot selection prob-
lem completely by using a maximal set of knots. the complexity of the    t
is controlled by id173. consider the following problem: among all
functions f (x) with two continuous derivatives,    nd one that minimizes the
penalized residual sum of squares

rss(f,   ) =

{yi     f (xi)}2 +   z {f       (t)}2dt,

(5.9)

nxi=1

where    is a    xed smoothing parameter. the    rst term measures closeness
to the data, while the second term penalizes curvature in the function, and
   establishes a tradeo    between the two. two special cases are:

   = 0 : f can be any function that interpolates the data.
   =     : the simple least squares line    t, since no second derivative can

be tolerated.

these vary from very rough to very smooth, and the hope is that        (0,   )
indexes an interesting class of functions in between.
the criterion (5.9) is de   ned on an in   nite-dimensional function space   
in fact, a sobolev space of functions for which the second term is de   ned.
remarkably, it can be shown that (5.9) has an explicit,    nite-dimensional,
unique minimizer which is a natural cubic spline with knots at the unique
values of the xi, i = 1, . . . , n (exercise 5.7). at face value it seems that
the family is still over-parametrized, since there are as many as n knots,
which implies n degrees of freedom. however, the penalty term translates
to a penalty on the spline coe   cients, which are shrunk some of the way
toward the linear    t.

since the solution is a natural spline, we can write it as

f (x) =

nxj=1

nj(x)  j,

(5.10)

152

5. basis expansions and id173

   

   

   

   

   

   

   

   

   

   
   

   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   
   
   

   
   

   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   

   

   

   

   
   
   

   

   

   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
      
   
   
   

   

   

   
   
   
   
   

d
m
b

 
l

i

a
n
p
s
n

 

i
 

e
g
n
a
h
c
e
v
i
t

 

l

a
e
r

0
2

.

0

5
1

.

0

0
1

.

0

5
0

.

0

0

.

0

5
0
0
-

.

   

   

   
   
   
   

   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   

   
   
   
   
   
   
   

   
   

   
   
   
   
   
   
   
   

   

   

   

   

   

male
female

   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   
   
   
   
   
   
   
   

   
   
   
   
   

   

   

   

   
   
   
   
   
   
   
   

   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   

   
   

   
   

   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   
   
   

   
   

   

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   

10

15

20

25

age

figure 5.6. the response is the relative change in bone mineral density mea-
sured at the spine in adolescents, as a function of age. a separate smoothing spline
was    t to the males and females, with        0.00022. this choice corresponds to
about 12 degrees of freedom.

where the nj(x) are an n -dimensional set of basis functions for repre-
senting this family of natural splines (section 5.2.1 and exercise 5.4). the
criterion thus reduces to

rss(  ,   ) = (y     n  )t (y     n  ) +     t    n   ,

where {n}ij = nj(xi) and {   n}jk = r n       

easily seen to be

     = (nt n +      n )   1nt y,

j (t)n       

k (t)dt. the solution is

(5.11)

(5.12)

a generalized ridge regression. the    tted smoothing spline is given by

  f (x) =

nj(x)    j.

nxj=1

(5.13)

e   cient computational techniques for smoothing splines are discussed in
the appendix to this chapter.

figure 5.6 shows a smoothing spline    t to some data on bone mineral
density (bmd) in adolescents. the response is relative change in spinal
bmd over two consecutive visits, typically about one year apart. the data
are color coded by gender, and two separate curves were    t. this simple

5.4 smoothing splines

153

summary reinforces the evidence in the data that the growth spurt for
females precedes that for males by about two years. in both cases the
smoothing parameter    was approximately 0.00022; this choice is discussed
in the next section.

5.4.1 degrees of freedom and smoother matrices

we have not yet indicated how    is chosen for the smoothing spline. later
in this chapter we describe automatic methods using techniques such as
cross-validation. in this section we discuss intuitive ways of prespecifying
the amount of smoothing.

a smoothing spline with prechosen    is an example of a linear smoother
(as in linear operator). this is because the estimated parameters in (5.12)
are a linear combination of the yi. denote by   f the n -vector of    tted values
  f (xi) at the training predictors xi. then

  f = n(nt n +      n )   1nt y

= s  y.

(5.14)

again the    t is linear in y, and the    nite linear operator s   is known as
the smoother matrix. one consequence of this linearity is that the recipe
for producing   f from y does not depend on y itself; s   depends only on
the xi and   .

linear operators are familiar in more traditional least squares    tting as
well. suppose b   is a n    m matrix of m cubic-spline basis functions
evaluated at the n training points xi, with knot sequence   , and m     n .
then the vector of    tted spline values is given by

  f = b  (bt

   b  )   1bt

   y

= h  y.

(5.15)

here the linear operator h   is a projection operator, also known as the hat
matrix in statistics. there are some important similarities and di   erences
between h   and s  :

    both are symmetric, positive semide   nite matrices.
    h  h   = h   (idempotent), while s  s   (cid:22) s  , meaning that the right-
hand side exceeds the left-hand side by a positive semide   nite matrix.
this is a consequence of the shrinking nature of s  , which we discuss
further below.

    h   has rank m , while s   has rank n .

the expression m = trace(h  ) gives the dimension of the projection space,
which is also the number of basis functions, and hence the number of pa-
rameters involved in the    t. by analogy we de   ne the e   ective degrees of

154

5. basis expansions and id173

freedom of a smoothing spline to be

df   = trace(s  ),

(5.16)

the sum of the diagonal elements of s  . this very useful de   nition allows
us a more intuitive way to parameterize the smoothing spline, and indeed
many other smoothers as well, in a consistent fashion. for example, in fig-
ure 5.6 we speci   ed df   = 12 for each of the curves, and the corresponding
       0.00022 was derived numerically by solving trace(s  ) = 12. there are
many arguments supporting this de   nition of degrees of freedom, and we
cover some of them here.

since s   is symmetric (and positive semide   nite), it has a real eigen-
decomposition. before we proceed, it is convenient to rewrite s   in the
reinsch form

where k does not depend on    (exercise 5.9). since   f = s  y solves

s   = (i +   k)   1,

min

f

(y     f )t (y     f ) +   f t kf ,

(5.17)

(5.18)

k is known as the penalty matrix, and indeed a quadratic form in k has
a representation in terms of a weighted sum of squared (divided) second
di   erences. the eigen-decomposition of s   is

with

s   =

nxk=1

  k(  )ukut
k

  k(  ) =

1

1 +   dk

,

(5.19)

(5.20)

and dk the corresponding eigenvalue of k. figure 5.7 (top) shows the re-
sults of applying a cubic smoothing spline to some air pollution data (128
observations). two    ts are given: a smoother    t corresponding to a larger
penalty    and a rougher    t for a smaller penalty. the lower panels repre-
sent the eigenvalues (lower left) and some eigenvectors (lower right) of the
corresponding smoother matrices. some of the highlights of the eigenrep-
resentation are the following:

    the eigenvectors are not a   ected by changes in   , and hence the whole
family of smoothing splines (for a particular sequence x) indexed by
   have the same eigenvectors.

    s  y = pn

k=1 uk  k(  )huk, yi, and hence the smoothing spline oper-
ates by decomposing y w.r.t. the (complete) basis {uk}, and di   er-
entially shrinking the contributions using   k(  ). this is to be con-
trasted with a basis-regression method, where the components are

5.4 smoothing splines

155

   

   

   

   

   

   

   

   

   
   

   

   
   
   
   

   

   

   

   

   

   

   
   
   

   
   
   
   
   

   

   
   
   

   
      
   
      

   
   

   
   
      
       
   
      
   
   
   

   

-50

   

   
   
   
   

   
   
      
   

   

      

0

   
   
   
   
   
         

   

   

   
      

   

   
      
   
   
   

      

   

   

   

   

   

   
      
   

   
   
   
   

   
   
   

   

   
   
      
   
   

   

   

   

   

   

   

50

   
   

   

   

100

n
o

i
t

a
r
t

n
e
c
n
o
c
e
n
o
z
o

 

0
3

0
2

0
1

0

   

   

   
   
       

daggot pressure gradient

df=5
df=11

           
                       

   

   

   

   

   

   

   

   

   

   

   

                                               
                                                                       

l

s
e
u
a
v
n
e
g
e

i

2

.

1

0

.

1

8

.

0

6
0

.

4
0

.

2

.

0

0

.

0

2

.

0
-

5

10

15

20

25

order

-50

0

50

100

-50

0

50

100

figure 5.7. (top:) smoothing spline    t of ozone concentration versus daggot
pressure gradient. the two    ts correspond to di   erent values of the smoothing
parameter, chosen to achieve    ve and eleven e   ective degrees of freedom, de   ned
by df   = trace(s  ). (lower left:) first 25 eigenvalues for the two smoothing-spline
matrices. the    rst two are exactly 1, and all are     0. (lower right:) third to
sixth eigenvectors of the spline smoother matrices. in each case, uk is plotted
against x, and as such is viewed as a function of x. the rug at the base of the
plots indicate the occurrence of data points. the damped functions represent the
smoothed versions of these functions (using the 5 df smoother).

156

5. basis expansions and id173

either left alone, or shrunk to zero   that is, a projection matrix such
as h   above has m eigenvalues equal to 1, and the rest are 0. for
this reason smoothing splines are referred to as shrinking smoothers,
while regression splines are projection smoothers (see figure 3.17 on
page 80).

    the sequence of uk, ordered by decreasing   k(  ), appear to increase
in complexity. indeed, they have the zero-crossing behavior of polyno-
mials of increasing degree. since s  uk =   k(  )uk, we see how each of
the eigenvectors themselves are shrunk by the smoothing spline: the
higher the complexity, the more they are shrunk. if the domain of x
is periodic, then the uk are sines and cosines at di   erent frequencies.
    the    rst two eigenvalues are always one, and they correspond to the
two-dimensional eigenspace of functions linear in x (exercise 5.11),
which are never shrunk.

    the eigenvalues   k(  ) = 1/(1 +   dk) are an inverse function of the
eigenvalues dk of the penalty matrix k, moderated by   ;    controls
the rate at which the   k(  ) decrease to zero. d1 = d2 = 0 and again
linear functions are not penalized.

    one can reparametrize the smoothing spline using the basis vectors
uk (the demid113r   reinsch basis). in this case the smoothing spline
solves

   ky     u  k2 +     t d  ,
min

(5.21)

where u has columns uk and d is a diagonal matrix with elements
dk.

    df   = trace(s  ) = pn

k=1   k(  ). for projection smoothers, all the
eigenvalues are 1, each one corresponding to a dimension of the pro-
jection subspace.

figure 5.8 depicts a smoothing spline matrix, with the rows ordered with
x. the banded nature of this representation suggests that a smoothing
spline is a local    tting method, much like the locally weighted regression
procedures in chapter 6. the right panel shows in detail selected rows of
s, which we call the equivalent kernels. as        0, df       n , and s       i,
the n -dimensional identity matrix. as           , df       2, and s       h, the
hat matrix for id75 on x.

5.5 automatic selection of the smoothing

parameters

the smoothing parameters for regression splines encompass the degree of
the splines, and the number and placement of the knots. for smoothing

5.5 automatic selection of the smoothing parameters

157

equivalent kernels

row 12

                                                                                                                                                                                                                                                                                                                                                                                                   

smoother matrix

row 25

                                                                                                                                                                                                                                                                                                                                                                              

   

                

row 50

                                                                                                                                                                                                                                                                                                                                                                                                   

row 75

                                                                                                                                                                                                                                                                                                                                                                                                   

row 100

                                                                                                                                                                                                                                                                                                                                                                                               

   

row 115

                                                                                                                                                                                                                                                                                                                                                                                                   

   

   

   

   

   

   

12

25

50

75

100

115

figure 5.8. the smoother matrix for a smoothing spline is nearly banded,
indicating an equivalent kernel with local support. the left panel represents the
elements of s as an image. the right panel shows the equivalent kernel or weight-
ing function in detail for the indicated rows.

158

5. basis expansions and id173

splines, we have only the penalty parameter    to select, since the knots are
at all the unique training x   s, and cubic degree is almost always used in
practice.

selecting the placement and number of knots for regression splines can be
a combinatorially complex task, unless some simpli   cations are enforced.
the mars procedure in chapter 9 uses a greedy algorithm with some
additional approximations to achieve a practical compromise. we will not
discuss this further here.

5.5.1 fixing the degrees of freedom

since df   = trace(s  ) is monotone in    for smoothing splines, we can in-
vert the relationship and specify    by    xing df. in practice this can be
achieved by simple numerical methods. so, for example, in r one can use
smooth.spline(x,y,df=6) to specify the amount of smoothing. this encour-
ages a more traditional mode of model selection, where we might try a cou-
ple of di   erent values of df, and select one based on approximate f -tests,
residual plots and other more subjective criteria. using df in this way pro-
vides a uniform approach to compare many di   erent smoothing methods.
it is particularly useful in generalized additive models (chapter 9), where
several smoothing methods can be simultaneously used in one model.

5.5.2 the bias   variance tradeo   

figure 5.9 shows the e   ect of the choice of df   when using a smoothing
spline on a simple example:

y = f (x) +   ,

f (x) =

sin(12(x + 0.2))

x + 0.2

,

(5.22)

with x     u [0, 1] and        n (0, 1). our training sample consists of n = 100
pairs xi, yi drawn independently from this model.
the    tted splines for three di   erent values of df   are shown. the yellow
shaded region in the    gure represents the pointwise standard error of   f  ,
that is, we have shaded the region between   f  (x)    2    se(   f  (x)). since
  f = s  y,

cov(  f ) = s  cov(y)st
  

= s  st
   .

(5.23)

the diagonal contains the pointwise variances at the training xi. the bias
is given by

bias(  f ) = f     e(  f )
= f     s  f ,

(5.24)

)
  
(
v
c
d
n
a

)
  
(
e
p
e

y

5
1

.

4

.

1

3

.

1

2
1

.

1

.

1

0
.
1

2

0

2
   

4
   

5.5 automatic selection of the smoothing parameters

159

cross-validation

df   = 5

cv
epe

y

2

0

2
   

4
   

o

o
o
o
o

o
o

o

o

o
o
oo
o

o
oo

o
o
o
oo

o

o
o

o

o

o
o
o
o

o
o
o

o

o

o

o
o

o

o

o

o

o

o

o

o

o
o
o
oo
o
o

o

ooo
o
o
o

o

o

o

o

o

o

o

o

o
o
o
o
o
o
oo
o

o

o

o
o
o
o

o
o
o

o

6

8

10

12

14

0.0

0.2

0.4

0.6

0.8

1.0

df  

df   = 9

o
o
o
o

o
o
o

o

o
o
o
o

o
o

o

o

o
o
oo
o

o
oo

o
o
o
oo

o

o
o

o

o

o

o

o

o
o

o

o

o

o

o

o

o

o

o
o
o
oo
o
o

o

ooo
o
o
o

o

o

o

o

o

o

o

o

o
o
o
o
o
o
oo
o

o

o

o
o
o
o

o
o
o

o

x

df   = 15

o
o
o
o

o
o
o

o

oo
o

o

oo
o
o

o
o
o

y

2

0

2
   

4
   

o

o
o
o
o

o
o

o

o

o
o
oo
o

o
oo

o
o
o
oo

o

o
o

o

o

o

o

o

o
o

o

o

o

o

o

o

o

o

o
o
o
oo
o
o

o

ooo
o
o
o

o

o

o

o

o

o

o

o

o
o
o
o
o
o
oo
o

o

o

o
o
o
o

o
o
o

o

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

x

x

figure 5.9. the top left panel shows the epe(  ) and cv(  ) curves for a
realization from a nonlinear additive error model (5.22). the remaining panels
show the data, the true functions (in purple), and the    tted curves (in green) with
yellow shaded   2   standard error bands, for three di   erent values of df  .

o

oo
o

o

oo
o
o

o
o
o

o

oo
o

o

oo
o
o

o
o
o

160

5. basis expansions and id173

where f is the (unknown) vector of evaluations of the true f at the training
x   s. the expectations and variances are with respect to repeated draws
of samples of size n = 100 from the model (5.22). in a similar fashion
var(   f  (x0)) and bias(   f  (x0)) can be computed at any point x0 (exer-
cise 5.10). the three    ts displayed in the    gure give a visual demonstration
of the bias-variance tradeo    associated with selecting the smoothing
parameter.

df   = 5: the spline under    ts, and clearly trims down the hills and    lls in
the valleys. this leads to a bias that is most dramatic in regions of
high curvature. the standard error band is very narrow, so we esti-
mate a badly biased version of the true function with great reliability!

df   = 9: here the    tted function is close to the true function, although a
slight amount of bias seems evident. the variance has not increased
appreciably.

df   = 15: the    tted function is somewhat wiggly, but close to the true
function. the wiggliness also accounts for the increased width of the
standard error bands   the curve is starting to follow some individual
points too closely.

note that in these    gures we are seeing a single realization of data and
hence    tted spline   f in each case, while the bias involves an expectation
e(   f ). we leave it as an exercise (5.10) to compute similar    gures where the
bias is shown as well. the middle curve seems    just right,    in that it has
achieved a good compromise between bias and variance.

the integrated squared prediction error (epe) combines both bias and

variance in a single summary:

epe(   f  ) = e(y       f  (x))2

= var(y ) + ehbias2(   f  (x)) + var(   f  (x))i

=   2 + mse(   f  ).

(5.25)

note that this is averaged both over the training sample (giving rise to   f  ),
and the values of the (independently chosen) prediction points (x, y ). epe
is a natural quantity of interest, and does create a tradeo    between bias
and variance. the blue points in the top left panel of figure 5.9 suggest
that df   = 9 is spot on!

since we don   t know the true function, we do not have access to epe, and
need an estimate. this topic is discussed in some detail in chapter 7, and
techniques such as k-fold cross-validation, gcv and cp are all in common
use. in figure 5.9 we include the n -fold (leave-one-out) cross-validation
curve:

5.6 nonparametric id28

161

cv(   f  ) =

=

1
n

1
n

  

(xi))2

(yi       f (   i)

nxi=1
1     s  (i, i)!2
nxi=1  yi       f  (xi)

(5.26)

,

(5.27)

which can (remarkably) be computed for each value of    from the original
   tted values and the diagonal elements s  (i, i) of s   (exercise 5.13).

the epe and cv curves have a similar shape, but the entire cv curve
is above the epe curve. for some realizations this is reversed, and overall
the cv curve is approximately unbiased as an estimate of the epe curve.

5.6 nonparametric id28

the smoothing spline problem (5.9) in section 5.4 is posed in a regression
setting. it is typically straightforward to transfer this technology to other
domains. here we consider id28 with a single quantitative
input x. the model is

which implies

log

pr(y = 1|x = x)
pr(y = 0|x = x)

= f (x),

(5.28)

pr(y = 1|x = x) =

ef (x)

1 + ef (x) .

(5.29)

fitting f (x) in a smooth fashion leads to a smooth estimate of the condi-
tional id203 pr(y = 1|x), which can be used for classi   cation or risk
scoring.

we construct the penalized log-likelihood criterion

   (f ;   ) =

=

[yi log p(xi) + (1     yi) log(1     p(xi))]    

nxi=1
nxi=1hyif (xi)     log(1 + ef (xi))i    

1
2

  z {f       (t)}2dt,

1
2

  z {f       (t)}2dt

(5.30)

where we have abbreviated p(x) = pr(y = 1|x). the    rst term in this ex-
pression is the log-likelihood based on the binomial distribution (c.f. chap-
ter 4, page 120). arguments similar to those used in section 5.4 show that
the optimal f is a    nite-dimensional natural spline with knots at the unique

162

5. basis expansions and id173

values of x. this means that we can represent f (x) =pn

compute the    rst and second derivatives

j=1 nj(x)  j. we

      (  )

     

= nt (y     p)            ,
   2   (  )
          t =    nt wn          ,

(5.31)

(5.32)

where p is the n -vector with elements p(xi), and w is a diagonal matrix
of weights p(xi)(1     p(xi)). the    rst derivative (5.31) is nonlinear in   , so
we need to use an iterative algorithm as in section 4.4.1. using newton   
raphson as in (4.23) and (4.26) for linear id28, the update
equation can be written

  new = (nt wn +      )   1nt w(cid:0)n  old + w   1(y     p)(cid:1)

= (nt wn +      )   1nt wz.

(5.33)

we can also express this update in terms of the    tted values

f new = n(nt wn +      )   1nt w(cid:0)f old + w   1(y     p)(cid:1)

= s  ,wz.

(5.34)

referring back to (5.12) and (5.14), we see that the update    ts a weighted
smoothing spline to the working response z (exercise 5.12).

the form of (5.34) is suggestive. it is tempting to replace s  ,w by any
nonparametric (weighted) regression operator, and obtain general fami-
lies of nonparametric id28 models. although here x is one-
dimensional, this procedure generalizes naturally to higher-dimensional x.
these extensions are at the heart of generalized additive models, which we
pursue in chapter 9.

5.7 multidimensional splines

so far we have focused on one-dimensional spline models. each of the ap-
proaches have multidimensional analogs. suppose x     ir2, and we have
a basis of functions h1k(x1), k = 1, . . . , m1 for representing functions of
coordinate x1, and likewise a set of m2 functions h2k(x2) for coordinate
x2. then the m1    m2 dimensional tensor product basis de   ned by

gjk(x) = h1j(x1)h2k(x2), j = 1, . . . , m1, k = 1, . . . , m2

(5.35)

can be used for representing a two-dimensional function:

g(x) =

m1xj=1

m2xk=1

  jkgjk(x).

(5.36)

5.7 multidimensional splines

163

figure 5.10. a tensor product basis of b-splines, showing some selected pairs.
each two-dimensional function is the tensor product of the corresponding one
dimensional marginals.

figure 5.10 illustrates a tensor product basis using b-splines. the coe   -
cients can be    t by least squares, as before. this can be generalized to d
dimensions, but note that the dimension of the basis grows exponentially
fast   yet another manifestation of the curse of dimensionality. the mars
procedure discussed in chapter 9 is a greedy forward algorithm for includ-
ing only those tensor products that are deemed necessary by least squares.

figure 5.11 illustrates the di   erence between additive and tensor product
(natural) splines on the simulated classi   cation example from chapter 2.
a id28 model logit[pr(t|x)] = h(x)t    is    t to the binary re-
sponse, and the estimated decision boundary is the contour h(x)t      = 0.
the tensor product basis can achieve more    exibility at the decision bound-
ary, but introduces some spurious structure along the way.

164

5. basis expansions and id173

additive natural cubic splines - 4 df each

o

o

o

o

o

o

o

o

. . .. .. .. ...

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
oo
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.. .. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.23
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.28
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.21

o
o
o
o
o
o
o
o

o
o
o
o

o
o

o
o
o

o
o

o
o

o

o

o

o

o

o

o

o

natural cubic splines - tensor product - 4 df each

o

o

o

o

o

o

o

o

o

o
o

o
o
o

. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . ..
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
training error: 0.230
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
test error:       0.282
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
bayes error:    0.210

o
o
oo

o
o

o
o
o

o
o

o
o

o

o

o

o

o

o

o

figure 5.11. the simulation example of figure 2.1. the upper panel shows the
decision boundary of an additive id28 model, using natural splines
in each of the two coordinates (total df = 1 + (4     1) + (4     1) = 7). the lower
panel shows the results of using a tensor product of natural spline bases in each
coordinate (total df = 4    4 = 16). the broken purple boundary is the bayes
decision boundary for this problem.

5.7 multidimensional splines

165

one-dimensional smoothing splines (via id173) generalize to high-
er dimensions as well. suppose we have pairs yi, xi with xi     ird, and we
seek a d-dimensional regression function f (x). the idea is to set up the
problem

min

f

{yi     f (xi)}2 +   j[f ],

(5.37)

nxi=1

where j is an appropriate penalty functional for stabilizing a function f in
ird. for example, a natural generalization of the one-dimensional roughness
penalty (5.9) for functions on ir2 is

j[f ] =z zir2h(cid:18)    2f (x)
1 (cid:19)2

   x2

   x1   x2(cid:19)2
+ 2(cid:18)    2f (x)

+(cid:18)    2f (x)

2 (cid:19)2idx1dx2. (5.38)

   x2

optimizing (5.37) with this penalty leads to a smooth two-dimensional
surface, known as a thin-plate spline. it shares many properties with the
one-dimensional cubic smoothing spline:

    as        0, the solution approaches an interpolating function [the one

with smallest penalty (5.38)];

    as           , the solution approaches the least squares plane;
    for intermediate values of   , the solution can be represented as a
linear expansion of basis functions, whose coe   cients are obtained
by a form of generalized ridge regression.

the solution has the form

f (x) =   0 +   t x +

nxj=1

  jhj(x),

(5.39)

where hj(x) = ||x     xj||2 log ||x     xj||. these hj are examples of radial
basis functions, which are discussed in more detail in the next section. the
coe   cients are found by plugging (5.39) into (5.37), which reduces to a
   nite-dimensional penalized least squares problem. for the penalty to be
   nite, the coe   cients   j have to satisfy a set of linear constraints; see
exercise 5.14.

thin-plate splines are de   ned more generally for arbitrary dimension d,

for which an appropriately more general j is used.

there are a number of hybrid approaches that are popular in practice,
both for computational and conceptual simplicity. unlike one-dimensional
smoothing splines, the computational complexity for thin-plate splines is
o(n 3), since there is not in general any sparse structure that can be ex-
ploited. however, as with univariate smoothing splines, we can get away
with substantially less than the n knots prescribed by the solution (5.39).

166

5. basis expansions and id173

systolic blood pressure

y
t
i
s
e
b
o

45

40

35

30

25

20

15

   

   

   

   

   

   
   
   

   
   

   

   

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   

   

   

   

   

   
   
   
   

   

   

   
   
   

   
   
   

   
   
   
   
   

   

   

   

   

   

   

   

   

   
   
   
   
   
   
   
   
   
   

   

   

   

   
   
   
   
   
   
   
   
   
   

   

   
   

   

   
   
   
   
   

   
   

   
125

   
   

   
   
   

   

   

   
   
   

   
   
   
   

   
   
   
   
   

   
   
   
   

   

   
   
   
   
   
   
   
   

   
   
   
   
   
   
   

   

   
   
   

20

30

   

   

   

   
   
   

   
   
   
   
   
   
   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   

   

   

   

   
   
   
   
   
   

   
   
   

   
   
   
   
   
   
   

   

   
   
   

   
   

   
   
   
   

   

   

   
   
   

   
   
   
   
   
   
   

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   
   
   
   
   

   
   

   

   

135

   

   
   

   

   
   
   
   
   
   
   
   
   
   

   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
       

   
   
   

   
130

   
   

   
   
   
   
   
   
   
   
   
   

   

   

   
   
   
   
   
   
   
   
   
   
   

   

   

   

50

   

   

   

   

   
   

   

   
   
   
   
   
   
   

   
   
   

   
   
   

   

   

   

   

   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   

   
   

   

   
   
   
   
   

   

   

   

   

   

   

   

   

   

   
   
   

   
   

   
   
   
   
   
   
   
   
   
   

   

   

   

   
   
   
   

   
   
   

   
   

   

   
   
   

   
   
   
   
   
   
   
   
   

40

age

   

   

   

   

   
   

   
   
   

   
   
   
   
   
   
   
   

   
   
155
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
150
   
   
   
   
   
   
   
   
   
   
   
145
   
   
   
140

   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   

   

   
   

   

   

60

160

155

150

145

140

135

130

125

120

figure 5.12. a thin-plate spline    t to the heart disease data, displayed as a
contour plot. the response is systolic blood pressure, modeled as a function
of age and obesity. the data points are indicated, as well as the lattice of points
used as knots. care should be taken to use knots from the lattice inside the convex
hull of the data (red), and ignore those outside (green).

in practice, it is usually su   cient to work with a lattice of knots covering
the domain. the penalty is computed for the reduced expansion just as
before. using k knots reduces the computations to o(n k 2 + k 3). fig-
ure 5.12 shows the result of    tting a thin-plate spline to some heart disease
risk factors, representing the surface as a contour plot. indicated are the
location of the input features, as well as the knots used in the    t. note that
   was speci   ed via df   = trace(s  ) = 15.

more generally one can represent f     ird as an expansion in any arbi-
trarily large collection of basis functions, and control the complexity by ap-
plying a regularizer such as (5.38). for example, we could construct a basis
by forming the tensor products of all pairs of univariate smoothing-spline
basis functions as in (5.35), using, for example, the univariate b-splines
recommended in section 5.9.2 as ingredients. this leads to an exponential

5.8 id173 and reproducing kernel hilbert spaces

167

growth in basis functions as the dimension increases, and typically we have
to reduce the number of functions per coordinate accordingly.

the additive spline models discussed in chapter 9 are a restricted class
of multidimensional splines. they can be represented in this general formu-
lation as well; that is, there exists a penalty j[f ] that guarantees that the
solution has the form f (x) =    + f1(x1) +        + fd(xd) and that each of
the functions fj are univariate splines. in this case the penalty is somewhat
degenerate, and it is more natural to assume that f is additive, and then
simply impose an additional penalty on each of the component functions:

these are naturally extended to anova spline decompositions,

j[f ] = j(f1 + f2 +        + fd)

=

dxj=1z f       
fj(xj) +xj<k

f (x) =    +xj

j (tj)2dtj.

(5.40)

fjk(xj, xk) +        ,

(5.41)

where each of the components are splines of the required dimension. there
are many choices to be made:

    the maximum order of interaction   we have shown up to order 2

above.

    which terms to include   not all main e   ects and interactions are

necessarily needed.

    what representation to use   some choices are:

    regression splines with a relatively small number of basis func-
tions per coordinate, and their tensor products for interactions;

    a complete basis as in smoothing splines, and include appropri-

ate regularizers for each term in the expansion.

in many cases when the number of potential dimensions (features) is large,
automatic methods are more desirable. the mars and mart procedures
(chapters 9 and 10, respectively), both fall into this category.

5.8 id173 and reproducing kernel

hilbert spaces

in this section we cast splines into the larger context of id173 meth-
ods and reproducing kernel hilbert spaces. this section is quite technical
and can be skipped by the disinterested or intimidated reader.

168

5. basis expansions and id173

a general class of id173 problems has the form

min

f    h" nxi=1

l(yi, f (xi)) +   j(f )#

(5.42)

where l(y, f (x)) is a id168, j(f ) is a penalty functional, and h is
a space of functions on which j(f ) is de   ned. girosi et al. (1995) describe
quite general penalty functionals of the form
|   f (s)|2
  g(s)

(5.43)

j(f ) =zird

ds,

where   f denotes the fourier transform of f , and   g is some positive function
that falls o    to zero as ||s||        . the idea is that 1/   g increases the penalty
for high-frequency components of f . under some additional assumptions
they show that the solutions have the form

f (x) =

  k  k(x) +

  ig(x     xi),

(5.44)

kxk=1

nxi=1

where the   k span the null space of the penalty functional j, and g is the
inverse fourier transform of   g. smoothing splines and thin-plate splines
fall into this framework. the remarkable feature of this solution is that
while the criterion (5.42) is de   ned over an in   nite-dimensional space, the
solution is    nite-dimensional. in the next sections we look at some speci   c
examples.

5.8.1 spaces of functions generated by kernels

an important subclass of problems of the form (5.42) are generated by
a positive de   nite kernel k(x, y), and the corresponding space of func-
tions hk is called a reproducing kernel hilbert space (rkhs). the penalty
functional j is de   ned in terms of the kernel as well. we give a brief and
simpli   ed introduction to this class of models, adapted from wahba (1990)
and girosi et al. (1995), and nicely summarized in evgeniou et al. (2000).
let x, y     irp. we consider the space of functions generated by the linear
span of {k(  , y), y     irp)}; i.e arbitrary linear combinations of the form
f (x) = pm   mk(x, ym), where each kernel term is viewed as a function
of the    rst argument, and indexed by the second. suppose that k has an
eigen-expansion

i=1   2
these eigen-functions,

with   i     0,p   

k(x, y) =

  i  i(x)  i(y)

(5.45)

i <    . elements of hk have an expansion in terms of

f (x) =

ci  i(x),

(5.46)

   xi=1
   xi=1

5.8 id173 and reproducing kernel hilbert spaces

169

with the constraint that

||f||2

hk

def=

   xi=1

c2
i /  i <    ,

(5.47)

where ||f||hk is the norm induced by k. the penalty functional in (5.42)
for the space hk is de   ned to be the squared norm j(f ) = ||f||2
. the
quantity j(f ) can be interpreted as a generalized ridge penalty, where
functions with large eigenvalues in the expansion (5.45) get penalized less,
and vice versa.

hk

rewriting (5.42) we have

or equivalently

min

f    hk" nxi=1
1       

nxi=1

l(yi,

hk#
l(yi, f (xi)) +   ||f||2
j /  j       .

cj  j(xi)) +   

   xj=1

   xj=1

c2

min
{cj }   

(5.48)

(5.49)

it can be shown (wahba, 1990, see also exercise 5.15) that the solution

to (5.48) is    nite-dimensional, and has the form

f (x) =

nxi=1

  ik(x, xi).

(5.50)

the basis function hi(x) = k(x, xi) (as a function of the    rst argument) is
known as the representer of evaluation at xi in hk , since for f     hk, it is
easily seen that hk(  , xi), fihk = f (xi). similarly hk(  , xi), k(  , xj)ihk =
k(xi, xj) (the reproducing property of hk), and hence

j(f ) =

nxi=1

nxj=1

k(xi, xj)  i  j

(5.51)

for f (x) =pn

rion

i=1   ik(x, xi).

in light of (5.50) and (5.51), (5.48) reduces to a    nite-dimensional crite-

min

  

l(y, k  ) +     t k  .

(5.52)

we are using a vector notation, in which k is the n    n matrix with ijth
entry k(xi, xj) and so on. simple numerical algorithms can be used to
optimize (5.52). this phenomenon, whereby the in   nite-dimensional prob-
lem (5.48) or (5.49) reduces to a    nite dimensional optimization problem,
has been dubbed the kernel property in the literature on support-vector
machines (see chapter 12).

170

5. basis expansions and id173

there is a bayesian interpretation of this class of models, in which f
is interpreted as a realization of a zero-mean stationary gaussian process,
with prior covariance function k. the eigen-decomposition produces a se-
ries of orthogonal eigen-functions   j(x) with associated variances   j. the
typical scenario is that    smooth    functions   j have large prior variance,
while    rough      j have small prior variances. the penalty in (5.48) is the
contribution of the prior to the joint likelihood, and penalizes more those
components with smaller prior variance (compare with (5.43)).

for simplicity we have dealt with the case here where all members of h
are penalized, as in (5.48). more generally, there may be some components
in h that we wish to leave alone, such as the linear functions for cubic
smoothing splines in section 5.4. the multidimensional thin-plate splines
of section 5.7 and tensor product splines fall into this category as well.
in these cases there is a more convenient representation h = h0     h1,
with the null space h0 consisting of, for example, low degree polynomi-
als in x that do not get penalized. the penalty becomes j(f ) = kp1fk,
where p1 is the orthogonal projection of f onto h1. the solution has the
i=1   ik(x, xi), where the    rst term repre-
sents an expansion in h0. from a bayesian perspective, the coe   cients of
components in h0 have improper priors, with in   nite variance.

j=1   jhj(x) +pn

form f (x) =pm

5.8.2 examples of rkhs

the machinery above is driven by the choice of the kernel k and the loss
function l. we consider    rst regression using squared-error loss. in this
case (5.48) specializes to penalized least squares, and the solution can be
characterized in two equivalent ways corresponding to (5.49) or (5.52):

min
{cj }   
1

nxi=1

      yi    

   xj=1

2

cj  j(xi)      

+   

c2
j
  j

   xj=1

an in   nite-dimensional, generalized ridge regression problem, or

min

  

(y     k  )t (y     k  ) +     t k  .

the solution for    is obtained simply as

     = (k +   i)   1y,

and

(5.53)

(5.54)

(5.55)

  f (x) =

nxj=1

    jk(x, xj).

(5.56)

5.8 id173 and reproducing kernel hilbert spaces

171

the vector of n    tted values is given by

  f = k     

= k(k +   i)   1y
= (i +   k   1)   1y.

(5.57)
(5.58)

the estimate (5.57) also arises as the kriging estimate of a gaussian ran-
dom    eld in spatial statistics (cressie, 1993). compare also (5.58) with the
smoothing spline    t (5.17) on page 154.

penalized polynomial regression
the kernel k(x, y) = (hx, yi + 1)d (vapnik, 1996), for x, y     irp, has
m = (cid:0)p+d
d (cid:1) eigen-functions that span the space of polynomials in irp of
total degree d. for example, with p = 2 and d = 2, m = 6 and

k(x, y) = 1 + 2x1y1 + 2x2y2 + x2

1y2

1 + x2

2y2

2 + 2x1x2y1y2 (5.59)

=

mxm=1

hm(x)hm(y)

with

h(x)t = (1,   2x1,   2x2, x2

2,   2x1x2).

1, x2

(5.60)

(5.61)

one can represent h in terms of the m orthogonal eigen-functions and
eigenvalues of k,

h(x) = vd

1
2

     (x),

(5.62)

where d   = diag(  1,   2, . . . ,   m ), and v is m    m and orthogonal.

suppose we wish to solve the penalized polynomial regression problem

min
{  m}m
1

nxi=1 yi    

mxm=1

  mhm(xi)!2

+   

  2
m.

mxm=1

(5.63)

substituting (5.62) into (5.63), we get an expression of the form (5.53) to
optimize (exercise 5.16).

the number of basis functions m =(cid:0)p+d

d (cid:1) can be very large, often much

larger than n . equation (5.55) tells us that if we use the kernel represen-
tation for the solution function, we have only to evaluate the kernel n 2
times, and can compute the solution in o(n 3) operations.

this simplicity is not without implications. each of the polynomials hm
in (5.61) inherits a scaling factor from the particular form of k, which has
a bearing on the impact of the penalty in (5.63). we elaborate on this in
the next section.

172

5. basis expansions and id173

radial kernel in ir1

8
.
0

)
m
x

,
  
(

4
.
0

k

0
.
0

   2

   1

0

1

x

2

3

4

figure 5.13. radial kernels kk(x) for the mixture data, with scale parameter
   = 1. the kernels are centered at    ve points xm chosen at random from the 200.

gaussian radial basis functions

in the preceding example, the kernel is chosen because it represents an
expansion of polynomials and can conveniently compute high-dimensional
inner products. in this example the kernel is chosen because of its functional
form in the representation (5.50).

the gaussian kernel k(x, y) = e     ||x   y||2

along with squared-error loss,
for example, leads to a regression model that is an expansion in gaussian
radial basis functions,

km(x) = e     ||x   xm||2

, m = 1, . . . , n,

(5.64)

each one centered at one of the training feature vectors xm. the coe   cients
are estimated using (5.54).

figure 5.13 illustrates radial kernels in ir1 using the    rst coordinate of
the mixture example from chapter 2. we show    ve of the 200 kernel basis
functions km(x) = k(x, xm).

figure 5.14 illustrates the implicit feature space for the radial kernel
with x     ir1. we computed the 200    200 kernel matrix k, and its eigen-
decomposition   d     t . we can think of the columns of    and the corre-
sponding eigenvalues in d   as empirical estimates of the eigen expansion
(5.45)2. although the eigenvectors are discrete, we can represent them as
functions on ir1 (exercise 5.17). figure 5.15 shows the largest 50 eigenval-
ues of k. the leading eigenfunctions are smooth, and they are successively
more wiggly as the order increases. this brings to life the penalty in (5.49),
where we see the coe   cients of higher-order functions get penalized more
than lower-order ones. the right panel in figure 5.14 shows the correspond-

2the    th column of    is an estimate of      , evaluated at each of the n observations.
alternatively, the ith row of    is the estimated vector of basis functions   (xi), evaluated
at the point xi. although in principle, there can be in   nitely many elements in   , our
estimate has at most n elements.

5.8 id173 and reproducing kernel hilbert spaces

173

orthonormal basis   

feature space h

*
*
*
*
**
*
*
*

*
*

*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
*
*
***
*
* *
*
*
*
*
**
*
*
******
*
*
*
*
*
*
**
*
*
*** *** *
*
**
*
*
*
*
* *
*
*
*
*
*
*
* *
**
*
*
**
*
**
**
**
*
*
*
*
**
*
** *
**
*
**
*
*
*
* *
*
*
*
*
*
**
*
*
* **
*
*
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*

**
*
*
*

*

*
*
*
*
**
*
*
*

*
*

*
*
*
*
*****
*
*
*
*
*
*
*
*
*** *
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
**
**
*
*
*
***
*
*
**
*
*
*
*
*
*
*
*
*
*
* *
*
*
**
*
**
**
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
***
*
*
**
*
**
*
*
*
*
*
*

*
**
*
*

*

*
*
*
*
**
*
*
*

*
*

*
*
*****
*
*
*
**
* *
**
*
*
*
** *
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
**
**
*
*
**
**
*
**
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
**
**
*
**
*
*
*
**
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*

**
*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* **
**
**
*
*
*
* *
*
**
*
*
*
*
*
*
*
**
*
*
*
*
* *
*
**
*
**
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*****
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*** **
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*

*
*
*
*
**
*
*
*

*
*

*
**
*
*

*

*
*
*
*
**
*
*
*

*
*

*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
**
*
*
**
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
*
*
*
*
** *
*
****
*
**
*
*
*
*
*
*
**
*
*
*
* *
*
*
*
*
*
*
*
*
*
**
*
*
**
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
*
*

*
**
*
*
*
*
*
*

*
*

*

*

**
*
*
*
*
*
*
*

*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
* **
*
**
*
*
*
*
*
*****
*
*
*
*
*
**
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
**
*
**
*
*
**
*
*
*
**
**
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
**
*
*
*
**
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
**
*
*

*

*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
** **
**
*
*
****
*
*
*
**
**
**
**
*
**
**
*
*
*
*
**
* *
*
*
** *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
**
*
*
*
*
*
*
* *
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
* *
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*

*
*

**
*
*
*

*

*
*
*
*
**
*
*
*

*
*
*
**
*
*
*
*

*
*
*
*
*
**
*
*
*
*
*
*
*
***
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
**
**
*
* *
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*****
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*

*
*

*
*

*
*
*
**
*
*

*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
**
**
*
*
*
**
*
*
**
*** *
*
*
**
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
*
*

*
*

*
*

*
*

*
*

*
*
**
*

*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
** **
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
**
**
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
*
*

*
*
*
**
*

*
*

*

*
*

*
*
*
*
*

*

*

*
*

*
*

*

*

*
*
*
*
*
*
*
**
*
*
*
**
*
*
***
*
*
*
*
****
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
* *
*
** *
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*

*
*
*
*
*

*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
****
*
*
**
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
**
*
*
*
**
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*

*
*
*
*
*

*
*

*
*
*
*
**
*

*
*

*
*
*

*
*

*

*

*
*

***
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
**
*
*
*
*
**
**
*
*
**
*
*
*
*
*
*
*
*
*** *
*
*
*
**
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
**
**
*
**
*
*
*
**
**
*
*
*
*
*
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
* *
**
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*

*

**
*
*
*

*

*
*
*
**
*
*
*
*

**
*
*
*
*
*

*
*

*

*

*

*
*
*
*
*

*

*

*
*
*
*
*

*
*

*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
***
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*

*
**
*
*

*

*
*
*
*
**
*
*
*

*
*

****
*
*
*
*
**
*
*
**
*
*
*
** *
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

**
*
*
*

*

*
*
*
*
**
*
*
*

*
*

*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
**
*
*
*
*
*
*
**
*
*
*
**
*
**
*
*
*
**
*
*
*
*
*
*
**
*
*
*****
* **
*
*
*
*
*
** *
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*

*
**
*
*

*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
***
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
**
*
*
*
*

**
*
*
*

*

*
*
*
*
**
*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
**
*
*
****
*
*
*
*
**
*
**
**
*
**
**
*
*
*
*
**
* *
*
*
** *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
*
* *
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
* *
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*

*
*

*
*
*
*
*
**
*
*
*
*
*
*
*
*
***
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
**
**
*
* *
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*****
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*

*
*

*
*
*
**
*
*
*
*

*

**
*
*
*

*
*
*
*
**
*
*
*

**
*
*
*

*

*
**
*
*

*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
* *
*
*
*
**
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*** *
*
*
**
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*

*

**
*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
*
*
**
*
*
* *
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
**
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
**
*
**
**
*
**
*
*
*
*
*
*
*
*
*
*
*
** *
*
****
*
**
*
*
*
*
*
*
**
*
*
*
* *
*
*
*
*
*
*
*
*
**
*
**
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*

*
*

*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
* **
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
** *
**
*
*
*
**
*
*
*
*
*
*
**
*
**
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*

*
*

*
**
*
*

*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
**
**
**
*
*
***
*
**
*** *** *
*
*
*
**
*
*
*
*
*
*
*
*
*
**
*
*
*
*
* *
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
******
*
***
**
*
*
*
**
*
*
*
*
*
*
*
*
*
* **
*
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
*
*

*
*
*
**
*
*
*
*

*
*

*
*
*
**
*
*
*
*

*

*
*
*
*
*
*
*
*
**
*
**
**
**
* *
***
*
*
**
*
**
*****
*** *** *
*
*
*
*
**
*
**
*
*
*
*
*
**
*
*
* *
*
** *
**
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
* *
*
*
*
* **
* *
*
**
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*

**
*
*
*

*

*
*
*
*
*
*
**
*

*

*
*

***
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
* *
**
*
*
*
**
*
**
**
**
*
*
**
*
* *
*
*
** *
**
*** *** *
*
*
*
*
**
*
*
*
*
*
* *
*
*
*
**
*
* *
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
* **
******
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
**
**
**
*
**
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

**
*
*
*

**
*
*
*
*
*
*
*

*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
******
*
*
*
*
*
*
**
*
*
*
*
*
*
* *
*
**
*** *** *
*
**
* *
*
*
**
*
*
* **
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
**
*
**
*
*
**
*
*
**
*
**
*
*
*
*
*
*
*
*
* *
*
***
***
*
** *
*
*
**
*
**
*
*
*
**
*
* *
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*

*
*

*
**
*
*

*

*

*
*

*
*
*
**
*
*
**
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
*
*
**
*
*
** *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*

*
*

*

*
*
*
*
*
*
**
*

*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
* **
*
*
*
**
* *
***
*
*
*
**
******
**
*
*
**
*** *** *
*
*
*
*
*
** *
*
*
**
*
**
*
*
*
* *
*
*
*
*
*
*
**
* *
*
*
**
*
**
*
*
*
*
*
*
*
*
*
**
*
**
*
*
*
*
*
*
***
*
*
*
*
*
* *
*
**
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
**
*
*
*
*
**
*
**
**
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
**
*
*

*

*
*
**
*
*
*
*
*

*
*

*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
* *
*
**
*
*
*
* *
*
*
******
**
**
*** *** *
*
*
*
*
*
***
*
* *
*
*
*
**
**
*
** *
*
*
*
*
*
*
*
*
*
*
*
*
*
**
* *
*
**
*
*
*
*
*
*
**
*
*
*
**
*
**
*
*
*
*
*
*
* **
*
*
*
*
**
*
**
*
*
**
*
*
**
**
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
**
*
*

*

*
*
**
*
*
*
*
*

*
*

*
*
***
*
*
*
*
*
*
*
*** *** *
* **
******
*
*
*
*
*
*
*
*
*
**
*
*
**
*
**
*
*
**
***
**
*
*
* *
**
*
*
*
*
*
*
* *
*
*
*
** *
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
* *
**
*
**
**
*
*
*
*
**
**
*
*
*
*
*
*
**
*
*
*
*
*
*
**
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

**
*
*
*

*

*
*
**
*
*
*
*
*

*
*

*
*
*
*
**
*
*
*
**
*
*
**
*
**
*
*
**
*
* *
*
**
*
*
***
*
* *
*
******
*
*
**
*
* *
**
**
**
** *
**
*
*
**
*
*
*
**
**
*** *** *
*
*
***
* **
*
*
* *
*
*
*
*
*
*
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
**
*
*

*

figure 5.14. (left panel) the    rst 16 normalized eigenvectors of k, the
200    200 kernel matrix for the    rst coordinate of the mixture data. these are
viewed as estimates         of the eigenfunctions in (5.45), and are represented as
functions in ir1 with the observed values superimposed in color. they are arranged
in rows, starting at the top left. (right panel) rescaled versions h    =           
        of
the functions in the left panel, for which the kernel computes the    inner product.   

l

e
u
a
v
n
e
g
e

i

1
0
+
e
1

3
0
   
e
1

7
0
   
e
1

1
1
   
e
1

5
1
   
e
1

0

10

20

30

40

50

figure 5.15. the largest 50 eigenvalues of k; all those beyond the 30th are
e   ectively zero.

174

5. basis expansions and id173

ing feature space representation of the eigenfunctions

h   (x) =p       

       (x),     = 1, . . . , n.

(5.65)

note that hh(xi), h(xi    )i = k(xi, xi    ). the scaling by the eigenvalues quickly
shrinks most of the functions down to zero, leaving an e   ective dimension
of about 12 in this case. the corresponding optimization problem is a stan-
dard ridge regression, as in (5.63). so although in principle the implicit
feature space is in   nite dimensional, the e   ective dimension is dramat-
ically lower because of the relative amounts of shrinkage applied to each
basis function. the kernel scale parameter    plays a role here as well; larger
   implies more local km functions, and increases the e   ective dimension of
the feature space. see hastie and zhu (2006) for more details.

it is also known (girosi et al., 1995) that a thin-plate spline (section 5.7)

is an expansion in radial basis functions, generated by the kernel

k(x, y) = kx     yk2 log(kx     yk).

(5.66)

radial basis functions are discussed in more detail in section 6.7.

support vector classi   ers

the support vector machines of chapter 12 for a two-class classi   cation
i=1   ik(x, xi), where the parameters

problem have the form f (x) =   0 +pn

are chosen to minimize

min

  0,  ( nxi=1

[1     yif (xi)]+ +

  t k  ) ,

  
2

(5.67)

where yi     {   1, 1}, and [z]+ denotes the positive part of z. this can be
viewed as a quadratic optimization problem with linear constraints, and
requires a quadratic programming algorithm for its solution. the name
support vector arises from the fact that typically many of the     i = 0 [due
to the piecewise-zero nature of the id168 in (5.67)], and so   f is an
expansion in a subset of the k(  , xi). see section 12.3.3 for more details.

5.9 wavelet smoothing

we have seen two di   erent modes of operation with dictionaries of basis
functions. with regression splines, we select a subset of the bases, using
either subject-matter knowledge, or else automatically. the more adaptive
procedures such as mars (chapter 9) can capture both smooth and non-
smooth behavior. with smoothing splines, we use a complete basis, but
then shrink the coe   cients toward smoothness.

haar wavelets

symid113t-8 wavelets

5.9 wavelet smoothing

175

  6,35

  6,15

  5,15

  5,1

  4,9

  4,4

  3,5

  3,2

  2,3

  2,1

  1,0

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

time

time

figure 5.16. some selected wavelets at di   erent translations and dilations
for the haar and symid113t families. the functions have been scaled to suit the
display.

wavelets typically use a complete orthonormal basis to represent func-
tions, but then shrink and select the coe   cients toward a sparse represen-
tation. just as a smooth function can be represented by a few spline basis
functions, a mostly    at function with a few isolated bumps can be repre-
sented with a few (bumpy) basis functions. wavelets bases are very popular
in signal processing and compression, since they are able to represent both
smooth and/or locally bumpy functions in an e   cient way   a phenomenon
dubbed time and frequency localization. in contrast, the traditional fourier
basis allows only frequency localization.

before we give details, let   s look at the haar wavelets in the left panel
of figure 5.16 to get an intuitive idea of how wavelet smoothing works.
the vertical axis indicates the scale (frequency) of the wavelets, from low
scale at the bottom to high scale at the top. at each scale the wavelets are
   packed in    side-by-side to completely    ll the time axis: we have only shown

176

5. basis expansions and id173

a selected subset. wavelet smoothing    ts the coe   cients for this basis by
least squares, and then thresholds (discards,    lters) the smaller coe   cients.
since there are many basis functions at each scale, it can use bases where
it needs them and discard the ones it does not need, to achieve time and
frequency localization. the haar wavelets are simple to understand, but not
smooth enough for most purposes. the symid113t wavelets in the right panel
of figure 5.16 have the same orthonormal properties, but are smoother.

figure 5.17 displays an nmr (nuclear magnetic resonance) signal, which
appears to be composed of smooth components and isolated spikes, plus
some noise. the wavelet transform, using a symid113t basis, is shown in the
lower left panel. the wavelet coe   cients are arranged in rows, from lowest
scale at the bottom, to highest scale at the top. the length of each line
segment indicates the size of the coe   cient. the bottom right panel shows
the wavelet coe   cients after they have been thresholded. the threshold
procedure, given below in equation (5.69), is the same soft-thresholding
rule that arises in the lasso procedure for id75 (section 3.4.2).
notice that many of the smaller coe   cients have been set to zero. the
green curve in the top panel shows the back-transform of the thresholded
coe   cients: this is the smoothed version of the original signal. in the next
section we give the details of this process, including the construction of
wavelets and the thresholding rule.

5.9.1 wavelet bases and the wavelet transform

in this section we give details on the construction and    ltering of wavelets.
wavelet bases are generated by translations and dilations of a single scal-
ing function   (x) (also known as the father). the red curves in figure 5.18
are the haar and symid113t-8 scaling functions. the haar basis is particu-
larly easy to understand, especially for anyone with experience in analysis
of variance or trees, since it produces a piecewise-constant representation.
thus if   (x) = i(x     [0, 1]), then   0,k(x) =   (x   k), k an integer, generates
an orthonormal basis for functions with jumps at the integers. call this ref-
erence space v0. the dilations   1,k(x) =    2  (2x   k) form an orthonormal
basis for a space v1     v0 of functions piecewise constant on intervals of
length 1
2 . in fact, more generally we have            v1     v0     v   1            where
each vj is spanned by   j,k = 2j/2  (2jx     k).
now to the de   nition of wavelets. in analysis of variance, we often rep-
resent a pair of means   1 and   2 by their grand mean    = 1
2 (  1 +   2), and
then a contrast    = 1
2 (  1       2). a simpli   cation occurs if the contrast    is
very small, because then we can set it to zero. in a similar manner we might
represent a function in vj+1 by a component in vj plus the component in
the orthogonal complement wj of vj to vj+1, written as vj+1 = vj     wj.
the component in wj represents detail, and we might wish to set some ele-
ments of this component to zero. it is easy to see that the functions   (x   k)

5.9 wavelet smoothing

177

nmr signal

0
6

0
4

0
2

0

0

200

400

600

800

1000

wavelet transform - original signal

wavelet transform - waveshrunk signal

signal

signal

w9

w8

w7

w6

w5

w4

v4

w9

w8

w7

w6

w5

w4

v4

0

200

400

600

800

1000

0

200

400

600

800

1000

figure 5.17. the top panel shows an nmr signal, with the wavelet-shrunk
version superimposed in green. the lower left panel represents the wavelet trans-
form of the original signal, down to v4, using the symid113t-8 basis. each coe   -
cient is represented by the height (positive or negative) of the vertical bar. the
lower right panel represents the wavelet coe   cients after being shrunken using
the waveshrink function in s-plus, which implements the sureshrink method
of wavelet adaptation of donoho and johnstone.

178

5. basis expansions and id173

haar basis

symid113t basis

  (x)

  (x)

  (x)

  (x)

figure 5.18. the haar and symid113t father (scaling) wavelet   (x) and mother
wavelet   (x).

generated by the mother wavelet   (x) =   (2x)     (2x   1) form an orthonor-
mal basis for w0 for the haar family. likewise   j,k = 2j/2  (2jx     k) form
a basis for wj.
now vj+1 = vj     wj = vj   1     wj   1     wj, so besides representing a
function by its level-j detail and level-j rough components, the latter can
be broken down to level-(j     1) detail and rough, and so on. finally we get
a representation of the form vj = v0     w0     w1            wj   1. figure 5.16
on page 175 shows particular wavelets   j,k(x).
notice that since these spaces are orthogonal, all the basis functions are
orthonormal. in fact, if the domain is discrete with n = 2j (time) points,
this is as far as we can go. there are 2j basis elements at level j, and
adding up, we have a total of 2j     1 elements in the wj, and one in v0.
this structured orthonormal basis allows for a multiresolution analysis,
which we illustrate in the next section.

while helpful for understanding the construction above, the haar basis
is often too coarse for practical purposes. fortunately, many clever wavelet
bases have been invented. figures 5.16 and 5.18 include the daubechies
symid113t-8 basis. this basis has smoother elements than the corresponding
haar basis, but there is a tradeo   :

    each wavelet has a support covering 15 consecutive time intervals,
rather than one for the haar basis. more generally, the symid113t-p
family has a support of 2p     1 consecutive intervals. the wider the
support, the more time the wavelet has to die to zero, and so it can

5.9 wavelet smoothing

179

achieve this more smoothly. note that the e   ective support seems to
be much narrower.

    the symid113t-p wavelet   (x) has p vanishing moments; that is,

z   (x)xjdx = 0, j = 0, . . . , p     1.

one implication is that any order-p polynomial over the n = 2j times
points is reproduced exactly in v0 (exercise 5.18). in this sense v0
is equivalent to the null space of the smoothing-spline penalty. the
haar wavelets have one vanishing moment, and v0 can reproduce any
constant function.

the symid113t-p scaling functions are one of many families of wavelet

generators. the operations are similar to those for the haar basis:

h(k).

    if v0 is spanned by   (x     k), then v1     v0 is spanned by   1,k(x) =

   2  (2x   k) and   (x) =pk   z h(k)  1,k(x), for some    lter coe   cients
    w0 is spanned by   (x) = pk   z g(k)  1,k(x), with    lter coe   cients
g(k) = (   1)1   kh(1     k).

5.9.2 adaptive wavelet filtering

wavelets are particularly useful when the data are measured on a uniform
lattice, such as a discretized signal, image, or a time series. we will focus on
the one-dimensional case, and having n = 2j lattice-points is convenient.
suppose y is the response vector, and w is the n   n orthonormal wavelet
basis matrix evaluated at the n uniformly spaced observations. then y    =
wt y is called the wavelet transform of y (and is the full least squares
regression coe   cient). a popular method for adaptive wavelet    tting is
known as sure shrinkage (stein unbiased risk estimation, donoho and
johnstone (1994)). we start with the criterion

   ||y     w  ||2
min

2 + 2  ||  ||1,

(5.68)

which is the same as the lasso criterion in chapter 3. because w is or-
thonormal, this leads to the simple solution:

    j = sign(y   

j )(|y   

j|       )+.

(5.69)

the least squares coe   cients are translated toward zero, and truncated
at zero. the    tted function (vector) is then given by the inverse wavelet
transform   f = w    .

180

5. basis expansions and id173

a simple choice for    is    =      2 log n , where    is an estimate of the
standard deviation of the noise. we can give some motivation for this choice.
since w is an orthonormal transformation, if the elements of y are white
noise (independent gaussian variates with mean 0 and variance   2), then
so are y   . furthermore if random variables z1, z2, . . . , zn are white noise,
the expected maximum of |zj|, j = 1, . . . , n is approximately      2 log n .
hence all coe   cients below      2 log n are likely to be noise and are set to
zero.

the space w could be any basis of orthonormal functions: polynomials,
natural splines or cosinusoids. what makes wavelets special is the particular
form of basis functions used, which allows for a representation localized in
time and in frequency.

let   s look again at the nmr signal of figure 5.17. the wavelet transform
was computed using a symid113t   8 basis. notice that the coe   cients do not
descend all the way to v0, but stop at v4 which has 16 basis functions.
as we ascend to each level of detail, the coe   cients get smaller, except in
locations where spiky behavior is present. the wavelet coe   cients represent
characteristics of the signal localized in time (the basis functions at each
level are translations of each other) and localized in frequency. each dilation
increases the detail by a factor of two, and in this sense corresponds to
doubling the frequency in a traditional fourier representation. in fact, a
more mathematical understanding of wavelets reveals that the wavelets at
a particular scale have a fourier transform that is restricted to a limited
range or octave of frequencies.

the shrinking/truncation in the right panel was achieved using the sure
approach described in the introduction to this section. the orthonormal
n    n basis matrix w has columns which are the wavelet basis functions
evaluated at the n time points. in particular, in this case there will be 16
columns corresponding to the   4,k(x), and the remainder devoted to the
  j,k(x), j = 4, . . . , 11. in practice    depends on the noise variance, and has
to be estimated from the data (such as the variance of the coe   cients at
the highest level).

notice the similarity between the sure criterion (5.68) on page 179,

and the smoothing spline criterion (5.21) on page 156:

    both are hierarchically structured from coarse to    ne detail, although

wavelets are also localized in time within each resolution level.

    the splines build in a bias toward smooth functions by imposing
di   erential shrinking constants dk. early versions of sure shrinkage
treated all scales equally. the s+wavelets function waveshrink() has
many options, some of which allow for di   erential shrinkage.

    the spline l2 penalty cause pure shrinkage, while the sure l1

penalty does shrinkage and selection.

exercises

181

more generally smoothing splines achieve compression of the original signal
by imposing smoothness, while wavelets impose sparsity. figure 5.19 com-
pares a wavelet    t (using sure shrinkage) to a smoothing spline    t (using
cross-validation) on two examples di   erent in nature. for the nmr data in
the upper panel, the smoothing spline introduces detail everywhere in order
to capture the detail in the isolated spikes; the wavelet    t nicely localizes
the spikes. in the lower panel, the true function is smooth, and the noise is
relatively high. the wavelet    t has let in some additional and unnecessary
wiggles   a price it pays in variance for the additional adaptivity.

the wavelet transform is not performed by id127 as in
y    = wt y. in fact, using clever pyramidal schemes y    can be obtained
in o(n ) computations, which is even faster than the n log(n ) of the fast
fourier transform (fft). while the general construction is beyond the
scope of this book, it is easy to see for the haar basis (exercise 5.19).
likewise, the inverse wavelet transform w     is also o(n ).

this has been a very brief glimpse of this vast and growing    eld. there is
a very large mathematical and computational base built on wavelets. mod-
ern image compression is often performed using two-dimensional wavelet
representations.

bibliographic notes

splines and b-splines are discussed in detail in de boor (1978). green
and silverman (1994) and wahba (1990) give a thorough treatment of
smoothing splines and thin-plate splines; the latter also covers reproducing
kernel hilbert spaces. see also girosi et al. (1995) and evgeniou et al.
(2000) for connections between many nonparametric regression techniques
using rkhs approaches. modeling functional data, as in section 5.2.3, is
covered in detail in ramsay and silverman (1997).

daubechies (1992) is a classic and mathematical treatment of wavelets.
other useful sources are chui (1992) and wickerhauser (1994). donoho and
johnstone (1994) developed the sure shrinkage and selection technology
from a statistical estimation framework; see also vidakovic (1999). bruce
and gao (1996) is a useful applied introduction, which also describes the
wavelet software in s-plus.

exercises

ex. 5.1 show that the truncated power basis functions in (5.3) represent a
basis for a cubic spline with the two knots as indicated.

182

5. basis expansions and id173

0
6

0
4

0
2

0

4

2

2
-

4
-

spline
wavelet

0

200

400

600

800

1000

nmr signal

   

   

   

   
   
   

   

   
   
   
   
   

   
   

   
   

   
   
   
      
   
   
   
      
            
   

   

   
   

   
   
   
   
   

   

   
   
      
            
   

   
      

   
      
   
      
   

   
      

   

   
   
   

   

      
   

   

   
   

   
   

   
   
      
   

   
      

   
   
   

   

   

   
      
   
      
   
   

   
   

   
   
   

   

spline
wavelet
true

   

   

   

   

   

   

   
      

   
         

   

   
      
   

   

0.2

0.0

0.4

0.6

0.8

1.0

smooth function (simulated)

   
      
   

   

n

0

   
   

figure 5.19. wavelet smoothing compared with smoothing splines on two
examples. each panel compares the sure-shrunk wavelet    t to the cross-validated
smoothing spline    t.

exercises

183

ex. 5.2 suppose that bi,m (x) is an order-m b-spline de   ned in the ap-
pendix on page 186 through the sequence (5.77)   (5.78).

(a) show by induction that bi,m (x) = 0 for x 6    [  i,   i+m ]. this shows, for

example, that the support of cubic b-splines is at most 5 knots.

(b) show by induction that bi,m (x) > 0 for x     (  i,   i+m ). the b-splines

are positive in the interior of their support.

(c) show by induction thatpk+m

i=1 bi,m (x) = 1   x     [  0,   k+1].

(d) show that bi,m is a piecewise polynomial of order m (degree m     1)

on [  0,   k+1], with breaks only at the knots   1, . . . ,   k .

(e) show that an order-m b-spline basis function is the density function

of a convolution of m uniform random variables.

ex. 5.3 write a program to reproduce figure 5.3 on page 145.

ex. 5.4 consider the truncated power series representation for cubic splines
with k interior knots. let

f (x) =

  jx j +

3xj=0

kxk=1

  k(x       k)3
+.

(5.70)

prove that the natural boundary conditions for natural cubic splines (sec-
tion 5.2.1) imply the following linear constraints on the coe   cients:

  2 = 0,

pk
  3 = 0, pk

k=1   k = 0,
k=1   k  k = 0.

hence derive the basis (5.4) and (5.5).

(5.71)

ex. 5.5 write a program to classify the phoneme data using a quadratic dis-
criminant analysis (section 4.3). since there are many correlated features,
you should    lter them using a smooth basis of natural cubic splines (sec-
tion 5.2.3). decide beforehand on a series of    ve di   erent choices for the
number and position of the knots, and use tenfold cross-validation to make
the    nal selection. the phoneme data are available from the book website
www-stat.stanford.edu/elemstatlearn.

ex. 5.6 suppose you wish to    t a periodic function, with a known period t .
describe how you could modify the truncated power series basis to achieve
this goal.

ex. 5.7 derivation of smoothing splines (green and silverman, 1994). sup-
pose that n     2, and that g is the natural cubic spline interpolant to the
pairs {xi, zi}n
1 , with a < x1 <        < xn < b. this is a natural spline

184

5. basis expansions and id173

with a knot at every xi; being an n -dimensional space of functions, we can
determine the coe   cients such that it interpolates the sequence zi exactly.
let   g be any other di   erentiable function on [a, b] that interpolates the n
pairs.
(a) let h(x) =   g(x)     g(x). use integration by parts and the fact that g is

a natural cubic spline to show that

z b

a

g      (x)h      (x)dx =    
= 0.

n    1xj=1

g         (x+

j ){h(xj+1)     h(xj)} (5.72)

(b) hence show that

z b

a

  g      (t)2dt    z b

a

g      (t)2dt,

and that equality can only hold if h is identically zero in [a, b].

(c) consider the penalized least squares problem

min

f " nxi=1

(yi     f (xi))2 +   z b

a

f       (t)2dt# .

use (b) to argue that the minimizer must be a cubic spline with knots
at each of the xi.

ex. 5.8 in the appendix to this chapter we show how the smoothing spline
computations could be more e   ciently carried out using a (n + 4) dimen-
sional basis of b-splines. describe a slightly simpler scheme using a (n + 2)
dimensional b-spline basis de   ned on the n     2 interior knots.
ex. 5.9 derive the reinsch form s   = (i +   k)   1 for the smoothing spline.

ex. 5.10 derive an expression for var(   f  (x0)) and bias(   f  (x0)). using the
example (5.22), create a version of figure 5.9 where the mean and several
(pointwise) quantiles of   f  (x) are shown.

ex. 5.11 prove that for a smoothing spline the null space of k is spanned
by functions linear in x.

ex. 5.12 characterize the solution to the following problem,

min

f

rss(f,   ) =

wi{yi     f (xi)}2 +   z {f       (t)}2dt,

nxi=1

(5.73)

where the wi     0 are observation weights.
the training data have ties in x.

characterize the solution to the smoothing spline problem (5.9) when

exercises

185

ex. 5.13 you have    tted a smoothing spline   f   to a sample of n pairs
(xi, yi). suppose you augment your original sample with the pair x0,   f  (x0),
and re   t; describe the result. use this to derive the n -fold cross-validation
formula (5.26).

ex. 5.14 derive the constraints on the   j in the thin-plate spline expan-
sion (5.39) to guarantee that the penalty j(f ) is    nite. how else could one
ensure that the penalty was    nite?

ex. 5.15 this exercise derives some of the results quoted in section 5.8.1.
suppose k(x, y) satisfying the conditions (5.45) and let f (x)     hk. show
that

(a) hk(  , xi), fihk = f (xi).
(b) hk(  , xi), k(  , xj)ihk = k(xi, xj).

i=1   ik(x, xi), then

(c) if g(x) =pn

j(g) =

nxi=1

nxj=1

k(xi, xj)  i  j.

suppose that   g(x) = g(x) +   (x), with   (x)     hk, and orthogonal in hk
to each of k(x, xi), i = 1, . . . , n . show that

(d)

nxi=1

l(yi,   g(xi)) +   j(  g)    

nxi=1

with equality i      (x) = 0.

l(yi, g(xi)) +   j(g)

(5.74)

ex. 5.16 consider the ridge regression problem (5.53), and assume m     n .
assume you have a kernel k that computes the inner product k(x, y) =

m=1 hm(x)hm(y).

pm

(a) derive (5.62) on page 171 in the text. how would you compute the
matrices v and d  , given k? hence show that (5.63) is equivalent
to (5.53).

(b) show that

  f = h    

= k(k +   i)   1y,

(5.75)

where h is the n    m matrix of evaluations hm(xi), and k = hht
the n    n matrix of inner-products h(xi)t h(xj).

186

5. basis expansions and id173

(c) show that

  f (x) = h(x)t     

=

nxi=1

k(x, xi)     i

(5.76)

and      = (k +   i)   1y.

(d) how would you modify your solution if m < n ?

ex. 5.17 show how to convert the discrete eigen-decomposition of k in
section 5.8.2 to estimates of the eigenfunctions of k.

ex. 5.18 the wavelet function   (x) of the symid113t-p wavelet basis has
vanishing moments up to order p. show that this implies that polynomials
of order p are represented exactly in v0, de   ned on page 176.

ex. 5.19 show that the haar wavelet transform of a signal of length n = 2j
can be computed in o(n ) computations.

appendix: computations for splines

in this appendix, we describe the b-spline basis for representing polyno-
mial splines. we also discuss their use in the computations of smoothing
splines.

b-splines

before we can get started, we need to augment the knot sequence de   ned
in section 5.2. let   0 <   1 and   k <   k+1 be two boundary knots, which
typically de   ne the domain over which we wish to evaluate our spline. we
now de   ne the augmented knot sequence    such that

      1       2                  m       0;
      j+m =   j, j = 1,       , k;
      k+1       k+m +1       k+m +2                  k+2m .

the actual values of these additional knots beyond the boundary are arbi-
trary, and it is customary to make them all the same and equal to   0 and
  k+1, respectively.

denote by bi,m(x) the ith b-spline basis function of order m for the
knot-sequence    , m     m . they are de   ned recursively in terms of divided

di   erences as follows:

bi,1(x) = (cid:26) 1

0

appendix: computations for splines

187

if   i     x <   i+1
otherwise

(5.77)

for i = 1, . . . , k + 2m     1. these are also known as haar basis functions.

x       i

bi,m(x) =

  i+m   1       i
for i = 1, . . . , k + 2m     m.

bi,m   1(x) +

  i+m     x
  i+m       i+1

bi+1,m   1(x)

(5.78)

thus with m = 4, bi,4, i = 1,       , k + 4 are the k + 4 cubic b-spline
basis functions for the knot sequence   . this recursion can be contin-
ued and will generate the b-spline basis for any order spline. figure 5.20
shows the sequence of b-splines up to order four with knots at the points
0.0, 0.1, . . . , 1.0. since we have created some duplicate knots, some care
has to be taken to avoid division by zero. if we adopt the convention
that bi,1 = 0 if   i =   i+1, then by induction bi,m = 0 if   i =   i+1 =
. . . =   i+m. note also that in the construction above, only the subset
i = m     m + 1, . . . , m + k are required for the b-spline basis
bi,m,
of order m < m with knots   .
to fully understand the properties of these functions, and to show that
they do indeed span the space of cubic splines for the knot sequence, re-
quires additional mathematical machinery, including the properties of di-
vided di   erences. exercise 5.2 explores these issues.

the scope of b-splines is in fact bigger than advertised here, and has to
do with knot duplication. if we duplicate an interior knot in the construc-
tion of the    sequence above, and then generate the b-spline sequence as
before, the resulting basis spans the space of piecewise polynomials with
one less continuous derivative at the duplicated knot. in general, if in ad-
dition to the repeated boundary knots, we include the interior knot   j
1     rj     m times, then the lowest-order derivative to be discontinuous
at x =   j will be order m     rj. thus for cubic splines with no repeats,
rj = 1, j = 1, . . . , k, and at each interior knot the third derivatives (4    1)
are discontinuous. repeating the jth knot three times leads to a discontin-
uous 1st derivative; repeating it four times leads to a discontinuous zeroth
derivative, i.e., the function is discontinuous at x =   j. this is exactly what
happens at the boundary knots; we repeat the knots m times, so the spline
becomes discontinuous at the boundary knots (i.e., unde   ned beyond the
boundary).

the local support of b-splines has important computational implica-
tions, especially when the number of knots k is large. least squares com-
putations with n observations and k + m variables (basis functions) take
o(n (k + m )2 + (k + m )3)    ops (   oating point operations.) if k is some
appreciable fraction of n , this leads to o(n 3) algorithms which becomes

188

5. basis expansions and id173

b-splines of order 1

2
1

.

8
0

.

4
0

.

0
0

.

2

.

1

8

.

0

4

.

0

0

.

0

2
1

.

8

.

0

4
0

.

0

.

0

2
1

.

8

.

0

4
0

.

0

.

0

0.0

0.2

0.4

0.6

0.8

1.0

b-splines of order 2

0.0

0.2

0.4

0.6

0.8

1.0

b-splines of order 3

0.0

0.2

0.4

0.6

0.8

1.0

b-splines of order 4

0.0

0.2

0.4

0.6

0.8

1.0

figure 5.20. the sequence of b-splines up to order four with ten knots evenly
spaced from 0 to 1. the b-splines have local support; they are nonzero on an
interval spanned by m + 1 knots.

appendix: computations for splines

189

unacceptable for large n . if the n observations are sorted, the n  (k +m )
regression matrix consisting of the k + m b-spline basis functions evalu-
ated at the n points has many zeros, which can be exploited to reduce the
computational complexity back to o(n ). we take this up further in the
next section.

computations for smoothing splines

although natural splines (section 5.2.1) provide a basis for smoothing
splines, it is computationally more convenient to operate in the larger space
  jbj(x), where   j are
coe   cients and the bj are the cubic b-spline basis functions. the solution
looks the same as before,

of unconstrained b-splines. we write f (x) =pn +4

1

     = (bt b +      b)   1bt y,

(5.79)

except now the n    n matrix n is replaced by the n    (n + 4) matrix
b, and similarly the (n + 4)    (n + 4) penalty matrix    b replaces the
n    n dimensional    n . although at face value it seems that there are
no boundary derivative constraints, it turns out that the penalty term
automatically imposes them by giving e   ectively in   nite weight to any non
zero derivative beyond the boundary. in practice,      is restricted to a linear
subspace for which the penalty is always    nite.

since the columns of b are the evaluated b-splines, in order from left
to right and evaluated at the sorted values of x, and the cubic b-splines
have local support, b is lower 4-banded. consequently the matrix m =
(btb +      ) is 4-banded and hence its cholesky decomposition m = llt
can be computed easily. one then solves llt    = bt y by back-substitution
to give    and hence the solution   f in o(n ) operations.

in practice, when n is large, it is unnecessary to use all n interior knots,
and any reasonable thinning strategy will save in computations and have
negligible e   ect on the    t. for example, the smooth.spline function in s-
plus uses an approximately logarithmic strategy: if n < 50 all knots are
included, but even at n = 5, 000 only 204 knots are used.

190

5. basis expansions and id173

6
kernel smoothing methods

this is page 191
printer: opaque this

in this chapter we describe a class of regression techniques that achieve
   exibility in estimating the regression function f (x) over the domain irp
by    tting a di   erent but simple model separately at each query point x0.
this is done by using only those observations close to the target point x0 to
   t the simple model, and in such a way that the resulting estimated function
  f (x) is smooth in irp. this localization is achieved via a weighting function
or kernel k  (x0, xi), which assigns a weight to xi based on its distance from
x0. the kernels k   are typically indexed by a parameter    that dictates
the width of the neighborhood. these memory-based methods require in
principle little or no training; all the work gets done at evaluation time.
the only parameter that needs to be determined from the training data is
  . the model, however, is the entire training data set.

we also discuss more general classes of kernel-based techniques , which
tie in with structured methods in other chapters, and are useful for density
estimation and classi   cation.

the techniques in this chapter should not be confused with those asso-
ciated with the more recent usage of the phrase    kernel methods   . in this
chapter kernels are mostly used as a device for localization. we discuss ker-
nel methods in sections 5.8, 14.5.4, 18.5 and chapter 12; in those contexts
the kernel computes an inner product in a high-dimensional (implicit) fea-
ture space, and is used for regularized nonlinear modeling. we make some
connections to the methodology in this chapter at the end of section 6.7.

192

6. kernel smoothing methods

nearest-neighbor kernel

epanechnikov kernel

o
o

o
o

o

o
o
o

o
o
o
o
o

o

o
o
o

o
o

o

o
o

o
o

o
o
o
o
o
o

o
o
o
o

o
o
o
o
oo
oo

o

o

o

o

ooo

o
o

o

o
oo
o
o
o

o

o

oo
o

o

o

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

o
o
o
o
o
o
  f (x0)
o
o
   
oo
oo
oo
oo
oo
oo

o
o

o
o

o
o
o o
o

o

o

oo

oo
oo

o
o

o
o

o
o

o
o

o
o
o
o

o
o

o
o

o
o

o
o

o

o
o
o

o
o
o
o
o
o
o
o
o
o

o
o

o
o
o

o
o

o
o

o
o

o
o

o
o
o
o
o
o

o
o
o
o

o
o
o
o
oo
oo

o

o

o

o

ooo

o
o

o

o
oo
o
o
o

o

o

oo
o

o

o

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

o

o

o

o

o

o
o

o

o

o

o

o

o

o

o

o

o
o
  f (x0)
o
o
o
o
o
o
   
oo
oo
oo
oo
oo
oo

o
o

o
o

o
o
o o
o o

o
o

o
o

o
o
o
o

o
o

o
o

o
o

o

oo

oo
oo
oo

o
o

o
o

o

o

o

o

o

o
o

o

o

o

o

o

o

o

o

o

0.0

0.2

0.4

x0

0.6

0.8

1.0

0.0

0.2

0.4

x0

0.6

0.8

1.0

figure 6.1. in each panel 100 pairs xi, yi are generated at random from the
blue curve with gaussian errors: y = sin(4x) +   , x     u [0, 1],        n (0, 1/3). in
the left panel the green curve is the result of a 30-nearest-neighbor running-mean
smoother. the red point is the    tted constant   f (x0), and the red circles indicate
those observations contributing to the    t at x0. the solid yellow region indicates
the weights assigned to observations. in the right panel, the green curve is the
kernel-weighted average, using an epanechnikov kernel with (half ) window width
   = 0.2.

6.1 one-dimensional kernel smoothers

in chapter 2, we motivated the k   nearest-neighbor average

  f (x) = ave(yi|xi     nk(x))

(6.1)

as an estimate of the regression function e(y |x = x). here nk(x) is the set
of k points nearest to x in squared distance, and ave denotes the average
(mean). the idea is to relax the de   nition of conditional expectation, as
illustrated in the left panel of figure 6.1, and compute an average in a
neighborhood of the target point. in this case we have used the 30-nearest
neighborhood   the    t at x0 is the average of the 30 pairs whose xi values
are closest to x0. the green curve is traced out as we apply this de   nition
at di   erent values x0. the green curve is bumpy, since   f (x) is discontinuous
in x. as we move x0 from left to right, the k-nearest neighborhood remains
constant, until a point xi to the right of x0 becomes closer than the furthest
point xi    in the neighborhood to the left of x0, at which time xi replaces xi   .
the average in (6.1) changes in a discrete way, leading to a discontinuous
  f (x).

this discontinuity is ugly and unnecessary. rather than give all the
points in the neighborhood equal weight, we can assign weights that die
o    smoothly with distance from the target point. the right panel shows
an example of this, using the so-called nadaraya   watson kernel-weighted

with the epanechnikov quadratic kernel

average

with

6.1 one-dimensional kernel smoothers

193

,

(6.2)

i=1 k  (x0, xi)yi
i=1 k  (x0, xi)

  f (x0) = pn
pn
k  (x0, x) = d(cid:18)|x     x0|
d(t) =(cid:26) 3

4 (1     t2)
0

  

(cid:19) ,

if |t|     1;
otherwise.

(6.3)

(6.4)

the    tted function is now continuous, and quite smooth in the right panel
of figure 6.1. as we move the target from left to right, points enter the
neighborhood initially with weight zero, and then their contribution slowly
increases (see exercise 6.1).

in the right panel we used a metric window size    = 0.2 for the kernel
   t, which does not change as we move the target point x0, while the size
of the 30-nearest-neighbor smoothing window adapts to the local density
of the xi. one can, however, also use such adaptive neighborhoods with
kernels, but we need to use a more general notation. let h  (x0) be a width
function (indexed by   ) that determines the width of the neighborhood at
x0. then more generally we have

h  (x0)(cid:19) .
k  (x0, x) = d(cid:18)|x     x0|

(6.5)

in (6.3), h  (x0) =    is constant. for k-nearest neighborhoods, the neigh-
borhood size k replaces   , and we have hk(x0) = |x0     x[k]| where x[k] is
the kth closest xi to x0.

there are a number of details that one has to attend to in practice:
    the smoothing parameter   , which determines the width of the local
neighborhood, has to be determined. large    implies lower variance
(averages over more observations) but higher bias (we essentially as-
sume the true function is constant within the window).

    metric window widths (constant h  (x)) tend to keep the bias of the
estimate constant, but the variance is inversely proportional to the
local density. nearest-neighbor window widths exhibit the opposite
behavior; the variance stays constant and the absolute bias varies
inversely with local density.

    issues arise with nearest-neighbors when there are ties in the xi. with
most smoothing techniques one can simply reduce the data set by
averaging the yi at tied values of x, and supplementing these new
observations at the unique values of xi with an additional weight wi
(which multiples the kernel weight).

194

6. kernel smoothing methods

)
x
,
0
x
(
  
k

8

.

0

4
0

.

0

.

0

epanechnikov
tri-cube
gaussian

-3

-2

-1

0

1

2

3

figure 6.2. a comparison of three popular kernels for local smoothing. each
has been calibrated to integrate to 1. the tri-cube kernel is compact and has two
continuous derivatives at the boundary of its support, while the epanechnikov ker-
nel has none. the gaussian kernel is continuously di   erentiable, but has in   nite
support.

    this leaves a more general problem to deal with: observation weights
wi. operationally we simply multiply them by the kernel weights be-
fore computing the weighted average. with nearest neighborhoods, it
is now natural to insist on neighborhoods with a total weight content

k (relative to p wi). in the event of over   ow (the last observation

needed in a neighborhood has a weight wj which causes the sum of
weights to exceed the budget k), then fractional parts can be used.

    boundary issues arise. the metric neighborhoods tend to contain less
points on the boundaries, while the nearest-neighborhoods get wider.

    the epanechnikov kernel has compact support (needed when used
with nearest-neighbor window size). another popular compact kernel
is based on the tri-cube function

d(t) =(cid:26) (1     |t|3)3

0

if |t|     1;
otherwise

(6.6)

this is    atter on the top (like the nearest-neighbor box) and is di   er-
entiable at the boundary of its support. the gaussian density func-
tion d(t) =   (t) is a popular noncompact kernel, with the standard-
deviation playing the role of the window size. figure 6.2 compares
the three.

6.1.1 local id75

we have progressed from the raw moving average to a smoothly varying
locally weighted average by using kernel weighting. the smooth kernel    t
still has problems, however, as exhibited in figure 6.3 (left panel). locally-
weighted averages can be badly biased on the boundaries of the domain,

6.1 one-dimensional kernel smoothers

195

n-w kernel at boundary

local id75 at boundary

5

.

1

0

.

1

5

.

0

o
o

o
o

o

o

oo
o

o
o
oo

o

o

oo
o

o

o
o
o
o
o
o
o
o

o

o

o

oo
o

o

o

o
o

o

o
o
o
o
o
o

o
o

o
o

o
o

o
o
o
o

o
o
o
o
  f (x0)
o
o
oo
oo
o
o
   
o
o

oo
oo

o
o

o
o

o

o
o
o
o
o
o

o
o

o
o
o
o
o
o

0

.

0

o
o

5

.

0
-

0

.

1
-

5

.

1

0

.

1

5

.

0

o
o

o
o

o
o

oo
oo

o
o
o
o

o
o
o
o
o
o
oo
oo
o
o
  f (x0)
o
o
   

o
o

o
o

o
o
o
o
o
o

o
o

0

.

0

o
o

o
o

o

o

o

o

o
o
o
o
o
o

o
o

o

o

oo
o

o
o
oo

o

o

oo
o

o

o
o
o
o
o
o
o
o

o

o

o

oo
o

o

o

o
o

o

o
o
o
o
o
o

o
o

o

o

o o
o

o

o

o
o

o
o
o
oo
o

o
o

o

o
o

o
o
o

o
o

o

o

o

o

o

o o
o

o

o

o
o

o
o
o
oo
o

o
o

o

o
o

o
o
o

o

o

o

o
o

o

o

o
o

5

.

0
-

0

.

1
-

o

o

o

o
o

o

o

o
o

0.0

x0

0.2

0.4

0.6

0.8

1.0

0.0

x0

0.2

0.4

0.6

0.8

1.0

figure 6.3. the locally weighted average has bias problems at or near the
boundaries of the domain. the true function is approximately linear here, but
most of the observations in the neighborhood have a higher mean than the target
point, so despite weighting, their mean will be biased upwards. by    tting a locally
weighted id75 (right panel), this bias is removed to    rst order.

because of the asymmetry of the kernel in that region. by    tting straight
lines rather than constants locally, we can remove this bias exactly to    rst
order; see figure 6.3 (right panel). actually, this bias can be present in the
interior of the domain as well, if the x values are not equally spaced (for
the same reasons, but usually less severe). again locally weighted linear
regression will make a    rst-order correction.

locally weighted regression solves a separate weighted least squares prob-

lem at each target point x0:

min

  (x0),  (x0)

nxi=1

k  (x0, xi) [yi       (x0)       (x0)xi]2 .

(6.7)

the estimate is then   f (x0) =     (x0) +     (x0)x0. notice that although we    t
an entire linear model to the data in the region, we only use it to evaluate
the    t at the single point x0.

de   ne the vector-valued function b(x)t = (1, x). let b be the n    2
regression matrix with ith row b(xi)t , and w(x0) the n    n diagonal
matrix with ith diagonal element k  (x0, xi). then

  f (x0) = b(x0)t (bt w(x0)b)   1bt w(x0)y

=

nxi=1

li(x0)yi.

(6.8)

(6.9)

equation (6.8) gives an explicit expression for the local id75
estimate, and (6.9) highlights the fact that the estimate is linear in the

196

6. kernel smoothing methods

local linear equivalent kernel at boundary

local linear equivalent kernel in interior

o

o

o

o
o

o

oo

o
o

o
o

o
o

      

o

ooo
ooo

o

oo
o
o
o

o
o

o

o

oo

o
o

o

o

o

o

o

o

o

o
o

o
o

o
o

o
o

o
o
o
o

o
o

      
  f (x0)
            
   
                                                                                                                                                                                                                                                          

o
o
oo
o

o
o
oo
o

o
oo

o
o
o
o

o o

o
o

o
o

o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

oo
o

o

o

oo
o
o
o
o
o
oo

oo

o

o

               

         

   

o
o

o
o
o
o

o
o

o
o

o
o

0.0

x0

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

o
o

o
o

o
o

o

o
o
o
o

o

o

o

o
o

o

oo
oo
o
o
o
o
o
o

o
o
o
o

         
oo
oo

o
o
o
o

ooo

o

o

o

o
o

o
o
o
o

o
o

o
o
o
o

                                                                                                                                     

o
o

o
o

o

o

o

o

o

o

o

o
o

oo
oo

                                         
            
  f (x0)
   

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o

   
      

o
o
o
o
oo
oo
o
o

o
o

   
   
o
o

o
o
oo
o

o

o

o
o

o

o

o
oo

o
                                                                                           
o

o o

oo
o

oo
o
o
o
o
o
oo

oo

o

o

x0

0.6

0.8

1.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

figure 6.4. the green points show the equivalent kernel li(x0) for local re-
gression. these are the weights in   f (x0) = pn
i=1 li(x0)yi, plotted against their
corresponding xi. for display purposes, these have been rescaled, since in fact
they sum to 1. since the yellow shaded region is the (rescaled) equivalent kernel
for the nadaraya   watson local average, we see how local regression automati-
cally modi   es the weighting kernel to correct for biases due to asymmetry in the
smoothing window.

yi (the li(x0) do not involve y). these weights li(x0) combine the weight-
ing kernel k  (x0,  ) and the least squares operations, and are sometimes
referred to as the equivalent kernel. figure 6.4 illustrates the e   ect of lo-
cal id75 on the equivalent kernel. historically, the bias in the
nadaraya   watson and other local average kernel methods were corrected
by modifying the kernel. these modi   cations were based on theoretical
asymptotic mean-square-error considerations, and besides being tedious to
implement, are only approximate for    nite sample sizes. local linear re-
gression automatically modi   es the kernel to correct the bias exactly to
   rst order, a phenomenon dubbed as automatic kernel carpentry. consider
the following expansion for e   f (x0), using the linearity of local regression
and a series expansion of the true function f around x0,

e   f (x0) =

nxi=1

li(x0)f (xi)

= f (x0)

li(x0) + f    (x0)

nxi=1

f       (x0)

+

2

nxi=1

nxi=1

(xi     x0)li(x0)

(xi     x0)2li(x0) + r,

(6.10)

where the remainder term r involves third- and higher-order derivatives of
f , and is typically small under suitable smoothness assumptions. it can be

6.1 one-dimensional kernel smoothers

197

o
o

oo
oo

o
o

o

o

oo
oo
o
o

o

o
o

o
o

o
o

o
o

local linear in interior

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o
o
o
o
o

o
o

  f (x0)
   

o
o
o
o
o
o

o
o
o
o
o o
o o

o
o
o
o
o
o
o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o

o

o

o

oo
oo

o

o

o

o

o

o

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

o

o

o
o
o

o
oo

o

o
o

o
o

o

oo

o

o

o
o

o
o

o

o

o

o

o

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

local quadratic in interior

o
o

o
o

o
o

oo
oo

o
o

o

o

o
o

o
o

o
o

o
o

oo
oo
o
o

o

o
o

o
o

o
o

o
o

o

o

oo
oo

o

o

o

o

o

o

o
o

o
o
o
o
o
o

o
o

  f (x0)
   

o
o
o
o
o
o

o
o
o
o
o o
o o

o
o
o
o
o
o
o
o

o
o

o
o

o
o

o
o

o
o

o
o

o
o

o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o

o

o

o

o
o
o

o
oo

o

o
o

o
o

o

oo

o

o

o
o

o
o

o

o

o

o

o

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

figure 6.5. local linear    ts exhibit bias in regions of curvature of the true
function. local quadratic    ts tend to eliminate this bias.

shown (exercise 6.2) that for local id75, pn
i=1 li(x0) = 1 and
pn
i=1(xi     x0)li(x0) = 0. hence the middle term equals f (x0), and since
the bias is e   f (x0)     f (x0), we see that it depends only on quadratic and
higher   order terms in the expansion of f .

6.1.2 local polynomial regression

why stop at local linear    ts? we can    t local polynomial    ts of any de-
gree d,

min

  (x0),  j (x0), j=1,...,d

nxi=1

k  (x0, xi)      yi       (x0)    

2

(6.11)

dxj=1

  j(x0)xj

i      

    j(x0)xj

with solution   f (x0) =     (x0)+pd

0. in fact, an expansion such as
(6.10) will tell us that the bias will only have components of degree d+1 and
higher (exercise 6.2). figure 6.5 illustrates local quadratic regression. local
linear    ts tend to be biased in regions of curvature of the true function, a
phenomenon referred to as trimming the hills and    lling the valleys. local
quadratic regression is generally able to correct this bias.

j=1

there is of course a price to be paid for this bias reduction, and that is
increased variance. the    t in the right panel of figure 6.5 is slightly more
wiggly, especially in the tails. assuming the model yi = f (xi) +   i, with
  i independent and identically distributed with mean zero and variance
  2, var(   f (x0)) =   2||l(x0)||2, where l(x0) is the vector of equivalent kernel
weights at x0. it can be shown (exercise 6.3) that ||l(x0)|| increases with d,
and so there is a bias   variance tradeo    in selecting the polynomial degree.
figure 6.6 illustrates these variance curves for degree zero, one and two

198

6. kernel smoothing methods

e
c
n
a
i
r
a
v

5
0

.

4
0

.

3
0

.

2
0

.

1
0

.

0
0

.

constant
linear
quadratic

0.0

0.2

0.4

0.6

0.8

1.0

figure 6.6. the variances functions ||l(x)||2 for local constant, linear and
quadratic regression, for a metric bandwidth (   = 0.2) tri-cube kernel.

local polynomials. to summarize some collected wisdom on this issue:

    local linear    ts can help bias dramatically at the boundaries at a
modest cost in variance. local quadratic    ts do little at the bound-
aries for bias, but increase the variance a lot.

    local quadratic    ts tend to be most helpful in reducing bias due to

curvature in the interior of the domain.

    asymptotic analysis suggest that local polynomials of odd degree
dominate those of even degree. this is largely due to the fact that
asymptotically the mse is dominated by boundary e   ects.

while it may be helpful to tinker, and move from local linear    ts at the
boundary to local quadratic    ts in the interior, we do not recommend such
strategies. usually the application will dictate the degree of the    t. for
example, if we are interested in extrapolation, then the boundary is of
more interest, and local linear    ts are probably more reliable.

6.2 selecting the width of the kernel

in each of the kernels k  ,    is a parameter that controls its width:

    for the epanechnikov or tri-cube kernel with metric width,    is the

radius of the support region.

    for the gaussian kernel,    is the standard deviation.
       is the number k of nearest neighbors in k-nearest neighborhoods,
often expressed as a fraction or span k/n of the total training sample.

6.2 selecting the width of the kernel

199

      
   
         
      
      
                  
      
   
   

      
   
   
      
      
               
   
               
      
                                                                                                                                                                                                                              
                                                                                                                                                                                                                                        

                                                                        
                                                                           
                  
                  

   
   

                                                                                        
                                                                                     

                                                                                                                
         
                                                                                                       

figure 6.7. equivalent kernels for a local id75 smoother (tri-cube
kernel; orange) and a smoothing spline (blue), with matching degrees of freedom.
the vertical spikes indicates the target points.

there is a natural bias   variance tradeo    as we change the width of the
averaging window, which is most explicit for local averages:

    if the window is narrow,   f (x0) is an average of a small number of yi
close to x0, and its variance will be relatively large   close to that of
an individual yi. the bias will tend to be small, again because each
of the e(yi) = f (xi) should be close to f (x0).

    if the window is wide, the variance of   f (x0) will be small relative to
the variance of any yi, because of the e   ects of averaging. the bias
will be higher, because we are now using observations xi further from
x0, and there is no guarantee that f (xi) will be close to f (x0).

similar arguments apply to local regression estimates, say local linear: as
the width goes to zero, the estimates approach a piecewise-linear function
that interpolates the training data1; as the width gets in   nitely large, the
   t approaches the global linear least-squares    t to the data.

the discussion in chapter 5 on selecting the id173 parameter for
smoothing splines applies here, and will not be repeated. local regression
smoothers are linear estimators; the smoother matrix in   f = s  y is built up
from the equivalent kernels (6.8), and has ijth entry {s  }ij = li(xj). leave-
one-out cross-validation is particularly simple (exercise 6.7), as is general-
ized cross-validation, cp (exercise 6.10), and k-fold cross-validation. the
e   ective degrees of freedom is again de   ned as trace(s  ), and can be used
to calibrate the amount of smoothing. figure 6.7 compares the equivalent
kernels for a smoothing spline and local id75. the local regres-
sion smoother has a span of 40%, which results in df = trace(s  ) = 5.86.
the smoothing spline was calibrated to have the same df, and their equiv-
alent kernels are qualitatively quite similar.

1with uniformly spaced xi; with irregularly spaced xi, the behavior can deteriorate.

200

6. kernel smoothing methods

6.3 local regression in irp

kernel smoothing and local regression generalize very naturally to two or
more dimensions. the nadaraya   watson kernel smoother    ts a constant
locally with weights supplied by a p-dimensional kernel. local linear re-
gression will    t a hyperplane locally in x, by weighted least squares, with
weights supplied by a p-dimensional kernel. it is simple to implement and
is generally preferred to the local constant    t for its superior performance
on the boundaries.

let b(x) be a vector of polynomial terms in x of maximum degree d.
for example, with d = 1 and p = 2 we get b(x) = (1, x1, x2); with d = 2
we get b(x) = (1, x1, x2, x 2
1 , x 2
2 , x1x2); and trivially with d = 0 we get
b(x) = 1. at each x0     irp solve

min
  (x0)

nxi=1

k  (x0, xi)(yi     b(xi)t   (x0))2

(6.12)

to produce the    t   f (x0) = b(x0)t     (x0). typically the kernel will be a radial
function, such as the radial epanechnikov or tri-cube kernel

k  (x0, x) = d(cid:18)||x     x0||

  

(cid:19) ,

(6.13)

where ||  || is the euclidean norm. since the euclidean norm depends on the
units in each coordinate, it makes most sense to standardize each predictor,
for example, to unit standard deviation, prior to smoothing.

while boundary e   ects are a problem in one-dimensional smoothing,
they are a much bigger problem in two or higher dimensions, since the
fraction of points on the boundary is larger. in fact, one of the manifesta-
tions of the curse of dimensionality is that the fraction of points close to the
boundary increases to one as the dimension grows. directly modifying the
kernel to accommodate two-dimensional boundaries becomes very messy,
especially for irregular boundaries. local polynomial regression seaid113ssly
performs boundary correction to the desired order in any dimensions. fig-
ure 6.8 illustrates local id75 on some measurements from an
astronomical study with an unusual predictor design (star-shaped). here
the boundary is extremely irregular, and the    tted surface must also inter-
polate over regions of increasing data sparsity as we approach the boundary.
local regression becomes less useful in dimensions much higher than two
or three. we have discussed in some detail the problems of dimensional-
ity, for example, in chapter 2. it is impossible to simultaneously main-
tain localness (    low bias) and a sizable sample in the neighborhood (   
low variance) as the dimension increases, without the total sample size in-
creasing exponentially in p. visualization of   f (x) also becomes di   cult in
higher dimensions, and this is often one of the primary goals of smoothing.

6.4 structured local regression models in irp

201

velocity

velocity

south-north

south-north

east-west

east-west

figure 6.8. the left panel shows three-dimensional data, where the response
is the velocity measurements on a galaxy, and the two predictors record positions
on the celestial sphere. the unusual    star   -shaped design indicates the way the
measurements were made, and results in an extremely irregular boundary. the
right panel shows the results of local id75 smoothing in ir2, using a
nearest-neighbor window with 15% of the data.

although the scatter-cloud and wire-frame pictures in figure 6.8 look at-
tractive, it is quite di   cult to interpret the results except at a gross level.
from a data analysis perspective, conditional plots are far more useful.

figure 6.9 shows an analysis of some environmental data with three pre-
dictors. the trellis display here shows ozone as a function of radiation,
conditioned on the other two variables, temperature and wind speed. how-
ever, conditioning on the value of a variable really implies local to that
value (as in local regression). above each of the panels in figure 6.9 is an
indication of the range of values present in that panel for each of the condi-
tioning values. in the panel itself the data subsets are displayed (response
versus remaining variable), and a one-dimensional local id75 is
   t to the data. although this is not quite the same as looking at slices of
a    tted three-dimensional surface, it is probably more useful in terms of
understanding the joint behavior of the data.

6.4 structured local regression models in irp

when the dimension to sample-size ratio is unfavorable, local regression
does not help us much, unless we are willing to make some structural as-
sumptions about the model. much of this book is about structured regres-
sion and classi   cation models. here we focus on some approaches directly
related to kernel methods.

202

6. kernel smoothing methods

0

50

150

250

0

50

150

250

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

wind
temp

5

4

3

2

1

5

4

3

2

1

)
b
p
p

 
t

o
o
r
 

e
b
u
c
(
 

e
n
o
z
o

 
t

 

o
o
r
e
b
u
c

5

4

3

2

1

5

4

3

2

1

0

50

150

250

0

50

150

250

solar radiation (langleys)

figure 6.9. three-dimensional smoothing example. the response is (cube-root
of ) ozone concentration, and the three predictors are temperature, wind speed and
radiation. the trellis display shows ozone as a function of radiation, conditioned
on intervals of temperature and wind speed (indicated by darker green or orange
shaded bars). each panel contains about 40% of the range of each of the condi-
tioned variables. the curve in each panel is a univariate local id75,
   t to the data in the panel.

6.4 structured local regression models in irp

203

6.4.1 structured kernels

one line of approach is to modify the kernel. the default spherical ker-
nel (6.13) gives equal weight to each coordinate, and so a natural default
strategy is to standardize each variable to unit standard deviation. a more
general approach is to use a positive semide   nite matrix a to weigh the
di   erent coordinates:

k  ,a(x0, x) = d(cid:18) (x     x0)t a(x     x0)

  

(cid:19) .

(6.14)

entire coordinates or directions can be downgraded or omitted by imposing
appropriate restrictions on a. for example, if a is diagonal, then we can
increase or decrease the in   uence of individual predictors xj by increasing
or decreasing ajj. often the predictors are many and highly correlated,
such as those arising from digitized analog signals or images. the covariance
function of the predictors can be used to tailor a metric a that focuses less,
say, on high-frequency contrasts (exercise 6.4). proposals have been made
for learning the parameters for multidimensional kernels. for example, the
projection-pursuit regression model discussed in chapter 11 is of this    avor,
where low-rank versions of a imply ridge functions for   f (x). more general
models for a are cumbersome, and we favor instead the structured forms
for the regression function discussed next.

6.4.2 structured regression functions
we are trying to    t a regression function e(y |x) = f (x1, x2, . . . , xp) in
irp, in which every level of interaction is potentially present. it is natural
to consider analysis-of-variance (anova) decompositions of the form

f (x1, x2, . . . , xp) =    +xj

gj(xj) +xk<   

gk   (xk, x   ) +       

(6.15)

additive models assume only main e   ect terms: f (x) =    +pp

and then introduce structure by eliminating some of the higher-order terms.
j=1 gj(xj);
second-order models will have terms with interactions of order at most
two, and so on. in chapter 9, we describe iterative back   tting algorithms
for    tting such low-order interaction models. in the additive model, for
example, if all but the kth term is assumed known, then we can estimate gk

by local regression of y    pj6=k gj(xj) on xk. this is done for each function

in turn, repeatedly, until convergence. the important detail is that at any
stage, one-dimensional local regression is all that is needed. the same ideas
can be used to    t low-dimensional anova decompositions.

an important special case of these structured models are the class of
varying coe   cient models. suppose, for example, that we divide the p pre-
dictors in x into a set (x1, x2, . . . , xq) with q < p, and the remainder of

204

6. kernel smoothing methods

aortic diameter vs age

male
depth

20

30

60

50
40
male
depth

male
depth

20

30

60

50
40
male
depth

male
depth

20

30

60

50
40
male
depth

t

r
e
e
m
a
d

i

24

22

20

18

16

14

12

10

female
depth

female
depth

female
depth

female
depth

female
depth

female
depth

24

22

20

18

16

14

12

10

20

30

40

50

60

20

30

40

50

60

20

30

40

50

60

age

figure 6.10. in each panel the aorta diameter is modeled as a linear func-
tion of age. the coe   cients of this model vary with gender and depth down
the aorta (left is near the top, right is low down). there is a clear trend in the
coe   cients of the linear model.

the variables we collect in the vector z. we then assume the conditionally
linear model

f (x) =   (z) +   1(z)x1 +        +   q(z)xq.

(6.16)

for given z, this is a linear model, but each of the coe   cients can vary
with z. it is natural to    t such a model by locally weighted least squares:

nxi=1

min

  (z0),  (z0)

k  (z0, zi) (yi       (z0)     x1i  1(z0)                xqi  q(z0))2 .

(6.17)
figure 6.10 illustrates the idea on measurements of the human aorta.
a longstanding claim has been that the aorta thickens with age. here we
model the diameter of the aorta as a linear function of age, but allow the
coe   cients to vary with gender and depth down the aorta. we used a local
regression model separately for males and females. while the aorta clearly
does thicken with age at the higher regions of the aorta, the relationship
fades with distance down the aorta. figure 6.11 shows the intercept and
slope as a function of depth.

6.5 local likelihood and other models

205

male

female

0
2

8
1

6
1

4
1

2

.

1

.

8
0

4
.
0

0
.
0

t

p
e
c
r
e
n

t

i
 

e
g
a

l

e
p
o
s
 
e
g
a

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

distance down aorta

distance down aorta

figure 6.11. the intercept and slope of age as a function of distance down
the aorta, separately for males and females. the yellow bands indicate one stan-
dard error.

6.5 local likelihood and other models

the concept of local regression and varying coe   cient models is extremely
broad: any parametric model can be made local if the    tting method ac-
commodates observation weights. here are some examples:

    associated with each observation yi is a parameter   i =   (xi) = xt
i   
linear in the covariate(s) xi, and id136 for    is based on the log-
i   ). we can model   (x) more    exibly
0   (x0):

by using the likelihood local to x0 for id136 of   (x0) = xt

likelihood l(  ) =pn

i=1 l(yi, xt

l(  (x0)) =

nxi=1

k  (x0, xi)l(yi, xt

i   (x0)).

many likelihood models, in particular the family of generalized linear
models including logistic and id148, involve the covariates
in a linear fashion. local likelihood allows a relaxation from a globally
linear model to one that is locally linear.

206

6. kernel smoothing methods

    as above, except di   erent variables are associated with    from those

used for de   ning the local likelihood:

l(  (z0)) =

nxi=1

k  (z0, zi)l(yi,   (xi,   (z0))).

for example,   (x,   ) = xt    could be a linear model in x. this will    t
a varying coe   cient model   (z) by maximizing the local likelihood.

    autoregressive time series models of order k have the form yt =
  0 +   1yt   1 +   2yt   2 +        +   kyt   k +   t. denoting the lag set by
zt = (yt   1, yt   2, . . . , yt   k), the model looks like a standard linear
model yt = zt
t    +   t, and is typically    t by least squares. fitting
by local least squares with a kernel k(z0, zt) allows the model to
vary according to the short-term history of the series. this is to be
distinguished from the more traditional dynamic linear models that
vary by windowing time.

as an illustration of local likelihood, we consider the local version of the
multiclass linear id28 model (4.36) of chapter 4. the data
consist of features xi and an associated categorical response gi     {1, 2, . . . , j},
and the linear model has the form

e  j0+  t
j x
k=1 e  k0+  t
k x

.

(6.18)

the local log-likelihood for this j class model can be written

pr(g = j|x = x) =

1 +pj   1
k  (x0, xi)(  gi0(x0) +   gi (x0)t (xi     x0)

nxi=1

    log"1 +

j   1xk=1

exp(cid:0)  k0(x0) +   k(x0)t (xi     x0)(cid:1)#) .

(6.19)

notice that

    we have used gi as a subscript in the    rst line to pick out the appro-

priate numerator;

      j0 = 0 and   j = 0 by the de   nition of the model;
    we have centered the local regressions at x0, so that the    tted poste-

rior probabilities at x0 are simply

  pr(g = j|x = x0) =

e     j0(x0)
k=1 e     k0(x0)

1 +pj   1

.

(6.20)

6.5 local likelihood and other models

207

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

 

d
h
c
e
c
n
e
a
v
e
r
p

l

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

 

d
h
c
e
c
n
e
a
v
e
r
p

l

100

140

180

220

15

25

35

45

systolic blood pressure

obesity

figure 6.12. each plot shows the binary response chd (coronary heart dis-
ease) as a function of a risk factor for the south african heart disease data.
for each plot we have computed the    tted prevalence of chd using a local linear
id28 model. the unexpected increase in the prevalence of chd at
the lower ends of the ranges is because these are retrospective data, and some of
the subjects had already undergone treatment to reduce their blood pressure and
weight. the shaded region in the plot indicates an estimated pointwise standard
error band.

this model can be used for    exible multiclass classi   cation in moderately
low dimensions, although successes have been reported with the high-
dimensional zip-code classi   cation problem. generalized additive models
(chapter 9) using kernel smoothing methods are closely related, and avoid
dimensionality problems by assuming an additive structure for the regres-
sion function.

as a simple illustration we    t a two-class local linear logistic model to
the heart disease data of chapter 4. figure 6.12 shows the univariate local
logistic models    t to two of the risk factors (separately). this is a useful
screening device for detecting nonlinearities, when the data themselves have
little visual information to o   er. in this case an unexpected anomaly is
uncovered in the data, which may have gone unnoticed with traditional
methods.

since chd is a binary indicator, we could estimate the conditional preva-
lence pr(g = j|x0) by simply smoothing this binary response directly with-
out resorting to a likelihood formulation. this amounts to    tting a locally
constant id28 model (exercise 6.5). in order to enjoy the bias-
correction of local-linear smoothing, it is more natural to operate on the
unrestricted logit scale.

typically with id28, we compute parameter estimates as
well as their standard errors. this can be done locally as well, and so

208

6. kernel smoothing methods

e
t
a
m

i
t
s
e
 
y
t
i
s
n
e
d

0
2
0
.
0

5
1
0
.
0

0
1
0
.
0

5
0
0
.
0

0
.
0

100

120

140

160

180

200

220

systolic blood pressure (for chd group)

figure 6.13. a kernel density estimate for systolic blood pressure (for the
chd group). the density estimate at each point is the average contribution from
each of the kernels at that point. we have scaled the kernels down by a factor of
10 to make the graph readable.

we can produce, as shown in the plot, estimated pointwise standard-error
bands about our    tted prevalence.

6.6 kernel density estimation and classi   cation

kernel density estimation is an unsupervised learning procedure, which
historically precedes kernel regression. it also leads naturally to a simple
family of procedures for nonparametric classi   cation.

6.6.1 kernel density estimation

suppose we have a random sample x1, . . . , xn drawn from a id203
density fx (x), and we wish to estimate fx at a point x0. for simplicity we
assume for now that x     ir. arguing as before, a natural local estimate
has the form

  fx (x0) =

#xi     n (x0)

n   

,

(6.21)

where n (x0) is a small metric neighborhood around x0 of width   . this
estimate is bumpy, and the smooth parzen estimate is preferred

  fx (x0) =

1
n   

nxi=1

k  (x0, xi),

(6.22)

6.6 kernel density estimation and classi   cation

209

s
e

t

a
m

i
t
s
e
 
y
t
i
s
n
e
d

0
2
0

.

0

0
1
0

.

0

0

.

0

chd
no chd

e

t

a
m

i
t
s
e

 
r
o
i
r
e

t
s
o
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

100

140

180

220

100

140

180

220

systolic blood pressure

systolic blood pressure

figure 6.14. the left panel shows the two separate density estimates for
systolic blood pressure in the chd versus no-chd groups, using a gaussian
kernel density estimate in each. the right panel shows the estimated posterior
probabilities for chd, using (6.25).

because it counts observations close to x0 with weights that decrease with
distance from x0. in this case a popular choice for k   is the gaussian kernel
k  (x0, x) =   (|x     x0|/  ). figure 6.13 shows a gaussian kernel density    t
to the sample values for systolic blood pressure for the chd group. letting
     denote the gaussian density with mean zero and standard-deviation   ,
then (6.22) has the form

  fx (x) =

1
n

    (x     xi)

nxi=1

= (   f         )(x),

(6.23)

the convolution of the sample empirical distribution   f with     . the dis-
tribution   f (x) puts mass 1/n at each of the observed xi, and is jumpy; in
  fx (x) we have smoothed   f by adding independent gaussian noise to each
observation xi.

the parzen density estimate is the equivalent of the local average, and
improvements have been proposed along the lines of local regression [on the
log scale for densities; see loader (1999)]. we will not pursue these here.
in irp the natural generalization of the gaussian density estimate amounts
to using the gaussian product kernel in (6.23),

  fx (x0) =

1

n (2  2  )

p
2

nxi=1

e    1

2 (||xi   x0||/  )2

.

(6.24)

210

6. kernel smoothing methods

1.0

0.5

0.0

figure 6.15. the population class densities may have interesting structure
(left) that disappears when the posterior probabilities are formed (right).

6.6.2 kernel density classi   cation

one can use nonparametric density estimates for classi   cation in a straight-
forward fashion using bayes    theorem. suppose for a j class problem we    t
nonparametric density estimates   fj(x), j = 1, . . . , j separately in each of
the classes, and we also have estimates of the class priors     j (usually the
sample proportions). then

  pr(g = j|x = x0) =

.

(6.25)

    j   fj(x0)
k=1     k   fk(x0)

pj

figure 6.14 uses this method to estimate the prevalence of chd for the
heart risk factor study, and should be compared with the left panel of fig-
ure 6.12. the main di   erence occurs in the region of high sbp in the right
panel of figure 6.14. in this region the data are sparse for both classes, and
since the gaussian kernel density estimates use metric kernels, the density
estimates are low and of poor quality (high variance) in these regions. the
local id28 method (6.20) uses the tri-cube kernel with id92
bandwidth; this e   ectively widens the kernel in this region, and makes use
of the local linear assumption to smooth out the estimate (on the logit
scale).

if classi   cation is the ultimate goal, then learning the separate class den-
sities well may be unnecessary, and can in fact be misleading. figure 6.15
shows an example where the densities are both multimodal, but the pos-
terior ratio is quite smooth. in learning the separate densities from data,
one might decide to settle for a rougher, high-variance    t to capture these
features, which are irrelevant for the purposes of estimating the posterior
probabilities. in fact, if classi   cation is the ultimate goal, then we need only
to estimate the posterior well near the decision boundary (for two classes,
this is the set {x|pr(g = 1|x = x) = 1

2}).

6.6.3 the naive bayes classi   er

this is a technique that has remained popular over the years, despite its
name (also known as    idiot   s bayes   !) it is especially appropriate when

6.6 kernel density estimation and classi   cation

211

the dimension p of the feature space is high, making density estimation
unattractive. the naive bayes model assumes that given a class g = j, the
features xk are independent:

fj(x) =

pyk=1

fjk(xk).

(6.26)

while this assumption is generally not true, it does simplify the estimation
dramatically:

    the individual class-conditional marginal densities fjk can each be
estimated separately using one-dimensional kernel density estimates.
this is in fact a generalization of the original naive bayes procedures,
which used univariate gaussians to represent these marginals.

    if a component xj of x is discrete, then an appropriate histogram
estimate can be used. this provides a seaid113ss way of mixing variable
types in a feature vector.

despite these rather optimistic assumptions, naive bayes classi   ers often
outperform far more sophisticated alternatives. the reasons are related to
figure 6.15: although the individual class density estimates may be biased,
this bias might not hurt the posterior probabilities as much, especially
near the decision regions. in fact, the problem may be able to withstand
considerable bias for the savings in variance such a    naive    assumption
earns.

starting from (6.26) we can derive the logit-transform (using class j as

the base):

log

pr(g =    |x)
pr(g = j|x)

= log

= log

= log

     f   (x)
  j fj (x)

     
  j

     qp
  jqp
pxk=1
pxk=1

+

=       +

g   k(xk).

k=1 f   k(xk)
k=1 fjk(xk)

log

f   k(xk)
fjk(xk)

(6.27)

this has the form of a generalized additive model, which is described in more
detail in chapter 9. the models are    t in quite di   erent ways though; their
di   erences are explored in exercise 6.9. the relationship between naive
bayes and generalized additive models is analogous to that between linear
discriminant analysis and id28 (section 4.4.5).

212

6. kernel smoothing methods

6.7 radial basis functions and kernels

f (x) =pm

in chapter 5, functions are represented as expansions in basis functions:
j=1   jhj(x). the art of    exible modeling using basis expansions
consists of picking an appropriate family of basis functions, and then con-
trolling the complexity of the representation by selection, id173, or
both. some of the families of basis functions have elements that are de   ned
locally; for example, b-splines are de   ned locally in ir. if more    exibility
is desired in a particular region, then that region needs to be represented
by more basis functions (which in the case of b-splines translates to more
knots). tensor products of ir-local basis functions deliver basis functions
local in irp. not all basis functions are local   for example, the truncated
power bases for splines, or the sigmoidal basis functions   (  0 +   x) used
in neural-networks (see chapter 11). the composed function f (x) can nev-
ertheless show local behavior, because of the particular signs and values
of the coe   cients causing cancellations of global e   ects. for example, the
truncated power basis has an equivalent b-spline basis for the same space
of functions; the cancellation is exact in this case.

kernel methods achieve    exibility by    tting simple models in a region
local to the target point x0. localization is achieved via a weighting kernel
k  , and individual observations receive weights k  (x0, xi).

radial basis functions combine these ideas, by treating the kernel func-

tions k  (  , x) as basis functions. this leads to the model

f (x) =

=

mxj=1
mxj=1

k  j (  j, x)  j

d(cid:18)||x       j||

  j

(cid:19)   j,

(6.28)

where each basis element is indexed by a location or prototype parameter   j
and a scale parameter   j. a popular choice for d is the standard gaussian
density function. there are several approaches to learning the parameters
{  j,   j,   j}, j = 1, . . . , m . for simplicity we will focus on least squares
methods for regression, and use the gaussian kernel.

    optimize the sum-of-squares with respect to all the parameters:

min

{  j ,  j ,  j }m
1

nxi=1

      yi       0    

mxj=1

  j exp(   

(xi       j)t (xi       j)

  2
j

2

.

)      

(6.29)
this model is commonly referred to as an rbf network, an alterna-
tive to the sigmoidal neural network discussed in chapter 11; the   j
and   j playing the role of the weights. this criterion is nonconvex

6.7 radial basis functions and kernels

213

8

6

4

2

0

2

.

1

8

.

0

4

.

0

0

.

0

figure 6.16. gaussian radial basis functions in ir with    xed width can leave
holes (top panel). renormalized gaussian radial basis functions avoid this prob-
lem, and produce basis functions similar in some respects to b-splines.

with multiple local minima, and the algorithms for optimization are
similar to those used for neural networks.

    estimate the {  j,   j} separately from the   j. given the former, the
estimation of the latter is a simple least squares problem. often the
kernel parameters   j and   j are chosen in an unsupervised way using
the x distribution alone. one of the methods is to    t a gaussian
mixture density model to the training xi, which provides both the
centers   j and the scales   j. other even more adhoc approaches use
id91 methods to locate the prototypes   j, and treat   j =   
as a hyper-parameter. the obvious drawback of these approaches is
that the conditional distribution pr(y |x) and in particular e(y |x)
is having no say in where the action is concentrated. on the positive
side, they are much simpler to implement.

while it would seem attractive to reduce the parameter set and assume
a constant value for   j =   , this can have an undesirable side e   ect of
creating holes   regions of irp where none of the kernels has appreciable
support, as illustrated in figure 6.16 (upper panel). renormalized radial
basis functions,

hj(x) =

,

(6.30)

avoid this problem (lower panel).

the nadaraya   watson kernel regression estimator (6.2) in irp can be

viewed as an expansion in renormalized radial basis functions,

i=1 yi

  f (x0) =pn
=pn

k  (x0,xi)
i=1 k  (x0,xi)

pn

i=1 yihi(x0)

(6.31)

d(||x       j||/  )
k=1 d(||x       k||/  )

pm

214

6. kernel smoothing methods

with a basis function hi located at every observation and coe   cients yi;
that is,   i = xi,     i = yi, i = 1, . . . , n .

note the similarity between the expansion (6.31) and the solution (5.50)
on page 169 to the id173 problem induced by the kernel k. radial
basis functions form the bridge between the modern    kernel methods    and
local    tting technology.

6.8 mixture models for density estimation and

classi   cation

the mixture model is a useful tool for density estimation, and can be viewed
as a kind of kernel method. the gaussian mixture model has the form

mxm=1

f (x) =

  m  (x;   m,   m)

(6.32)

with mixing proportions   m, pm   m = 1, and each gaussian density has

a mean   m and covariance matrix   m. in general, mixture models can use
any component densities in place of the gaussian in (6.32): the gaussian
mixture model is by far the most popular.

the parameters are usually    t by maximum likelihood, using the em

algorithm as described in chapter 8. some special cases arise:

    if the covariance matrices are constrained to be scalar:   m =   mi,

then (6.32) has the form of a radial basis expansion.

    if in addition   m =    > 0 is    xed, and m     n , then the max-
imum likelihood estimate for (6.32) approaches the kernel density
estimate (6.22) where     m = 1/n and     m = xm.

using bayes    theorem, separate mixture densities in each class lead to    ex-
ible models for pr(g|x); this is taken up in some detail in chapter 12.
figure 6.17 shows an application of mixtures to the heart disease risk-
factor study. in the top row are histograms of age for the no chd and chd
groups separately, and then combined on the right. using the combined
data, we    t a two-component mixture of the form (6.32) with the (scalars)
  1 and   2 not constrained to be equal. fitting was done via the em
algorithm (chapter 8): note that the procedure does not use knowledge of
the chd labels. the resulting estimates were
    1 = 157.7,
    2 = 15.6,

    1 = 36.4,
    2 = 58.0,

    1 = 0.7,
    2 = 0.3.

the component densities   (    1,     1) and   (    2,     2) are shown in the lower-
left and middle panels. the lower-right panel shows these component den-
sities (orange and blue) along with the estimated mixture density (green).

6.8 mixture models for density estimation and classi   cation

215

no chd

chd

combined

20

30

40

50

60

age

t

n
u
o
c

e

t

a
m

i
t
s
e
 
e
r
u
t
x
m

i

0
2

5
1

0
1

5

0

0
1
0

.

8
0
0

.

6
0

.

0

4
0
.
0

2
0
.
0

0
0
.
0

t

n
u
o
c

e

t

a
m

i
t
s
e
 
e
r
u
t
x
m

i

5
1

0
1

5

0

0
1
0

.

8
0
0

.

6
0

.

0

4
0
.
0

2
0
.
0

0
0
.
0

20

30

50

60

40

age

0
3

5
2

0
2

5
1

0
1

5

0

0
1
0

.

8
0
0

.

6
0

.

0

4
0
.
0

2
0
.
0

0
0
.
0

t

n
u
o
c

e

t

a
m

i
t
s
e
 
e
r
u
t
x
m

i

20

30

40

50

60

age

20

30

40

age

50

60

20

30

40

age

50

60

20

30

50

60

40

age

figure 6.17. application of mixtures to the heart disease risk-factor study.
(top row:) histograms of age for the no chd and chd groups separately, and
combined. (bottom row:) estimated component densities from a gaussian mix-
ture model, (bottom left, bottom middle); (bottom right:) estimated component
densities (blue and orange) along with the estimated mixture density (green). the
orange density has a very large standard deviation, and approximates a uniform
density.

the mixture model also provides an estimate of the id203 that

observation i belongs to component m,

    m  (xi;     m,     m)
k=1     k  (xi;     k,     k)

  rim =

pm

,

(6.33)

where xi is age in our example. suppose we threshold each value   ri2 and
hence de   ne     i = i(  ri2 > 0.5). then we can compare the classi   cation of
each observation by chd and the mixture model:

mixture model
     = 1
     = 0
232
70
84
76

chd no
yes

although the mixture model did not use the chd labels, it has done a fair
job in discovering the two chd subpopulations. linear id28,
using the chd as a response, achieves the same error rate (32%) when    t to
these data using maximum-likelihood (section 4.4).

216

6. kernel smoothing methods

6.9 computational considerations

kernel and local regression and density estimation are memory-based meth-
ods: the model is the entire training data set, and the    tting is done at
evaluation or prediction time. for many real-time applications, this can
make this class of methods infeasible.

the computational cost to    t at a single observation x0 is o(n )    ops,
except in oversimpli   ed cases (such as square kernels). by comparison,
an expansion in m basis functions costs o(m ) for one evaluation, and
typically m     o(log n ). basis function methods have an initial cost of at
least o(n m 2 + m 3).
the smoothing parameter(s)    for kernel methods are typically deter-
mined o   -line, for example using cross-validation, at a cost of o(n 2)    ops.
popular implementations of local regression, such as the loess function in
s-plus and r and the locfit procedure (loader, 1999), use triangulation
schemes to reduce the computations. they compute the    t exactly at m
carefully chosen locations (o(n m )), and then use blending techniques to
interpolate the    t elsewhere (o(m ) per evaluation).

bibliographic notes

there is a vast literature on kernel methods which we will not attempt to
summarize. rather we will point to a few good references that themselves
have extensive bibliographies. loader (1999) gives excellent coverage of lo-
cal regression and likelihood, and also describes state-of-the-art software
for    tting these models. fan and gijbels (1996) cover these models from
a more theoretical aspect. hastie and tibshirani (1990) discuss local re-
gression in the context of additive modeling. silverman (1986) gives a good
overview of density estimation, as does scott (1992).

exercises

ex. 6.1 show that the nadaraya   watson kernel smooth with    xed metric
bandwidth    and a gaussian kernel is di   erentiable. what can be said for
the epanechnikov kernel? what can be said for the epanechnikov kernel
with adaptive nearest-neighbor bandwidth   (x0)?

ex. 6.2 show thatpn
bj(x0) =pn

i=1(xi   x0)li(x0) = 0 for local id75. de   ne
i=1(xi     x0)jli(x0). show that b0(x0) = 1 for local polynomial
regression of any degree (including local constants). show that bj(x0) = 0
for all j     {1, 2, . . . , k} for local polynomial regression of degree k. what
are the implications of this on the bias?

exercises

217

ex. 6.3 show that ||l(x)|| (section 6.1.2) increases with the degree of the
local polynomial.

ex. 6.4 suppose that the p predictors x arise from sampling relatively
smooth analog curves at p uniformly spaced abscissa values. denote by
cov(x|y ) =    the conditional covariance matrix of the predictors, and
assume this does not change much with y . discuss the nature of maha-
lanobis choice a =      1 for the metric in (6.14). how does this compare
with a = i? how might you construct a kernel a that (a) downweights
high-frequency components in the distance metric; (b) ignores them
completely?

ex. 6.5 show that    tting a locally constant multinomial logit model of
the form (6.19) amounts to smoothing the binary response indicators for
each class separately using a nadaraya   watson kernel smoother with kernel
weights k  (x0, xi).

ex. 6.6 suppose that all you have is software for    tting local regression,
but you can specify exactly which monomials are included in the    t. how
could you use this software to    t a varying-coe   cient model in some of the
variables?

ex. 6.7 derive an expression for the leave-one-out cross-validated residual
sum-of-squares for local polynomial regression.

ex. 6.8 suppose that for continuous response y and predictor x, we model
the joint density of x, y using a multivariate gaussian kernel estimator.
note that the kernel in this case would be the product kernel     (x)    (y ).
show that the conditional mean e(y |x) derived from this estimate is a
nadaraya   watson estimator. extend this result to classi   cation by pro-
viding a suitable kernel for the estimation of the joint distribution of a
continuous x and discrete y .

ex. 6.9 explore the di   erences between the naive bayes model (6.27) and
a generalized additive id28 model, in terms of (a) model as-
sumptions and (b) estimation. if all the variables xk are discrete, what can
you say about the corresponding gam?

ex. 6.10 suppose we have n samples generated from the model yi = f (xi)+
  i, with   i independent and identically distributed with mean zero and
variance   2, the xi assumed    xed (non random). we estimate f using a
linear smoother (local regression, smoothing spline, etc.) with smoothing
parameter   . thus the vector of    tted values is given by   f = s  y. consider
the in-sample prediction error

pe(  ) = e

1
n

nxi=1

i       f  (xi))2
(y   

(6.34)

218

6. kernel smoothing methods

for predicting new responses at the n input values. show that the aver-
age squared residual on the training data, asr(  ), is a biased estimate
(optimistic) for pe(  ), while

c   = asr(  ) +

2  2
n

trace(s  )

(6.35)

is unbiased.

ex. 6.11 show that for the gaussian mixture model (6.32) the likelihood
is maximized at +   , and describe how.
ex. 6.12 write a computer program to perform a local linear discrimi-
nant analysis. at each query point x0, the training data receive weights
k  (x0, xi) from a weighting kernel, and the ingredients for the linear deci-
sion boundaries (see section 4.3) are computed by weighted averages. try
out your program on the zipcode data, and show the training and test er-
rors for a series of    ve pre-chosen values of   . the zipcode data are available
from the book website www-stat.stanford.edu/elemstatlearn.

7
model assessment and selection

this is page 219
printer: opaque this

7.1

introduction

the generalization performance of a learning method relates to its predic-
tion capability on independent test data. assessment of this performance
is extremely important in practice, since it guides the choice of learning
method or model, and gives us a measure of the quality of the ultimately
chosen model.

in this chapter we describe and illustrate the key methods for perfor-
mance assessment, and show how they are used to select models. we begin
the chapter with a discussion of the interplay between bias, variance and
model complexity.

7.2 bias, variance and model complexity

figure 7.1 illustrates the important issue in assessing the ability of a learn-
ing method to generalize. consider    rst the case of a quantitative or interval
scale response. we have a target variable y , a vector of inputs x, and a
prediction model   f (x) that has been estimated from a training set t .
the id168 for measuring errors between y and   f (x) is denoted by
l(y,   f (x)). typical choices are

l(y,   f (x)) =((y       f (x))2
|y       f (x)|

squared error
absolute error.

(7.1)

220

7. model assessment and selection

high bias
low variance

low bias
high variance

r
o
r
r

e
 
n
o
i
t
c
d
e
r
p

i

2
.
1

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
0

.

0

5

10

15

20

25

30

35

model complexity (df)

figure 7.1. behavior of test sample and training sample error as the model
complexity is varied. the light blue curves show the training error err, while the
light red curves show the conditional test error errt for 100 training sets of size
50 each, as the model complexity is increased. the solid curves show the expected
test error err and the expected training error e[err].

test error, also referred to as generalization error, is the prediction error
over an independent test sample

errt = e[l(y,   f (x))|t ]

(7.2)

where both x and y are drawn randomly from their joint distribution
(population). here the training set t is    xed, and test error refers to the
error for this speci   c training set. a related quantity is the expected pre-
diction error (or expected test error)

err = e[l(y,   f (x))] = e[errt ].

(7.3)

note that this expectation averages over everything that is random, includ-
ing the randomness in the training set that produced   f .

figure 7.1 shows the prediction error (light red curves) errt for 100
simulated training sets each of size 50. the lasso (section 3.4.2) was used
to produce the sequence of    ts. the solid red curve is the average, and
hence an estimate of err.

estimation of errt will be our goal, although we will see that err is
more amenable to statistical analysis, and most methods e   ectively esti-
mate the expected error. it does not seem possible to estimate conditional

7.2 bias, variance and model complexity

221

error e   ectively, given only the information in the same training set. some
discussion of this point is given in section 7.12.

training error is the average loss over the training sample

err =

1
n

nxi=1

l(yi,   f (xi)).

(7.4)

we would like to know the expected test error of our estimated model
  f . as the model becomes more and more complex, it uses the training
data more and is able to adapt to more complicated underlying structures.
hence there is a decrease in bias but an increase in variance. there is some
intermediate model complexity that gives minimum expected test error.

unfortunately training error is not a good estimate of the test error,
as seen in figure 7.1. training error consistently decreases with model
complexity, typically dropping to zero if we increase the model complexity
enough. however, a model with zero training error is over   t to the training
data and will typically generalize poorly.

the story is similar for a qualitative or categorical response g taking
one of k values in a set g, labeled for convenience as 1, 2, . . . , k. typically
we model the probabilities pk(x) = pr(g = k|x) (or some monotone
transformations fk(x)), and then   g(x) = arg maxk   pk(x). in some cases,
such as 1-nearest neighbor classi   cation (chapters 2 and 13) we produce
  g(x) directly. typical id168s are
l(g,   g(x)) = i(g 6=   g(x))
l(g,   p(x)) =    2

i(g = k) log   pk(x)

(0   1 loss),

(7.5)

=    2 log   pg(x)

(   2    log-likelihood).

(7.6)

the quantity    2    the log-likelihood is sometimes referred to as the
deviance.

again, test error here is errt = e[l(g,   g(x))|t ], the population mis-
classi   cation error of the classi   er trained on t , and err is the expected
misclassi   cation error.

training error is the sample analogue, for example,

kxk=1

err =    

2
n

nxi=1

log   pgi(xi),

(7.7)

the sample log-likelihood for the model.

the log-likelihood can be used as a loss-function for general response
densities, such as the poisson, gamma, exponential, log-normal and others.
if pr  (x)(y ) is the density of y , indexed by a parameter   (x) that depends
on the predictor x, then

l(y,   (x)) =    2    log pr  (x)(y ).

(7.8)

222

7. model assessment and selection

the       2    in the de   nition makes the log-likelihood loss for the gaussian
distribution match squared-error loss.
for ease of exposition, for the remainder of this chapter we will use y and
f (x) to represent all of the above situations, since we focus mainly on the
quantitative response (squared-error loss) setting. for the other situations,
the appropriate translations are obvious.

in this chapter we describe a number of methods for estimating the
expected test error for a model. typically our model will have a tuning
parameter or parameters    and so we can write our predictions as   f  (x).
the tuning parameter varies the complexity of our model, and we wish to
   nd the value of    that minimizes error, that is, produces the minimum of
the average test error curve in figure 7.1. having said this, for brevity we
will often suppress the dependence of   f (x) on   .

it is important to note that there are in fact two separate goals that we

might have in mind:

model selection: estimating the performance of di   erent models in order

to choose the best one.

model assessment: having chosen a    nal model, estimating its predic-

tion error (generalization error) on new data.

if we are in a data-rich situation, the best approach for both problems is
to randomly divide the dataset into three parts: a training set, a validation
set, and a test set. the training set is used to    t the models; the validation
set is used to estimate prediction error for model selection; the test set is
used for assessment of the generalization error of the    nal chosen model.
ideally, the test set should be kept in a    vault,    and be brought out only
at the end of the data analysis. suppose instead that we use the test-set
repeatedly, choosing the model with smallest test-set error. then the test
set error of the    nal chosen model will underestimate the true test error,
sometimes substantially.

it is di   cult to give a general rule on how to choose the number of
observations in each of the three parts, as this depends on the signal-to-
noise ratio in the data and the training sample size. a typical split might
be 50% for training, and 25% each for validation and testing:

train
train
train
train

validation
validation
validation
validation

test
test
test
test

the methods in this chapter are designed for situations where there is
insu   cient data to split it into three parts. again it is too di   cult to give
a general rule on how much training data is enough; among other things,
this depends on the signal-to-noise ratio of the underlying function, and
the complexity of the models being    t to the data.

7.3 the bias   variance decomposition

223

the methods of this chapter approximate the validation step either an-
alytically (aic, bic, mdl, srm) or by e   cient sample re-use (cross-
validation and the bootstrap). besides their use in model selection, we also
examine to what extent each method provides a reliable estimate of test
error of the    nal chosen model.

before jumping into these topics, we    rst explore in more detail the

nature of test error and the bias   variance tradeo   .

7.3 the bias   variance decomposition

as in chapter 2, if we assume that y = f (x) +    where e(  ) = 0 and
var(  ) =   2
   , we can derive an expression for the expected prediction error
of a regression    t   f (x) at an input point x = x0, using squared-error loss:

err(x0) = e[(y       f (x0))2|x = x0]

   + [e   f (x0)     f (x0)]2 + e[   f (x0)     e   f (x0)]2
   + bias2(   f (x0)) + var(   f (x0))

=   2
=   2
= irreducible error + bias2 + variance.

(7.9)

the    rst term is the variance of the target around its true mean f (x0), and
cannot be avoided no matter how well we estimate f (x0), unless   2
   = 0.
the second term is the squared bias, the amount by which the average of
our estimate di   ers from the true mean; the last term is the variance; the
expected squared deviation of   f (x0) around its mean. typically the more
complex we make the model   f , the lower the (squared) bias but the higher
the variance.

for the k-nearest-neighbor regression    t, these expressions have the sim-

ple form

err(x0) = e[(y       fk(x0))2|x = x0]

=   2

   +"f (x0)    

f (x(   ))#2

1
k

kx   =1

+

  2
  
k

.

(7.10)

here we assume for simplicity that training inputs xi are    xed, and the ran-
domness arises from the yi. the number of neighbors k is inversely related
to the model complexity. for small k, the estimate   fk(x) can potentially
adapt itself better to the underlying f (x). as we increase k, the bias   the
squared di   erence between f (x0) and the average of f (x) at the k-nearest
neighbors   will typically increase, while the variance decreases.

for a linear model    t   fp(x) = xt     , where the parameter vector    with

p components is    t by least squares, we have

err(x0) = e[(y       fp(x0))2|x = x0]

224

7. model assessment and selection

=   2

   + [f (x0)     e   fp(x0)]2 + ||h(x0)||2  2
   .

(7.11)

here h(x0) = x(xt x)   1x0, the n -vector of linear weights that produce
t (xt x)   1xt y, and hence var[   fp(x0)] = ||h(x0)||2  2
the    t   fp(x0) = x0
   .
while this variance changes with x0, its average (with x0 taken to be each
of the sample values xi) is (p/n )  2

   , and hence

1
n

nxi=1

err(xi) =   2

   +

1
n

nxi=1

[f (xi)     e   f (xi)]2 +

p
n

  2
   ,

(7.12)

the in-sample error. here model complexity is directly related to the num-
ber of parameters p.

the test error err(x0) for a ridge regression    t   f  (x0) is identical in
form to (7.11), except the linear weights in the variance term are di   erent:
h(x0) = x(xt x +   i)   1x0. the bias term will also be di   erent.

for a linear model family such as ridge regression, we can break down
the bias more    nely. let       denote the parameters of the best-   tting linear
approximation to f :

      = arg min

  

e(cid:0)f (x)     x t   (cid:1)2

.

(7.13)

here the expectation is taken with respect to the distribution of the input
variables x. then we can write the average squared bias as

ex0hf (x0)     e   f  (x0)i2

= ex0(cid:2)f (x0)     xt

0      (cid:3)2

+ ex0hxt

= ave[model bias]2 + ave[estimation bias]2

0           ext

0

      i2

(7.14)

the    rst term on the right-hand side is the average squared model bias, the
error between the best-   tting linear approximation and the true function.
the second term is the average squared estimation bias, the error between
the average estimate e(xt
0

    ) and the best-   tting linear approximation.

for linear models    t by ordinary least squares, the estimation bias is zero.
for restricted    ts, such as ridge regression, it is positive, and we trade it o   
with the bene   ts of a reduced variance. the model bias can only be reduced
by enlarging the class of linear models to a richer collection of models, by
including interactions and transformations of the variables in the model.

figure 7.2 shows the bias   variance tradeo    schematically. in the case
of linear models, the model space is the set of all linear predictions from
p inputs and the black dot labeled    closest    t    is xt      . the blue-shaded
region indicates the error      with which we see the truth in the training
sample.

also shown is the variance of the least squares    t, indicated by the large
yellow circle centered at the black dot labeled    closest    t in population,   

7.3 the bias   variance decomposition

225

closest fit in population

realization

closest fit

truth

model bias

estimation bias

estimation
variance

model
space

shrunken fit

restricted
model space

figure 7.2. schematic of the behavior of bias and variance. the model space
is the set of all possible predictions from the model, with the    closest    t    labeled
with a black dot. the model bias from the truth is shown, along with the variance,
indicated by the large yellow circle centered at the black dot labeled    closest    t
in population.    a shrunken or regularized    t is also shown, having additional
estimation bias, but smaller prediction error due to its decreased variance.

226

7. model assessment and selection

now if we were to    t a model with fewer predictors, or regularize the coef-
   cients by shrinking them toward zero (say), we would get the    shrunken
   t    shown in the    gure. this    t has an additional estimation bias, due to
the fact that it is not the closest    t in the model space. on the other hand,
it has smaller variance. if the decrease in variance exceeds the increase in
(squared) bias, then this is worthwhile.

7.3.1 example: bias   variance tradeo   

figure 7.3 shows the bias   variance tradeo    for two simulated examples.
there are 80 observations and 20 predictors, uniformly distributed in the
hypercube [0, 1]20. the situations are as follows:

left panels: y is 0 if x1     1/2 and 1 if x1 > 1/2, and we apply k-nearest

neighbors.

right panels: y is 1 ifp10

use best subset id75 of size p.

j=1 xj is greater than 5 and 0 otherwise, and we

the top row is regression with squared error loss; the bottom row is classi-
   cation with 0   1 loss. the    gures show the prediction error (red), squared
bias (green) and variance (blue), all computed for a large test sample.

in the regression problems, bias and variance add to produce the predic-
tion error curves, with minima at about k = 5 for k-nearest neighbors, and
p     10 for the linear model. for classi   cation loss (bottom    gures), some
interesting phenomena can be seen. the bias and variance curves are the
same as in the top    gures, and prediction error now refers to misclassi   -
cation rate. we see that prediction error is no longer the sum of squared
bias and variance. for the k-nearest neighbor classi   er, prediction error
decreases or stays the same as the number of neighbors is increased to 20,
despite the fact that the squared bias is rising. for the linear model classi-
   er the minimum occurs for p     10 as in regression, but the improvement
over the p = 1 model is more dramatic. we see that bias and variance seem
to interact in determining prediction error.

why does this happen? there is a simple explanation for the    rst phe-
nomenon. suppose at a given input point, the true id203 of class 1 is
0.9 while the expected value of our estimate is 0.6. then the squared bias   
(0.6    0.9)2   is considerable, but the prediction error is zero since we make
the correct decision. in other words, estimation errors that leave us on the
right side of the decision boundary don   t hurt. exercise 7.2 demonstrates
this phenomenon analytically, and also shows the interaction e   ect between
bias and variance.

the overall point is that the bias   variance tradeo    behaves di   erently
for 0   1 loss than it does for squared error loss. this in turn means that
the best choices of tuning parameters may di   er substantially in the two

7.3 the bias   variance decomposition

227

k   nn     regression

linear model     regression

4
.
0

3
.
0

2
.
0

1
.
0

0

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

50

40

30

20

10

0

number of neighbors k

k   nn     classification

4
.
0

3
.
0

2
.
0

1
.
0

0

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

5

10

15

20

subset size p

linear model     classification

50

40

30

20

10

0

5

10

15

20

number of neighbors k

subset size p

figure 7.3. expected prediction error (orange), squared bias (green) and vari-
ance (blue) for a simulated example. the top row is regression with squared error
loss; the bottom row is classi   cation with 0   1 loss. the models are k-nearest
neighbors (left) and best subset regression of size p (right). the variance and bias
curves are the same in regression and classi   cation, but the prediction error curve
is di   erent.

228

7. model assessment and selection

settings. one should base the choice of tuning parameter on an estimate of
prediction error, as described in the following sections.

7.4 optimism of the training error rate

discussions of error rate estimation can be confusing, because we have
to make clear which quantities are    xed and which are random1. before
we continue, we need a few de   nitions, elaborating on the material of sec-
tion 7.2. given a training set t = {(x1, y1), (x2, y2), . . . (xn , yn )} the gen-
eralization error of a model   f is

errt = ex 0,y 0[l(y 0,   f (x 0))|t ];

(7.15)
note that the training set t is    xed in expression (7.15). the point (x 0, y 0)
is a new test data point, drawn from f , the joint distribution of the data.
averaging over training sets t yields the expected error

err = et ex 0,y 0 [l(y 0,   f (x 0))|t ],

(7.16)

which is more amenable to statistical analysis. as mentioned earlier, it
turns out that most methods e   ectively estimate the expected error rather
than et ; see section 7.12 for more on this point.

now typically, the training error

err =

1
n

nxi=1

l(yi,   f (xi))

(7.17)

will be less than the true error errt , because the same data is being used
to    t the method and assess its error (see exercise 2.9). a    tting method
typically adapts to the training data, and hence the apparent or training
error err will be an overly optimistic estimate of the generalization error
errt .

part of the discrepancy is due to where the evaluation points occur. the
quantity errt can be thought of as extra-sample error, since the test input
vectors don   t need to coincide with the training input vectors. the nature
of the optimism in err is easiest to understand when we focus instead on
the in-sample error

errin =

1
n

nxi=1

ey 0 [l(y 0

i ,   f (xi))|t ]

(7.18)

the y 0 notation indicates that we observe n new response values at
each of the training points xi, i = 1, 2, . . . , n . we de   ne the optimism as

1indeed, in the    rst edition of our book, this section wasn   t su   ciently clear.

7.4 optimism of the training error rate

229

the di   erence between errin and the training error err:

op     errin     err.

(7.19)

this is typically positive since err is usually biased downward as an estimate
of prediction error. finally, the average optimism is the expectation of the
optimism over training sets

       ey(op).

(7.20)

here the predictors in the training set are    xed, and the expectation is
over the training set outcome values; hence we have used the notation ey
instead of et . we can usually estimate only the expected error    rather
than op, in the same way that we can estimate the expected error err
rather than the conditional error errt .

for squared error, 0   1, and other id168s, one can show quite

generally that

   =

2
n

nxi=1

cov(  yi, yi),

(7.21)

where cov indicates covariance. thus the amount by which err underesti-
mates the true error depends on how strongly yi a   ects its own prediction.
the harder we    t the data, the greater cov(  yi, yi) will be, thereby increas-
ing the optimism. exercise 7.4 proves this result for squared error loss where
  yi is the    tted value from the regression. for 0   1 loss,   yi     {0, 1} is the
classi   cation at xi, and for id178 loss,   yi     [0, 1] is the    tted id203
of class 1 at xi.

in summary, we have the important relation

ey(errin) = ey(err) +

2
n

nxi=1

cov(  yi, yi).

(7.22)

this expression simpli   es if   yi is obtained by a linear    t with d inputs

or basis functions. for example,

cov(  yi, yi) = d  2
  

nxi=1

for the additive error model y = f (x) +   , and so

ey(errin) = ey(err) + 2   

d
n

  2
   .

(7.23)

(7.24)

expression (7.23) is the basis for the de   nition of the e   ective number of
parameters discussed in section 7.6 the optimism increases linearly with

230

7. model assessment and selection

the number d of inputs or basis functions we use, but decreases as the
training sample size increases. versions of (7.24) hold approximately for
other error models, such as binary data and id178 loss.

an obvious way to estimate prediction error is to estimate the optimism
and then add it to the training error err. the methods described in the
next section   cp, aic, bic and others   work in this way, for a special
class of estimates that are linear in their parameters.

in contrast, cross-validation and bootstrap methods, described later in
the chapter, are direct estimates of the extra-sample error err. these gen-
eral tools can be used with any id168, and with nonlinear, adaptive
   tting techniques.

in-sample error is not usually of direct interest since future values of the
features are not likely to coincide with their training set values. but for
comparison between models, in-sample error is convenient and often leads
to e   ective model selection. the reason is that the relative (rather than
absolute) size of the error is what matters.

7.5 estimates of in-sample prediction error

the general form of the in-sample estimates is

where      is an estimate of the average optimism.

derrin = err +     ,

(7.25)

using expression (7.24), applicable when d parameters are    t under

squared error loss, leads to a version of the so-called cp statistic,

cp = err + 2   

d
n

      

2.

(7.26)

2 is an estimate of the noise variance, obtained from the mean-
here       
squared error of a low-bias model. using this criterion we adjust the training
error by a factor proportional to the number of basis functions used.

the akaike information criterion is a similar but more generally appli-
cable estimate of errin when a log-likelihood id168 is used. it relies
on a relationship similar to (7.24) that holds asymptotically as n        :

   2    e[log pr    (y )]        

2
n    e[loglik] + 2   

d
n

.

(7.27)

here pr  (y ) is a family of densities for y (containing the    true    density),
     is the maximum-likelihood estimate of   , and    loglik    is the maximized
log-likelihood:

loglik =

nxi=1

log pr    (yi).

(7.28)

7.5 estimates of in-sample prediction error

231

for example, for the id28 model, using the binomial log-
likelihood, we have

aic =    

2
n    loglik + 2   

d
n

.

(7.29)

2 assumed known), the aic
for the gaussian model (with variance   2
statistic is equivalent to cp, and so we refer to them collectively as aic.

   =       

to use aic for model selection, we simply choose the model giving small-
est aic over the set of models considered. for nonlinear and other complex
models, we need to replace d by some measure of model complexity. we
discuss this in section 7.6.

given a set of models f  (x) indexed by a tuning parameter   , denote
by err(  ) and d(  ) the training error and number of parameters for each
model. then for this set of models we de   ne

aic(  ) = err(  ) + 2   

d(  )

n

      

2.

(7.30)

the function aic(  ) provides an estimate of the test error curve, and we
   nd the tuning parameter      that minimizes it. our    nal chosen model
is f     (x). note that if the basis functions are chosen adaptively, (7.23) no
longer holds. for example, if we have a total of p inputs, and we choose
the best-   tting linear model with d < p inputs, the optimism will exceed
(2d/n )  2
   . put another way, by choosing the best-   tting model with d
inputs, the e   ective number of parameters    t is more than d.

figure 7.4 shows aic in action for the phoneme recognition example
of section 5.2.3 on page 148. the input vector is the log-periodogram of
the spoken vowel, quantized to 256 uniformly spaced frequencies. a lin-
ear id28 model is used to predict the phoneme class, with
m=1 hm(f )  m, an expansion in m spline ba-
sis functions. for any given m , a basis of natural cubic splines is used
for the hm, with knots chosen uniformly over the range of frequencies (so
d(  ) = d(m ) = m ). using aic to select the number of basis functions will
approximately minimize err(m ) for both id178 and 0   1 loss.

coe   cient function   (f ) =pm

the simple formula

(2/n )

nxi=1

cov(  yi, yi) = (2d/n )  2
  

holds exactly for linear models with additive errors and squared error loss,
and approximately for linear models and log-likelihoods. in particular, the
formula does not hold in general for 0   1 loss (efron, 1986), although many
authors nevertheless use it in that context (right panel of figure 7.4).

232

7. model assessment and selection

log-likelihood loss

0-1 loss

d
o
o
h

i
l

e
k

i
l
-
g
o
l

5

.

2

0

.

2

5

.

1

0

.

1

5

.

0

train
test
aic

o
o
o

o
o
o

o
o
o

o
o
o

o
o
o

o
o
o

o
o

o

o

o

o

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

o
o

o

5
3

.

0

0
3

.

0

5
2

.

0

0
2

.

0

5
1

.

0

0
1

.

0

o
o
o

o
o
o

o

o
o

o
o
o

o

o
o

o

o

o

o

o

o

2

4

8

16

32

64 128

2

4

8

16

32

64 128

number of basis functions

number of basis functions

figure 7.4. aic used for model selection for the phoneme recogni-
tion example of section 5.2.3. the id28 coe   cient
function
  (f ) = pm
m=1 hm(f )  m is modeled as an expansion in m spline basis functions.
in the left panel we see the aic statistic used to estimate errin using log-likeli-
hood loss. included is an estimate of err based on an independent test sample. it
does well except for the extremely over-parametrized case (m = 256 parameters
for n = 1000 observations). in the right panel the same is done for 0   1 loss.
although the aic formula does not strictly apply here, it does a reasonable job in
this case.

7.6 the e   ective number of parameters

the concept of    number of parameters    can be generalized, especially to
models where id173 is used in the    tting. suppose we stack the
outcomes y1, y2, . . . , yn into a vector y, and similarly for the predictions
  y. then a linear    tting method is one for which we can write

  y = sy,

(7.31)

where s is an n    n matrix depending on the input vectors xi but not on
the yi. linear    tting methods include id75 on the original fea-
tures or on a derived basis set, and smoothing methods that use quadratic
shrinkage, such as ridge regression and cubic smoothing splines. then the
e   ective number of parameters is de   ned as

df(s) = trace(s),

(7.32)

the sum of the diagonal elements of s (also known as the e   ective degrees-
of-freedom). note that if s is an orthogonal-projection matrix onto a basis

7.7 the bayesian approach and bic

233

set spanned by m features, then trace(s) = m . it turns out that trace(s) is
exactly the correct quantity to replace d as the number of parameters in the
cp statistic (7.26). if y arises from an additive-error model y = f (x) +   
i=1 cov(  yi, yi) = trace(s)  2
with var(  ) =   2
   ,
which motivates the more general de   nition

   , then one can show thatpn

df(  y) = pn

i=1 cov(  yi, yi)

  2
  

(7.33)

(exercises 7.4 and 7.5). section 5.4.1 on page 153 gives some more intuition
for the de   nition df = trace(s) in the context of smoothing splines.

for models like neural networks, in which we minimize an error function
m, the e   ective

r(w) with weight decay penalty (id173)   pm w2

number of parameters has the form

df(  ) =

mxm=1

  m

  m +   

,

(7.34)

where the   m are the eigenvalues of the hessian matrix    2r(w)/   w   wt .
expression (7.34) follows from (7.32) if we make a quadratic approximation
to the error function at the solution (bishop, 1995).

7.7 the bayesian approach and bic

the bayesian information criterion (bic), like aic, is applicable in settings
where the    tting is carried out by maximization of a log-likelihood. the
generic form of bic is

bic =    2    loglik + (log n )    d.

(7.35)

the bic statistic (times 1/2) is also known as the schwarz criterion (schwarz,
1978).

under the gaussian model, assuming the variance   2

error loss. hence we can write

equals (up to a constant)pi(yi      f (xi))2/  2
  herr + (log n )   

bic =

n
  2

   is known,    2  loglik
   for squared

   , which is n  err/  2

d
n

  2

  i.

(7.36)

therefore bic is proportional to aic (cp), with the factor 2 replaced
by log n . assuming n > e2     7.4, bic tends to penalize complex models
more heavily, giving preference to simpler models in selection. as with aic,
  2
   is typically estimated by the mean squared error of a low-bias model.
for classi   cation problems, use of the multinomial log-likelihood leads to a
similar relationship with the aic, using cross-id178 as the error measure.

234

7. model assessment and selection

note however that the misclassi   cation error measure does not arise in the
bic context, since it does not correspond to the log-likelihood of the data
under any id203 model.

despite its similarity with aic, bic is motivated in quite a di   erent
way. it arises in the bayesian approach to model selection, which we now
describe.

suppose we have a set of candidate models mm, m = 1, . . . , m and
corresponding model parameters   m, and we wish to choose a best model
from among them. assuming we have a prior distribution pr(  m|mm) for
the parameters of each model mm, the posterior id203 of a given
model is

pr(mm|z)     pr(mm)    pr(z|mm)

    pr(mm)   z pr(z|  m,mm)pr(  m|mm)d  m,

(7.37)

where z represents the training data {xi, yi}n
mm and m   , we form the posterior odds
pr(mm)
pr(m   )   

pr(mm|z)
pr(m   |z)

=

pr(z|mm)
pr(z|m   )

1 . to compare two models

.

(7.38)

if the odds are greater than one we choose model m, otherwise we choose
model    . the rightmost quantity

bf(z) =

pr(z|mm)
pr(z|m   )

(7.39)

is called the bayes factor, the contribution of the data toward the posterior
odds.

typically we assume that the prior over models is uniform, so that
pr(mm) is constant. we need some way of approximating pr(z|mm).
a so-called laplace approximation to the integral followed by some other
simpli   cations (ripley, 1996, page 64) to (7.37) gives

log pr(z|mm) = log pr(z|    m,mm)    

dm
2    log n + o(1).

(7.40)

here     m is a maximum likelihood estimate and dm is the number of free
parameters in model mm. if we de   ne our id168 to be

   2 log pr(z|    m,mm),

this is equivalent to the bic criterion of equation (7.35).

therefore, choosing the model with minimum bic is equivalent to choos-
ing the model with largest (approximate) posterior id203. but this
framework gives us more. if we compute the bic criterion for a set of m ,

7.8 minimum description length

235

pm

models, giving bicm, m = 1, 2, . . . , m , then we can estimate the posterior
id203 of each model mm as
e    1
   =1 e    1

(7.41)

2   bicm

2   bic   

.

thus we can estimate not only the best model, but also assess the relative
merits of the models considered.

for model selection purposes, there is no clear choice between aic and
bic. bic is asymptotically consistent as a selection criterion. what this
means is that given a family of models, including the true model, the prob-
ability that bic will select the correct model approaches one as the sample
size n        . this is not the case for aic, which tends to choose models
which are too complex as n        . on the other hand, for    nite samples,
bic often chooses models that are too simple, because of its heavy penalty
on complexity.

7.8 minimum description length

the minimum description length (mdl) approach gives a selection cri-
terion formally identical to the bic approach, but is motivated from an
optimal coding viewpoint. we    rst review the theory of coding for data
compression, and then apply it to model selection.

we think of our datum z as a message that we want to encode and
send to someone else (the    receiver   ). we think of our model as a way of
encoding the datum, and will choose the most parsimonious model, that is
the shortest code, for the transmission.

suppose    rst that the possible messages we might want to transmit are
z1, z2, . . . , zm. our code uses a    nite alphabet of length a: for example, we
might use a binary code {0, 1} of length a = 2. here is an example with
four possible messages and a binary coding:

message
code

z1
0

z2
10

z3
110

z4
111

(7.42)

this code is known as an instantaneous pre   x code: no code is the pre-
   x of any other, and the receiver (who knows all of the possible codes),
knows exactly when the message has been completely sent. we restrict our
discussion to such instantaneous pre   x codes.

one could use the coding in (7.42) or we could permute the codes, for
example use codes 110, 10, 111, 0 for z1, z2, z3, z4. how do we decide which
to use? it depends on how often we will be sending each of the messages.
if, for example, we will be sending z1 most often, it makes sense to use the
shortest code 0 for z1. using this kind of strategy   shorter codes for more
frequent messages   the average message length will be shorter.

236

7. model assessment and selection

in general, if messages are sent with probabilities pr(zi), i = 1, 2, . . . , 4,
a famous theorem due to shannon says we should use code lengths li =
    log2 pr(zi) and the average message length satis   es

e(length)        x pr(zi) log2 (pr(zi)).

(7.43)

the right-hand side above is also called the id178 of the distribution
pr(zi). the inequality is an equality when the probabilities satisfy pi =
a   li . in our example, if pr(zi) = 1/2, 1/4, 1/8, 1/8, respectively, then the
coding shown in (7.42) is optimal and achieves the id178 lower bound.

in general the lower bound cannot be achieved, but procedures like the
hu   man coding scheme can get close to the bound. note that with an

in   nite set of messages, the id178 is replaced by    r pr(z) log2 pr(z)dz.

from this result we glean the following:

to transmit a random variable z having id203 density func-
tion pr(z), we require about     log2 pr(z) bits of information.

we henceforth change notation from log2 pr(z) to log pr(z) = loge pr(z);
this is for convenience, and just introduces an unimportant multiplicative
constant.

now we apply this result to the problem of model selection. we have
a model m with parameters   , and data z = (x, y) consisting of both
inputs and outputs. let the (conditional) id203 of the outputs under
the model be pr(y|  , m, x), assume the receiver knows all of the inputs,
and we wish to transmit the outputs. then the message length required to
transmit the outputs is

length =     log pr(y|  , m, x)     log pr(  |m ),

(7.44)

the log-id203 of the target values given the inputs. the second term
is the average code length for transmitting the model parameters   , while
the    rst term is the average code length for transmitting the discrepancy
between the model and actual target values. for example suppose we have
a single target y with y     n (  ,   2), parameter        n (0, 1) and no input
(for simplicity). then the message length is

length = constant + log    +

(y       )2

2  2 +

  2
2

.

(7.45)

note that the smaller    is, the shorter on average is the message length,
since y is more concentrated around   .

the mdl principle says that we should choose the model that mini-
mizes (7.44). we recognize (7.44) as the (negative) log-posterior distribu-
tion, and hence minimizing description length is equivalent to maximizing
posterior id203. hence the bic criterion, derived as approximation to
log-posterior id203, can also be viewed as a device for (approximate)
model choice by minimum description length.

7.9 vapnik   chervonenkis dimension

237

)
x

  
0
5
(
n
i
s

0
1

.

0

.

0

0

.

1
-

0.0

0.2

0.4

0.6

0.8

1.0

x

figure 7.5. the solid curve is the function sin(50x) for x     [0, 1]. the green
(solid) and blue (hollow) points illustrate how the associated indicator function
i(sin(  x) > 0) can shatter (separate) an arbitrarily large number of points by
choosing an appropriately high frequency   .

note that we have ignored the precision with which a random variable
z is coded. with a    nite code length we cannot code a continuous variable
exactly. however, if we code z within a tolerance   z, the message length
needed is the log of the id203 in the interval [z, z+  z] which is well ap-
proximated by   zpr(z) if   z is small. since log   zpr(z) = log   z + log pr(z),
this means we can just ignore the constant log   z and use log pr(z) as our
measure of message length, as we did above.

the preceding view of mdl for model selection says that we should
choose the model with highest posterior id203. however, many bayes-
ians would instead do id136 by sampling from the posterior distribution.

7.9 vapnik   chervonenkis dimension

a di   culty in using estimates of in-sample error is the need to specify the
number of parameters (or the complexity) d used in the    t. although the
e   ective number of parameters introduced in section 7.6 is useful for some
nonlinear models, it is not fully general. the vapnik   chervonenkis (vc)
theory provides such a general measure of complexity, and gives associated
bounds on the optimism. here we give a brief review of this theory.

suppose we have a class of functions {f (x,   )} indexed by a parameter
vector   , with x     irp. assume for now that f is an indicator function,
that is, takes the values 0 or 1. if    = (  0,   1) and f is the linear indi-
cator function i(  0 +   t
1 x > 0), then it seems reasonable to say that the
complexity of the class f is the number of parameters p + 1. but what
about f (x,   ) = i(sin       x) where    is any real number and x     ir? the
function sin(50    x) is shown in figure 7.5. this is a very wiggly function
that gets even rougher as the frequency    increases, but it has only one
parameter: despite this, it doesn   t seem reasonable to conclude that it has
less complexity than the linear indicator function i(  0 +   1x) in p = 1
dimension.

238

7. model assessment and selection

figure 7.6. the    rst three panels show that the class of lines in the plane
can shatter three points. the last panel shows that this class cannot shatter four
points, as no line will put the hollow points on one side and the solid points on
the other. hence the vc dimension of the class of straight lines in the plane is
three. note that a class of nonlinear curves could shatter four points, and hence
has vc dimension greater than three.

the vapnik   chervonenkis dimension is a way of measuring the com-
plexity of a class of functions by assessing how wiggly its members can
be.

the vc dimension of the class {f (x,   )} is de   ned to be the
largest number of points (in some con   guration) that can be
shattered by members of {f (x,   )}.

a set of points is said to be shattered by a class of functions if, no matter
how we assign a binary label to each point, a member of the class can
perfectly separate them.

figure 7.6 shows that the vc dimension of linear indicator functions
in the plane is 3 but not 4, since no four points can be shattered by a
set of lines. in general, a linear indicator function in p dimensions has vc
dimension p + 1, which is also the number of free parameters. on the other
hand, it can be shown that the family sin(  x) has in   nite vc dimension,
as figure 7.5 suggests. by appropriate choice of   , any set of points can be
shattered by this class (exercise 7.8).

so far we have discussed the vc dimension only of indicator functions,
but this can be extended to real-valued functions. the vc dimension of a
class of real-valued functions {g(x,   )} is de   ned to be the vc dimension
of the indicator class {i(g(x,   )        > 0)}, where    takes values over the
range of g.
one can use the vc dimension in constructing an estimate of (extra-
sample) prediction error; di   erent types of results are available. using the
concept of vc dimension, one can prove results about the optimism of the
training error when using a class of functions. an example of such a result is
the following. if we    t n training points using a class of functions {f (x,   )}
having vc dimension h, then with id203 at least 1        over training

7.9 vapnik   chervonenkis dimension

239

sets:

  

2(cid:16)1 +r1 +

4    err

   (cid:17) (binary classi   cation)

(regression)

(7.46)

errt     err +
errt    

err

(1     c     )+

h[log (a2n/h) + 1]     log (  /4)

,

where    = a1
n
and 0 < a1     4, 0 < a2     2

these bounds hold simultaneously for all members f (x,   ), and are taken
from cherkassky and mulier (2007, pages 116   118). they recommend the
value c = 1. for regression they suggest a1 = a2 = 1, and for classi   cation
they make no recommendation, with a1 = 4 and a2 = 2 corresponding
to worst-case scenarios. they also give an alternative practical bound for
regression

errt     err 1    r          log    +

log n

2n !   1

+

(7.47)

with    = h
n , which is free of tuning constants. the bounds suggest that the
optimism increases with h and decreases with n in qualitative agreement
with the aic correction d/n given in (7.24). however, the results in (7.46)
are stronger: rather than giving the expected optimism for each    xed func-
tion f (x,   ), they give probabilistic upper bounds for all functions f (x,   ),
and hence allow for searching over the class.

vapnik   s structural risk minimization (srm) approach    ts a nested se-
quence of models of increasing vc dimensions h1 < h2 <        , and then
chooses the model with the smallest value of the upper bound.
we note that upper bounds like the ones in (7.46) are often very loose,
but that doesn   t rule them out as good criteria for model selection, where
the relative (not absolute) size of the test error is important. the main
drawback of this approach is the di   culty in calculating the vc dimension
of a class of functions. often only a crude upper bound for vc dimension
is obtainable, and this may not be adequate. an example in which the
structural risk minimization program can be successfully carried out is the
support vector classi   er, discussed in section 12.2.

7.9.1 example (continued)

figure 7.7 shows the results when aic, bic and srm are used to select
the model size for the examples of figure 7.3. for the examples labeled knn,
the model index    refers to neighborhood size, while for those labeled reg   
refers to subset size. using each selection method (e.g., aic) we estimated
the best model      and found its true prediction error errt (    ) on a test
set. for the same training set we computed the prediction error of the best

240

7. model assessment and selection

aic

t
s
e
b

 

 
r
e
v
o
e
s
a
e
r
c
n

i
 

%

t
s
e
b

 

 
r
e
v
o
e
s
a
e
r
c
n

i
 

%

t
s
e
b

 

 
r
e
v
o
e
s
a
e
r
c
n

i
 

%

0
0
1

0
8

0
6

0
4

0
2

0

0
0
1

0
8

0
6

0
4

0
2

0

0
0
1

0
8

0
6

0
4

0
2

0

reg/knn

reg/linear

class/knn

class/linear

bic

reg/knn

reg/linear

class/knn

class/linear

srm

reg/knn

reg/linear

class/knn

class/linear

show the distribution of

figure 7.7. boxplots
error
100    [errt (     )     min   errt (  )]/[max   errt (  )     min   errt (  )] over the four
scenarios of figure 7.3. this is the error in using the chosen model relative to
the best model. there are 100 training sets each of size 80 represented in each
boxplot, with the errors computed on test sets of size 10, 000.

relative

the

and worst possible model choices: min   errt (  ) and max   errt (  ). the
boxplots show the distribution of the quantity

7.10 cross-validation

241

100   

errt (    )     min   errt (  )

max   errt (  )     min   errt (  )

,

which represents the error in using the chosen model relative to the best
model. for id75 the model complexity was measured by the
number of features; as mentioned in section 7.5, this underestimates the
df, since it does not charge for the search for the best model of that size.
this was also used for the vc dimension of the linear classi   er. for k-
nearest neighbors, we used the quantity n/k. under an additive-error re-
gression model, this can be justi   ed as the exact e   ective degrees of free-
dom (exercise 7.6); we do not know if it corresponds to the vc dimen-
sion. we used a1 = a2 = 1 for the constants in (7.46); the results for srm
changed with di   erent constants, and this choice gave the most favorable re-
sults. we repeated the srm selection using the alternative practical bound
(7.47), and got almost identical results. for misclassi   cation error we used
2 = [n/(n     d)]    err(  ) for the least restrictive model (k = 5 for knn,
      
since k = 1 results in zero training error). the aic criterion seems to work
well in all four scenarios, despite the lack of theoretical support with 0   1
loss. bic does nearly as well, while the performance of srm is mixed.

7.10 cross-validation

probably the simplest and most widely used method for estimating predic-
tion error is cross-validation. this method directly estimates the expected
extra-sample error err = e[l(y,   f (x))], the average generalization error
when the method   f (x) is applied to an independent test sample from the
joint distribution of x and y . as mentioned earlier, we might hope that
cross-validation estimates the conditional error, with the training set t
held    xed. but as we will see in section 7.12, cross-validation typically
estimates well only the expected prediction error.

7.10.1 k-fold cross-validation

ideally, if we had enough data, we would set aside a validation set and use
it to assess the performance of our prediction model. since data are often
scarce, this is usually not possible. to    nesse the problem, k-fold cross-
validation uses part of the available data to    t the model, and a di   erent
part to test it. we split the data into k roughly equal-sized parts; for
example, when k = 5, the scenario looks like this:

242

7. model assessment and selection

1

2

3

4

5

train

train

validation

train

train

for the kth part (third above), we    t the model to the other k     1 parts
of the data, and calculate the prediction error of the    tted model when
predicting the kth part of the data. we do this for k = 1, 2, . . . , k and
combine the k estimates of prediction error.

here are more details. let    : {1, . . . , n} 7    {1, . . . , k} be an indexing
function that indicates the partition to which observation i is allocated by
the randomization. denote by   f    k(x) the    tted function, computed with
the kth part of the data removed. then the cross-validation estimate of
prediction error is

cv(   f ) =

1
n

nxi=1

l(yi,   f      (i)(xi)).

(7.48)

typical choices of k are 5 or 10 (see below). the case k = n is known
as leave-one-out cross-validation. in this case   (i) = i, and for the ith
observation the    t is computed using all the data except the ith.

given a set of models f (x,   ) indexed by a tuning parameter   , denote
by   f    k(x,   ) the   th model    t with the kth part of the data removed. then
for this set of models we de   ne

cv(   f ,   ) =

1
n

nxi=1

l(yi,   f      (i)(xi,   )).

(7.49)

the function cv(   f ,   ) provides an estimate of the test error curve, and we
   nd the tuning parameter      that minimizes it. our    nal chosen model is
f (x,     ), which we then    t to all the data.

it is interesting to wonder about what quantity k-fold cross-validation
estimates. with k = 5 or 10, we might guess that it estimates the ex-
pected error err, since the training sets in each fold are quite di   erent
from the original training set. on the other hand, if k = n we might
guess that cross-validation estimates the conditional error errt . it turns
out that cross-validation only estimates e   ectively the average error err,
as discussed in section 7.12.

what value should we choose for k? with k = n , the cross-validation
estimator is approximately unbiased for the true (expected) prediction er-
ror, but can have high variance because the n    training sets    are so similar
to one another. the computational burden is also considerable, requiring
n applications of the learning method. in certain special problems, this
computation can be done quickly   see exercises 7.3 and 5.13.

7.10 cross-validation

243

8

.

0

6

.

0

r
r

e
-
1

4

.

0

2

.

0

0

.

0

0

50

100

150

200

size of training set

figure 7.8. hypothetical learning curve for a classi   er on a given task: a
plot of 1     err versus the size of the training set n . with a dataset of 200
observations, 5-fold cross-validation would use training sets of size 160, which
would behave much like the full set. however, with a dataset of 50 observations
   vefold cross-validation would use training sets of size 40, and this would result
in a considerable overestimate of prediction error.

on the other hand, with k = 5 say, cross-validation has lower variance.
but bias could be a problem, depending on how the performance of the
learning method varies with the size of the training set. figure 7.8 shows
a hypothetical    learning curve    for a classi   er on a given task, a plot of
1     err versus the size of the training set n . the performance of the
classi   er improves as the training set size increases to 100 observations;
increasing the number further to 200 brings only a small bene   t. if our
training set had 200 observations,    vefold cross-validation would estimate
the performance of our classi   er over training sets of size 160, which from
figure 7.8 is virtually the same as the performance for training set size
200. thus cross-validation would not su   er from much bias. however if the
training set had 50 observations,    vefold cross-validation would estimate
the performance of our classi   er over training sets of size 40, and from the
   gure that would be an underestimate of 1     err. hence as an estimate of
err, cross-validation would be biased upward.
to summarize, if the learning curve has a considerable slope at the given
training set size,    ve- or tenfold cross-validation will overestimate the true
prediction error. whether this bias is a drawback in practice depends on
the objective. on the other hand, leave-one-out cross-validation has low
bias but can have high variance. overall,    ve- or tenfold cross-validation
are recommended as a good compromise: see breiman and spector (1992)
and kohavi (1995).

figure 7.9 shows the prediction error and tenfold cross-validation curve
estimated from a single training set, from the scenario in the bottom right
panel of figure 7.3. this is a two-class classi   cation problem, using a lin-

244

7. model assessment and selection

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

   

   

   

   

       
   

   

6
0

.

5

.

0

4
0

.

3
0

.

2

.

0

1

.

0

0
0

.

   
   

       
   
   

   
   

5

                   
   
                           
                                           

10

15

20

subset size p

figure 7.9. prediction error (orange) and tenfold cross-validation curve
(blue) estimated from a single training set, from the scenario in the bottom right
panel of figure 7.3.

ear model with best subsets regression of subset size p. standard error bars
are shown, which are the standard errors of the individual misclassi   cation
error rates for each of the ten parts. both curves have minima at p = 10,
although the cv curve is rather    at beyond 10. often a    one-standard
error    rule is used with cross-validation, in which we choose the most par-
simonious model whose error is no more than one standard error above
the error of the best model. here it looks like a model with about p = 9
predictors would be chosen, while the true model uses p = 10.

generalized cross-validation provides a convenient approximation to leave-
one out cross-validation, for linear    tting under squared-error loss. as de-
   ned in section 7.6, a linear    tting method is one for which we can write

now for many linear    tting methods,

  y = sy.

1
n

nxi=1

[yi       f    i(xi)]2 =

1
n

1     sii i2
nxi=1h yi       f (xi)

,

(7.50)

(7.51)

where sii is the ith diagonal element of s (see exercise 7.3). the gcv
approximation is

gcv(   f ) =

1
n

nxi=1"

yi       f (xi)

1     trace(s)/n#2

.

(7.52)

7.10 cross-validation

245

the quantity trace(s) is the e   ective number of parameters, as de   ned in
section 7.6.

gcv can have a computational advantage in some settings, where the
trace of s can be computed more easily than the individual elements sii.
in smoothing problems, gcv can also alleviate the tendency of cross-
validation to undersmooth. the similarity between gcv and aic can be
seen from the approximation 1/(1     x)2     1 + 2x (exercise 7.7).

7.10.2 the wrong and right way to do cross-validation

consider a classi   cation problem with a large number of predictors, as may
arise, for example, in genomic or proteomic applications. a typical strategy
for analysis might be as follows:

1. screen the predictors:    nd a subset of    good    predictors that show

fairly strong (univariate) correlation with the class labels

2. using just this subset of predictors, build a multivariate classi   er.

3. use cross-validation to estimate the unknown tuning parameters and

to estimate the prediction error of the    nal model.

is this a correct application of cross-validation? consider a scenario with
n = 50 samples in two equal-sized classes, and p = 5000 quantitative
predictors (standard gaussian) that are independent of the class labels.
the true (test) error rate of any classi   er is 50%. we carried out the above
recipe, choosing in step (1) the 100 predictors having highest correlation
with the class labels, and then using a 1-nearest neighbor classi   er, based
on just these 100 predictors, in step (2). over 50 simulations from this
setting, the average cv error rate was 3%. this is far lower than the true
error rate of 50%.

what has happened? the problem is that the predictors have an unfair
advantage, as they were chosen in step (1) on the basis of all of the samples.
leaving samples out after the variables have been selected does not cor-
rectly mimic the application of the classi   er to a completely independent
test set, since these predictors    have already seen    the left out samples.

figure 7.10 (top panel) illustrates the problem. we selected the 100 pre-
dictors having largest correlation with the class labels over all 50 samples.
then we chose a random set of 10 samples, as we would do in    ve-fold cross-
validation, and computed the correlations of the pre-selected 100 predictors
with the class labels over just these 10 samples (top panel). we see that
the correlations average about 0.28, rather than 0, as one might expect.
here is the correct way to carry out cross-validation in this example:

1. divide the samples into k cross-validation folds (groups) at random.

2. for each fold k = 1, 2, . . . , k

246

7. model assessment and selection

wrong way

y
c
n
e
u
q
e
r
f

0
3

0
2

0
1

0

y
c
n
e
u
q
e
r
f

0
3

0
2

0
1

0

   1.0

   0.5

0.0

0.5

1.0

correlations of selected predictors with outcome

right way

   1.0

   0.5

0.0

0.5

1.0

correlations of selected predictors with outcome

figure 7.10. cross-validation the wrong and right way: histograms shows the
correlation of class labels, in 10 randomly chosen samples, with the 100 predic-
tors chosen using the incorrect (upper red) and correct (lower green) versions of
cross-validation.

(a) find a subset of    good    predictors that show fairly strong (uni-
variate) correlation with the class labels, using all of the samples
except those in fold k.

(b) using just this subset of predictors, build a multivariate classi-

   er, using all of the samples except those in fold k.

(c) use the classi   er to predict the class labels for the samples in

fold k.

the error estimates from step 2(c) are then accumulated over all k folds, to
produce the cross-validation estimate of prediction error. the lower panel
of figure 7.10 shows the correlations of class labels with the 100 predictors
chosen in step 2(a) of the correct procedure, over the samples in a typical
fold k. we see that they average about zero, as they should.

in general, with a multistep modeling procedure, cross-validation must
be applied to the entire sequence of modeling steps. in particular, samples
must be    left out    before any selection or    ltering steps are applied. there
is one quali   cation: initial unsupervised screening steps can be done be-
fore samples are left out. for example, we could select the 1000 predictors

7.10 cross-validation

247

with highest variance across all 50 samples, before starting cross-validation.
since this    ltering does not involve the class labels, it does not give the
predictors an unfair advantage.

while this point may seem obvious to the reader, we have seen this
blunder committed many times in published papers in top rank journals.
with the large numbers of predictors that are so common in genomic and
other areas, the potential consequences of this error have also increased
dramatically; see ambroise and mclachlan (2002) for a detailed discussion
of this issue.

7.10.3 does cross-validation really work?

we once again examine the behavior of cross-validation in a high-dimensional
classi   cation problem. consider a scenario with n = 20 samples in two
equal-sized classes, and p = 500 quantitative predictors that are indepen-
dent of the class labels. once again, the true error rate of any classi   er is
50%. consider a simple univariate classi   er: a single split that minimizes
the misclassi   cation error (a    stump   ). stumps are trees with a single split,
and are used in boosting methods (chapter 10). a simple argument sug-
gests that cross-validation will not work properly in this setting2:

fitting to the entire training set, we will    nd a predictor that
splits the data very well. if we do 5-fold cross-validation, this
same predictor should split any 4/5ths and 1/5th of the data
well too, and hence its cross-validation error will be small (much
less than 50%.) thus cv does not give an accurate estimate of
error.

to investigate whether this argument is correct, figure 7.11 shows the
result of a simulation from this setting. there are 500 predictors and 20
samples, in each of two equal-sized classes, with all predictors having a
standard gaussian distribution. the panel in the top left shows the number
of training errors for each of the 500 stumps    t to the training data. we
have marked in color the six predictors yielding the fewest errors. in the top
right panel, the training errors are shown for stumps    t to a random 4/5ths
partition of the data (16 samples), and tested on the remaining 1/5th (four
samples). the colored points indicate the same predictors marked in the
top left panel. we see that the stump for the blue predictor (whose stump
was the best in the top left panel), makes two out of four test errors (50%),
and is no better than random.

what has happened? the preceding argument has ignored the fact that
in cross-validation, the model must be completely retrained for each fold

2this argument was made to us by a scientist at a proteomics lab meeting, and led

to material in this section.

248

7. model assessment and selection

t
e
s
 
g
n
n
a
r
t

i

i

 
l
l

u
f
 
n
o
 
r
o
r
r

e

l

e
b
a
l

 
s
s
a
c

l

9

8

7

6

5

4

3

2

1

0

5
/
1
 
n
o
 
r
o
r
r

e

4

3

2

1

0

0

100

200

300

400

500

1

2

3

4

5

6

7

8

predictor

error on 4/5

full
4/5

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

   1

0

1

2

predictor 436 (blue)

cv errors

figure 7.11. simulation study to investigate the performance of cross vali-
dation in a high-dimensional problem where the predictors are independent of the
class labels. the top-left panel shows the number of errors made by individual
stump classi   ers on the full training set (20 observations). the top right panel
shows the errors made by individual stumps trained on a random split of the
dataset into 4/5ths (16 observations) and tested on the remaining 1/5th (4 ob-
servations). the best performers are depicted by colored dots in each panel. the
bottom left panel shows the e   ect of re-estimating the split point in each fold: the
colored points correspond to the four samples in the 1/5th validation set. the split
point derived from the full dataset classi   es all four samples correctly, but when
the split point is re-estimated on the 4/5ths data (as it should be), it commits
two errors on the four validation samples. in the bottom right we see the overall
result of    ve-fold cross-validation applied to 50 simulated datasets. the average
error rate is about 50%, as it should be.

7.11 bootstrap methods

249

of the process. in the present example, this means that the best predictor
and corresponding split point are found from 4/5ths of the data. the e   ect
of predictor choice is seen in the top right panel. since the class labels are
independent of the predictors, the performance of a stump on the 4/5ths
training data contains no information about its performance in the remain-
ing 1/5th. the e   ect of the choice of split point is shown in the bottom left
panel. here we see the data for predictor 436, corresponding to the blue
dot in the top left plot. the colored points indicate the 1/5th data, while
the remaining points belong to the 4/5ths. the optimal split points for this
predictor based on both the full training set and 4/5ths data are indicated.
the split based on the full data makes no errors on the 1/5ths data. but
cross-validation must base its split on the 4/5ths data, and this incurs two
errors out of four samples.

the results of applying    ve-fold cross-validation to each of 50 simulated
datasets is shown in the bottom right panel. as we would hope, the average
cross-validation error is around 50%, which is the true expected prediction
error for this classi   er. hence cross-validation has behaved as it should.
on the other hand, there is considerable variability in the error, underscor-
ing the importance of reporting the estimated standard error of the cv
estimate. see exercise 7.10 for another variation of this problem.

7.11 bootstrap methods

the bootstrap is a general tool for assessing statistical accuracy. first we
describe the bootstrap in general, and then show how it can be used to
estimate extra-sample prediction error. as with cross-validation, the boot-
strap seeks to estimate the conditional error errt , but typically estimates
well only the expected prediction error err.

suppose we have a model    t to a set of training data. we denote the
training set by z = (z1, z2, . . . , zn ) where zi = (xi, yi). the basic idea is
to randomly draw datasets with replacement from the training data, each
sample the same size as the original training set. this is done b times
(b = 100 say), producing b bootstrap datasets, as shown in figure 7.12.
then we re   t the model to each of the bootstrap datasets, and examine
the behavior of the    ts over the b replications.

in the    gure, s(z) is any quantity computed from the data z, for ex-
ample, the prediction at some input point. from the bootstrap sampling
we can estimate any aspect of the distribution of s(z), for example, its
variance,

dvar[s(z)] =

1

b     1

bxb=1

(s(z   b)       s   )2,

(7.53)

250

7. model assessment and selection

s(z   1)

s(z   2)

s(z   b)

z   1

z   2

z   b

bootstrap
replications

bootstrap
samples

z = (z1, z2, . . . , zn )

  

training
sample

figure 7.12. schematic of the bootstrap process. we wish to assess the sta-
tistical accuracy of a quantity s(z) computed from our dataset. b training sets
z   b, b = 1, . . . , b each of size n are drawn with replacement from the original
dataset. the quantity of interest s(z) is computed from each bootstrap training
set, and the values s(z   1), . . . , s(z   b) are used to assess the statistical accuracy
of s(z).

where   s    = pb s(z   b)/b. note that dvar[s(z)] can be thought of as a

monte-carlo estimate of the variance of s(z) under sampling from the
empirical distribution function   f for the data (z1, z2, . . . , zn ).

how can we apply the bootstrap to estimate prediction error? one ap-
proach would be to    t the model in question on a set of bootstrap samples,
and then keep track of how well it predicts the original training set. if
  f    b(xi) is the predicted value at xi, from the model    tted to the bth boot-
strap dataset, our estimate is

derrboot =

1
b

1
n

bxb=1

nxi=1

l(yi,   f    b(xi)).

(7.54)

however, it is easy to see thatderrboot does not provide a good estimate in

general. the reason is that the bootstrap datasets are acting as the training
samples, while the original training set is acting as the test sample, and
these two samples have observations in common. this overlap can make
over   t predictions look unrealistically good, and is the reason that cross-
validation explicitly uses non-overlapping data for the training and test
samples. consider for example a 1-nearest neighbor classi   er applied to a
two-class classi   cation problem with the same number of observations in

7.11 bootstrap methods

251

each class, in which the predictors and class labels are in fact independent.
then the true error rate is 0.5. but the contributions to the bootstrap

estimatederrboot will be zero unless the observation i does not appear in the

bootstrap sample b. in this latter case it will have the correct expectation
0.5. now

pr{observation i     bootstrap sample b} = 1    (cid:16)1    
    1     e   1
= 0.632.

1

n(cid:17)n

(7.55)

the correct error rate 0.5.

hence the expectation of derrboot is about 0.5    0.368 = 0.184, far below

by mimicking cross-validation, a better bootstrap estimate can be ob-
tained. for each observation, we only keep track of predictions from boot-
strap samples not containing that observation. the leave-one-out bootstrap
estimate of prediction error is de   ned by

(1)

=

1
n

nxi=1

1

|c   i| xb   c   i

l(yi,   f    b(xi)).

(7.56)

derr

the leave-one out bootstrap solves the over   tting problem su   ered by

here c   i is the set of indices of the bootstrap samples b that do not contain
(1)
observation i, and |c   i| is the number of such samples. in computingderr
,
we either have to choose b large enough to ensure that all of the |c   i| are
greater than zero, or we can just leave out the terms in (7.56) corresponding
to |c   i|   s that are zero.
derrboot, but has the training-set-size bias mentioned in the discussion of

cross-validation. the average number of distinct observations in each boot-
strap sample is about 0.632    n , so its bias will roughly behave like that of
twofold cross-validation. thus if the learning curve has considerable slope
at sample size n/2, the leave-one out bootstrap will be biased upward as
an estimate of the true error.

the    .632 estimator    is designed to alleviate this bias. it is de   ned by

(.632)

derr

= .368    err + .632   derr

(1)

.

(7.57)

the derivation of the .632 estimator is complex; intuitively it pulls the
leave-one out bootstrap estimate down toward the training error rate, and
hence reduces its upward bias. the use of the constant .632 relates to (7.55).
the .632 estimator works well in    light    tting    situations, but can break
down in over   t ones. here is an example due to breiman et al. (1984).
suppose we have two equal-size classes, with the targets independent of
the class labels, and we apply a one-nearest neighbor rule. then err = 0,

252

7. model assessment and selection

(1)

(.632)

rate is 0.5.

derr

= 0.5 and so derr

= .632    0.5 = .316. however, the true error
one can improve the .632 estimator by taking into account the amount
of over   tting. first we de   ne    to be the no-information error rate: this
is the error rate of our prediction rule if the inputs and class labels were
independent. an estimate of    is obtained by evaluating the prediction rule
on all possible combinations of targets yi and predictors xi   

     =

1
n 2

nxi=1

nxi   =1

l(yi,   f (xi    )).

(7.58)

for example, consider the dichotomous classi   cation problem: let   p1 be
the observed proportion of responses yi equaling 1, and let   q1 be the ob-
served proportion of predictions   f (xi    ) equaling 1. then

     =   p1(1       q1) + (1       p1)  q1.

(7.59)

with a rule like 1-nearest neighbors for which   q1 =   p1 the value of      is

2  p1(1      p1). the multi-category generalization of (7.59) is      =p      p   (1      q   ).

using this, the relative over   tting rate is de   ned to be

(1)

  r = derr

    err

         err

,

(7.60)

a quantity that ranges from 0 if there is no over   tting (derr

= err) to 1
if the over   tting equals the no-information value          err. finally, we de   ne
the    .632+    estimator by

(1)

(.632+)

derr

with   w =

(1)

= (1       w)    err +   w   derr

.632

.

1     .368   r

(7.61)

(.632+)

(1)

(.632)

to derr

. again, the derivation of (7.61) is compli-
cated: roughly speaking, it produces a compromise between the leave-one-
out bootstrap and the training error rate that depends on the amount of
over   tting. for the 1-nearest-neighbor problem with class labels indepen-

the weight w ranges from .632 if   r = 0 to 1 if   r = 1, so derr
ranges from derr
dent of the inputs,   w =   r = 1, soderr
expectation of 0.5. in other problems with less over   tting, derr
lie somewhere between err andderr

, which has the correct

=derr

(.632+)

(.632+)

will

(1)

(1)

.

7.11.1 example (continued)

figure 7.13 shows the results of tenfold cross-validation and the .632+ boot-
strap estimate in the same four problems of figures 7.7. as in that    gure,

7.11 bootstrap methods

253

cross   validation

t
s
e
b

 
r
e
v
o
 
e
s
a
e
r
c
n
i
 

%

t
s
e
b

 

 
r
e
v
o
e
s
a
e
r
c
n

i
 

%

0
0
1

0
8

0
6

0
4

0
2

0

0
0
1

0
8

0
6

0
4

0
2

0

reg/knn

reg/linear

class/knn

class/linear

bootstrap

reg/knn

reg/linear

class/knn

class/linear

figure 7.13. boxplots
error
100    [err          min   err(  )]/[max   err(  )     min   err(  )] over the four scenar-
ios of figure 7.3. this is the error in using the chosen model relative to the best
model. there are 100 training sets represented in each boxplot.

show the distribution of

relative

the

figure 7.13 shows boxplots of 100    [err          min   err(  )]/[max   err(  )    
min   err(  )], the error in using the chosen model relative to the best model.
there are 100 di   erent training sets represented in each boxplot. both mea-
sures perform well overall, perhaps the same or slightly worse than the aic
in figure 7.7.

our conclusion is that for these particular problems and    tting methods,
minimization of either aic, cross-validation or bootstrap yields a model
fairly close to the best available. note that for the purpose of model selec-
tion, any of the measures could be biased and it wouldn   t a   ect things, as
long as the bias did not change the relative performance of the methods.
for example, the addition of a constant to any of the measures would not
change the resulting chosen model. however, for many adaptive, nonlinear
techniques (like trees), estimation of the e   ective number of parameters is
very di   cult. this makes methods like aic impractical and leaves us with
cross-validation or bootstrap as the methods of choice.

a di   erent question is: how well does each method estimate test error?
on the average the aic criterion overestimated prediction error of its cho-

254

7. model assessment and selection

sen model by 38%, 37%, 51%, and 30%, respectively, over the four scenarios,
with bic performing similarly. in contrast, cross-validation overestimated
the error by 1%, 4%, 0%, and 4%, with the bootstrap doing about the
same. hence the extra work involved in computing a cross-validation or
bootstrap measure is worthwhile, if an accurate estimate of test error is
required. with other    tting methods like trees, cross-validation and boot-
strap can underestimate the true error by 10%, because the search for best
tree is strongly a   ected by the validation set. in these situations only a
separate test set will provide an unbiased estimate of test error.

7.12 conditional or expected test error?

figures 7.14 and 7.15 examine the question of whether cross-validation does
a good job in estimating errt , the error conditional on a given training set
t (expression (7.15) on page 228), as opposed to the expected test error.
for each of 100 training sets generated from the    reg/linear    setting in
the top-right panel of figure 7.3, figure 7.14 shows the conditional error
curves errt as a function of subset size (top left). the next two panels show
10-fold and n -fold cross-validation, the latter also known as leave-one-out
(loo). the thick red curve in each plot is the expected error err, while
the thick black curves are the expected cross-validation curves. the lower
right panel shows how well cross-validation approximates the conditional
and expected error.

one might have expected n -fold cv to approximate errt well, since it
almost uses the full training sample to    t a new test point. 10-fold cv, on
the other hand, might be expected to estimate err well, since it averages
over somewhat di   erent training sets. from the    gure it appears 10-fold
does a better job than n -fold in estimating errt , and estimates err even
better. indeed, the similarity of the two black curves with the red curve
suggests both cv curves are approximately unbiased for err, with 10-fold
having less variance. similar trends were reported by efron (1983).

figure 7.15 shows scatterplots of both 10-fold and n -fold cross-validation
error estimates versus the true conditional error for the 100 simulations.
although the scatterplots do not indicate much correlation, the lower right
panel shows that for the most part the correlations are negative, a curi-
ous phenomenon that has been observed before. this negative correlation
explains why neither form of cv estimates errt well. the broken lines in
each plot are drawn at err(p), the expected error for the best subset of
size p. we see again that both forms of cv are approximately unbiased for
expected error, but the variation in test error for di   erent training sets is
quite substantial.

among the four experimental conditions in 7.3, this    reg/linear    scenario
showed the highest correlation between actual and predicted test error. this

7.12 conditional or expected test error?

255

prediction error

10   fold cv error

r
o
r
r

e

4
.
0

3
.
0

2
.
0

1

.

0

5

10

15

20

5

10

15

20

subset size p

subset size p

leave   one   out cv error

approximation error

n
o

i
t

i

a
v
e
d
e

 

t

l

u
o
s
b
a
n
a
e
m

 

et |cv10    err|
et |cv10    errt |
et |cvn    errt |

5
4
0

.

0

5
3
0

.

0

5
2
0

.

0

5
1
0

.

0

r
o
r
r

e

r
o
r
r

e

4
.
0

3
.
0

2
.
0

1

.

0

4

.

0

3

.

0

2

.

0

1

.

0

5

10

15

20

5

10

15

20

subset size p

subset size p

figure 7.14. conditional prediction-error errt , 10-fold cross-validation, and
leave-one-out cross-validation curves for a 100 simulations from the top-right
panel in figure 7.3. the thick red curve is the expected prediction error err,
while the thick black curves are the expected cv curves et cv10 and et cvn .
the lower-right panel shows the mean absolute deviation of the cv curves from
the conditional error, et |cvk     errt | for k = 10 (blue) and k = n (green),
as well as from the expected error et |cv10     err| (orange).

256

7. model assessment and selection

0
4
.
0

0
3
.
0

0
2
.
0

0
1
0

.

0
4
0

.

0
3
0

.

0
2
0

.

0
1
0

.

r
o
r
r

 

e
v
c

r
o
r
r

 

e
v
c

subset size 1

subset size 5

0
4
.
0

0
3
.
0

0
2
.
0

0
1
0

.

r
o
r
r

 

e
v
c

0.10 0.15 0.20 0.25 0.30 0.35 0.40

0.10 0.15 0.20 0.25 0.30 0.35 0.40

prediction error

prediction error

subset size 10

n
o

i
t

l

a
e
r
r
o
c

2
0

.

0
0

.

.

2
0
   

.

4
0
   

.

6
0
   

leave   one   out
10   fold

0.10 0.15 0.20 0.25 0.30 0.35 0.40

5

10

15

20

prediction error

subset size

figure 7.15. plots of the cv estimates of error versus the true conditional
error for each of the 100 training sets, for the simulation setup in the top right
panel figure 7.3. both 10-fold and leave-one-out cv are depicted in di   erent
colors. the    rst three panels correspond to di   erent subset sizes p, and vertical
and horizontal lines are drawn at err(p). although there appears to be little cor-
relation in these plots, we see in the lower right panel that for the most part the
correlation is negative.

exercises

257

phenomenon also occurs for bootstrap estimates of error, and we would
guess, for any other estimate of conditional prediction error.

we conclude that estimation of test error for a particular training set is
not easy in general, given just the data from that same training set. instead,
cross-validation and related methods may provide reasonable estimates of
the expected error err.

bibliographic notes

key references for cross-validation are stone (1974), stone (1977) and
allen (1974). the aic was proposed by akaike (1973), while the bic
was introduced by schwarz (1978). madigan and raftery (1994) give an
overview of bayesian model selection. the mdl criterion is due to rissa-
nen (1983). cover and thomas (1991) contains a good description of coding
theory and complexity. vc dimension is described in vapnik (1996). stone
(1977) showed that the aic and leave-one out cross-validation are asymp-
totically equivalent. generalized cross-validation is described by golub et
al. (1979) and wahba (1980); a further discussion of the topic may be found
in the monograph by wahba (1990). see also hastie and tibshirani (1990),
chapter 3. the bootstrap is due to efron (1979); see efron and tibshi-
rani (1993) for an overview. efron (1983) proposes a number of bootstrap
estimates of prediction error, including the optimism and .632 estimates.
efron (1986) compares cv, gcv and bootstrap estimates of error rates.
the use of cross-validation and the bootstrap for model selection is stud-
ied by breiman and spector (1992), breiman (1992), shao (1996), zhang
(1993) and kohavi (1995). the .632+ estimator was proposed by efron
and tibshirani (1997).

cherkassky and ma (2003) published a study on the performance of
srm for model selection in regression, in response to our study of section
7.9.1. they complained that we had been unfair to srm because had not
applied it properly. our response can be found in the same issue of the
journal (hastie et al. (2003)).

exercises

ex. 7.1 derive the estimate of in-sample error (7.24).

ex. 7.2 for 0   1 loss with y     {0, 1} and pr(y = 1|x0) = f (x0), show that

err(x0) = pr(y 6=   g(x0)|x = x0)

= errb(x0) + |2f (x0)     1|pr(   g(x0) 6= g(x0)|x = x0),

(7.62)

2     f (x0))(e   f (x0)     1
2 )
qvar(   f (x0))
exp(   t2/2)dt,

258

7. model assessment and selection

2 ), g(x) = i(f (x) > 1

where   g(x) = i(   f (x) > 1
2 ) is the bayes classi   er,
and errb(x0) = pr(y 6= g(x0)|x = x0), the irreducible bayes error at x0.
using the approximation   f (x0)     n (e   f (x0), var(   f (x0)), show that
pr(   g(x0) 6= g(x0)|x = x0)         sign( 1

!. (7.63)

in the above,

  (t) =

1

   2  z t

      

we can think of sign( 1

2     f (x0))(e   f (x0)     1

the cumulative gaussian distribution function. this is an increasing func-
tion, with value 0 at t =        and value 1 at t = +   .
2 ) as a kind of boundary-
bias term, as it depends on the true f (x0) only through which side of the
boundary ( 1
2 ) that it lies. notice also that the bias and variance combine
in a multiplicative rather than additive fashion. if e   f (x0) is on the same
side of 1
2 as f (x0), then the bias is negative, and decreasing the variance
will decrease the misclassi   cation error. on the other hand, if e   f (x0) is
on the opposite side of 1
2 to f (x0), then the bias is positive and it pays to
increase the variance! such an increase will improve the chance that   f (x0)
falls on the correct side of 1

2 (friedman, 1997).

ex. 7.3 let   f = sy be a linear smoothing of y.

(a) if sii is the ith diagonal element of s, show that for s arising from least
squares projections and cubic smoothing splines, the cross-validated
residual can be written as

yi       f    i(xi) =

yi       f (xi)
1     sii

.

(7.64)

(b) use this result to show that |yi       f    i(xi)|     |yi       f (xi)|.
(c) find general conditions on any smoother s to make result (7.64) hold.

ex. 7.4 consider the in-sample prediction error (7.18) and the training
error err in the case of squared-error loss:

errin =

err =

1
n

1
n

nxi=1
nxi=1

ey 0(y 0

i       f (xi))2

(yi       f (xi))2.

add and subtract f (xi) and e   f (xi) in each expression and expand. hence
establish that the average optimism in the training error is

exercises

259

2
n

nxi=1

cov(  yi, yi),

as given in (7.21).

ex. 7.5 for a linear smoother   y = sy, show that

cov(  yi, yi) = trace(s)  2
   ,

(7.65)

nxi=1

which justi   es its use as the e   ective number of parameters.

ex. 7.6 show that for an additive-error model, the e   ective degrees-of-
freedom for the k-nearest-neighbors regression    t is n/k.
ex. 7.7 use the approximation 1/(1   x)2     1+2x to expose the relationship
between cp/aic (7.26) and gcv (7.52), the main di   erence being the
model used to estimate the noise variance   2
   .

ex. 7.8 show that the set of functions {i(sin(  x) > 0)} can shatter the
following points on the line:

z1 = 10   1, . . . , z    = 10      ,

(7.66)

for any    . hence the vc dimension of the class {i(sin(  x) > 0)} is in   nite.
ex. 7.9 for the prostate data of chapter 3, carry out a best-subset linear
regression analysis, as in table 3.3 (third column from left). compute the
aic, bic,    ve- and tenfold cross-validation, and bootstrap .632 estimates
of prediction error. discuss the results.

ex. 7.10 referring to the example in section 7.10.3, suppose instead that
all of the p predictors are binary, and hence there is no need to estimate
split points. the predictors are independent of the class labels as before.
then if p is very large, we can probably    nd a predictor that splits the
entire training data perfectly, and hence would split the validation data
(one-   fth of data) perfectly as well. this predictor would therefore have
zero cross-validation error. does this mean that cross-validation does not
provide a good estimate of test error in this situation? [this question was
suggested by li ma.]

260

7. model assessment and selection

8
model id136 and averaging

this is page 261
printer: opaque this

8.1

introduction

for most of this book, the    tting (learning) of models has been achieved by
minimizing a sum of squares for regression, or by minimizing cross-id178
for classi   cation. in fact, both of these minimizations are instances of the
maximum likelihood approach to    tting.

in this chapter we provide a general exposition of the maximum likeli-
hood approach, as well as the bayesian method for id136. the boot-
strap, introduced in chapter 7, is discussed in this context, and its relation
to maximum likelihood and bayes is described. finally, we present some
related techniques for model averaging and improvement, including com-
mittee methods, id112, stacking and bumping.

8.2 the bootstrap and maximum likelihood

methods

8.2.1 a smoothing example

the bootstrap method provides a direct computational way of assessing
uncertainty, by sampling from the training data. here we illustrate the
bootstrap in a simple one-dimensional smoothing problem, and show its
connection to maximum likelihood.

262

8. model id136 and averaging

   

   
   

         
   
   
   
   

   
   

y

5

4

3

2

1

0

1
-

   

   

   
   
                      
   
   
   
             

   
   
      

   

   

   
      
      
   
   

   
      

   
       

   

i

s
s
a
b
e
n

 

i
l

p
s
-
b

0

.

1

8

.

0

6
0

.

4
0

.

2
0

.

0
0

.

0.0

0.5

1.0

1.5

2.0

2.5

3.0

0.0

0.5

1.0

1.5

2.0

2.5

3.0

x

x

figure 8.1. (left panel): data for smoothing example. (right panel:) set of
seven b-spline basis functions. the broken vertical lines indicate the placement
of the three knots.

denote the training data by z = {z1, z2, . . . , zn}, with zi = (xi, yi),
i = 1, 2, . . . , n . here xi is a one-dimensional input, and yi the outcome,
either continuous or categorical. as an example, consider the n = 50 data
points shown in the left panel of figure 8.1.

suppose we decide to    t a cubic spline to the data, with three knots
placed at the quartiles of the x values. this is a seven-dimensional lin-
ear space of functions, and can be represented, for example, by a linear
expansion of b-spline basis functions (see section 5.9.2):

  (x) =

7xj=1

  jhj(x).

(8.1)

here the hj(x), j = 1, 2, . . . , 7 are the seven functions shown in the right
panel of figure 8.1. we can think of   (x) as representing the conditional
mean e(y |x = x).
let h be the n    7 matrix with ijth element hj(xi). the usual estimate
of   , obtained by minimizing the squared error over the training set, is
given by

     = (ht h)   1ht y.

(8.2)

the corresponding    t     (x) =p7

of figure 8.2.

the estimated covariance matrix of      is

j=1

    jhj(x) is shown in the top left panel

i=1(yi         (xi))2/n .
letting h(x)t = (h1(x), h2(x), . . . , h7(x)), the standard error of a predic-

where we have estimated the noise variance by     2 =pn

dvar(     ) = (ht h)   1     2,

(8.3)

8.2 the bootstrap and maximum likelihood methods

263

   

   
   

         
   
   
   
   

   

   

   
      
      
   
   

   
   

y

2.0

2.5

3.0

   

   
   

         
   
   
   
   

   

   

   
      
      
   
   

   
   

y

5

4

3

2

1

0

1
-

5

4

3

2

1

0

1
-

5

4

3

2

1

0

1
-

5

4

3

2

1

0

1
-

y

y

   

   

   

   
                      
   
   
   
             

   
   
      

0.0

0.5

1.0

   
      

   
       

   

1.5
x

   

   

   

   
          
            
   
   
   
             

   
   
      

0.0

0.5

1.0

   
      

   
       

   

1.5
x

   

   
   

         
   
   
   
   

   
   

   

   

   
      
      
   
   

2.0

2.5

3.0

   

   
   

         
   
   
   
   

   
   

   

   

   
      
      
   
   

2.0

2.5

3.0

   

   

   

   
                      
   
   
   
             

   
   
      

0.0

0.5

1.0

   
      

   
       

   

1.5
x

   

   

   

   
          
            
   
   
   
             

   
   
      

   
      

   
       

   

1.5
x

2.0

2.5

3.0

0.0

0.5

1.0

figure 8.2. (top left:) b-spline smooth of data. (top right:) b-spline smooth
plus and minus 1.96   standard error bands. (bottom left:) ten bootstrap repli-
cates of the b-spline smooth. (bottom right:) b-spline smooth with 95% standard
error bands computed from the bootstrap distribution.

264

8. model id136 and averaging

tion     (x) = h(x)t      is

1

2     .

(8.4)

bse[    (x)] = [h(x)t (ht h)   1h(x)]

in the top right panel of figure 8.2 we have plotted     (x)   1.96  bse[    (x)].

since 1.96 is the 97.5% point of the standard normal distribution, these
represent approximate 100     2    2.5% = 95% pointwise con   dence bands
for   (x).
here is how we could apply the bootstrap in this example. we draw b
datasets each of size n = 50 with replacement from our training data, the
sampling unit being the pair zi = (xi, yi). to each bootstrap dataset z   
we    t a cubic spline        (x); the    ts from ten such samples are shown in the
bottom left panel of figure 8.2. using b = 200 bootstrap samples, we can
form a 95% pointwise con   dence band from the percentiles at each x: we
   nd the 2.5%   200 =    fth largest and smallest values at each x. these are
plotted in the bottom right panel of figure 8.2. the bands look similar to
those in the top right, being a little wider at the endpoints.

there is actually a close connection between the least squares estimates
(8.2) and (8.3), the bootstrap, and maximum likelihood. suppose we further
assume that the model errors are gaussian,

y =   (x) +   ;        n (0,   2),

  (x) =

  jhj(x).

(8.5)

7xj=1

the bootstrap method described above, in which we sample with re-
placement from the training data, is called the nonparametric bootstrap.
this really means that the method is    model-free,    since it uses the raw
data, not a speci   c parametric model, to generate new datasets. consider
a variation of the bootstrap, called the parametric bootstrap, in which we
simulate new responses by adding gaussian noise to the predicted values:

i =     (xi) +      
y   
i ;

     
i     n (0,     2);

i = 1, 2, . . . , n.

(8.6)

this process is repeated b times, where b = 200 say. the resulting boot-
strap datasets have the form (x1, y   
n ) and we recompute the
b-spline smooth on each. the con   dence bands from this method will ex-
actly equal the least squares bands in the top right panel, as the number of
bootstrap samples goes to in   nity. a function estimated from a bootstrap
sample y    is given by        (x) = h(x)t (ht h)   1ht y   , and has distribution

1), . . . , (xn , y   

       (x)     n (    (x), h(x)t (ht h)   1h(x)    2).

(8.7)

notice that the mean of this distribution is the least squares estimate, and
the standard deviation is the same as the approximate formula (8.4).

8.2 the bootstrap and maximum likelihood methods

265

8.2.2 maximum likelihood id136

it turns out that the parametric bootstrap agrees with least squares in the
previous example because the model (8.5) has additive gaussian errors. in
general, the parametric bootstrap agrees not with least squares but with
maximum likelihood, which we now review.

we begin by specifying a id203 density or id203 mass function

for our observations

zi     g  (z).

(8.8)

in this expression    represents one or more unknown parameters that gov-
ern the distribution of z. this is called a parametric model for z. as an
example, if z has a normal distribution with mean    and variance   2, then

and

   = (  ,   2),

g  (z) =

1

   2    

e    1

2 (z     )2/  2

.

maximum likelihood is based on the likelihood function, given by

l(  ; z) =

g  (zi),

nyi=1

(8.9)

(8.10)

(8.11)

the id203 of the observed data under the model g  . the likelihood is
de   ned only up to a positive multiplier, which we have taken to be one.
we think of l(  ; z) as a function of   , with our data z    xed.

denote the logarithm of l(  ; z) by

   (  ; z) =

=

nxi=1
nxi=1

   (  ; zi)

log g  (zi),

(8.12)

which we will sometimes abbreviate as    (  ). this expression is called the
log-likelihood, and each value    (  ; zi) = log g  (zi) is called a log-likelihood
component. the method of maximum likelihood chooses the value    =     
to maximize    (  ; z).

the likelihood function can be used to assess the precision of     . we need

a few more de   nitions. the score function is de   ned by

     (  ; z) =

     (  ; zi),

nxi=1

(8.13)

266

8. model id136 and averaging

where      (  ; zi) =       (  ; zi)/     . assuming that the likelihood takes its maxi-
mum in the interior of the parameter space,      (    ; z) = 0. the information
matrix is

i(  ) =    

   2   (  ; zi)
          t .

nxi=1

(8.14)

when i(  ) is evaluated at    =     , it is often called the observed information.
the fisher information (or expected information) is

i(  ) = e  [i(  )].

(8.15)

finally, let   0 denote the true value of   .

a standard result says that the sampling distribution of the maximum

likelihood estimator has a limiting normal distribution

         n (  0, i(  0)   1),

(8.16)

as n        . here we are independently sampling from g  0(z). this suggests
that the sampling distribution of      may be approximated by

n (    , i(    )   1) or n (    , i(    )   1),

(8.17)

where      represents the maximum likelihood estimate from the observed
data.

the corresponding estimates for the standard errors of     j are obtained

from

qi(    )   1

jj

and

qi(    )   1

jj .

(8.18)

con   dence points for   j can be constructed from either approximation

in (8.17). such a con   dence point has the form

    j     z(1     )   qi(    )   1

jj

or

    j     z(1     )   qi(    )   1

jj ,

respectively, where z(1     ) is the 1        percentile of the standard normal
distribution. more accurate con   dence intervals can be derived from the
likelihood function, by using the chi-squared approximation

2[   (    )        (  0)]       2
p,

(8.19)

where p is the number of components in   . the resulting 1     2   con   -
(1   2  ),
dence interval is the set of all   0 such that 2[   (    )        (  0)]       2
(1   2  ) is the 1    2   percentile of the chi-squared distribution with
where   2
p
p degrees of freedom.

p

8.3 bayesian methods

267

let   s return to our smoothing example to see what maximum likelihood

yields. the parameters are    = (  ,   2). the log-likelihood is

   (  ) =    

n
2

log   22      

1
2  2

nxi=1

(yi     h(xi)t   )2.

(8.20)

the maximum likelihood estimate is obtained by setting       /      = 0 and
      /     2 = 0, giving

     = (ht h)   1ht y,

(8.21)

    2 =

1

n x(yi         (xi))2,

which are the same as the usual estimates given in (8.2) and below (8.3).
the information matrix for    = (  ,   2) is block-diagonal, and the block

corresponding to    is

i(  ) = (ht h)/  2,

(8.22)

so that the estimated variance (ht h)   1     2 agrees with the least squares
estimate (8.3).

8.2.3 bootstrap versus maximum likelihood

in essence the bootstrap is a computer implementation of nonparametric or
parametric maximum likelihood. the advantage of the bootstrap over the
maximum likelihood formula is that it allows us to compute maximum like-
lihood estimates of standard errors and other quantities in settings where
no formulas are available.

in our example, suppose that we adaptively choose by cross-validation
the number and position of the knots that de   ne the b-splines, rather
than    x them in advance. denote by    the collection of knots and their
positions. then the standard errors and con   dence bands should account
for the adaptive choice of   , but there is no way to do this analytically.
with the bootstrap, we compute the b-spline smooth with an adaptive
choice of knots for each bootstrap sample. the percentiles of the resulting
curves capture the variability from both the noise in the targets as well as
that from     . in this particular example the con   dence bands (not shown)
don   t look much di   erent than the    xed    bands. but in other problems,
where more adaptation is used, this can be an important e   ect to capture.

8.3 bayesian methods

in the bayesian approach to id136, we specify a sampling model pr(z|  )
(density or id203 mass function) for our data given the parameters,

268

8. model id136 and averaging

and a prior distribution for the parameters pr(  ) re   ecting our knowledge
about    before we see the data. we then compute the posterior distribution

pr(  |z) =

pr(z|  )    pr(  )

r pr(z|  )    pr(  )d  

,

(8.23)

which represents our updated knowledge about    after we see the data. to
understand this posterior distribution, one might draw samples from it or
summarize by computing its mean or mode. the bayesian approach di   ers
from the standard (   frequentist   ) method for id136 in its use of a prior
distribution to express the uncertainty present before seeing the data, and
to allow the uncertainty remaining after seeing the data to be expressed in
the form of a posterior distribution.

the posterior distribution also provides the basis for predicting the values

of a future observation znew, via the predictive distribution:

pr(znew|z) =z pr(znew|  )    pr(  |z)d  .

(8.24)

in contrast, the maximum likelihood approach would use pr(znew|    ),
the data density evaluated at the maximum likelihood estimate, to predict
future data. unlike the predictive distribution (8.24), this does not account
for the uncertainty in estimating   .

let   s walk through the bayesian approach in our smoothing example.
we start with the parametric model given by equation (8.5), and assume
for the moment that   2 is known. we assume that the observed feature
values x1, x2, . . . , xn are    xed, so that the randomness in the data comes
solely from y varying around its mean   (x).

the second ingredient we need is a prior distribution. distributions on
functions are fairly complex entities: one approach is to use a gaussian
process prior in which we specify the prior covariance between any two
function values   (x) and   (x   ) (wahba, 1990; neal, 1996).

here we take a simpler route: by considering a    nite b-spline basis for
  (x), we can instead provide a prior for the coe   cients   , and this implicitly
de   nes a prior for   (x). we choose a gaussian prior centered at zero

       n (0,      )

(8.25)

with the choices of the prior correlation matrix    and variance    to be
discussed below. the implicit process prior for   (x) is hence gaussian,
with covariance kernel

k(x, x   ) = cov[  (x),   (x   )]
=       h(x)t   h(x   ).

(8.26)

8.3 bayesian methods

269

)
x
(
  

3

2

1

0

1
-

2
-

3
-

0.0

0.5

1.0

1.5
x

2.0

2.5

3.0

figure 8.3. smoothing example: ten draws from the gaussian prior distri-
bution for the function   (x).

the posterior distribution for    is also gaussian, with mean and covariance

with the corresponding posterior values for   (x),

ht y,

  2
  
  2
  

e(  |z) =(cid:18)ht h +
cov(  |z) =(cid:18)ht h +
e(  (x)|z) = h(x)t(cid:18)ht h +
cov[  (x),   (x   )|z] = h(x)t(cid:18)ht h +

  2,

     1(cid:19)   1
     1(cid:19)   1
     1(cid:19)   1
     1(cid:19)   1

  2
  
  2
  

(8.27)

ht y,

h(x   )  2.

(8.28)

how do we choose the prior correlation matrix   ? in some settings the
prior can be chosen from subject matter knowledge about the parameters.
here we are willing to say the function   (x) should be smooth, and have
guaranteed this by expressing    in a smooth low-dimensional basis of b-
splines. hence we can take the prior correlation matrix to be the identity
   = i. when the number of basis functions is large, this might not be suf-
   cient, and additional smoothness can be enforced by imposing restrictions
on   ; this is exactly the case with smoothing splines (section 5.8.1).

figure 8.3 shows ten draws from the corresponding prior for   (x). to
generate posterior values of the function   (x), we generate values       from its
jhj(x).
ten such posterior curves are shown in figure 8.4. two di   erent values
were used for the prior variance    , 1 and 1000. notice how similar the
right panel looks to the bootstrap distribution in the bottom left panel

posterior (8.27), giving corresponding posterior value      (x) =p7

1      

270

8. model id136 and averaging

   = 1

   = 1000

   

   
   

         
   
   
   
   

   
   

   

   

   
      
      
   
   

)
x
(
  

5

4

3

2

1

0

1
-

)
x
(
  

5

4

3

2

1

0

1
-

   

   

   

   
                      
   
   
   
             

   
   
      

0.0

0.5

1.0

   
      

   
       

   

1.5
x

   

   

   

   
                      
   
   
   
             

   
   
      

   

   
   

         
   
   
   
   

   
   

   

   

   
      
      
   
   

2.0

2.5

3.0

   
      

   
       

   

1.5
x

2.0

2.5

3.0

0.0

0.5

1.0

figure 8.4. smoothing example: ten draws from the posterior distribution
for the function   (x), for two di   erent values of the prior variance    . the purple
curves are the posterior means.

of figure 8.2 on page 263. this similarity is no accident. as           , the
posterior distribution (8.27) and the bootstrap distribution (8.7) coincide.
on the other hand, for    = 1, the posterior curves   (x) in the left panel
of figure 8.4 are smoother than the bootstrap curves, because we have
imposed more prior weight on smoothness.

the distribution (8.25) with            is called a noninformative prior for
  . in gaussian models, maximum likelihood and parametric bootstrap anal-
yses tend to agree with bayesian analyses that use a noninformative prior
for the free parameters. these tend to agree, because with a constant prior,
the posterior distribution is proportional to the likelihood. this correspon-
dence also extends to the nonparametric case, where the nonparametric
bootstrap approximates a noninformative bayes analysis; section 8.4 has
the details.

we have, however, done some things that are not proper from a bayesian
point of view. we have used a noninformative (constant) prior for   2 and
replaced it with the maximum likelihood estimate     2 in the posterior. a
more standard bayesian analysis would also put a prior on    (typically
g(  )     1/  ), calculate a joint posterior for   (x) and   , and then integrate
out   , rather than just extract the maximum of the posterior distribution
(   map    estimate).

8.4 relationship between the bootstrap and bayesian id136

271

8.4 relationship between the bootstrap and

bayesian id136

consider    rst a very simple example, in which we observe a single obser-
vation z from a normal distribution

z     n (  , 1).

(8.29)

to carry out a bayesian analysis for   , we need to specify a prior. the
most convenient and common choice would be        n (0,    ) giving posterior
distribution

  |z     n(cid:18)

z

1 + 1/  

,

1

1 + 1/  (cid:19) .

(8.30)

now the larger we take    , the more concentrated the posterior becomes
around the maximum likelihood estimate      = z. in the limit as            we
obtain a noninformative (constant) prior, and the posterior distribution is

  |z     n (z, 1).

(8.31)

this is the same as a parametric bootstrap distribution in which we gen-
erate bootstrap values z    from the maximum likelihood estimate of the
sampling density n (z, 1).

there are three ingredients that make this correspondence work:

1. the choice of noninformative prior for   .

2. the dependence of the log-likelihood    (  ; z) on the data z only
through the maximum likelihood estimate     . hence we can write the
log-likelihood as    (  ;     ).

3. the symmetry of the log-likelihood in    and     , that is,    (  ;     ) =

   (    ;   ) + constant.

properties (2) and (3) essentially only hold for the gaussian distribu-
tion. however, they also hold approximately for the multinomial distribu-
tion, leading to a correspondence between the nonparametric bootstrap
and bayes id136, which we outline next.

assume that we have a discrete sample space with l categories. let wj be
the id203 that a sample point falls in category j, and   wj the observed
proportion in category j. let w = (w1, w2, . . . , wl),   w = (   w1,   w2, . . . ,   wl).
denote our estimator by s(   w); take as a prior distribution for w a sym-
metric dirichlet distribution with parameter a:

w     dil(a1),

(8.32)

272

8. model id136 and averaging

that is, the prior id203 mass function is proportional toql

then the posterior density of w is

   =1 wa   1

   

.

w     dil(a1 + n   w),

(8.33)

where n is the sample size. letting a     0 to obtain a noninformative prior
gives

w     dil(n   w).

(8.34)

now the bootstrap distribution, obtained by sampling with replacement
from the data, can be expressed as sampling the category proportions from
a multinomial distribution. speci   cally,

(8.35)

n   w        mult(n,   w),
l(cid:1)q   wn   w   

   

n

mass function(cid:0)

where mult(n,   w) denotes a multinomial distribution, having id203
. this distribution is similar to the pos-
terior distribution above, having the same support, same mean, and nearly
the same covariance matrix. hence the bootstrap distribution of s(   w   ) will
closely approximate the posterior distribution of s(w).

1 ,...,n   w   

n   w   

   

in this sense, the bootstrap distribution represents an (approximate)
nonparametric, noninformative posterior distribution for our parameter.
but this bootstrap distribution is obtained painlessly   without having to
formally specify a prior and without having to sample from the posterior
distribution. hence we might think of the bootstrap distribution as a    poor
man   s    bayes posterior. by perturbing the data, the bootstrap approxi-
mates the bayesian e   ect of perturbing the parameters, and is typically
much simpler to carry out.

8.5 the em algorithm

the em algorithm is a popular tool for simplifying di   cult maximum
likelihood problems. we    rst describe it in the context of a simple mixture
model.

8.5.1 two-component mixture model

in this section we describe a simple mixture model for density estimation,
and the associated em algorithm for carrying out maximum likelihood
estimation. this has a natural connection to id150 methods for
bayesian id136. mixture models are discussed and demonstrated in sev-
eral other parts of the book, in particular sections 6.8, 12.7 and 13.2.3.

the left panel of figure 8.5 shows a histogram of the 20    ctitious data

points in table 8.1.

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

8.5 the em algorithm

273

0                      

.
1

            

y
t
i
s
n
e
d

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

   

   

                         

   

0

2

4

6

0

2

4

6

y

y

figure 8.5. mixture example. (left panel:) histogram of data. (right panel:)
maximum likelihood    t of gaussian densities (solid red) and responsibility (dotted
green) of the left component density for observation y, as a function of y.

table 8.1. twenty    ctitious data points used in the two-component mixture
example in figure 8.5.

-0.39
0.06

0.12
0.48

0.94
1.01

1.67
1.68

1.76
1.80

2.44
3.25

3.72
4.12

4.28
4.60

4.92
5.28

5.53
6.22

we would like to model the density of the data points, and due to the
apparent bi-modality, a gaussian distribution would not be appropriate.
there seems to be two separate underlying regimes, so instead we model
y as a mixture of two normal distributions:

y1     n (  1,   2
1),
y2     n (  2,   2
2),
y = (1        )    y1 +        y2,

(8.36)

where         {0, 1} with pr(    = 1) =   . this generative representation is
explicit: generate a         {0, 1} with id203   , and then depending on
the outcome, deliver either y1 or y2. let     (x) denote the normal density
with parameters    = (  ,   2). then the density of y is

gy (y) = (1       )    1(y) +       2(y).

(8.37)

now suppose we wish to    t this model to the data in figure 8.5 by maxi-
mum likelihood. the parameters are

   = (  ,   1,   2) = (  ,   1,   2

1,   2,   2

2).

the log-likelihood based on the n training cases is

   (  ; z) =

nxi=1

log[(1       )    1(yi) +       2(yi)].

(8.38)

(8.39)

274

8. model id136 and averaging

direct maximization of    (  ; z) is quite di   cult numerically, because of
the sum of terms inside the logarithm. there is, however, a simpler ap-
proach. we consider unobserved latent variables    i taking values 0 or 1 as
in (8.36): if    i = 1 then yi comes from model 2, otherwise it comes from
model 1. suppose we knew the values of the    i   s. then the log-likelihood
would be

   0(  ; z,    ) =

nxi=1

[(1        i) log     1(yi) +    i log     2(yi)]

+

nxi=1

[(1        i) log(1       ) +    i log   ] ,

(8.40)

and the maximum likelihood estimates of   1 and   2
1 would be the sample
mean and variance for those data with    i = 0, and similarly those for   2
and   2
2 would be the sample mean and variance of the data with    i = 1.
the estimate of    would be the proportion of    i = 1.

since the values of the    i   s are actually unknown, we proceed in an

iterative fashion, substituting for each    i in (8.40) its expected value

  i(  ) = e(   i|  , z) = pr(   i = 1|  , z),

(8.41)

also called the responsibility of model 2 for observation i. we use a proce-
dure called the em algorithm, given in algorithm 8.1 for the special case of
gaussian mixtures. in the expectation step, we do a soft assignment of each
observation to each model: the current estimates of the parameters are used
to assign responsibilities according to the relative density of the training
points under each model. in the maximization step, these responsibilities
are used in weighted maximum-likelihood    ts to update the estimates of
the parameters.

1 and     2

at the value 0.5.

two of the yi at random. both     2

sample variancepn

a good way to construct initial guesses for     1 and     2 is simply to choose
2 can be set equal to the overall
i=1(yi       y)2/n . the mixing proportion      can be started
note that the actual maximizer of the likelihood occurs when we put a
spike of in   nite height at any one data point, that is,     1 = yi for some
i and     2
1 = 0. this gives in   nite likelihood, but is not a useful solution.
hence we are actually looking for a good local maximum of the likelihood,
one for which     2
2 > 0. to further complicate matters, there can be
more than one local maximum having     2
2 > 0. in our example, we
ran the em algorithm with a number of di   erent initial guesses for the
parameters, all having     2
k > 0.5, and chose the run that gave us the highest
maximized likelihood. figure 8.6 shows the progress of the em algorithm in

maximizing the log-likelihood. table 8.2 shows      =pi     i/n , the maximum

likelihood estimate of the proportion of observations in class 2, at selected
iterations of the em procedure.

1,     2

1,     2

algorithm 8.1 em algorithm for two-component gaussian mixture.

8.5 the em algorithm

275

1. take initial guesses for the parameters     1,     2

1,     2,     2

2,      (see text).

2. expectation step: compute the responsibilities

    i =

          2

(yi)

(1         )      1

(yi) +           2

(yi)

, i = 1, 2, . . . , n.

(8.42)

3. maximization step: compute the weighted means and variances:

    1 = pn
pn
    2 = pn
pn

i=1(1         i)yi
i=1(1         i)
i=1     iyi
i=1     i

,

,

    2

    2

1 = pn
2 = pn

i=1(1         i)(yi         1)2

,

i=1(1         i)
i=1     i(yi         2)2
,

i=1     i

pn
pn

and the mixing id203      =pn

4. iterate steps 2 and 3 until convergence.

i=1     i/n .

table 8.2. selected iterations of the em algorithm for mixture example.

iteration

1
5
10
15
20

    
0.485
0.493
0.523
0.544
0.546

the    nal maximum likelihood estimates are

    1 = 4.62,
    2 = 1.06,

    2
1 = 0.87,
    2
2 = 0.77,

     = 0.546.

the right panel of figure 8.5 shows the estimated gaussian mixture density
from this procedure (solid red curve), along with the responsibilities (dotted
green curve). note that mixtures are also useful for supervised learning; in
section 6.7 we show how the gaussian mixture model leads to a version of
radial basis functions.

276

8. model id136 and averaging

o o o o o o o o o o o o

o

o

o

o

o o o

9
3
-

0
4
-

1
4
-

2
4
-

3
4
-

4
4
-

d
o
o
h

i
l

e
k

i
l
-
g
o
l
a

 

t

 

a
d
d
e
v
r
e
s
b
o

o

5

10
iteration

15

20

figure 8.6. em algorithm: observed data log-likelihood as a function of the
iteration number.

8.5.2 the em algorithm in general

the above procedure is an example of the em (or baum   welch) algorithm
for maximizing likelihoods in certain classes of problems. these problems
are ones for which maximization of the likelihood is di   cult, but made
easier by enlarging the sample with latent (unobserved) data. this is called
data augmentation. here the latent data are the model memberships    i.
in other problems, the latent data are actual data that should have been
observed but are missing.

algorithm 8.2 gives the general formulation of the em algorithm. our
observed data is z, having log-likelihood    (  ; z) depending on parameters
  . the latent or missing data is zm, so that the complete data is t =
(z, zm) with log-likelihood    0(  ; t),    0 based on the complete density. in
the mixture problem (z, zm) = (y,    ), and    0(  ; t) is given in (8.40).

in our mixture example, e(   0(     ; t)|z,     (j)) is simply (8.40) with the    i
replaced by the responsibilities     i(    ), and the maximizers in step 3 are just
weighted means and variances.

we now give an explanation of why the em algorithm works in general.
since

pr(zm|z,      ) =

we can write

pr(zm, z|     )

pr(z|     )

,

(8.44)

pr(z|     ) =

pr(t|     )
pr(zm|z,      )

.

(8.45)

in terms of log-likelihoods, we have    (     ; z) =    0(     ; t)      1(     ; zm|z), where
   1 is based on the conditional density pr(zm|z,      ). taking conditional
expectations with respect to the distribution of t|z governed by parameter
   gives

   (     ; z) = e[   0(     ; t)|z,   ]     e[   1(     ; zm|z)|z,   ]

8.5 the em algorithm

277

algorithm 8.2 the em algorithm.

1. start with initial guesses for the parameters     (0).

2. expectation step: at the jth step, compute

q(     ,     (j)) = e(   0(     ; t)|z,     (j))

(8.43)

as a function of the dummy argument      .

3. maximization step: determine the new estimate     (j+1) as the maxi-

mizer of q(     ,     (j)) over      .

4. iterate steps 2 and 3 until convergence.

    q(     ,   )     r(     ,   ).

(8.46)

in the m step, the em algorithm maximizes q(     ,   ) over      , rather than
the actual objective function    (     ; z). why does it succeed in maximizing
   (     ; z)? note that r(     ,   ) is the expectation of a log-likelihood of a density
(indexed by      ), with respect to the same density indexed by   , and hence
(by jensen   s inequality) is maximized as a function of      , when       =    (see
exercise 8.1). so if       maximizes q(     ,   ), we see that

   (     ; z)        (  ; z) = [q(     ,   )     q(  ,   )]     [r(     ,   )     r(  ,   )]

    0.

(8.47)

hence the em iteration never decreases the log-likelihood.

this argument also makes it clear that a full maximization in the m
step is not necessary: we need only to    nd a value     (j+1) so that q(     ,     (j))
increases as a function of the    rst argument, that is, q(    (j+1),     (j)) >
q(    (j),     (j)). such procedures are called gem (generalized em) algorithms.
the em algorithm can also be viewed as a minorization procedure: see
exercise 8.7.

8.5.3 em as a maximization   maximization procedure

here is a di   erent view of the em procedure, as a joint maximization
algorithm. consider the function

f (     ,   p ) = e   p [   0(     ; t)]     e   p [log   p (zm)].

(8.48)

here   p (zm) is any distribution over the latent data zm. in the mixture
example,   p (zm) comprises the set of probabilities   i = pr(   i = 1|  , z).
note that f evaluated at   p (zm) = pr(zm|z,      ), is the log-likelihood of

278

8. model id136 and averaging

t

s
r
e
e
m
a
r
a
p

 
l

e
d
o
m

4

3

2

1

0

0.3

0.1

0.7

0.5

0.9

e

m

m

e

1

2

3

4

5

latent data parameters

figure 8.7. maximization   maximization view of the em algorithm. shown
are the contours of the (augmented) observed data log-likelihood f (     ,   p ). the
e step is equivalent to maximizing the log-likelihood over the parameters of the
latent data distribution. the m step maximizes it over the parameters of the
log-likelihood. the red curve corresponds to the observed data log-likelihood, a
pro   le obtained by maximizing f (     ,   p ) for each value of      .

the observed data, from (8.46)1. the function f expands the domain of
the log-likelihood, to facilitate its maximization.

the em algorithm can be viewed as a joint maximization method for f
over       and   p (zm), by    xing one argument and maximizing over the other.
the maximizer over   p (zm) for    xed       can be shown to be

  p (zm) = pr(zm|z,      )

(8.49)

(exercise 8.2). this is the distribution computed by the e step, for example,
(8.42) in the mixture example. in the m step, we maximize f (     ,   p ) over      
with   p    xed: this is the same as maximizing the    rst term e   p [   0(     ; t)|z,   ]
since the second term does not involve      .
finally, since f (     ,   p ) and the observed data log-likelihood agree when
  p (zm) = pr(zm|z,      ), maximization of the former accomplishes maxi-
mization of the latter. figure 8.7 shows a schematic view of this process.
this view of the em algorithm leads to alternative maximization proce-

1 (8.46) holds for all   , including    =      .

8.6 mcmc for sampling from the posterior

279

algorithm 8.3 gibbs sampler.

1. take some initial values u (0)

k , k = 1, 2, . . . , k.

2. repeat for t = 1, 2, . . . , . :

for k = 1, 2, . . . , k generate u (t)
k
k   1, u (t   1)

1 , . . . , u (t)

pr(u (t)

k |u (t)

from

k+1 , . . . , u (t   1)

k

).

3. continue step 2 until the joint distribution of (u (t)

1 , u (t)

2 , . . . , u (t)
k )

does not change.

dures. for example, one does not need to maximize with respect to all of
the latent data parameters at once, but could instead maximize over one
of them at a time, alternating with the m step.

8.6 mcmc for sampling from the posterior

having de   ned a bayesian model, one would like to draw samples from
the resulting posterior distribution, in order to make id136s about the
parameters. except for simple models, this is often a di   cult computa-
tional problem. in this section we discuss the id115
(mcmc) approach to posterior sampling. we will see that id150,
an mcmc procedure, is closely related to the em algorithm: the main dif-
ference is that it samples from the conditional distributions rather than
maximizing over them.

consider    rst the following abstract problem. we have random variables
u1, u2, . . . , uk and we wish to draw a sample from their joint distribution.
suppose this is di   cult to do, but it is easy to simulate from the conditional
distributions pr(uj|u1, u2, . . . , uj   1, uj+1, . . . , uk ), j = 1, 2, . . . , k. the
id150 procedure alternatively simulates from each of these distri-
butions and when the process stabilizes, provides a sample from the desired
joint distribution. the procedure is de   ned in algorithm 8.3.

under regularity conditions it can be shown that this procedure even-
tually stabilizes, and the resulting random variables are indeed a sample
from the joint distribution of u1, u2, . . . , uk . this occurs despite the fact
that the samples (u (t)
k ) are clearly not independent for dif-
ferent t. more formally, id150 produces a markov chain whose
stationary distribution is the true joint distribution, and hence the term
   id115.    it is not surprising that the true joint dis-
tribution is stationary under this process, as the successive steps leave the
marginal distributions of the uk   s unchanged.

2 , . . . , u (t)

1 , u (t)

280

8. model id136 and averaging

note that we don   t need to know the explicit form of the conditional
densities, but just need to be able to sample from them. after the procedure
reaches stationarity, the marginal density of any subset of the variables
can be approximated by a density estimate applied to the sample values.
however if the explicit form of the conditional density pr(uk,|u   ,     6= k)
is available, a better estimate of say the marginal density of uk can be
obtained from (exercise 8.3):

cpruk (u) =

1

(m     m + 1)

mxt=m

pr(u|u (t)

   

,     6= k).

(8.50)

here we have averaged over the last m     m + 1 members of the sequence,
to allow for an initial    burn-in    period before stationarity is reached.
now getting back to bayesian id136, our goal is to draw a sample from
the joint posterior of the parameters given the data z. id150 will
be helpful if it is easy to sample from the conditional distribution of each
parameter given the other parameters and z. an example   the gaussian
mixture problem   is detailed next.

there is a close connection between id150 from a posterior and
the em algorithm in exponential family models. the key is to consider the
latent data zm from the em procedure to be another parameter for the
gibbs sampler. to make this explicit for the gaussian mixture problem,
we take our parameters to be (  , zm). for simplicity we    x the variances
1,   2
  2
2 and mixing proportion    at their maximum likelihood values so that
the only unknown parameters in    are the means   1 and   2. the gibbs
sampler for the mixture problem is given in algorithm 8.4. we see that
steps 2(a) and 2(b) are the same as the e and m steps of the em pro-
cedure, except that we sample rather than maximize. in step 2(a), rather
than compute the maximum likelihood responsibilities   i = e(   i|  , z),
the id150 procedure simulates the latent data    i from the distri-
butions pr(   i|  , z). in step 2(b), rather than compute the maximizers of
the posterior pr(  1,   2,    |z) we simulate from the conditional distribution
pr(  1,   2|   , z).
figure 8.8 shows 200 iterations of id150, with the mean param-
eters   1 (lower) and   2 (upper) shown in the left panel, and the proportion

of class 2 observationspi    i/n on the right. horizontal broken lines have
been drawn at the maximum likelihood estimate values     1,     2 andpi     i/n

in each case. the values seem to stabilize quite quickly, and are distributed
evenly around the maximum likelihood values.

the above mixture model was simpli   ed, in order to make the clear
connection between id150 and the em algorithm. more realisti-
cally, one would put a prior distribution on the variances   2
2 and mixing
proportion   , and include separate id150 steps in which we sam-
ple from their posterior distributions, conditional on the other parameters.
one can also incorporate proper (informative) priors for the mean param-

1,   2

8.6 mcmc for sampling from the posterior

281

algorithm 8.4 id150 for mixtures.

1. take some initial values   (0) = (  (0)

1 ,   (0)
2 ).

2. repeat for t = 1, 2, . . . , .

(a) for i = 1, 2, . . . , n generate    (t)

    i(  (t)), from equation (8.42).

(b) set

i     {0, 1} with pr(   (t)

i = 1) =

,

    1 = pn
i=1(1        (t)
i )    yi
pn
i=1(1        (t)
i )
    2 = pn
i=1    (t)
pn

   yi
i=1    (t)

,

i

i

and generate   (t)

2     n (    2,     2
2).
3. continue step 2 until the joint distribution of (   (t),   (t)

1     n (    1,     2

1) and   (t)

1 ,   (t)

2 ) doesn   t

change

8

6

4

2

0

t

s
r
e
e
m
a
r
a
p
n
a
e
m

 

n
o

i
t
r
o
p
o
r
p
g
n
x
m

 

i

i

7

.

0

6

.

0

5
0

.

4
0

.

3

.

0

0

50

100

150

200

0

50

100

150

200

gibbs iteration

gibbs iteration

figure 8.8. mixture example. (left panel:) 200 values of the two mean param-
eters from id150; horizontal lines are drawn at the maximum likelihood
estimates     1,     2. (right panel:) proportion of values with    i = 1, for each of the
200 id150 iterations; a horizontal line is drawn at pi     i/n .

282

8. model id136 and averaging

eters. these priors must not be improper as this will lead to a degenerate
posterior, with all the mixing weight on one component.

id150 is just one of a number of recently developed procedures
for sampling from posterior distributions. it uses conditional sampling of
each parameter given the rest, and is useful when the structure of the prob-
lem makes this sampling easy to carry out. other methods do not require
such structure, for example the metropolis   hastings algorithm. these and
other computational bayesian methods have been applied to sophisticated
learning algorithms such as gaussian process models and neural networks.
details may be found in the references given in the bibliographic notes at
the end of this chapter.

8.7 id112

earlier we introduced the bootstrap as a way of assessing the accuracy of a
parameter estimate or a prediction. here we show how to use the bootstrap
to improve the estimate or prediction itself. in section 8.4 we investigated
the relationship between the bootstrap and bayes approaches, and found
that the bootstrap mean is approximately a posterior average. id112
further exploits this connection.

consider    rst the regression problem. suppose we    t a model to our
training data z = {(x1, y1), (x2, y2), . . . , (xn , yn )}, obtaining the predic-
tion   f (x) at input x. bootstrap aggregation or id112 averages this predic-
tion over a collection of bootstrap samples, thereby reducing its variance.
for each bootstrap sample z   b, b = 1, 2, . . . , b, we    t our model, giving
prediction   f    b(x). the id112 estimate is de   ned by

  fbag(x) =

  f    b(x).

1
b

bxb=1

(8.51)

1, y   

2, y   

i , y   

1), (x   

2), . . . , (x   

  f    (x), where z    = {(x   

denote by   p the empirical distribution putting equal id203 1/n on
each of the data points (xi, yi). in fact the    true    id112 estimate is
n )} and each
de   ned by e   p
i )       p. expression (8.51) is a monte carlo estimate of the true
(x   
id112 estimate, approaching it as b        .
the bagged estimate (8.51) will di   er from the original estimate   f (x)
only when the latter is a nonlinear or adaptive function of the data. for
example, to bag the b-spline smooth of section 8.2.1, we average the curves
in the bottom left panel of figure 8.2 at each value of x. the b-spline
smoother is linear in the data if we    x the inputs; hence if we sample using
the parametric bootstrap in equation (8.6), then   fbag(x)       f (x) as b        
(exercise 8.4). hence id112 just reproduces the original smooth in the

n , y   

8.7 id112

283

top left panel of figure 8.2. the same is approximately true if we were to
bag using the nonparametric bootstrap.

a more interesting example is a regression tree, where   f (x) denotes the
tree   s prediction at input vector x (regression trees are described in chap-
ter 9). each bootstrap tree will typically involve di   erent features than the
original, and might have a di   erent number of terminal nodes. the bagged
estimate is the average prediction at x from these b trees.

now suppose our tree produces a classi   er   g(x) for a k-class response.
here it is useful to consider an underlying indicator-vector function   f (x),
with value a single one and k     1 zeroes, such that   g(x) = arg maxk   f (x).
then the bagged estimate   fbag(x) (8.51) is a k-vector [p1(x), p2(x), . . . ,
pk (x)], with pk(x) equal to the proportion of trees predicting class k at x.
the bagged classi   er selects the class with the most    votes    from the b
trees,   gbag(x) = arg maxk   fbag(x).

often we require the class-id203 estimates at x, rather than the
classi   cations themselves. it is tempting to treat the voting proportions
pk(x) as estimates of these probabilities. a simple two-class example shows
that they fail in this regard. suppose the true id203 of class 1 at x is
0.75, and each of the bagged classi   ers accurately predict a 1. then p1(x) =
1, which is incorrect. for many classi   ers   g(x), however, there is already
an underlying function   f (x) that estimates the class probabilities at x (for
trees, the class proportions in the terminal node). an alternative id112
strategy is to average these instead, rather than the vote indicator vectors.
not only does this produce improved estimates of the class probabilities,
but it also tends to produce bagged classi   ers with lower variance, especially
for small b (see figure 8.10 in the next example).

8.7.1 example: trees with simulated data

we generated a sample of size n = 30, with two classes and p = 5 features,
each having a standard gaussian distribution with pairwise correlation
0.95. the response y was generated according to pr(y = 1|x1     0.5) = 0.2,
pr(y = 1|x1 > 0.5) = 0.8. the bayes error is 0.2. a test sample of size 2000
was also generated from the same population. we    t classi   cation trees to
the training sample and to each of 200 bootstrap samples (classi   cation
trees are described in chapter 9). no pruning was used. figure 8.9 shows
the original tree and eleven bootstrap trees. notice how the trees are all
di   erent, with di   erent splitting features and cutpoints. the test error for
the original tree and the bagged tree is shown in figure 8.10. in this ex-
ample the trees have high variance due to the correlation in the predictors.
id112 succeeds in smoothing out this variance and hence reducing the
test error.

id112 can dramatically reduce the variance of unstable procedures
like trees, leading to improved prediction. a simple argument shows why

284

8. model id136 and averaging

original tree

x.1 < 0.395

|

1

1

0

0

1

0

0

1

b = 3

x.2 < 0.285

|

0

0

1

0

0

1

1

1

b = 6

x.1 < 0.395

|

1

b = 1

x.1 < 0.555

|

0

1

b = 2

x.2 < 0.205

|

0

1

1

0

0

1

0

1

b = 4

x.3 < 0.985

|

b = 5

x.4 <    1.36

|

0

1

1

0

1

b = 7

x.1 < 0.395

|

0

1

1

0

1

1

0

b = 8

x.3 < 0.985

|

1

1

0

0

0

1

0

1

1

1

0

0

1

0

0

b = 9

x.1 < 0.395

|

b = 10

x.1 < 0.555

|

1

b = 11

x.1 < 0.555

|

1

0

1

0

0

1

0

1

1

0

0

1

0

1

figure 8.9. id112 trees on simulated dataset. the top left panel shows the
original tree. eleven trees grown on bootstrap samples are shown. for each tree,
the top split is annotated.

r
o
r
r

e

 
t
s
e
t

0
5

.

0

5
4

.

0

0
4

.

0

5
3

.

0

0
3

.

0

5
2

.

0

0
2

.

0

8.7 id112

285

consensus
id203

original tree

bagged trees

bayes

0

50

100

150

200

number of bootstrap samples

figure 8.10. error curves for the id112 example of figure 8.9. shown is
the test error of the original tree and bagged trees as a function of the number of
bootstrap samples. the orange points correspond to the consensus vote, while the
green points average the probabilities.

id112 helps under squared-error loss, in short because averaging reduces
variance and leaves bias unchanged.

assume our training observations (xi, yi),

i = 1, . . . , n are indepen-
dently drawn from a distribution p, and consider the ideal aggregate es-
timator fag(x) = ep   f    (x). here x is    xed and the bootstrap dataset z   
consists of observations x   
i , i = 1, 2, . . . , n sampled from p. note that
fag(x) is a id112 estimate, drawing bootstrap samples from the actual
population p rather than the data. it is not an estimate that we can use
in practice, but is convenient for analysis. we can write

i , y   

ep [y       f    (x)]2 = ep [y     fag(x) + fag(x)       f    (x)]2

= ep [y     fag(x)]2 + ep [   f    (x)     fag(x)]2
    ep [y     fag(x)]2.

(8.52)

the extra error on the right-hand side comes from the variance of   f    (x)
around its mean fag(x). therefore true population aggregation never in-
creases mean squared error. this suggests that id112   drawing samples
from the training data    will often decrease mean-squared error.

the above argument does not hold for classi   cation under 0-1 loss, be-
cause of the nonadditivity of bias and variance. in that setting, id112 a

286

8. model id136 and averaging

good classi   er can make it better, but id112 a bad classi   er can make it
worse. here is a simple example, using a randomized rule. suppose y = 1
for all x, and the classi   er   g(x) predicts y = 1 (for all x) with proba-
bility 0.4 and predicts y = 0 (for all x) with id203 0.6. then the
misclassi   cation error of   g(x) is 0.6 but that of the bagged classi   er is 1.0.
for classi   cation we can understand the id112 e   ect in terms of a
consensus of independent weak learners (dietterich, 2000a). let the bayes
optimal decision at x be g(x) = 1 in a two-class example. suppose each
of the weak learners g   
b have an error-rate eb = e < 0.5, and let s1(x) =
b (x) = 1) be the consensus vote for class 1. since the weak learn-
ers are assumed to be independent, s1(x)     bin(b, 1     e), and pr(s1 >
b/2)     1 as b gets large. this concept has been popularized outside of
statistics as the    wisdom of crowds    (surowiecki, 2004)     the collective
knowledge of a diverse and independent body of people typically exceeds
the knowledge of any single individual, and can be harnessed by voting.
of course, the main caveat here is    independent,    and bagged trees are
not. figure 8.11 illustrates the power of a consensus vote in a simulated
example, where only 30% of the voters have some knowledge.

pb

b=1 i(g   

in chapter 15 we see how id79s improve on id112 by reducing

the correlation between the sampled trees.

note that when we bag a model, any simple structure in the model is
lost. as an example, a bagged tree is no longer a tree. for interpretation
of the model this is clearly a drawback. more stable procedures like near-
est neighbors are typically not a   ected much by id112. unfortunately,
the unstable models most helped by id112 are unstable because of the
emphasis on interpretability, and this is lost in the id112 process.

figure 8.12 shows an example where id112 doesn   t help. the 100 data
points shown have two features and two classes, separated by the gray
linear boundary x1 + x2 = 1. we choose as our classi   er   g(x) a single
axis-oriented split, choosing the split along either x1 or x2 that produces
the largest decrease in training misclassi   cation error.

the decision boundary obtained from id112 the 0-1 decision rule over
b = 50 bootstrap samples is shown by the blue curve in the left panel.
it does a poor job of capturing the true boundary. the single split rule,
derived from the training data, splits near 0 (the middle of the range of x1
or x2), and hence has little contribution away from the center. averaging
the probabilities rather than the classi   cations does not help here. id112
estimates the expected class probabilities from the single split rule, that is,
averaged over many replications. note that the expected class probabilities
computed by id112 cannot be realized on any single replication, in the
same way that a woman cannot have 2.4 children. in this sense, id112
increases somewhat the space of models of the individual base classi   er.
however, it doesn   t help in this and many other examples where a greater
enlargement of the model class is needed.    boosting    is a way of doing this

8.7 id112

287

wisdom of crowds

consensus
individual

0
1

 
f

o

 
t

u
o

 
t
c
e
r
r
o
c
d
e

 

t
c
e
p
x
e

0
1

8

6

4

2

0

0.25

0.50

0.75

1.00

p      id203 of informed person being correct

figure 8.11. simulated academy awards voting. 50 members vote in 10 cat-
egories, each with 4 nominations. for any category, only 15 voters have some
knowledge, represented by their id203 of selecting the    correct    candidate in
that category (so p = 0.25 means they have no knowledge). for each category, the
15 experts are chosen at random from the 50. results show the expected correct
(based on 50 simulations) for the consensus, as well as for the individuals. the
error bars indicate one standard deviation. we see, for example, that if the 15
informed for a category have a 50% chance of selecting the correct candidate, the
consensus doubles the expected performance of an individual.

288

8. model id136 and averaging

bagged decision rule

boosted decision rule

   

   

   

   

   

   
   
   
   
   
   
   
   
   
   
   

   
   

   

   

   

   

   

   
   
   
   
   
   
   

   

   

   

   

   

   

   
   

   
   

      

   
   

   
   
   

   
   

   
   

   
   

   

   

   

   

   

   
   

   
   

   

   

   
   

   
   

   
      

   
   

   

   

   

   
   

   

   
   
   
   

   

   

   

   
   
   
   
   

   

   
   
   

   

   

   

   

   

   

   

   
   

   
   
   

   
   

   

   
   
   

   
   
   
   

   

   

   

   

   

   

   
   
   

   
   
   

   

   

   
   
   
   

   
   
   

      

   

   
   
   
   

   

   
   

   

   
   

   

   
   
   
   
   

   

   
   

   

   

   
   

   

   
   
   
   

   

   
   

   
   
   

   
   

   
   
   

   

   
   

   

   

   

   

   

   
   
   
   
   

   
   
   

   
   
   

   

   

   
   

   

   

   
   

   

   
   
   
   

   

   
   

   
   
   

   

   

   

   

   

   
   
   
   
   
   
   
   
   
   
   

   
   

   

   

   

   

   

   
   
   
   
   
   
   

   

   

   

   

   

   

   
   

   
   

      

   
   

   

   

   

   

   
   

   
   
   

   
   

   
   

   

   
   

   
   

   

   

   
   

   
   

   
      

   
   

   

   

   

   
   

   

   
   
   
   

   

   

   

   
   
   
   
   

   

   
   
   

   

   

   

   

   

   
   
   

   
   
   

   

   

   

   

   

   

   

   

   
   

   
   
   

   
   

   

   
   
   

   
   
   
   

   

   
   
   

   

   

   
   
   
   

   
   
   

      

   

   
   
   
   

   

   
   

   

   
   

   

   
   
   
   
   

   
   

   
   
   

   

   
   

   

   

   

   

   

   
   
   
   
   

   
   
   

figure 8.12. data with two features and two classes, separated by a linear
boundary. (left panel:) decision boundary estimated from id112 the decision
rule from a single split, axis-oriented classi   er. (right panel:) decision boundary
from boosting the decision rule of the same classi   er. the test error rates are
0.166, and 0.065, respectively. boosting is described in chapter 10.

and is described in chapter 10. the decision boundary in the right panel is
the result of the boosting procedure, and it roughly captures the diagonal
boundary.

8.8 model averaging and stacking

in section 8.4 we viewed bootstrap values of an estimator as approximate
posterior values of a corresponding parameter, from a kind of nonparamet-
ric bayesian analysis. viewed in this way, the bagged estimate (8.51) is
an approximate posterior bayesian mean. in contrast, the training sample
estimate   f (x) corresponds to the mode of the posterior. since the posterior
mean (not mode) minimizes squared-error loss, it is not surprising that
id112 can often reduce mean squared-error.

here we discuss bayesian model averaging more generally. we have a
set of candidate models mm, m = 1, . . . , m for our training set z. these
models may be of the same type with di   erent parameter values (e.g.,
subsets in id75), or di   erent models for the same task (e.g.,
neural networks and regression trees).

suppose    is some quantity of interest, for example, a prediction f (x) at

some    xed feature value x. the posterior distribution of    is

pr(  |z) =

mxm=1

pr(  |mm, z)pr(mm|z),

(8.53)

8.8 model averaging and stacking

289

with posterior mean

e(  |z) =

mxm=1

e(  |mm, z)pr(mm|z).

(8.54)

this bayesian prediction is a weighted average of the individual predictions,
with weights proportional to the posterior id203 of each model.

this formulation leads to a number of di   erent model-averaging strate-
gies. committee methods take a simple unweighted average of the predic-
tions from each model, essentially giving equal id203 to each model.
more ambitiously, the development in section 7.7 shows the bic criterion
can be used to estimate posterior model probabilities. this is applicable
in cases where the di   erent models arise from the same parametric model,
with di   erent parameter values. the bic gives weight to each model de-
pending on how well it    ts and how many parameters it uses. one can also
carry out the bayesian recipe in full. if each model mm has parameters
  m, we write

pr(mm|z)     pr(mm)    pr(z|mm)

    pr(mm)   z pr(z|  m,mm)pr(  m|mm)d  m.

(8.55)

in principle one can specify priors pr(  m|mm) and numerically com-
pute the posterior probabilities from (8.55), to be used as model-averaging
weights. however, we have seen no real evidence that this is worth all of
the e   ort, relative to the much simpler bic approximation.

how can we approach model averaging from a frequentist viewpoint?
given predictions   f1(x),   f2(x), . . . ,   fm (x), under squared-error loss, we can
seek the weights w = (w1, w2, . . . , wm ) such that

  w = argmin

w

ephy    

mxm=1

wm   fm(x)i2

.

(8.56)

here the input value x is    xed and the n observations in the dataset z (and
the target y ) are distributed according to p. the solution is the population
id75 of y on   f (x)t     [   f1(x),   f2(x), . . . ,   fm (x)]:

  w = ep [   f (x)   f (x)t ]   1ep [   f (x)y ].

(8.57)

now the full regression has smaller error than any single model

ep"y    

mxm=1

  wm   fm(x)#2

    ephy       fm(x)i2

   m

(8.58)

so combining models never makes things worse, at the population level.

290

8. model id136 and averaging

of course the population id75 (8.57) is not available, and it
is natural to replace it with the id75 over the training set. but
there are simple examples where this does not work well. for example, if
  fm(x), m = 1, 2, . . . , m represent the prediction from the best subset of
inputs of size m among m total inputs, then id75 would put all
of the weight on the largest model, that is,   wm = 1,   wm = 0, m < m . the
problem is that we have not put each of the models on the same footing
by taking into account their complexity (the number of inputs m in this
example).

stacked generalization, or stacking, is a way of doing this. let   f    i

m (x)
be the prediction at x, using model m, applied to the dataset with the
ith training observation removed. the stacking estimate of the weights is
obtained from the least squares id75 of yi on   f    i
m (xi), m =
1, 2, . . . , m . in detail the stacking weights are given by

wm   f    i

m (xi)#2

.

(8.59)

  wst = argmin

nxi=1"yi    
the    nal prediction is pm   wst

m

w

mxm=1

  fm(x). by using the cross-validated pre-
dictions   f    i
m (x), stacking avoids giving unfairly high weight to models with
higher complexity. better results can be obtained by restricting the weights
to be nonnegative, and to sum to 1. this seems like a reasonable restriction
if we interpret the weights as posterior model probabilities as in equation
(8.54), and it leads to a tractable quadratic programming problem.

there is a close connection between stacking and model selection via
leave-one-out cross-validation (section 7.10). if we restrict the minimization
in (8.59) to weight vectors w that have one unit weight and the rest zero,
this leads to a model choice   m with smallest leave-one-out cross-validation
error. rather than choose a single model, stacking combines them with
estimated optimal weights. this will often lead to better prediction, but
less interpretability than the choice of only one of the m models.

the stacking idea is actually more general than described above. one
can use any learning method, not just id75, to combine the
models as in (8.59); the weights could also depend on the input location
x. in this way, learning methods are    stacked    on top of one another, to
improve prediction performance.

8.9 stochastic search: bumping

the    nal method described in this chapter does not involve averaging or
combining models, but rather is a technique for    nding a better single
model. bumping uses bootstrap sampling to move randomly through model
space. for problems where    tting method    nds many local minima, bump-
ing can help the method to avoid getting stuck in poor solutions.

8.9 stochastic search: bumping

291

regular 4-node tree

bumped 4-node tree

   

   

   

   

   
   
   
   
   

   

   

   

   

   
   
   

   

   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   

   

   
   
   
   

   
   
   

   
   
   
   
   
   
   
   

   

   

   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   
   

   
   
   
   
   
   
   

   

   

   
   

   
   
   
   
   

   
   
   
   
   

   
      
   
   
   

   

   
   

   
   

   
   
   

   
   

   
   

   

   

   
   
   

   

   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   
   
   

   

   

   
   

   

   

   
   

   

   

   
   

   
   
   

   

   
   
   

   

   

   

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   
      
   
   
   
   
   
   
   
   

   

   
   
   
   
   
   
   
   
   
   

   

   

   
   
   

   

   

   
   
   
   
   
   
   
   
   

   

   
   
   

   
   
   

   

   
   
   

   
   
   

   

   
   

   
   
   

   
   
   
   

   
   
   

   

   
   
   

   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   

   

   
   
   

   

   
   
   

   

   
   
   
   
   
   
   

   
   
   
   

   

   

   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   

   

   

   

   
   
   
   
   

   

   

   

   

   
   
   

   

   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   

   

   
   
   
   

   
   
   

   
   
   
   
   
   
   
   

   

   

   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   
   

   
   
   
   
   
   
   

   

   

   
   

   
   
   
   
   

   
   
   
   
   

   
      
   
   
   

   

   
   

   
   

   
   
   

   
   

   
   

   

   

   
   
   

   

   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   
   
   

   

   

   
   

   

   

   
   

   

   

   
   

   
   
   

   

   
   
   

   

   

   

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

   
      
   
   
   
   
   
   
   
   

   

   
   
   
   
   
   
   
   
   
   

   

   

   
   
   

   

   

   
   
   
   
   
   
   
   
   

   

   
   
   

   
   
   

   

   
   
   

   
   
   

   

   
   

   
   
   

   
   
   
   

   
   
   

   

   
   
   

   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   

   

   
   
   

   

   
   
   

   

   
   
   
   
   
   
   

   
   
   
   

   

   

   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   

figure 8.13. data with two features and two classes (blue and orange), dis-
playing a pure interaction. the left panel shows the partition found by three splits
of a standard, greedy, tree-growing algorithm. the vertical grey line near the left
edge is the    rst split, and the broken lines are the two subsequent splits. the al-
gorithm has no idea where to make a good initial split, and makes a poor choice.
the right panel shows the near-optimal splits found by bumping the tree-growing
algorithm 20 times.

as in id112, we draw bootstrap samples and    t a model to each. but
rather than average the predictions, we choose the model estimated from a
bootstrap sample that best    ts the training data. in detail, we draw boot-
strap samples z   1, . . . , z   b and    t our model to each, giving predictions
  f    b(x), b = 1, 2, . . . , b at input point x. we then choose the model that
produces the smallest prediction error, averaged over the original training
set. for squared error, for example, we choose the model obtained from
bootstrap sample   b, where

  b = arg min

b

nxi=1
[yi       f    b(xi)]2.

(8.60)

the corresponding model predictions are   f      b(x). by convention we also
include the original training sample in the set of bootstrap samples, so that
the method is free to pick the original model if it has the lowest training
error.

by perturbing the data, bumping tries to move the    tting procedure
around to good areas of model space. for example, if a few data points are
causing the procedure to    nd a poor solution, any bootstrap sample that
omits those data points should procedure a better solution.

for another example, consider the classi   cation data in figure 8.13, the
notorious exclusive or (xor) problem. there are two classes (blue and
orange) and two input features, with the features exhibiting a pure inter-

292

8. model id136 and averaging

action. by splitting the data at x1 = 0 and then splitting each resulting
strata at x2 = 0, (or vice versa) a tree-based classi   er could achieve per-
fect discrimination. however, the greedy, short-sighted cart algorithm
(section 9.2) tries to    nd the best split on either feature, and then splits
the resulting strata. because of the balanced nature of the data, all initial
splits on x1 or x2 appear to be useless, and the procedure essentially gener-
ates a random split at the top level. the actual split found for these data is
shown in the left panel of figure 8.13. by bootstrap sampling from the data,
bumping breaks the balance in the classes, and with a reasonable number
of bootstrap samples (here 20), it will by chance produce at least one tree
with initial split near either x1 = 0 or x2 = 0. using just 20 bootstrap
samples, bumping found the near optimal splits shown in the right panel
of figure 8.13. this shortcoming of the greedy tree-growing algorithm is
exacerbated if we add a number of noise features that are independent of
the class label. then the tree-growing algorithm cannot distinguish x1 or
x2 from the others, and gets seriously lost.

since bumping compares di   erent models on the training data, one must
ensure that the models have roughly the same complexity. in the case of
trees, this would mean growing trees with the same number of terminal
nodes on each bootstrap sample. bumping can also help in problems where
it is di   cult to optimize the    tting criterion, perhaps because of a lack of
smoothness. the trick is to optimize a di   erent, more convenient criterion
over the bootstrap samples, and then choose the model producing the best
results for the desired criterion on the training sample.

bibliographic notes

there are many books on classical statistical id136: cox and hink-
ley (1974) and silvey (1975) give nontechnical accounts. the bootstrap
is due to efron (1979) and is described more fully in efron and tibshi-
rani (1993) and hall (1992). a good modern book on bayesian id136
is gelman et al. (1995). a lucid account of the application of bayesian
methods to neural networks is given in neal (1996). the statistical appli-
cation of id150 is due to geman and geman (1984), and gelfand
and smith (1990), with related work by tanner and wong (1987). markov
chain monte carlo methods, including id150 and the metropolis   
hastings algorithm, are discussed in spiegelhalter et al. (1996). the em
algorithm is due to dempster et al. (1977); as the discussants in that pa-
per make clear, there was much related, earlier work. the view of em as
a joint maximization scheme for a penalized complete-data log-likelihood
was elucidated by neal and hinton (1998); they credit csiszar and tusn  ady
(1984) and hathaway (1986) as having noticed this connection earlier. bag-
ging was proposed by breiman (1996a). stacking is due to wolpert (1992);

exercises

293

breiman (1996b) contains an accessible discussion for statisticians. leblanc
and tibshirani (1996) describe variations on stacking based on the boot-
strap. model averaging in the bayesian framework has been recently advo-
cated by madigan and raftery (1994). bumping was proposed by tibshi-
rani and knight (1999).

exercises

ex. 8.1 let r(y) and q(y) be id203 density functions. jensen   s in-
equality states that for a random variable x and a convex function   (x),
e[  (x)]       [e(x)]. use jensen   s inequality to show that

eq log[r(y )/q(y )]

(8.61)

is maximized as a function of r(y) when r(y) = q(y). hence show that
r(  ,   )     r(     ,   ) as stated below equation (8.46).
ex. 8.2 consider the maximization of the log-likelihood (8.48), over dis-
tributions   p (zm) such that   p (zm)     0 and pzm   p (zm) = 1. use la-
grange multipliers to show that the solution is the conditional distribution
  p (zm) = pr(zm|z,      ), as in (8.49).
ex. 8.3 justify the estimate (8.50), using the relationship

pr(a) =z pr(a|b)d(pr(b)).

ex. 8.4 consider the id112 method of section 8.7. let our estimate   f (x)
be the b-spline smoother     (x) of section 8.2.1. consider the parametric
bootstrap of equation (8.6), applied to this estimator. show that if we bag
  f (x), using the parametric bootstrap to generate the bootstrap samples,
the id112 estimate   fbag(x) converges to the original estimate   f (x) as
b        .
ex. 8.5 suggest generalizations of each of the id168s in figure 10.4
to more than two classes, and design an appropriate plot to compare them.

ex. 8.6 consider the bone mineral density data of figure 5.6.

(a) fit a cubic smooth spline to the relative change in spinal bmd, as a
function of age. use cross-validation to estimate the optimal amount
of smoothing. construct pointwise 90% con   dence bands for the un-
derlying function.

(b) compute the posterior mean and covariance for the true function via

(8.28), and compare the posterior bands to those obtained in (a).

294

8. model id136 and averaging

(c) compute 100 bootstrap replicates of the    tted curves, as in the bottom
left panel of figure 8.2. compare the results to those obtained in (a)
and (b).

ex. 8.7 em as a minorization algorithm(hunter and lange, 2004; wu and
lange, 2007). a function g(x, y) to said to minorize a function f (x) if

g(x, y)     f (x), g(x, x) = f (x)

(8.62)

for all x, y in the domain. this is useful for maximizing f (x) since it is easy
to show that f (x) is non-decreasing under the update

xs+1 = argmaxxg(x, xs)

(8.63)

there are analogous de   nitions for majorization, for minimizing a function
f (x). the resulting algorithms are known as mm algorithms, for    minorize-
maximize    or    majorize-minimize.   

show that the em algorithm (section 8.5.2) is an example of an mm al-
gorithm, using q(     ,   )+log pr(z|  )   q(  ,   ) to minorize the observed data
log-likelihood    (     ; z). (note that only the    rst term involves the relevant
parameter      ).

this is page 295
printer: opaque this

9
additive models, trees, and related
methods

in this chapter we begin our discussion of some speci   c methods for super-
vised learning. these techniques each assume a (di   erent) structured form
for the unknown regression function, and by doing so they    nesse the curse
of dimensionality. of course, they pay the possible price of misspecifying
the model, and so in each case there is a tradeo    that has to be made. they
take o    where chapters 3   6 left o   . we describe    ve related techniques:
generalized additive models, trees, multivariate adaptive regression splines,
the patient rule induction method, and hierarchical mixtures of experts.

9.1 generalized additive models

regression models play an important role in many data analyses, providing
prediction and classi   cation rules, and data analytic tools for understand-
ing the importance of di   erent inputs.

although attractively simple, the traditional linear model often fails in
these situations: in real life, e   ects are often not linear. in earlier chapters
we described techniques that used prede   ned basis functions to achieve
nonlinearities. this section describes more automatic    exible statistical
methods that may be used to identify and characterize nonid75
e   ects. these methods are called    generalized additive models.   

in the regression setting, a generalized additive model has the form

e(y |x1, x2, . . . , xp) =    + f1(x1) + f2(x2) +        + fp(xp).

(9.1)

296

9. additive models, trees, and related methods

as usual x1, x2, . . . , xp represent predictors and y is the outcome; the fj   s
are unspeci   ed smooth (   nonparametric   ) functions. if we were to model
each function using an expansion of basis functions (as in chapter 5), the
resulting model could then be    t by simple least squares. our approach
here is di   erent: we    t each function using a scatterplot smoother (e.g., a
cubic smoothing spline or kernel smoother), and provide an algorithm for
simultaneously estimating all p functions (section 9.1.1).

for two-class classi   cation, recall the id28 model for binary
data discussed in section 4.4. we relate the mean of the binary response
  (x) = pr(y = 1|x) to the predictors via a id75 model and
the logit link function:

log(cid:18)   (x)

1       (x)(cid:19) =    +   1x1 +        +   pxp.

(9.2)

the additive id28 model replaces each linear term by a more

general functional form

log(cid:18)   (x)

1       (x)(cid:19) =    + f1(x1) +        + fp(xp),

(9.3)

where again each fj is an unspeci   ed smooth function. while the non-
parametric form for the functions fj makes the model more    exible, the
additivity is retained and allows us to interpret the model in much the
same way as before. the additive id28 model is an example
of a generalized additive model. in general, the conditional mean   (x) of
a response y is related to an additive function of the predictors via a link
function g:

examples of classical link functions are the following:

g[  (x)] =    + f1(x1) +        + fp(xp).

(9.4)

    g(  ) =    is the identity link, used for linear and additive models for

gaussian response data.

    g(  ) = logit(  ) as above, or g(  ) = probit(  ), the probit link function,
for modeling binomial probabilities. the probit function is the inverse
gaussian cumulative distribution function: probit(  ) =      1(  ).

    g(  ) = log(  ) for log-linear or log-additive models for poisson count

data.

all three of these arise from exponential family sampling models, which
in addition include the gamma and negative-binomial distributions. these
families generate the well-known class of generalized linear models, which
are all extended in the same way to generalized additive models.

the functions fj are estimated in a    exible manner, using an algorithm
whose basic building block is a scatterplot smoother. the estimated func-
tion   fj can then reveal possible nonlinearities in the e   ect of xj. not all

9.1 generalized additive models

297

of the functions fj need to be nonlinear. we can easily mix in linear and
other parametric forms with the nonlinear terms, a necessity when some of
the inputs are qualitative variables (factors). the nonlinear terms are not
restricted to main e   ects either; we can have nonlinear components in two
or more variables, or separate curves in xj for each level of the factor xk.
thus each of the following would qualify:

    g(  ) = x t    +   k + f (z)   a semiparametric model, where x is a
vector of predictors to be modeled linearly,   k the e   ect for the kth
level of a qualitative input v , and the e   ect of predictor z is modeled
nonparametrically.

    g(  ) = f (x) + gk(z)   again k indexes the levels of a qualitative
input v , and thus creates an interaction term g(v, z) = gk(z) for
the e   ect of v and z.

    g(  ) = f (x) + g(z, w ) where g is a nonparametric function in two

features.

additive models can replace linear models in a wide variety of settings,

for example an additive decomposition of time series,

yt = st + tt +   t,

(9.5)

where st is a seasonal component, tt is a trend and    is an error term.

9.1.1 fitting additive models

in this section we describe a modular algorithm for    tting additive models
and their generalizations. the building block is the scatterplot smoother
for    tting nonlinear e   ects in a    exible way. for concreteness we use as our
scatterplot smoother the cubic smoothing spline described in chapter 5.

the additive model has the form

y =    +

pxj=1

fj(xj) +   ,

(9.6)

where the error term    has mean zero. given observations xi, yi, a criterion
like the penalized sum of squares (5.9) of section 5.4 can be speci   ed for
this problem,

prss(  , f1, f2, . . . , fp) =

nxi=1 yi         

pxj=1

fj(xij)!2

+

  jz f

pxj=1

      

j (tj)2dtj,

(9.7)
where the   j     0 are tuning parameters. it can be shown that the minimizer
of (9.7) is an additive cubic spline model; each of the functions fj is a

298

9. additive models, trees, and related methods

algorithm 9.1 the back   tting algorithm for additive models.

1. initialize:      = 1

2. cycle: j = 1, 2, . . . , p, . . . , 1, 2, . . . , p, . . . ,

1 yi,   fj     0,   i, j.

npn
  fj     sj"{yi             xk6=j

  fj       fj    

1
n

  fj(xij).

nxi=1

1#,
  fk(xik)}n

until the functions   fj change less than a prespeci   ed threshold.

   accordingly. the standard convention is to assume that pn

cubic spline in the component xj, with knots at each of the unique values
of xij, i = 1, . . . , n . however, without further restrictions on the model,
the solution is not unique. the constant    is not identi   able, since we
can add or subtract any constants to each of the functions fj, and adjust
1 fj(xij) =
0    j   the functions average zero over the data. it is easily seen that      =
ave(yi) in this case. if in addition to this restriction, the matrix of input
values (having ijth entry xij) has full column rank, then (9.7) is a strictly
convex criterion and the minimizer is unique. if the matrix is singular, then
the linear part of the components fj cannot be uniquely determined (while
the nonlinear parts can!)(buja et al., 1989).

furthermore, a simple iterative procedure exists for    nding the solution.
we set      = ave(yi), and it never changes. we apply a cubic smoothing
spline sj to the targets {yi             pk6=j
1 , as a function of xij,
to obtain a new estimate   fj. this is done for each predictor in turn, using
the current estimates of the other functions   fk when computing yi             
pk6=j
  fk(xik). the process is continued until the estimates   fj stabilize. this
procedure, given in detail in algorithm 9.1, is known as    back   tting    and
the resulting    t is analogous to a multiple regression for linear models.

  fk(xik)}n

in principle, the second step in (2) of algorithm 9.1 is not needed, since
the smoothing spline    t to a mean-zero response has mean zero (exer-
cise 9.1). in practice, machine rounding can cause slippage, and the ad-
justment is advised.

this same algorithm can accommodate other    tting methods in exactly

the same way, by specifying appropriate smoothing operators sj:

    other univariate regression smoothers such as local polynomial re-

gression and kernel methods;

9.1 generalized additive models

299

    id75 operators yielding polynomial    ts, piecewise con-

stant    ts, parametric spline    ts, series and fourier    ts;

    more complicated operators such as surface smoothers for second or
higher-order interactions or periodic smoothers for seasonal e   ects.
if we consider the operation of smoother sj only at the training points, it
can be represented by an n    n operator matrix sj (see section 5.4.1).
then the degrees of freedom for the jth term are (approximately) computed
as df j = trace[sj]     1, by analogy with degrees of freedom for smoothers
discussed in chapters 5 and 6.
for a large class of linear smoothers sj, back   tting is equivalent to a
gauss   seidel algorithm for solving a certain linear system of equations.
details are given in exercise 9.2.

for the id28 model and other generalized additive models,
the appropriate criterion is a penalized log-likelihood. to maximize it, the
back   tting procedure is used in conjunction with a likelihood maximizer.
the usual newton   raphson routine for maximizing log-likelihoods in gen-
eralized linear models can be recast as an irls (iteratively reweighted
least squares) algorithm. this involves repeatedly    tting a weighted linear
regression of a working response variable on the covariates; each regression
yields a new value of the parameter estimates, which in turn give new work-
ing responses and weights, and the process is iterated (see section 4.4.1).
in the generalized additive model, the weighted id75 is simply
replaced by a weighted back   tting algorithm. we describe the algorithm in
more detail for id28 below, and more generally in chapter 6
of hastie and tibshirani (1990).

9.1.2 example: additive id28

probably the most widely used model in medical research is the logistic
model for binary data. in this model the outcome y can be coded as 0
or 1, with 1 indicating an event (like death or relapse of a disease) and
0 indicating no event. we wish to model pr(y = 1|x), the id203 of
an event given values of the prognostic factors x t = (x1, . . . , xp). the
goal is usually to understand the roles of the prognostic factors, rather
than to classify new individuals. logistic models are also used in applica-
tions where one is interested in estimating the class probabilities, for use
in risk screening. apart from medical applications, credit risk screening is
a popular application.

the generalized additive logistic model has the form

log

pr(y = 1|x)
pr(y = 0|x)

=    + f1(x1) +        + fp(xp).

(9.8)

the functions f1, f2, . . . , fp are estimated by a back   tting algorithm

within a newton   raphson procedure, shown in algorithm 9.2.

300

9. additive models, trees, and related methods

algorithm 9.2 local scoring algorithm for the additive logistic regres-
sion model.

1. compute starting values:      = log[  y/(1       y)], where   y = ave(yi), the

sample proportion of ones, and set   fj     0    j.
2. de   ne     i =      +pj

  fj(xij) and   pi = 1/[1 + exp(       i)].

iterate:

(a) construct the working target variable

zi =     i +

(yi       pi)
  pi(1       pi)

.

(b) construct weights wi =   pi(1       pi)
(c) fit an additive model to the targets zi with weights wi, us-
ing a weighted back   tting algorithm. this gives new estimates
    ,   fj,    j

3. continue step 2. until the change in the functions falls below a pre-

speci   ed threshold.

the additive model    tting in step (2) of algorithm 9.2 requires a weighted
scatterplot smoother. most smoothing procedures can accept observation
weights (exercise 5.12); see chapter 3 of hastie and tibshirani (1990) for
further details.

the additive id28 model can be generalized further to han-
dle more than two classes, using the multilogit formulation as outlined in
section 4.4. while the formulation is a straightforward extension of (9.8),
the algorithms for    tting such models are more complex. see yee and wild
(1996) for details, and the vgam software currently available from:

http://www.stat.auckland.ac.nz/   yee.

example: predicting email spam

we apply a generalized additive model to the spam data introduced in
chapter 1. the data consists of information from 4601 email messages, in
a study to screen email for    spam    (i.e., junk email). the data is publicly
available at ftp.ics.uci.edu, and was donated by george forman from
hewlett-packard laboratories, palo alto, california.

the response variable is binary, with values email or spam, and there are

57 predictors as described below:

    48 quantitative predictors   the percentage of words in the email that
match a given word. examples include business, address, internet,

9.1 generalized additive models

301

table 9.1. test data confusion matrix for the additive id28 model
   t to the spam training data. the overall test error rate is 5.5%.

true class
email (0)
spam (1)

predicted class

email (0)
58.3%
3.0%

spam (1)
2.5%
36.3%

free, and george. the idea was that these could be customized for
individual users.

    6 quantitative predictors   the percentage of characters in the email
that match a given character. the characters are ch;, ch(, ch[, ch!,
ch$, and ch#.

    the average length of uninterrupted sequences of capital letters:

capave.

    the length of the longest uninterrupted sequence of capital letters:

capmax.

    the sum of the length of uninterrupted sequences of capital letters:

captot.

we coded spam as 1 and email as zero. a test set of size 1536 was randomly
chosen, leaving 3065 observations in the training set. a generalized additive
model was    t, using a cubic smoothing spline with a nominal four degrees of
freedom for each predictor. what this means is that for each predictor xj,
the smoothing-spline parameter   j was chosen so that trace[sj(  j)]   1 = 4,
where sj(  ) is the smoothing spline operator matrix constructed using the
observed values xij, i = 1, . . . , n . this is a convenient way of specifying
the amount of smoothing in such a complex model.

most of the spam predictors have a very long-tailed distribution. before
   tting the gam model, we log-transformed each variable (actually log(x +
0.1)), but the plots in figure 9.1 are shown as a function of the original
variables.

the test error rates are shown in table 9.1; the overall error rate is 5.3%.
by comparison, a linear id28 has a test error rate of 7.6%.
table 9.2 shows the predictors that are highly signi   cant in the additive
model.

for ease of interpretation, in table 9.2 the contribution for each variable
is decomposed into a linear component and the remaining nonlinear com-
ponent. the top block of predictors are positively correlated with spam,
while the bottom block is negatively correlated. the linear component is a
weighted least squares linear    t of the    tted curve on the predictor, while
the nonlinear part is the residual. the linear component of an estimated

302

9. additive models, trees, and related methods

table 9.2. signi   cant predictors from the additive model    t to the spam train-
ing data. the coe   cients represent the linear part of   fj, along with their standard
errors and z-score. the nonlinear p-value is for a test of nonlinearity of   fj.

name

num.

df

coe   cient

std. error z score nonlinear
p -value

our
over
remove
internet
free
business
hpl
ch!
ch$
capmax
captot

hp
george
1999
re
edu

5
6
7
8
16
17
26
52
53
56
57

25
27
37
45
46

3.9
3.9
4.0
4.0
3.9
3.8
3.8
4.0
3.9
3.8
4.0

3.9
3.7
3.8
3.9
4.0

positive e   ects
0.566
0.244
0.949
0.524
0.507
0.779
0.045
0.674
1.419
0.247
0.755
negative e   ects
   1.404
   5.003
   0.672
   0.620
   1.183

0.114
0.195
0.183
0.176
0.127
0.186
0.250
0.128
0.280
0.228
0.165

0.224
0.744
0.191
0.133
0.209

4.970
1.249
5.201
2.974
4.010
4.179
0.181
5.283
5.062
1.080
4.566

   6.262
   6.722
   3.512
   4.649
   5.647

0.052
0.004
0.093
0.028
0.065
0.194
0.002
0.164
0.354
0.000
0.063

0.140
0.045
0.011
0.597
0.000

function is summarized by the coe   cient, standard error and z-score; the
latter is the coe   cient divided by its standard error, and is considered
signi   cant if it exceeds the appropriate quantile of a standard normal dis-
tribution. the column labeled nonlinear p -value is a test of nonlinearity
of the estimated function. note, however, that the e   ect of each predictor
is fully adjusted for the entire e   ects of the other predictors, not just for
their linear parts. the predictors shown in the table were judged signi   -
cant by at least one of the tests (linear or nonlinear) at the p = 0.01 level
(two-sided).

figure 9.1 shows the estimated functions for the signi   cant predictors
appearing in table 9.2. many of the nonlinear e   ects appear to account for
a strong discontinuity at zero. for example, the id203 of spam drops
signi   cantly as the frequency of george increases from zero, but then does
not change much after that. this suggests that one might replace each of
the frequency predictors by an indicator variable for a zero count, and resort
to a linear logistic model. this gave a test error rate of 7.4%; including the
linear e   ects of the frequencies as well dropped the test error to 6.6%. it
appears that the nonlinearities in the additive model have an additional
predictive power.

9.1 generalized additive models

303

0
1

5

0

5
-

)
e
v
o
m
e
r
(
  f

)
t
e
n
r
e
t
n
i
(
  f

0
1

5

0

5
-

0

1

2

3

0

2

4

6

0

2

over

remove

8

6

4
10
internet

0

5
-

0
1
-

)
l
p
h
(
  f

0

5

10
hp

15

20

0

5

10

hpl

0

5
-

0
1
-

)
u
d
e
(
  f

)
p
h
(
  f

)
e
r
(
  f

0

5
-

0
1
-

5

0

5
-

0
1
-

0

2

8

6

4
our

)
r
e
v
o
(
  f

)
s
s
e
n
i
s
u
b
(
  f

5

0

5
-

0
1

5

0

5
-

0

2

6

4
free

8

10

0

2

4

6
business

5

0

5
-

)
9
9
9
1
(
  f

)
r
u
o
(
  f

)
e
e
r
f
(
  f

)
e
g
r
o
e
g
(
  f

)
!
h
c
(
  f

5

0

5
-

0
1

5

0

5
-

5

0

5
-

0
1
-

0
1

5

0

5
-

0

10

20

30

0

2

george

4
1999

6

0

5

10
re

15

20

0

5

15

10
edu

0
1

5

0

5
-

)
$
h
c
(
  f

5

0

5
-

)
x
a
m
p
a
c
(
  f

)
t
o
t
p
a
c
(
  f

5

0

5
-

0

10

20
ch!

30

0

1

2

3

4

5

6

0

2000

6000

10000

0

5000

10000

15000

ch$

capmax

captot

figure 9.1. spam analysis: estimated functions for signi   cant predictors. the
rug plot along the bottom of each frame indicates the observed values of the cor-
responding predictor. for many of the predictors the nonlinearity picks up the
discontinuity at zero.

304

9. additive models, trees, and related methods

it is more serious to classify a genuine email message as spam, since then
a good email would be    ltered out and would not reach the user. we can
alter the balance between the class error rates by changing the losses (see
section 2.4). if we assign a loss l01 for predicting a true class 0 as class 1,
and l10 for predicting a true class 1 as class 0, then the estimated bayes
rule predicts class 1 if its id203 is greater than l01/(l01 + l10). for
example, if we take l01 = 10, l10 = 1 then the (true) class 0 and class 1
error rates change to 0.8% and 8.7%.

more ambitiously, we can encourage the model to    t better data in the
class 0 by using weights l01 for the class 0 observations and l10 for the
class 1 observations. as above, we then use the estimated bayes rule to
predict. this gave error rates of 1.2% and 8.0% in (true) class 0 and class 1,
respectively. we discuss below the issue of unequal losses further, in the
context of tree-based models.

after    tting an additive model, one should check whether the inclusion
of some interactions can signi   cantly improve the    t. this can be done
   manually,    by inserting products of some or all of the signi   cant inputs,
or automatically via the mars procedure (section 9.4).

this example uses the additive model in an automatic fashion. as a data
analysis tool, additive models are often used in a more interactive fashion,
adding and dropping terms to determine their e   ect. by calibrating the
amount of smoothing in terms of df j, one can move seaid113ssly between
linear models (df j = 1) and partially linear models, where some terms are
modeled more    exibly. see hastie and tibshirani (1990) for more details.

9.1.3 summary

additive models provide a useful extension of linear models, making them
more    exible while still retaining much of their interpretability. the familiar
tools for modeling and id136 in linear models are also available for
additive models, seen for example in table 9.2. the back   tting procedure
for    tting these models is simple and modular, allowing one to choose a
   tting method appropriate for each input variable. as a result they have
become widely used in the statistical community.

however additive models can have limitations for large data-mining ap-
plications. the back   tting algorithm    ts all predictors, which is not feasi-
ble or desirable when a large number are available. the bruto procedure
(hastie and tibshirani, 1990, chapter 9) combines back   tting with selec-
tion of inputs, but is not designed for large data-mining problems. there
has also been recent work using lasso-type penalties to estimate sparse ad-
ditive models, for example the cosso procedure of lin and zhang (2006)
and the spam proposal of ravikumar et al. (2008). for large problems a
forward stagewise approach such as boosting (chapter 10) is more e   ective,
and also allows for interactions to be included in the model.

9.2 tree-based methods

305

9.2 tree-based methods

9.2.1 background

tree-based methods partition the feature space into a set of rectangles, and
then    t a simple model (like a constant) in each one. they are conceptually
simple yet powerful. we    rst describe a popular method for tree-based
regression and classi   cation called cart, and later contrast it with c4.5,
a major competitor.

let   s consider a regression problem with continuous response y and in-
puts x1 and x2, each taking values in the unit interval. the top left panel
of figure 9.2 shows a partition of the feature space by lines that are parallel
to the coordinate axes. in each partition element we can model y with a
di   erent constant. however, there is a problem: although each partitioning
line has a simple description like x1 = c, some of the resulting regions are
complicated to describe.

to simplify matters, we restrict attention to recursive binary partitions
like that in the top right panel of figure 9.2. we    rst split the space into
two regions, and model the response by the mean of y in each region.
we choose the variable and split-point to achieve the best    t. then one
or both of these regions are split into two more regions, and this process
is continued, until some stopping rule is applied. for example, in the top
right panel of figure 9.2, we    rst split at x1 = t1. then the region x1     t1
is split at x2 = t2 and the region x1 > t1 is split at x1 = t3. finally, the
region x1 > t3 is split at x2 = t4. the result of this process is a partition
into the    ve regions r1, r2, . . . , r5 shown in the    gure. the corresponding
regression model predicts y with a constant cm in region rm, that is,

  f (x) =

5xm=1

cmi{(x1, x2)     rm}.

(9.9)

this same model can be represented by the binary tree in the bottom left
panel of figure 9.2. the full dataset sits at the top of the tree. observations
satisfying the condition at each junction are assigned to the left branch,
and the others to the right branch. the terminal nodes or leaves of the
tree correspond to the regions r1, r2, . . . , r5. the bottom right panel of
figure 9.2 is a perspective plot of the regression surface from this model.
for illustration, we chose the node means c1 =    5, c2 =    7, c3 = 0, c4 =
2, c5 = 4 to make this plot.
a key advantage of the recursive binary tree is its interpretability. the
feature space partition is fully described by a single tree. with more than
two inputs, partitions like that in the top right panel of figure 9.2 are
di   cult to draw, but the binary tree representation works in the same
way. this representation is also popular among medical scientists, perhaps
because it mimics the way that a doctor thinks. the tree strati   es the

306

9. additive models, trees, and related methods

t4

r5

r4

r3

2
x

t2

r2

r1

t1

t3

x1

2
x

x1

x1     t1

|

x2     t2

x1     t3

x2     t4

r1

r2

r3

x2

x1

r4

r5

figure 9.2. partitions and cart. top right panel shows a partition of a
two-dimensional feature space by recursive binary splitting, as used in cart,
applied to some fake data. top left panel shows a general partition that cannot
be obtained from recursive binary splitting. bottom left panel shows the tree cor-
responding to the partition in the top right panel, and a perspective plot of the
prediction surface appears in the bottom right panel.

9.2 tree-based methods

307

population into strata of high and low outcome, on the basis of patient
characteristics.

9.2.2 regression trees

we now turn to the question of how to grow a regression tree. our data
consists of p inputs and a response, for each of n observations: that is,
(xi, yi) for i = 1, 2, . . . , n , with xi = (xi1, xi2, . . . , xip). the algorithm
needs to automatically decide on the splitting variables and split points,
and also what topology (shape) the tree should have. suppose    rst that we
have a partition into m regions r1, r2, . . . , rm , and we model the response
as a constant cm in each region:

mxm=1

f (x) =

cmi(x     rm).

(9.10)

if we adopt as our criterion minimization of the sum of squares p(yi    

f (xi))2, it is easy to see that the best   cm is just the average of yi in region
rm:

  cm = ave(yi|xi     rm).

(9.11)

now    nding the best binary partition in terms of minimum sum of squares
is generally computationally infeasible. hence we proceed with a greedy
algorithm. starting with all of the data, consider a splitting variable j and
split point s, and de   ne the pair of half-planes

r1(j, s) = {x|xj     s} and r2(j, s) = {x|xj > s}.

(9.12)

then we seek the splitting variable j and split point s that solve

min

j, shmin

c1 xxi   r1(j,s)

(yi     c1)2 + min

c2 xxi   r2(j,s)

(yi     c2)2i.

(9.13)

for any choice j and s, the inner minimization is solved by

  c1 = ave(yi|xi     r1(j, s)) and   c2 = ave(yi|xi     r2(j, s)).

(9.14)

for each splitting variable, the determination of the split point s can
be done very quickly and hence by scanning through all of the inputs,
determination of the best pair (j, s) is feasible.

having found the best split, we partition the data into the two resulting
regions and repeat the splitting process on each of the two regions. then
this process is repeated on all of the resulting regions.

how large should we grow the tree? clearly a very large tree might over   t
the data, while a small tree might not capture the important structure.

308

9. additive models, trees, and related methods

tree size is a tuning parameter governing the model   s complexity, and the
optimal tree size should be adaptively chosen from the data. one approach
would be to split tree nodes only if the decrease in sum-of-squares due to the
split exceeds some threshold. this strategy is too short-sighted, however,
since a seemingly worthless split might lead to a very good split below it.
the preferred strategy is to grow a large tree t0, stopping the splitting
process only when some minimum node size (say 5) is reached. then this
large tree is pruned using cost-complexity pruning, which we now describe.
we de   ne a subtree t     t0 to be any tree that can be obtained by
pruning t0, that is, collapsing any number of its internal (non-terminal)
nodes. we index terminal nodes by m, with node m representing region
rm. let |t| denote the number of terminal nodes in t . letting

nm = #{xi     rm},
yi,
  cm =

1

nm xxi   rm
nm xxi   rm

1

qm(t ) =

(yi       cm)2,

(9.15)

we de   ne the cost complexity criterion

c  (t ) =

|t |xm=1

nmqm(t ) +   |t|.

(9.16)

the idea is to    nd, for each   , the subtree t       t0 to minimize c  (t ).
the tuning parameter        0 governs the tradeo    between tree size and its
goodness of    t to the data. large values of    result in smaller trees t  , and
conversely for smaller values of   . as the notation suggests, with    = 0 the
solution is the full tree t0. we discuss how to adaptively choose    below.
for each    one can show that there is a unique smallest subtree t   that
minimizes c  (t ). to    nd t   we use weakest link pruning: we successively
collapse the internal node that produces the smallest per-node increase in

pm nmqm(t ), and continue until we produce the single-node (root) tree.

this gives a (   nite) sequence of subtrees, and one can show this sequence
must contain t  . see breiman et al. (1984) or ripley (1996) for details.
estimation of    is achieved by    ve- or tenfold cross-validation: we choose
the value      to minimize the cross-validated sum of squares. our    nal tree
is t     .

9.2.3 classi   cation trees

if the target is a classi   cation outcome taking values 1, 2, . . . , k, the only
changes needed in the tree algorithm pertain to the criteria for splitting
nodes and pruning the tree. for regression we used the squared-error node

9.2 tree-based methods

309

e

ntr

o

p

y

gini index

misclassification error

5

.

0

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

0.0

0.2

0.4

0.6

0.8

1.0

p

figure 9.3. node impurity measures for two-class classi   cation, as a function
of the proportion p in class 2. cross-id178 has been scaled to pass through
(0.5, 0.5).

impurity measure qm(t ) de   ned in (9.15), but this is not suitable for
classi   cation. in a node m, representing a region rm with nm observations,
let

  pmk =

i(yi = k),

1

nm xxi   rm

the proportion of class k observations in node m. we classify the obser-
vations in node m to class k(m) = arg maxk   pmk, the majority class in
node m. di   erent measures qm(t ) of node impurity include the following:

misclassi   cation error:

gini index:

1

nmpi   rm
pk6=k      pmk   pmk    =pk

k=1   pmk log   pmk.

i(yi 6= k(m)) = 1       pmk(m).
k=1   pmk(1       pmk).

cross-id178 or deviance:    pk

(9.17)
for two classes, if p is the proportion in the second class, these three mea-
sures are 1     max(p, 1     p), 2p(1     p) and    p log p     (1     p) log (1     p),
respectively. they are shown in figure 9.3. all three are similar, but cross-
id178 and the gini index are di   erentiable, and hence more amenable to
numerical optimization. comparing (9.13) and (9.15), we see that we need
to weight the node impurity measures by the number nml and nmr of
observations in the two child nodes created by splitting node m.

in addition, cross-id178 and the gini index are more sensitive to changes
in the node probabilities than the misclassi   cation rate. for example, in
a two-class problem with 400 observations in each class (denote this by
(400, 400)), suppose one split created nodes (300, 100) and (100, 300), while

310

9. additive models, trees, and related methods

the other created nodes (200, 400) and (200, 0). both splits produce a mis-
classi   cation rate of 0.25, but the second split produces a pure node and is
probably preferable. both the gini index and cross-id178 are lower for the
second split. for this reason, either the gini index or cross-id178 should
be used when growing the tree. to guide cost-complexity pruning, any of
the three measures can be used, but typically it is the misclassi   cation rate.
the gini index can be interpreted in two interesting ways. rather than
classify observations to the majority class in the node, we could classify
them to class k with id203   pmk. then the expected training error

rate of this rule in the node ispk6=k      pmk   pmk      the gini index. similarly,

if we code each observation as 1 for class k and zero otherwise, the variance
over the node of this 0-1 response is   pmk(1       pmk). summing over classes
k again gives the gini index.

9.2.4 other issues

categorical predictors

when splitting a predictor having q possible unordered values, there are
2q   1     1 possible partitions of the q values into two groups, and the com-
putations become prohibitive for large q. however, with a 0     1 outcome,
this computation simpli   es. we order the predictor classes according to the
proportion falling in outcome class 1. then we split this predictor as if it
were an ordered predictor. one can show this gives the optimal split, in
terms of cross-id178 or gini index, among all possible 2q   1   1 splits. this
result also holds for a quantitative outcome and square error loss   the cat-
egories are ordered by increasing mean of the outcome. although intuitive,
the proofs of these assertions are not trivial. the proof for binary outcomes
is given in breiman et al. (1984) and ripley (1996); the proof for quantita-
tive outcomes can be found in fisher (1958). for multicategory outcomes,
no such simpli   cations are possible, although various approximations have
been proposed (loh and vanichsetakul, 1988).

the partitioning algorithm tends to favor categorical predictors with
many levels q; the number of partitions grows exponentially in q, and the
more choices we have, the more likely we can    nd a good one for the data
at hand. this can lead to severe over   tting if q is large, and such variables
should be avoided.

the loss matrix

in classi   cation problems, the consequences of misclassifying observations
are more serious in some classes than others. for example, it is probably
worse to predict that a person will not have a heart attack when he/she
actually will, than vice versa. to account for this, we de   ne a k    k loss
matrix l, with lkk    being the loss incurred for classifying a class k obser-
vation as class k   . typically no loss is incurred for correct classi   cations,

9.2 tree-based methods

311

we could modify the gini index topk6=k    lkk      pmk   pmk   ; this would be the

that is, lkk = 0    k. to incorporate the losses into the modeling process,
expected loss incurred by the randomized rule. this works for the multi-
class case, but in the two-class case has no e   ect, since the coe   cient of
  pmk   pmk    is lkk    + lk   k. for two classes a better approach is to weight the
observations in class k by lkk   . this can be used in the multiclass case only
if, as a function of k, lkk    doesn   t depend on k   . observation weighting can
be used with the deviance as well. the e   ect of observation weighting is to
alter the prior id203 on the classes. in a terminal node, the empirical

bayes rule implies that we classify to class k(m) = arg minkp    l   k   pm   .

missing predictor values

suppose our data has some missing predictor values in some or all of the
variables. we might discard any observation with some missing values, but
this could lead to serious depletion of the training set. alternatively we
might try to    ll in (impute) the missing values, with say the mean of that
predictor over the nonmissing observations. for tree-based models, there
are two better approaches. the    rst is applicable to categorical predictors:
we simply make a new category for    missing.    from this we might dis-
cover that observations with missing values for some measurement behave
di   erently than those with nonmissing values. the second more general
approach is the construction of surrogate variables. when considering a
predictor for a split, we use only the observations for which that predictor
is not missing. having chosen the best (primary) predictor and split point,
we form a list of surrogate predictors and split points. the    rst surrogate
is the predictor and corresponding split point that best mimics the split of
the training data achieved by the primary split. the second surrogate is
the predictor and corresponding split point that does second best, and so
on. when sending observations down the tree either in the training phase
or during prediction, we use the surrogate splits in order, if the primary
splitting predictor is missing. surrogate splits exploit correlations between
predictors to try and alleviate the e   ect of missing data. the higher the cor-
relation between the missing predictor and the other predictors, the smaller
the loss of information due to the missing value. the general problem of
missing data is discussed in section 9.6.

why binary splits?

rather than splitting each node into just two groups at each stage (as
above), we might consider multiway splits into more than two groups. while
this can sometimes be useful, it is not a good general strategy. the problem
is that multiway splits fragment the data too quickly, leaving insu   cient
data at the next level down. hence we would want to use such splits only
when needed. since multiway splits can be achieved by a series of binary
splits, the latter are preferred.

312

9. additive models, trees, and related methods

other tree-building procedures

the discussion above focuses on the cart (classi   cation and regression
tree) implementation of trees. the other popular methodology is   and
its later versions, c4.5 and c5.0 (quinlan, 1993). early versions of the
program were limited to categorical predictors, and used a top-down rule
with no pruning. with more recent developments, c5.0 has become quite
similar to cart. the most signi   cant feature unique to c5.0 is a scheme
for deriving rule sets. after a tree is grown, the splitting rules that de   ne the
terminal nodes can sometimes be simpli   ed: that is, one or more condition
can be dropped without changing the subset of observations that fall in
the node. we end up with a simpli   ed set of rules de   ning each terminal
node; these no longer follow a tree structure, but their simplicity might
make them more attractive to the user.

along linear combinations of the form p ajxj     s. the weights aj and

linear combination splits
rather than restricting splits to be of the form xj     s, one can allow splits
split point s are optimized to minimize the relevant criterion (such as the
gini index). while this can improve the predictive power of the tree, it can
hurt interpretability. computationally, the discreteness of the split point
search precludes the use of a smooth optimization for the weights. a better
way to incorporate linear combination splits is in the hierarchical mixtures
of experts (hme) model, the topic of section 9.5.

instability of trees

one major problem with trees is their high variance. often a small change
in the data can result in a very di   erent series of splits, making interpre-
tation somewhat precarious. the major reason for this instability is the
hierarchical nature of the process: the e   ect of an error in the top split
is propagated down to all of the splits below it. one can alleviate this to
some degree by trying to use a more stable split criterion, but the inherent
instability is not removed. it is the price to be paid for estimating a simple,
tree-based structure from the data. id112 (section 8.7) averages many
trees to reduce this variance.

lack of smoothness

another limitation of trees is the lack of smoothness of the prediction sur-
face, as can be seen in the bottom right panel of figure 9.2. in classi   cation
with 0/1 loss, this doesn   t hurt much, since bias in estimation of the class
probabilities has a limited e   ect. however, this can degrade performance
in the regression setting, where we would normally expect the underlying
function to be smooth. the mars procedure, described in section 9.4,

9.2 tree-based methods

313

table 9.3. spam data: confusion rates for the 17-node tree (chosen by cross   
validation) on the test data. overall error rate is 9.3%.

true
email
spam

predicted

spam
email
4.0%
57.3%
5.3% 33.4%

can be viewed as a modi   cation of cart designed to alleviate this lack of
smoothness.

di   culty in capturing additive structure

another problem with trees is their di   culty in modeling additive struc-
ture. in regression, suppose, for example, that y = c1i(x1 < t1)+c2i(x2 <
t2) +    where    is zero-mean noise. then a binary tree might make its    rst
split on x1 near t1. at the next level down it would have to split both nodes
on x2 at t2 in order to capture the additive structure. this might happen
with su   cient data, but the model is given no special encouragement to    nd
such structure. if there were ten rather than two additive e   ects, it would
take many fortuitous splits to recreate the structure, and the data analyst
would be hard pressed to recognize it in the estimated tree. the    blame   
here can again be attributed to the binary tree structure, which has both
advantages and drawbacks. again the mars method (section 9.4) gives
up this tree structure in order to capture additive structure.

9.2.5 spam example (continued)

we applied the classi   cation tree methodology to the spam example intro-
duced earlier. we used the deviance measure to grow the tree and mis-
classi   cation rate to prune it. figure 9.4 shows the 10-fold cross-validation
error rate as a function of the size of the pruned tree, along with   2 stan-
dard errors of the mean, from the ten replications. the test error curve is
shown in orange. note that the cross-validation error rates are indexed by
a sequence of values of    and not tree size; for trees grown in di   erent folds,
a value of    might imply di   erent sizes. the sizes shown at the base of the
plot refer to |t  |, the sizes of the pruned original tree.
the error    attens out at around 17 terminal nodes, giving the pruned tree
in figure 9.5. of the 13 distinct features chosen by the tree, 11 overlap with
the 16 signi   cant features in the additive model (table 9.2). the overall
error rate shown in table 9.3 is about 50% higher than for the additive
model in table 9.1.

consider the rightmost branches of the tree. we branch to the right
with a spam warning if more than 5.5% of the characters are the $ sign.

314

9. additive models, trees, and related methods

176

21

7

5

3

2

0

  

e

t

a
r
n
o

 

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

4

.

0

3

.

0

2

.

0

1

.

0

0

.

0

0

10

20

tree size

30

40

figure 9.4. results for spam example. the blue curve is the 10-fold cross-val-
idation estimate of misclassi   cation rate as a function of tree size, with standard
error bars. the minimum occurs at a tree size with about 17 terminal nodes (using
the    one-standard-error    rule). the orange curve is the test error, which tracks
the cv error quite closely. the cross-validation is indexed by values of   , shown
above. the tree sizes shown below refer to |t  |, the size of the original tree indexed
by   .

however, if in addition the phrase hp occurs frequently, then this is likely
to be company business and we classify as email. all of the 22 cases in
the test set satisfying these criteria were correctly classi   ed. if the second
condition is not met, and in addition the average length of repeated capital
letters capave is larger than 2.9, then we classify as spam. of the 227 test
cases, only seven were misclassi   ed.

in medical classi   cation problems, the terms sensitivity and speci   city

are used to characterize a rule. they are de   ned as follows:

sensitivity: id203 of predicting disease given true state is disease.

speci   city: id203 of predicting non-disease given true state is non-

disease.

9.2 tree-based methods

315

email
600/1536

ch$<0.0555

email
280/1177

remove<0.06

ch$>0.0555

spam
 48/359

hp<0.405

remove>0.06

hp>0.405

email
180/1065

spam
  9/112

spam
 26/337

email
  0/22

ch!<0.191

george<0.15

capave<2.907

ch!>0.191

george>0.15

capave>2.907

email
 80/861

email
100/204

spam
  6/109

email
  0/3

spam
 19/110

spam
  7/227

george<0.005

capave<2.7505

1999<0.58

george>0.005

capave>2.7505

1999>0.58

email
 80/652

email
  0/209

email
 36/123

spam
 16/81

spam
 18/109

email
  0/1

hp<0.03

free<0.065

hp>0.03

free>0.065

email
 77/423

email
  3/229

email
 16/94

spam
  9/29

capmax<10.5

business<0.145

capmax>10.5

business>0.145

email
 20/238

email
 57/185

email
 14/89

spam
  3/5

receive<0.125

edu<0.045

receive>0.125

edu>0.045

email
 19/236

spam
  1/2

email
 48/113

email
  9/72

our<1.2

our>1.2

email
 37/101

spam
  1/12

figure 9.5. the pruned tree for the spam example. the split variables are
shown in blue on the branches, and the classi   cation is shown in every node.the
numbers under the terminal nodes indicate misclassi   cation rates on the test data.

316

9. additive models, trees, and related methods

0

.

1

   
   

   

   

   

   

y
t
i
v
i
t
i
s
n
e
s

8

.

0

6

.

0

4

.

0

2
0

.

0

.

0

       

                                                                                                                                     

   
   
   

   

tree (0.95)
gam (0.98)
weighted tree (0.90)

   

   
         
   
   
   
   
   

   
      

   

   

   
   
   

0.0

0.2

0.4

0.6

0.8

1.0

specificity

figure 9.6. roc curves for the classi   cation rules    t to the spam data. curves
that are closer to the northeast corner represent better classi   ers. in this case the
gam classi   er dominates the trees. the weighted tree achieves better sensitivity
for higher speci   city than the unweighted tree. the numbers in the legend repre-
sent the area under the curve.

if we think of spam and email as the presence and absence of disease, re-
spectively, then from table 9.3 we have

sensitivity = 100   
speci   city = 100   

33.4

33.4 + 5.3

57.3

57.3 + 4.0

= 86.3%,

= 93.4%.

in this analysis we have used equal losses. as before let lkk    be the
loss associated with predicting a class k object as class k   . by varying the
relative sizes of the losses l01 and l10, we increase the sensitivity and
decrease the speci   city of the rule, or vice versa. in this example, we want
to avoid marking good email as spam, and thus we want the speci   city to
be very high. we can achieve this by setting l01 > 1 say, with l10 = 1.
the bayes    rule in each terminal node classi   es to class 1 (spam) if the
proportion of spam is     l01/(l10 + l01), and class zero otherwise. the

9.3 prim: bump hunting

317

receiver operating characteristic curve (roc) is a commonly used summary
for assessing the tradeo    between sensitivity and speci   city. it is a plot of
the sensitivity versus speci   city as we vary the parameters of a classi   cation
rule. varying the loss l01 between 0.1 and 10, and applying bayes    rule to
the 17-node tree selected in figure 9.4, produced the roc curve shown
in figure 9.6. the standard error of each curve near 0.9 is approximately

p0.9(1     0.9)/1536 = 0.008, and hence the standard error of the di   erence

is about 0.01. we see that in order to achieve a speci   city of close to 100%,
the sensitivity has to drop to about 50%. the area under the curve is a
commonly used quantitative summary; extending the curve linearly in each
direction so that it is de   ned over [0, 100], the area is approximately 0.95.
for comparison, we have included the roc curve for the gam model    t
to these data in section 9.2; it gives a better classi   cation rule for any loss,
with an area of 0.98.

rather than just modifying the bayes rule in the nodes, it is better to
take full account of the unequal losses in growing the tree, as was done
in section 9.2. with just two classes 0 and 1, losses may be incorporated
into the tree-growing process by using weight lk,1   k for an observation in
class k. here we chose l01 = 5, l10 = 1 and    t the same size tree as before
(|t  | = 17). this tree has higher sensitivity at high values of the speci   city
than the original tree, but does more poorly at the other extreme. its top
few splits are the same as the original tree, and then it departs from it.
for this application the tree grown using l01 = 5 is clearly better than the
original tree.

the area under the roc curve, used above, is sometimes called the c-
statistic. interestingly, it can be shown that the area under the roc curve
is equivalent to the mann-whitney u statistic (or wilcoxon rank-sum test),
for the median di   erence between the prediction scores in the two groups
(hanley and mcneil, 1982). for evaluating the contribution of an additional
predictor when added to a standard model, the c-statistic may not be an
informative measure. the new predictor can be very signi   cant in terms
of the change in model deviance, but show only a small increase in the c-
statistic. for example, removal of the highly signi   cant term george from
the model of table 9.2 results in a decrease in the c-statistic of less than
0.01. instead, it is useful to examine how the additional predictor changes
the classi   cation on an individual sample basis. a good discussion of this
point appears in cook (2007).

9.3 prim: bump hunting

tree-based methods (for regression) partition the feature space into box-
shaped regions, to try to make the response averages in each box as di   er-

318

9. additive models, trees, and related methods

ent as possible. the splitting rules de   ning the boxes are related to each
through a binary tree, facilitating their interpretation.

the patient rule induction method (prim) also    nds boxes in the feature
space, but seeks boxes in which the response average is high. hence it looks
for maxima in the target function, an exercise known as bump hunting. (if
minima rather than maxima are desired, one simply works with the negative
response values.)

prim also di   ers from tree-based partitioning methods in that the box
de   nitions are not described by a binary tree. this makes interpretation of
the collection of rules more di   cult; however, by removing the binary tree
constraint, the individual rules are often simpler.

the main box construction method in prim works from the top down,
starting with a box containing all of the data. the box is compressed along
one face by a small amount, and the observations then falling outside the
box are peeled o   . the face chosen for compression is the one resulting in
the largest box mean, after the compression is performed. then the process
is repeated, stopping when the current box contains some minimum number
of data points.

this process is illustrated in figure 9.7. there are 200 data points uni-
formly distributed over the unit square. the color-coded plot indicates the
response y taking the value 1 (red) when 0.5 < x1 < 0.8 and 0.4 < x2 <
0.6. and zero (blue) otherwise. the panels shows the successive boxes found
by the top-down peeling procedure, peeling o    a proportion    = 0.1 of the
remaining data points at each stage.

figure 9.8 shows the mean of the response values in the box, as the box

is compressed.

after the top-down sequence is computed, prim reverses the process,
expanding along any edge, if such an expansion increases the box mean.
this is called pasting. since the top-down procedure is greedy at each step,
such an expansion is often possible.

the result of these steps is a sequence of boxes, with di   erent numbers
of observation in each box. cross-validation, combined with the judgment
of the data analyst, is used to choose the optimal box size.

denote by b1 the indices of the observations in the box found in step 1.
the prim procedure then removes the observations in b1 from the training
set, and the two-step process   top down peeling, followed by bottom-up
pasting   is repeated on the remaining dataset. this entire process is re-
peated several times, producing a sequence of boxes b1, b2, . . . , bk. each
box is de   ned by a set of rules involving a subset of predictors like

(a1     x1     b1) and (b1     x3     b2).

a summary of the prim procedure is given algorithm 9.3.

prim can handle a categorical predictor by considering all partitions of
the predictor, as in cart. missing values are also handled in a manner
similar to cart. prim is designed for regression (quantitative response

9.3 prim: bump hunting

319

1

2

3

4

5

6

7

8

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o
o

o

o
o
o

o
o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o
o

o

o
o
o

o
o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o
o

o

o
o
o

o
o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

o
o

o
o
o

o
o
o
o
o

o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o

o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o

o
o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o

o
o

o
o

o
o
o
o
o
o
o
oo
o
o
o
o
o

o

o

12

17

22

27

figure 9.7. illustration of prim algorithm. there are two classes, indicated
by the blue (class 0) and red (class 1) points. the procedure starts with a rectangle
(broken black lines) surrounding all of the data, and then peels away points along
one edge by a prespeci   ed amount in order to maximize the mean of the points
remaining in the box. starting at the top left panel, the sequence of peelings is
shown, until a pure red region is isolated in the bottom right panel. the iteration
number is indicated at the top of each panel.

   
   
   
   
   
   
   

   

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

n
a
e
m
 
x
o
b

                                 

   

   

   

   

   

50

100

number of observations in box

   

   

150

   

figure 9.8. box mean as a function of number of observations in the box.

320

9. additive models, trees, and related methods

algorithm 9.3 patient rule induction method.

1. start with all of the training data, and a maximal box containing all

of the data.

2. consider shrinking the box by compressing one face, so as to peel o   
the proportion    of observations having either the highest values of
a predictor xj, or the lowest. choose the peeling that produces the
highest response mean in the remaining box. (typically    = 0.05 or
0.10.)

3. repeat step 2 until some minimal number of observations (say 10)

remain in the box.

4. expand the box along any face, as long as the resulting box mean

increases.

5. steps 1   4 give a sequence of boxes, with di   erent numbers of obser-
vations in each box. use cross-validation to choose a member of the
sequence. call the box b1.

6. remove the data in box b1 from the dataset and repeat steps 2   5 to
obtain a second box, and continue to get as many boxes as desired.

variable); a two-class outcome can be handled simply by coding it as 0 and
1. there is no simple way to deal with k > 2 classes simultaneously: one
approach is to run prim separately for each class versus a baseline class.
an advantage of prim over cart is its patience. because of its bi-
nary splits, cart fragments the data quite quickly. assuming splits of
equal size, with n observations it can only make log2(n )     1 splits before
running out of data. if prim peels o    a proportion    of training points
at each stage, it can perform approximately     log(n )/ log(1       ) peeling
steps before running out of data. for example, if n = 128 and    = 0.10,
then log2(n )    1 = 6 while     log(n )/ log(1      )     46. taking into account
that there must be an integer number of observations at each stage, prim
in fact can peel only 29 times. in any case, the ability of prim to be more
patient should help the top-down greedy algorithm    nd a better solution.

9.3.1 spam example (continued)

we applied prim to the spam data, with the response coded as 1 for spam
and 0 for email.

the    rst two boxes found by prim are summarized below:

9.4 mars: multivariate adaptive regression splines

321

rule 1
training
test

rule 2
training
test

global mean box mean box support
0.1413
0.1536

0.3931
0.3958

0.9607
1.0000

rule 1

ch! > 0.029
capave > 2.331
your > 0.705
1999 < 0.040
captot > 79.50
edu < 0.070
re < 0.535
ch; < 0.030

                                                                     

remain mean box mean box support
0.1043
0.1061

0.2998
0.2862

0.9560
0.9264

rule 2(cid:26) remove > 0.010

george < 0.110

the box support is the proportion of observations falling in the box.
the    rst box is purely spam, and contains about 15% of the test data.
the second box contains 10.6% of the test observations, 92.6% of which
are spam. together the two boxes contain 26% of the data and are about
97% spam. the next few boxes (not shown) are quite small, containing only
about 3% of the data.

the predictors are listed in order of importance. interestingly the top
splitting variables in the cart tree (figure 9.5) do not appear in prim   s
   rst box.

9.4 mars: multivariate adaptive regression

splines

mars is an adaptive procedure for regression, and is well suited for high-
dimensional problems (i.e., a large number of inputs). it can be viewed as a
generalization of stepwise id75 or a modi   cation of the cart
method to improve the latter   s performance in the regression setting. we
introduce mars from the    rst point of view, and later make the connection
to cart.

mars uses expansions in piecewise linear basis functions of the form

(x     t)+ and (t     x)+. the    +    means positive part, so

(x   t)+ =(cid:26) x     t,

0,

if x > t,
otherwise,

and

(t   x)+ =(cid:26) t     x,

0,

if x < t,
otherwise.

322

9. additive models, trees, and related methods

n
o
i
t
c
n
u
f

s
i
s
a
b

5
0

.

4
0

.

3

.

0

2

.

0

1
0

.

.

0
0

(t     x)+

(x     t)+

0.0

0.2

0.4

0.6

0.8

1.0

t
x

figure 9.9. the basis functions (x     t)+ (solid orange) and (t     x)+ (broken
blue) used by mars.

as an example, the functions (x     0.5)+ and (0.5     x)+ are shown in fig-
ure 9.9.
each function is piecewise linear, with a knot at the value t. in the
terminology of chapter 5, these are linear splines. we call the two functions
a re   ected pair in the discussion below. the idea is to form re   ected pairs
for each input xj with knots at each observed value xij of that input.
therefore, the collection of basis functions is

c = {(xj     t)+, (t     xj)+} t     {x1j , x2j , . . . , xn j }

j = 1, 2, . . . , p.

(9.18)

if all of the input values are distinct, there are 2n p basis functions alto-
gether. note that although each basis function depends only on a single
xj, for example, h(x) = (xj     t)+, it is considered as a function over the
entire input space irp.
the model-building strategy is like a forward stepwise id75,
but instead of using the original inputs, we are allowed to use functions
from the set c and their products. thus the model has the form

f (x) =   0 +

  mhm(x),

(9.19)

mxm=1

where each hm(x) is a function in c, or a product of two or more such
functions.
given a choice for the hm, the coe   cients   m are estimated by minimiz-
ing the residual sum-of-squares, that is, by standard id75. the
real art, however, is in the construction of the functions hm(x). we start
with only the constant function h0(x) = 1 in our model, and all functions
in the set c are candidate functions. this is depicted in figure 9.10.
at each stage we consider as a new basis function pair all products of a
function hm in the model set m with one of the re   ected pairs in c. we
add to the model m the term of the form

    m +1h   (x)    (xj     t)+ +     m +2h   (x)    (t     xj)+, h        m,

9.4 mars: multivariate adaptive regression splines

323

constant

x2

x2

x1

x1

x2

xp

x1

x2

xp

x1

x2

xp

figure 9.10. schematic of the mars forward model-building procedure. on
the left are the basis functions currently in the model: initially, this is the constant
function h(x) = 1. on the right are all candidate basis functions to be considered
in building the model. these are pairs of piecewise linear basis functions as in
figure 9.9, with knots t at all unique observed values xij of each predictor xj.
at each stage we consider all products of a candidate pair with a basis function
in the model. the product that decreases the residual error the most is added into
the current model. above we illustrate the    rst three steps of the procedure, with
the selected functions shown in red.

324

9. additive models, trees, and related methods

h(x1, x2)

x2

x1

figure 9.11. the function h(x1, x2) = (x1     x51)+    (x72     x2)+, resulting
from multiplication of two piecewise linear mars basis functions.

that produces the largest decrease in training error. here     m +1 and     m +2
are coe   cients estimated by least squares, along with all the other m + 1
coe   cients in the model. then the winning products are added to the
model and the process is continued until the model set m contains some
preset maximum number of terms.
for example, at the    rst stage we consider adding to the model a function
of the form   1(xj     t)+ +   2(t     xj)+; t     {xij}, since multiplication by
the constant function just produces the function itself. suppose the best
choice is     1(x2     x72)+ +     2(x72     x2)+. then this pair of basis functions
is added to the set m, and at the next stage we consider including a pair
of products the form

hm(x)    (xj     t)+

and

hm(x)    (t     xj)+, t     {xij},

where for hm we have the choices

h0(x) = 1,
h1(x) = (x2     x72)+, or
h2(x) = (x72     x2)+.

the third choice produces functions such as (x1     x51)+    (x72     x2)+,
depicted in figure 9.11.
at the end of this process we have a large model of the form (9.19). this
model typically over   ts the data, and so a backward deletion procedure
is applied. the term whose removal causes the smallest increase in resid-
ual squared error is deleted from the model at each stage, producing an
estimated best model   f   of each size (number of terms)   . one could use
cross-validation to estimate the optimal value of   , but for computational

9.4 mars: multivariate adaptive regression splines

325

savings the mars procedure instead uses generalized cross-validation. this
criterion is de   ned as

gcv(  ) = pn

i=1(yi       f  (xi))2
(1     m (  )/n )2

.

(9.20)

the value m (  ) is the e   ective number of parameters in the model: this
accounts both for the number of terms in the models, plus the number
of parameters used in selecting the optimal positions of the knots. some
mathematical and simulation results suggest that one should pay a price
of three parameters for selecting a knot in a piecewise id75.

thus if there are r linearly independent basis functions in the model, and
k knots were selected in the forward process, the formula is m (  ) = r+ck,
where c = 3. (when the model is restricted to be additive   details below   
a penalty of c = 2 is used). using this, we choose the model along the
backward sequence that minimizes gcv(  ).

why these piecewise linear basis functions, and why this particular model
strategy? a key property of the functions of figure 9.9 is their ability to
operate locally; they are zero over part of their range. when they are mul-
tiplied together, as in figure 9.11, the result is nonzero only over the small
part of the feature space where both component functions are nonzero. as
a result, the regression surface is built up parsimoniously, using nonzero
components locally   only where they are needed. this is important, since
one should    spend    parameters carefully in high dimensions, as they can
run out quickly. the use of other basis functions such as polynomials, would
produce a nonzero product everywhere, and would not work as well.

the second important advantage of the piecewise linear basis function
concerns computation. consider the product of a function in m with each
of the n re   ected pairs for an input xj. this appears to require the    tting
of n single-input id75 models, each of which uses o(n ) oper-
ations, making a total of o(n 2) operations. however, we can exploit the
simple form of the piecewise linear function. we    rst    t the re   ected pair
with rightmost knot. as the knot is moved successively one position at a
time to the left, the basis functions di   er by zero over the left part of the
domain, and by a constant over the right part. hence after each such move
we can update the    t in o(1) operations. this allows us to try every knot
in only o(n ) operations.

the forward modeling strategy in mars is hierarchical, in the sense that
multiway products are built up from products involving terms already in
the model. for example, a four-way product can only be added to the model
if one of its three-way components is already in the model. the philosophy
here is that a high-order interaction will likely only exist if some of its lower-
order    footprints    exist as well. this need not be true, but is a reasonable
working assumption and avoids the search over an exponentially growing
space of alternatives.

326

9. additive models, trees, and related methods

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

 
t
s
e
t

4
0

.

3
0

.

2
0

.

1
0

.

0.055

   

   
      

                                                                                                                                                                                                                                                                                                

gcv choice

0

20

40

60

80

100

rank of model

figure 9.12. spam data: test error misclassi   cation rate for the mars pro-
cedure, as a function of the rank (number of independent basis functions) in the
model.

there is one restriction put on the formation of model terms: each input
can appear at most once in a product. this prevents the formation of
higher-order powers of an input, which increase or decrease too sharply
near the boundaries of the feature space. such powers can be approximated
in a more stable way with piecewise linear functions.

a useful option in the mars procedure is to set an upper limit on
the order of interaction. for example, one can set a limit of two, allowing
pairwise products of piecewise linear functions, but not three- or higher-
way products. this can aid in the interpretation of the    nal model. an
upper limit of one results in an additive model.

9.4.1 spam example (continued)

we applied mars to the    spam    data analyzed earlier in this chapter. to
enhance interpretability, we restricted mars to second-degree interactions.
although the target is a two-class variable, we used the squared-error loss
function nonetheless (see section 9.4.3). figure 9.12 shows the test error
misclassi   cation rate as a function of the rank (number of independent ba-
sis functions) in the model. the error rate levels o    at about 5.5%, which is
slightly higher than that of the generalized additive model (5.3%) discussed
earlier. gcv chose a model size of 60, which is roughly the smallest model
giving optimal performance. the leading interactions found by mars in-
volved inputs (ch$, remove), (ch$, free) and (hp, captot). however, these
interactions give no improvement in performance over the generalized ad-
ditive model.

9.4 mars: multivariate adaptive regression splines

327

9.4.2 example (simulated data)

here we examine the performance of mars in three contrasting scenarios.
there are n = 100 observations, and the predictors x1, x2, . . . , xp and
errors    have independent standard normal distributions.

scenario 1: the data generation model is

y = (x1     1)+ + (x1     1)+    (x2     .8)+ + 0.12      .

(9.21)

the noise standard deviation 0.12 was chosen so that the signal-to-
noise ratio was about 5. we call this the tensor-product scenario; the
product term gives a surface that looks like that of figure 9.11.

scenario 2: this is the same as scenario 1, but with p = 20 total predictors;

that is, there are 18 inputs that are independent of the response.

scenario 3: this has the structure of a neural network:

   1 = x1 + x2 + x3 + x4 + x5,
   2 = x6     x7 + x8     x9 + x10,
y =   (   1) +   (   2) + 0.12      .

  (t) = 1/(1 + e   t),

(9.22)

scenarios 1 and 2 are ideally suited for mars, while scenario 3 contains
high-order interactions and may be di   cult for mars to approximate. we
ran    ve simulations from each model, and recorded the results.

in scenario 1, mars typically uncovered the correct model almost per-
fectly. in scenario 2, it found the correct structure but also found a few
extraneous terms involving other predictors.

let   (x) be the true mean of y , and let

mse0 = avex   test(  y       (x))2,
mse = avex   test(   f (x)       (x))2.

(9.23)

these represent the mean-square error of the constant model and the    tted
mars model, estimated by averaging at the 1000 test values of x. table 9.4
shows the proportional decrease in model error or r2 for each scenario:

r2 =

mse0     mse

mse0

.

(9.24)

the values shown are means and standard error over the    ve simulations.
the performance of mars is degraded only slightly by the inclusion of the
useless inputs in scenario 2; it performs substantially worse in scenario 3.

328

9. additive models, trees, and related methods

table 9.4. proportional decrease in model error (r2) when mars is applied
to three di   erent scenarios.

scenario
1: tensor product p = 2
2: tensor product p = 20
3: neural network

mean (s.e.)
0.97 (0.01)
0.96 (0.01)
0.79 (0.01)

9.4.3 other issues

mars for classi   cation

the mars method and algorithm can be extended to handle classi   cation
problems. several strategies have been suggested.

for two classes, one can code the output as 0/1 and treat the problem as
a regression; we did this for the spam example. for more than two classes,
one can use the indicator response approach described in section 4.2. one
codes the k response classes via 0/1 indicator variables, and then per-
forms a multi-response mars regression. for the latter we use a common
set of basis functions for all response variables. classi   cation is made to
the class with the largest predicted response value. there are, however, po-
tential masking problems with this approach, as described in section 4.2.
a generally superior approach is the    optimal scoring    method discussed
in section 12.5.

stone et al. (1997) developed a hybrid of mars called polymars specif-
ically designed to handle classi   cation problems. it uses the multiple logistic
framework described in section 4.4. it grows the model in a forward stage-
wise fashion like mars, but at each stage uses a quadratic approximation
to the multinomial log-likelihood to search for the next basis-function pair.
once found, the enlarged model is    t by maximum likelihood, and the
process is repeated.

relationship of mars to cart

although they might seem quite di   erent, the mars and cart strategies
actually have strong similarities. suppose we take the mars procedure and
make the following changes:

    replace the piecewise linear basis functions by step functions i(x   t >

0) and i(x     t     0).

    when a model term is involved in a multiplication by a candidate
term, it gets replaced by the interaction, and hence is not available
for further interactions.

with these changes, the mars forward procedure is the same as the cart
tree-growing algorithm. multiplying a step function by a pair of re   ected

9.5 hierarchical mixtures of experts

329

step functions is equivalent to splitting a node at the step. the second
restriction implies that a node may not be split more than once, and leads
to the attractive binary-tree representation of the cart model. on the
other hand, it is this restriction that makes it di   cult for cart to model
additive structures. mars forgoes the tree structure and gains the ability
to capture additive e   ects.

mixed inputs

mars can handle    mixed    predictors   quantitative and qualitative   in a
natural way, much like cart does. mars considers all possible binary
partitions of the categories for a qualitative predictor into two groups.
each such partition generates a pair of piecewise constant basis functions   
indicator functions for the two sets of categories. this basis pair is now
treated as any other, and is used in forming tensor products with other
basis functions already in the model.

9.5 hierarchical mixtures of experts

the hierarchical mixtures of experts (hme) procedure can be viewed as a
variant of tree-based methods. the main di   erence is that the tree splits
are not hard decisions but rather soft probabilistic ones. at each node an
observation goes left or right with probabilities depending on its input val-
ues. this has some computational advantages since the resulting parameter
optimization problem is smooth, unlike the discrete split point search in the
tree-based approach. the soft splits might also help in prediction accuracy
and provide a useful alternative description of the data.

there are other di   erences between hmes and the cart implementa-
tion of trees. in an hme, a linear (or id28) model is    t in
each terminal node, instead of a constant as in cart. the splits can be
multiway, not just binary, and the splits are probabilistic functions of a
linear combination of inputs, rather than a single input as in the standard
use of cart. however, the relative merits of these choices are not clear,
and most were discussed at the end of section 9.2.

a simple two-level hme model in shown in figure 9.13. it can be thought
of as a tree with soft splits at each non-terminal node. however, the inven-
tors of this methodology use a di   erent terminology. the terminal nodes
are called experts, and the non-terminal nodes are called gating networks.
the idea is that each expert provides an opinion (prediction) about the
response, and these are combined together by the gating networks. as we
will see, the model is formally a mixture model, and the two-level model
in the    gure can be extend to multiple levels, hence the name hierarchical
mixtures of experts.

330

9. additive models, trees, and related methods

gating
gating
gating
network
network
network

g1

g2

gating
gating
gating
network
network
network

gating
gating
gating
network
network
network

g1|1

g2|1

g1|2

g2|2

expert
expert
expert
network
network
network

expert
expert
expert
network
network
network

expert
expert
network
network

expert
expert
expert
network
network
network

pr(y|x,   11)

pr(y|x,   21)

pr(y|x,   12)

pr(y|x,   22)

figure 9.13. a two-level hierarchical mixture of experts (hme) model.

consider the regression or classi   cation problem, as described earlier in
the chapter. the data is (xi, yi), i = 1, 2, . . . , n , with yi either a continuous
or binary-valued response, and xi a vector-valued input. for ease of nota-
tion we assume that the    rst element of xi is one, to account for intercepts.
here is how an hme is de   ned. the top gating network has the output

e  t
j x
k=1 e  t
k x

pk

gj(x,   j) =

, j = 1, 2, . . . , k,

(9.25)

where each   j is a vector of unknown parameters. this represents a soft
k-way split (k = 2 in figure 9.13.) each gj(x,   j) is the id203 of
assigning an observation with feature vector x to the jth branch. notice
that with k = 2 groups, if we take the coe   cient of one of the elements of
x to be +   , then we get a logistic curve with in   nite slope. in this case,
the gating probabilities are either 0 or 1, corresponding to a hard split on
that input.

at the second level, the gating networks have a similar form:

g   |j(x,   j   ) =

e  t
j   x
k=1 e  t
jkx

pk

,     = 1, 2, . . . , k.

(9.26)

9.5 hierarchical mixtures of experts

331

this is the id203 of assignment to the    th branch, given assignment
to the jth branch at the level above.

at each expert (terminal node), we have a model for the response variable

of the form

this di   ers according to the problem.

y     pr(y|x,   j   ).

(9.27)

regression: the gaussian id75 model is used, with   j    =

(  j   ,   2

j   ):

y =   t

j   x +    and        n (0,   2

j   ).

classi   cation: the linear id28 model is used:

pr(y = 1|x,   j   ) =

1

1 + e     t

j   x

.

(9.28)

(9.29)

denoting the collection of all parameters by    = {  j,   j   ,   j   }, the total

id203 that y = y is

pr(y|x,   ) =

kxj=1

gj(x,   j)

kx   =1

g   |j(x,   j   )pr(y|x,   j   ).

(9.30)

this is a mixture model, with the mixture probabilities determined by the
gating network models.

to estimate the parameters, we maximize the log-likelihood of the data,

pi log pr(yi|xi,   ), over the parameters in   . the most convenient method

for doing this is the em algorithm, which we describe for mixtures in
section 8.5. we de   ne latent variables    j, all of which are zero except for
a single one. we interpret these as the branching decisions made by the top
level gating network. similarly we de   ne latent variables       |j to describe
the gating decisions at the second level.

in the e-step, the em algorithm computes the expectations of the    j
and       |j given the current values of the parameters. these expectations
are then used as observation weights in the m-step of the procedure, to
estimate the parameters in the expert networks. the parameters in the
internal nodes are estimated by a version of multiple id28.
the expectations of the    j or       |j are id203 pro   les, and these are
used as the response vectors for these id28s.

the hierarchical mixtures of experts approach is a promising competitor
to cart trees. by using soft splits rather than hard decision rules it can
capture situations where the transition from low to high response is gradual.
the log-likelihood is a smooth function of the unknown weights and hence
is amenable to numerical optimization. the model is similar to cart with
linear combination splits, but the latter is more di   cult to optimize. on

332

9. additive models, trees, and related methods

the other hand, to our knowledge there are no methods for    nding a good
tree topology for the hme model, as there are in cart. typically one uses
a    xed tree of some depth, possibly the output of the cart procedure.
the emphasis in the research on hmes has been on prediction rather than
interpretation of the    nal model. a close cousin of the hme is the latent
class model (lin et al., 2000), which typically has only one layer; here
the nodes or latent classes are interpreted as groups of subjects that show
similar response behavior.

9.6 missing data

it is quite common to have observations with missing values for one or more
input features. the usual approach is to impute (   ll-in) the missing values
in some way.

however, the    rst issue in dealing with the problem is determining wheth-
er the missing data mechanism has distorted the observed data. roughly
speaking, data are missing at random if the mechanism resulting in its
omission is independent of its (unobserved) value. a more precise de   nition
is given in little and rubin (2002). suppose y is the response vector and x
is the n    p matrix of inputs (some of which are missing). denote by xobs
the observed entries in x and let z = (y, x), zobs = (y, xobs). finally, if r
is an indicator matrix with ijth entry 1 if xij is missing and zero otherwise,
then the data is said to be missing at random (mar) if the distribution of
r depends on the data z only through zobs:

pr(r|z,   ) = pr(r|zobs,   ).

(9.31)

here    are any parameters in the distribution of r. data are said to be
missing completely at random (mcar) if the distribution of r doesn   t
depend on the observed or missing data:

pr(r|z,   ) = pr(r|  ).

(9.32)

mcar is a stronger assumption than mar: most imputation methods rely
on mcar for their validity.

for example, if a patient   s measurement was not taken because the doctor
felt he was too sick, that observation would not be mar or mcar. in this
case the missing data mechanism causes our observed training data to give a
distorted picture of the true population, and data imputation is dangerous
in this instance. often the determination of whether features are mcar
must be made from information about the data collection process. for
categorical features, one way to diagnose this problem is to code    missing   
as an additional class. then we    t our model to the training data and see
if class    missing    is predictive of the response.

9.6 missing data

333

assuming the features are missing completely at random, there are a

number of ways of proceeding:

1. discard observations with any missing values.

2. rely on the learning algorithm to deal with missing values in its

training phase.

3. impute all missing values before training.

approach (1) can be used if the relative amount of missing data is small,
but otherwise should be avoided. regarding (2), cart is one learning
algorithm that deals e   ectively with missing values, through surrogate splits
(section 9.2.4). mars and prim use similar approaches. in generalized
additive modeling, all observations missing for a given input feature are
omitted when the partial residuals are smoothed against that feature in
the back   tting algorithm, and their    tted values are set to zero. since the
   tted curves have mean zero (when the model includes an intercept), this
amounts to assigning the average    tted value to the missing observations.
for most learning methods, the imputation approach (3) is necessary.
the simplest tactic is to impute the missing value with the mean or median
of the nonmissing values for that feature. (note that the above procedure
for generalized additive models is analogous to this.)

if the features have at least some moderate degree of dependence, one
can do better by estimating a predictive model for each feature given the
other features and then imputing each missing value by its prediction from
the model. in choosing the learning method for imputation of the features,
one must remember that this choice is distinct from the method used for
predicting y from x. thus a    exible, adaptive method will often be pre-
ferred, even for the eventual purpose of carrying out a id75 of y
on x. in addition, if there are many missing feature values in the training
set, the learning method must itself be able to deal with missing feature
values. cart therefore is an ideal choice for this imputation    engine.   

after imputation, missing values are typically treated as if they were ac-
tually observed. this ignores the uncertainty due to the imputation, which
will itself introduce additional uncertainty into estimates and predictions
from the response model. one can measure this additional uncertainty by
doing multiple imputations and hence creating many di   erent training sets.
the predictive model for y can be    t to each training set, and the variation
across training sets can be assessed. if cart was used for the imputation
engine, the multiple imputations could be done by sampling from the values
in the corresponding terminal nodes.

334

9. additive models, trees, and related methods

9.7 computational considerations

with n observations and p predictors, additive model    tting requires some
number mp of applications of a one-dimensional smoother or regression
method. the required number of cycles m of the back   tting algorithm is
usually less than 20 and often less than 10, and depends on the amount
of correlation in the inputs. with cubic smoothing splines, for example,
n log n operations are needed for an initial sort and n operations for the
spline    t. hence the total operations for an additive model    t is pn log n +
mpn .

trees require pn log n operations for an initial sort for each predictor,
and typically another pn log n operations for the split computations. if the
splits occurred near the edges of the predictor ranges, this number could
increase to n 2p.

mars requires n m2 + pmn operations to add a basis function to a
model with m terms already present, from a pool of p predictors. hence to
build an m -term model requires n m 3 + pm 2n computations, which can
be quite prohibitive if m is a reasonable fraction of n .

each of the components of an hme are typically inexpensive to    t at
each m-step: n p2 for the regressions, and n p2k 2 for a k-class logistic
regression. the em algorithm, however, can take a long time to converge,
and so sizable hme models are considered costly to    t.

bibliographic notes

the most comprehensive source for generalized additive models is the text
of that name by hastie and tibshirani (1990). di   erent applications of
this work in medical problems are discussed in hastie et al. (1989) and
hastie and herman (1990), and the software implementation in splus is
described in chambers and hastie (1991). green and silverman (1994)
discuss penalization and spline models in a variety of settings. efron and
tibshirani (1991) give an exposition of modern developments in statistics
(including generalized additive models), for a nonmathematical audience.
classi   cation and regression trees date back at least as far as morgan and
sonquist (1963). we have followed the modern approaches of breiman et
al. (1984) and quinlan (1993). the prim method is due to friedman
and fisher (1999), while mars is introduced in friedman (1991), with an
additive precursor in friedman and silverman (1989). hierarchical mixtures
of experts were proposed in jordan and jacobs (1994); see also jacobs et
al. (1991).

exercises

335

exercises

ex. 9.1 show that a smoothing spline    t of yi to xi preserves the linear
part of the    t. in other words, if yi =   yi + ri, where   yi represents the
id75    ts, and s is the smoothing matrix, then sy =   y + sr.
show that the same is true for local id75 (section 6.1.1). hence
argue that the adjustment step in the second line of (2) in algorithm 9.1
is unnecessary.
ex. 9.2 let a be a known k    k matrix, b be a known k-vector, and z
be an unknown k-vector. a gauss   seidel algorithm for solving the linear
system of equations az = b works by successively solving for element zj in
the jth equation,    xing all other zj   s at their current guesses. this process
is repeated for j = 1, 2, . . . , k, 1, 2, . . . , k, . . . , until convergence (golub and
van loan, 1983).

(a) consider an additive model with n observations and p terms, with
the jth term to be    t by a linear smoother sj. consider the following
system of equations:

               

s1 s1
i
s2
i
s2
...
...
...
sp sp sp

       s1
       s2
...
. . .
      
i

               

               

f1
f2
...
fp

               

=               

s1y
s2y
...
spy

               

.

(9.33)

here each fj is an n -vector of evaluations of the jth function at
the data points, and y is an n -vector of the response values. show
that back   tting is a blockwise gauss   seidel algorithm for solving this
system of equations.

(b) let s1 and s2 be symmetric smoothing operators (matrices) with
eigenvalues in [0, 1). consider a back   tting algorithm with response
vector y and smoothers s1, s2. show that with any starting values,
the algorithm converges and give a formula for the    nal iterates.

ex. 9.3 back   tting equations. consider a back   tting procedure with orthog-
onal projections, and let d be the overall regression matrix whose columns
span v = lcol(s1)     lcol(s2)                lcol(sp), where lcol(s) denotes the
column space of a matrix s. show that the estimating equations

               

i
s1 s1
s2
s2
i
...
...
...
sp sp sp

       s1
       s2
...
. . .
      
i

               

               

f1
f2
...
fp

               

=               

s1y
s2y
...
spy

               

are equivalent to the least squares normal equations dt d   = dt y where
   is the vector of coe   cients.

336

9. additive models, trees, and related methods

ex. 9.4 suppose the same smoother s is used to estimate both terms in a
two-term additive model (i.e., both variables are identical). assume that s
is symmetric with eigenvalues in [0, 1). show that the back   tting residual
converges to (i + s)   1(i     s)y, and that the residual sum of squares con-
verges upward. can the residual sum of squares converge upward in less
structured situations? how does this    t compare to the    t with a single
term    t by s? [hint: use the eigen-decomposition of s to help with this
comparison.]

ex. 9.5 degrees of freedom of a tree. given data yi with mean f (xi) and
variance   2, and a    tting operation y       y, let   s de   ne the degrees of
consider a    t   y estimated by a regression tree,    t to a set of predictors

freedom of a    t bypi cov(yi,   yi)/  2.

x1, x2, . . . , xp.

(a) in terms of the number of terminal nodes m, give a rough formula for

the degrees of freedom of the    t.

(b) generate 100 observations with predictors x1, x2, . . . , x10 as inde-

pendent standard gaussian variates and    x these values.

(c) generate response values also as standard gaussian (  2 = 1), indepen-
dent of the predictors. fit regression trees to the data of    xed size 1,5
and 10 terminal nodes and hence estimate the degrees of freedom of
each    t. [do ten simulations of the response and average the results,
to get a good estimate of degrees of freedom.]

(d) compare your estimates of degrees of freedom in (a) and (c) and

discuss.

(e) if the regression tree    t were a linear operation, we could write   y = sy
for some matrix s. then the degrees of freedom would be tr(s).
suggest a way to compute an approximate s matrix for a regression
tree, compute it and compare the resulting degrees of freedom to
those in (a) and (c).

ex. 9.6 consider the ozone data of figure 6.9.

(a) fit an additive model to the cube root of ozone concentration. as a
function of temperature, wind speed, and radiation. compare your
results to those obtained via the trellis display in figure 6.9.

(b) fit trees, mars, and prim to the same data, and compare the results

to those found in (a) and in figure 6.9.

10
boosting and additive trees

this is page 337
printer: opaque this

10.1 boosting methods

boosting is one of the most powerful learning ideas introduced in the last
twenty years. it was originally designed for classi   cation problems, but as
will be seen in this chapter, it can pro   tably be extended to regression
as well. the motivation for boosting was a procedure that combines the
outputs of many    weak    classi   ers to produce a powerful    committee.   
from this perspective boosting bears a resemblance to id112 and other
committee-based approaches (section 8.8). however we shall see that the
connection is at best super   cial and that boosting is fundamentally di   er-
ent.

we begin by describing the most popular boosting algorithm due to
freund and schapire (1997) called    adaboost.m1.    consider a two-class
problem, with the output variable coded as y     {   1, 1}. given a vector of
predictor variables x, a classi   er g(x) produces a prediction taking one
of the two values {   1, 1}. the error rate on the training sample is

err =

1
n

nxi=1

i(yi 6= g(xi)),

and the expected error rate on future predictions is exy i(y 6= g(x)).
a weak classi   er is one whose error rate is only slightly better than
random guessing. the purpose of boosting is to sequentially apply the
weak classi   cation algorithm to repeatedly modi   ed versions of the data,
thereby producing a sequence of weak classi   ers gm(x), m = 1, 2, . . . , m .

338

10. boosting and additive trees

final classifier

g(x) = signhpm

m=1   mgm(x)i

weighted  sample
weighted  sample
weighted  sample
weighted  sample
weighted  sample

gm (x)

weighted  sample
weighted  sample
weighted  sample
weighted  sample
weighted  sample

g3(x)

weighted  sample
weighted  sample
weighted  sample
weighted  sample
weighted  sample

g2(x)

training  sample
training  sample
training  sample
training  sample
training  sample

g1(x)

figure 10.1. schematic of adaboost. classi   ers are trained on weighted ver-
sions of the dataset, and then combined to produce a    nal prediction.

the predictions from all of them are then combined through a weighted
majority vote to produce the    nal prediction:

g(x) = sign  mxm=1

  mgm(x)! .

(10.1)

here   1,   2, . . . ,   m are computed by the boosting algorithm, and weight
the contribution of each respective gm(x). their e   ect is to give higher
in   uence to the more accurate classi   ers in the sequence. figure 10.1 shows
a schematic of the adaboost procedure.

the data modi   cations at each boosting step consist of applying weights
w1, w2, . . . , wn to each of the training observations (xi, yi), i = 1, 2, . . . , n .
initially all of the weights are set to wi = 1/n , so that the    rst step simply
trains the classi   er on the data in the usual manner. for each successive
iteration m = 2, 3, . . . , m the observation weights are individually modi-
   ed and the classi   cation algorithm is reapplied to the weighted observa-
tions. at step m, those observations that were misclassi   ed by the classi   er
gm   1(x) induced at the previous step have their weights increased, whereas
the weights are decreased for those that were classi   ed correctly. thus as
iterations proceed, observations that are di   cult to classify correctly re-
ceive ever-increasing in   uence. each successive classi   er is thereby forced

10.1 boosting methods

339

algorithm 10.1 adaboost.m1.

1. initialize the observation weights wi = 1/n, i = 1, 2, . . . , n .

2. for m = 1 to m :

(a) fit a classi   er gm(x) to the training data using weights wi.
(b) compute

i=1 wii(yi 6= gm(xi))

.

i=1 wi
(c) compute   m = log((1     errm)/errm).
(d) set wi     wi    exp[  m    i(yi 6= gm(xi))], i = 1, 2, . . . , n .

errm = pn

pn
m=1   mgm(x)i.

3. output g(x) = signhpm

to concentrate on those training observations that are missed by previous
ones in the sequence.

algorithm 10.1 shows the details of the adaboost.m1 algorithm. the
current classi   er gm(x) is induced on the weighted observations at line 2a.
the resulting weighted error rate is computed at line 2b. line 2c calculates
the weight   m given to gm(x) in producing the    nal classi   er g(x) (line
3). the individual weights of each of the observations are updated for the
next iteration at line 2d. observations misclassi   ed by gm(x) have their
weights scaled by a factor exp(  m), increasing their relative in   uence for
inducing the next classi   er gm+1(x) in the sequence.

the adaboost.m1 algorithm is known as    discrete adaboost    in fried-
man et al. (2000), because the base classi   er gm(x) returns a discrete class
label. if the base classi   er instead returns a real-valued prediction (e.g.,
a id203 mapped to the interval [   1, 1]), adaboost can be modi   ed
appropriately (see    real adaboost    in friedman et al. (2000)).
the power of adaboost to dramatically increase the performance of even
a very weak classi   er is illustrated in figure 10.2. the features x1, . . . , x10
are standard independent gaussian, and the deterministic target y is de-
   ned by

y =(cid:26) 1

   1

j=1 x 2
otherwise.

ifp10

j >   2

10(0.5),

(10.2)

here   2
10(0.5) = 9.34 is the median of a chi-squared random variable with
10 degrees of freedom (sum of squares of 10 standard gaussians). there are
2000 training cases, with approximately 1000 cases in each class, and 10,000
test observations. here the weak classi   er is just a    stump   : a two terminal-
node classi   cation tree. applying this classi   er alone to the training data
set yields a very poor test set error rate of 45.8%, compared to 50% for

340

10. boosting and additive trees

single stump

244 node tree

r
o
r
r

e

 
t
s
e
t

5

.

0

4

.

0

3
0

.

2

.

0

1

.

0

0
0

.

0

100

200

300

400

boosting iterations

figure 10.2. simulated data (10.2): test error rate for boosting with stumps,
as a function of the number of iterations. also shown are the test error rate for
a single stump, and a 244-node classi   cation tree.

random guessing. however, as boosting iterations proceed the error rate
steadily decreases, reaching 5.8% after 400 iterations. thus, boosting this
simple very weak classi   er reduces its prediction error rate by almost a
factor of four. it also outperforms a single large classi   cation tree (error
rate 24.7%). since its introduction, much has been written to explain the
success of adaboost in producing accurate classi   ers. most of this work
has centered on using classi   cation trees as the    base learner    g(x), where
improvements are often most dramatic. in fact, breiman (nips workshop,
1996) referred to adaboost with trees as the    best o   -the-shelf classi   er in
the world    (see also breiman (1998)). this is especially the case for data-
mining applications, as discussed more fully in section 10.7 later in this
chapter.

10.1.1 outline of this chapter

here is an outline of the developments in this chapter:

    we show that adaboost    ts an additive model in a base learner,
optimizing a novel exponential id168. this id168 is

10.2 boosting fits an additive model

341

very similar to the (negative) binomial log-likelihood (sections 10.2   
10.4).

    the population minimizer of the exponential id168 is shown

to be the log-odds of the class probabilities (section 10.5).

    we describe id168s for regression and classi   cation that are

more robust than squared error or exponential loss (section 10.6).

    it is argued that id90 are an ideal base learner for data

mining applications of boosting (sections 10.7 and 10.9).

    we develop a class of gradient boosted models (gbms), for boosting

trees with any id168 (section 10.10).

    the importance of    slow learning    is emphasized, and implemented
by shrinkage of each new term that enters the model (section 10.12),
as well as randomization (section 10.12.2).

    tools for interpretation of the    tted model are described (section 10.13).

10.2 boosting fits an additive model

mxm=1

the success of boosting is really not very mysterious. the key lies in ex-
pression (10.1). boosting is a way of    tting an additive expansion in a set
of elementary    basis    functions. here the basis functions are the individual
classi   ers gm(x)     {   1, 1}. more generally, basis function expansions take
the form

f (x) =

  mb(x;   m),

(10.3)

where   m, m = 1, 2, . . . , m are the expansion coe   cients, and b(x;   )     ir
are usually simple functions of the multivariate argument x, characterized
by a set of parameters   . we discuss basis expansions in some detail in
chapter 5.

additive expansions like this are at the heart of many of the learning

techniques covered in this book:

    in single-hidden-layer neural networks (chapter 11), b(x;   ) =   (  0 +
1 x), where   (t) = 1/(1 + e   t) is the sigmoid function, and    param-
  t
eterizes a linear combination of the input variables.

    in signal processing, wavelets (section 5.9.1) are a popular choice with
   parameterizing the location and scale shifts of a    mother    wavelet.
    multivariate adaptive regression splines (section 9.4) uses truncated-
power spline basis functions where    parameterizes the variables and
values for the knots.

342

10. boosting and additive trees

algorithm 10.2 forward stagewise additive modeling.

1. initialize f0(x) = 0.

2. for m = 1 to m :

(a) compute

(  m,   m) = arg min
  ,  

nxi=1

l(yi, fm   1(xi) +   b(xi;   )).

(b) set fm(x) = fm   1(x) +   mb(x;   m).

    for trees,    parameterizes the split variables and split points at the

internal nodes, and the predictions at the terminal nodes.

typically these models are    t by minimizing a id168 averaged
over the training data, such as the squared-error or a likelihood-based loss
function,

min

{  m,  m}m
1

nxi=1

l yi,

mxm=1

  mb(xi;   m)! .

(10.4)

for many id168s l(y, f (x)) and/or basis functions b(x;   ), this re-
quires computationally intensive numerical optimization techniques. how-
ever, a simple alternative often can be found when it is feasible to rapidly
solve the subproblem of    tting just a single basis function,

min
  ,  

nxi=1

l (yi,   b(xi;   )) .

(10.5)

10.3 forward stagewise additive modeling

forward stagewise modeling approximates the solution to (10.4) by sequen-
tially adding new basis functions to the expansion without adjusting the
parameters and coe   cients of those that have already been added. this is
outlined in algorithm 10.2. at each iteration m, one solves for the optimal
basis function b(x;   m) and corresponding coe   cient   m to add to the cur-
rent expansion fm   1(x). this produces fm(x), and the process is repeated.
previously added terms are not modi   ed.

for squared-error loss

l(y, f (x)) = (y     f (x))2,

(10.6)

10.4 exponential loss and adaboost

343

one has

l(yi, fm   1(xi) +   b(xi;   )) = (yi     fm   1(xi)       b(xi;   ))2

= (rim       b(xi;   ))2,

(10.7)

where rim = yi     fm   1(xi) is simply the residual of the current model
on the ith observation. thus, for squared-error loss, the term   mb(x;   m)
that best    ts the current residuals is added to the expansion at each step.
this idea is the basis for    least squares    regression boosting discussed in
section 10.10.2. however, as we show near the end of the next section,
squared-error loss is generally not a good choice for classi   cation; hence
the need to consider other loss criteria.

10.4 exponential loss and adaboost

we now show that adaboost.m1 (algorithm 10.1) is equivalent to forward
stagewise additive modeling (algorithm 10.2) using the id168

l(y, f (x)) = exp(   y f (x)).

(10.8)

the appropriateness of this criterion is addressed in the next section.

for adaboost the basis functions are the individual classi   ers gm(x)    

{   1, 1}. using the exponential id168, one must solve

(  m, gm) = arg min
  ,g

nxi=1

exp[   yi(fm   1(xi) +    g(xi))]

for the classi   er gm and corresponding coe   cient   m to be added at each
step. this can be expressed as

(  m, gm) = arg min
  ,g

nxi=1

w(m)

i

exp(      yi g(xi))

(10.9)

i

= exp(   yi fm   1(xi)). since each w(m)

with w(m)
depends neither on   
nor g(x), it can be regarded as a weight that is applied to each observa-
tion. this weight depends on fm   1(xi), and so the individual weight values
change with each iteration m.

i

the solution to (10.9) can be obtained in two steps. first, for any value

of    > 0, the solution to (10.9) for gm(x) is

gm = arg min
g

nxi=1

w(m)

i

i(yi 6= g(xi)),

(10.10)

344

10. boosting and additive trees

which is the classi   er that minimizes the weighted error rate in predicting
y. this can be easily seen by expressing the criterion in (10.9) as

e         xyi=g(xi)

w(m)

i + e      xyi6=g(xi)

w(m)

i

,

which in turn can be written as

(cid:0)e       e     (cid:1)   

nxi=1

w(m)

i

i(yi 6= g(xi)) + e        

w(m)

i

.

nxi=1

plugging this gm into (10.9) and solving for    one obtains

  m =

1
2

log

1     errm
errm

,

where errm is the minimized weighted error rate

(10.11)

(10.12)

errm = pn

i=1 w(m)

i

i(yi 6= gm(xi))
i=1 w(m)

i

pn

the approximation is then updated

fm(x) = fm   1(x) +   mgm(x),

which causes the weights for the next iteration to be
   e     myigm(xi).

= w(m)

w(m+1)

i

i

.

(10.13)

(10.14)

using the fact that    yigm(xi) = 2    i(yi 6= gm(xi))     1, (10.14) becomes
(10.15)

= w(m)

w(m+1)

   e  mi(yi6=gm(xi))    e     m ,

i

i

where   m = 2  m is the quantity de   ned at line 2(c) of adaboost.m1
(algorithm 10.1). the factor e     m in (10.15) multiplies all weights by the
same value, so it has no e   ect. thus (10.15) is equivalent to line 2(d) of
algorithm 10.1.

one can view line 2(a) of the adaboost.m1 algorithm as a method for
approximately solving the minimization in (10.11) and hence (10.10). hence
we conclude that adaboost.m1 minimizes the exponential loss criterion
(10.8) via a forward-stagewise additive modeling approach.

figure 10.3 shows the training-set misclassi   cation error rate and aver-
age exponential loss for the simulated data problem (10.2) of figure 10.2.
the training-set misclassi   cation error decreases to zero at around 250 it-
erations (and remains there), but the exponential loss keeps decreasing.
notice also in figure 10.2 that the test-set misclassi   cation error continues
to improve after iteration 250. clearly adaboost is not optimizing training-
set misclassi   cation error; the exponential loss is more sensitive to changes
in the estimated class probabilities.

10.5 why exponential loss?

345

0

.

1

8
0

.

6

.

0

r
o
r
r

i

 

e
g
n
n
a
r
t

i

4
0

.

2
.

0

.

0
0

exponential loss

misclassification rate

0

100

200

300

400

boosting iterations

figure 10.3. simulated data, boosting with stumps: misclassi   cation error
rate on the training set, and average exponential loss: (1/n )pn
i=1 exp(   yif (xi)).
after about 250 iterations, the misclassi   cation error is zero, while the exponential
loss continues to decrease.

10.5 why exponential loss?

the adaboost.m1 algorithm was originally motivated from a very di   er-
ent perspective than presented in the previous section. its equivalence to
forward stagewise additive modeling based on exponential loss was only
discovered    ve years after its inception. by studying the properties of the
exponential loss criterion, one can gain insight into the procedure and dis-
cover ways it might be improved.

the principal attraction of exponential loss in the context of additive
modeling is computational; it leads to the simple modular reweighting ad-
aboost algorithm. however, it is of interest to inquire about its statistical
properties. what does it estimate and how well is it being estimated? the
   rst question is answered by seeking its population minimizer.

it is easy to show (friedman et al., 2000) that

f    (x) = arg min
f (x)

ey |x(e   y f (x)) =

1
2

log

pr(y = 1|x)
pr(y =    1|x)

,

(10.16)

346

10. boosting and additive trees

or equivalently

pr(y = 1|x) =

1

1 + e   2f    (x) .

thus, the additive expansion produced by adaboost is estimating one-
half the log-odds of p (y = 1|x). this justi   es using its sign as the classi   -
cation rule in (10.1).
another loss criterion with the same population minimizer is the bi-
nomial negative log-likelihood or deviance (also known as cross-id178),
interpreting f as the logit transform. let

p(x) = pr(y = 1| x) =

ef (x)

e   f (x) + ef (x) =

1

1 + e   2f (x)

(10.17)

and de   ne y     = (y + 1)/2     {0, 1}. then the binomial log-likelihood loss
function is

l(y, p(x)) = y     log p(x) + (1     y    ) log(1     p(x)),

or equivalently the deviance is

   l(y, f (x)) = log(cid:16)1 + e   2y f (x)(cid:17) .

(10.18)

since the population maximizer of log-likelihood is at the true probabilities
p(x) = pr(y = 1| x), we see from (10.17) that the population minimizers of
the deviance ey |x[   l(y, f (x))] and ey |x[e   y f (x)] are the same. thus, using
either criterion leads to the same solution at the population level. note that
e   y f itself is not a proper log-likelihood, since it is not the logarithm of
any id203 mass function for a binary random variable y     {   1, 1}.

10.6 id168s and robustness

in this section we examine the di   erent id168s for classi   cation and
regression more closely, and characterize them in terms of their robustness
to extreme data.

robust id168s for classi   cation

although both the exponential (10.8) and binomial deviance (10.18) yield
the same solution when applied to the population joint distribution, the
same is not true for    nite data sets. both criteria are monotone decreasing
functions of the    margin    yf (x). in classi   cation (with a    1/1 response)
the margin plays a role analogous to the residuals y   f (x) in regression. the
classi   cation rule g(x) = sign[f (x)] implies that observations with positive
margin yif (xi) > 0 are classi   ed correctly whereas those with negative
margin yif (xi) < 0 are misclassi   ed. the decision boundary is de   ned by

10.6 id168s and robustness

347

misclassification
exponential
binomial deviance
squared error
support vector

s
s
o
l

0

.

3

5

.

2

0
2

.

5
1

.

0

.

1

5

.

0

0
0

.

   2

   1

1

2

0

yf

figure 10.4. id168s for two-class classi   cation. the response is
y =   1; the prediction is f , with class prediction sign(f ). the losses are
misclassi   cation: i(sign(f ) 6= y); exponential: exp(   yf ); binomial deviance:
log(1 + exp(   2yf )); squared error: (y     f )2; and support vector: (1     yf )+ (see
section 12.3). each function has been scaled so that it passes through the point
(0, 1).

f (x) = 0. the goal of the classi   cation algorithm is to produce positive
margins as frequently as possible. any loss criterion used for classi   cation
should penalize negative margins more heavily than positive ones since
positive margin observations are already correctly classi   ed.

figure 10.4 shows both the exponential (10.8) and binomial deviance
criteria as a function of the margin yf (x). also shown is misclassi   cation
loss l(y, f (x)) = i(yf (x) < 0), which gives unit penalty for negative mar-
gin values, and no penalty at all for positive ones. both the exponential
and deviance loss can be viewed as monotone continuous approximations
to misclassi   cation loss. they continuously penalize increasingly negative
margin values more heavily than they reward increasingly positive ones.
the di   erence between them is in degree. the penalty associated with bi-
nomial deviance increases linearly for large increasingly negative margin,
whereas the exponential criterion increases the in   uence of such observa-
tions exponentially.

at any point in the training process the exponential criterion concen-
trates much more in   uence on observations with large negative margins.
binomial deviance concentrates relatively less in   uence on such observa-

348

10. boosting and additive trees

tions, more evenly spreading the in   uence among all of the data. it is
therefore far more robust in noisy settings where the bayes error rate is
not close to zero, and especially in situations where there is misspeci   cation
of the class labels in the training data. the performance of adaboost has
been empirically observed to dramatically degrade in such situations.

also shown in the    gure is squared-error loss. the minimizer of the cor-

responding risk on the population is

f    (x) = arg min
f (x)

ey |x(y    f (x))2 = e(y | x) = 2  pr(y = 1| x)   1. (10.19)

as before the classi   cation rule is g(x) = sign[f (x)]. squared-error loss
is not a good surrogate for misclassi   cation error. as seen in figure 10.4, it
is not a monotone decreasing function of increasing margin yf (x). for mar-
gin values yif (xi) > 1 it increases quadratically, thereby placing increasing
in   uence (error) on observations that are correctly classi   ed with increas-
ing certainty, thereby reducing the relative in   uence of those incorrectly
classi   ed yif (xi) < 0. thus, if class assignment is the goal, a monotone de-
creasing criterion serves as a better surrogate id168. figure 12.4 on
page 426 in chapter 12 includes a modi   cation of quadratic loss, the    hu-
berized    square hinge loss (rosset et al., 2004b), which enjoys the favorable
properties of the binomial deviance, quadratic loss and the id166 hinge loss.
it has the same population minimizer as the quadratic (10.19), is zero for
yf (x) > 1, and becomes linear for yf (x) <    1. since quadratic functions
are easier to compute with than exponentials, our experience suggests this
to be a useful alternative to the binomial deviance.

with k-class classi   cation, the response y takes values in the unordered
set g = {g1, . . . ,gk} (see sections 2.4 and 4.4). we now seek a classi   er
g(x) taking values in g. it is su   cient to know the class conditional proba-
bilities pk(x) = pr(y = gk|x), k = 1, 2, . . . , k, for then the bayes classi   er
is
(10.20)

p   (x).

g(x) = gk where k = arg max

   

in principal, though, we need not learn the pk(x), but simply which one is
largest. however, in data mining applications the interest is often more in
the class probabilities p   (x),     = 1, . . . , k themselves, rather than in per-
forming a class assignment. as in section 4.4, the logistic model generalizes
naturally to k classes,

pk(x) =

,

(10.21)

efk(x)
l=1 efl(x)

pk

which ensures that 0     pk(x)     1 and that they sum to one. note that
here we have k di   erent functions, one per class. there is a redundancy
in the functions fk(x), since adding an arbitrary h(x) to each leaves the
model unchanged. traditionally one of them is set to zero: for example,

10.6 id168s and robustness

349

fk (x) = 0, as in (4.17). here we prefer to retain the symmetry, and impose
k=1 fk(x) = 0. the binomial deviance extends naturally

to the k-class multinomial deviance id168:

the constraint pk

l(y, p(x)) =    

=    

kxk=1
kxk=1

i(y = gk) log pk(x)

i(y = gk)fk(x) + log  kx   =1

ef   (x)! . (10.22)

as in the two-class case, the criterion (10.22) penalizes incorrect predictions
only linearly in their degree of incorrectness.

zhu et al. (2005) generalize the exponential loss for k-class problems.

see exercise 10.5 for details.

robust id168s for regression

in the regression setting, analogous to the relationship between exponential
loss and binomial log-likelihood is the relationship between squared-error
loss l(y, f (x)) = (y   f (x))2 and absolute loss l(y, f (x)) = | y   f (x)|. the
population solutions are f (x) = e(y |x) for squared-error loss, and f (x) =
median(y |x) for absolute loss; for symmetric error distributions these are
the same. however, on    nite samples squared-error loss places much more
emphasis on observations with large absolute residuals | yi     f (xi)| during
the    tting process. it is thus far less robust, and its performance severely
degrades for long-tailed error distributions and especially for grossly mis-
measured y-values (   outliers   ). other more robust criteria, such as abso-
lute loss, perform much better in these situations. in the statistical ro-
bustness literature, a variety of regression loss criteria have been proposed
that provide strong resistance (if not absolute immunity) to gross outliers
while being nearly as e   cient as least squares for gaussian errors. they
are often better than either for error distributions with moderately heavy
tails. one such criterion is the huber loss criterion used for m-regression
(huber, 1964)

l(y, f (x)) =(cid:26)

[y     f (x)]2

2  | y     f (x)|       2

for | y     f (x)|       ,
otherwise.

(10.23)

figure 10.5 compares these three id168s.

these considerations suggest that when robustness is a concern, as is
especially the case in data mining applications (see section 10.7), squared-
error loss for regression and exponential loss for classi   cation are not the
best criteria from a statistical perspective. however, they both lead to the
elegant modular boosting algorithms in the context of forward stagewise
additive modeling. for squared-error loss one simply    ts the base learner
to the residuals from the current model yi     fm   1(xi) at each step. for

350

10. boosting and additive trees

squared error
absolute error
huber

s
s
o
l

8

6

4

2

0

   3

   2

   1

1

2

3

0

y     f

figure 10.5. a comparison of three id168s for regression, plotted as a
function of the margin y   f . the huber id168 combines the good properties
of squared-error loss near zero and absolute error loss when |y     f| is large.

exponential loss one performs a weighted    t of the base learner to the
output values yi, with weights wi = exp(   yifm   1(xi)). using other more
robust criteria directly in their place does not give rise to such simple
feasible boosting algorithms. however, in section 10.10.2 we show how one
can derive simple elegant boosting algorithms based on any di   erentiable
loss criterion, thereby producing highly robust boosting procedures for data
mining.

10.7

   o   -the-shelf    procedures for data mining

predictive learning is an important aspect of data mining. as can be seen
from this book, a wide variety of methods have been developed for predic-
tive learning from data. for each particular method there are situations
for which it is particularly well suited, and others where it performs badly
compared to the best that can be done with that data. we have attempted
to characterize appropriate situations in our discussions of each of the re-
spective methods. however, it is seldom known in advance which procedure
will perform best or even well for any given problem. table 10.1 summarizes
some of the characteristics of a number of learning methods.

industrial and commercial data mining applications tend to be especially
challenging in terms of the requirements placed on learning procedures.
data sets are often very large in terms of number of observations and
number of variables measured on each of them. thus, computational con-

10.7    o   -the-shelf    procedures for data mining

351

table 10.1. some characteristics of di   erent learning methods. key:    = good,
   =fair, and    =poor.

characteristic

neural

id166 trees mars

id92,

nets

kernels

natural handling of data
of    mixed    type

handling of missing values

robustness to outliers in
input space

insensitive to monotone
transformations of inputs

computational scalability
(large n )

ability to deal with irrel-
evant inputs

ability to extract linear
combinations of features

interpretability

predictive power

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

   

siderations play an important role. also, the data are usually messy: the
inputs tend to be mixtures of quantitative, binary, and categorical vari-
ables, the latter often with many levels. there are generally many missing
values, complete observations being rare. distributions of numeric predic-
tor and response variables are often long-tailed and highly skewed. this
is the case for the spam data (section 9.1.2); when    tting a generalized
additive model, we    rst log-transformed each of the predictors in order to
get a reasonable    t. in addition they usually contain a substantial fraction
of gross mis-measurements (outliers). the predictor variables are generally
measured on very di   erent scales.

in data mining applications, usually only a small fraction of the large
number of predictor variables that have been included in the analysis are
actually relevant to prediction. also, unlike many applications such as pat-
tern recognition, there is seldom reliable domain knowledge to help create
especially relevant features and/or    lter out the irrelevant ones, the inclu-
sion of which dramatically degrades the performance of many methods.

in addition, data mining applications generally require interpretable mod-
els. it is not enough to simply produce predictions. it is also desirable to
have information providing qualitative understanding of the relationship

352

10. boosting and additive trees

between joint values of the input variables and the resulting predicted re-
sponse value. thus, black box methods such as neural networks, which can
be quite useful in purely predictive settings such as pattern recognition,
are far less useful for data mining.

these requirements of speed, interpretability and the messy nature of
the data sharply limit the usefulness of most learning procedures as o   -
the-shelf methods for data mining. an    o   -the-shelf    method is one that
can be directly applied to the data without requiring a great deal of time-
consuming id174 or careful tuning of the learning procedure.
of all the well-known learning methods, id90 come closest to
meeting the requirements for serving as an o   -the-shelf procedure for data
mining. they are relatively fast to construct and they produce interpretable
models (if the trees are small). as discussed in section 9.2, they naturally
incorporate mixtures of numeric and categorical predictor variables and
missing values. they are invariant under (strictly monotone) transforma-
tions of the individual predictors. as a result, scaling and/or more general
transformations are not an issue, and they are immune to the e   ects of pre-
dictor outliers. they perform internal feature selection as an integral part
of the procedure. they are thereby resistant, if not completely immune,
to the inclusion of many irrelevant predictor variables. these properties of
id90 are largely the reason that they have emerged as the most
popular learning method for data mining.

trees have one aspect that prevents them from being the ideal tool for
predictive learning, namely inaccuracy. they seldom provide predictive ac-
curacy comparable to the best that can be achieved with the data at hand.
as seen in section 10.1, boosting id90 improves their accuracy,
often dramatically. at the same time it maintains most of their desirable
properties for data mining. some advantages of trees that are sacri   ced by
boosting are speed, interpretability, and, for adaboost, robustness against
overlapping class distributions and especially mislabeling of the training
data. a gradient boosted model (gbm) is a generalization of tree boosting
that attempts to mitigate these problems, so as to produce an accurate and
e   ective o   -the-shelf procedure for data mining.

10.8 example: spam data

before we go into the details of gradient boosting, we demonstrate its abili-
ties on a two-class classi   cation problem. the spam data are introduced in
chapter 1, and used as an example for many of the procedures in chapter 9
(sections 9.1.2, 9.2.5, 9.3.1 and 9.4.1).

applying gradient boosting to these data resulted in a test error rate of
4.5%, using the same test set as was used in section 9.1.2. by comparison,
an additive id28 achieved 5.5%, a cart tree fully grown and

10.9 boosting trees

353

pruned by cross-validation 8.7%, and mars 5.5%. the standard error of
these estimates is around 0.6%, although gradient boosting is signi   cantly
better than all of them using the mcnemar test (exercise 10.6).

in section 10.13 below we develop a relative importance measure for
each predictor, as well as a partial dependence plot describing a predictor   s
contribution to the    tted model. we now illustrate these for the spam data.
figure 10.6 displays the relative importance spectrum for all 57 predictor
variables. clearly some predictors are more important than others in sep-
arating spam from email. the frequencies of the character strings !, $, hp,
and remove are estimated to be the four most relevant predictor variables.
at the other end of the spectrum, the character strings 857, 415, table, and
3d have virtually no relevance.

the quantity being modeled here is the log-odds of spam versus email

f (x) = log

pr(spam|x)
pr(email|x)

(10.24)

(see section 10.13 below). figure 10.7 shows the partial dependence of the
log-odds on selected important predictors, two positively associated with
spam (! and remove), and two negatively associated (edu and hp). these
particular dependencies are seen to be essentially monotonic. there is a
general agreement with the corresponding functions found by the additive
id28 model; see figure 9.1 on page 303.

running a gradient boosted model on these data with j = 2 terminal-
node trees produces a purely additive (main e   ects) model for the log-
odds, with a corresponding error rate of 4.7%, as compared to 4.5% for the
full gradient boosted model (with j = 5 terminal-node trees). although
not signi   cant, this slightly higher error rate suggests that there may be
interactions among some of the important predictor variables. this can
be diagnosed through two-variable partial dependence plots. figure 10.8
shows one of the several such plots displaying strong interaction e   ects.

one sees that for very low frequencies of hp, the log-odds of spam are
greatly increased. for high frequencies of hp, the log-odds of spam tend to
be much lower and roughly constant as a function of !. as the frequency
of hp decreases, the functional relationship with ! strengthens.

10.9 boosting trees

regression and classi   cation trees are discussed in detail in section 9.2.
they partition the space of all joint predictor variable values into disjoint
regions rj, j = 1, 2, . . . , j, as represented by the terminal nodes of the tree.
a constant   j is assigned to each such region and the predictive rule is

x     rj     f (x) =   j.

354

10. boosting and additive trees

3d
addresses
labs
telnet
857
415
direct
cs
table
85
#
parts
credit
[
lab
conference
report
original
data
project
font
make
address
order
all
hpl
technology
people
pm
mail
over
650
;
meeting
email
000
internet
receive
(
re
business
1999
will
money
our
you
edu
captot
george
capmax
your
capave
free
remove
hp
$
!

0

20

40

60

80

100

relative importance

figure 10.6. predictor variable importance spectrum for the spam data. the
variable names are written on the vertical axis.

0.0

0.2

0.4

0.8

1.0

0.6
!

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
-

2

.

0

0

.

0

2

.

0
-

6

.

0
-

0

.

1
-

10.9 boosting trees

355

0.0

0.2

0.4
remove

0.6

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
-

2

.

0

0

.

0

.

2
0
-

6

.

0
-

0

.

1
-

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.5

1.0

edu

2.0

2.5

3.0

1.5
hp

figure 10.7. partial dependence of log-odds of spam on four important pre-
dictors. the red ticks at the base of the plots are deciles of the input variable.

 1.0

 0.5

 0.0

-0.5

-1.0

1.0

0.8

!

0.6

0.4

0.2

3.0

2.5

2.0

1.0

0.5

1.5

hp

figure 10.8. partial dependence of the log-odds of spam vs. email as a func-
tion of joint frequencies of hp and the character !.

356

10. boosting and additive trees

thus a tree can be formally expressed as

t (x;   ) =

jxj=1

  ji(x     rj),

(10.25)

with parameters    = {rj,   j}j
the parameters are found by minimizing the empirical risk

1 . j is usually treated as a meta-parameter.

     = arg min
  

jxj=1 xxi   rj

l(yi,   j).

(10.26)

this is a formidable combinatorial optimization problem, and we usually
settle for approximate suboptimal solutions. it is useful to divide the opti-
mization problem into two parts:

finding   j given rj: given the rj, estimating the   j is typically trivial,
and often     j =   yj, the mean of the yi falling in region rj. for mis-
classi   cation loss,     j is the modal class of the observations falling in
region rj.

finding rj: this is the di   cult part, for which approximate solutions are
found. note also that    nding the rj entails estimating the   j as well.
a typical strategy is to use a greedy, top-down recursive partitioning
algorithm to    nd the rj. in addition, it is sometimes necessary to
approximate (10.26) by a smoother and more convenient criterion for
optimizing the rj:

     = arg min
  

nxi=1

  l(yi, t (xi,   )).

(10.27)

then given the   rj =   rj, the   j can be estimated more precisely
using the original criterion.

in section 9.2 we described such a strategy for classi   cation trees. the gini
index replaced misclassi   cation loss in the growing of the tree (identifying
the rj).

the boosted tree model is a sum of such trees,

fm (x) =

mxm=1

t (x;   m),

(10.28)

induced in a forward stagewise manner (algorithm 10.2). at each step in
the forward stagewise procedure one must solve

    m = arg min
  m

nxi=1

l (yi, fm   1(xi) + t (xi;   m))

(10.29)

10.9 boosting trees

357

for the region set and constants   m = {rjm,   jm}jm
the current model fm   1(x).

1

of the next tree, given

given the regions rjm,    nding the optimal constants   jm in each region

is typically straightforward:

    jm = arg min

  jm xxi   rjm

l (yi, fm   1(xi) +   jm) .

(10.30)

finding the regions is di   cult, and even more di   cult than for a single
tree. for a few special cases, the problem simpli   es.

for squared-error loss, the solution to (10.29) is no harder than for a
single tree. it is simply the regression tree that best predicts the current
residuals yi     fm   1(xi), and     jm is the mean of these residuals in each
corresponding region.
for two-class classi   cation and exponential loss, this stagewise approach
gives rise to the adaboost method for boosting classi   cation trees (algo-
rithm 10.1). in particular, if the trees t (x;   m) are restricted to be scaled
classi   cation trees, then we showed in section 10.4 that the solution to
i(yi 6=
= e   yifm   1(xi). by a scaled classi   cation

(10.29) is the tree that minimizes the weighted error ratepn

t (xi;   m)) with weights w(m)
tree, we mean   mt (x;   m), with the restriction that   jm     {   1, 1}).
weighted exponential criterion for the new tree:

without this restriction, (10.29) still simpli   es for exponential loss to a

i=1 w(m)

i

i

    m = arg min
  m

nxi=1

w(m)

i

exp[   yit (xi;   m)].

(10.31)

it is straightforward to implement a greedy recursive-partitioning algorithm
using this weighted exponential loss as a splitting criterion. given the rjm,
one can show (exercise 10.7) that the solution to (10.30) is the weighted
log-odds in each corresponding region

    jm =

1
2

log pxi   rjm
pxi   rjm

i

w(m)
w(m)

i

i(yi = 1)
i(yi =    1)

.

(10.32)

this requires a specialized tree-growing algorithm; in practice, we prefer
the approximation presented below that uses a weighted least squares re-
gression tree.

using loss criteria such as the absolute error or the huber loss (10.23) in
place of squared-error loss for regression, and the deviance (10.22) in place
of exponential loss for classi   cation, will serve to robustify boosting trees.
unfortunately, unlike their nonrobust counterparts, these robust criteria
do not give rise to simple fast boosting algorithms.

for more general loss criteria the solution to (10.30), given the rjm,
is typically straightforward since it is a simple    location    estimate. for

358

10. boosting and additive trees

absolute loss it is just the median of the residuals in each respective region.
for the other criteria fast iterative algorithms exist for solving (10.30),
and usually their faster    single-step    approximations are adequate. the
problem is tree induction. simple fast algorithms do not exist for solving
(10.29) for these more general loss criteria, and approximations like (10.27)
become essential.

10.10 numerical optimization via gradient

boosting

fast approximate algorithms for solving (10.29) with any di   erentiable loss
criterion can be derived by analogy to numerical optimization. the loss in
using f (x) to predict y on the training data is

l(f ) =

nxi=1

l(yi, f (xi)).

(10.33)

the goal is to minimize l(f ) with respect to f , where here f (x) is con-
strained to be a sum of trees (10.28). ignoring this constraint, minimizing
(10.33) can be viewed as a numerical optimization

  f = arg min

f

l(f ),

(10.34)

where the    parameters    f     irn are the values of the approximating func-
tion f (xi) at each of the n data points xi:

f = {f (x1), f (x2), . . . , f (xn )}t .

numerical optimization procedures solve (10.34) as a sum of component

vectors

fm =

mxm=0

hm , hm     irn ,

where f0 = h0 is an initial guess, and each successive fm is induced based
on the current parameter vector fm   1, which is the sum of the previously
induced updates. numerical optimization methods di   er in their prescrip-
tions for computing each increment vector hm (   step   ).

10.10.1 steepest descent
steepest descent chooses hm =      mgm where   m is a scalar and gm     irn
is the gradient of l(f ) evaluated at f = fm   1. the components of the
gradient gm are

gim =(cid:20)    l(yi, f (xi))

   f (xi)

(cid:21)f (xi)=fm   1(xi)

(10.35)

10.10 numerical optimization via gradient boosting

359

the step length   m is the solution to

  m = arg min

  

l(fm   1       gm).

(10.36)

the current solution is then updated

fm = fm   1       mgm

and the process repeated at the next iteration. steepest descent can be
viewed as a very greedy strategy, since    gm is the local direction in irn
for which l(f ) is most rapidly decreasing at f = fm   1.

10.10.2 gradient boosting

forward stagewise boosting (algorithm 10.2) is also a very greedy strategy.
at each step the solution tree is the one that maximally reduces (10.29),
given the current model fm   1 and its    ts fm   1(xi). thus, the tree predic-
tions t (xi;   m) are analogous to the components of the negative gradient
(10.35). the principal di   erence between them is that the tree components
tm = {t (x1;   m), . . . , t (xn ;   m)}t are not independent. they are con-
strained to be the predictions of a jm-terminal node decision tree, whereas
the negative gradient is the unconstrained maximal descent direction.

the solution to (10.30) in the stagewise approach is analogous to the line
search (10.36) in steepest descent. the di   erence is that (10.30) performs
a separate line search for those components of tm that correspond to each
separate terminal region {t (xi;   m)}xi   rjm.
if minimizing loss on the training data (10.33) were the only goal, steep-
est descent would be the preferred strategy. the gradient (10.35) is trivial
to calculate for any di   erentiable id168 l(y, f (x)), whereas solving
(10.29) is di   cult for the robust criteria discussed in section 10.6. unfor-
tunately the gradient (10.35) is de   ned only at the training data points xi,
whereas the ultimate goal is to generalize fm (x) to new data not repre-
sented in the training set.

a possible resolution to this dilemma is to induce a tree t (x;   m) at the
mth iteration whose predictions tm are as close as possible to the negative
gradient. using squared error to measure closeness, this leads us to

    m = arg min
  

nxi=1

(   gim     t (xi;   ))2 .

(10.37)

that is, one    ts the tree t to the negative gradient values (10.35) by least
squares. as noted in section 10.9 fast algorithms exist for least squares
decision tree induction. although the solution regions   rjm to (10.37) will
not be identical to the regions rjm that solve (10.29), it is generally sim-
ilar enough to serve the same purpose. in any case, the forward stagewise

360

10. boosting and additive trees

table 10.2. gradients for commonly used id168s.

setting

regression

regression

regression

1

id168       l(yi, f (xi))/   f (xi)
2 [yi     f (xi)]2
|yi     f (xi)|
huber

yi     f (xi)
sign[yi     f (xi)]
yi     f (xi) for |yi     f (xi)|       m
  msign[yi     f (xi)] for |yi     f (xi)| >   m
where   m =   th-quantile{|yi     f (xi)|}
kth component: i(yi = gk)     pk(xi)

classi   cation deviance

boosting procedure, and top-down decision tree induction, are themselves
approximation procedures. after constructing the tree (10.37), the corre-
sponding constants in each region are given by (10.30).

table 10.2 summarizes the gradients for commonly used id168s.
for squared error loss, the negative gradient is just the ordinary residual
   gim = yi     fm   1(xi), so that (10.37) on its own is equivalent to standard
least-squares boosting. with absolute error loss, the negative gradient is
the sign of the residual, so at each iteration (10.37)    ts the tree to the
sign of the current residuals by least squares. for huber m-regression, the
negative gradient is a compromise between these two (see the table).

for classi   cation the id168 is the multinomial deviance (10.22),
and k least squares trees are constructed at each iteration. each tree tkm
is    t to its respective negative gradient vector gkm,

   gikm = (cid:20)    l (yi, f1(xi), . . . , fk (xi))

   fk(xi)
= i(yi = gk)     pk(xi),

(cid:21)f (xi)=fm   1(xi)

(10.38)

with pk(x) given by (10.21). although k separate trees are built at each
iteration, they are related through (10.21). for binary classi   cation (k =
2), only one tree is needed (exercise 10.10).

10.10.3 implementations of gradient boosting

algorithm 10.3 presents the generic gradient tree-boosting algorithm for
regression. speci   c algorithms are obtained by inserting di   erent loss cri-
teria l(y, f (x)). the    rst line of the algorithm initializes to the optimal
constant model, which is just a single terminal node tree. the components
of the negative gradient computed at line 2(a) are referred to as general-
ized or pseudo residuals, r. gradients for commonly used id168s are
summarized in table 10.2.

10.11 right-sized trees for boosting

361

algorithm 10.3 gradient tree boosting algorithm.

1. initialize f0(x) = arg min  pn

2. for m = 1 to m :

i=1 l(yi,   ).

(a) for i = 1, 2, . . . , n compute

rim =    (cid:20)    l(yi, f (xi))

   f (xi)

(cid:21)f =fm   1

.

(b) fit a regression tree to the targets rim giving terminal regions

rjm, j = 1, 2, . . . , jm.

(c) for j = 1, 2, . . . , jm compute

  jm = arg min

   xxi   rjm
(d) update fm(x) = fm   1(x) +pjm

3. output   f (x) = fm (x).

l (yi, fm   1(xi) +   ) .

j=1   jmi(x     rjm).

the algorithm for classi   cation is similar. lines 2(a)   (d) are repeated
k times at each iteration m, once for each class using (10.38). the result
at line 3 is k di   erent (coupled) tree expansions fkm (x), k = 1, 2, . . . , k.
these produce probabilities via (10.21) or do classi   cation as in (10.20).
details are given in exercise 10.9. two basic tuning parameters are the
number of iterations m and the sizes of each of the constituent trees
jm, m = 1, 2, . . . , m .

the original implementation of this algorithm was called mart for
   multiple additive regression trees,    and was referred to in the    rst edi-
tion of this book. many of the    gures in this chapter were produced by
mart. gradient boosting as described here is implemented in the r gbm
package (ridgeway, 1999,    gradient boosted models   ), and is freely avail-
able. the gbm package is used in section 10.14.2, and extensively in chap-
ters 16 and 15. another r implementation of boosting is mboost (hothorn
and b  uhlmann, 2006). a commercial implementation of gradient boost-
ing/mart called treenet    is available from salford systems, inc.

10.11 right-sized trees for boosting

historically, boosting was considered to be a technique for combining mod-
els, here trees. as such, the tree building algorithm was regarded as a

362

10. boosting and additive trees

primitive that produced models to be combined by the boosting proce-
dure. in this scenario, the optimal size of each tree is estimated separately
in the usual manner when it is built (section 9.2). a very large (oversized)
tree is    rst induced, and then a bottom-up procedure is employed to prune
it to the estimated optimal number of terminal nodes. this approach as-
sumes implicitly that each tree is the last one in the expansion (10.28).
except perhaps for the very last tree, this is clearly a very poor assump-
tion. the result is that trees tend to be much too large, especially during
the early iterations. this substantially degrades performance and increases
computation.

the simplest strategy for avoiding this problem is to restrict all trees
to be the same size, jm = j    m. at each iteration a j-terminal node
regression tree is induced. thus j becomes a meta-parameter of the entire
boosting procedure, to be adjusted to maximize estimated performance for
the data at hand.

one can get an idea of useful values for j by considering the properties

of the    target    function

   = arg min

f

exy l(y, f (x)).

(10.39)

here the expected value is over the population joint distribution of (x, y ).
the target function   (x) is the one with minimum prediction risk on future
data. this is the function we are trying to approximate.

one relevant property of   (x) is the degree to which the coordinate vari-
ables x t = (x1, x2, . . . , xp) interact with one another. this is captured
by its anova (analysis of variance) expansion

  (x) =xj

  j(xj) +xjk

  jk(xj, xk) +xjkl

  jkl(xj, xk, xl) +       . (10.40)

the    rst sum in (10.40) is over functions of only a single predictor variable
xj. the particular functions   j(xj) are those that jointly best approximate
  (x) under the loss criterion being used. each such   j(xj) is called the
   main e   ect    of xj. the second sum is over those two-variable functions
that when added to the main e   ects best    t   (x). these are called the
second-order interactions of each respective variable pair (xj, xk). the
third sum represents third-order interactions, and so on. for many problems
encountered in practice, low-order interaction e   ects tend to dominate.
when this is the case, models that produce strong higher-order interaction
e   ects, such as large id90, su   er in accuracy.

the interaction level of tree-based approximations is limited by the tree
size j. namely, no interaction e   ects of level greater than j     1 are pos-
sible. since boosted models are additive in the trees (10.28), this limit
extends to them as well. setting j = 2 (single split    decision stump   )
produces boosted models with only main e   ects; no interactions are per-
mitted. with j = 3, two-variable interaction e   ects are also allowed, and

10.11 right-sized trees for boosting

363

stumps
10 node
100 node
adaboost

r
o
r
r

e

 
t
s
e
t

4
0

.

3
0

.

2

.

0

1

.

0

.

0
0

0

100

200

300

400

number of terms

figure 10.9. boosting with di   erent sized trees, applied to the example (10.2)
used in figure 10.2. since the generative model is additive, stumps perform the
best. the boosting algorithm used the binomial deviance loss in algorithm 10.3;
shown for comparison is the adaboost algorithm 10.1.

so on. this suggests that the value chosen for j should re   ect the level
of dominant interactions of   (x). this is of course generally unknown, but
in most situations it will tend to be low. figure 10.9 illustrates the e   ect
of interaction order (choice of j) on the simulation example (10.2). the
generative function is additive (sum of quadratic monomials), so boosting
models with j > 2 incurs unnecessary variance and hence the higher test
error. figure 10.10 compares the coordinate functions found by boosted
stumps with the true functions.

although in many applications j = 2 will be insu   cient, it is unlikely
that j > 10 will be required. experience so far indicates that 4     j     8
works well in the context of boosting, with results being fairly insensitive
to particular choices in this range. one can    ne-tune the value for j by
trying several di   erent values and choosing the one that produces the low-
est risk on a validation sample. however, this seldom provides signi   cant
improvement over using j     6.

364

10. boosting and additive trees

coordinate functions for additive logistic trees

f1(x1)

f2(x2)

f3(x3)

f4(x4)

f5(x5)

f6(x6)

f7(x7)

f8(x8)

f9(x9)

f10(x10)

figure 10.10. coordinate functions estimated by boosting stumps for the sim-
ulated example used in figure 10.9. the true quadratic functions are shown for
comparison.

10.12 id173

besides the size of the constituent trees, j, the other meta-parameter of
gradient boosting is the number of boosting iterations m . each iteration
usually reduces the training risk l(fm ), so that for m large enough this risk
can be made arbitrarily small. however,    tting the training data too well
can lead to over   tting, which degrades the risk on future predictions. thus,
there is an optimal number m     minimizing future risk that is application
dependent. a convenient way to estimate m     is to monitor prediction risk
as a function of m on a validation sample. the value of m that minimizes
this risk is taken to be an estimate of m    . this is analogous to the early
stopping strategy often used with neural networks (section 11.4).

10.12.1 shrinkage

controlling the value of m is not the only possible id173 strategy.
as with ridge regression and neural networks, shrinkage techniques can be
employed as well (see sections 3.4.1 and 11.5). the simplest implementation
of shrinkage in the context of boosting is to scale the contribution of each
tree by a factor 0 <    < 1 when it is added to the current approximation.
that is, line 2(d) of algorithm 10.3 is replaced by

fm(x) = fm   1(x) +      

jxj=1

  jmi(x     rjm).

(10.41)

the parameter    can be regarded as controlling the learning rate of the
boosting procedure. smaller values of    (more shrinkage) result in larger
training risk for the same number of iterations m . thus, both    and m
control prediction risk on the training data. however, these parameters do

10.12 id173

365

not operate independently. smaller values of    lead to larger values of m
for the same training risk, so that there is a tradeo    between them.

empirically it has been found (friedman, 2001) that smaller values of   
favor better test error, and require correspondingly larger values of m . in
fact, the best strategy appears to be to set    to be very small (   < 0.1)
and then choose m by early stopping. this yields dramatic improvements
(over no shrinkage    = 1) for regression and for id203 estimation. the
corresponding improvements in misclassi   cation risk via (10.20) are less,
but still substantial. the price paid for these improvements is computa-
tional: smaller values of    give rise to larger values of m , and computation
is proportional to the latter. however, as seen below, many iterations are
generally computationally feasible even on very large data sets. this is
partly due to the fact that small trees are induced at each step with no
pruning.

figure 10.11 shows test error curves for the simulated example (10.2) of
figure 10.2. a gradient boosted model (mart) was trained using binomial
deviance, using either stumps or six terminal-node trees, and with or with-
out shrinkage. the bene   ts of shrinkage are evident, especially when the
binomial deviance is tracked. with shrinkage, each test error curve reaches
a lower value, and stays there for many iterations.

section 16.2.1 draws a connection between forward stagewise shrinkage
in boosting and the use of an l1 penalty for regularizing model parame-
ters (the    lasso   ). we argue that l1 penalties may be superior to the l2
penalties used by methods such as the support vector machine.

10.12.2 subsampling

we saw in section 8.7 that bootstrap averaging (id112) improves the
performance of a noisy classi   er through averaging. chapter 15 discusses
in some detail the variance-reduction mechanism of this sampling followed
by averaging. we can exploit the same device in gradient boosting, both
to improve performance and computational e   ciency.

with stochastic gradient boosting (friedman, 1999), at each iteration we
sample a fraction    of the training observations (without replacement),
and grow the next tree using that subsample. the rest of the algorithm is
identical. a typical value for    can be 1
2 , although for large n ,    can be
substantially smaller than 1
2 .

not only does the sampling reduce the computing time by the same
fraction   , but in many cases it actually produces a more accurate model.
figure 10.12 illustrates the e   ect of subsampling using the simulated
example (10.2), both as a classi   cation and as a regression example. we
see in both cases that sampling along with shrinkage slightly outperformed
the rest. it appears here that subsampling without shrinkage does poorly.

366

10. boosting and additive trees

stumps
deviance

no shrinkage
shrinkage=0.2

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

 
t

e
s

 
t
s
e
t

stumps

misclassification error

no shrinkage
shrinkage=0.2

5

.

0

4
0

.

3

.

0

2
0

.

1

.

0

0
.
0

0

500

1000

1500

2000

0

500

1000

1500

2000

boosting iterations

boosting iterations

6-node trees

deviance

no shrinkage
shrinkage=0.6

r
o
r
r

e
 
n
o
i
t
a
c
i
f
i
s
s
a
c
s
m

i

l

 
t
e
s

 
t
s
e
t

6-node trees

misclassification error

no shrinkage
shrinkage=0.6

5
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

e
c
n
a
v
e
d

i

 
t

e
s

 
t
s
e
t

e
c
n
a
v
e
d

i

 
t
e
s

 
t
s
e
t

0

.

2

5

.

1

0

.

1

5

.

0

0
.
0

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

0

500

1000

1500

2000

0

500

1000

1500

2000

boosting iterations

boosting iterations

figure 10.11. test error curves for simulated example (10.2) of figure 10.9,
using gradient boosting (mart). the models were trained using binomial de-
viance, either stumps or six terminal-node trees, and with or without shrinkage.
the left panels report test deviance, while the right panels show misclassi   cation
error. the bene   cial e   ect of shrinkage can be seen in all cases, especially for
deviance in the left panels.

10.13 interpretation

367

4   node trees

deviance

absolute error

0
5
.
0

5
4
.
0

0
4
.
0

5
3
.
0

0
3
.
0

r
o
r
r

e
 
e
t
u
o
s
b
a

l

 
t
e
s

 
t
s
e
t

no shrinkage
shrink=0.1
sample=0.5
shrink=0.1 sample=0.5

e
c
n
a
v
e
d

i

 
t
e
s

 
t
s
e
t

4
.
1

2
.
1

0
.
1

8
.
0

6
.
0

4
.
0

0

200

400

600

800

1000

0

200

400

600

800

1000

boosting iterations

boosting iterations

figure 10.12. test-error curves for the simulated example (10.2), showing
the e   ect of stochasticity. for the curves labeled    sample= 0.5   , a di   erent 50%
subsample of the training data was used each time a tree was grown. in the left
panel the models were    t by gbm using a binomial deviance id168; in the
right-hand panel using square-error loss.

the downside is that we now have four parameters to set: j, m ,    and
  . typically some early explorations determine suitable values for j,    and
  , leaving m as the primary parameter.

10.13

interpretation

single id90 are highly interpretable. the entire model can be com-
pletely represented by a simple two-dimensional graphic (binary tree) that
is easily visualized. linear combinations of trees (10.28) lose this important
feature, and must therefore be interpreted in a di   erent way.

10.13.1 relative importance of predictor variables

in data mining applications the input predictor variables are seldom equally
relevant. often only a few of them have substantial in   uence on the re-
sponse; the vast majority are irrelevant and could just as well have not
been included. it is often useful to learn the relative importance or contri-
bution of each input variable in predicting the response.

368

10. boosting and additive trees

for a single decision tree t , breiman et al. (1984) proposed

i 2
    (t ) =

j   1xt=1

    2
t i(v(t) =    )

(10.42)

as a measure of relevance for each predictor variable x   . the sum is over
the j     1 internal nodes of the tree. at each such node t, one of the input
variables xv(t) is used to partition the region associated with that node into
two subregions; within each a separate constant is    t to the response values.
the particular variable chosen is the one that gives maximal estimated
improvement     2
t in squared error risk over that for a constant    t over the
entire region. the squared relative importance of variable x    is the sum of
such squared improvements over all internal nodes for which it was chosen
as the splitting variable.

this importance measure is easily generalized to additive tree expansions

(10.28); it is simply averaged over the trees

i 2
    =

1
m

mxm=1

i 2
    (tm).

(10.43)

due to the stabilizing e   ect of averaging, this measure turns out to be more
reliable than is its counterpart (10.42) for a single tree. also, because of
shrinkage (section 10.12.1) the masking of important variables by others
with which they are highly correlated is much less of a problem. note
that (10.42) and (10.43) refer to squared relevance; the actual relevances
are their respective square roots. since these measures are relative, it is
customary to assign the largest a value of 100 and then scale the others
accordingly. figure 10.6 shows the relevant importance of the 57 inputs in
predicting spam versus email.

for k-class classi   cation, k separate models fk(x), k = 1, 2, . . . , k are

induced, each consisting of a sum of trees

fk(x) =

in this case (10.43) generalizes to

i 2
   k =

1
m

mxm=1

mxm=1

tkm(x).

(10.44)

i 2
    (tkm).

(10.45)

here i   k is the relevance of x    in separating the class k observations from
the other classes. the overall relevance of x    is obtained by averaging over
all of the classes

i 2
    =

1
k

kxk=1

i 2
   k.

(10.46)

10.13 interpretation

369

figures 10.23 and 10.24 illustrate the use of these averaged and separate
relative importances.

10.13.2 partial dependence plots

after the most relevant variables have been identi   ed, the next step is to
attempt to understand the nature of the dependence of the approximation
f (x) on their joint values. graphical renderings of the f (x) as a function
of its arguments provides a comprehensive summary of its dependence on
the joint values of the input variables.

unfortunately, such visualization is limited to low-dimensional views.
we can easily display functions of one or two arguments, either continuous
or discrete (or mixed), in a variety of di   erent ways; this book is    lled
with such displays. functions of slightly higher dimensions can be plotted
by conditioning on particular sets of values of all but one or two of the
arguments, producing a trellis of plots (becker et al., 1996).1

for more than two or three variables, viewing functions of the corre-
sponding higher-dimensional arguments is more di   cult. a useful alterna-
tive can sometimes be to view a collection of plots, each one of which shows
the partial dependence of the approximation f (x) on a selected small sub-
set of the input variables. although such a collection can seldom provide a
comprehensive depiction of the approximation, it can often produce helpful
clues, especially when f (x) is dominated by low-order interactions (10.40).
consider the subvector xs of     < p of the input predictor variables x t =
(x1, x2, . . . , xp), indexed by s     {1, 2, . . . , p}. let c be the complement
set, with s     c = {1, 2, . . . , p}. a general function f (x) will in principle
depend on all of the input variables: f (x) = f (xs , xc). one way to de   ne
the average or partial dependence of f (x) on xs is

fs (xs ) = exc f (xs , xc).

(10.47)

this is a marginal average of f , and can serve as a useful description of the
e   ect of the chosen subset on f (x) when, for example, the variables in xs
do not have strong interactions with those in xc.

partial dependence functions can be used to interpret the results of any

   black box    learning method. they can be estimated by

  fs (xs ) =

1
n

nxi=1

f (xs , xic),

(10.48)

where {x1c, x2c, . . . , xn c} are the values of xc occurring in the training
data. this requires a pass over the data for each set of joint values of xs for
which   fs (xs ) is to be evaluated. this can be computationally intensive,

1lattice in r.

370

10. boosting and additive trees

even for moderately sized data sets. fortunately with id90,   fs (xs )
(10.48) can be rapidly computed from the tree itself without reference to
the data (exercise 10.11).

it is important to note that partial dependence functions de   ned in
(10.47) represent the e   ect of xs on f (x) after accounting for the (av-
erage) e   ects of the other variables xc on f (x). they are not the e   ect
of xs on f (x) ignoring the e   ects of xc. the latter is given by the con-
ditional expectation

  fs (xs ) = e(f (xs , xc)|xs ),

(10.49)

and is the best least squares approximation to f (x) by a function of xs
alone. the quantities   fs (xs ) and   fs (xs ) will be the same only in the
unlikely event that xs and xc are independent. for example, if the e   ect
of the chosen variable subset happens to be purely additive,

f (x) = h1(xs ) + h2(xc).

(10.50)

then (10.47) produces the h1(xs ) up to an additive constant. if the e   ect
is purely multiplicative,

f (x) = h1(xs )    h2(xc),

(10.51)

then (10.47) produces h1(xs ) up to a multiplicative constant factor. on
the other hand, (10.49) will not produce h1(xs ) in either case. in fact,
(10.49) can produce strong e   ects on variable subsets for which f (x) has
no dependence at all.

viewing plots of the partial dependence of the boosted-tree approxima-
tion (10.28) on selected variables subsets can help to provide a qualitative
description of its properties. illustrations are shown in sections 10.8 and
10.14. owing to the limitations of computer graphics, and human percep-
tion, the size of the subsets xs must be small (l     1, 2, 3). there are of
course a large number of such subsets, but only those chosen from among
the usually much smaller set of highly relevant predictors are likely to be
informative. also, those subsets whose e   ect on f (x) is approximately
additive (10.50) or multiplicative (10.51) will be most revealing.

for k-class classi   cation, there are k separate models (10.44), one for
each class. each one is related to the respective probabilities (10.21) through

fk(x) = log pk(x)    

1
k

kxl=1

log pl(x).

(10.52)

thus each fk(x) is a monotone increasing function of its respective prob-
ability on a logarithmic scale. partial dependence plots of each respective
fk(x) (10.44) on its most relevant predictors (10.45) can help reveal how
the log-odds of realizing that class depend on the respective input variables.

10.14 illustrations

371

10.14

illustrations

in this section we illustrate gradient boosting on a number of larger datasets,
using di   erent id168s as appropriate.

10.14.1 california housing

this data set (pace and barry, 1997) is available from the carnegie-mellon
statlib repository2. it consists of aggregated data from each of 20,460
neighborhoods (1990 census block groups) in california. the response vari-
able y is the median house value in each neighborhood measured in units of
$100,000. the predictor variables are demographics such as median income
medinc, housing density as re   ected by the number of houses house, and the
average occupancy in each house aveoccup. also included as predictors are
the location of each neighborhood (longitude and latitude), and several
quantities re   ecting the properties of the houses in the neighborhood: av-
erage number of rooms averooms and bedrooms avebedrms. there are thus
a total of eight predictors, all numeric.

we    t a gradient boosting model using the mart procedure, with j = 6
terminal nodes, a learning rate (10.41) of    = 0.1, and the huber loss
criterion for predicting the numeric response. we randomly divided the
dataset into a training set (80%) and a test set (20%).

figure 10.13 shows the average absolute error

aae = e|y       fm (x)|

(10.53)

as a function for number of iterations m on both the training data and test
data. the test error is seen to decrease monotonically with increasing m ,
more rapidly during the early stages and then leveling o    to being nearly
constant as iterations increase. thus, the choice of a particular value of m
is not critical, as long as it is not too small. this tends to be the case in
many applications. the shrinkage strategy (10.41) tends to eliminate the
problem of over   tting, especially for larger data sets.

the value of aae after 800 iterations is 0.31. this can be compared to
that of the optimal constant predictor median{yi} which is 0.89. in terms of
more familiar quantities, the squared multiple correlation coe   cient of this
model is r2 = 0.84. pace and barry (1997) use a sophisticated spatial auto-
regression procedure, where prediction for each neighborhood is based on
median house values in nearby neighborhoods, using the other predictors as
covariates. experimenting with transformations they achieved r2 = 0.85,
predicting log y . using log y as the response the corresponding value for
gradient boosting was r2 = 0.86.

2http://lib.stat.cmu.edu.

372

10. boosting and additive trees

training and test absolute error

train error
test error

r
o
r
r

e
e

 

t

l

u
o
s
b
a

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0

200

400

600

800

iterations m

figure 10.13. average-absolute error as a function of number of iterations
for the california housing data.

figure 10.14 displays the relative variable importances for each of the
eight predictor variables. not surprisingly, median income in the neigh-
borhood is the most relevant predictor. longitude, latitude, and average
occupancy all have roughly half the relevance of income, whereas the others
are somewhat less in   uential.

figure 10.15 shows single-variable partial dependence plots on the most
relevant nonlocation predictors. note that the plots are not strictly smooth.
this is a consequence of using tree-based models. id90 produce
discontinuous piecewise constant models (10.25). this carries over to sums
of trees (10.28), with of course many more pieces. unlike most of the meth-
ods discussed in this book, there is no smoothness constraint imposed on
the result. arbitrarily sharp discontinuities can be modeled. the fact that
these curves generally exhibit a smooth trend is because that is what is
estimated to best predict the response for this problem. this is often the
case.

the hash marks at the base of each plot delineate the deciles of the
data distribution of the corresponding variables. note that here the data
density is lower near the edges, especially for larger values. this causes the
curves to be somewhat less well determined in those regions. the vertical
scales of the plots are the same, and give a visual comparison of the relative
importance of the di   erent variables.

the partial dependence of median house value on median income is
monotonic increasing, being nearly linear over the main body of data. house
value is generally monotonic decreasing with increasing average occupancy,
except perhaps for average occupancy rates less than one. median house

10.14 illustrations

373

population

avebedrms

averooms

houseage

latitude

aveoccup

longitude

medinc

0

20

40

60

80

100

relative importance

figure 10.14. relative importance of the predictors for the california housing
data.

value has a nonmonotonic partial dependence on average number of rooms.
it has a minimum at approximately three rooms and is increasing both for
smaller and larger values.

median house value is seen to have a very weak partial dependence on
house age that is inconsistent with its importance ranking (figure 10.14).
this suggests that this weak main e   ect may be masking stronger interac-
tion e   ects with other variables. figure 10.16 shows the two-variable partial
dependence of housing value on joint values of median age and average oc-
cupancy. an interaction between these two variables is apparent. for values
of average occupancy greater than two, house value is nearly independent
of median age, whereas for values less than two there is a strong dependence
on age.

figure 10.17 shows the two-variable partial dependence of the    tted
model on joint values of longitude and latitude, displayed as a shaded
contour plot. there is clearly a very strong dependence of median house
value on the neighborhood location in california. note that figure 10.17 is
not a plot of house value versus location ignoring the e   ects of the other
predictors (10.49). like all partial dependence plots, it represents the e   ect
of location after accounting for the e   ects of the other neighborhood and
house attributes (10.47). it can be viewed as representing an extra premium
one pays for location. this premium is seen to be relatively large near the
paci   c coast especially in the bay area and los angeles   san diego re-

374

10. boosting and additive trees

0
.
2

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
-

0
.
1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

e
c
n
e
d
n
e
p
e
d

 
l
a
i
t
r
a
p

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

2

4

6

8

10

medinc

5
.
1

0
.
1

5
.
0

0
.
0

5
.
0
-

0
.
1
-

5
.
1

0

.

1

5

.

0

0

.

0

5

.

0
-

0

.

1
-

e
c
n
e
d
n
e
p
e
d

 
l
a
i
t
r
a
p

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

2

3
aveoccup

4

5

10

20

30

40

50

4

6

8

10

houseage

averooms

figure 10.15. partial dependence of housing value on the nonlocation vari-
ables for the california housing data. the red ticks at the base of the plot are
deciles of the input variables.

1.0

0.5

0.0

50

40

30

houseage

20

10

5

2

3

4

aveoccup

figure 10.16. partial dependence of house value on median age and aver-
age occupancy. there appears to be a strong interaction e   ect between these two
variables.

2
4

0
4

8
3

6
3

4
3

e
d
u

t
i
t

a
l

10.14 illustrations

375

 1.0

 0.5

 0.0

   0.5

   1.0

   124

   122

   120

   118

   116

   114

longitude

figure 10.17. partial dependence of median house value on location in cal-
ifornia. one unit is $100, 000, at 1990 prices, and the values plotted are relative
to the overall median of $180, 000.

gions. in the northern, central valley, and southeastern desert regions of
california, location costs considerably less.

10.14.2 new zealand fish

plant and animal ecologists use regression models to predict species pres-
ence, abundance and richness as a function of environmental variables.
although for many years simple linear and parametric models were popu-
lar, recent literature shows increasing interest in more sophisticated mod-
els such as generalized additive models (section 9.1, gam), multivariate
adaptive regression splines (section 9.4, mars) and boosted regression
trees (leathwick et al., 2005; leathwick et al., 2006). here we model the

376

10. boosting and additive trees

presence and abundance of the black oreo dory, a marine    sh found in the
oceanic waters around new zealand.3

figure 10.18 shows the locations of 17,000 trawls (deep-water net    shing,
with a maximum depth of 2km), and the red points indicate those 2353
trawls for which the black oreo was present, one of over a hundred species
regularly recorded. the catch size in kg for each species was recorded for
each trawl. along with the species catch, a number of environmental mea-
surements are available for each trawl. these include the average depth of
the trawl (avgdepth), and the temperature and salinity of the water. since
the latter two are strongly correlated with depth, leathwick et al. (2006)
derived instead tempresid and salresid, the residuals obtained when these
two measures are adjusted for depth (via separate non-parametric regres-
sions). sstgrad is a measure of the gradient of the sea surface temperature,
and chla is a broad indicator of ecosytem productivity via satellite-image
measurements. suspartmatter provides a measure of suspended particulate
matter, particularly in coastal waters, and is also satellite derived.

the goal of this analysis is to estimate the id203 of    nding black
oreo in a trawl, as well as the expected catch size, standardized to take
into account the e   ects of variation in trawl speed and distance, as well
as the mesh size of the trawl net. the authors used id28
for estimating the id203. for the catch size, it might seem natural
to assume a poisson distribution and model the log of the mean count,
but this is often not appropriate because of the excessive number of zeros.
although specialized approaches have been developed, such as the zero-
in   ated poisson (lambert, 1992), they chose a simpler approach. if y is
the (non-negative) catch size,

e(y |x) = e(y |y > 0, x)    pr(y > 0|x).

(10.54)

the second term is estimated by the id28, and the    rst term
can be estimated using only the 2353 trawls with a positive catch.

for the id28 the authors used a gradient boosted model
(gbm)4 with binomial deviance id168, depth-10 trees, and a shrink-
age factor    = 0.025. for the positive-catch regression, they modeled
log(y ) using a gbm with squared-error loss (also depth-10 trees, but
   = 0.01), and un-logged the predictions. in both cases they used 10-fold
cross-validation for selecting the number of terms, as well as the shrinkage
factor.

3the models, data, and maps shown here were kindly provided by dr john leathwick
of the national institute of water and atmospheric research in new zealand, and dr
jane elith, school of botany, university of melbourne. the collection of the research
trawl data took place from 1979   2005, and was funded by the new zealand ministry of
fisheries.

4version 1.5-7 of package gbm in r, ver. 2.2.0.

10.14 illustrations

377

figure 10.18. map of new zealand and its surrounding exclusive economic
zone, showing the locations of 17,000 trawls (small blue dots) taken between 1979
and 2005. the red points indicate trawls for which the species black oreo dory
were present.

378

10. boosting and additive trees

i

e
c
n
a
v
e
d
n
a
e
m

 

4
3

.

0

2
3
0

.

0
3

.

0

8
2
0

.

6
2
0

.

4
2

.

0

gbm test
gbm cv
gam test

y
t
i
v
i
t
i
s
n
e
s

0

.

1

8

.

0

6

.

0

4
0

.

2

.

0

0

.

0

auc
gam 0.97
gbm 0.98

0

500

1000

1500

0.0

0.2

0.4

0.6

0.8

1.0

number of trees

specificity

figure 10.19. the left panel shows the mean deviance as a function of the
number of trees for the gbm id28 model    t to the presence/absence
data. shown are 10-fold cross-validation on the training data (and 1    s.e. bars),
and test deviance on the test data. also shown for comparison is the test deviance
using a gam model with 8 df for each term. the right panel shows roc curves
on the test data for the chosen gbm model (vertical line in left plot) and the
gam model.

figure 10.19 (left panel) shows the mean binomial deviance for the se-
quence of gbm models, both for 10-fold cv and test data. there is a mod-
est improvement over the performance of a gam model,    t using smoothing
splines with 8 degrees-of-freedom (df) per term. the right panel shows the
roc curves (see section 9.2.5) for both models, which measures predictive
performance. from this point of view, the performance looks very simi-
lar, with gbm perhaps having a slight edge as summarized by the auc
(area under the curve). at the point of equal sensitivity/speci   city, gbm
achieves 91%, and gam 90%.

figure 10.20 summarizes the contributions of the variables in the logistic
gbm    t. we see that there is a well-de   ned depth range over which black
oreo are caught, with much more frequent capture in colder waters. we do
not give details of the quantitative catch model; the important variables
were much the same.

all the predictors used in these models are available on a    ne geographi-
cal grid; in fact they were derived from environmental atlases, satellite im-
ages and the like   see leathwick et al. (2006) for details. this also means
that predictions can be made on this grid, and imported into gis mapping
systems. figure 10.21 shows prediction maps for both presence and catch
size, with both standardized to a common set of trawl conditions; since the
predictors vary in a continuous fashion with geographical location, so do
the predictions.

tempresid
avgdepth
suspartmatter
salresid
sstgrad
chlacase2
slope
tidalcurr
pentade
codendsize
disorgmatter
distance
speed
orbvel

i

)
d
s
e
r
p
m
e
t
(
f

0 10

25

relative influence

)
r
e

t
t

a
m

t
r
a
p
s
u
s

(
f

3
   

5
   

7
   

i

)
d
s
e
r
a
s

l

(
f

10.14 illustrations

379

t

)
h
p
e
d
g
v
a

(
f

2
   

4
   

6
   

   4

0

2

4

6

0

500 1000

2000

tempresid

avgdepth

1
   

3
   

5
   

7
   

)
d
a
r
g
t
s
s

(
f

1
   

3
   

5
   

7
   

1
   

3
   

5
   

7
   

0

5

10

15

   0.8

   0.4

0.0

0.4

0.00

0.05

0.10

0.15

suspartmatter

salresid

sstgrad

figure 10.20. the top-left panel shows the relative in   uence computed from
the gbm id28 model. the remaining panels show the partial de-
pendence plots for the leading    ve variables, all plotted on the same scale for
comparison.

because of their ability to model interactions and automatically select
variables, as well as robustness to outliers and missing data, gbm models
are rapidly gaining popularity in this data-rich and enthusiastic community.

10.14.3 demographics data

in this section we illustrate gradient boosting on a multiclass classi   ca-
tion problem, using mart. the data come from 9243 questionnaires    lled
out by shopping mall customers in the san francisco bay area (impact
resources, inc., columbus, oh). among the questions are 14 concerning
demographics. for this illustration the goal is to predict occupation us-
ing the other 13 variables as predictors, and hence identify demographic
variables that discriminate between di   erent occupational categories. we
randomly divided the data into a training set (80%) and test set (20%),
and used j = 6 node trees with a learning rate    = 0.1.

figure 10.22 shows the k = 9 occupation class values along with their
corresponding error rates. the overall error rate is 42.5%, which can be
compared to the null rate of 69% obtained by predicting the most numerous

380

10. boosting and additive trees

figure 10.21. geological prediction maps of the presence id203 (left
map) and catch size (right map) obtained from the gradient boosted models.

class prof/man (professional/managerial). the four best predicted classes
are seen to be retired, student, prof/man, and homemaker.

figure 10.23 shows the relative predictor variable importances as aver-
aged over all classes (10.46). figure 10.24 displays the individual relative
importance distributions (10.45) for each of the four best predicted classes.
one sees that the most relevant predictors are generally di   erent for each
respective class. an exception is age which is among the three most relevant
for predicting retired, student, and prof/man.

figure 10.25 shows the partial dependence of the log-odds (10.52) on age
for these three classes. the abscissa values are ordered codes for respective
equally spaced age intervals. one sees that after accounting for the contri-
butions of the other variables, the odds of being retired are higher for older
people, whereas the opposite is the case for being a student. the odds of
being professional/managerial are highest for middle-aged people. these
results are of course not surprising. they illustrate that inspecting partial
dependences separately for each class can lead to sensible results.

bibliographic notes

schapire (1990) developed the    rst simple boosting procedure in the pac
learning framework (valiant, 1984; kearns and vazirani, 1994). schapire

10.14 illustrations

381

overall error rate = 0.425

student

retired

prof/man

homemaker

labor

clerical

military

unemployed

sales

0.0

0.2

0.4

0.6

0.8

1.0

error rate

figure 10.22. error rate for each occupation in the demographics data.

yrs-ba

children

num-hsld

lang

typ-home

mar-stat

ethnic

sex

mar-dlinc

hsld-stat

edu

income

age

0

20

40

60

80

100

relative importance

figure 10.23. relative importance of the predictors as averaged over all
classes for the demographics data.

382

10. boosting and additive trees

class =  retired

class =  student

children
yrs-ba
lang
mar-dlinc
sex
typ-home
num-hsld
ethnic
edu
mar-stat
income
age
hsld-stat

0

20

40

60

80

100

0

20

40

60

80

100

relative importance

class =  prof/man

relative importance

class =  homemaker

yrs-ba
hsld-stat
age
income
typ-home
lang
mar-stat
edu
num-hsld
ethnic
children
mar-dlinc
sex

yrs-ba
num-hsld
edu
children
typ-home
lang
mar-stat
hsld-stat
income
ethnic
sex
mar-dlinc
age

children
yrs-ba
mar-stat
lang
num-hsld
sex
typ-home
hsld-stat
ethnic
mar-dlinc
age
income
edu

0

20

40

60

80

100

0

20

40

60

80

100

relative importance

relative importance

figure 10.24. predictor variable importances separately for each of the four
classes with lowest error rate for the demographics data.

retired

student

10.14 illustrations

383

e
c
n
e
d
n
e
p
e
d

 
l
a
i
t
r
a
p

2

1

0

1
-

2
-

1

2

3

4
age

prof/man

5

6

7

1

2

3

5

6

7

4
age

e
c
n
e
d
n
e
p
e
d

 
l
a
i
t
r
a
p

e
c
n
e
d
n
e
p
e
d

 
l

a

i
t
r
a
p

4

3

2

1

0

2

1

0

1
-

2
-

1

2

3

5

6

7

4
age

figure 10.25. partial dependence of the odds of three di   erent occupations
on age, for the demographics data.

showed that a weak learner could always improve its performance by train-
ing two additional classi   ers on    ltered versions of the input data stream.
a weak learner is an algorithm for producing a two-class classi   er with
performance guaranteed (with high id203) to be signi   cantly better
than a coin-   ip. after learning an initial classi   er g1 on the    rst n training
points,

    g2 is learned on a new sample of n points, half of which are misclas-

si   ed by g1;

    g3 is learned on n points for which g1 and g2 disagree;
    the boosted classi   er is gb = majority vote(g1, g2, g3).

schapire   s    strength of weak learnability    theorem proves that gb has
improved performance over g1.

freund (1995) proposed a    boost by majority    variation which combined
many weak learners simultaneously and improved the performance of the
simple boosting algorithm of schapire. the theory supporting both of these

384

10. boosting and additive trees

algorithms requires the weak learner to produce a classi   er with a    xed
error rate. this led to the more adaptive and realistic adaboost (freund
and schapire, 1996a) and its o   spring, where this assumption was dropped.
freund and schapire (1996a) and schapire and singer (1999) provide
some theory to support their algorithms, in the form of upper bounds on
generalization error. this theory has evolved in the computational learning
community, initially based on the concepts of pac learning. other theo-
ries attempting to explain boosting come from game theory (freund and
schapire, 1996b; breiman, 1999; breiman, 1998), and vc theory (schapire
et al., 1998). the bounds and the theory associated with the adaboost
algorithms are interesting, but tend to be too loose to be of practical im-
portance. in practice, boosting achieves results far more impressive than
the bounds would imply. schapire (2002) and meir and r  atsch (2003) give
useful overviews more recent than the    rst edition of this book.

friedman et al. (2000) and friedman (2001) form the basis for our expo-
sition in this chapter. friedman et al. (2000) analyze adaboost statistically,
derive the exponential criterion, and show that it estimates the log-odds
of the class id203. they propose additive tree models, the right-sized
trees and anova representation of section 10.11, and the multiclass logit
formulation. friedman (2001) developed gradient boosting and shrinkage
for classi   cation and regression, while friedman (1999) explored stochastic
variants of boosting. mason et al. (2000) also embraced a gradient approach
to boosting. as the published discussions of friedman et al. (2000) shows,
there is some controversy about how and why boosting works.

since the publication of the    rst edition of this book, these debates have
continued, and spread into the statistical community with a series of papers
on consistency of boosting (jiang, 2004; lugosi and vayatis, 2004; zhang
and yu, 2005; bartlett and traskin, 2007). mease and wyner (2008),
through a series of simulation examples, challenge some of our interpre-
tations of boosting; our response (friedman et al., 2008a) puts most of
these objections to rest. a recent survey by b  uhlmann and hothorn (2007)
supports our approach to boosting.

exercises

ex. 10.1 derive expression (10.12) for the update parameter in adaboost.

ex. 10.2 prove result (10.16), that is, the minimizer of the population
version of the adaboost criterion, is one-half of the log odds.

ex. 10.3 show that the marginal average (10.47) recovers additive and
multiplicative functions (10.50) and (10.51), while the conditional expec-
tation (10.49) does not.

exercises

385

ex. 10.4

(a) write a program implementing adaboost with trees.

(b) redo the computations for the example of figure 10.2. plot the train-

ing error as well as test error, and discuss its behavior.

(c) investigate the number of iterations needed to make the test error

   nally start to rise.

(d) change the setup of this example as follows: de   ne two classes, with
the features in class 1 being x1, x2, . . . , x10, standard indepen-
dent gaussian variates. in class 2, the features x1, x2, . . . , x10 are
also standard independent gaussian, but conditioned on the event
j > 12. now the classes have signi   cant overlap in feature space.
repeat the adaboost experiments as in figure 10.2 and discuss the
results.

pj x 2

ex. 10.5 multiclass exponential loss (zhu et al., 2005). for a k-class clas-
si   cation problem, consider the coding y = (y1, . . . , yk )t with

yk =(cid:26) 1,
    1
k   1 ,
let f = (f1, . . . , fk )t withpk
k=1 fk = 0, and de   ne

if g = gk
otherwise.

l(y, f ) = exp(cid:18)   

1
k

y t f(cid:19) .

(10.55)

(10.56)

(a) using lagrange multipliers, derive the population minimizer f     of
l(y, f ), subject to the zero-sum constraint, and relate these to the
class probabilities.

(b) show that a multiclass boosting using this id168 leads to a

reweighting algorithm similar to adaboost, as in section 10.4.

ex. 10.6 mcnemar test (agresti, 1996). we report the test error rates on
the spam data to be 5.5% for a generalized additive model (gam), and
4.5% for gradient boosting (gbm), with a test sample of size 1536.

(a) show that the standard error of these estimates is about 0.6%.

since the same test data are used for both methods, the error rates are
correlated, and we cannot perform a two-sample t-test. we can compare
the methods directly on each test observation, leading to the summary

gbm

gam
correct
error

correct error
18
51

1434
33

386

10. boosting and additive trees

the mcnemar test focuses on the discordant errors, 33 vs. 18.

(b) conduct a test to show that gam makes signi   cantly more errors

than gradient boosting, with a two-sided p-value of 0.036.

ex. 10.7 derive expression (10.32).

ex. 10.8 consider a k-class problem where the targets yik are coded as
1 if observation i is in class k and zero otherwise. suppose we have a
k=1 fk(x) = 0 (see (10.21) in
section 10.6). we wish to update the model for observations in a region r
in predictor space, by adding constants fk(x) +   k, with   k = 0.

current model fk(x), k = 1, . . . , k, with pk

(a) write down the multinomial log-likelihood for this problem, and its

   rst and second derivatives.

(b) using only the diagonal of the hessian matrix in (1), and starting
from   k = 0    k, show that a one-step approximate newton update
for   k is

(c) we prefer our update to sum to zero, as the current model does. using

  1

k = pxi   r(yik     pik)
pxi   r pik(1     pik)
where pik = exp(fk(xi))/ exp(pk
kx   =1

symmetry arguments, show that

k     1
k

k    

    k =

1
k

(  1

is an appropriate update, where   1
k = 1, . . . , k.

, k = 1, . . . , k     1,

(10.57)

   =1 f   (xi)).

  1
    ), k = 1, . . . , k

(10.58)

k is de   ned as in (10.57) for all

ex. 10.9 consider a k-class problem where the targets yik are coded as
1 if observation i is in class k and zero otherwise. using the multinomial
deviance id168 (10.22) and the symmetric logistic transform, use
the arguments leading to the gradient boosting algorithm 10.3 to derive
algorithm 10.4. hint: see exercise 10.8 for step 2(b)iii.

ex. 10.10 show that for k = 2 class classi   cation, only one tree needs to
be grown at each gradient-boosting iteration.

ex. 10.11 show how to compute the partial dependence function fs (xs )
in (10.47) e   ciently.
ex. 10.12 referring to (10.49), let s = {1} and c = {2}, with f (x1, x2) =
x1. assume x1 and x2 are bivariate gaussian, each with mean zero, vari-
ance one, and e(x1x2) =   . show that e(f (x1, x2)|x2) =   x2, even
though f is not a function of x2.

exercises

387

algorithm 10.4 gradient boosting for k-class classi   cation.

1. initialize fk0(x) = 0, k = 1, 2, . . . , k.

2. for m=1 to m :

(a) set

pk(x) =

(b) for k = 1 to k:

, k = 1, 2, . . . , k.

efk(x)
   =1 ef   (x)

pk

i. compute rikm = yik     pk(xi), i = 1, 2, . . . , n .
ii. fit a regression tree to the targets rikm, i = 1, 2, . . . , n ,

giving terminal regions rjkm, j = 1, 2, . . . , jm.

iii. compute

  jkm =

k     1
k

rikm

pxi   rjkm

pxi   rjkm |rikm|(1     |rikm|)

iv. update fkm(x) = fk,m   1(x) +pjm

3. output   fk(x) = fkm (x), k = 1, 2, . . . , k.

j=1   jkmi(x     rjkm).

, j = 1, 2, . . . , jm.

388

10. boosting and additive trees

11
neural networks

this is page 389
printer: opaque this

11.1

introduction

in this chapter we describe a class of learning methods that was developed
separately in di   erent    elds   statistics and arti   cial intelligence   based
on essentially identical models. the central idea is to extract linear com-
binations of the inputs as derived features, and then model the target as
a nonlinear function of these features. the result is a powerful learning
method, with widespread applications in many    elds. we    rst discuss the
projection pursuit model, which evolved in the domain of semiparamet-
ric statistics and smoothing. the rest of the chapter is devoted to neural
network models.

11.2 projection pursuit regression

as in our generic supervised learning problem, assume we have an input
vector x with p components, and a target y . let   m, m = 1, 2, . . . , m, be
unit p-vectors of unknown parameters. the projection pursuit regression
(ppr) model has the form

f (x) =

gm(  t

mx).

(11.1)

mxm=1

this is an additive model, but in the derived features vm =   t
mx rather
than the inputs themselves. the functions gm are unspeci   ed and are esti-

390

neural networks

g(v )

g(v )

x2

x1

x2

x1

figure 11.1. perspective plots of two ridge functions.

(left:) g(v ) = 1/[1 + exp(   5(v     0.5))], where v = (x1 + x2)/   2.

(right:) g(v ) = (v + 0.1) sin(1/(v /3 + 0.1)), where v = x1.

mated along with the directions   m using some    exible smoothing method
(see below).

the function gm(  t

mx) is called a ridge function in irp. it varies only
in the direction de   ned by the vector   m. the scalar variable vm =   t
mx
is the projection of x onto the unit vector   m, and we seek   m so that
the model    ts well, hence the name    projection pursuit.    figure 11.1 shows
some examples of ridge functions. in the example on the left    = (1/   2)(1, 1)t ,
so that the function only varies in the direction x1 + x2. in the example
on the right,    = (1, 0).

the ppr model (11.1) is very general, since the operation of forming
nonlinear functions of linear combinations generates a surprisingly large
class of models. for example, the product x1    x2 can be written as [(x1 +
x2)2     (x1     x2)2]/4, and higher-order products can be represented simi-
larly.
in fact, if m is taken arbitrarily large, for appropriate choice of gm the
ppr model can approximate any continuous function in irp arbitrarily
well. such a class of models is called a universal approximator. however
this generality comes at a price. interpretation of the    tted model is usually
di   cult, because each input enters into the model in a complex and multi-
faceted way. as a result, the ppr model is most useful for prediction, and
not very useful for producing an understandable model for the data. the
m = 1 model, known as the single index model in econometrics, is an
exception. it is slightly more general than the id75 model, and
o   ers a similar interpretation.

how do we    t a ppr model, given training data (xi, yi), i = 1, 2, . . . , n ?

we seek the approximate minimizers of the error function

nxi=1"yi    

mxm=1

gm(  t

mxi)#2

(11.2)

11.2 projection pursuit regression

391

over functions gm and direction vectors   m, m = 1, 2, . . . , m . as in other
smoothing problems, we need either explicitly or implicitly to impose com-
plexity constraints on the gm, to avoid over   t solutions.

consider just one term (m = 1, and drop the subscript). given the
direction vector   , we form the derived variables vi =   t xi. then we have
a one-dimensional smoothing problem, and we can apply any scatterplot
smoother, such as a smoothing spline, to obtain an estimate of g.

on the other hand, given g, we want to minimize (11.2) over   . a gauss   
newton search is convenient for this task. this is a quasi-id77,
in which the part of the hessian involving the second derivative of g is
discarded. it can be simply derived as follows. let   old be the current
estimate for   . we write

g(  t xi)     g(  t

oldxi) + g   (  t

oldxi)(         old)t xi

(11.3)

to give

nxi=1(cid:2)yi     g(  t xi)(cid:3)2

   

nxi=1

g   (  t

oldxi)2(cid:20)(cid:18)  t

oldxi +

yi     g(  t
g   (  t

oldxi)

oldxi) (cid:19)       t xi(cid:21)2

.

(11.4)
to minimize the right-hand side, we carry out a least squares regression
oldxi) on the input xi, with weights
oldxi)2 and no intercept (bias) term. this produces the updated coef-

oldxi +(yi   g(  t

oldxi))/g   (  t

with target   t
g   (  t
   cient vector   new.

these two steps, estimation of g and   , are iterated until convergence.
with more than one term in the ppr model, the model is built in a forward
stage-wise manner, adding a pair (  m, gm) at each stage.

there are a number of implementation details.

    although any smoothing method can in principle be used, it is conve-
nient if the method provides derivatives. local regression and smooth-
ing splines are convenient.

    after each step the gm   s from previous steps can be readjusted using
the back   tting procedure described in chapter 9. while this may
lead ultimately to fewer terms, it is not clear whether it improves
prediction performance.

    usually the   m are not readjusted (partly to avoid excessive compu-

tation), although in principle they could be as well.

    the number of terms m is usually estimated as part of the forward
stage-wise strategy. the model building stops when the next term
does not appreciably improve the    t of the model. cross-validation
can also be used to determine m .

392

neural networks

there are many other applications, such as density estimation (friedman
et al., 1984; friedman, 1987), where the projection pursuit idea can be used.
in particular, see the discussion of ica in section 14.7 and its relationship
with exploratory projection pursuit. however the projection pursuit re-
gression model has not been widely used in the    eld of statistics, perhaps
because at the time of its introduction (1981), its computational demands
exceeded the capabilities of most readily available computers. but it does
represent an important intellectual advance, one that has blossomed in its
reincarnation in the    eld of neural networks, the topic of the rest of this
chapter.

11.3 neural networks

the term neural network has evolved to encompass a large class of models
and learning methods. here we describe the most widely used    vanilla    neu-
ral net, sometimes called the single hidden layer back-propagation network,
or single layer id88. there has been a great deal of hype surrounding
neural networks, making them seem magical and mysterious. as we make
clear in this section, they are just nonlinear statistical models, much like
the projection pursuit regression model discussed above.

a neural network is a two-stage regression or classi   cation model, typ-
ically represented by a network diagram as in figure 11.2. this network
applies both to regression or classi   cation. for regression, typically k = 1
and there is only one output unit y1 at the top. however, these networks
can handle multiple quantitative responses in a seaid113ss fashion, so we will
deal with the general case.

for k-class classi   cation, there are k units at the top, with the kth
unit modeling the id203 of class k. there are k target measurements
yk, k = 1, . . . , k, each being coded as a 0     1 variable for the kth class.
derived features zm are created from linear combinations of the inputs,
and then the target yk is modeled as a function of linear combinations of
the zm,

zm =   (  0m +   t
tk =   0k +   t

k z, k = 1, . . . , k,

mx), m = 1, . . . , m,

(11.5)

fk(x) = gk(t ), k = 1, . . . , k,

where z = (z1, z2, . . . , zm ), and t = (t1, t2, . . . , tk ).

the activation function   (v) is usually chosen to be the sigmoid   (v) =
1/(1 + e   v); see figure 11.3 for a plot of 1/(1 + e   v). sometimes gaussian
radial basis functions (chapter 6) are used for the   (v), producing what is
known as a id80.

neural network diagrams like figure 11.2 are sometimes drawn with an
additional bias unit feeding into every unit in the hidden and output layers.

11.3 neural networks

393

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 y
 y
1
1

 y
 y
2
2

 y
 y
k
k

 z1
 z1

 z2
 z2

 z
 z

3
3

 z
 z
m
m

 x x
1
1

 x
 x2
2

 x
 x3
3

 x p-1
 x p-1

 xp
 x
p

figure 11.2. schematic of a single hidden layer, feed-forward neural network.

                                                                                                                                                

thinking of the constant    1    as an additional input feature, this bias unit
captures the intercepts   0m and   0k in model (11.5).

the output function gk(t ) allows a    nal transformation of the vector of
outputs t . for regression we typically choose the identity function gk(t ) =
tk. early work in k-class classi   cation also used the identity function, but
this was later abandoned in favor of the softmax function

gk(t ) =

.

(11.6)

etk
   =1 et   

pk

this is of course exactly the transformation used in the multilogit model
(section 4.4), and produces positive estimates that sum to one. in sec-
tion 4.2 we discuss other problems with linear id180, in par-
ticular potentially severe masking e   ects.

the units in the middle of the network, computing the derived features
zm, are called hidden units because the values zm are not directly ob-
served. in general there can be more than one hidden layer, as illustrated
in the example at the end of this chapter. we can think of the zm as a
basis expansion of the original inputs x; the neural network is then a stan-
dard linear model, or linear multilogit model, using these transformations
as inputs. there is, however, an important enhancement over the basis-
expansion techniques discussed in chapter 5; here the parameters of the
basis functions are learned from the data.

394

neural networks

)
v
   
e
+
1
(
/
1

0
.
1

5
.
0

0
.
0

-10

-5

5

10

0

v

figure 11.3. plot of the sigmoid function   (v) = 1/(1+exp(   v)) (red curve),
commonly used in the hidden layer of a neural network. included are   (sv) for
s = 1
2 (blue curve) and s = 10 (purple curve). the scale parameter s controls
the activation rate, and we can see that large s amounts to a hard activation at
v = 0. note that   (s(v     v0)) shifts the activation threshold from 0 to v0.

notice that if    is the identity function, then the entire model collapses
to a linear model in the inputs. hence a neural network can be thought of
as a nonlinear generalization of the linear model, both for regression and
classi   cation. by introducing the nonlinear transformation   , it greatly
enlarges the class of linear models. in figure 11.3 we see that the rate of
activation of the sigmoid depends on the norm of   m, and if k  mk is very
small, the unit will indeed be operating in the linear part of its activation
function.

notice also that the neural network model with one hidden layer has
exactly the same form as the projection pursuit model described above.
the di   erence is that the ppr model uses nonparametric functions gm(v),
while the neural network uses a far simpler function based on   (v), with
three free parameters in its argument. in detail, viewing the neural network
model as a ppr model, we identify

gm(  t

mx) =   m  (  0m +   t

mx)

=   m  (  0m + k  mk(  t

mx)),

(11.7)

where   m =   m/k  mk is the mth unit-vector. since     ,  0,s(v) =     (  0 +
sv) has lower complexity than a more general nonparametric g(v), it is not
surprising that a neural network might use 20 or 100 such functions, while
the ppr model typically uses fewer terms (m = 5 or 10, for example).

finally, we note that the name    neural networks    derives from the fact
that they were    rst developed as models for the human brain. each unit
represents a neuron, and the connections (links in figure 11.2) represent
synapses. in early models, the neurons    red when the total signal passed to
that unit exceeded a certain threshold. in the model above, this corresponds

11.4 fitting neural networks

395

to use of a step function for   (z) and gm(t ). later the neural network was
recognized as a useful tool for nonlinear statistical modeling, and for this
purpose the step function is not smooth enough for optimization. hence the
step function was replaced by a smoother threshold function, the sigmoid
in figure 11.3.

11.4 fitting neural networks

the neural network model has unknown parameters, often called weights,
and we seek values for them that make the model    t the training data well.
we denote the complete set of weights by   , which consists of

{  0m,   m; m = 1, 2, . . . , m} m (p + 1) weights,
{  0k,   k; k = 1, 2, . . . , k} k(m + 1) weights.

(11.8)

for regression, we use sum-of-squared errors as our measure of    t (error
function)

r(  ) =

kxk=1

nxi=1

(yik     fk(xi))2.

(11.9)

for classi   cation we use either squared error or cross-id178 (deviance):

r(  ) =    

nxi=1

kxk=1

yik log fk(xi),

(11.10)

and the corresponding classi   er is g(x) = argmaxkfk(x). with the softmax
activation function and the cross-id178 error function, the neural network
model is exactly a linear id28 model in the hidden units, and
all the parameters are estimated by maximum likelihood.

typically we don   t want the global minimizer of r(  ), as this is likely
to be an over   t solution. instead some id173 is needed: this is
achieved directly through a penalty term, or indirectly by early stopping.
details are given in the next section.

the generic approach to minimizing r(  ) is by id119, called
back-propagation in this setting. because of the compositional form of the
model, the gradient can be easily derived using the chain rule for di   eren-
tiation. this can be computed by a forward and backward sweep over the
network, keeping track only of quantities local to each unit.

396

neural networks

here is back-propagation in detail for squared error loss. let zmi =
mxi), from (11.5) and let zi = (z1i, z2i, . . . , zm i). then we have

  (  0m +   t

r(  )    

=

nxi=1
nxi=1

ri

kxk=1
(yik     fk(xi))2,

with derivatives

   ri
     km

   ri
     m   

=    2(yik     fk(xi))g   

k(  t

k zi)zmi,

=    

kxk=1

2(yik     fk(xi))g   

k(  t

k zi)  km     (  t

mxi)xi   .

(11.11)

(11.12)

given these derivatives, a id119 update at the (r + 1)st iter-

ation has the form

  (r+1)
km =   (r)

km       r

m    =   (r)
  (r+1)

m          r

nxi=1
nxi=1

   ri
     (r)
km

,

   ri
     (r)
m   

,

where   r is the learning rate, discussed below.

now write (11.12) as

   ri
     km
   ri
     m   

=   kizmi,

= smixi   .

(11.13)

(11.14)

the quantities   ki and smi are    errors    from the current model at the
output and hidden layer units, respectively. from their de   nitions, these
errors satisfy

smi =      (  t

mxi)

kxk=1

  km  ki,

(11.15)

known as the back-propagation equations. using this, the updates in (11.13)
can be implemented with a two-pass algorithm. in the forward pass, the
current weights are    xed and the predicted values   fk(xi) are computed
from formula (11.5). in the backward pass, the errors   ki are computed,
and then back-propagated via (11.15) to give the errors smi. both sets of
errors are then used to compute the gradients for the updates in (11.13),
via (11.14).

11.5 some issues in training neural networks

397

this two-pass procedure is what is known as back-propagation. it has
also been called the delta rule (widrow and ho   , 1960). the computational
components for cross-id178 have the same form as those for the sum of
squares error function, and are derived in exercise 11.3.

the advantages of back-propagation are its simple, local nature. in the
back propagation algorithm, each hidden unit passes and receives infor-
mation only to and from units that share a connection. hence it can be
implemented e   ciently on a parallel architecture computer.

the updates in (11.13) are a kind of batch learning, with the parame-
ter updates being a sum over all of the training cases. learning can also
be carried out online   processing each observation one at a time, updat-
ing the gradient after each training case, and cycling through the training
cases many times. in this case, the sums in equations (11.13) are replaced
by a single summand. a training epoch refers to one sweep through the
entire training set. online training allows the network to handle very large
training sets, and also to update the weights as new observations come in.
the learning rate   r for batch learning is usually taken to be a con-
stant, and can also be optimized by a line search that minimizes the error
function at each update. with online learning   r should decrease to zero
as the iteration r        . this learning is a form of stochastic approxima-
tion (robbins and munro, 1951); results in this    eld ensure convergence if
r <     (satis   ed, for example, by   r = 1/r).
back-propagation can be very slow, and for that reason is usually not
the method of choice. second-order techniques such as newton   s method
are not attractive here, because the second derivative matrix of r (the
hessian) can be very large. better approaches to    tting include conjugate
gradients and variable metric methods. these avoid explicit computation
of the second derivative matrix while still providing faster convergence.

  r     0,pr   r =    , andpr   2

11.5 some issues in training neural networks

there is quite an art in training neural networks. the model is generally
overparametrized, and the optimization problem is nonconvex and unstable
unless certain guidelines are followed. in this section we summarize some
of the important issues.

11.5.1 starting values

note that if the weights are near zero, then the operative part of the sigmoid
(figure 11.3) is roughly linear, and hence the neural network collapses into
an approximately linear model (exercise 11.2). usually starting values for
weights are chosen to be random values near zero. hence the model starts
out nearly linear, and becomes nonlinear as the weights increase. individual

398

neural networks

units localize to directions and introduce nonlinearities where needed. use
of exact zero weights leads to zero derivatives and perfect symmetry, and
the algorithm never moves. starting instead with large weights often leads
to poor solutions.

11.5.2 over   tting

often neural networks have too many weights and will over   t the data at
the global minimum of r. in early developments of neural networks, either
by design or by accident, an early stopping rule was used to avoid over-
   tting. here we train the model only for a while, and stop well before we
approach the global minimum. since the weights start at a highly regular-
ized (linear) solution, this has the e   ect of shrinking the    nal model toward
a linear model. a validation dataset is useful for determining when to stop,
since we expect the validation error to start increasing.

a more explicit method for id173 is weight decay, which is anal-
ogous to ridge regression used for linear models (section 3.4.1). we add a
penalty to the error function r(  ) +   j(  ), where

j(  ) =xk,m

  2

km +xm,   

  2

m   

(11.16)

and        0 is a tuning parameter. larger values of    will tend to shrink
the weights toward zero: typically cross-validation is used to estimate   .
the e   ect of the penalty is to simply add terms 2  km and 2  m    to the
respective gradient expressions (11.13). other forms for the penalty have
been proposed, for example,

j(  ) =xk,m

  2
km
1 +   2

km

+xm,   

  2

m   
1 +   2

m   

,

(11.17)

known as the weight elimination penalty. this has the e   ect of shrinking
smaller weights more than (11.16) does.

figure 11.4 shows the result of training a neural network with ten hidden
units, without weight decay (upper panel) and with weight decay (lower
panel), to the mixture example of chapter 2. weight decay has clearly
improved the prediction. figure 11.5 shows heat maps of the estimated
weights from the training (grayscale versions of these are called hinton
diagrams.) we see that weight decay has dampened the weights in both
layers: the resulting weights are spread fairly evenly over the ten hidden
units.

11.5.3 scaling of the inputs

since the scaling of the inputs determines the e   ective scaling of the weights
in the bottom layer, it can have a large e   ect on the quality of the    nal

11.5 some issues in training neural networks

399

neural network - 10 units, no weight decay

o

o

o

o

o

o
o

o
o
o
o

. . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . .
. . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
. . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
. .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. .
. .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . .
. . . . .
o
o
o
.
. . . . .
. . .
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . .
o
o
o
oo
o
.
. . . . .
. . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . .
o
o
oo
o
o
.
. . . . .
. . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . .
. . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . .
o
o
o
o
. . . . . . . . .
. . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . .
. . . . .
. . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . .
o
o
o
o
oo
. . . . . . . . . . .
. . . . . . . . . . .
. . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
o
o
. . . . .
. . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . .
. . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . .
o
o
. . . . .
. . . . . . . . . .
. . .
. . . . . . . . . .
. . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . .
o
. . . . . . . . .
. . . . .
. . . . . . . . .
. . . .
. . . . .
. . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . .
o
o
o
o
. . . . . . . . .
. . . . .
. . . . . . .
. . . . . . . . .
. .
. . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . .
o
o
o
o
o
. . . .
. . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . .
. . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
. . . .
. . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . . . . . . . .
. . . . . . . .
. . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . .
. . . .
. . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . .
. . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . .
. . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . .
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
oo
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
.
. . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
. . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . .
. . .
. . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . .
.
. . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.100
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.259
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o
o
o

o
o

o
o
o

o
o

o

o

o

o

o

neural network - 10 units, weight decay=0.02 

o

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
.
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
.
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
. .
o
o
oo
o
o
. . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . .
. . . . . . . .
o
oo
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . .
. . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
o
o
o
. . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.160
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.223
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .. .. .. .. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o
o
o

o
o

o
o
o

o
o

o
o

o
o

o

o

o

o

o

o

o

o

figure 11.4. a neural network on the mixture example of chapter 2. the
upper panel uses no weight decay, and over   ts the training data. the lower panel
uses weight decay, and achieves close to the bayes error rate (broken purple
boundary). both use the softmax activation function and cross-id178 error.

400

neural networks

no weight decay

weight decay

y2
y1

1

1
z

2
z

3
z

1
z

5
z

6
z

7
z

8
z

9
z

0
1
z

1

1
z

2
z

3
z

1
z

5
z

6
z

7
z

8
z

9
z

0
1
z

x2

x1

1

y2
y1

x2

x1

1

1
z

2
z

3
z

1
z

5
z

6
z

7
z

8
z

9
z

0
1
z

1
z

2
z

3
z

1
z

5
z

6
z

7
z

8
z

9
z

0
1
z

figure 11.5. heat maps of the estimated weights from the training of neural
networks from figure 11.4. the display ranges from bright green (negative) to
bright red (positive).

solution. at the outset it is best to standardize all inputs to have mean zero
and standard deviation one. this ensures all inputs are treated equally in
the id173 process, and allows one to choose a meaningful range for
the random starting weights. with standardized inputs, it is typical to take
random uniform weights over the range [   0.7, +0.7].

11.5.4 number of hidden units and layers

generally speaking it is better to have too many hidden units than too few.
with too few hidden units, the model might not have enough    exibility to
capture the nonlinearities in the data; with too many hidden units, the
extra weights can be shrunk toward zero if appropriate id173 is
used. typically the number of hidden units is somewhere in the range of
5 to 100, with the number increasing with the number of inputs and num-
ber of training cases. it is most common to put down a reasonably large
number of units and train them with id173. some researchers use
cross-validation to estimate the optimal number, but this seems unneces-
sary if cross-validation is used to estimate the id173 parameter.
choice of the number of hidden layers is guided by background knowledge
and experimentation. each layer extracts features of the input for regres-
sion or classi   cation. use of multiple hidden layers allows construction of
hierarchical features at di   erent levels of resolution. an example of the
e   ective use of multiple layers is given in section 11.6.

11.5.5 multiple minima

the error function r(  ) is nonconvex, possessing many local minima. as a
result, the    nal solution obtained is quite dependent on the choice of start-

11.6 example: simulated data

401

ing weights. one must at least try a number of random starting con   gura-
tions, and choose the solution giving lowest (penalized) error. probably a
better approach is to use the average predictions over the collection of net-
works as the    nal prediction (ripley, 1996). this is preferable to averaging
the weights, since the nonlinearity of the model implies that this averaged
solution could be quite poor. another approach is via id112, which aver-
ages the predictions of networks training from randomly perturbed versions
of the training data. this is described in section 8.7.

11.6 example: simulated data

we generated data from two additive error models y = f (x) +   :

sum of sigmoids: y =   (at

1 x) +   (at

2 x) +   1;

radial: y =

  (xm) +   2.

10ym=1

here x t = (x1, x2, . . . , xp), each xj being a standard gaussian variate,
with p = 2 in the    rst model, and p = 10 in the second.

for the sigmoid model, a1 = (3, 3), a2 = (3,   3); for the radial model,
  (t) = (1/2  )1/2 exp(   t2/2). both   1 and   2 are gaussian errors, with
variance chosen so that the signal-to-noise ratio

var(e(y |x))

var(y     e(y |x))

=

var(f (x))

var(  )

(11.18)

is 4 in both models. we took a training sample of size 100 and a test sample
of size 10, 000. we    t neural networks with weight decay and various num-
bers of hidden units, and recorded the average test error etest(y       f (x))2
for each of 10 random starting weights. only one training set was gen-
erated, but the results are typical for an    average    training set. the test
errors are shown in figure 11.6. note that the zero hidden unit model refers
to linear least squares regression. the neural network is perfectly suited to
the sum of sigmoids model, and the two-unit model does perform the best,
achieving an error close to the bayes rate. (recall that the bayes rate for
regression with squared error is the error variance; in the    gures, we report
test error relative to the bayes error). notice, however, that with more hid-
den units, over   tting quickly creeps in, and with some starting weights the
model does worse than the linear model (zero hidden unit) model. even
with two hidden units, two of the ten starting weight con   gurations pro-
duced results no better than the linear model, con   rming the importance
of multiple starting values.

a radial function is in a sense the most di   cult for the neural net, as it is
spherically symmetric and with no preferred directions. we see in the right

402

neural networks

sum of sigmoids

radial

r
o
r
r

e

 
t
s
e
t

0

.

3

5
2

.

0

.

2

5
1

.

0

.

1

r
o
r
r

e

 
t
s
e
t

0
3

5
2

0
2

5
1

0
1

5

0

0 1 2 3 4 5 6 7 8 9 10

0 1 2 3 4 5 6 7 8 9 10

number of hidden units

number of hidden units

figure 11.6. boxplots of test error, for simulated data example, relative to
the bayes error (broken horizontal line). true function is a sum of two sigmoids
on the left, and a radial function is on the right. the test error is displayed for
10 di   erent starting weights, for a single hidden layer neural network with the
number of units as indicated.

panel of figure 11.6 that it does poorly in this case, with the test error
staying well above the bayes error (note the di   erent vertical scale from
the left panel). in fact, since a constant    t (such as the sample average)
achieves a relative error of 5 (when the snr is 4), we see that the neural
networks perform increasingly worse than the mean.

in this example we used a    xed weight decay parameter of 0.0005, rep-
resenting a mild amount of id173. the results in the left panel of
figure 11.6 suggest that more id173 is needed with greater num-
bers of hidden units.

in figure 11.7 we repeated the experiment for the sum of sigmoids model,
with no weight decay in the left panel, and stronger weight decay (   = 0.1)
in the right panel. with no weight decay, over   tting becomes even more
severe for larger numbers of hidden units. the weight decay value    = 0.1
produces good results for all numbers of hidden units, and there does not
appear to be over   tting as the number of units increase. finally, figure 11.8
shows the test error for a ten hidden unit network, varying the weight decay
parameter over a wide range. the value 0.1 is approximately optimal.

in summary, there are two free parameters to select: the weight decay   
and number of hidden units m . as a learning strategy, one could    x either
parameter at the value corresponding to the least constrained model, to
ensure that the model is rich enough, and use cross-validation to choose
the other parameter. here the least constrained values are zero weight decay
and ten hidden units. comparing the left panel of figure 11.7 to figure
11.8, we see that the test error is less sensitive to the value of the weight

11.6 example: simulated data

403

no weight decay

weight decay=0.1

r
o
r
r

e

 
t
s
e
t

0

.

3

5

.

2

0

.

2

5
1

.

0

.

1

r
o
r
r

e

 
t
s
e
t

0

.

3

5

.

2

0

.

2

5
1

.

0

.

1

0 1 2 3 4 5 6 7 8 9 10

0 1 2 3 4 5 6 7 8 9 10

number of hidden units

number of hidden units

figure 11.7. boxplots of test error, for simulated data example, relative to the
bayes error. true function is a sum of two sigmoids. the test error is displayed
for ten di   erent starting weights, for a single hidden layer neural network with
the number units as indicated. the two panels represent no weight decay (left)
and strong weight decay    = 0.1 (right).

sum of sigmoids, 10 hidden unit model

r
o
r
r

e

 
t
s
e
t

2

.

2

0

.

2

8

.

1

6

.

1

4

.

1

2

.

1

0
1

.

0.00

0.02

0.04

0.06

0.08

0.10

0.12

0.14

weight decay parameter

figure 11.8. boxplots of test error, for simulated data example. true function
is a sum of two sigmoids. the test error is displayed for ten di   erent starting
weights, for a single hidden layer neural network with ten hidden units and weight
decay parameter value as indicated.

404

neural networks

figure 11.9. examples of training cases from zip code data. each image is
a 16    16 8-bit grayscale representation of a handwritten digit.

decay parameter, and hence cross-validation of this parameter would be
preferred.

11.7 example: zip code data

this example is a character recognition task: classi   cation of handwritten
numerals. this problem captured the attention of the machine learning and
neural network community for many years, and has remained a benchmark
problem in the    eld. figure 11.9 shows some examples of normalized hand-
written digits, automatically scanned from envelopes by the u.s. postal
service. the original scanned digits are binary and of di   erent sizes and
orientations; the images shown here have been deslanted and size normal-
ized, resulting in 16   16 grayscale images (le cun et al., 1990). these 256
pixel values are used as inputs to the neural network classi   er.
a black box neural network is not ideally suited to this pattern recogni-
tion task, partly because the pixel representation of the images lack certain
invariances (such as small rotations of the image). consequently early at-
tempts with neural networks yielded misclassi   cation rates around 4.5%
on various examples of the problem. in this section we show some of the
pioneering e   orts to handcraft the neural network to overcome some these
de   ciencies (le cun, 1989), which ultimately led to the state of the art in
neural network performance(le cun et al., 1998)1.

although current digit datasets have tens of thousands of training and
test examples, the sample size here is deliberately modest in order to em-

1the    gures and tables in this example were recreated from le cun (1989).

11.7 example: zip code data

405

10

10

12

10

4x4

8x8

16x16

16x16

16x16

net-1

net-2

10

4x4

8x8x2

net-3

local connectivity

10

4x4x4

8x8x2

16x16

16x16

net-4

shared weights

net-5

figure 11.10. architecture of the    ve networks used in the zip code example.

phasize the e   ects. the examples were obtained by scanning some actual
hand-drawn digits, and then generating additional images by random hor-
izontal shifts. details may be found in le cun (1989). there are 320 digits
in the training set, and 160 in the test set.

five di   erent networks were    t to the data:

net-1: no hidden layer, equivalent to multinomial id28.

net-2: one hidden layer, 12 hidden units fully connected.

net-3: two hidden layers locally connected.

net-4: two hidden layers, locally connected with weight sharing.

net-5: two hidden layers, locally connected, two levels of weight sharing.

these are depicted in figure 11.10. net-1 for example has 256 inputs, one
each for the 16   16 input pixels, and ten output units for each of the digits
0   9. the predicted value   fk(x) represents the estimated id203 that
an image x has digit class k, for k = 0, 1, 2, . . . , 9.

406

neural networks

a
t
a
d

 
t
s
e
t
 
n
o
 
t
c
e
r
r
o
c
%

 

100

90

80

70

60

net-5

net-4

net-3

net-2

net-1

0

5

10

15

20

25

30

training epochs

figure 11.11. test performance curves, as a function of the number of train-
ing epochs, for the    ve networks of table 11.1 applied to the zip code data.
(le cun, 1989)

the networks all have sigmoidal output units, and were all    t with the
sum-of-squares error function. the    rst network has no hidden layer, and
hence is nearly equivalent to a linear multinomial regression model (exer-
cise 11.4). net-2 is a single hidden layer network with 12 hidden units, of
the kind described above.

the training set error for all of the networks was 0%, since in all cases
there are more parameters than training observations. the evolution of the
test error during the training epochs is shown in figure 11.11. the linear
network (net-1) starts to over   t fairly quickly, while test performance of
the others level o    at successively superior values.

the other three networks have additional features which demonstrate
the power and    exibility of the neural network paradigm. they introduce
constraints on the network, natural for the problem at hand, which allow
for more complex connectivity but fewer parameters.

net-3 uses local connectivity: this means that each hidden unit is con-
nected to only a small patch of units in the layer below. in the    rst hidden
layer (an 8   8 array), each unit takes inputs from a 3   3 patch of the input
layer; for units in the    rst hidden layer that are one unit apart, their recep-
tive    elds overlap by one row or column, and hence are two pixels apart.
in the second hidden layer, inputs are from a 5    5 patch, and again units
that are one unit apart have receptive    elds that are two units apart. the
weights for all other connections are set to zero. local connectivity makes
each unit responsible for extracting local features from the layer below, and

11.7 example: zip code data

407

table 11.1. test set performance of    ve di   erent neural networks on a hand-
written digit classi   cation example (le cun, 1989).

network architecture
single layer network

net-1:
net-2: two layer network
net-3: locally connected
net-4: constrained network 1
net-5: constrained network 2

links weights % correct
80.0%
2570
3214
87.0%
88.5%
1226
94.0%
2266
5194
98.4%

2570
3214
1226
1132
1060

reduces considerably the total number of weights. with many more hidden
units than net-2, net-3 has fewer links and hence weights (1226 vs. 3214),
and achieves similar performance.

net-4 and net-5 have local connectivity with shared weights. all units
in a local feature map perform the same operation on di   erent parts of the
image, achieved by sharing the same weights. the    rst hidden layer of net-
4 has two 8  8 arrays, and each unit takes input from a 3  3 patch just like
in net-3. however, each of the units in a single 8   8 feature map share the
same set of nine weights (but have their own bias parameter). this forces
the extracted features in di   erent parts of the image to be computed by
the same linear functional, and consequently these networks are sometimes
known as convolutional networks. the second hidden layer of net-4 has
no weight sharing, and is the same as in net-3. the gradient of the error
function r with respect to a shared weight is the sum of the gradients of
r with respect to each connection controlled by the weights in question.

table 11.1 gives the number of links, the number of weights and the
optimal test performance for each of the networks. we see that net-4 has
more links but fewer weights than net-3, and superior test performance.
net-5 has four 4    4 feature maps in the second hidden layer, each unit
connected to a 5    5 local patch in the layer below. weights are shared
in each of these feature maps. we see that net-5 does the best, having
errors of only 1.6%, compared to 13% for the    vanilla    network net-2.
the clever design of network net-5, motivated by the fact that features of
handwriting style should appear in more than one part of a digit, was the
result of many person years of experimentation. this and similar networks
gave better performance on zip code problems than any other learning
method at that time (early 1990s). this example also shows that neural
networks are not a fully automatic tool, as they are sometimes advertised.
as with all statistical models, subject matter knowledge can and should be
used to improve their performance.

this network was later outperformed by the tangent distance approach
(simard et al., 1993) described in section 13.3.3, which explicitly incorpo-
rates natural a   ne invariances. at this point the digit recognition datasets
become test beds for every new learning procedure, and researchers worked

408

neural networks

hard to drive down the error rates. as of this writing, the best error rates on
a large database (60, 000 training, 10, 000 test observations), derived from
standard nist2 databases, were reported to be the following: (le cun et
al., 1998):

    1.1% for tangent distance with a 1-nearest neighbor classi   er (sec-

tion 13.3.3);

    0.8% for a degree-9 polynomial id166 (section 12.3);
    0.8% for lenet-5, a more complex version of the convolutional net-

work described here;

    0.7% for boosted lenet-4. boosting is described in chapter 8. lenet-

4 is a predecessor of lenet-5.

le cun et al. (1998) report a much larger table of performance results, and
it is evident that many groups have been working very hard to bring these
test error rates down. they report a standard error of 0.1% on the error
estimates, which is based on a binomial average with n = 10, 000 and
p     0.01. this implies that error rates within 0.1   0.2% of one another
are statistically equivalent. realistically the standard error is even higher,
since the test data has been implicitly used in the tuning of the various
procedures.

11.8 discussion

both projection pursuit regression and neural networks take nonlinear func-
tions of linear combinations (   derived features   ) of the inputs. this is a
powerful and very general approach for regression and classi   cation, and
has been shown to compete well with the best learning methods on many
problems.

these tools are especially e   ective in problems with a high signal-to-noise
ratio and settings where prediction without interpretation is the goal. they
are less e   ective for problems where the goal is to describe the physical pro-
cess that generated the data and the roles of individual inputs. each input
enters into the model in many places, in a nonlinear fashion. some authors
(hinton, 1989) plot a diagram of the estimated weights into each hidden
unit, to try to understand the feature that each unit is extracting. this
is limited however by the lack of identi   ability of the parameter vectors
  m, m = 1, . . . , m . often there are solutions with   m spanning the same
linear space as the ones found during training, giving predicted values that

2the national institute of standards and technology maintain large databases, in-

cluding handwritten character databases; http://www.nist.gov/srd/.

11.9 bayesian neural nets and the nips 2003 challenge

409

are roughly the same. some authors suggest carrying out a principal com-
ponent analysis of these weights, to try to    nd an interpretable solution. in
general, the di   culty of interpreting these models has limited their use in
   elds like medicine, where interpretation of the model is very important.

there has been a great deal of research on the training of neural net-
works. unlike methods like cart and mars, neural networks are smooth
functions of real-valued parameters. this facilitates the development of
bayesian id136 for these models. the next sections discusses a success-
ful bayesian implementation of neural networks.

11.9 bayesian neural nets and the nips 2003

challenge

a classi   cation competition was held in 2003, in which    ve labeled train-
ing datasets were provided to participants. it was organized for a neural
information processing systems (nips) workshop. each of the data sets
constituted a two-class classi   cation problems, with di   erent sizes and from
a variety of domains (see table 11.2). feature measurements for a valida-
tion dataset were also available.

participants developed and applied statistical learning procedures to
make predictions on the datasets, and could submit predictions to a web-
site on the validation set for a period of 12 weeks. with this feedback,
participants were then asked to submit predictions for a separate test set
and they received their results. finally, the class labels for the validation
set were released and participants had one week to train their algorithms
on the combined training and validation sets, and submit their    nal pre-
dictions to the competition website. a total of 75 groups participated, with
20 and 16 eventually making submissions on the validation and test sets,
respectively.

there was an emphasis on feature extraction in the competition. arti-
   cial    probes    were added to the data: these are noise features with dis-
tributions resembling the real features but independent of the class labels.
the percentage of probes that were added to each dataset, relative to the
total set of features, is shown on table 11.2. thus each learning algorithm
had to    gure out a way of identifying the probes and downweighting or
eliminating them.

a number of metrics were used to evaluate the entries, including the
percentage correct on the test set, the area under the roc curve, and a
combined score that compared each pair of classi   ers head-to-head. the
results of the competition are very interesting and are detailed in guyon et
al. (2006). the most notable result: the entries of neal and zhang (2006)
were the clear overall winners. in the    nal competition they    nished    rst

410

neural networks

table 11.2. nips 2003 challenge data sets. the column labeled p is the number
of features. for the dorothea dataset the features are binary. ntr, nval and nte
are the number of training, validation and test cases, respectively

dataset

domain

feature
type
mass spectrometry dense
text classi   cation
sparse
sparse
dense
dense

arcene
dexter
dorothea drug discovery
gisette
madelon

digit recognition
arti   cial

p

10,000
20,000
100,000
5000
500

percent ntr
probes
30
50
50
30
96

100
300
800
6000
2000

nval nte

100
300
350
1000
600

700
2000
800
6500
1800

in three of the    ve datasets, and were 5th and 7th on the remaining two
datasets.

in their winning entries, neal and zhang (2006) used a series of pre-
processing feature-selection steps, followed by bayesian neural networks,
dirichlet di   usion trees, and combinations of these methods. here we focus
only on the bayesian neural network approach, and try to discern which
aspects of their approach were important for its success. we rerun their
programs and compare the results to boosted neural networks and boosted
trees, and other related methods.

11.9.1 bayes, boosting and id112

let us    rst review brie   y the bayesian approach to id136 and its appli-
cation to neural networks. given training data xtr, ytr, we assume a sam-
pling model with parameters   ; neal and zhang (2006) use a two-hidden-
layer neural network, with output nodes the class probabilities pr(y |x,   )
for the binary outcomes. given a prior distribution pr(  ), the posterior
distribution for the parameters is

(11.19)

pr(  |xtr, ytr) =

pr(  )pr(ytr|xtr,   )

r pr(  )pr(ytr|xtr,   )d  

for a test case with features xnew, the predictive distribution for the

label ynew is

pr(ynew|xnew, xtr, ytr) =z pr(ynew|xnew,   )pr(  |xtr, ytr)d   (11.20)

(c.f. equation 8.24). since the integral in (11.20) is intractable, sophisticated
id115 (mcmc) methods are used to sample from the
posterior distribution pr(ynew|xnew, xtr, ytr). a few hundred values    are
generated and then a simple average of these values estimates the integral.
neal and zhang (2006) use di   use gaussian priors for all of the parame-
ters. the particular mcmc approach that was used is called hybrid monte
carlo, and may be important for the success of the method. it includes
an auxiliary momentum vector and implements hamiltonian dynamics in
which the potential function is the target density. this is done to avoid

11.9 bayesian neural nets and the nips 2003 challenge

411

random walk behavior; the successive candidates move across the sample
space in larger steps. they tend to be less correlated and hence converge
to the target distribution more rapidly.

neal and zhang (2006) also tried di   erent forms of pre-processing of the

features:

1. univariate screening using t-tests, and

2. automatic relevance determination.

in the latter method (ard), the weights (coe   cients) for the jth feature
to each of the    rst hidden layer units all share a common prior variance
  2
j , and prior mean zero. the posterior distributions for each variance   2
j
are computed, and the features whose posterior variance concentrates on
small values are discarded.

there are thus three main features of this approach that could be im-

portant for its success:

(a) the feature selection and pre-processing,

(b) the neural network model, and

(c) the bayesian id136 for the model using mcmc.

according to neal and zhang (2006), feature screening in (a) is carried
out purely for computational e   ciency; the mcmc procedure is slow with
a large number of features. there is no need to use feature selection to avoid
over   tting. the posterior average (11.20) takes care of this automatically.
we would like to understand the reasons for the success of the bayesian
method. in our view, power of modern bayesian methods does not lie in
their use as a formal id136 procedure; most people would not believe
that the priors in a high-dimensional, complex neural network model are
actually correct. rather the bayesian/mcmc approach gives an e   cient
way of sampling the relevant parts of model space, and then averaging the
predictions for the high-id203 models.

id112 and boosting are non-bayesian procedures that have some simi-
larity to mcmc in a bayesian model. the bayesian approach    xes the data
and perturbs the parameters, according to current estimate of the poste-
rior distribution. id112 perturbs the data in an i.i.d fashion and then
re-estimates the model to give a new set of model parameters. at the end,
a simple average of the model predictions from di   erent bagged samples is
computed. boosting is similar to id112, but    ts a model that is additive
in the models of each individual base learner, which are learned using non
i.i.d. samples. we can write all of these models in the form

  f (xnew) =

lx   =1

w   e(ynew|xnew,        )

(11.21)

412

neural networks

in all cases the         are a large collection of model parameters. for the
bayesian model the w    = 1/l, and the average estimates the posterior
mean (11.21) by sampling       from the posterior distribution. for id112,
w    = 1/l as well, and the         are the parameters re   t to bootstrap re-
samples of the training data. for boosting, the weights are all equal to
1, but the         are typically chosen in a nonrandom sequential fashion to
constantly improve the    t.

11.9.2 performance comparisons

based on the similarities above, we decided to compare bayesian neural
networks to boosted trees, boosted neural networks, id79s and
bagged neural networks on the    ve datasets in table 11.2. id112 and
boosting of neural networks are not methods that we have previously used
in our work. we decided to try them here, because of the success of bayesian
neural networks in this competition, and the good performance of id112
and boosting with trees. we also felt that by id112 and boosting neural
nets, we could assess both the choice of model as well as the model search
strategy.

here are the details of the learning methods that were compared:

bayesian neural nets. the results here are taken from neal and zhang
(2006), using their bayesian approach to    tting neural networks. the
models had two hidden layers of 20 and 8 units. we re-ran some
networks for timing purposes only.

boosted trees. we used the gbm package (version 1.5-7) in the r language.
tree depth and shrinkage factors varied from dataset to dataset. we
consistently bagged 80% of the data at each boosting iteration (the
default is 50%). shrinkage was between 0.001 and 0.1. tree depth was
between 2 and 9.

boosted neural networks. since boosting is typically most e   ective with
   weak    learners, we boosted a single hidden layer neural network with
two or four units,    t with the nnet package (version 7.2-36) in r.

id79s. we used the r package randomforest (version 4.5-16)

with default settings for the parameters.

bagged neural networks. we used the same architecture as in the bayesian
neural network above (two hidden layers of 20 and 8 units),    t using
both neal   s c language package    flexible bayesian modeling    (2004-
11-10 release), and matlab neural-net toolbox (version 5.1).

11.9 bayesian neural nets and the nips 2003 challenge

413

univariate screened features

ard reduced features

bayesian neural nets
boosted trees 
boosted neural nets
id79s
bagged neural networks 

)

%

(
 
r
o
r
r

e

 
t
s
e
t

5
2

5
1

5

)

%

(
 
r
o
r
r

e

 
t
s
e
t

5
2

5
1

5

arcene

dexter

dorothea

gisette

madelon

arcene

dexter

dorothea

gisette

madelon

figure 11.12. performance of di   erent learning methods on    ve problems,
using both univariate screening of features (top panel) and a reduced feature set
from automatic relevance determination. the error bars at the top of each plot
have width equal to one standard error of the di   erence between two error rates.
on most of the problems several competitors are within this error bound.

this analysis was carried out by nicholas johnson, and full details may
be found in johnson (2008)3. the results are shown in figure 11.12 and
table 11.3.

the    gure and table show bayesian, boosted and bagged neural networks,
boosted trees, and id79s, using both the screened and reduced
features sets. the error bars at the top of each plot indicate one standard
error of the di   erence between two error rates. bayesian neural networks
again emerge as the winner, although for some datasets the di   erences
between the test error rates is not statistically signi   cant. id79s
performs the best among the competitors using the selected feature set,
while the boosted neural networks perform best with the reduced feature
set, and nearly match the bayesian neural net.

the superiority of boosted neural networks over boosted trees suggest
that the neural network model is better suited to these particular prob-
lems. speci   cally, individual features might not be good predictors here

3we also thank isabelle guyon for help in preparing the results of this section.

414

neural networks

table 11.3. performance of di   erent methods. values are average rank of test
error across the    ve problems (low is good), and mean computation time and
standard error of the mean, in minutes.

method

screened features
average

average

ard reduced features
average

average

bayesian neural networks
boosted trees
boosted neural networks
id79s
bagged neural networks

rank

1.5
3.4
3.8
2.7
3.6

time

384(138)
3.03(2.5)
9.4(8.6)
1.9(1.7)
3.5(1.1)

rank

1.6
4.0
2.2
3.2
4.0

time
600(186)
34.1(32.4)
35.6(33.5)
11.2(9.3)
6.4(4.4)

and linear combinations of features work better. however the impressive
performance of id79s is at odds with this explanation, and came
as a surprise to us.

since the reduced feature sets come from the bayesian neural network
approach, only the methods that use the screened features are legitimate,
self-contained procedures. however, this does suggest that better methods
for internal feature selection might help the overall performance of boosted
neural networks.

the table also shows the approximate training time required for each

method. here the non-bayesian methods show a clear advantage.

overall, the superior performance of bayesian neural networks here may

be due to the fact that

(a) the neural network model is well suited to these    ve problems, and

(b) the mcmc approach provides an e   cient way of exploring the im-
portant part of the parameter space, and then averaging the resulting
models according to their quality.

the bayesian approach works well for smoothly parametrized models like
neural nets; it is not yet clear that it works as well for non-smooth models
like trees.

11.10 computational considerations

with n observations, p predictors, m hidden units and l training epochs, a
neural network    t typically requires o(n pm l) operations. there are many
packages available for    tting neural networks, probably many more than
exist for mainstream statistical methods. because the available software
varies widely in quality, and the learning problem for neural networks is
sensitive to issues such as input scaling, such software should be carefully
chosen and tested.

exercises

415

bibliographic notes

projection pursuit was proposed by friedman and tukey (1974), and spe-
cialized to regression by friedman and stuetzle (1981). huber (1985) gives
a scholarly overview, and roosen and hastie (1994) present a formulation
using smoothing splines. the motivation for neural networks dates back
to mcculloch and pitts (1943), widrow and ho    (1960) (reprinted in an-
derson and rosenfeld (1988)) and rosenblatt (1962). hebb (1949) heavily
in   uenced the development of learning algorithms. the resurgence of neural
networks in the mid 1980s was due to werbos (1974), parker (1985) and
rumelhart et al. (1986), who proposed the back-propagation algorithm.
today there are many books written on the topic, for a broad range of
audiences. for readers of this book, hertz et al. (1991), bishop (1995) and
ripley (1996) may be the most informative. bayesian learning for neural
networks is described in neal (1996). the zip code example was taken from
le cun (1989); see also le cun et al. (1990) and le cun et al. (1998).

we do not discuss theoretical topics such as approximation properties of
neural networks, such as the work of barron (1993), girosi et al. (1995)
and jones (1992). some of these results are summarized by ripley (1996).

exercises

ex. 11.1 establish the exact correspondence between the projection pur-
suit regression model (11.1) and the neural network (11.5). in particular,
show that the single-layer regression network is equivalent to a ppr model
with gm(  t
mx)), where   m is the mth unit vector.
establish a similar equivalence for a classi   cation network.

mx) =   m  (  0m + sm(  t

ex. 11.2 consider a neural network for a quantitative outcome as in (11.5),
using squared-error loss and identity output function gk(t) = t. suppose
that the weights   m from the input to hidden layer are nearly zero. show
that the resulting model is nearly linear in the inputs.

ex. 11.3 derive the forward and backward propagation equations for the
cross-id178 id168.

ex. 11.4 consider a neural network for a k class outcome that uses cross-
id178 loss. if the network has no hidden layer, show that the model is
equivalent to the multinomial logistic model described in chapter 4.

ex. 11.5

(a) write a program to    t a single hidden layer neural network (ten hidden

units) via back-propagation and weight decay.

416

neural networks

(b) apply it to 100 observations from the model

y =   (at

1 x) + (at

2 x)2 + 0.30    z,

where    is the sigmoid function, z is standard normal, x t = (x1, x2),
each xj being independent standard normal, and a1 = (3, 3), a2 =
(3,   3). generate a test sample of size 1000, and plot the training and
test error curves as a function of the number of training epochs, for
di   erent values of the weight decay parameter. discuss the over   tting
behavior in each case.

(c) vary the number of hidden units in the network, from 1 up to 10, and
determine the minimum number needed to perform well for this task.

ex. 11.6 write a program to carry out projection pursuit regression, using
cubic smoothing splines with    xed degrees of freedom. fit it to the data
from the previous exercise, for various values of the smoothing parameter
and number of model terms. find the minimum number of model terms
necessary for the model to perform well and compare this to the number
of hidden units from the previous exercise.

ex. 11.7 fit a neural network to the spam data of section 9.1.2, and compare
the results to those for the additive model given in that chapter. compare
both the classi   cation performance and interpretability of the    nal model.

this is page 417
printer: opaque this

12
support vector machines and
flexible discriminants

12.1

introduction

in this chapter we describe generalizations of linear decision boundaries
for classi   cation. optimal separating hyperplanes are introduced in chap-
ter 4 for the case when two classes are linearly separable. here we cover
extensions to the nonseparable case, where the classes overlap. these tech-
niques are then generalized to what is known as the support vector machine,
which produces nonlinear boundaries by constructing a linear boundary in
a large, transformed version of the feature space. the second set of methods
generalize fisher   s id156 (lda). the generalizations
include    exible discriminant analysis which facilitates construction of non-
linear boundaries in a manner very similar to the support vector machines,
penalized discriminant analysis for problems such as signal and image clas-
si   cation where the large number of features are highly correlated, and
mixture discriminant analysis for irregularly shaped classes.

12.2 the support vector classi   er

in chapter 4 we discussed a technique for constructing an optimal separat-
ing hyperplane between two perfectly separated classes. we review this and
generalize to the nonseparable case, where the classes may not be separable
by a linear boundary.

418

12. flexible discriminants

xt    +   0 = 0

   

   
   

   
   
   

   

   

   

   

   

   
   

   

   

       
   

m = 1
k  k

   

   

margin

m = 1
k  k

   
   
   
     
5
   

   
   
   

   

xt    +   0 = 0

   

     
4     
4     
4

   
   
     
1     
1     
1

   

3     
     
3
   
     
2     
2     
2

       
   

   

   

   

   
   

   

   

m = 1
k  k

margin

m = 1
k  k

figure 12.1. support vector classi   ers. the left panel shows the separable
case. the decision boundary is the solid line, while broken lines bound the shaded
maximal margin of width 2m = 2/k  k. the right panel shows the nonseparable
(overlap) case. the points labeled      
j are on the wrong side of their margin by
an amount      
j = 0. the margin is
maximized subject to a total budget p   i     constant. hence p      
j is the total
distance of points on the wrong side of their margin.

j = m   j; points on the correct side have      

our training data consists of n pairs (x1, y1), (x2, y2), . . . , (xn , yn ), with

xi     irp and yi     {   1, 1}. de   ne a hyperplane by

{x : f (x) = xt    +   0 = 0},

(12.1)

where    is a unit vector: k  k = 1. a classi   cation rule induced by f (x) is
(12.2)

g(x) = sign[xt    +   0].

the geometry of hyperplanes is reviewed in section 4.5, where we show that
f (x) in (12.1) gives the signed distance from a point x to the hyperplane
f (x) = xt    +  0 = 0. since the classes are separable, we can    nd a function
f (x) = xt    +   0 with yif (xi) > 0    i. hence we are able to    nd the
hyperplane that creates the biggest margin between the training points for
class 1 and    1 (see figure 12.1). the optimization problem

max

  ,  0,k  k=1

m

subject to yi(xt

i    +   0)     m, i = 1, . . . , n,

(12.3)

captures this concept. the band in the    gure is m units away from the
hyperplane on either side, and hence 2m units wide. it is called the margin.

we showed that this problem can be more conveniently rephrased as

  ,  0 k  k
min

subject to yi(xt

i    +   0)     1, i = 1, . . . , n,

(12.4)

12.2 the support vector classi   er

419

where we have dropped the norm constraint on   . note that m = 1/k  k.
expression (12.4) is the usual way of writing the support vector criterion
for separated data. this is a id76 problem (quadratic cri-
terion, linear inequality constraints), and the solution is characterized in
section 4.5.2.

suppose now that the classes overlap in feature space. one way to deal
with the overlap is to still maximize m , but allow for some points to be on
the wrong side of the margin. de   ne the slack variables    = (  1,   2, . . . ,   n ).
there are two natural ways to modify the constraint in (12.3):

yi(xt

i    +   0)     m       i,

or

yi(xt

i    +   0)     m (1       i),

(12.5)

(12.6)

   i,   i     0, pn

i=1   i     constant. the two choices lead to di   erent solutions.
the    rst choice seems more natural, since it measures overlap in actual
distance from the margin; the second choice measures the overlap in relative
distance, which changes with the width of the margin m . however, the    rst
choice results in a nonid76 problem, while the second is
convex; thus (12.6) leads to the    standard    support vector classi   er, which
we use from here on.

here is the idea of the formulation. the value   i in the constraint yi(xt

i   +
  0)     m (1       i) is the proportional amount by which the prediction
f (xi) = xt
i    +  0 is on the wrong side of its margin. hence by bounding the

fall on the wrong side of their margin. misclassi   cations occur when   i > 1,

sum p   i, we bound the total proportional amount by which predictions
so bounding p   i at a value k say, bounds the total number of training

as in (4.48) in section 4.5.2, we can drop the norm constraint on   ,

misclassi   cations at k.

(12.7)

de   ne m = 1/k  k, and write (12.4) in the equivalent form
i    +   0)     1       i    i,

mink  k subject to ( yi(xt

  i     0, p   i     constant.

this is the usual way the support vector classi   er is de   ned for the non-
separable case. however we    nd confusing the presence of the    xed scale
   1    in the constraint yi(xt
i    +   0)     1      i, and prefer to start with (12.6).
the right panel of figure 12.1 illustrates this overlapping case.
by the nature of the criterion (12.7), we see that points well inside their
class boundary do not play a big role in shaping the boundary. this seems
like an attractive property, and one that di   erentiates it from linear dis-
criminant analysis (section 4.3). in lda, the decision boundary is deter-
mined by the covariance of the class distributions and the positions of the
class centroids. we will see in section 12.3.3 that id28 is more
similar to the support vector classi   er in this regard.

420

12. flexible discriminants

12.2.1 computing the support vector classi   er

the problem (12.7) is quadratic with linear inequality constraints, hence it
is a id76 problem. we describe a quadratic programming
solution using lagrange multipliers. computationally it is convenient to
re-express (12.7) in the equivalent form

min
  ,  0

1
2k  k2 + c

nxi=1

  i

(12.8)

subject to   i     0, yi(xt

i    +   0)     1       i    i,

where the    cost    parameter c replaces the constant in (12.7); the separable
case corresponds to c =    .

the lagrange (primal) function is

lp =

1
2k  k2 + c

nxi=1

  i    

nxi=1

  i[yi(xt

i    +   0)     (1       i)]    

  i  i, (12.9)

nxi=1

which we minimize w.r.t   ,   0 and   i. setting the respective derivatives to
zero, we get

   =

0 =

  iyixi,

  iyi,

nxi=1
nxi=1

  i = c       i,    i,

(12.10)

(12.11)

(12.12)

as well as the positivity constraints   i,   i,   i     0    i. by substituting
(12.10)   (12.12) into (12.9), we obtain the lagrangian (wolfe) dual objec-
tive function

ld =

nxi=1

  i    

1
2

nxi=1

nxi   =1

  i  i    yiyi   xt

i xi    ,

(12.13)

which gives a lower bound on the objective function (12.8) for any feasible
i=1   iyi = 0. in
addition to (12.10)   (12.12), the karush   kuhn   tucker conditions include
the constraints

point. we maximize ld subject to 0       i     c and pn

  i[yi(xt

i    +   0)     (1       i)] = 0,
  i  i = 0,
i    +   0)     (1       i)     0,

yi(xt

(12.14)
(12.15)

(12.16)

for i = 1, . . . , n . together these equations (12.10)   (12.16) uniquely char-
acterize the solution to the primal and dual problem.

12.2 the support vector classi   er

421

from (12.10) we see that the solution for    has the form

     =

nxi=1

    iyixi,

(12.17)

with nonzero coe   cients     i only for those observations i for which the
constraints in (12.16) are exactly met (due to (12.14)). these observations
are called the support vectors, since      is represented in terms of them
alone. among these support points, some will lie on the edge of the margin
(     i = 0), and hence from (12.15) and (12.12) will be characterized by
0 <     i < c; the remainder (     i > 0) have     i = c. from (12.14) we can
see that any of these margin points (0 <     i,     i = 0) can be used to solve
for   0, and we typically use an average of all the solutions for numerical
stability.

maximizing the dual (12.13) is a simpler convex quadratic programming
problem than the primal (12.9), and can be solved with standard techniques
(murray et al., 1981, for example).

given the solutions     0 and     , the decision function can be written as

  g(x) = sign[   f (x)]

= sign[xt      +     0].

(12.18)

the tuning parameter of this procedure is the cost parameter c.

12.2.2 mixture example (continued)

figure 12.2 shows the support vector boundary for the mixture example
of figure 2.5 on page 21, with two overlapping classes, for two di   erent
values of the cost parameter c. the classi   ers are rather similar in their
performance. points on the wrong side of the boundary are support vectors.
in addition, points on the correct side of the boundary but close to it (in
the margin), are also support vectors. the margin is larger for c = 0.01
than it is for c = 10, 000. hence larger values of c focus attention more
on (correctly classi   ed) points near the decision boundary, while smaller
values involve data further away. either way, misclassi   ed points are given
weight, no matter how far away. in this example the procedure is not very
sensitive to choices of c, because of the rigidity of a linear boundary.

the optimal value for c can be estimated by cross-validation, as dis-
cussed in chapter 7. interestingly, the leave-one-out cross-validation error
can be bounded above by the proportion of support points in the data. the
reason is that leaving out an observation that is not a support vector will
not change the solution. hence these observations, being classi   ed correctly
by the original boundary, will be classi   ed correctly in the cross-validation
process. however this bound tends to be too high, and not generally useful
for choosing c (62% and 85%, respectively, in our examples).

422

12. flexible discriminants

o

o

o

o

o

o

o

o

o
o

o
   
o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . ..
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
oo
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.. . . .
o
o
   
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.270
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.288
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o
o
o
o
o
o
o

o
o
o
o

o
o

o
o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

c = 10000

o

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
   
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . .. ..
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.. .. . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.26
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.30
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.21

o
o
o
o
o
o
o
o

o
o
o
o

o
o

o
o
o

o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

c = 0.01

figure 12.2. the linear support vector boundary for the mixture data exam-
ple with two overlapping classes, for two di   erent values of c. the broken lines
indicate the margins, where f (x) =   1. the support points (  i > 0) are all the
points on the wrong side of their margin. the black solid dots are those support
points falling exactly on the margin (  i = 0,   i > 0). in the upper panel 62% of
the observations are support points, while in the lower panel 85% are. the broken
purple curve in the background is the bayes decision boundary.

12.3 support vector machines and kernels

423

12.3 support vector machines and kernels

the support vector classi   er described so far    nds linear boundaries in the
input feature space. as with other linear methods, we can make the pro-
cedure more    exible by enlarging the feature space using basis expansions
such as polynomials or splines (chapter 5). generally linear boundaries
in the enlarged space achieve better training-class separation, and trans-
late to nonlinear boundaries in the original space. once the basis functions
hm(x), m = 1, . . . , m are selected, the procedure is the same as before. we
   t the sv classi   er using input features h(xi) = (h1(xi), h2(xi), . . . , hm (xi)),
i = 1, . . . , n , and produce the (nonlinear) function   f (x) = h(x)t      +     0.
the classi   er is   g(x) = sign(   f (x)) as before.

the support vector machine classi   er is an extension of this idea, where
the dimension of the enlarged space is allowed to get very large, in   nite
in some cases. it might seem that the computations would become pro-
hibitive. it would also seem that with su   cient basis functions, the data
would be separable, and over   tting would occur. we    rst show how the
id166 technology deals with these issues. we then see that in fact the id166
classi   er is solving a function-   tting problem using a particular criterion
and form of id173, and is part of a much bigger class of problems
that includes the smoothing splines of chapter 5. the reader may wish
to consult section 5.8, which provides background material and overlaps
somewhat with the next two sections.

12.3.1 computing the id166 for classi   cation

we can represent the optimization problem (12.9) and its solution in a
special way that only involves the input features via inner products. we do
this directly for the transformed feature vectors h(xi). we then see that for
particular choices of h, these inner products can be computed very cheaply.

the lagrange dual function (12.13) has the form

ld =

nxi=1

  i    

1
2

nxi=1

nxi   =1

  i  i    yiyi   hh(xi), h(xi    )i.

(12.19)

from (12.10) we see that the solution function f (x) can be written

f (x) = h(x)t    +   0

=

nxi=1

  iyihh(x), h(xi)i +   0.

(12.20)

as before, given   i,   0 can be determined by solving yif (xi) = 1 in (12.20)
for any (or all) xi for which 0 <   i < c.

424

12. flexible discriminants

so both (12.19) and (12.20) involve h(x) only through inner products. in
fact, we need not specify the transformation h(x) at all, but require only
knowledge of the id81

k(x, x   ) = hh(x), h(x   )i

(12.21)

that computes inner products in the transformed space. k should be a
symmetric positive (semi-) de   nite function; see section 5.8.1.

three popular choices for k in the id166 literature are

dth-degree polynomial: k(x, x   ) = (1 + hx, x   i)d,

radial basis: k(x, x   ) = exp(     kx     x   k2),
neural network: k(x, x   ) = tanh(  1hx, x   i +   2).

(12.22)

consider for example a feature space with two inputs x1 and x2, and a
polynomial kernel of degree 2. then

k(x, x    ) = (1 + hx, x    i)2

= (1 + x1x    
= 1 + 2x1x    

1 + x2x    
2)2
1 + 2x2x    
2 + (x1x    

1)2 + (x2x    

1x2x    
2.
(12.23)
then m = 6, and if we choose h1(x) = 1, h2(x) =    2x1, h3(x) =
2 , and h6(x) =    2x1x2, then k(x, x    ) =

   2x2, h4(x) = x 2
hh(x), h(x    )i. from (12.20) we see that the solution can be written

1 , h5(x) = x 2

2)2 + 2x1x    

  f (x) =

nxi=1

    iyik(x, xi) +     0.

(12.24)

the role of the parameter c is clearer in an enlarged feature space,
since perfect separation is often achievable there. a large value of c will
discourage any positive   i, and lead to an over   t wiggly boundary in the
original feature space; a small value of c will encourage a small value of
k  k, which in turn causes f (x) and hence the boundary to be smoother.
figure 12.3 show two nonlinear support vector machines applied to the
mixture example of chapter 2. the id173 parameter was chosen
in both cases to achieve good test error. the radial basis kernel produces
a boundary quite similar to the bayes optimal boundary for this example;
compare figure 2.5.

in the early literature on support vectors, there were claims that the
kernel property of the support vector machine is unique to it and allows
one to    nesse the curse of dimensionality. neither of these claims is true,
and we go into both of these issues in the next three subsections.

12.3 support vector machines and kernels

425

id166 - degree-4 polynomial in feature space

   
o

o

o

o

o

o

o

   
o

o
o

o
o

   
o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . .
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
o
o
   
   
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
   
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . .
   
o
o
o
. . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
   
o
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
oo
o
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
   
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
oo
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
   
       
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.180
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.245
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o
o
o

o
o

o
o
o

o
o

o
o

   
o
o

o
o

o

o

o

o

o

id166 - radial kernel in feature space

   
o

o

o

o

o

   
o

   
o

o
o

   
o
o
   
o

o
o
   
o

o
o
o
o

. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
   
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
   
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
   
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
   
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
   
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
   
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
   
o o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
   
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
   
o
oo
o
   
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
oo
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . .
o
o
o
o
o
o
. . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
o
o
o
   
oo
. . . . . . . . . . . .
. . . . . . . . .
.
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . .
.
   
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
.
o
o
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
.
. . . . . . . . .
. . . . . . . . . . . . .
o
. . . . . . . . . . . .
. .
. . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
   
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. .
. . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. .
. . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . .
. . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . .
. . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
   
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . .
. . . .
. . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . .
o
. .
. . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
   
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
training error: 0.160
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
test error:       0.218
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o

o
   
o

o
o
o

   
o
o

   
o

   
o

   
o

   
o

o

o

o
o
o
   
o
   
o

figure 12.3. two nonlinear id166s for the mixture data. the upper plot uses
a 4th degree polynomial kernel, the lower a radial basis kernel (with    = 1). in
each case c was tuned to approximately achieve the best test error performance,
and c = 1 worked well in both cases. the radial basis kernel performs the best
(close to bayes optimal), as might be expected given the data arise from mixtures
of gaussians. the broken purple curve in the background is the bayes decision
boundary.

hinge loss
binomial deviance
squared error
class huber

426

12. flexible discriminants

s
s
o
l

0

.

3

5

.

2

0

.

2

5

.

1

0

.

1

5

.

0

0

.

0

   3

   2

   1

1

2

3

0

yf

figure 12.4. the support vector id168 (hinge loss), compared to the
negative log-likelihood loss (binomial deviance) for id28, squared-er-
ror loss, and a    huberized    version of the squared hinge loss. all are shown as a
function of yf rather than f , because of the symmetry between the y = +1 and
y =    1 case. the deviance and huber have the same asymptotes as the id166
loss, but are rounded in the interior. all are scaled to have the limiting left-tail
slope of    1.

12.3.2 the id166 as a penalization method

with f (x) = h(x)t    +   0, consider the optimization problem

min
  0,   

nxi=1

[1     yif (xi)]+ +

  
2k  k2

(12.25)

where the subscript    +    indicates positive part. this has the form loss +
penalty, which is a familiar paradigm in function estimation. it is easy to
show (exercise 12.1) that the solution to (12.25), with    = 1/c, is the
same as that for (12.8).

examination of the    hinge    id168 l(y, f ) = [1    yf ]+ shows that
it is reasonable for two-class classi   cation, when compared to other more
traditional id168s. figure 12.4 compares it to the log-likelihood loss
for id28, as well as squared-error loss and a variant thereof.
the (negative) log-likelihood or binomial deviance has similar tails as the
id166 loss, giving zero penalty to points well inside their margin, and a

12.3 support vector machines and kernels

427

table 12.1. the population minimizers for the di   erent id168s in fig-
ure 12.4. id28 uses the binomial log-likelihood or deviance. linear
discriminant analysis (exercise 4.2) uses squared-error loss. the id166 hinge loss
estimates the mode of the posterior class probabilities, whereas the others estimate
a linear transformation of these probabilities.

id168

l[y, f (x)]

minimizing function

binomial
deviance

id166 hinge
loss

squared
error

   huberised   
square
hinge loss

log[1 + e   yf (x)]

f (x) = log

pr(y = +1|x)
pr(y = -1|x)

[1     yf (x)]+

f (x) = sign[pr(y = +1|x)     1
2 ]

[y     f (x)]2 = [1     yf (x)]2

f (x) = 2pr(y = +1|x)     1

   4yf (x),
[1     yf (x)]2

+ otherwise

yf (x) < -1

f (x) = 2pr(y = +1|x)     1

linear penalty to points on the wrong side and far away. squared-error, on
the other hand gives a quadratic penalty, and points well inside their own
margin have a strong in   uence on the model as well. the squared hinge
loss l(y, f ) = [1     yf ]2
+ is like the quadratic, except it is zero for points
inside their margin. it still rises quadratically in the left tail, and will be
less robust than hinge or deviance to misclassi   ed observations. recently
rosset and zhu (2007) proposed a    huberized    version of the squared hinge
loss, which converts smoothly to a linear loss at yf =    1.
we can characterize these id168s in terms of what they are es-
timating at the population level. we consider minimizing el(y, f (x)).
table 12.1 summarizes the results. whereas the hinge loss estimates the
classi   er g(x) itself, all the others estimate a transformation of the class
posterior probabilities. the    huberized    square hinge loss shares attractive
properties of id28 (smooth id168, estimates probabili-
ties), as well as the id166 hinge loss (support points).

formulation (12.25) casts the id166 as a regularized function estimation
problem, where the coe   cients of the linear expansion f (x) =   0 + h(x)t   
are shrunk toward zero (excluding the constant). if h(x) represents a hierar-
chical basis having some ordered structure (such as ordered in roughness),

428

12. flexible discriminants

then the uniform shrinkage makes more sense if the rougher elements hj in
the vector h have smaller norm.

all the loss-functions in table 12.1 except squared-error are so called
   margin maximizing loss-functions    (rosset et al., 2004b). this means that
if the data are separable, then the limit of        in (12.25) as        0 de   nes
the optimal separating hyperplane1.

12.3.3 function estimation and reproducing kernels

here we describe id166s in terms of function estimation in reproducing
kernel hilbert spaces, where the kernel property abounds. this material is
discussed in some detail in section 5.8. this provides another view of the
support vector classi   er, and helps to clarify how it works.

suppose the basis h arises from the (possibly    nite) eigen-expansion of

a positive de   nite kernel k,

k(x, x   ) =

  m(x)  m(x   )  m

(12.26)

   xm=1
  m  m(xi))#+

and hm(x) =      m  m(x). then with   m =      m  m, we can write (12.25)
as

min
  0,   

nxi=1"1     yi(  0 +

   xm=1

+

  
2

   xm=1

  2
m
  m

.

(12.27)

now (12.27) is identical in form to (5.49) on page 169 in section 5.8, and
the theory of reproducing kernel hilbert spaces described there guarantees
a    nite-dimensional solution of the form

f (x) =   0 +

nxi=1

  ik(x, xi).

(12.28)

in particular we see there an equivalent version of the optimization crite-
rion (12.19) [equation (5.67) in section 5.8.2; see also wahba et al. (2000)],

min
  0,  

nxi=1

(1     yif (xi))+ +

  
2

  t k  ,

(12.29)

where k is the n    n matrix of kernel evaluations for all pairs of training
features (exercise 12.2).
these models are quite general, and include, for example, the entire fam-
ily of smoothing splines, additive and interaction spline models discussed

1for id28 with separable data,        diverges, but       /k       k converges to

the optimal separating direction.

12.3 support vector machines and kernels

429

in chapters 5 and 9, and in more detail in wahba (1990) and hastie and
tibshirani (1990). they can be expressed more generally as

min
f    h

nxi=1

[1     yif (xi)]+ +   j(f ),

(12.30)

where h is the structured space of functions, and j(f ) an appropriate reg-
ularizer on that space. for example, suppose h is the space of additive
j(xj)}2dxj. then the
solution to (12.30) is an additive cubic spline, and has a kernel representa-
j). each of the kj is the kernel

j=1 fj(xj), and j(f ) =pjr {f       

functions f (x) =pp
tion (12.28) with k(x, x   ) =pp

appropriate for the univariate smoothing spline in xj (wahba, 1990).

conversely this discussion also shows that, for example, any of the kernels
described in (12.22) above can be used with any convex id168, and
will also lead to a    nite-dimensional representation of the form (12.28).
figure 12.5 uses the same id81s as in figure 12.3, except using
the binomial log-likelihood as a id1682. the    tted function is hence
an estimate of the log-odds,

j=1 kj(xj, x   

  f (x) = log

  pr(y = +1|x)
  pr(y =    1|x)
nxi=1

    ik(x, xi),

=     0 +

or conversely we get an estimate of the class probabilities

  pr(y = +1|x) =

1
1 + e        0   pn
i=1     ik(x,xi)

.

(12.31)

(12.32)

the    tted models are quite similar in shape and performance. examples
and more details are given in section 5.8.

it does happen that for id166s, a sizable fraction of the n values of   i
can be zero (the nonsupport points). in the two examples in figure 12.3,
these fractions are 42% and 45%, respectively. this is a consequence of the
piecewise linear nature of the    rst part of the criterion (12.25). the lower
the class overlap (on the training data), the greater this fraction will be.
reducing    will generally reduce the overlap (allowing a more    exible f ).
a small number of support points means that   f (x) can be evaluated more
quickly, which is important at lookup time. of course, reducing the overlap
too much can lead to poor generalization.

2ji zhu assisted in the preparation of these examples.

430

12. flexible discriminants

lr - degree-4 polynomial in feature space

. . .

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
oo
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
oo
o
o
. . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . .
o
o
o
o
oo
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . .
. . .
o
o
. . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.190
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.263
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o
o
o
o
o
o
o

o
o
o
o

o
o

o
o
o

o
o

o
o

o
o

o

o

o

o

o

o

o

o

lr - radial kernel in feature space

o

o

o

o

o

o

o

o

o

. . .
. . . . .

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
.
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
oo
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
oo
o
o
.
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
o
.
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
o
. . .
.
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
o
o
. . . . . .
.
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. .
. . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . .
o
o
o
oo
. . . . . . . . . .
. . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . .
. .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . .
o
o
. . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . .
. . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . .
o
o
o
o
. . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
o
o
. . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . .
. . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
. . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
o
o
o
. . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . .
o
o
o
. . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . .
o
. . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . .
. . .
o
o
o
. . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . .
. . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . .
. . .
o
o
o
o
. . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. .
. . . . . . . . . . . . . .
o
. . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . .
.
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
. . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.150
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.221
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

o
o
o
o

o
o

o
o
o

o
o

o
o

o
o

o

o

o

o

o

o

o

figure 12.5. the id28 versions of the id166 models in fig-
ure 12.3, using the identical kernels and hence penalties, but the log-likelihood
loss instead of the id166 id168. the two broken contours correspond to
posterior probabilities of 0.75 and 0.25 for the +1 class (or vice versa). the bro-
ken purple curve in the background is the bayes decision boundary.

12.3 support vector machines and kernels

431

table 12.2. skin of the orange: shown are mean (standard error of the mean)
of the test error over 50 simulations. bruto    ts an additive spline model adap-
tively, while mars    ts a low-order interaction model adaptively.

method
sv classi   er
id166/poly 2
id166/poly 5
id166/poly 10

1
2
3
4
5 bruto
6 mars
bayes

test error (se)

no noise features

six noise features

0.450 (0.003)
0.078 (0.003)
0.180 (0.004)
0.230 (0.003)
0.084 (0.003)
0.156 (0.004)

0.029

0.472 (0.003)
0.152 (0.004)
0.370 (0.004)
0.434 (0.002)
0.090 (0.003)
0.173 (0.005)

0.029

12.3.4 id166s and the curse of dimensionality

in this section, we address the question of whether id166s have some edge
on the curse of dimensionality. notice that in expression (12.23) we are not
allowed a fully general inner product in the space of powers and products.
for example, all terms of the form 2xjx    
j are given equal weight, and the
kernel cannot adapt itself to concentrate on subspaces. if the number of
features p were large, but the class separation occurred only in the linear
subspace spanned by say x1 and x2, this kernel would not easily    nd the
structure and would su   er from having many dimensions to search over.
one would have to build knowledge about the subspace into the kernel;
that is, tell it to ignore all but the    rst two inputs. if such knowledge were
available a priori, much of statistical learning would be made much easier.
a major goal of adaptive methods is to discover such structure.

standard normal independent features, but conditioned on 9    p x 2

we support these statements with an illustrative example. we generated
100 observations in each of two classes. the    rst class has four standard
normal independent features x1, x2, x3, x4. the second class also has four
j     16.
this is a relatively easy problem. as a second harder problem, we aug-
mented the features with an additional six standard gaussian noise fea-
tures. hence the second class almost completely surrounds the    rst, like the
skin surrounding the orange, in a four-dimensional subspace. the bayes er-
ror rate for this problem is 0.029 (irrespective of dimension). we generated
1000 test observations to compare di   erent procedures. the average test
errors over 50 simulations, with and without noise features, are shown in
table 12.2.

line 1 uses the support vector classi   er in the original feature space.
lines 2   4 refer to the support vector machine with a 2-, 5- and 10-dimension-
al polynomial kernel. for all support vector procedures, we chose the cost
parameter c to minimize the test error, to be as fair as possible to the

432

12. flexible discriminants

test error curves     id166 with radial kernel

   = 5

   = 1

   = 0.5

   = 0.1

r
o
r
r

e

 
t
s
e
t

5
3

.

0

0
3
0

.

5
2
0

.

0
2
0

.

1e   01

1e+01

1e+03

1e   01

1e+01

1e+03

1e   01

1e+01

1e+03

1e   01

1e+01

1e+03

c

figure 12.6. test-error curves as a function of the cost parameter c for
the radial-kernel id166 classi   er on the mixture data. at the top of each plot is
the scale parameter    for the radial kernel: k  (x, y) = exp (     ||x     y||2). the
optimal value for c depends quite strongly on the scale of the kernel. the bayes
error rate is indicated by the broken horizontal lines.

method. line 5    ts an additive spline model to the (   1, +1) response by
least squares, using the bruto algorithm for additive models, described
in hastie and tibshirani (1990). line 6 uses mars (multivariate adaptive
regression splines) allowing interaction of all orders, as described in chap-
ter 9; as such it is comparable with the id166/poly 10. both bruto and
mars have the ability to ignore redundant variables. test error was not
used to choose the smoothing parameters in either of lines 5 or 6.

in the original feature space, a hyperplane cannot separate the classes,
and the support vector classi   er (line 1) does poorly. the polynomial sup-
port vector machine makes a substantial improvement in test error rate,
but is adversely a   ected by the six noise features. it is also very sensitive to
the choice of kernel: the second degree polynomial kernel (line 2) does best,
since the true decision boundary is a second-degree polynomial. however,
higher-degree polynomial kernels (lines 3 and 4) do much worse. bruto
performs well, since the boundary is additive. bruto and mars adapt
well: their performance does not deteriorate much in the presence of noise.

12.3.5 a path algorithm for the id166 classi   er

the id173 parameter for the id166 classi   er is the cost parameter
c, or its inverse    in (12.25). common usage is to set c high, leading often
to somewhat over   t classi   ers.

figure 12.6 shows the test error on the mixture data as a function of
c, using di   erent radial-kernel parameters   . when    = 5 (narrow peaked
kernels), the heaviest id173 (small c) is called for. with    = 1

12.3 support vector machines and kernels

433

9

11

8

7

f (x) = +1

f (x) = 0

5

10

3

12

6

f (x) =    1

2

1

5

.

1

0

.

1

5

.

0

0

.

0

5

.

0
   

.

0
1
   

1/||  ||

  

0
1

8

6

4

2

0

   0.5

0.0

0.5

1.0

1.5

4

2.0

9
4

8
1

11

6

2
7

10
3

12
5

0.0

0.2

0.4

0.6

0.8

1.0

  i(  )

figure 12.7. a simple example illustrates the id166 path algorithm. (left
panel:) this plot illustrates the state of the model at    = 1/2. the        + 1    points
are orange, the       1    blue. the width of the soft margin is 2/||  || = 2    0.587.
two blue points {3, 5} are misclassi   ed, while the two orange points {10, 12} are
correctly classi   ed, but on the wrong side of their margin f (x) = +1; each of
these has yif (xi) < 1. the three square shaped points {2, 6, 7} are exactly on
their margins. (right panel:) this plot shows the piecewise linear pro   les   i(  ).
the horizontal broken line at    = 1/2 indicates the state of the   i for the model
in the left plot.

(the value used in figure 12.3), an intermediate value of c is required.
clearly in situations such as these, we need to determine a good choice
for c, perhaps by cross-validation. here we describe a path algorithm (in
the spirit of section 3.8) for e   ciently    tting the entire sequence of id166
models obtained by varying c.

it is convenient to use the loss+penalty formulation (12.25), along with

figure 12.4. this leads to a solution for    at a given value of   :

     =

1
  

nxi=1

  iyixi.

(12.33)

the   i are again lagrange multipliers, but in this case they all lie in [0, 1].
figure 12.7 illustrates the setup. it can be shown that the kkt optimal-
ity conditions imply that the labeled points (xi, yi) fall into three distinct
groups:

434

12. flexible discriminants

    observations correctly classi   ed and outside their margins. they have
yif (xi) > 1, and lagrange multipliers   i = 0. examples are the
orange points 8, 9 and 11, and the blue points 1 and 4.

    observations sitting on their margins with yif (xi) = 1, with lagrange
multipliers   i     [0, 1]. examples are the orange 7 and the blue 2 and
6.

    observations inside their margins have yif (xi) < 1, with   i = 1.

examples are the blue 3 and 5, and the orange 10 and 12.

the idea for the path algorithm is as follows. initially    is large, the
margin 1/||    || is wide, and all points are inside their margin and have
  i = 1. as    decreases, 1/||    || decreases, and the margin gets narrower.
some points will move from inside their margins to outside their margins,
and their   i will change from 1 to 0. by continuity of the   i(  ), these points
will linger on the margin during this transition. from (12.33) we see that
the points with   i = 1 make    xed contributions to   (  ), and those with
  i = 0 make no contribution. so all that changes as    decreases are the
  i     [0, 1] of those (small number) of points on the margin. since all these
points have yif (xi) = 1, this results in a small set of linear equations that
prescribe how   i(  ) and hence      changes during these transitions. this
results in piecewise linear paths for each of the   i(  ). the breaks occur
when points cross the margin. figure 12.7 (right panel) shows the   i(  )
pro   les for the small example in the left panel.

although we have described this for linear id166s, exactly the same idea

works for nonlinear models, in which (12.33) is replaced by

f  (x) =

1
  

nxi=1

  iyik(x, xi).

(12.34)

details can be found in hastie et al. (2004). an r package id166path is
available on cran for    tting these models.

12.3.6 support vector machines for regression

in this section we show how id166s can be adapted for regression with a
quantitative response, in ways that inherit some of the properties of the
id166 classi   er. we    rst discuss the id75 model

f (x) = xt    +   0,

(12.35)

and then handle nonlinear generalizations. to estimate   , we consider min-
imization of

h(  ,   0) =

nxi=1

v (yi     f (xi)) +

  
2k  k2,

(12.36)

)
r
(
  
v

4

3

2

1

0

1
-

12.3 support vector machines and kernels

435

)
r
(
h
v

2
1

0
1

8

6

4

2

0

     
-2

-4

  

0
r

2

4

   c

-2

-4

c

2

4

0
r

figure 12.8. the left panel shows the   -insensitive error function used by the
support vector regression machine. the right panel shows the error function used
in huber   s robust regression (blue curve). beyond |c|, the function changes from
quadratic to linear.

where

v  (r) =(0

|r|       ,

if |r| <   ,
otherwise.

(12.37)

this is an      -insensitive    error measure, ignoring errors of size less than
   (left panel of figure 12.8). there is a rough analogy with the support
vector classi   cation setup, where points on the correct side of the deci-
sion boundary and far away from it, are ignored in the optimization. in
regression, these    low error    points are the ones with small residuals.

it is interesting to contrast this with error measures used in robust re-
gression in statistics. the most popular, due to huber (1964), has the form

vh (r) =(r2/2

c|r|     c2/2,

if |r|     c,
|r| > c,

(12.38)

shown in the right panel of figure 12.8. this function reduces from quadratic
to linear the contributions of observations with absolute residual greater
than a prechosen constant c. this makes the    tting less sensitive to out-
liers. the support vector error measure (12.37) also has linear tails (beyond
  ), but in addition it    attens the contributions of those cases with small
residuals.

if     ,     0 are the minimizers of h, the solution function can be shown to

have the form

     =

  f (x) =

nxi=1
nxi=1

(       

i         i)xi,

(       

i         i)hx, xii +   0,

(12.39)

(12.40)

436

12. flexible discriminants

where     i,        

i are positive and solve the quadratic programming problem

min
  i,     
i

  

nxi=1

(     

i +   i)    

nxi=1

subject to the constraints

yi(     

i       i) +

1
2

nxi,i   =1

(     

i       i)(     

i          i    )hxi, xi   i

0       i,      
nxi=1

i     1/  ,
i       i) = 0,
  i     
i = 0.

(     

(12.41)

due to the nature of these constraints, typically only a subset of the solution
values (       
i         i) are nonzero, and the associated data values are called the
support vectors. as was the case in the classi   cation setting, the solution
depends on the input values only through the inner products hxi, xi   i. thus
we can generalize the methods to richer spaces by de   ning an appropriate
inner product, for example, one of those de   ned in (12.22).

note that there are parameters,    and   , associated with the criterion
(12.36). these seem to play di   erent roles.    is a parameter of the loss
function v  , just like c is for vh . note that both v   and vh depend on the
scale of y and hence r. if we scale our response (and hence use vh (r/  ) and
v  (r/  ) instead), then we might consider using preset values for c and    (the
value c = 1.345 achieves 95% e   ciency for the gaussian). the quantity   
is a more traditional id173 parameter, and can be estimated for
example by cross-validation.

12.3.7 regression and kernels

as discussed in section 12.3.3, this kernel property is not unique to sup-
port vector machines. suppose we consider approximation of the regression
function in terms of a set of basis functions {hm(x)}, m = 1, 2, . . . , m :

f (x) =

  mhm(x) +   0.

(12.42)

to estimate    and   0 we minimize

h(  ,   0) =

v (yi     f (xi)) +

(12.43)

  

2x   2

m

for some general error measure v (r). for any choice of v (r), the solution

mxm=1
nxi=1

  f (x) =p     mhm(x) +     0 has the form
nxi=1

  f (x) =

  aik(x, xi)

(12.44)

12.3 support vector machines and kernels

437

with k(x, y) = pm

m=1 hm(x)hm(y). notice that this has the same form
as both the radial basis function expansion and a id173 estimate,
discussed in chapters 5 and 6.

for concreteness, let   s work out the case v (r) = r2. let h be the n    m
basis matrix with imth element hm(xi), and suppose that m > n is large.
for simplicity we assume that   0 = 0, or that the constant is absorbed in
h; see exercise 12.3 for an alternative.

we estimate    by minimizing the penalized least squares criterion

h(  ) = (y     h  )t (y     h  ) +   k  k2.

the solution is

with      determined by

  y = h     

   ht (y     h     ) +         = 0.

(12.45)

(12.46)

(12.47)

from this it appears that we need to evaluate the m    m matrix of inner
products in the transformed space. however, we can premultiply by h to
give

h      = (hht +   i)   1hht y.

(12.48)

the n    n matrix hht consists of inner products between pairs of obser-
vations i, i   ; that is, the evaluation of an inner product kernel {hht}i,i    =
k(xi, xi    ). it is easy to show (12.44) directly in this case, that the predicted
values at an arbitrary x satisfy

  f (x) = h(x)t     

=

nxi=1

    ik(x, xi),

(12.49)

where      = (hht +   i)   1y. as in the support vector machine, we need not
specify or evaluate the large set of functions h1(x), h2(x), . . . , hm (x). only
the inner product kernel k(xi, xi    ) need be evaluated, at the n training
points for each i, i    and at points x for predictions there. careful choice
of hm (such as the eigenfunctions of particular, easy-to-evaluate kernels
k) means, for example, that hht can be computed at a cost of n 2/2
evaluations of k, rather than the direct cost n 2m .

note, however, that this property depends on the choice of squared norm
k  k2 in the penalty. it does not hold, for example, for the l1 norm |  |,
which may lead to a superior model.

438

12. flexible discriminants

12.3.8 discussion

the support vector machine can be extended to multiclass problems, es-
sentially by solving many two-class problems. a classi   er is built for each
pair of classes, and the    nal classi   er is the one that dominates the most
(kressel, 1999; friedman, 1996; hastie and tibshirani, 1998). alternatively,
one could use the multinomial id168 along with a suitable kernel,
as in section 12.3.3. id166s have applications in many other supervised
and unsupervised learning problems. at the time of this writing, empirical
evidence suggests that it performs well in many real learning problems.

finally, we mention the connection of the support vector machine and
structural risk minimization (7.9). suppose the training points (or their
basis expansion) are contained in a sphere of radius r, and let g(x) =
sign[f (x)] = sign[  t x +   0] as in (12.2). then one can show that the class
of functions {g(x),k  k     a} has vc-dimension h satisfying

(12.50)
if f (x) separates the training data, optimally for k  k     a, then with
id203 at least 1        over training sets (vapnik, 1996, page 139):

h     r2a2.

error test     4

h[log (2n/h) + 1]     log (  /4)

n

.

(12.51)

the support vector classi   er was one of the    rst practical learning pro-
cedures for which useful bounds on the vc dimension could be obtained,
and hence the srm program could be carried out. however in the deriva-
tion, balls are put around the data points   a process that depends on the
observed values of the features. hence in a strict sense, the vc complexity
of the class is not    xed a priori, before seeing the features.

the id173 parameter c controls an upper bound on the vc
dimension of the classi   er. following the srm paradigm, we could choose c
by minimizing the upper bound on the test error, given in (12.51). however,
it is not clear that this has any advantage over the use of cross-validation
for choice of c.

12.4 generalizing id156

in section 4.3 we discussed id156 (lda), a funda-
mental tool for classi   cation. for the remainder of this chapter we discuss
a class of techniques that produce better classi   ers than lda by directly
generalizing lda.

some of the virtues of lda are as follows:
    it is a simple prototype classi   er. a new observation is classi   ed to the
class with closest centroid. a slight twist is that distance is measured
in the mahalanobis metric, using a pooled covariance estimate.

12.4 generalizing id156

439

    lda is the estimated bayes classi   er if the observations are multi-
variate gaussian in each class, with a common covariance matrix.
since this assumption is unlikely to be true, this might not seem to
be much of a virtue.

    the decision boundaries created by lda are linear, leading to deci-

sion rules that are simple to describe and implement.

    lda provides natural low-dimensional views of the data. for exam-
ple, figure 12.12 is an informative two-dimensional view of data in
256 dimensions with ten classes.

    often lda produces the best classi   cation results, because of its
simplicity and low variance. lda was among the top three classi   ers
for 7 of the 22 datasets studied in the statlog project (michie et
al., 1994)3.

unfortunately the simplicity of lda causes it to fail in a number of situa-
tions as well:

    often linear decision boundaries do not adequately separate the classes.
when n is large, it is possible to estimate more complex decision
boundaries. quadratic discriminant analysis (qda) is often useful
here, and allows for quadratic decision boundaries. more generally
we would like to be able to model irregular decision boundaries.

    the aforementioned shortcoming of lda can often be paraphrased
by saying that a single prototype per class is insu   cient. lda uses
a single prototype (class centroid) plus a common covariance matrix
to describe the spread of the data in each class. in many situations,
several prototypes are more appropriate.

    at the other end of the spectrum, we may have way too many (corre-
lated) predictors, for example, in the case of digitized analogue signals
and images. in this case lda uses too many parameters, which are
estimated with high variance, and its performance su   ers. in cases
such as this we need to restrict or regularize lda even further.

in the remainder of this chapter we describe a class of techniques that
attend to all these issues by generalizing the lda model. this is achieved
largely by three di   erent ideas.

the    rst idea is to recast the lda problem as a id75 problem.
many techniques exist for generalizing id75 to more    exible,
nonparametric forms of regression. this in turn leads to more    exible forms
of discriminant analysis, which we call fda. in most cases of interest, the

3this study predated the emergence of id166s.

440

12. flexible discriminants

regression procedures can be seen to identify an enlarged set of predictors
via basis expansions. fda amounts to lda in this enlarged space, the
same paradigm used in id166s.

in the case of too many predictors, such as the pixels of a digitized image,
we do not want to expand the set: it is already too large. the second idea is
to    t an lda model, but penalize its coe   cients to be smooth or otherwise
coherent in the spatial domain, that is, as an image. we call this procedure
penalized discriminant analysis or pda. with fda itself, the expanded
basis set is often so large that id173 is also required (again as in
id166s). both of these can be achieved via a suitably regularized regression
in the context of the fda model.

the third idea is to model each class by a mixture of two or more gaus-
sians with di   erent centroids, but with every component gaussian, both
within and between classes, sharing the same covariance matrix. this allows
for more complex decision boundaries, and allows for subspace reduction
as in lda. we call this extension mixture discriminant analysis or mda.
all three of these generalizations use a common framework by exploiting

their connection with lda.

12.5 flexible discriminant analysis

in this section we describe a method for performing lda using linear re-
gression on derived responses. this in turn leads to nonparametric and    ex-
ible alternatives to lda. as in chapter 4, we assume we have observations
with a quantitative response g falling into one of k classes g = {1, . . . , k},
each having measured features x. suppose    : g 7    ir1 is a function that
assigns scores to the classes, such that the transformed class labels are op-
timally predicted by id75 on x: if our training sample has the
form (gi, xi), i = 1, 2, . . . , n , then we solve

min
  ,  

nxi=1(cid:0)  (gi)     xt
i   (cid:1)2

,

(12.52)

with restrictions on    to avoid a trivial solution (mean zero and unit vari-
ance over the training data). this produces a one-dimensional separation
between the classes.

more generally, we can    nd up to l     k     1 sets of independent scorings
for the class labels,   1,   2, . . . ,   l, and l corresponding linear maps      (x) =
x t      ,     = 1, . . . , l, chosen to be optimal for multiple regression in irp. the
scores      (g) and the maps       are chosen to minimize the average squared
residual,

asr =

1
n

lx   =1" nxi=1(cid:0)     (gi)     xt

i      (cid:1)2# .

(12.53)

12.5 flexible discriminant analysis

441

the set of scores are assumed to be mutually orthogonal and normalized
with respect to an appropriate inner product to prevent trivial zero
solutions.

why are we going down this road? it can be shown that the sequence
of discriminant (canonical) vectors       derived in section 4.3.3 are identical
to the sequence       up to a constant (mardia et al., 1979; hastie et al.,
1995). moreover, the mahalanobis distance of a test point x to the kth
class centroid     k is given by

  j (x,     k) =

k   1x   =1

w   (       (x)         k

    )2 + d(x),

(12.54)

where     k
    is the mean of the        (xi) in the kth class, and d(x) does not
depend on k. here w    are coordinate weights that are de   ned in terms of
the mean squared residual r2

    of the    th optimally scored    t

w    =

1

r2
    (1     r2
    )

.

(12.55)

in section 4.3.2 we saw that these canonical distances are all that is needed
for classi   cation in the gaussian setup, with equal covariances in each class.
to summarize:

lda can be performed by a sequence of id75s, fol-
lowed by classi   cation to the closest class centroid in the space
of    ts. the analogy applies both to the reduced rank version,
or the full rank case when l = k     1.

the real power of this result is in the generalizations that it invites. we
can replace the id75    ts      (x) = xt       by far more    exible,
nonparametric    ts, and by analogy achieve a more    exible classi   er than
lda. we have in mind generalized additive    ts, spline functions, mars
models and the like. in this more general form the regression problems are
de   ned via the criterion

asr({     ,      }l

   =1) =

1
n

lx   =1" nxi=1

(     (gi)          (xi))2 +   j(     )# ,

(12.56)

where j is a regularizer appropriate for some forms of nonparametric regres-
sion, such as smoothing splines, additive splines and lower-order anova
spline models. also included are the classes of functions and associated
penalties generated by kernels, as in section 12.3.3.

before we describe the computations involved in this generalization, let
us consider a very simple example. suppose we use degree-2 polynomial
regression for each      . the decision boundaries implied by the (12.54) will
be quadratic surfaces, since each of the    tted functions is quadratic, and as

442

12. flexible discriminants

2

0

2
-

o

o

o

o

o
o

o

o

o

o

o
o

o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o

o

o

o

o

o

o

o

o
o
o
o

o
o

o

o
o
o

o
o

o

o
o

o

o
o
o
o o
o
o
o
o
o
o
o

o
o

o
o
oo
o

o

o
o
o
o
o
o
o
o
o
o

o

o

o

o

o

o
o o

o

o

o

o

o

o

o

o

o

o

0

2

-2

figure 12.9. the data consist of 50 points generated from each of n (0, i) and
n (0, 9
4 i). the solid black ellipse is the decision boundary found by fda using
degree-two polynomial regression. the dashed purple circle is the bayes decision
boundary.

in lda their squares cancel out when comparing distances. we could have
achieved identical quadratic boundaries in a more conventional way, by
augmenting our original predictors with their squares and cross-products.
in the enlarged space one performs an lda, and the linear boundaries in
the enlarged space map down to quadratic boundaries in the original space.
a classic example is a pair of multivariate gaussians centered at the origin,
one having covariance matrix i, and the other ci for c > 1; figure 12.9
illustrates. the bayes decision boundary is the sphere kxk = pc log c
2(c   1) , which
is a linear boundary in the enlarged space.

many nonparametric regression procedures operate by generating a basis
expansion of derived variables, and then performing a id75 in
the enlarged space. the mars procedure (chapter 9) is exactly of this
form. smoothing splines and additive spline models generate an extremely
large basis set (n   p basis functions for additive splines), but then perform
a penalized regression    t in the enlarged space. id166s do as well; see also
the kernel-based regression example in section 12.3.7. fda in this case can
be shown to perform a penalized id156 in the enlarged
space. we elaborate in section 12.6. linear boundaries in the enlarged space
map down to nonlinear boundaries in the reduced space. this is exactly the
same paradigm that is used with support vector machines (section 12.3).
we illustrate fda on the id103 example used in chapter 4,
with k = 11 classes and p = 10 predictors. the classes correspond to

12.5 flexible discriminant analysis

443

id156

flexible discriminant analysis -- bruto

a

t

i

 

a
d
g
n
n
a
r
t
 
r
o

i

f
 

2

 

i

t

e
a
n
d
r
o
o
c

oooooo

oooooo
ooooo

oo
oooo

oo o
o

oo
ooo o

o
o
o o oo

o
o
ooooo
o
o o
oo
oo
oooo
oooooo
o o oo oo
o

o

oooooo
o

oooooo
o
oooooo
oooo
o
oo
oooo
o
o
o
o
ooo
o
o
oo
oooo
o
oooo
oo
ooooo
o
ooooo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o

oooo
o

oooo
o ooooo
oooooo
oooooo
oo
oooooo
o
ooooo
o
o
o
o
oo
o
ooo
ooo
o
o o
oo
o o
o o
oooo
o
o
o
o
o
o
oooo
ooo
o ooooo
oo
oooo
oooooo
o
o
oo
o
o
oo
oo o
o
o
oo
o
o
o
o o o
o oo
o
o
o
oo
o
oo
oo o
o
o
o
ooo oo
o
o
o
oo
o
oooooo
o
o
oo
o
o
o
o oo
o
o
oooo
o
oo
oo o
o
o
o
oo
ooo
oo
oooooo o
o
o
oo
oo
oo
oo
ooo
o
o
oooooo
o
o
o
o
o
o
oo o
o
ooo
ooo
oo
oooo
o
o
o
o
o
o
o
oo
o
o
ooo
o
o
o o o
o
o
o
ooo
o
o
o
o
o
o
oooooo
o
o
o o
oooo
o
ooooo
o
oo
o o
o
oo
o
ooo
o ooo
o
o
o oo
o
o
o
o
oooooo
oo o
o
o
o
o
o
ooo o

o oo

o

o

oooooo
oooooo
o

oo
o
o
o
o
o
o
oooo
oo

o

o

o

o

o

o
o

o

a

t

i

 

a
d
g
n
n
a
r
t
 
r
o

i

f
 

2

 

i

t

e
a
n
d
r
o
o
c

oo
oo
o o
o
o
oooo
o
o
o
o
o
o
oo
o
oo
ooooo
o
oooooo
ooo
o
o
oooo
o
o
o
o

ooo
o
oo
o o
oo
o
o
o
o
oo
o
oo
o
o
o
ooo
oo
o
oo
o
o
oo
o
o
o
oooo o
oo
o o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
ooooo
oo
o
ooo
o o
o
o
o
oooo o
ooo
o
o
o
o oo
o
o
o
o
ooo
oo
o
o
o
oooo
oo
o
o
o
o
oo
o
ooooo
oo ooo
o
o
o
oooo
oooo
o
o
o
o
ooooo
o
oo
o
oooo
oo
o
o
o
oo o
o
o
oooooo
o
ooo
o
o
o
o
o
o
oooooo
o
oo
o
o
oooooo oo
o
o
o
o
ooooo
o
o
oo
o
oooo
ooo
o
ooooo
o
o oo
o
oo
o
o
o o
o
o
ooo
oo
o
oooooo
o
oo
o
o
o
o
ooooo
ooo
oooo
o ooo
oo
o
oo
o
o
oooooo
ooo
ooo
o
o
o
ooo
oo
oo
o
o
o
o
o
o
o
o
ooooo
oo
oo
oo
o
oo
oo
o
o
oo
oo
o
o
o
oo
o
oooooo ooooo
o
o
ooooo
o ooo
o
o
o
o o
o
o
o
oooo
oooo o
o
oooooo
o
o
oo
o
oooo
o
oooooo
o
o

o
oooooo
oo
ooo

o

o

o

o

o

oooo

oo
ooo
o

o
oooo
o oo oo
o
o
oooo
o o oo
o
o
oooooo
o
o
o
ooooo o o
o
oo
o
o
o
oo
o

coordinate 1 for training data

coordinate 1 for training data

figure 12.10. the left plot shows the    rst two lda canonical variates for
the vowel training data. the right plot shows the corresponding projection when
fda/bruto is used to    t the model; plotted are the    tted regression functions
    1(xi) and     2(xi). notice the improved separation. the colors represent the eleven
di   erent vowel sounds.

11 vowel sounds, each contained in 11 di   erent words. here are the words,
preceded by the symbols that represent them:

vowel word vowel word vowel word vowel word
hoard
i:
e
who   d
a:

hod
i
hood
a
heard y

heed
head
hard

hid
had
hud

o
u
3:

c:
u:

each of eight speakers spoke each word six times in the training set, and
likewise seven speakers in the test set. the ten predictors are derived from
the digitized speech in a rather complicated way, but standard in the speech
recognition world. there are thus 528 training observations, and 462 test
observations. figure 12.10 shows two-dimensional projections produced by
lda and fda. the fda model used adaptive additive-spline regression
functions to model the      (x), and the points plotted in the right plot have
coordinates     1(xi) and     2(xi). the routine used in s-plus is called bruto,
hence the heading on the plot and in table 12.3. we see that    exible model-
ing has helped to separate the classes in this case. table 12.3 shows training
and test error rates for a number of classi   cation techniques. fda/mars
refers to friedman   s multivariate adaptive regression splines; degree = 2
means pairwise products are permitted. notice that for fda/mars, the
best classi   cation results are obtained in a reduced-rank subspace.

444

12. flexible discriminants

table 12.3. vowel recognition data performance results. the results for neural
networks are the best among a much larger set, taken from a neural network
archive. the notation fda/bruto refers to the regression method used with
fda.

technique

(1) lda

softmax

single-layer id88

(2) qda
(3) cart
(4) cart (linear combination splits)
(5)
(6) multi-layer id88 (88 hidden units)
(7) gaussian node network (528 hidden units)
(8) nearest neighbor
(9) fda/bruto

softmax

(10) fda/mars (degree = 1)

best reduced dimension (=2)
softmax

(11) fda/mars (degree = 2)

best reduced dimension (=6)
softmax

error rates

training test

0.32
0.48
0.01
0.05
0.05

0.06
0.11
0.09
0.18
0.14
0.02
0.13
0.10

0.56
0.67
0.53
0.56
0.54
0.67
0.49
0.45
0.44
0.44
0.50
0.45
0.42
0.48
0.42
0.39
0.50

12.5.1 computing the fda estimates

the computations for the fda coordinates can be simpli   ed in many im-
portant cases, in particular when the nonparametric regression procedure
can be represented as a linear operator. we will denote this operator by
s  ; that is,   y = s  y, where y is the vector of responses and   y the vector
of    ts. additive splines have this property, if the smoothing parameters are
   xed, as does mars once the basis functions are selected. the subscript   
denotes the entire set of smoothing parameters. in this case optimal scoring
is equivalent to a canonical correlation problem, and the solution can be
computed by a single eigen-decomposition. this is pursued in exercise 12.6,
and the resulting algorithm is presented here.

we create an n    k indicator response matrix y from the responses gi,
such that yik = 1 if gi = k, otherwise yik = 0. for a    ve-class problem y
might look like the following:

12.5 flexible discriminant analysis

445

g1 = 2
g2 = 1
g3 = 1
g4 = 5
g5 = 4
...
gn = 3

   

                              

c1 c2 c3 c4 c5
0
0
0
1
0
1
1
0
0
0

1
0
0
0
0

0
0
0
0
1

0

0

0

0

0
0
0
0
0
...
1

   

                              

here are the computational steps:

1. multivariate nonparametric regression. fit a multiresponse, adaptive
nonparametric regression of y on x, giving    tted values   y. let s  
be the linear operator that    ts the    nal chosen model, and      (x) be
the vector of    tted regression functions.

2. optimal scores. compute the eigen-decomposition of yt   y = yt s  y,
where the eigenvectors    are normalized:   t d     = i. here d   =
yt y/n is a diagonal matrix of the estimated class prior
probabilities.

3. update the model from step 1 using the optimal scores:   (x) =   t      (x).

the    rst of the k functions in   (x) is the constant function    a trivial
solution; the remaining k    1 functions are the discriminant functions. the
constant function, along with the id172, causes all the remaining
functions to be centered.

again s   can correspond to any regression method. when s   = hx , the
id75 projection operator, then fda is linear discriminant anal-
ysis. the software that we reference in the computational considerations
section on page 455 makes good use of this modularity; the fda function
has a method= argument that allows one to supply any regression function,
as long as it follows some natural conventions. the regression functions
we provide allow for polynomial regression, adaptive additive models and
mars. they all e   ciently handle multiple responses, so step (1) is a single
call to a regression routine. the eigen-decomposition in step (2) simulta-
neously computes all the optimal scoring functions.

in section 4.2 we discussed the pitfalls of using id75 on an
indicator response matrix as a method for classi   cation. in particular, se-
vere masking can occur with three or more classes. fda uses the    ts from
such a regression in step (1), but then transforms them further to produce
useful discriminant functions that are devoid of these pitfalls. exercise 12.9
takes another view of this phenomenon.

446

12. flexible discriminants

12.6 penalized discriminant analysis

although fda is motivated by generalizing optimal scoring, it can also be
viewed directly as a form of regularized discriminant analysis. suppose the
regression procedure used in fda amounts to a id75 onto a
basis expansion h(x), with a quadratic penalty on the coe   cients:

asr({     ,      }l

   =1) =

1
n

lx   =1" nxi=1

(     (gi)     ht (xi)     )2 +     t

            # . (12.57)

the choice of     depends on the problem. if      (x) = h(x)      is an expansion
on spline basis functions,     might constrain       to be smooth over irp. in
the case of additive splines, there are n spline basis functions for each
coordinate, resulting in a total of n p basis functions in h(x);     in this case
is n p    n p and block diagonal.
which we call penalized discriminant analysis, or pda:

the steps in fda can then be viewed as a generalized form of lda,

    enlarge the set of predictors x via a basis expansion h(x).
    use (penalized) lda in the enlarged space, where the penalized

mahalanobis distance is given by

d(x,   ) = (h(x)     h(  ))t (  w +      )   1(h(x)     h(  )),

(12.58)

where   w is the within-class covariance matrix of the derived vari-
ables h(xi).

    decompose the classi   cation subspace using a penalized metric:

max ut   betu subject to ut (  w +      )u = 1.

loosely speaking, the penalized mahalanobis distance tends to give less
weight to    rough    coordinates, and more weight to    smooth    ones; since
the penalty is not diagonal, the same applies to linear combinations that
are rough or smooth.

for some classes of problems, the    rst step, involving the basis expansion,
is not needed; we already have far too many (correlated) predictors. a
leading example is when the objects to be classi   ed are digitized analog
signals:

    the log-periodogram of a fragment of spoken speech, sampled at a set

of 256 frequencies; see figure 5.5 on page 149.

    the grayscale pixel values in a digitized image of a handwritten digit.

12.6 penalized discriminant analysis

447

lda: coefficient 1 pda: coefficient 1

lda: coefficient 2 pda: coefficient 2

lda: coefficient 3 pda: coefficient 3

lda: coefficient 4 pda: coefficient 4

lda: coefficient 5 pda: coefficient 5

lda: coefficient 6 pda: coefficient 6

lda: coefficient 7 pda: coefficient 7

lda: coefficient 8 pda: coefficient 8

lda: coefficient 9 pda: coefficient 9

figure 12.11. the images appear in pairs, and represent the nine discrim-
inant coe   cient functions for the digit recognition problem. the left member of
each pair is the lda coe   cient, while the right member is the pda coe   cient,
regularized to enforce spatial smoothness.

it is also intuitively clear in these cases why id173 is needed.
take the digitized image as an example. neighboring pixel values will tend
to be correlated, being often almost the same. this implies that the pair
of corresponding lda coe   cients for these pixels can be wildly di   erent
and opposite in sign, and thus cancel when applied to similar pixel values.
positively correlated predictors lead to noisy, negatively correlated coe   -
cient estimates, and this noise results in unwanted sampling variance. a
reasonable strategy is to regularize the coe   cients to be smooth over the
spatial domain, as with images. this is what pda does. the computations
proceed just as for fda, except that an appropriate penalized regression
method is used. here ht (x)      = x     , and     is chosen so that   t
            
penalizes roughness in       when viewed as an image. figure 1.2 on page 4
shows some examples of handwritten digits. figure 12.11 shows the dis-
criminant variates using lda and pda. those produced by lda appear
as salt-and-pepper images, while those produced by pda are smooth im-
ages. the    rst smooth image can be seen as the coe   cients of a linear
contrast functional for separating images with a dark central vertical strip
(ones, possibly sevens) from images that are hollow in the middle (zeros,
some fours). figure 12.12 supports this interpretation, and with more dif-
   culty allows an interpretation of the second coordinate. this and other

448

12. flexible discriminants

0

6

4

2

0

2
-

4
-

6
-

2
e

 

t

i

a
n
d
r
o
o
c

 
t

n
a
n
m

i

i
r
c
s
d

i

 
:

a
d
p

6

6

6

1

1

1

1

1

4
1

2

1

1
1
1

1
1
1

1
1
1
1
1
11
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1 1
11
11 11
1
1
1
11
1
1
11
1
1
1
1
1
1
1
1
1
11
1
1
11
11
11
1 1
1
1
1
1
1
1
1
1
1
1
111
1
1
1
1
1
1
1
1
1 1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
11
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
11
1
1
1
1
4

1
1

1
1
1
1
1

1

1

1

0

0
0

0

0
0
0

6

6

6

6

6

6

5

5

4

4

2

1

1

1

0

0

5

2

0
2

2
2

0
0

6
6

5
8

5
8
6

0
6

6
1
6

0
0

8
2
6
4

8
5
2
2

6
6
6
6
6
6

6
5
2
0

6
6
6 6
6
6
6
6
6
66
6
66
6
5
6
6
6
6
6
6
6
0
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
0
2
5
6
6
0
6
5
6
0
6
6
6
0
6
6
6
0
6
6
0
6
6
6
0
6
6 6
6
5
6
0
6
66
6
6
2
0
6
6
6
0
0
0
6
0
6
6
66
0
6
6
6
0
6
0
6
0
6
6
6
6
6
6
0
0
0
6
6
6
0
6
6
0
0
0
6
0
6
0
0
0
0
5
0
6
6
5
6
5
0
0
0
0
0
6
5
6
0
5
0
0
6
0
6
0
0
6
0
0
6
6
0
0
00
8
00
0
0
0
0
0
0
0
0
6
6
0
55
6
6
6
6
0
0
5
00
0
5
0
0
0
2
5
0
6
0
5
0
6
0
6
2
0
5
0
5
0
0
0
0
0
6
6
0
0
2
0
0
0
1
0
0
5
66
4
0
0
00
0 0
0
4
0
5
0
0
0
6
0
2
0 0
0
0
6
0
0
0
0
6
0
0
8
0
6
2
5
6
0
4
0
0
0
0
0
2
5
0
0
0
0
5
6
0
6
0
0
0
6
0
0
2
8
5
5
0
0
0
0
5
0
0
0
5
0
0
2
0
6
00
0
0
0
5
0
0
0
0
5
00 0
0
0
0
2
00
0
5
00
0
6
5
6
00
0
55
0
0
0
2
0
0
6
0
00 0
5
0
0
0
0
0
0
0
5
0
0
0
8
0
8
5
0
0
2
0
2
8
0
0
0
0
0
0
5
00
2
0
8
5
0
2
6
0
0
5
2
0
0
0
3
0
0
2
0
5
0
0
0
0
0 0
2
5
0
0
0
5
0
0
2
5
0
0
0
0
5
5
5
0
0
0
0
6
0
5
0
6
00
5
4
6
5
0
0
0
0
0
5
0
2
5
6
4
3
0
0
2
3
5
5
5
5
5
0
0
2
5
5
6
0
0
0 0
0
0
0
2
5
0
0
0
0
0
0
0
0
0 0
5
2
8
0
6
0
2
2
2
8
5
5
6
8
5
0
2
5
0
0
0
0
3
5
0
6
2
2
2
0
0
0
5
5
3
0
0
0
0
0
2
5
2
5
0
0
0
2
0
5
0
0
0
55
0
8
2
5
4
5
5
2
5
0
0
0
8
6
5
0
0
0
0
0
3
6
5
5
5
0
0
5
0
2
5
0
4
8
0
00
5
8
8
8
3
2
5
4
6
0
5
0
4
2
2
3
5
0
55
2
0
8
3
8
2
2
3
5
5
8
2
5
5
3
8
3
5
8
2
8
5
0
5
5
0 0
8
5
8
5
0
2
0
0
0
0
2
0
3
4
2
0
8
0
5
8
5
8
2
8
5
0
8
2
5
2
8
8
3
8
0
2
2
2
2
5
2
2
00
2
2
2
5
5
2
0
2
2
8
5
5
0
22
2
0
2
5
0
0
2
2 2
8
5
2
3
2
22
3
3
22
8 8
0
3
2
2
8
2
3
0
2
2
5
3
8
8
5
8
0
2
2
8
8
2
2
8
6
8
8
0
8
3
2
0
3
5 5
2
2
2
8
5
2
8
5
2 2
3
0
2
5
2
5
8
2
2
3
5
8
3
2
5
5
8
2
5
2
3
8
8
5
3
2
2
3
2
5
0
2
6
2
4
8
2
2 2
2
8
2
2
5
0
8
8
8
2
2
8
5
5
2
5
2
5
5
0
2
8
0
3
8
8
2
4
8
3
0
8
2
8
8
2
5
3
3
0
8
2
8
0
4
2
2
8
0
8
5
2
3
0
2
8
2
0
2
5
5
2
8
4
2
2
2
3
3
5
8
8
3
4
3
4
2
8
8
8
3
3
5
3
8
3
2
8
8
2
8
8
8
3
3
3
8
8
8
3
2
3
5
8
2
3
3
5
4
5
3
3
8
5
2
3
5
3
2
2
8
8
8
3
4
2
2
22
4
3
2
5
8
2
2
8
4
2
3
2
2
2
8
3
8
8
8
2
8
8
2
3
3
5
4
2
8
0
8
4
8
3
8
3
8
7
8
3
3
2
8
3
2
8
3
3
4
5
4
2
2
2
5
8
5
4
2
5
3
8
5
3
0
3
2
2
2
2
9
8
3
8
8
2
3
2
2
88
0
2
8
8
8
2
8
33
4
8
4
8
8
4
8
3
3
2
8
3
8
2
8
4
8
2
5
9
3
8
3
4
3
1
8
33
9
3
3
4
8
8
4
2
2
4
3
4
5
3
3
3
2
2
2
4
9
3
3
8
3
4
2
8
8
88
7
8
8
0
4
3
4
3
33
4
8
8
3
4
4
3
9
3
2
3
3
3
4
1
4
4
8
3
3
3
9
3
4
3
8
8
8
3
4
5
4
9
4
4
3
1
3
4
33
9
5
4
9
3
4
3
22
4
9
3
4
7
9
0
3
4
3
33
9
6
4
9
8
3
4
3
2
8
3
4
4
3
3
5
3
4
4
4
3
4
2
3
4
3
3
4
9
4
4
3
33 3
0
3
3
4
8
9
3
3
4
4
3
7
4
33
0
9
4
3
4
3
7
9
2
2
4
4
8
3
3
2
4
4
9
4
9
3
4
9
0
4
8
7
4
9
3
4
8
9
3
4
9
3
4
4
4
4
4
4
4
2
4
7
9
3
3
7
4
3
3
4
3
4
2
4
4
4
9
9
4
3
4
3
3
9
3
4
8
4
7
4
4
4
77
4
4
9
3
7
8
4
4 4
7
3
3
9 9
7
3
4
4
7
7
4
9
9
9
4
4
2
4
4
9
7
7
9
4
4
4
9
5
4
7
4
3
4
4
4
4
4
4
4
9
4
4
4
9
4
9
9
3
4
77
7
7
9
9
7
2
7
4
3
7
99
99
9
9
7
4
9
7
4
4
9
9
7
3
4
9
9
4
9
9
4
7
9
4
9
4
4
9
7
7 7
7
9
4
8
9
9
9
4
7
7
7
7
4
9
7
4 4
4
9
4
5
7
4
44
9
7
9
9
9
9
9
9
7
7
4
9
99
7
4
9
9
9
9
4
9
4
7
4
4
9
9
77
9
7
4
9
4
9
9
4
7
7
9
4
9
99
7
9
9
9
7
7
7
7
9
9
99
9
7
9
4
9
7
7
4
7
9
7
4
4
9
7
9
9
9
9
7
9
7
4
9
7
9
9
7
4
9
4
9
7
7
7
7
7
9
9
9
9
4
7
7
7
9
9
9
9
9
9
4
4
9
9
7
9
7
9
1
9
9
9
9
7
4
9
7
7
7
4
7
9
9
7
99
7
7
77
9
9
7
9
9
9
7
7
7
7
9
77
9
7
7
7
9
7
9
7
9
77
7
9
7
7
7
7
1
9
9
7
9
9
7
7
7
7
9
9
9
9
9
9
9
9
7
7
9
7
7
9 9
9
7
7
7
7
9
9
7
9
9
9
77
7
9
7
7
7
7
9
7
7
7
7
7
7
9
7
7
7
9
7
7
7
7
7

8
88
8
8

2
8
8
8

0
0
0

0
8
8

0
0

5
3

4
4

0

1

3

3

3

3

4

4

4

4

4

4

6

7

4

1

1

1
1
1

9

7
7
4
1

8
4

4
4
4
9
4
1

1

4

4
4
9
9
9
7
7

4
4

7

7

4

9

9
7
9

7

-5

0

5

pda: discriminant coordinate 1

figure 12.12. the    rst two penalized canonical variates, evaluated for the
test data. the circles indicate the class centroids. the    rst coordinate contrasts
mainly 0   s and 1   s, while the second contrasts 6   s and 7/9   s.

12.7 mixture discriminant analysis

449

examples are discussed in more detail in hastie et al. (1995), who also show
that the id173 improves the classi   cation performance of lda on
independent test data by a factor of around 25% in the cases they tried.

12.7 mixture discriminant analysis

id156 can be viewed as a prototype classi   er. each
class is represented by its centroid, and we classify to the closest using an
appropriate metric. in many situations a single prototype is not su   cient
to represent inhomogeneous classes, and mixture models are more appro-
priate. in this section we review gaussian mixture models and show how
they can be generalized via the fda and pda methods discussed earlier.
a gaussian mixture model for the kth class has density

p (x|g = k) =

rkxr=1

  kr  (x;   kr,   ),

(12.59)

where the mixing proportions   kr sum to one. this has rk prototypes for
the kth class, and in our speci   cation, the same covariance matrix    is
used as the metric throughout. given such a model for each class, the class
posterior probabilities are given by

p (g = k|x = x) = prk
pk
   =1pr   

r=1   kr  (x;   kr,   )  k

r=1      r  (x;      r,   )     

,

(12.60)

where   k represent the class prior probabilities.

we saw these calculations for the special case of two components in
chapter 8. as in lda, we estimate the parameters by maximum likelihood,
using the joint log-likelihood based on p (g, x):

kxk=1xgi=k

log" rkxr=1

  kr  (xi;   kr,   )  k# .

(12.61)

the sum within the log makes this a rather messy optimization problem
if tackled directly. the classical and natural method for computing the
maximum-likelihood estimates (id113s) for mixture distributions is the em
algorithm (dempster et al., 1977), which is known to possess good conver-
gence properties. em alternates between the two steps:

450

12. flexible discriminants

e-step: given the current parameters, compute the responsibility of sub-
class ckr within class k for each of the class-k observations (gi = k):

w (ckr|xi, gi) =

  kr  (xi;   kr,   )
   =1   k     (xi;   k   ,   )

.

(12.62)

prk

m-step: compute the weighted id113s for the parameters of each of the
component gaussians within each of the classes, using the weights
from the e-step.

in the e-step, the algorithm apportions the unit weight of an observation
in class k to the various subclasses assigned to that class. if it is close to the
centroid of a particular subclass, and far from the others, it will receive a
mass close to one for that subclass. on the other hand, observations halfway
between two subclasses will get approximately equal weight for both.

in the m-step, an observation in class k is used rk times, to estimate the
parameters in each of the rk component densities, with a di   erent weight
for each. the em algorithm is studied in detail in chapter 8. the algorithm
requires initialization, which can have an impact, since mixture likelihoods
are generally multimodal. our software (referenced in the computational
considerations on page 455) allows several strategies; here we describe the
default. the user supplies the number rk of subclasses per class. within
class k, a id116 id91 model, with multiple random starts, is    tted
to the data. this partitions the observations into rk disjoint groups, from
which an initial weight matrix, consisting of zeros and ones, is created.

rank{  k   } = l.

our assumption of an equal component covariance matrix    throughout
buys an additional simplicity; we can incorporate rank restrictions in the
mixture formulation just like in lda. to understand this, we review a little-
known fact about lda. the rank-l lda    t (section 4.3.3) is equivalent to
the maximum-likelihood    t of a gaussian model,where the di   erent mean
vectors in each class are con   ned to a rank-l subspace of irp (exercise 4.8).
we can inherit this property for the mixture model, and maximize the log-

likelihood (12.61) subject to rank constraints on all thepk rk centroids:
a weighted version of lda, with r = pk

again the em algorithm is available, and the m-step turns out to be
k=1 rk    classes.    furthermore,
we can use optimal scoring as before to solve the weighted lda problem,
which allows us to use a weighted version of fda or pda at this stage.
one would expect, in addition to an increase in the number of    classes,    a
similar increase in the number of    observations    in the kth class by a factor
of rk. it turns out that this is not the case if linear operators are used for
the optimal scoring regression. the enlarged indicator y matrix collapses
in this case to a blurred response matrix z, which is intuitively pleasing.
for example, suppose there are k = 3 classes, and rk = 3 subclasses per
class. then z might be

12.7 mixture discriminant analysis

451

g1 = 2
g2 = 1
g3 = 1
g4 = 3
g5 = 2
...
gn = 3

                                 

c11
0
0.9
0.1
0
0

c12
0
0.1
0.8
0
0

c13
0
0.0
0.1
0
0

0

0

0

c21
0.3
0
0
0
0.7
...
0

c22
0.5
0
0
0
0.1

c23
0.2
0
0
0
0.2

c31
0
0
0
0.5
0

c32
0
0
0
0.4
0

c33
0
0
0
0.1
0

0

0

0.1

0.1

0.8

                                 

where the entries in a class-k row correspond to w (ckr|x, gi).

the remaining steps are the same:

,

(12.63)

  z = sz
zt   z =   d  t
update   s and   s

         

m-step of mda.

these simple modi   cations add considerable    exibility to the mixture

model:

    the dimension reduction step in lda, fda or pda is limited by
the number of classes; in particular, for k = 2 classes no reduction is
possible. mda substitutes subclasses for classes, and then allows us
to look at low-dimensional views of the subspace spanned by these
subclass centroids. this subspace will often be an important one for
discrimination.

    by using fda or pda in the m-step, we can adapt even more to par-
ticular situations. for example, we can    t mda models to digitized
analog signals and images, with smoothness constraints built in.

figure 12.13 compares fda and mda on the mixture example.

12.7.1 example: waveform data

we now illustrate some of these ideas on a popular simulated example,
taken from breiman et al. (1984, pages 49   55), and used in hastie and
tibshirani (1996b) and elsewhere. it is a three-class problem with 21 vari-
ables, and is considered to be a di   cult pattern recognition problem. the
predictors are de   ned by

xj = u h1(j) + (1     u )h2(j) +   j
xj = u h1(j) + (1     u )h3(j) +   j
xj = u h2(j) + (1     u )h3(j) +   j

class 1,

class 2,
class 3,

(12.64)

where j = 1, 2, . . . , 21, u is uniform on (0, 1),   j are standard normal vari-
ates, and the h    are the shifted triangular waveforms: h1(j) = max(6    

452

12. flexible discriminants

fda / mars - degree 2

o

o

o

o

o

o

o

o
o

o
o

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
o
o
oo
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. .
o
o
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
.
. . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
. . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
. . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
oo
.
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
.
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
training error: 0.185
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
test error:       0.235
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
bayes error:    0.210

o
o
o
o

o
o

o
o
o

o
o

o
o

o
o

o

o

o

o

o

mda - 5 subclasses per class

o

o

o

o

o

o

o

o

o

o
o

o
o

   
   

   
   

o
o
o

o
o
o
o

. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
   
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
   
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
   
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
   
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
   
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
       
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
oo
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
   
o
o
o
. .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
   
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . .
. . . . . .
. . . . .
o
o
. . . . . .
. . . .
o
. . . . . .
. . . .
. . . . .
. . .
o
o
o
. . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
.
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
.
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.22
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.21

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
o
o

   
   

   
   

   
   

o
o

o
o
o

o
o

o
o

o
o

.... .. .. . .

o

o

o

o

o

o

o

o

figure 12.13. fda and mda on the mixture data. the upper plot uses
fda with mars as the regression procedure. the lower plot uses mda with
   ve mixture centers per class (indicated). the mda solution is close to bayes
optimal, as might be expected given the data arise from mixtures of gaussians.
the broken purple curve in the background is the bayes decision boundary.

12.7 mixture discriminant analysis

453

class 1

4
3
5

2
1

4
3
5

2
1

4
3
5

2
1

class 2

3
2
1

5
4

3
2
1

5
4

3
2
1

5
4

class 3

5
4
2
1
3

5
3
1
2
4

3
1
2
4
5

4
3
5

2
1

5
3
2
4
1

5
4
2
1

3

1
2

5
3
4

3
2
1

5
4

3

1
2
4
5

1
2

5
3
4

3
2
1
5
4

3

1
2
4
5

1
2

5
3
4

3
2
1

5
4

3

1
2
4
5

5
3
2
4
1

3
2
1

5
4

3

1
2
4
5

1
2

5
3
4

1
2

5
3
4

1
2

5
3
4

1
2
5
3
4

5
3
2
4
1

5
3
4
1
2

5
3
4
1
2

5
3
4
1
2

5
3
4
1
2

5
3
4
1
2

3

1
2
4
5

3
1
2
4
5

3
1
2
4
5

3
1
2
4
5

5
3
1
2
4

4
3
5

2
1

4
5

1
2
3

5
4
2
1

3

4
3
5

2
1

4
5

1
2
3

5
4
2
1

3

4
3
5
2
1

4
5

1
2
3

5
4
2
1

3

3
5
1
2
4

3
5
1
2
4

3
5
1
2
4

3
5
1
2
4

3
5
1
2
4

4
5

1
2
3

5
4
2
1

3

4
5

1
2
3

5
4
2
1
3

4
5

1
2
3

5
4
2
1
3

4
5
1
2
3

3
5
1
2
4

5
4
2
1
3

3
5
1
2
4

figure 12.14. some examples of the waveforms generated from model (12.64)
before the gaussian noise is added.

|j     11|, 0), h2(j) = h1(j     4) and h3(j) = h1(j + 4). figure 12.14 shows
some example waveforms from each class.
table 12.4 shows the results of mda applied to the waveform data, as
well as several other methods from this and other chapters. each train-
ing sample has 300 observations, and equal priors were used, so there are
roughly 100 observations in each class. we used test samples of size 500.
the two mda models are described in the caption.

figure 12.15 shows the leading canonical variates for the penalized mda
model, evaluated at the test data. as we might have guessed, the classes
appear to lie on the edges of a triangle. this is because the hj(i) are repre-
sented by three points in 21-space, thereby forming vertices of a triangle,
and each class is represented as a convex combination of a pair of vertices,
and hence lie on an edge. also it is clear visually that all the information
lies in the    rst two dimensions; the percentage of variance explained by the
   rst two coordinates is 99.8%, and we would lose nothing by truncating the
solution there. the bayes risk for this problem has been estimated to be
about 0.14 (breiman et al., 1984). mda comes close to the optimal rate,
which is not surprising since the structure of the mda model is similar to
the generating model.

454

12. flexible discriminants

table 12.4. results for waveform data. the values are averages over ten sim-
ulations, with the standard error of the average in parentheses. the    ve entries
above the line are taken from hastie et al. (1994). the    rst model below the line
is mda with three subclasses per class. the next line is the same, except that the
discriminant coe   cients are penalized via a roughness penalty to e   ectively 4df.
the third is the corresponding penalized lda or pda model.

technique

error rates

training

test

lda
qda
cart
fda/mars (degree = 1)
fda/mars (degree = 2)
mda (3 subclasses)
mda (3 subclasses, penalized 4 df)
pda (penalized 4 df)
bayes

0.121(0.006)
0.039(0.004)
0.072(0.003)
0.100(0.006)
0.068(0.004)
0.087(0.005)
0.137(0.006)
0.150(0.005)

0.191(0.006)
0.205(0.006)
0.289(0.004)
0.191(0.006)
0.215(0.002)
0.169(0.006)
0.157(0.005)
0.171(0.005)

0.140

3 subclasses, penalized 4 df

3 subclasses, penalized 4 df

1

3

2

 
r
a
v

 
t

n
a
n
m

i

i
r
c
s
d

i

4

2

0

2
-

4
-

6
-

1
11
1
3
3
1

1

3

3

3

3

3

3
3
3

3
3
33

1
1
1
1

3
3
3
3
3
3

3
1

3
3
3
3
3
3
3
3
3
1
3
3
3
1
1
3
3
3
3
1
3
3
3
3
3
3
3
1
1
3
1
1
3
1
3
3
3
1
3
1
1
1
1
3
1
1
3
1
1
3
1
1
1
1
1
1
11
1
1
1
11
1
3
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1 1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
2
1
1
1
1
2
1

1

1
1
1
1
1

1
1

1

1

1

1

1

1

1

3

3

3

3

3
3
3

3
3

3
3
3
3
3
3
3
33
3
3
3
3 3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3

3
3

3
3

3

3

3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
2

3
3
3
3
3
3
3
3
3
3
3

3

3

3

3
3
3
2

2

3
3
3

2
3
2
2
2
2
2
2
2
2
2
2
3
2
2
2
2
2

3

2

3
3
3
3
3
3
3
2
2
2
3
2
2
2
3
3
3
3
2
2
3
2
2
3
2
3
2
3
2
2
2
2
3
2
3
2
2
3
3
2
2
3
2
2
2
2
2
2
2
3
2
3
2
3
3
3
2
2
2
3
2
3
2
2
2
2
2
2

2

2
2

2

2
2
2
2
2
2
2
2
2
2
2
2
2
2
22
2
2
2

2

2
1

2
2
2
2
2
2
2

1
1
1
1

1

2

2

2
1
1
2
2
1
2
1
1
2
1
1
2
2
1
2
2
2
1
1
1
1
2
2
2
2
1
1
1
2
1
2
1
1
2
1
1
2
2
2
2
1
2
1
1
1
2
2
2
1
2
2
2
2
1
2
1
2
2
2
1
1
1
1
1

1
1

1

2
2
2
2
2
2
2
2
2
2
1
2
2
2
2
2
2
2
2

1

2
2
2
2
2
2
2

2

1
1

4

 
r
a
v

 
t

n
a
n
m

i

i
r
c
s
d

i

0

.

1

5

.

0

0

.

0

0

.

1
-

3

3

3
2

3
3
1
3
1

1

3

3

3
1
1
2

3
3

2
2
3
3
1
3
1

3

3

1

3

3

3

2
3
1
1

2

1

1

1

1

2
2

1
3
3
3
2
1
3
3
3
2
3
3
1
3
2
3
1
3
1
2
1
1
3
1
2
3
2
3
2
3
2
2
1
1
2
3
3
2
1
2
2
2
3
3
3
1
2
2
3
3
1
1
3
1
2
2
3
1
3
2
2
2
2
3
1
2
2
2
1 1
1
33
3
3
1
2
1
1
2
3
3
1
3
3
1
1
3
2
2
3
2
2
1
1
2
3
2
3
1
3
1
3
2
22
1
3
2
1
2
3
3
1
2
2
1
3
1
3
3
3
1
3
3
1
2
33
1
1
2
3
2
2
3
1
1
1
1
1
1
1
3
3
1
3
2
3
3
1
3
2
3
1
1
1
3
3
1
2
1
2
1
111
3
1
2
2
1
3
1
2
1
2
2
2
3
3
3
1
2
3
3
1
3
2
2
1
2
2
1
1
1
1
1
2
2
3
3
1
1
2
1
22
3
2
2
2
2
1
3
1
2
3
1
3
3
3
3
3
2
2
2
2
1
3
1
3
3 3
1
1
1
2
1
2
2
3
1
2
1
1
2
2
3
3
3
2
2
1
1
2
3
1
1
2
3
1
2
2
1
2
1
1
1
1
1
2
3
2
2
3
3
3
2
2
2
2
1
3
1
3
1
1
1
3
2
3
3
2
1
1
3
3
1
2
1
3
1
2
11
2
3
2
3
3
1
2
1
2
1
3
1
2
3
1
2
3
2
2
2
3
3
3
1
1
2
3
1
1
3
2
1
3
3
3
3
2
3
2
3
1
2
1
2
1
1
1
2
2
2
1
1
1
2
2
1
1
1
2
2
2
3
1
2
2 2
3
3
1
2
3
2
2
1
3
2
3
3
3
1
1
1
1
3
1
1
2
2

1
3
1

3
3

1
1

1

1

1

2

3

3

3

3

3

3

1
1
2

2

2
3
1
3

1
3
3
2
3
1
3
1
3
3
2
3
3
3
2
3
3
1
3
2
2
1
2
1

1
2
2

1
3

2

1

1
2
1
1
3
2
2

1

-6

-4

-2

0

2

4

6

-2

-1

0

3

3

1

2

3

1

2

2

1

discriminant var 1

discriminant var 3

figure 12.15. some two-dimensional views of the mda model    tted to a
sample of the waveform model. the points are independent test data, projected
on to the leading two canonical coordinates (left panel), and the third and fourth
(right panel). the subclass centers are indicated.

exercises

455

computational considerations

with n training cases, p predictors, and m support vectors, the support
vector machine requires m3 + mn + mpn operations, assuming m     n .
they do not scale well with n , although computational shortcuts are avail-
able (platt, 1999). since these are evolving rapidly, the reader is urged to
search the web for the latest technology.

lda requires n p2 + p3 operations, as does pda. the complexity of
fda depends on the regression method used. many techniques are linear
in n , such as additive models and mars. general splines and kernel-based
regression methods will typically require n 3 operations.

software is available for    tting fda, pda and mda models in the r

package mda, which is also available in s-plus.

bibliographic notes

the theory behind support vector machines is due to vapnik and is de-
scribed in vapnik (1996). there is a burgeoning literature on id166s; an
online bibliography, created and maintained by alex smola and bernhard
sch  olkopf, can be found at:

http://www.kernel-machines.org.

our treatment is based on wahba et al. (2000) and evgeniou et al. (2000),
and the tutorial by burges (burges, 1998).

id156 is due to fisher (1936) and rao (1973). the
connection with optimal scoring dates back at least to breiman and ihaka
(1984), and in a simple form to fisher (1936). there are strong connections
with correspondence analysis (greenacre, 1984). the description of    exible,
penalized and mixture discriminant analysis is taken from hastie et al.
(1994), hastie et al. (1995) and hastie and tibshirani (1996b), and all
three are summarized in hastie et al. (2000); see also ripley (1996).

exercises

ex. 12.1 show that the criteria (12.25) and (12.8) are equivalent.

ex. 12.2 show that the solution to (12.29) is the same as the solution to
(12.25) for a particular kernel.

ex. 12.3 consider a modi   cation to (12.43) where you do not penalize the
constant. formulate the problem, and characterize its solution.

ex. 12.4 suppose you perform a reduced-subspace linear discriminant anal-
ysis for a k-group problem. you compute the canonical variables of di-

456

12. flexible discriminants

mension l     k     1 given by z = ut x, where u is the p    l matrix of
discriminant coe   cients, and p > k is the dimension of x.
(a) if l = k     1 show that

kz       zkk2     kz       zk   k2 = kx       xkk2

w     kx       xk   k2
w ,

where k  kw denotes mahalanobis distance with respect to the covari-
ance w.

(b) if l < k     1, show that the same expression on the left measures
the di   erence in mahalanobis squared distances for the distributions
projected onto the subspace spanned by u.

ex. 12.5 the data in phoneme.subset, available from this book   s website

http://www-stat.stanford.edu/elemstatlearn

consists of digitized log-periodograms for phonemes uttered by 60 speakers,
each speaker having produced phonemes from each of    ve classes. it is
appropriate to plot each vector of 256    features    against the frequencies
0   255.

(a) produce a separate plot of all the phoneme curves against frequency

for each class.

(b) you plan to use a nearest prototype classi   cation scheme to classify
the curves into phoneme classes. in particular, you will use a id116
id91 algorithm in each class (kmeans() in r), and then classify
observations to the class of the closest cluster center. the curves are
high-dimensional and you have a rather small sample-size-to-variables
ratio. you decide to restrict all the prototypes to be smooth functions
of frequency. in particular, you decide to represent each prototype m
as m = b   where b is a 256    j matrix of natural spline basis
functions with j knots uniformly chosen in (0, 255) and boundary
knots at 0 and 255. describe how to proceed analytically, and in
particular, how to avoid costly high-dimensional    tting procedures.
(hint: it may help to restrict b to be orthogonal.)

(c) implement your procedure on the phoneme data, and try it out. divide
the data into a training set and a test set (50-50), making sure that
speakers are not split across sets (why?). use k = 1, 3, 5, 7 centers
per class, and for each use j = 5, 10, 15 knots (taking care to start
the id116 procedure at the same starting values for each value of
j), and compare the results.

ex. 12.6 suppose that the regression procedure used in fda (section 12.5.1)
is a linear expansion of basis functions hm(x), m = 1, . . . , m . let d   =
yt y/n be the diagonal matrix of class proportions.

exercises

457

(a) show that the optimal scoring problem (12.52) can be written in vector

notation as

  ,   ky       h  k2 ,
min

(12.65)

where    is a vector of k real numbers, and h is the n    m matrix
of evaluations hj(xi).

(b) suppose that the id172 on    is   t d  1 = 0 and   t d     = 1.
interpret these id172s in terms of the original scored   (gi).

(c) show that, with this id172, (12.65) can be partially optimized

w.r.t.   , and leads to

  t yt sy  ,

max

  

(12.66)

subject to the id172 constraints, where s is the projection
operator corresponding to the basis matrix h.

(d) suppose that the hj include the constant function. show that the

largest eigenvalue of s is 1.

(e) let    be a k    k matrix of scores (in columns), and suppose the
id172 is   t d     = i. show that the solution to (12.53) is
given by the complete set of eigenvectors of s; the    rst eigenvector is
trivial, and takes care of the centering of the scores. the remainder
characterize the optimal scoring solution.

ex. 12.7 derive the solution to the penalized optimal scoring problem
(12.57).

ex. 12.8 show that coe   cients       found by optimal scoring are proportional
to the discriminant directions       found by id156.
ex. 12.9 let   y = x   b be the    tted n    k indicator response matrix after
id75 on the n  p matrix x, where p > k. consider the reduced
features x   
i is equivalent to lda in the
original space.

i =   bt xi. show that lda using x   

ex. 12.10 kernels and id156. suppose you wish to
carry out a id156 (two classes) using a vector of
transformations of the input variables h(x). since h(x) is high-dimensional,
you will use a regularized within-class covariance matrix wh +   i. show
that the model can be estimated using only the inner products k(xi, xi    ) =
hh(xi), h(xi    )i. hence the kernel property of support vector machines is also
shared by regularized id156.

ex. 12.11 the mda procedure models each class as a mixture of gaussians.
hence each mixture center belongs to one and only one class. a more
general model allows each mixture center to be shared by all classes. we
take the joint density of labels and features to be

458

12. flexible discriminants

p (g, x) =

rxr=1

  rpr(g, x),

(12.67)

a mixture of

joint densities. furthermore we assume

pr(g, x) = pr(g)  (x;   r,   ).

(12.68)

this model consists of regions centered at   r, and for each there is a class
pro   le pr(g). the posterior class distribution is given by

p (g = k|x = x) = pr

r=1   rpr(g = k)  (x;   r,   )

r=1   r  (x;   r,   )

,

(12.69)

where the denominator is the marginal distribution p (x).

pr

(a) show that this model (called mda2) can be viewed as a generalization

of mda since

p (x|g = k) = pr

pr
where   rk =   rpr(g = k)/pr

mixing proportions for the kth class.

r=1   rpr(g = k)  (x;   r,   )

r=1   rpr(g = k)

,

(12.70)

r=1   rpr(g = k) corresponds to the

(b) derive the em algorithm for mda2.

(c) show that if the initial weight matrix is constructed as in mda, in-
volving separate id116 id91 in each class, then the algorithm
for mda2 is identical to the original mda procedure.

this is page 459
printer: opaque this

13
prototype methods and
nearest-neighbors

13.1

introduction

in this chapter we discuss some simple and essentially model-free methods
for classi   cation and pattern recognition. because they are highly unstruc-
tured, they typically are not useful for understanding the nature of the
relationship between the features and class outcome. however, as black box
prediction engines, they can be very e   ective, and are often among the best
performers in real data problems. the nearest-neighbor technique can also
be used in regression; this was touched on in chapter 2 and works reason-
ably well for low-dimensional problems. however, with high-dimensional
features, the bias   variance tradeo    does not work as favorably for nearest-
neighbor regression as it does for classi   cation.

13.2 prototype methods

throughout this chapter, our training data consists of the n pairs (x1, g1),
. . . , (xn, gn ) where gi is a class label taking values in {1, 2, . . . , k}. pro-
totype methods represent the training data by a set of points in feature
space. these prototypes are typically not examples from the training sam-
ple, except in the case of 1-nearest-neighbor classi   cation discussed later.
each prototype has an associated class label, and classi   cation of a query
point x is made to the class of the closest prototype.    closest    is usually
de   ned by euclidean distance in the feature space, after each feature has

460

13. prototypes and nearest-neighbors

been standardized to have overall mean 0 and variance 1 in the training
sample. euclidean distance is appropriate for quantitative features. we
discuss distance measures between qualitative and other kinds of feature
values in chapter 14.

these methods can be very e   ective if the prototypes are well positioned
to capture the distribution of each class. irregular class boundaries can be
represented, with enough prototypes in the right places in feature space.
the main challenge is to    gure out how many prototypes to use and where
to put them. methods di   er according to the number and way in which
prototypes are selected.

13.2.1 id116 id91

id116 id91 is a method for    nding clusters and cluster centers in a
set of unlabeled data. one chooses the desired number of cluster centers, say
r, and the id116 procedure iteratively moves the centers to minimize
the total within cluster variance.1 given an initial set of centers, the k-
means algorithm alternates the two steps:

    for each center we identify the subset of training points (its cluster)

that is closer to it than any other center;

    the means of each feature for the data points in each cluster are
computed, and this mean vector becomes the new center for that
cluster.

these two steps are iterated until convergence. typically the initial centers
are r randomly chosen observations from the training data. details of the
id116 procedure, as well as generalizations allowing for di   erent variable
types and more general distance measures, are given in chapter 14.

to use id116 id91 for classi   cation of labeled data, the steps

are:

    apply id116 id91 to the training data in each class sepa-

rately, using r prototypes per class;

    assign a class label to each of the k    r prototypes;
    classify a new feature x to the class of the closest prototype.
figure 13.1 (upper panel) shows a simulated example with three classes
and two features. we used r = 5 prototypes per class, and show the clas-
si   cation regions and the decision boundary. notice that a number of the

1the    k    in id116 refers to the number of cluster centers. since we have already

reserved k to denote the number of classes, we denote the number of clusters by r.

13.2 prototype methods

461

id116 - 5 prototypes  per class

o

o

o

o

o

o

o
o

o
o
o
o

   
   

   
   

   
   

   
   

o
o
o
o
o

o
o
o
o
o
o
o
o
o

...................................................................................................
...............
.................................................................................................
.................
................................................................................................
..................
...............................................................................................
...................
..............................................................................................
....................
............................................................................................
......................
...........................................................................................
.......................
o
..........................................................................................
........................
.........................................................................................
.........................
.......................................................................................
...........................
......................................................................................
............................
.....................................................................................
.............................
o
o
....................................................................................
..............................
o
..................................................................................
................................
.................................................................................
.................................
................................................................................
..................................
...............................................................................
...................................
o
.............................................................................
.....................................
o
............................................................................
......................................
...........................................................................
.......................................
o
..........................................................................
........................................
o
o
o
o
o
o
....
..........................................
..........................................
..........................
o
o
o
o
.........................................
......................
........
...........................................
o
o
o
o
.........................................
.................
............
............................................
o
o
o
o
........................................
............
.................
.............................................
o
o
o
o
o
o
o
o
.......................................
.......
..............................................
......................
o
o
o
o
o
o
o
o
........
.......................................
..........................................
.........................
o
o
o
o
......................................
.............
..........................
.....................................
o
o
o
o
o
o
o
o
.....................................
.................
.................................
...........................
o
o
o
o
o
o
o o
.......................
.....................................
..........................
............................
o
o
o
o
o
o
o
o
....................................
...........................
........................
...........................
o
o
o
o
o
o
o
o
...................................
...............................
.............................
...................
o
o
o
o
o
o
o
.................................
...................................
...............................
...............
o
o
o
o
o
o
o
...................................
....................................
.................................
..........
o
o
o
oo
o
   
o
o
o
o
   
o
o
o
....................................
.......................................
.................................
......
o
o
o
....................................
..........................................
...................................
.
   
o
o
o
o
   
ooo
o
....................................
..........................................
....................................
o
o
o
o
o
o
o
o
o
........................................
....................................
......................................
o
o
o
o
o
o
o
o
o
o
...................................
.......................................
.
.......................................
o
o
o
o
o
o
o
o
.....................................
......
...............................
...
............
.........................
oo
o
o
o
o
o
o
........
...........
.........
.........................
...................................
..........................
o
o
o
...................................
................
.....................
.............
.......................
......
o
o
o
o
o
o
o
.......................................
.....................
.................
................
...
..................
o
o
o
..........................
..........................................
............
.....................
.............
o
o
o
.....
...............................
.............................................
........................
.........
o
o
o
o
o
o
o
o
o
o
o
o
o
..........................................................
.............................................
...........
o
o
o
o
.............................................
........................................................
.............
o
o
o
o
o
o
o
.............................................
......................................................
...............
o
o
o
o
o
....................................................
.............................................
.................
o
o
o
o
o
o
o
..................................................
.............................................
...................
o
o
o
o
o
................................................
.............................................
.....................
o
o
o
o
..............................................
.............................................
.......................
o
.............................................
............................................
.........................
o
o
o
o
o
............................................
..........................................
............................
o
o
o
o
............................................
..........................................
............................
o
o
............................................
..........................................
............................
o
o
o
............................................
..........................................
............................
o
.........................................
............................................
.............................
o
............................................
............................................
..........................
............................................
...............................................
.......................
o
............................................
..................................................
....................
oo
o
.....................................................
............................................
.................
o
..............................................
........................................................
............
................................................
...........................................................
.......
..................................................
..............................................................
..
o
o
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................

o
o o
o
o
o
o
o
o

   
   
   
   

   
   
   
   

o
o
o
o
o
o
o
o

o
o
o
o

   
   

   
   

   
   

   
   

   
   

o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

lvq - 5 prototypes per class

o

o

o

o

o

o

o
o

o
o
o
o

   
   

   
   

o
o
o
o
o

o
o
o
o
o
o
o
o
o

   
   
   
   

.............................................................................................
.....................
............................................................................................
......................
...........................................................................................
.......................
..........................................................................................
........................
.........................................................................................
.........................
........................................................................................
..........................
.......................................................................................
...........................
o
......................................................................................
............................
......................................................................................
............................
.....................................................................................
.............................
....................................................................................
..............................
...................................................................................
...............................
o
o
..................................................................................
................................
o
.................................................................................
.................................
................................................................................
..................................
...............................................................................
...................................
..............................................................................
....................................
o
.............................................................................
.....................................
o
............................................................................
......................................
...........................................................................
.......................................
o
..........................................................................
........................................
o
o
o
o
o
o
.........................................
..............................
..
.........................................
o
o
o
o
........................................
........................
........
..........................................
o
o
o
o
........................................
.................
..............
...........................................
o
o
o
o
..........
.......................................
.....................
............................................
o
o
o
o
o
o
o
o
......................................
.........
.......................................
............................
o
o
o
o
o
o
o
o
.....................................
.............
..................................
..............................
o
   
o
o
o
.....................................
..................
.............................
..............................
o
o
o
o
o
o
o
o
.......................
....................................
........................
...............................
o
   
o
o
o
o
o
o o
............................
...................................
................................
...................
o
o
o
o
o
o
o
o
.................................
....................................
...............................
..............
o
o
o
o
o
o
o
o
.....................................
.....................................
...............................
.........
o
o
o
o
o
   
o
o
.........................................
.....................................
................................
....
o
o
o
o
o
o
o
.....................................
............................................
.................................
o
o
o
oo
o
o
o
o
   
o
o
o
o
.....................................
...........................................
..................................
o
o
o
....................................
..........................................
....................................
o
o
o
o
ooo
....................................
.........................................
.....................................
o
o
o
o
o
o
o
o
o
.........................................
....................................
.....................................
o
o
o
o
o
o
o
o
o
o
........................................
....................................
......................................
o
o
o
o
o
o
o
o
....................................
.......................................
.......................................
oo
o
o
o
o
o
o
.
......................................
...................................
........................................
o
o
o
........
.............
.....................................
............................
............................
o
o
   
o
o
o
o
o
................
...............
..................
....................................
.............................
o
o
o
......................
....................
.....................................
........
...........................
o
   
o
o
o
..................................................
.......................................
.........................
o
o
o
o
o
o
o
o
o
o
o
o
o
..................................................
........................................
........................
o
o
o
o
...................................................
.........................................
......................
o
o
o
o
o
o
o
o
..........................................
...................................................
.....................
o
o
o
o
o
............................................
....................................................
..................
o
o
o
o
o
o
o
.............................................
....................................................
.................
o
o
o
o
o
..............................................
....................................................
................
o
o
o
o
...............................................
.....................................................
..............
o
............
.....................................................
.................................................
o
o
o
o
..................................................
......................................................
..........
o
o
o
o
...................................................
......................................................
.........
o
o
.....................................................
......................................................
.......
o
o
o
......................................................
.......................................................
.....
o
.......................................................
.......................................................
.......
o
........................................................
.......................................................
..................................................................................................................
o
..................................................................................................................
oo
o
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................

o
o o
o
o
o
o
o
o

   
   
   
   

o
o
o
o
o
o
o
o

o
o
o
o

   
   

   
   

   
   

   
   

   
   

   
   

o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

o

figure 13.1. simulated example with three classes and    ve prototypes per
class. the data in each class are generated from a mixture of gaussians. in the
upper panel, the prototypes were found by applying the id116 id91 algo-
rithm separately in each class. in the lower panel, the lvq algorithm (starting
from the id116 solution) moves the prototypes away from the decision bound-
ary. the broken purple curve in the background is the bayes decision boundary.

462

13. prototypes and nearest-neighbors

algorithm 13.1 learning vector quantization   lvq.

1. choose r initial prototypes for each class: m1(k), m2(k), . . . , mr(k),
k = 1, 2, . . . , k, for example, by sampling r training points at random
from each class.

2. sample a training point xi randomly (with replacement), and let (j, k)

index the closest prototype mj(k) to xi.

(a) if gi = k (i.e., they are in the same class), move the prototype

towards the training point:

mj(k)     mj(k) +   (xi     mj(k)),

where    is the learning rate.

(b) if gi 6= k (i.e., they are in di   erent classes), move the prototype

away from the training point:

mj(k)     mj(k)       (xi     mj(k)).

3. repeat step 2, decreasing the learning rate    with each iteration to-

wards zero.

prototypes are near the class boundaries, leading to potential misclassi   ca-
tion errors for points near these boundaries. this results from an obvious
shortcoming with this method: for each class, the other classes do not have
a say in the positioning of the prototypes for that class. a better approach,
discussed next, uses all of the data to position all prototypes.

13.2.2 learning vector quantization

in this technique due to kohonen (1989), prototypes are placed strategically
with respect to the decision boundaries in an ad-hoc way. lvq is an online
algorithm   observations are processed one at a time.

the idea is that the training points attract prototypes of the correct class,
and repel other prototypes. when the iterations settle down, prototypes
should be close to the training points in their class. the learning rate    is
decreased to zero with each iteration, following the guidelines for stochastic
approximation learning rates (section 11.4.)

figure 13.1 (lower panel) shows the result of lvq, using the id116
solution as starting values. the prototypes have tended to move away from
the decision boundaries, and away from prototypes of competing classes.

the procedure just described is actually called lvq1. modi   cations
(lvq2, lvq3, etc.) have been proposed, that can sometimes improve per-
formance. a drawback of learning vector quantization methods is the fact

13.3 k-nearest-neighbor classi   ers

463

that they are de   ned by algorithms, rather than optimization of some    xed
criteria; this makes it di   cult to understand their properties.

13.2.3 gaussian mixtures

the gaussian mixture model can also be thought of as a prototype method,
similar in spirit to id116 and lvq. we discuss gaussian mixtures in
some detail in sections 6.8, 8.5 and 12.7. each cluster is described in terms
of a gaussian density, which has a centroid (as in id116), and a covari-
ance matrix. the comparison becomes crisper if we restrict the component
gaussians to have a scalar covariance matrix (exercise 13.1). the two steps
of the alternating em algorithm are very similar to the two steps in k-
means:

    in the e-step, each observation is assigned a responsibility or weight
for each cluster, based on the likelihood of each of the correspond-
ing gaussians. observations close to the center of a cluster will most
likely get weight 1 for that cluster, and weight 0 for every other clus-
ter. observations half-way between two clusters divide their weight
accordingly.

    in the m-step, each observation contributes to the weighted means

(and covariances) for every cluster.

as a consequence, the gaussian mixture model is often referred to as a soft
id91 method, while id116 is hard.

similarly, when gaussian mixture models are used to represent the fea-
ture density in each class, it produces smooth posterior probabilities   p(x) =
{  p1(x), . . . ,   pk (x)} for classifying x (see (12.60) on page 449.) often this
is interpreted as a soft classi   cation, while in fact the classi   cation rule is
  g(x) = arg maxk   pk(x). figure 13.2 compares the results of id116 and
gaussian mixtures on the simulated mixture problem of chapter 2. we
see that although the decision boundaries are roughly similar, those for the
mixture model are smoother (although the prototypes are in approximately
the same positions.) we also see that while both procedures devote a blue
prototype (incorrectly) to a region in the northwest, the gaussian mixture
classi   er can ultimately ignore this region, while id116 cannot. lvq
gave very similar results to id116 on this example, and is not shown.

13.3 k-nearest-neighbor classi   ers

these classi   ers are memory-based, and require no model to be    t. given
a query point x0, we    nd the k training points x(r), r = 1, . . . , k closest in
distance to x0, and then classify using majority vote among the k neighbors.

464

13. prototypes and nearest-neighbors

id116 - 5 prototypes per class

o

o

o

o

o

o
o

   
   

   
   

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
.
. . . . . . . . . . . .
. .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
. . . . . . . . . . . .
. . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
   
. . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
   
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
   
o
o
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
   
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
.
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
.
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
. . .
. . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
o
o
o
. . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
. . . . . . . . . .
. . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
. . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
o
. . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . .
   
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . .
o
   
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . .
. . . . . . .
. . . . . . . . . .
. . . . . .
. . . . . . . . .
. . . . . .
. . . . . . . .
. . . . .
. . . . . . . .
. . . .
. . . . . . .
. . .
o
o
o
. . . . . .
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. .
. . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
.
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
... .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.170
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.243
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
   
oo
   

o
o
o
o

   
   

   
   

   
   

   
   

o
o

o
o
o

o
o

o

o

o

o

o

o

gaussian mixtures - 5 subclasses per class

o

o

o

o

o

o

o

o

o

o
o

o
o

   
   

   
   

o
o
o

o
o
o
o

. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
   
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
   
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
o
o
. . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
o
oo
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
   
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
   
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
       
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
oo
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
   
o
o
o
. .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
. . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
o
o
. . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
   
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
   
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . .
. . . . . .
. . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . .
o
o
. . . . . .
. . . .
o
. . . . . .
. . . .
. . . . .
. . .
o
o
o
. . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
. .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
.
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . .
.
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.17
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.22
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.21

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
o
o

   
   

   
   

   
   

o
o

o
o
o

o
o

o
o

o
o

.... .. .. . .

o

o

o

o

o

o

o

o

figure 13.2. the upper panel shows the id116 classi   er applied to the
mixture data example. the decision boundary is piecewise linear. the lower panel
shows a gaussian mixture model with a common covariance for all component
gaussians. the em algorithm for the mixture model was started at the id116
solution. the broken purple curve in the background is the bayes decision
boundary.

13.3 k-nearest-neighbor classi   ers

465

ties are broken at random. for simplicity we will assume that the features
are real-valued, and we use euclidean distance in feature space:

d(i) = ||x(i)     x0||.

(13.1)

typically we    rst standardize each of the features to have mean zero and
variance 1, since it is possible that they are measured in di   erent units. in
chapter 14 we discuss distance measures appropriate for qualitative and
ordinal features, and how to combine them for mixed data. adaptively
chosen distance metrics are discussed later in this chapter.

despite its simplicity, k-nearest-neighbors has been successful in a large
number of classi   cation problems, including handwritten digits, satellite
image scenes and ekg patterns. it is often successful where each class
has many possible prototypes, and the decision boundary is very irregular.
figure 13.3 (upper panel) shows the decision boundary of a 15-nearest-
neighbor classi   er applied to the three-class simulated data. the decision
boundary is fairly smooth compared to the lower panel, where a 1-nearest-
neighbor classi   er was used. there is a close relationship between nearest-
neighbor and prototype methods: in 1-nearest-neighbor classi   cation, each
training point is a prototype.

figure 13.4 shows the training, test and tenfold cross-validation errors
as a function of the neighborhood size, for the two-class mixture problem.
since the tenfold cv errors are averages of ten numbers, we can estimate
a standard error.

because it uses only the training point closest to the query point, the bias
of the 1-nearest-neighbor estimate is often low, but the variance is high.
a famous result of cover and hart (1967) shows that asymptotically the
error rate of the 1-nearest-neighbor classi   er is never more than twice the
bayes rate. the rough idea of the proof is as follows (using squared-error
loss). we assume that the query point coincides with one of the training
points, so that the bias is zero. this is true asymptotically if the dimension
of the feature space is    xed and the training data    lls up the space in a
dense fashion. then the error of the bayes rule is just the variance of a
bernoulli random variate (the target at the query point), while the error of
1-nearest-neighbor rule is twice the variance of a bernoulli random variate,
one contribution each for the training and query targets.

we now give more detail for misclassi   cation loss. at x let k    be the
dominant class, and pk(x) the true id155 for class k. then

1-nearest-neighbor error =

pk(x)(1     pk(x)),

bayes error = 1     pk   (x),

kxk=1

    1     pk   (x).

(13.2)

(13.3)

(13.4)

the asymptotic 1-nearest-neighbor error rate is that of a random rule; we
pick both the classi   cation and the test point at random with probabili-

466

13. prototypes and nearest-neighbors

15-nearest neighbors

o

o

o

o

o

o

o
o

o
o
o
o

o
o
o
o
o

o
o
o
o
o
o
o
o
o

..............................................................................................
....................
.....................
.............................................................................................
...........................................................................................
.......................
........................
..........................................................................................
.........................
.........................................................................................
..........................
........................................................................................
...........................
.......................................................................................
o
...........................
.......................................................................................
............................
......................................................................................
.............................
.....................................................................................
..............................
....................................................................................
...............................
...................................................................................
o
o
...............................
...................................................................................
o
................................
..................................................................................
.................................
.................................................................................
..................................
................................................................................
...................................
...............................................................................
o
....................................
..............................................................................
o
......................................
............................................................................
.......................................
...........................................................................
o
.
........................................
.
................................................................... ... ..
o
o
o
o
o
o
.........................................
.........................................................................
o
o
o
o
..........................................
.
..
..........
.................................................. .........
o
o
o
o
.............
.............................................
......
..................................................
o
o
o
o
.................................................................
.................................................
o
o
o
o
o
o
o
o
. ..................................................................
.............................................. .
o
o
o
o
o
o
o
o
........................................................................
..........................................
o
o
o
o
...............................................................................
...................................
o
o
o
o
o
o
o
o
.............................................
.............................
.....
...................................
o
o
o
o
o
o
o o
..........................................
........................
............
....................................
o
o
o
o
o
o
o
........................
........................................
..................................
................
o
o
o
o
o
o
o
o
..................................... .. .
.
..................................
.. .
. .................
.
o
o
o
o
o
o
o
...................................
.. .. .
.
. ................................ ..
...................................
.
..
o
o
o
o
o
o
o
........................................
...................................
.......................................
o
o
o
oo
o
o
o
o
o
o
o
o
. ...................................
........................................
..................................... .
o
o
o
..................................
........................................
........................................
o
o
o
o
ooo
o
.....................................
........................................
.....................................
o
o
o
o
o
o
o
o
o
.......................................
......................................
.....................................
o
o
o
o
o
o
o
o
o
o
.........
......
................................
........................................
...........................
o
o
o
o
o
o
o
o
.
..........
.............
................................
........................................
.. ................
oo
o
o
o
o
o
o
.................................
.......................................
....................
....................
o
o
o
..........................................
.................................
.......................................
o
o
o
o
o
o
o
.......
..........................................
.
...
....
...........................
.......... ....................
o
o
o
.................
....
............................................
.........................
....................
....
o
o
o
o
o
..........
................................................
........................
.................
...............
o
o
o
o
o
o
o
o
o
o
o
o
o
........................
. .
..............
...................................................
....
............
.. . ..
o
o
o
o
....................
..................................
................................................
..........
..
o
o
o
o
o
o
o
o
...................................................
.......................................................
........
o
o
o
o o
o
o
....................................................
.......................................................
.......
o
o
o
o
o
o
o
o
.....................................................
.............................................
......
....
......
o
o
o
o
o
o
o
.........................................................
...........................................
....
.....
.....
o
o
o
o
..... ....................................................
...........................................
.....
...
.
.....
o
..........................................
..........................................................
......
......
..
o
o
o
o
o
o
...........................................................
.....................................................
..
o
o
o
o
..................................................................................................................
o
o
....................................................
...........................................................
. ....
o
o
o
................................................. . ..............................................................
o
o
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
o
o
..................................................................................................................
oo
o
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................

o
o
o
o
o
o
o
o

o
o
o
o

o
....
.........

o
o

o
o

o
o

o

o

o

o

o

o

o

....

..

..

o

o

1-nearest neighbor

o

o

o

o

o

o

o
o

o
o
o
o

o
o
o
o
o

o
o
o
o
o
o
o
o
o

......................
............................................................................................
.......................
...........................................................................................
.......................
...........................................................................................
........................
..........................................................................................
.........................
.........................................................................................
..........................
........................................................................................
..........................
........................................................................................
o
...........................
.......................................................................................
............................
......................................................................................
.............................
.....................................................................................
.............................
.....................................................................................
..............................
....................................................................................
o
o
...............................
...................................................................................
o
................................
..................................................................................
................................
..................................................................................
..................................
................................................................................
....................................
..............................................................................
o
....................................
..............................................................................
o
...................................
...............................................................................
...................................
..
......................................................................
.......
o
...................................
...
......................................................................
......
o
o
o
o
o
o
...................................
.....
.....................................................................
.....
o
o
o
o
..............................................
..
.........................................................
.........
o
o
o
o
.......... .......................................
...
.....
.
........................................................
o
o
o
o
......
...................................
..........
......
.......
............................................
..
....
o
o
o
o
o
o
o
o
.......... ...
...................................
........
.............................................
.
......
......
o
o
o
o
o
o
o
o
........
......
...................................
...
......
..............................................
.....
.....
o
o
o
o
.............. .....................
....
......... .....
...
.
.............................
....
.......
.
.
........... ....
o
o
o
o
o
o
o
o
......
...
....
.......................
................
.....................
.....
............................
..
..
..
..
o
o
o
o
o
o
o o
....
.......
.......... ...............
...
........
.....................
............................
.......
...
...
..
.
..
o
o
o
o
o
o
o
o
.....................
.
.....
.....................
........................
........
............................. .....
o
o
o
o
o
o
o
o
.......................
......
....................
........................
...................................
......
o
o
o
o
o
o
o
......
.......................
...................
.........................................
......................
...
o
o
o
o
o
o
o
...
..................
............ .......................
.
..
................................
.......................
o
o
o
oo
o
o
o
o
o
o
o
o
..............................
.....................
........
..................
..
...................................
o
o
o
..
.......................
..............
....... ............
...........................
.
......
.............
.....
....
o
o
o
o
o
ooo
....... ..
......
........................
......
.
......
........................
.
......
...........
.......
.....
......
..
o
o
o
o
o
o
o
o
o
.. ..
.....
..
.......................
.
...
....................................
..
............
.......
.........
.....
.
....
o
o
o
o
o
o
o
o
o
o
.......................
.......
.............
............
..............................
.........
.............
....
...
o
o
o
o
o
o
o
o
.............
..........
...................
.........
...........
...........................
....
..........
....
.....
..
oo
o
o
o
o
o
o
...............
...
......
.......
.....
....................
.
...................
............
.....
...
...........
.......
o
o
o
........
.............
.....
.....
..................
..
....................................
............
...
...
.
........
o
o
o
o
o
o
o
........
..............
...
.....
...
...............
.............
....................................
.........
.....
...
o
o
o
....
......
............
.....
...
............
..................................
.....
..............
.........
..........
o
o
o
o
............
........
...
..........
...............................
..............
..........
..................
........
o
o
o
o
o
o
o
o
o
o
o
o
o
.....
..
.
..
....
....
..........
...............
................................
..........
.............
..... ...
..
......
o
o
o
o
.
....... ...
..........
..
..............
.............
... ...................................
............
.
......
.......
o
o
o
o
o
o
o
o
.
..
.....
......
......
.............
.............. .....
.....
.....................................
............
...
.....
o
o
o
o o
o
o
..................
..
..........
......
..
.............................................
...............
.............
...
o
o
o
o
o
o
o
o
....
.....
.....
.....
.......
.......
...
..................
............
...
....
.........................................
o
o
o
o
o
o
o
....
...........
........ .....
.
..
....................
..............
.................................................
o
o
o
o
o
.........
....
.....
.............
....................
..
......
................................................
.......
o
.............
....
.....
...........
.....................
........................................
......
....
..........
o
o
o
o
o
o
.............
..
.....
..........
.....................
............
.........................................
...
.......
o
o
o
o
o
...
.............
...
.......
.......................
..............
.......................................
...
.........
o
o
..........
...........
..................................
...............
............................................
o
o
o
.......
...........
..................
...........................................
...................................
o
o
...
..........
...........................................
.......................
...................................
o
.........
..
.................................................
...........................................
...........
........
.....
..............................................
.............................................
..........
o
o
...........
.............................................
..........................................................
oo
o
.
................................................................... ..............................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
o
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................
..................................................................................................................

o
o
o
o
o
o
o
o

o
o
o
o

o
o

o
o

o

o

o

o

o

o

o

o

o

o

figure 13.3. k-nearest-neighbor classi   ers applied to the simulation data of
figure 13.1. the broken purple curve in the background is the bayes decision
boundary.

13.3 k-nearest-neighbor classi   ers

467

s
r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

3

0    

0

.

5
2

.

0

0
2
0

.

5
1

.

0

0
1
0

.

5
0
0

.

0

.
0

   

   

               
   

                                       
                   
                           

   

                           
       

       

   
       

test error
10-fold cv
training error
bayes error

0

5

10

15

20

25

30

number of neighbors
7-nearest neighbors

o

o

o

o

o

o

o

o
o

. . . . . . . . . . . . . . .

o
o
o

o
o
o
o

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . .
o
. . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . .
. . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . .
. . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
.
. . . . . . . . . . .
.
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
.
. . . . . . . . . . . . . . . . . . . . . . . . . .
o
oo
.
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
.
.
. . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. .
o
o
o
o
. . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
oo
o
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
.
. . . . . . . . . . . .
.
.
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . .
. .
. . . . .
o
o
o
o
. . . . . . . . . . . .
.
.
. .
. . . .
.
. . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . .
. . . .
. .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . . . .
. . . .
. . . . .
.
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
o
o
o
oo
. . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . .
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . .
.
. . . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
o
o
o
. . . . . . . . . . .
. . . . . . . . . . . . . . . . . . .
.
. . . . . . . . . .
. . . .
. . . . . . . . . . . . . . . . . . .
. . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . .
. . . . . . . . .
. . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . .
. . . . . . . .
. . . . . . . .
. . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . .
. . . . . . . . .
. . . . . . . .
. . . . . . . . . . . .
o
o
o
o
. . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . .
. . . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . . . .
. . . . . . .
. . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . .
. . . .
. . . . . . .
. . . . . . . . .
o
oo
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . .
. . .
. . . . . . .
. . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . .
.
. . . . . .
. . . . . . . .
. . . . .
. . . . . . .
o
o
. . . . .
. . . . . . .
o
o
. . . . .
. . . . . .
o
. . . .
. . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. . . .
o
o
o
. . .
. . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . .
. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
.
.. .. .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
o
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
training error: 0.145
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
test error:       0.225
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
bayes error:    0.210

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

o
o
o
o

o
o
oo

o
o

o
o
o

o
o

o
o

o
o

o

o

o

o

o

o

o

figure 13.4. k-nearest-neighbors on the two-class mixture data. the upper
panel shows the misclassi   cation errors as a function of neighborhood size. stan-
dard error bars are included for 10-fold cross validation. the lower panel shows
the decision boundary for 7-nearest-neighbors, which appears to be optimal for
minimizing test error. the broken purple curve in the background is the bayes
decision boundary.

468

13. prototypes and nearest-neighbors

ties pk(x), k = 1, . . . , k. for k = 2 the 1-nearest-neighbor error rate is
2pk   (x)(1     pk   (x))     2(1     pk   (x)) (twice the bayes error rate). more
generally, one can show (exercise 13.3)

kxk=1

pk(x)(1     pk(x))     2(1     pk   (x))    

k
k     1

(1     pk   (x))2.

(13.5)

many additional results of this kind have been derived; ripley (1996) sum-
marizes a number of them.

this result can provide a rough idea about the best performance that
is possible in a given problem. for example, if the 1-nearest-neighbor rule
has a 10% error rate, then asymptotically the bayes error rate is at least
5%. the kicker here is the asymptotic part, which assumes the bias of the
nearest-neighbor rule is zero. in real problems the bias can be substantial.
the adaptive nearest-neighbor rules, described later in this chapter, are an
attempt to alleviate this bias. for simple nearest-neighbors, the bias and
variance characteristics can dictate the optimal number of near neighbors
for a given problem. this is illustrated in the next example.

13.3.1 example: a comparative study

we tested the nearest-neighbors, id116 and lvq classi   ers on two sim-
ulated problems. there are 10 independent features xj, each uniformly
distributed on [0, 1]. the two-class 0-1 target variable is de   ned as follows:

problem 1:    easy   ,

1

2(cid:19) ;
y = i(cid:18)x1 >
y = i      sign         
3yj=1(cid:18)xj    

1

2(cid:19)         

> 0       ;

problem 2:    di   cult.   

(13.6)

hence in the    rst problem the two classes are separated by the hyperplane
x1 = 1/2; in the second problem, the two classes form a checkerboard
pattern in the hypercube de   ned by the    rst three features. the bayes
error rate is zero in both problems. there were 100 training and 1000 test
observations.

figure 13.5 shows the mean and standard error of the misclassi   cation
error for nearest-neighbors, id116 and lvq over ten realizations, as
the tuning parameters are varied. we see that id116 and lvq give
nearly identical results. for the best choices of their tuning parameters,
id116 and lvq outperform nearest-neighbors for the    rst problem, and
they perform similarly for the second problem. notice that the best value
of each tuning parameter is clearly situation dependent. for example 25-
nearest-neighbors outperforms 1-nearest-neighbor by a factor of 70% in the

13.3 k-nearest-neighbor classi   ers

469

nearest neighbors / easy

id116 & lvq / easy

5

.

0

4
0

.

3
0

.

2
.
0

1
.
0

0
6
.
0

5
5
.
0

0
5
.
0

5
4
.
0

0
4
.
0

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

0

20

40

60

number of neighbors

nearest neighbors / difficult

r
o
r
r

e
 
n
o
i
t
a
c
i
f
i
s
s
a
c
s
m

i

l

5

.

0

4
0

.

3
0

.

2
.
0

1
.
0

0
6
.
0

5
5
.
0

0
5
.
0

5
4
.
0

0
4
.
0

0

5

10

15

20

25

30

number of prototypes per class

id116 & lvq / difficult

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

r
o
r
r

e
 
n
o
i
t
a
c
i
f
i
s
s
a
c
s
m

i

l

0

20

40

60

0

5

10

15

20

25

30

number of neighbors

number of prototypes per class

figure 13.5. mean    one standard error of misclassi   cation error for near-
est-neighbors, id116 (blue) and lvq (red) over ten realizations for two sim-
ulated problems:    easy    and    di   cult,    described in the text.

470

13. prototypes and nearest-neighbors

spectral band 1

spectral band 2

spectral band 3

spectral band 4

land usage

predicted land usage

figure 13.6. the    rst four panels are landsat images for an agricultural
area in four spectral bands, depicted by heatmap shading. the remaining two
panels give the actual land usage (color coded) and the predicted land usage using
a    ve-nearest-neighbor rule described in the text.

   rst problem, while 1-nearest-neighbor is best in the second problem by a
factor of 18%. these results underline the importance of using an objective,
data-based method like cross-validation to estimate the best value of a
tuning parameter (see figure 13.4 and chapter 7).

13.3.2 example: k-nearest-neighbors and image scene

classi   cation

the statlog project (michie et al., 1994) used part of a landsat
image as a benchmark for classi   cation (82   100 pixels). figure 13.6 shows
four heat-map images, two in the visible spectrum and two in the infrared,
for an area of agricultural land in australia. each pixel has a class label
from the 7-element set g = {red soil, cotton, vegetation stubble, mixture,
gray soil, damp gray soil, very damp gray soil}, determined manually by
research assistants surveying the area. the lower middle panel shows the
actual land usage, shaded by di   erent colors to indicate the classes. the
objective is to classify the land usage at a pixel, based on the information
in the four spectral bands.

five-nearest-neighbors produced the predicted map shown in the bot-
tom right panel, and was computed as follows. for each pixel we extracted
an 8-neighbor feature map   the pixel itself and its 8 immediate neighbors

13.3 k-nearest-neighbor classi   ers

471

n(cid:10)

n(cid:10)

n(cid:10)

n(cid:10)

x(cid:10)

n(cid:10)

n(cid:10)

n(cid:10)

n(cid:10)

figure 13.7. a pixel and its 8-neighbor feature map.

(see figure 13.7). this is done separately in the four spectral bands, giving
(1 + 8)   4 = 36 input features per pixel. then    ve-nearest-neighbors classi-
   cation was carried out in this 36-dimensional feature space. the resulting
test error rate was about 9.5% (see figure 13.8). of all the methods used
in the statlog project, including lvq, cart, neural networks, linear
discriminant analysis and many others, k-nearest-neighbors performed best
on this task. hence it is likely that the decision boundaries in ir36 are quite
irregular.

13.3.3 invariant metrics and tangent distance

in some problems, the training features are invariant under certain natural
transformations. the nearest-neighbor classi   er can exploit such invari-
ances by incorporating them into the metric used to measure the distances
between objects. here we give an example where this idea was used with
great success, and the resulting classi   er outperformed all others at the
time of its development (simard et al., 1993).

the problem is handwritten digit recognition, as discussed is chapter 1
and section 11.7. the inputs are grayscale images with 16    16 = 256
pixels; some examples are shown in figure 13.9. at the top of figure 13.10,
a    3    is shown, in its actual orientation (middle) and rotated 7.5    and 15   
in either direction. such rotations can often occur in real handwriting, and
it is obvious to our eye that this    3    is still a    3    after small rotations.
hence we want our nearest-neighbor classi   er to consider these two    3   s
to be close together (similar). however the 256 grayscale pixel values for a
rotated    3    will look quite di   erent from those in the original image, and
hence the two objects can be far apart in euclidean distance in ir256.

we wish to remove the e   ect of rotation in measuring distances between
two digits of the same class. consider the set of pixel values consisting of
the original    3    and its rotated versions. this is a one-dimensional curve in
ir256, depicted by the green curve passing through the    3    in figure 13.10.
figure 13.11 shows a stylized version of ir256, with two images indicated by
xi and xi    . these might be two di   erent    3   s, for example. through each
image we have drawn the curve of rotated versions of that image, called

472

13. prototypes and nearest-neighbors

statlog results

lda

logistic

smart

qda

newid c4.5

cart

alloc80

neural

rbf

5
1

.

0

0
1

.

0

lvq

id92

dann

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

 
t
s
e
t

5
0

.

0

.

0
0

2

4

6

8

10

12

14

method

figure 13.8. test-error performance for a number of classi   ers, as reported
by the statlog project. the entry dann is a variant of k-nearest neighbors,
using an adaptive metric (section 13.4.2).

figure 13.9. examples of grayscale images of handwritten digits.

13.3 k-nearest-neighbor classi   ers

473

   15

   7.5

0

7.5

15

transformations of 3

3

tangent

pixel space

a=- 0.2

a=- 0.1

a=0

a=0.1

a=0.2

linear equation for 
images above

+ a

.

figure 13.10. the top row shows a     3    in its original orientation (middle)
and rotated versions of it. the green curve in the middle of the    gure depicts
this set of rotated     3    in 256-dimensional space. the red line is the tangent line
to the curve at the original image, with some     3   s on this tangent line, and its
equation shown at the bottom of the    gure.

invariance manifolds in this context. now, rather than using the usual
euclidean distance between the two images, we use the shortest distance
between the two curves. in other words, the distance between the two
images is taken to be the shortest euclidean distance between any rotated
version of    rst image, and any rotated version of the second image. this
distance is called an invariant metric.

in principle one could carry out 1-nearest-neighbor classi   cation using
this invariant metric. however there are two problems with it. first, it is
very di   cult to calculate for real images. second, it allows large trans-
formations that can lead to poor performance. for example a    6    would
be considered close to a    9    after a rotation of 180   . we need to restrict
attention to small rotations.

the use of tangent distance solves both of these problems. as shown in
figure 13.10, we can approximate the invariance manifold of the image
   3    by its tangent at the original image. this tangent can be computed
by estimating the direction vector from small rotations of the image, or by
more sophisticated spatial smoothing methods (exercise 13.4.) for large
rotations, the tangent image no longer looks like a    3,    so the problem
with large transformations is alleviated.

474

13. prototypes and nearest-neighbors

transformations
of xi

tangent distance

distance between
transformed
xi and xi   

xi

xi   

euclidean distance
between xi and xi   

transformations
of xi   

figure 13.11. tangent distance computation for two images xi and xi    .
rather than using the euclidean distance between xi and xi    , or the shortest
distance between the two curves, we use the shortest distance between the two
tangent lines.

the idea then is to compute the invariant tangent line for each training
image. for a query image to be classi   ed, we compute its invariant tangent
line, and    nd the closest line to it among the lines in the training set. the
class (digit) corresponding to this closest line is our predicted class for the
query image. in figure 13.11 the two tangent lines intersect, but this is only
because we have been forced to draw a two-dimensional representation of
the actual 256-dimensional situation. in ir256 the id203 of two such
lines intersecting is e   ectively zero.

now a simpler way to achieve this invariance would be to add into the
training set a number of rotated versions of each training image, and then
just use a standard nearest-neighbor classi   er. this idea is called    hints    in
abu-mostafa (1995), and works well when the space of invariances is small.
so far we have presented a simpli   ed version of the problem. in addition to
rotation, there are six other types of transformations under which we would
like our classi   er to be invariant. there are translation (two directions),
scaling (two directions), sheer, and character thickness. hence the curves
and tangent lines in figures 13.10 and 13.11 are actually 7-dimensional
manifolds and hyperplanes. it is infeasible to add transformed versions
of each training image to capture all of these possibilities. the tangent
manifolds provide an elegant way of capturing the invariances.

table 13.1 shows the test misclassi   cation error for a problem with 7291
training images and 2007 test digits (the u.s. postal services database), for
a carefully constructed neural network, and simple 1-nearest-neighbor and

13.4 adaptive nearest-neighbor methods

475

table 13.1. test error rates for the handwritten zip code problem.

method
neural-net
1-nearest-neighbor/euclidean distance
1-nearest-neighbor/tangent distance

error rate
0.049
0.055
0.026

tangent distance 1-nearest-neighbor rules. the tangent distance nearest-
neighbor classi   er works remarkably well, with test error rates near those
for the human eye (this is a notoriously di   cult test set). in practice,
it turned out that nearest-neighbors are too slow for online classi   cation
in this application (see section 13.5), and neural network classi   ers were
subsequently developed to mimic it.

13.4 adaptive nearest-neighbor methods

when nearest-neighbor classi   cation is carried out in a high-dimensional
feature space, the nearest neighbors of a point can be very far away, causing
bias and degrading the performance of the rule.

to quantify this, consider n data points uniformly distributed in the unit
2 ]p. let r be the radius of a 1-nearest-neighborhood centered at

2 , 1

cube [    1
the origin. then

(cid:16)1    

1
2

1/n(cid:17)1/p

median(r) = v   1/p

p

,

(13.7)

where vprp is the volume of the sphere of radius r in p dimensions. fig-
ure 13.12 shows the median radius for various training sample sizes and
dimensions. we see that median radius quickly approaches 0.5, the dis-
tance to the edge of the cube.

what can be done about this problem? consider the two-class situation
in figure 13.13. there are two features, and a nearest-neighborhood at
a query point is depicted by the circular region. implicit in near-neighbor
classi   cation is the assumption that the class probabilities are roughly con-
stant in the neighborhood, and hence simple averages give good estimates.
however, in this example the class probabilities vary only in the horizontal
direction. if we knew this, we would stretch the neighborhood in the verti-
cal direction, as shown by the tall rectangular region. this will reduce the
bias of our estimate and leave the variance the same.

in general, this calls for adapting the metric used in nearest-neighbor
classi   cation, so that the resulting neighborhoods stretch out in directions
for which the class probabilities don   t change much. in high-dimensional
feature space, the class probabilities might change only a low-dimensional
subspace and hence there can be considerable advantage to adapting the
metric.

476

13. prototypes and nearest-neighbors

i

 

s
u
d
a
r
n
a
d
e
m

i

n=1,000

n=100

n=10,000

6

.

0

5
0

.

4

.

0

3

.

0

2
0

.

1

.

0

0

.

0

0

5

10

15

dimension

figure 13.12. median radius of a 1-nearest-neighborhood, for uniform data
with n observations in p dimensions.

5-nearest neighborhoods

o

o

o
o

o
o

o

o
o

   

o
o
o

o

o

o

o

o

o

o

o

o

o

o

o
o

figure 13.13. the points are uniform in the cube, with the vertical line sepa-
rating class red and green. the vertical strip denotes the 5-nearest-neighbor region
using only the horizontal coordinate to    nd the nearest-neighbors for the target
point (solid dot). the sphere shows the 5-nearest-neighbor region using both co-
ordinates, and we see in this case it has extended into the class-red region (and
is dominated by the wrong class in this instance).

13.4 adaptive nearest-neighbor methods

477

friedman (1994a) proposed a method in which rectangular neighbor-
hoods are found adaptively by successively carving away edges of a box
containing the training data. here we describe the discriminant adaptive
nearest-neighbor (dann) rule of hastie and tibshirani (1996a). earlier,
related proposals appear in short and fukunaga (1981) and myles and
hand (1990).

at each query point a neighborhood of say 50 points is formed, and the
class distribution among the points is used to decide how to deform the
neighborhood   that is, to adapt the metric. the adapted metric is then
used in a nearest-neighbor rule at the query point. thus at each query
point a potentially di   erent metric is used.

in figure 13.13 it is clear that the neighborhood should be stretched in
the direction orthogonal to line joining the class centroids. this direction
also coincides with the linear discriminant boundary, and is the direction
in which the class probabilities change the least. in general this direction
of maximum change will not be orthogonal to the line joining the class cen-
troids (see figure 4.9 on page 116.) assuming a local discriminant model,
the information contained in the local within- and between-class covari-
ance matrices is all that is needed to determine the optimal shape of the
neighborhood.

the discriminant adaptive nearest-neighbor (dann) metric at a query

point x0 is de   ned by

d(x, x0) = (x     x0)t   (x     x0),

(13.8)

where

   = w   1/2[w   1/2bw   1/2 +   i]w   1/2

= w   1/2[b    +   i]w   1/2.

(13.9)

here w is the pooled within-class covariance matrix pk
is the between class covariance matrix pk

k=1   kwk and b
k=1   k(  xk       x)(  xk       x)t , with
w and b computed using only the 50 nearest neighbors around x0. after
computation of the metric, it is used in a nearest-neighbor rule at x0.

this complicated formula is actually quite simple in its operation. it    rst
spheres the data with respect to w, and then stretches the neighborhood
in the zero-eigenvalue directions of b    (the between-matrix for the sphered
data ). this makes sense, since locally the observed class means do not dif-
fer in these directions. the    parameter rounds the neighborhood, from an
in   nite strip to an ellipsoid, to avoid using points far away from the query
point. the value of    = 1 seems to work well in general. figure 13.14 shows
the resulting neighborhoods for a problem where the classes form two con-
centric circles. notice how the neighborhoods stretch out orthogonally to
the decision boundaries when both classes are present in the neighborhood.
in the pure regions with only one class, the neighborhoods remain circular;

478

13. prototypes and nearest-neighbors

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o

o
o
o

o

o

o

o

figure 13.14. neighborhoods found by the dann procedure, at various query
points (centers of the crosses). there are two classes in the data, with one class
surrounding the other. 50 nearest-neighbors were used to estimate the local met-
rics. shown are the resulting metrics used to form 15-nearest-neighborhoods.

in these cases the between matrix b = 0, and the    in (13.8) is the identity
matrix.

13.4.1 example

here we generate two-class data in ten dimensions, analogous to the two-
dimensional example of figure 13.14. all ten predictors in class 1 are inde-
pendent standard normal, conditioned on the squared radius being greater
than 22.4 and less than 40, while the predictors in class 2 are independent
standard normal without the restriction. there are 250 observations in each
class. hence the    rst class almost completely surrounds the second class in
the full ten-dimensional space.

in this example there are no pure noise variables, the kind that a nearest-
neighbor subset selection rule might be able to weed out. at any given
point in the feature space, the class discrimination occurs along only one
direction. however, this direction changes as we move across the feature
space and all variables are important somewhere in the space.

figure 13.15 shows boxplots of the test error rates over ten realiza-
tions, for standard 5-nearest-neighbors, lvq, and discriminant adaptive
5-nearest-neighbors. we used 50 prototypes per class for lvq, to make
it comparable to 5 nearest-neighbors (since 250/5 = 50). the adaptive
metric signi   cantly reduces the error rate, compared to lvq or standard
nearest-neighbors.

13.4 adaptive nearest-neighbor methods

479

r
o
r
r

e

 
t
s
e
t

4
0

.

3
0

.

2
0

.

1

.

0

0
0

.

5n n

lv q

d a n n

figure 13.15. ten-dimensional simulated example: boxplots of the test error
rates over ten realizations, for standard 5-nearest-neighbors, lvq with 50 centers,
and discriminant-adaptive 5-nearest-neighbors

13.4.2 global dimension reduction for nearest-neighbors

the discriminant-adaptive nearest-neighbor method carries out local di-
mension reduction   that is, dimension reduction separately at each query
point. in many problems we can also bene   t from global dimension re-
duction, that is, apply a nearest-neighbor rule in some optimally chosen
subspace of the original feature space. for example, suppose that the two
classes form two nested spheres in four dimensions of feature space, and
there are an additional six noise features whose distribution is independent
of class. then we would like to discover the important four-dimensional
subspace, and carry out nearest-neighbor classi   cation in that reduced sub-
space. hastie and tibshirani (1996a) discuss a variation of the discriminant-
adaptive nearest-neighbor method for this purpose. at each training point
xi, the between-centroids sum of squares matrix bi is computed, and then
these matrices are averaged over all training points:

  b =

1
n

nxi=1

bi.

(13.10)

let e1, e2, . . . , ep be the eigenvectors of the matrix   b, ordered from largest
to smallest eigenvalue   k. then these eigenvectors span the optimal sub-
spaces for global subspace reduction. the derivation is based on the fact
    , solves the

least squares problem

that the best rank-l approximation to   b,   b[l] =pl
trace[(bi     m)2].

   =1      e   et

(13.11)

rank(m)=l

min

nxi=1

since each bi contains information on (a) the local discriminant subspace,
and (b) the strength of discrimination in that subspace, (13.11) can be seen

480

13. prototypes and nearest-neighbors

as a way of    nding the best approximating subspace of dimension l to a
series of n subspaces by weighted least squares (exercise 13.5.)

in the four-dimensional sphere example mentioned above and examined
in hastie and tibshirani (1996a), four of the eigenvalues       turn out to be
large (having eigenvectors nearly spanning the interesting subspace), and
the remaining six are near zero. operationally, we project the data into
the leading four-dimensional subspace, and then carry out nearest neighbor
classi   cation. in the satellite image classi   cation example in section 13.3.2,
the technique labeled dann in figure 13.8 used 5-nearest-neighbors in a
globally reduced subspace. there are also connections of this technique
with the sliced inverse regression proposal of duan and li (1991). these
authors use similar ideas in the regression setting, but do global rather
than local computations. they assume and exploit spherical symmetry of
the feature distribution to estimate interesting subspaces.

13.5 computational considerations

one drawback of nearest-neighbor rules in general is the computational
load, both in    nding the neighbors and storing the entire training set. with
n observations and p predictors, nearest-neighbor classi   cation requires n p
operations to    nd the neighbors per query point. there are fast algorithms
for    nding nearest-neighbors (friedman et al., 1975; friedman et al., 1977)
which can reduce this load somewhat. hastie and simard (1998) reduce
the computations for tangent distance by developing analogs of id116
id91 in the context of this invariant metric.

reducing the storage requirements is more di   cult, and various editing
and condensing procedures have been proposed. the idea is to isolate a
subset of the training set that su   ces for nearest-neighbor predictions, and
throw away the remaining training data. intuitively, it seems important to
keep the training points that are near the decision boundaries and on the
correct side of those boundaries, while some points far from the boundaries
could be discarded.

the multi-edit algorithm of devijver and kittler (1982) divides the data
cyclically into training and test sets, computing a nearest neighbor rule on
the training set and deleting test points that are misclassi   ed. the idea is
to keep homogeneous clusters of training observations.

the condensing procedure of hart (1968) goes further, trying to keep
only important exterior points of these clusters. starting with a single ran-
domly chosen observation as the training set, each additional data item is
processed one at a time, adding it to the training set only if it is misclas-
si   ed by a nearest-neighbor rule computed on the current training set.

these procedures are surveyed in dasarathy (1991) and ripley (1996).
they can also be applied to other learning procedures besides nearest-

neighbors. while such methods are sometimes useful, we have not had
much practical experience with them, nor have we found any systematic
comparison of their performance in the literature.

exercises

481

bibliographic notes

the nearest-neighbor method goes back at least to fix and hodges (1951).
the extensive literature on the topic is reviewed by dasarathy (1991);
chapter 6 of ripley (1996) contains a good summary. id116 cluster-
ing is due to lloyd (1957) and macqueen (1967). kohonen (1989) intro-
duced learning vector quantization. the tangent distance method is due to
simard et al. (1993). hastie and tibshirani (1996a) proposed the discrim-
inant adaptive nearest-neighbor technique.

exercises

ex. 13.1 consider a gaussian mixture model where the covariance matrices
are assumed to be scalar:   r =   i    r = 1, . . . , r, and    is a    xed param-
eter. discuss the analogy between the id116 id91 algorithm and
the em algorithm for    tting this mixture model in detail. show that in the
limit        0 the two methods coincide.
ex. 13.2 derive formula (13.7) for the median radius of the 1-nearest-
neighborhood.

ex. 13.3 let e    be the error rate of the bayes rule in a k-class problem,
where the true class probabilities are given by pk(x), k = 1, . . . , k. as-
suming the test point and training point have identical features x, prove
(13.5)

kxk=1

pk(x)(1     pk(x))     2(1     pk   (x))    

k
k     1

(1     pk   (x))2.

where k    = arg maxk pk(x). hence argue that the error rate of the 1-
nearest-neighbor rule converges in l1, as the size of the training set in-
creases, to a value e1, bounded above by

e   (cid:16)2     e    k

k     1(cid:17).

(13.12)

[this statement of the theorem of cover and hart (1967) is taken from
chapter 6 of ripley (1996), where a short proof is also given].

482

13. prototypes and nearest-neighbors

ex. 13.4 consider an image to be a function f (x) : ir2 7    ir1 over the two-
dimensional spatial domain (paper coordinates). then f (c+x0+a(x   x0))
represents an a   ne transformation of the image f , where a is a 2    2
matrix.

1. decompose a (via q-r) in such a way that parameters identifying
the four a   ne transformations (two scale, shear and rotation) are
clearly identi   ed.

2. using the chain rule, show that the derivative of f (c+x0 +a(x   x0))
w.r.t. each of these parameters can be represented in terms of the two
spatial derivatives of f .

3. using a two-dimensional kernel smoother (chapter 6), describe how
to implement this procedure when the images are quantized to 16  16
pixels.

ex. 13.5 let bi, i = 1, 2, . . . , n be square p    p positive semi-de   nite ma-
    with                   1                  1. show that the best rank-l approx-

trices and let   b = (1/n )p bi. write the eigen-decomposition of   b as
pp

imation for the bi,

   =1      e   et

min

rank(m)=l

   =1      e   et

nxi=1
trace[(bi     m)2],
    . (hint: writepn

trace[(bi       b)2] +

trace[(m       b)2]).

nxi=1

is given by   b[l] =pl
nxi=1

i=1 trace[(bi     m)2] as

ex. 13.6 here we consider the problem of shape averaging. in particular,
li, i = 1, . . . , m are each n    2 matrices of points in ir2, each sampled
from corresponding positions of handwritten (cursive) letters. we seek an
a   ne invariant average v, also n    2, vt v = i, of the m letters li with
the following property: v minimizes

mxj=1

aj klj     vajk2 .
min

characterize the solution.

this solution can su   er if some of the letters are big and dominate the

average. an alternative approach is to minimize instead:

mxj=1

min

aj (cid:13)(cid:13)lj a   

j     v(cid:13)(cid:13)2

.

derive the solution to this problem. how do the criteria di   er? use the
svd of the lj to simplify the comparison of the two approaches.

exercises

483

ex. 13.7 consider the application of nearest-neighbors to the    easy    and
   hard    problems in the left panel of figure 13.5.

1. replicate the results in the left panel of figure 13.5.

2. estimate the misclassi   cation errors using    vefold cross-validation,

and compare the error rate curves to those in 1.

3. consider an    aic-like    penalization of the training set misclassi   ca-
tion error. speci   cally, add 2t/n to the training set misclassi   cation
error, where t is the approximate number of parameters n/r, r be-
ing the number of nearest-neighbors. compare plots of the resulting
penalized misclassi   cation error to those in 1 and 2. which method
gives a better estimate of the optimal number of nearest-neighbors:
cross-validation or aic?

ex. 13.8 generate data in two classes, with two features. these features
are all independent gaussian variates with standard deviation 1. their
mean vectors are (   1,   1) in class 1 and (1, 1) in class 2. to each feature
vector apply a random rotation of angle   ,    chosen uniformly from 0 to
2  . generate 50 observations from each class to form the training set, and
500 in each class as the test set. apply four di   erent classi   ers:

1. nearest-neighbors.

2. nearest-neighbors with hints: ten randomly rotated versions of each
data point are added to the training set before applying nearest-
neighbors.

3. invariant metric nearest-neighbors, using euclidean distance invari-

ant to rotations about the origin.

4. tangent distance nearest-neighbors.

in each case choose the number of neighbors by tenfold cross-validation.
compare the results.

484

13. prototypes and nearest-neighbors

14
unsupervised learning

this is page 485
printer: opaque this

14.1

introduction

the previous chapters have been concerned with predicting the values
of one or more outputs or response variables y = (y1, . . . , ym) for a
given set of input or predictor variables x t = (x1, . . . , xp). denote by
xt
i = (xi1, . . . , xip) the inputs for the ith training case, and let yi be a
response measurement. the predictions are based on the training sample
(x1, y1), . . . , (xn , yn ) of previously solved cases, where the joint values of
all of the variables are known. this is called supervised learning or    learn-
ing with a teacher.    under this metaphor the    student    presents an an-
swer   yi for each xi in the training sample, and the supervisor or    teacher   
provides either the correct answer and/or an error associated with the stu-
dent   s answer. this is usually characterized by some id168 l(y,   y),
for example, l(y,   y) = (y       y)2.
if one supposes that (x, y ) are random variables represented by some
joint id203 density pr(x, y ), then supervised learning can be formally
characterized as a density estimation problem where one is concerned with
determining properties of the conditional density pr(y |x). usually the
properties of interest are the    location    parameters    that minimize the
expected error at each x,

  (x) = argmin

  

ey |x l(y,   ).

(14.1)

486

14. unsupervised learning

conditioning one has

pr(x, y ) = pr(y |x)    pr(x),

where pr(x) is the joint marginal density of the x values alone. in su-
pervised learning pr(x) is typically of no direct concern. one is interested
mainly in the properties of the conditional density pr(y |x). since y is of-
ten of low dimension (usually one), and only its location   (x) is of interest,
the problem is greatly simpli   ed. as discussed in the previous chapters,
there are many approaches for successfully addressing supervised learning
in a variety of contexts.

in this chapter we address unsupervised learning or    learning without a
teacher.    in this case one has a set of n observations (x1, x2, . . . , xn ) of a
random p-vector x having joint density pr(x). the goal is to directly infer
the properties of this id203 density without the help of a supervisor or
teacher providing correct answers or degree-of-error for each observation.
the dimension of x is sometimes much higher than in supervised learn-
ing, and the properties of interest are often more complicated than simple
location estimates. these factors are somewhat mitigated by the fact that
x represents all of the variables under consideration; one is not required
to infer how the properties of pr(x) change, conditioned on the changing
values of another set of variables.

in low-dimensional problems (say p     3), there are a variety of e   ective
nonparametric methods for directly estimating the density pr(x) itself at
all x-values, and representing it graphically (silverman, 1986, e.g.). owing
to the curse of dimensionality, these methods fail in high dimensions. one
must settle for estimating rather crude global models, such as gaussian
mixtures or various simple descriptive statistics that characterize pr(x).

generally, these descriptive statistics attempt to characterize x-values,
or collections of such values, where pr(x) is relatively large. principal
components, multidimensional scaling, self-organizing maps, and principal
curves, for example, attempt to identify low-dimensional manifolds within
the x-space that represent high data density. this provides information
about the associations among the variables and whether or not they can be
considered as functions of a smaller set of    latent    variables. cluster anal-
ysis attempts to    nd multiple convex regions of the x-space that contain
modes of pr(x). this can tell whether or not pr(x) can be represented by
a mixture of simpler densities representing distinct types or classes of ob-
servations. mixture modeling has a similar goal. association rules attempt
to construct simple descriptions (conjunctive rules) that describe regions
of high density in the special case of very high dimensional binary-valued
data.

with supervised learning there is a clear measure of success, or lack
thereof, that can be used to judge adequacy in particular situations and
to compare the e   ectiveness of di   erent methods over various situations.

14.2 association rules

487

lack of success is directly measured by expected loss over the joint dis-
tribution pr(x, y ). this can be estimated in a variety of ways including
cross-validation. in the context of unsupervised learning, there is no such
direct measure of success. it is di   cult to ascertain the validity of id136s
drawn from the output of most unsupervised learning algorithms. one must
resort to heuristic arguments not only for motivating the algorithms, as is
often the case in supervised learning as well, but also for judgments as to
the quality of the results. this uncomfortable situation has led to heavy
proliferation of proposed methods, since e   ectiveness is a matter of opinion
and cannot be veri   ed directly.

in this chapter we present those unsupervised learning techniques that
are among the most commonly used in practice, and additionally, a few
others that are favored by the authors.

14.2 association rules

association rule analysis has emerged as a popular tool for mining com-
mercial data bases. the goal is to    nd joint values of the variables x =
(x1, x2, . . . , xp) that appear most frequently in the data base. it is most
often applied to binary-valued data xj     {0, 1}, where it is referred to as
   market basket    analysis. in this context the observations are sales trans-
actions, such as those occurring at the checkout counter of a store. the
variables represent all of the items sold in the store. for observation i, each
variable xj is assigned one of two values; xij = 1 if the jth item is pur-
chased as part of the transaction, whereas xij = 0 if it was not purchased.
those variables that frequently have joint values of one represent items that
are frequently purchased together. this information can be quite useful for
stocking shelves, cross-marketing in sales promotions, catalog design, and
consumer segmentation based on buying patterns.

more generally, the basic goal of association rule analysis is to    nd a
collection of prototype x-values v1, . . . , vl for the feature vector x, such
that the id203 density pr(vl) evaluated at each of those values is rela-
tively large. in this general framework, the problem can be viewed as    mode
   nding    or    bump hunting.    as formulated, this problem is impossibly dif-
   cult. a natural estimator for each pr(vl) is the fraction of observations
for which x = vl. for problems that involve more than a small number
of variables, each of which can assume more than a small number of val-
ues, the number of observations for which x = vl will nearly always be too
small for reliable estimation. in order to have a tractable problem, both the
goals of the analysis and the generality of the data to which it is applied
must be greatly simpli   ed.

the    rst simpli   cation modi   es the goal. instead of seeking values x
where pr(x) is large, one seeks regions of the x-space with high id203

488

14. unsupervised learning

content relative to their size or support. let sj represent the set of all
possible values of the jth variable (its support), and let sj     sj be a subset
of these values. the modi   ed goal can be stated as attempting to    nd
subsets of variable values s1, . . . , sp such that the id203 of each of the
variables simultaneously assuming a value within its respective subset,

pr      

p\j=1

(xj     sj)       ,

(14.2)

is relatively large. the intersection of subsets    p
j=1(xj     sj) is called a
conjunctive rule. for quantitative variables the subsets sj are contiguous
intervals; for categorical variables the subsets are delineated explicitly. note
that if the subset sj is in fact the entire set of values sj = sj, as is often
the case, the variable xj is said not to appear in the rule (14.2).

14.2.1 market basket analysis

general approaches to solving (14.2) are discussed in section 14.2.5. these
can be quite useful in many applications. however, they are not feasible
for the very large (p     104, n     108) commercial data bases to which
market basket analysis is often applied. several further simpli   cations of
(14.2) are required. first, only two types of subsets are considered; either
sj consists of a single value of xj, sj = v0j, or it consists of the entire set
of values that xj can assume, sj = sj. this simpli   es the problem (14.2)
to    nding subsets of the integers j     {1, . . . , p}, and corresponding values
v0j, j     j , such that

(14.3)

pr      \j   j

(xj = v0j)      

is large. figure 14.1 illustrates this assumption.

one can apply the technique of dummy variables to turn (14.3) into
a problem involving only binary-valued variables. here we assume that
the support sj is    nite for each variable xj. speci   cally, a new set of
variables z1, . . . , zk is created, one such variable for each of the values
vlj attainable by each of the original variables x1, . . . , xp. the number of
dummy variables k is

k =

pxj=1

|sj|,

where |sj| is the number of distinct values attainable by xj. each dummy
variable is assigned the value zk = 1 if the variable with which it is as-
sociated takes on the corresponding value to which zk is assigned, and
zk = 0 otherwise. this transforms (14.3) to    nding a subset of the integers
k     {1, . . . , k} such that

14.2 association rules

489

2
x

2
x

2
x

x1

x1

x1

figure 14.1. simpli   cations for association rules. here there are two inputs
x1 and x2, taking four and six distinct values, respectively. the red squares
indicate areas of high density. to simplify the computations, we assume that the
derived subset corresponds to either a single value of an input or all values. with
this assumption we could    nd either the middle or right pattern, but not the left
one.

pr"\k   k

(zk = 1)# = pr"yk   k

zk = 1#

(14.4)

is large. this is the standard formulation of the market basket problem.
the set k is called an    item set.    the number of variables zk in the item
set is called its    size    (note that the size is no bigger than p). the estimated
value of (14.4) is taken to be the fraction of observations in the data base
for which the conjunction in (14.4) is true:

cpr"yk   k

(zk = 1)# =

1
n

nxi=1 yk   k

zik.

(14.5)

here zik is the value of zk for this ith case. this is called the    support    or

   prevalence    t (k) of the item set k. an observation i for whichqk   k zik =

1 is said to    contain    the item set k.
in association rule mining a lower support bound t is speci   ed, and one
seeks all item sets kl that can be formed from the variables z1, . . . , zk
with support in the data base greater than this lower bound t

{kl| t (kl) > t}.

(14.6)

14.2.2 the apriori algorithm

the solution to this problem (14.6) can be obtained with feasible compu-
tation for very large data bases provided the threshold t is adjusted so that
(14.6) consists of only a small fraction of all 2k possible item sets. the
   apriori    algorithm (agrawal et al., 1995) exploits several aspects of the

490

14. unsupervised learning

curse of dimensionality to solve (14.6) with a small number of passes over
the data. speci   cally, for a given support threshold t:

    the cardinality |{k| t (k) > t}| is relatively small.
    any item set l consisting of a subset of the items in k must have
support greater than or equal to that of k, l     k     t (l)     t (k).
the    rst pass over the data computes the support of all single-item sets.
those whose support is less than the threshold are discarded. the second
pass computes the support of all item sets of size two that can be formed
from pairs of the single items surviving the    rst pass. in other words, to
generate all frequent itemsets with |k| = m, we need to consider only
candidates such that all of their m ancestral item sets of size m     1 are
frequent. those size-two item sets with support less than the threshold are
discarded. each successive pass over the data considers only those item
sets that can be formed by combining those that survived the previous
pass with those retained from the    rst pass. passes over the data continue
until all candidate rules from the previous pass have support less than the
speci   ed threshold. the apriori algorithm requires only one pass over the
data for each value of |k|, which is crucial since we assume the data cannot
be    tted into a computer   s main memory. if the data are su   ciently sparse
(or if the threshold t is high enough), then the process will terminate in
reasonable time even for huge data sets.

there are many additional tricks that can be used as part of this strat-
egy to increase speed and convergence (agrawal et al., 1995). the apriori
algorithm represents one of the major advances in data mining technology.
each high support item set k (14.6) returned by the apriori algorithm is
cast into a set of    association rules.    the items zk, k     k, are partitioned
into two disjoint subsets, a     b = k, and written

a     b.

(14.7)

the    rst item subset a is called the    antecedent    and the second b the
   consequent.    association rules are de   ned to have several properties based
on the prevalence of the antecedent and consequent item sets in the data
base. the    support    of the rule t (a     b) is the fraction of observations
in the union of the antecedent and consequent, which is just the support
of the item set k from which they were derived. it can be viewed as an
estimate (14.5) of the id203 of simultaneously observing both item
sets pr(a and b) in a randomly selected market basket. the    con   dence   
or    predictability    c(a     b) of the rule is its support divided by the
support of the antecedent

c(a     b) =

t (a     b)

t (a)

,

(14.8)

which can be viewed as an estimate of pr(b | a). the notation pr(a), the
id203 of an item set a occurring in a basket, is an abbreviation for

14.2 association rules

491

pr(qk   a zk = 1). the    expected con   dence    is de   ned as the support of

the consequent t (b), which is an estimate of the unid155
pr(b). finally, the    lift    of the rule is de   ned as the con   dence divided by
the expected con   dence

l(a     b) =

c(a     b)

t (b)

.

this is an estimate of the association measure pr(a and b)/pr(a)pr(b).
as an example, suppose the item set k = {peanut butter, jelly, bread}
and consider the rule {peanut butter, jelly}     {bread}. a support value
of 0.03 for this rule means that peanut butter, jelly, and bread appeared
together in 3% of the market baskets. a con   dence of 0.82 for this rule im-
plies that when peanut butter and jelly were purchased, 82% of the time
bread was also purchased. if bread appeared in 43% of all market baskets
then the rule {peanut butter, jelly}     {bread} would have a lift of 1.95.
the goal of this analysis is to produce association rules (14.7) with both
high values of support and con   dence (14.8). the apriori algorithm returns
all item sets with high support as de   ned by the support threshold t (14.6).
a con   dence threshold c is set, and all rules that can be formed from those
item sets (14.6) with con   dence greater than this value

{a     b | c(a     b) > c}

(14.9)
are reported. for each item set k of size |k| there are 2|k|   1     1 rules of
the form a     (k     a), a     k. agrawal et al. (1995) present a variant of
the apriori algorithm that can rapidly determine which rules survive the
con   dence threshold (14.9) from all possible rules that can be formed from
the solution item sets (14.6).

the output of the entire analysis is a collection of association rules (14.7)

that satisfy the constraints

t (a     b) > t

and c(a     b) > c.

these are generally stored in a data base that can be queried by the user.
typical requests might be to display the rules in sorted order of con   dence,
lift or support. more speci   cally, one might request such a list conditioned
on particular items in the antecedent or especially the consequent. for
example, a request might be the following:

display all transactions in which ice skates are the consequent
that have con   dence over 80% and support of more than 2%.

this could provide information on those items (antecedent) that predicate
sales of ice skates. focusing on a particular consequent casts the problem
into the framework of supervised learning.

association rules have become a popular tool for analyzing very large
commercial data bases in settings where market basket is relevant. that is

492

14. unsupervised learning

when the data can be cast in the form of a multidimensional contingency
table. the output is in the form of conjunctive rules (14.4) that are easily
understood and interpreted. the apriori algorithm allows this analysis to
be applied to huge data bases, much larger that are amenable to other types
of analyses. association rules are among data mining   s biggest successes.

besides the restrictive form of the data to which they can be applied, as-
sociation rules have other limitations. critical to computational feasibility
is the support threshold (14.6). the number of solution item sets, their size,
and the number of passes required over the data can grow exponentially
with decreasing size of this lower bound. thus, rules with high con   dence
or lift, but low support, will not be discovered. for example, a high con   -
dence rule such as vodka     caviar will not be uncovered owing to the low
sales volume of the consequent caviar.

14.2.3 example: market basket analysis

we illustrate the use of apriori on a moderately sized demographics data
base. this data set consists of n = 9409 questionnaires    lled out by shop-
ping mall customers in the san francisco bay area (impact resources, inc.,
columbus oh, 1987). here we use answers to the    rst 14 questions, relat-
ing to demographics, for illustration. these questions are listed in table
14.1. the data are seen to consist of a mixture of ordinal and (unordered)
categorical variables, many of the latter having more than a few values.
there are many missing values.

we used a freeware implementation of the apriori algorithm due to chris-
tian borgelt1. after removing observations with missing values, each ordinal
predictor was cut at its median and coded by two dummy variables; each
categorical predictor with k categories was coded by k dummy variables.
this resulted in a 6876    50 matrix of 6876 observations on 50 dummy
variables.
the algorithm found a total of 6288 association rules, involving     5
predictors, with support of at least 10%. understanding this large set of
rules is itself a challenging data analysis task. we will not attempt this here,
but only illustrate in figure 14.2 the relative frequency of each dummy
variable in the data (top) and the association rules (bottom). prevalent
categories tend to appear more often in the rules, for example, the    rst
category in language (english). however, others such as occupation are
under-represented, with the exception of the    rst and    fth level.

here are three examples of association rules found by the apriori algo-

rithm:

association rule 1: support 25%, con   dence 99.7% and lift 1.03.

1see http://fuzzy.cs.uni-magdeburg.de/   borgelt.

b
y

t
h
e
a
p
r
i
o
r
i

a

l
g
o
r
i
t
h
m

(
b
o
t
t
o
m

)
.

a
b
l
e

(
c
o
d
i
n
g

a
n

i
n
p
u
t

c
a
t
e
g
o
r
y
)

i
n

t
h
e

d
a
t
a

(
t
o
p
)
,

a
n
d

t
h
e

a
s
s
o
c
i
a
t
i
o
n

r
u
l
e
s

f
o
u
n
d

f
i
g
u
r
e

1
4
.
2
.

m
a
r
k
e
t

b
a
s
k
e
t

a
n
a

l
y
s
i
s
:

r
e
l
a
t
i
v
e

f
r
e
q
u
e
n
c
y

o
f

e
a
c
h

d
u
m
m
y

v
a
r
i
-

relative frequency in association rules

relative frequency in data

0.0

0.04

0.08

0.12

0.0

0.02

0.04

0.06

a

t
t
r
i
b
u
e

t

0

1
0

2
0

3
0

4
0

5
0

income

sex

marstat

age

educ

occup

yrs   bay

dualinc

perhous

peryoung

house

typehome

ethnic

language

a

t
t
r
i
b
u
e

t

0

1
0

2
0

3
0

4
0

5
0

income

sex

marstat

age

educ

occup

yrs   bay

dualinc

perhous

peryoung

house

typehome

ethnic

language

1
4
.
2
a
s
s
o
c
i
a
t
i
o
n
r
u
l
e
s

4
9
3

494

14. unsupervised learning

table 14.1. inputs for the demographic data.

feature demographic

# values type

1
2
3
4
5
6
7
8
9
10
11
12
13
14

sex
marital status
age
education
occupation
income
years in bay area
dual incomes
number in household
number of children
householder status
type of home
ethnic classi   cation
language in home

2
5
7
6
9
9
5
3
9
9
3
5
8
3

categorical
categorical
ordinal
ordinal
categorical
ordinal
ordinal
categorical
ordinal
ordinal
categorical
categorical
categorical
categorical

(cid:20) number in household = 1
number of children = 0 (cid:21)

language in home = english

   

   

   

association rule 2: support 13.4%, con   dence 80.8%, and lift 2.13.

      

language in home = english
householder status = own

occupation = {professional/managerial}

      

income     $40,000

association rule 3: support 26.5%, con   dence 82.8% and lift 2.15.

            

language in home = english
income < $40,000

marital status = not married

number of children = 0

            

education /    {college graduate, graduate study}

14.2 association rules

495

we chose the    rst and third rules based on their high support. the second
rule is an association rule with a high-income consequent, and could be
used to try to target high-income individuals.

as stated above, we created dummy variables for each category of the
input predictors, for example, z1 = i(income < $40, 000) and z2 =
i(income     $40, 000) for below and above the median income. if we were
interested only in    nding associations with the high-income category, we
would include z2 but not z1. this is often the case in actual market basket
problems, where we are interested in    nding associations with the presence
of a relatively rare item, but not associations with its absence.

14.2.4 unsupervised as supervised learning

here we discuss a technique for transforming the density estimation prob-
lem into one of supervised function approximation. this forms the basis
for the generalized association rules described in the next section.

let g(x) be the unknown data id203 density to be estimated, and
g0(x) be a speci   ed id203 density function used for reference. for ex-
ample, g0(x) might be the uniform density over the range of the variables.
other possibilities are discussed below. the data set x1, x2, . . . , xn is pre-
sumed to be an i.i.d. random sample drawn from g(x). a sample of size n0
can be drawn from g0(x) using monte carlo methods. pooling these two
data sets, and assigning mass w = n0/(n + n0) to those drawn from g(x),
and w0 = n/(n + n0) to those drawn from g0(x), results in a random
sample drawn from the mixture density (g(x) + g0(x)) /2. if one assigns
the value y = 1 to each sample point drawn from g(x) and y = 0 those
drawn from g0(x), then

  (x) = e(y | x) =

=

g(x)

g(x) + g0(x)
g(x)/g0(x)

1 + g(x)/g0(x)

(14.10)

can be estimated by supervised learning using the combined sample

(y1, x1), (y2, x2), . . . , (yn +n0 , xn +n0 )

(14.11)

as training data. the resulting estimate     (x) can be inverted to provide an
estimate for g(x)

  g(x) = g0(x)

.

(14.12)

    (x)
1         (x)

generalized versions of id28 (section 4.4) are especially well

suited for this application since the log-odds,

f (x) = log

g(x)
g0(x)

,

(14.13)

are estimated directly. in this case one has

496

14. unsupervised learning

   

   
   

   
   
   

6

4

2
x

2

0

2
-

   

   
   
   

   
   

   

   

6

4

   

   

2
x

2

0

2
-

2

   
   
   
   
   
   
   
   
   
   
   
   

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
         
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
0

   

   

-1

1

   
   
   

   

   

   

   

   

   
   

   
   

      

   
   
   

   
   
   

   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
         
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
0

   
   
   

   
   

   
   

   

-1

1

   
   
   
   
   
   
   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   

   
   
   

   

   
   
   
   
   

   
   
   
   
   

   
   

   
   
   
   

2

x1

x1

figure 14.3. density estimation via classi   cation. (left panel:) training set
of 200 data points. (right panel:) training set plus 200 reference data points,
generated uniformly over the rectangle containing the training data. the training
sample was labeled as class 1, and the reference sample class 0, and a semipara-
metric id28 model was    t to the data. some contours for   g(x) are
shown.

  g(x) = g0(x) e

  f (x).

(14.14)

an example is shown in figure 14.3. we generated a training set of size
200 shown in the left panel. the right panel shows the reference data (blue)
generated uniformly over the rectangle containing the training data. the
training sample was labeled as class 1, and the reference sample class 0,
and a id28 model, using a tensor product of natural splines
(section 5.2.1), was    t to the data. some id203 contours of     (x) are
shown in the right panel; these are also the contours of the density estimate
  g(x), since   g(x) =     (x)/(1         (x)), is a monotone function. the contours
roughly capture the data density.
in principle any reference density can be used for g0(x) in (14.14). in
practice the accuracy of the estimate   g(x) can depend greatly on partic-
ular choices. good choices will depend on the data density g(x) and the
procedure used to estimate (14.10) or (14.13). if accuracy is the goal, g0(x)
should be chosen so that the resulting functions   (x) or f (x) are approx-
imated easily by the method being used. however, accuracy is not always
the primary goal. both   (x) and f (x) are monotonic functions of the den-
sity ratio g(x)/g0(x). they can thus be viewed as    contrast    statistics that
provide information concerning departures of the data density g(x) from
the chosen reference density g0(x). therefore, in data analytic settings, a
choice for g0(x) is dictated by types of departures that are deemed most
interesting in the context of the speci   c problem at hand. for example, if
departures from uniformity are of interest, g0(x) might be the a uniform
density over the range of the variables. if departures from joint normality

14.2 association rules

497

are of interest, a good choice for g0(x) would be a gaussian distribution
with the same mean vector and covariance matrix as the data. departures
from independence could be investigated by using

g0(x) =

gj(xj),

pyj=1

(14.15)

where gj(xj) is the marginal data density of xj, the jth coordinate of x.
a sample from this independent density (14.15) is easily generated from the
data itself by applying a di   erent random permutation to the data values
of each of the variables.

as discussed above, unsupervised learning is concerned with revealing
properties of the data density g(x). each technique focuses on a particu-
lar property or set of properties. although this approach of transforming
the problem to one of supervised learning (14.10)   (14.14) seems to have
been part of the statistics folklore for some time, it does not appear to
have had much impact despite its potential to bring well-developed su-
pervised learning methodology to bear on unsupervised learning problems.
one reason may be that the problem must be enlarged with a simulated
data set generated by monte carlo techniques. since the size of this data
set should be at least as large as the data sample n0     n , the compu-
tation and memory requirements of the estimation procedure are at least
doubled. also, substantial computation may be required to generate the
monte carlo sample itself. although perhaps a deterrent in the past, these
increased computational requirements are becoming much less of a burden
as increased resources become routinely available. we illustrate the use of
supervised learning methods for unsupervised learning in the next section.

14.2.5 generalized association rules

the more general problem (14.2) of    nding high-density regions in the data
space can be addressed using the supervised learning approach described
above. although not applicable to the huge data bases for which market
basket analysis is feasible, useful information can be obtained from mod-
erately sized data sets. the problem (14.2) can be formulated as    nding
subsets of the integers j     {1, 2, . . . , p} and corresponding value subsets
sj, j     j for the corresponding variables xj, such that

cpr      \j   j

(xj     sj)       =

1
n

nxi=1

i      \j   j

(xij     sj)      

(14.16)

is large. following the nomenclature of association rule analysis, {(xj    
sj)}j   j will be called a    generalized    item set. the subsets sj correspond-
ing to quantitative variables are taken to be contiguous intervals within

498

14. unsupervised learning

their range of values, and subsets for categorical variables can involve more
than a single value. the ambitious nature of this formulation precludes a
thorough search for all generalized item sets with support (14.16) greater
than a speci   ed minimum threshold, as was possible in the more restric-
tive setting of market basket analysis. heuristic search methods must be
employed, and the most one can hope for is to    nd a useful collection of
such generalized item sets.

both market basket analysis (14.5) and the generalized formulation (14.16)

implicitly reference the uniform id203 distribution. one seeks item
sets that are more frequent than would be expected if all joint data values
(x1, x2, . . . , xn ) were uniformly distributed. this favors the discovery of
item sets whose marginal constituents (xj     sj) are individually frequent,
that is, the quantity

1
n

nxi=1

i(xij     sj)

(14.17)

is large. conjunctions of frequent subsets (14.17) will tend to appear more
often among item sets of high support (14.16) than conjunctions of margin-
ally less frequent subsets. this is why the rule vodka     caviar is not likely
to be discovered in spite of a high association (lift); neither item has high
marginal support, so that their joint support is especially small. reference
to the uniform distribution can cause highly frequent item sets with low
associations among their constituents to dominate the collection of highest
support item sets.

highly frequent subsets sj are formed as disjunctions of the most fre-
quent xj-values. using the product of the variable marginal data densities
(14.15) as a reference distribution removes the preference for highly fre-
quent values of the individual variables in the discovered item sets. this is
because the density ratio g(x)/g0(x) is uniform if there are no associations
among the variables (complete independence), regardless of the frequency
distribution of the individual variable values. rules like vodka     caviar
would have a chance to emerge. it is not clear however, how to incorporate
reference distributions other than the uniform into the apriori algorithm.
as explained in section 14.2.4, it is straightforward to generate a sample
from the product density (14.15), given the original data set.

after choosing a reference distribution, and drawing a sample from it
as in (14.11), one has a supervised learning problem with a binary-valued
output variable y     {0, 1}. the goal is to use this training data to    nd
regions

r = \j   j

(xj     sj)

(14.18)

for which the target function   (x) = e(y | x) is relatively large. in addition,
one might wish to require that the data support of these regions

14.2 association rules

499

g(x) dx

(14.19)

t (r) =zx   r

not be too small.

14.2.6 choice of supervised learning method

the regions (14.18) are de   ned by conjunctive rules. hence supervised
methods that learn such rules would be most appropriate in this context.
the terminal nodes of a cart decision tree are de   ned by rules precisely
of the form (14.18). applying cart to the pooled data (14.11) will pro-
duce a decision tree that attempts to model the target (14.10) over the
entire data space by a disjoint set of regions (terminal nodes). each region
is de   ned by a rule of the form (14.18). those terminal nodes t with high
average y-values

  yt = ave(yi | xi     t)

are candidates for high-support generalized item sets (14.16). the actual
(data) support is given by

t (r) =   yt   

nt

n + n0

,

where nt is the number of (pooled) observations within the region repre-
sented by the terminal node. by examining the resulting decision tree, one
might discover interesting generalized item sets of relatively high-support.
these can then be partitioned into antecedents and consequents in a search
for generalized association rules of high con   dence and/or lift.

another natural learning method for this purpose is the patient rule
induction method prim described in section 9.3. prim also produces
rules precisely of the form (14.18), but it is especially designed for    nding
high-support regions that maximize the average target (14.10) value within
them, rather than trying to model the target function over the entire data
space. it also provides more control over the support/average-target-value
tradeo   .

exercise 14.3 addresses an issue that arises with either of these methods
when we generate random data from the product of the marginal distribu-
tions.

14.2.7 example: market basket analysis (continued)

we illustrate the use of prim on the demographics data of table 14.1.

three of the high-support generalized item sets emerging from the prim

analysis were the following:

item set 1: support= 24%.

500

14. unsupervised learning

      

marital status = married

householder status = own

type of home

6= apartment       

item set 2: support= 24%.

            
                        

            
                        

      

age     24

marital status     {living together-not married, single}
/    {professional, homemaker, retired}

occupation

householder status     {rent, live with family}

item set 3: support= 15%.

householder status = rent
6= house

type of home

number in household     2
number of children = 0

occupation

/    {homemaker, student, unemployed}

income     [$20,000, $150,000]

generalized association rules derived from these item sets with con   dence
(14.8) greater than 95% are the following:

association rule 1: support 25%, con   dence 99.7% and lift 1.35.

(cid:20)

marital status = married

householder status = own

   

type of home 6= apartment

(cid:21)

association rule 2: support 25%, con   dence 98.7% and lift 1.97.

      

age     24

occupation

householder status     {rent, live with family}

/    {professional, homemaker, retired}

marital status     {single, living together-not married}

   

association rule 3: support 25%, con   dence 95.9% and lift 2.61.

(cid:20) householder status = own

6= apartment (cid:21)

type of home
   

marital status = married

association rule 4: support 15%, con   dence 95.4% and lift 1.50.

14.3 cluster analysis

501

                  

type of home

householder status = rent
6= house
/    {homemaker, student, unemployed}

number in household     2

occupation

income     [$20,000, $150,000]

   

number of children = 0

                  

there are no great surprises among these particular rules. for the most
part they verify intuition. in other contexts where there is less prior in-
formation available, unexpected results have a greater chance to emerge.
these results do illustrate the type of information generalized association
rules can provide, and that the supervised learning approach, coupled with
a ruled induction method such as cart or prim, can uncover item sets
exhibiting high associations among their constituents.

how do these generalized association rules compare to those found earlier
by the apriori algorithm? since the apriori procedure gives thousands of
rules, it is di   cult to compare them. however some general points can be
made. the apriori algorithm is exhaustive   it    nds all rules with support
greater than a speci   ed amount. in contrast, prim is a greedy algorithm
and is not guaranteed to give an    optimal    set of rules. on the other hand,
the apriori algorithm can deal only with dummy variables and hence could
not    nd some of the above rules. for example, since type of home is a
categorical input, with a dummy variable for each level, apriori could not
   nd a rule involving the set

type of home 6= apartment.

to    nd this set, we would have to code a dummy variable for apartment
versus the other categories of type of home. it will not generally be feasible
to precode all such potentially interesting comparisons.

14.3 cluster analysis

cluster analysis, also called data segmentation, has a variety of goals. all
relate to grouping or segmenting a collection of objects into subsets or
   clusters,    such that those within each cluster are more closely related to
one another than objects assigned to di   erent clusters. an object can be
described by a set of measurements, or by its relation to other objects.
in addition, the goal is sometimes to arrange the clusters into a natural
hierarchy. this involves successively grouping the clusters themselves so

502

14. unsupervised learning

   
   

   
   

   
   
   

   
   
   
   
   
   
   
   

   

   
   

      
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
      
       
   
   
       
   
   
   
   

   

   

   
   

2
x

   
   

   

   
   

   

   

   

   
   
   
   
   
   

   

   
   

   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
      
   

   
   
   
   
   
   

   
   
   
   
   
   

   
   
   
   
   
      
   
   
   
   
   
       
       
   
   
   
   
   
   
   

x1

   

   
   
   
   
   

   
   

   
   
   

   
   
   

   

   
   

   

   

   
   
   
   
   
   
   
   

figure 14.4. simulated data in the plane, clustered into three classes (repre-
sented by orange, blue and green) by the id116 id91 algorithm

that at each level of the hierarchy, clusters within the same group are more
similar to each other than those in di   erent groups.

cluster analysis is also used to form descriptive statistics to ascertain
whether or not the data consists of a set distinct subgroups, each group
representing objects with substantially di   erent properties. this latter goal
requires an assessment of the degree of di   erence between the objects as-
signed to the respective clusters.

central to all of the goals of cluster analysis is the notion of the degree of
similarity (or dissimilarity) between the individual objects being clustered.
a id91 method attempts to group the objects based on the de   nition
of similarity supplied to it. this can only come from subject matter consid-
erations. the situation is somewhat similar to the speci   cation of a loss or
cost function in prediction problems (supervised learning). there the cost
associated with an inaccurate prediction depends on considerations outside
the data.

figure 14.4 shows some simulated data clustered into three groups via
the popular id116 algorithm. in this case two of the clusters are not
well separated, so that    segmentation    more accurately describes the part
of this process than    id91.    id116 id91 starts with guesses
for the three cluster centers. then it alternates the following steps until
convergence:

    for each data point, the closest cluster center (in euclidean distance)

is identi   ed;

14.3 cluster analysis

503

    each cluster center is replaced by the coordinate-wise average of all

data points that are closest to it.

we describe id116 id91 in more detail later, including the prob-
lem of how to choose the number of clusters (three in this example). k-
means id91 is a top-down procedure, while other cluster approaches
that we discuss are bottom-up. fundamental to all id91 techniques is
the choice of distance or dissimilarity measure between two objects. we
   rst discuss distance measures before describing a variety of algorithms for
id91.

14.3.1 proximity matrices

sometimes the data is represented directly in terms of the proximity (alike-
ness or a   nity) between pairs of objects. these can be either similarities or
dissimilarities (di   erence or lack of a   nity). for example, in social science
experiments, participants are asked to judge by how much certain objects
di   er from one another. dissimilarities can then be computed by averaging
over the collection of such judgments. this type of data can be represented
by an n   n matrix d, where n is the number of objects, and each element
dii    records the proximity between the ith and i   th objects. this matrix is
then provided as input to the id91 algorithm.

most algorithms presume a matrix of dissimilarities with nonnegative
entries and zero diagonal elements: dii = 0, i = 1, 2, . . . , n. if the original
data were collected as similarities, a suitable monotone-decreasing function
can be used to convert them to dissimilarities. also, most algorithms as-
sume symmetric dissimilarity matrices, so if the original matrix d is not
symmetric it must be replaced by (d+dt )/2. subjectively judged dissimi-
larities are seldom distances in the strict sense, since the triangle inequality
dii        dik +di   k, for all k     {1, . . . , n} does not hold. thus, some algorithms
that assume distances cannot be used with such data.

14.3.2 dissimilarities based on attributes

most often we have measurements xij for i = 1, 2, . . . , n , on variables
j = 1, 2, . . . , p (also called attributes). since most of the popular id91
algorithms take a dissimilarity matrix as their input, we must    rst construct
pairwise dissimilarities between the observations. in the most common case,
we de   ne a dissimilarity dj(xij, xi   j) between values of the jth attribute,
and then de   ne

d(xi, xi    ) =

pxj=1

dj(xij, xi   j)

(14.20)

as the dissimilarity between objects i and i   . by far the most common
choice is squared distance

504

14. unsupervised learning

dj(xij, xi   j) = (xij     xi   j)2.

(14.21)

however, other choices are possible, and can lead to potentially di   erent
results. for nonquantitative attributes (e.g., categorical data), squared dis-
tance may not be appropriate. in addition, it is sometimes desirable to
weigh attributes di   erently rather than giving them equal weight as in
(14.20).

we    rst discuss alternatives in terms of the attribute type:

quantitative variables. measurements of this type of variable or attribute
are represented by continuous real-valued numbers. it is natural to
de   ne the    error    between them as a monotone-increasing function
of their absolute di   erence

d(xi, xi    ) = l(|xi     xi   |).

besides squared-error loss (xi    xi    )2, a common choice is the identity
(absolute error). the former places more emphasis on larger di   er-
ences than smaller ones. alternatively, id91 can be based on the
correlation

  (xi, xi    ) =

,

(14.22)

pj(xij       xi)(xi   j       xi    )
qpj(xij       xi)2pj(xi   j       xi    )2

with   xi =pj xij/p. note that this is averaged over variables, not ob-
servations. if the observations are    rst standardized, thenpj(xij    

xi   j)2     2(1      (xi, xi    )). hence id91 based on correlation (simi-
larity) is equivalent to that based on squared distance (dissimilarity).

ordinal variables. the values of this type of variable are often represented
as contiguous integers, and the realizable values are considered to be
an ordered set. examples are academic grades (a, b, c, d, f), degree
of preference (can   t stand, dislike, ok, like, terri   c). rank data are a
special kind of ordinal data. error measures for ordinal variables are
generally de   ned by replacing their m original values with

i     1/2

m

, i = 1, . . . , m

(14.23)

in the prescribed order of their original values. they are then treated
as quantitative variables on this scale.

categorical variables. with unordered categorical (also called nominal)
variables, the degree-of-di   erence between pairs of values must be
delineated explicitly. if the variable assumes m distinct values, these
can be arranged in a symmetric m    m matrix with elements lrr    =
lr   r, lrr = 0, lrr        0. the most common choice is lrr    = 1 for all
r 6= r   , while unequal losses can be used to emphasize some errors
more than others.

14.3 cluster analysis

505

14.3.3 object dissimilarity

next we de   ne a procedure for combining the p-individual attribute dissim-
ilarities dj(xij, xi   j), j = 1, 2, . . . , p into a single overall measure of dissim-
ilarity d(xi, xi    ) between two objects or observations (xi, xi    ) possessing
the respective attribute values. this is nearly always done by means of a
weighted average (convex combination)

d(xi, xi    ) =

pxj=1

wj    dj(xij, xi   j);

pxj=1

wj = 1.

(14.24)

here wj is a weight assigned to the jth attribute regulating the relative
in   uence of that variable in determining the overall dissimilarity between
objects. this choice should be based on subject matter considerations.

it is important to realize that setting the weight wj to the same value
for each variable (say, wj = 1     j) does not necessarily give all attributes
equal in   uence. the in   uence of the jth attribute xj on object dissimilarity
d(xi, xi    ) (14.24) depends upon its relative contribution to the average
object dissimilarity measure over all pairs of observations in the data set

  d =

1
n 2

nxi=1

d(xi, xi    ) =

pxj=1

wj      dj,

nxi   =1
nxi=1

1
n 2

nxi   =1

with

  dj =

dj(xij, xi   j)

(14.25)

being the average dissimilarity on the jth attribute. thus, the relative in-
   uence of the jth variable is wj      dj, and setting wj     1/   dj would give all
attributes equal in   uence in characterizing overall dissimilarity between ob-
jects. for example, with p quantitative variables and squared-error distance
used for each coordinate, then (14.24) becomes the (weighted) squared eu-
clidean distance

di (xi, xi    ) =

pxj=1

wj    (xij     xi   j)2

(14.26)

between pairs of points in an irp, with the quantitative variables as axes.
in this case (14.25) becomes

  dj =

1
n 2

nxi=1

nxi   =1
(xij     xi   j)2 = 2    varj,

(14.27)

where varj is the sample estimate of var(xj). thus, the relative impor-
tance of each such variable is proportional to its variance over the data

506

14. unsupervised learning

   

   

   
   
   
   
   
   
      
   
   
   
   
   
      
         
   
   
   
   
   
   
   
   
   
   
      
       
   
      
   
   
   
   
   
   
   
   

   
   
   
   

   

   

   

   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
         
   
   
       
      
   
   
   
   
   
           
   
   
   
   
      
      
   
   
   
   
   
   

   

2
x

2
x

4

2

0

2
-

4
-

6
-

   

2

1

0

1
-

2
-

   

   

   

   
   
   
   
   
   
   
   
   
   
   
         
   
   
   
   
   
   
   
   
   
   
   
                  
   
   
   
   
      
   
   
   
   
   

-6

-4

-2

0

2

4

-2

-1

x1

   
   

   

   

   

   
   

   
      

   

   

      

   
   
   
   
   
   
   

   
   
   
   
   
      
   
      
   
   
   
   
   
   
   
   
   
       
   
   
       
   
   
   
   
   
   

   

   

1

0

x1

   

2

figure 14.5. simulated data: on the left, id116 id91 (with k=2) has
been applied to the raw data. the two colors indicate the cluster memberships. on
the right, the features were    rst standardized before id91. this is equivalent
to using feature weights 1/[2   var(xj)]. the standardization has obscured the two
well-separated groups. note that each plot uses the same units in the horizontal
and vertical axes.

set. in general, setting wj = 1/   dj for all attributes, irrespective of type,
will cause each one of them to equally in   uence the overall dissimilarity
between pairs of objects (xi, xi    ). although this may seem reasonable, and
is often recommended, it can be highly counterproductive. if the goal is to
segment the data into groups of similar objects, all attributes may not con-
tribute equally to the (problem-dependent) notion of dissimilarity between
objects. some attribute value di   erences may re   ect greater actual object
dissimilarity in the context of the problem domain.

if the goal is to discover natural groupings in the data, some attributes
may exhibit more of a grouping tendency than others. variables that are
more relevant in separating the groups should be assigned a higher in   u-
ence in de   ning object dissimilarity. giving all attributes equal in   uence
in this case will tend to obscure the groups to the point where a id91
algorithm cannot uncover them. figure 14.5 shows an example.

although simple generic prescriptions for choosing the individual at-
tribute dissimilarities dj(xij, xi   j) and their weights wj can be comforting,
there is no substitute for careful thought in the context of each individ-
ual problem. specifying an appropriate dissimilarity measure is far more
important in obtaining success with id91 than choice of id91
algorithm. this aspect of the problem is emphasized less in the cluster-
ing literature than the algorithms themselves, since it depends on domain
knowledge speci   cs and is less amenable to general research.

14.3 cluster analysis

507

finally, often observations have missing values in one or more of the
attributes. the most common method of incorporating missing values in
dissimilarity calculations (14.24) is to omit each observation pair xij, xi   j
having at least one value missing, when computing the dissimilarity be-
tween observations xi and x   
i. this method can fail in the circumstance
when both observations have no measured values in common. in this case
both observations could be deleted from the analysis. alternatively, the
missing values could be imputed using the mean or median of each attribute
over the nonmissing data. for categorical variables, one could consider the
value    missing    as just another categorical value, if it were reasonable to
consider two objects as being similar if they both have missing values on
the same variables.

14.3.4 id91 algorithms

the goal of cluster analysis is to partition the observations into groups
(   clusters   ) so that the pairwise dissimilarities between those assigned to
the same cluster tend to be smaller than those in di   erent clusters. clus-
tering algorithms fall into three distinct types: combinatorial algorithms,
mixture modeling, and mode seeking.

combinatorial algorithms work directly on the observed data with no
direct reference to an underlying id203 model. mixture modeling sup-
poses that the data is an i.i.d sample from some population described by a
id203 density function. this density function is characterized by a pa-
rameterized model taken to be a mixture of component density functions;
each component density describes one of the clusters. this model is then    t
to the data by maximum likelihood or corresponding bayesian approaches.
mode seekers (   bump hunters   ) take a nonparametric perspective, attempt-
ing to directly estimate distinct modes of the id203 density function.
observations    closest    to each respective mode then de   ne the individual
clusters.

mixture modeling is described in section 6.8. the prim algorithm, dis-
cussed in sections 9.3 and 14.2.5, is an example of mode seeking or    bump
hunting.    we discuss combinatorial algorithms next.

14.3.5 combinatorial algorithms

the most popular id91 algorithms directly assign each observation
to a group or cluster without regard to a id203 model describing the
data. each observation is uniquely labeled by an integer i     {1,        , n}.
a prespeci   ed number of clusters k < n is postulated, and each one is
labeled by an integer k     {1, . . . , k}. each observation is assigned to one
and only one cluster. these assignments can be characterized by a many-
to-one mapping, or encoder k = c(i), that assigns the ith observation to
the kth cluster. one seeks the particular encoder c   (i) that achieves the

508

14. unsupervised learning

required goal (details below), based on the dissimilarities d(xi, xi    ) between
every pair of observations. these are speci   ed by the user as described
above. generally, the encoder c(i) is explicitly delineated by giving its
value (cluster assignment) for each observation i. thus, the    parameters   
of the procedure are the individual cluster assignments for each of the n
observations. these are adjusted so as to minimize a    loss    function that
characterizes the degree to which the id91 goal is not met.

one approach is to directly specify a mathematical id168 and
attempt to minimize it through some combinatorial optimization algorithm.
since the goal is to assign close points to the same cluster, a natural loss
(or    energy   ) function would be

w (c) =

1
2

kxk=1 xc(i)=k xc(i   )=k

d(xi, xi    ).

(14.28)

this criterion characterizes the extent to which observations assigned to
the same cluster tend to be close to one another. it is sometimes referred
to as the    within cluster    point scatter since

t =

1
2

nxi=1

nxi   =1

dii    =

1
2

kxk=1 xc(i)=k

or

       xc(i   )=k

dii    + xc(i   )6=k

dii          ,

t = w (c) + b(c),

where dii    = d(xi, xi    ). here t is the total point scatter, which is a constant
given the data, independent of cluster assignment. the quantity

b(c) =

1
2

kxk=1 xc(i)=k xc(i   )6=k

dii   

(14.29)

is the between-cluster point scatter. this will tend to be large when obser-
vations assigned to di   erent clusters are far apart. thus one has

w (c) = t     b(c)

and minimizing w (c) is equivalent to maximizing b(c).

cluster analysis by combinatorial optimization is straightforward in prin-
ciple. one simply minimizes w or equivalently maximizes b over all pos-
sible assignments of the n data points to k clusters. unfortunately, such
optimization by complete enumeration is feasible only for very small data
sets. the number of distinct assignments is (jain and dubes, 1988)

s(n, k) =

1
k!

(   1)k   k(cid:18)k
kxk=1

k(cid:19) kn .

(14.30)

for example, s(10, 4) = 34, 105 which is quite feasible. but, s(n, k) grows
very rapidly with increasing values of its arguments. already s(19, 4)    

14.3 cluster analysis

509

1010, and most id91 problems involve much larger data sets than
n = 19. for this reason, practical id91 algorithms are able to examine
only a very small fraction of all possible encoders k = c(i). the goal is to
identify a small subset that is likely to contain the optimal one, or at least
a good suboptimal partition.

such feasible strategies are based on iterative greedy descent. an initial
partition is speci   ed. at each iterative step, the cluster assignments are
changed in such a way that the value of the criterion is improved from
its previous value. id91 algorithms of this type di   er in their pre-
scriptions for modifying the cluster assignments at each iteration. when
the prescription is unable to provide an improvement, the algorithm ter-
minates with the current assignments as its solution. since the assignment
of observations to clusters at any iteration is a perturbation of that for the
previous iteration, only a very small fraction of all possible assignments
(14.30) are examined. however, these algorithms converge to local optima
which may be highly suboptimal when compared to the global optimum.

14.3.6 id116

the id116 algorithm is one of the most popular iterative descent clus-
tering methods. it is intended for situations in which all variables are of
the quantitative type, and squared euclidean distance

d(xi, xi    ) =

pxj=1

(xij     xi   j)2 = ||xi     xi   ||2

is chosen as the dissimilarity measure. note that weighted euclidean dis-
tance can be used by rede   ning the xij values (exercise 14.1).

the within-point scatter (14.28) can be written as

w (c) =

=

||xi     xi   ||2

1
2

kxk=1 xc(i)=k xc(i   )=k
kxk=1
nk xc(i)=k

||xi       xk||2,

(14.31)

ter, and nk = pn

where   xk = (  x1k, . . . ,   xpk) is the mean vector associated with the kth clus-
i=1 i(c(i) = k). thus, the criterion is minimized by
assigning the n observations to the k clusters in such a way that within
each cluster the average dissimilarity of the observations from the cluster
mean, as de   ned by the points in that cluster, is minimized.

an iterative descent algorithm for solving

510

14. unsupervised learning

algorithm 14.1 id116 id91.

1. for a given cluster assignment c, the total cluster variance (14.33) is
minimized with respect to {m1, . . . , mk} yielding the means of the
currently assigned clusters (14.32).

2. given a current set of means {m1, . . . , mk}, (14.33) is minimized by
assigning each observation to the closest (current) cluster mean. that
is,

c(i) = argmin

1   k   k ||xi     mk||2.

(14.34)

3. steps 1 and 2 are iterated until the assignments do not change.

(14.32)

(14.33)

kxk=1
nk xc(i)=k
m xi   s
kxk=1
nk xc(i)=k

c    = min
c

||xi       xk||2

can be obtained by noting that for any set of observations s

  xs = argmin

||xi     m||2.

hence we can obtain c    by solving the enlarged optimization problem

min

c,{mk}k
1

||xi     mk||2.

this can be minimized by an alternating optimization procedure given in
algorithm 14.1.

each of steps 1 and 2 reduces the value of the criterion (14.33), so that
convergence is assured. however, the result may represent a suboptimal
local minimum. the algorithm of hartigan and wong (1979) goes further,
and ensures that there is no single switch of an observation from one group
to another group that will decrease the objective. in addition, one should
start the algorithm with many di   erent random choices for the starting
means, and choose the solution having smallest value of the objective func-
tion.

figure 14.6 shows some of the id116 iterations for the simulated data
of figure 14.4. the centroids are depicted by    o   s. the straight lines show
the partitioning of points, each sector being the set of points closest to
each centroid. this partitioning is called the voronoi tessellation. after 20
iterations the procedure has converged.

14.3.7 gaussian mixtures as soft id116 id91

the id116 id91 procedure is closely related to the em algorithm
for estimating a certain gaussian mixture model. (sections 6.8 and 8.5.1).

14.3 cluster analysis

511

initial centroids

initial partition

6

4

2

0

2
-

   
   

   
   

   
   
   
   
   
   
   
   

   
   

   
      
   
      
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
              
   
      
   
   
       
   
   
   
   
   
   

   
   

   
   

   
   

   
   
   
   
   
   
   
   

   
   

   
      
   
      
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
              
   
      
   
   
       
   
   
   
   
   
   

   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   

   
      
   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
       
   
       
   
       
      
   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
      
       
   
   
   

   
   

   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   

   
      
   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
       
   
       
   
       
      
   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
      
       
   
   
   

   
   

   
   
   

-4

-2

0

2

4

6

iteration number  2

iteration number  20

   
   

   
   

   
   
   
   
   
   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   

   
   

   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
       
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
       
       
   
       
      
   
       
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
       
   
   

   
   
   
   
   
   
   
   
   
   
   
   
      
   

   
   

   
   

   
   
   
   
   
   
   
   

   
      
   
   
   
      
         
   
   
   
   
   
   
   
   
   
   
   
   
   
   
              
   
   
      
   
       
   
   
   
   
   
   

   
   

   
   
   

   

   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
       
       
   
       
      
   
       
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
       
   
   

   
   
   
   
   
   
   
   
   
   
   
   
      
   

   
   
   

figure 14.6. successive iterations of the id116 id91 algorithm for
the simulated data of figure 14.4.

512

14. unsupervised learning

   = 1.0

   = 1.0

s
e

i
t
i
l
i

i

b
s
n
o
p
s
e
r

s
e

i
t
i
l
i

i

b
s
n
o
p
s
e
r

   

   = 0.2

   

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

   

   = 0.2

   

figure 14.7. (left panels:) two gaussian densities g0(x) and g1(x) (blue and
orange) on the real line, and a single data point (green dot) at x = 0.5. the colored
squares are plotted at x =    1.0 and x = 1.0, the means of each density. (right
panels:) the relative densities g0(x)/(g0(x) + g1(x)) and g1(x)/(g0(x) + g1(x)),
called the    responsibilities    of each cluster, for this data point. in the top panels,
the gaussian standard deviation    = 1.0; in the bottom panels    = 0.2. the
em algorithm uses these responsibilities to make a    soft    assignment of each
data point to each of the two clusters. when    is fairly large, the responsibilities
can be near 0.5 (they are 0.36 and 0.64 in the top right panel). as        0, the
responsibilities     1, for the cluster center closest to the target point, and 0 for
all other clusters. this    hard    assignment is seen in the bottom right panel.

the e-step of the em algorithm assigns    responsibilities    for each data
point based in its relative density under each mixture component, while
the m-step recomputes the component density parameters based on the
current responsibilities. suppose we specify k mixture components, each
with a gaussian density having scalar covariance matrix   2i. then the
relative density under each mixture component is a monotone function of
the euclidean distance between the data point and the mixture center.
hence in this setup em is a    soft    version of id116 id91, making
probabilistic (rather than deterministic) assignments of points to cluster
centers. as the variance   2     0, these probabilities become 0 and 1, and
the two methods coincide. details are given in exercise 14.2. figure 14.7
illustrates this result for two clusters on the real line.

14.3.8 example: human tumor microarray data

we apply id116 id91 to the human tumor microarray data de-
scribed in chapter 1. this is an example of high-dimensional id91.

14.3 cluster analysis

513

   

s
e
r
a
u
q
s

 
f

 

o
m
u
s

0
0
0
0
4
2

0
0
0
0
0
2

0
0
0
0
6
1

   

   

2

   

4

   

   

6

number of clusters k

   

   

8

   

   

10

figure 14.8. total within-cluster sum of squares for id116 id91 ap-
plied to the human tumor microarray data.

table 14.2. human tumor data: number of cancer cases of each type, in each
of the three clusters from id116 id91.

cluster
1
2
3
cluster
1
2
3

breast

3
2
2

cns
5
0
0

colon

k562

leukemia

mcf7

0
0
7

0
2
0

0
6
0

0
2
0

melanoma

nsclc

ovarian

prostate

renal

unknown

1
7
0

7
2
0

6
0
0

2
0
0

9
0
0

1
0
0

the data are a 6830    64 matrix of real numbers, each representing an
expression measurement for a gene (row) and sample (column). here we
cluster the samples, each of which is a vector of length 6830, correspond-
ing to expression values for the 6830 genes. each sample has a label such
as breast (for breast cancer), melanoma, and so on; we don   t use these la-
bels in the id91, but will examine posthoc which labels fall into which
clusters.

we applied id116 id91 with k running from 1 to 10, and com-
puted the total within-sum of squares for each id91, shown in fig-
ure 14.8. typically one looks for a kink in the sum of squares curve (or its
logarithm) to locate the optimal number of clusters (see section 14.3.11).
here there is no clear indication: for illustration we chose k = 3 giving the
three clusters shown in table 14.2.

514

14. unsupervised learning

figure 14.9. sir ronald a. fisher (1890     1962) was one of the founders
of modern day statistics, to whom we owe maximum-likelihood, su   ciency, and
many other fundamental concepts. the image on the left is a 1024  1024 grayscale
image at 8 bits per pixel. the center image is the result of 2    2 block vq, using
200 code vectors, with a compression rate of 1.9 bits/pixel. the right image uses
only four code vectors, with a compression rate of 0.50 bits/pixel

we see that the procedure is successful at grouping together samples of
the same cancer. in fact, the two breast cancers in the second cluster were
later found to be misdiagnosed and were melanomas that had metastasized.
however, id116 id91 has shortcomings in this application. for one,
it does not give a linear ordering of objects within a cluster: we have simply
listed them in alphabetic order above. secondly, as the number of clusters
k is changed, the cluster memberships can change in arbitrary ways. that
is, with say four clusters, the clusters need not be nested within the three
clusters above. for these reasons, hierarchical id91 (described later),
is probably preferable for this application.

14.3.9 vector quantization

the id116 id91 algorithm represents a key tool in the apparently
unrelated area of image and signal compression, particularly in vector quan-
tization or vq (gersho and gray, 1992). the left image in figure 14.92 is a
digitized photograph of a famous statistician, sir ronald fisher. it consists
of 1024    1024 pixels, where each pixel is a grayscale value ranging from 0
to 255, and hence requires 8 bits of storage per pixel. the entire image oc-
cupies 1 megabyte of storage. the center image is a vq-compressed version
of the left panel, and requires 0.239 of the storage (at some loss in quality).
the right image is compressed even more, and requires only 0.0625 of the
storage (at a considerable loss in quality).

the version of vq implemented here    rst breaks the image into small
blocks, in this case 2  2 blocks of pixels. each of the 512  512 blocks of four

2this example was prepared by maya gupta.

14.3 cluster analysis

515

numbers is regarded as a vector in ir4. a id116 id91 algorithm
(also known as lloyd   s algorithm in this context) is run in this space.
the center image uses k = 200, while the right image k = 4. each of
the 512   512 pixel blocks (or points) is approximated by its closest cluster
centroid, known as a codeword. the id91 process is called the encoding
step, and the collection of centroids is called the codebook.

to represent the approximated image, we need to supply for each block
the identity of the codebook entry that approximates it. this will require
log2(k) bits per block. we also need to supply the codebook itself, which
is k    4 real numbers (typically negligible). overall, the storage for the
compressed image amounts to log2(k)/(4    8) of the original (0.239 for
k = 200, 0.063 for k = 4). this is typically expressed as a rate in bits
per pixel: log2(k)/4, which are 1.91 and 0.50, respectively. the process
of constructing the approximate image from the centroids is called the
decoding step.

why do we expect vq to work at all? the reason is that for typical
everyday images like photographs, many of the blocks look the same. in
this case there are many almost pure white blocks, and similarly pure gray
blocks of various shades. these require only one block each to represent
them, and then multiple pointers to that block.

what we have described is known as lossy compression, since our im-
ages are degraded versions of the original. the degradation or distortion is
usually measured in terms of mean squared error. in this case d = 0.89
for k = 200 and d = 16.95 for k = 4. more generally a rate/distortion
curve would be used to assess the tradeo   . one can also perform lossless
compression using block id91, and still capitalize on the repeated pat-
terns. if you took the original image and losslessly compressed it, the best
you would do is 4.48 bits per pixel.

will do better, and the rate then becomes    pk

we claimed above that log2(k) bits were needed to identify each of the k
codewords in the codebook. this uses a    xed-length code, and is ine   cient
if some codewords occur many more times than others in the image. using
shannon coding theory, we know that in general a variable length code
   =1 p    log2(p   )/4. the term
in the numerator is the id178 of the distribution p    of the codewords
in the image. using variable length coding our rates come down to 1.42
and 0.39, respectively. finally, there are many generalizations of vq that
have been developed: for example, tree-structured vq    nds the centroids
with a top-down, 2-means style algorithm, as alluded to in section 14.3.12.
this allows successive re   nement of the compression. further details may
be found in gersho and gray (1992).

14.3.10 k-medoids

as discussed above, the id116 algorithm is appropriate when the dis-
similarity measure is taken to be squared euclidean distance d(xi, xi    )

516

14. unsupervised learning

algorithm 14.2 k-medoids id91.

1. for a given cluster assignment c    nd the observation in the cluster

minimizing total distance to other points in that cluster:

i   
k = argmin

{i:c(i)=k} xc(i   )=k

d(xi, xi    ).

(14.35)

then mk = xi   
cluster centers.

k

, k = 1, 2, . . . , k are the current estimates of the

2. given a current set of cluster centers {m1, . . . , mk}, minimize the to-
tal error by assigning each observation to the closest (current) cluster
center:

c(i) = argmin
1   k   k

d(xi, mk).

(14.36)

3. iterate steps 1 and 2 until the assignments do not change.

(14.112). this requires all of the variables to be of the quantitative type. in
addition, using squared euclidean distance places the highest in   uence on
the largest distances. this causes the procedure to lack robustness against
outliers that produce very large distances. these restrictions can be re-
moved at the expense of computation.

the only part of the id116 algorithm that assumes squared eu-
clidean distance is the minimization step (14.32); the cluster representatives
{m1, . . . , mk} in (14.33) are taken to be the means of the currently assigned
clusters. the algorithm can be generalized for use with arbitrarily de   ned
dissimilarities d(xi, xi    ) by replacing this step by an explicit optimization
with respect to {m1, . . . , mk} in (14.33). in the most common form, cen-
ters for each cluster are restricted to be one of the observations assigned
to the cluster, as summarized in algorithm 14.2. this algorithm assumes
attribute data, but the approach can also be applied to data described
only by proximity matrices (section 14.3.1). there is no need to explicitly
compute cluster centers; rather we just keep track of the indices i   
k.

solving (14.32) for each provisional cluster k requires an amount of com-
putation proportional to the number of observations assigned to it, whereas
for solving (14.35) the computation increases to o(n 2
k ). given a set of clus-
ter    centers,    {i1, . . . , ik}, obtaining the new assignments

c(i) = argmin
1   k   k

dii   

k

(14.37)

requires computation proportional to k    n as before. thus, k-medoids is
far more computationally intensive than id116.
alternating between (14.35) and (14.37) represents a particular heuristic

search strategy for trying to solve

14.3 cluster analysis

517

table 14.3. data from a political science survey: values are average pairwise
dissimilarities of countries from a questionnaire given to political science students.

bel bra chi cub egy fra ind isr usa uss yug

7.00 6.50

bra 5.58
chi
cub 7.08 7.00 3.83
egy 4.83 5.08 8.17 5.83
fra 2.17 5.75 6.67 6.92
ind 6.42 5.00 5.58 6.00
isr 3.42 5.50 6.42 6.42
usa 2.50 4.92 6.25 7.33
6.08 6.67 4.25 2.67
uss
yug 5.25 6.83 4.50 3.75
zai
4.75 3.00 6.08 6.67

4.92
4.67
5.00
4.50
6.00
5.75
5.00

6.42
3.92 6.17
2.25 6.33 2.75
6.17 6.17 6.92 6.17
5.42 6.08 5.83 6.67 3.67
5.58 4.83 6.17 5.67 6.50 6.92

min

c, {ik}k
1

kxk=1 xc(i)=k

diik .

(14.38)

kaufman and rousseeuw (1990) propose an alternative strategy for directly
solving (14.38) that provisionally exchanges each center ik with an obser-
vation that is not currently a center, selecting the exchange that produces
the greatest reduction in the value of the criterion (14.38). this is repeated
until no advantageous exchanges can be found. massart et al. (1983) derive
a branch-and-bound combinatorial method that    nds the global minimum
of (14.38) that is practical only for very small data sets.

example: country dissimilarities

this example, taken from kaufman and rousseeuw (1990), comes from a
study in which political science students were asked to provide pairwise dis-
similarity measures for 12 countries: belgium, brazil, chile, cuba, egypt,
france, india, israel, united states, union of soviet socialist republics,
yugoslavia and zaire. the average dissimilarity scores are given in ta-
ble 14.3. we applied 3-medoid id91 to these dissimilarities. note that
id116 id91 could not be applied because we have only distances
rather than raw observations. the left panel of figure 14.10 shows the
dissimilarities reordered and blocked according to the 3-medoid id91.
the right panel is a two-dimensional multidimensional scaling plot, with
the 3-medoid clusters assignments indicated by colors (multidimensional
scaling is discussed in section 14.8.) both plots show three well-separated
clusters, but the mds display indicates that    egypt    falls about halfway
between two clusters.

518

14. unsupervised learning

i

h
c

b
u
c

s
s
u

g
u
y

a
r
b

d
n

i

i

a
z

l
e
b

y
g
e

a
r
f

r
s

i

usa

isr

fra

egy

bel

zai

ind

bra

yug

uss

cub

i

t

 

e
a
n
d
r
o
o
c
s
d
m
d
n
o
c
e
s

 

3

2

1

0

1
-

2
-

zai

bra

ind

egy

chi

cub

uss

yug

usa

isr

bel

fra

-2

0

2

4

reordered dissimilarity matrix

first mds coordinate

figure 14.10. survey of country dissimilarities. (left panel:) dissimilarities
reordered and blocked according to 3-medoid id91. heat map is coded from
most similar (dark red) to least similar (bright red). (right panel:) two-dimen-
sional multidimensional scaling plot, with 3-medoid clusters indicated by di   erent
colors.

14.3.11 practical issues

in order to apply id116 or k-medoids one must select the number of
clusters k    and an initialization. the latter can be de   ned by specifying
an initial set of centers {m1, . . . , mk} or {i1, . . . , ik} or an initial encoder
c(i). usually specifying the centers is more convenient. suggestions range
from simple random selection to a deliberate strategy based on forward
stepwise assignment. at each step a new center ik is chosen to minimize
the criterion (14.33) or (14.38), given the centers i1, . . . , ik   1 chosen at the
previous steps. this continues for k steps, thereby producing k initial
centers with which to begin the optimization algorithm.

a choice for the number of clusters k depends on the goal. for data
segmentation k is usually de   ned as part of the problem. for example,
a company may employ k sales people, and the goal is to partition a
customer database into k segments, one for each sales person, such that the
customers assigned to each one are as similar as possible. often, however,
cluster analysis is used to provide a descriptive statistic for ascertaining the
extent to which the observations comprising the data base fall into natural
distinct groupings. here the number of such groups k    is unknown and
one requires that it, as well as the groupings themselves, be estimated from
the data.

data-based methods for estimating k    typically examine the within-
cluster dissimilarity wk as a function of the number of clusters k. separate
solutions are obtained for k     {1, 2, . . . , kmax}. the corresponding values

14.3 cluster analysis

519

{w1, w2, . . . , wkmax} generally decrease with increasing k. this will be
the case even when the criterion is evaluated on an independent test set,
since a large number of cluster centers will tend to    ll the feature space
densely and thus will be close to all data points. thus cross-validation
techniques, so useful for model selection in supervised learning, cannot be
utilized in this context.

the intuition underlying the approach is that if there are actually k   
distinct groupings of the observations (as de   ned by the dissimilarity mea-
sure), then for k < k    the clusters returned by the algorithm will each
contain a subset of the true underlying groups. that is, the solution will
not assign observations in the same naturally occurring group to di   erent
estimated clusters. to the extent that this is the case, the solution criterion
value will tend to decrease substantially with each successive increase in the
number of speci   ed clusters, wk+1     wk , as the natural groups are suc-
cessively assigned to separate clusters. for k > k   , one of the estimated
clusters must partition at least one of the natural groups into two sub-
groups. this will tend to provide a smaller decrease in the criterion as k is
further increased. splitting a natural group, within which the observations
are all quite close to each other, reduces the criterion less than partitioning
the union of two well-separated groups into their proper constituents.

to the extent this scenario is realized, there will be a sharp decrease in
successive di   erences in criterion value, wk     wk+1, at k = k   . that
is, {wk     wk+1 | k < k   }     {wk     wk+1 | k     k   }. an estimate
  k    for k    is then obtained by identifying a    kink    in the plot of wk as a
function of k. as with other aspects of id91 procedures, this approach
is somewhat heuristic.

the recently proposed gap statistic (tibshirani et al., 2001b) compares
the curve log wk to the curve obtained from data uniformly distributed
over a rectangle containing the data. it estimates the optimal number of
clusters to be the place where the gap between the two curves is largest.
essentially this is an automatic way of locating the aforementioned    kink.   
it also works reasonably well when the data fall into a single cluster, and
in that case will tend to estimate the optimal number of clusters to be one.
this is the scenario where most other competing methods fail.

figure 14.11 shows the result of the gap statistic applied to simulated
data of figure 14.4. the left panel shows log wk for k = 1, 2, . . . , 8 clusters
(green curve) and the expected value of log wk over 20 simulations from
uniform data (blue curve). the right panel shows the gap curve, which is the
expected curve minus the observed curve. shown also are error bars of half-
width s   
over the 20 simulations. the gap curve is maximized at k = 2 clusters. if
g(k) is the gap curve at k clusters, the formal rule for estimating k    is

k = skp1 + 1/20, where sk is the standard deviation of log wk

k    = argmin

k {k|g(k)     g(k + 1)     s   

k+1}.

(14.39)

520

14. unsupervised learning

0    
   

0

.

1

w
g
o
l

   
k
w
g
o
l

5

.

0
-

0

.

1
-

5

.

1
-

0

.

2
-

5

.

2
-

0

.

3
-

   
   

   
   

   
   

   
   

   
   

   
   

4

2
number of clusters

6

0

.

1

.

5
0

p
a
g

0

.

0

   

5

.

0
-

   
   

8

   

   

   

   

   

   

4

2
number of clusters

6

   

8

figure 14.11. (left panel): observed (green) and expected (blue) values of
log wk for the simulated data of figure 14.4. both curves have been translated
to equal zero at one cluster. (right panel): gap curve, equal to the di   erence
between the observed and expected values of log wk . the gap estimate k     is the
smallest k producing a gap within one standard deviation of the gap at k + 1;
here k     = 2.

this gives k    = 2, which looks reasonable from figure 14.4.

14.3.12 hierarchical id91

the results of applying id116 or k-medoids id91 algorithms de-
pend on the choice for the number of clusters to be searched and a starting
con   guration assignment. in contrast, hierarchical id91 methods do
not require such speci   cations. instead, they require the user to specify a
measure of dissimilarity between (disjoint) groups of observations, based
on the pairwise dissimilarities among the observations in the two groups.
as the name suggests, they produce hierarchical representations in which
the clusters at each level of the hierarchy are created by merging clusters
at the next lower level. at the lowest level, each cluster contains a single
observation. at the highest level there is only one cluster containing all of
the data.

strategies for hierarchical id91 divide into two basic paradigms: ag-
glomerative (bottom-up) and divisive (top-down). agglomerative strategies
start at the bottom and at each level recursively merge a selected pair of
clusters into a single cluster. this produces a grouping at the next higher
level with one less cluster. the pair chosen for merging consist of the two
groups with the smallest intergroup dissimilarity. divisive methods start
at the top and at each level recursively split one of the existing clusters at

14.3 cluster analysis

521

that level into two new clusters. the split is chosen to produce two new
groups with the largest between-group dissimilarity. with both paradigms
there are n     1 levels in the hierarchy.
each level of the hierarchy represents a particular grouping of the data
into disjoint clusters of observations. the entire hierarchy represents an
ordered sequence of such groupings. it is up to the user to decide which
level (if any) actually represents a    natural    id91 in the sense that
observations within each of its groups are su   ciently more similar to each
other than to observations assigned to di   erent groups at that level. the
gap statistic described earlier can be used for this purpose.

recursive binary splitting/agglomeration can be represented by a rooted
binary tree. the nodes of the trees represent groups. the root node repre-
sents the entire data set. the n terminal nodes each represent one of the
individual observations (singleton clusters). each nonterminal node (   par-
ent   ) has two daughter nodes. for divisive id91 the two daughters
represent the two groups resulting from the split of the parent; for agglom-
erative id91 the daughters represent the two groups that were merged
to form the parent.

most agglomerative and some divisive methods (when viewed bottom-
up) possess a monotonicity property. that is, the dissimilarity between
merged clusters is monotone increasing with the level of the merger. thus
the binary tree can be plotted so that the height of each node is proportional
to the value of the intergroup dissimilarity between its two daughters. the
terminal nodes representing individual observations are all plotted at zero
height. this type of graphical display is called a dendrogram.

a dendrogram provides a highly interpretable complete description of
the hierarchical id91 in a graphical format. this is one of the main
reasons for the popularity of hierarchical id91 methods.

for the microarray data, figure 14.12 shows the dendrogram resulting
from agglomerative id91 with average linkage; agglomerative cluster-
ing and this example are discussed in more detail later in this chapter.
cutting the dendrogram horizontally at a particular height partitions the
data into disjoint clusters represented by the vertical lines that intersect
it. these are the clusters that would be produced by terminating the pro-
cedure when the optimal intergroup dissimilarity exceeds that threshold
cut value. groups that merge at high values, relative to the merger values
of the subgroups contained within them lower in the tree, are candidates
for natural clusters. note that this may occur at several di   erent levels,
indicating a id91 hierarchy: that is, clusters nested within clusters.

such a dendrogram is often viewed as a graphical summary of the data
itself, rather than a description of the results of the algorithm. however,
such interpretations should be treated with caution. first, di   erent hierar-
chical methods (see below), as well as small changes in the data, can lead
to quite di   erent dendrograms. also, such a summary will be valid only to
the extent that the pairwise observation dissimilarities possess the hierar-

522

14. unsupervised learning

i

a
m
e
k
u
e
l

c
l
c
s
n

t
s
a
e
r
b

l
a
n
e
r

c
l
c
s
n

t
s
a
e
r
b

i

i

n
a
r
a
v
o

n
a
r
a
v
o

l
a
n
e
r

c
l
c
s
n

a
m
o
n
a
l
e
m

i

i

a
m
e
k
u
e
l

a
m
e
k
u
e
l

a
m
o
n
a
l
e
m

i

i

a
m
e
k
u
e
l

a
m
e
k
u
e
l

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

a
m
o
n
a
l
e
m

i

a
m
e
k
u
e
l

o
r
p
e
r
-

o
r
p
e
r
-

b
2
6
5
k

a
2
6
5
k

i

n
a
r
a
v
o

n
w
o
n
k
n
u

t
s
a
e
r
b

t
s
a
e
r
b

l
a
n
e
r

c
l
c
s
n

c
l
c
s
n

s
n
sc
n
c

t
s
a
e
r
b

c
l
c
s
n

c
l
c
s
n

i

i

n
a
r
a
v
o

n
a
r
a
v
o

l
a
n
e
r

l
a
n
e
r

l
a
n
e
r

l
a
n
e
r

l
a
n
e
r

l
a
n
e
r

e
t
a
t
s
o
r
p

i

n
a
r
a
v
o

e
t
a
t
s
o
r
p

c
l
c
s
n

c
l
c
s
n

s
n
c

s
n
c

s
n
c

n
o
l
o
c

n
o
l
o
c

n
o
l
o
c

n
o
l
o
c

n
o
l
o
c

n
o
l
o
c

n
o
l
o
c

t
s
a
e
r
b

o
r
p
e
r
-

a
7
f
c
m

t
s
a
e
r
b

o
r
p
e
r
-

d
7
f
c
m

figure 14.12. dendrogram from agglomerative hierarchical id91 with
average linkage to the human tumor microarray data.

chical structure produced by the algorithm. hierarchical methods impose
hierarchical structure whether or not such structure actually exists in the
data.

the extent to which the hierarchical structure produced by a dendro-
gram actually represents the data itself can be judged by the cophenetic
correlation coe   cient. this is the correlation between the n (n    1)/2 pair-
wise observation dissimilarities dii    input to the algorithm and their corre-
sponding cophenetic dissimilarities cii    derived from the dendrogram. the
cophenetic dissimilarity cii    between two observations (i, i   ) is the inter-
group dissimilarity at which observations i and i    are    rst joined together
in the same cluster.

the cophenetic dissimilarity is a very restrictive dissimilarity measure.
first, the cii    over the observations must contain many ties, since only n   1
of the total n (n     1)/2 values can be distinct. also these dissimilarities
obey the ultrametric inequality

cii        max{cik, ci   k}

(14.40)

14.3 cluster analysis

523

for any three observations (i, i   , k). as a geometric example, suppose the
data were represented as points in a euclidean coordinate system. in order
for the set of interpoint distances over the data to conform to (14.40), the
triangles formed by all triples of points must be isosceles triangles with the
unequal length no longer than the length of the two equal sides (jain and
dubes, 1988). therefore it is unrealistic to expect general dissimilarities
over arbitrary data sets to closely resemble their corresponding cophenetic
dissimilarities as calculated from a dendrogram, especially if there are not
many tied values. thus the dendrogram should be viewed mainly as a de-
scription of the id91 structure of the data as imposed by the particular
algorithm employed.

agglomerative id91

agglomerative id91 algorithms begin with every observation repre-
senting a singleton cluster. at each of the n     1 steps the closest two (least
dissimilar) clusters are merged into a single cluster, producing one less clus-
ter at the next higher level. therefore, a measure of dissimilarity between
two clusters (groups of observations) must be de   ned.

let g and h represent two such groups. the dissimilarity d(g, h) be-
tween g and h is computed from the set of pairwise observation dissim-
ilarities dii    where one member of the pair i is in g and the other i    is
in h. single linkage (sl) agglomerative id91 takes the intergroup
dissimilarity to be that of the closest (least dissimilar) pair

dsl(g, h) = min
i   g
i      h

dii    .

(14.41)

this is also often called the nearest-neighbor technique. complete linkage
(cl) agglomerative id91 (furthest-neighbor technique) takes the in-
tergroup dissimilarity to be that of the furthest (most dissimilar) pair

dcl(g, h) = max
i   g
i      h

dii    .

(14.42)

group average (ga) id91 uses the average dissimilarity between the
groups

dga(g, h) =

dii   

(14.43)

1

ngnh xi   gxi      h

where ng and nh are the respective number of observations in each group.
although there have been many other proposals for de   ning intergroup
dissimilarity in the context of agglomerative id91, the above three are
the ones most commonly used. figure 14.13 shows examples of all three.
if the data dissimilarities {dii   } exhibit a strong id91 tendency, with
each of the clusters being compact and well separated from others, then all
three methods produce similar results. clusters are compact if all of the

524

14. unsupervised learning

average linkage

complete linkage

single linkage

figure 14.13. dendrograms from agglomerative hierarchical id91 of hu-
man tumor microarray data.

observations within them are relatively close together (small dissimilarities)
as compared with observations in di   erent clusters. to the extent this is
not the case, results will di   er.

single linkage (14.41) only requires that a single dissimilarity dii    , i     g
and i        h, be small for two groups g and h to be considered close
together, irrespective of the other observation dissimilarities between the
groups. it will therefore have a tendency to combine, at relatively low
thresholds, observations linked by a series of close intermediate observa-
tions. this phenomenon, referred to as chaining, is often considered a de-
fect of the method. the clusters produced by single linkage can violate the
   compactness    property that all observations within each cluster tend to
be similar to one another, based on the supplied observation dissimilari-
ties {dii   }. if we de   ne the diameter dg of a group of observations as the
largest dissimilarity among its members

dg = max
i   g
i      g

dii    ,

(14.44)

then single linkage can produce clusters with very large diameters.

complete linkage (14.42) represents the opposite extreme. two groups
g and h are considered close only if all of the observations in their union
are relatively similar. it will tend to produce compact clusters with small
diameters (14.44). however, it can produce clusters that violate the    close-
ness    property. that is, observations assigned to a cluster can be much

14.3 cluster analysis

525

closer to members of other clusters than they are to some members of their
own cluster.

group average id91 (14.43) represents a compromise between the
two extremes of single and complete linkage. it attempts to produce rel-
atively compact clusters that are relatively far apart. however, its results
depend on the numerical scale on which the observation dissimilarities dii   
are measured. applying a monotone strictly increasing transformation h(  )
to the dii    , hii    = h(dii    ), can change the result produced by (14.43). in
contrast, (14.41) and (14.42) depend only on the ordering of the dii    and
are thus invariant to such monotone transformations. this invariance is
often used as an argument in favor of single or complete linkage over group
average methods.

one can argue that group average id91 has a statistical consis-
tency property violated by single and complete linkage. assume we have
attribute-value data x t = (x1, . . . , xp) and that each cluster k is a ran-
dom sample from some population joint density pk(x). the complete data
set is a random sample from a mixture of k such densities. the group
average dissimilarity dga(g, h) (14.43) is an estimate of

z z d(x, x   ) pg(x) ph (x   ) dx dx   ,

(14.45)

where d(x, x   ) is the dissimilarity between points x and x    in the space
of attribute values. as the sample size n approaches in   nity dga(g, h)
(14.43) approaches (14.45), which is a characteristic of the relationship
between the two densities pg(x) and ph (x) . for single linkage, dsl(g, h)
(14.41) approaches zero as n         independent of pg(x) and ph (x) . for
complete linkage, dcl(g, h) (14.42) becomes in   nite as n        , again
independent of the two densities. thus, it is not clear what aspects of the
population distribution are being estimated by dsl(g, h) and dcl(g, h).

example: human cancer microarray data (continued)

the left panel of figure 14.13 shows the dendrogram resulting from average
linkage agglomerative id91 of the samples (columns) of the microarray
data. the middle and right panels show the result using complete and single
linkage. average and complete linkage gave similar results, while single
linkage produced unbalanced groups with long thin clusters. we focus on
the average linkage id91.

like id116 id91, hierarchical id91 is successful at id91
simple cancers together. however it has other nice features. by cutting o   
the dendrogram at various heights, di   erent numbers of clusters emerge,
and the sets of clusters are nested within one another. secondly, it gives
some partial ordering information about the samples. in figure 14.14, we
have arranged the genes (rows) and samples (columns) of the expression
matrix in orderings derived from hierarchical id91.

526

14. unsupervised learning

note that if we    ip the orientation of the branches of a dendrogram at any
merge, the resulting dendrogram is still consistent with the series of hierar-
chical id91 operations. hence to determine an ordering of the leaves,
we must add a constraint. to produce the row ordering of figure 14.14,
we have used the default rule in s-plus: at each merge, the subtree with
the tighter cluster is placed to the left (toward the bottom in the rotated
dendrogram in the    gure.) individual genes are the tightest clusters possi-
ble, and merges involving two individual genes place them in order by their
observation number. the same rule was used for the columns. many other
rules are possible   for example, ordering by a multidimensional scaling of
the genes; see section 14.8.

the two-way rearrangement of figure 14.14 produces an informative pic-
ture of the genes and samples. this picture is more informative than the
randomly ordered rows and columns of figure 1.3 of chapter 1. further-
more, the dendrograms themselves are useful, as biologists can, for example,
interpret the gene clusters in terms of biological processes.

divisive id91

divisive id91 algorithms begin with the entire data set as a single
cluster, and recursively divide one of the existing clusters into two daugh-
ter clusters at each iteration in a top-down fashion. this approach has not
been studied nearly as extensively as agglomerative methods in the cluster-
ing literature. it has been explored somewhat in the engineering literature
(gersho and gray, 1992) in the context of compression. in the id91
setting, a potential advantage of divisive over agglomerative methods can
occur when interest is focused on partitioning the data into a relatively
small number of clusters.

the divisive paradigm can be employed by recursively applying any of
the combinatorial methods such as id116 (section 14.3.6) or k-medoids
(section 14.3.10), with k = 2, to perform the splits at each iteration. how-
ever, such an approach would depend on the starting con   guration speci   ed
at each step. in addition, it would not necessarily produce a splitting se-
quence that possesses the monotonicity property required for dendrogram
representation.

a divisive algorithm that avoids these problems was proposed by mac-
naughton smith et al. (1965). it begins by placing all observations in a
single cluster g. it then chooses that observation whose average dissimi-
larity from all the other observations is largest. this observation forms the
   rst member of a second cluster h. at each successive step that observation
in g whose average distance from those in h, minus that for the remaining
observations in g is largest, is transferred to h. this continues until the
corresponding di   erence in averages becomes negative. that is, there are
no longer any observations in g that are, on average, closer to those in
h. the result is a split of the original cluster into two daughter clusters,

14.3 cluster analysis

527

figure 14.14. dna microarray data: average linkage hierarchical id91
has been applied independently to the rows (genes) and columns (samples), de-
termining the ordering of the rows and columns (see text). the colors range from
bright green (negative, under-expressed) to bright red (positive, over-expressed).

528

14. unsupervised learning

the observations transferred to h, and those remaining in g. these two
clusters represent the second level of the hierarchy. each successive level
is produced by applying this splitting procedure to one of the clusters at
the previous level. kaufman and rousseeuw (1990) suggest choosing the
cluster at each level with the largest diameter (14.44) for splitting. an al-
ternative would be to choose the one with the largest average dissimilarity
among its members

  dg =

1
ng

2xi   gxi      g

dii    .

the recursive splitting continues until all clusters either become singletons
or all members of each one have zero dissimilarity from one another.

14.4 self-organizing maps

this method can be viewed as a constrained version of id116 id91,
in which the prototypes are encouraged to lie in a one- or two-dimensional
manifold in the feature space. the resulting manifold is also referred to
as a constrained topological map, since the original high-dimensional obser-
vations can be mapped down onto the two-dimensional coordinate system.
the original som algorithm was online   observations are processed one at
a time   and later a batch version was proposed. the technique also bears
a close relationship to principal curves and surfaces, which are discussed in
the next section.

we consider a som with a two-dimensional rectangular grid of k proto-
types mj     irp (other choices, such as hexagonal grids, can also be used).
each of the k prototypes are parametrized with respect to an integer
coordinate pair    j     q1    q2. here q1 = {1, 2, . . . , q1}, similarly q2, and
k = q1  q2. the mj are initialized, for example, to lie in the two-dimensional
principal component plane of the data (next section). we can think of the
prototypes as    buttons,       sewn    on the principal component plane in a
regular pattern. the som procedure tries to bend the plane so that the
buttons approximate the data points as well as possible. once the model is
   t, the observations can be mapped down onto the two-dimensional grid.
the observations xi are processed one at a time. we    nd the closest
prototype mj to xi in euclidean distance in irp, and then for all neighbors
mk of mj, move mk toward xi via the update

mk     mk +   (xi     mk).

(14.46)

the    neighbors    of mj are de   ned to be all mk such that the distance
between    j and    k is small. the simplest approach uses euclidean distance,
and    small    is determined by a threshold r. this neighborhood always
includes the closest prototype mj itself.

14.4 self-organizing maps

529

notice that distance is de   ned in the space q1  q2 of integer topological
coordinates of the prototypes, rather than in the feature space irp. the
e   ect of the update (14.46) is to move the prototypes closer to the data,
but also to maintain a smooth two-dimensional spatial relationship between
the prototypes.

the performance of the som algorithm depends on the learning rate
   and the distance threshold r. typically    is decreased from say 1.0 to
0.0 over a few thousand iterations (one per observation). similarly r is
decreased linearly from starting value r to 1 over a few thousand iterations.
we illustrate a method for choosing r in the example below.

we have described the simplest version of the som. more sophisticated

versions modify the update step according to distance:

mk     mk +   h(k   j        kk)(xi     mk),

(14.47)

where the neighborhood function h gives more weight to prototypes mk with
indices    k closer to    j than to those further away.

if we take the distance r small enough so that each neighborhood contains
only one point, then the spatial connection between prototypes is lost. in
that case one can show that the som algorithm is an online version of
id116 id91, and eventually stabilizes at one of the local minima
found by id116. since the som is a constrained version of id116
id91, it is important to check whether the constraint is reasonable
in any given problem. one can do this by computing the reconstruction
error kx     mjk2, summed over observations, for both methods. this will
necessarily be smaller for id116, but should not be much smaller if the
som is a reasonable approximation.

as an illustrative example, we generated 90 data points in three dimen-
sions, near the surface of a half sphere of radius 1. the points were in each
of three clusters   red, green, and blue   located near (0, 1, 0), (0, 0, 1) and
(1, 0, 0). the data are shown in figure 14.15

by design, the red cluster was much tighter than the green or blue ones.
(full details of the data generation are given in exercise 14.5.) a 5   5 grid
of prototypes was used, with initial grid size r = 2; this meant that about
a third of the prototypes were initially in each neighborhood. we did a
total of 40 passes through the dataset of 90 observations, and let r and   
decrease linearly over the 3600 iterations.

in figure 14.16 the prototypes are indicated by circles, and the points
that project to each prototype are plotted randomly within the correspond-
ing circle. the left panel shows the initial con   guration, while the right
panel shows the    nal one. the algorithm has succeeded in separating the
clusters; however, the separation of the red cluster indicates that the man-
ifold has folded back on itself (see figure 14.17). since the distances in the
two-dimensional display are not used, there is little indication in the som
projection that the red cluster is tighter than the others.

530

14. unsupervised learning

1.5

1

0.5

0

   0.5

   1
1.5

1

0.5

0

   0.5

0

   0.5

   1

   1

1.5

1

0.5

figure 14.15. simulated data in three classes, near the surface of a half   
sphere.

5

4

3

2

1

   

   

   

   

   
      
   
   
       
   
   
   

   
   
   
   
   
   

   
   
       
      
   
   
      
   
   
   
   
      
   
      
   
       
      

1

2

   

   
   

   
   
      
   
   
   
   
   
   
   
   
   
   
   
   

   

   
   
   

3

   
   

   
   

   

   

   

   
   
   
   

   

   
   
   
   
      
   
   

   
   

   

5

4

3

2

1

   
      
   
   
         
   
   
       
   
   
   

   

   
   
   
   
   
   

   

   

   
   
      
      

   

       
   

      
   
   

   

   

   
   
   

   

   

   

   

      
   
   
   
   

   

   
   
   
   

4

5

1

2

3

   
   
   

   
   
   

   
   

   
   
   

4

   
   
      
   
   
       
   
   

   
   
   

   
   

   

      
   
   
   

5

figure 14.16. self-organizing map applied to half-sphere data example. left
panel is the initial con   guration, right panel the    nal one. the 5    5 grid of
prototypes are indicated by circles, and the points that project to each prototype
are plotted randomly within the corresponding circle.

14.4 self-organizing maps

531

figure 14.17. wiremesh representation of the    tted som model in ir3. the
lines represent the horizontal and vertical edges of the topological lattice. the
double lines indicate that the surface was folded diagonally back on itself in order
to model the red points. the cluster members have been jittered to indicate their
color, and the purple points are the node centers.

figure 14.18 shows the reconstruction error, equal to the total sum of
squares of each data point around its prototype. for comparison we carried
out a id116 id91 with 25 centroids, and indicate its reconstruction
error by the horizontal line on the graph. we see that the som signi   cantly
decreases the error, nearly to the level of the id116 solution. this pro-
vides evidence that the two-dimensional constraint used by the som is
reasonable for this particular dataset.

in the batch version of the som, we update each mj via

mj = p wkxkp wk

.

(14.48)

the sum is over points xk that mapped (i.e., were closest to) neighbors mk
of mj. the weight function may be rectangular, that is, equal to 1 for the
neighbors of mk, or may decrease smoothly with distance k   k      jk as before.
if the neighborhood size is chosen small enough so that it consists only
of mk, with rectangular weights, this reduces to the id116 id91
procedure described earlier. it can also be thought of as a discrete version
of principal curves and surfaces, described in section 14.5.

532

14. unsupervised learning

r
o
r
r

 

e
n
o

i
t
c
u
r
t
s
n
o
c
e
r

0
5

0
4

0
3

0
2

0
1

0

   

   

   

   

   

      

   

      
   
   
   
      
      
   
         
   
                                                                                    
   

0

500

1000

1500

2000

2500

iteration

figure 14.18. half-sphere data: reconstruction error for the som as a func-
tion of iteration. error for id116 id91 is indicated by the horizontal line.

example: document organization and retrieval

document retrieval has gained importance with the rapid development of
the internet and the web, and soms have proved to be useful for organiz-
ing and indexing large corpora. this example is taken from the websom
homepage http://websom.hut.fi/ (kohonen et al., 2000). figure 14.19 rep-
resents a som    t to 12,088 newsgroup comp.ai.neural-nets articles. the
labels are generated automatically by the websom software and provide
a guide as to the typical content of a node.

in applications such as this, the documents have to be preprocessed in
order to create a feature vector. a term-document matrix is created, where
each row represents a single document. the entries in each row are the
relative frequency of each of a prede   ned set of terms. these terms could
be a large set of dictionary entries (50,000 words), or an even larger set
of bigrams (word pairs), or subsets of these. these matrices are typically
very sparse, and so often some preprocessing is done to reduce the number
of features (columns). sometimes the svd (next section) is used to reduce
the matrix; kohonen et al. (2000) use a randomized variant thereof. these
reduced vectors are then the input to the som.

14.4 self-organizing maps

533

figure 14.19. heatmap representation of the som model    t to a corpus
of 12,088 newsgroup comp.ai.neural-nets contributions (courtesy websom
homepage). the lighter areas indicate higher-density areas. populated nodes are
automatically labeled according to typical content.

534

14. unsupervised learning

v1v1v1v1v1v1v1v1

   
   

   
   

   
   

ui1d1
ui1d1
ui1d1
ui1d1
ui1d1
ui1d1
ui1d1
ui1d1

   
   

xixixixixixixixi

   
   

   
   
   
   
   
   
   
   
   
   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

   
   

figure 14.20. the    rst linear principal component of a set of data. the line
minimizes the total squared distance from each point to its orthogonal projection
onto the line.

in this application the authors have developed a    zoom    feature, which
allows one to interact with the map in order to get more detail. the    nal
level of zooming retrieves the actual news articles, which can then be read.

14.5 principal components, curves and surfaces

principal components are discussed in sections 3.4.1, where they shed light
on the shrinkage mechanism of ridge regression. principal components are
a sequence of projections of the data, mutually uncorrelated and ordered
in variance. in the next section we present principal components as linear
manifolds approximating a set of n points xi     irp. we then present
some nonlinear generalizations in section 14.5.2. other recent proposals
for nonlinear approximating manifolds are discussed in section 14.9.

14.5.1 principal components
the principal components of a set of data in irp provide a sequence of best
linear approximations to that data, of all ranks q     p.
model for representing them

denote the observations by x1, x2, . . . , xn , and consider the rank-q linear

14.5 principal components, curves and surfaces

535

f (  ) =    + vq  ,

(14.49)

where    is a location vector in irp, vq is a p    q matrix with q orthogonal
unit vectors as columns, and    is a q vector of parameters. this is the
parametric representation of an a   ne hyperplane of rank q. figures 14.20
and 14.21 illustrate for q = 1 and q = 2, respectively. fitting such a model
to the data by least squares amounts to minimizing the reconstruction error

min

  ,{  i}, vq

nxi=1

kxi            vq  ik2.

(14.50)

we can partially optimize for    and the   i (exercise 14.7) to obtain

     =   x,
    i = vt

q (xi       x).

this leaves us to    nd the orthogonal matrix vq:

min
vq

nxi=1

||(xi       x)     vqvt

q (xi       x)||2.

(14.51)

(14.52)

(14.53)

for convenience we assume that   x = 0 (otherwise we simply replace the
observations by their centered versions   xi = xi       x). the p    p matrix
hq = vqvt
q is a projection matrix, and maps each point xi onto its rank-
q reconstruction hqxi, the orthogonal projection of xi onto the subspace
spanned by the columns of vq. the solution can be expressed as follows.
stack the (centered) observations into the rows of an n    p matrix x. we
construct the singular value decomposition of x:

x = udvt .

(14.54)

this is a standard decomposition in numerical analysis, and many algo-
rithms exist for its computation (golub and van loan, 1983, for example).
here u is an n    p orthogonal matrix (ut u = ip) whose columns uj are
called the left singular vectors; v is a p   p orthogonal matrix (vt v = ip)
with columns vj called the right singular vectors, and d is a p   p diagonal
matrix, with diagonal elements d1     d2                dp     0 known as the sin-
gular values. for each rank q, the solution vq to (14.53) consists of the    rst
q columns of v. the columns of ud are called the principal components
of x (see section 3.5.1). the n optimal     i in (14.52) are given by the    rst
q principal components (the n rows of the n    q matrix uqdq).
the one-dimensional principal component line in ir2 is illustrated in fig-
ure 14.20. for each data point xi, there is a closest point on the line, given
by ui1d1v1. here v1 is the direction of the line and     i = ui1d1 measures
distance along the line from the origin. similarly figure 14.21 shows the

536

14. unsupervised learning

t

n
e
n
o
p
m
o
c
 
l

i

a
p
c
n
i
r
p

 

d
n
o
c
e
s

0

.

1

5

.

0

0

.

0

5

.

0
   

0

.

1
   

   
   
   

   

   

   

   

   
   

   
   
       
   
   
   
   

   
   

   
   
   

   

   

   
   
   

   

   

   

   
   
   

   
   
   
      
   
   
   
   
          
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   

   

   

   
   

   

   
   
   
       
   
   
   
   

   

   

   

   

   

   

   

   
   

   

   
   
       

   1.0

   0.5

0.0

0.5
first principal component

1.0

figure 14.21. the best rank-two linear approximation to the half-sphere data.
the right panel shows the projected points with coordinates given by u2d2, the
   rst two principal components of the data.

two-dimensional principal component surface    t to the half-sphere data
(left panel). the right panel shows the projection of the data onto the
   rst two principal components. this projection was the basis for the initial
con   guration for the som method shown earlier. the procedure is quite
successful at separating the clusters. since the half-sphere is nonlinear, a
nonlinear projection will do a better job, and this is the topic of the next
section.

principal components have many other nice properties, for example, the
linear combination xv1 has the highest variance among all linear com-
binations of the features; xv2 has the highest variance among all linear
combinations satisfying v2 orthogonal to v1, and so on.

example: handwritten digits

principal components are a useful tool for dimension reduction and com-
pression. we illustrate this feature on the handwritten digits data described
in chapter 1. figure 14.22 shows a sample of 130 handwritten 3   s, each a
digitized 16    16 grayscale image, from a total of 658 such 3   s. we see
considerable variation in writing styles, character thickness and orienta-
tion. we consider these images as points xi in ir256, and compute their
principal components via the svd (14.54).

figure 14.23 shows the    rst two principal components of these data. for
each of these    rst two principal components ui1d1 and ui2d2, we computed
the 5%, 25%, 50%, 75% and 95% quantile points, and used them to de   ne
the rectangular grid superimposed on the plot. the circled points indicate

14.5 principal components, curves and surfaces

537

figure 14.22. a sample of 130 handwritten 3   s shows a variety of writing
styles.

those images close to the vertices of the grid, where the distance measure
focuses mainly on these projected coordinates, but gives some weight to the
components in the orthogonal subspace. the right plot shows the images
corresponding to these circled points. this allows us to visualize the nature
of the    rst two principal components. we see that the v1 (horizontal move-
ment) mainly accounts for the lengthening of the lower tail of the three,
while v2 (vertical movement) accounts for character thickness. in terms of
the parametrized model (14.49), this two-component model has the form

  f (  ) =   x +   1v1 +   2v2

=

+   1   

+   2   

.

(14.55)

here we have displayed the    rst two principal component directions, v1
and v2, as images. although there are a possible 256 principal components,
approximately 50 account for 90% of the variation in the threes, 12 ac-
count for 63%. figure 14.24 compares the singular values to those obtained
for equivalent uncorrelated data, obtained by randomly scrambling each
column of x. the pixels in a digitized image are inherently correlated,
and since these are all the same digit the correlations are even stronger.

538

14. unsupervised learning

   

   
   

   

   

   

   
   
   

   
   

   
   
   
   

   
   
o

   
   
   
   

   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   

   
   
   
   

   
   
   

   
   

   

   
   

   

   
   

   

   

   

   
   
   

   
o

   

   
   
   
   
   
   
   
   
   

   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   
   
   
   
   

   
   
   

   
o

   

   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
      
   
   
   
   

   
   
   
o
   
   
o
   

   
   

   

   

   

   
   
   
o

   
   
   
   

   
   

   

   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   
   
o
   

   
   
   
   
   
   
   

   
   

   
   

   

   

   

   

   
   
   

   
   
   

   

   

   

   

   
   

   

   

   

   

   
   

   

   

   

   

   
o
   
   

   

   

   

   

   
   
   
   
   
   
   
   

   

   
   

   

   

   
   
   
   
   
o
   
   
   
   
   
   
   
       
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   

   

   
   
   
   

   
   
o
   

   

   

   

   

   

   
   

   
   
   

   
   

   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
   
o

   
   

   

   

   
   

   
   

   
   
   

   
   
   
   

   

t

n
e
n
o
p
m
o
c

 
l

i

a
p
c
n
i
r

 

p
d
n
o
c
e
s

5

0

5
-

-6

-4

-2

0

2

4

6

8

first principal component

figure 14.23. (left panel:) the    rst two principal components of the hand-
written threes. the circled points are the closest projected images to the vertices
of a grid, de   ned by the marginal quantiles of the principal components. (right
panel:) the images corresponding to the circled points. these show the nature of
the    rst two principal components.

   

   
      
   
      

s
e
u
a
v

l

l

 
r
a
u
g
n
s

i

0
8

0
6

0
4

0
2

0

   
   

real trace
randomized trace

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                

0

50

100

150

200

250

dimension

figure 14.24. the 256 singular values for the digitized threes, compared to
those for a randomized version of the data (each column of x was scrambled).

14.5 principal components, curves and surfaces

539

a relatively small subset of the principal components serve as excellent
lower-dimensional features for representing the high-dimensional data.

example: procrustes transformations and shape averaging

figure 14.25. (left panel:) two di   erent digitized handwritten ss, each rep-
resented by 96 corresponding points in ir2. the green s has been deliberately
rotated and translated for visual e   ect. (right panel:) a procrustes transforma-
tion applies a translation and rotation to best match up the two set of points.

figure 14.25 represents two sets of points, the orange and green, in the
same plot. in this instance these points represent two digitized versions
of a handwritten s, extracted from the signature of a subject    suresh.   
figure 14.26 shows the entire signatures from which these were extracted
(third and fourth panels). the signatures are recorded dynamically using
touch-screen devices, familiar sights in modern supermarkets. there are
n = 96 points representing each s, which we denote by the n    2 matrices
x1 and x2. there is a correspondence between the points   the ith rows
of x1 and x2 are meant to represent the same positions along the two s   s.
in the language of morphometrics, these points represent landmarks on
the two objects. how one    nds such corresponding landmarks is in general
di   cult and subject speci   c. in this particular case we used dynamic time
warping of the speed signal along each signature (hastie et al., 1992), but
will not go into details here.

in the right panel we have applied a translation and rotation to the green
points so as best to match the orange   a so-called procrustes3 transforma-
tion (mardia et al., 1979, for example).

consider the problem

  ,r ||x2     (x1r + 1  t )||f ,
min

(14.56)

3procrustes was an african bandit in greek mythology, who stretched or squashed

his visitors to    t his iron bed (eventually killing them).

540

14. unsupervised learning

f = trace(xt x) is the squared frobenius matrix norm.

with x1 and x2 both n    p matrices of corresponding points, r an or-
thonormal p    p matrix4, and    a p-vector of location coordinates. here
||x||2
let   x1 and   x2 be the column mean vectors of the matrices, and   x1 and
  x2 be the versions of these matrices with the means removed. consider
the svd   xt
  x2 = udvt . then the solution to (14.56) is given by (exer-
1
cise 14.8)

  r = uvt
     =   x2       r  x1,

(14.57)

and the minimal distances is referred to as the procrustes distance. from
the form of the solution, we can center each matrix at its column centroid,
and then ignore location completely. hereafter we assume this is the case.

the procrustes distance with scaling solves a slightly more general

problem,

  ,r ||x2       x1r||f ,
min

(14.58)

where    > 0 is a positive scalar. the solution for r is as before, with
     = trace(d)/||x1||2
f .
of l shapes, which solves the problem

related to procrustes distance is the procrustes average of a collection

min
{r   }l

1 ,m

lx   =1

||x   r        m||2
f ;

(14.59)

that is,    nd the shape m closest in average squared procrustes distance to
all the shapes. this is solved by a simple alternating algorithm:

0. initialize m = x1 (for example).

1. solve the l procrustes rotation problems with m    xed, yielding

        x   r   .
x   
lpl
2. let m     1

   =1 x   
   .

steps 1. and 2. are repeated until the criterion (14.59) converges.

figure 14.26 shows a simple example with three shapes. note that we can
only expect a solution up to a rotation; alternatively, we can impose a
constraint, such as that m be upper-triangular, to force uniqueness. one
can easily incorporate scaling in the de   nition (14.59); see exercise 14.9.

most generally we can de   ne the a   ne-invariant average of a set of

shapes via

4to simplify matters, we consider only orthogonal matrices which include re   ections
as well as rotations [the o(p) group]; although re   ections are unlikely here, these methods
can be restricted further to allow only rotations [so(p) group].

14.5 principal components, curves and surfaces

541

figure 14.26. the procrustes average of three versions of the leading s in
suresh   s signatures. the left panel shows the preshape average, with each of the
shapes x   
    in preshape space superimposed. the right three panels map the pre-
shape m separately to match each of the original s   s.

min
{a   }l

1 ,m

||x   a        m||2
f ,

(14.60)

lx   =1

where the a    are any p    p nonsingular matrices. here we require a stan-
dardization, such as mt m = i, to avoid a trivial solution. the solution is
attractive, and can be computed without iteration (exercise 14.10):

1. let h    = x   (xt

    x   )   1xt

    be the rank-p projection matrix de   ned

by x   .

1

2. m is the n  p matrix formed from the p largest eigenvectors of   h =
lpl

   =1 h   .

14.5.2 principal curves and surfaces

principal curves generalize the principal component line, providing a smooth
one-dimensional curved approximation to a set of data points in irp. a prin-
cipal surface is more general, providing a curved manifold approximation
of dimension 2 or more.

we will    rst de   ne principal curves for random variables x     irp, and
then move to the    nite data case. let f (  ) be a parameterized smooth
curve in irp. hence f (  ) is a vector function with p coordinates, each a
smooth function of the single parameter   . the parameter    can be chosen,
for example, to be arc-length along the curve from some    xed origin. for
each data value x, let   f (x) de   ne the closest point on the curve to x. then
f (  ) is called a principal curve for the distribution of the random vector
x if

f (  ) = e(x|  f (x) =   ).

(14.61)

this says f (  ) is the average of all data points that project to it, that is, the
points for which it is    responsible.    this is also known as a self-consistency
property. although in practice, continuous multivariate distributes have
in   nitely many principal curves (duchamp and stuetzle, 1996), we are

542

14. unsupervised learning

   
   

   
   

   
   

   
   

   
   
   
   

   
   

   
   

   
   

   
   

   
   
   
   
   
   

       
       
   
   
   
   

   
   

   
   

   
   

   
   

....
.....

   
   

   
   

   
   

   
   

   
   

   
   

f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]
f (  ) = [f1(  ), f2(  )]

   
   

   
   

   
   

   
   

figure 14.27. the principal curve of a set of data. each point on the curve
is the average of all data points that project there.

interested mainly in the smooth ones. a principal curve is illustrated in
figure 14.27.

principal points are an interesting related concept. consider a set of k
prototypes and for each point x in the support of a distribution, identify
the closest prototype, that is, the prototype that is responsible for it. this
induces a partition of the feature space into so-called voronoi regions. the
set of k points that minimize the expected distance from x to its prototype
are called the principal points of the distribution. each principal point is
self-consistent, in that it equals the mean of x in its voronoi region. for
example, with k = 1, the principal point of a circular normal distribution is
the mean vector; with k = 2 they are a pair of points symmetrically placed
on a ray through the mean vector. principal points are the distributional
analogs of centroids found by id116 id91. principal curves can be
viewed as k =     principal points, but constrained to lie on a smooth curve,
in a similar way that a som constrains id116 cluster centers to fall on
a smooth manifold.

to    nd a principal curve f (  ) of a distribution, we consider its coordinate
functions f (  ) = [f1(  ), f2(  ), . . . , fp(  )] and let x t = (x1, x2, . . . , xp).
consider the following alternating steps:

(a)
(b)

  fj(  )     e(xj|  (x) =   ); j = 1, 2, . . . , p,
    f (x)     argmin      ||x       f (     )||2.

(14.62)

the    rst equation    xes    and enforces the self-consistency requirement
(14.61). the second equation    xes the curve and    nds the closest point on

14.5 principal components, curves and surfaces

543

2

.

0

1

.

0

0

.

0

2
  

1

.

0
-

2

.

0
-

   
   
   

   

   

   

   

   
   

   

   
   
   

   

   
   
   
   
   

   
   
   

   

   

   
   
   
   
   
      
   
   
   
      
   
   
   
   
   
   
   
   
   
   
   
   

   
   
   
   

   
   

   
   

   

   

   
   

   

   

   

   
   
   

-0.1

0.0
  1

   

   

   

   

   
   

   
   

   
0.1

   

   

   
   

   

   

   

   
   
   

   
   
   
   

   

0.2

figure 14.28. principal surface    t to half-sphere data. (left panel:)    tted
two-dimensional surface. (right panel:) projections of data points onto the sur-
face, resulting in coordinates     1,     2.

the curve to each data point. with    nite data, the principal curve algorithm
starts with the linear principal component, and iterates the two steps in
(14.62) until convergence. a scatterplot smoother is used to estimate the
conditional expectations in step (a) by smoothing each xj as a function of
the arc-length     (x), and the projection in (b) is done for each of the ob-
served data points. proving convergence in general is di   cult, but one can
show that if a linear least squares    t is used for the scatterplot smoothing,
then the procedure converges to the    rst linear principal component, and
is equivalent to the power method for    nding the largest eigenvector of a
matrix.

principal surfaces have exactly the same form as principal curves, but
are of higher dimension. the mostly commonly used is the two-dimensional
principal surface, with coordinate functions

f (  1,   2) = [f1(  1,   2), . . . , fp(  1,   2)].

the estimates in step (a) above are obtained from two-dimensional surface
smoothers. principal surfaces of dimension greater than two are rarely used,
since the visualization aspect is less attractive, as is smoothing in high
dimensions.

figure 14.28 shows the result of a principal surface    t to the half-sphere
data. plotted are the data points as a function of the estimated nonlinear
coordinates     1(xi),     2(xi). the class separation is evident.

principal surfaces are very similar to self-organizing maps. if we use a
kernel surface smoother to estimate each coordinate function fj(  1,   2),
this has the same form as the batch version of soms (14.48). the som
weights wk are just the weights in the kernel. there is a di   erence, however:

544

14. unsupervised learning

the principal surface estimates a separate prototype f (  1(xi),   2(xi)) for
each data point xi, while the som shares a smaller number of prototypes
for all data points. as a result, the som and principal surface will agree
only as the number of som prototypes grows very large.

there also is a conceptual di   erence between the two. principal sur-
faces provide a smooth parameterization of the entire manifold in terms
of its coordinate functions, while soms are discrete and produce only the
estimated prototypes for approximating the data. the smooth parameter-
ization in principal surfaces preserves distance locally: in figure 14.28 it
reveals that the red cluster is tighter than the green or blue clusters. in
simple examples the estimates coordinate functions themselves can be in-
formative: see exercise 14.13.

14.5.3 spectral id91

traditional id91 methods like id116 use a spherical or elliptical
metric to group data points. hence they will not work well when the clus-
ters are non-convex, such as the concentric circles in the top left panel of
figure 14.29. spectral id91 is a generalization of standard id91
methods, and is designed for these situations. it has close connections with
the local multidimensional-scaling techniques (section 14.9) that generalize
mds.

the starting point is a n    n matrix of pairwise similarities sii        0 be-
tween all observation pairs. we represent the observations in an undirected
similarity graph g = hv, ei. the n vertices vi represent the observations,
and pairs of vertices are connected by an edge if their similarity is positive
(or exceeds some threshold). the edges are weighted by the sii    . id91
is now rephrased as a graph-partition problem, where we identify connected
components with clusters. we wish to partition the graph, such that edges
between di   erent groups have low weight, and within a group have high
weight. the idea in spectral id91 is to construct similarity graphs that
represent the local neighborhood relationships between observations.

to make things more concrete, consider a set of n points xi     irp, and let
dii    be the euclidean distance between xi and xi   . we will use as similarity
matrix the radial-kernel gram matrix; that is, sii    = exp(   d2
ii    /c), where
c > 0 is a scale parameter.

there are many ways to de   ne a similarity matrix and its associated
similarity graph that re   ect local behavior. the most popular is the mutual
k-nearest-neighbor graph. de   ne nk to be the symmetric set of nearby
pairs of points; speci   cally a pair (i, i   ) is in nk if point i is among the
k-nearest neighbors of i   , or vice-versa. then we connect all symmetric
nearest neighbors, and give them edge weight wii    = sii    ; otherwise the
edge weight is zero. equivalently we set to zero all the pairwise similarities
not in nk, and draw the graph for this modi   ed similarity matrix.

14.5 principal components, curves and surfaces

545

alternatively, a fully connected graph includes all pairwise edges with
weights wii    = sii    , and the local behavior is controlled by the scale param-
eter c.

the adjacency matrix. the degree of vertex i is gi = pi    wii    , the sum of

the matrix of edge weights w = {wii   } from a similarity graph is called
the weights of the edges connected to it. let g be a diagonal matrix with
diagonal elements gi.

finally, the graph laplacian is de   ned by

l = g     w

(14.63)

this is called the unnormalized graph laplacian; a number of normalized
versions have been proposed   these standardize the laplacian with respect
to the node degrees gi, for example,   l = i     g   1w.
spectral id91    nds the m eigenvectors zn   m corresponding to the
m smallest eigenvalues of l (ignoring the trivial constant eigenvector).
using a standard method like id116, we then cluster the rows of z to
yield a id91 of the original data points.

an example is presented in figure 14.29. the top left panel shows 450
simulated data points in three circular clusters indicated by the colors. k-
means id91 would clearly have di   culty identifying the outer clusters.
we applied spectral id91 using a 10-nearest neighbor similarity graph,
and display the eigenvector corresponding to the second and third smallest
eigenvalue of the graph laplacian in the lower left. the 15 smallest eigen-
values are shown in the top right panel. the two eigenvectors shown have
identi   ed the three clusters, and a scatterplot of the rows of the eigenvector
matrix y in the bottom right clearly separates the clusters. a procedure
such as id116 id91 applied to these transformed points would eas-
ily identify the three groups.

why does spectral id91 work? for any vector f we have

f t lf =

=

gif 2

i    

nxi=1
nxi=1

1
2

nxi   =1

fifi    wii   

nxi   =1

nxi=1
wii    (fi     fi    )2.

(14.64)

formula 14.64 suggests that a small value of f t lf will be achieved if pairs
of points with large adjacencies have coordinates fi and fi    close together.
since 1t l1 = 0 for any graph, the constant vector is a trivial eigenvector
with eigenvalue zero. not so obvious is the fact that if the graph is con-
nected5, it is the only zero eigenvector (exercise 14.21). generalizing this
argument, it is easy to show that for a graph with m connected components,

5a graph is connected if any two nodes can be reached via a path of connected nodes.

546

14. unsupervised learning

4

2

2
x

0

2
   

4
   

5
0

.

0

 

5
0

.

0
   

5
0

.

0

 

5
0

.

0
   

t
s
e

l
l

a
m
s
d
n
2

 

t
s
e

l
l

a
m
s
d
r
3

 

l

e
u
a
v
n
e
g
e

i

5
.
0

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

   4

   2

0

x1

2

4

1

3

5

10

15

number

eigenvectors

spectral id91

r
o

t
c
e
v
n
e
g
e

i

 
t
s
e

l
l

 

a
m
s
d
r
i
h
t

6
0

.

0

2
0

.

0

2
0

.

0
   

6
0

.

0
   

0

100

200

300

400

   0.04

   0.02

0.00

0.02

index

second smallest eigenvector

figure 14.29. toy example illustrating spectral id91. data in top left are
450 points falling in three concentric clusters of 150 points each. the points are
uniformly distributed in angle, with radius 1, 2.8 and 5 in the three groups, and
gaussian noise with standard deviation 0.25 added to each point. using a k = 10
nearest-neighbor similarity graph, the eigenvector corresponding to the second and
third smallest eigenvalues of l are shown in the bottom left; the smallest eigen-
vector is constant. the data points are colored in the same way as in the top left.
the 15 smallest eigenvalues are shown in the top right panel. the coordinates of
the 2nd and 3rd eigenvectors (the 450 rows of z) are plotted in the bottom right
panel. spectral id91 does standard (e.g., id116) id91 of these points
and will easily recover the three original clusters.

14.5 principal components, curves and surfaces

547

the nodes can be reordered so that l is block diagonal with a block for each
connected component. then l has m eigenvectors of eigenvalue zero, and
the eigenspace of eigenvalue zero is spanned by the indicator vectors of the
connected components. in practice one has strong and weak connections,
so zero eigenvalues are approximated by small eigenvalues.

spectral id91 is an interesting approach for    nding non-convex clus-
ters. when a normalized graph laplacian is used, there is another way to
view this method. de   ning p = g   1w, we consider a random walk on
the graph with transition id203 matrix p. then spectral id91
yields groups of nodes such that the random walk seldom transitions from
one group to another.

there are a number of issues that one must deal with in applying spec-
tral id91 in practice. we must choose the type of similarity graph   eg.
fully connected or nearest neighbors, and associated parameters such as the
number of nearest of neighbors k or the scale parameter of the kernel c. we
must also choose the number of eigenvectors to extract from l and    nally,
as with all id91 methods, the number of clusters. in the toy example
of figure 14.29 we obtained good results for k     [5, 200], the value 200 cor-
responding to a fully connected graph. with k < 5 the results deteriorated.
looking at the top-right panel of figure 14.29, we see no strong separation
between the smallest three eigenvalues and the rest. hence it is not clear
how many eigenvectors to select.

14.5.4 kernel principal components

spectral id91 is related to kernel principal components, a non-linear
version of linear principal components. standard linear principal compo-
nents (pca) are obtained from the eigenvectors of the covariance matrix,
and give directions in which the data have maximal variance. kernel pca
(sch  olkopf et al., 1999) expand the scope of pca, mimicking what we would
obtain if we were to expand the features by non-linear transformations, and
then apply pca in this transformed feature space.

we show in section 18.5.2 that the principal components variables z of
a data matrix x can be computed from the inner-product (gram) matrix
k = xxt . in detail, we compute the eigen-decomposition of the double-
centered version of the gram matrix

with m = 11t /n , and then z = ud. exercise 18.15 shows how to com-
pute the projections of new observations in this space.

ek = (i     m)k(i     m) = ud2ut ,

(14.65)

kernel pca simply mimics this procedure, interpreting the kernel ma-
trix k = {k(xi, xi    )} as an inner-product matrix of the implicit fea-
tures h  (xi),   (xi    )i and    nding its eigenvectors. the elements of the mth
component zm (mth column of z) can be written (up to centering) as

j=1   jmk(xi, xj), where   jm = ujm/dm (exercise 14.16).

zim =pn

548

14. unsupervised learning

we can gain more insight into kernel pca by viewing the zm as sam-
ple evaluations of principal component functions gm     hk, with hk the
reproducing kernel hilbert space generated by k (see section 5.8.1). the
   rst principal component function g1 solves

max
g1   hk

vart g1(x) subject to ||g1||hk = 1

(14.66)

pn

here vart refers to the sample variance over training data t . the norm
constraint ||g1||hk = 1 controls the size and roughness of the function g1,
as dictated by the kernel k. as in the regression case it can be shown that
the solution to (14.66) is    nite dimensional with representation g1(x) =
j=1 cjk(x, xj). exercise 14.17 shows that the solution is de   ned by   cj =
  j1, j = 1, . . . , n above. the second principal component function is de-
   ned in a similar way, with the additional constraint that hg1, g2ihk = 0,
and so on.6
sch  olkopf et al. (1999) demonstrate the use of kernel principal compo-
nents as features for handwritten-digit classi   cation, and show that they
can improve the performance of a classi   er when these are used instead of
linear principal components.

note that if we use the radial kernel

k(x, x   ) = exp(   kx     x   k2/c),

(14.67)

then the kernel matrix k has the same form as the similarity matrix s in
spectral id91. the matrix of edge weights w is a localized version of
k, setting to zero all similarities for pairs of points that are not nearest
neighbors.

kernel pca    nds the eigenvectors corresponding to the largest eigenval-

ues of ek; this is equivalent to    nding the eigenvectors corresponding to the

smallest eigenvalues of

(14.68)

this is almost the same as the laplacian (14.63), the di   erences being the

diagonal.

centering of ek and the fact that g has the degrees of the nodes along the

figure 14.30 examines the performance of kernel principal components
in the toy example of figure 14.29. in the upper left panel we used the ra-
dial kernel with c = 2, the same value that was used in spectral id91.
this does not separate the groups, but with c = 10 (upper right panel), the
   rst component separates the groups well. in the lower-left panel we ap-
plied kernel pca using the nearest-neighbor radial kernel w from spectral
id91. in the lower right panel we use the kernel matrix itself as the

i     ek.

6this section bene   ted from helpful discussions with jonathan taylor.

14.5 principal components, curves and surfaces

549

radial kernel (c=2)

radial kernel (c=10)

r
o
t
c
e
v
n
e
g
e

i

 
t
s
e
g
r
a
l
 
d
n
o
c
e
s

5
0
.
0

0
0
.
0

5
0

.

0
   

   0.10

   0.06

   0.02

0.02

   0.06

   0.02

0.02

0.06

first largest eigenvector

first largest eigenvector

nn radial kernel (c=2)

radial kernel laplacian (c=2)

r
o

t
c
e
v
n
e
g
e

i

 
t
s
e

l
l

 

a
m
s
d
r
i
h
t

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

0
1

.

0
   

r
o
t
c
e
v
n
e
g
e

i

 
t
s
e
g
r
a
l
 
d
n
o
c
e
s

r
o

t
c
e
v
n
e
g
e

i

 
t
s
e
g
r
a
l

 

d
n
o
c
e
s

0
1
.
0

5
0
.
0

0
0
.
0

5
0

.

0
   

0
1

.

0
   

2

.

0

1

.

0

0

.

0

1

.

0
   

2

.

0
   

0.00

0.05

0.10

0.15

   0.05

0.00

0.05

0.10

0.15

first largest eigenvector

second smallest eigenvector

figure 14.30. kernel principal components applied to the toy example of fig-
ure 14.29, using di   erent kernels. (top left:) radial kernel (14.67) with c = 2.
(top right:) radial kernel with c = 10. (bottom left): nearest neighbor radial ker-
nel w from spectral id91. (bottom right:) spectral id91 with laplacian
constructed from the radial kernel.

550

14. unsupervised learning

similarity matrix for constructing the laplacian (14.63) in spectral cluster-
ing. in neither case do the projections separate the two groups. adjusting
c did not help either.

in this toy example, we see that kernel pca is quite sensitive to the scale
and nature of the kernel. we also see that the nearest-neighbor truncation
of the kernel is important for the success of spectral id91.

14.5.5 sparse principal components

we often interpret principal components by examining the direction vectors
vj, also known as loadings, to see which variables play a role. we did this
with the image loadings in (14.55). often this interpretation is made easier
if the loadings are sparse. in this section we brie   y discuss some methods
for deriving principal components with sparse loadings. they are all based
on lasso (l1) penalties.

we start with an n    p data matrix x, with centered columns. the
proposed methods focus on either the maximum-variance property of prin-
cipal components, or the minimum reconstruction error. the scotlass
procedure of joli   e et al. (2003) takes the    rst approach, by solving

max vt (xt x)v, subject topp

j=1 |vj|     t, vt v = 1.

(14.69)

the absolute-value constraint encourages some of the loadings to be zero
and hence v to be sparse. further sparse principal components are found
in the same way, by forcing the kth component to be orthogonal to the
   rst k     1 components. unfortunately this problem is not convex and the
computations are di   cult.
zou et al. (2006) start instead with the regression/reconstruction prop-
erty of pca, similar to the approach in section 14.5.1. let xi be the ith row
of x. for a single component, their sparse principal component technique
solves

min
  ,v

nxi=1

||xi       vt xi||2

2 +   ||v||2
2 +   1||v||1
subject to ||  ||2 = 1.

(14.70)

let   s examine this formulation in more detail.

    if both    and   1 are zero and n > p, it is easy to show that v =   

and is the largest principal component direction.

    when p     n the solution is not necessarily unique unless    > 0. for
any    > 0 and   1 = 0 the solution for v is proportional to the largest
principal component direction.

    the second penalty on v encourages sparseness of the loadings.

14.5 principal components, curves and surfaces

551

walking speed

verbal fluency

principal components

sparse principal components

figure 14.31. standard and sparse principal components from a study of
the corpus callosum variation. the shape variations corresponding to signi   cant
principal components (red curves) are overlaid on the mean cc shape (black
curves).

for multiple components, the sparse principal components procedures

minimizes

nxi=1

||xi       vt xi||2 +   

kxk=1

||vk||2

2 +

kxk=1

  1k||vk||1,

(14.71)

subject to   t    = ik. here v is a p    k matrix with columns vk and   
is also p    k.
criterion (14.71) is not jointly convex in v and   , but it is convex in
each parameter with the other parameter    xed7. minimization over v with
      xed is equivalent to k elastic net problems (section 18.4) and can be
done e   ciently. on the other hand, minimization over    with v    xed is a
version of the procrustes problem (14.56), and is solved by a simple svd
calculation (exercise 14.12). these steps are alternated until convergence.
figure 14.31 shows an example of sparse principal components analysis
using (14.71), taken from sj  ostrand et al. (2007). here the shape of the
mid-sagittal cross-section of the corpus callosum (cc) is related to various
clinical parameters in a study involving 569 elderly persons8. in this exam-

7note that the usual principal component criterion, for example (14.50), is not jointly
convex in the parameters either. nevertheless, the solution is well de   ned and an e   cient
algorithm is available.

8we thank rasmus larsen and karl sj  ostrand for suggesting this application, and

supplying us with the postscript    gures reproduced here.

552

14. unsupervised learning

figure 14.32. an example of a mid-saggital brain slice, with the corpus col-
losum annotated with landmarks.

ple pca is applied to shape data, and is a popular tool in morphometrics.
for such applications, a number of landmarks are identi   ed along the cir-
cumference of the shape; an example is given in figure 14.32. these are
aligned by procrustes analysis to allow for rotations, and in this case scal-
ing as well (see section 14.5.1). the features used for pca are the sequence
of coordinate pairs for each landmark, unpacked into a single vector.

in this analysis, both standard and sparse principal components were
computed, and components that were signi   cantly associated with various
clinical parameters were identi   ed. in the    gure, the shape variations cor-
responding to signi   cant principal components (red curves) are overlaid on
the mean cc shape (black curves). low walking speed relates to ccs that
are thinner (displaying atrophy) in regions connecting the motor control
and cognitive centers of the brain. low verbal    uency relates to ccs that
are thinner in regions connecting auditory/visual/cognitive centers. the
sparse principal components procedure gives a more parsimonious, and po-
tentially more informative picture of the important di   erences.

14.6 non-negative id105

553

14.6 non-negative id105

non-negative id105 (lee and seung, 1999) is a recent al-
ternative approach to principal components analysis, in which the data
and components are assumed to be non-negative. it is useful for modeling
non-negative data such as images.

the n    p data matrix x is approximated by

x     wh

(14.72)

where w is n    r and h is r    p, r     max(n, p). we assume that
xij, wik, hkj     0.

the matrices w and h are found by maximizing

l(w, h) =

nxi=1

pxj=1

[xij log(wh)ij     (wh)ij].

(14.73)

this is the log-likelihood from a model in which xij has a poisson dis-

tribution with mean (wh)ij   quite reasonable for positive data.

the following alternating algorithm (lee and seung, 2001) converges to

a local maximum of l(w, h):

wik     wikpp
hkj     hkjpn

j=1 hkjxij/(wh)ij

i=1 wikxij/(wh)ij

j=1 hkj

i=1 wik

pp
pn

(14.74)

this algorithm can be derived as a minorization procedure for maximizing
l(w, h) (exercise 14.23) and is also related to the iterative-proportional-
scaling algorithm for id148 (exercise 14.24).

figure 14.33 shows an example taken from lee and seung (1999)9, com-
paring non-negative id105 (nmf), vector quantization (vq,
equivalent to id116 id91) and principal components analysis (pca).
the three learning methods were applied to a database of n = 2, 429 fa-
cial images, each consisting of 19    19 pixels, resulting in a 2, 429    381
matrix x. as shown in the 7   7 array of montages (each a 19   19 image),
each method has learned a set of r = 49 basis images. positive values are
illustrated with black pixels and negative values with red pixels. a par-
ticular instance of a face, shown at top right, is approximated by a linear
superposition of basis images. the coe   cients of the linear superposition
are shown next to each montage, in a 7    7 array10, and the resulting su-
perpositions are shown to the right of the equality sign. the authors point

9we thank sebastian seung for providing this image.
10these 7    7 arrangements allow for a compact display, and have no structural

signi   cance.

554

14. unsupervised learning

out that unlike vq and pca, nmf learns to represent faces with a set of
basis images resembling parts of faces.

donoho and stodden (2004) point out a potentially serious problem with
non-negative id105. even in situations where x = wh holds
exactly, the decomposition may not be unique. figure 14.34 illustrates the
problem. the data points lie in p = 2 dimensions, and there is    open space   
between the data and the coordinate axes. we can choose the basis vectors
h1 and h2 anywhere in this open space, and represent each data point
exactly with a nonnegative linear combination of these vectors. this non-
uniqueness means that the solution found by the above algorithm depends
on the starting values, and it would seem to hamper the interpretability of
the factorization. despite this interpretational drawback, the non-negative
id105 and its applications has attracted a lot of interest.

14.6.1 archetypal analysis

this method, due to cutler and breiman (1994), approximates data points
by prototypes that are themselves linear combinations of data points. in
this sense it has a similar    avor to id116 id91. however, rather
than approximating each data point by a single nearby prototype, archety-
pal analysis approximates each data point by a convex combination of a
collection of prototypes. the use of a convex combination forces the proto-
types to lie on the convex hull of the data cloud. in this sense, the prototypes
are    pure,   , or    archetypal.   

as in (14.72), the n    p data matrix x is modeled as

x     wh

(14.75)

where w is n    r and h is r   p. we assume that wik     0 andpr

k=1 wik =
1    i. hence the n data points (rows of x) in p-dimensional space are
represented by convex combinations of the r archetypes (rows of h). we
also assume that

h = bx

(14.76)

where b is r    n with bki     0 andpn

i=1 bki = 1    k. thus the archetypes
themselves are convex combinations of the data points. using both (14.75)
and (14.76) we minimize

j(w, b) = ||x     wh||2
= ||x     wbx||2

(14.77)

over the weights w and b. this function is minimized in an alternating
fashion, with each separate minimization involving a id76.
the overall problem is not convex however, and so the algorithm converges
to a local minimum of the criterion.

14.6 non-negative id105

555

original

nmf

vq

pca

=

=

=

figure 14.33. non-negative id105 (nmf), vector quantization
(vq, equivalent to id116 id91) and principal components analysis (pca)
applied to a database of facial images. details are given in the text. unlike vq
and pca, nmf learns to represent faces with a set of basis images resembling
parts of faces.

  
(cid:13)
  
(cid:13)
  
(cid:13)
556

14. unsupervised learning

h1

h2

figure 14.34. non-uniqueness of the non-negative id105.
there are 11 data points in two dimensions. any choice of the basis vectors h1
and h2 in the open space between the coordinate axes and data, gives an exact
reconstruction of the data.

figure 14.35 shows an example with simulated data in two dimensions.
the top panel displays the results of archetypal analysis, while the bottom
panel shows the results from id116 id91. in order to best recon-
struct the data from convex combinations of the prototypes, it pays to
locate the prototypes on the convex hull of the data. this is seen in the top
panels of figure 14.35 and is the case in general, as proven by cutler and
breiman (1994). id116 id91, shown in the bottom panels, chooses
prototypes in the middle of the data cloud.

we can think of id116 id91 as a special case of the archetypal
model, in which each row of w has a single one and the rest of the entries
are zero.

notice also that the archetypal model (14.75) has the same general form
as the non-negative id105 model (14.72). however, the two
models are applied in di   erent settings, and have somewhat di   erent goals.
non-negative id105 aims to approximate the columns of the
data matrix x, and the main output of interest are the columns of w
representing the primary non-negative components in the data. archetypal
analysis focuses instead on the approximation of the rows of x using the
rows of h, which represent the archetypal data points. non-negative matrix
factorization also assumes that r     p. with r = p, we can get an exact
reconstruction simply choosing w to be the data x with columns scaled
so that they sum to 1. in contrast, archetypal analysis requires r     n ,
but allows r > p. in figure 14.35, for example, p = 2, n = 50 while
r = 2, 4 or 8. the additional constraint (14.76) implies that the archetypal
approximation will not be perfect, even if r > p.

figure 14.36 shows the results of archetypal analysis applied to the
database of 3   s displayed in figure 14.22. the three rows in figure 14.36
are the resulting archetypes from three runs, specifying two, three and four

14.7 independent component analysisand exploratory projection pursuit

557

2 prototypes

4 prototypes

8 prototypes

figure 14.35. archetypal analysis (top panels) and id116 id91 (bot-
tom panels) applied to 50 data points drawn from a bivariate gaussian distribu-
tion. the colored points show the positions of the prototypes in each case.

archetypes, respectively. as expected, the algorithm has produced extreme
3   s both in size and shape.

14.7

independent component analysis and
exploratory projection pursuit

multivariate data are often viewed as multiple indirect measurements aris-
ing from an underlying source, which typically cannot be directly measured.
examples include the following:

    educational and psychological tests use the answers to questionnaires
to measure the underlying intelligence and other mental abilities of
subjects.

    eeg brain scans measure the neuronal activity in various parts of
the brain indirectly via electromagnetic signals recorded at sensors
placed at various positions on the head.

    the trading prices of stocks change constantly over time, and re   ect
various unmeasured factors such as market con   dence, external in-

558

14. unsupervised learning

figure 14.36. archetypal analysis applied to the database of digitized 3   s. the
rows in the    gure show the resulting archetypes from three runs, specifying two,
three and four archetypes, respectively.

   uences, and other driving forces that may be hard to identify or
measure.

factor analysis is a classical technique developed in the statistical liter-
ature that aims to identify these latent sources. factor analysis models
are typically wed to gaussian distributions, which has to some extent hin-
dered their usefulness. more recently, independent component analysis has
emerged as a strong competitor to factor analysis, and as we will see, relies
on the non-gaussian nature of the underlying sources for its success.

14.7.1 latent variables and factor analysis

the id166 x = udvt (14.54) has a latent variable
representation. writing s =    n u and at = dvt /   n , we have x =
sat , and hence each of the columns of x is a linear combination of the
columns of s. now since u is orthogonal, and assuming as before that the
columns of x (and hence u) each have mean zero, this implies that the
columns of s have zero mean, are uncorrelated and have unit variance. in
terms of random variables, we can interpret the svd, or the corresponding
principal component analysis (pca) as an estimate of a latent variable
model

14.7 independent component analysis and exploratory projection pursuit

559

x1 = a11s1 + a12s2 +        + a1psp
x2 = a21s1 + a22s2 +        + a2psp

...

...

(14.78)

xp = ap1s1 + ap2s2 +        + appsp,

or simply x = as. the correlated xj are each represented as a linear
expansion in the uncorrelated, unit variance variables s   . this is not too
satisfactory, though, because given any orthogonal p    p matrix r, we can
write

x = as

= art rs
= a   s   ,

(14.79)

and cov(s   ) = r cov(s) rt = i. hence there are many such decom-
positions, and it is therefore impossible to identify any particular latent
variables as unique underlying sources. the svd decomposition does have
the property that any rank q < p truncated decomposition approximates
x in an optimal way.

the classical factor analysis model, developed primarily by researchers in
psychometrics, alleviates these problems to some extent; see, for example,
mardia et al. (1979). with q < p, a factor analysis model has the form

x1 = a11s1 +        + a1qsq +   1
x2 = a21s1 +        + a2qsq +   2

...

...

(14.80)

xp = ap1s1 +        + apqsq +   p,

or x = as +   . here s is a vector of q < p underlying latent variables or
factors, a is a p    q matrix of factor loadings, and the   j are uncorrelated
zero-mean disturbances. the idea is that the latent variables s    are com-
mon sources of variation amongst the xj, and account for their correlation
structure, while the uncorrelated   j are unique to each xj and pick up the
remaining unaccounted variation. typically the s    and the   j are modeled
as gaussian random variables, and the model is    t by maximum likelihood.
the parameters all reside in the covariance matrix

   = aat + d  ,

(14.81)

where d   = diag[var(  1), . . . , var(  p)]. the s    being gaussian and un-
correlated makes them statistically independent random variables. thus a
battery of educational test scores would be thought to be driven by the
independent underlying factors such as intelligence, drive and so on. the
columns of a are referred to as the factor loadings, and are used to name
and interpret the factors.

560

14. unsupervised learning

unfortunately the identi   ability issue (14.79) remains, since a and art
are equivalent in (14.81) for any q    q orthogonal r. this leaves a certain
subjectivity in the use of factor analysis, since the user can search for ro-
tated versions of the factors that are more easily interpretable. this aspect
has left many analysts skeptical of factor analysis, and may account for its
lack of popularity in contemporary statistics. although we will not go into
details here, the svd plays a key role in the estimation of (14.81). for ex-
ample, if the var(  j) are all assumed to be equal, the leading q components
of the svd identify the subspace determined by a.

because of the separate disturbances   j for each xj, factor analysis can
be seen to be modeling the correlation structure of the xj rather than the
covariance structure. this can be easily seen by standardizing the covari-
ance structure in (14.81) (exercise 14.14). this is an important distinction
between factor analysis and pca, although not central to the discussion
here. exercise 14.15 discusses a simple example where the solutions from
factor analysis and pca di   er dramatically because of this distinction.

14.7.2 independent component analysis

the independent component analysis (ica) model has exactly the same
form as (14.78), except the s    are assumed to be statistically indepen-
dent rather than uncorrelated. intuitively, lack of correlation determines
the second-degree cross-moments (covariances) of a multivariate distribu-
tion, while in general statistical independence determines all of the cross-
moments. these extra moment conditions allow us to identify the elements
of a uniquely. since the multivariate gaussian distribution is determined
by its second moments alone, it is the exception, and any gaussian inde-
pendent components can be determined only up to a rotation, as before.
hence identi   ability problems in (14.78) and (14.80) can be avoided if we
assume that the s    are independent and non-gaussian.

here we will discuss the full p-component model as in (14.78), where the
s    are independent with unit variance; ica versions of the factor analysis
model (14.80) exist as well. our treatment is based on the survey article
by hyv  arinen and oja (2000).

we wish to recover the mixing matrix a in x = as. without loss
of generality, we can assume that x has already been whitened to have
cov(x) = i; this is typically achieved via the svd described above. this
in turn implies that a is orthogonal, since s also has covariance i. so
solving the ica problem amounts to    nding an orthogonal a such that
the components of the vector random variable s = at x are independent
(and non-gaussian).

figure 14.37 shows the power of ica in separating two mixed signals.
this is an example of the classical cocktail party problem, where di   er-
ent microphones xj pick up mixtures of di   erent independent sources s   
(music, speech from di   erent speakers, etc.). ica is able to perform blind

14.7 independent component analysis and exploratory projection pursuit

561

source signals

measured signals

pca solution

ica solution

figure 14.37. illustration of ica vs. pca on arti   cial time-series data. the
upper left panel shows the two source signals, measured at 1000 uniformly spaced
time points. the upper right panel shows the observed mixed signals. the lower
two panels show the principal components and independent component solutions.

source separation, by exploiting the independence and non-gaussianity of
the original sources.

many of the popular approaches to ica are based on id178. the dif-

ferential id178 h of a random variable y with density g(y) is given by

h(y ) =    z g(y) log g(y)dy.

(14.82)

a well-known result in id205 says that among all random
variables with equal variance, gaussian variables have the maximum en-
tropy. finally, the mutual information i(y ) between the components of the
random vector y is a natural measure of dependence:

i(y ) =

pxj=1

h(yj)     h(y ).

(14.83)

density g(y) of y and its independence versionqp

the quantity i(y ) is called the kullback   leibler distance between the
j=1 gj(yj), where gj(yj)
is the marginal density of yj. now if x has covariance i, and y = at x
with a orthogonal, then it is easy to show that

i(y ) =

=

pxj=1
pxj=1

h(yj)     h(x)     log | det a|

h(yj)     h(x).

(14.84)

(14.85)

finding an a to minimize i(y ) = i(at x) looks for the orthogonal trans-
formation that leads to the most independence between its components. in

562

14. unsupervised learning

source s

data x

*

*

*
*
*

*
*

*
*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
pca solution

*
*
*
*
*
*
*
*
*

*
*
*
*
*

*
*

*

*

*
*
*
*
*
*
*
***
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*
*
*
*
*
*

*
*

*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
**
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*

*

ica solution

*

*

*
*
*

*
*

*
*
*
*

*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* * *
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
**
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*

*
*

*

figure 14.38. mixtures of independent uniform random variables. the upper
left panel shows 500 realizations from the two independent uniform sources, the
upper right panel their mixed versions. the lower two panels show the pca and
ica solutions, respectively.

light of (14.84) this is equivalent to minimizing the sum of the entropies of
the separate components of y , which in turn amounts to maximizing their
departures from gaussianity.

for convenience, rather than using the id178 h(yj), hyv  arinen and

oja (2000) use the negid178 measure j(yj) de   ned by

j(yj) = h(zj)     h(yj),

(14.86)

where zj is a gaussian random variable with the same variance as yj. ne-
gid178 is non-negative, and measures the departure of yj from gaussian-
ity. they propose simple approximations to negid178 which can be com-
puted and optimized on data. the ica solutions shown in figures 14.37   
14.39 use the approximation

j(yj)     [eg(yj)     eg(zj)]2,

(14.87)

where g(u) = 1
a log cosh(au) for 1     a     2. when applied to a sample
of xi, the expectations are replaced by data averages. this is one of the
options in the fastica software provided by these authors. more classical
(and less robust) measures are based on fourth moments, and hence look for
departures from the gaussian via kurtosis. see hyv  arinen and oja (2000)
for more details. in section 14.7.4 we describe their approximate newton
algorithm for    nding the optimal directions.

in summary then, ica applied to multivariate data looks for a sequence
of orthogonal projections such that the projected data look as far from

14.7 independent component analysis and exploratory projection pursuit

563

ica components

s
t

n
e
n
o
p
m
o
c
a
c
p

 

component

 1

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oo
o
oo
o
o
oo
o
o
o
o o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
ooo
o
o
oo o
o
oo oo
o o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o o
o
o
o
o
o
o
o
o o
o
o
o
o
o
ooo
o
oo
o o
o
o
o
o
o
oo
o
o
oo
oo
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo o
o
oo
oo
o
o
o
o
o
oo
o
o oo
o
o
oo
o o
oo
oo
oo
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
ooo
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
ooo
o o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
oo
o
o
oo
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
o
oo
ooo
o
o
o
o
o o
o
o
oo
o
o
o
o
o
oo
o
oo
o
o
oo
o
o
o
ooo
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
o
o
oo
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
oo o
o
o o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
oo
ooo
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
oo
oo o
o
o
o
o
o
oo
o
oo
o
oo
o
o
o
o o
o
o
o
o
o
o o
o
o
o
o o
o
o
o
oo
oo
o
o
o
o
o
o
oo
o
o
o
o
oo
oo
o
o
o o
o o o
o
o
o
o
o
oo
o
o
o
o
o
o oooo
o
oo
o
o
o
o
o
o
oo oo
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
oo
oo
o
o
o
o o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
oo
o o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o
ooo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
oo
o
oo
o
o
o
o
o
o
o
oo
o o
o
o
o
oo
o
o
o
oo
o o
o
o oo
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o oo
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
oo
o
oo
o
o
oo
o
o
ooo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
oo
o
oo
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
oooo
o
oo
oo
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
oo
oo
oo
o
o
o
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o o o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o

o
o

o

o
o

o

o

o

o

o
o

o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o
o
oo
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
oo
oo oo
oo
o
o
oo
o
o
o
ooo ooo
oo
o o
ooo o
o o
o
o ooo
o
o
o
ooo
o
ooooo
o
o
o
oo
o oo
oo
o
o
o o
o
o
o
o
o
o
o
ooo
oo
o
oo
oo
o
oo
o
oo
o
o
oo
ooo
o ooo
o
oo
o
o o
o
o
oooo ooooooo
o
o
o
o
o
ooo o
oo
o
o
o
o
o
o
ooo
o
o
oo
o
ooo
oo
oo
o oo
o
oooo oo ooo
ooo
ooooo
oo
ooo
ooo
o oo
oo
o
o
oo ooo
o
o
ooo
o
o
o
o
o
o
o oo
ooo
o
o
oo
o
o
o o
oo
ooooooo
o
o
oo o
ooo
oooo
o oo
ooo
ooo
oo
ooo
o
o
o
o
o ooo o
oo o
oo
o oooo
o
o
o
oo o
o o
o
oo
oooo
oo
oo
o
o o
o o
o
o
o
o
o
oo ooo
oo
o
o
ooo
oo
o oo
oo
oo
o
oo
o
o oo
oo
oo
oo
oo o
o
o
o
oo
o
ooo
o
oo o
ooo oo
ooo
o
oo o
oooo oo
o
oo
oo o
o
oo
o
o
oo
ooo
o
o
oo oo
ooo o
o
ooo
o
o
o
oo
oo
o
o o
o
oo
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o ooo
o
oo
o
o
oo
o
o
o
o
o o
o
oo
o o o
o
o
oo
oo o
o
o
o
o
o
oo
ooo
oo
oo
o
ooo
o
oo
o
oo
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o

o

o

o

o
o

o

o
o
o
o

o

o

component

 2

o

o
o

o

o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o ooo
o
o
o
o
ooo o
o
o
o
o
o
o
o o
o
o
ooo o
oo
o
o
o
oo
o
o
oo
o
ooo
o
o
o oo
o
o
oooo
oo
o
ooo
o
oo
o
o o
oo ooo o
oo
oooo
ooo
o
oo oo
o
o
o
o
o
o
o
ooo
ooo
o o
oo
o
oo
o
oo
o
o
o
o
o
oo
o
oo
ooo
o
o
oo
oo
o
oo
o
o oo
ooo o
o
o o
oo
o o
o
o ooo
o
o
o
o
o
o
oooo
oo
o
o
o
ooo
o
o
oo
o
o
o oo
oo
ooooooooo
o
ooo
ooo
o
o
ooo
ooooo
oo
ooo
oo
o o
o
o
o oo o
o
oo
ooo
o
o
o
o
o
o
oo ooo
o
ooo
o
o
oo
oo
oo ooo oo
o
oo ooo
o
o
ooo
ooo
ooo o
o
oooo
ooo
oo
ooo
o o
o
o
ooo oo
o o
ooo
o
oo
ooo o
o oo
oo
o
o o
o
o
o
oo
ooooo
o o
oo
o
o oooo
o
o
o
o
o
o
o
oo
oo oo
o
o
ooo
oo
o
oo
o
oo
ooo
oo
o oo
oo
oo
o
oo
ooo ooo
oooo
o
o
o
o
ooo oo
ooo
o
o
ooo
ooo ooo
oo
o
oo
ooo
oo
o
oo
o
oo o
o
o oo
o
o
o
o
o
o
o oo
o
o
oo
oo
oo
oo
o
o
o
o
o
o
o o
oo
o
o
o
oo
o
o
o ooo
oo
o
o
o o
o
o
oo
o
o
oo
o
o oo
o
o
o
oo
o
o
o
o
o
o o
o
oo
o
o
o
o
ooo
o
oo
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o

o
o

o
o

o

o

o

o

o

o
o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo o
oo
o
oo
o
o
o
oo
oooo
o
o
ooooo oo o
o
o
o
o
o
o
o o
oo
o
o
o o
o
oooo o
oo oo
o
o
o
o ooo o
o
o
oo
oo
oo
oo
oo
oo
ooo
o
oo o
o
o
o
o
o o
oo
o
o
o
ooooo
o oo
oo o
oo
oo
o
o o
o
ooo o
oooo
ooo
oo
o
o
o
o o
oooo
o
o o
o
ooo
o
o
o oo
o
o
oo
o o
oo
o o
ooo
o
oo ooo
ooo oo
o
o
o ooo o
ooo
o
ooo
o
oooo
o
o
o
o
oo o
o
o
oo
o
o
o
o
o
oo
ooo oo
o
oo
oooooo o
o
ooo o
oo
o
ooo
o
o
o
o
oo
oo
o
o
o
oooo
o
o
oo
o
o
oo
o
oo
oo o
o
o
o
o
o
o
oo
ooo
o
o
ooo
oo
o
ooo
o
o
o
o
ooo
o
oo
o o
o
o
o
oo
o
o
oo o
o
o
oo oo o
o
o
o
o oo
oo
o
oo
o o
o
o
o
o
o
ooo oo
o
o
oo o
o
oo
o
oo
o
o
o
ooo
oo
o
o
o
ooooo o
oo
o
o
oooo
oo o
oo
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o o
o o
o o
o
oo
oo
o o
o
o
o
o
o
o
o o
o
oo
oo o
o
ooo
o oo
o
oo
o
oo
o
o
o
o
o
o
o
oo
o
oo
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
ooo
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o o
o o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o o
o
o
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o
o
o o
o
o
o
o
o
o oo
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o o
o
o
o
oo
o
o
o o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o ooo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o o
o
o
o
o
o
ooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
oo
oo
o
o
ooo
o
o
o
o
oo
o
o
oo
o
o
o
o o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o ooo oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
ooo o
o
o
o
o
o
o
o
o o
oo
o
o
oo
o
o
o
o
o
o o
o
o
o
o o
o
o o
o
o
o o
oo
o
o
o
o
o
o
o
oo
o o
o
o
o
o
o
o
o
o
o
oo
ooo
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
ooo oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
ooo
o
o
o
o
o
o
o
o
oo
o
o
o
o
ooo
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o o o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
oo
o
o
oo
oo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oo
o
oo
oo
o
o
o
o
o
o
o
oo
o
oo
oo
ooo
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o o o
o
o
o
o o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo ooo
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o o
o
oo
o
o
o
oo
o
o
o
o
oo
o
o o
oo
oo
oo
o
o
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
oo
o
o
o o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oo o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
o
o
o
o
o
oo o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o o
o
o o
o
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
oo
oo
o
ooo
o
o
o
oo o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o o
o
o
o
o
o
o oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

component

 3

o

o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooooo ooo
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
oo o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o oo o
oo oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o o
o
o
o
o
oo
o
o
o o o
oo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
oo
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o o
o
oo
o
o
o o
o
oo
o
o
oo
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
oo
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
oo o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
oo
o
o
o
o
o
oo
o
o o
o
oo
oo
o
o
o
o o
o
o oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
oo
oo
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o o
o
o
o
o
o
o o
o
o o
o
o
o
oo
o
o
o
oo
o
o
o
o
oooo o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
oo
o
oo
o
o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
o
oo
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
oo
o
ooo
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o
o

o
o
o
o
o

o

o

o

o

o

o

o

o

o

o

o

o
o
o
o
o

o

o
o

o

o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo oo
o
o
o
o
o
ooo o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
ooo
o
o
o
o
oo
oooo
o o
o
o
o
o
oo
o
o
oooo oo
o o
oooo
o oo o
o
oooo
o
o
o
o
o
o
ooo
ooooo
ooo
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
oo
o
oo
ooo
o
o
o
o o
o
o
oo
o
oo
o
oo
o o
oo
o
oo oo
o
o
o
o
o
o
ooo
o o
oo
o
o
o
ooo
o
o
o o
o
o
o
o
ooo
o
ooo oo oo
o
o
ooo
o
ooo
oo
o o
o
ooo
o
o o
o
o
o o ooo
o
o
oo
oo
o
o
o
o
o
o
o
o
o
oo o
o
o
oo
o o
ooooooo
o
o oooo
o
ooo
ooo
oo oo
ooo
oo
ooo
o o
o
o
oo
o
ooooo
oo
oo o
oo
oo
o
o oo oo
o oo
o
o
o
o
oo
oo
oo
ooo oo
oo
o
o oooo
o
o
o
o
o
o
o
o
oo
ooooo
o
ooo
oo
o
oo
o
o
o oo
oo
o
oo
o o o
o o
oo
o
o o
o
o
o
ooo o
oo
o
oo
o o
o
ooo
oo
o
ooo
o
ooo
o
o
o
o
o
o
ooo
oo
o
o
oo
o
o
ooo
o
o
ooo
o
o
oooo
o
o
oo
o
oo
o
ooo
oo
o o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
oo
o
o
oo
o
o
o
oo
o
o
o
o
o o
o
o
ooo
o
o
o
o
oo
o
o
o
o
oo
o
oo
oo
o o
oo
o
oo o
o
oo
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o

o

o

o

o

o

o

o

o

o

o

o
o

o

o

o

o
o
o
o
o
o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o
o
oo
o
o o
o
o
o
oo
o
o
o o
o
o
o
o
o
o
o
o
o
o
oo
o
o
oooo
oo
o
o
oo
o
o
o
o
o
oo
oo
o ooo ooo o
o
oooo
o
o
o
o
oo
oooo
o
o
oo
o
o
o ooo
o
o
o
o
oo
oo
ooo
oo
o o
o o
ooo
o
ooooo
o
o
o
o
o
ooooo
oo
o
o
o
oo
o
o
o
oo
o
o
oo
oooo
ooo
o o
oo
o
o
o
o
oo oo
o
ooo
oo
o
o
o
o
o ooooo
o
o
oo
oo
o o
oo
oo
ooo
o
o oo oo
ooo
oo
o
ooooo
o
o
oo oo
o
ooo
o
o
o
o
o
o oo
o
oo
o
o
o
o
oooo
o
oooo
o
o o
o
oooo ooo
o
ooo
oooo
ooo oo
oo
o
ooo
o
o
o
oo
o o
oooo
o
o
oooo
o
o
o
o
o
oo
o
ooo
o
o
o
o
o
o
oo
ooo
o
oo
o
o oo
o
oo
o
o
o
o
o
oo
oo
o
o
oo
o o
o
o
o
o
oo
o
o
o
o
o oooo
o
o o
o
o
o
o
o
oo
o
o
o
oooo
o oo
o
o
o o
o
o o
ooo
oo
o
o
oo
o
o
oooo
o
o
oo o
oo
o oo
o
o
o
o
o
ooo
o
oo
o
o
o
o
o
oooo
o o
o
o
o
o
o
o
o
oo
o
o
oo
o
oo
o
o
o
o
o
o
o
o
oo
o oo
o
o
oo
oo
oo
o
o
o
o
o
o
o
o
o
o oo
o
oo
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo o
oo
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
oo
oo oo
o
o
ooo
o
o
ooo
o
o
o
oo
o
o
o
o
oo
o
o
o
oo
o
o
o
oo
o
o
oo
ooo oo
o
o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o oo
o
oo
o
o
o
oo
oo
o o
o
o
ooo
o
oo
o
o
o
oo
o
o
oo
o
o
o o
o
o o oooo
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o ooo
o
o
oo
o
o
ooo
oo
o
o
oo
ooo
o
oooo o
o
o
o
o
o
oo
oo
o
o
oo oo
oo
oo
o
o
o
o
o
oo
o
oo
ooo
oo
o o
o
oooo
o
ooo
oooo
ooo
o
o
o
o
oo
oooooo
o
o
o
o o
o
oo
o
oo ooo
o oo
oo
oooo oo
oo
oo o
o
oo
o
ooo o
o
o
oo oo
o
o o
o
o
o
o ooo
o
oo
oo
ooo
o
oooo
oo
ooo
oo
oo
o
o
o o
oo
o
o
o
oo
o oo
oo
o
o
o
o
o
o
oo
oo
o
o
o
oo
o
ooo
o
oooo
ooo
o ooo o
oooo
o o
o
o
oo
oo
oo
oo
o
o ooo
o
o
o o
oo
oo
o
oo
o o
oo
oo
oo ooo
oo
o
o
oo o
o
o
oo
oo
o
o
o
o
o
o
o
o
oo
oo
o
oo
o
ooo
o
oo
o
oo
o
o
o
o
oo
ooo
oo
oo
oo
oo
oo
o
o o
o
o
o
o
o
o
o
o
oo
o
oo
oo
o
o
o
oooo
o o
o o
o
oo
o
o
ooo
oo
o
o
o
o
o
o
o
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o

o

o

o

o

o

o
o

o
o

o
o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o
o
o
o

o

oo
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o o
o
o
o
o
oo
o
o
oo
o
o
o
o
o
o
oo
ooo
oo
o
ooo
o
o
o
o
o o oooo
o o
o ooo
o
oo ooo
oo oo
o oo o
o
o
o
o
o
o
o
ooooo
o o
o
oo
o
o
o
oo
o
o
o
o
o
o
o oo
o
oo
o
o
o
oo
o
oo
o
ooo
oo oo
o
o
oo
oo
oo
o
oo
o
o
o
o
o
o
o
o
oo
oo
o
o
o
oo o
o
o
oo
o
o
ooo
o
oooo o
oo
oo
ooo
o
oo o
oo ooo
o
ooo
o
oo
o
o
oo
o
ooo
o
o
o
o
o
o
o o
oo
oo
o
oo o
o
o
oo
oo
o
o
o
ooo
o
o oo
oo
o oo
oo o
oooo
ooo
oo
o o
oo
o
oo ooooooo
oo ooo
oo
o
oo
ooo
ooo
o
o
oo
o
oo
o oo
oo
oo
oo
o
o
o
o
o
o
o
o
o
o
oo
o
oo o
o
o
oo
o
o
oo
oo
oo
oo
o
oo
o
o ooo
oo
o
o
o o
ooo
o
ooo
ooo
o
o
o
o
o
ooo
o
ooo ooo
o
o
oo
o
o
oo
oo
o
o
oo
o
ooo
o
ooo o
ooo
o
o
o
o o
o
oooooo
ooo
ooo
oo
o o
o o
o
o o
o
o
o
oo
o
o
o
o
o
oo
oo
o
oo
o
o
o
oo
o
ooo
oo
o o
o
o
oo
o
o
o
o o
o
o o
o o
oo o
o
o
ooo
oo
o
o
o
o
o
oo
o oo
oo
o
o
oo
o
oo
o
o o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
oo
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o

o

o

o

o

o

o
o
o

o
o
o

o

o

o

o

o
o

o

o

o
o

o

o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
oo
o
o
o
oo
o o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
oo
o
o
o o
o
o
o
oo
ooooo o
oo
o oooo oo o
o
o ooo
o
o
o
o
oo
ooo o
o
o
oo
o
oooo
o
o
oooo
o
o
oo
o o
ooo
oo
oo
oo
ooo
o
ooo
o
o
o
o
o
o o
oo
o
oo oo
o
ooo o oo
o oo
oo
o o
o
o
oo
o
ooo
oooo
ooo
oo
o
o
o
o
o
ooo
o
o ooo
o
o
o oo
o
oo
oo
o
o
oo
o o
oo
o
o
ooo
o
ooo
ooooo
o o
o
ooooo
oo
o
o
ooo
o
o
o
ooo
o
oo
oo
o
o
o
o
oo
o
o
oo
oo oo
ooo o o
oo
o oo
o
oo
o
oo
oo
ooo
o
oo o
o
oo
o
oo
ooo o
o
o
o
o
o
o
o
o o
o
ooo
ooo
o
o
o
o
oo
o
o
o
o
o
oo
o
oo
oo
o
o
o
o
o
o
o
oo o
o
o o o
o
o
o
oooo
o
o
oo
oo
o
o
o
ooo
oo o
o
ooo
o
o
oo
o
o
o
oo
oo
oo
o
o
o
ooo
oo ooooooo
o
o
oo o
o
o o
o
o
o
oo
o
o o
o
o
o
o
oo oo
oo
o
o
o
o
o
o
o
oo
oo
o
o
oo
o o
o
o
o
o
o
oo
oo
o
o ooo
o oo
o
ooo
o o
oo
o
oo
o
o
o
oo
o
ooo
o
oo
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
ooo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o
o

o

o
o
o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o

o
oo
o

o

o
o
o
o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
oo
o
o
o
o
o
ooo
oo ooo
o
o
o oo
o
o
ooo
o
oo o
o
o
o
o
o
o
o
o
o
oo
o
o
o o
o
o
o
o
o o
oo
o
o
oo
o o
oo o
o
o
o
o
o
o
o
o
o o
o
o
o
o
oo o
o
oo
o
oo o
o
o o
oo
oo
o
o
o oo
o
oo
o
o
o
o o
o
o
ooo
o
o
o
oooooo
o
o
o
o o
o
o
o
o
oo
o
o
o
o
o
o
oo
oo
o
o
o
o
o
o
oo
ooo
o
o
oo
o
o
o
oo
oooo
o o
o
o
o
o
o
oo
o o
oo
o
o oo
oo
o
oo
o
oo
o
oooo
o
o oo
oooo
o
o
o
o
oooo
o
oo
oo oooo
o
o
o
oo
o
ooo oo
o o
o
oo
oo ooo
o
o
o
o
o
o
o
oo
oo
o
o
o
o
oo
oooo
oo
o
o ooo
oooo
o o
oo
oo
oo ooo
o
o
o
o o
oo
o
oo
o o
ooo
oo
o oo
o
o
o
oo
o
oo o
o o
o
o
oo
o
oooo
o oo
o
ooo
o
o oooo
oo ooo
oo
o
oo
oo
oo
o o
o
ooo o
o o
o
o
o
o
o
oo o
oo
oo
o
oo
o
o
oo
o
oo
oo
ooo
oo o
o
o
o
o
o
oo
o
o
o
o
o
o
o o
o o
oo
o
o
o
o
ooo
oo
o
o
o
oo
oo
oo
oo
oo
o
oo
o
o
o
o
o
o
o
oo
oo
o
o o
o o
o
ooo oo
oo
oo
oo
o
o
oo
o
ooo
oo
o
ooo
o
o
oo
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
ooo
oo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
oo
o
o
o
o

o

o
o
o
o
o
o
o

o

o

o

component

 4

o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
oo
o
oo
o o
o
o
o
o
o
o
o
o
oo
oo
o
oo
oo
o
o
o o
oo ooo
o
o
oo
o
o
o
o
oooo
o
o
o
ooo
o
o
oo
o
o
o
o
oo
oooo
o
o
o
o
o
o
o
o
o
o
oooo
o
o
o
o
o
o
ooo
oo
oo
o
o
oo o
ooo oooo
o
oooo
o
o
o
oo
o
o
oo
oo
o
o
oo
o
o
o
oo
o
o o
o
o
ooo
o
o
o
ooo
o
o
o
oo
oo
oooo
o
o
o o
o
o
o
o
o
o o
o
oo
o
o
ooo
o
o
ooo
oo o
o
o
o
o
o oo oo
ooo
oo
o
o
oo
ooo o
oo o
oo
o
o
o
oo
oo
oo
ooo
o
o
o
oo
o
o
oo
o
ooo
o
o
o
o
o
o
o
o
oo
o
o ooo
o
ooo
o
oo
o
ooo
o
o
o
o
o
o
o
ooo
o oo
oooo
o
o
oo
oo oo
oo
ooo
ooo
o o ooo o
o
o
o
oo
o
o
o
o
oo
o o
o
o
ooo
o
o
oo
o
o
o
o
oo
o
o
o
o
o
oo
ooo
oo
o
oo
o
o
o oo
o
o
o
oooo
o
oo
o
o
o
ooo
o
o
o
o
o
o oo
o
o
o
o
o
o o
o o
oo
o
oo
o
o
o
o
o o
o
o
oo
o o oo
oo
o
ooo
o oo
o
o
o
o
o
o
o
o
o
oo
o o
o
o
oo
o
o
o
o
o o
o
oo
o
o
o
o
o
o
oo
o
o
o
o
oo
o
oo o
o
o
o
o
o
oooo
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o

o

o

o

o
o
o
o

o

o
o

o

o

o
o
o
o

o

o

o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o

o
o
o
o

o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o
oo
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o o
o
o
o
o
o
o o
o
oo
o
o
o
o
oo
oo
o
o
o
o
o
ooo
oo
o
o
oo
o o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
ooo
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o oo
oo
o
o
ooo
o
oo
o
oo
o
oo
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
ooo
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
oo
o
o
o
o
o o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o o
o
o
o
o
o
o
oo
o
o o
oo
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
oo
o
oo
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
oo
o
o
o
o
o
oo
oo
o
o
o
ooo
o oo
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
ooo
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
ooo o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

component

 5

figure 14.39. a comparison of the    rst    ve ica components computed using
fastica (above diagonal) with the    rst    ve pca components(below diagonal).
each component is standardized to have unit variance.

gaussian as possible. with pre-whitened data, this amounts to looking for
components that are as independent as possible.

ica starts from essentially a factor analysis solution, and looks for rota-
tions that lead to independent components. from this point of view, ica is
just another factor rotation method, along with the traditional    varimax   
and    quartimax    methods used in psychometrics.

example: handwritten digits

we revisit the handwritten threes analyzed by pca in section 14.5.1. fig-
ure 14.39 compares the    rst    ve (standardized) principal components with
the    rst    ve ica components, all shown in the same standardized units.
note that each plot is a two-dimensional projection from a 256-dimensional

564

14. unsupervised learning

mean

ica 1

ica 2

ica 3

ica 4

ica 5

figure 14.40. the highlighted digits from figure 14.39. by comparing with
the mean digits, we see the nature of the ica component.

space. while the pca components all appear to have joint gaussian distri-
butions, the ica components have long-tailed distributions. this is not too
surprising, since pca focuses on variance, while ica speci   cally looks for
non-gaussian distributions. all the components have been standardized,
so we do not see the decreasing variances of the principal components.

for each ica component we have highlighted two of the extreme digits,
as well as a pair of central digits and displayed them in figure 14.40.
this illustrates the nature of each of the components. for example, ica
component    ve picks up the long sweeping tailed threes.

example: eeg time courses

ica has become an important tool in the study of brain dynamics   the
example we present here uses ica to untangle the components of signals
in multi-channel electroencephalographic (eeg) data (onton and makeig,
2006).

subjects wear a cap embedded with a lattice of 100 eeg electrodes,
which record brain activity at di   erent locations on the scalp. figure 14.4111
(top panel) shows 15 seconds of output from a subset of nine of these elec-
trodes from a subject performing a standard    two-back    learning task over
a 30 minute period. the subject is presented with a letter (b, h, j, c, f, or
k) at roughly 1500-ms intervals, and responds by pressing one of two but-
tons to indicate whether the letter presented is the same or di   erent from
that presented two steps back. depending on the answer, the subject earns
or loses points, and occasionally earns bonus or loses penalty points. the
time-course data show spatial correlation in the eeg signals   the signals
of nearby sensors look very similar.

the key assumption here is that signals recorded at each scalp electrode
are a mixture of independent potentials arising from di   erent cortical ac-

11reprinted from progress in brain research, vol. 159, julie onton and scott makeig,
   information based modeling of event-related brain dynamics,    page 106 , copyright
(2006), with permission from elsevier. we thank julie onton and scott makeig for
supplying an electronic version of the image.

14.7 independent component analysis and exploratory projection pursuit

565

tivities, as well as non-cortical artifact domains; see the reference for a
detailed overview of ica in this domain.

the lower part of figure 14.41 shows a selection of ica components.
the colored images represent the estimated unmixing coe   cient vectors   aj
as heatmap images superimposed on the scalp, indicating the location of
activity. the corresponding time-courses show the activity of the learned
ica components.

for example, the subject blinked after each performance feedback signal
(colored vertical lines), which accounts for the location and artifact signal
in ic1 and ic3. ic12 is an artifact associated with the cardiac pulse. ic4
and ic7 account for frontal theta-band activities, and appear after a stretch
of correct performance. see onton and makeig (2006) for a more detailed
discussion of this example, and the use of ica in eeg modeling.

14.7.3 exploratory projection pursuit

friedman and tukey (1974) proposed exploratory projection pursuit, a
graphical exploration technique for visualizing high-dimensional data. their
view was that most low (one- or two-dimensional) projections of high-
dimensional data look gaussian. interesting structure, such as clusters or
long tails, would be revealed by non-gaussian projections. they proposed
a number of projection indices for optimization, each focusing on a di   er-
ent departure from gaussianity. since their initial proposal, a variety of
improvements have been suggested (huber, 1985; friedman, 1987), and a
variety of indices, including id178, are implemented in the interactive
graphics package xgobi (swayne et al., 1991, now called ggobi). these
projection indices are exactly of the same form as j(yj) above, where
yj = at
j x, a normalized linear combination of the components of x. in
fact, some of the approximations and substitutions for cross-id178 coin-
cide with indices proposed for projection pursuit. typically with projection
pursuit, the directions aj are not constrained to be orthogonal. friedman
(1987) transforms the data to look gaussian in the chosen projection, and
then searches for subsequent directions. despite their di   erent origins, ica
and exploratory projection pursuit are quite similar, at least in the repre-
sentation described here.

14.7.4 a direct approach to ica

independent components have by de   nition a joint product density

fs(s) =

fj(sj),

pyj=1

(14.88)

so here we present an approach that estimates this density directly us-
ing generalized additive models (section 9.1). full details can be found in

566

14. unsupervised learning

figure 14.41. fifteen seconds of eeg data (of 1917 seconds) at nine (of
100) scalp channels (top panel), as well as nine ica components (lower panel).
while nearby electrodes record nearly identical mixtures of brain and non-brain
activity, ica components are temporally distinct. the colored scalps represent the
ica unmixing coe   cients   aj as a heatmap, showing brain or scalp location of the
source.

14.7 independent component analysis and exploratory projection pursuit

567

hastie and tibshirani (2003), and the method is implemented in the r
package prodenica, available from cran.

in the spirit of representing departures from gaussianity, we represent

each fj as

fj(sj) =   (sj)egj (sj ),

(14.89)

a tilted gaussian density. here    is the standard gaussian density, and
gj satis   es the id172 conditions required of a density. assuming
as before that x is pre-whitened, the log-likelihood for the observed data
x = as is

   (a,{gj}p

1; x) =

nxi=1

pxj=1(cid:2)log   j(at

j xi(cid:1)],

j xi) + gj(at

(14.90)

which we wish to maximize subject to the constraints that a is orthogonal
and that the gj result in densities in (14.89). without imposing any further
restrictions on gj, the model (14.90) is over-parametrized, so we instead
maximize a regularized version

pxj=1" 1

n

nxi=1(cid:2)log   (at

j xi) + gj(at

j xi)(cid:3)    z   (t)egj (t)dt       jz {g         

j (t)}2(t)dt# .

(14.91)
we have subtracted two penalty terms (for each j) in (14.91), inspired by
silverman (1986, section 5.4.4):

    the    rst enforces the density constraint r   (t)e  gj (t)dt = 1 on any

solution   gj.

  gj is a quartic-spline with knots at the observed values of sij = at

    the second is a roughness penalty, which guarantees that the solution
j xi.
it can further be shown that the solution densities   fj =   e  gj each have
mean zero and variance one (exercise 14.18). as we increase   j, these
solutions approach the standard gaussian   .

algorithm 14.3 product density ica algorithm: prodenica

1. initialize a (random gaussian matrix followed by orthogonalization).

2. alternate until convergence of a:

(a) given a, optimize (14.91) w.r.t. gj (separately for each j).
(b) given gj, j = 1, . . . , p, perform one step of a    xed point algo-

rithm towards    nding the optimal a.

we    t the functions gj and directions aj by optimizing (14.91) in an

alternating fashion, as described in algorithm 14.3.

568

14. unsupervised learning

step 2(a) amounts to a semi-parametric density estimation, which can
be solved using a novel application of generalized additive models. for
convenience we extract one of the p separate problems,

1
n

nxi=1

[log   (si) + g(si)]    z   (t)eg(t)dt       z {g         (t)}2(t)dt.

(14.92)

although the second integral in (14.92) leads to a smoothing spline, the
   rst integral is problematic, and requires an approximation. we construct
a    ne grid of l values s   
    in increments     covering the observed values si,
and count the number of si in the resulting bins:

y   
    =

#si     (s   

           /2, s   

    +    /2)

n

.

(14.93)

typically we pick l to be 1000, which is more than adequate. we can then
approximate (14.92) by

lx   =1ny   

i [log(  (s   

    )) + g(s   

    )]          (s   

    )eg(s   

    )o       z g         2(s)ds.

(14.94)

this last expression can be seen to be proportional to a penalized poisson
log-likelihood with response y   
    /    and penalty parameter   /   , and mean
  (s) =   (s)eg(s). this is a generalized additive spline model (hastie and
tibshirani, 1990; efron and tibshirani, 1996), with an o   set term log   (s),
and can be    t using a newton algorithm in o(l) operations. although
a quartic spline is called for, we    nd in practice that a cubic spline is
adequate. we have p tuning parameters   j to set; in practice we make
them all the same, and specify the amount of smoothing via the e   ective
degrees-of-freedom df(  ). our software uses 5df as a default value.

step 2(b) in algorithm 14.3 requires optimizing (14.91) with respect to
a, holding the   gj    xed. only the    rst terms in the sum involve a, and
since a is orthogonal, the collection of terms involving    do not depend on
a (exercise 14.19). hence we need to maximize

c(a) =

=

  gj(at

j xi)

nxi=1

(14.95)

1
n

pxj=1
pxj=1

cj(aj)

c(a) is a log-likelihood ratio between the    tted density and a gaussian,
and can be seen as an estimate of negid178 (14.86), with each   gj a con-
trast function as in (14.87). the    xed point update in step 2(b) is a modi   ed
newton step (exercise 14.20)

14.7 independent component analysis and exploratory projection pursuit

569

1. for each j update

aj     e(cid:8)x   g   

j(at

j x)     e[  g      

j (at

j x)]aj(cid:9) ,

(14.96)

where e represents expectation w.r.t the sample xi. since   gj is a    tted
quartic (or cubic) spline, the    rst and second derivatives are readily
available.

2. orthogonalize a using the symmetric square-root transformation

2 a. if a = udvt is the svd of a, it is easy to show that

(aat )    1
this leads to the update a     uvt .

our prodenica algorithm works as well as fastica on the arti   cial time
series data of figure 14.37, the mixture of uniforms data of figure 14.38,
and the digit data in figure 14.39.

example: simulations

a

d

g

j

m

p

b

e

h

k

n

q

c

f

i

l

o

r

 

a
e
u
r
t
m
o
r
f
 

 

e
c
n
a

t
s
d

i

 
i
r
a
m
a

fastica
kernelica
proddenica

0
5

.

0

0
2

.

0

0
1

.

0

5
0

.

0

2
0

.

0

1
0

.

0

a

b

c

d

e

f

g

h

i

j

k

l m n

o

p

q

r

distribution

figure 14.42. the left panel shows 18 distributions used for comparisons.
these include the    t   , uniform, exponential, mixtures of exponentials, symmetric
and asymmetric gaussian mixtures. the right panel shows (on the log scale)
the average amari metric for each method and each distribution, based on 30
simulations in ir2 for each distribution.

figure 14.42 shows the results of a simulation comparing prodenica to
fastica, and another semi-parametric competitor kernelica (bach and
jordan, 2002). the left panel shows the 18 distributions used as a basis
of comparison. for each distribution, we generated a pair of independent
components (n = 1024), and a random mixing matrix in ir2 with condition
number between 1 and 2. we used our r implementations of fastica, using
the negid178 criterion (14.87), and prodenica. for kernelica we used

570

14. unsupervised learning

the authors matlab code.12 since the search criteria are nonconvex, we
used    ve random starts for each method. each of the algorithms delivers
an orthogonal mixing matrix a (the data were pre-whitened), which is
available for comparison with the generating orthogonalized mixing matrix
a0. we used the amari metric (bach and jordan, 2002) as a measure of
the closeness of the two frames:

d(a0, a) =

1
2p

pxi=1 pp

j=1 |rij|

maxj |rij|     1!+

1
2p

pxj=1(cid:18)pp

i=1 |rij|

maxi |rij|     1(cid:19) , (14.97)

where rij = (aoa   1)ij. the right panel in figure 14.42 compares the
averages (on the log scale) of the amari metric between the truth and the
estimated mixing matrices. prodenica is competitive with fastica and
kernelica in all situations, and dominates most of the mixture simulations.

14.8 multidimensional scaling

both self-organizing maps and principal curves and surfaces map data
points in irp to a lower-dimensional manifold. multidimensional scaling
(mds) has a similar goal, but approaches the problem in a somewhat dif-
ferent way.

we start with observations x1, x2, . . . , xn     irp, and let dij be the dis-
tance between observations i and j. often we choose euclidean distance
dij = ||xi     xj||, but other distances may be used. further, in some ap-
plications we may not even have available the data points xi, but only
have some dissimilarity measure dij (see section 14.3.10). for example, in
a wine tasting experiment, dij might be a measure of how di   erent a sub-
ject judged wines i and j, and the subject provides such a measure for all
pairs of wines i, j. mds requires only the dissimilarities dij, in contrast to
the som and principal curves and surfaces which need the data points xi.
multidimensional scaling seeks values z1, z2, . . . , zn     irk to minimize

the so-called stress function13

sm (z1, z2, . . . , zn ) =xi6=i   

(dii        ||zi     zi   ||)2.

(14.98)

this is known as least squares or kruskal   shephard scaling. the idea is
to    nd a lower-dimensional representation of the data that preserves the
pairwise distances as well as possible. notice that the approximation is

12francis bach kindly supplied this code, and helped us set up the simulations.
13some authors de   ne stress as the square-root of sm ; since it does not a   ect the

optimization, we leave it squared to make comparisons with other criteria simpler.

14.8 multidimensional scaling

571

in terms of the distances rather than squared distances (which results in
slightly messier algebra). a id119 algorithm is used to minimize
sm .

a variation on least squares scaling is the so-called sammon mapping

which minimizes

ssm(z1, z2, . . . , zn ) =xi6=i   

(dii        ||zi     zi   ||)2

dii   

.

(14.99)

here more emphasis is put on preserving smaller pairwise distances.

in classical scaling, we instead start with similarities sii    : often we use
the centered inner product sii    = hxi       x, xi          xi. the problem then is to
minimize

sc(z1, z2, . . . , zn ) =xi,i   

(sii        hzi       z, zi          zi)2

(14.100)

over z1, z2, . . . , zn     irk. this is attractive because there is an explicit
solution in terms of eigenvectors: see exercise 14.11. if we have distances
rather than inner-products, we can convert them to centered inner-products
if the distances are euclidean;14 see (18.31) on page 671 in chapter 18.
if the similarities are in fact centered inner-products, classical scaling is
exactly equivalent to principal components, an inherently linear dimension-
reduction technique. classical scaling is not equivalent to least squares
scaling; the id168s are di   erent, and the mapping can be nonlinear.
least squares and classical scaling are referred to as metric scaling meth-
ods, in the sense that the actual dissimilarities or similarities are approx-
imated. shephard   kruskal nonmetric scaling e   ectively uses only ranks.
nonmetric scaling seeks to minimize the stress function

(14.101)

snm(z1, z2, . . . , zn ) = pi6=i   (cid:2)||zi     zi   ||       (dii    )(cid:3)2

pi6=i    ||zi     zi   ||2

over the zi and an arbitrary increasing function   . with       xed, we min-
imize over zi by id119. with the zi    xed, the method of iso-
tonic regression is used to    nd the best monotonic approximation   (dii    )
to ||zi     zi   ||. these steps are iterated until the solutions stabilize.
like the self-organizing map and principal surfaces, multidimensional
scaling represents high-dimensional data in a low-dimensional coordinate
system. principal surfaces and soms go a step further, and approximate
the original data by a low-dimensional manifold, parametrized in the low
dimensional coordinate system. in a principal surface and som, points

14an n    n distance matrix is euclidean if the entries represent pairwise euclidean

distances between n points in some dimensional space.

572

14. unsupervised learning

i

 

e
t
a
n
d
r
o
o
c
s
d
m
 
d
n
o
c
e
s

0
.
1

5
.
0

0
.
0

5
.
0
-

.

0
1
-

   
   
   

   

   

   

   

   
   

   
   
       
   
   
   
   

   
   

   
   
   

   

   

   
   
   

   

   

   

   
   
   

   
   
   
      
   
   
   
   
          
   
   
   
   
   
   
   
   
   
   
   
   
   
   

   

   

   

   

   

   

   

   

   

   

   
   

   

   
   
   
       
   
   
   
   

   

   
   

   

   

0.0

-1.0

-0.5

   
   
       

0.5

1.0

first mds coordinate

figure 14.43. first two coordinates for half-sphere data, from classical multi-
dimensional scaling.

close together in the original feature space should map close together on
the manifold, but points far apart in feature space might also map close
together. this is less likely in multidimensional scaling since it explicitly
tries to preserve all pairwise distances.

figure 14.43 shows the    rst two mds coordinates from classical scaling
for the half-sphere example. there is clear separation of the clusters, and
the tighter nature of the red cluster is apparent.

14.9 nonlinear dimension reduction and local

multidimensional scaling

several methods have been recently proposed for nonlinear dimension re-
duction, similar in spirit to principal surfaces. the idea is that the data lie
close to an intrinsically low-dimensional nonlinear manifold embedded in a
high-dimensional space. these methods can be thought of as       attening   
the manifold, and hence reducing the data to a set of low-dimensional co-
ordinates that represent their relative positions in the manifold. they are
useful for problems where signal-to-noise ratio is very high (e.g., physical
systems), and are probably not as useful for observational data with lower
signal-to-noise ratios.

the basic goal is illustrated in the left panel of figure 14.44. the data
lie near a parabola with substantial curvature. classical mds does not pre-

14.9 nonlinear dimension reduction and local multidimensional scaling

573

classical mds

local mds

2
x

0

5
   

0
1
   

5
1
   

2
x

0

5
   

0
1
   

5
1
   

   5

0

x1

5

   5

5

0

x1

figure 14.44. the orange points show data lying on a parabola, while the blue
points shows multidimensional scaling representations in one dimension. classical
multidimensional scaling (left panel) does not preserve the ordering of the points
along the curve, because it judges points on opposite ends of the curve to be close
together. in contrast, local multidimensional scaling (right panel) does a good job
of preserving the ordering of the points along the curve.

serve the ordering of the points along the curve, because it judges points
on opposite ends of the curve to be close together. the right panel shows
the results of local multi-dimensional scaling, one of the three methods for
non-linear multi-dimensional scaling that we discuss below. these meth-
ods use only the coordinates of the points in p dimensions, and have no
other information about the manifold. local mds has done a good job of
preserving the ordering of the points along the curve.

we now brie   y describe three new approaches to nonlinear dimension

reduction and manifold mapping.

isometric feature mapping (isomap) (tenenbaum et al., 2000) con-
structs a graph to approximate the geodesic distance between points along
the manifold. speci   cally, for each data point we    nd its neighbors   points
within some small euclidean distance of that point. we construct a graph
with an edge between any two neighboring points. the geodesic distance
between any two points is then approximated by the shortest path be-
tween points on the graph. finally, classical scaling is applied to the graph
distances, to produce a low-dimensional mapping.

local linear embedding (roweis and saul, 2000) takes a very di   erent ap-
proach, trying to preserve the local a   ne structure of the high-dimensional
data. each data point is approximated by a linear combination of neigh-
boring points. then a lower dimensional representation is constructed that

574

14. unsupervised learning

best preserves these local approximations. the details are interesting, so
we give them here.

1. for each data point xi in p dimensions, we    nd its k-nearest neigh-

bors n (i) in euclidean distance.

2. we approximate each point by an a   ne mixture of the points in its

neighborhood:

min

wik ||xi     xk   n (i)

wikxk||2

(14.102)

over weights wik satisfying wik = 0, k /    n (i), pn

k=1 wik = 1. wik
is the contribution of point k to the reconstruction of point i. note
that for a hope of a unique solution, we must have k < p.

3. finally, we    nd points yi in a space of dimension d < p to minimize

nxi=1

||yi    

nxk=1

wikyk||2

(14.103)

with wik    xed.

in step 3, we minimize

tr[(y     wy)t (y     wy)] = tr[yt (i     w)t (i     w)y]

(14.104)

where w is n    n ; y is n    d, for some small d < p. the solutions   y
are the trailing eigenvectors of m = (i     w)t (i     w). since 1 is a trivial
eigenvector with eigenvalue 0, we discard it and keep the next d. this has
the side e   ect that 1t y = 0, and hence the embedding coordinates are
mean centered.

local mds (chen and buja, 2008) takes the simplest and arguably the
most direct approach. we de   ne n to be the symmetric set of nearby pairs
of points; speci   cally a pair (i, i   ) is in n if point i is among the k-nearest
neighbors of i   , or vice-versa. then we construct the stress function

sl(z1, z2, . . . , zn ) = x(i,i   )   n

(dii        ||zi     zi   ||)2

+ x(i,i   ) /   n

w    (d     ||zi     zi   ||)2. (14.105)

here d is some large constant and w is a weight. the idea is that points
that are not neighbors are considered to be very far apart; such pairs are
given a small weight w so that they don   t dominate the overall stress func-
tion. to simplify the expression, we take w     1/d, and let d        .
expanding (14.105), this gives

14.9 nonlinear dimension reduction and local multidimensional scaling

575

figure 14.45. images of faces mapped into the embedding space described by
the    rst two coordinates of lle. next to the circled points, representative faces
are shown in di   erent parts of the space. the images at the bottom of the plot
correspond to points along the top right path (linked by solid line), and illustrate
one particular mode of variability in pose and expression.

576

14. unsupervised learning

sl(z1, z2, . . . , zn ) = x(i,i   )   n

(dii        ||zi     zi   ||)2        x(i,i   ) /   n

||zi     zi   ||,

(14.106)
where    = 2wd. the    rst term in (14.106) tries to preserve local structure
in the data, while the second term encourages the representations zi, zi   
for pairs (i, i   ) that are non-neighbors to be farther apart. local mds
minimizes the stress function (14.106) over zi, for    xed values of the number
of neighbors k and the tuning parameter    .

the right panel of figure 14.44 shows the result of local mds, using k = 2
neighbors and    = 0.01. we used coordinate descent with multiple starting
values to    nd a good minimum of the (nonconvex) stress function (14.106).
the ordering of the points along the curve has been largely preserved,

figure 14.45 shows a more interesting application of one of these meth-
ods (lle)15. the data consist of 1965 photographs, digitized as 20    28
grayscale images. the result of the    rst two-coordinates of lle are shown
and reveal some variability in pose and expression. similar pictures were
produced by local mds.

in experiments reported in chen and buja (2008), local mds shows su-
perior performance, as compared to isomap and lle. they also demon-
strate the usefulness of local mds for graph layout. there are also close
connections between the methods discussed here, spectral id91 (sec-
tion 14.5.3) and kernel pca (section 14.5.4).

14.10 the google id95 algorithm

in this section we give a brief description of the original id95 algo-
rithm used by the google search engine, an interesting recent application
of unsupervised learning methods.

we suppose that we have n web pages and wish to rank them in terms
of importance. for example, the n pages might all contain a string match
to    statistical learning    and we might wish to rank the pages in terms of
their likely relevance to a websurfer.

the id95 algorithm considers a webpage to be important if many
other webpages point to it. however the linking webpages that point to a
given page are not treated equally: the algorithm also takes into account
both the importance (id95 ) of the linking pages and the number of
outgoing links that they have. linking pages with higher id95 are
given more weight, while pages with more outgoing links are given less
weight. these ideas lead to a recursive de   nition for id95, detailed
next.

15sam roweis and lawrence saul kindly provided this    gure.

14.10 the google id95 algorithm

577

let lij = 1 if page j points to page i, and zero otherwise. let cj =
i=1 lij equal the number of pages pointed to by page j (number of out-
links). then the google id95s pi are de   ned by the recursive rela-
tionship

pn

pi = (1     d) + d

nxj=1(cid:0) lij
cj (cid:1)pj

(14.107)

where d is a positive constant (apparently set to 0.85).

the idea is that the importance of page i is the sum of the importances of
pages that point to that page. the sums are weighted by 1/cj, that is, each
page distributes a total vote of 1 to other pages. the constant d ensures
that each page gets a id95 of at least 1     d. in matrix notation

p = (1     d)e + d    ld   1
c p

(14.108)

where e is a vector of n ones and dc = diag(c) is a diagonal matrix with
diagonal elements cj. introducing the id172 et p = n (i.e., the
average id95 is 1), we can write (14.108) as

c (cid:3)p
p = (cid:2)(1     d)eet /n + dld   1

= ap

(14.109)

where the matrix a is the expression in square braces.

exploiting a connection with markov chains (see below), it can be shown
that the matrix a has a real eigenvalue equal to one, and one is its largest
eigenvalue. this means that we can    nd   p by the power method: starting
with some p = p0 we iterate

pk     apk   1; pk     n

pk

et pk

.

(14.110)

the    xed points   p are the desired id95s.

in the original paper of page et al. (1998), the authors considered pager-
ank as a model of user behavior, where a random web surfer clicks on links
at random, without regard to content. the surfer does a random walk on
the web, choosing among available outgoing links at random. the factor
1     d is the id203 that he does not click on a link, but jumps instead
to a random webpage.
some descriptions of id95 have (1     d)/n as the    rst term in def-
inition (14.107), which would better coincide with the random surfer in-
terpretation. then the page rank solution (divided by n ) is the stationary
distribution of an irreducible, aperiodic markov chain over the n webpages.
de   nition (14.107) also corresponds to an irreducible, aperiodic markov
chain, with di   erent transition probabilities than those from the (1    d)/n
version. viewing id95 as a markov chain makes clear why the matrix
a has a maximal real eigenvalue of 1. since a has positive entries with

578

14. unsupervised learning

page 1

page 2

page 4

page 3

figure 14.46. id95 algorithm: example of a small network

each column summing to one, markov chain theory tells us that it has a
unique eigenvector with eigenvalue one, corresponding to the stationary
distribution of the chain (bremaud, 1999).

a small network is shown for illustration in figure 14.46. the link matrix

is

l =            

0
1
1
0

0 1
0 0
1 0
0 0

0
0
1
0

            

and the number of outlinks is c = (2, 1, 1, 1).

(14.111)

the id95 solution is   p = (1.49, 0.78, 1.58, 0.15). notice that page 4

has no incoming links, and hence gets the minimum id95 of 0.15.

bibliographic notes

there are many books on id91, including hartigan (1975), gordon
(1999) and kaufman and rousseeuw (1990). id116 id91 goes back
at least to lloyd (1957), forgy (1965), jancey (1966) and macqueen (1967).
applications in engineering, especially in image compression via vector
quantization, can be found in gersho and gray (1992). the k-medoid pro-
cedure is described in kaufman and rousseeuw (1990). association rules
are outlined in agrawal et al. (1995). the self-organizing map was proposed
by kohonen (1989) and kohonen (1990); kohonen et al. (2000) give a more
recent account. principal components analysis and multidimensional scal-
ing are described in standard books on multivariate analysis, for example,
mardia et al. (1979). buja et al. (2008) have implemented a powerful en-
vironment called ggvis for multidimensional scaling, and the user manual

exercises

579

contains a lucid overview of the subject. figures 14.17, 14.21 (left panel)
and 14.28 (left panel) were produced in xgobi, a multidimensional data
visualization package by the same authors. ggobi is a more recent im-
plementation (cook and swayne, 2007). goodall (1991) gives a technical
overview of procrustes methods in statistics, and ramsay and silverman
(1997) discuss the shape registration problem. principal curves and surfaces
were proposed in hastie (1984) and hastie and stuetzle (1989). the idea of
principal points was formulated in flury (1990), tarpey and flury (1996)
give an exposition of the general concept of self-consistency. an excellent
tutorial on spectral id91 can be found in von luxburg (2007); this was
the main source for section 14.5.3. luxborg credits donath and ho   man
(1973) and fiedler (1973) with the earliest work on the subject. a history
of spectral id91 my be found in spielman and teng (1996). indepen-
dent component analysis was proposed by comon (1994), with subsequent
developments by bell and sejnowski (1995); our treatment in section 14.7
is based on hyv  arinen and oja (2000). projection pursuit was proposed by
friedman and tukey (1974), and is discussed in detail in huber (1985). a
dynamic projection pursuit algorithm is implemented in ggobi.

exercises

ex. 14.1 weights for id91. show that weighted euclidean distance

satis   es

where

d(w)
e

(xi, xi    ) = de(zi, zi    ) =

d(w)
e

l=1 wl

(xi, xi    ) = pp

l=1 wl(xil     xi   l)2

pp
pxl=1
(zil     zi   l)2,
l=1 wl(cid:19)1/2
zil = xil   (cid:18) wlpp

.

thus weighted euclidean distance based on x is equivalent to unweighted
euclidean distance based on z.

ex. 14.2 consider a mixture model density in p-dimensional feature space,

g(x) =

  kgk(x),

(14.114)

kxk=1

where gk = n (  k, l    2) and   k     0    k withpk   k = 1. here {  k,   k}, k =

1, . . . , k and   2 are unknown parameters.

(14.112)

(14.113)

580

14. unsupervised learning

suppose we have data x1, x2, . . . , xn     g(x) and we wish to    t the mix-

ture model.

1. write down the log-likelihood of the data

2. derive an em algorithm for computing the maximum likelihood es-

timates (see section 8.1).

3. show that if    has a known value in the mixture model and we take
       0, then in a sense this em algorithm coincides with id116
id91.

ex. 14.3 in section 14.2.6 we discuss the use of cart or prim for con-
structing generalized association rules. show that a problem occurs with ei-
ther of these methods when we generate the random data from the product-
marginal distribution; i.e., by randomly permuting the values for each of
the variables. propose ways to overcome this problem.

ex. 14.4 cluster the demographic data of table 14.1 using a classi   cation
tree. speci   cally, generate a reference sample of the same size of the train-
ing set, by randomly permuting the values within each feature. build a
classi   cation tree to the training sample (class 1) and the reference sample
(class 0) and describe the terminal nodes having highest estimated class 1
id203. compare the results to the prim results near table 14.1 and
also to the results of id116 id91 applied to the same data.

ex. 14.5 generate data with three features, with 30 data points in each of
three classes as follows:

  1 = u (     /8,   /8)
  1 = u (0, 2  )
x1 = sin(  1) cos(  1) + w11
y1 = sin(  1) sin(  1) + w12
z1 = cos(  1) + w13

  2 = u (  /2       /4,   /2 +   /4)
  2 = u (     /4,   /4)
x2 = sin(  2) cos(  2) + w21
y2 = sin(  2) sin(  2) + w22
z2 = cos(  2) + w23

  3 = u (  /2       /4,   /2 +   /4)
  3 = u (  /2       /4,   /2 +   /4)
x3 = sin(  3) cos(  3) + w31
y3 = sin(  3) sin(  3) + w32
z3 = cos(  3) + w33

here u (a, b) indicates a uniform variate on the range [a, b] and wjk are
independent normal variates with standard deviation 0.6. hence the data

exercises

581

lie near the surface of a sphere in three clusters centered at (1, 0, 0), (0, 1, 0)
and (0, 0, 1).

write a program to    t a som to these data, using the learning rates
given in the text. carry out a id116 id91 of the same data, and
compare the results to those in the text.

ex. 14.6 write programs to implement id116 id91 and a self-
organizing map (som), with the prototype lying on a two-dimensional
grid. apply them to the columns of the human tumor microarray data, us-
ing k = 2, 5, 10, 20 centroids for both. demonstrate that as the size of the
som neighborhood is taken to be smaller and smaller, the som solution
becomes more similar to the id116 solution.

ex. 14.7 derive (14.51) and (14.52) in section 14.5.1. show that      is not
unique, and characterize the family of equivalent solutions.

ex. 14.8 derive the solution (14.57) to the procrustes problem (14.56).
derive also the solution to the procrustes problem with scaling (14.58).

ex. 14.9 write an algorithm to solve

min
{     ,r   }l

1 ,m

||x   r        m||2
f .

(14.115)

lx   =1

apply it to the three s   s, and compare the results to those shown in fig-
ure 14.26.

ex. 14.10 derive the solution to the a   ne-invariant average problem (14.60).
apply it to the three s   s, and compare the results to those computed in
exercise 14.9.

ex. 14.11 classical multidimensional scaling. let s be the centered in-
ner product matrix with elements hxi       x, xj       xi. let   1 >   2 >        >
  k be the k largest eigenvalues of s, with associated eigenvectors ek =
(e1, e2, . . . , ek). let dk be a diagonal matrix with diagonal entries      1,
     2, . . . ,     k. show that the solutions zi to the classical scaling problem
(14.100) are the rows of ekdk.

ex. 14.12 consider the sparse pca criterion (14.71).

1. show that with       xed, solving for v amounts to k separate elastic-

net regression problems, with responses the k elements of   t xi.

2. show that with v    xed, solving for    amounts to a reduced-rank

version of the procrustes problem, which reduces to
trace(  t m) subject to   t    = ik,

max

  

(14.116)

where m and    are both p    k with k     p. if m = udqt is the
svd of m, show that the optimal    = uqt .

582

14. unsupervised learning

ex. 14.13 generate 200 data points with three features, lying close to a
helix. in detail, de   ne x1 = cos(s) + 0.1    z1, x2 = sin(s) + 0.1    z2, x3 =
s + 0.1    z3 where s takes on 200 equally spaced values between 0 and 2  ,
and z1, z2, z3 are independent and have standard gaussian distributions.

(a) fit a principal curve to the data and plot the estimated coordinate
functions. compare them to the underlying functions cos(s), sin(s)
and s.

(b) fit a self-organizing map to the same data, and see if you can discover

the helical shape of the original point cloud.

ex. 14.14 pre- and post-multiply equation (14.81) by a diagonal matrix
containing the inverse variances of the xj. hence obtain an equivalent
decomposition for the correlation matrix, in the sense that a simple scaling
is applied to the matrix a.

ex. 14.15 generate 200 observations of three variates x1, x2, x3 according
to

x1     z1
x2 = x1 + 0.001    z2
x3 = 10    z3

(14.117)

where z1, z2, z3 are independent standard normal variates. compute the
leading principal component and factor analysis directions. hence show
that the leading principal component aligns itself in the maximal variance
direction x3, while the leading factor essentially ignores the uncorrelated
component x3, and picks up the correlated component x2 + x1 (geo   rey
hinton, personal communication).

ex. 14.16 consider the kernel principal component procedure outlined in
section 14.5.4. argue that the number m of principal components is equal
to the rank of k, which is the number of non-zero elements in d. show
that the mth component zm (mth column of z) can be written (up to
j=1   jmk(xi, xj), where   jm = ujm/dm. show that
the mapping of a new observation x0 to the mth component is given by

centering) as zim =pn
z0m =pn
ex. 14.17 show that with g1(x) =pn

j=1   jmk(x0, xj).

j=1 cjk(x, xj), the solution to (14.66)
is given by   cj = uj1/d1, where u1 is the    rst column of u in (14.65), and
d1 the    rst diagonal element of d. show that the second and subsequent
principal component functions are de   ned in a similar manner (hint: see
section 5.8.1.)

ex. 14.18 consider the regularized log-likelihood for the density estimation
problem arising in ica,

1
n

nxi=1

[log   (si) + g(si)]    z   (t)eg(t)dt       z {g         (t)}2(t)dt.

(14.118)

exercises

583

the solution   g is a quartic smoothing spline, and can be written as   g(s) =
  q(s) +   q   (s), where q is a quadratic function (in the null space of the
penalty). let q(s) =   0 +   1s +   2s2. by examining the stationarity condi-
tions for     k, k = 1, 2, 3, show that the solution   f =   e  g is a density, and
has mean zero and variance one. if we used a second-derivative penalty

r {g      (t)}2(t)dt instead, what simple modi   cation could we make to the

problem to maintain the three moment conditions?

ex. 14.19 if a is p    p orthogonal, show that the    rst term in (14.91) on
page 567

pxj=1

nxi=1

log   (at

j xi),

with aj the jth column of a, does not depend on a.

ex. 14.20 fixed point algorithm for ica (hyv  arinen et al., 2001). consider
maximizing c(a) = e{g(at x)} with respect to a, with ||a|| = 1 and
cov(x) = i. use a lagrange multiplier to enforce the norm constraint,
and write down the    rst two derivatives of the modi   ed criterion. use the
approximation

e{xx t g      (at x)}     e{xx t}e{g      (at x)}

to show that the newton update can be written as the    xed-point update
(14.96).

ex. 14.21 consider an undirected graph with non-negative edge weights
wii    and graph laplacian l. suppose there are m connected components
a1, a2, . . . , am in the graph. show that there are m eigenvectors of l corre-
sponding to eigenvalue zero, and the indicator vectors of these components
ia1, ia2 , . . . , iam span the zero eigenspace.

ex. 14.22

(a) show that de   nition (14.108) implies that the sum of the id95s

pi is n , the number of web pages.

(b) write a program to compute the id95 solutions by the power
method using formulation (14.107). apply it to the network of fig-
ure 14.47.

ex. 14.23 algorithm for non-negative id105 (wu and lange,
2007). a function g(x, y) to said to minorize a function f (x) if

584

14. unsupervised learning

page 1

page 4

page 2

page 6

page 3

page 5

figure 14.47. example of a small network.

g(x, y)     f (x), g(x, x) = f (x)

(14.119)

for all x, y in the domain. this is useful for maximizing f (x) since it is easy
to show that f (x) is nondecreasing under the update

xs+1 = argmaxxg(x, xs)

(14.120)

there are analogous de   nitions for majorization, for minimizing a function
f (x). the resulting algorithms are known as mm algorithms, for    minorize-
maximize    or    majorize-minimize    (lange, 2004).
it also can be shown
that the em algorithm (8.5) is an example of an mm algorithm: see sec-
tion 8.5.3 and exercise 8.2 for details.

(a) consider maximization of the function l(w, h) in (14.73), written

here without the matrix notation

pxj=1"xij log  rxk=1

wikhkj!    

wikhkj# .

rxk=1

using the concavity of log(x), show that for any set of r values yk     0

k=1 ck = 1,

l(w, h) =

nxi=1
and 0     ck     1 withpr
log  rxk=1
yk!    
wikhkj!    

log  rxk=1

hence

where

as
ikj = ws

ikhs

kj and bs

ij =

and s indicates the current iteration.

ck log(yk/ck)

rxk=1
rxk=1

as
ikj
bs
ij

wikhkj! ,

ij
as
ikj

log  bs
rxk=1

ws

ikhs

kj,

(b) hence show that, ignoring constants, the function

exercises

585

g(w, h | ws, hs) =

nxi=1

xij

as
ikj
bs

ij (cid:16) log wik + log hkj(cid:17)

pxj=1
rxk=1
pxj=1
nxi=1

   

wikhkj

rxk=1

minorizes l(w, h).

(c) set the partial derivatives of g(w, h | ws, hs) to zero and hence

derive the updating steps (14.74).

ex. 14.24 consider the non-negative id105 (14.72) in the
rank one case (r = 1).

(a) show that the updates (14.74) reduce to

wi     wi pp
pp
hj     hj pn
pn

j=1 xij
j=1 wihj
i=1 xij
i=1 wihj

(14.121)

where wi = wi1, hj = h1j. this is an example of the iterative pro-
portional scaling procedure, applied to the independence model for a
two-way contingency table (fienberg, 1977, for example).

(b) show that the    nal iterates have the explicit form

j=1 xij

wi = c    pp
pn
i=1pp

j=1 xij

i=1 xik

1

c    pn
pn
i=1pp

j=1 xij

,

hk =

(14.122)

for any constant c > 0. these are equivalent to the usual row and
column estimates for a two-way independence model.

ex. 14.25 fit a non-negative id105 model to the collection
of two   s in the digits database. use 25 basis elements, and compare with a
24- component (plus mean) pca model. in both cases display the w and
h matrices as in figure 14.33.

586

14. unsupervised learning

15
id79s

this is page 587
printer: opaque this

15.1

introduction

id112 or bootstrap aggregation (section 8.7) is a technique for reducing
the variance of an estimated prediction function. id112 seems to work
especially well for high-variance, low-bias procedures, such as trees. for
regression, we simply    t the same regression tree many times to bootstrap-
sampled versions of the training data, and average the result. for classi   -
cation, a committee of trees each cast a vote for the predicted class.

boosting in chapter 10 was initially proposed as a committee method as
well, although unlike id112, the committee of weak learners evolves over
time, and the members cast a weighted vote. boosting appears to dominate
id112 on most problems, and became the preferred choice.

id79s (breiman, 2001) is a substantial modi   cation of id112
that builds a large collection of de-correlated trees, and then averages them.
on many problems the performance of id79s is very similar to
boosting, and they are simpler to train and tune. as a consequence, random
forests are popular, and are implemented in a variety of packages.

15.2 de   nition of id79s

the essential idea in id112 (section 8.7) is to average many noisy but
approximately unbiased models, and hence reduce the variance. trees are
ideal candidates for id112, since they can capture complex interaction

588

15. id79s

algorithm 15.1 id79 for regression or classi   cation.

1. for b = 1 to b:

(a) draw a bootstrap sample z    of size n from the training data.

(b) grow a random-forest tree tb to the bootstrapped data, by re-
cursively repeating the following steps for each terminal node of
the tree, until the minimum node size nmin is reached.
i. select m variables at random from the p variables.
ii. pick the best variable/split-point among the m.
iii. split the node into two daughter nodes.

2. output the ensemble of trees {tb}b
1 .

to make a prediction at a new point x:

regression:   f b

rf (x) = 1

tree. then   c b

b=1 tb(x).

bpb
rf (x) = majority vote {   cb(x)}b
1 .

classi   cation: let   cb(x) be the class prediction of the bth random-forest

structures in the data, and if grown su   ciently deep, have relatively low
bias. since trees are notoriously noisy, they bene   t greatly from the averag-
ing. moreover, since each tree generated in id112 is identically distributed
(i.d.), the expectation of an average of b such trees is the same as the ex-
pectation of any one of them. this means the bias of bagged trees is the
same as that of the individual (bootstrap) trees, and the only hope of im-
provement is through variance reduction. this is in contrast to boosting,
where the trees are grown in an adaptive way to remove bias, and hence
are not i.d.

an average of b i.i.d. random variables, each with variance   2, has vari-
ance 1
b   2. if the variables are simply i.d. (identically distributed, but not
necessarily independent) with positive pairwise correlation   , the variance
of the average is (exercise 15.1)

    2 +

1       
b

  2.

(15.1)

as b increases, the second term disappears, but the    rst remains, and
hence the size of the correlation of pairs of bagged trees limits the bene   ts
of averaging. the idea in id79s (algorithm 15.1) is to improve
the variance reduction of id112 by reducing the correlation between the
trees, without increasing the variance too much. this is achieved in the
tree-growing process through random selection of the input variables.

speci   cally, when growing a tree on a bootstrapped dataset:

15.2 de   nition of id79s

589

before each split, select m     p of the input variables at random
as candidates for splitting.

typically values for m are    p or even as low as 1.

after b such trees {t (x;   b)}b

predictor is

1 are grown, the id79 (regression)

  f b
rf (x) =

1
b

bxb=1

t (x;   b).

(15.2)

as in section 10.9 (page 356),   b characterizes the bth id79 tree in
terms of split variables, cutpoints at each node, and terminal-node values.
intuitively, reducing m will reduce the correlation between any pair of trees
in the ensemble, and hence by (15.1) reduce the variance of the average.

spam data

id112
id79
gradient boosting (5 node)

r
o
r
r

e

 
t
s
e
t

0
7
0
.
0

5
6
0
.
0

0
6
0
.
0

5
5
0
.
0

0
5
0
.
0

5
4
0
.
0

0
4
0
.
0

0

500

1000

1500

2000

2500

number of trees

figure 15.1. id112, id79, and gradient boosting, applied to the
spam data. for boosting, 5-node trees were used, and the number of trees were
chosen by 10-fold cross-validation (2500 trees). each    step    in the    gure corre-
sponds to a change in a single misclassi   cation (in a test set of 1536).

not all estimators can be improved by shaking up the data like this.
it seems that highly nonlinear estimators, such as trees, bene   t the most.
for bootstrapped trees,    is typically small (0.05 or lower is typical; see
figure 15.9), while   2 is not much larger than the variance for the original
tree. on the other hand, id112 does not change linear estimates, such
as the sample mean (hence its variance either); the pairwise correlation
between bootstrapped means is about 50% (exercise 15.4).

590

15. id79s

id79s are popular. leo breiman   s1 collaborator adele cutler
maintains a id79 website2 where the software is freely available,
with more than 3000 downloads reported by 2002. there is a randomforest
package in r, maintained by andy liaw, available from the cran website.
the authors make grand claims about the success of id79s:
   most accurate,       most interpretable,    and the like. in our experience ran-
dom forests do remarkably well, with very little tuning required. a ran-
dom forest classi   er achieves 4.88% misclassi   cation error on the spam test
data, which compares well with all other methods, and is not signi   cantly
worse than gradient boosting at 4.5%. id112 achieves 5.4% which is
signi   cantly worse than either (using the mcnemar test outlined in ex-
ercise 10.6), so it appears on this example the additional randomization
helps.

nested spheres

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

 
t
s
e
t

5
1

.

0

0
1

.

0

5
0

.

0

0
0

.

0

bayes error

rf   1

rf   3

id112

gbm   1

gbm   6

figure 15.2. the results of 50 simulations from the    nested spheres    model in
ir10. the bayes decision boundary is the surface of a sphere (additive).    rf-3   
refers to a id79 with m = 3, and    gbm-6    a gradient boosted model
with interaction order six; similarly for    rf-1    and    gbm-1.    the training sets
were of size 2000, and the test sets 10, 000.

figure 15.1 shows the test-error progression on 2500 trees for the three
methods. in this case there is some evidence that gradient boosting has
started to over   t, although 10-fold cross-validation chose all 2500 trees.

1sadly, leo breiman died in july, 2005.
2http://www.math.usu.edu/   adele/forests/

15.2 de   nition of id79s

591

california housing data

rf m=2
rf m=6
gbm depth=4
gbm depth=6

4
4

.

0

2
4

.

0

0
4

.

0

8
3

.

0

6
3

.

0

4
3

.

0

2
3

.

0

r
o
r
r

e
e

 

t

l

 

u
o
s
b
a
e
g
a
r
e
v
a

 
t
s
e
t

0

200

400

600

800

1000

number of trees

figure 15.3. id79s compared to gradient boosting on the california
housing data. the curves represent mean absolute error on the test data as a
function of the number of trees in the models. two id79s are shown, with
m = 2 and m = 6. the two gradient boosted models use a shrinkage parameter
   = 0.05 in (10.41), and have interaction depths of 4 and 6. the boosted models
outperform id79s.

figure 15.2 shows the results of a simulation3 comparing id79s
to gradient boosting on the nested spheres problem [equation (10.2) in
chapter 10]. boosting easily outperforms id79s here. notice that
smaller m is better here, although part of the reason could be that the true
decision boundary is additive.

figure 15.3 compares id79s to boosting (with shrinkage) in a
regression problem, using the california housing data (section 10.14.1).
two strong features that emerge are

    id79s stabilize at about 200 trees, while at 1000 trees boost-
ing continues to improve. boosting is slowed down by the shrinkage,
as well as the fact that the trees are much smaller.

    boosting outperforms id79s here. at 1000 terms, the weaker
boosting model (gbm depth 4) has a smaller error than the stronger

3details: the id79s were    t using the r package randomforest 4.5-11,
with 500 trees. the gradient boosting models were    t using r package gbm 1.5, with
shrinkage parameter set to 0.05, and 2000 trees.

592

15. id79s

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

oob error
test error

5
7
0
0

.

5
6
0
0

.

5
5
0

.

0

5
4
0
0

.

0

500

1000

1500

2000

2500

number of trees

figure 15.4. oob error computed on the spam training data, compared to the
test error computed on the test set.

id79 (rf m = 6); a wilcoxon test on the mean di   erences
in absolute errors has a p-value of 0.007. for larger m the random
forests performed no better.

15.3 details of id79s

we have glossed over the distinction between id79s for classi   ca-
tion versus regression. when used for classi   cation, a id79 obtains
a class vote from each tree, and then classi   es using majority vote (see sec-
tion 8.7 on id112 for a similar discussion). when used for regression, the
predictions from each tree at a target point x are simply averaged, as in
(15.2). in addition, the inventors make the following recommendations:

    for classi   cation, the default value for m is       p    and the minimum

node size is one.

    for regression, the default value for m is    p/3    and the minimum

node size is    ve.

in practice the best values for these parameters will depend on the problem,
and they should be treated as tuning parameters. in figure 15.3 m = 6
performs much better than the default value    8/3    = 2.

15.3.1 out of bag samples

an important feature of id79s is its use of out-of-bag (oob) sam-
ples:

15.3 details of id79s

593

for each observation zi = (xi, yi), construct its id79
predictor by averaging only those trees corresponding to boot-
strap samples in which zi did not appear.

an oob error estimate is almost identical to that obtained by n -fold cross-
validation; see exercise 15.2. hence unlike many other nonlinear estimators,
id79s can be    t in one sequence, with cross-validation being per-
formed along the way. once the oob error stabilizes, the training can be
terminated.

figure 15.4 shows the oob misclassi   cation error for the spam data, com-
pared to the test error. although 2500 trees are averaged here, it appears
from the plot that about 200 would be su   cient.

15.3.2 variable importance

variable importance plots can be constructed for id79s in exactly
the same way as they were for gradient-boosted models (section 10.13).
at each split in each tree, the improvement in the split-criterion is the
importance measure attributed to the splitting variable, and is accumulated
over all the trees in the forest separately for each variable. the left plot
of figure 15.5 shows the variable importances computed in this way for
the spam data; compare with the corresponding figure 10.6 on page 354 for
gradient boosting. boosting ignores some variables completely, while the
id79 does not. the candidate split-variable selection increases
the chance that any single variable gets included in a id79, while
no such selection occurs with boosting.

id79s also use the oob samples to construct a di   erent variable-
importance measure, apparently to measure the prediction strength of each
variable. when the bth tree is grown, the oob samples are passed down
the tree, and the prediction accuracy is recorded. then the values for the
jth variable are randomly permuted in the oob samples, and the accuracy
is again computed. the decrease in accuracy as a result of this permuting
is averaged over all trees, and is used as a measure of the importance of
variable j in the id79. these are expressed as a percent of the
maximum in the right plot in figure 15.5. although the rankings of the
two methods are similar, the importances in the right plot are more uni-
form over the variables. the randomization e   ectively voids the e   ect of
a variable, much like setting a coe   cient to zero in a linear model (exer-
cise 15.7). this does not measure the e   ect on prediction were this variable
not available, because if the model was re   tted without the variable, other
variables could be used as surrogates.

594

15. id79s

gini

randomization

table
parts
cs
3d
addresses
857
415
direct
conference
project
original
report
telnet
lab
[
85
technology
data
font
credit
#
make
people
pm
address
order
labs
meeting
650
;
mail
over
receive
re
email
all
will
(
internet
1999
business
hpl
edu
000
george
you
our
money
captot
hp
capmax
your
capave
free
remove
$
!

table
parts
3d
addresses
direct
report
cs
make
415
#
857
conference
credit
data
project
people
telnet
lab
original
address
85
[
labs
all
order
technology
mail
font
;
email
over
receive
pm
650
internet
will
(
money
meeting
000
business
hpl
you
re
1999
our
your
captot
george
edu
capmax
free
hp
capave
$
remove
!

0

20

40

60

80

100

0

20

40

60

80

100

variable importance

variable importance

figure 15.5. variable importance plots for a classi   cation id79
grown on the spam data. the left plot bases the importance on the gini split-
ting index, as in gradient boosting. the rankings compare well with the rankings
produced by gradient boosting (figure 10.6 on page 354). the right plot uses oob
randomization to compute variable importances, and tends to spread the impor-
tances more uniformly.

15.3 details of id79s

595

proximity plot

id79 classifier

2

3

1

2

n
o
i
s
n
e
m
d

i

4

5

6

2
x

6

5

3

4

2

1

dimension 1

x1

figure 15.6. (left): proximity plot for a id79 classi   er grown to
the mixture data. (right): decision boundary and training data for id79
on mixture data. six points have been identi   ed in each plot.

15.3.3 proximity plots

one of the advertised outputs of a id79 is a proximity plot. fig-
ure 15.6 shows a proximity plot for the mixture data de   ned in section 2.3.3
in chapter 2. in growing a id79, an n    n proximity matrix is
accumulated for the training data. for every tree, any pair of oob obser-
vations sharing a terminal node has their proximity increased by one. this
proximity matrix is then represented in two dimensions using multidimen-
sional scaling (section 14.8). the idea is that even though the data may be
high-dimensional, involving mixed variables, etc., the proximity plot gives
an indication of which observations are e   ectively close together in the eyes
of the id79 classi   er.

proximity plots for id79s often look very similar, irrespective of
the data, which casts doubt on their utility. they tend to have a star shape,
one arm per class, which is more pronounced the better the classi   cation
performance.

since the mixture data are two-dimensional, we can map points from the
proximity plot to the original coordinates, and get a better understanding of
what they represent. it seems that points in pure regions class-wise map to
the extremities of the star, while points nearer the decision boundaries map
nearer the center. this is not surprising when we consider the construction
of the proximity matrices. neighboring points in pure regions will often
end up sharing a bucket, since when a terminal node is pure, it is no longer

596

15. id79s

split by a id79 tree-growing algorithm. on the other hand, pairs
of points that are close but belong to di   erent classes will sometimes share
a terminal node, but not always.

15.3.4 id79s and over   tting

when the number of variables is large, but the fraction of relevant variables
small, id79s are likely to perform poorly with small m. at each
split the chance can be small that the relevant variables will be selected.
figure 15.7 shows the results of a simulation that supports this claim. de-
tails are given in the    gure caption and exercise 15.3. at the top of each
pair we see the hyper-geometric id203 that a relevant variable will be
selected at any split by a id79 tree (in this simulation, the relevant
variables are all equal in stature). as this id203 gets small, the gap
between boosting and id79s increases. when the number of rele-
vant variables increases, the performance of id79s is surprisingly
robust to an increase in the number of noise variables. for example, with 6
relevant and 100 noise variables, the id203 of a relevant variable being

selected at any split is 0.46, assuming m =p(6 + 100)     10. according to

figure 15.7, this does not hurt the performance of id79s compared
with boosting. this robustness is largely due to the relative insensitivity of
misclassi   cation cost to the bias and variance of the id203 estimates
in each tree. we consider id79s for regression in the next section.

another claim is that id79s    cannot over   t    the data. it is
certainly true that increasing b does not cause the id79 sequence
to over   t; like id112, the id79 estimate (15.2) approximates the
expectation

  frf (x) = e  t (x;   ) = lim
b      

  f (x)b
rf

(15.3)

with an average over b realizations of   . the distribution of    here is con-
ditional on the training data. however, this limit can over   t the data; the
average of fully grown trees can result in too rich a model, and incur unnec-
essary variance. segal (2004) demonstrates small gains in performance by
controlling the depths of the individual trees grown in id79s. our
experience is that using full-grown trees seldom costs much, and results in
one less tuning parameter.

figure 15.8 shows the modest e   ect of depth control in a simple regression
example. classi   ers are less sensitive to variance, and this e   ect of over-
   tting is seldom seen with random-forest classi   cation.

15.4 analysis of id79s

597

0.52

0.34

0.25

0.19

0.15

id79
gradient boosting

bayes error

(2, 5)

(2, 25)

(2, 50)

(2, 100)

(2, 150)

number of (relevant, noise) variables

0
3
.
0

5
2
.
0

0
2
.
0

5
1
.
0

0
1
0

.

r
o
r
r

e
 
n
o
i
t
a
c
i
f
i
s
s
a
c
s
m

i

l

 
t
s
e
t

figure 15.7. a comparison of id79s and gradient boosting on prob-
lems with increasing numbers of noise variables. in each case the true decision
boundary depends on two variables, and an increasing number of noise variables
are included. id79s uses its default value m =    p. at the top of each
pair is the id203 that one of the relevant variables is chosen at any split.
the results are based on 50 simulations for each pair, with a training sample of
300, and a test sample of 500. see exercise 15.3.

15.4 analysis of id79s

in this section we analyze the mechanisms at play with the additional
randomization employed by id79s. for this discussion we focus
on regression and squared error loss, since this gets at the main points,
and bias and variance are more complex with 0   1 loss (see section 7.3.1).
furthermore, even in the case of a classi   cation problem, we can consider
the random-forest average as an estimate of the class posterior probabilities,
for which bias and variance are appropriate descriptors.

15.4.1 variance and the de-correlation e   ect
the limiting form (b        ) of the id79 regression estimator is
(15.4)

  frf (x) = e  |zt (x;   (z)),

where we have made explicit the dependence on the training data z. here
we consider estimation at a single target point x. from (15.1) we see that

598

15. id79s

shallow

deep

r
o
r
r

e

 

 
t
s
e
t
d
e
r
a
u
q
s
n
a
e
m

 

0
1

.

1

5
0

.

1

0
0

.

1

50

30

20

10

5

minimum node size

figure 15.8. the e   ect of tree size on the error in id79 regres-
sion. in this example, the true surface was additive in two of the 12 variables,
plus additive unit-variance gaussian noise. tree depth is controlled here by the
minimum node size; the smaller the minimum node size, the deeper the trees.

var   frf (x) =   (x)  2(x).

(15.5)

here

      (x) is the sampling correlation between any pair of trees used in the
(15.6)

  (x) = corr[t (x;   1(z)), t (x;   2(z))],

averaging:

where   1(z) and   2(z) are a randomly drawn pair of id79
trees grown to the randomly sampled z;

      2(x) is the sampling variance of any single randomly drawn tree,

  2(x) = var t (x;   (z)).

(15.7)

it is easy to confuse   (x) with the average correlation between    tted trees
in a given random-forest ensemble; that is, think of the    tted trees as n -
vectors, and compute the average pairwise correlation between these vec-
tors, conditioned on the data. this is not the case; this conditional corre-
lation is not directly relevant in the averaging process, and the dependence
on x in   (x) warns us of the distinction. rather,   (x) is the theoretical
correlation between a pair of random-forest trees evaluated at x, induced
by repeatedly making training sample draws z from the population, and
then drawing a pair of id79 trees. in statistical jargon, this is the
correlation induced by the sampling distribution of z and   .

more precisely, the variability averaged over in the calculations in (15.6)

and (15.7) is both

15.4 analysis of id79s

599

    conditional on z: due to the bootstrap sampling and feature sampling

at each split, and

    a result of the sampling variability of z itself.

in fact, the conditional covariance of a pair of tree    ts at x is zero, because
the bootstrap and feature sampling is i.i.d; see exercise 15.5.

s
e
e
r
t
 
n
e
e
w
t
e
b
 
n
o

i
t

l

a
e
r
r
o
c

8
0

.

0

6
0

.

0

4
0
.
0

2
0
.
0

0
0
.
0

1 4 7

13

19

25

31

37

43

49

number of randomly selected splitting variables m

figure 15.9. correlations between pairs of trees drawn by a random-forest
regression algorithm, as a function of m. the boxplots represent the correlations
at 600 randomly chosen prediction points x.

the following demonstrations are based on a simulation model

y =

1
   50

50xj=1

xj +   ,

(15.8)

with all the xj and    iid gaussian. we use 500 training sets of size 100, and
a single set of test locations of size 600. since regression trees are nonlinear
in z, the patterns we see below will di   er somewhat depending on the
structure of the model.

figure 15.9 shows how the correlation (15.6) between pairs of trees de-
creases as m decreases: pairs of tree predictions at x for di   erent training
sets z are likely to be less similar if they do not use the same splitting
variables.

in the left panel of figure 15.10 we consider the variances of single tree
predictors, vart (x;   (z)) (averaged over 600 prediction points x drawn
randomly from our simulation model). this is the total variance, and can be

600

15. id79s

decomposed into two parts using standard conditional variance arguments
(see exercise 15.5):

var  ,zt (x;   (z)) = varze  |zt (x;   (z)) + ezvar  |zt (x;   (z))

total variance

=

varz   frf (x)

+

within-z variance

(15.9)
the second term is the within-z variance   a result of the randomization,
which increases as m decreases. the    rst term is in fact the sampling vari-
ance of the id79 ensemble (shown in the right panel), which de-
creases as m decreases. the variance of the individual trees does not change
appreciably over much of the range of m, hence in light of (15.5), the vari-
ance of the ensemble is dramatically lower than this tree variance.

single tree

id79 ensemble

e
c
n
a
i
r
a
v

5
9

.

1

0
9
1

.

5
8
1

.

0
8

.

1

within z
total

i

 

s
a
b
d
e
r
a
u
q
s
d
n
a

 

 
r
o
r
r

 

e
d
e
r
a
u
q
s
n
a
e
m

 

5
8
0

.

0
8

.

0

5
7
0

.

0
7

.

0

5
6

.

0

mean squared error
squared bias
variance

0
2

.

0

5
1

.

0

0
1

.

0

5
0
0

.

0

.

0

e
c
n
a
i
r
a
v

0

10

20

30

40

50

0

10

20

30

40

50

m

m

figure 15.10. simulation results. the left panel shows the average variance of
a single id79 tree, as a function of m.    within z    refers to the average
within-sample contribution to the variance, resulting from the bootstrap sampling
and split-variable sampling (15.9).    total    includes the sampling variability of
z. the horizontal line is the average variance of a single fully grown tree (with-
out bootstrap sampling). the right panel shows the average mean-squared error,
squared bias and variance of the ensemble, as a function of m. note that the
variance axis is on the right (same scale, di   erent level). the horizontal line is
the average squared-bias of a fully grown tree.

15.4.2 bias

as in id112, the bias of a id79 is the same as the bias of any
of the individual sampled trees t (x;   (z)):

15.4 analysis of id79s

601

bias(x) =   (x)     ez   frf (x)

=   (x)     eze  |zt (x;   (z)).

(15.10)

this is also typically greater (in absolute terms) than the bias of an un-
pruned tree grown to z, since the randomization and reduced sample space
impose restrictions. hence the improvements in prediction obtained by bag-
ging or id79s are solely a result of variance reduction.

any discussion of bias depends on the unknown true function. fig-
ure 15.10 (right panel) shows the squared bias for our additive model simu-
lation (estimated from the 500 realizations). although for di   erent models
the shape and rate of the bias curves may di   er, the general trend is that
as m decreases, the bias increases. shown in the    gure is the mean-squared
error, and we see a classical bias-variance trade-o    in the choice of m. for
all m the squared bias of the id79 is greater than that for a single
tree (horizontal line).

these patterns suggest a similarity with ridge regression (section 3.4.1).
ridge regression is useful (in linear models) when one has a large number
of variables with similarly sized coe   cients; ridge shrinks their coe   cients
toward zero, and those of strongly correlated variables toward each other.
although the size of the training sample might not permit all the variables
to be in the model, this id173 via ridge stabilizes the model and al-
lows all the variables to have their say (albeit diminished). id79s
with small m perform a similar averaging. each of the relevant variables
get their turn to be the primary split, and the ensemble averaging reduces
the contribution of any individual variable. since this simulation exam-
ple (15.8) is based on a linear model in all the variables, ridge regression
achieves a lower mean-squared error (about 0.45 with df(  opt)     29).

15.4.3 adaptive nearest neighbors

the id79 classi   er has much in common with the k-nearest neigh-
bor classi   er (section 13.3); in fact a weighted version thereof. since each
tree is grown to maximal size, for a particular      , t (x;      (z)) is the re-
sponse value for one of the training samples4. the tree-growing algorithm
   nds an    optimal    path to that observation, choosing the most informative
predictors from those at its disposal. the averaging process assigns weights
to these training responses, which ultimately vote for the prediction. hence
via the random-forest voting mechanism, those observations close to the
target point get assigned weights   an equivalent kernel   which combine to
form the classi   cation decision.

figure 15.11 demonstrates the similarity between the decision boundary

of 3-nearest neighbors and id79s on the mixture data.

4we gloss over the fact that pure nodes are not split further, and hence there can be

more than one observation in a terminal node

602

15. id79s

id79 classifier

3   nearest neighbors

o

o

o
o
o
o
o

o

o

o
o

o
o
o

o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o

o
o

o

o

training error: 0.000
test error:       0.238
bayes error:    0.210

o

o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o

o

o
o
o

o
o

o
o
o
o
o

o

o

o
o
o
o
o

o

o

o

o
o
oo
o

o
o

o
o
o

o
o
o

o

o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
oo
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o
o

o
o
o

o
o

o

o

o
o
o
o
o
o
o
o
o
o
o

o
o
o
o
o
o

o

o

o
o
oo
o

o
o
o

o
o

o
o
o
o
o

training error: 0.130
test error:       0.242
bayes error:    0.210

o

figure 15.11. id79s versus 3-nn on the mixture data. the axis-ori-
ented nature of the individual trees in a id79 lead to decision regions
with an axis-oriented    avor.

bibliographic notes

id79s as described here were introduced by breiman (2001), al-
though many of the ideas had cropped up earlier in the literature in dif-
ferent forms. notably ho (1995) introduced the term    id79,    and
used a consensus of trees grown in random subspaces of the features. the
idea of using stochastic perturbation and averaging to avoid over   tting was
introduced by kleinberg (1990), and later in kleinberg (1996). amit and
geman (1997) used randomized trees grown on image features for image
classi   cation problems. breiman (1996a) introduced id112, a precursor
to his version of id79s. dietterich (2000b) also proposed an im-
provement on id112 using additional randomization. his approach was
to rank the top 20 candidate splits at each node, and then select from the
list at random. he showed through simulations and real examples that this
additional randomization improved over the performance of id112. fried-
man and hall (2007) showed that sub-sampling (without replacement) is
an e   ective alternative to id112. they showed that growing and aver-
aging trees on samples of size n/2 is approximately equivalent (in terms
bias/variance considerations) to id112, while using smaller fractions of
n reduces the variance even further (through decorrelation).

there are several free software implementations of id79s. in
this chapter we used the randomforest package in r, maintained by andy
liaw, available from the cran website. this allows both split-variable se-
lection, as well as sub-sampling. adele cutler maintains a id79
website http://www.math.usu.edu/   adele/forests/ where (as of au-
gust 2008) the software written by leo breiman and adele cutler is freely

exercises

603

available. their code, and the name    id79s   , is exclusively li-
censed to salford systems for commercial release. the weka machine learn-
ing archive http://www.cs.waikato.ac.nz/ml/weka/ at waikato univer-
sity, new zealand, o   ers a free java implementation of id79s.

exercises

ex. 15.1 derive the variance formula (15.1). this appears to fail if    is
negative; diagnose the problem in this case.

ex. 15.2 show that as the number of bootstrap samples b gets large, the
oob error estimate for a id79 approaches its n -fold cv error
estimate, and that in the limit, the identity is exact.

ex. 15.3 consider the simulation model used in figure 15.7 (mease and
wyner, 2008). binary observations are generated with probabilities

pr(y = 1|x) = q + (1     2q)    i      

jxj=1

xj > j/2      ,

(15.11)

where x     u [0, 1]p, 0     q     1
2 , and j     p is some prede   ned (even)
number. describe this id203 surface, and give the bayes error rate.

ex. 15.4 suppose xi, i = 1, . . . , n are iid (  ,   2). let   x   
2 be two
bootstrap realizations of the sample mean. show that the sampling cor-
relation corr(  x   
1) and
the variance of the bagged mean   xbag. here   x is a linear statistic; id112
produces no reduction in variance for linear statistics.

2n   1     50%. along the way, derive var(  x   

2) = n

1 and   x   

1,   x   

ex. 15.5 show that the sampling correlation between a pair of random-
forest trees at a point x is given by

  (x) =

varz[e  |zt (x;   (z))]

varz[e  |zt (x;   (z))] + ezvar  |z[t (x;   (z)]

.

(15.12)

the term in the numerator is varz[   frf (x)], and the second term in the
denominator is the expected conditional variance due to the randomization
in id79s.

ex. 15.6 fit a series of random-forest classi   ers to the spam data, to explore
the sensitivity to the parameter m. plot both the oob error as well as the
test error against a suitably chosen range of values for m.

604

15. id79s

ex. 15.7 suppose we    t a id75 model to n observations with
response yi and predictors xi1, . . . , xip. assume that all variables are stan-
dardized to have mean zero and standard deviation one. let rss be the
mean-squared residual on the training data, and      the estimated coe   cient.
denote by rss   
j the mean-squared residual on the training data using the
same     , but with the n values for the jth variable randomly permuted
before the predictions are calculated. show that
j     rss] = 2     2
j ,

ep [rss   

(15.13)

where ep denotes expectation with respect to the permutation distribution.
argue that this is approximately true when the evaluations are done using
an independent test set.

16
id108

this is page 605
printer: opaque this

16.1

introduction

the idea of id108 is to build a prediction model by combining
the strengths of a collection of simpler base models. we have already seen
a number of examples that fall into this category.

id112 in section 8.7 and id79s in chapter 15 are ensemble
methods for classi   cation, where a committee of trees each cast a vote for
the predicted class. boosting in chapter 10 was initially proposed as a
committee method as well, although unlike id79s, the committee
of weak learners evolves over time, and the members cast a weighted vote.
stacking (section 8.8) is a novel approach to combining the strengths of
a number of    tted models. in fact one could characterize any dictionary
method, such as regression splines, as an ensemble method, with the basis
functions serving the role of weak learners.

bayesian methods for nonparametric regression can also be viewed as
ensemble methods: a large number of candidate models are averaged with
respect to the posterior distribution of their parameter settings (e.g. (neal
and zhang, 2006)).

id108 can be broken down into two tasks: developing a pop-
ulation of base learners from the training data, and then combining them
to form the composite predictor. in this chapter we discuss boosting tech-
nology that goes a step further; it builds an ensemble model by conducting
a regularized and supervised search in a high-dimensional space of weak
learners.

606

16. id108

an early example of a learning ensemble is a method designed for multi-
class classi   cation using error-correcting output codes (dietterich and bakiri,
1995, ecoc). consider the 10-class digit classi   cation problem, and the
coding matrix c given in table 16.1.

table 16.1. part of a 15-bit error-correcting coding matrix c for the 10-class
digit classi   cation problem. each column de   nes a two-class classi   cation prob-
lem.

digit

0
1
2
..
.
8
9

c1
1
0
1
..
.
1
0

c2
1
0
0
..
.
1
1

c3
0
1
0
..
.
0
1

c4
0
1
1
..
.
1
1

c5
0
1
0
..
.
0
0

c6
0
1
0
..
.
1
0

        
        
        
        

        
        
        

c15

1
0
1
..
.
1
0

note that the    th column of the coding matrix c    de   nes a two-class
variable that merges all the original classes into two groups. the method
works as follows:

1. learn a separate classi   er for each of the l = 15 two class problems

de   ned by the columns of the coding matrix.

2. at a test point x, let   p   (x) be the predicted id203 of a one for

the    th response.

3. de   ne   k(x) =pl

   =1 |ck          p   (x)|, the discriminant function for the
kth class, where ck    is the entry for row k and column     in table 16.1.

each row of c is a binary code for representing that class. the rows have
more bits than is necessary, and the idea is that the redundant    error-
correcting    bits allow for some inaccuracies, and can improve performance.
in fact, the full code matrix c above has a minimum hamming distance1
of 7 between any pair of rows. note that even the indicator response coding
(section 4.2) is redundant, since 10 classes require only    log2 10 = 4 bits for
their unique representation. dietterich and bakiri (1995) showed impressive
improvements in performance for a variety of multiclass problems when
classi   cation trees were used as the base classi   er.

james and hastie (1998) analyzed the ecoc approach, and showed
that random code assignment worked as well as the optimally constructed
error-correcting codes. they also argued that the main bene   t of the coding
was in variance reduction (as in id112 and id79s), because the
di   erent coded problems resulted in di   erent trees, and the decoding step
(3) above has a similar e   ect as averaging.

1the hamming distance between two vectors is the number of mismatches between

corresponding entries.

16.2 boosting and id173 paths

607

16.2 boosting and id173 paths

in section 10.12.2 of the    rst edition of this book, we suggested an analogy
between the sequence of models produced by a gradient boosting algorithm
and regularized model    tting in high-dimensional feature spaces. this was
primarily motivated by observing the close connection between a boosted
version of id75 and the lasso (section 3.4.2). these connec-
tions have been pursued by us and others, and here we present our current
thinking in this area. we start with the original motivation, which    ts more
naturally in this chapter on id108.

16.2.1 penalized regression

intuition for the success of the shrinkage strategy (10.41) of gradient boost-
ing (page 364 in chapter 10) can be obtained by drawing analogies with
penalized id75 with a large basis expansion. consider the dic-
tionary of all possible j-terminal node regression trees t = {tk} that could
be realized on the training data as basis functions in irp. the linear model
is

f (x) =

kxk=1

  ktk(x),

(16.1)

where k = card(t ). suppose the coe   cients are to be estimated by least
squares. since the number of such trees is likely to be much larger than
even the largest training data sets, some form of id173 is required.
let     (  ) solve

nxi=1 yi    

kxk=1

  ktk(xi)!2

min

            

+       j(  )         

,

(16.2)

j(  ) is a function of the coe   cients that generally penalizes larger values.
examples are

j(  ) =

j(  ) =

kxk=1
kxk=1

|  k|2

|  k|

ridge regression,

(16.3)

lasso,

(16.4)

(16.5)

both covered in section 3.4. as discussed there, the solution to the lasso
problem with moderate to large    tends to be sparse; many of the     k(  ) =
0. that is, only a small fraction of all possible trees enter the model (16.1).

608

16. id108

algorithm 16.1 forward stagewise id75.

1. initialize     k = 0, k = 1, . . . , k. set    > 0 to some small constant,

and m large.

2. for m = 1 to m :

(a) (     , k   ) = arg min  ,kpn
(b)     k            k    +       sign(     ).
3. output fm (x) =pk

k=1     ktk(x).

i=1(cid:16)yi    pk

l=1     ltl(xi)       tk(xi)(cid:17)2

.

this seems reasonable since it is likely that only a small fraction of all pos-
sible trees will be relevant in approximating any particular target function.
however, the relevant subset will be di   erent for di   erent targets. those
coe   cients that are not set to zero are shrunk by the lasso in that their
absolute values are smaller than their corresponding least squares values2:
|     k(  )| < |     k(0)|. as    increases, the coe   cients all shrink, each one
ultimately becoming zero.
owing to the very large number of basis functions tk, directly solving
(16.2) with the lasso penalty (16.4) is not possible. however, a feasible
forward stagewise strategy exists that closely approximates the e   ect of
the lasso, and is very similar to boosting and the forward stagewise algo-
rithm 10.2. algorithm 16.1 gives the details. although phrased in terms
of tree basis functions tk, the algorithm can be used with any set of ba-
sis functions. initially all coe   cients are zero in line 1; this corresponds
to    =     in (16.2). at each successive step, the tree tk    is selected that
best    ts the current residuals in line 2(a). its corresponding coe   cient     k   
is then incremented or decremented by an in   nitesimal amount in 2(b),
while all other coe   cients     k, k 6= k    are left unchanged. in principle, this
process could be iterated until either all the residuals are zero, or       = 0.
the latter case can occur if k < n , and at that point the coe   cient values
represent a least squares solution. this corresponds to    = 0 in (16.2).

after applying algorithm 16.1 with m <     iterations, many of the coef-
   cients will be zero, namely, those that have yet to be incremented. the oth-
ers will tend to have absolute values smaller than their corresponding least
squares solution values, |     k(m )| < |     k(0)|. therefore this m -iteration
solution qualitatively resembles the lasso, with m inversely related to   .
figure 16.1 shows an example, using the prostate data studied in chap-
ter 3. here, instead of using trees tk(x) as basis functions, we use the origi-

2if k > n , there is in general no unique    least squares value,    since in   nitely many
solutions will exist that    t the data perfectly. we can pick the minimum l1-norm solution
amongst these, which is the unique lasso solution.

16.2 boosting and id173 paths

609

lasso

forward stagewise

lcavol

lcavol

s
t
n
e
i
c
   
e
o
c

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
   

s
t
n
e
i
c
   
e
o
c

6

.

0

4

.

0

2

.

0

0

.

0

2

.

0
   

svi

lweight
pgg45
lbph

gleason

age

lcp

svi

lweight
pgg45
lbph

gleason

age

lcp

0.0

0.5

1.0

1.5

2.0

0

50

100

150

200

t = pk |  k|

iteration

figure 16.1. pro   les of estimated coe   cients from id75, for the
prostate data studied in chapter 3. the left panel shows the results from the lasso,
for di   erent values of the bound parameter t = pk |  k|. the right panel shows
the results of the stagewise id75 algorithm 16.1, using m = 220
consecutive steps of size    = .01.

nal variables xk themselves; that is, a multiple id75 model. the
left panel displays the pro   les of estimated coe   cients from the lasso, for

di   erent values of the bound parameter t =pk |  k|. the right panel shows

the results of the stagewise algorithm 16.1, with m = 250 and    = 0.01.
[the left and right panels of figure 16.1 are the same as figure 3.10 and
the left panel of figure 3.19, respectively.] the similarity between the two
graphs is striking.

lasso for bound parameter t =pk |  k| (and likewise for all solutions along

in some situations the resemblance is more than qualitative. for example,
if all of the basis functions tk are mutually uncorrelated, then as        0, m    
such that m        t, algorithm 16.1 yields exactly the same solution as the
the path). of course, tree-based regressors are not uncorrelated. however,
the solution sets are also identical if the coe   cients     k(  ) are all monotone
functions of   . this is often the case when the correlation between the
variables is low. when the     k(  ) are not monotone in   , then the solution
sets are not identical. the solution sets for algorithm 16.1 tend to change
less rapidly with changing values of the id173 parameter than those
of the lasso.

610

16. id108

efron et al. (2004) make the connections more precise, by characterizing
the exact solution paths in the   -limiting case. they show that the coe   -
cient paths are piece-wise linear functions, both for the lasso and forward
stagewise. this facilitates e   cient algorithms which allow the entire paths
to be computed with the same cost as a single least-squares    t. this least
angle regression algorithm is described in more detail in section 3.8.1.

hastie et al. (2007) show that this in   nitesimal forward stagewise algo-
rithm (fs0)    ts a monotone version of the lasso, which optimally reduces
at each step the id168 for a given increase in the arc length of the
coe   cient path (see sections 16.2.3 and 3.8.1). the arc-length for the    > 0
case is m   , and hence proportional to the number of steps.

tree boosting (algorithm 10.3) with shrinkage (10.41) closely resembles
algorithm 16.1, with the learning rate parameter    corresponding to   . for
squared error loss, the only di   erence is that the optimal tree to be selected
at each iteration tk    is approximated by the standard top-down greedy
tree-induction algorithm. for other id168s, such as the exponential
loss of adaboost and the binomial deviance, rosset et al. (2004a) show
similar results to what we see here. thus, one can view tree boosting with
shrinkage as a form of monotone ill-posed regression on all possible (j-
terminal node) trees, with the lasso penalty (16.4) as a regularizer. we
return to this topic in section 16.2.3.

the choice of no shrinkage [   = 1 in equation (10.41)] is analogous to
forward-stepwise regression, and its more aggressive cousin best-subset se-

lection, which penalizes the number of non zero coe   cients j(  ) =pk |  k|0.

with a small fraction of dominant variables, best subset approaches often
work well. but with a moderate fraction of strong variables, it is well known
that subset selection can be excessively greedy (copas, 1983), often yielding
poor results when compared to less aggressive strategies such as the lasso
or ridge regression. the dramatic improvements often seen when shrinkage
is used with boosting are yet another con   rmation of this approach.

16.2.2 the    bet on sparsity    principle

as shown in the previous section, boosting   s forward stagewise strategy
with shrinkage approximately minimizes the same id168 with a
lasso-style l1 penalty. the model is built up slowly, searching through
   model space    and adding shrunken basis functions derived from impor-
tant predictors. in contrast, the l2 penalty is computationally much easier
to deal with, as shown in section 12.3.7. with the basis functions and l2
penalty chosen to match a particular positive-de   nite kernel, one can solve
the corresponding optimization problem without explicitly searching over
individual basis functions.

however, the sometimes superior performance of boosting over proce-
dures such as the support vector machine may be largely due to the im-
plicit use of the l1 versus l2 penalty. the shrinkage resulting from the

16.2 boosting and id173 paths

611

l1 penalty is better suited to sparse situations, where there are few basis
functions with nonzero coe   cients (among all possible choices).

we can strengthen this argument through a simple example, taken from
friedman et al. (2004). suppose we have 10, 000 data points and our model
is a linear combination of a million trees. if the true population coe   cients
of these trees arose from a gaussian distribution, then we know that in a
bayesian sense the best predictor is ridge regression (exercise 3.6). that is,
we should use an l2 rather than an l1 penalty when    tting the coe   cients.
on the other hand, if there are only a small number (e.g., 1000) coe   cients
that are nonzero, the lasso (l1 penalty) will work better. we think of this
as a sparse scenario, while the    rst case (gaussian coe   cients) is dense.
note however that in the dense scenario, although the l2 penalty is best,
neither method does very well since there is too little data from which to
estimate such a large number of nonzero coe   cients. this is the curse of
dimensionality taking its toll. in a sparse setting, we can potentially do
well with the l1 penalty, since the number of nonzero coe   cients is small.
the l2 penalty fails again.

in other words, use of the l1 penalty follows what we call the    bet on

sparsity    principle for high-dimensional problems:

use a procedure that does well in sparse problems, since no pro-
cedure does well in dense problems.

these comments need some quali   cation:

    for any given application, the degree of sparseness/denseness depends
on the unknown true target function, and the chosen dictionary t .
    the notion of sparse versus dense is relative to the size of the train-
ing data set and/or the noise-to-signal ratio (nsr). larger training
sets allow us to estimate coe   cients with smaller standard errors.
likewise in situations with small nsr, we can identify more nonzero
coe   cients with a given sample size than in situations where the nsr
is larger.

    the size of the dictionary plays a role as well. increasing the size of the
dictionary may lead to a sparser representation for our function, but
the search problem becomes more di   cult leading to higher variance.

figure 16.2 illustrates these points in the context of linear models us-
ing simulation. we compare ridge regression and lasso, both for classi   -
cation and regression problems. each run has 50 observations with 300
independent gaussian predictors. in the top row all 300 coe   cients are
nonzero, generated from a gaussian distribution. in the middle row, only
10 are nonzero and generated from a gaussian, and the last row has 30
non zero gaussian coe   cients. for regression, standard gaussian noise is

612

16. id108

regression

classification

lasso/gaussian

ridge/gaussian

lasso/gaussian

ridge/gaussian

i

d
e
n
a
p
x
e

l

 
r
o
r
r

 

e
n
o

i

 

i
t
c
d
e
r
p
d
e
r
a
u
q
s
e
g
a

 

t

n
e
c
r
e
p

0
0
1
1

.
.

8
8
0
0

.
.

6
6
0
0

.
.

4
4
0
0

.
.

2
2
0
0

.
.

0
0
0
0

.
.

0
0
.
.
1
1

8
8
.
.
0
0

6
6
.
.
0
0

4
4
.
.
0
0

2
2
.
.
0
0

0
0
.
.
0
0

0
0
.
.
1
1

8
8
.
.
0
0

6
6
.
.
0
0

4
4
.
.
0
0

2
2
.
.
0
0

0
0
.
.
0
0

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

lasso/subset 10

ridge/subset 10

i

d
e
n
a
p
x
e

l

 
r
o
r
r

 

e
n
o

i
t

l

a
c
i
f
i
s
s
a
c
s
m
e
g
a

 

i

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

lasso/subset 30

ridge/subset 30

t

n
e
c
r
e
p

0
0
1
1

.
.

8
8
0
0

.
.

6
6
0
0

.
.

4
4
0
0

.
.

2
2
0
0

.
.

0
0
0
0

.
.

0
0
.
.
1
1

8
8
.
.
0
0

6
6
.
.
0
0

4
4
.
.
0
0

2
2
.
.
0
0

0
0
.
.
0
0

0
0
.
.
1
1

8
8
.
.
0
0

6
6
.
.
0
0

4
4
.
.
0
0

2
2
.
.
0
0

0
0
.
.
0
0

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

lasso/subset 10

ridge/subset 10

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

lasso/subset 30

ridge/subset 30

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

0.1 0.2 0.3 0.4 0.5

noise   to   signal ratio

noise   to   signal ratio

figure 16.2. simulations that show the superiority of the l1 (lasso) penalty
over l2 (ridge) in regression and classi   cation. each run has 50 observations
with 300 independent gaussian predictors. in the top row all 300 coe   cients are
nonzero, generated from a gaussian distribution. in the middle row, only 10 are
nonzero, and the last row has 30 nonzero. gaussian errors are added to the linear
predictor   (x) for the regression problems, and binary responses generated via the
inverse-logit transform for the classi   cation problems. scaling of   (x) resulted in
the noise-to-signal ratios shown. lasso is used in the left sub-columns, ridge in the
right. we report the optimal percentage of error explained on test data (relative
to the error of a constant model), displayed as boxplots over 20 realizations for
each combination. in the only situation where ridge beats lasso (top row), neither
do well.

16.2 boosting and id173 paths

613

added to the linear predictor   (x) = x t    to produce a continuous re-
sponse. for classi   cation the linear predictor is transformed via the inverse-
logit to a id203, and a binary response is generated. five di   er-
ent noise-to-signal ratios are presented, obtained by scaling   (x) prior
to generating the response. in both cases this is de   ned to be nsr =
var(y |  (x))/var(  (x)). both the ridge regression and lasso coe   cient
paths were    t using a series of 50 values of    corresponding to a range of
df from 1 to 50 (see chapter 3 for details). the models were evaluated on
a large test set (in   nite for gaussian, 5000 for binary), and in each case the
value for    was chosen to minimize the test-set error. we report percentage
variance explained for the regression problems, and percentage misclassi   -
cation error explained for the classi   cation problems (relative to a baseline
error of 0.5). there are 20 simulation runs for each scenario.

note that for the classi   cation problems, we are using squared-error loss
to    t the binary response. note also that we do not using the training
data to select   , but rather are reporting the best possible behavior for
each method in the di   erent scenarios. the l2 penalty performs poorly
everywhere. the lasso performs reasonably well in the only two situations
where it can (sparse coe   cients). as expected the performance gets worse
as the nsr increases (less so for classi   cation), and as the model becomes
denser. the di   erences are less marked for classi   cation than for regression.
these empirical results are supported by a large body of theoretical
results (donoho and johnstone, 1994; donoho and elad, 2003; donoho,
2006b; candes and tao, 2007) that support the superiority of l1 estimation
in sparse settings.

16.2.3 id173 paths, over-   tting and margins

it has often been observed that boosting    does not over   t,    or more as-
tutely is    slow to over   t.    part of the explanation for this phenomenon was
made earlier for id79s     misclassi   cation error is less sensitive to
variance than is mean-squared error, and classi   cation is the major focus
in the boosting community. in this section we show that the regulariza-
tion paths of boosted models are    well behaved,    and that for certain loss
functions they have an appealing limiting form.

figure 16.3 shows the coe   cient paths for lasso and in   nitesimal forward
stagewise (fs0) in a simulated regression setting. the data consists of a
dictionary of 1000 gaussian variables, strongly correlated (   = 0.95) within
blocks of 20, but uncorrelated between blocks. the generating model has
nonzero coe   cients for 50 variables, one drawn from each block, and the
coe   cient values are drawn from a standard gaussian. finally, gaussian
noise is added, with a noise-to-signal ratio of 0.72 (exercise 16.1.) the
fs0 algorithm is a limiting form of algorithm 16.1, where the step size   
is shrunk to zero (section 3.8.1). the grouping of the variables is intended
to mimic the correlations of nearby trees, and with the forward-stagewise

614

16. id108

lasso

forward stagewise

i

s
t
n
e
c
i
f
f
e
o
c
 
d
e
z
d
r
a
d
n
a
t
s

i

0
3

0
2

0
1

0

0
1
   

0
2
   

i

s
t
n
e
c
i
f
f
e
o
c
 
d
e
z
d
r
a
d
n
a
t
s

i

0
3

0
2

0
1

0

0
1
   

0
2
   

0.0

0.2

0.4

0.6

0.8

1.0

0.0

0.2

0.4

0.6

0.8

1.0

|  (m)|/|  (   )|

|  (m)|/|  (   )|

figure 16.3. comparison of lasso and in   nitesimal forward stagewise paths
on simulated regression data. the number of samples is 60 and the number of
variables is 1000. the forward-stagewise paths    uctuate less than those of lasso
in the    nal stages of the algorithms.

algorithm, this setup is intended as an idealized version of gradient boosting
with shrinkage. for both these algorithms, the coe   cient paths can be
computed exactly, since they are piecewise linear (see the lars algorithm
in section 3.8.1).

here the coe   cient pro   les are similar only in the early stages of the
paths. for the later stages, the forward stagewise paths tend to be mono-
tone and smoother, while those for the lasso    uctuate widely. this is due
to the strong correlations among subsets of the variables    lasso su   ers
somewhat from the multi-collinearity problem (exercise 3.28).

the performance of the two models is rather similar (figure 16.4), and
they achieve about the same minimum. in the later stages forward stagewise
takes longer to over   t, a likely consequence of the smoother paths.

hastie et al. (2007) show that fs0 solves a monotone version of the lasso
problem for squared error loss. let t a = t     {   t } be the augmented
dictionary obtained by including a negative copy of every basis element

in t . we consider models f (x) =ptk   t a   ktk(x) with non-negative co-

e   cients   k     0. in this expanded space, the lasso coe   cient paths are
positive, while those of fs0 are monotone nondecreasing.

the monotone lasso path is characterized by a di   erential equation

     
      

=   ml(  (   )),

(16.6)

16.2 boosting and id173 paths

615

lasso
forward stagewise

ooooooooooo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo
ooooooooo ooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

ooooooooooooo

oooooooo
ooooooo

o
o
oo
oo
o
o

o
oo

oo
oo

o
o
ooo
oo
ooooo
oooo

o
oooooo
oo

r
o
r
r
e

d
e
r
a
u
q
s

n
a
e
m

5
5

0
5

5
4

0
4

5
3

0
3

5
2

0

10

20

30

40

50

60

70

|  (m)|

figure 16.4. mean squared error for lasso and in   nitesimal forward stagewise
on the simulated data. despite the di   erence in the coe   cient paths, the two
models perform similarly over the critical part of the id173 path. in the
right tail, lasso appears to over   t more rapidly.

with initial condition   (0) = 0, where     is the l1 arc-length of the path
  (   ) (exercise 16.2). the monotone lasso move direction (velocity vector)
  ml(  (   )) decreases the loss at the optimal quadratic rate per unit increase
in the l1 arc-length of the path. since   ml
k (  (   ))     0    k,    , the solution
paths are monotone.
the lasso can similarly be characterized as the solution to a di   erential
equation as in (16.6), except that the move directions decrease the loss
optimally per unit increase in the l1 norm of the path. as a consequence,
they are not necessarily positive, and hence the lasso paths need not be
monotone.

in this augmented dictionary, restricting the coe   cients to be positive is
natural, since it avoids an obvious ambiguity. it also ties in more naturally
with tree boosting   we always    nd trees positively correlated with the
current residual.

there have been suggestions that boosting performs well (for two-class
classi   cation) because it exhibits maximal-margin properties, much like the
support-vector machines of chapters 4.5.2 and 12. schapire et al. (1998)

de   ne the normalized l1 margin of a    tted model f (x) =pk   ktk(x) as

(16.7)

m(f ) = min

i

yif (xi)
k=1 |  k|

.

pk

here the minimum is taken over the training sample, and yi     {   1, +1}.
unlike the l2 margin (4.40) of support vector machines, the l1 margin
m(f ) measures the distance to the closest training point in l    units (max-
imum coordinate distance).

616

16. id108

i

n
g
r
a
m

1

.

0

0

.

0

1

.

0
   

2

.

0
   

3

.

0
   

r
o
r
r

e

 
t
s
e
t

8
2

.

0

7
2

.

0

6
2

.

0

5
2

.

0

0

2k

4k

6k

8k

10k

0

2k

4k

6k

8k

10k

number of trees

number of trees

figure 16.5. the left panel shows the l1 margin m(f ) for the adaboost clas-
si   er on the mixture data, as a function of the number of 4-node trees. the model
was    t using the r package gbm, with a shrinkage factor of 0.02. after 10, 000
trees, m(f ) has settled down. note that when the margin crosses zero, the training
error becomes zero. the right panel shows the test error, which is minimized at
240 trees. in this case, adaboost over   ts dramatically if run to convergence.

schapire et al. (1998) prove that with separable data, adaboost in-
creases m(f ) with each iteration, converging to a margin-symmetric so-
lution. r  atsch and warmuth (2002) prove the asymptotic convergence of
adaboost with shrinkage to a l1-margin-maximizing solution. rosset et
al. (2004a) consider regularized models of the form (16.2) for general loss
functions. they show that as        0, for particular id168s the solution
converges to a margin-maximizing con   guration. in particular they show
this to be the case for the exponential loss of adaboost, as well as binomial
deviance.

collecting together the results of this section, we reach the following

summary for boosted classi   ers:

the sequence of boosted classi   ers form an l1-regularized mono-
tone path to a margin-maximizing solution.

of course the margin-maximizing end of the path can be a very poor, over   t
solution, as it is in the example in figure 16.5. early stopping amounts
to picking a point along the path, and should be done with the aid of a
validation dataset.

16.3 learning ensembles

the insights learned from the previous sections can be harnessed to produce
a more e   ective and e   cient ensemble model. again we consider functions

16.3 learning ensembles

617

  ktk(x),

(16.8)

of the form

f (x) =   0 + xtk   t

where t is a dictionary of basis functions, typically trees. for gradient
boosting and id79s, |t | is very large, and it is quite typical for the
   nal model to involve many thousands of trees. in the previous section we
argue that gradient boosting with shrinkage    ts an l1 regularized monotone
path in this space of trees.

friedman and popescu (2003) propose a hybrid approach which breaks

this process down into two stages:

    a    nite dictionary tl = {t1(x), t2(x), . . . , tm (x)} of basis functions

is induced from the training data;

    a family of functions f  (x) is built by    tting a lasso path in this

dictionary:

  (  ) = arg min

  

l[yi,   0 +

nxi=1

mxm=1

  mtm(xi)] +   

mxm=1

|  m|.

(16.9)

in its simplest form this model could be seen as a way of post-processing
boosting or id79s, taking for tl the collection of trees produced
by the gradient boosting or id79 algorithms. by    tting the lasso
path to these trees, we would typically use a much reduced set, which would
save in computations and storage for future predictions. in the next section
we describe modi   cations of this prescription that reduce the correlations in
the ensemble tl, and improve the performance of the lasso post processor.
as an initial illustration, we apply this procedure to a id79
ensemble grown on the spam data.

figure 16.6 shows that a lasso post-processing o   ers modest improve-
ment over the id79 (blue curve), and reduces the forest to about
40 trees, rather than the original 1000. the post-processed performance
matches that of gradient boosting. the orange curves represent a modi   ed
version of id79s, designed to reduce the correlations between trees
even more. here a random sub-sample (without replacement) of 5% of the
training sample is used to grow each tree, and the trees are restricted to be
shallow (about six terminal nodes). the post-processing o   ers more dra-
matic improvements here, and the training costs are reduced by a factor
of about 100. however, the performance of the post-processed model falls
somewhat short of the blue curves.

16.3.1 learning a good ensemble
not all ensembles tl will perform well with post-processing. in terms of
basis functions, we want a collection that covers the space well in places

spam data

id79
id79 (5%, 6)
gradient boost (5 node)

618

16. id108

r
o
r
r

e

 
t
s
e
t

9
0

.

0

8
0

.

0

7
0
0

.

6
0
0

.

5
0

.

0

4
0

.
0

0

100

200

300

400

500

number of trees

figure 16.6. application of the lasso post-processing (16.9) to the spam data.
the horizontal blue line is the test error of a id79    t to the spam data,
using 1000 trees grown to maximum depth (with m = 7; see algorithm 15.1).
the jagged blue curve is the test error after post-processing the    rst 500 trees
using the lasso, as a function of the number of trees with nonzero coe   cients.
the orange curve/line use a modi   ed form of id79, where a random
draw of 5% of the data are used to grow each tree, and the trees are forced to
be shallow (typically six terminal nodes). here the post-processing o   ers much
greater improvement over the id79 that generated the ensemble.

where they are needed, and are su   ciently di   erent from each other for
the post-processor to be e   ective.

friedman and popescu (2003) gain insights from numerical quadrature
and importance sampling. they view the unknown function as an integral

f (x) =z   (  )b(x;   )d  ,

(16.10)

where           indexes the basis functions b(x;   ). for example, if the basis
functions are trees, then    indexes the splitting variables, the split-points
and the values in the terminal nodes. numerical quadrature amounts to
   nding a set of m evaluation points   m        and corresponding weights
m=1   mb(x;   m) approximates f (x) well over
the domain of x. importance sampling amounts to sampling    at random,
but giving more weight to relevant regions of the space   . friedman and
popescu (2003) suggest a measure of (lack of) relevance that uses the loss
function (16.9):

  m so that fm (x) =   0 +pm

16.3 learning ensembles

619

l(yi, c0 + c1b(xi;   )),

(16.11)

q(  ) = min
c0,c1

nxi=1

evaluated on the training data.

if a single basis function were to be selected (e.g., a tree), it would be
the global minimizer       = arg min        q(  ). introducing randomness in the
selection of    would necessarily produce less optimal values with q(  )    
q(     ). they propose a natural measure of the characteristic width    of the
sampling scheme s,

   = es [q(  )     q(     )].

(16.12)

       too narrow suggests too many of the b(x;   m) look alike, and similar

to b(x;      );

       too wide implies a large spread in the b(x;   m), but possibly con-

sisting of many irrelevant cases.

friedman and popescu (2003) use sub-sampling as a mechanism for intro-
ducing randomness, leading to their ensemble-generation algorithm 16.2.

algorithm 16.2 isle ensemble generation.

i=1 l(yi, c)

2. for m = 1 to m do

1. f0(x) = arg mincpn
(a)   m = arg min  pi   sm(  ) l(yi, fm   1(xi) + b(xi;   ))

(b) fm(x) = fm   1(x) +   b(x;   m)

3. tisle = {b(x;   1), b(x;   2), . . . , b(x;   m )}.

sm(  ) refers to a subsample of n       (       (0, 1]) of the training obser-
vations, typically without replacement. their simulations suggest picking
2 , and for large n picking        1/   n . reducing    increases the
       1
randomness, and hence the width   . the parameter        [0, 1] introduces
memory into the randomization process; the larger   , the more the pro-
cedure avoids b(x;   ) similar to those found before. a number of familiar
randomization schemes are special cases of algorithm 16.2:

id112 has    = 1, but samples with replacement, and has    = 0. fried-
man and hall (2007) argue that sampling without replacement with
   = 1/2 is equivalent to sampling with replacement with    = 1, and
the former is much more e   cient.

620

16. id108

id79 sampling is similar, with more randomness introduced by
the selection of the splitting variable. reducing    < 1/2 in algo-
rithm 16.2 has a similar e   ect to reducing m in id79s, but
does not su   er from the potential biases discussed in section 15.4.2.

gradient boosting with shrinkage (10.41) uses    = 1, but typically does

not produce su   cient width   .

stochastic gradient boosting (friedman, 1999) follows the recipe exactly.

the authors recommend values    = 0.1 and        1
2 , and call their combined
procedure (ensemble generation and post processing) importance sampled
learning ensemble (isle).

figure 16.7 shows the performance of an isle on the spam data. it does

spam data

gradient boosting (5 node)
lasso post   processed

r
o
r
r

e

 
t
s
e
t

0
6
0
.
0

5
5
0
.
0

0
5
0
.
0

5
4
0
.
0

0
4
0
.
0

0

500

1000

1500

2000

2500

number of trees

figure 16.7. importance sampling learning ensemble (isle)    t to the spam
data. here we used    = 1/2,    = 0.05, and trees with    ve terminal nodes. the
lasso post-processed ensemble does not improve the prediction error in this case,
but it reduces the number of trees by a factor of    ve.

not improve the predictive performance, but is able to produce a more
parsimonious model. note that in practice the post-processing includes
the selection of the id173 parameter    in (16.9), which would be

16.3 learning ensembles

621

chosen by cross-validation. here we simply demonstrate the e   ects of post-
processing by showing the entire path on the test data.

figure 16.8 shows various isles on a regression example. the generating

gbm (1, 0.01)
gbm (0.1, 0.01)
isle  gb
isle rf
id79

5

.

3

0

.

3

5

.

2

0

.

2

5
.
1

0
.
1

r
o
r
r

 

e
d
e
r
a
u
q
s
 
n
a
e
m

0

500

1000

1500

2000

2500

number of trees

figure 16.8. demonstration of ensemble methods on a regression simulation
example. the notation gbm (0.1, 0.01) refers to a gradient boosted model, with
parameters (  ,   ). we report mean-squared error from the true (known) function.
note that the sub-sampled gbm model (green) outperforms the full gbm model
(orange). the lasso post-processed version achieves similar error. the random
forest is outperformed by its post-processed version, but both fall short of the
other models.

function is

f (x) = 10   

e   2x 2

j +

5yj=1

xj,

35xj=6

(16.13)

where x     u [0, 1]100 (the last 65 elements are noise variables). the re-
sponse y = f (x) +    where        n (0,   2); we chose    = 1.3 resulting in a
signal-to-noise ratio of approximately 2. we used a training sample of size
1000, and estimated the mean squared error e(   f (x)   f (x))2 by averaging
over a test set of 500 samples. the sub-sampled gbm curve (light blue)
is an instance of stochastic gradient boosting (friedman, 1999) discussed in
section 10.12, and it outperforms gradient boosting on this example.

622

16. id108

16.3.2 rule ensembles

here we describe a modi   cation of the tree-ensemble method that focuses
on individual rules (friedman and popescu, 2003). we encountered rules
in section 9.3 in the discussion of the prim method. the idea is to enlarge
an ensemble of trees by constructing a set of rules from each of the trees
in the collection.

0

x1 < 2.1

x1     2.1

1

2

x3     {s}

x3     {m, l}

3

4

x7 < 4.5

x7     4.5

5

6

figure 16.9. a typical tree in an ensemble, from which rules can be derived.

figure 16.9 depicts a small tree, with numbered nodes. the following

rules can be derived from this tree:

r1(x) = i(x1 < 2.1)
r2(x) = i(x1     2.1)
r3(x) = i(x1     2.1)    i(x3     {s})
r4(x) = i(x1     2.1)    i(x3     {m, l})
r5(x) = i(x1     2.1)    i(x3     {s})    i(x7 < 4.5)
r6(x) = i(x1     2.1)    i(x3     {s})    i(x7     4.5)

(16.14)

a linear expansion in rules 1, 4, 5 and 6 is equivalent to the tree itself
(exercise 16.3); hence (16.14) is an over-complete basis for the tree.

for each tree tm in an ensemble t , we can construct its mini-ensemble

rule, and then combine them all to form a larger ensemble

of rules t m

trule =

m[m=1

t m
rule.

(16.15)

this is then treated like any other ensemble, and post-processed via the
lasso or similar regularized procedure.

there are several advantages to this approach of deriving rules from the

more complex trees:

    the space of models is enlarged, and can lead to improved perfor-

mance.

16.3 learning ensembles

623

3
.
1

2
.
1

1
.
1

0
.
1

9
.
0

r
o
r
r

e
 
d
e
r
a
u
q
s
 
n
a
e
m

rules

rules + linear

figure 16.10. mean squared error for rule ensembles, using 20 realizations
of the simulation example (16.13).

    rules are easier to interpret than trees, so there is the potential for

a simpli   ed model.

    it is often natural to augment t rule by including each variable xj
separately as well, thus allowing the ensemble to model linear func-
tions well.

friedman and popescu (2008) demonstrate the power of this procedure on a
number of illustrative examples, including the simulation example (16.13).
figure 16.10 shows boxplots of the mean-squared error from the true model
for twenty realizations from this model. the models were all    t using the
rulefit software, available on the esl homepage3, which runs in an auto-
matic mode.

on the same training set as used in figure 16.8, the rule based model
achieved a mean-squared error of 1.06. although slightly worse than the
best achieved in that    gure, the results are not comparable because cross-
validation was used here to select the    nal model.

bibliographic notes

as noted in the introduction, many of the new methods in machine learning
have been dubbed    ensemble    methods. these include neural networks
boosting, id112 and id79s; dietterich (2000a) gives a survey of
tree-based ensemble methods. neural networks (chapter 11) are perhaps
more deserving of the name, since they simultaneously learn the parameters

3esl homepage: www-stat.stanford.edu/elemstatlearn

624

16. id108

of the hidden units (basis functions), along with how to combine them.
bishop (2006) discusses neural networks in some detail, along with the
bayesian perspective (mackay, 1992; neal, 1996). support vector machines
(chapter 12) can also be regarded as an ensemble method; they perform
l2 regularized model    tting in high-dimensional feature spaces. boosting
and lasso exploit sparsity through l1 id173 to overcome the high-
dimensionality, while id166s rely on the    kernel trick    characteristic of l2
id173.

c5.0 (quinlan, 2004) is a commercial tree and rule generation package,

with some goals in common with rulefit.

there is a vast and varied literature often referred to as    combining clas-
si   ers    which abounds in ad-hoc schemes for mixing methods of di   erent
types to achieve better performance. for a principled approach, see kittler
et al. (1998).

exercises

ex. 16.1 describe exactly how to generate the block correlated data used
in the simulation in section 16.2.3.
ex. 16.2 let   (t)     irp be a piecewise-di   erentiable and continuous coef-
   cient pro   le, with   (0) = 0. the l1 arc-length of    from time 0 to t is
de   ned by

  (t) =z t

0 |     (t)|1dt.

(16.16)

show that   (t)     |  (t)|1, with equality i      (t) is monotone.
ex. 16.3 show that    tting a id75 model using rules 1, 4, 5 and
6 in equation (16.14) gives the same    t as the regression tree corresponding
to this tree. show the same is true for classi   cation, if a id28
model is    t.

ex. 16.4 program and run the simulation study described in figure 16.2.

17
undirected id114

this is page 625
printer: opaque this

17.1

introduction

a graph consists of a set of vertices (nodes), along with a set of edges join-
ing some pairs of the vertices. in id114, each vertex represents
a random variable, and the graph gives a visual way of understanding the
joint distribution of the entire set of random variables. they can be use-
ful for either unsupervised or supervised learning. in an undirected graph,
the edges have no directional arrows. we restrict our discussion to undi-
rected id114, also known as markov random    elds or markov
networks. in these graphs, the absence of an edge between two vertices has
a special meaning: the corresponding random variables are conditionally
independent, given the other variables.

figure 17.1 shows an example of a graphical model for a    ow-cytometry
dataset with p = 11 proteins measured on n = 7466 cells, from sachs
et al. (2005). each vertex in the graph corresponds to the real-valued ex-
pression level of a protein. the network structure was estimated assuming
a multivariate gaussian distribution, using the graphical lasso procedure
discussed later in this chapter.

sparse graphs have a relatively small number of edges, and are convenient
for interpretation. they are useful in a variety of domains, including ge-
nomics and proteomics, where they provide rough models of cell pathways.
much work has been done in de   ning and understanding the structure of
id114; see the bibliographic notes for references.

626

17. undirected id114

raf

mek

jnk

plcg

pip2

p38

pkc

pip3

pka

erk

akt

figure 17.1. example of a sparse undirected graph, estimated from a    ow
cytometry dataset, with p = 11 proteins measured on n = 7466 cells. the net-
work structure was estimated using the graphical lasso procedure discussed in this
chapter.

as we will see, the edges in a graph are parametrized by values or po-
tentials that encode the strength of the conditional dependence between
the random variables at the corresponding vertices. the main challenges in
working with id114 are model selection (choosing the structure
of the graph), estimation of the edge parameters from data, and compu-
tation of marginal vertex probabilities and expectations, from their joint
distribution. the last two tasks are sometimes called learning and id136
in the computer science literature.

we do not attempt a comprehensive treatment of this interesting area.
instead, we introduce some basic concepts, and then discuss a few sim-
ple methods for estimation of the parameters and structure of undirected
id114; methods that relate to the techniques already discussed
in this book. the estimation approaches that we present for continuous
and discrete-valued vertices are di   erent, so we treat them separately. sec-
tions 17.3.1 and 17.3.2 may be of particular interest, as they describe new,
regression-based procedures for estimating id114.

there is a large and active literature on directed id114 or
id110s; these are id114 in which the edges have
directional arrows (but no directed cycles). directed id114 rep-
resent id203 distributions that can be factored into products of condi-
tional distributions, and have the potential for causal interpretations. we
refer the reader to wasserman (2004) for a brief overview of both undi-
rected and directed graphs; the next section follows closely his chapter 18.

17.2 markov graphs and their properties

627

y

(a)

z

z

w

x

y

x

y

x

w

z

(b)

x

y

z

w

(c)

(d)

figure 17.2. examples of undirected id114 or markov networks.
each node or vertex represents a random variable, and the lack of an edge between
two nodes indicates conditional independence. for example, in graph (a), x and
z are conditionally independent, given y . in graph (b), z is independent of each
of x, y , and w .

a longer list of useful references is given in the bibliographic notes on
page 645.

17.2 markov graphs and their properties

in this section we discuss the basic properties of graphs as models for the
joint distribution of a set of random variables. we defer discussion of (a)
parametrization and estimation of the edge parameters from data, and (b)
estimation of the topology of a graph, to later sections.

figure 17.2 shows four examples of undirected graphs. a graph g consists
of a pair (v, e), where v is a set of vertices and e the set of edges (de   ned
by pairs of vertices). two vertices x and y are called adjacent if there
is a edge joining them; this is denoted by x     y . a path x1, x2, . . . , xn
is a set of vertices that are joined, that is xi   1     xi for i = 2, . . . , n. a
complete graph is a graph with every pair of vertices joined by an edge.
a subgraph u     v is a subset of vertices together with their edges. for
example, (x, y, z) in figure 17.2(a) form a path but not a complete graph.
suppose that we have a graph g whose vertex set v represents a set of
random variables having joint distribution p . in a markov graph g, the
absence of an edge implies that the corresponding random variables are
conditionally independent given the variables at the other vertices. this is
expressed with the following notation:

628

17. undirected id114

no edge joining x and y        x     y |rest

(17.1)

where    rest    refers to all of the other vertices in the graph. for example
in figure 17.2(a) x     z|y . these are known as the pairwise markov
independencies of g.
if a, b and c are subgraphs, then c is said to separate a and b if every
path between a and b intersects a node in c. for example, y separates
x and z in figures 17.2(a) and (d), and z separates y and w in (d). in
figure 17.2(b) z is not connected to x, y, w so we say that the two sets
are separated by the empty set. in figure 17.2(c), c = {x, z} separates
y and w .
separators have the nice property that they break the graph into con-
ditionally independent pieces. speci   cally, in a markov graph g with sub-
graphs a, b and c,

if c separates a and b then a     b|c.

(17.2)

these are known as the global markov properties of g. it turns out that the
pairwise and global markov properties of a graph are equivalent (for graphs
with positive distributions). that is, the set of graphs with associated prob-
ability distributions that satisfy the pairwise markov independencies and
global markov assumptions are the same. this result is useful for inferring
global independence relations from simple pairwise properties. for example
in figure 17.2(d) x     z|{y, w} since it is a markov graph and there is no
link joining x and z. but y also separates x from z and w and hence by
the global markov assumption we conclude that x     z|y and x     w|y .
similarly we have y     w|z.
the global markov property allows us to decompose graphs into smaller
more manageable pieces and thus leads to essential simpli   cations in com-
putation and interpretation. for this purpose we separate the graph into
cliques. a clique is a complete subgraph    a set of vertices that are all
adjacent to one another; it is called maximal if it is a clique and no other
vertices can be added to it and still yield a clique. the maximal cliques for
the graphs of figure 17.2 are

(a) {x, y },{y, z},
(b) {x, y, w},{z},
(c) {x, y },{y, z},{z, w},{x, w}, and
(d) {x, y },{y, z},{z, w}.
although the following applies to both continuous and discrete distri-
butions, much of the development has been for the latter. a id203
density function f over a markov graph g can be can represented as

17.2 markov graphs and their properties

629

f (x) =

  c(xc)

(17.3)

1

z yc   c

z = xx   x yc   c

where c is the set of maximal cliques, and the positive functions   c(  ) are
called clique potentials. these are not in general density functions1, but
rather are a   nities that capture the dependence in xc by scoring certain
instances xc higher than others. the quantity

  c(xc)

(17.4)

is the normalizing constant, also known as the partition function. alterna-
tively, the representation (17.3) implies a graph with independence prop-
erties de   ned by the cliques in the product. this result holds for markov
networks g with positive distributions, and is known as the hammersley-
cli   ord theorem (hammersley and cli   ord, 1971; cli   ord, 1990).
many of the methods for estimation and computation on graphs    rst de-
compose the graph into its maximal cliques. relevant quantities are com-
puted in the individual cliques and then accumulated across the entire
graph. a prominent example is the join tree or junction tree algorithm for
computing marginal and low order probabilities from the joint distribution
on a graph. details can be found in pearl (1986), lauritzen and spiegel-
halter (1988), pearl (1988), shenoy and shafer (1988), jensen et al. (1990),
or koller and friedman (2007).

y

x

z

figure 17.3. a complete graph does not uniquely specify the higher-order
dependence structure in the joint distribution of the variables.

a graphical model does not always uniquely specify the higher-order
dependence structure of a joint id203 distribution. consider the com-
plete three-node graph in figure 17.3. it could represent the dependence
structure of either of the following distributions:

f (2)(x, y, z) = 1
f (3)(x, y, z) = 1

z   (x, y)  (x, z)  (y, z);
z   (x, y, z).

(17.5)

the    rst speci   es only second order dependence (and can be represented
with fewer parameters). id114 for discrete data are a special

1if the cliques are separated, then the potentials can be densities, but this is in general

not the case.

630

17. undirected id114

case of loglinear models for multiway contingency tables (bishop et al.,
1975, e.g.); in that language f (2) is referred to as the    no second-order
interaction    model.

for the remainder of this chapter we focus on pairwise markov graphs
(koller and friedman, 2007). here there is a potential function for each
edge (pair of variables as in f (2) above), and at most second   order interac-
tions are represented. these are more parsimonious in terms of parameters,
easier to work with, and give the minimal complexity implied by the graph
structure. the models for both continuous and discrete data are functions
of only the pairwise marginal distributions of the variables represented in
the edge set.

17.3 undirected id114 for continuous

variables

here we consider markov networks where all the variables are continuous.
the gaussian distribution is almost always used for such id114,
because of its convenient analytical properties. we assume that the observa-
tions have a multivariate gaussian distribution with mean    and covariance
matrix   . since the gaussian distribution represents at most second-order
relationships, it automatically encodes a pairwise markov graph. the graph
in figure 17.1 is an example of a gaussian graphical model.

the gaussian distribution has the property that all conditional distri-
butions are also gaussian. the inverse covariance matrix      1 contains
information about the partial covariances between the variables; that is,
the covariances between pairs i and j, conditioned on all other variables.
in particular, if the ijth component of    =      1 is zero, then variables i and
j are conditionally independent, given the other variables (exercise 17.3).
it is instructive to examine the conditional distribution of one variable
versus the rest, where the role of    is explicit. suppose we partition x =
(z, y ) where z = (x1, . . . , xp   1) consists of the    rst p     1 variables and
y = xp is the last. then we have the conditional distribution of y give z
(mardia et al., 1979, e.g.)

y |z = z     n(cid:0)  y + (z       z)t      1

where we have partitioned    as

zz  zy ,   y y       t

zy      1

zz   zy(cid:1) ,

   =(cid:18)  zz   zy
  y y(cid:19) .

  t
zy

(17.6)

(17.7)

the conditional mean in (17.6) has exactly the same form as the pop-
ulation multiple id75 of y on z, with regression coe   cient
   =      1
zz  zy [see (2.16) on page 19]. if we partition    in the same way,
since      = i standard formulas for partitioned inverses give

17.3 undirected id114 for continuous variables

631

where 1/  y y =   y y       t

  zy =      y y         1
zy      1

zz   zy > 0. hence

zz  zy ,

   =      1

zz   zy

=      zy /  y y .

(17.8)

(17.9)

we have learned two things here:

    the dependence of y on z in (17.6) is in the mean term alone. here
we see explicitly that zero elements in    and hence   zy mean that
the corresponding elements of z are conditionally independent of y ,
given the rest.

    we can learn about this dependence structure through multiple linear

regression.

thus    captures all the second-order information (both structural and
quantitative) needed to describe the conditional distribution of each node
given the rest, and is the so-called    natural    parameter for the gaussian
graphical model2.

another (di   erent) kind of graphical model is the covariance graph or rel-
evance network, in which vertices are connected by bidirectional edges if the
covariance (rather than the partial covariance) between the corresponding
variables is nonzero. these are popular in genomics, see especially butte
et al. (2000). the negative log-likelihood from these models is not convex,
making the computations more challenging (chaudhuri et al., 2007).

17.3.1 estimation of the parameters when the graph

structure is known

given some realizations of x, we would like to estimate the parameters
of an undirected graph that approximates their joint distribution. suppose
   rst that the graph is complete (fully connected). we assume that we have
n multivariate normal realizations xi, i = 1, . . . , n with population mean
   and covariance   . let

s =

1
n

nxi=1

(xi       x)(xi       x)t

(17.10)

be the empirical covariance matrix, with   x the sample mean vector. ignoring
constants, the log-likelihood of the data can be written as

2the distribution arising from a gaussian graphical model is a wishart distribution.
this is a member of the exponential family, with canonical or    natural    parameter
   =      1. indeed, the partially maximized log-likelihood (17.11) is (up to constants)
the wishart log-likelihood.

632

17. undirected id114

   (  ) = log det        trace(s  ).

(17.11)

in (17.11) we have partially maximized with respect to the mean parameter
  . the quantity       (  ) is a convex function of   . it is easy to show that
the maximum likelihood estimate of    is simply s.
now to make the graph more useful (especially in high-dimensional set-
tings) let   s assume that some of the edges are missing; for example, the
edge between pip3 and erk is one of several missing in figure 17.1. as we
have seen, for the gaussian distribution this implies that the correspond-
ing entries of    =      1 are zero. hence we now would like to maximize
(17.11) under the constraints that some pre-de   ned subset of the parame-
ters are zero. this is an equality-constrained id76 problem,
and a number of methods have been proposed for solving it, in particular
the iterative proportional    tting procedure (speed and kiiveri, 1986). this
and other methods are summarized for example in whittaker (1990) and
lauritzen (1996). these methods exploit the simpli   cations that arise from
decomposing the graph into its maximal cliques, as described in the previ-
ous section. here we outline a simple alternate approach, that exploits the
sparsity in a di   erent way. the fruits of this approach will become apparent
later when we discuss the problem of estimation of the graph structure.

the idea is based on id75, as inspired by (17.6) and (17.9).
in particular, suppose that we want to estimate the edge parameters   ij for
the vertices that are joined to a given vertex i, restricting those that are not
joined to be zero. then it would seem that the id75 of the node
i values on the other relevant vertices might provide a reasonable estimate.
but this ignores the dependence structure among the predictors in this
regression. it turns out that if instead we use our current (model-based)
estimate of the cross-product matrix of the predictors when we perform
our regressions, this gives the correct solutions and solves the constrained
maximum-likelihood problem exactly. we now give details.

to constrain the log-likelihood (17.11), we add lagrange constants for

all missing edges

   c(  ) = log det        trace(s  )     x(j,k)6   e

  jk  jk.

(17.12)

the gradient equation for maximizing (17.12) can be written as

     1     s        = 0,

(17.13)

using the fact that the derivative of log det    equals      1 (boyd and van-
denberghe, 2004, for example, page 641).    is a matrix of lagrange param-
eters with nonzero values for all pairs with edges absent.

we will show how we can use regression to solve for    and its inverse
w =      1 one row and column at a time. for simplicity let   s focus on the
last row and column. then the upper right block of equation (17.13) can
be written as

17.3 undirected id114 for continuous variables

633

w12     s12       12 = 0.

(17.14)

here we have partitioned the matrices into two parts as in (17.7): part 1
being the    rst p    1 rows and columns, and part 2 the pth row and column.
with w and its inverse    partitioned in a similar fashion, we have

(cid:18)w11 w12
12 w22(cid:19)(cid:18)  11

  t
12

wt

  12

  22(cid:19) =(cid:18) i

0t

this implies

w12 =    w11  12/  22

= w11  

0

1(cid:19) .

(17.15)

(17.16)

(17.17)

where    =      12/  22 as in (17.9). now substituting (17.17) into (17.14)
gives
(17.18)
these can be interpreted as the p     1 estimating equations for the con-
strained regression of xp on the other predictors, except that the observed
mean cross-products matrix s11 is replaced by w11, the current estimated
covariance matrix from the model.

w11       s12       12 = 0.

now we can solve (17.18) by simple subset regression. suppose there are
p   q nonzero elements in   12   i.e., p   q edges constrained to be zero. these
p     q rows carry no information and can be removed. furthermore we can
reduce    to       by removing its p     q zero elements, yielding the reduced
q    q system of equations

(17.19)
12. this is padded with p     q zeros to give     .
although it appears from (17.16) that we only recover the elements   12

with solution         = w   
11

12 = 0,

   1s   

11          s   

w   

up to a scale factor 1/  22, it is easy to show that

1
  22

= w22     wt
12  

(17.20)

(using partitioned inverse formulas). also w22 = s22, since the diagonal of
   in (17.13) is zero.

this leads to the simple iterative procedure given in algorithm 17.1 for
estimating both   w and its inverse     , subject to the constraints of the
missing edges.

note that this algorithm makes conceptual sense. the graph estimation
problem is not p separate regression problems, but rather p coupled prob-
lems. the use of the common w in step (b), in place of the observed
cross-products matrix, couples the problems together in the appropriate
fashion. surprisingly, we were not able to    nd this procedure in the lit-
erature. however it is related to the covariance selection procedures of

634

17. undirected id114

algorithm 17.1 a modi   ed regression algorithm for estimation of an
undirected gaussian graphical model with known structure.

1. initialize w = s.

2. repeat for j = 1, 2, . . . , p, 1, . . . until convergence:

(a) partition the matrix w into part 1: all but the jth row and

column, and part 2: the jth row and column.

(b) solve w   

11          s   

12 = 0 for the unconstrained edge parameters
     , using the reduced system of equations as in (17.19). obtain
     by padding         with zeros in the appropriate positions.

(c) update w12 = w11     

3. in the    nal cycle (for each j) solve for     12 =                 22, with 1/    22 =

s22     wt

12

    .

x3

x2

x4

x1

s =            

10
1
5
4

1
10
2
6

5
2
10
3

4
6
3
10

            

figure 17.4. a simple graph for illustration, along with the empirical covari-
ance matrix.

dempster (1972), and is similar in    avor to the iterative conditional    tting
procedure for covariance graphs, proposed by chaudhuri et al. (2007).

here is a little example, borrowed from whittaker (1990). suppose that
our model is as depicted in figure 17.4, along with its empirical covariance
matrix s. we apply algorithm (17.1) to this problem; for example, in the
modi   ed regression for variable 1 in step (b), variable 3 is left out. the
procedure quickly converged to the solutions:

     =         

10.00
1.00
1.31
4.00

1.00
10.00
2.00
0.87

1.31
2.00
10.00
3.00

4.00
0.87
3.00
10.00

         ,        1 =         

0.12    0.01
   0.01
0.00    0.02
   0.05

0.11    0.02
0.00    0.03

0.00    0.05
0.00
0.11    0.03
0.13

          .

note the zeroes in        1, corresponding to the missing edges (1,3) and (2,4).
note also that the corresponding elements in      are the only elements dif-
ferent from s. the estimation of      is an example of what is sometimes
called the positive de   nite    completion    of s.

17.3 undirected id114 for continuous variables

635

17.3.2 estimation of the graph structure

in most cases we do not know which edges to omit from our graph, and
so would like to try to discover this from the data itself. in recent years a
number of authors have proposed the use of l1 (lasso) id173 for
this purpose.

meinshausen and b  uhlmann (2006) take a simple approach to the prob-
lem: rather than trying to fully estimate    or    =      1, they only estimate
which components of   ij are nonzero. to do this, they    t a lasso regression
using each variable as the response and the others as predictors. the com-
ponent   ij is then estimated to be nonzero if either the estimated coe   cient
of variable i on j is nonzero, or the estimated coe   cient of variable j on
i is nonzero (alternatively they use an and rule). they show that asymp-
totically this procedure consistently estimates the set of nonzero elements
of   .

we can take a more systematic approach with the lasso penalty, following
the development of the previous section. consider maximizing the penalized
log-likelihood

log det        trace(s  )       ||  ||1,

(17.21)

where ||  ||1 is the l1 norm   the sum of the absolute values of the elements
of      1, and we have ignored constants. the negative of this penalized
likelihood is a convex function of   .

it turns out that one can adapt the lasso to give the exact maximizer of
the penalized log-likelihood. in particular, we simply replace the modi   ed
regression step (b) in algorithm 17.1 by a modi   ed lasso step. here are the
details.

the analog of the gradient equation (17.13) is now

     1     s           sign(  ) = 0.

(17.22)

here we use sub-gradient notation, with sign(  jk) = sign(  jk) if   jk 6= 0,
else sign(  jk)     [   1, 1] if   jk = 0. continuing the development in the
previous section, we reach the analog of (17.18)

w11       s12 +       sign(  ) = 0

(17.23)

(recall that    and   12 have opposite signs). we will now see that this system
is exactly equivalent to the estimating equations for a lasso regression.

consider the usual regression setup with outcome variables y and pre-

dictor matrix z. there the lasso minimizes

1

2 (y     z  )t (y     z  ) +       ||  ||1
[see (3.52) on page 68; here we have added a factor 1
gradient of this expression is

(17.24)

2 for convenience]. the

636

17. undirected id114

algorithm 17.2 graphical lasso.

1. initialize w = s +   i. the diagonal of w remains unchanged in

what follows.

2. repeat for j = 1, 2, . . . p, 1, 2, . . . p, . . . until convergence:

(a) partition the matrix w into part 1: all but the jth row and

column, and part 2: the jth row and column.

(b) solve the estimating equations w11       s12 +       sign(  ) = 0
using the cyclical coordinate-descent algorithm (17.26) for the
modi   ed lasso.

(c) update w12 = w11     

3. in the    nal cycle (for each j) solve for     12 =                 22, with 1/    22 =

w22     wt

12

    .

zt z       zt y +       sign(  ) = 0

(17.25)

so up to a factor 1/n , zt y is the analog of s12, and we replace zt z by
w11, the estimated cross-product matrix from our current model.

the resulting procedure is called the graphical lasso, proposed by fried-
man et al. (2008b) building on the work of banerjee et al. (2008). it is
summarized in algorithm 17.2.

friedman et al. (2008b) use the pathwise coordinate descent method
(section 3.8.6) to solve the modi   ed lasso problem at each stage. here are
the details of pathwise coordinate descent for the graphical lasso algorithm.
letting v = w11, the update has the form

    j     s(cid:16)s12j    xk6=j

vkj     k,   (cid:17)/vjj

(17.26)

for j = 1, 2, . . . , p     1, 1, 2, . . . , p     1, . . ., where s is the soft-threshold
operator:
(17.27)

s(x, t) = sign(x)(|x|     t)+.

the procedure cycles through the predictors until convergence.

it is easy to show that the diagonal elements wjj of the solution matrix

w are simply sjj +   , and these are    xed in step 1 of algorithm 17.23.

the graphical lasso algorithm is extremely fast, and can solve a moder-
ately sparse problem with 1000 nodes in less than a minute. it is easy to
modify the algorithm to have edge-speci   c penalty parameters   jk; since

3an alternative formulation of the problem (17.21) can be posed, where we don   t
penalize the diagonal of   . then the diagonal elements wjj of the solution matrix are
sjj , and the rest of the algorithm is unchanged.

17.3 undirected id114 for continuous variables

637

  jk =     will force     jk to be zero, this algorithm subsumes algorithm 17.1.
by casting the sparse inverse-covariance problem as a series of regressions,
one can also quickly compute and examine the solution paths as a function
of the penalty parameter   . more details can be found in friedman et al.
(2008b).

   = 36

raf

jnk

mek

   = 27

raf

jnk

mek

plcg

pip2

p38

plcg

pkc

pip2

p38

pkc

pip3

pka

pip3

pka

erk

akt

erk

akt

   = 7

raf

   = 0

raf

mek

jnk

mek

jnk

plcg

pip2

p38

plcg

pkc

pip2

p38

pkc

pip3

pka

pip3

pka

erk

akt

erk

akt

figure 17.5. four di   erent graphical-lasso solutions for the    ow-cytometry
data.

figure 17.1 shows the result of applying the graphical lasso to the    ow-
cytometry dataset. here the lasso penalty parameter    was set at 14. in
practice it is informative to examine the di   erent sets of graphs that are
obtained as    is varied. figure 17.5 shows four di   erent solutions. the
graph becomes more sparse as the penalty parameter is increased.

finally note that the values at some of the nodes in a graphical model can
be unobserved; that is, missing or hidden. if only some values are missing
at a node, the em algorithm can be used to impute the missing values

638

17. undirected id114

(exercise 17.9). however, sometimes the entire node is hidden or latent.
in the gaussian model, if a node has all missing values, due to linearity
one can simply average over the missing nodes to yield another gaussian
model over the observed nodes. hence the inclusion of hidden nodes does
not enrich the resulting model for the observed nodes; in fact, it imposes
additional structure on its covariance matrix. however in the discrete model
(described next) the inherent nonlinearities make hidden units a powerful
way of expanding the model.

17.4 undirected id114 for discrete

variables

undirected markov networks with all discrete variables are popular, and
in particular pairwise markov networks with binary variables being the
most common. they are sometimes called ising models in the statistical
mechanics literature, and id82s in the machine learning lit-
erature, where the vertices are referred to as    nodes    or    units    and are
binary-valued.

in addition, the values at each node can be observed (   visible   ) or un-
observed (   hidden   ). the nodes are often organized in layers, similar to a
neural network. id82s are useful both for unsupervised and
supervised learning, especially for structured input data such as images,
but have been hampered by computational di   culties. figure 17.6 shows
a restricted id82 (discussed later), in which some variables
are hidden, and only some pairs of nodes are connected. we    rst consider
the simpler case in which all p nodes are visible with edge pairs (j, k) enu-
merated in e.

denoting the binary valued variable at node j by xj, the ising model

for their joint probabilities is given by

p(x,   ) = exph x(j,k)   e

  jkxjxk       (  )i for x     x ,

(17.28)

with x = {0, 1}p. as with the gaussian model of the previous section,
only pairwise interactions are modeled. the ising model was developed in
statistical mechanics, and is now used more generally to model the joint
e   ects of pairwise interactions.   (  ) is the log of the partition function,
and is de   ned by

  (  ) = logxx   xhexp(cid:16) x(j,k)   e

  jkxjxk(cid:17)i.

(17.29)

the partition function ensures that the probabilities add to one over the
sample space. the terms   jkxjxk represent a particular parametrization

17.4 undirected id114 for discrete variables

639

of the (log) potential functions (17.5), and for technical reasons requires
a constant node x0     1 to be included (exercise 17.10), with    edges    to
all the other nodes. in the statistics literature, this model is equivalent
to a    rst-order-interaction poisson log-linear model for multiway tables of
counts (bishop et al., 1975; mccullagh and nelder, 1989; agresti, 2002).

the ising model implies a logistic form for each node conditional on the

others (exercise 17.11):

pr(xj = 1|x   j = x   j) =

1

1 + exp(     j0    p(j,k)   e   jkxk)

,

(17.30)

where x   j denotes all of the nodes except j. hence the parameter   jk
measures the dependence of xj on xk, conditional on the other nodes.

17.4.1 estimation of the parameters when the graph

structure is known

given some data from this model, how can we estimate the parameters?
suppose we have observations xi = (xi1, xi2, . . . , xip)     {0, 1}p, i = 1, . . . , n .
the log-likelihood is

   (  ) =

=

nxi=1
nxi=1

log pr  (xi = xi)

  jkxijxik       (  )      

       x(j,k)   e
nxi=1

= xx   x

xijxik     n

     (  )
     jk

xjxk    p(x,   )

= e  (xjxk)

the gradient of the log-likelihood is

and

      (  )
     jk

=

     (  )
     jk

setting the gradient to zero gives

  e(xjxk)     e  (xjxk) = 0

where we have de   ned

(17.31)

(17.32)

(17.33)

(17.34)

640

17. undirected id114

  e(xjxk) =

1
n

nxi=1

xijxik,

(17.35)

the expectation taken with respect to the empirical distribution of the data.
looking at (17.34), we see that the maximum likelihood estimates simply
match the estimated inner products between the nodes to their observed
inner products. this is a standard form for the score (gradient) equation
for exponential family models, in which su   cient statistics are set equal to
their expectations under the model.

to    nd the maximum likelihood estimates, we can use gradient search
or id77s. however the computation of e  (xjxk) involves enu-
meration of p(x,   ) over 2p   2 of the |x| = 2p possible values of x, and is
not generally feasible for large p (e.g., larger than about 30). for smaller
p, a number of standard statistical approaches are available:

poisson log-linear modeling, where we treat the problem as a large regres-
sion problem (exercise 17.12). the response vector y is the vector of
2p counts in each of the cells of the multiway tabulation of the data4.
the predictor matrix z has 2p rows and up to 1 + p + p2 columns that
characterize each of the cells, although this number depends on the
sparsity of the graph. the computational cost is essentially that of a
regression problem of this size, which is o(p42p) and is manageable
for p < 20. the newton updates are typically computed by iteratively
reweighted least squares, and the number of steps is usually in the
single digits. see agresti (2002) and mccullagh and nelder (1989) for
details. standard software (such as the r package glm) can be used
to    t this model.

id119 requires at most o(p22p   2) computations to compute
the gradient, but may require many more gradient steps than the
second   order id77s. nevertheless, it can handle slightly
larger problems with p     30. these computations can be reduced
by exploiting the special clique structure in sparse graphs, using the
junction-tree algorithm. details are not given here.

iterative proportional    tting (ipf) performs cyclical coordinate descent on
the gradient equations (17.34). at each step a parameter is updated
so that its gradient equation is exactly zero. this is done in a cyclical
fashion until all the gradients are zero. one complete cycle costs the
same as a gradient evaluation, but may be more e   cient. jirou  sek and
p  reu  cil (1995) implement an e   cient version of ipf, using junction
trees.

4each of the cell counts is treated as an independent poisson variable. we get the
multinomial model corresponding to (17.28) by conditioning on the total count n (which
is also poisson under this framework).

17.4 undirected id114 for discrete variables

641

when p is large (> 30) other approaches have been used to approximate

the gradient.

    the mean    eld approximation (peterson and anderson, 1987) esti-
mates e  (xjxk) by e  (xj)e  (xj), and replaces the input vari-
ables by their means, leading to a set of nonlinear equations for the
parameters   jk.

    to obtain near-exact solutions, id150 (section 8.6) is used
to approximate e  (xjxk) by successively sampling from the esti-
mated model probabilities pr  (xj|x   j) (see e.g. ripley (1996)).

we have not discussed decomposable models, for which the maximum
likelihood estimates can be found in closed form without any iteration
whatsoever. these models arise, for example, in trees: special graphs with
tree-structured topology. when computational tractability is a concern,
trees represent a useful class of models and they sidestep the computational
concerns raised in this section. for details, see for example chapter 12 of
whittaker (1990).

17.4.2 hidden nodes

we can increase the complexity of a discrete markov network by including
latent or hidden nodes. suppose that a subset of the variables xh are
unobserved or    hidden   , and the remainder xv are observed or    visible.   
then the log-likelihood of the observed data is

log[pr  (xv = xiv )]

   (  ) =

=

nxi=1
nxi=1hlog xxh   xh

exp x(j,k)   e

(  jkxijxik       (  ))i. (17.36)

the sum over xh means that we are summing over all possible {0, 1} values
for the hidden units. the gradient works out to be

d   (  )
d  jk

=   ev e  (xjxk|xv )     e  (xjxk)

(17.37)

the    rst term is an empirical average of xjxk if both are visible; if one
or both are hidden, they are    rst imputed given the visible data, and then
averaged over the hidden variables. the second term is the unconditional
expectation of xjxk.

the inner expectation in the    rst term can be evaluated using basic rules
of conditional expectation and properties of bernoulli random variables. in
detail, for observation i

642

17. undirected id114

xijpr  (xk = 1|xv = xiv )
pr  (xj = 1, xk = 1|xv = xiv )

e  (xjxk|xv = xiv ) =(cid:8) xijxik

if j, k     v
if j     v, k     h
if j, k     h.
(17.38)
now two separate runs of id150 are required; the    rst to estimate
e  (xjxk) by sampling from the model as above, and the second to esti-
mate e  (xjxk|xv = xiv ). in this latter run, the visible units are    xed
(   clamped   ) at their observed values and only the hidden variables are
sampled. id150 must be done for each observation in the training
set, at each stage of the gradient search. as a result this procedure can be
very slow, even for moderate-sized models. in section 17.4.4 we consider
further model restrictions to make these computations manageable.

17.4.3 estimation of the graph structure

the use of a lasso penalty with binary pairwise markov networks has been
suggested by lee et al. (2007) and wainwright et al. (2007). the    rst au-
thors investigate a conjugate gradient procedure for exact maximization of
a penalized log-likelihood. the bottleneck is the computation of e  (xjxk)
in the gradient; exact computation via the junction tree algorithm is man-
ageable for sparse graphs but becomes unwieldy for dense graphs.

the second authors propose an approximate solution, analogous to the
meinshausen and b  uhlmann (2006) approach for the gaussian graphical
model. they    t an l1-penalized id28 model to each node as
a function of the other nodes, and then symmetrize the edge parameter
estimates in some fashion. for example if     jk is the estimate of the j-k
edge parameter from the logistic model for outcome node j, the    min   
symmetrization sets     jk to either     jk or     kj, whichever is smallest in abso-
lute value. the    max    criterion is de   ned similarly. they show that under
certain conditions either approximation estimates the nonzero edges cor-
rectly as the sample size goes to in   nity. hoe   ing and tibshirani (2008)
extend the graphical lasso to discrete markov networks, obtaining a pro-
cedure which is somewhat faster than conjugate gradients, but still must
deal with computation of e  (xjxk). they also compare the exact and
approximate solutions in an extensive simulation study and    nd the    min   
or    max    approximations are only slightly less accurate than the exact pro-
cedure, both for estimating the nonzero edges and for estimating the actual
values of the edge parameters, and are much faster. furthermore, they can
handle denser graphs because they never need to compute the quantities
e  (xjxk).

finally, we point out a key di   erence between the gaussian and binary
models. in the gaussian case, both    and its inverse will often be of interest,
and the graphical lasso procedure delivers estimates for both of these quan-
tities. however, the approximation of meinshausen and b  uhlmann (2006)
for gaussian id114, analogous to the wainwright et al. (2007)

17.4 undirected id114 for discrete variables

643

xk

hidden h

  jk

xj

visible v1

x   
visible v2

figure 17.6. a restricted id82 (rbm) in which there are no
connections between nodes in the same layer. the visible units are subdivided to
allow the rbm to model the joint density of feature v1 and their labels v2.
approximation for the binary case, only yields an estimate of      1. in con-
trast, in the markov model for binary data,    is the object of interest, and
its inverse is not of interest. the approximate method of wainwright et al.
(2007) estimates    e   ciently and hence is an attractive solution for the
binary problem.

17.4.4 restricted id82s

in this section we consider a particular architecture for id114
inspired by neural networks, where the units are organized in layers. a
restricted id82 (rbm) consists of one layer of visible units
and one layer of hidden units with no connections within each layer. it is
much simpler to compute the conditional expectations (as in (17.37) and
(17.38)) if the connections between hidden units are removed 5. figure 17.6
shows an example; the visible layer is divided into input variables v1 and
output variables v2, and there is a hidden layer h. we denote such a
network by
(17.39)
for example, v1 could be the binary pixels of an image of a handwritten
digit, and v2 could have 10 units, one for each of the observed class labels
0-9.
the restricted form of this model simpli   es the id150 for es-
timating the expectations in (17.37), since the variables in each layer are
independent of one another, given the variables in the other layers. hence
they can be sampled together, using the conditional probabilities given by
expression (17.30).

v1     h     v2.

the resulting model is less general than a id82, but is still
useful; for example it can learn to extract interesting features from images.

5we thank geo   rey hinton for assistance in the preparation of the material on rbms.

644

17. undirected id114

by alternately sampling the variables in each layer of the rbm shown
in figure 17.6, it is possible to generate samples from the joint density
model. if the v1 part of the visible layer is clamped at a particular feature
vector during the alternating sampling, it is possible to sample from the
distribution over labels given v1. alternatively classi   cation of test items
can also be achieved by comparing the unnormalized joint densities of each
label category with the observed features. we do not need to compute the
partition function as it is the same for all of these combinations.

as noted the restricted id82 has the same generic form
as a single hidden layer neural network (section 11.3). the edges in the
latter model are directed, the hidden units are usually real-valued, and the
   tting criterion is di   erent. the neural network minimizes the error (cross-
id178) between the targets and their model predictions, conditional on
the input features. in contrast, the restricted id82 maxi-
mizes the log-likelihood for the joint distribution of all visible units   that
is, the features and targets. it can extract information from the input fea-
tures that is useful for predicting the labels, but, unlike supervised learning
methods, it may also use some of its hidden units to model structure in the
feature vectors that is not immediately relevant for predicting the labels.
these features may turn out to be useful, however, when combined with
features derived from other hidden layers.

unfortunately, id150 in a restricted id82 can
be very slow, as it can take a long time to reach stationarity. as the net-
work weights get larger, the chain mixes more slowly and we need to run
more steps to get the unconditional estimates. hinton (2002) noticed em-
pirically that learning still works well if we estimate the second expectation
in (17.37) by starting the markov chain at the data and only running for a
few steps (instead of to convergence). he calls this contrastive divergence:
we sample h given v1,v2, then v1,v2 given h and    nally h given v1,v2
again. the idea is that when the parameters are far from the solution, it
may be wasteful to iterate the gibbs sampler to stationarity, as just a single
iteration will reveal a good direction for moving the estimates.

we now give an example to illustrate the use of an rbm. using con-
trastive divergence, it is possible to train an rbm to recognize hand-written
digits from the mnist dataset (lecun et al., 1998). with 2000 hidden
units, 784 visible units for representing binary pixel intensities and one
10-way multinomial visible unit for representing labels, the rbm achieves
an error rate of 1.9% on the test set. this is a little higher than the 1.4%
achieved by a support vector machine and comparable to the error rate
achieved by a neural network trained with id26. the error rate
of the rbm, however, can be reduced to 1.25% by replacing the 784 pixel
intensities by 500 features that are produced from the images without using
any label information. first, an rbm with 784 visible units and 500 hidden
units is trained, using contrastive divergence, to model the set of images.
then the hidden states of the    rst rbm are used as data for training a

exercises

645

figure 17.7. example of a restricted id82 for handwritten
digit classi   cation. the network is depicted in the schematic on the left. displayed
on the right are some di   cult test images that the model classi   es correctly.

second rbm that has 500 visible units and 500 hidden units. finally, the
hidden states of the second rbm are used as the features for training an
rbm with 2000 hidden units as a joint density model. the details and
justi   cation for learning features in this greedy, layer-by-layer way are de-
scribed in hinton et al. (2006). figure 17.7 gives a representation of the
composite model that is learned in this way and also shows some examples
of the types of distortion that it can cope with.

bibliographic notes

much work has been done in de   ning and understanding the structure of
id114. comprehensive treatments of id114 can be
found in whittaker (1990), lauritzen (1996), cox and wermuth (1996),
edwards (2000), pearl (2000), anderson (2003), jordan (2004), and koller
and friedman (2007). wasserman (2004) gives a brief introduction, and
chapter 8 of bishop (2006) gives a more detailed overview. boltzmann
machines were proposed in ackley et al. (1985). ripley (1996) has a detailed
chapter on topics in id114 that relate to machine learning. we
found this particularly useful for its discussion of id82s.

exercises

ex. 17.1 for the markov graph of figure 17.8, list all of the implied condi-
tional independence relations and    nd the maximal cliques.

646

17. undirected id114

x1

x2

x3

x4

x5

x6

figure 17.8.

ex. 17.2 consider random variables x1, x2, x3, x4. in each of the following
cases draw a graph that has the given independence relations:
(a) x1     x3|x2 and x2     x4|x3.
(b) x1     x4|x2, x3 and x2     x4|x1, x3.
(c) x1     x4|x2, x3, x1     x3|x2, x4 and x3     x4|x1, x2.
ex. 17.3 let    be the covariance matrix of a set of p variables x. consider
the partial covariance matrix   a.b =   aa       ab     1
bb   ba between the two
subsets of variables xa = (x1, x2) consisting of the    rst two, and xb
the rest. this is the covariance matrix between these two variables, after
linear adjustment for all the rest. in the gaussian distribution, this is the
covariance matrix of the conditional distribution of xa|xb. the partial
correlation coe   cient   jk|rest between the pair xa conditional on the rest
xb, is simply computed from this partial covariance. de   ne    =      1.

1. show that   a.b =      1
aa .

2. show that if any o   -diagonal element of    is zero, then the partial

correlation coe   cient between the corresponding variables is zero.

3. show that if we treat    as if it were a covariance matrix, and compute

the corresponding    correlation    matrix

r = diag(  )   1/2          diag(  )   1/2,

(17.40)

then rjk =      jk|rest

ex. 17.4 denote by

f (x1|x2, x3, . . . , xp)
the conditional density of x1 given x2, . . . , xp. if

f (x1|x2, x3, . . . , xp) = f (x1|x3, . . . , xp),

show that x1     x2|x3, . . . , xp.

exercises

647

ex. 17.5 consider the setup in section 17.3.1 with no missing edges. show
that

are the estimating equations for the multiple regression coe   cients of the
last variable on the rest.

s11       s12 = 0

ex. 17.6 recovery of      =        1 from algorithm 17.1. use expression (17.16)
to derive the standard partitioned inverse expressions

(17.41)

  12 =    w   1
11 w12  22
  22 = 1/(w22     wt

12w   1

11 w12).
11 w12, show that     22 = 1/(w22     wt

since      = w   1
thus     12 is a simply rescaling of      by        22.
ex. 17.7 write a program to implement the modi   ed regression procedure in
algorithm 17.1 for    tting the gaussian graphical model with pre-speci   ed
edges missing. test it on the    ow cytometry data from the book website,
using the graph of figure 17.1.

(17.42)
    ) and     12 =              22.

12

ex. 17.8

(a) write a program to    t the lasso using the coordinate descent procedure
(17.26). compare its results to those from the lars program or some
other convex optimizer, to check that it is working correctly.

(b) using the program from (a), write code to implement the graphical
lasso (algorithm 17.2). apply it to the    ow cytometry data from the
book website. vary the id173 parameter and examine the
resulting networks.

ex. 17.9 suppose that we have a gaussian graphical model in which some
or all of the data at some vertices are missing.

(a) consider the em algorithm for a dataset of n i.i.d. multivariate ob-
servations xi     irp with mean    and covariance matrix   . for each
sample i, let oi and mi index the predictors that are observed and
missing, respectively. show that in the e step, the observations are
imputed from the current estimates of    and   :

  xi,mi = e(xi,mi|xi,oi ,   ) =     mi +     mi,oi

       1
oi,oi (xi,oi         oi )

(17.43)

while in the m step,    and    are re-estimated from the empirical
mean and (modi   ed) covariance of the imputed data:

    j =

  xij/n

nxi=1

648

17. undirected id114

    jj     =

nxi=1

[(  xij         j)(  xij             j     ) + ci,jj     ]/n

(17.44)

where ci,jj     =     jj     if j, j        mi and zero otherwise. explain the reason
for the correction term ci,jj     (little and rubin, 2002).

(b) implement the em algorithm for the gaussian graphical model using
the modi   ed regression procedure from exercise 17.7 for the m-step.

(c) for the    ow cytometry data on the book website, set the data for the
last protein jnk in the    rst 1000 observations to missing,    t the model
of figure 17.1, and compare the predicted values to the actual values
for jnk. compare the results to those obtained from a regression of
jnk on the other vertices with edges to jnk in figure 17.1, using only
the non-missing data.

ex. 17.10 using a simple binary graphical model with just two variables,
show why it is essential to include a constant node x0     1 in the model.
ex. 17.11 show that the ising model (17.28) for the joint probabilities in
a discrete graphical model implies that the conditional distributions have
the logistic form (17.30).

ex. 17.12 consider a poisson regression problem with p binary variables
xij, j = 1, . . . , p and response variable yi which measures the number of
observations with predictor xi     {0, 1}p. the design is balanced, in that all
n = 2p possible combinations are measured. we assume a log-linear model
for the poisson mean in each cell

log   (x) =   00 + x(j,k)   e

xijxik  jk,

(17.45)

using the same notation as in section 17.4.1 (including the constant variable
xi0 = 1   i). we assume the response is distributed as
e     (x)  (x)y

pr(y = y|x = x) =

.

y!

(17.46)

write down the conditional log-likelihood for the observed responses yi,
and compute the gradient.

(a) show that the gradient equation for   00 computes the partition func-

tion (17.29).

(b) show that the gradient equations for the remainder of the parameters

are equivalent to the gradient (17.34).

this is page 649
printer: opaque this

18
high-dimensional problems: p     n

18.1 when p is much bigger than n

in this chapter we discuss prediction problems in which the number of
features p is much larger than the number of observations n , often written
p     n . such problems have become of increasing importance, especially in
genomics and other areas of computational biology. we will see that high
variance and over   tting are a major concern in this setting. as a result,
simple, highly regularized approaches often become the methods of choice.
the    rst part of the chapter focuses on prediction in both the classi   cation
and regression settings, while the second part discusses the more basic
problem of feature selection and assessment.

to get us started, figure 18.1 summarizes a small simulation study that
demonstrates the    less    tting is better    principle that applies when p     n .
for each of n = 100 samples, we generated p standard gaussian features
x with pairwise correlation 0.2. the outcome y was generated according
to a linear model

y =

pxj=1

xj  j +     

(18.1)

where    was generated from a standard gaussian distribution. for each
dataset, the set of coe   cients   j were also generated from a standard gaus-
sian distribution. we investigated three cases: p = 20, 100, and 1000. the
standard deviation    was chosen in each case so that the signal-to-noise
ratio var[e(y |x)]/  2 equaled 2. as a result, the number of signi   cant uni-

650

18. high-dimensional problems: p     n

20 features

100 features

1000 features

r
o
r
r
e
 
e
v
i
t
a
e
r

l

r
o
r
r

e

 
t
s
e
t

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

0
.
3

5
.
2

0
.
2

5
.
1

0
.
1

20

9

2

99

35

7

99

87

43

effective degrees of freedom

figure 18.1. test-error results for simulation experiments. shown are box-
plots of the relative test errors over 100 simulations, for three di   erent values
of p, the number of features. the relative error is the test error divided by the
bayes error,   2. from left to right, results are shown for ridge regression with
three di   erent values of the id173 parameter   : 0.001, 100 and 1000. the
(average) e   ective degrees of freedom in the    t is indicated below each plot.

variate regression coe   cients1 was 9, 33 and 331, respectively, averaged
over the 100 simulation runs. the p = 1000 case is designed to mimic the
kind of data that we might see in a high-dimensional genomic or proteomic
dataset, for example.

we    t a ridge regression to the data, with three di   erent values for the
id173 parameter   : 0.001, 100, and 1000. when    = 0.001, this
is nearly the same as least squares regression, with a little id173
just to ensure that the problem is non-singular when p > n . figure 18.1
shows boxplots of the relative test error achieved by the di   erent estimators
in each scenario. the corresponding average degrees of freedom used in
each ridge-regression    t is indicated (computed using formula (3.50) on
page 682). the degrees of freedom is a more interpretable parameter than
  . we see that ridge regression with    = 0.001 (20 df) wins when p = 20;
   = 100 (35 df) wins when p = 100, and    = 1000 (43 df) wins when
p = 1000.

here is an explanation for these results. when p = 20, we    t all the way
and we can identify as many of the signi   cant coe   cients as possible with

1we call a regression coe   cient signi   cant if |b  j /bsej |     2, where     j is the estimated
(univariate) coe   cient and bsej is its estimated standard error.
2for a    xed value of the id173 parameter   , the degrees of freedom depends
on the observed predictor values in each simulation. hence we compute the average
degrees of freedom over simulations.

18.2 nearest shrunken centroids

651

low bias. when p = 100, we can identify some non-zero coe   cients using
moderate shrinkage. finally, when p = 1000, even though there are many
nonzero coe   cients, we don   t have a hope for    nding them and we need

to shrink all the way down. as evidence of this, let tj = b  j/bsej, where     j
is the ridge regression estimate and bsej its estimated standard error. then

using the optimal ridge parameter in each of the three cases, the median
value of |tj| was 2.0, 0.6 and 0.2, and the average number of |tj| values
exceeding 2 was equal to 9.8, 1.2 and 0.0.
ridge regression with    = 0.001 successfully exploits the correlation in
the features when p < n , but cannot do so when p     n . in the latter case
there is not enough information in the relatively small number of samples
to e   ciently estimate the high-dimensional covariance matrix. in that case,
more id173 leads to superior prediction performance.

thus it is not surprising that the analysis of high-dimensional data re-
quires either modi   cation of procedures designed for the n > p scenario, or
entirely new procedures. in this chapter we discuss examples of both kinds
of approaches for high dimensional classi   cation and regression; these meth-
ods tend to regularize quite heavily, using scienti   c contextual knowledge
to suggest the appropriate form for this id173. the chapter ends
with a discussion of feature selection and multiple testing.

18.2 diagonal id156 and

nearest shrunken centroids

gene expression arrays are an important new technology in biology, and
are discussed in chapters 1 and 14. the data in our next example form
a matrix of 2308 genes (columns) and 63 samples (rows), from a set of
microarray experiments. each expression value is a log-ratio log(r/g). r
is the amount of gene-speci   c rna in the target sample that hybridizes
to a particular (gene-speci   c) spot on the microarray, and g is the corre-
sponding amount of rna from a reference sample. the samples arose from
small, round blue-cell tumors (srbct) found in children, and are classi   ed
into four major types: bl (burkitt lymphoma), ews (ewing   s sarcoma),
nb (neuroblastoma), and rms (rhabdomyosarcoma). there is an addi-
tional test data set of 20 observations. we will not go into the scienti   c
background here.

since p     n , we cannot    t a full id156 (lda) to
the data; some sort of id173 is needed. the method we describe
here is similar to the methods of section 4.3.1, but with important modi   -
cations that achieve feature selection. the simplest form of id173
assumes that the features are independent within each class, that is, the
within-class covariance matrix is diagonal. despite the fact that features
will rarely be independent within a class, when p     n we don   t have

652

18. high-dimensional problems: p     n

enough data to estimate their dependencies. the assumption of indepen-
dence greatly reduces the number of parameters in the model and often
results in an e   ective and interpretable classi   er.

thus we consider the diagonal-covariance lda rule for classifying the

classes. the discriminant score [see (4.12) on page 110] for class k is

  k(x   ) =    

pxj=1

(x   

j       xkj)2

s2
j

+ 2 log   k.

(18.2)

1, x   

2, . . . , x   

here x    = (x   
p)t is a vector of expression values for a test ob-
servation, sj is the pooled within-class standard deviation of the jth gene,
xij/nk is the mean of the nk values for gene j in class
k, with ck being the index set for class k. we call   xk = (  xk1,   xk2, . . .   xkp)t
the centroid of class k. the    rst part of (18.2) is simply the (negative)
standardized squared distance of x    to the kth centroid. the second part
k=1   k = 1.

and   xkj =pi   ck
is a correction based on the class prior id203   k, wherepk

the classi   cation rule is then

c(x   ) =     if      (x   ) = maxk   k(x   ).

(18.3)

we see that the diagonal lda classi   er is equivalent to a nearest centroid
classi   er after appropriate standardization. it is also a special case of the
naive-bayes classi   er, as described in section 6.6.3. it assumes that the
features in each class have independent gaussian distributions with the
same variance.

the diagonal lda classi   er is often e   ective in high dimensional set-
tings. it is also called the    independence rule    in bickel and levina (2004),
who demonstrate theoretically that it will often outperform standard lin-
ear discriminant analysis in high-dimensional problems. here the diagonal
lda classi   er yielded    ve misclassi   cation errors for the 20 test samples.
one drawback of the diagonal lda classi   er is that it uses all of the fea-
tures (genes), and hence is not convenient for interpretation. with further
id173 we can do better   both in terms of test error and inter-
pretability.

we would like to regularize in a way that automatically drops out fea-
tures that are not contributing to the class predictions. we can do this
by shrinking the classwise mean toward the overall mean, for each feature
separately. the result is a regularized version of the nearest centroid clas-
si   er, or equivalently a regularized version of the diagonal-covariance form
of lda. we call the procedure nearest shrunken centroids (nsc).

the shrinkage procedure is de   ned as follows. let

dkj =

  xkj       xj
mk(sj + s0)

,

(18.4)

where   xj is the overall mean for gene j, m2
k = 1/nk     1/n and s0 is a
small positive constant, typically chosen to be the median of the sj values.

18.2 nearest shrunken centroids

653

   

(0,0)

figure 18.2. soft thresholding function sign(x)(|x|       )+ is shown in orange,
along with the 45    line in red.

this constant guards against large dkj values that arise from expression
values near zero. with constant within-class variance   2, the variance of
k  2, and hence the form of the
the contrast   xkj       xj in the numerator is m2
standardization in the denominator. we shrink the dkj toward zero using
soft thresholding

d   
kj = sign(dkj)(|dkj|        )+;

(18.5)

see figure 18.2. here     is a parameter to be determined; we used 10-fold
cross-validation in the example (see the top panel of figure 18.4). each dkj
is reduced by an amount     in absolute value, and is set to zero if its value
is less than zero. the soft-thresholding function is shown in figure 18.2;
the same thresholding is applied to wavelet coe   cients in section 5.9. an
alternative is to use hard thresholding

d   
kj = dkj    i(|dkj|        );

(18.6)

we prefer soft-thresholding, as it is a smoother operation and typically
works better. the shrunken versions of   xkj are then obtained by reversing
the transformation in (18.4):

  x   
kj =   xj + mk(sj + s0)d   

kj.

(18.7)

we then use the shrunken centroids   x   
kj in place of the original   xkj in the
discriminant score (18.2). the estimator (18.7) can also be viewed as a
lasso-style estimator for the class means (exercise 18.2).

notice that only the genes that have a nonzero d   

kj for at least one of the
classes play a role in the classi   cation rule, and hence the vast majority
of genes can often be discarded. in this example, all but 43 genes were
discarded, leaving a small interpretable set of genes that characterize each
class. figure 18.3 represents the genes in a heatmap.

figure 18.4 (top panel) demonstrates the e   ectiveness of the shrinkage.
with no shrinkage we make 5/20 errors on the test data, and several errors

654

18. high-dimensional problems: p     n

on the training and cv data. the shrunken centroids achieve zero test er-
rors for a fairly broad band of values for    . the bottom panel of figure 18.4
shows the four centroids for the srbct data (gray), relative to the overall
centroid. the blue bars are shrunken versions of these centroids, obtained
by soft-thresholding the gray bars, using     = 4.3. the discriminant scores
(18.2) can be used to construct class id203 estimates:

  pk(x   ) =

.

(18.8)

1

2   k(x   )

e
   =1 e 1

2      (x   )

pk

these can be used to rate the classi   cations, or to decide not to classify a
particular sample at all.

note that other forms of feature selection can be used in this setting,
including hard thresholding. fan and fan (2008) show theoretically the
importance of carrying out some kind of feature selection with diagonal
id156 in high-dimensional problems.

18.3 linear classi   ers with quadratic

id173

ramaswamy et al. (2001) present a more di   cult microarray classi   cation
problem, involving a training set of 144 patients with 14 di   erent types of
cancer, and a test set of 54 patients. gene expression measurements were
available for 16, 063 genes.

table 18.1 shows the prediction results from eight di   erent classi   cation
methods. the data from each patient was    rst standardized to have mean
0 and variance 1; this seems to improve prediction accuracy overall this
example, suggesting that the    shape    of each gene-expression pro   le is
important, rather than the absolute expression levels. in each case, the

bl

ews

nb

rms

figure 18.3. heat-map of the chosen 43 genes. within each of the horizontal
partitions, we have ordered the genes by hierarchical id91, and similarly
for the samples within each vertical partition. yellow represents over- and blue
under-expression.

18.3 linear classi   ers with quadratic id173

655

number of genes

2308 2059 1223

598

284

159

81

43

23

15

10

5

1

training
10   fold cv
test

8
0

.

6
0

.

4
0

.

.

2
0

.

0
0

0

2

4

6

amount of shrinkage    

bl

ews

nb

rms

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

e
n
e
g

0
0
0
2

0
0
5
1

0
0
0
1

0
0
5

0

   1.0    0.5

0.0

0.5

1.0

   1.0    0.5

0.0

0.5

1.0

   1.0    0.5

0.0

0.5

1.0

   1.0    0.5

0.0

0.5

1.0

centroids: average expression centered at overall centroid

figure 18.4. (top): error curves for the srbct data. shown are the train-
ing, 10-fold cross-validation, and test misclassi   cation errors as the threshold
parameter     is varied. the value     = 4.34 is chosen by cv, resulting in a sub-
set of 43 selected genes. (bottom): four centroids pro   les dkj for the srbct
data (gray), relative to the overall centroid. each centroid has 2308 components,
and we see considerable noise. the blue bars are shrunken versions d   
kj of these
centroids, obtained by soft-thresholding the gray bars, using     = 4.3.

656

18. high-dimensional problems: p     n

table 18.1. prediction results for microarray data with 14 cancer classes.
method 1 is described in section 18.2. methods 2, 3 and 6 are discussed in sec-
tion 18.3, while 4, 7 and 8 are discussed in section 18.4. method 5 is described in
section 13.3. the elastic-net penalized multinomial does the best on the test data,
but the standard error of each test-error estimate is about 3, so such comparisons
are inconclusive.

methods

cv errors (se) test errors
out of 144

out of 54

number of
genes used

1. nearest shrunken centroids
2. l2-penalized discriminant

35 (5.0)
25 (4.1)

analysis

3. support vector classi   er
4. lasso regression (one vs all)
5. k-nearest neighbors
6. l2-penalized multinomial
7. l1-penalized multinomial
8. elastic-net penalized

26 (4.2)
30.7 (1.8)
41 (4.6)
26 (4.2)
17 (2.8)
22 (3.7)

multinomial

17
12

14
12.5
26
15
13
11.8

6,520
16,063

16,063
1,429
16,063
16,063
269
384

id173 parameter has been chosen to minimize the cross-validation
error, and the test error at that value of the parameter is shown. when
more than one value of the id173 parameter yields the minimal
cross-validation error, the average test error at these values is reported.

rda (regularized discriminant analysis), regularized multinomial logistic
regression, and the support vector machine are more complex methods that
try to exploit multivariate information in the data. we describe each in
turn, as well as a variety of id173 methods, including both l1 and
l2 and some in between.

18.3.1 regularized discriminant analysis

regularized discriminant analysis (rda) is described in section 4.3.1. lin-
ear discriminant analysis involves the inversion of a p   p within-covariance
matrix. when p     n , this matrix can be huge, has rank at most n < p,
and hence is singular. rda overcomes the singularity issues by regulariz-
ing the within-covariance estimate     . here we use a version of rda that
shrinks      towards its diagonal:

    (  ) =         + (1       )diag(     ), with        [0, 1].

(18.9)

note that    = 0 corresponds to diagonal lda, which is the    no shrinkage   
version of nearest shrunken centroids. the form of shrinkage in (18.9) is

18.3 linear classi   ers with quadratic id173

657

much like ridge regression (section 3.4.1), which shrinks the total covariance
matrix of the features towards a diagonal (scalar) matrix. in fact, viewing
id156 as id75 with optimal scoring of the
categorical response (see (12.57) in section 12.6), the equivalence becomes
more precise.

the computational burden of inverting this large p  p matrix is overcome
using the methods discussed in section 18.3.5. the value of    was chosen
by cross-validation in line 2 of table 18.1; all values of        (0.002, 0.550)
gave the same cv and test error. further development of rda, including
shrinkage of the centroids in addition to the covariance matrix, can be
found in guo et al. (2006).

18.3.2 id28 with quadratic id173

id28 (section 4.4) can be modi   ed in a similar way, to deal
with the p     n case. with k classes, we use a symmetric version of the
multiclass logistic model (4.17) on page 119:

pr(g = k|x = x) =

.

(18.10)

exp(  k0 + xt   k)
   =1 exp(     0 + xt      )

pk

this has k coe   cient vectors of log-odds parameters   1,   2, . . . ,   k . we
regularize the    tting by maximizing the penalized log-likelihood

max

{  0k,  k}k

1 " nxi=1

log pr(gi|xi)    

2# .
||  k||2

  
2

kxk=1

(18.11)

k=1

rization, and forcespk

this id173 automatically resolves the redundancy in the paramet-
    kj = 0, j = 1, . . . , p (exercise 18.3). note that
the constant terms   k0 are not regularized (and so one should be set to
zero). the resulting optimization problem is convex, and can be solved by
a newton algorithm or other numerical techniques. details are given in zhu
and hastie (2004). friedman et al. (2010) provide software for computing
the id173 path for the two- and multiclass id28 mod-
els. table 18.1, line 6 reports the results for the multiclass logistic regres-
sion model, referred to there as    multinomial   . it can be shown (rosset
et al., 2004a) that for separable data, as        0, the regularized (two-
class) id28 estimate (renormalized) converges to the maximal
margin classi   er (section 12.2). this gives an attractive alternative to the
support-vector machine, discussed next, especially in the multiclass case.

18.3.3 the support vector classi   er

the support vector classi   er is described for the two-class case in sec-
tion 12.2. when p > n , it is especially attractive because in general the

658

18. high-dimensional problems: p     n

classes are perfectly separable by a hyperplane unless there are identical
feature vectors in di   erent classes. without any id173 the support
vector classi   er    nds the separating hyperplane with the largest margin;
that is, the hyperplane yielding the biggest gap between the classes in
the training data. somewhat surprisingly, when p     n the unregularized
support vector classi   er often works about as well as the best regularized
version. over   tting often does not seem to be a problem, partly because of
the insensitivity of misclassi   cation loss.

there are many di   erent methods for generalizing the two-class support-
vector classi   er to k > 2 classes. in the    one versus one    (ovo) approach,

we compute all(cid:0)k

2(cid:1) pairwise classi   ers. for each test point, the predicted

class is the one that wins the most pairwise contests. in the    one versus all   
(ova) approach, each class is compared to all of the others in k two-class
comparisons. to classify a test point, we compute the con   dences (signed
distance from the hyperplane) for each of the k classi   ers. the winner is the
class with the highest con   dence. finally, vapnik (1998) and weston and
watkins (1999) suggested (somewhat complex) multiclass criteria which
generalize the two-class criterion (12.7).

tibshirani and hastie (2007) propose the margin tree classi   er, in which
support-vector classi   ers are used in a binary tree, much as in cart
(chapter 9). the classes are organized in a hierarchical manner, which can
be useful for classifying patients into di   erent cancer types, for example.

line 3 of table 18.1 shows the results for the support vector classi   er
using the ova method; ramaswamy et al. (2001) reported (and we con-
   rmed) that this approach worked best for this problem. the errors are
very similar to those in line 6, as we might expect from the comments
at the end of the previous section. the error rates are insensitive to the
choice of c [the id173 parameter in (12.8) on page 420], for values
of c > 0.001. since p > n , the support vector hyperplane can perfectly
separate the training data by setting c =    .

18.3.4 feature selection

feature selection is an important scienti   c requirement for a classi   er when
p is large. neither discriminant analysis, id28, nor the support-
vector classi   er perform feature selection automatically, because all use
quadratic id173. all features have nonzero weights in both models.
ad-hoc methods for feature selection have been proposed, for example,
removing genes with small coe   cients, and re   tting the classi   er. this is
done in a backward stepwise manner, starting with the smallest weights and
moving on to larger weights. this is known as recursive feature elimination
(guyon et al., 2002). it was not successful in this example; ramaswamy
et al. (2001) report, for example, that the accuracy of the support-vector
classi   er starts to degrade as the number of genes is reduced from the full

18.3 linear classi   ers with quadratic id173

659

set of 16, 063. this is rather remarkable, as the number of training samples
is only 144. we do not have an explanation for this behavior.

all three methods discussed in this section (rda, lr and id166) can
be modi   ed to    t nonlinear decision boundaries using kernels. usually the
motivation for such an approach is to increase the model complexity. with
p     n the models are already su   ciently complex and over   tting is always
a danger. yet despite the high dimensionality, radial kernels (section 12.3.3)
sometimes deliver superior results in these high dimensional problems. the
radial kernel tends to dampen inner products between points far away from
each other, which in turn leads to robustness to outliers. this occurs often
in high dimensions, and may explain the positive results. we tried a radial
kernel with the id166 in table 18.1, but in this case the performance was
inferior.

18.3.5 computational shortcuts when p     n

the computational techniques discussed in this section apply to any method
that    ts a linear model with quadratic id173 on the coe   cients.
that includes all the methods discussed in this section, and many more.
when p > n , the computations can be carried out in an n -dimensional
space, rather than p, via the singular value decomposition introduced in
section 14.5. here is the geometric intuition: just like two points in three-
dimensional space always lie on a line, n points in p-dimensional space lie
in an (n     1)-dimensional a   ne subspace.

given the n    p data matrix x, let

x = udvt

= rvt

(18.12)

(18.13)

be the id166 (svd) of x; that is, v is p    n with
orthonormal columns, u is n    n orthogonal, and d a diagonal matrix
with elements d1     d2     dn     0. the matrix r is n    n , with rows rt
i .
as a simple example, let   s    rst consider the estimates from a ridge re-
gression:

     = (xt x +   i)   1xt y.

(18.14)

replacing x by rvt and after some further manipulations, this can be
shown to equal

     = v(rt r +   i)   1rt y

(18.15)

(exercise 18.4). thus      = v    , where      is the ridge-regression estimate
using the n observations (ri, yi), i = 1, 2, . . . , n . in other words, we can
simply reduce the data matrix from x to r, and work with the rows of
r. this trick reduces the computational cost from o(p3) to o(pn 2) when
p > n .

660

18. high-dimensional problems: p     n

these results can be generalized to all models that are linear in the
parameters and have quadratic penalties. consider any supervised learning
problem where we use a linear function f (x) =   0 + x t    to model a
parameter in the conditional distribution of y |x. we    t the parameters   
i=1 l(yi, f (xi)) over the data with a
quadratic penalty on   . id28 is a useful example to have in
mind. then we have the following simple theorem:
let f    (ri) =   0 + rt
optimization problems:

by minimizing some id168 pn

i    with ri de   ned in (18.13), and consider the pair of

(     0,     ) = arg min

  0,     irp

(    0,     ) = arg min

  0,     irn

nxi=1
nxi=1

l(yi,   0 + xt

i   ) +     t   ;

(18.16)

l(yi,   0 + rt

i   ) +     t   .

(18.17)

then the     0 =     0, and      = v    .

the theorem says that we can simply replace the p vectors xi by the
n -vectors ri, and perform our penalized    t as before, but with far fewer
predictors. the n -vector solution      is then transformed back to the p-
vector solution via a simple id127. this result is part of
the statistics folklore, and deserves to be known more widely   see hastie
and tibshirani (2004) for further details.

geometrically, we are rotating the features to a coordinate system in
which all but the    rst n coordinates are zero. such rotations are allowed
since the quadratic penalty is invariant under rotations, and linear models
are equivariant.

this result can be applied to many of the learning methods discussed
in this chapter, such as regularized (multiclass) id28, linear
discriminant analysis (exercise 18.6), and support vector machines. it also
applies to neural networks with quadratic id173 (section 11.5.2).
note, however, that it does not apply to methods such as the lasso, which
uses nonquadratic (l1) penalties on the coe   cients.

typically we use cross-validation to select the parameter   . it can be
seen (exercise 18.12) that we only need to construct r once, on the original
data, and use it as the data for each of the cv folds.

the support vector    kernel trick    of section 12.3.7 exploits the same re-
duction used in this section, in a slightly di   erent context. suppose we have
at our disposal the n    n gram (inner-product) matrix k = xxt . from
(18.12) we have k = ud2ut , and so k captures the same information as
r. exercise 18.13 shows how we can exploit the ideas in this section to    t
a ridged id28 with k using its svd.

18.4 linear classi   ers with l1 id173

661

18.4 linear classi   ers with l1 id173

the methods of section 18.3 use an l2 penalty to regularize their pa-
rameters, just as in ridge regression. all of the estimated coe   cients are
nonzero, and hence no feature selection is performed. in this section we dis-
cuss methods that use l1 penalties instead, and hence provide automatic
feature selection.

recall the lasso of section 3.4.2,

min

  

1
2

nxi=1(cid:16)yi       0    

pxj=1

xij  j(cid:17)2

+   

pxj=1

|  j|,

(18.18)

which we have written in the lagrange form (3.52). as discussed there, the
use of the l1 penalty causes a subset of the solution coe   cients     j to be
exactly zero, for a su   ciently large value of the tuning parameter   .

in section 3.8.1 we discussed the lars algorithm, an e   cient procedure
for computing the lasso solution for all   . when p > n (as in this chapter),
as    approaches zero, the lasso    ts the training data exactly. in fact, by
convex duality one can show that when p > n the number of non-zero
coe   cients is at most n for all values of    (rosset and zhu, 2007, for
example). thus the lasso provides a (severe) form of feature selection.

lasso regression can be applied to a two-class classi   cation problem by
coding the outcome   1, and applying a cuto    (usually 0) to the predictions.
for more than two classes, there are many possible approaches, including
the ova and ovo methods discussed in section 18.3.3. we tried the ova-
approach on the cancer data in section 18.3. the results are shown in
line (4) of table 18.1. its performance is among the best.

a more natural approach for classi   cation problems is to use the lasso
penalty to regularize id28. several implementations have been
proposed in the literature, including path algorithms similar to lars (park
and hastie, 2007). because the paths are piecewise smooth but nonlinear,
exact methods are slower than the lars algorithm, and are less feasible
when p is large.

friedman et al. (2010) provide very fast algorithms for    tting l1-pen-
alized logistic and multinomial regression models. they use the symmetric
multinomial id28 model as in (18.10) in section 18.3.2, and
maximize the penalized log-likelihood

max

{  0k,  k   irp}k

nxi=1

log pr(gi|xi)       

kxk=1

pxj=1

1       

|  kj|       ;

(18.19)

compare with (18.11). their algorithm computes the exact solution at a
pre-chosen sequence of values for    by cyclical coordinate descent (sec-
tion 3.8.6), and exploits the fact that solutions are sparse when p     n ,

662

18. high-dimensional problems: p     n

as well as the fact that solutions for neighboring values of    tend to be
very similar. this method was used in line (7) of table 18.1, with the over-
all tuning parameter    chosen by cross-validation. the performance was
similar to that of the best methods, except here the automatic feature se-
lection chose 269 genes altogether. a similar approach is used in genkin
et al. (2007); although they present their model from a bayesian point of
view, they in fact compute the posterior mode, which solves the penalized
maximum-likelihood problem.

lasso

elastic net

s
t
n
e
i
c
   
e
o
c

0

.

2

5
1

.

0

.

1

5
0

.

0
0

.

s
t
n
e
i
c
   
e
o
c

0

.

2

5
1

.

0

.

1

5
0

.

0
0

.

   8

   7

   6

   5

   4

   3

   2

   1

   8

   7

   6

   5

   4

   3

   2

   1

log(  )

log(  )

figure 18.5. regularized id28 paths for the leukemia data. the
left panel is the lasso path, the right panel the elastic-net path with    = 0.8. at
the ends of the path (extreme left), there are 19 nonzero coe   cients for the lasso,
and 39 for the elastic net. the averaging e   ect of the elastic net results in more
non-zero coe   cients than the lasso, but with smaller magnitudes.

in genomic applications, there are often strong correlations among the
variables; genes tend to operate in molecular pathways. the lasso penalty
is somewhat indi   erent to the choice among a set of strong but corre-
lated variables (exercise 3.28). the ridge penalty, on the other hand, tends
to shrink the coe   cients of correlated variables toward each other (exer-
cise 3.29 on page 99). the elastic net penalty (zou and hastie, 2005) is a
compromise, and has the form

pxj=1(cid:0)  |  j| + (1       )  2
j(cid:1) .

(18.20)

the second term encourages highly correlated features to be averaged, while
the    rst term encourages a sparse solution in the coe   cients of these aver-

18.4 linear classi   ers with l1 id173

663

aged features. the elastic net penalty can be used with any linear model,
in particular for regression or classi   cation.

hence the multinomial problem above with elastic-net penalty becomes

max

{  0k,  k   irp}k

log pr(gi|xi)       

1       

nxi=1

kxk=1

kj(cid:1)       .
pxj=1(cid:0)  |  kj| + (1       )  2

(18.21)
the parameter    determines the mix of the penalties, and is often pre-
chosen on qualitative grounds. the elastic net can yield more that n non-
zero coe   cients when p > n , a potential advantage over the lasso. line
(8) in table 18.1 uses this model, with    and    chosen by cross-validation.
we used a sequence of 20 values of    between 0.05 and 1.0, and a 100
values of    uniform on the log scale covering the entire range. values of
       [0.75, 0.80] gave the minimum cv error, with values of    < 0.001 for all
tied solutions. although it has the lowest test error among all methods, the
margin is small and not signi   cant. interestingly, when cv is performed
separately for each value of   , a minimum test error of 8.8 is achieved at
   = 0.10, but this is not the value chosen in the two-dimensional cv.

r
o
r
r

 

e
n
o

i
t

a
c
i
f
i
s
s
a
c
s
m

i

l

training
test
10   fold cv

4

.

0

3
0

.

2

.

0

1
0

.

.

0
0

e
c
n
a
v
e
d

i

0
3

5
2

0
2

5
1

0
1

5

0

   8

   7

   6

   5

   4

   3

   2

   1

   8

   7

   6

   5

   4

   3

   2

   1

log(  )

log(  )

figure 18.6. training, test, and 10-fold cross validation curves for lasso logis-
tic regression on the leukemia data. the left panel shows misclassi   cation errors,
the right panel shows deviance.

figure 18.5 shows the lasso and elastic-net coe   cient paths on the two-
class leukemia data (golub et al., 1999). there are 7129 gene-expression
measurements on 38 samples, 27 of them in class all (acute lymphocytic
leukemia), and 11 in class aml (acute myelogenous leukemia). there is
also a test set with 34 samples (20, 14). since the data are linearly separa-
ble, the solution is unde   ned at    = 0 (exercise 18.11), and degrades for
very small values of   . hence the paths have been truncated as the    tted
probabilities approach 0 and 1. there are 19 non-zero coe   cients in the
left plot, and 39 in the right. figure 18.6 (left panel) shows the misclas-

664

18. high-dimensional problems: p     n

si   cation errors for the lasso id28 on the training and test
data, as well as for 10-fold cross-validation on the training data. the right
panel uses binomial deviance to measure errors, and is much smoother. the
small sample sizes lead to considerable sampling variance in these curves,
even though individual curves are relatively smooth (see, for example, fig-
ure 7.1 on page 220). both of these plots suggest that the limiting solution
       0 is adequate, leading to 3/34 misclassi   cations in the test set. the
corresponding    gures for the elastic net are qualitatively similar and are
not shown.

for p     n , the limiting coe   cients diverge for all regularized logistic
regression models, so in practical software implementations a minimum
value for    > 0 is either explicitly or implicitly set. however, renormalized
versions of the coe   cients converge, and these limiting solutions can be
thought of as interesting alternatives to the linear optimal separating hy-
perplane (id166). with    = 0 the limiting solution coincides with the id166
(see end of section 18.3.2), but all the 7129 genes are selected. with    = 1,
the limiting solution coincides with an l1 separating hyperplane (rosset
et al., 2004a), and includes at most 38 genes. as    decreases from 1, the
elastic-net solutions include more genes in the separating hyperplane.

18.4.1 application of lasso to protein mass spectroscopy

protein mass spectrometry has become a popular technology for analyzing
the proteins in blood, and can be used to diagnose a disease or understand
the processes underlying it.

for each blood serum sample i, we observe the intensity xij for many
time of    ight values tj. this intensity is related to the number of particles
observed to take approximately tj time to pass from the emitter to the
detector during a cycle of operation of the machine. the time of    ight has
a known relationship to the mass over charge ratio (m/z) of the constituent
proteins in the blood. hence the identi   cation of a peak in the spectrum
at a certain tj tells us that there is a protein with a corresponding mass
and charge. the identity of this protein can then be determined by other
means.

figure 18.7 shows an example taken from adam et al. (2003). it shows
the average spectra for healthy patients and those with prostate cancer.
there are 16,898 m/z sites in total, ranging in value from 2000 to 40,000.
the full dataset consists of 157 healthy patients and 167 with cancer, and
the goal is to    nd m/z sites that discriminate between the two groups.
this is an example of functional data; the predictors can be viewed as a
function of m/z. there has been much interest in this problem in the past
few years; see e.g. petricoin et al. (2002).

the data were    rst standardized (baseline subtraction and normaliza-
tion), and we restricted attention to m/z values between 2000 and 40,000
(spectra outside of this range were not of interest). we then applied near-

18.4 linear classi   ers with l1 id173

665

normal
cancer

y
t
i
s
n
e
n

t

i

0
4

0
3

0
2

0
1

2e+03

5e+03

1e+04

2e+04

5e+04

1e+05

2e+05

m/z

figure 18.7. protein mass spectrometry data: average pro   les from normal
and prostate cancer patients.

est shrunken centroids and lasso regression to the data, with the results for
both methods shown in table 18.2.

by    tting harder to the data, the lasso achieves a considerably lower
test error rate. however, it may not provide a scienti   cally useful solu-
tion. ideally, protein mass spectrometry resolves a biological sample into
its constituent proteins, and these should appear as peaks in the spectra.
the lasso doesn   t treat peaks in any special way, so not surprisingly only
some of the non-zero lasso weights were situated near peaks in the spectra.
furthermore, the same protein may yield a peak at slightly di   erent m/z
values in di   erent spectra. in order to identify common peaks, some kind
of m/z warping is needed from sample to sample.

to address this, we applied a standard peak-extraction algorithm to each
spectrum, yielding a total of 5178 peaks in the 217 training spectra. our
idea was to pool the collection of peaks from all patients, and hence con-
struct a set of common peaks. for this purpose, we applied hierarchical
id91 to the positions of these peaks along the log m/z axis. we cut
the resulting dendrogram horizontally at height log(0.005)3, and computed
averages of the peak positions in each resulting cluster. this process yielded
728 common clusters and their corresponding peak centers.

given these 728 common peaks, we determined which of these were
present in each individual spectrum, and if present, the height of the peak.
a peak height of zero was assigned if that peak was not found. this pro-
duced a 217    728 matrix of peak heights as features, which was used in a
lasso regression. we scored the test spectra for the same 728 peaks.

3use of the value 0.005 means that peaks with positions less than 0.5% apart are

considered the same peak, a fairly common assumption.

666

18. high-dimensional problems: p     n

table 18.2. results for the prostate data example. the standard deviation for
the test errors is about 4.5.

method
1. nearest shrunken centroids
2. lasso
3. lasso on peaks

test errors/108 number of sites
459
113
35

34
22
28

the prediction results for this application of the lasso to the peaks are
shown in the last line of table 18.2: it does fairly well, but not as well
as the lasso on the raw spectra. however, the    tted model may be more
useful to the biologist as it yields 35 peak positions for further study. on
the other hand, the results suggest that there may be useful discriminatory
information between the peaks of the spectra, and the positions of the lasso
sites from line (2) of the table also deserve further examination.

18.4.2 the fused lasso for functional data

in the previous example, the features had a natural order, determined by
the mass-to-charge ratio m/z. more generally, we may have functional fea-
tures xi(t) that are ordered according to some index variable t. we have
already discussed several approaches for exploiting such structure.

we can represent xi(t) by their coe   cients in a basis of functions in t,
such as splines, wavelets or fourier bases, and then apply a regression using
these coe   cients as predictors. equivalently, one can instead represent the
coe   cients of the original features in these bases. these approaches are
described in section 5.3.

in the classi   cation setting, we discuss the analogous approach of penal-
ized discriminant analysis in section 12.6. this uses a penalty that explicitly
controls the resulting smoothness of the coe   cient vector.

the above methods tend to smooth the coe   cients uniformly. here we
present a more adaptive strategy that modi   es the lasso penalty to take
into account the ordering of the features. the fused lasso (tibshirani et
al., 2005) solves

     irp( nxi=1

min

(yi       0    

pxj=1

xij  j)2 +   1

pxj=1

|  j| +   2

p   1xj=1

|  j+1       j|). (18.22)

this criterion is strictly convex in   , so a unique solution exists. the    rst
penalty encourages the solution to be sparse, while the second encourages
it to be smooth in the index j.

the di   erence penalty in (18.22) assumes an uniformly spaced index j. if
instead the underlying index variable t has nonuniform values tj, a natural
generalization of (18.22) would be based on divided di   erences

18.4 linear classi   ers with l1 id173

667

o
i
t
a
r

2
g
o

l

4

2

0

2
   

0

200

400

600

800

1000

genome order

figure 18.8. fused lasso applied to cgh data. each point represents the
copy-number of a gene in a tumor sample, relative to that of a control (on the log
base-2 scale).

  2

p   1xj=1

|  j+1       j|
|tj+1     tj|

.

(18.23)

this amounts to having a penalty modi   er for each of the terms in the
series.

a particularly useful special case arises when the predictor matrix x =
in , the n    n identity matrix. this is a special case of the fused lasso,
used to approximate a sequence {yi}n
1 . the fused lasso signal approximator
solves

     irn( nxi=1

min

(yi       0       i)2 +   1

nxi=1

|  i| +   2

|  i+1       i|).

n    1xi=1

(18.24)

figure 18.8 shows an example taken from tibshirani and wang (2007). the
data in the panel come from a comparative genomic hybridization (cgh)
array, measuring the approximate log (base-two) ratio of the number of
copies of each gene in a tumor sample, as compared to a normal sample.
the horizontal axis represents the chromosomal location of each gene. the
idea is that in cancer cells, genes are often ampli   ed (duplicated) or deleted,
and it is of interest to detect these events. furthermore, these events tend
to occur in contiguous regions. the smoothed signal estimate from the
fused lasso signal approximator is shown in dark red (with appropriately
chosen values for   1 and   2). the signi   cantly nonzero regions can be used
to detect locations of gains and losses of genes in the tumor.

there is also a two-dimensional version of the fused lasso, in which the
parameters are laid out in a grid of pixels, and a penalty is applied to the

668

18. high-dimensional problems: p     n

   rst di   erences to the left, right, above and below the target pixel. this
can be useful for denoising or classifying images. friedman et al. (2007)
develop fast generalized coordinate descent algorithms for the one- and
two-dimensional fused lasso.

18.5 classi   cation when features are unavailable

in some applications the objects under study are more abstract in nature,
and it is not obvious how to de   ne a feature vector. as long as we can    ll
in an n    n proximity matrix of similarities between pairs of objects in our
database, it turns out we can put to use many of the classi   ers in our arsenal
by interpreting the proximities as inner-products. protein structures fall
into this category, and we explore an example in section 18.5.1 below.

in other applications, such as document classi   cation, feature vectors are
available but can be extremely high-dimensional. here we may not wish
to compute with such high-dimensional data, but rather store the inner-
products between pairs of documents. often these inner-products can be
approximated by sampling techniques.

pairwise distances serve a similar purpose, because they can be turned
into centered inner-products. proximity matrices are discussed in more de-
tail in chapter 14.

18.5.1 example: string kernels and protein classi   cation

an important problem in computational biology is to classify proteins into
functional and structural classes based on their sequence similarities. pro-
tein molecules are strings of amino acids, di   ering in both length and com-
position. in the example we consider, the lengths vary between 75   160
amino-acid molecules, each of which can be one of 20 di   erent types, labeled
using letters. here are two examples, of length 110 and 153, respectively:

iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv

erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi

phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla

rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk

lwglkvlqelsqwtvrsihdlrfisshqtgip

there have been many proposals for measuring the similarity between a
pair of protein molecules. here we focus on a measure based on the count
of matching substrings (leslie et al., 2004), such as the lqe above.

to construct our features, we count the number of times that a given
sequence of length m occurs in our string, and we compute this number

18.5 classi   cation when features are unavailable

669

for all possible sequences of length m. formally, for a string x, we de   ne a
feature map

  m(x) = {  a(x)}a   am

(18.25)

where am is the set of subsequences of length m, and   a(x) is the number
of times that    a    occurs in our string x. using this, we de   ne the inner
product

km(x1, x2) = h  m(x1),   m(x2)i,

(18.26)

which measures the similarity between the two strings x1, x2. this can be
used to drive, for example, a support vector classi   er for classifying strings
into di   erent protein classes.

now the number of possible sequences a is |am| = 20m, which can be
very large for moderate m, and the vast majority of the subsequences do
not match the strings in our training set. it turns out that we can compute
the n    n inner-product matrix or string kernel km (18.26) e   ciently
using tree-structures, without actually computing the individual vectors.
this methodology, and the data to follow, come from leslie et al. (2004).4
the data consist of 1708 proteins in two classes    negative (1663) and
positive (45). the two examples above, which we will call    x1    and    x2   ,
are from this set. we have marked the occurrences of subsequence lqe,
which appears in both proteins. there are 203 possible subsequences, so
  3(x) will be a vector of length 8000. for this example   lqe(x1) = 1 and
  lqe(x2) = 2.

using software from leslie et al. (2004), we computed the string kernel
for m = 4, which was then used in a support vector classi   er to    nd the
maximal margin solution in this 204 = 160, 000-dimensional feature space.
we used 10-fold cross-validation to compute the id166 predictions on all of
the training data. the orange curve in figure 18.9 shows the cross-validated
roc curve for the support vector classi   er, computed by varying the cut-
point on the real-valued predictions from the cross-validated support vector
classi   er. the area under the curve is 0.84. leslie et al. (2004) show that
the string kernel method is competitive with, but perhaps not as accurate
as, more specialized methods for protein string matching.

many other classi   ers can be computed using only the information in the
kernel matrix; some details are given in the next section. the results for
the nearest centroid classi   er (green), and distance-weighted one-nearest
neighbors (blue) are shown in figure 18.9. their performance is similar to
that of the support vector classi   er.

4we thank christina leslie for her help and for providing the data, which is available

on our book website.

670

18. high-dimensional problems: p     n

roc curves for string kernel

y
t
i
v
i
t
i
s
n
e
s

0

.

1

8

.

0

6
0

.

4

.

0

2

.
0

0
.
0

id166  0.84
nearest centroid  0.84
one   nearest neighbor 0.86

0.0

0.2

0.4

0.6

0.8

1.0

specificity

figure 18.9. cross-validated roc curves for protein example using the string
kernel. the numbers next to each method in the legend give the area under the
curve, an overall measure of accuracy. the id166 achieves better sensitivities than
the other two, which achieve better speci   cities.

18.5.2 classi   cation and other models using inner-product

kernels and pairwise distances

there are a number of other classi   ers, besides the support-vector ma-
chine, that can be implemented using only inner-product matrices. this
also implies they can be    kernelized    like the id166.

an obvious example is nearest-neighbor classi   cation, since we can trans-

form pairwise inner-products to pairwise distances:

||xi     xi   ||2 = hxi, xii + hxi    , xi   i     2hxi, xi   i.

(18.27)

a variation of 1-nn classi   cation is used in figure 18.9, which produces
a continuous discriminant score needed to construct a roc curve. this
distance-weighted 1-nn makes use of the distance of a test points to the
closest member of each class; see exercise 18.14.

nearest-centroid classi   cation follows easily as well. for training pairs
(xi, gi), i = 1, . . . , n , a test point x0, and class centroids   xk, k = 1, . . . , k
we can write
||x0       xk||2 = hx0, x0i    

hxi, xi   i, (18.28)

hx0, xii +

1
n 2

k xgi=k xgi    =k

2

nk xgi=k

18.5 classi   cation when features are unavailable

671

hence we can compute the distance of the test point to each of the cen-
troids, and perform nearest centroid classi   cation. this also implies that
methods like id116 id91 can also be implemented, using only the
inner products of the data points.

logistic and multinomial regression with quadratic id173 can
also be implemented with inner-product kernels; see section 12.3.3 and
exercise 18.13. exercise 12.10 derives id156 using an
inner-product kernel.

principal components can be computed using inner-product kernels as
well; since this is frequently useful, we give some details. suppose    rst
that we have a centered data matrix x, and let x = udvt be its svd
(18.12). then z = ud is the matrix of principal component variables (see
section 14.5.1). but if k = xxt , then it follows that k = ud2ut , and
hence we can compute z from the eigen decomposition of k. if x is not
centered, then we can center it using   x = (i     m)x, where m = 1
n 11t
is the mean operator. thus we compute the eigenvectors of the double-
centered kernel (i     m)k(i     m) for the principal components from an
uncentered inner-product matrix. exercise 18.15 explores this further, and
section 14.5.4 discusses in more detail kernel pca for general kernels, such
as the radial kernel used in id166s.

if instead we had available only the pairwise (squared) euclidean dis-

tances between observations,

   2
ii    = ||xi     xi   ||2,

(18.29)

it turns out we can do all of the above as well. the trick is to convert the
pairwise distances to centered inner-products, and then proceed as before.
we write

ii    = ||xi       x||2 + ||xi          x||2     2hxi       x, xi          xi.
   2

(18.30)

de   ning b = {      2

ii    /2}, we double center b:

  k = (i     m)b(i     m);

(18.31)
it is easy to check that   kii    = hxi       x, xi          xi, the centered inner-product
matrix.
distances and inner-products also allow us to compute the medoid in each
class   the observation with smallest average distance to other observations
in that class. this can be used for classi   cation (closest medoids), as well as
to drive k-medoids id91 (section 14.3.10). with abstract data objects
like proteins, medoids have a practical advantage over means. the medoid is
one of the training examples, and can be displayed. we tried closest medoids
in the example in the next section (see table 18.3), and its performance is
disappointing.

it is useful to consider what we cannot do with inner-product kernels and

distances:

672

18. high-dimensional problems: p     n

table 18.3. cross-validated error rates for the abstracts example. the nearest
shrunken centroids ended up using no-shrinkage, but does use a word-by-word
standardization (section 18.2). this standardization gives it a distinct advantage
over the other methods.

method

id166

1. nearest shrunken centroids
2.
3. nearest medoids
4.
5. nearest centroids

1-nn

cv error (se)
0.17 (0.05)
0.23 (0.06)
0.65 (0.07)
0.44 (0.07)
0.29 (0.07)

    we cannot standardize the variables; standardization signi   cantly im-

proves performance in the example in the next section.

    we cannot assess directly the contributions of individual variables.
in particular, we cannot perform individual t-tests,    t the nearest
shrunken centroids model, or    t any model that uses the lasso penalty.

    we cannot separate the good variables from the noise: all variables get
an equal say. if, as is often the case, the ratio of relevant to irrelevant
variables is small, methods that use kernels are not likely to work as
well as methods that do feature selection.

18.5.3 example: abstracts classi   cation

this somewhat whimsical example serves to illustrate a limitation of ker-
nel approaches. we collected the abstracts from 48 papers, 16 each from
bradley efron (be), trevor hastie and rob tibshirani (ht) (frequent co-
authors), and jerome friedman (jf). we extracted all unique words from
these abstracts, and de   ned features xij to be the number of times word
j appears in abstract i. this is the so-called bag of words representation.
quotations, parentheses and special characters were    rst removed from the
abstracts, and all characters were converted to lower case. we also removed
the word    we   , which could unfairly discriminate ht abstracts from the
others.

there were 4492 total words, of which p = 1310 were unique. we sought
to classify the documents into be, ht or jf on the basis of the features
xij. although it is arti   cial, this example allows us to assess the possible
degradation in performance if information speci   c to the raw features is
not used.

we    rst applied the nearest shrunken centroid classi   er to the data, using
10-fold cross-validation. it essentially chose no shrinkage, and so used all the
features; see the    rst line of table 18.3. the error rate is 17%; the number
of features can be reduced to about 500 without much loss in accuracy.

18.5 classi   cation when features are unavailable

673

note that the nearest shrunken classi   er requires the raw feature matrix
x in order to standardize the features individually. figure 18.10 shows the

be

ht

jf

problems
method
presented
propose
frequentist
bayesian
those
id136
when
variables
accuracy
values
technology
procedure
are
algorithm
than
using
bayes
predictive

figure 18.10. abstracts example: top 20 scores from nearest shrunken cen-
troids. each score is the standardized di   erence in frequency for the word in the
given class (be, ht or jf) versus all classes. thus a positive score (to the right
of the vertical grey zero lines) indicates a higher frequency in that class; a negative
score indicates a lower relative frequency.

top 20 discriminating words, with a positive score indicating that a word
appears more in that class than in the other classes.

some of these terms make sense: for example    frequentist    and    bayesian   
re   ect efron   s greater emphasis on statistical id136. however, many oth-
ers are surprising, and re   ect personal writing styles: for example, fried-
man   s use of    presented    and ht   s use of    propose   .

we then applied the support vector classi   er with linear kernel and no
id173, using the    all pairs    (ovo) method to handle the three
classes (id173 of the id166 did not improve its performance). the
result is shown in table 18.3. it does somewhat worse than the nearest
shrunken centroid classi   er.

as mentioned, the    rst line of table 18.3 represents nearest shrunken cen-
troids (with no shrinkage). denote by sj the pooled within-class standard
deviation for feature j, and s0 the median of the sj values. then line (1)
also corresponds to nearest centroid classi   cation, after    rst standardizing
each feature by sj + s0 [recall (18.4) on page 652].

line (3) shows that the performance of nearest medoids is very poor,
something which surprised us. it is perhaps due to the small sample sizes

674

18. high-dimensional problems: p     n

and high dimensions, with medoids having much higher variance than
means. the performance of the one-nearest neighbor classi   er is also poor.
the performance of the nearest centroid classi   er is also shown in ta-
ble 18.3 in line (5): it is better than nearest medoids, but worse than that
of nearest shrunken centroids, even with no shrinkage. the di   erence seems
to be the standardization of each feature that is done in nearest shrunken
centroids. this standardization is important here, and requires access to
the individual feature values. nearest centroids uses a spherical metric, and
relies on the fact that the features are in similar units. the support vector
machine estimates a linear combination of the features and can better deal
with unstandardized features.

18.6 high-dimensional regression: supervised

principal components

in this section we describe a simple approach to regression and generalized
regression that is especially useful when p     n . we illustrate the method
on another microarray data example. the data is taken from rosenwald
et al. (2002) and consists of 240 samples from patients with di   use large
b-cell lymphoma (dlbcl), with gene expression measurements for 7399
genes. the outcome is survival time, either observed or right censored. we
randomly divided the lymphoma samples into a training set of size 160 and
a test set of size 80.

although supervised principal components is useful for id75,
its most interesting applications may be in survival studies, which is the
focus of this example.

we have not yet discussed regression with censored survival data in this
book; it represents a generalized form of regression in which the outcome
variable (survival time) is only partly observed for some individuals. sup-
pose for example we carry out a medical study that lasts for 365 days, and
for simplicity all subjects are recruited on day one. we might observe one
individual to die 200 days after the start of the study. another individ-
ual might still be alive at 365 days when the study ends. this individual
is said to be    right censored    at 365 days. we know only that he or she
lived at least 365 days. although we do not know how long past 365 days
the individual actually lived, the censored observation is still informative.
this is illustrated in figure 18.11. figure 18.12 shows the survival curve
estimated by the kaplan   meier method for the 80 patients in the test set.
see for example kalb   eisch and prentice (1980) for a description of the
kaplan   meier method.

our objective in this example is to    nd a set of features (genes) that
can predict the survival of an independent set of patients. this could be

18.6 high-dimensional regression: supervised principal components

675

t

n
e

i
t

a
p

4

3

2

1

0 

100 

200 

300 

365 

time(days)

figure 18.11. censored survival data. for illustration there are four patients.
the    rst and third patients die before the study ends. the second patient is alive
at the end of the study (365 days), while the fourth patient is lost to follow-up
before the study ends. for example, this patient might have moved out of the
country. the survival times for patients two and four are said to be    censored.   

survival function

l

l

ll

lll

lll

l

ll

ll

l l

ll

l lllll

l ll

l

l

l

l

0
1

.

8
0

.

6
0

.

)
t

   

t
(
r
p

4
0

.

2
0

.

0

.
0

0

5

10

15

20

months t

figure 18.12. lymphoma data. the kaplan   meier estimate of the survival
function for the 80 patients in the test set, along with one-standard-error curves.
the curve estimates the id203 of surviving past t months. the ticks indicate
censored observations.

676

18. high-dimensional problems: p     n

poor cell type

good cell type

survival time

figure 18.13. underlying conceptual model for supervised principal compo-
nents. there are two cell types, and patients with the good cell type live longer on
the average. supervised principal components estimate the cell type, by averaging
the expression of genes that re   ect it.

useful as a prognostic indicator to aid in choosing treatments, or to help
understand the biological basis for the disease.

the underlying conceptual model for supervised principal components
is shown in figure 18.13. we imagine that there are two cell types, and
patients with the good cell type live longer on the average. however there
is considerable overlap in the two sets of survival times. we might think
of survival time as a    noisy surrogate    for cell type. a fully supervised
approach would give the most weight to those genes having the strongest
relationship with survival. these genes are partially, but not perfectly, re-
lated to cell type. if we could instead discover the underlying cell types of
the patients, often re   ected by a sizable signature of genes acting together
in pathways, then we might do a better job of predicting patient survival.
although the cell type in figure 18.13 is discrete, it is useful to imagine
a continuous cell type, de   ne by some linear combination of the features.
we will estimate the cell type as a continuous quantity, and then discretize
it for display and interpretation.

how can we    nd the linear combination that de   nes the important under-
lying cell types? principal components analysis (section 14.5) is an e   ective
method for    nding linear combinations of features that exhibit large varia-
tion in a dataset. but what we seek here are linear combinations with both
high variance and signi   cant correlation with the outcome. the lower right
panel of figure 18.14 shows the result of applying standard principal com-
ponents in this example; the leading component does not correlate strongly
with survival (details are given in the    gure caption).

hence we want to encourage principal component analysis to    nd linear
combinations of features that have high correlation with the outcome. to
do this, we restrict attention to features which by themselves have a siz-
able correlation with the outcome. this is summarized in the supervised
principal components algorithm 18.1, and illustrated in figure 18.14.

the details in steps (1) and (2b) will depend on the type of outcome
variable. for a standard regression problem, we use the univariate linear
least squares coe   cients in step (1) and a linear least squares model in

18.6 high-dimensional regression: supervised principal components

677

supervised pc

best single gene

l

i

a
v
v
r
u
s

 
f

o

 
y
t
i
l
i

b
a
b
o
r
p

l

i

a
v
v
r
u
s

 
f

o
 
y
t
i
l
i

b
a
b
o
r
p

l

i

a
v
v
r
u
s

 
f
o
 
y
t
i
l
i

b
a
b
o
r
p

low score
high score

p=0.15

0

5

10

15

20

supervised principal component     27 genes

p=0.006

0

5

10

15

20

principal component     7399 genes

p=0.14

0

.

1

8
0

.

6

.

0

4

.

0

2

.

0

0

.

0

0
1

.

8

.

0

.

6
0

4
.
0

2
.
0

0
.
0

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

80

160

0

2

4

0

5

10

15

20

patients

absolute cox score

months

1 

27 

s
e
n
e
g

50 

7350 

7399 

1

figure 18.14. supervised principal components on the lymphoma data. the
left panel shows a heatmap of a subset of the gene-expression training data. the
rows are ordered by the magnitude of the univariate cox-score, shown in the mid-
dle vertical column. the top 50 and bottom 50 genes are shown. the supervised
principal component uses the top 27 genes (chosen by 10-fold cv). it is repre-
sented by the bar at the top of the heatmap, and is used to order the columns
of the expression matrix. in addition, each row is multiplied by the sign of the
cox-score. the middle panel on the right shows the survival curves on the test
data when we create a low and high group by splitting this supervised pc at zero
(training data mean). the curves are well separated, as indicated by the p-value
for the log-rank test. the top panel does the same, using the top-scoring gene on
the training data. the curves are somewhat separated, but not signi   cantly. the
bottom panel uses the    rst principal component on all the genes, and the separa-
tion is also poor. each of the top genes can be interpreted as noisy surrogates for
a latent underlying cell-type characteristic, and supervised principal components
uses them all to estimate this latent factor.

678

18. high-dimensional problems: p     n

algorithm 18.1 supervised principal components.

1. compute the standardized univariate regression coe   cients for the

outcome as a function of each feature separately.

2. for each value of the threshold    from the list 0       1 <   2 <        <   k:
(a) form a reduced data matrix consisting of only those features
whose univariate coe   cient exceeds    in absolute value, and
compute the    rst m principal components of this matrix.

(b) use these principal components in a regression model to predict

the outcome.

3. pick    (and m) by cross-validation.

step (2b). for survival problems, cox   s proportional hazards regression
model is widely used; hence we use the score test from this model in step (1)
and the multivariate cox model in step (2b). the details are not essential
for understanding the basic method; they may be found in bair et al. (2006).
figure 18.14 shows the results of supervised principal components in this
example. we used a cox-score cuto    of 3.53, yielding 27 genes, where the
value 3.53 was found through 10-fold cross-validation. we then computed
the    rst principal component (m = 1) using just this subset of the data,
as well as its value for each of the test observations. we included this as
a quantitative predictor in a cox regression model, and its likelihood-ratio
signi   cance was p = 0.005. when dichotomized (using the mean score on
the training data as a threshold), it clearly separates the patients in the
test set into low and high risk groups (middle-right panel of figure 18.14,
p = 0.006).

the top-right panel of figure 18.14 uses the top scoring gene (dichot-
omized) alone as a predictor of survival. it is not signi   cant on the test set.
likewise, the lower-right panel shows the dichotomized principal compo-
nent using all the training data, which is also not signi   cant.

our procedure allows m > 1 principal components in step (2a). however,
the supervision in step (1) encourages the principal components to align
with the outcome, and thus in most cases only the    rst or    rst few com-
ponents tend to be useful for prediction. in the mathematical development
below, we consider only the    rst component, but extensions to more than
one component can be derived in a similar way.

18.6.1 connection to latent-variable modeling

a formal connection between supervised principal components and the un-
derlying cell type model (figure 18.13) can be seen through a latent variable
model for the data. suppose we have a response variable y which is related

18.6 high-dimensional regression: supervised principal components

679

to an underlying latent variable u by a linear model

y =   0 +   1u +   .

(18.32)

in addition, we have measurements on a set of features xj indexed by j     p
(for pathway), for which

xj =   0j +   1ju +   j,

j     p.

(18.33)

the errors    and   j are assumed to have mean zero and are independent of
all other random variables in their respective models.

we also have many additional features xk, k 6    p which are independent
of u . we would like to identify p, estimate u , and hence    t the predic-
tion model (18.32). this is a special case of a latent-structure model, or
single-component factor-analysis model (mardia et al., 1979, see also sec-
tion 14.7). the latent factor u is a continuous version of the cell type
conceptualized in figure 18.13.

the supervised principal component algorithm can be seen as a method

for    tting this model:

    the screening step (1) estimates the set p.

    given bp, the largest principal component in step (2a) estimates the

latent factor u .

    finally, the regression    t in step (2b) estimates the coe   cient in

model (18.32).

step (1) is natural, since on average the regression coe   cient is nonzero
only if   1j is non-zero. hence this step should select the features j     p.
step (2a) is natural if we assume that the errors   j have a gaussian dis-
tribution, with the same variance. in this case the principal component is
the maximum likelihood estimate for the single factor model (mardia et
al., 1979). the regression in (2b) is an obvious    nal step.

suppose there are a total of p features, with p1 features in the relevant set
p. then if p and p1 grow but p1 is small relative to p, one can show (under
reasonable conditions) that the leading supervised principal component
is consistent for the underlying latent factor. the usual leading principal
component may not be consistent, since it can be contaminated by the
presence of a large number of    noise    features.

finally, suppose that the threshold used in step (1) of the supervised
principal component procedure yields a large number of features for com-
putation of the principal component. then for interpretational purposes, as
well as for practical uses, we would like some way of    nding a reduced a set
of features that approximates the model. pre-conditioning (section 18.6.3)
is one way of doing this.

680

18. high-dimensional problems: p     n

18.6.2 relationship with partial least squares

supervised principal components is closely related to partial least squares
regression (section 3.5.2). bair et al. (2006) found that the key to the good
performance of supervised principal components was the    ltering out of
noisy features in step (2a). partial least squares (section 3.5.2) downweights
noisy features, but does not throw them away; as a result a large number
of noisy features can contaminate the predictions. however, a modi   cation
of the partial least squares procedure has been proposed that has a similar
   avor to supervised principal components [brown et al. (1991),nadler and
coifman (2005), for example]. we select the features as in steps (1) and
(2a) of supervised principal components, but then apply pls (rather than
principal components) to these features. for our current discussion, we call
this    thresholded pls.   

thresholded pls can be viewed as a noisy version of supervised principal
components, and hence we might not expect it to work as well in practice.
assume the variables are all standardized. the    rst pls variate has the
form

z =xj   p

hy, xjixj,

(18.34)

and can be thought of as an estimate of the latent factor u in model (18.33).
in contrast, the supervised principal components direction   u satis   es

  u =

1

d2 xj   p

h  u, xjixj,

(18.35)

where d is the leading singular value of xp . this follows from the de   nition
of the leading principal component. hence thresholded pls uses weights
which are the inner product of y with each of the features, while supervised
principal components uses the features to derive a    self-consistent    estimate
  u. since many features contribute to the estimate   u, rather than just the
single outcome y, we can expect   u to be less noisy than z. in fact, if there
are p1 features in the set p, and n, p and p1 go to in   nity with p1/n     0,
then it can be shown using the techniques in bair et al. (2006) that

z = u + op(1)

  u = u + op(pp1/n ),

(18.36)

where u is the true (unobservable) latent variable in the model (18.32),
(18.33).

we now present a simulation example to compare the methods numeri-
cally. there are n = 100 samples and p = 5000 genes. we generated the
data as follows:

18.6 high-dimensional regression: supervised principal components

681

figure 18.15. heatmap of the outcome (left column) and    rst 500 genes from
a realization from model (18.37). the genes are in the columns, and the samples
are in the rows.

4 +   ij

xij = (3 +   ij
xij = (1.5 +   ij
xij =   ij
50p50
yi = 2    1

5.5 +   ij

if i     50,
if i > 50
if 1     i     25 or 51     i     75
if 26     i     50 or 76     i     100

j = 1, . . . , 50

j = 51, . . . , 250

j = 251, . . . , 5000

j=1 xij +   i

(18.37)

where   ij and   i are independent normal random variables with mean 0 and
standard deviations 1 and 1.5, respectively. thus in the    rst 50 genes, there
is an average di   erence of 1 unit between samples 1   50 and 51   100, and this
di   erence correlates with the outcome y. the next 200 genes have a large
average di   erence of 4 units between samples (1   25, 51   75) and (26   50,
76   100), but this di   erence is uncorrelated with the outcome. the rest of
the genes are noise. figure 18.15 shows a heatmap of a typical realization,
with the outcome at the left, and the    rst 500 genes to the right.

we generated 100 simulations from this model, and summarize the test
error results in figure 18.16. the test errors of principal components and
partial least squares are shown at the right of the plot; both are badly
a   ected by the noisy features in the data. supervised principal components
and thresholded pls work best over a wide range of the number of selected
features, with the former showing consistently lower test errors.

while this example seems    tailor-made    for supervised principal com-
ponents, its good performance seems to hold in other simulated and real
datasets (bair et al., 2006).

18.6.3 pre-conditioning for feature selection

supervised principal components can yield lower test errors than competing
methods, as shown in figure 18.16. however, it does not always produce a
sparse model involving only a small number of features (genes). even if the
thresholding in step (1) of the algorithm yields a relatively small number

682

18. high-dimensional problems: p     n

r
o
r
r

e

 

 
t
s
e
t
e
r
a
u
q
s
n
a
e
m

 

 
t

o
o
r
e
v
i
t

 

l

a
e
r

thresholded pls
supervised principal components

5
2

.

1

0
2
1

.

5
1
1

.

0
1

.

1

5
0
1

.

0
0
1

.

0

50

100

150

200

250

300

...

5000

number of features

figure 18.16. root mean squared test error (   one standard error), for
supervised principal components and thresholded pls on 100 realizations from
model (18.37). all methods use one component, and the errors are relative to
the noise standard deviation (the bayes error is 1.0). for both methods, di   erent
values for the    ltering threshold were tried and the number of features retained
is shown on the horizontal axis. the extreme right points correspond to regular
principal components and partial least squares, using all the genes.

of features, it may be that some of the omitted features have sizable inner
products with the supervised principal component (and could act as a good
surrogate). in addition, highly correlated features will tend to be chosen
together, and there may be great deal of redundancy in the set of selected
features.

the lasso (sections 18.4 and 3.4.2), on the other hand, produces a sparse
model from the data. how do the test errors of the two methods compare on
the simulated example of the last section? figure 18.17 shows the test errors
for one realization from model (18.37) for the lasso, supervised principal
components, and the pre-conditioned lasso (described below).

we see that supervised principal components (orange curve) reaches its
lowest error when about 50 features are included in the model, which is
the correct number for the simulation. although a linear model in the    rst
50 features is optimal, the lasso (green) is adversely a   ected by the large
number of noisy features, and starts over   tting when far fewer are in the
model.

can we get the low test error of supervised principal components along
with the sparsity of the lasso? this is the goal of pre-conditioning (paul
et al., 2008). in this approach, one    rst computes the supervised principal
component predictor   yi for each observation in the training set (with the

18.7 feature assessment and the multiple-testing problem

683

lasso
supervised principal components
preconditioned lasso

r
o
r
r

e

 

 
t
s
e
t
n
a
e
m

4

.

4

2
4

.

0

.

4

8
3

.

6
3

.

4
3

.

2
3

.

0
3

.

0

50

100

150

200

250

number of features in model

figure 18.17. test errors for the lasso, supervised principal components,
and pre-conditioned lasso, for one realization from model (18.37). each model is
indexed by the number of non-zero features. the supervised principal component
path is truncated at 250 features. the lasso self-truncates at 100, the sample size
(see section 18.4). in this case, the pre-conditioned lasso achieves the lowest error
with about 25 features.

threshold selected by cross-validation). then we apply the lasso with   yi as
the outcome variable, in place of the usual outcome yi. all features are used
in the lasso    t, not just those that were retained in the thresholding step
in supervised principal components. the idea is that by    rst denoising the
outcome variable, the lasso should not be as adversely a   ected by the large
number of noise features. figure 18.17 shows that pre-conditioning (purple
curve) has been successful here, yielding much lower test error than the
usual lasso, and as low (in this case) as for supervised principal components.
it also can achieve this using less features. the usual lasso, applied to
the raw outcome, starts to over   t more quickly than the pre-conditioned
version. over   tting is not a problem, since the outcome variable has been
denoised. we usually select the tuning parameter for the pre-conditioned
lasso on more subjective grounds, like parsimony.

pre-conditioning can be applied in a variety of settings, using initial
estimates other than supervised principal components and post-processors
other than the lasso. more details may be found in paul et al. (2008).

18.7 feature assessment and the multiple-testing

problem

in the    rst part of this chapter we discuss prediction models in the p     n
setting. here we consider the more basic problem of assessing the signif-

684

18. high-dimensional problems: p     n

icance of each of the p features. consider the protein mass spectrometry
example of section 18.4.1. in that problem, the scientist might not be inter-
ested in predicting whether a given patient has prostate cancer. rather the
goal might be to identify proteins whose abundance di   ers between nor-
mal and cancer samples, in order to enhance understanding of the disease
and suggest targets for drug development. thus our goal is to assess the
signi   cance of individual features. this assessment is usually done without
the use of a multivariate predictive model like those in the    rst part of this
chapter. the feature assessment problem moves our focus from prediction
to the traditional statistical topic of multiple hypothesis testing. for the
remainder of this chapter we will use m instead of p to denote the number
of features, since we will frequently be referring to p-values.

table 18.4. subset of the 12, 625 genes from microarray study of radiation
sensitivity. there are a total of 44 samples in the normal group and 14 in the
radiation sensitive group; we only show three samples from each group.

gene 1
gene 2
gene 3
gene 4
...
gene 12,625

7.85
15.44
-1.79
-11.74
...
-14.09

normal

29.74
2.70
15.52
22.35
...
32.77

29.50
19.37
-3.13
-36.11
...
57.78

. . .
. . .
. . .
. . .
...
. . .

radiation sensitive
-18.89
79.18
4.75
-2.32
...
-101.44

-50.75
-7.41
12.64
7.24
...
24.09

17.20
6.57
-8.32
-52.17
...
-32.84

. . .
. . .
. . .
. . .
...
. . .

consider, for example, the microarray data in table 18.4, taken from a
study on the sensitivity of cancer patients to ionizing radiation treatment
(rieger et al., 2004). each row consists of the expression of genes in 58
patient samples: 44 samples were from patients with a normal reaction, and
14 from patients who had a severe reaction to radiation. the measurements
were made on oligo-nucleotide microarrays. the object of the experiment
was to    nd genes whose expression was di   erent in the radiation sensitive
group of patients. there are m = 12, 625 genes altogether; the table shows
the data for some of the genes and samples for illustration.

to identify informative genes, we construct a two-sample t-statistic for

each gene.

tj =

  x2j       x1j

sej

,

(18.38)

where   xkj =pi   c   

xij/n   . here c    are the indices of the n    samples in
group    , where     = 1 is the normal group and     = 2 is the sensitive group.
the quantity sej is the pooled within-group standard error for gene j:

18.7 feature assessment and the multiple-testing problem

685

0
0
8

0
0
6

0
0
4

0
0
2

0

   4

   2

0

2

4

t   statistics

figure 18.18. radiation sensitivity microarray example. a histogram of the
12, 625 t-statistics comparing the radiation-sensitive versus insensitive groups.
overlaid in blue is the histogram of the t-statistics from 1000 permutations of the
sample labels.

sej =     jq 1

n1

+ 1
n2

;     2

j =

1

n1+n2   2 xi   c1

(xij       x1j)2 + xi   c2

(xij       x2j)2! .

(18.39)
a histogram of the 12,625 t-statistics is shown in orange in figure 18.18,
ranging in value from    4.7 to 5.0. if the tj values were normally distributed
we could consider any value greater than two in absolute value to be sig-
ni   cantly large. this would correspond to a signi   cance level of about 5%.
here there are 1189 genes with |tj|     2. however with 12,625 genes we
would expect many large values to occur by chance, even if the group-
ing is unrelated to any gene. for example, if the genes were independent
(which they are surely not), the number of falsely signi   cant genes would
have a binomial distribution with mean 12, 625   0.05 = 631.3 and standard
deviation 24.5; the actual 1189 is way out of range.
how do we assess the results for all 12,625 genes? this is called the mul-
tiple testing problem. we can start as above by computing a p-value for
each gene. this can be done using the theoretical t-distribution probabil-
ities, which assumes the features are normally distributed. an attractive
alternative approach is to use the permutation distribution, since it avoids
assumptions about the distribution of the data. we compute (in principle)

14(cid:1) permutations of the sample labels, and for each permutation

j . then the p-value for gene j is

all k =(cid:0)58

k compute the t-statistics tk

686

18. high-dimensional problems: p     n

pj =

1
k

kxk=1

i(|tk

j| > |tj|).

(18.40)

of course, (cid:0)58

14(cid:1) is a large number (around 1013) and so we can   t enumer-

ate all of the possible permutations. instead we take a random sample of
the possible permutations; here we took a random sample of k = 1000
permutations.

to exploit the fact that the genes are similar (e.g., measured on the
same scale), we can instead pool the results for all genes in computing the
p-values.

pj =

1

m k

mxj    =1

kxk=1

i(|tk

j    | > |tj|).

(18.41)

this also gives more granular p-values than does (18.40), since there many
more values in the pooled null distribution than there are in each individual
null distribution.

using this set of p-values, we would like to test the hypotheses:

h0j = treatment has no e   ect on gene j

versus

(18.42)

h1j = treatment has an e   ect on gene j

for all j = 1, 2, . . . , m . we reject h0j at level    if pj <   . this test has
type-i error equal to   ; that is, the id203 of falsely rejecting h0j is   .
now with many tests to consider, it is not clear what we should use
as an overall measure of error. let aj be the event that h0j is falsely
rejected; by de   nition pr(aj) =   . the family-wise error rate (fwer)
is the id203 of at least one false rejection, and is a commonly used
overall measure of error. in detail, if a =    m
j=1aj is the event of at least
one false rejection, then the fwer is pr(a). generally pr(a)        for
large m , and depends on the correlation between the tests. if the tests are
independent each with type-i error rate   , then the family-wise error rate
of the collection of tests is (1     (1       )m ). on the other hand, if the tests
have positive dependence, that is pr(aj|ak) > pr(aj), then the fwer
will be less than (1     (1       )m ). positive dependence between tests often
occurs in practice, in particular in genomic studies.
one of the simplest approaches to multiple testing is the bonferroni
method. it makes each individual test more stringent, in order to make the
fwer equal to at most   : we reject h0j if pj <   /m . it is easy to show
that the resulting fwer is        (exercise 18.16). the bonferroni method
can be useful if m is relatively small, but for large m it is too conservative,
that is, it calls too few genes signi   cant.

in our example, if we test at level say    = 0.05, then we must use the
threshold 0.05/12, 625 = 3.9  10   6. none of the 12, 625 genes had a p-value
this small.

18.7 feature assessment and the multiple-testing problem

687

there are variations to this approach that adjust the individual p-values
to achieve an fwer of at most   , with some approaches avoiding the
assumption of independence; see, e.g., dudoit et al. (2002b).

18.7.1 the false discovery rate

a di   erent approach to multiple testing does not try to control the fwer,
but focuses instead on the proportion of falsely signi   cant genes. as we will
see, this approach has a strong practical appeal.

table 18.5 summarizes the theoretical outcomes of m hypothesis tests.
note that the family-wise error rate is pr(v     1). here we instead focus

table 18.5. possible outcomes from m hypothesis tests. note that v is the
number of false-positive tests; the type-i error rate is e(v )/m0. the type-ii error
rate is e(t )/m1, and the power is 1     e(t )/m1.

called

called

not signi   cant

h0 true
h0 false

total

u
t

m     r

signi   cant total
m0
m1
m

v
s
r

on the false discovery rate

fdr = e(v /r).

(18.43)

in the microarray setting, this is the expected proportion of genes that
are incorrectly called signi   cant, among the r genes that are called signif-
icant. the expectation is taken over the population from which the data
are generated. benjamini and hochberg (1995)    rst proposed the notion of
false discovery rate, and gave a testing procedure (algorithm 18.2) whose
fdr is bounded by a user-de   ned level   . the benjamini   hochberg (bh)
procedure is based on p-values; these can be obtained from an asymptotic
approximation to the test statistic (e.g., gaussian), or a permutation dis-
tribution, as is done here.

if the hypotheses are independent, benjamini and hochberg (1995) show
that regardless of how many null hypotheses are true and regardless of the
distribution of the p-values when the null hypothesis is false, this procedure
has the property

fdr    

m0
m

         .

(18.45)

for illustration we chose    = 0.15. figure 18.19 shows a plot of the or-

dered p-values p(j), and the line with slope 0.15/12625.

688

18. high-dimensional problems: p     n

algorithm 18.2 benjamini   hochberg (bh) method.

1. fix the false discovery rate    and let p(1)     p(2)                p(m ) denote

the ordered p-values

2. de   ne

l = maxnj : p(j) <      

j

mo.

(18.44)

3. reject all hypotheses h0j for which pj     p(l), the bh rejection

threshold.

l

e
u
a
v
   
p

3
   
^
0
1
*
5

4
   
^
0
1
*
5

5
   
^
0
1
*
5

6
   
^
0
1
*
5

   

1

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      

                 

   

       

   

5

10

50

100

genes ordered by p   value

figure 18.19. microarray example continued. shown is a plot of the ordered
p-values p(j) and the line 0.15    (j/12, 625), for the benjamini   hochberg method.
the largest j for which the p-value p(j) falls below the line, gives the bh threshold.
here this occurs at j = 11, indicated by the vertical line. thus the bh method
calls signi   cant the 11 genes (in red) with smallest p-values.

18.7 feature assessment and the multiple-testing problem

689

algorithm 18.3 the plug-in estimate of the false discovery rate.

1. create k permutations of the data, producing t-statistics tk

j for fea-

tures j = 1, 2, . . . , m and permutations k = 1, 2, . . . , k.

2. for a range of values of the cut-point c, let

robs =

mxj=1

i(|tj| > c), [e(v ) =

1
k

mxj=1

kxk=1

i(|tk

j| > c).

(18.46)

3. estimate the fdr by [fdr = [e(v )/robs.

starting at the left and moving right, the bh method    nds the last time
that the p-values fall below the line. this occurs at j = 11, so we reject
the 11 genes with smallest p-values. note that the cuto    occurs at the 11th
smallest p-value, 0.00012, and the 11th largest of the values |tj| is 4.101
thus we reject the 11 genes with |tj|     4.101.
from our brief description, it is not clear how the bh procedure works;
that is, why the corresponding fdr is at most 0.15, the value used for   .
indeed, the proof of this fact is quite complicated (benjamini and hochberg,
1995).

a more direct way to proceed is a plug-in approach. rather than starting
with a value for   , we    x a cut-point for our t-statistics, say the value
4.101 that appeared above. the number of observed values |tj| equal or
greater than 4.101 is 11. the total number of permutation values |tk
j| equal
or greater than 4.101 is 1518, for an average of 1518/1000 = 1.518 per
permutation. thus a direct estimate of the false discovery rate is [fdr =
1.518/11     14%. note that 14% is approximately equal to the value of
   = 0.15 used above (the di   erence is due to discreteness). this procedure
is summarized in algorithm 18.3. to recap:

the plug-in estimate of fdr of algorithm 18.3 is equivalent to the bh
procedure of algorithm 18.2, using the permutation p-values (18.40).

this correspondence between the bh method and the plug-in estimate is
not a coincidence. exercise 18.17 shows that they are equivalent in general.
note that this procedure makes no reference to p-values at all, but rather
works directly with the test statistics.

the plug-in estimate is based on the approximation

e(v /r)    

e(v )
e(r)

,

(18.47)

and in general [fdr is a consistent estimate of fdr (storey, 2002; storey et
al., 2004). note that the numerator [e(v ) actually estimates (m/m0)e(v ),

690

18. high-dimensional problems: p     n

since the permutation distribution uses m rather m0 null hypotheses.
hence if an estimate of m0 is available, a better estimate of fdr can be
obtained from (   m0/m )   [fdr. exercise 18.19 shows a way to estimate m0.
the most conservative (upwardly biased) estimate of fdr uses m0 = m .
equivalently, an estimate of m0 can be used to improve the bh method,
through relation (18.45).

the reader might be surprised that we chose a value as large as 0.15 for
  , the fdr bound. we must remember that the fdr is not the same as
type-i error, for which 0.05 is the customary choice. for the scientist, the
false discovery rate is the expected proportion of false positive genes among
the list of genes that the statistician tells him are signi   cant. microarray
experiments with fdrs as high as 0.15 might still be useful, especially if
they are exploratory in nature.

18.7.2 asymmetric cutpoints and the sam procedure

in the testing methods described above, we used the absolute value of the
test statistic tj, and hence applied the same cut-points to both positive and
negative values of the statistic. in some experiments, it might happen that
most or all of the di   erentially expressed genes change in the positive direc-
tion (or all in the negative direction). for this situation it is advantageous
to derive separate cut-points for the two cases.

k=1 tk

(j), where tk

tk
(m ) are the ordered test statistics from permutation k.

permutations of the data:   t(j) = (1/k)pk

the signi   cance analysis of microarrays (sam) approach o   ers a way of
doing this. the basis of the sam method is shown in figure 18.20. on the
vertical axis we have plotted the ordered test statistics t(1)     t(2)               
t(m ), while the horizontal axis shows the expected order statistics from the
(2)               
two lines are drawn, parallel to the 45    line,     units away. starting at
the origin and moving to the right, we    nd the    rst place that the genes
leave the band. this de   nes the upper cutpoint chi and all genes beyond
that point are called signi   cant (marked red). similarly we    nd the lower
cutpoint clow for genes in the bottom left corner. thus each value of the
tuning parameter     de   nes upper and lower cutpoints, and the plug-in
estimate [fdr for each of these cutpoints is estimated as before. typically
a range of values of     and associated [fdr values are computed, from which
a particular pair are chosen on subjective grounds.

(1)     tk

the advantage of the sam approach lies in the possible asymmetry of
the cutpoints. in the example of figure 18.20, with     = 0.71 we obtain
11 signi   cant genes; they are all in the upper right. the data points in the
bottom left never leave the band, and hence clow =       . hence for this
value of    , no genes are called signi   cant on the left (negative) side. we
do not impose symmetry on the cutpoints, as was done in section 18.7.1,
as there is no reason to assume similar behavior at the two ends.

18.7 feature assessment and the multiple-testing problem

691

       
       

   

               
               
               

c
i
t
s
i
t

a

t
s
   

t

4

2

0

2
   

4
   

   

         

chi

                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             

   4

   2

0

2

4

expected order statistics

figure 18.20. sam plot for the radiation sensitivity microarray data. on the
vertical axis we have plotted the ordered test statistics, while the horizontal axis
shows the expected order statistics of the test statistics from permutations of the
data. two lines are drawn, parallel to the 45    line,     units away from it. starting
at the origin and moving to the right, we    nd the    rst place that the genes leave
the band. this de   nes the upper cut-point chi and all genes beyond that point are
called signi   cant (marked in red). similarly we de   ne a lower cutpoint clow. for
the particular value of     = 0.71 in the plot, no genes are called signi   cant in the
bottom left.

692

18. high-dimensional problems: p     n

there is some similarity between this approach and the asymmetry possi-
ble with likelihood-ratio tests. suppose we have a log-likelihood    0(tj) under
the null-hypothesis of no e   ect, and a log-likelihood    (tj) under the alterna-
tive. then a likelihood ratio test amounts to rejecting the null-hypothesis
if

   (tj)        0(tj) >    ,

(18.48)

for some    . depending on the likelihoods, and particularly their relative
values, this can result in a di   erent threshold for tj than for    tj. the sam
procedure rejects the null-hypothesis if

|t(j)       t(j)| >    

(18.49)

again, the threshold for each t(j) depends on the corresponding value of
the null value   t(j).

18.7.3 a bayesian interpretation of the fdr

there is an interesting bayesian view of the fdr, developed in storey
(2002) and efron and tibshirani (2002). first we need to de   ne the positive
false discovery rate (pfdr) as

pfdr = e(cid:20) v

r(cid:12)(cid:12)(cid:12)(cid:12) r > 0(cid:21) .

(18.50)

the additional term positive refers to the fact that we are only interested
in estimating an error rate where positive    ndings have occurred. it is
this slightly modi   ed version of the fdr that has a clean bayesian inter-
pretation. note that the usual fdr [expression (18.43)] is not de   ned if
pr(r = 0) > 0.

let    be a rejection region for a single test; in the example above we used
   = (      ,   4.10)     (4.10,   ). suppose that m identical simple hypothe-
sis tests are performed with the i.i.d. statistics t1, . . . , tm and rejection
region   . we de   ne a random variable zj which equals 0 if the jth null
hypothesis is true, and 1 otherwise. we assume that each pair (tj, zj) are
i.i.d random variables with

tj|zj     (1     zj)    f0 + zj    f1

(18.51)

for some distributions f0 and f1. this says that each test statistic tj comes
from one of two distributions: f0 if the null hypothesis is true, and f1
otherwise. letting pr(zj = 0) =   0, marginally we have:

tj       0    f0 + (1       0)    f1.

(18.52)

then it can be shown (efron et al., 2001; storey, 2002) that

18.8 bibliographic notes

693

pfdr(  ) = pr(zj = 0|tj       ).

(18.53)

hence under the mixture model (18.51), the pfdr is the posterior proba-
bility that the null hypothesis it true, given that test statistic falls in the
rejection region for the test; that is, given that we reject the null hypothesis
(exercise 18.20).

the false discovery rate provides a measure of accuracy for tests based
on an entire rejection region, such as |tj|     2. but if the fdr of such a test
is say 10%, then a gene with say tj = 5 will be more signi   cant than a gene
with tj = 2. thus it is of interest to derive a local (gene-speci   c) version
of the fdr. the q-value (storey, 2003) of a test statistic tj is de   ned to
be the smallest fdr over all rejection regions that reject tj. that is, for
symmetric rejection regions, the q-value for tj = 2 is de   ned to be the
fdr for the rejection region    = {   (   ,   2)     (2,   )}. thus the q-value
for tj = 5 will be smaller than that for tj = 2, re   ecting the fact that tj = 5
is more signi   cant than tj = 2. the local false discovery rate (efron and
tibshirani, 2002) at t = t0 is de   ned to be

pr(zj = 0|tj = t0).

(18.54)

this is the (positive) fdr for an in   nitesimal rejection region surrounding
the value tj = t0.

18.8 bibliographic notes

many references were given at speci   c points in this chapter; we give some
additional ones here. dudoit et al. (2002a) give an overview and compar-
ison of discrimination methods for gene expression data. levina (2002)
does some mathematical analysis comparing diagonal lda to full lda, as
p, n         with p > n . she shows that with reasonable assumptions diago-
nal lda has a lower asymptotic error rate than full lda. tibshirani et al.
(2001a) and tibshirani et al. (2003) proposed the nearest shrunken-centroid
classi   er. zhu and hastie (2004) study regularized id28. high-
dimensional regression and the lasso are very active areas of research, and
many references are given in section 3.8.5. the fused lasso was proposed
by tibshirani et al. (2005), while zou and hastie (2005) introduced the
elastic net. supervised principal components is discussed in bair and tib-
shirani (2004) and bair et al. (2006). for an introduction to the analysis
of censored survival data, see kalb   eisch and prentice (1980).

microarray technology has led to a    urry of statistical research: see for
example the books by speed (2003), parmigiani et al. (2003), simon et al.
(2004), and lee (2004).

the false discovery rate was proposed by benjamini and hochberg (1995),
and studied and generalized in subsequent papers by these authors and

694

18. high-dimensional problems: p     n

many others. a partial list of papers on fdr may be found on yoav ben-
jamini   s homepage. some more recent papers include efron and tibshirani
(2002), storey (2002), genovese and wasserman (2004), storey and tib-
shirani (2003) and benjamini and yekutieli (2005). dudoit et al. (2002b)
review methods for identifying di   erentially expressed genes in microarray
studies.

exercises

ex. 18.1 for a coe   cient estimate     j, let     j/||     j||2 be the normalized ver-
sion. show that as           , the normalized ridge-regression estimates con-
verge to the renormalized partial-least-squares one-component estimates.

ex. 18.2 nearest shrunken centroids and the lasso. consider a (naive bayes)
gaussian model for classi   cation in which the features j = 1, 2, . . . , p are
assumed to be independent within each class k = 1, 2, . . . , k. with ob-
servations i = 1, 2, . . . , n and ck equal to the set of indices of the nk
observations in class k, we observe xij     n (  j +   jk,   2
j ) for i     ck with
j , the pooled within-class variance for feature j,

k=1   jk = 0. set     2

j = s2

and consider the lasso-style minimization problem

.         

1
2

pxj=1

kxk=1xi   ck

(xij       j       jk)2

s2
j

+   pnk

pxj=1

kxk=1

|  jk|
sj

(18.55)

show that the solution is equivalent to the nearest shrunken centroid es-
timator (18.7), with s0 set to zero, and m2
k equal to 1/nk instead of
1/nk     1/n as before.
ex. 18.3 show that the    tted coe   cients for the regularized multiclass
    kj = 0, j = 1, . . . , p.
what about the     k0? discuss issues with these constant parameters, and
how they can be resolved.

id28 problem (18.10) satisfy pk

k=1

ex. 18.4 derive the computational formula (18.15) for ridge regression.
[hint: use the    rst derivative of the penalized sum-of-squares criterion to
show that if    > 0, then      = xt s for some s     irn .]
ex. 18.5 prove the theorem (18.16)   (18.17) in section 18.3.5, by decom-
posing    and the rows of x into their projections into the column space of
v and its complement in irp.

ex. 18.6 show how the theorem in section 18.3.5 can be applied to regu-
larized discriminant analysis [equations 4.14 and (18.9)].

pk
{  j ,  jk}         

min

exercises

695

ex. 18.7 consider a id75 problem where p     n , and assume
the rank of x is n . let the svd of x = udvt = rvt , where r is
n    n nonsingular, and v is p    n with orthonormal columns.
(a) show that there are in   nitely many least-squares solutions all with

zero residuals.

(b) show that the ridge-regression estimate for    can be written

       = v(rt r +   i)   1rt y

(18.56)

(c) show that when    = 0, the solution     0 = vd   1ut y has residuals
all equal to zero, and is unique in that it has the smallest euclidean
norm amongst all zero-residual solutions.

ex. 18.8 data piling. exercise 4.2 shows that the two-class lda solution
can be obtained by a id75 of a binary response vector y con-
sisting of    1s and +1s. the prediction     t x for any x is (up to a scale and
shift) the lda score   (x). suppose now that p     n .
(a) consider the id75 model f (x) =    +   t x    t to a binary
response y     {   1, +1}. using exercise 18.7, show that there are
in   nitely many directions de   ned by      in irp onto which the data
project to exactly two points, one for each class. these are known as
data piling directions (ahn and marron, 2005).

(b) show that the distance between the projected points is 2/||     ||, and
hence these directions de   ne separating hyperplanes with that mar-
gin.

(c) argue that there is a single maximal data piling direction for which
this distance is largest, and is de   ned by     0 = vd   1ut y = x   y,
where x = udvt is the svd of x.

ex. 18.9 compare the data piling direction of exercise 18.8 to the direction
of the optimal separating hyperplane (section 4.5.2) qualitatively. which
makes the widest margin, and why? use a small simulation to demonstrate
the di   erence.

ex. 18.10 when p     n , id156 (see section 4.3) is
degenerate because the within-class covariance matrix w is singular. one
version of regularized discriminant analysis (4.14) replaces w by a ridged
version w +   i, leading to a regularized discriminant function     (x) =
xt (w +   i)   1(  x1       x   1). show that   0(x) = lim     0     (x) corresponds to
the maximal data piling direction de   ned in exercise 18.8.

ex. 18.11 suppose you have a sample of n pairs (xi, yi), with yi binary
and xi     ir1. suppose also that the two classes are separable; e.g., for each

696

18. high-dimensional problems: p     n

pair i, i    with yi = 0 and yi    = 1, xi        xi     c for some c > 0. you wish
to    t a linear id28 model logitpr(y = 1|x) =    +   x by
maximum-likelihood. show that      is unde   ned.

ex. 18.12 suppose we wish to select the ridge parameter    by 10-fold cross-
validation in a p     n situation (for any linear model). we wish to use the
computational shortcuts described in section 18.3.5. show that we need
only to reduce the n    p matrix x to the n    n matrix r once, and can
use it in all the cross-validation runs.

ex. 18.13 suppose our p > n predictors are presented as an n    n inner-
product matrix k = xxt , and we wish to    t the equivalent of a linear
id28 model in the original features with quadratic regulariza-
tion. our predictions are also to be made using inner products; a new x0
is presented as k0 = xx0. let k = ud2ut be the eigen-decomposition of
k. show that the predictions are given by   f0 = kt

0     , where

(a)      = ud   1     , and

(b)      is the ridged id28 estimate with input matrix r =

ud.

argue that the same approach can be used for any appropriate kernel
matrix k.

ex. 18.14 distance weighted 1-nn classi   cation. consider the 1-nearest-
neighbor method (section 13.3) in a two-class classi   cation problem. let
d+(x0) be the shortest distance to a training observation in class +1, and
likewise d   (x0) the shortest distance for class    1. let n    be the number
of samples in class    1, n+ the number in class +1, and n = n    + n+.
(a) show that

  (x0) = log

d   (x0)
d+(x0)

(18.57)

can be viewed as a nonparametric discriminant function correspond-
ing to 1-nn classi   cation. [hint: show that   f+(x0) =
n+d+(x0) can
be viewed as a nonparametric estimate of the density in class +1 at
x0].

1

(b) how would you modify this function to introduce class prior probabil-
ities   + and       di   erent from the sample-priors n+/n and n   /n ?

(c) how would you generalize this approach for id92 classi   cation?

ex. 18.15 kernel pca. in section 18.5.2 we show how to compute the
principal component variables z from an uncentered inner-product matrix
k. we compute the eigen-decomposition (i     m)k(i     m) = ud2ut ,
with m = 11t /n , and then z = ud. suppose we have the inner-product

exercises

697

vector k0, containing the n inner-products between a new point x0 and
each of the xi in our training set. show that the (centered) projections of
x0 onto the principal-component directions are given by
z0 = d   1ut (i     m) [k0     k1/n ] .

(18.58)

ex. 18.16 bonferroni method for multiple comparisons. suppose we are in
a multiple-testing scenario with null hypotheses h0j, j = 1, 2, . . . , m , and
corresponding p-values pj, i = 1, 2, . . . , m . let a be the event that at least
one null hypothesis is falsely rejected, and let aj be the event that the
jth null hypothesis is falsely rejected. suppose that we use the bonferroni
method, rejecting the jth null hypothesis if pj <   /m .

(a) show that pr(a)       . [hint: pr(aj     aj     ) = pr(aj) + pr(aj     )    

pr(aj     aj     )]

(b) if the hypotheses h0j, j = 1, 2, . . . , m , are independent, then pr(a) =
j ) = 1     (1       /m )m . use this to show

j=1 pr(ac

1     pr(ac) = 1    qm

that pr(a)        in this case.

ex. 18.17 equivalence between benjamini   hochberg and plug-in methods.

(a) in the notation of algorithm 18.2, show that for rejection threshold
p0 = p(l), a proportion of at most p0 of the permuted values tk
j
exceed |t|(l) where |t|(l) is the lth largest value among the |tj|.
hence show that the plug-in fdr estimate [fdr is less than or equal
to p0    m/l =   .

(b) show that the cut-point |t|(l+1) produces a test with estimated fdr

greater than   .

ex. 18.18 use result (18.53) to show that

pfdr =

  0    {type i error of   }

  0    {type i error of   } +   1{power of   }

(18.59)

(storey, 2003).

ex. 18.19 consider the data in table 18.4 of section (18.7), available from
the book website.

(a) using a symmetric two-sided rejection region based on the t-statistic,
compute the plug-in estimate of the fdr for various values of the
cut-point.

(b) carry out the bh procedure for various fdr levels    and show the

equivalence of your results, with those from part (a).

698

18. high-dimensional problems: p     n

(c) let (q.25, q.75) be the quartiles of the t-statistics from the permuted
datasets. let     0 = {#tj     (q.25, q.75)}/(.5m ), and set     0 = min(    0, 1).
multiply the fdr estimates from (a) by     0 and examine the results.

(d) give a motivation for the estimate in part (c).

(storey, 2003)

ex. 18.20 proof of result (18.53). write

r|r > 0!
pfdr = e  v
e" v
r|r = k#pr(r = k|r > 0)
mxk=1

=

(18.60)

(18.61)

use the fact that given r = k, v is a binomial random variable, with k
trials and id203 of success pr(h = 0|t       ), to complete the proof.

references

this is page 699
printer: opaque this

abu-mostafa, y. (1995). hints, neural computation 7: 639   671.

ackley, d. h., hinton, g. and sejnowski, t. (1985). a learning algorithm

for id82s, trends in cognitive sciences 9: 147   169.

adam, b.-l., qu, y., davis, j. w., ward, m. d., clements, m. a.,
cazares, l. h., semmes, o. j., schellhammer, p. f., yasui, y.,
feng, z. and wright, g. (2003). serum protein    ngerprinting cou-
pled with a pattern-matching algorithm distinguishes prostate cancer
from benign prostate hyperplasia and healthy mean, cancer research
63(10): 3609   3614.

agrawal, r., mannila, h., srikant, r., toivonen, h. and verkamo, a. i.
(1995). fast discovery of association rules, advances in knowledge
discovery and data mining, aaai/mit press, cambridge, ma.

agresti, a. (1996). an introduction to categorical data analysis, wiley,

new york.

agresti, a. (2002). categorical data analysis (2nd ed.), wiley, new york.

ahn, j. and marron, j. (2005). the direction of maximal data piling in high
dimensional space, technical report, statistics department, university
of north carolina, chapel hill.

akaike, h. (1973). id205 and an extension of the maximum
likelihood principle, second international symposium on information
theory, pp. 267   281.

700

references

allen, d. (1974). the relationship between variable selection and data
augmentation and a method of prediction, technometrics 16: 125   7.

ambroise, c. and mclachlan, g. (2002). selection bias in gene extraction
on the basis of microarray gene-expression data, proceedings of the
national academy of sciences 99: 6562   6566.

amit, y. and geman, d. (1997). shape quantization and recognition with

randomized trees, neural computation 9: 1545   1588.

anderson, j. and rosenfeld, e. (eds) (1988). neurocomputing: foundations

of research, mit press, cambridge, ma.

anderson, t. (2003). an introduction to multivariate statistical analysis,

3rd ed., wiley, new york.

bach, f. and jordan, m. (2002). kernel independent component analysis,

journal of machine learning research 3: 1   48.

bair, e. and tibshirani, r. (2004). semi-supervised methods to predict
patient survival from gene expression data, plos biology 2: 511   522.

bair, e., hastie, t., paul, d. and tibshirani, r. (2006). prediction by
supervised principal components, journal of the american statistical
association 101: 119   137.

bakin, s. (1999). adaptive regression and model selection in data mining
problems, technical report, phd. thesis, australian national univer-
sity, canberra.

banerjee, o., ghaoui, l. e. and d   aspremont, a. (2008). model selection
through sparse id113 for multivariate gaus-
sian or binary data, journal of machine learning research 9: 485   516.

barron, a. (1993). universal approximation bounds for superpositions of a
sigmoid function, ieee transactions on id205 39: 930   
945.

bartlett, p. and traskin, m.

in
b. sch  olkopf, j. platt and t. ho   man (eds), advances in neural infor-
mation processing systems 19, mit press, cambridge, ma, pp. 105   
112.

is consistent,

adaboost

(2007).

becker, r., cleveland, w. and shyu, m. (1996). the visual design and con-
trol of trellis display, journal of computational and graphical statis-
tics 5: 123   155.

bell, a. and sejnowski, t. (1995). an information-maximization approach
to blind separation and blind deconvolution, neural computation
7: 1129   1159.

references

701

bellman, r. e. (1961). adaptive control processes, princeton university

press.

benjamini, y. and hochberg, y. (1995). controlling the false discovery
rate: a practical and powerful approach to multiple testing, journal of
the royal statistical society series b. 85: 289   300.

benjamini, y. and yekutieli, y. (2005). false discovery rate controlling
con   dence intervals for selected parameters, journal of the american
statistical association 100: 71   80.

bickel, p. and levina, e. (2004). some theory for fisher   s linear discrim-
inant function,   naive bayes   , and some alternatives when there are
many more variables than observations, bernoulli 10: 989   1010.

bickel, p. j., ritov, y. and tsybakov, a. (2008). simultaneous analysis of

lasso and dantzig selector, annals of statistics. to appear.

bishop, c. (1995). neural networks for pattern recognition, clarendon

press, oxford.

bishop, c. (2006). pattern recognition and machine learning, springer,

new york.

bishop, y., fienberg, s. and holland, p. (1975). discrete multivariate

analysis, mit press, cambridge, ma.

boyd, s. and vandenberghe, l. (2004). id76, cambridge

university press.

breiman, l. (1992). the little bootstrap and other methods for dimension-
ality selection in regression: x-   xed prediction error, journal of the
american statistical association 87: 738   754.

breiman, l. (1996a). id112 predictors, machine learning 26: 123   140.

breiman, l. (1996b). stacked regressions, machine learning 24: 51   64.

breiman, l. (1998). arcing classi   ers (with discussion), annals of statistics

26: 801   849.

breiman, l. (1999). prediction games and arcing algorithms, neural com-

putation 11(7): 1493   1517.

breiman, l. (2001). id79s, machine learning 45: 5   32.

breiman, l. and friedman, j. (1997). predicting multivariate responses
in multiple id75 (with discussion), journal of the royal
statistical society series b. 59: 3   37.

702

references

breiman, l. and ihaka, r. (1984). nonid156 via
scaling and ace, technical report, university of california, berkeley.

breiman, l. and spector, p. (1992). submodel selection and evaluation
in regression: the x-random case, international statistical review
60: 291   319.

breiman, l., friedman, j., olshen, r. and stone, c. (1984). classi   cation

and regression trees, wadsworth, new york.

bremaud, p. (1999). markov chains: gibbs fields, monte carlo simula-

tion, and queues, springer, new york.

brown, p., spiegelman, c. and denham, m. (1991). chemometrics and
spectral frequency selection, transactions of the royal society of lon-
don series a. 337: 311   322.

bruce, a. and gao, h. (1996). applied wavelet analysis with s-plus,

springer, new york.

b  uhlmann, p. and hothorn, t. (2007). boosting algorithms: regulariza-
tion, prediction and model    tting (with discussion), statistical science
22(4): 477   505.

buja, a., hastie, t. and tibshirani, r. (1989). linear smoothers and

additive models (with discussion), annals of statistics 17: 453   555.

buja, a., swayne, d., littman, m., hofmann, h. and chen, l. (2008). data
vizualization with multidimensional scaling, journal of computational
and graphical statistics. to appear.

bunea, f., tsybakov, a. and wegkamp, m. (2007). sparsity oracle inequal-

ities for the lasso, electronic journal of statistics 1: 169   194.

burges, c. (1998). a tutorial on support vector machines for pattern recog-

nition, knowledge discovery and data mining 2(2): 121   167.

butte, a., tamayo, p., slonim, d., golub, t. and kohane, i. (2000).
discovering functional relationships between rna expression and
chemotherapeutic susceptibility using relevance networks, proceedings
of the national academy of sciences pp. 12182   12186.

candes, e. (2006). compressive sampling, proceedings of the interna-
tional congress of mathematicians, european mathematical society,
madrid, spain.

candes, e. and tao, t. (2007). the dantzig selector: statistical estimation
when p is much larger than n, annals of statistics 35(6): 2313   2351.

references

703

chambers, j. and hastie, t.

(1991).

statistical models

in s,

wadsworth/brooks cole, paci   c grove, ca.

chaudhuri, s., drton, m. and richardson, t. s. (2007). estimation of a

covariance matrix with zeros, biometrika 94(1): 1   18.

chen, l. and buja, a. (2008). local multidimensional scaling for nonlinear
dimension reduction, graph drawing and proximity analysis, journal
of the american statistical association.

chen, s. s., donoho, d. and saunders, m. (1998). atomic decomposition
by basis pursuit, siam journal on scienti   c computing 20(1): 33   61.

cherkassky, v. and ma, y. (2003). comparison of model selection for

regression, neural computation 15(7): 1691   1714.

cherkassky, v. and mulier, f. (2007). learning from data (2nd edition),

wiley, new york.

chui, c. (1992). an introduction to wavelets, academic press, london.

cli   ord, p. (1990). markov random    elds in statistics, in g. r. grimmett
and d. j. a. welsh (eds), disorder in physical systems. a volume in
honour of john m. hammersley, clarendon press, oxford, pp. 19   32.

comon, p. (1994). independent component analysis   a new concept?, sig-

nal processing 36: 287   314.

cook, d. and swayne, d. (2007). interactive and dynamic graphics for
data analysis; with r and ggobi, springer, new york. with con-
tributions from a. buja, d. temple lang, h. hofmann, h. wickham
and m. lawrence.

cook, n. (2007). use and misuse of the receiver operating characteristic

curve in risk prediction, circulation 116(6): 928   35.

copas, j. b. (1983). regression, prediction and shrinkage (with discus-
sion), journal of the royal statistical society, series b, methodological
45: 311   354.

cover, t. and hart, p. (1967). nearest neighbor pattern classi   cation,

ieee transactions on id205 it-11: 21   27.

cover, t. and thomas, j. (1991). elements of id205, wiley,

new york.

cox, d. and hinkley, d. (1974). theoretical statistics, chapman and hall,

london.

704

references

cox, d. and wermuth, n. (1996). multivariate dependencies: models,

analysis and interpretation, chapman and hall, london.

cressie, n. (1993). statistics for spatial data (revised edition), wiley-

interscience, new york.

csiszar, i. and tusn  ady, g. (1984). information geometry and alternat-
ing minimization procedures, statistics & decisions supplement issue
1: 205   237.

cutler, a. and breiman, l. (1994). archetypal analysis, technometrics

36(4): 338   347.

dasarathy, b. (1991). nearest neighbor pattern classi   cation techniques,

ieee computer society press, los alamitos, ca.

daubechies, i. (1992). ten lectures on wavelets, society for industrial and

applied mathematics, philadelphia, pa.

daubechies, i., defrise, m. and de mol, c. (2004). an iterative threshold-
ing algorithm for linear inverse problems with a sparsity constraint,
communications on pure and applied mathematics 57: 1413   1457.

de boor, c. (1978). a practical guide to splines, springer, new york.

dempster, a. (1972). covariance selection, biometrics 28: 157   175.

dempster, a., laird, n. and rubin, d. (1977). maximum likelihood from
incomplete data via the em algorithm (with discussion), journal of
the royal statistical society series b 39: 1   38.

devijver, p. and kittler, j. (1982). pattern recognition: a statistical ap-

proach, prentice-hall, englewood cli   s, n.j.

dietterich, t. (2000a). ensemble methods in machine learning, lecture

notes in computer science 1857: 1   15.

dietterich, t. (2000b). an experimental comparison of three methods for
constructing ensembles of id90: id112, boosting, and ran-
domization, machine learning 40(2): 139   157.

dietterich, t. and bakiri, g. (1995). solving multiclass learning problems
via error-correcting output codes, journal of arti   cial intelligence re-
search 2: 263   286.

donath, w. e. and ho   man, a. j. (1973). lower bounds for the partition-
ing of graphs, ibm journal of research and development pp. 420   425.

donoho, d. (2006a). compressed sensing, ieee transactions on informa-

tion theory 52(4): 1289   1306.

references

705

donoho, d. (2006b). for most large underdetermined systems of equations,
the minimal    1-norm solution is the sparsest solution, communications
on pure and applied mathematics 59: 797   829.

donoho, d. and elad, m. (2003). optimally sparse representation from
overcomplete dictionaries via    1-norm minimization, proceedings of
the national academy of sciences 100: 2197   2202.

donoho, d. and johnstone, i. (1994). ideal spatial adaptation by wavelet

shrinkage, biometrika 81: 425   455.

donoho, d. and stodden, v. (2004). when does non-negative matrix
factorization give a correct decomposition into parts?, in s. thrun,
l. saul and b. sch  olkopf (eds), advances in neural information pro-
cessing systems 16, mit press, cambridge, ma.

duan, n. and li, k.-c. (1991). slicing regression: a link-free regression

method, annals of statistics 19: 505   530.

duchamp, t. and stuetzle, w. (1996). extremal properties of principal

curves in the plane, annals of statistics 24: 1511   1520.

duda, r., hart, p. and stork, d. (2000). pattern classi   cation (2nd edi-

tion), wiley, new york.

dudoit, s., fridlyand, j. and speed, t. (2002a). comparison of discrimi-
nation methods for the classi   cation of tumors using gene expression
data, journal of the american statistical association 97(457): 77   87.

dudoit, s., yang, y., callow, m. and speed, t. (2002b). statistical meth-
ods for identifying di   erentially expressed genes in replicated cdna
microarray experiments, statistica sinica pp. 111   139.

edwards, d. (2000).

introduction to graphical modelling, 2nd edition,

springer, new york.

efron, b. (1975). the e   ciency of id28 compared to normal
discriminant analysis, journal of the american statistical association
70: 892   898.

efron, b. (1979). bootstrap methods: another look at the jackknife, annals

of statistics 7: 1   26.

efron, b. (1983). estimating the error rate of a prediction rule: some
improvements on cross-validation, journal of the american statistical
association 78: 316   331.

efron, b. (1986). how biased is the apparent error rate of a prediction

rule?, journal of the american statistical association 81: 461   70.

706

references

efron, b. and tibshirani, r. (1991). statistical analysis in the computer

age, science 253: 390   395.

efron, b. and tibshirani, r. (1993). an introduction to the bootstrap,

chapman and hall, london.

efron, b. and tibshirani, r. (1996). using specially designed exponential
families for density estimation, annals of statistics 24(6): 2431   2461.

efron, b. and tibshirani, r. (1997). improvements on cross-validation: the
632+ bootstrap: method, journal of the american statistical associ-
ation 92: 548   560.

efron, b. and tibshirani, r. (2002). microarrays, empirical bayes methods,

and false discovery rates, genetic epidemiology 1: 70   86.

efron, b., hastie, t. and tibshirani, r. (2007). discussion of    dantzig
selector    by candes and tao, annals of statistics 35(6): 2358   2364.

efron, b., hastie, t., johnstone, i. and tibshirani, r. (2004). least angle

regression (with discussion), annals of statistics 32(2): 407   499.

efron, b., tibshirani, r., storey, j. and tusher, v. (2001). empirical
bayes analysis of a microarray experiment, journal of the american
statistical association 96: 1151   1160.

evgeniou, t., pontil, m. and poggio, t. (2000). id173 networks
and support vector machines, advances in computational mathemat-
ics 13(1): 1   50.

fan, j. and fan, y. (2008). high dimensional classi   cation using features

annealed independence rules, annals of statistics. to appear.

fan, j. and gijbels, i. (1996). local polynomial modelling and its appli-

cations, chapman and hall, london.

fan, j. and li, r. (2005). variable selection via nonconcave penalized
likelihood and its oracle properties, journal of the american statistical
association 96: 1348   1360.

fiedler, m. (1973). algebraic connectivity of graphs, czechoslovak mathe-

matics journal 23(98): 298   305.

fienberg, s. (1977). the analysis of cross-classi   ed categorical data,

mit press, cambridge.

fisher, r. a. (1936). the use of multiple measurements in taxonomic

problems, eugen. 7: 179   188.

references

707

fisher, w. (1958). on grouping for maximum homogeniety, journal of the

american statistical association 53(284): 789   798.

fix, e. and hodges, j. (1951). discriminatory analysis   nonparametric
discrimination: consistency properties, technical report 21-49-004,4,
u.s. air force, school of aviation medicine, randolph field, tx.

flury, b. (1990). principal points, biometrika 77: 33   41.

forgy, e. (1965). cluster analysis of multivariate data: e   ciency vs. inter-

pretability of classi   cations, biometrics 21: 768   769.

frank, i. and friedman, j. (1993). a statistical view of some chemometrics

regression tools (with discussion), technometrics 35(2): 109   148.

freund, y. (1995). boosting a weak learning algorithm by majority, infor-

mation and computation 121(2): 256   285.

freund, y. and schapire, r. (1996a). experiments with a new boosting
algorithm, machine learning: proceedings of the thirteenth interna-
tional conference, morgan kau   man, san francisco, pp. 148   156.

freund, y. and schapire, r. (1996b). game theory, on-line prediction and
boosting, proceedings of the ninth annual conference on computa-
tional learning theory, desenzano del garda, italy, pp. 325   332.

freund, y. and schapire, r. (1997). a decision-theoretic generalization of
online learning and an application to boosting, journal of computer
and system sciences 55: 119   139.

friedman, j. (1987). exploratory projection pursuit, journal of the amer-

ican statistical association 82: 249   266.

friedman, j. (1989). regularized discriminant analysis, journal of the

american statistical association 84: 165   175.

friedman, j. (1991). multivariate adaptive regression splines (with discus-

sion), annals of statistics 19(1): 1   141.

friedman, j. (1994a). flexible metric nearest-neighbor classi   cation, tech-

nical report, stanford university.

friedman, j. (1994b). an overview of predictive learning and function
approximation, in v. cherkassky, j. friedman and h. wechsler (eds),
from statistics to neural networks, vol. 136 of nato isi series f,
springer, new york.

friedman, j. (1996). another approach to polychotomous classi   cation,

technical report, stanford university.

708

references

friedman, j. (1997). on bias, variance, 0-1 loss and the curse of dimen-
sionality, journal of data mining and knowledge discovery 1: 55   77.

friedman, j. (1999). stochastic gradient boosting, technical report, stan-

ford university.

friedman, j. (2001). greedy function approximation: a gradient boosting

machine, annals of statistics 29(5): 1189   1232.

friedman, j. and fisher, n. (1999). bump hunting in high dimensional

data, statistics and computing 9: 123   143.

friedman, j. and hall, p. (2007). on id112 and nonlinear estimation,

journal of statistical planning and id136 137: 669   683.

friedman, j. and popescu, b. (2003). importance sampled learning ensem-
bles, technical report, stanford university, department of statistics.

friedman, j. and popescu, b. (2008). predictive learning via rule ensem-

bles, annals of applied statistics, to appear.

friedman, j. and silverman, b. (1989). flexible parsimonious smoothing

and additive modelling (with discussion), technometrics 31: 3   39.

friedman, j. and stuetzle, w. (1981). projection pursuit regression, jour-

nal of the american statistical association 76: 817   823.

friedman, j. and tukey, j. (1974). a projection pursuit algorithm for
exploratory data analysis, ieee transactions on computers, series
c 23: 881   889.

friedman, j., baskett, f. and shustek, l. (1975). an algorithm for    nding
nearest neighbors, ieee transactions on computers 24: 1000   1006.

friedman, j., bentley, j. and finkel, r. (1977). an algorthm for    nd-
ing best matches in logarithmic expected time, acm transactions on
mathematical software 3: 209   226.

friedman, j., hastie, t. and tibshirani, r. (2000). additive logistic re-
gression: a statistical view of boosting (with discussion), annals of
statistics 28: 337   307.

friedman, j., hastie, t. and tibshirani, r. (2008a). response to    mease
and wyner: evidence contrary to the statistical view of boosting   ,
journal of machine learning research 9: 175   180.

friedman, j., hastie, t. and tibshirani, r. (2008b). sparse inverse covari-

ance estimation with the graphical lasso, biostatistics 9: 432   441.

references

709

friedman, j., hastie, t. and tibshirani, r. (2010). id173 paths for
generalized linear models via coordinate descent, journal of statistical
software 33(1): 1   22.

friedman, j., hastie, t., hoe   ing, h. and tibshirani, r. (2007). pathwise

coordinate optimization, annals of applied statistics 2(1): 302   332.

friedman, j., hastie, t., rosset, s., tibshirani, r. and zhu, j. (2004).
discussion of three boosting papers by jiang, lugosi and vayatis, and
zhang, annals of statistics 32: 102   107.

friedman, j., stuetzle, w. and schroeder, a. (1984). projection pursuit
density estimation, journal of the american statistical association
79: 599   608.

fu, w. (1998). penalized regressions: the bridge vs. the lasso, journal of

computational and graphical statistics 7(3): 397   416.

furnival, g. and wilson, r. (1974). regression by leaps and bounds, tech-

nometrics 16: 499   511.

gelfand, a. and smith, a. (1990). sampling based approaches to calculat-
ing marginal densities, journal of the american statistical association
85: 398   409.

gelman, a., carlin, j., stern, h. and rubin, d. (1995). bayesian data

analysis, crc press, boca raton, fl.

geman, s. and geman, d. (1984). stochastic relaxation, gibbs distribu-
tions and the bayesian restoration of images, ieee transactions on
pattern analysis and machine intelligence 6: 721   741.

genkin, a., lewis, d. and madigan, d. (2007). large-scale bayesian logis-
tic regression for text categorization, technometrics 49(3): 291   304.

genovese, c. and wasserman, l. (2004). a stochastic process approach to

false discovery rates, annals of statistics 32(3): 1035   1061.

gersho, a. and gray, r. (1992). vector quantization and signal compres-

sion, kluwer academic publishers, boston, ma.

girosi, f., jones, m. and poggio, t. (1995). id173 theory and

neural network architectures, neural computation 7: 219   269.

golub, g. and van loan, c. (1983). matrix computations, johns hopkins

university press, baltimore.

golub, g., heath, m. and wahba, g. (1979). generalized cross-validation
as a method for choosing a good ridge parameter, technometrics
21: 215   224.

710

references

golub, t., slonim, d., tamayo, p., huard, c., gaasenbeek, m., mesirov,
j., coller, h., loh, m., downing, j., caligiuri, m., bloom   eld, c. and
lander, e. (1999). molecular classi   cation of cancer: class discovery
and class prediction by gene expression monitoring, science 286: 531   
536.

goodall, c. (1991). procrustes methods in the statistical analysis of shape,

journal of the royal statistical society, series b 53: 285   321.

gordon, a. (1999). classi   cation (2nd edition), chapman and hall/crc

press, london.

green, p. and silverman, b. (1994). nonparametric regression and gener-
alized linear models: a roughness penalty approach, chapman and
hall, london.

greenacre, m. (1984). theory and applications of correspondence analy-

sis, academic press, new york.

greenshtein, e. and ritov, y. (2004). persistence in high-dimensional lin-
ear predictor selection and the virtue of overparametrization, bernoulli
10: 971   988.

guo, y., hastie, t. and tibshirani, r. (2006). regularized linear discrim-
inant analysis and its application in microarrays, biostatistics 8: 86   
100.

guyon, i., gunn, s., nikravesh, m. and zadeh, l. (eds) (2006). feature

extraction, foundations and applications, springer, new york.

guyon, i., weston, j., barnhill, s. and vapnik, v. (2002). gene selection for
cancer classi   cation using support vector machines, machine learning
46: 389   422.

hall, p. (1992). the bootstrap and edgeworth expansion, springer, new

york.

hammersley, j. m. and cli   ord, p. (1971). markov    eld on    nite graphs

and lattices, unpublished.

hand, d. (1981). discrimination and classi   cation, wiley, chichester.

hanley, j. and mcneil, b. (1982). the meaning and use of the area under
a receiver operating characteristic (roc) curve, radiology 143: 29   36.

hart, p. (1968). the condensed nearest-neighbor rule, ieee transactions

on id205 14: 515   516.

hartigan, j. a. (1975). id91 algorithms, wiley, new york.

references

711

hartigan, j. a. and wong, m. a. (1979). [(algorithm as 136] a id116
id91 algorithm (as r39: 81v30 p355-356), applied statistics
28: 100   108.

hastie, t. (1984). principal curves and surfaces, phd thesis, stanford

university.

hastie, t. and herman, a. (1990). an analysis of gestational age, neona-
tal size and neonatal death using nonparametric id28,
journal of clinical epidemiology 43: 1179   90.

hastie, t. and simard, p. (1998). models and metrics for handwritten digit

recognition, statistical science 13: 54   65.

hastie, t. and stuetzle, w. (1989). principal curves, journal of the amer-

ican statistical association 84(406): 502   516.

hastie, t. and tibshirani, r. (1987). nonparametric logistic and propor-

tional odds regression, applied statistics 36: 260   276.

hastie, t. and tibshirani, r. (1990). generalized additive models, chap-

man and hall, london.

hastie, t. and tibshirani, r. (1996a). discriminant adaptive nearest-
neighbor classi   cation, ieee pattern recognition and machine in-
telligence 18: 607   616.

hastie, t. and tibshirani, r. (1996b). discriminant analysis by gaussian
mixtures, journal of the royal statistical society series b. 58: 155   
176.

hastie, t. and tibshirani, r. (1998). classi   cation by pairwise coupling,

annals of statistics 26(2): 451   471.

hastie, t. and tibshirani, r. (2003).

independent components analysis
through product density estimation, in s. t. s. becker and k. ober-
mayer (eds), advances in neural information processing systems 15,
mit press, cambridge, ma, pp. 649   656.

hastie, t. and tibshirani, r. (2004). e   cient quadratic id173 for

expression arrays, biostatistics 5(3): 329   340.

hastie, t. and zhu, j. (2006). discussion of    support vector machines
with applications    by javier moguerza and alberto munoz, statistical
science 21(3): 352   357.

hastie, t., botha, j. and schnitzler, c. (1989). regression with an ordered

categorical response, statistics in medicine 43: 884   889.

712

references

hastie, t., buja, a. and tibshirani, r. (1995). penalized discriminant

analysis, annals of statistics 23: 73   102.

hastie, t., kishon, e., clark, m. and fan, j. (1992). a model for
signature veri   cation, technical report, at&t bell laboratories.
http://www-stat.stanford.edu/   hastie/papers/signature.pdf.

hastie, t., rosset, s., tibshirani, r. and zhu, j. (2004). the entire reg-
ularization path for the support vector machine, journal of machine
learning research 5: 1391   1415.

hastie, t., taylor, j., tibshirani, r. and walther, g. (2007). forward
stagewise regression and the monotone lasso, electronic journal of
statistics 1: 1   29.

hastie, t., tibshirani, r. and buja, a. (1994). flexible discriminant analy-
sis by optimal scoring, journal of the american statistical association
89: 1255   1270.

hastie, t., tibshirani, r. and buja, a. (2000). flexible discriminant and
mixture models, in j. kay and m. titterington (eds), statistics and
arti   cial neural networks, oxford university press.

hastie, t., tibshirani, r. and friedman, j. (2003). a note on    compari-
son of model selection for regression    by cherkassky and ma, neural
computation 15(7): 1477   1480.

hathaway, r. j. (1986). another interpretation of the em algorithm for

mixture distributions, statistics & id203 letters 4: 53   56.

hebb, d. (1949). the organization of behavior, wiley, new york.

hertz, j., krogh, a. and palmer, r. (1991). introduction to the theory of

neural computation, addison wesley, redwood city, ca.

hinton, g. (1989). connectionist learning procedures, arti   cial intelli-

gence 40: 185   234.

hinton, g. (2002). training products of experts by minimizing contrastive

divergence, neural computation 14: 1771   1800.

hinton, g., osindero, s. and teh, y.-w. (2006). a fast learning algorithm

for deep belief nets, neural computation 18: 1527   1554.

ho, t. k. (1995). random decision forests, in m. kavavaugh and p. storms
(eds), proc. third international conference on document analysis
and recognition, vol. 1, ieee computer society press, new york,
pp. 278   282.

references

713

hoe   ing, h. and tibshirani, r. (2008). estimation of sparse markov net-

works using modi   ed id28 and the lasso, submitted.

hoerl, a. e. and kennard, r. (1970). ridge regression: biased estimation

for nonorthogonal problems, technometrics 12: 55   67.

hothorn, t. and b  uhlmann, p. (2006). model-based boosting in high di-

mensions, bioinformatics 22(22): 2828   2829.

huber, p. (1964). robust estimation of a location parameter, annals of

mathematical statistics 53: 73   101.

huber, p. (1985). projection pursuit, annals of statistics 13: 435   475.

hunter, d. and lange, k. (2004). a tutorial on mm algorithms, the

american statistician 58(1): 30   37.

hyv  arinen, a. and oja, e. (2000). independent component analysis: algo-

rithms and applications, neural networks 13: 411   430.

hyv  arinen, a., karhunen, j. and oja, e. (2001). independent component

analysis, wiley, new york.

izenman, a. (1975). reduced-rank regression for the multivariate linear

model, journal of multivariate analysis 5: 248   264.

jacobs, r., jordan, m., nowlan, s. and hinton, g. (1991). adaptive mix-

tures of local experts, neural computation 3: 79   87.

jain, a. and dubes, r. (1988). algorithms for id91 data, prentice-

hall, englewood cli   s, n.j.

james, g. and hastie, t. (1998). the error coding method and picts,

journal of computational and graphical statistics 7(3): 377   387.

jancey, r. (1966). multidimensional group analysis, australian journal of

botany 14: 127   130.

jensen, f. v., lauritzen, s. and olesen, k. g. (1990). bayesian updating
in recursive id114 by local computation, computational
statistics quarterly 4: 269   282.

jiang, w. (2004). process consistency for adaboost, annals of statistics

32(1): 13   29.

jirou  sek, r. and p  reu  cil, s. (1995). on the e   ective implementation of the
iterative proportional    tting procedure, computational statistics and
data analysis 19: 177   189.

johnson, n. (2008). a study of the nips feature selection challenge, sub-

mitted.

714

references

joli   e, i. t., trenda   lov, n. t. and uddin, m. (2003). a modi   ed principal
component technique based on the lasso, journal of computational
and graphical statistics 12: 531   547.

jones, l. (1992). a simple lemma on greedy approximation in hilbert space
and convergence rates for projection pursuit regression and neural
network training, annals of statistics 20: 608   613.

jordan, m. (2004). id114, statistical science (special issue on

bayesian statistics) 19: 140   155.

jordan, m. and jacobs, r. (1994). hierachical mixtures of experts and the

em algorithm, neural computation 6: 181   214.

kalb   eisch, j. and prentice, r. (1980). the statistical analysis of failure

time data, wiley, new york.

kaufman, l. and rousseeuw, p. (1990). finding groups in data: an in-

troduction to cluster analysis, wiley, new york.

kearns, m. and vazirani, u. (1994). an introduction to computational

learning theory, mit press, cambridge, ma.

kittler, j., hatef, m., duin, r. and matas, j. (1998). on combining classi-
   ers, ieee transaction on pattern analysis and machine intelligence
20(3): 226   239.

kleinberg, e. m. (1990). stochastic discrimination, annals of mathematical

arti   cial intelligence 1: 207   239.

kleinberg, e. m. (1996). an overtraining-resistant stochastic modeling

method for pattern recognition, annals of statistics 24: 2319   2349.

knight, k. and fu, w. (2000). asymptotics for lasso-type estimators,

annals of statistics 28(5): 1356   1378.

koh, k., kim, s.-j. and boyd, s. (2007). an interior-point method
for large-scale l1-regularized id28, journal of machine
learning research 8: 1519   1555.

kohavi, r. (1995). a study of cross-validation and bootstrap for accu-
racy estimation and model selection, international joint conference
on arti   cial intelligence (ijcai), morgan kaufmann, pp. 1137   1143.

kohonen, t. (1989). self-organization and associative memory (3rd edi-

tion), springer, berlin.

kohonen, t. (1990). the self-organizing map, proceedings of the ieee

78: 1464   1479.

references

715

kohonen, t., kaski, s., lagus, k., saloj  arvi, j., paatero, a. and saarela,
a. (2000). self-organization of a massive document collection, ieee
transactions on neural networks 11(3): 574   585. special issue on
neural networks for data mining and knowledge discovery.

koller, d. and friedman, n. (2007). structured probabilistic models, stan-

ford bookstore custom publishing. (unpublished draft).

kressel, u. (1999). pairwise classi   cation and support vector machines,
in b. sch  olkopf, c. burges and a. smola (eds), advances in ker-
nel methods - support vector learning, mit press, cambridge, ma.,
pp. 255   268.

lambert, d. (1992). zero-in   ated poisson regression, with an application

to defects in manufacturing, technometrics 34(1): 1   14.

lange, k. (2004). optimization, springer, new york.

lauritzen, s. (1996). id114, oxford university press.

lauritzen, s. and spiegelhalter, d. (1988). local computations with proba-
bilities on graphical structures and their application to id109,
j. royal statistical society b. 50: 157   224.

lawson, c. and hansen, r. (1974).

solving least squares problems,

prentice-hall, englewood cli   s, nj.

le cun, y. (1989). generalization and network design strategies, techni-
cal report crg-tr-89-4, department of computer science, univ. of
toronto.

le cun, y., boser, b., denker, j., henderson, d., howard, r., hubbard,
w. and jackel, l. (1990). handwritten digit recognition with a back-
propogation network, in d. touretzky (ed.), advances in neural in-
formation processing systems, vol. 2, morgan kaufman, denver, co,
pp. 386   404.

le cun, y., bottou, l., bengio, y. and ha   ner, p. (1998). gradient-based
learning applied to document recognition, proceedings of the ieee
86(11): 2278   2324.

leathwick, j., elith, j., francis, m., hastie, t. and taylor, p. (2006). vari-
ation in demersal    sh species richness in the oceans surrounding new
zealand: an analysis using boosted regression trees, marine ecology
progress series 77: 802   813.

leathwick, j., rowe, d., richardson, j., elith, j. and hastie, t. (2005).
using multivariate adaptive regression splines to predict the distribu-
tions of new zealand   s freshwater diadromous    sh, freshwater biology
50: 2034   2051.

716

references

leblanc, m. and tibshirani, r. (1996). combining estimates in regres-
sion and classi   cation, journal of the american statistical association
91: 1641   1650.

lecun, y., bottou, l., bengio, y. and ha   ner, p. (1998). gradient-based
learning applied to document recognition, proceedings of the ieee
86(11): 2278   2324.

lee, d. and seung, h. (1999). learning the parts of objects by non-negative

id105, nature 401: 788.

lee, d. and seung, h. (2001). algorithms for non-negative matrix factor-
ization, advances in neural information processing systems, (nips
2001), vol. 13, morgan kaufman, denver., pp. 556   562.

lee, m.-l. (2004). analysis of microarray gene expression data, kluwer

academic publishers.

lee, s.-i., ganapathi, v. and koller, d. (2007). e   cient structure learning
of markov networks using l1-id173, in b. sch  olkopf, j. platt
and t. ho   man (eds), advances in neural information processing
systems 19, mit press, cambridge, ma, pp. 817   824.

leslie, c., eskin, e., cohen, a., weston, j. and noble, w. s. (2004). mis-
match string kernels for discriminative protein classi   cation, bioinfor-
matics 20(4): 467   476.

levina, e. (2002). statistical issues in texture analysis, phd thesis, de-

partment. of statistics, university of california, berkeley.

lin, h., mcculloch, c., turnbull, b., slate, e. and clark, l. (2000). a
latent class mixed model for analyzing biomarker trajectories in lon-
gitudinal data with irregularly scheduled observations, statistics in
medicine 19: 1303   1318.

lin, y. and zhang, h. (2006). component selection and smoothing in
smoothing spline analysis of variance models, annals of statistics
34: 2272   2297.

little, r. and rubin, d. (2002). statistical analysis with missing data

(2nd edition), wiley, new york.

lloyd, s. (1957). least squares quantization in pcm., technical report, bell
laboratories. published in 1982 in ieee transactions on information
theory 28 128-137.

loader, c. (1999). local regression and likelihood, springer, new york.

references

717

loh, w. and vanichsetakul, n. (1988). tree structured classi   cation via
generalized discriminant analysis, journal of the american statistical
association 83: 715   728.

lugosi, g. and vayatis, n. (2004). on the bayes-risk consistency of regu-

larized boosting methods, annals of statistics 32(1): 30   55.

macnaughton smith, p., williams, w., dale, m. and mockett, l. (1965).
dissimilarity analysis: a new technique of hierarchical subdivision, na-
ture 202: 1034   1035.

mackay, d. (1992). a practical bayesian framework for id26

neural networks, neural computation 4: 448   472.

macqueen, j. (1967). some methods for classi   cation and analysis of mul-
tivariate observations, proceedings of the fifth berkeley symposium
on mathematical statistics and id203, eds. l.m. lecam and j.
neyman, university of california press, pp. 281   297.

madigan, d. and raftery, a. (1994). model selection and accounting for
model uncertainty using occam   s window, journal of the american
statistical association 89: 1535   46.

mardia, k., kent, j. and bibby, j. (1979). multivariate analysis, academic

press.

mason, l., baxter, j., bartlett, p. and frean, m. (2000). boosting algo-

rithms as id119, 12: 512   518.

massart, d., plastria, f. and kaufman, l. (1983). non-hierarchical clus-
tering with masloc, the journal of the pattern recognition society
16: 507   516.

mccullagh, p. and nelder, j. (1989). generalized linear models, chapman

and hall, london.

mcculloch, w. and pitts, w. (1943). a logical calculus of the ideas immi-
nent in nervous activity, bulletin of mathematical biophysics 5: 115   
133. reprinted in anderson and rosenfeld (1988), pp 96-104.

mclachlan, g. (1992). discriminant analysis and statistical pattern

recognition, wiley, new york.

mease, d. and wyner, a. (2008). evidence contrary to the statistical view
of boosting (with discussion), journal of machine learning research
9: 131   156.

meinshausen, n. (2007). relaxed lasso, computational statistics and data

analysis 52(1): 374   393.

718

references

meinshausen, n. and b  uhlmann, p. (2006). high-dimensional graphs and
variable selection with the lasso, annals of statistics 34: 1436   1462.

meir, r. and r  atsch, g. (2003). an introduction to boosting and leverag-
ing, in s. mendelson and a. smola (eds), lecture notes in computer
science, advanced lectures in machine learning, springer, new york.

michie, d., spiegelhalter, d. and taylor, c. (eds) (1994). machine learn-
ing, neural and statistical classi   cation, ellis horwood series in ar-
ti   cial intelligence, ellis horwood.

morgan, j. n. and sonquist, j. a. (1963). problems in the analysis of survey
data, and a proposal, journal of the american statistical association
58: 415   434.

murray, w., gill, p. and wright, m. (1981). practical optimization, aca-

demic press.

myles, j. and hand, d. (1990). the multiclass metric problem in nearest

neighbor classi   cation, pattern recognition 23: 1291   1297.

nadler, b. and coifman, r. r. (2005). an exact asymptotic formula for the
error in cls and in pls: the importance of dimensional reduction in
multivariate calibration, journal of chemometrics 102: 107   118.

neal, r. (1996). bayesian learning for neural networks, springer, new

york.

neal, r. and hinton, g. (1998). a view of the em algorithm that justi   es
incremental, sparse, and other variants; in learning in graphical mod-
els, m. jordan (ed.), dordrecht: kluwer academic publishers, boston,
ma., pp. 355   368.

neal, r. and zhang, j. (2006). high dimensional classi   cation with
bayesian neural networks and dirichlet di   usion trees, in i. guyon,
s. gunn, m. nikravesh and l. zadeh (eds), feature extraction, foun-
dations and applications, springer, new york, pp. 265   296.

onton, j. and makeig, s. (2006).

information-based modeling of event-
related brain dynamics, in neuper and klimesch (eds), progress in
brain research, vol. 159, elsevier, pp. 99   120.

osborne, m., presnell, b. and turlach, b. (2000a). a new approach to
variable selection in least squares problems, ima journal of numerical
analysis 20: 389   404.

osborne, m., presnell, b. and turlach, b. (2000b). on the lasso and its
dual, journal of computational and graphical statistics 9: 319   337.

pace, r. k. and barry, r. (1997). sparse spatial autoregressions, statistics

and id203 letters 33: 291   297.

references

719

page, l., brin, s., motwani, r. and winograd, t. (1998).

the
to the web, tech-
report, stanford digital library technologies project.

id95 citation ranking: bringing order
nical
http://citeseer.ist.psu.edu/page98id95.html.

park, m. y. and hastie, t. (2007). l1-id173 path algorithm for gen-
eralized linear models, journal of the royal statistical society series
b 69: 659   677.

parker, d. (1985). learning logic, technical report tr-87, cambridge ma:
mit center for research in computational economics and manage-
ment science.

parmigiani, g., garett, e. s., irizarry, r. a. and zeger, s. l. (eds) (2003).

the analysis of gene expression data, springer, new york.

paul, d., bair, e., hastie, t. and tibshirani, r. (2008).    pre-conditioning   
for feature selection and regression in high-dimensional problems, an-
nals of statistics 36(4): 1595   1618.

pearl, j. (1986). on evidential reasoning in a hierarchy of hypotheses,

arti   cial intelligence 28: 9   15.

pearl, j. (1988). probabilistic reasoning in intelligent systems: networks of

plausible id136, morgan kaufmann, san francisco, ca.

pearl, j. (2000). causality: models, reasoning and id136, cambridge

university press.

peterson and anderson, j. r. (1987). a mean    eld theory learning algo-

rithm for neural networks, complex systems 1: 995   1019.

petricoin, e. f., ardekani, a. m., hitt, b. a., levine, p. j., fusaro, v.,
steinberg, s. m., mills, g. b., simone, c., fishman, d. a., kohn,
e. and liotta, l. a. (2002). use of proteomic patterns in serum to
identify ovarian cancer, lancet 359: 572   577.

platt, j. (1999). fast training of support vector machines using sequen-
tial minimal optimization; in advances in kernel methods   support
vector learning, b. sch  olkopf and c. j. c. burges and a. j. smola
(eds), mit press, cambridge, ma., pp. 185   208.

quinlan, r. (1993). c4.5: programs for machine learning, morgan kauf-

mann, san mateo.

quinlan, r. (2004). c5.0, www.rulequest.com.

720

references

ramaswamy, s., tamayo, p., rifkin, r., mukherjee, s., yeang, c., angelo,
m., ladd, c., reich, m., latulippe, e., mesirov, j., poggio, t., gerald,
w., loda, m., lander, e. and golub, t. (2001). multiclass cancer
diagnosis using tumor gene expression signature, pnas 98: 15149   
15154.

ramsay, j. and silverman, b. (1997). functional data analysis, springer,

new york.

rao, c. r. (1973). linear statistical id136 and its applications, wiley,

new york.

r  atsch, g. and warmuth, m. (2002). maximizing the margin with boost-
ing, proceedings of the 15th annual conference on computational
learning theory, pp. 334   350.

ravikumar, p., liu, h., la   erty, j. and wasserman, l. (2008). spam:
sparse additive models, in j. platt, d. koller, y. singer and s. roweis
(eds), advances in neural information processing systems 20, mit
press, cambridge, ma, pp. 1201   1208.

ridgeway, g. (1999). the state of boosting, computing science and statis-

tics 31: 172   181.

rieger, k., hong, w., tusher, v., tang, j., tibshirani, r. and chu, g.
(2004). toxicity from radiation therapy associated with abnormal
transcriptional responses to dna damage, proceedings of the national
academy of sciences 101: 6634   6640.

ripley, b. d. (1996). pattern recognition and neural networks, cambridge

university press.

rissanen, j. (1983). a universal prior for integers and estimation by mini-

mum description length, annals of statistics 11: 416   431.

robbins, h. and munro, s. (1951). a stochastic approximation method,

annals of mathematical statistics 22: 400   407.

roosen, c. and hastie, t. (1994). automatic smoothing spline projection
pursuit, journal of computational and graphical statistics 3: 235   248.

rosenblatt, f. (1958). the id88: a probabilistic model for infor-
mation storage and organization in the brain, psychological review
65: 386   408.

rosenblatt, f. (1962). principles of neurodynamics: id88s and the

theory of brain mechanisms, spartan, washington, d.c.

references

721

rosenwald, a., wright, g., chan, w. c., connors, j. m., campo, e.,
fisher, r. i., gascoyne, r. d., muller-hermelink, h. k., smeland,
e. b. and staudt, l. m. (2002). the use of molecular pro   ling to
predict survival after chemotherapy for di   use large b-cell lymphoma,
the new england journal of medicine 346: 1937   1947.

rosset, s. and zhu, j. (2007). piecewise linear regularized solution paths,

annals of statistics 35(3): 1012   1030.

rosset, s., zhu, j. and hastie, t. (2004a). boosting as a regularized path to
a maximum margin classi   er, journal of machine learning research
5: 941   973.

rosset, s., zhu, j. and hastie, t. (2004b). margin maximizing loss func-
tions, in s. thrun, l. saul and b. sch  olkopf (eds), advances in neural
information processing systems 16, mit press, cambridge, ma.

rousseauw, j., du plessis, j., benade, a., jordaan, p., kotze, j., jooste, p.
and ferreira, j. (1983). coronary risk factor screening in three rural
communities, south african medical journal 64: 430   436.

roweis, s. t. and saul, l. k. (2000). locally linear embedding, science

290: 2323   2326.

rumelhart, d., hinton, g. and williams, r. (1986). learning internal rep-
resentations by error propagation, in d. rumelhart and j. mcclelland
(eds), parallel distributed processing: explorations in the microstruc-
ture of cognition, the mit press, cambridge, ma., pp. 318   362.

sachs, k., perez, o., pe   er, d., lau   enburger, d. and nolan, g. (2005).
causal protein-signaling networks derived from multiparameter single-
cell data, science 308: 523   529.

schapire, r. (1990). the strength of weak learnability, machine learning

5(2): 197   227.

schapire, r. (2002). the boosting approach to machine learning: an
overview, in d. denison, m. hansen, c. holmes, b. mallick and b. yu
(eds), msri workshop on nonlinear estimation and classi   cation,
springer, new york.

schapire, r. and singer, y. (1999). improved boosting algorithms using

con   dence-rated predictions, machine learning 37(3): 297   336.

schapire, r., freund, y., bartlett, p. and lee, w. (1998). boosting the
margin: a new explanation for the e   ectiveness of voting methods,
annals of statistics 26(5): 1651   1686.

722

references

sch  olkopf, b., smola, a. and m  uller, k.-r. (1999). kernel principal compo-
nent analysis, in b. sch  olkopf, c. burges and a. smola (eds), advances
in kernel methods   support vector learning, mit press, cambridge,
ma, usa, pp. 327   352.

schwarz, g. (1978). estimating the dimension of a model, annals of statis-

tics 6(2): 461   464.

scott, d. (1992). multivariate density estimation: theory, practice, and

visualization, wiley, new york.

seber, g. (1984). multivariate observations, wiley, new york.

segal, m. (2004). machine learning benchmarks and id79 regres-
sion, technical report, escholarship repository, university of califor-
nia. http://repositories.edlib.org/cbmb/bench rf regn.

shao, j. (1996). bootstrap model selection, journal of the american sta-

tistical association 91: 655   665.

shenoy, p. and shafer, g. (1988). an axiomatic framework for bayesian
and belief-function propagation, aaai workshop on uncertainty in
ai, north-holland, pp. 307   314.

short, r. and fukunaga, k. (1981). the optimal distance measure for near-
est neighbor classi   cation, ieee transactions on id205
27: 622   627.

silverman, b. (1986). density estimation for statistics and data analysis,

chapman and hall, london.

silvey, s. (1975). statistical id136, chapman and hall, london.

simard, p., cun, y. l. and denker, j. (1993). e   cient pattern recognition
using a new transformation distance, advances in neural information
processing systems, morgan kaufman, san mateo, ca, pp. 50   58.

simon, r. m., korn, e. l., mcshane, l. m., radmacher, m. d., wright,
g. and zhao, y. (2004). design and analysis of dna microarray
investigations, springer, new york.

sj  ostrand, k., rostrup, e., ryberg, c., larsen, r., studholme, c., baezner,
h., ferro, j., fazekas, f., pantoni, l., inzitari, d. and waldemar,
g. (2007). sparse decomposition and modeling of anatomical shape
variation, ieee transactions on medical imaging 26(12): 1625   1635.

speed, t. and kiiveri, h. t. (1986). gaussian markov distributions over

   nite graphs, annals of statistics 14: 138   150.

references

723

speed, t. (ed.) (2003). statistical analysis of gene expression microarray

data, chapman and hall, london.

spiegelhalter, d., best, n., gilks, w. and inskip, h. (1996). hepatitis
b: a case study in mcmc methods, in w. gilks, s. richardson and
d. spegelhalter (eds), id115 in practice, inter-
disciplinary statistics, chapman and hall, london, pp. 21   43.

spielman, d. a. and teng, s.-h. (1996). spectral partitioning works: pla-
nar graphs and    nite element meshes, ieee symposium on founda-
tions of computer science, pp. 96   105.

stamey, t., kabalin, j., mcneal, j., johnstone, i., freiha, f., redwine, e.
and yang, n. (1989). prostate speci   c antigen in the diagnosis and
treatment of adenocarcinoma of the prostate ii radical prostatectomy
treated patients, journal of urology 16: 1076   1083.

stone, c., hansen, m., kooperberg, c. and truong, y. (1997). polynomial
splines and their tensor products (with discussion), annals of statistics
25(4): 1371   1470.

stone, m. (1974). cross-validatory choice and assessment of statistical
predictions, journal of the royal statistical society series b 36: 111   
147.

stone, m. (1977). an asymptotic equivalence of choice of model by cross-
validation and akaike   s criterion, journal of the royal statistical so-
ciety series b. 39: 44   7.

stone, m. and brooks, r. j. (1990). continuum regression: cross-validated
sequentially constructed prediction embracing ordinary least squares,
partial least squares and principal components regression (corr: v54
p906-907), journal of the royal statistical society, series b 52: 237   
269.

storey, j. (2002). a direct approach to false discovery rates, journal of the

royal statistical society b. 64(3): 479   498.

storey, j. (2003). the positive false discovery rate: a bayesian interpreta-

tion and the q-value, annals of statistics 31: 2013   2025.

storey, j. and tibshirani, r. (2003). statistical signi   cance for genomewide
studies, proceedings of the national academy of sciences 100-: 9440   
9445.

storey, j., taylor, j. and siegmund, d. (2004). strong control, conservative
point estimation, and simultaneous conservative consistency of false
discovery rates: a uni   ed approach., journal of the royal statistical
society, series b 66: 187   205.

724

references

surowiecki, j. (2004). the wisdom of crowds: why the many are smarter
than the few and how collective wisdom shapes business, eco-
nomics, societies and nations., little, brown.

swayne, d., cook, d. and buja, a. (1991). xgobi: interactive dynamic
graphics in the x window system with a link to s, asa proceedings
of section on statistical graphics, pp. 1   8.

tanner, m. and wong, w. (1987). the calculation of posterior distribu-
tions by data augmentation (with discussion), journal of the american
statistical association 82: 528   550.

tarpey, t. and flury, b. (1996). self-consistency: a fundamental concept

in statistics, statistical science 11: 229   243.

tenenbaum, j. b., de silva, v. and langford, j. c. (2000). a global
geometric framework for nonlinear id84, science
290: 2319   2323.

tibshirani, r. (1996). regression shrinkage and selection via the lasso,

journal of the royal statistical society, series b 58: 267   288.

tibshirani, r. and hastie, t. (2007). margin trees for high-dimensional

classi   cation, journal of machine learning research 8: 637   652.

tibshirani, r. and knight, k. (1999). model search and id136 by boot-
strap    bumping, journal of computational and graphical statistics
8: 671   686.

tibshirani, r. and wang, p. (2007). spatial smoothing and hot spot de-

tection for cgh data using the fused lasso, biostatistics 9: 18   29.

tibshirani, r., hastie, t., narasimhan, b. and chu, g. (2001a). diagnosis
of multiple cancer types by shrunken centroids of gene expression,
proceedings of the national academy of sciences 99: 6567   6572.

tibshirani, r., hastie, t., narasimhan, b. and chu, g. (2003). class
prediction by nearest shrunken centroids, with applications to dna
microarrays, statistical science 18(1): 104   117.

tibshirani, r., saunders, m., rosset, s., zhu, j. and knight, k. (2005).
sparsity and smoothness via the fused lasso, journal of the royal
statistical society, series b 67: 91   108.

tibshirani, r., walther, g. and hastie, t. (2001b). estimating the number
of clusters in a dataset via the gap statistic, journal of the royal
statistical society, series b. 32(2): 411   423.

tropp, j. (2004). greed is good: algorithmic results for sparse approxima-

tion, ieee transactions on id205 50: 2231    2242.

references

725

tropp, j. (2006). just relax: convex programming methods for identify-
ing sparse signals in noise, ieee transactions on id205
52: 1030   1051.

valiant, l. g. (1984). a theory of the learnable, communications of the

acm 27: 1134   1142.

van der merwe, a. and zidek, j. (1980). multivariate regression analysis
and canonical variates, the canadian journal of statistics 8: 27   39.

vapnik, v. (1996). the nature of statistical learning theory, springer,

new york.

vapnik, v. (1998). statistical learning theory, wiley, new york.

vidakovic, b. (1999). statistical modeling by wavelets, wiley, new york.

von luxburg, u. (2007). a tutorial on spectral id91, statistics and

computing 17(4): 395   416.

wahba, g. (1980). spline bases, id173, and generalized cross-
validation for solving approximation problems with large quantities
of noisy data, proceedings of the international conference on approx-
imation theory in honour of george lorenz, academic press, austin,
texas, pp. 905   912.

wahba, g. (1990). spline models for observational data, siam, philadel-

phia.

wahba, g., lin, y. and zhang, h. (2000). gacv for support vector ma-
chines, in a. smola, p. bartlett, b. sch  olkopf and d. schuurmans
(eds), advances in large margin classi   ers, mit press, cambridge,
ma., pp. 297   311.

wainwright, m. (2006). sharp thresholds for noisy and high-dimensional
recovery of sparsity using    1-constrained quadratic programming,
technical report, department of statistics, university of california,
berkeley.

wainwright, m. j., ravikumar, p. and la   erty, j. d. (2007). high-
dimensional graphical model selection using    1-regularized logistic re-
gression, in b. sch  olkopf, j. platt and t. ho   man (eds), advances
in neural information processing systems 19, mit press, cambridge,
ma, pp. 1465   1472.

wasserman, l. (2004). all of statistics: a concise course in statistical

id136, springer, new york.

weisberg, s. (1980). applied id75, wiley, new york.

726

references

werbos, p. (1974). beyond regression, phd thesis, harvard university.

weston, j. and watkins, c. (1999). multiclass support vector machines, in
m. verleysen (ed.), proceedings of esann99, d. facto press, brussels.

whittaker, j. (1990). id114 in applied multivariate statistics,

wiley, chichester.

wickerhauser, m. (1994). adapted wavelet analysis from theory to soft-

ware, a.k. peters ltd, natick, ma.

widrow, b. and ho   , m. (1960). adaptive switching circuits, ire
wescon convention record, vol. 4. pp 96-104; reprinted in an-
dersen and rosenfeld (1988).

wold, h. (1975). soft modelling by latent variables: the nonlinear iterative
partial least squares (nipals) approach, perspectives in id203
and statistics, in honor of m. s. bartlett, pp. 117   144.

wolpert, d. (1992). stacked generalization, neural networks 5: 241   259.

wu, t. and lange, k. (2007). the mm alternative to em, unpublished.

wu, t. and lange, k. (2008). coordinate descent procedures for lasso

penalized regression, annals of applied statistics 2(1): 224   244.

yee, t. and wild, c. (1996). vector generalized additive models, journal

of the royal statistical society, series b. 58: 481   493.

yuan, m. and lin, y. (2007). model selection and estimation in regression
with grouped variables, journal of the royal statistical society, series
b 68(1): 49   67.

zhang, p. (1993). model selection via multifold cross-validation, annals of

statistics 21: 299   311.

zhang, t. and yu, b. (2005). boosting with early stopping: convergence

and consistency, annals of statistics 33: 1538   1579.

zhao, p. and yu, b. (2006). on model selection consistency of lasso, jour-

nal of machine learning research 7: 2541   2563.

zhao, p., rocha, g. and yu, b. (2008). the composite absolute penalties
for grouped and hierarchichal variable selection, annals of statistics.
(to appear).

zhu, j. and hastie, t. (2004). classi   cation of gene microarrays by penal-

ized id28, biostatistics 5(2): 427   443.

zhu, j., zou, h., rosset, s. and hastie, t. (2005). multiclass adaboost,

unpublished.

references

727

zou, h. (2006). the adaptive lasso and its oracle properties, journal of

the american statistical association 101: 1418   1429.

zou, h. and hastie, t. (2005). id173 and variable selection via
the elastic net, journal of the royal statistical society series b.
67(2): 301   320.

zou, h., hastie, t. and tibshirani, r. (2006). sparse principal com-
ponent analysis, journal of computational and graphical statistics
15(2): 265   28.

zou, h., hastie, t. and tibshirani, r. (2007). on the degrees of freedom

of the lasso, annals of statistics 35(5): 2173   2192.

728

references

author index

this is page 729
printer: opaque this

abu-mostafa, y. 95, 474
ackley, d. h. 645
adam, b.-l. 664
agrawal, r. 489   491, 578
agresti, a. 385, 638, 640
ahn, j. 695
akaike, h. 257
allen, d. 257
ambroise, c. 247
amit, y. 602
anderson, j. r. 641
anderson, t. 645
angelo, m. 654, 658
ardekani, a. m. 664

bach, f. 569
baezner, h. 551
bair, e. 676, 679   683, 693
bakin, s. 90
bakiri, g. 605, 606
banerjee, o. 636
barnhill, s. 658
barron, a. 415
barry, r. 371
bartlett, p. 384, 615

baskett, f. 480
baxter, j. 384
becker, r. 369
bell, a. 578
bellman, r. e. 22
benade, a. 122
bengio, y. 404, 407, 408, 414, 644
benjamini, y. 687, 689, 693
bentley, j. 480
best, n. 292
bibby, j. 94, 135, 441, 539, 559,

578, 630, 679

bickel, p. 652
bickel, p. j. 89
bishop, c. 38, 233, 414, 623, 645
bishop, y. 629, 638
bloom   eld, c. 663
boser, b. 404, 414
botha, j. 334
bottou, l. 404, 407, 408, 414, 644
boyd, s. 125, 632
breiman, l. 85, 243, 251, 257, 292,
308, 310, 334, 339, 367,
384, 451, 453, 455, 554,
587, 602

730

author index

bremaud, p. 577
brin, s. 577
brooks, r. j. 81
brown, p. 679
bruce, a. 181
b  uhlmann, p. 87, 361, 384
buja, a. 110, 297, 441, 446, 451,

455, 565, 574, 576, 578

bunea, f. 91
burges, c. 455
butte, a. 631

caligiuri, m. 663
callow, m. 686, 693
campo, e. 674
candes, e. 86, 89, 613
carlin, j. 292
cazares, l. h. 664
chambers, j. 334
chan, w. c. 674
chaudhuri, s. 631, 633
chen, l. 574, 576, 578
chen, s. s. 68, 94
cherkassky, v. 38, 239, 257
chu, g. 684, 693
chui, c. 181
clark, l. 331
clark, m. 539
clements, m. a. 664
cleveland, w. 369
cli   ord, p. 629
cohen, a. 668, 669
coifman, r. r. 679
coller, h. 663
comon, p. 578
connors, j. m. 674
cook, d. 565, 578
cook, n. 317
copas, j. b. 94, 610
cover, t. 257, 465, 481
cox, d. 292, 645
cressie, n. 171
csiszar, i. 292
cun, y. l. 407, 471, 481
cutler, a. 554

dale, m. 526
dasarathy, b. 480, 481
d   aspremont, a. 636
daubechies, i. 92, 181
davis, j. w. 664
de boor, c. 181
de mol, c. 92
de silva, v. 573
defrise, m. 92
dempster, a. 292, 449, 633
denham, m. 679
denker, j. 404, 407, 414, 471, 481
devijver, p. 480
dietterich, t. 286, 602, 605, 606,

623

donath, w. e. 578
donoho, d. 68, 86, 91, 94, 179,

181, 554, 613

downing, j. 663
drton, m. 631, 633
du plessis, j. 122
duan, n. 480
dubes, r. 508, 522
duchamp, t. 541
duda, r. 38, 135
dudoit, s. 686, 693
duin, r. 624

edwards, d. 645
efron, b. 73, 86, 90, 94, 97, 98,
128, 231, 254, 257, 292,
334, 568, 609, 692, 693

elad, m. 613
elith, j. 375, 376, 378
eskin, e. 668, 669
evgeniou, t. 168, 181, 455

fan, j. 92, 216, 539, 654
fan, y. 654
fazekas, f. 551
feng, z. 664
ferreira, j. 122
ferro, j. 551
fiedler, m. 578
fienberg, s. 585, 629, 638

finkel, r. 480
fisher, n. 334
fisher, r. a. 136, 455
fisher, r. i. 674
fisher, w. 310
fishman, d. a. 664
fix, e. 481
flury, b. 578
forgy, e. 578
francis, m. 375, 376, 378
frank, i. 81, 82, 94
frean, m. 384
freiha, f. 3, 49
freund, y. 337, 383, 384, 615
fridlyand, j. 693
friedman, j. 38, 81, 82, 85, 92   94,
111, 121, 126, 251, 257,
258, 308, 310, 334, 339,
345, 365, 367, 384, 391,
414, 437, 451, 453, 475,
480, 565, 578, 602, 611,
617   621, 623, 636, 657,
661, 667

friedman, n. 629, 630, 645
fu, w. 91, 92
fukunaga, k. 475
furnival, g. 57
fusaro, v. 664

gaasenbeek, m. 663
ganapathi, v. 642
gao, h. 181
gascoyne, r. d. 674
gelfand, a. 292
gelman, a. 292
geman, d. 292, 602
geman, s. 292
genkin, a. 661
genovese, c. 693
gerald, w. 654, 658
gersho, a. 514, 515, 526, 578
ghaoui, l. e. 636
gijbels, i. 216
gilks, w. 292
gill, p. 96, 421

author index

731

girosi, f. 168, 174, 181, 415
golub, g. 257, 335, 535
golub, t. 631, 654, 658, 663
goodall, c. 578
gordon, a. 578
gray, r. 514, 515, 526, 578
green, p. 181, 183, 334
greenacre, m. 455
greenshtein, e. 91
guo, y. 657
guyon, i. 658

ha   ner, p. 404, 407, 408, 414, 644
hall, p. 292, 602, 619
hammersley, j. m. 629
hand, d. 135, 475
hanley, j. 317
hansen, m. 328
hansen, r. 93
hart, p. 38, 135, 465, 480, 481
hartigan, j. a. 510, 578
hastie, t. 72, 73, 78, 86, 88, 90,
92   94, 97, 98, 110, 121,
122, 126, 137, 174, 216,
257, 297, 299, 304, 334,
339, 345, 348, 349, 375,
376, 378, 384, 385, 414,
428, 431, 434, 437, 441,
446, 451, 455, 475, 478,
480, 481, 519, 539, 550,
565, 568, 578, 606, 609   
611, 614, 615, 636, 657,
658, 660   662, 664, 667,
676, 679   683, 693

hatef, m. 624
hathaway, r. j. 292
heath, m. 257
hebb, d. 414
henderson, d. 404, 414
herman, a. 334
hertz, j. 414
hinkley, d. 292
hinton, g. 292, 334, 408, 414, 644,

645

hitt, b. a. 664

732

author index

ho, t. k. 602
hochberg, y. 687, 689, 693
hodges, j. 481
hoe   ing, h. 92, 93, 642, 667
hoerl, a. e. 64, 94
ho   , m. 396, 414
ho   man, a. j. 578
hofmann, h. 578
holland, p. 629, 638
hong, w. 684
hothorn, t. 87, 361, 384
howard, r. 404, 414
huard, c. 663
hubbard, w. 404, 414
huber, p. 349, 414, 435, 565, 578
hunter, d. 294
hyv  arinen, a. 560, 562, 578, 583

ihaka, r. 455
inskip, h. 292
inzitari, d. 551
izenman, a. 84

jackel, l. 404, 414
jacobs, r. 334
jain, a. 508, 522
james, g. 606
jancey, r. 578
jensen, f. v. 629
jiang, w. 384
jirou  sek, r. 640
johnson, n. 412
johnstone, i. 3, 49, 73, 86, 94, 97,

98, 179, 181, 609, 613

joli   e, i. t. 550
jones, l. 415
jones, m. 168, 174, 181, 415
jooste, p. 122
jordaan, p. 122
jordan, m. 334, 569, 645

kabalin, j. 3, 49
kalb   eisch, j. 674, 693
karhunen, j. 583
kaski, s. 531, 532, 578

kaufman, l. 517, 526, 578
kearns, m. 380
kennard, r. 64, 94
kent, j. 94, 135, 441, 539, 559,

578, 630, 679

kiiveri, h. t. 632
kim, s.-j. 125
kishon, e. 539
kittler, j. 480, 624
kleinberg, e. m. 602
knight, k. 91, 292, 666, 693
koh, k. 125
kohane, i. 631
kohavi, r. 243, 257
kohn, e. 664
kohonen, t. 462, 481, 531, 532,

578

koller, d. 629, 630, 642, 645
kooperberg, c. 328
korn, e. l. 693
kotze, j. 122
kressel, u. 437
krogh, a. 414

ladd, c. 654, 658
la   erty, j. 90, 304
la   erty, j. d. 642
lagus, k. 531, 532, 578
laird, n. 292, 449
lambert, d. 376
lander, e. 654, 658, 663
lange, k. 92, 294, 583, 584
langford, j. c. 573
larsen, r. 551
latulippe, e. 654, 658
lau   enburger, d. 625
lauritzen, s. 629, 632, 645
lawson, c. 93
le cun, y. 404, 406   408, 414
leathwick, j. 375, 376, 378
leblanc, m. 292
lecun, y. 644
lee, d. 552, 553
lee, m.-l. 693
lee, s.-i. 642

lee, w. 384, 615
leslie, c. 668, 669
levina, e. 652, 693
levine, p. j. 664
lewis, d. 661
li, k.-c. 480
li, r. 92
lin, h. 331
lin, y. 90, 304, 428, 455
liotta, l. a. 664
little, r. 332, 647
littman, m. 578
liu, h. 90, 304
lloyd, s. 481, 578
loader, c. 209, 216
loda, m. 654, 658
loh, m. 663
loh, w. 310
lugosi, g. 384

ma, y. 257
macnaughton smith, p. 526
mackay, d. 623
macqueen, j. 481, 578
madigan, d. 257, 292, 661
makeig, s. 564, 565
mannila, h. 489   491, 578
mardia, k. 94, 135, 441, 539, 559,

578, 630, 679

marron, j. 695
mason, l. 384
massart, d. 517
matas, j. 624
mccullagh, p. 638, 640
mcculloch, c. 331
mcculloch, w. 414
mclachlan, g. 135, 247
mcneal, j. 3, 49
mcneil, b. 317
mcshane, l. m. 693
mease, d. 384, 603
meinshausen, n. 91, 635, 642
meir, r. 384
mesirov, j. 654, 658, 663
mills, g. b. 664

author index

733

mockett, l. 526
morgan, j. n. 334
motwani, r. 577
mukherjee, s. 654, 658
mulier, f. 38, 239
muller-hermelink, h. k. 674
m  uller, k.-r. 547, 548
munro, s. 397
murray, w. 96, 421
myles, j. 475

nadler, b. 679
narasimhan, b. 693
neal, r. 268, 292, 409   412, 414,

605, 623

nelder, j. 638, 640
noble, w. s. 668, 669
nolan, g. 625
nowlan, s. 334

oja, e. 560, 562, 578, 583
olesen, k. g. 629
olshen, r. 251, 308, 310, 334, 367,

451, 453
onton, j. 564, 565
osborne, m. 76, 94
osindero, s. 644

paatero, a. 531, 532, 578
pace, r. k. 371
page, l. 577
palmer, r. 414
pantoni, l. 551
park, m. y. 94, 126, 661
parker, d. 414
paul, d. 676, 679   683, 693
pearl, j. 629, 645
pe   er, d. 625
perez, o. 625
peterson 641
petricoin, e. f. 664
pitts, w. 414
plastria, f. 517
platt, j. 453
poggio, t. 168, 174, 181, 415, 455,

654, 658

734

author index

pontil, m. 168, 181, 455
popescu, b. 617   619, 621, 623
prentice, r. 674, 693
presnell, b. 76, 94
p  reu  cil, s. 640

qu, y. 664
quinlan, r. 312, 334, 624

radmacher, m. d. 693
raftery, a. 257, 292
ramaswamy, s. 654, 658
ramsay, j. 181, 578
rao, c. r. 455
r  atsch, g. 384, 615
ravikumar, p. 90, 304, 642
redwine, e. 3, 49
reich, m. 654, 658
richardson, j. 375
richardson, t. s. 631, 633
ridgeway, g. 361
rieger, k. 684
rifkin, r. 654, 658
ripley, b. d. 38, 131, 135, 136,
234, 308, 310, 400, 414,
415, 455, 468, 480, 481,
641, 645

rissanen, j. 257
ritov, y. 89, 91
robbins, h. 397
rocha, g. 90
roosen, c. 414
rosenblatt, f. 102, 129, 414
rosenwald, a. 674
rosset, s. 89, 98, 348, 349, 385,
426, 428, 434, 610, 611,
615, 657, 661, 664, 666,
693

rostrup, e. 551
rousseauw, j. 122
rousseeuw, p. 517, 526, 578
rowe, d. 375
roweis, s. t. 573
rubin, d. 292, 332, 449, 647
rumelhart, d. 414

ryberg, c. 551

saarela, a. 531, 532, 578
sachs, k. 625
saloj  arvi, j. 531, 532, 578
saul, l. k. 573
saunders, m. 68, 94, 666, 693
schapire, r. 337, 380, 383, 384,

615

schellhammer, p. f. 664
schnitzler, c. 334
sch  olkopf, b. 547, 548
schroeder, a. 391
schwarz, g. 233, 257
scott, d. 216
seber, g. 94
segal, m. 596
sejnowski, t. 578, 645
semmes, o. j. 664
seung, h. 552, 553
shafer, g. 629
shao, j. 257
shenoy, p. 629
short, r. 475
shustek, l. 480
shyu, m. 369
siegmund, d. 689
silverman, b. 181, 183, 216, 334,

486, 567, 578

silvey, s. 292
simard, p. 407, 471, 480, 481
simon, r. m. 693
simone, c. 664
singer, y. 384
sj  ostrand, k. 551
slate, e. 331
slonim, d. 631, 663
smeland, e. b. 674
smith, a. 292
smola, a. 547, 548
sonquist, j. a. 334
spector, p. 243, 257
speed, t. 632, 686, 693
spiegelhalter, d. 292, 629
spiegelman, c. 679

spielman, d. a. 578
srikant, r. 489   491, 578
stamey, t. 3, 49
staudt, l. m. 674
steinberg, s. m. 664
stern, h. 292
stodden, v. 554
stone, c. 251, 308, 310, 328, 334,

367, 451, 453

stone, m. 81, 257
storey, j. 689, 692, 693, 697, 698
stork, d. 38, 135
studholme, c. 551
stuetzle, w. 391, 414, 541, 578
surowiecki, j. 286
swayne, d. 565, 578

tamayo, p. 631, 654, 658, 663
tang, j. 684
tanner, m. 292
tao, t. 89, 613
tarpey, t. 578
taylor, j. 88, 94, 610, 614, 689
taylor, p. 375, 376, 378
teh, y.-w. 644
tenenbaum, j. b. 573
teng, s.-h. 578
thomas, j. 257
tibshirani, r. 73, 78, 86, 88, 90,
92   94, 97, 98, 110, 121,
122, 126, 137, 216, 257,
292, 297, 299, 304, 334,
339, 345, 384, 428, 431,
434, 437, 441, 446, 451,
455, 475, 478, 480, 481,
519, 550, 565, 568, 609   
611, 614, 636, 642, 657,
658, 660, 661, 666, 667,
676, 679   684, 692, 693

toivonen, h. 489   491, 578
traskin, m. 384
trenda   lov, n. t. 550
tropp, j. 91
truong, y. 328
tsybakov, a. 89, 91

author index

735

tukey, j. 414, 565, 578
turlach, b. 76, 94
turnbull, b. 331
tusher, v. 684, 692
tusn  ady, g. 292

uddin, m. 550

valiant, l. g. 380
van der merwe, a. 84
van loan, c. 335, 535
vandenberghe, l. 632
vanichsetakul, n. 310
vapnik, v. 38, 102, 132, 135, 171,

257, 438, 455, 658

vayatis, n. 384
vazirani, u. 380
verkamo, a. i. 489   491, 578
vidakovic, b. 181
von luxburg, u. 578

wahba, g. 168, 169, 181, 257, 268,

428, 429, 455

wainwright, m. 91
wainwright, m. j. 642
waldemar, g. 551
walther, g. 88, 94, 519, 610, 614
wang, p. 667
ward, m. d. 664
warmuth, m. 615
wasserman, l. 90, 304, 626, 645,

693

watkins, c. 658
wegkamp, m. 91
weisberg, s. 94
werbos, p. 414
wermuth, n. 645
weston, j. 658, 668, 669
whittaker, j. 632, 633, 641, 645
wickerhauser, m. 181
widrow, b. 396, 414
wild, c. 300
williams, r. 414
williams, w. 526
wilson, r. 57

736

author index

winograd, t. 577
wold, h. 94
wolpert, d. 292
wong, m. a. 510
wong, w. 292
wright, g. 664, 674, 693
wright, m. 96, 421
wu, t. 92, 294, 583
wyner, a. 384, 603

yang, n. 3, 49
yang, y. 686, 693
yasui, y. 664
yeang, c. 654, 658
yee, t. 300
yekutieli, y. 693
yu, b. 90, 91, 384
yuan, m. 90

zhang, h. 90, 304, 428, 455
zhang, j. 409   412, 605
zhang, p. 257
zhang, t. 384
zhao, p. 90, 91
zhao, y. 693
zhu, j. 89, 98, 174, 348, 349, 385,
426, 428, 434, 610, 611,
615, 657, 661, 664, 666,
693
zidek, j. 84
zou, h. 72, 78, 92, 349, 385, 550,
662, 693

index

this is page 737
printer: opaque this

l1 id173, see lasso

activation function, 392   395
adaboost, 337   346
adaptive lasso, 92
adaptive methods, 429
adaptive nearest neighbor meth-

ods, 475   478

adaptive wavelet    ltering, 181
additive model, 295   304
adjusted response, 297
a   ne set, 130
a   ne-invariant average, 482, 540
aic, see akaike information cri-

terion

akaike information criterion (aic),

230

analysis of deviance, 124
applications

abstracts, 672
aorta, 204
bone, 152
california housing, 371   372,

591

countries, 517

demographics, 379   380
document, 532
   ow cytometry, 637
galaxy, 201
heart attack, 122, 146, 207
lymphoma, 674
marketing, 488
microarray, 5, 505, 532
nested spheres, 590
new zealand    sh, 375   379
nuclear magnetic resonance,

176

ozone, 201
prostate cancer, 3, 49, 61, 608
protein mass spectrometry, 664
satellite image, 470
skin of the orange, 429   432
spam, 2, 300   304, 313, 320,

328, 352, 593

vowel, 440, 464
waveform, 451
zip code, 4, 404, 536   539

archetypal analysis, 554   557
association rules, 492   495, 499   

501

738

index

automatic relevance determination,

411

automatic selection of smoothing

parameters , 156

b-spline, 186
back-propagation, 392   397, 408   

409

back   tting, 297, 391
backward

selection, 58
stepwise selection, 59

backward pass, 396
id112, 282   288, 409, 587
basis expansions and regulariza-

tion, 139   189

basis functions, 141, 186, 189, 321,

328

batch learning, 397
baum   welch algorithm, 272
bayes

classi   er, 21
factor, 234
methods, 233   235, 267   272
rate, 21

bayesian, 409
bayesian information criterion (bic),

233

benjamini   hochberg method, 688
best-subset selection, 57, 610
between class covariance matrix,

114

bias, 16, 24, 37, 160, 219
bias-variance decomposition, 24,

37, 219

bias-variance tradeo   , 37, 219
bic, see bayesian information cri-

terion

id82s, 638   648
bonferroni method, 686
boosting, 337   386, 409

as lasso regression, 607   609
exponential loss and adaboost,

343

implementations, 360
margin maximization, 613
numerical optimization, 358
partial-dependence plots, 369
id173 path, 607
shrinkage, 364
stochastic gradient boosting,

365

tree size, 361
variable importance, 367

bootstrap, 249, 261   264, 267, 271   

282, 587

relationship to bayesian method,

271

relationship to maximum like-

lihood method, 267

bottom-up id91, 520   528
bump hunting, see patient rule

induction method

bumping, 290   292

c5.0, 624
canonical variates, 441
cart, see classi   cation and re-

gression trees

categorical predictors, 10, 310
censored data, 674
classical multidimensional scaling,

570

classi   cation, 22, 101   137, 305   

317, 417   429

classi   cation and regression trees

(cart), 305   317

clique, 628
id91, 501   528

id116, 509   510
agglomerative, 523   528
hierarchical, 520   528

codebook, 515
combinatorial algorithms, 507
combining models, 288   290
committee, 289, 587, 605
comparison of learning methods,

350   352

gradient boosting, 358

complete data, 276

complexity parameter, 37
computational shortcuts

quadratic penalty, 659
condensing procedure, 480
conditional likelihood, 31
confusion matrix, 301
conjugate gradients, 396
consensus, 285   286
convolutional networks, 407
coordinate descent, 92, 636, 668
cosso, 304
cost complexity pruning, 308
covariance graph, 631
cp statistic, 230
cross-id178, 308   310
cross-validation, 241   245
cubic smoothing spline, 151   153
cubic spline, 151   153
curse of dimensionality, 22   26

dantzig selector, 89
data augmentation, 276
daubechies symid113t-8 wavelets,

176

de-correlation, 597
decision boundary, 13   15, 21
id90, 305   317
decoder, 515, see encoder
decomposable models, 641
degrees of freedom

in an additive model, 302
in ridge regression, 68
of a tree, 336
of smoother matrices, 153   154,

158

delta rule, 397
demid113r-reinsch basis for splines,

156

density estimation, 208   215
deviance, 124, 309
diagonal linear discriminant anal-

ysis, 651   654

dimension reduction, 658

for nearest neighbors, 479
discrete variables, 10, 310   311

index

739

discriminant

adaptive nearest neighbor clas-

si   er, 475   480

analysis, 106   119
coordinates, 108
functions, 109   110

dissimilarity measure, 503   504
dummy variables, 10

early stopping, 398
e   ective degrees of freedom, 17,

68, 153   154, 158, 232, 302,
336

e   ective number of parameters,
15, 68, 153   154, 158, 232,
302, 336

eigenvalues of a smoother matrix,

154

elastic net, 662
em algorithm, 272   279

as a maximization-maximization

procedure, 277

for two component gaussian

mixture, 272

encoder, 514   515
ensemble, 616   623
id108, 605   624
id178, 309
equivalent kernel, 156
error rate, 219   230
error-correcting codes, 606
estimates of in-sample prediction

error, 230

expectation-maximization algorithm,

see em algorithm

extra-sample error, 228

false discovery rate, 687   690, 692,

693

feature, 1

extraction, 150
selection, 409, 658, 681   683

feed-forward neural networks, 392   

408

740

index

fisher   s linear discriminant, 106   

119, 438

flexible discriminant analysis, 440   

445

forward

selection, 58
stagewise, 86, 608
stagewise additive modeling,

342

stepwise, 73

forward pass algorithm, 395
fourier transform, 168
frequentist methods, 267
function approximation, 28   36
fused lasso, 666

gap statistic, 519
gating networks, 329
gauss-markov theorem, 51   52
gauss-id77, 391
gaussian (normal) distribution, 16
gaussian graphical model, 630
gaussian mixtures, 273, 463, 492,

509

gaussian radial basis functions,

212

gbm, see gradient boosting
gbm package, see gradient boost-

ing

gcv, see generalized cross-validation
gem (generalized em), 277
generalization
error, 220
performance, 220

generalized additive model, 295   

304

generalized association rules, 497   

499

generalized cross-validation, 244
generalized linear discriminant anal-

ysis, 438

generalized linear models, 125
gibbs sampler, 279   280, 641

for mixtures, 280

gini index, 309

global markov property, 628
gradient boosting, 359   361
id119, 358, 395   397
graph laplacian, 545
graphical lasso, 636
grouped lasso, 90

haar basis function, 176
hammersley-cli   ord theorem, 629
hard-thresholding, 653
hat matrix, 46
helix, 582
hessian matrix, 121
hidden nodes, 641   642
hidden units, 393   394
hierarchical id91, 520   528
hierarchical mixtures of experts,

329   332

high-dimensional problems, 649
hints, 96
hyperplane, see separating hy-

perplane

ica, see independent components

analysis

importance sampling, 617
in-sample prediction error, 230
incomplete data, 332
independent components analysis,

557   570

independent variables, 9
indicator response matrix, 103
id136, 261   294
information

fisher, 266
observed, 274

id205, 236, 561
inner product, 53, 668, 670
inputs, 10
instability of trees, 312
intercept, 11
invariance manifold, 471
invariant metric, 471
inverse wavelet transform, 179

irls, see iteratively reweighted

least squares
irreducible error, 224
ising model, 638
isomap, 572
isometric feature mapping, 572
iterative proportional scaling, 585
iteratively reweighted least squares

(irls), 121

jensen   s inequality, 293
join tree, 629
junction tree, 629

id116 id91, 460, 509   514
k-medoid id91, 515   520
k-nearest neighbor classi   ers, 463
karhunen-loeve transformation (prin-

index

741

fused, 666

latent

factor, 674
variable, 678

learning, 1
learning rate, 396
learning vector quantization, 462
least angle regression, 73   79, 86,

610

least squares, 11, 32
leave-one-out cross-validation, 243
lenet, 406
likelihood function, 265, 273
linear basis expansion, 139   148
linear combination splits, 312
linear discriminant function, 106   

119

linear methods

cipal components), 66   
67, 79, 534   539

for classi   cation, 101   137
for regression, 43   99

karush-kuhn-tucker conditions,

linear models and least squares,

133, 420

kernel

classi   cation, 670
density classi   cation, 210
density estimation, 208   215
function, 209
id28, 654
principal component, 547   550
string, 668   669
trick, 660

kernel methods, 167   176, 208   215,

423   438, 659

knot, 141, 322
kriging, 171
kruskal-shephard scaling, 570
kullback-leibler distance, 561

lagrange multipliers, 293
landmark, 539
laplacian, 545
laplacian distribution, 72
lar, see least angle regression
lasso, 68   69, 86   90, 609, 635, 636,

661

11

id75 of an indicator

matrix, 103

linear separability, 129
linear smoother, 153
link function, 296
lle, see local linear embedding
local false discovery rate, 693
local likelihood, 205
local linear embedding, 572
local methods in high dimensions,

22   27

local minima, 400
local polynomial regression, 197
local regression, 194, 200
localization in time/frequency, 175
loess (local regression), 194, 200
log-linear model, 639
log-odds ratio (logit), 119
logistic (sigmoid) function, 393
id28, 119   128, 299
logit (log-odds ratio), 119
id168, 18, 21, 219   223, 346
loss matrix, 310

742

index

lossless compression, 515
lossy compression, 515
lvq, see learning vector quan-

tization

mahalanobis distance, 441
majority vote, 337
majorization, 294, 553
majorize-minimize algorithm, 294,

584

map (maximum aposteriori) es-

timate, 270

margin, 134, 418
market basket analysis, 488, 499
id115 (mcmc)

methods, 279

markov graph, 627
markov networks, 638   648
mars, see multivariate adaptive

regression splines

mixture of experts, 329   332
mixtures and the em algorithm,

272   275

mm algorithm, 294, 584
mode seekers, 507
model averaging and stacking, 288
model combination, 289
model complexity, 221   222
model selection, 57, 222   223, 230   

231

modi   ed regression, 634
monte carlo method, 250, 495
mother wavelet, 178
multidimensional scaling, 570   572
multidimensional splines, 162
multiedit algorithm, 480
multilayer id88, 400, 401
multinomial distribution, 120
multiple additive regression trees

(mart), 361

mart, see multiple additive re-

multiple hypothesis testing, 683   

gression trees

id113,

31, 261, 265

mcmc, see markov chain monte

carlo methods

mdl, see minimum description

length

mean    eld approximation, 641
mean squared error, 24, 285
memory-based method, 463
metropolis-hastings algorithm, 282
minimum description length (mdl),

235

693

multiple minima, 291, 400
multiple outcome shrinkage and

selection, 84

multiple outputs, 56, 84, 103   106
multiple regression from simple uni-

variate regression, 52

multiresolution analysis, 178
multivariate adaptive regression
splines (mars), 321   327
multivariate nonparametric regres-

sion, 445

minorization, 294, 553
minorize-maximize algorithm, 294,

nadaraya   watson estimate, 193
naive bayes classi   er, 108, 210   

584

misclassi   cation error, 17, 309
missing data, 276, 332   333
missing predictor values, 332   333
mixing proportions, 214
mixture discriminant analysis, 449   

455

211, 694

natural cubic splines, 144   146
nearest centroids, 670
nearest neighbor methods, 463   

483

nearest shrunken centroids, 651   

654, 694

mixture modeling, 214   215, 272   

275, 449   455, 692

network diagram, 392
neural networks, 389   416

index

743

newton   s method (newton-raphson

procedure), 120   122

id88, 392   416
piecewise polynomials and splines,

non-negative id105,

36, 143

553   554

posterior

nonparametric id28,

299   304

normal (gaussian) distribution,

16, 31

normal equations, 12
numerical optimization, 395   396

object dissimilarity, 505   507
online algorithm, 397
optimal scoring, 445, 450   451
optimal separating hyperplane, 132   

135

optimism of the training error rate,

228   230

distribution, 268
id203, 233   235, 268

power method, 577
pre-conditioning, 681   683
prediction accuracy, 329
prediction error, 18
predictive distribution, 268
prim, see patient rule induction

method

principal components, 66   67, 79   

80, 534   539, 547

regression, 79   80
sparse, 550
supervised, 674

ordered categorical (ordinal) pre-

principal curves and surfaces, 541   

dictor, 10, 504

ordered features, 666
orthogonal predictors, 53
over   tting, 220, 228   230, 364

id95, 576
pairwise distance, 668
pairwise markov property, 628
parametric bootstrap, 264
partial dependence plots, 369   370
partial least squares, 80   82, 680
partition function, 638
parzen window, 208
pasting, 318
path algorithm, 73   79, 86   89, 432
patient rule induction method(prim),

544

principal points, 541
prior distribution, 268   272
procrustes

average, 540
distance, 539

projection pursuit, 389   392, 565

regression, 389   392

prototype classi   er, 459   463
prototype methods, 459   463
proximity matrices, 503
pruning, 308

qr decomposition, 55
quadratic approximations and in-

ference, 124

317   321, 499   501

quadratic discriminant function,

peeling, 318
penalization, 607, see regulariza-

108, 110

tion

radial basis function (rbf) net-

penalized discriminant analysis, 446   

work, 392

449

penalized polynomial regression,

171

penalized regression, 34, 61   69, 171
penalty matrix, 152, 189

radial basis functions, 212   214,

275, 393
radial kernel, 548
id79, 409, 587   604

algorithm, 588

744

index

bias, 596   601
comparison to boosting, 589
example, 589
out-of-bag (oob), 592
over   t, 596
proximity plot, 595
variable importance, 593
variance, 597   601

rao score test, 125
rayleigh quotient, 116
receiver operating characteristic

(roc) curve, 317

reduced-rank linear discriminant

analysis, 113

regression, 11   14, 43   99, 200   204
regression spline, 144
id173, 34, 167   176
regularized discriminant analysis,

separating hyperplanes, 136, 417   

419

separator, 628
shape average, 482, 540
shrinkage methods, 61   69, 652
sigmoid, 393
signi   cance analysis of microar-

rays, 690   693

similarity measure, see dissimi-

larity measure
single index model, 390
singular value decomposition, 64,

535   536, 659

singular values, 535
singular vectors, 535

sliced inverse regression, 480
smoother, 139   156, 192   199

matrix, 153

112   113, 654

smoothing parameter, 37, 156   161,

relevance network, 631
representer of evaluation, 169
reproducing kernel hilbert space,

167   176, 428   429
reproducing property, 169
responsibilities, 274   275
ridge regression, 61   68, 650, 659
risk factor, 122
robust    tting, 346   350
rosenblatt   s id88 learning

algorithm, 130

rug plot, 303
rule   t, 623

sam, 690   693, see signi   cance anal-

ysis of microarrays

sammon mapping, 571
scad, 92
scaling of the inputs, 398
schwarz   s criterion, 230   235
score equations, 120, 265
self-consistency property, 541   543
self-organizing map (som), 528   

198   199

smoothing spline, 151   156
soft id91, 512
soft-thresholding, 653
softmax function, 393
som, see self-organizing map
sparse, 175, 304, 610   613, 636

additive model, 91
graph, 625, 635

speci   city of a test, 314   317
spectral id91, 544   547
spline, 186

additive, 297   299
cubic, 151   153
cubic smoothing, 151   153
interaction, 428
regression, 144
smoothing, 151   156
thin plate, 165

squared error loss, 18, 24, 37, 219
srm, see structural risk minimiza-

tion

stacking (stacked generalization),

534

290

sensitivity of a test, 314   317
separating hyperplane, 132   135

starting values, 397
statistical decision theory, 18   22

index

745

undirected graph, 625   648
universal approximator, 390
unsupervised learning, 2, 485   585
unsupervised learning as super-
vised learning, 495   497

validation set, 222
vapnik-chervonenkis (vc) dimen-

sion, 237   239

variable importance plot, 594
variable types and terminology, 9
variance, 16, 25, 37, 158   161, 219

between, 114
within, 114, 446

variance reduction, 588
varying coe   cient models, 203   

204

vc dimension, see vapnik   chervon-

enkis dimension

vector quantization, 514   515
voronoi regions, 510

wald test, 125
wavelet

basis functions, 176   179
smoothing, 174
transform, 176   179
weak learner, 383, 605
weakest link pruning, 308
webpages, 576
website for book, 8
weight decay, 398
weight elimination, 398
weights in a neural network, 395
within class covariance matrix, 114,

446

statistical model, 28   29
steepest descent, 358, 395   397
stepwise selection, 60
stochastic approximation, 397
stochastic search (bumping), 290   

292

stress function, 570   572
structural risk minimization (srm),

239   241

subset selection, 57   60
supervised learning, 2
supervised principal components,

674   681

support vector classi   er, 417   421,

654

multiclass, 657

support vector machine, 423   437
sure shrinkage method, 179
survival analysis, 674
survival curve, 674
svd, see singular value decom-

position

symid113t basis, 176

tangent distance, 471   475
tanh activation function, 424
target variables, 10
tensor product basis, 162
test error, 220   223
test set, 220
thin plate spline, 165
thinning strategy, 189
trace of a matrix, 153
training epoch, 397
training error, 220   223
training set, 219   223
tree for regression, 307   308
tree-based methods, 305   317
trees for classi   cation, 308   310
trellis display, 202

