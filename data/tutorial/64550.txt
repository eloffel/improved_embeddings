mathematical foundations of data sciences

gabriel peyr  e
cnrs & dma

  ecole normale sup  erieure
gabriel.peyre@ens.fr

https://mathematical-tours.github.io

www.numerical-tours.com

january 7, 2018

236

chapter 16

machine learning

this chapter gives a rapid overview of the main concepts in machine learning. the goal is not to be
exhaustive, but to highlight representative problems and insist on the distinction between unsupervised
(vizualization and id91) and supervised (regression and classi   cation) setups. we also shed light on
the tight connexions between machine learning and inverse problems.

while imaging science problems are generally concern with processing a single data (e.g. an image),
machine learning problem is rather concern with analysing large collection of data. the focus (goal and
performance measures) is thus radically di   erent, but quite surprisingly, it uses very similar tools and
algorithm (in particular linear models and id76).

16.1 unsupervised learning

in unsupervised learning setups, one observes n points (xi)n

i=1. the problem is now to infer some
properties for this points, typically for vizualization or unsupervised classication (often called id91).
for simplicity, we assume the data are points in euclidean space xi     rp (p is the so-called number of
features). these points are conveniently stored as the rows of a matrix x     rn  d.

16.1.1 id84 and pca

id84 is useful for vizualization. it can also be understood as the problem of feature
extraction (determining which are the relevant parameters) and this can be later used for doing other tasks
more e   ciently (faster and/or with better performances). the simplest method is the principal component
analysis (pca), which performs an orthogonal linear projection on the principal axes (eigenvectors) of the
covariance matrix.

the empirical mean is de   ned as

  m def.=

and covariance

n(cid:88)

i=1

  c def.=

1
n

n(cid:88)

i=1

1
n

xi     rp

(xi       m)(xi       m)        rp  p.

(16.1)

note that if the points (xi)i are modelled as i.i.d. variables, and denoting x one of these random variables,

denoting   x def.= x     1p   m   , one has   c =   x      x/n.
one has, using the law of large numbers, the almost sure convergence as n     +   
  c     c def.= e((x     m)(x     m)   ).

  m     m def.= e(x)

and

(16.2)

237

c

svd of c

figure 16.1: empirical covariance of the data and its associated singular values.

denoting    the distribution (radon measure) on rp of x, one can alternatively write

(cid:90)

(cid:90)

m =

xd  (x)

and c =

rp

rp

(x     m)(x     m)   d  (x).

the pca ortho-basis, already introduced in section 20, corresponds to the
right singular vectors of the centred data matrix, as de   ned using the (reduced)
svd decomposition

  x = u diag(  )v    

where u     rn  r and v     rp  r, and where r = rank(   x) (cid:54) min(n, p). we
denote v = (vk)r
k=1 the orthogonal columns (which forms a orthogonal system
of eigenvectors of   c), vk     rp. the intuition is that they are the main axes
of    gravity    of the point cloud (xi)i in rp. we assume the singular values are
ordered,   1 (cid:62) . . . (cid:62)   r, so that the    rst singular values capture most of the
variance of the data.

figure 16.2: pca main
axes capture variance

figure 16.1 displays an example of covariance and its associated spectrum   . the points (xi)i correspond
to the celebrated iris dataset1 of fisher. this dataset consists of 50 samples from each of three species
of iris (iris setosa, iris virginica and iris versicolor). the dimensionality of the features is p = 4, and the
dimensions corresponds to the length and the width of the sepals and petals.
the pca id84 embedding xi     rp (cid:55)    zi     rd in dimension d (cid:54) p is obtained by

projecting the data on the    rst d singular vector

one has that   xi = proj   t (xi) where   t def.= m + spand
shows that pca is optimal in term of (cid:96)2 distance if one consider only a   ne spaces.

k=1(vk) is an a   ne space. the following proposition

from these low-dimensional embedding, one can reconstruct back an approximation as

def.= ((cid:104)xi     m, vk(cid:105))d

k=1     rd.

zi

  xi

def.= m +

zi,kvk     rp.

(cid:88)

k

(cid:40)(cid:88)

i

proposition 52. one has

(  x,   t )     argmin
(  x,   t )

(cid:41)

||xi       xi||2 ;     i,   xi       t

where   t is constrained to be a d-dimensional a   ne space.

figure 16.3 shows an example of pca for 2-d and 3-d vizualization.

1https://en.wikipedia.org/wiki/iris_flower_data_set

238

figure 16.3: 2-d and 3-d pca vizualization of the input clouds.

16.1.2 id91 and id116

a typical unsupervised learning task is to infer a class label yi     {1, . . . , k} for each input point xi, and
this is often called a id91 problem (since the set of points associated to a given label can be thought
as a cluster).

id116 a way to infer these labels is by assuming that the clusters are compact, and optimizing some
compactness criterion. assuming for simplicity that the data are in euclidean space (which can be relaxed
to an arbitrary metric space, although the computations become more complicated), the id116 approach
(cid:96)=1, where each c(cid:96)     rp. the
minimizes the distance between the points and their class centroids c = (c(cid:96))k
corresponding variational problem becomes

e(y, c) def.=

min
(y,c)

||xi     c(cid:96)||2.

k(cid:88)

(cid:88)

(cid:96)=1

i:yi=(cid:96)

the id116 algorithm can be seen as a block coordinate relaxation, which
alternatively updates the class labels and the centroids. the centroids c are
   rst initialized (more on this later), for instance, using a well-spread set of
points from the samples. for a given set c of centroids, minimizing y (cid:55)    e(y, c)
is obtained in closed form by assigning as class label the index of the closest
centroids

    i     {1, . . . , n},

yi     argmin
1(cid:54)(cid:96)(cid:54)k

||xi     c(cid:96)||.

(16.3)

for a given set y of labels, minimizing c (cid:55)    e(y, c) is obtained in closed form
by computing the barycenter of each class

(cid:80)

    (cid:96)     {1, . . . , k},

c(cid:96)    

i:yi=(cid:96) xi
|{i ; yi = (cid:96)}|

figure 16.4: id116 clus-
ters according to vornoi
cells.

(16.4)

if during the iterates, one of the cluster associated to some c(cid:96) becomes empty, then one can either decide to
destroy it and replace k by k     1, or try to    teleport    the center c(cid:96) to another location (this might increase
the objective function e however).
since the energy e is decaying during each of these two steps, it is converging to some limit value.
since there is a    nite number of possible labels assignments, it is actually constant after a    nite number of
iterations, and the algorithm stops.

of course, since the energy is non-convex, little can be said about the property of the clusters output
by id116. to try to reach lower energy level, it is possible to    teleport    during the iterations centroids
c(cid:96) associated to clusters with high energy to locations within clusters with lower energy (because optimal
solutions should somehow balance the energy).

figure 16.5 shows an example of id116 iterations on the iris dataset.

239

figure 16.5: left: iteration of id116 algorithm. right: histogram of points belonging to each class after
the id116 optimization.

id116++ to obtain good results when using id116, it is crucial to have an e   cient initialization
scheme. in practice, the best results are obtained by seeding them as far as possible from one another (a
greedy strategy works great in practice).
quite surprisingly, there exists a randomized seeding strategy which can be shown to be close to optimal
in term of value of e, even without running the id116 iterations (although in practice it still needs to be
used to polish the results). the corresponding id116++ initialization is obtained by selecting c1 uniformly
at random among the xi, and then assuming c(cid:96) has been seeded, drawing c(cid:96)+1 among the sample according
to the id203   ((cid:96)) on {1, . . . , n} proportional to the square of the distance to the previously seeded points

    i     {1, . . . , n},

  ((cid:96))
i

def.=

where dj

def.= min

1(cid:54)r(cid:54)(cid:96)   1

||xi     cr||.

i(cid:80)n

d2
j=1 d2
j

this means that points which are located far away from the preciously seeded centers are more likely to be
picked.

the following results, due to david arthur and sergei vassilvitskii, shows that this seeding is optimal

up to log factor on the energy. note that    nding a global optimum is known to be np-hard.

theorem 50. for the centroids c(cid:63) de   ned by the id116++ strategy, denoting y(cid:63) the associated nearest
neighbor labels de   ned as in (16.3), one has

e(e(y(cid:63), c(cid:63))) (cid:54) 8(2 + log(k))min

(y,c)

e(y, v),

where the expectation is on the random draws performed by the algorithm.

lloyd algorithm and continuous densities. the id116 iterations are also called    lloyd    algorithm,
which also    nd applications to optimal vector quantization for compression.
it can also be used in the
   continuous    setting where the empirical samples (xi)i are replaced by an arbitrary measure over rp. the
energy to minimize becomes

(cid:90)

k(cid:88)

(cid:96)=1

v(cid:96)

min
(v,c)

||x     c(cid:96)||2d  (x)

where (v(cid:96))(cid:96) is a partition of the domain. step (16.3) is replaced by the computation of a voronoi cell

    (cid:96)     {1, . . . , k}, v(cid:96)

def.= {x ;     (cid:96)(cid:48) (cid:54)= (cid:96),||x     c(cid:96)|| (cid:54) ||x     c(cid:96)(cid:48)||} .

these voronoi cells are polyhedra delimited by segments of mediatrix between centroids, and this voronoi
segmentation can be computed e   ciently using tools from algorithmic geometry in low dimension. step (16.4)
are then replaced by

(cid:82)
(cid:82)

c(cid:96)
c(cid:96)

xd  (x)

d  (x)

.

    (cid:96)     {1, . . . , k},

c(cid:96)    

240

iter #1iter #2iter #3iter #1612300.5112300.5112300.51iteration of id116 algorithm (lloyd algorithm) on continuous densities   . top: uniform.
figure 16.6:
bottom: non-uniform (the densities of    with respect to the lebesgue measure is displayed as a grayscale
image in the background).

in the case of    being uniform distribution, optimal solution corresponds to the hexagonal lattice. figure 16.6
displays two examples of lloyd iterations on 2-d densities on a square domain.

16.2 empirical risk minimization

before diving into the speci   cs of regression and classi   cation problems, let us give describe a generic
methodology which can be applied in both case (possibly with minor modi   cation for classi   cation, typically
considering class probabilities instead of class labels).
in order to make the problem tractable computationally, and also in order to obtain e   cient prediction
scores, it is important to restrict the    t to the data yi     f (xi) using a    small enough    class of functions.
intuitively, in order to avoid over   tting, the    size    of this class of functions should grows with the number
n of samples.

16.2.1 empirical risk

denoting fn some class of functions (which depends on the number of available samples), one of the

most usual way to do the learning is to perform an empirical risk minimization (erm)

l(f (xi), yi).

(16.5)

here l : y 2     r+ is the so-called id168, and it should typically satis   es l(y, y(cid:48)) = 0 if and only if
y = y(cid:48). the speci   cs of l depend on the application at hand (in particular, one should use di   erent losses
for classi   cation and regression tasks). to highlight the dependency of   f on n, we occasionally write   fn.

16.2.2 prediction and consistency

when doing a mathematically analysis, one usually assumes that (xi, yi) are drawn from a distribution

   on x    y, and the large n limit de   nes the ideal estimator

n(cid:88)

i=1

  f     argmin
f   fn

1
n

(cid:90)

  f     argmin
f   f   

x  y

l(f (x), y)d  (x, y) = e(x,y)     (l(f (x), y).

(16.6)

241

intuitively, one should have   fn       f as n     +   , which can be captured in expectation of the prediction
error over the samples (xi, yi)i, i.e.

def.= e(   l(   fn(x),   f (x)))        0.

en

one should be careful that here the expectation is over both x (distributed according to the marginal   x
of    on x ), and also the n i.i.d. pairs (xi, yi)        used to de   ne   fn (so a better notation should rather be
(xi, yi)i. here   l is some id168 on y (one can use   l = l for instance). one can also study convergence
in id203, i.e.

       > 0, e  ,n

def.= p(   l(   fn(x),   f (x)) >   )     0.

if this holds, then one says that the estimation method is consistent (in expectation or in id203). the
question is then to derive convergence rates, i.e. to upper bound en or e  ,n by some explicitly decay rate.
note that when   l(y, y(cid:48)) = |y   y(cid:48)|r, then convergence in expectation is stronger (implies) than convergence

in id203 since using markov   s inequality

e  ,n = p(|   fn(x)     f (x)|r (cid:62)   ) (cid:54) 1
  

e(|   fn(x)     f (x)|r) =

en
  

.

16.2.3 parametric approaches and id173

instead of directly de   ning the class fn and using it as a constraint, it is possible to rather use a
penalization using some prior to favor    simple    or    regular    functions. a typical way to achieve this is by
using a parametric model y     f (x,   ) where        b parametrizes the function f (  ,   ) : x     y. the empirical
risk minimization procedure (16.5) now becomes

l(f (xi,   ), yi) +   nj(  ).

(16.7)

where j is some id173 function, for instance j = ||    ||2
2 (to avoid blowing-up of the parameter) or
j = ||    ||1 (to perform model selection, i.e. using only a sparse set of feature among a possibly very large
pool of p features). here   n > 0 is a id173 parameter, and it should tend to 0 when n     +   .
then one similarly de   nes the ideal parameter      as in (16.6) so that the limiting estimator as n     +   
is of the form   f = f (  ,     ) for      de   ned as

n(cid:88)

i=1

         argmin
     b

1
n

l(f (x,   ), y)d  (x, y) = e(x,y)     (l(f (x,   ), y).

(16.8)

(cid:90)

         argmin

  

x  y

prediction vs. estimation risks.
in this parametric approach, one could be interested in also studying
how close      is to     . this can be measured by controlling how fast some estimation error ||             || (for some
norm ||    ||) goes to zero. note however that in most cases, controlling the estimation error is more di   cult
than doing the same for the prediction error. in general, doing a good parameter estimation implies doing
a good prediction, but the converse is not true.

16.2.4 testing set and cross-validation

it is not possible to access en or e  ,n because the optimal   f is unknown.

in order to tune some
parameters of the methods (for instance the id173 parameter   ), one rather wants to minimize the
risk e(l(   f (x), y)), but this one should not be approximated using the training samples (xi, yi)i.

one thus rather ressorts to a second set of data (  xj,   yj)  n

j=1, called    testing set   . from a modelling
perspective, this set should also be distributed i.i.d. according to   . the validation (or testing) risk is then

l(   f (  xj),   yj)

(16.9)

r  n =

1
  n

  n(cid:88)

j=1

242

figure 16.8: conditional expectation.

which converges to e(l(   f (x), y)) for large   n. minimizing r  n to setup to some meta-parameter of the method
(for instance the id173 parameter   n) is called    cross validation    in the literature.

16.3 supervised learning: regression

in supervised learning, one has access to training data, consisting in pairs
(xi, yi)     x    y. here x = rp for simplicity. the goal is to infer some
relationship, typically of the form yi     f (xi) for some deterministic function
f : x     y, in order, when some un-observed data x without associated value
in y is given, to be able to    predict    the associated value using y = f (x).
if the set y is discrete and    nite, then this problem is called a supervised
classi   cation problem, and this is studied in section 16.4. the simplest example
being the binary classi   cation case, where y = {0, 1}. it    nds applications for
instance in medical diagnosis, where yi = 0 indicates a healthy subject, why
yi = 0 a pathological one. if y is continuous (the typical example being y = r),
then this problem is called a regression problem.

16.3.1 id75

figure 16.7: probabilistic
modelling.

we now specialize the empirical risk minimization approach to regression

problems, and even more speci   cally, we consider y = r and use a quadratic loss l(y, y(cid:48)) = 1

2|y     y(cid:48)|2.

note that non-id75 can be achieved using approximation in dictionary (e.g. polynomial
interpolation), and this is equivalent to using lifting to a higher dimensional space, and is also equivalent to
kernelization technics studied in section 16.5.

least square and conditional expectation.
if one do not put any constraint on f (beside being mea-
surable), then the optimal limit estimator   f (x) de   ned in (16.6) is simply averaging the values y sharing
the same x, which is the so-called conditional expectation. assuming for simplicity that    has some density
d  
dxdy with respect to a tensor product measure dxdy (for instance the lebegues mesure), one has

    x     x ,

  f (x) = e(y|x = x) =

(cid:82)
(cid:82)

y y d  
y d  

dxdy (x, y)dy
dxdy (x, y)dy

where (x, y) are distributed according to   .

in the simple case where x and y are discrete, denoting   x,y the id203 of (x = x, y = y), one has

(cid:80)
(cid:80)

y y  x,y
y   x,y

    x     x ,

  f (x) =

243

and it is unspeci   ed if the marginal of    along x vanishes at x.

the main issue is that this estimator   f performs poorly on    nite samples, and f (x) is actually unde   ned
if there is no sample xi equal to x. this is due to the fact that the class of functions is too large, and one
should impose some regularity or simplicity on the set of admissible f .

penalized linear models. a very simple class of models is obtained by
imposing that f is linear, and set f (x,   ) = (cid:104)x,   (cid:105), for parameters        b =
rp. note that one can also treat this way a   ne functions by remarking that
(cid:104)x,   (cid:105) +   0 = (cid:104)(x, 1), (  ,   0)(cid:105) and replacing x by (x, 1). so in the following,
without loss of generality, we only treat the vectorial (non-a   ne) case.

under the square loss, the regularized erm (16.7) is conveniently rewritten

as

         argmin
     b

1
2

(cid:104)   c  ,   (cid:105)     (cid:104)  u,   (cid:105) +   nj(  )

(16.10)

where we introduced the empirical correlation (already introduced in (16.1))
and observations

  c def.=

x   x =

1
n

1
n

xix   

i

and   u def.=

1
n

yixi =

x   y     rp.

1
n

n(cid:88)

i=1

n(cid:88)

i=1

figure 16.9: linear regres-
sion.

as n     0, under weak condition on   , one has with the law of large numbers the almost sure convergence

(16.11)
when considering   n     0, in some cases, one can shows that in the limit n     +   , one retrieves the
following ideal parameter

  c     c def.= e(x   x)

and   u     u def.= e(yx).

         argmin

{j(  ) ; c   = u} .

  

problem (16.10) is equivalent to the regularized resolution of inverse problems (10.9), with   c in place
of    and   u in place of      y. the major, and in fact only di   erence between machine learning and inverse
problems is that the linear operator is also noisy since   c can be viewed as a noisy version of c. the    noise
level   , in this setting, is 1/

   

n in the sense that
e(||   c     c||)     1   
n

and e(||  u     u||)     1   
n

,

under the assumption that e(y4) < +   , e(||x||4) < +    so ensure that one can use the central limit theorem
on x2 and xy. note that, although we use here linear estimator, one does not need to assume a    linear   
relation of the form y = (cid:104)x,   (cid:105) + w with a noise w independent from x, but rather hope to do    as best as
possible   , i.e. estimate a linear model as close as possible to     .

the general take home message is that it is possible to generalize theorems 31, 42 and 43 to cope with

the noise on the covariance matrix to obtain prediction convergence rates of the form

and estimation rates of the form

e(|(cid:104)     , x(cid:105)     (cid:104)     , x(cid:105)|2) = o(n     )

e(||              ||2) = o(n     (cid:48)

),

under some suitable source condition involving c and u. since the noise level is roughly n    1
2 , the ideal cases
are when    =   (cid:48) = 1, which is the so-called linear rate regime. it is also possible to derive sparsistency
theorems by extending theorem 44. for the sake of simplicity, we now focus our attention to quadratic
penalization, which is by far the most popular regression technic. it is fair to say that sparse (e.g. (cid:96)1 type)
methods are not routinely used in machine learning, because they typically do not improve the estimation
performances, and are mostly useful to do model selection (isolate a few useful coordinates in the features).
this is in sharp contrast with the situation for inverse problems in imaging sciences, where sparsity is a key
feature because it corresponds to a modelling assumption on the structure of the data to recover.

244

ridge regression (quadratic penalization). for j = ||  ||2/2, the estimator (16.10) is obtained in closed
form as

     = (x   x + n  nidp)   1x   y = (   c + n  nid)   1   u.

(16.12)

this is often called ridge regression in the literature. note that thanks to the woodbury formula, this
estimator can also be re-written as

     = x   (xx    + n  nidn)   1y.

(16.13)
if n (cid:29) p (which is the usual setup in machine learning), then (16.13) is preferable. in some cases however (in
particular when using rkhs technics), it makes sense to consider very large p (even in   nite dimensional),
so that (16.12) must be used.

if   n     0, then using (16.11), one has the convergence in expectation and id203

theorems 31 and 42 can be extended to this setting and one obtains the following result.

              = c +u.

theorem 51. if

for 0 <    (cid:54) 2, then

     = c   z where

||z|| (cid:54)   

e(||              ||2) (cid:54) c  2 1

  +1 n

      

  +1

for a constant c depending only on   .

(16.14)

(16.15)

it is important to note that, since      = c +u, the source condition (16.14) is always satis   ed. what trully
matters here is that the rate (16.15) does not depend on the dimension p of the features, but rather only on
  , which can be much smaller. this theoretical analysis actually works perfectly    ne in in   nite dimension
p =     (which is the setup considered when dealing with rkhs bellow).

16.4 supervised learning: classi   cation

we now focus on the case of discrete labels yi     y = {1, . . . , k}, which is the classi   cation setup. we
now detail two popular classi   cation methods: nearest neighbors and logistic classi   cation. it is faire to say
that a signi   cant part of successful applications of machine learning technics consists in using one of these
two approaches, which should be considered as baselines. note that the nearest neighbors approach, while
popular for classi   cation could as well be used for regression.

16.4.1 nearest neighbors classi   cation

probably the simplest method for supervised classi   cation is r nearest neighbors (r-nn), where r is
a parameter indexing the number of neighbors. increasing r is important to cope with noise and obtain
smoother decision boundaries, and hence better generalization performances. it should typically decreases as
the number of training samples n increases. despite its simplicity, id92 is surprisingly successful in practice,
specially in low dimension p.
the class   f (x)     y predicted for a point x is the one which is the most represented among the r points
(xi)i which are the closed to x. this is a non-parametric method, and   f depends on the numbers n of
samples (its    complexity    increases with n).

one    rst compute the euclidean distance between this x and all other xi in
the training set. sorting the distances generates an indexing    (a permutation
of {1, . . . , n}) such that

||x     x  (1)|| (cid:54) ||x     x  (2)|| (cid:54) . . . (cid:54) ||x     x  (n)||.

245

figure
neighbors.

16.10:

nearest

k = 1

k = 5

k = 10

k = 40

figure 16.11: k-nearest-neighbor classi   cation boundary function.

for a given r, one can compute the    local    histogram of classes around x

(cid:8)i ; y  (i)     {1, . . . , r}(cid:9) .

h(cid:96)(x) def.=

1
r

the decision class for x is then a maximum of the histogram

  f (x)     argmax

(cid:96)

h(cid:96)(x).

in practice, the parameter r can be setup through cross-validation, by minimizing the testing risk r  n

de   ned in (16.9), which typically uses a 0-1 loss for counting the number of mis-classi   cations

  n(cid:88)

r  n

def.=

  (  yj       f (xi))

j=1

where   (0) = 0 and   (s) = 1 if s (cid:54)= 0. of course the method extends to arbitrary metric space in place of
euclidean space rp for the features. note also that instead of explicitly sorting all the euclidean distance,
one can use fast nearest neighbor search methods.
figure 16.11 shows, for the iris dataset, the classi   cation domains (i.e. {x ; f (x) = (cid:96)} for (cid:96) = 1, . . . , k)

using a 2-d projection for vizualization. increasing r leads to smoother class boundaries.

16.4.2 two classes logistic classi   cation

the logistic classi   cation method (for 2 classes and multi-classes) is one of the most popular (maybe
   the    most) popular machine learning technics. this is due in large part of both its simplicity and because
it also outputs a id203 of belonging to each class (in place of just a class membership), which is useful
to (somehow . . . ) quantify the    uncertainty    of the estimation. note that logistic classi   cation is actually
called    id28    in the literature, but it is in fact a classi   cation method.

another very popular (and very similar) approach is support vector machine (id166). id166 is both more
di   cult to train (because the loss is non-smooth) and does not give class membership id203, so the
general rule of thumb is that logistic classi   cation is preferable.
to simplify the expression, classes indexes are set to yi     y = {   1, 1} in the following. note that for
logistic classi   cation, the prediction function f (  ,   )     [0, 1] outputs the id203 of belonging to the    rst
class, and not the class indexes. with a slight abuse of notation, we still denote it as f .
logistic classi   cation can be understood as a linear model as introduced in section 16.3.1, although the
decision function f (  ,   ) is not linear. indeed, one needs to    remap    the linear value (cid:104)x,   (cid:105) in the interval
[0, 1]. in logistic classi   cation, we de   ne the predicted id203 of x belonging to class with label    1 as

f (x,   ) def.=   ((cid:104)x,   (cid:105)) where

  (s) def.=

es

1 + es = (1 + e   s)   1,

(16.16)

246

r=1r=5r=10r=40r=1r=5r=10r=40r=1r=5r=10r=40r=1r=5r=10r=40figure 16.12:
classi   cation boundary.

1-d and 2-d logistic classi   cation, showing the impact of ||  || on the sharpness of the

which is often called the    logit    model. using a linear decision model might seems overly simplistic, but in
high dimension p, the number of degrees of freedom is actually enough to reach surprisingly good classi   cation
performances. note that the id203 of belonging to the second class is 1     f (x,   ) =   (   s). this
symmetry of the    function is important because it means that both classes are treated equally, which makes
sense for    balanced    problem (where the total mass of each class are roughly equal).
intuitively,   /||  || controls the separating hyperplane direction, while 1/||  || is roughly the fuzziness of the
the separation. as ||  ||     +   , one obtains sharp devision boundary, and logistic classi   cation ressembles
id166.

note that f (x,   ) can be interpreted as a single layer id88 with a logistic (sigmoid) rectifying unit,

more details on this in chapter 17.

since the (xi, yi) are modelled as i.i.d. variables, it makes sense to de   ne      from the observation using
a maximum likelihood, assuming that each yi conditioned on xi is a bernoulli variable with associated
id203 (pi, 1     pi) with pi = f (xi,   ). the id203 of observing yi     {0, 1} is thus, denoting
si = (cid:104)xi,   (cid:105)

p(y = yi|x = xi) = p1     yi

i

(1     pi)  yi =

(cid:18) esi

(cid:19)1     yi(cid:18) 1

(cid:19)  yi

1 + esi

1 + esi

where we denoted   yi = yi+1

2     {0, 1}.

one can then minimize minus the sum of the log of the likelihoods, which reads

    n(cid:88)

         argmin
     rp

log(p(y = yi|x = xi)) =

i=1

i=1

   (1       yi) log

esi

1 + esi

      yi log

1

1 + esi

some algebraic manipulations shows that this is equivalent to an erm-type form (16.7) with a logistic loss
function

         argmin
     rp

e(  ) =

1
n

l((cid:104)xi,   (cid:105), yi)

where the logistic loss reads

l(s, y) def.= log(1 + exp(   sy)).

(16.17)

(16.18)

problem (16.17) is a smooth convex minimization. if x is injective, e is also strictly convex, hence it has a
single global minimum.

figure (16.13) compares the binary (ideal) 0-1 loss, the logistic loss and the hinge loss (the one used for

id166).

247

n(cid:88)

n(cid:88)

i=1

figure 16.13: comparison of id168s.

figure 16.14:

in   uence on the separation distance between the class on the classi   cation id203.

re-writing the energy to minimize

e(  ) = l(x  , y) where l(s, y) =

(cid:88)

i

1
n

l(si, yi),

its gradient reads

   e(  ) = x      l(x  , y) where    l(s, y) =

(cid:12)   (   y (cid:12) s),

y
n

where (cid:12) is the pointwise multiplication operator, i.e. .* in matlab. once   ((cid:96)=0)     rp is initialized (for
instance at 0p), one step of id119 (13.2) reads

  ((cid:96)+1) =   ((cid:96))       (cid:96)   e(  ((cid:96))).

to understand the behavior of the method, in figure 16.14 we generate synthetic data distributed ac-
cording to a mixture of gaussian with an overlap governed by an o   set   . one can display the data overlaid
on top of the classi   cation id203, this highlight the separating hyperplane {x ; (cid:104)  , x(cid:105) = 0}.

16.4.3 multi-classes logistic classi   cation

of weight vectors    = (  (cid:96))k

the logistic classi   cation method is extended to an arbitrary number k of classes by considering a family
this allows one to model probabilistically the belonging of a point x     rp to the classes using the logit

(cid:96)=1, which are conveniently stored as columns of a matrix        rp  k.

model

this vector h(x)     [0, 1]k describes the id203 of x belonging to the di   erent classes, and(cid:80)

m e   (cid:104)x,   m(cid:105)

f (x,   ) =

(cid:96)

(cid:96) h(x)(cid:96) = 1.

(cid:18) e   (cid:104)x,   (cid:96)(cid:105)
(cid:80)

(cid:19)

248

-3-2-101230123binarylogistichingefigure 16.15: 2-d and 3-d pca vizualization of the digits images.

the computation of    is obtained by solving a maximum likelihood estimator

max
     rp  k

1
n

log(f (xi,   )yi)

where we recall that yi     y = {1, . . . , k} is the class index of the point xi.

this is conveniently rewritten as

e(  ) def.=

min
     rp  k

lse(x  )i     (cid:104)x  , d(cid:105)

where d     {0, 1}n  k is the binary class index matrices

and lse is the log-sum-exp operator

di,(cid:96) =

if

yi = (cid:96),

0otherwise.

(cid:33)

    rn.

exp(si,(cid:96))

lse(s) = log

n(cid:88)

i=1

i

(cid:88)
(cid:26) 1
(cid:32)(cid:88)

(cid:96)

note that in the case of k = 2 classes y = {   1, 1}, this model can be shown to be equivalent to the
two-classes logistic classi   cations methods exposed in section (16.4.2), with a solution vector being equal to
  1       2 (so it is computationally more e   cient to only consider a single vector as we did).

the computation of the lse operator is unstable for large value of si,(cid:96) (numerical over   ow, producing

nan), but this can be    xed by subtracting the largest element in each row, since

lse(s + a) = lse(s) + a

if a is constant along the rows. this is often referred to as the    lse trick    and is very important to use
in practice (in particular if some classes are well separated, since the corresponding   (cid:96) vector might become
large).

the gradient of the lse operator is the soft-max operator

   lse(s) = sm(s) def.=

(cid:19)

(cid:18) esi,(cid:96)(cid:80)

m esi,m

similarly to the lse, it needs to be stabilized by subtracting the maximum value along rows before compu-
tation.

once d matrix is computed, the gradient of e is computed as

   e(  ) =

1
n

x   (sm(x  )     d).

249

figure 16.16: results of digit classi   cation left: id203 h(x)(cid:96) of belonging to each of the 9    rst classes
(displayed over a 2-d pca space). right: colors re   ect id203 h(x) of belonging to classes.

and one can minimize e using for instance a id119 scheme.

to illustrate the method, we use a dataset of n images of size p = 8    8, representing digits from 0 to 9
(so there are k = 10 classes). figure 16.15 displays a few representative examples as well as 2-d and 3-d
pca projections. figure (16.16) displays the    fuzzy    decision boundaries by vizualizing the value of h(x)
using colors on an image regular grid.

16.5 kernel methods

linear methods are parametric and cannot generate complex regression or decision functions. the lin-
earity assumption is often too restrictive and in some case the geometry of the input functions or classes is
not well capture by these models. in many cases (e.g. for text data) the input data is not even in a linear
space, so one cannot even apply these model.

kernel method is a simple yet surprisingly powerful remedy for these issues. by lifting the features to
a high dimensional embedding space, it allows to generate non-linear decision and regression functions, but
still re-use the machinery (linear system solvers or id76 algorithm) of linear models. also,
by the use of the so-called    kernel-trick   , the computation cost does not depends on the dimension of the
embedding space, but of the number n of points. it is the perfect example of so-called    non-parametric   
methods, where the number of degrees of freedom (number of variables involved when    tting the model)
grows with the number of samples. this is often desirable when one wants the precisions of the result to
improve with n, and also to mathematically model the data using    continuous    models (e.g.
functional
spaces such as sobolev).

the general rule of thumb is that any machine learning algorithm which only makes use of inner products
(and not directly of the features xi themselves) can be    kernelized    to obtain a non-parametric algorithm.
this is for instance the case for linear and nearest neighbor regression, id166 classi   cation, logistic classi   ca-
tion and pca id84. we    rst explain the general machinery, and instantiate this in two
representative setup (ridge regression, nearest-neighbor regression and logistic classi   cation)

16.5.1 reproducing kernel hilbert space

we consider a general lifting    : x     rp       x =   (x)     h where h is a hilbert space. we denote
i=1 the    matrix    where each row is a lifted feature   (xi). for instance, if h = r   p is    nite
  x = (  x   
dimensional, one can view this as a matrix   x     rn     p, but the rows of the matrix can be in   nite dimensional
vectors.

def.=   (xi)   )n

i

250

the following proposition is the crux of the rkhs approaches. when using a id173 which is a
squared euclidean norm, ||  ||2h, it states that the solutions actually belongs to a data-driven linear sub-space
of dimension n. although the proof is straightforward, its implications are very profound, since it leads to
tractable algorithms even when using an in   nite dimensional lifting space h. as we elaborate next. it is
often called the    representer    theorem in rkhs theory.
proposition 53. the solution   (cid:63)     h of

is unique and can be written as

where q     rn is a solution of

||  ||2h

  
2

min

     h l(   x  , y) +
(cid:88)

   =   x   q(cid:63) =

i   (xi)     h
q(cid:63)

i

min
q   rn

l(kp, y) +

(cid:104)kq, q(cid:105)rn

  
2

(16.19)

(16.20)

(16.21)

where we de   ned

k def.=   x      x = ((cid:104)  (xi),   (xj)(cid:105)h)n

i,j=1     rn  n.

proof. the    rst order condition of (16.19) reads

0       x      l(   x     (cid:63), y) +     (cid:63) = 0

i.e. there exists u(cid:63)        l(   x     (cid:63), y) such that

which is the desired result.

  (cid:63) =     1
  

  x   u(cid:63)     im(   x   )

equation (16.20) expresses the fact that the solution only lives in the n dimensional space spanned by
the lifted observed points   (xi). a crucial by product of this results is that all the computations as well as
the prediction procedure can be expressed using the so-called kernel    : x    x     r associated to   

    (x, x(cid:48))     x 2,

  (x, x(cid:48)) def.= (cid:104)  (x(cid:48)),   (x(cid:48))(cid:105)h.

indeed, one has k = (  (xi, xj))i,j and the prediction operator, as a function of x and not   (x) (which makes
it non-linear) is a weighted sum of id81s centered at the xi

(cid:104)  x,   (cid:63)(cid:105)h =

i (cid:104)  (x),   (xi)(cid:105)h =
p(cid:63)

p(cid:63)
i   (xi, x).

(16.22)

i=1

i=1

this means that one actually never needs to manipulate quantities in h (which can be in   nite dimensional).
but more importantly, one can reverse the process, and instead of starting from a lifting   , directly
consider a kernel   (x, x(cid:48)). this is actually the way this is done in practice, since it is easier to design kernel
and think in term of their geometrical properties (for instance, one can sum kernels). in order for this to make
sense, the kernel needs to be positive de   nite, i.e. one should have that (  (xi, xj))i,j should be symmetric
positive de   nite for any choice of sampling points (xi)i. this can be shown to be equivalent to the existence
of a lifting function    generating the kernel. note that such a kernel can be de   ned on arbitrary space (not
necessarily euclidean).
when using the linear kernel   (x, y) = (cid:104)x, y(cid:105), one retrieves the linear models studied in the previous
section, and the lifting is trivial   (x) = x. a family of popular kernels are polynomial ones,   (x, x(cid:48)) =

251

n(cid:88)

n(cid:88)

   = 0.1

   = 0.5

   = 1

   = 5

figure 16.17: regression using a gaussian kernel.

((cid:104)x, y(cid:105) + c)a for a     n    and c > 0, which corresponds to a lifting in    nite dimension. for instance, for a = 2
and p = 2, one has a lifting in dimension 6

  (x, x(cid:48)) = (x1x(cid:48)
in euclidean spaces, the gaussian kernel is the most well known and used kernel

1 + c)2 = (cid:104)  (x),   (x(cid:48))(cid:105) where   (x) = (x2

1 + x1x(cid:48)

2x1x2,

1, x2
2,

   

   

   

2cx1,

2cx2, c)        r6.

(16.23)

  (x, y) def.= e

    ||x   y||2
2  2

.

the bandwidth parameter    > 0 is crucial and controls the locality of the model.
it is typically tuned
    ||x     ||2
through cross validation. it corresponds to an in   nite dimensional lifting x (cid:55)    e
2(  /2)2     l2(rp). another
related popular kernel is the laplacian kernel exp(   ||x    y||/  ). more generally, when considering translation
invariant kernels   (x, x(cid:48)) = k(x     x(cid:48)) on rp, being positive de   nite is equivalent to   k(  ) > 0 where   k is the
     and   (x) = h(x     )     l2(rp).
fourier transform, and the associated lifting is obtained by considering   h =

   

16.5.2 examples of kernelized algorithms

we illustrate this general machinery by applying it to three typical problems.

kernelized ridge regression. the simplest instantiation of this kernelization approach is when using the
2|y     y(cid:48)|2, which is the ridge regression problem studied in section 16.3.1. the obtain
square loss l(y, y(cid:48)) = 1
regression model (16.22) corresponds to approximating the data using a weighted sum of data-centered kernel
function   (xi,  ). when using a gaussian kernel (16.23), the bandwidth    controls the smoothness of the
approximation. this is illustrated in figure 16.17.
in this special case of a square loss, one can solve in closed form (16.21) by solving a n    n linear system

this expression matches exactly (16.13) when using k in place of   c

q(cid:63) = (kk +   k)   1ky = (k +   idn )   1y

kernelized logistic classi   cation. logistic classi   cation tries to separate the classes using a linear
separating hyperplane {x ; (cid:104)  , x(cid:105) = 0}. in order to generate a non-linear decision boundary, one can replace
the parametric linear model by a non-linear non-parametric model, thanks to kernelization. this allows in
particular to generate decision boundaries of arbitrary complexity.

in the two class problem, as detailed in section 16.4.2, one solves (16.21) using the logistic loss (16.18).
this can be for instance achieved by a id119 method. once the solution q(cid:63) is obtained, the
id203 of x belonging to the    rst class is then

n(cid:88)

  (

q(cid:63)
i   (xi, x)).

figure 16.18 illustrate such a non-linear decision function on a simple 2-d problem.

i=1

252

figure 16.18: non-linear classi   cation using a gaussian kernel.

kernelized nearest-neihbors.
it is also possible to extend nearest neighbor classi   cation (as detailed
in section 16.4.1) and regression over a lifted space by making use only of kernel evaluation, simply noticing
that

||  (xi)       (xj)||2h =   (xi, xi) +   (xj, xj)     2  (xi, xj).

kernel on strings.

[todo: write me]

253

-11bibliography

[1] p. alliez and c. gotsman. recent advances in compression of 3d meshes. in n. a. dodgson, m. s.
floater, and m. a. sabin, editors, advances in multiresolution for geometric modelling, pages 3   26.
springer verlag, 2005.

[2] p. alliez, g. ucelli, c. gotsman, and m. attene. recent advances in remeshing of surfaces.

in

aim@shape repport. 2005.

[3] amir beck. introduction to nonlinear optimization: theory, algorithms, and applications with mat-

lab. siam, 2014.

[4] stephen boyd, neal parikh, eric chu, borja peleato, and jonathan eckstein. distributed optimization
and statistical learning via the alternating direction method of multipliers. foundations and trends r(cid:13)
in machine learning, 3(1):1   122, 2011.

[5] stephen boyd and lieven vandenberghe. id76. cambridge university press, 2004.

[6] e. cand`es and d. donoho. new tight frames of curvelets and optimal representations of objects with

piecewise c2 singularities. commun. on pure and appl. math., 57(2):219   266, 2004.

[7] e. j. cand`es, l. demanet, d. l. donoho, and l. ying. fast discrete curvelet transforms. siam

multiscale modeling and simulation, 5:861   899, 2005.

[8] a. chambolle. an algorithm for total variation minimization and applications. j. math. imaging vis.,

20:89   97, 2004.

[9] antonin chambolle, vicent caselles, daniel cremers, matteo novaga, and thomas pock. an intro-
duction to total variation for image analysis. theoretical foundations and numerical methods for sparse
recovery, 9(263-340):227, 2010.

[10] antonin chambolle and thomas pock. an introduction to continuous optimization for imaging. acta

numerica, 25:161   319, 2016.

[11] s.s. chen, d.l. donoho, and m.a. saunders. atomic decomposition by basis pursuit. siam journal

on scienti   c computing, 20(1):33   61, 1999.

[12] f. r. k. chung. spectral id207. regional conference series in mathematics, american mathe-

matical society, 92:1   212, 1997.

[13] philippe g ciarlet. introduction `a l   analyse num  erique matricielle et `a l   optimisation. 1982.

[14] p. l. combettes and v. r. wajs. signal recovery by proximal forward-backward splitting. siam

multiscale modeling and simulation, 4(4), 2005.

[15] p. schroeder et al. d. zorin. subdivision surfaces in character animation. in course notes at siggraph

2000, july 2000.

295

[16] i. daubechies, m. defrise, and c. de mol. an iterative thresholding algorithm for linear inverse problems

with a sparsity constraint. commun. on pure and appl. math., 57:1413   1541, 2004.

[17] i. daubechies and w. sweldens. factoring wavelet transforms into lifting steps. j. fourier anal. appl.,

4(3):245   267, 1998.

[18] d. donoho and i. johnstone. ideal spatial adaptation via wavelet shrinkage. biometrika, 81:425   455,

dec 1994.

[19] heinz werner engl, martin hanke, and andreas neubauer. id173 of inverse problems, volume

375. springer science & business media, 1996.

[20] m. figueiredo and r. nowak. an em algorithm for wavelet-based image restoration. ieee trans.

image proc., 12(8):906   916, 2003.

[21] m. s. floater and k. hormann. surface parameterization: a tutorial and survey. in n. a. dodgson,
m. s. floater, and m. a. sabin, editors, advances in multiresolution for geometric modelling, pages
157   186. springer verlag, 2005.

[22] simon foucart and holger rauhut. a mathematical introduction to compressive sensing, volume 1.

birkh  auser basel, 2013.

[23] i. guskov, w. sweldens, and p. schr  oder. multiresolution signal processing for meshes.

in alyn
rockwood, editor, proceedings of the conference on computer graphics (siggraph99), pages 325   334.
acm press, august8   13 1999.

[24] a. khodakovsky, p. schr  oder, and w. sweldens. progressive geometry compression. in proceedings
of the computer graphics conference 2000 (siggraph-00), pages 271   278, new york, july 23   28
2000. acmpress.

[25] l. kobbelt.

3 subdivision. in sheila ho   meyer, editor, proc. of siggraph   00, pages 103   112, new

york, july 23   28 2000. acmpress.

[26] m. lounsbery, t. d. derose, and j. warren. multiresolution analysis for surfaces of arbitrary topological

type. acm trans. graph., 16(1):34   73, 1997.

[27] s. mallat. a wavelet tour of signal processing, 3rd edition. academic press, san diego, 2009.

[28] stephane mallat. a wavelet tour of signal processing: the sparse way. academic press, 2008.

[29] d. mumford and j. shah. optimal approximation by piecewise smooth functions and associated varia-

tional problems. commun. on pure and appl. math., 42:577   685, 1989.

[30] neal parikh, stephen boyd, et al. proximal algorithms. foundations and trends r(cid:13) in optimization,

1(3):127   239, 2014.

[31] gabriel peyr  e. l   alg`ebre discr`ete de la transform  ee de fourier. ellipses, 2004.

[32] j. portilla, v. strela, m.j. wainwright, and simoncelli e.p. image denoising using scale mixtures of

gaussians in the wavelet domain. ieee trans. image proc., 12(11):1338   1351, november 2003.

[33] e. praun and h. hoppe. spherical parametrization and remeshing. acm transactions on graphics,

22(3):340   349, july 2003.

[34] l. i. rudin, s. osher, and e. fatemi. nonlinear total variation based noise removal algorithms. phys.

d, 60(1-4):259   268, 1992.

[35] otmar scherzer, markus grasmair, harald grossauer, markus haltmeier, frank lenzen, and l sirovich.

variational methods in imaging. springer, 2009.

296

   

[36] p. schr  oder and w. sweldens. spherical wavelets: e   ciently representing functions on the sphere.

in proc. of siggraph 95, pages 161   172, 1995.

[37] p. schr  oder and w. sweldens. spherical wavelets: texture processing. in p. hanrahan and w. pur-

gathofer, editors, rendering techniques    95. springer verlag, wien, new york, august 1995.

[38] c. e. shannon. a mathematical theory of communication. the bell system technical journal,

27(3):379   423, 1948.

[39] a. she   er, e. praun, and k. rose. mesh parameterization methods and their applications. found.

trends. comput. graph. vis., 2(2):105   171, 2006.

[40] jean-luc starck, fionn murtagh, and jalal fadili. sparse image and signal processing: wavelets and

related geometric multiscale analysis. cambridge university press, 2015.

[41] w. sweldens. the lifting scheme: a custom-design construction of biorthogonal wavelets. applied and

computation harmonic analysis, 3(2):186   200, 1996.

[42] w. sweldens. the lifting scheme: a construction of second generation wavelets. siam j. math. anal.,

29(2):511   546, 1997.

297

