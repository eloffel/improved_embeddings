applications of id48s 

any sequence tagging problem 
       id39 

alan spoon , recently named newsweek president , 
said newsweek ad rates would increase in january .

       morpheme boundary detection 
       simple machine translation (no reordering) 

signs of a slowing economy are increasing

das blaue haus   
the blue  house

       id103  testing one

 

generative story for id48s 
       to create observation sequence o1o2   on 
      for each time step i, generate a latent state si 
conditioned on si-1 
 
 

state"transiaon"model""

      for each time step i, generate an observation 
symbol oi conditioned on si 

channel"model"

id48 decoding 

       given observation, find best sequence of 

latent states 
 
argmax
 
state sequences s
 
 

state"transiaon"model""

p(s |o) = argmax
state sequences s

p(s)p(o| s)

channel"model"

      what is the size of the search space (number 
of state sequences) for an observation of 
   
a m i c
g ! 
a m m i n
length n? 
r
g

n

y

d

o

r

p

id48 decoding example 

       i see this word from some language 

 
 
 

 

 

 

 

 

 "#$ 

       which symbols are vowels and which are 

consonants? 
      given: state (consonant-vowel) transition 
probabilities, channel "emission" probabilities 

viterbi algorithm 

prob"of"state"sequence"that"
maximizes"p(s)p(o|s)'where""

o"=""#""

and""

s"ends"with"c

v

c

#" "  #  $ 

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

p(c|#)p("|c)"
=0.7*0.08=0.056

v

c

#" "  #  $ 

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

v

c

0.056"

p(v|#)p("|v)"
=0.3*0.01=0.003

#" "  #  $ 

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

v

c

0.003"

0.056"

0.056*p(c|c)p(#|c)"

=0.056*0.4*0.02=0.000448"

0.003*p(c|v)*p(#|c)"

=0.003*0.9*0.02=0.000054

#" "  #  $ 

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

v

c

0.003"

0.056"

0.056*p(v|c)p(#|v)"

0.000448
=0.056*0.6*0.14=0.004704"

0.003*p(v|v)*p(#|v)"

=0.003*0.1*0.14=0.000042

#" "  #  $ 

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

v

c

0.003"

0.004704

0.056"

0.000448

#" "  #  $ 

0.000448*p(c|c)p($|c)"

=0.000448*0.4*0.07=0.0000125"

0.004704*p(c|v)*p($|c)"

=0.004704*0.9*0.07=0.000296

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

0.003"

0.004704

0.056"

0.000448 0.000296

v

c

#" "  #  $ 

0.000448*p(v|c)p($|v)"

=0.000448*0.6*0.20=0.00005376"

0.004704*p(v|v)*p($|v)"

=0.004704*0.1*0.20=0.0000941

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

0.003"

0.004704 0.000094

0.056"

0.000448 0.000296

v

c

#" "  #  $ 

given"these""
probabiliaes:""
p(c|#)=0.7"
p(c|c)=0.4""
p(v|v)=0.1""
"
p("|c)=0.08""
p("|v)=0.01""
p(#|c)=0.02""
p(#|v)=0.14""
p($|c)=0.07""
p($|v)=0.20""

viterbi algorithm 

0.003"

0.004704 0.000094

0.056"

0.000448 0.000296

v

c

take the highest 
id203 on 
the last column 
and trace back 
the pointers 

#" "  #  $ 

viterbi algorithm 

0.003"

0.004704 0.000094

0.056"

0.000448 0.000296

v

c

#" "  #  $ 

take the highest 
id203 on 
the last column 
and trace back 
the pointers 

c
$ 

viterbi algorithm 

0.003"

0.004704 0.000094

0.056"

0.000448 0.000296

v

c

#" "  #  $ 

take the highest 
id203 on 
the last column 
and trace back 
the pointers 

c
v
# 
$ 

viterbi algorithm 

0.003"

0.004704 0.000094

0.056"

0.000448 0.000296

v

c

#" "  #  $ 

take the highest 
id203 on 
the last column 
and trace back 
the pointers 

c
v
c
" 
# 
$ 

viterbi algorithm 

       complexity? 

for sequence of length t and id48 with n states 
       runtime is o(n2t): n x t cells, n computations per cell 
       space is o(nt) 

       why does it work? 
that is, how does it find 
 
 
without enumerating all exponentially many state 
sequences? 
       best state sequence for first few time steps will be part of 

p(s |o) = argmax
state sequences s

argmax
state sequences s

p(s)p(o| s)

convince"
yourself"
of"this."

eventual best state sequence. only change comes from 
transition id203 from time t to t+1 

id48s: three canonical problems 
       finding the best sequence of states for an 

observation 
      done: viterbi algorithm.    

       finding the total id203 of an 

observation 
      coming up on thursday: forward algorithm 

       estimating the transition and emission 

probabilities from a corpus 
      coming up next week: baum-welch 

why do we need to find the  

best state sequence? 

       id52: resolving ambiguity 

 
 
 

noun%%%verb%%%%%%%det%%%%%%%%%%%%noun%
%%%%%%%%%or%noun?%
i shot an elephant

       predicting pronunciations of letters 

%%%%%%aa%%%%%%%%%%%%%%%r%%%%%%%%%%%%%%%%%%s%
k%
or%s?%%%%or%ae?%%%%%%%or%sil?%%%%%%%%%%or%z?%%
c   a    r    s

questions about viterbi 

       difference between markov models and id48s? 
       what is the generative story for id48s 
       where does p(s)p(o|s) come from? 
       what is end result produced by viterbi? 
       where do the probabilities come from? 
       is there an algorithm-model relationship 

analogous to viterbi-id48? 

       what does the viterbi table really mean? 

why do we need to find  

the total id203 of observation? 
       can use id48s as a language model, 

instead of id165s 

       recall: language models assign 

probabilities to strings  
      text classification 
      random generation 
      machine translation 
      id103 

id48 observation id203 
       compute total id203 of observation 

as generated by the id48 
 
 
 p(o) =

=

p(s,o)
   
state sequences s

   

state sequences s

state%transifon%model%%

p(s)p(o| s)

channel%model%

id48 best state sequence 

       compare previous slide to this formula for 
the best state sequence for an observation 
(last class) 

state%transifon%model%%

argmax
state sequences s

p(s |o) = argmax
state sequences s

p(s)p(o| s)

channel%model%

recall: viterbi algorithm 

v

c

prob%of%state%sequence%that%
maximizes%p(s)p(o|s)'where%%

o%=%!"%%

and%%

s%ends%with%c

what small 
modification 
can we make to 
viterbi? 
 
change what 
each cell  
represents    

#% !  "  # 

total id203 of observation 
v

marginal%prob.%of%all%state%

sequences%where%%

c

o%=%!"%%

and%%

s%ends%with%c

#% !  "  # 

forward algorithm 

p(c|#)p(!|c)%
=0.7*0.08=0.056

v

c

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

forward algorithm 

v

c

0.056%

p(v|#)p(!|v)%
=0.3*0.01=0.003

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

forward algorithm 

v

c

0.003%

0.056%

0.056*p(c|c)p("|c)%

=0.056*0.4*0.02=0.000448%

0.003*p(c|v)*p("|c)%

=0.003*0.9*0.02=0.000054

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

0.056*p(v|c)p("|v)%
forward algorithm 
=0.056*0.6*0.14=0.004704%
0.003*p(v|v)*p("|v)%
=0.003*0.1*0.14=0.000042

v

c

0.003%

0.056%

0.000502%

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

0.000502*p(c|c)p(#|c)%
forward algorithm 
0.004746*p(c|v)*p(#|c)%

=0.000502*0.4*0.07=0.000014%

=0.004746*0.9*0.07=0.000299
0.004746%

0.003%

v

c

0.056%

0.000502%

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

=0.000502*0.6*0.2=0.000060%

0.000502*p(v|c)p(#|v)%
forward algorithm 
0.004746*p(v|v)*p(#|v)%
=0.004746*0.1*0.2=0.000095
0.004746%

0.003%

v

c

0.056%

0.000502% 0.000313%

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

forward algorithm 

0.003%

0.004746% 0.000016%

0.056%

0.000502% 0.000313%

v

c

#% !  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

forward algorithm 

v

c

0.003%

0.004746% 0.000016%

0.056%

0.000502% 0.000313%

total%id203%
of%!"#%under%
this%model:%
%
0.000016%
+%%
0.000313%
=%0.000329%

#% !  "  # 

where do the parameters come from? 

       who gives these to us? 

 

 

       ideally, we want to estimate them from data 

       remember the maximum likelihood 

principle? 
      find parameters that maximize id203 of 
data, compared to all other parameter values 

id113 
       id48 parameters:  

transition & emission probabilities 

       how to find id113 parameters? 

      suppose the data includes annotations of best 
hidden states for each time step 

cvcvc   vc   v   ccvvc   cvc
$%!"# &' " ()*"$ !"# 

easy: just take relative frequencies! 

(# times c+c, # times c+!, etc. and normalize.) 

id113 
       id48 parameters:  

transition & emission probabilities 

       how to find id113 parameters? 

      what if the data does not contain annotations 
of best hidden states (more realistic)? 

$%!"# &' " ()*"$ !"# 

id113 
       id48 parameters:  

transition & emission probabilities 

       how to find id113 parameters? 

      what if the data does not contain annotations 
of best hidden states (more realistic)? 
      can we get relative counts of state transitions 
and emissions then? 

$%!"# &' " ()*"$ !"# 

yes! 

unsupervised learning 
       learn structure from data without any 

annotations about the structure 
      eg: learn part-of-speech state transitions and 
emissions, given only the observed words 
 

       but why? 

1.    isn't this how we learn? 
2.    getting annotations is difficult, expensive, 

sometimes impossible 

what if    

           we had a corpus of observation 
annotated with the state sequences? 
      can estimate the id113 parameters by 
counting 
 

           we had the id48 parameters 

      can find the best state sequences for the 
observations in the corpus with viterbi 

expectation maximization 

1.    guess parameters of model 
2.    tag observations with best state sequence 
3.    re-estimate model parameters by counting 
4.    repeat: go to step 2. 

locking!parameters:!

if%we%like%some%of%the%original%model%parameters,%can%
ignore%the%data%and%just%not%change%those%parameters.%

soft expectation maximization 
1.    guess parameters of model 
2.    tag observations with best state sequence 
3.    re-estimate model parameters by counting 
4.    repeat: go to step 2. 

bad%guess%can%corner%model%

into%bad%local%maximum%

instead%of%gebng%counts%from%the%best%state%sequence,%

get%expected!counts!(weighted%average)%from%all%
possible%state%sequences%with%their%probabilifes%

building block: backward algorithm 

       forward algorithm computes total 

id203 of observation going left to right 
 

       backward algorithm computes total 

id203 of observation going right to left 

backward algorithm 

v

c

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

id203%of%starfng%at%

c%at%this%posifon%and%

generafng%the%

remainder%of%the%word%

!  "  # 

backward algorithm 

v

c

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

1%

1%

!  "  # 

backward algorithm 

v

c

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

p(c|c)*p(#|c)*1%%
=%0.4*0.07*1%=%0.028%
p(v|c)*p(#|v)*1%%
=%0.6*0.20*1%=%0.12%

1%

1%

!  "  # 

backward algorithm 

v

c

p(c|v)*p(#|c)*1%%
=%0.9*0.07*1%=%0.063%
p(v|v)*p(#|v)*1%%
=%0.1*0.20*1%=%0.02%

0.148%

1%

1%

!  "  # 

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

backward algorithm 

v

c

0.083%

0.148%

1%

1%

p(c|c)*p("|c)*0.148%%

!  "  # 

=%0.4*0.02*0.148%=%0.001184%

p(v|c)*p("|v)*0.083%%

=%0.6*0.14*0.083%=%0.006972%

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

backward algorithm 

=%0.9*0.02*0.148%=%0.002664%

p(c|v)*p("|c)*0.148%%

p(v|v)*p("|v)*0.083%%

=%0.1*0.14*0.083%=%0.001162%

v

c

given%these%%
probabilifes:%%
p(c|#)=0.7%
p(c|c)=0.4%%
p(v|v)=0.1%%
%
p(!|c)=0.08%%
p(!|v)=0.01%%
p("|c)=0.02%%
p("|v)=0.14%%
p(#|c)=0.07%%
p(#|v)=0.20%%

0.083%

0.00816%

0.148%

1%

1%

!  "  # 

backward algorithm 

v

c

total%
id203%
of%!"#%:%
%
p(c|#)%
*p(!|c)%
*0.00816%
+%%
p(v|#)%
*p(!|v)%
*0.00383%
=%0.00045696%
+%0.00001149%
=%0.000468%

0.00383%

0.083%

0.00816%

0.148%

1%

1%

!  "  # 

forward prob = backward prob 
v

0.004746% 1.55x10d4%

0.003%

total%%
id203%%
of%!"#%=%
%
4.68x10d4%

c

#%

0.056%

0.000502% 3.13x10d4%

d

a

y

soft expectation maximization 
1.    guess parameters of model 
2.    tag observations with best state sequence 
3.    re-estimate model parameters by counting 
4.    repeat: go to step 2. 

bad%guess%can%corner%model%

into%bad%local%maximum%

instead%of%gebng%counts%from%the%best%state%sequence,%

get%expected!counts!(weighted%average)%from%all%
possible%state%sequences%with%their%probabilifes%

forward backward algorithm 

       how to avoid enumerating all 

exponentially many state sequences in the 
e-step? 
 

      id145 magic! 

emission counts 

       what is the total id203 of all state 

sequences where  state2 is c given #$%? 
       gives expected number of times c + $ 

at state2 

c c c
c c v
v c v
v c c
#$% 

p(day,#ccc)#
+#p(day,#ccv)#
+#p(day,#vcv)#
+#p(day,#vcc)#

emission counts 

v

c

v

c

0.003#

0.004746#

1.55x10[4#

0.056#

0.000502#

3.13x10[4#

# 

$ 

% 

0.00383#

0.083#

0.00816#

0.148#

1#

1#

# 

$ 

% 

p(#$%,#state2#=#c)#
=#forward(state2#=#c)#
*#backward(state2#=#c)#
=#0.000502#
*#0.148#
=#7.4296x10[5#

p(state2#=#c|#$%)#
=#p(#$%,#state2#=#c)#
###############p(#$%)#
#
=#7.4296x10[5/4.68x10[4#
=#0.1588!

emission counts 

       what is the total id203 of all state 

sequences where  state2 is v given #$%? 
       gives expected number of times v + $  

at state2 

c v c
c v v
v v v
v v c
#$% 

p(day,#cvc)#
+#p(day,#cvv)#
+#p(day,#vvv)#
+#p(day,#vvc)#

emission counts 

v

c

v

c

0.003#

0.004746#

1.55x10[4#

0.056#

0.000502#

3.13x10[4#

# 

$ 

% 

0.00383#

0.083#

0.00816#

0.148#

1#

1#

# 

$ 

% 

p(#$%,#state2#=#v)#
=#forward(state2#=#v)#
*#backward(state2#=#v)#
=#0.004746#
*#0.083#
=#3.939x10[4#

p(state2#=#v|#$%)#
=#p(#$%,#state2#=#v)#
###############p(#$%)#
#
=#3.939x10[4/4.68x10[4#
=#0.8412!

emission counts 

v

c

v

0.003#

0.004746#

1.55x10[4#

0.056#

0.000502#

3.13x10[4#

# 

$ 

% 

0.00383#

0.083#

1#

0.00816#

sanity#check##2:##

c
p(statei=c|obs)#+p(statei=v|obs)#=#1#
% 

0.148#

$ 

# 

1#

p(#$%,#state2#=#v)#
=#forward(state2#=#v)#
*#backward(state2#=#v)#
=#0.004746#
*#0.083#
=#3.939x10[4#

p(state2#=#v|#$%)#
=#p(#$%,#state2#=#v)#
###############p(#$%)#
#
=#3.939x10[4/4.68x10[4#
=#0.8412!

expected emission counts 
       to get expected number of times v + $  

 

      for every string in corpus, at all positions i 
where $ occurs, compute total conditional 
id203 of v at position i 

      add up these probabilities  

how about transition counts? 
       what is the total id203 of all state 

sequences where state1 is c and state2 is v 
given #$%? 

       gives expected number of times c + v 

from state1 to state2 

c v c
c v v
#$% 

p(day,#cvc)#
+#p(day,#cvv)#

v

c

v

c

transition counts 
p(#$%,##
state1#=#c,#state2#=#v)#
=#forward(state1#=#c)#
*#p(v|c)#*#p($|v)#
*#backward(state2#=#v)#

0.004746#

0.000502#

1.55x10[4#

3.13x10[4#

0.003#

0.056#

# 

$ 

% 

0.00383#

0.083#

0.00816#

0.148#

1#

1#

# 

$ 

% 

=#0.056#
*#0.6#*#0.14#
*#0.083#=#3.904x10[4#
p(state1#=#c,#state2#=#v|
#$%)#
=#p(#$%,#state1#=#c,#
state2#=#v)/p(#$%)#
=#3.904x10[4/4.68x10[4#
=#0.8341#

v

c

v

c

transition counts 
p(#$%,##
state1#=#c,#state2#=#c)#
=#forward(state1#=#c)#
*#p(c|c)#*#p($|c)#
*#backward(state2#=#c)#

0.004746#

0.000502#

1.55x10[4#

3.13x10[4#

0.003#

0.056#

# 

$ 

% 

0.00383#

0.083#

0.00816#

0.148#

1#

1#

# 

$ 

% 

=#0.056#
*#0.4#*#0.02#
*#0.148#=#6.6304x10[5#

p(state1#=#c,#state2#=#c|
#$%)#
=#p(#$%,#state1#=#c,#
state2#=#c)/p(#$%)#
=#6.6304x10[5/4.68x10[4#
=#0.1417#

v

c

v

c

transition counts 
p(#$%,##
state1#=#v,#state2#=#c)#
=#forward(state1#=#v)#
*#p(c|v)#*#p($|c)#
*#backward(state2#=#c)#

0.004746#

0.000502#

1.55x10[4#

3.13x10[4#

0.003#

0.056#

# 

$ 

% 

0.00383#

0.083#

0.00816#

0.148#

1#

1#

# 

$ 

% 

=#0.003#
*#0.9#*#0.02#
*#0.148#=#7.992x10[6#
p(state1#=#v,#state2#=#c|
#$%)#
=#p(#$%,#state1#=#v,#
state2#=#c)/p(#$%)#
=#7.992x10[6/4.68x10[4#
=#0.0171#

v

c

v

c

transition counts 
p(#$%,##
state1#=#v,#state2#=#v)#
=#forward(state1#=#v)#
*#p(v|v)#*#p($|v)#
*#backward(state2#=#v)#

0.004746#

0.000502#

1.55x10[4#

3.13x10[4#

0.003#

0.056#

# 

$ 

% 

0.00383#

0.083#

0.00816#

0.148#

1#

1#

# 

$ 

% 

=#0.003#
*#0.1#*#0.14#
*#0.083#=#3.486x10[6#
p(state1#=#v,#state2#=#v|
#$%)#
=#p(#$%,#state1#=#v,#
state2#=#v)/p(#$%)#
=#3.486x10[6/4.68x10[4#
=#0.0074#

transition counts sanity check 
       p(state1 = c, state2 = c | #$%) = 0.1417 
       p(state1 = c, state2 = v | #$%) = 0.8341 
       p(state1 = v, state2 = v | #$%) = 0.0074 
       p(state1 = v, state2 = c | #$%) = 0.0171 
 
 

sanity#check##3:##

these#should#add#to#1#

expected transition counts 
       to get expected number of times c + v  

 

      for every string in corpus, at all positions i 
compute total id155 of c at 
position i and v at position i+1 

      add up these probabilities  

what are id48s used for? 

       any sequence modeling problem! 

      id52 
      id103 
      machine translation 
      gene prediction in computational biology 
      modeling human gait or  
facial expressions 
      financial analysis 

techniques are even more general 
       id145 

 

      shortest path in graphs  
(map route-finding, network routing) 
      audio alignment 
      ai game-playing 
      fast id127 

techniques are even more general 
       expectation maximization:  

any problem to find unknown structure of data 
 

      id91 
      id161 
      signal processing 
      financial modeling 

variants  

discrete(states!

con.nuous(states!

discrete(
observa.ons!

con.nuous(
observa.ons!

(

id48s(with((

discrete(emission(

probabili.es(

(text, biology)

id48s(with((

con.nuous(emission(

probabili.es(

(speech, vision)

kalman(filters,(
par.cle(filters(
(finance, 

gps, 

astronomy)

dynamic id110s 

       id48s assume each observation symbol is 

generated from a single state 

       what if there's more to the story? 

 

      word generated from part-of-speech, topic, 
sentiment, previous word 
 
      sound generated from phoneme, voice 
quality, gender 

machine learning fundamentals 
       id136: answering questions about data 
under an existing model 
      the cross id178 of a text under an id165 
language model 
      the best id48 state sequence for an 
observation 
      computing the perplexity of a text under a 
language model 
      id153 two strings under a set of edit 
costs 
      best parse of a sentence under a grammar 

machine learning fundamentals 
       learning: computing the parameters of a 

model from a collection of data 
      estimating an id165 language model from a 
corpus 
      estimating the channel model spelling error 
costs from a corpus of spelling mistakes 
      estimating the id48 transition and emission 
probabilities  

       our favorite strategy: maximum likelihood 

