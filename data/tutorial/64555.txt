unseen patterns: using latent-variable models for

natural language

institute for language, cognition and computation

shay cohen

school of informatics
university of edinburgh

july 13, 2017

thanks to...

natural language processing

main challenge: ambiguity

ambiguity: natural language utterances have many possible analyses

to

prune

out
need
thousands of
interpre-
tations even for simple
sentences (for example:
parse trees)

variability

many surface forms for a single meaning:

there is a bird singing
a bird standing on a branch singing
a bird opening its mouth to sing
a black and yellow bird singing in nature
a rufous whistler singing
a bird with a white patch on its neck

approach to nlp

1980s - rule based systems

1990s and onwards - data-driven (machine learning)

approach to nlp

1980s - rule based systems

1990s and onwards - data-driven (machine learning)

challenge: the labeled data bottleneck

labeled data bottleneck

approach to nlp since 1990s:
use labeled data. leads to the
labeled data bottleneck     never
enough data

how to solve the labeled data bottleneck?

ignore it

unsupervised learning

latent-variable modelling

incomplete data

xzyid96

(image from blei, 2011)

machine translation

    alignment is a hidden variable in translation models
    with deep learning, this is embodied in    attention    models

bayesian learning

with bayesian id136, the parameters are a    latent    variable:

p(  , h | x) =

(cid:82)

  

(cid:80)

p(  , h, x)

h p(  , h, x)

    popularized latent-variable models (where structure is missing as

well)

    has been used for problems in morphology, id40,

syntax, semantics and others

this talk in a nutshell

how do we learn from incomplete data?

    the case of syntactic parsing

    other uses of grammars for learning from incomplete data

    the canonical correlation principle and its uses

why parsing?

do we need to work on parsing when we can build direct    transducers?   
(such as with deep learning)

why parsing?

do we need to work on parsing when we can build direct    transducers?   
(such as with deep learning) yes!

    we develop algorithms that generalize to id170

    we see recent results that even with deep learning, incorporating
parse structures can help applications such as machine translation
(bastings et al., 2017; kim et al., 2017)

    we develop theories for syntax in language and test them

empirically

    one of the classic problems that demonstrates so well ambiguity in

natural language

ambiguity: example from abney (1996)

in a general way such speculation is epistemologically relevant,
as suggesting how organisms maturing and evolving in the
physical environment we know might conceivably end up
discoursing of abstract objects as we do

(quine, 1960, p. 123)

    should be interpreted: organisms might end up ...

ambiguity revisited

sin a general way suchspeculation isppepistemologically relevant as suggesting howpporganisms maturing and evolving in the physicalenvironmentabsolutenpwevpknowsobjects as we domightapptcplconceivably end updiscoursing of abstractlatent-state syntax (matsuzaki et al., 2005;
prescher, 2005; petrov et al., 2006)

s

s1

np

vp

=   

np3

vp2

d

n

v

p

d1

n2

v4

p1

the

dog

saw

him

the

dog

saw

him

improves the accuracy of a pid18 model from     70% to     90%.

latent-state syntax (matsuzaki et al., 2005;
prescher, 2005; petrov et al., 2006)

s

s1

np

vp

=   

np3

vp2

d

n

v

p

d1

n2

v4

p1

the

dog

saw

him

the

dog

saw

him

improves the accuracy of a pid18 model from     70% to     90%.

generative process

generative process

generative process

generative process

generative process

generative process

generative process

generative process

generative process

    derivational process is similar to that of pid18 together with

contextual information

    we read the grammar o    the treebank, but not the latent states

evolution of l-pid18s

evolution of l-pid18s

evolution of l-pid18s

evolution of l-pid18s

the estimation problem

goal: given a treebank, estimate rule probabilities, including for latent
states.

traditional way: use the id83:

    e-step - infer values for latent states using id145

    m-step - re-estimate the model parameters based on the values

inferred

local maxima with em

id76

non-id76

em    nds a local maximum of a non-convex objective

especially problematic with unsupervised learning

how problematic are local maxima?

for unsupervised learning, local maxima are a very serious problem:

for deep learning, can also be a problem. for l-pid18s, variability is
smaller

depends on the problem and the model

0510152025303520-3031-4041-5051-6061-7071-80frequency bracketing f1 ccm random restarts (length <= 10) basic intuition

at node vp:

s

outside tree o =

s

np

vp

d

n

v

p

the

dog

saw

him

inside tree t =

np

vp

d

n

the

dog

vp

v

p

saw

him

conditionally independent given the label and the hidden state

p(o, t|vp, h) = p(o|vp, h)    p(t|vp, h)

cross-covariance matrix

create a cross-covariance matrix and apply singular value decomposition
to get the latent space:

            

inside tree 1

inside tree 10

outside tree 1

outside tree 10

1
0
...
1

0
. . .
1 . . .
...
. . .
. . .
0

1
0
...
1

            

based on the method of moments     set up a set of equations that mix
moments and parameters and have a unique solution

previous work

the idea of using a co-ocurrence matrix to extract latent information is
an old idea. it has been used for:

    learning id48 and    nite state automata (hsu et

al., 2012; balle et al., 2013)

    learning id27s (dhillon et al., 2011)
    learning dependency and other types of grammars (bailly et al.,

2010; luque et al., 2012; dhillon et al., 2012)

    learning document-topic structure (anandkumar et al., 2012)

much of this work falls under the use of canonical correlation analysis
(hotelling, 1935)

feature functions

need to de   ne feature functions for inside and outside trees

p

v

  ( vp
  ( s

saw

him

np

d

n

) = (0,. . . ,1,. . . ,1,0,1,0)
vp ) = (0,. . . ,1,. . . ,0,0,0,1)

the

dog

inside features used

consider the vp node in the following tree:

s

np

vp

d

n

v

np

the

cat

saw

d

n

the

dog

the inside features consist of:

    the pairs (vp, v) and (vp, np)
    the rule vp     v np
    the tree fragment (vp (v saw) np)
    the tree fragment (vp v (np d n))
    the pair of head part-of-speech tag with vp: (vp, v)
    the width of the subtree spanned by vp: (vp, 2)

outside features used

consider the d node in
the following tree:

s

np

vp

d

n

v

np

the

cat

saw

d

n

the

dog

the outside features consist of:

    the fragments

np

d   

,
n

vp

and

s

v

np

d   

n

np

vp

v

np

    the pair (d, np) and triplet (d, np, vp)
    the pair of head part-of-speech tag with d: (d, n)
    the widths of the spans left and right to d: (d, 3) and (d, 1)

n

d   

outside features used

consider the d node in
the following tree:

s

np

vp

d

n

v

np

the

cat

saw

d

n

the

dog

the outside features consist of:

    the fragments

np

d   

,
n

vp

and

s

v

np

d   

n

np

vp

v

np

    the pair (d, np) and triplet (d, np, vp)
    the pair of head part-of-speech tag with d: (d, n)
    the widths of the spans left and right to d: (d, 3) and (d, 1)

n

d   

final results on multilingual parsing

narayan and cohen (2016):

language

berkeley

spectral

basque
french
german
hebrew

hungarian

korean
polish
swedish

74.7
80.4
78.3
87.0
85.2
78.6
86.8
80.6

cluster svd
81.4
80.5
79.1
75.6
78.2
76.0
89.0
87.2
89.2
88.4
80.0
78.4
91.8
91.2
80.9
79.4

parsing is far from being solved in the multilingual setting

what do we learn?

closed-word tags essentially do lexicalization:

state frequent words

in (preposition)

of   323
about   248
than   661, as   648, because   209
from   313, at   324
into   178
over   122
under   127

0
1
2
3
4
5
6

what do we learn?

state frequent words
dt (determiners)
these   105
some   204
that   190
both   102
any   613
the   574
those   247, all   242
all   105
another   276, no   211

0
1
2
3
4
5
6
7
8

what do we learn?

state frequent words
cd (numbers)

0
1

0
1
2
3
4
5

rb (adverb)

8   132
million   451, billion   248
up   175
as   271
not   490, n   t   2695
not   236
only   159
well   129

what do we learn?

state frequent words

cc (conjunction)
but   255
and   101
and   218
but   196
or   162
and   478

0
1
2
3
4
5

what do we learn?

example latent for np:

       james mccall , vice president , materials , at battelle , a

technology and management-research giant based in columbus ,
ohio   

       frank kline jr. , partner in lambda funds , a beverly hills ,
       allen hadhazy , senior analyst at the institute for econometric

calif. , venture capital concern   

research , fort lauderdale , fla. , which publishes the new issues
newsletter on ipos   
       a group of investment banks headed by first boston corp. and
co-managed by goldman , sachs & co. , merrill lynch capital
markets , morgan stanley & co. , and salomon brothers inc   
       charles j. o   connell , deputy district director in los angeles of
the california department of transportation , nicknamed caltrans   
       francis j. mcneil , who , as deputy assistant secretary of state for
inter-american a   airs ,    rst ran across reports about mr. noriega
in 1977   

what do we learn?

example latent state for np:
   aug. 30 , 1988   ,    aug. 31 , 1987   ,    dec. 31 , 1988   ,    oct. 16 ,
1996   ,    oct. 1 , 1999   ,    oct. 1 , 2019   ,    nov. 8 , 1996   ,    oct. 15 ,
1999   ,    april 30 , 1988   ,    nov. 8 , 1994   
another state:

       american building maintenance industries inc. ,

san francisco , provider of maintenance services , annual revenue
of $ 582 million , nyse ,   

       diasonics inc. , south san francisco , maker of magnetic
resonance imaging equipment , annual sales of $ 281 million ,
amex ,   

       everex systems inc. , fremont , maker of personal

computers and peripherals , annual sales of $ 377 million , otc ,   

       anthem electronics inc. , san jose , distributor of

electronic parts , annual sales of about $ 300 million , nyse ,   

lpid18viewer

if you are interested in further looking at such patterns for other
languages (english, french, german, hebrew, hungarian, korean,
polish, swedish, basque), consider visiting
http://cohort.inf.ed.ac.uk/lpid18viewer/index.php.

this talk in a nutshell

how do we learn from incomplete data?

    the case of syntactic parsing

    other uses of grammars for learning from incomplete data

    the canonical correlation principle and its uses

a di   erent perspective

a di   erent perspective

a di   erent perspective

a di   erent perspective

a di   erent perspective

2010; socher et al., 2013)

    related to neural network models with grammars (socher et al.,
    also related to compositional id65 (baroni and
bernardi, 2014; grefenstette and sadrzadeh, 2010; coecke et al.,
2010)

id53 (narayan et al., 2016)

id53 (narayan et al., 2016)

discussion forums

discussion forums

p0 bob: when i play a recorded video on my camera,
it looks and sounds    ne. on my computer,
it plays at a really fast rate and sounds like
alvin and the chipmunks!
i   d    nd and install the machine   s latest audio
driver.

p1 kate:

p2 mary: the motherboard supplies the clocks for au-
so update the audio and

dio feedback.
motherboard drivers.

p3 chris: another    ne mess in audio is volume and

speaker settings. you checked these?

p4 jane: yes, under speaker settings, look for hard-
ware acceleration. turning it o    worked for
me.

p5 matt: audio drivers are at this link. rather than
just audio drivers, i would also just do all
drivers.

p0 bob: when i play a recorded video on my camera,
it looks and sounds    ne. on my computer,
it plays at a really fast rate and sounds like
alvin and the chipmunks!
i   d    nd and install the machine   s latest audio
driver.

p1 kate:

p2 mary: the motherboard supplies the clocks for au-
so update the audio and

dio feedback.
motherboard drivers.

p3 chris: another    ne mess in audio is volume and

speaker settings. you checked these?

p4 jane: yes, under speaker settings, look for hard-
ware acceleration. turning it o    worked for
me.

p5 matt: audio drivers are at this link. rather than
just audio drivers, i would also just do all
drivers.

p0 bob: when i play a recorded video on my camera,
it looks and sounds    ne. on my computer,
it plays at a really fast rate and sounds like
alvin and the chipmunks!
i   d    nd and install the machine   s latest audio
driver.

p1 kate:

p2 mary: the motherboard supplies the clocks for au-
so update the audio and

dio feedback.
motherboard drivers.

p3 chris: another    ne mess in audio is volume and

speaker settings. you checked these?

p4 jane: yes, under speaker settings, look for hard-
ware acceleration. turning it o    worked for
me.

p5 matt: audio drivers are at this link. rather than
just audio drivers, i would also just do all
drivers.

p0 bob: when i play a recorded video on my camera,
it looks and sounds    ne. on my computer,
it plays at a really fast rate and sounds like
alvin and the chipmunks!
i   d    nd and install the machine   s latest audio
driver.

p1 kate:

p2 mary: the motherboard supplies the clocks for au-
so update the audio and

dio feedback.
motherboard drivers.

p3 chris: another    ne mess in audio is volume and

speaker settings. you checked these?

p4 jane: yes, under speaker settings, look for hard-
ware acceleration. turning it o    worked for
me.

p5 matt: audio drivers are at this link. rather than
just audio drivers, i would also just do all
drivers.

conversation trees

(louis and cohen, 2015)

conversation trees

(louis and cohen, 2015)

this talk in a nutshell

how do we learn from incomplete data?

    the case of syntactic parsing

    other uses of grammars for learning from incomplete data

    the canonical correlation principle and its uses

canonical correlation analysis

    assume a    confounding    variable that explains two separate views

    correlation between x and y gives z     the two are independent

given z

    in the case of l-pid18s: x and y are inside and outside trees

    where else can this principle be used?

xzyid27s

            

mouse

cat

the (cid:63) chased

a (cid:63) ran

1
0
...
1

0 . . .
1
. . .
...
. . .
0 . . .

1
0
...
1

            

    co-occurrence matrix of words and contexts (   the cat chased   ,

   the mouse chased   )

    apply cca on this matrix to get id27s (dhillon et al.,

2011)

    inject prior knowledge into matrix (osborne et al., 2016)

canonical correlation id136

jenny is holding an owl.

    requires also generation (using sampling techniques)

    the id203 of text we sample is proportional to the    similarity   

of the text to the image

example predictions

good predictions:

mike and jenny are camping mike is holding a bat

jenny is throwing the frisbee

bad predictions:

mike is kicking a blass

jenny wants the bear

the rocket is behind mike

unsupervised parsing

the dog, true to form, chased the cat.

(parikh et al., 2014)

    very challenging problem
    sensitive to local maxima with existing techniques such as em
    what if the tree for each pair of words in the sentence is a latent,

confounding, hierarchical variable?

dogchasedz =conclusion

latent-variable grammars are useful for problems outside of syntax

    their symbolic component is interpretable

    their probabilistic component helps reasoning under uncertainty

    latent variables help detect unseen patterns

i have shown you how grammars can be used for several problems, and
also how the principle behind learning latent-variable grammars can be
used for other problems.

references i

steven abney.
statistical methods and linguistics.
the balancing act: combining symbolic and statistical approaches
to language, pages 1   26, 1996.

anima anandkumar, dean p foster, daniel j hsu, sham m
kakade, and yi-kai liu.
a spectral algorithm for id44.
in advances in neural information processing systems, pages
917   925, 2012.

r. bailly, a. habrard, and f. denis.
a spectral approach for probabilistic grammatical id136 on trees.

in proceedings of alt, 2010.

references ii

borja balle, xavier carreras, franco m luque, and ariadna
quattoni.
spectral learning of weighted automata.
machine learning, 96(1-2):33   63, 2014.

marco baroni, ra   aela bernardi, and roberto zamparelli.
frege in space: a program of compositional distributional
semantics.
lilt (linguistic issues in language technology), 9, 2014.

joost bastings, ivan titov, wilker aziz, diego marcheggiani, and
khalil sima   an.
graph convolutional encoders for syntax-aware neural machine
translation.
arxiv preprint arxiv:1704.04675, 2017.

references iii

d. m. blei, a. ng, and m. jordan.
id44.
journal of machine learning research, 3:993   1022, 2003.

bob coecke, mehrnoosh sadrzadeh, and stephen clark.
mathematical foundations for a compositional distributional model
of meaning.
arxiv preprint arxiv:1003.4394, 2010.

s. b. cohen.
latent-variable pid18s: background and applications.
in proceedings of acl, 2017.

s. b. cohen, k. stratos, m. collins, d. f. foster, and l. ungar.
spectral learning of latent-variable pid18s.
in proceedings of acl, 2012.

references iv

s. b. cohen, k. stratos, m. collins, d. p. foster, and l. ungar.
experiments with spectral learning of latent-variable pid18s.
in proceedings of naacl, 2013.

shay b. cohen.
bayesian analysis in natural language processing.
synthesis lectures on human language technologies. morgan and
claypool, 2016.

m. collins.
head-driven statistical models for natural language processing.
computational linguistics, 29:589   637, 2003.

a. dempster, n. laird, and d. rubin.
id113 from incomplete data via the em
algorithm.
journal of the royal statistical society b, 39:1   38, 1977.

references v

p. s. dhillon, j. rodu, m. collins, d. p. foster, and l. h. ungar.
spectral id33 with latent variables.
in proceedings of conll-emnlp, 2012.

paramveer s dhillon, dean p foster, and lyle h ungar.
eigenwords: spectral id27s.
journal of machine learning research, 16:3035   3078, 2015.

j. eisner and g. satta.
e   cient parsing for bilexical context-free grammars and head
automaton grammars.
in proceedings of acl, 1999.

references vi

edward grefenstette and mehrnoosh sadrzadeh.
experimental support for a categorical compositional distributional
model of meaning.
in proceedings of the conference on empirical methods in natural
language processing, pages 1394   1404. association for
computational linguistics, 2011.

h hotelling.
canonical correlation analysis (cca).
journal of educational psychology, 1935.

d. hsu, s. m. kakade, and t. zhang.
a spectral algorithm for learning id48.
journal of computer and system sciences, 78(5):1460   1480, 2012.

references vii

helen jiang, nikos papasarantopoulos, and shay b. cohen.
canonical correlation id136 for mapping abstract scenes to text.
technical report, 2016.

m. johnson.
pid18 models of linguistic tree representations.
computational linguistics, 24(4):613   632, 1998.

yoon kim, carl denton, luong hoang, and alexander m rush.
structured attention networks.
arxiv preprint arxiv:1702.00887, 2017.

d. klein and c. d. manning.
accurate unlexicalized parsing.
in proceedings of acl, 2003.

references viii

a. louis and s. b. cohen.
conversation trees: a grammar model for topic structure in forums.
in proceedings of emnlp, 2015.

f. m. luque, a. quattoni, b. balle, and x. carreras.
spectral learning for non-deterministic id33.
in proceedings of eacl, 2012.

t. matsuzaki, y. miyao, and j. tsujii.
probabilistic id18 with latent annotations.
in proceedings of acl, 2005.

s. narayan and s. b. cohen.
optimizing spectral learning for parsing.
in proceedings of acl, 2016.

references ix

shashi narayan, siva reddy, and shay b. cohen.
paraphrase generation from latent-variable pid18s for semantic
parsing.
in proceedings of iid86, 2015.

a. p. parikh, s. b. cohen, and e. xing.
spectral unsupervised parsing with additive tree metrics.
in proceedings of acl, 2014.

s. petrov, l. barrett, r. thibaux, and d. klein.
learning accurate, compact, and interpretable tree annotation.
in proceedings of coling-acl, 2006.

d. prescher.
inducing head-driven pid18s with latent heads: re   ning a
tree-bank grammar for parsing.
in proceedings of ecml, 2005.

references x

a. saluja, c. dyer, and s. b. cohen.
latent-variable synchronous id18s for hierarchical translation.
in proceedings of emnlp, 2014.

r. socher, j. bauer, c. d. manning, and a. y. ng.
parsing with compositional vector grammars.
in proceedings of acl, 2013.

r. socher, c. d. manning, and a. y. ng.
learning continuous phrase representations and syntactic parsing
with id56s.
in proceedings of the nips deep learning and unsupervised
id171 workshop, 2010.

