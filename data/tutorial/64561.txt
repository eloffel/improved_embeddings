a class of submodular functions for

document summarization

hui lin, je    bilmes

university of washington, seattle

dept. of electrical engineering

june 20, 2011

lin and bilmes

submodular summarization

june 20, 2011

1 / 29

extractive document summarization

the    gure below represents the sentences of a document

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

extractive document summarization

we extract sentences (green) as a summary of the full document

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

extractive document summarization

we extract sentences (green) as a summary of the full document

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

extractive document summarization

we extract sentences (green) as a summary of the full document

the summary on the left is a subset of the summary on the right.

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

   extractive document summarization

we extract sentences (green) as a summary of the full document

the summary on the left is a subset of the summary on the right.
consider adding a new (blue) sentence to each of the two summaries.

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

   extractive document summarization

we extract sentences (green) as a summary of the full document

the summary on the left is a subset of the summary on the right.
consider adding a new (blue) sentence to each of the two summaries.
the marginal (incremental) bene   t of adding the new (blue) sentence
to the smaller (left) summary is no more than the marginal bene   t of
adding the new sentence to the larger (right) summary.

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

   extractive document summarization

we extract sentences (green) as a summary of the full document

the summary on the left is a subset of the summary on the right.
consider adding a new (blue) sentence to each of the two summaries.
the marginal (incremental) bene   t of adding the new (blue) sentence
to the smaller (left) summary is no more than the marginal bene   t of
adding the new sentence to the larger (right) summary.
diminishing returns     submodularity

lin and bilmes

submodular summarization

june 20, 2011

2 / 29

   background on submodularity

outline

1 background on submodularity

2 problem setup and algorithm

3 submodularity in summarization

4 new class of submodular functions for document summarization

5 experimental results

6 summary

lin and bilmes

submodular summarization

june 20, 2011

3 / 29

background on submodularity

submodular set functions

there is a    nite sized    ground set    of elements v

we use set functions of the form f : 2v     r
a set function f is monotone nondecreasing if    r     s, f (r)     f (s).

de   nition of submodular functions
for any r     s     v and k     v , k /    s, f (  ) is submodular
if

f (s + k)     f (s)     f (r + k)     f (r)

this is known as the principle of diminishing returns

lin and bilmes

submodular summarization

june 20, 2011

4 / 29

srbackground on submodularity

example: number of colors of balls in urns

given a set a of colored balls

f (a): the number of distinct colors contained in the urn
the incremental value of an object only diminishes in a larger
context (diminishing returns).

lin and bilmes

submodular summarization

june 20, 2011

5 / 29

f(r)=f()=3f(s)=f()=4background on submodularity

example: number of colors of balls in urns

given a set a of colored balls

f (a): the number of distinct colors contained in the urn
the incremental value of an object only diminishes in a larger
context (diminishing returns).

lin and bilmes

submodular summarization

june 20, 2011

5 / 29

f(r)=f()=3f(s)=f()=4f(r+k)=f()=4+f(+k)=f()=4+sbackground on submodularity

why is submodularity attractive?

lin and bilmes

submodular summarization

june 20, 2011

6 / 29

background on submodularity

why is submodularity attractive?

why is convexity attractive?

how about submodularity:

lin and bilmes

submodular summarization

june 20, 2011

6 / 29

background on submodularity

why is submodularity attractive?

why is convexity attractive?

convexity appears in many mathematical models in economy,
engineering and other sciences.

minimum can be found e   ciently.

convexity has many nice properties, e.g. convexity is preserved under
many natural operations and transformations.

how about submodularity:

lin and bilmes

submodular summarization

june 20, 2011

6 / 29

background on submodularity

why is submodularity attractive?

why is convexity attractive?

convexity appears in many mathematical models in economy,
engineering and other sciences.

minimum can be found e   ciently.

convexity has many nice properties, e.g. convexity is preserved under
many natural operations and transformations.

how about submodularity:

submodularity arises in many areas: combinatorics, economics, game
theory, operation research, machine learning, and (now) natural
language processing.

minimum can be found in polynomial time

submodularity has many nice properties, e.g. submodularity is
preserved under many natural operations and transformations (e.g.
scaling, addition, convolution, etc.)

lin and bilmes

submodular summarization

june 20, 2011

6 / 29

problem setup and algorithm

outline

1 background on submodularity

2 problem setup and algorithm

3 submodularity in summarization

4 new class of submodular functions for document summarization

5 experimental results

6 summary

lin and bilmes

submodular summarization

june 20, 2011

7 / 29

problem setup and algorithm

problem setup

the ground set v corresponds to all the sentences in a document.
extractive document summarization: select a small subset s     v
that accurately represents the entirety (ground set v ).

lin and bilmes

submodular summarization

june 20, 2011

8 / 29

problem setup and algorithm

problem setup

the ground set v corresponds to all the sentences in a document.
extractive document summarization: select a small subset s     v
that accurately represents the entirety (ground set v ).
the summary is usually required to be length-limited.

ci : cost (e.g., the number of words in sentence i),
b: the budget (e.g., the largest length allowed),

knapsack constraint: (cid:80)

i   s ci     b.

lin and bilmes

submodular summarization

june 20, 2011

8 / 29

problem setup and algorithm

problem setup

the ground set v corresponds to all the sentences in a document.
extractive document summarization: select a small subset s     v
that accurately represents the entirety (ground set v ).
the summary is usually required to be length-limited.

ci : cost (e.g., the number of words in sentence i),
b: the budget (e.g., the largest length allowed),

knapsack constraint: (cid:80)

i   s ci     b.

a set function f : 2v     r measures the quality of the summary s,

thus, the summarization problem is formalized as:

problem (document summarization optimization problem)

   

s

    argmax
s   v

f (s) subject to:

ci     b.

(1)

(cid:88)

i   s

lin and bilmes

submodular summarization

june 20, 2011

8 / 29

problem setup and algorithm

a practical algorithm for large-scale summarization

when f is both monotone and submodular:

a greedy algorithm with partial enumeration (sviridenko, 2004),
theoretical guarantee of near-optimal solution, but not practical for
large data sets.

lin and bilmes

submodular summarization

june 20, 2011

9 / 29

problem setup and algorithm

a practical algorithm for large-scale summarization

when f is both monotone and submodular:

a greedy algorithm with partial enumeration (sviridenko, 2004),
theoretical guarantee of near-optimal solution, but not practical for
large data sets.
a greedy algorithm (lin and bilmes, 2010): near-optimal with
theoretical guarantee, and practical/scalable!

we choose next element with largest ratio of gain over scaled cost:

k     argmax

i   u

f (g     {i})     f (g )

(ci )r

.

(2)

lin and bilmes

submodular summarization

june 20, 2011

9 / 29

problem setup and algorithm

a practical algorithm for large-scale summarization

when f is both monotone and submodular:

a greedy algorithm with partial enumeration (sviridenko, 2004),
theoretical guarantee of near-optimal solution, but not practical for
large data sets.
a greedy algorithm (lin and bilmes, 2010): near-optimal with
theoretical guarantee, and practical/scalable!

we choose next element with largest ratio of gain over scaled cost:

k     argmax

i   u

f (g     {i})     f (g )

(ci )r

.

(2)

scalability: the argmax above can be solved by o(log n) calls of f on
average, thanks to submodularity
integer id135 (ilp) takes 17 hours vs. greedy which takes
< 1 second!!

lin and bilmes

submodular summarization

june 20, 2011

9 / 29

problem setup and algorithm

objective function optimization: performance in practice

figure: the plots show the achieved objective function value as the number of
selected sentences grows. the plots stop when in each case adding more
sentences violates the budget.

lin and bilmes

submodular summarization

june 20, 2011

10 / 29

020406080100120140024681012opmalr=0r=0.5r=1r=1.5number of sentences in the summaryeulav noitcnuf evitcejboexact solutionsubmodularity in summarization

outline

1 background on submodularity

2 problem setup and algorithm

3 submodularity in summarization

4 new class of submodular functions for document summarization

5 experimental results

6 summary

lin and bilmes

submodular summarization

june 20, 2011

11 / 29

submodularity in summarization

mmr is non-monotone submodular

maximal margin relevance (mmr, carbonell and goldstein, 1998):

mmr is very popular in document summarization.

mmr corresponds to an objective function which is submodular but
non-monotone (see paper for details).

therefore, the greedy algorithm   s performance guarantee does not
apply in this case (since mmr is not monotone).

lin and bilmes

submodular summarization

june 20, 2011

12 / 29

submodularity in summarization

mmr is non-monotone submodular

maximal margin relevance (mmr, carbonell and goldstein, 1998):

mmr is very popular in document summarization.

mmr corresponds to an objective function which is submodular but
non-monotone (see paper for details).

therefore, the greedy algorithm   s performance guarantee does not
apply in this case (since mmr is not monotone).
moreover, the greedy algorithm of mmr does not take cost into
account, and therefore could lead to solutions that are signi   cantly
worse than the solutions found by the greedy algorithm with scaled
cost.

lin and bilmes

submodular summarization

june 20, 2011

12 / 29

submodularity in summarization

mmr is non-monotone submodular

maximal margin relevance (mmr, carbonell and goldstein, 1998):

mmr is very popular in document summarization.

mmr corresponds to an objective function which is submodular but
non-monotone (see paper for details).

therefore, the greedy algorithm   s performance guarantee does not
apply in this case (since mmr is not monotone).
moreover, the greedy algorithm of mmr does not take cost into
account, and therefore could lead to solutions that are signi   cantly
worse than the solutions found by the greedy algorithm with scaled
cost.

mmr-like approaches:

non-monotone because summary redundancy is penalized negatively.

lin and bilmes

submodular summarization

june 20, 2011

12 / 29

submodularity in summarization

concept-based approach

concepts: id165s, keywords, etc.

maximizes the weighted credit of concepts covered the summary

lin and bilmes

submodular summarization

june 20, 2011

13 / 29

submodularity in summarization

concept-based approach

concepts: id165s, keywords, etc.
maximizes the weighted credit of concepts covered the summary:
submodular! (similar to the colored ball examples we saw)
the objectives in the nice talk (berg-kirkpatrick et al., 2011) we saw

at the beginning of this section are, actually, submodular (cid:44) when
value(b)     0.

lin and bilmes

submodular summarization

june 20, 2011

13 / 29

submodularity in summarization

concept-based approach

concepts: id165s, keywords, etc.
maximizes the weighted credit of concepts covered the summary:
submodular! (similar to the colored ball examples we saw)
the objectives in the nice talk (berg-kirkpatrick et al., 2011) we saw

at the beginning of this section are, actually, submodular (cid:44) when
value(b)     0.

lin and bilmes

submodular summarization

june 20, 2011

13 / 29

submodularity in summarization

even id8-n itself is monotone submodular!!

id8-n: high correlation to human evaluation (lin 2004).

lin and bilmes

submodular summarization

june 20, 2011

14 / 29

8910111213141500.20.40.60.81id8-2 recall (%)(scale on cost)greedy algorithmhumanrsubmodularity in summarization

even id8-n itself is monotone submodular!!

id8-n: high correlation to human evaluation (lin 2004).

theorem (lin and bilmes, 2011)

id8-n is monotone submodular (proof in paper).

lin and bilmes

submodular summarization

june 20, 2011

14 / 29

8910111213141500.20.40.60.81id8-2 recall (%)(scale on cost)greedy algorithmhumanrsubmodularity in summarization

even id8-n itself is monotone submodular!!

id8-n: high correlation to human evaluation (lin 2004).

theorem (lin and bilmes, 2011)

id8-n is monotone submodular (proof in paper).

figure: oracle experiments on duc-05. the red dash line indicates the best
id8-2 recall score of human summaries (summary with id c).

lin and bilmes

submodular summarization

june 20, 2011

14 / 29

8910111213141500.20.40.60.81id8-2 recall (%)(scale on cost)greedy algorithmhumanrnew class of submodular functions for document summarization

outline

1 background on submodularity

2 problem setup and algorithm

3 submodularity in summarization

4 new class of submodular functions for document summarization

5 experimental results

6 summary

lin and bilmes

submodular summarization

june 20, 2011

15 / 29

new class of submodular functions for document summarization

the general form of our submodular functions

two properties of a good summary: relevance and non-redundancy.
common approaches (e.g., mmr): encourage relevance and
(negatively) penalize redundancy.
the redundancy penalty is usually what violates monotonicity.
our approach: we positively reward diversity instead of negatively
penalizing redundancy:

lin and bilmes

submodular summarization

june 20, 2011

16 / 29

new class of submodular functions for document summarization

the general form of our submodular functions

two properties of a good summary: relevance and non-redundancy.
common approaches (e.g., mmr): encourage relevance and
(negatively) penalize redundancy.
the redundancy penalty is usually what violates monotonicity.
our approach: we positively reward diversity instead of negatively
penalizing redundancy:

de   nition (the general form of our submodular functions)

f (s) = l(s) +   r(s)

l(s) measures the coverage (or    delity) of summary set s to the
document.
r(s) rewards diversity in s.
       0 is a trade-o    coe   cient.
analogous to the objectives widely used in machine learning:
loss + id173

lin and bilmes

submodular summarization

june 20, 2011

16 / 29

new class of submodular functions for document summarization

coverage function

coverage function

(cid:88)

i   v

l(s) =

min{ci (s),    ci (v )}

ci : 2v     r is monotone submodular, and measures how well i is
covered by s.
0            1 is a threshold coe   cient     su   cient coverage fraction.

lin and bilmes

submodular summarization

june 20, 2011

17 / 29

new class of submodular functions for document summarization

coverage function

coverage function

(cid:88)

i   v

l(s) =

min{ci (s),    ci (v )}

ci : 2v     r is monotone submodular, and measures how well i is
covered by s.
0            1 is a threshold coe   cient     su   cient coverage fraction.
if min{ci (s),   ci (v )} =   ci (v ), then sentence i is well covered by
summary s (saturated).

lin and bilmes

submodular summarization

june 20, 2011

17 / 29

new class of submodular functions for document summarization

coverage function

coverage function

(cid:88)

i   v

l(s) =

min{ci (s),    ci (v )}

ci : 2v     r is monotone submodular, and measures how well i is
covered by s.
0            1 is a threshold coe   cient     su   cient coverage fraction.
if min{ci (s),   ci (v )} =   ci (v ), then sentence i is well covered by
summary s (saturated).
after saturation, further increases in ci (s) won   t increase the
objective function values (return diminishes).
therefore, new sentence added to s should focus on sentences that
are not yet saturated, in order to increasing the objective function
value.

lin and bilmes

submodular summarization

june 20, 2011

17 / 29

new class of submodular functions for document summarization

coverage function

coverage function

(cid:88)

i   v

l(s) =

min{ci (s),    ci (v )}

ci measures how well i is covered by s.
one simple possible ci (that we use in our experiments and works
well) is:

(cid:88)

j   s

ci (s) =

wi,j ,

where wi,j     0 measures the similarity between i and j.
with this ci , l(s) is monotone submodular, as required.

lin and bilmes

submodular summarization

june 20, 2011

18 / 29

new class of submodular functions for document summarization

diversity reward function

diversity reward function

r(s) =

k(cid:88)

(cid:115) (cid:88)

i=1

j   pi   s

rj .

pi , i = 1,       k is a partition of the ground set v
rj     0: singleton reward of j, which represents the importance of j
to the summary.

square root over the sum of rewards of sentences belong to the same
partition (diminishing returns).
r(s) is monotone submodular as well.

lin and bilmes

submodular summarization

june 20, 2011

19 / 29

new class of submodular functions for document summarization

diversity reward function - how does it reward diversity?

3 partitions: p1, p2, p3.
singleton reward for sentence
1, 2, 3 and 4:
r1 = 5, r2 = 5, r3 = 4, r4 = 3.
current summary: s = {1, 2}
consider adding a new sentence,
3 or 4.

a diverse (non-redundant)
summary: {1, 2, 4}.

lin and bilmes

submodular summarization

june 20, 2011

20 / 29

1234p3p1p2new class of submodular functions for document summarization

diversity reward function - how does it reward diversity?

3 partitions: p1, p2, p3.
singleton reward for sentence
1, 2, 3 and 4:
r1 = 5, r2 = 5, r3 = 4, r4 = 3.
current summary: s = {1, 2}
consider adding a new sentence,
3 or 4.

a diverse (non-redundant)
summary: {1, 2, 4}.

modular objective:
r({1, 2, 3}) = 5 + 5 + 4 = 14 > r({1, 2, 4}) = 5 + 5 + 3 = 13
submodular objective:
r({1, 2, 3}) = 5 +    5 + 4 = 8 < r({1, 2, 4}) = 5 + 5 + 3 = 13

lin and bilmes

submodular summarization

june 20, 2011

20 / 29

1234p3p1p2new class of submodular functions for document summarization

diversity reward function

singleton reward of j: the importance of being j (to the summary).

query-independent (generic) case:

query-dependent case, given a query q,

(cid:88)

i   v

rj =

1
n

wi,j .

(cid:88)

i   v

rj =   

1
n

wi,j + (1       )rj,q

where rj,q measures the relevance between j and query q.

lin and bilmes

submodular summarization

june 20, 2011

21 / 29

new class of submodular functions for document summarization

diversity reward function

singleton reward of j: the importance of being j (to the summary).

query-independent (generic) case:

query-dependent case, given a query q,

(cid:88)

i   v

rj =

1
n

wi,j .

(cid:88)

i   v

rj =   

1
n

wi,j + (1       )rj,q

where rj,q measures the relevance between j and query q.

multi-resolution diversity reward

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)

k1(cid:88)

i=1

j   p (1)

i    s

k2(cid:88)

i=1

rj +

(cid:118)(cid:117)(cid:117)(cid:116) (cid:88)

j   p (2)

i    s

rj +       

r(s) =

lin and bilmes

submodular summarization

june 20, 2011

21 / 29

experimental results

outline

1 background on submodularity

2 problem setup and algorithm

3 submodularity in summarization

4 new class of submodular functions for document summarization

5 experimental results

6 summary

lin and bilmes

submodular summarization

june 20, 2011

22 / 29

experimental results

generic summarization

duc-04: generic summarization

table: id8-1 recall (r) and f-measure (f) results (%) on duc-04.
duc-03 was used as development set.

duc-04
l1(s)
r1(s)

l1(s) +   r1(s)

takamura and okumura (2009)

wang et al. (2009)

lin and bilmes (2010)

r

39.03
38.23
39.35

38.50
39.07

-

best system in duc-04 (peer 65)

38.28

f

38.65
37.81
38.90

-
-

38.39
37.94

lin and bilmes

submodular summarization

june 20, 2011

23 / 29

experimental results

generic summarization

duc-04: generic summarization

table: id8-1 recall (r) and f-measure (f) results (%) on duc-04.
duc-03 was used as development set.

duc-04
l1(s)
r1(s)

l1(s) +   r1(s)

takamura and okumura (2009)

wang et al. (2009)

lin and bilmes (2010)

r

39.03
38.23
39.35

38.50
39.07

-

best system in duc-04 (peer 65)

38.28

f

38.65
37.81
38.90

-
-

38.39
37.94

note: this is the best id8-1 result ever reported on duc-04.

lin and bilmes

submodular summarization

june 20, 2011

23 / 29

experimental results

query-focused summarization

duc-05,06,07: query-focused summarization

for each document cluster, a title and a narrative (query) describing
a user   s information need are provided.

nelder-mead (derivative-free) for parameter training.

lin and bilmes

submodular summarization

june 20, 2011

24 / 29

experimental results

duc-05 results

table: id8-2 recall (r) and f-measure (f) results (%)

l1(s) +p3

l1(s) +   rq (s)

  =1     rq,  (s)
daum  e iii and marcu (2006)

wei et al. (2010)

best system in duc-05 (peer 15)

r

7.82
8.19

6.98
8.02
7.44

f

7.72
8.13

-
-

7.43

duc-06 was used as training set for the objective function with single
diversity reward.
duc-06 and 07 were used as training sets for the objective function
with multi-resolution diversity reward (new results since our
camera-ready version of the paper)

lin and bilmes

submodular summarization

june 20, 2011

25 / 29

experimental results

duc-05 results

table: id8-2 recall (r) and f-measure (f) results (%)

l1(s) +p3

l1(s) +   rq (s)

  =1     rq,  (s)
daum  e iii and marcu (2006)

wei et al. (2010)

best system in duc-05 (peer 15)

r

7.82
8.19

6.98
8.02
7.44

f

7.72
8.13

-
-

7.43

duc-06 was used as training set for the objective function with single
diversity reward.
duc-06 and 07 were used as training sets for the objective function
with multi-resolution diversity reward (new results since our
camera-ready version of the paper)
note: this is the best id8-2 result ever reported on duc-05.

lin and bilmes

submodular summarization

june 20, 2011

25 / 29

experimental results

duc-06 results

table: id8-2 recall (r) and f-measure (f) results (%)

l1(s) +p3

l1(s) +   rq (s)

  =1     rq,  (s)

celikyilmaz and hakkani-t  ur (2010)

shen and li (2010)

best system in duc-06 (peer 24)

r

9.75
9.81

9.10
9.30
9.51

f

9.77
9.82

-
-

9.51

duc-05 was used as training set for the objective function with single
diversity reward.
duc-05 and 07 were used as training sets for the objective function
with multi-resolution diversity reward (new results since our
camera-ready version of the paper)

lin and bilmes

submodular summarization

june 20, 2011

26 / 29

experimental results

duc-06 results

table: id8-2 recall (r) and f-measure (f) results (%)

l1(s) +p3

l1(s) +   rq (s)

  =1     rq,  (s)

celikyilmaz and hakkani-t  ur (2010)

shen and li (2010)

best system in duc-06 (peer 24)

r

9.75
9.81

9.10
9.30
9.51

f

9.77
9.82

-
-

9.51

duc-05 was used as training set for the objective function with single
diversity reward.
duc-05 and 07 were used as training sets for the objective function
with multi-resolution diversity reward (new results since our
camera-ready version of the paper)
note: this is the best id8-2 result ever reported on duc-06.

lin and bilmes

submodular summarization

june 20, 2011

26 / 29

experimental results

duc-07 results

table: id8-2 recall (r) and f-measure (f) results (%)

l1(s) +p3

l1(s) +   rq (s)

  =1     rq,  (s)
toutanova et al. (2007)

haghighi and vanderwende (2009)
celikyilmaz and hakkani-t  ur (2010)

best system in duc-07 (peer 15), using web search

r

12.18
12.38

11.89
11.80
11.40
12.45

f

12.13
12.33

11.89

-
-

12.29

duc-05 was used as training set for the objective function with single
diversity reward.
duc-05 and 06 were used as training sets for the objective function
with multi-resolution diversity reward.

lin and bilmes

submodular summarization

june 20, 2011

27 / 29

experimental results

duc-07 results

table: id8-2 recall (r) and f-measure (f) results (%)

l1(s) +p3

l1(s) +   rq (s)

  =1     rq,  (s)
toutanova et al. (2007)

haghighi and vanderwende (2009)
celikyilmaz and hakkani-t  ur (2010)

best system in duc-07 (peer 15), using web search

r

12.18
12.38

11.89
11.80
11.40
12.45

f

12.13
12.33

11.89

-
-

12.29

duc-05 was used as training set for the objective function with single
diversity reward.
duc-05 and 06 were used as training sets for the objective function
with multi-resolution diversity reward.
note: this is the best id8-2 f-measure result ever reported on
duc-07, and best id8-2 r without web search expansion.

lin and bilmes

submodular summarization

june 20, 2011

27 / 29

outline

summary

1 background on submodularity

2 problem setup and algorithm

3 submodularity in summarization

4 new class of submodular functions for document summarization

5 experimental results

6 summary

lin and bilmes

submodular summarization

june 20, 2011

28 / 29

summary

summary

submodularity is natural    t for summarization problems (e.g., even
id8-n is submodular).
a greedy algorithm using scaled cost: both scalable and
near-optimal, thanks to submodularity.

we have introduced a class of submodular functions: expressive and
general (more advanced nlp techniques not used, but could be easily
incorporated into our objective functions).

we show the top results yet on duc-04, 05, 06 and 07.

lin and bilmes

submodular summarization

june 20, 2011

29 / 29

