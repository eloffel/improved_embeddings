this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

1

a survey on id21

sinno jialin pan and qiang yang fellow, ieee

abstract   a major assumption in many machine learning and data mining algorithms is that the training and future data must be
in the same feature space and have the same distribution. however, in many real-world applications, this assumption may not hold.
for example, we sometimes have a classi   cation task in one domain of interest, but we only have suf   cient training data in another
domain of interest, where the latter data may be in a different feature space or follow a different data distribution. in such cases,
knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling
efforts. in recent years, id21 has emerged as a new learning framework to address this problem. this survey focuses on
categorizing and reviewing the current progress on id21 for classi   cation, regression and id91 problems. in this survey,
we discuss the relationship between id21 and other related machine learning techniques such as id20, multi-
task learning and sample selection bias, as well as co-variate shift. we also explore some potential future issues in id21
research.

index terms   id21, survey, machine learning, data mining.

   

1 introduction
data mining and machine learning technologies have already
achieved signi   cant success in many knowledge engineering
areas including classi   cation, regression and id91 (e.g.,
[1], [2]). however, many machine learning methods work well
only under a common assumption: the training and test data are
drawn from the same feature space and the same distribution.
when the distribution changes, most statistical models need to
be rebuilt from scratch using newly collected training data. in
many real world applications, it is expensive or impossible to
re-collect the needed training data and rebuild the models. it
would be nice to reduce the need and effort to re-collect the
training data. in such cases, knowledge transfer or transfer
learning between task domains would be desirable.

many examples in knowledge engineering can be found
where id21 can truly be bene   cial. one example
is web document classi   cation [3], [4], [5], where our goal
is to classify a given web document into several prede   ned
categories. as an example in the area of web-document
classi   cation (see, e.g., [6]), the labeled examples may be
the university web pages that are associated with category
information obtained through previous manual-labeling efforts.
for a classi   cation task on a newly created web site where the
data features or data distributions may be different, there may
be a lack of labeled training data. as a result, we may not be
able to directly apply the web-page classi   ers learned on the
university web site to the new web site. in such cases, it would
be helpful if we could transfer the classi   cation knowledge
into the new domain.

the need for id21 may arise when the data can
be easily outdated. in this case, the labeled data obtained in
one time period may not follow the same distribution in a
later time period. for example, in indoor wifi localization

department of computer science and engineering, hong kong university of
science and technology, clearwater bay, kowloon, hong kong
emails: {sinnopan, qyang}@cse.ust.hk

problems, which aims to detect a user   s current location based
on previously collected wifi data, it is very expensive to
calibrate wifi data for building localization models in a large-
scale environment, because a user needs to label a large
collection of wifi signal data at each location. however, the
wifi signal-strength values may be a function of time, device
or other dynamic factors. a model trained in one time period
or on one device may cause the performance for location
estimation in another time period or on another device to be
reduced. to reduce the re-calibration effort, we might wish to
adapt the localization model trained in one time period (the
source domain) for a new time period (the target domain), or
to adapt the localization model trained on a mobile device (the
source domain) for a new mobile device (the target domain),
as done in [7].

as a third example, consider the problem of sentiment
classi   cation, where our task is to automatically classify the
reviews on a product, such as a brand of camera, into positive
and negative views. for this classi   cation task, we need to
   rst collect many reviews of the product and annotate them.
we would then train a classi   er on the reviews with their
corresponding labels. since the distribution of review data
among different types of products can be very different, to
maintain good classi   cation performance, we need to collect
a large amount of labeled data in order to train the review-
classi   cation models for each product. however, this data-
labeling process can be very expensive to do. to reduce the
effort for annotating reviews for various products, we may
want to adapt a classi   cation model that is trained on some
products to help learn classi   cation models for some other
products. in such cases, id21 can save a signi   cant
amount of labeling effort [8].

in this survey article, we give a comprehensive overview of
id21 for classi   cation, regression and id91
developed in machine learning and data mining areas. there
has been a large amount of work on id21 for
id23 in the machine learning literature (e.g.,

digital object indentifier 10.1109/tkde.2009.191

1041-4347/$25.00     2009 ieee

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

2

[9], [10]). however, in this paper, we only focus on transfer
learning for classi   cation, regression and id91 problems
that are related more closely to data mining tasks. by doing
the survey, we hope to provide a useful resource for the data
mining and machine learning community.

the rest of the survey is organized as follows. in the next
four sections, we    rst give a general overview and de   ne
some notations we will use later. we then brie   y survey the
history of id21, give a uni   ed de   nition of transfer
learning and categorize id21 into three different
settings (given in table 2 and figure 2). for each setting, we
review different approaches, given in table 3 in detail. after
that, in section 6, we review some current research on the
topic of    negative transfer   , which happens when knowledge
transfer has a negative impact on target learning. in section 7,
we introduce some successful applications of id21
and list some published data sets and software toolkits for
id21 research. finally, we conclude the article with
a discussion of future works in section 8.

2 overview
2.1 a brief history of id21
traditional data mining and machine learning algorithms make
predictions on the future data using statistical models that are
trained on previously collected labeled or unlabeled training
data [11], [12], [13]. semi-supervised classi   cation [14], [15],
[16], [17] addresses the problem that the labeled data may
be too few to build a good classi   er, by making use of a
large amount of unlabeled data and a small amount of labeled
data. variations of supervised and semi-supervised learning
for imperfect datasets have been studied; for example, zhu
and wu [18] have studied how to deal with the noisy class-
label problems. yang et al. considered cost-sensitive learning
[19] when additional tests can be made to future samples.
nevertheless, most of them assume that the distributions of
the labeled and unlabeled data are the same. id21,
in contrast, allows the domains, tasks, and distributions used
in training and testing to be different. in the real world, we
observe many examples of id21. for example,
we may    nd that learning to recognize apples might help to
recognize pears. similarly, learning to play the electronic organ
may help facilitate learning the piano. the study of transfer
learning is motivated by the fact that people can intelligently
apply knowledge learned previously to solve new problems
faster or with better solutions. the fundamental motivation
for id21 in the    eld of machine learning was
discussed in a nips-95 workshop on    learning to learn   
1, which focused on the need for lifelong machine-learning
methods that retain and reuse previously learned knowledge.
learning has attracted more and
more attention since 1995 in different names:
learning to
learn, life-long learning, knowledge transfer, inductive trans-
fer, id72, knowledge consolidation, context-
sensitive learning, knowledge-based inductive bias, meta learn-
ing, and incremental/cumulative learning [20]. among these,

research on transfer

1. http://socrates.acadiau.ca/courses/comp/dsilver/nips95 ltl/

transfer.workshop.1995.html

a closely related learning technique to id21 is
the id72 framework [21], which tries to learn
multiple tasks simultaneously even when they are different.
a typical approach for id72 is to uncover the
common (latent) features that can bene   t each individual task.
in 2005, the broad agency announcement (baa) 05-29
of defense advanced research projects agency (darpa)   s
information processing technology of   ce (ipto) 2 gave a
new mission of id21: the ability of a system to
recognize and apply knowledge and skills learned in previous
tasks to novel tasks. in this de   nition, id21 aims
to extract the knowledge from one or more source tasks and
applies the knowledge to a target task. in contrast to multi-task
learning, rather than learning all of the source and target tasks
simultaneously, id21 cares most about the target
task. the roles of the source and target tasks are no longer
symmetric in id21.

figure 1 shows

the difference between the learning
processes of traditional and id21 techniques. as
we can see, traditional machine learning techniques try to learn
each task from scratch, while id21 techniques try
to transfer the knowledge from some previous tasks to a target
task when the latter has fewer high-quality training data.

(a) traditional machine learning

(b) id21

fig. 1. different learning processes between traditional
machine learning and id21

today,

id21 methods appear in several

top
venues, most notably in data mining (acm kdd, ieee icdm
and pkdd, for example), machine learning (icml, nips,
ecml, aaai and ijcai, for example) and applications of
machine learning and data mining (acm sigir, www and
acl for example) 3. before we give different categorizations
of id21, we    rst describe the notations used in this
article.

2.2 notations and de   nitions
in this section, we introduce some notations and de   nitions
that are used in this survey. first of all, we give the de   nitions
of a    domain    and a    task   , respectively.
in this survey, a domain d consists of two components: a
feature space x and a marginal id203 distribution p (x),
where x = {x1, . . . , xn}     x . for example, if our learning

2. http://www.darpa.mil/ipto/programs/tl/tl.asp
3. we summarize a list of conferences and workshops where transfer
learning papers appear in these few years in the following webpage for
reference, http://www.cse.ust.hk/   sinnopan/conferencetl.htm

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

task is document classi   cation, and each term is taken as a
binary feature, then x is the space of all term vectors, xi
is the ith term vector corresponding to some documents, and
x is a particular learning sample. in general, if two domains
are different, then they may have different feature spaces or
different marginal id203 distributions.
given a speci   c domain, d = {x , p (x)}, a task consists
of two components: a label space y and an objective predictive
function f(  ) (denoted by t = {y, f(  )}), which is not
observed but can be learned from the training data, which
consist of pairs {xi, yi}, where xi     x and yi     y. the
function f(  ) can be used to predict the corresponding label,
f(x), of a new instance x. from a probabilistic viewpoint,
f(x) can be written as p (y|x). in our document classi   cation
example, y is the set of all labels, which is true, false for a
binary classi   cation task, and yi is    true    or    false   .
for simplicity, in this survey, we only consider the case
where there is one source domain ds, and one target domain,
dt , as this is by far the most popular of the research works in
the literature. more speci   cally, we denote the source domain
data as ds = {(xs1, ys1), . . . , (xsns , ysns )}, where xsi
   
xs is the data instance and ysi
    ys is the corresponding
class label. in our document classi   cation example, ds can
be a set of term vectors together with their associated true or
false class labels. similarly, we denote the target domain data
as dt = {(xt1, yt1), . . . , (xtnt , ytnt )}, where the input xti
    yt is the corresponding output. in most
is in xt and yti
cases, 0     nt (cid:4) ns.

we now give a uni   ed de   nition of id21.

de   nition 1 (id21) given a source domain ds
and learning task ts, a target domain dt and learning task
tt , id21 aims to help improve the learning of the
target predictive function ft (  ) in dt using the knowledge in
ds and ts, where ds (cid:5)= dt , or ts (cid:5)= tt .

in the above de   nition, a domain is a pair d = {x , p (x)}.
thus the condition ds (cid:5)= dt implies that either xs (cid:5)= xt or
ps(x) (cid:5)= pt (x). for example, in our document classi   cation
example, this means that between a source document set and
a target document set, either the term features are different
between the two sets (e.g., they use different languages), or
their marginal distributions are different.
similarly, a task is de   ned as a pair t = {y, p (y |x)}.
thus the condition ts (cid:5)= tt implies that either ys (cid:5)= yt
or p (ys|xs) (cid:5)= p (yt|xt ). when the target and source
domains are the same, i.e. ds = dt , and their learning tasks
are the same, i.e., ts = tt , the learning problem becomes
a traditional machine learning problem. when the domains
are different, then either (1) the feature spaces between the
i.e. xs (cid:5)= xt , or (2) the feature
domains are different,
spaces between the domains are the same but the marginal
id203 distributions between domain data are different;
i.e. p (xs) (cid:5)= p (xt ), where xsi     xs and xti     xt .
as an example, in our document classi   cation example, case
(1) corresponds to when the two sets of documents are
described in different languages, and case (2) may correspond
to when the source domain documents and the target domain
documents focus on different topics.

3

given speci   c domains ds and dt , when the learning
tasks ts and tt are different, then either (1) the label spaces
between the domains are different, i.e. ys (cid:5)= yt , or (2) the
id155 distributions between the domains are
different; i.e. p (ys|xs) (cid:5)= p (yt|xt ), where ysi     ys and
yti     yt . in our document classi   cation example, case (1)
corresponds to the situation where source domain has binary
document classes, whereas the target domain has ten classes to
classify the documents to. case (2) corresponds to the situation
where the source and target documents are very unbalanced
in terms of the user-de   ned classes.

in addition, when there exists some relationship, explicit or
implicit, between the feature spaces of the two domains, we
say that the source and target domains are related.

2.3 a categorization of id21 tech-
niques
in id21, we have the following three main research
issues: (1) what to transfer; (2) how to transfer; (3) when to
transfer.
   what

to transfer    asks which part of knowledge can
be transferred across domains or tasks. some knowledge is
speci   c for individual domains or tasks, and some knowledge
may be common between different domains such that they may
help improve performance for the target domain or task. after
discovering which knowledge can be transferred,
learning
algorithms need to be developed to transfer the knowledge,
which corresponds to the    how to transfer    issue.

   when to transfer    asks in which situations, transferring
skills should be done. likewise, we are interested in knowing
in which situations, knowledge should not be transferred. in
some situations, when the source domain and target domain are
not related to each other, brute-force transfer may be unsuc-
cessful. in the worst case, it may even hurt the performance
of learning in the target domain, a situation which is often
referred to as negative transfer. most current work on transfer
learning focuses on    what to transfer    and    how to transfer   ,
by implicitly assuming that the source and target domains be
related to each other. however, how to avoid negative transfer
is an important open issue that is attracting more and more
attention in the future.

based on the de   nition of id21, we summarize
the relationship between traditional machine learning and var-
ious id21 settings in table 1, where we categorize
inductive trans-
id21 under three sub-settings,
fer learning, transductive id21 and unsupervised
id21, based on different situations between the
source and target domains and tasks.

1) in the inductive id21 setting, the target task
is different from the source task, no matter when the
source and target domains are the same or not.
in this case, some labeled data in the target domain are
required to induce an objective predictive model ft (  )
for use in the target domain. in addition, according to
different situations of labeled and unlabeled data in the
source domain, we can further categorize the inductive
id21 setting into two cases:

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

4

relationship between traditional machine learning and various id21 settings

table 1

learning settings

traditional machine learning

inductive id21 /
unsupervised id21
transductive id21

id21

source and target domains

source and target tasks

the same
the same

different but related
different but related

the same

different but related
different but related

the same

(1.1) a lot of labeled data in the source domain are
available. in this case, the inductive id21
setting is similar to the id72 setting.
however, the inductive id21 setting only
aims at achieving high performance in the target task
by transferring knowledge from the source task while
id72 tries to learn the target and source
task simultaneously.
(1.2) no labeled data in the source domain are available.
in this case, the inductive id21 setting is
similar to the self-taught
learning setting, which is
   rst proposed by raina et al. [22]. in the self-taught
learning setting, the label spaces between the source
and target domains may be different, which implies
the side information of the source domain cannot be
used directly. thus, it   s similar to the inductive transfer
learning setting where the labeled data in the source
domain are unavailable.

2) in the transductive id21 setting, the source
and target tasks are the same, while the source and target
domains are different.
in this situation, no labeled data in the target domain
are available while a lot of labeled data in the source
domain are available. in addition, according to different
situations between the source and target domains, we
can further categorize the transductive id21
setting into two cases.
(2.1) the feature spaces between the source and target
domains are different, xs (cid:5)= xt .
(2.2) the feature spaces between domains are the same,
xs = xt , but the marginal id203 distributions of
the input data are different, p (xs) (cid:5)= p (xt ).
the latter case of the transductive id21
setting is related to id20 for knowledge
transfer in text classi   cation [23] and sample selection
bias [24] or co-variate shift [25], whose assumptions are
similar.

3) finally, in the unsupervised id21 setting,
similar to inductive id21 setting, the target
task is different from but related to the source task.
however, the unsupervised id21 focus on
solving unsupervised learning tasks in the target domain,
such as id91, id84 and density
estimation [26], [27]. in this case, there are no labeled
data available in both source and target domains in
training.

the relationship between the different settings of transfer
learning and the related areas are summarized in table 2 and
figure 2.

approaches to id21 in the above three different
settings can be summarized into four cases based on    what to
transfer   . table 3 shows these four cases and brief description.
the    rst context can be referred to as instance-based transfer-
learning (or instance-transfer) approach [6], [28], [29], [30],
[31], [24], [32], [33], [34], [35], which assumes that certain
parts of the data in the source domain can be reused for
learning in the target domain by re-weighting. instance re-
weighting and importance sampling are two major techniques
in this context.

a second case can be referred to as feature-representation-
transfer approach [22], [36], [37], [38], [39], [8], [40], [41],
[42], [43], [44]. the intuitive idea behind this case is to learn
a    good    feature representation for the target domain. in this
case, the knowledge used to transfer across domains is encoded
into the learned feature representation. with the new feature
representation, the performance of the target task is expected
to improve signi   cantly.

a third case can be referred to as parameter-transfer ap-
proach [45], [46], [47], [48], [49], which assumes that the
source tasks and the target tasks share some parameters or
prior distributions of the hyper-parameters of the models. the
transferred knowledge is encoded into the shared parameters
or priors. thus, by discovering the shared parameters or priors,
knowledge can be transferred across tasks.

finally, the last case can be referred to as the relational-
knowledge-transfer problem [50], which deals with transfer
learning for relational domains. the basic assumption behind
this context is that some relationship among the data in the
source and target domains are similar. thus, the knowledge
to be transferred is the relationship among the data. recently,
statistical relational learning techniques dominate this context
[51], [52].

table 4 shows the cases where the different approaches
are used for each id21 setting. we can see that
the inductive id21 setting has been studied in
many research works, while the unsupervised id21
setting is a relatively new research topic and only studied
in the context of the feature-representation-transfer case. in
addition, the feature-representation-transfer problem has been
proposed to all three settings of id21. however,
the parameter-transfer and the relational-knowledge-transfer
approach are only studied in the inductive id21
setting, which we discuss in detail below.

3 inductive id21
de   nition 2 (inductive id21) given a source
domain ds and a learning task ts, a target domain dt and

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

table 2

different settings of id21

id21 settings
inductive id21

related areas
id72

source domain labels
available

target domain labels
available

transductive id21

unsupervised id21

self-taught learning

id20, sample
selection bias, co-variate shift

unavailable

available

unavailable

available

unavailable

unavailable

5

tasks
regression,
classi   cation
regression,
classi   cation
regression,
classi   cation
id91,
dimensionality
reduction

fig. 2. an overview of different settings of transfer

different approaches to id21

table 3

id21 approaches
instance-transfer

feature-representation-transfer

parameter-transfer

relational-knowledge-transfer

brief description
to re-weight some labeled data in the source domain for use in the target domain [6], [28], [29],
[30], [31], [24], [32], [33], [34], [35].
find a    good    feature representation that reduces difference between the source and the target
domains and the error of classi   cation and regression models [22], [36], [37], [38], [39], [8],
[40], [41], [42], [43], [44].
discover shared parameters or priors between the source domain and target domain models, which
can bene   t for id21 [45], [46], [47], [48], [49].
build mapping of relational knowledge between the source domain and the target domains. both
domains are relational domains and i.i.d assumption is relaxed in each domain [50], [51], [52].

different approaches used in different settings

table 4

instance-transfer
feature-representation-transfer
parameter-transfer
relational-knowledge-transfer

   
inductive id21
   
   
   

   
transductive id21 unsupervised id21
   

   

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

a learning task tt , inductive id21 aims to help
improve the learning of the target predictive function ft (  ) in
dt using the knowledge in ds and ts, where ts (cid:5)= tt .

based on the above de   nition of the inductive transfer
learning setting, a few labeled data in the target domain are
required as the training data to induce the target predictive
function. as mentioned in section 2.3, this setting has two
cases: (1) labeled data in the source domain are available;
(2) labeled data in the source domain are unavailable while
unlabeled data in the source domain are available. most
id21 approaches in this setting focus on the former
case.

3.1 transferring knowledge of instances
the instance-transfer approach to the inductive transfer learn-
ing setting is intuitively appealing: although the source domain
data cannot be reused directly, there are certain parts of the
data that can still be reused together with a few labeled data
in the target domain.

dai et al. [6] proposed a boosting algorithm, tradaboost,
which is an extension of the adaboost algorithm, to address
the inductive id21 problems. tradaboost assumes
that the source and target domain data use exactly the same set
of features and labels, but the distributions of the data in the
two domains are different. in addition, tradaboost assumes
that, due to the difference in distributions between the source
and the target domains, some of the source domain data may
be useful in learning for the target domain but some of them
may not and could even be harmful. it attempts to iteratively
re-weight the source domain data to reduce the effect of the
   bad    source data while encourage the    good    source data
to contribute more for the target domain. for each round of
iteration, tradaboost trains the base classi   er on the weighted
source and target data. the error is only calculated on the
target data. furthermore, tradaboost uses the same strategy as
adaboost to update the incorrectly classi   ed examples in the
target domain while using a different strategy from adaboost
to update the incorrectly classi   ed source examples in the
source domain. theoretical analysis of tradaboost in also
given in [6].

jiang and zhai [30] proposed a heuristic method to remove
   misleading    training examples from the source domain based
on the difference between conditional probabilities p (yt|xt )
and p (ys|xs). liao et al. [31] proposed a new active learning
method to select the unlabeled data in a target domain to
be labeled with the help of the source domain data. wu and
dietterich [53] integrated the source domain (auxiliary) data an
id166 framework for improving the classi   cation performance.

3.2 transferring knowledge of feature representa-
tions
the feature-representation-transfer approach to the inductive
id21 problem aims at    nding    good    feature
representations to minimize domain divergence and classi   -
cation or regression model error. strategies to    nd    good   
feature representations are different for different types of the

6

source domain data. if a lot of labeled data in the source
domain are available, supervised learning methods can be
used to construct a feature representation. this is similar to
common id171 in the    eld of id72
[40]. if no labeled data in the source domain are available,
unsupervised learning methods are proposed to construct the
feature representation.

3.2.1 supervised feature construction
supervised feature construction methods for the inductive
id21 setting are similar to those used in multi-
task learning. the basic idea is to learn a low-dimensional
representation that is shared across related tasks. in addition,
the learned new representation can reduce the classi   cation
or regression model error of each task as well. argyriou et
al. [40] proposed a sparse id171 method for multi-
task learning. in the inductive id21 setting, the
common features can be learned by solving an optimization
problem, given as follows.
nt(cid:2)

l(yti,(cid:6)at, u

t

xti(cid:7)) +   (cid:8)a(cid:8)2

2,1 (1)

(cid:2)
t   {t,s}
u     od

i=1

arg min

a,u

s.t.

1

(cid:3)d
i=1 (cid:8)ai(cid:8)p
r)

in this equation, s and t denote the tasks in the source
domain and target domain, respectively. a = [as, at ]     rd  2
is a matrix of parameters. u is a d    d orthogonal matrix
(mapping function) for mapping the original high-dimensional
data to low-dimensional representations. the (r, p)-norm of
a is de   ned as (cid:8)a(cid:8)r,p := (
p . the optimization
problem (1) estimates the low-dimensional representations
u t xt , u t xs and the parameters, a, of the model at the
same time. the optimization problem (1) can be further trans-
formed into an equivalent id76 formulation and
be solved ef   ciently. in a follow-up work, argyriou et al. [41]
proposed a spectral id173 framework on matrices for
multi-task structure learning.

lee et al. [42] proposed a id76 algorithm for
simultaneously learning meta-priors and feature weights from
an ensemble of related prediction tasks. the meta-priors can
be transferred among different tasks. jebara [43] proposed to
select features for id72 with id166s. ruckert et
al. [54] designed a kernel-based approach to inductive transfer,
which aims at    nding a suitable kernel for the target data.

3.2.2 unsupervised feature construction
in [22], raina et al. proposed to apply sparse coding [55],
which is an unsupervised feature construction method, for
learning higher level features for id21. the basic
idea of this approach consists of two steps. in the    rst step,
higher-level basis vectors b = {b1, b2, . . . , bs} are learned on
the source domain data by solving the optimization problem
(2) as shown as follows,

min
a,b

(cid:2)

i

s.t.

(cid:8)xsi    (cid:3)
(cid:8)bj(cid:8)2     1,   j     1, . . . , s

sibj(cid:8)2

2 +   (cid:8)asi(cid:8)1

j
j a

(2)

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

j
in this equation, a
si is a new representation of basis bj for
input xsi and    is a coef   cient to balance the feature con-
struction term and the id173 term. after learning the
basis vectors b, in the second step, an optimization algorithm
(3) is applied on the target domain data to learn higher level
features based on the basis vectors b.
tibj(cid:8)2

   
ti = arg min
a

2 +   (cid:8)ati(cid:8)1

(cid:8)xti    

(cid:2)

(3)

j
a

ati

j

}(cid:4)
finally, discriminative algorithms can be applied to {a
   
s
ti
with corresponding labels to train classi   cation or regression
models for use in the target domain. one drawback of this
method is that the so-called higher-level basis vectors learned
on the source domain in the optimization problem (2) may not
be suitable for use in the target domain.

recently, manifold learning methods have been adapted for
id21. in [44], wang and mahadevan proposed
a procrustes analysis based approach to manifold alignment
without correspondences, which can be used to transfer the
knowledge across domains via the aligned manifolds.

3.3 transferring knowledge of parameters
most parameter-transfer approaches to the inductive transfer
learning setting assume that individual models for related tasks
should share some parameters or prior distributions of hyper-
parameters. most approaches described in this section, includ-
ing a id173 framework and a hierarchical bayesian
framework, are designed to work under id72.
however, they can be easily modi   ed for id21.
as mentioned above, id72 tries to learn both
the source and target tasks simultaneously and perfectly, while
id21 only aims at boosting the performance of
the target domain by utilizing the source domain data. thus,
in id72, weights of the id168s for the
source and target data are the same. in contrast, in transfer
learning, weights in the id168s for different domains
can be different. intuitively, we may assign a larger weight to
the id168 of the target domain to make sure that we
can achieve better performance in the target domain.

lawrence and platt [45] proposed an ef   cient algorithm
known as mt-ivm, which is based on gaussian processes
(gp), to handle the id72 case. mt-ivm tries to
learn parameters of a gaussian process over multiple tasks by
sharing the same gp prior. bonilla et al. [46] also investigated
id72 in the context of gp. the authors proposed
to use a free-form covariance matrix over tasks to model
inter-task dependencies, where a gp prior is used to induce
correlations between tasks. schwaighofer et al. [47] proposed
to use a hierarchical bayesian framework (hb) together with
gp for id72.

besides transferring the priors of the gp models, some
researchers also proposed to transfer parameters of id166s
under a id173 framework. evgeniou and pontil [48]
borrowed the idea of hb to id166s for id72.
the proposed method assumed that the parameter, w, in id166s
for each task can be separated into two terms. one is a
common term over tasks and the other is a task-speci   c term.

7

in inductive id21,

ws = w0 + vs and wt = w0 + vt ,

where, ws and wt are parameters of the id166s for the source
task and the target learning task, respectively. w0 is a common
parameter while vs and vt are speci   c parameters for the
source task and the target task, respectively. by assuming ft =
wt    x to be a hyper-plane for task t, an extension of id166s
to id72 case can be written as the following:

min

w0,vt,  ti

=

s.t.

(4)

j(w0, vt,   ti)
(cid:2)
nt(cid:2)
(cid:2)
t   {s,t}
t   {s,t}
yti(w0 + vt)    xti     1       ti,
  ti     0, i     {1, 2, ..., nt} and t     {s, t}.

  ti +   1
2

(cid:8)vt(cid:8)2 +   2(cid:8)w0(cid:8)2

i=1

by solving the optimization problem above, we can learn the
parameters w0, vs and vt simultaneously.

several researchers have pursued the parameter transfer
approach further. gao et al. [49] proposed a locally weighted
id108 framework to combine multiple models for
id21, where the weights are dynamically assigned
according to a model   s predictive power on each test example
in the target domain.

3.4 transferring relational knowledge
different from other three contexts, the relational-knowledge-
transfer approach deals with id21 problems in
relational domains, where the data are non-i.i.d. and can be
represented by multiple relations, such as networked data and
social network data. this approach does not assume that the
data drawn from each domain be independent and identically
distributed (i.i.d.) as traditionally assumed. it tries to transfer
the relationship among data from a source domain to a
target domain. in this context, statistical relational learning
techniques are proposed to solve these problems.

mihalkova et al. [50] proposed an algorithm tamar that
transfers relational knowledge with markov logic networks
(mlns) across relational domains. mlns [56] is a powerful
formalism, which combines the compact expressiveness of
   rst order logic with    exibility of id203, for statistical
relational learning. in mlns, entities in a relational domain
are represented by predicates and their relationships are rep-
resented in    rst-order logic. tamar is motivated by the fact
that if two domains are related to each other, there may exist
mappings to connect entities and their relationships from a
source domain to a target domain. for example, a professor
can be considered as playing a similar role in an academic
domain as a manager in an industrial management domain.
in addition, the relationship between a professor and his or
her students is similar to the relationship between a manager
and his or her workers. thus, there may exist a mapping
from professor to manager and a mapping from the professor-
student relationship to the manager-worker relationship. in this
vein, tamar tries to use an mln learned for a source domain
to aid in the learning of an mln for a target domain. basically,

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

tamar is a two-stage algorithm. in the    rst step, a mapping
is constructed from a source mln to the target domain based
on weighted pseudo loglikelihood measure (wpll). in the
second step, a revision is done for the mapped structure in the
target domain through the forte algorithm [57], which is an
inductive logic programming (ilp) algorithm for revising    rst
order theories. the revised mln can be used as a relational
model for id136 or reasoning in the target domain.

in the aaai-2008 workshop on id21 for com-
plex tasks 4, mihalkova et al. [51] extended tamar to the
single-entity-centered setting of id21, where only
one entity in a target domain is available. davis et al. [52]
proposed an approach to transferring relational knowledge
based on a form of second-order markov logic. the basic
idea of the algorithm is to discover structural regularities in
the source domain in the form of markov logic formulas
with predicate variables, by instantiating these formulas with
predicates from the target domain.

4 transductive id21
the term transductive id21 was    rst proposed
by arnold et al. [58], where they required that the source
and target tasks be the same, although the domains may be
different. on top of these conditions, they further required
that that all unlabeled data in the target domain are available
at training time, but we believe that this condition can be
relaxed; instead, in our de   nition of the transductive transfer
learning setting, we only require that part of the unlabeled
target data be seen at training time in order to obtain the
marginal id203 for the target data.

note that the word    transductive    is used with several mean-
ings. in the traditional machine learning setting, transductive
learning [59] refers to the situation where all test data are
required to be seen at training time, and that the learned model
cannot be reused for future data. thus, when some new test
data arrive, they must be classi   ed together with all existing
data. in our categorization of id21, in contrast, we
use the term transductive to emphasize the concept that in this
type of id21, the tasks must be the same and there
must be some unlabeled data available in the target domain.
de   nition 3 (transductive id21) given a source
domain ds and a corresponding learning task ts, a target
domain dt and a corresponding learning task tt , transductive
id21 aims to improve the learning of the target
predictive function ft (  ) in dt using the knowledge in ds
and ts, where ds (cid:5)= dt and ts = tt . in addition, some
unlabeled target domain data must be available at training time.
this de   nition covers the work of arnold et al. [58], since
the latter considered id20, where the difference
lies between the marginal id203 distributions of source
and target data; i.e., the tasks are the same but the domains
are different.

similar to the traditional transductive learning setting, which
aims to make the best use of the unlabeled test data for learn-
ing, in our classi   cation scheme under transductive transfer

4. http://www.cs.utexas.edu/   mtaylor/aaai08tl/

8

learning, we also assume that some target-domain unlabeled
data be given. in the above de   nition of transductive transfer
learning, the source and target tasks are the same, which
implies that one can adapt the predictive function learned
in the source domain for use in the target domain through
some unlabeled target-domain data. as mentioned in section
2.3, this setting can be split to two cases: (a) the feature
spaces between the source and target domains are different,
xs (cid:5)= xt , and (b) the feature spaces between domains are the
same, xs = xt , but the marginal id203 distributions of
the input data are different, p (xs) (cid:5)= p (xt ). this is similar
to the requirements in id20 and sample selection
bias. most approaches described in the following sections are
related to case (b) above.

4.1 transferring the knowledge of instances
most instance-transfer approaches to the transductive transfer
learning setting are motivated by importance sampling. to
see how importance sampling based methods may help in
this setting, we    rst review the problem of empirical risk
minimization (erm) [60]. in general, we might want to learn
    of the model by minimizing the
the optimal parameters   
expected risk,

   

  

= arg min

       

e(x,y)   p [l(x, y,   )],

where l(x, y,   ) is a id168 that depends on the para-
meter   . however, since it is hard to estimate the id203
distribution p , we choose to minimize the erm instead,

   

  

= arg min

       

1
n

[l(xi, yi,   )],

i=1
where n is size of the training data.

in the transductive id21 setting, we want to
learn an optimal model for the target domain by minimizing
the expected risk,

n(cid:2)

   

  

= arg min

       

(cid:2)
(x,y)   dt

p (dt )l(x, y,   ).

however, since no labeled data in the target domain are
observed in training data, we have to learn a model from the
source domain data instead. if p (ds) = p (dt ), then we may
simply learn the model by solving the following optimization
problem for use in the target domain,

   

  

= arg min

       

(cid:2)
(x,y)   ds

p (ds)l(x, y,   ).

otherwise, when p (ds) (cid:5)= p (dt ), we need to modify
the above optimization problem to learn a model with high
generalization ability for the target domain, as follows:

   

  

= arg min

       
    arg min
       

(cid:2)
(x,y)   ds
ns(cid:2)

i=1

p (dt )
p (ds) p (ds)l(x, y,   )

pt (xti, yti)
ps(xsi, ysi) l(xsi, ysi,   ).

(5)

therefore, by adding different penalty values to each instance
(xsi, ysi) with the corresponding weight pt (xti
) , we can
ps (xsi

,yti
,ysi

)

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

)

)

)

,yti
,ysi

learn a precise model for the target domain. furthermore,
since p (yt|xt ) = p (ys|xs). thus the difference between
p (ds) and p (dt ) is caused by p (xs) and p (xt ) and
pt (xti
) for each
ps (xsi
instance, we can solve the transductive id21
problems.

). if we can estimate p (xsi
p (xti

) = p (xsi
p (xti

there exist various ways to estimate p (xsi
p (xti

). zadrozny
[24] proposed to estimate the terms p (xsi) and p (xti)
independently by constructing simple classi   cation problems.
fan et al. [35] further analyzed the problems by using various
classi   ers to estimate the id203 ratio. huang et al. [32]
proposed a kernel-mean matching (kmm) algorithm to learn
p (xsi
) directly by matching the means between the source
p (xti
domain data and the target domain data in a reproducing-
kernel hilbert space (rkhs). kmm can be rewritten as the
following quadratic programming (qp) optimization problem.
(6)

2   t k         t   

1

)

)

min
  

s.t.   i     [0, b] and |(cid:3)ns

i=1   i     ns|     ns 

(cid:4)

(cid:5)

)

)

ks,s ks,t
kt,s kt,t

and kij = k(xi, xj). ks,s and
where k =
(cid:3)nt
kt,t are kernel matrices for the source domain data and the
target domain data, respectively.   i = ns
j=1 k(xi, xtj ),
where xi     xs
nt

xt , while xtj

    xt .

(cid:6)

it can be proved that   i = p (xsi
p (xti

) [32]. an advantage of
using kmm is that it can avoid performing density estimation
of either p (xsi) or p (xti), which is dif   cult when the size
of the data set is small. sugiyama et al. [34] proposed an
algorithm known as kullback-leibler importance estimation
procedure (kliep) to estimate p (xsi
) directly, based on the
p (xti
minimization of the id181. kliep can
be integrated with cross-validation to perform model selec-
tion automatically in two steps: (1) estimating the weights
of the source domain data; (2) training models on the re-
weighted data. bickel et al. [33] combined the two steps in
a uni   ed framework by deriving a kernel-id28
classi   er. besides sample re-weighting techniques, dai et
al. [28] extended a traditional naive bayesian classi   er for the
transductive id21 problems. for more information
on importance sampling and re-weighting methods for co-
variate shift or sample selection bias, readers can refer to a
recently published book [29] by quionero-candela et al. one
can also consult a tutorial on sample selection bias by fan
and sugiyama in icdm-08 5.

4.2 transferring knowledge of feature representa-
tions
most feature-representation transfer approaches to the trans-
ductive id21 setting are under unsupervised learn-
ing frameworks. blitzer et al. [38] proposed a structural cor-
respondence learning (scl) algorithm, which extends [37],
to make use of the unlabeled data from the target domain to
extract some revelent features that may reduce the difference
http://www.cs.columbia.edu/   

5. tutorial

found

can

at

be
fan/ppt/icdm08samplebias.ppt

slides

9

between the domains. the    rst step of scl is to de   ne a set
of pivot features 6 (the number of pivot feature is denoted
by m) on the unlabeled data from both domains. then, scl
removes these pivot features from the data and treats each pivot
feature as a new label vector. the m classi   cation problems
can be constructed. by assuming each problem can be solved
by linear classi   er, which is shown as follows,
   x), l = 1, . . . , m

t
fl(x) = sgn(w
l

if the pivot features are well designed,

scl can learn a matrix w = [w1w2 . . . wm] of parameters. in
the third step, singular value decomposition (svd) is applied
to matrix w = [w1w2 . . . wm]. let w = u dv t ,
then
[1:h,:] (h is the number of the shared features) is the
   = u t
matrix (linear mapping) whose rows are the top left singular
vectors of w . finally, standard discriminative algorithms can
be applied to the augmented feature vector to build models.
the augmented feature vector contains all the original feature
xi appended with the new shared features   xi. as mentioned
in [38],
then the
learned mapping    encodes the correspondence between the
features from the different domains. although ben-david and
schuller [61] showed experimentally that scl can reduce the
difference between domains, how to select the pivot features
is dif   cult and domain-dependent. in [38], blitzer et al. used a
heuristic method to select pivot features for natural language
processing (nlp) problems, such as tagging of sentences. in
their follow-up work, the researchers proposed to use mutual
information (mi) to choose the pivot features instead of using
more heuristic criteria [8]. mi-scl tries to    nd some pivot
features that have high dependence on the labels in the source
domain.

id21 in the nlp domain is sometimes re-
ferred to as id20. in this area, daum  e [39]
proposed a kernel-mapping function for nlp problems, which
maps the data from both source and target domains to a
high-dimensional feature space, where standard discriminative
learning methods are used to train the classi   ers. however,
the constructed kernel mapping function is domain knowledge
driven. it is not easy to generalize the kernel mapping to other
areas or applications. blitzer et al. [62] analyzed the uniform
convergence bounds for algorithms that minimized a convex
combination of source and target empirical risks.

in [36], dai et al. proposed a co-id91 based algorithm
to propagate the label information across different domains.
in [63], xing et al. proposed a novel algorithm known as
bridged re   nement to correct the labels predicted by a shift-
unaware classi   er towards a target distribution and take the
mixture distribution of the training and test data as a bridge to
better transfer from the training data to the test data. in [64],
ling et al. proposed a spectral classi   cation framework for
cross-domain id21 problem, where the objective
function is introduced to seek consistency between the in-
domain supervision and the out-of-domain intrinsic structure.
in [65], xue et al. proposed a cross-domain text classi   cation
algorithm that extended the traditional probabilistic latent
semantic analysis (plsa) algorithm to integrate labeled and

6. the pivot features are domain speci   c and depend on prior knowledge

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

unlabeled data from different but related domains,
into a
uni   ed probabilistic model. the new model is called topic-
bridged plsa, or tplsa.

id21 via id84 was re-
cently proposed by pan et al. [66]. in this work, pan et
al. exploited the maximum mean discrepancy embedding
(mmde) method, originally designed for dimensionality re-
duction,
to learn a low dimensional space to reduce the
difference of distributions between different domains for trans-
ductive id21. however, mmde may suffer from
in [67], pan et al. further
its computational burden. thus,
proposed an ef   cient feature extraction algorithm, known as
transfer component analysis (tca) to overcome the draw-
back of mmde.

5 unsupervised id21
de   nition 4 (unsupervised id21) given a
source domain ds with a learning task ts, a target domain dt
and a corresponding learning task tt , unsupervised transfer
learning aims to help improve the learning of the target
predictive function ft (  ) 7 in dt using the knowledge in ds
and ts, where ts (cid:5)= tt and ys and yt are not observable.
based on the de   nition of the unsupervised transfer learn-
ing setting, no labeled data are observed in the source and
target domains in training. so far, there is little research work
on this setting. recently, self-taught id91 (stc) [26]
and transferred discriminative analysis (tda) [27] algorithms
are proposed to transfer id91 and transfer dimensionality
reduction problems, respectively.

5.1 transferring knowledge of feature representa-
tions
dai et al. [26] studied a new case of id91 problems,
known as self-taught id91. self-taught id91 is an
instance of unsupervised id21, which aims at
id91 a small collection of unlabeled data in the target
domain with the help of a large amount of unlabeled data in
the source domain. stc tries to learn a common feature space
across domains, which helps in id91 in the target domain.
the objective function of stc is shown as follows.
(cid:8)
(7)
i(xs, z)     i(   xs,   z)

= i(xt , z)     i(   xt ,   z) +   

j(   xt ,   xs,   z)

(cid:7)

where xs and xt are the source and target domain data,
respectively. z is a shared feature space by xs and xt ,
and i(  ,  ) is the mutual information between two random
variables. suppose that there exist three id91 functions
cxt : xt       xt , cxs : xs       xs and cz : z       z, where
  xt ,   xs and   z are corresponding clusters of xt , xs and z,
respectively. the goal of stc is to learn   xt by solving the
optimization problem (7):

arg min
  xt ,   xs ,   z

j(   xt ,   xs,   z)

(8)

7. in unsupervised id21, the predicted labels are latent variables,

such as clusters or reduced dimensions

10

an iterative algorithm for solving the optimization function
(8) was given in [26].

similarly, [27] proposed a transferred discriminative analy-
sis (tda) algorithm to solve the transfer dimensionality
reduction problem. tda    rst applies id91 methods to
generate pseudo-class labels for the target unlabeled data. it
then applies id84 methods to the target
data and labeled source data to reduce the dimensions. these
two steps run iteratively to    nd the best subspace for the target
data.

and negative

6 transfer bounds
transfer
an important issue is to recognize the limit of the power of
id21. in [68], hassan mahmud and ray analyzed
the case of id21 using kolmogorov complexity,
where some theoretical bounds are proved. in particular, the
authors used conditional kolmogorov complexity to measure
relatedness between tasks and transfer the    right    amount
of information in a sequential id21 task under a
bayesian framework.

recently, eaton et al. [69] proposed a novel graph-based
method for knowledge transfer, where the relationships be-
tween source tasks are modeled by embedding the set of
learned source models in a graph using transferability as the
metric. transferring to a new task proceeds by mapping the
problem into the graph and then learning a function on this
graph that automatically determines the parameters to transfer
to the new learning task.

negative transfer happens when the source domain data and
task contribute to the reduced performance of learning in the
target domain. despite the fact that how to avoid negative
transfer is a very important issue, little research work has
been published on this topic. rosenstein et al. [70] empirically
showed that if two tasks are too dissimilar, then brute-force
transfer may hurt the performance of the target task. some
works have been exploited to analyze relatedness among tasks
and task id91 techniques, such as [71], [72], which
may help provide guidance on how to avoid negative transfer
automatically. bakker and heskes [72] adopted a bayesian
approach in which some of the model parameters are shared
for all tasks and others more loosely connected through a joint
prior distribution that can be learned from the data. thus,
the data are clustered based on the task parameters, where
tasks in the same cluster are supposed to be related to each
others. argyriou et al. [73] considered situations in which the
learning tasks can be divided into groups. tasks within each
group are related by sharing a low-dimensional representation,
which differs among different groups. as a result, tasks within
a group can    nd it easier to transfer useful knowledge.

7 applications of id21
recently, id21 techniques have been applied suc-
cessfully in many real-world applications. raina et al. [74] and
dai et al. [36], [28] proposed to use id21 tech-
niques to learn text data across domains, respectively. blitzer
et al. [38] proposed to use scl for solving nlp problems. an

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

11

extension of scl was proposed in [8] for solving sentiment
classi   cation problems. wu and dietterich [53] proposed to
use both inadequate target domain data and plenty of low
quality source domain data for image classi   cation problems.
arnold et al. [58] proposed to use transductive transfer learn-
ing methods to solve name-entity recognition problems. in
[75], [76], [77], [78], [79], id21 techniques are
proposed to extract knowledge from wifi localization models
across time periods, space and mobile devices, to bene   t wifi
localization tasks in other settings. zhuo et al. [80] studied how
to transfer domain knowledge to learn relational action models
across domains in automated planning.

in [81], raykar et al. proposed a novel bayesian multiple-
instance learning algorithm, which can automatically identify
the relevant feature subset and use inductive transfer for
learning multiple, but conceptually related, classi   ers, for
computer aided design (cad). in [82], ling et al. proposed an
information-theoretic approach for id21 to address
the cross-language classi   cation problem for translating web
pages from english to chinese. the approach addressed the
problem when there are plenty of labeled english text data
whereas there are only a small number of labeled chinese text
documents. id21 across the two feature spaces are
achieved by designing a suitable mapping function as a bridge.
so far, there are at least two international competitions based
on id21, which made available some much needed
public data. in the ecml/pkdd-2006 discovery challenge
8, the task was to handle personalized spam    ltering and
generalization across related learning tasks. for training a
spam-   ltering system, we need to collect a lot of emails from
a group of users with corresponding labels: spam or not spam,
and train a classi   er based on these data. for a new email user,
we might want to adapt the learned model for the user. the
challenge is that the distributions of emails for the    rst set of
users and the new user are different. thus, this problem can
be modeled as an inductive id21 problem, which
aims to adapt an old spam-   ltering model to a new situation
with fewer training data and less training time.

a second data set was made available through the icdm-
2007 contest, in which a task was to estimate a wifi client   s
indoor locations using the wifi signal data obtained over
different periods of time [83]. since the values of wifi
signal strength may be a function of time, space and devices,
distributions of wifi data over different time periods may be
very different. thus, id21 must be designed to
reduce the data re-labeling effort.
data sets for id21: so far, several data
sets have been published for id21 research. we
denote the id111 data sets, email spam-   ltering data
set, the wifi localization over time periods data set and the
sentiment classi   cation data set by text, email, wifi and
sen, respectively.

text three data sets, 20 newsgroups, sraa and reuters-
21578 9, have been preprocessed for a transfer learn-
ing setting by some researchers. the data in these

8. http://www.ecmlpkdd2006.org/challenge.html
9. http://apex.sjtu.edu.cn/apex wiki/dwyak

data sets are categorized to a hierarchical structure.
data from different sub-categories under the same
parent category are considered to be from different
but related domains. the task is to predict the labels
of the parent category.

email this data set is provided by the 2006 ecml/pkdd

discovery challenge.

sen

wifi this data set is provided by the icdm-2007 contest
10. the data were collected inside a building for
localization around 145.5   37.5 m2 in two different
time periods.
this data set was    rst used in [8]11. this data set con-
tains product reviews downloaded from amazon.com
from 4 product types (domains): kitchen, books,
dvds, and electronics. each domain has several
thousand reviews, but the exact number varies by
domain. reviews contain star ratings (1 to 5 stars).
empirical evaluation to show how much bene   t transfer
learning methods can bring as compared to traditional learning
methods, researchers have used some public data sets. we
show a list
taken from some published id21
papers in table 5. in [6], [84], [49], the authors used the 20
newsgroups data 12 as one of the evaluation data sets. due
to the differences in the preprocessing steps of the algorithms
by different researchers, it is hard to compare the proposed
methods directly. thus, we denote them by 20-newsgroups1,
20-newsgroups2 and 20-newsgroups3, respectively, and show
the comparison results between the proposed id21
methods and non-id21 methods in the table.

on the 20 newsgroups1 data, dai et al. [6] showed the
comparison experiments between standard support vector
machine (id166) and the proposed tradaboost algorithm. on
20 newsgroups2, shi et al. [84] applied an active learning
algorithm to select important instances for id21
(actrak) with tradaboost and standard id166. gao et al. [49]
evaluated their proposed locally weighted id108
algorithms, plwe and lwe, on the 20 newsgroups3, com-
pared to id166 and id28 (lr).

in addition, in the table, we also show the comparison
results on the sentiment classi   cation data set reported in [8].
on this data set, sgd denotes the stochastic gradient-descent
algorithm with huber loss, scl represents a linear predictor
on the new representations learned by structural correspon-
dence learning algorithm, and scl-mi is an extension of scl
by applying mutual information to select the pivot features for
the scl algorithm.

finally, on the wifi localization data set, we show the
comparison results reported in [67], where the baseline is a
regularized least square regression model (rlsr), which is
a standard regression model, and kpca, which represents to
apply rlsr on the new representations of the data learned by
kernel principle component analysis. the compared transfer
learning methods include kernel mean matching (kmm) and
the proposed algorithm, transfer component analysis (tca).

10. http://www.cse.ust.hk/   qyang/icdmdmc2007
11. http://www.cis.upenn.edu/   mdredze/datasets/sentiment/
12. http://people.csail.mit.edu/jrennie/20newsgroups/

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

12

for more detail about the experimental results, the readers
may refer to the reference papers showed in the table. from
these comparison results, we can    nd that the id21
methods designed appropriately for real world applications can
indeed improve the performance signi   cantly compared to the
non-id21 methods.
toolboxes for id21: researchers at uc berke-
ley provided a matlab toolkit for id21 13.
the toolkit contains algorithms and benchmark data sets for
id21. in addition, it provides a standard platform
for developing and testing new algorithms for id21.

7.1 other applications of id21
id21 has found many applications in sequential
machine learning as well. for example, [85] proposed a graph-
based method for identifying previously encountered games,
and applied this technique to automate domain mapping for
value function transfer and speed up id23
on variants of previously played games. a new approach to
transfer between entirely different feature spaces is proposed
in translated learning, which is made possible by learning
a mapping function for bridging features in two entirely
different domains (images and text) [86]. finally, li et al. [87],
[88] have applied id21 to collaborative    ltering
problems to solve the cold start and sparsity problems. in [87],
li et al. learned a shared rating-pattern mixture model, known
as a rating-matrix generative model (rmgm), in terms of
the latent user- and item-cluster variables. rmgm bridges
multiple rating matrices from different domains by mapping
the users and items in each rating matrix onto the shared latent
user and item spaces in order to transfer useful knowledge.
in [88], they applied co-id91 algorithms on users and
items in an auxiliary rating matrix. they then constructed a
cluster-level rating matrix known as a codebook. by assuming
the target rating matrix (on movies) is related to the auxiliary
one (on books), the target domain can be reconstructed by
expanding the codebook, completing the knowledge transfer
process.

8 conclusions
in this survey article, we have reviewed several current trends
of id21. id21 is classi   ed to three
different settings:
transductive
id21 and unsupervised id21. most pre-
vious works focused on the former two settings. unsupervised
id21 may attract more and more attention in the
future.

inductive transfer

learning,

furthermore, each of the approaches to id21
can be classi   ed into four contexts based on    what to trans-
fer    in learning. they include the instance-transfer approach,
the feature-representation-transfer approach,
the parameter-
transfer approach and the relational-knowledge-transfer ap-
proach, respectively. the former three contexts have an i.i.d
assumption on the data while the last context deals with
id21 on relational data. most of these approaches

13. http://multitask.cs.berkeley.edu/

assume that the selected source domain is related to the target
domain.

in the future, several important research issues need to be
addressed. first, how to avoid negative transfer is an open
problem. as mentioned in section 6, many proposed transfer
learning algorithms assume that the source and target domains
are related to each other in some sense. however, if the as-
sumption does not hold, negative transfer may happen, which
may cause the learner to perform worse than no transferring at
all. thus, how to make sure that no negative transfer happens
is a crucial issue in id21. in order to avoid negative
id21, we need to    rst study transferability between
source domains or tasks and target domains or tasks. based on
suitable transferability measures, we can then select relevant
source domains or tasks to extract knowledge from for learning
the target tasks. to de   ne the transferability between domains
and tasks, we also need to de   ne the criteria to measure the
similarity between domains or tasks. based on the distance
measures, we can then cluster domains or tasks, which may
help measure transferability. a related issue is when an entire
domain cannot be used for id21, whether we can
still transfer part of the domain for useful learning in the target
domain.

in addition, most existing id21 algorithms so far
focused on improving generalization across different distribu-
tions between source and target domains or tasks. in doing so,
they assumed that the feature spaces between the source and
target domains are the same. however, in many applications,
we may wish to transfer knowledge across domains or tasks
that have different feature spaces, and transfer from multiple
such source domains. we refer to this type of id21
as heterogeneous id21.

finally, so far id21 techniques have been mainly
applied to small scale applications with a limited variety, such
as sensor-network-based localization, text classi   cation and
image classi   cation problems. in the future, id21
techniques will be widely used to solve other challenging ap-
plications, such as video classi   cation, social network analysis
and logical id136.

acknowledgment
we thank the support of hong kong cerg project 621307
and a grant from nec china lab.

references
[1] x. wu, v. kumar, j. r. quinlan, j. ghosh, q. yang, h. motoda, g. j.
mclachlan, a. f. m. ng, b. liu, p. s. yu, z.-h. zhou, m. steinbach,
d. j. hand, and d. steinberg,    top 10 algorithms in data mining,   
knowl. inf. syst., vol. 14, no. 1, pp. 1   37, 2008.

[2] q. yang and x. wu,    10 challenging problems in data mining research,   
international journal of information technology and decision making,
vol. 5, no. 4, pp. 597   604, 2006.

[3] g. p. c. fung, j. x. yu, h. lu, and p. s. yu,    text classi   cation without
negative examples revisit,    ieee transactions on knowledge and data
engineering, vol. 18, no. 1, pp. 6   20, 2006.

[4] h. al-mubaid and s. a. umair,    a new text categorization technique
using distributional id91 and learning logic,    ieee transactions
on knowledge and data engineering, vol. 18, no. 9, pp. 1156   1165,
2006.

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

13

comparison between id21 and non-id21 methods

table 5

data set (reference)
20 newsgroups1 ([6])
acc (unit: %)

20 newsgroups2 ([84])
acc (unit: %)

20 newsgroups3 ([49])
acc (unit: %)

sentiment classi   cation ([8])
acc (unit: %)

wifi localization ([67])
aed (unit: meter)

source v.s. target

rec v.s. talk
rec v.s. sci
sci v.s. talk

rec v.s. talk
rec v.s. sci
comp v.s. talk
comp v.s. sci
comp v.s. rec
sci v.s. talk

comp v.s. sci
rec v.s. talk
rec v.s. sci
sci v.s. talk
comp v.s. rec
comp v.s. talk

dvd v.s. book
electronics v.s. book
kitchen v.s. book
book v.s. dvd
electronics v.s. dvd
kitchen v.s. dvd
book v.s. electronics
dvd v.s. electronics
kitchen v.s. electronics
book v.s. kitchen
dvd v.s. kitchen
electronics v.s. kitchen

time a v.s. time b

baselines

lr

id166
87.3%
83.6%
82.3%
id166
60.2%
59.1%
53.6%
52.7%
49.1%
57.6%
id166
71.18% 73.49%
68.24% 72.17%
78.16% 78.85%
75.77% 79.04%
81.56% 83.34%
93.89% 91.76%
sgd
72.8%
70.7%
70.9%
77.2%
70.6%
72.7%
70.8%
73.0%
82.7%
74.5%
74.0%
84.0%
rlsr
6.52

pca
3.16

tl methods

tradaboost
92.0%
90.3%
87.5%
tradaboost
72.3%
67.4%
74.4%
57.3%
77.2%
71.3%
plwe
78.72%
72.17%
88.45%
83.30%
91.93%
96.64%
scl
76.8%
75.4%
66.1%
74.0%
74.3%
75.4%
77.5%
74.1%
83.7%
78.7%
79.4%
84.4%
kmm
5.51

actrak
75.4%
70.6%
80.9%
78.0%
82.1%
75.1%
lwe
97.44%
99.23%
98.23%
96.92%
98.16%
98.90%
scl-mi
79.7%
75.4%
68.6%
75.8%
76.2%
76.9%
75.9%
74.1%
86.8%
78.9%
81.4%
85.9%
tca
2.37

[9]

[7]

[8]

[5] k. sarinnapakorn and m. kubat,    combining subclassi   ers in text cat-
egorization: a dst-based solution and a case study,    ieee transactions
on knowledge and data engineering, vol. 19, no. 12, pp. 1638   1651,
2007.

[6] w. dai, q. yang, g. xue, and y. yu,    boosting for id21,    in
proceedings of the 24th international conference on machine learning,
corvalis, oregon, usa, june 2007, pp. 193   200.
s. j. pan, v. w. zheng, q. yang, and d. h. hu,    id21
for wi   -based indoor localization,    in proceedings of the workshop on
id21 for complex task of the 23rd aaai conference on
arti   cial intelligence, chicago, illinois, usa, july 2008.
j. blitzer, m. dredze, and f. pereira,    biographies, bollywood, boom-
boxes and blenders: id20 for sentiment classi   cation,   
in proceedings of
the association of
computational linguistics, prague, czech republic, 2007, pp. 432   439.
j. ramon, k. driessens, and t. croonenborghs,    id21 in
id23 problems through partial policy recycling,    in
ecml    07: proceedings of the 18th european conference on machine
learning. berlin, heidelberg: springer-verlag, 2007, pp. 699   707.

the 45th annual meeting of

[10] m. e. taylor and p. stone,    cross-domain transfer for reinforcement
learning,    in icml    07: proceedings of the 24th international conference
on machine learning. new york, ny, usa: acm, 2007, pp. 879   886.
[11] x. yin, j. han, j. yang, and p. s. yu,    ef   cient classi   cation across
multiple database relations: a crossmine approach,    ieee transactions
on knowledge and data engineering, vol. 18, no. 6, pp. 770   783, 2006.
[12] l. i. kuncheva and j. j. rodr  guez,    classi   er ensembles with a random
linear oracle,    ieee transactions on knowledge and data engineering,
vol. 19, no. 4, pp. 500   508, 2007.

[13] e. baralis, s. chiusano, and p. garza,    a lazy approach to associative
classi   cation,    ieee transactions on knowledge and data engineering,
vol. 20, no. 2, pp. 156   171, 2008.

[14] x. zhu,    semi-supervised learning literature survey,    university of

wisconsin   madison, tech. rep. 1530, 2006.

[15] k. nigam, a. k. mccallum, s. thrun, and t. mitchell,    text clas-
si   cation from labeled and unlabeled documents using em,    machine
learning, vol. 39, no. 2-3, pp. 103   134, 2000.

[16] a. blum and t. mitchell,    combining labeled and unlabeled data with
the eleventh annual conference on

co-training,    in proceedings of
computational learning theory, 1998, pp. 92   100.

[17] t. joachims,    transductive id136 for text classi   cation using support
vector machines,    in proceedings of sixteenth international conference
on machine learning, 1999, pp. 825   830.

[18] x. zhu and x. wu,    class noise handling for effective cost-sensitive
learning by cost-guided iterative classi   cation    ltering,    ieee transac-
tions on knowledge and data engineering, vol. 18, no. 10, pp. 1435   
1440, 2006.

[19] q. yang, c. ling, x. chai, and r. pan,    test-cost sensitive classi   cation
on data with missing values,    ieee transactions on knowledge and
data engineering, vol. 18, no. 5, pp. 626   638, 2006.

[20] s. thrun and l. pratt, eds., learning to learn. norwell, ma, usa:

kluwer academic publishers, 1998.

[21] r. caruana,    multitask learning,    machine learning, vol. 28(1), pp. 41   

75, 1997.

[22] r. raina, a. battle, h. lee, b. packer, and a. y. ng,    self-taught
learning: id21 from unlabeled data,    in proceedings of the
24th international conference on machine learning, corvalis, oregon,
usa, june 2007, pp. 759   766.

[23] h. daum  eiii and d. marcu,    id20 for statistical classi-
   ers,    journal of arti   cial intelligence research, vol. 26, pp. 101   126,
2006.

[24] b. zadrozny,    learning and evaluating classi   ers under sample selection
bias,    in proceedings of the 21st international conference on machine
learning, banff, alberta, canada, july 2004.

[25] h. shimodaira,    improving predictive id136 under covariate shift by

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

14

weighting the log-likelihood function,    journal of statistical planning
and id136, vol. 90, pp. 227   244, 2000.

[26] w. dai, q. yang, g. xue, and y. yu,    self-taught id91,    in
proceedings of the 25th international conference of machine learning.
acm, july 2008, pp. 200   207.

[27] z. wang, y. song, and c. zhang,    transferred dimensionality reduc-
tion,    in machine learning and knowledge discovery in databases, eu-
ropean conference, ecml/pkdd 2008. antwerp, belgium: springer,
september 2008, pp. 550   565.

[28] w. dai, g. xue, q. yang, and y. yu,    transferring naive bayes classi   ers
for text classi   cation,    in proceedings of the 22rd aaai conference on
arti   cial intelligence, vancouver, british columbia, canada, july 2007,
pp. 540   545.

[29] j. quionero-candela, m. sugiyama, a. schwaighofer, and n. d.
lawrence, dataset shift in machine learning. the mit press, 2009.
[30] j. jiang and c. zhai,    instance weighting for id20 in
nlp,    in proceedings of the 45th annual meeting of the association of
computational linguistics.
prague, czech republic: association for
computational linguistics, june 2007, pp. 264   271.

[31] x. liao, y. xue, and l. carin,    id28 with an auxiliary
data source,    in proceedings of the 21st international conference on
machine learning, bonn, germany, august 2005, pp. 505   512.

[32] j. huang, a. smola, a. gretton, k. m. borgwardt, and b. sch  olkopf,
   correcting sample selection bias by unlabeled data,    in proceedings of
the 19th annual conference on neural information processing systems,
2007.

[33] s. bickel, m. br  uckner, and t. scheffer,    discriminative learning for
differing training and test distributions,    in proceedings of the 24th
international conference on machine learning. new york, ny, usa:
acm, 2007, pp. 81   88.

[34] m. sugiyama, s. nakajima, h. kashima, p. v. buenau, and m. kawan-
abe,    direct importance estimation with model selection and its appli-
cation to covariate shift adaptation,    in proceedings of the 20th annual
conference on neural information processing systems, vancouver,
british columbia, canada, december 2008.

[35] w. fan, i. davidson, b. zadrozny, and p. s. yu,    an improved
categorization of classi   er   s sensitivity on sample selection bias,    in
proceedings of the 5th ieee international conference on data mining,
2005.

[36] w. dai, g. xue, q. yang, and y. yu,    co-id91 based classi   ca-
tion for out-of-domain documents,    in proceedings of the 13th acm
sigkdd international conference on knowledge discovery and data
mining, san jose, california, usa, august 2007.

[37] r. k. ando and t. zhang,    a high-performance semi-supervised learn-
ing method for text chunking,    in proceedings of the 43rd annual
meeting on association for computational linguistics. morristown,
nj, usa: association for computational linguistics, 2005, pp. 1   9.

[38] j. blitzer, r. mcdonald, and f. pereira,    id20 with
structural correspondence learning,    in proceedings of the conference on
empirical methods in natural language, sydney, australia, july 2006,
pp. 120   128.

[39] h. daum  e iii,    frustratingly easy id20,    in proceedings
of the 45th annual meeting of the association of computational lin-
guistics, prague, czech republic, june 2007, pp. 256   263.

[40] a. argyriou, t. evgeniou, and m. pontil,    multi-task id171,   
in proceedings of the 19th annual conference on neural information
processing systems, vancouver, british columbia, canada, december
2007, pp. 41   48.

[41] a. argyriou, c. a. micchelli, m. pontil, and y. ying,    a spectral regu-
larization framework for multi-task structure learning,    in proceedings of
the 20th annual conference on neural information processing systems.
cambridge, ma: mit press, 2008, pp. 25   32.

[42] s.-i. lee, v. chatalbashev, d. vickrey, and d. koller,    learning a
meta-level prior for feature relevance from multiple related tasks,    in
proceedings of the 24th international conference on machine learning.
corvalis, oregon, usa: acm, july 2007, pp. 489   496.

[43] t. jebara,    multi-task feature and kernel selection for id166s,    in proceed-
ings of the 21st international conference on machine learning. banff,
alberta, canada: acm, july 2004.

[44] c. wang and s. mahadevan,    manifold alignment using procrustes
analysis,    in proceedings of
the 25th international conference on
machine learning. helsinki, finland: acm, july 2008, pp. 1120   1127.
[45] n. d. lawrence and j. c. platt,    learning to learn with the informative
vector machine,    in proceedings of the 21st international conference
on machine learning. banff, alberta, canada: acm, july 2004.

[46] e. bonilla, k. m. chai, and c. williams,    multi-task gaussian process
prediction,    in proceedings of the 20th annual conference on neural

information processing systems. cambridge, ma: mit press, 2008,
pp. 153   160.

[47] a. schwaighofer, v. tresp, and k. yu,    learning gaussian process
kernels via hierarchical bayes,    in proceedings of
the 17th annual
conference on neural information processing systems. cambridge,
ma: mit press, 2005, pp. 1209   1216.

[48] t. evgeniou and m. pontil,    regularized id72,    in
proceedings of the 10th acm sigkdd international conference on
knowledge discovery and data mining.
seattle, washington, usa:
acm, august 2004, pp. 109   117.

[49] j. gao, w. fan, j. jiang, and j. han,    knowledge transfer via multiple
the 14th acm
model
sigkdd international conference on knowledge discovery and data
mining. las vegas, nevada: acm, august 2008, pp. 283   291.

local structure mapping,    in proceedings of

[50] l. mihalkova, t. huynh, and r. j. mooney,    mapping and revising
markov logic networks for id21,    in proceedings of the
22nd aaai conference on arti   cial intelligence, vancouver, british
columbia, canada, july 2007, pp. 608   614.

[51] l. mihalkova and r. j. mooney,    id21 by mapping with
minimal target data,    in proceedings of the aaai-2008 workshop on
id21 for complex tasks, chicago, illinois, usa, july 2008.
[52] j. davis and p. domingos,    deep transfer via second-order markov
logic,    in proceedings of the aaai-2008 workshop on id21
for complex tasks, chicago, illinois, usa, july 2008.

[53] p. wu and t. g. dietterich,    improving id166 accuracy by training
on auxiliary data sources,    in proceedings of the 21st international
conference on machine learning. banff, alberta, canada: acm, july
2004.

[54] u. r  uckert and s. kramer,    kernel-based inductive transfer,    in machine
learning and knowledge discovery in databases, european confer-
ence, ecml/pkdd 2008, ser. lecture notes in computer science.
antwerp, belgium: springer, september 2008, pp. 220   233.

[55] h. lee, a. battle, r. raina, and a. y. ng,    ef   cient sparse coding
algorithms,    in proceedings of the the 19th annual conference on neural
information processing systemss. cambridge, ma: mit press, 2007,
pp. 801   808.

[56] m. richardson and p. domingos,    markov logic networks,    machine

learning journal, vol. 62, no. 1-2, pp. 107   136, 2006.

[57] s. ramachandran and r. j. mooney,    theory re   nement of bayesian
networks with hidden variables,    in proceedings of the 14th interna-
tional conference on machine learning, madison, wisconson, usa,
july 1998, pp. 454   462.

[58] a. arnold, r. nallapati, and w. w. cohen,    a comparative study of
methods for transductive id21,    in proceedings of the 7th
ieee international conference on data mining workshops. washing-
ton, dc, usa: ieee computer society, 2007, pp. 77   82.

[59] t. joachims,    transductive id136 for text classi   cation using support
vector machines,    in proceedings of the sixteenth international confer-
ence on machine learning, san francisco, ca, usa, 1999, pp. 200   
209.

[60] v. n. vapnik, statistical learning theory.

new york: wiley-

interscience, september 1998.

[61] s. ben-david, j. blitzer, k. crammer, and f. pereira,    analysis of rep-
resentations for id20,    in proceedings of the 20th annual
conference on neural information processing systems.
cambridge,
ma: mit press, 2007, pp. 137   144.

[62] j. blitzer, k. crammer, a. kulesza, f. pereira, and j. wortman,    learn-
ing bounds for id20,    in proceedings of the 21st annual
conference on neural information processing systems.
cambridge,
ma: mit press, 2008, pp. 129   136.

[63] d. xing, w. dai, g.-r. xue, and y. yu,    bridged re   nement for transfer
learning,    in 11th european conference on principles and practice of
knowledge discovery in databases, ser. lecture notes in computer
science. warsaw, poland: springer, september 2007, pp. 324   335.

[64] x. ling, w. dai, g.-r. xue, q. yang, and y. yu,    spectral domain-
id21,    in proceedings of the 14th acm sigkdd interna-
tional conference on knowledge discovery and data mining.
las
vegas, nevada: acm, august 2008, pp. 488   496.

[65] g.-r. xue, w. dai, q. yang, and y. yu,    topic-bridged plsa for
cross-domain text classi   cation,    in proceedings of the 31st annual
international acm sigir conference on research and development
in information retrieval. singapore: acm, july 2008, pp. 627   634.
[66] s. j. pan, j. t. kwok, and q. yang,    id21 via dimensionality
reduction,    in proceedings of the 23rd aaai conference on arti   cial
intelligence, chicago, illinois, usa, july 2008, pp. 677   682.

this article has been accepted for publication in a future issue of this journal, but has not been fully edited. content may change prior to final publication.

ieee transactions on knowledge and data engineering

15

26th international conference on machine learning, montreal, quebec,
canada, june 2009.

[88] b. li, q. yang, and x. xue,    can movies and books collaborate? - cross-
domain collaborative    ltering for sparsity reduction,    in proceedings
of the 21st international joint conference on arti   cial intelligence,
pasadena, california, usa, july 2009.

sinno jialin pan is a phd candidate in the
department of computer science and engineer-
ing, the hong kong university of science and
technology. he received the ms and bs de-
grees from applied mathematics department,
sun yat-sen university, china,
in 2003 and
2005, respectively. his research interests in-
clude id21, semi-supervised learn-
ing, and their applications in pervasive comput-
ing and web mining. he is a member of aaai.
the department of computer
contact him at
science and engineering, hong kong univ. of science and tech-
nology, clearwater bay, kowloon, hong kong; sinnopan@cse.ust.hk;
http://www.cse.ust.hk/   sinnopan.

qiang yang is a faculty member in the hong
kong university of science and technology   s
department of computer science and engineer-
ing. his research interests are data mining and
machine learning, ai planning and sensor based
activity recognition. he received his phd de-
gree in computer science from the university of
maryland, college park, and bachelor   s degree
from peking university in astrophysics. he is a
fellow of ieee, member of aaai and acm, a
former associate editor for the ieee tkde, and
a current associate editor for ieee intelligent systems. contact him
at the department of computer science and engineering, hong kong
univ. of science and technology, clearwater bay, kowloon, hong kong;
qyang@cse.ust.hk; http://www.cse.ust.hk/   qyang.

[67] s. j. pan, i. w. tsang, j. t. kwok, and q. yang,    id20 via
transfer component analysis,    in proceedings of the 21st international
joint conference on arti   cial intelligence, pasadena, california, 2009.
[68] m. m. h. mahmud and s. r. ray,    id21 using kolmogorov
complexity: basic theory and empirical evaluations,    in proceedings of
the 20th annual conference on neural information processing systems.
cambridge, ma: mit press, 2008, pp. 985   992.

[69] e. eaton, m. desjardins, and t. lane,    modeling transfer relationships
between learning tasks for improved inductive transfer,    in machine
learning and knowledge discovery in databases, european confer-
ence, ecml/pkdd 2008, ser. lecture notes in computer science.
antwerp, belgium: springer, september 2008, pp. 317   332.

[70] m. t. rosenstein, z. marx, and l. p. kaelbling,    to transfer or not to
transfer,    in a nips-05 workshop on inductive transfer: 10 years later,
december 2005.

[71] s. ben-david and r. schuller,    exploiting task relatedness for multiple
task learning,    in proceedings of the sixteenth annual conference on
learning theory. san francisco: morgan kaufmann, 2003, pp. 825   
830.

[72] b. bakker and t. heskes,    task id91 and gating for bayesian
multitask learning,    journal of machine learning reserch, vol. 4, pp.
83   99, 2003.

[73] a. argyriou, a. maurer, and m. pontil,    an algorithm for transfer learn-
ing in a heterogeneous environment,    in machine learning and knowl-
edge discovery in databases, european conference, ecml/pkdd
2008, ser. lecture notes in computer science. antwerp, belgium:
springer, september 2008, pp. 71   85.

[74] r. raina, a. y. ng, and d. koller,    constructing informative priors using
id21.    in proceedings of the 23rd international conference
on machine learning, pittsburgh, pennsylvania, usa, june 2006, pp.
713   720.

[75] j. yin, q. yang, and l. m. ni,    adaptive temporal radio maps for indoor
location estimation,    in proceedings of
the 3rd ieee international
conference on pervasive computing and communications, kauai island,
hawaii, usa, march 2005.

[76] s. j. pan, j. t. kwok, q. yang, and j. j. pan,    adaptive localization in
a dynamic wifi environment through multi-view learning,    in proceed-
ings of the 22nd aaai conference on arti   cial intelligence, vancouver,
british columbia, canada, july 2007, pp. 1108   1113.

[77] v. w. zheng, q. yang, w. xiang, and d. shen,    transferring localization
models over time,    in proceedings of the 23rd aaai conference on
arti   cial intelligence, chicago, illinois, usa, july 2008, pp. 1421   1426.
[78] s. j. pan, d. shen, q. yang, and j. t. kwok,    transferring localization
models across space,    in proceedings of the 23rd aaai conference on
arti   cial intelligence, chicago, illinois, usa, july 2008, pp. 1383   1388.
[79] v. w. zheng, s. j. pan, q. yang, and j. j. pan,    transferring multi-device
localization models using latent id72,    in proceedings of
the 23rd aaai conference on arti   cial intelligence, chicago, illinois,
usa, july 2008, pp. 1427   1432.

[80] h. zhuo, q. yang, d. h. hu, and l. li,    transferring knowledge
from another domain for learning action models,    in proceedings of
10th paci   c rim international conference on arti   cial intelligence,
december 2008.

[81] v. c. raykar, b. krishnapuram, j. bi, m. dundar, and r. b. rao,
   bayesian multiple instance learning: automatic feature selection and
inductive transfer,    in proceedings of the 25th international conference
on machine learning. helsinki, finland: acm, july 2008, pp. 808   815.
[82] x. ling, g.-r. xue, w. dai, y. jiang, q. yang, and y. yu,    can chinese
web pages be classi   ed with english data source?    in proceedings of
the 17th international conference on world wide web. beijing, china:
acm, april 2008, pp. 969   978.

[83] q. yang, s. j. pan, and v. w. zheng,    estimating location using wi-fi,   

ieee intelligent systems, vol. 23, no. 1, pp. 8   13, 2008.

[84] x. shi, w. fan, and j. ren,    actively transfer domain knowledge,    in
machine learning and knowledge discovery in databases, european
conference, ecml/pkdd 2008, ser. lecture notes in computer sci-
ence. antwerp, belgium: springer, september 2008, pp. 342   357.

[85] g. kuhlmann and p. stone,    graph-based domain mapping for transfer
learning in general games,    in 18th european conference on machine
learning, ser. lecture notes in computer science. warsaw, poland:
springer, september 2007, pp. 188   200.

[86] w. dai, y. chen, g.-r. xue, q. yang, and y. yu,    translated learning,   
in proceedings of 21st annual conference on neural information
processing systems, 2008.

[87] b. li, q. yang, and x. xue,    id21 for collaborative
   ltering via a rating-matrix generative model,    in proceedings of the

