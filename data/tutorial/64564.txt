understanding machine learning:

from theory to algorithms

c(cid:13) 2014 by shai shalev-shwartz and shai ben-david

published 2014 by cambridge university press.

this copy is for personal use only. not for distribution.

do not post. please link to:

http://www.cs.huji.ac.il/~shais/understandingmachinelearning

please note: this copy is almost, but not entirely, identical to the printed version
of the book. in particular, page numbers are not identical (but section numbers are the
same).

understandingmachinelearningmachinelearningisoneofthefastestgrowingareasofcomputerscience,withfar-reachingapplications.theaimofthistextbookistointroducemachinelearning,andthealgorithmicparadigmsitoffers,inaprinci-pledway.thebookprovidesanextensivetheoreticalaccountofthefundamentalideasunderlyingmachinelearningandthemathematicalderivationsthattransformtheseprinciplesintopracticalalgorithms.fol-lowingapresentationofthebasicsofthe   eld,thebookcoversawidearrayofcentraltopicsthathavenotbeenaddressedbyprevioustext-books.theseincludeadiscussionofthecomputationalcomplexityoflearningandtheconceptsofconvexityandstability;importantalgorith-micparadigmsincludingstochasticgradientdescent,neuralnetworks,andstructuredoutputlearning;andemergingtheoreticalconceptssuchasthepac-bayesapproachandcompression-basedbounds.designedforanadvancedundergraduateorbeginninggraduatecourse,thetextmakesthefundamentalsandalgorithmsofmachinelearningaccessibletostu-dentsandnonexpertreadersinstatistics,computerscience,mathematics,andengineering.shaishalev-shwartzisanassociateprofessorattheschoolofcomputerscienceandengineeringatthehebrewuniversity,israel.shaiben-davidisaprofessorintheschoolofcomputerscienceattheuniversityofwaterloo,canada.understandingmachinelearningfromtheorytoalgorithmsshaishalev-shwartzthehebrewuniversity,jerusalemshaiben-daviduniversityofwaterloo,canada32avenueoftheamericas,newyork,ny10013-2473,usacambridgeuniversitypressispartoftheuniversityofcambridge.itfurtherstheuniversity   smissionbydisseminatingknowledgeinthepursuitofeducation,learningandresearchatthehighestinternationallevelsofexcellence.www.cambridge.orginformationonthistitle:www.cambridge.org/9781107057135c   shaishalev-shwartzandshaiben-dav 014thispublicationisincopyright.subjecttostatutoryexceptionandtotheprovisionsofrelevantcollectivelicensingagreements,noreproductionofanypartmaytakeplacewithoutthewrittenpermissionofcambridgeuniversitypress.firstpublished2014printedintheunitedstatesofamericaacatalogrecordforthispublicationisavailablefromthebritishlibrarylibraryofcongresscataloginginpublicationdataisbn978-1-107-05713-5hardbackcambridgeuniversitypresshasnoresponsibilityforthepersistenceoraccuracyofurlsforexternalorthird-partyinternetwebsitesreferredtointhispublication,anddoesnotguaranteethatanycontentonsuchwebsitesis,orwillremain,accurateorappropriate.triple-sdedicatesthebooktotriple-mvii

preface

the term machine learning refers to the automated detection of meaningful
patterns in data. in the past couple of decades it has become a common tool in
almost any task that requires information extraction from large data sets. we are
surrounded by a machine learning based technology: search engines learn how
to bring us the best results (while placing pro   table ads), anti-spam software
learns to    lter our email messages, and credit card transactions are secured by
a software that learns how to detect frauds. digital cameras learn to detect
faces and intelligent personal assistance applications on smart-phones learn to
recognize voice commands. cars are equipped with accident prevention systems
that are built using machine learning algorithms. machine learning is also widely
used in scienti   c applications such as bioinformatics, medicine, and astronomy.
one common feature of all of these applications is that, in contrast to more
traditional uses of computers, in these cases, due to the complexity of the patterns
that need to be detected, a human programmer cannot provide an explicit,    ne-
detailed speci   cation of how such tasks should be executed. taking example from
intelligent beings, many of our skills are acquired or re   ned through learning from
our experience (rather than following explicit instructions given to us). machine
learning tools are concerned with endowing programs with the ability to    learn   
and adapt.

the    rst goal of this book is to provide a rigorous, yet easy to follow, intro-
duction to the main concepts underlying machine learning: what is learning?
how can a machine learn? how do we quantify the resources needed to learn a
given concept? is learning always possible? can we know if the learning process
succeeded or failed?

the second goal of this book is to present several key machine learning algo-
rithms. we chose to present algorithms that on one hand are successfully used
in practice and on the other hand give a wide spectrum of di   erent learning
techniques. additionally, we pay speci   c attention to algorithms appropriate for
large scale learning (a.k.a.    big data   ), since in recent years, our world has be-
come increasingly    digitized    and the amount of data available for learning is
dramatically increasing. as a result, in many applications data is plentiful and
computation time is the main bottleneck. we therefore explicitly quantify both
the amount of data and the amount of computation time needed to learn a given
concept.

the book is divided into four parts. the    rst part aims at giving an initial
rigorous answer to the fundamental questions of learning. we describe a gen-
eralization of valiant   s probably approximately correct (pac) learning model,
which is a    rst solid answer to the question    what is learning?   . we describe
the empirical risk minimization (erm), structural risk minimization (srm),
and minimum description length (mdl) learning rules, which shows    how can
a machine learn   . we quantify the amount of data needed for learning using
the erm, srm, and mdl rules and show how learning might fail by deriving

viii

a    no-free-lunch    theorem. we also discuss how much computation time is re-
quired for learning. in the second part of the book we describe various learning
algorithms. for some of the algorithms, we    rst present a more general learning
principle, and then show how the algorithm follows the principle. while the    rst
two parts of the book focus on the pac model, the third part extends the scope
by presenting a wider variety of learning models. finally, the last part of the
book is devoted to advanced theory.

we made an attempt to keep the book as self-contained as possible. however,
the reader is assumed to be comfortable with basic notions of id203, linear
algebra, analysis, and algorithms. the    rst three parts of the book are intended
for    rst year graduate students in computer science, engineering, mathematics, or
statistics. it can also be accessible to undergraduate students with the adequate
background. the more advanced chapters can be used by researchers intending
to gather a deeper theoretical understanding.

acknowledgements

the book is based on introduction to machine learning courses taught by shai
shalev-shwartz at the hebrew university and by shai ben-david at the univer-
sity of waterloo. the    rst draft of the book grew out of the lecture notes for
the course that was taught at the hebrew university by shai shalev-shwartz
during 2010   2013. we greatly appreciate the help of ohad shamir, who served
as a ta for the course in 2010, and of alon gonen, who served as a ta for the
course in 2011   2013. ohad and alon prepared few lecture notes and many of
the exercises. alon, to whom we are indebted for his help throughout the entire
making of the book, has also prepared a solution manual.

we are deeply grateful for the most valuable work of dana rubinstein. dana
has scienti   cally proofread and edited the manuscript, transforming it from
lecture-based chapters into    uent and coherent text.

special thanks to amit daniely, who helped us with a careful read of the
advanced part of the book and also wrote the advanced chapter on multiclass
learnability. we are also grateful for the members of a book reading club in
jerusalem that have carefully read and constructively criticized every line of
the manuscript. the members of the reading club are: maya alroy, yossi arje-
vani, aharon birnbaum, alon cohen, alon gonen, roi livni, ofer meshi, dan
rosenbaum, dana rubinstein, shahar somin, alon vinnikov, and yoav wald.
we would also like to thank gal elidan, amir globerson, nika haghtalab, shie
mannor, amnon shashua, nati srebro, and ruth urner for helpful discussions.

shai shalev-shwartz, jerusalem, israel
shai ben-david, waterloo, canada

contents

preface

page vii

1

introduction
1.1 what is learning?
1.2 when do we need machine learning?
1.3
1.4
1.5

types of learning
relations to other fields
how to read this book
1.5.1
notation

1.6

possible course plans based on this book

part i foundations

2

3

4

a gentle start
2.1
2.2

a formal model     the statistical learning framework
empirical risk minimization
2.2.1
something may go wrong     over   tting
empirical risk minimization with inductive bias
2.3.1
exercises

finite hypothesis classes

2.3

2.4

a formal learning model
3.1
3.2

pac learning
a more general learning model
3.2.1 releasing the realizability assumption     agnostic pac

learning

3.2.2 the scope of learning problems modeled
summary
bibliographic remarks
exercises

3.3
3.4
3.5

learning via uniform convergence
4.1
4.2

uniform convergence is su   cient for learnability
finite classes are agnostic pac learnable

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

19
19
21
22
24
25
26
27

31

33
33
35
35
36
37
41

43
43
44

45
47
49
50
50

54
54
55

x

contents

4.3
4.4
4.5

summary
bibliographic remarks
exercises

5

6

7

8

the bias-complexity tradeo   
5.1

the no-free-lunch theorem
5.1.1 no-free-lunch and prior knowledge
error decomposition
summary
bibliographic remarks
exercises

5.2
5.3
5.4
5.5

the vc-dimension
6.1
6.2
6.3

intervals

finite classes

in   nite-size classes can be learnable
the vc-dimension
examples
6.3.1 threshold functions
6.3.2
6.3.3 axis aligned rectangles
6.3.4
6.3.5 vc-dimension and the number of parameters
the fundamental theorem of pac learning
proof of theorem 6.7
6.5.1
6.5.2 uniform convergence for classes of small e   ective size
summary
bibliographic remarks
exercises

sauer   s lemma and the growth function

6.4
6.5

6.6
6.7
6.8

nonuniform learnability
7.1

nonuniform learnability
7.1.1 characterizing nonuniform learnability
structural risk minimization

7.2
7.3 minimum description length and occam   s razor

7.3.1 occam   s razor

7.4 other notions of learnability     consistency
7.5 discussing the di   erent notions of learnability
7.5.1 the no-free-lunch theorem revisited
summary
bibliographic remarks
exercises

7.6
7.7
7.8

the runtime of learning
8.1

computational complexity of learning

58
58
58

60
61
63
64
65
66
66

67
67
68
70
70
71
71
72
72
72
73
73
75
78
78
78

83
83
84
85
89
91
92
93
95
96
97
97

100
101

contents

xi

finite classes

formal de   nition*

boolean conjunctions
learning 3-term dnf

8.1.1
implementing the erm rule
8.2.1
8.2.2 axis aligned rectangles
8.2.3
8.2.4
e   ciently learnable, but not by a proper erm
hardness of learning*
summary
bibliographic remarks
exercises

8.2

8.3
8.4
8.5
8.6
8.7

part ii from theory to algorithms

9

id135 for the class of halfspaces
id88 for halfspaces

9.2

linear predictors
halfspaces
9.1
9.1.1
9.1.2
9.1.3 the vc dimension of halfspaces
id75
9.2.1
9.2.2
id28
summary
bibliographic remarks
exercises

9.3
9.4
9.5
9.6

least squares
id75 for polynomial regression tasks

10

boosting
10.1 weak learnability

10.1.1 e   cient implementation of erm for decision stumps

10.2 adaboost
10.3 linear combinations of base hypotheses

10.3.1 the vc-dimension of l(b, t )

10.4 adaboost for face recognition
10.5 summary
10.6 bibliographic remarks
10.7 exercises

11

model selection and validation
11.1 model selection using srm
11.2 validation

11.2.1 hold out set
11.2.2 validation for model selection
11.2.3 the model-selection curve

102
103
104
105
106
107
107
108
110
110
110

115

117
118
119
120
122
123
124
125
126
128
128
128

130
131
133
134
137
139
140
141
141
142

144
145
146
146
147
148

xii

contents

11.2.4 k-fold cross validation
11.2.5 train-validation-test split

11.3 what to do if learning fails
11.4 summary
11.5 exercises

12

convex learning problems
12.1 convexity, lipschitzness, and smoothness

12.1.1 convexity
12.1.2 lipschitzness
12.1.3 smoothness

12.2 convex learning problems

12.2.1 learnability of convex learning problems
12.2.2 convex-lipschitz/smooth-bounded learning problems

12.3 surrogate id168s
12.4 summary
12.5 bibliographic remarks
12.6 exercises

13

id173 and stability
13.1 regularized loss minimization

13.1.1 ridge regression

13.2 stable rules do not over   t
13.3 tikhonov id173 as a stabilizer

13.3.1 lipschitz loss
13.3.2 smooth and nonnegative loss

13.4 controlling the fitting-stability tradeo   
13.5 summary
13.6 bibliographic remarks
13.7 exercises

14

stochastic id119
14.1 id119

14.1.1 analysis of gd for convex-lipschitz functions

14.2 subgradients

14.2.1 calculating subgradients
14.2.2 subgradients of lipschitz functions
14.2.3 subid119

14.3 stochastic id119 (sgd)

14.3.1 analysis of sgd for convex-lipschitz-bounded functions

14.4 variants

14.4.1 adding a projection step
14.4.2 variable step size
14.4.3 other averaging techniques

149
150
151
154
154

156
156
156
160
162
163
164
166
167
168
169
169

171
171
172
173
174
176
177
178
180
180
181

184
185
186
188
189
190
190
191
191
193
193
194
195

contents

xiii

14.4.4 strongly convex functions*

14.5 learning with sgd

14.5.1 sgd for risk minimization
14.5.2 analyzing sgd for convex-smooth learning problems
14.5.3 sgd for regularized loss minimization

14.6 summary
14.7 bibliographic remarks
14.8 exercises

15

support vector machines
15.1 margin and hard-id166

15.1.1 the homogenous case
15.1.2 the sample complexity of hard-id166

15.2 soft-id166 and norm id173

15.2.1 the sample complexity of soft-id166
15.2.2 margin and norm-based bounds versus dimension
15.2.3 the ramp loss*

implementing soft-id166 using sgd

15.3 optimality conditions and    support vectors   *
15.4 duality*
15.5
15.6 summary
15.7 bibliographic remarks
15.8 exercises

kernel methods
16.1 embeddings into feature spaces
16.2 the kernel trick

16.2.1 kernels as a way to express prior knowledge
16.2.2 characterizing id81s*
implementing soft-id166 with kernels

16.3
16.4 summary
16.5 bibliographic remarks
16.6 exercises

multiclass, ranking, and complex prediction problems
17.1 one-versus-all and all-pairs
17.2 linear multiclass predictors
17.2.1 how to construct   
17.2.2 cost-sensitive classi   cation
17.2.3 erm
17.2.4 generalized hinge loss
17.2.5 multiclass id166 and sgd

17.3 structured output prediction
17.4 ranking

16

17

195
196
196
198
199
200
200
201

202
202
205
205
206
208
208
209
210
211
212
213
213
214

215
215
217
221
222
222
224
225
225

227
227
230
230
232
232
233
234
236
238

xiv

contents

18

19

20

17.4.1 linear predictors for ranking

17.5 bipartite ranking and multivariate performance measures

17.5.1 linear predictors for bipartite ranking

17.6 summary
17.7 bibliographic remarks
17.8 exercises

id90
18.1 sample complexity
18.2 decision tree algorithms

18.2.1 implementations of the gain measure
18.2.2 pruning
18.2.3 threshold-based splitting rules for real-valued features

18.3 id79s
18.4 summary
18.5 bibliographic remarks
18.6 exercises

nearest neighbor
19.1 k nearest neighbors
19.2 analysis

19.2.1 a generalization bound for the 1-nn rule
19.2.2 the    curse of dimensionality   

19.3 e   cient implementation*
19.4 summary
19.5 bibliographic remarks
19.6 exercises

neural networks
20.1 feedforward neural networks
20.2 learning neural networks
20.3 the expressive power of neural networks

20.3.1 geometric intuition

20.4 the sample complexity of neural networks
20.5 the runtime of learning neural networks
20.6 sgd and id26
20.7 summary
20.8 bibliographic remarks
20.9 exercises

part iii additional learning models

21

online learning
21.1 online classi   cation in the realizable case

240
243
245
247
247
248

250
251
252
253
254
255
255
256
256
256

258
258
259
260
263
264
264
264
265

268
269
270
271
273
274
276
277
281
281
282

285

287
288

contents

xv

21.1.1 online learnability

21.2 online classi   cation in the unrealizable case

21.2.1 weighted-majority

21.3 online id76
21.4 the online id88 algorithm
21.5 summary
21.6 bibliographic remarks
21.7 exercises

22

id91
22.1 linkage-based id91 algorithms
22.2 id116 and other cost minimization id91s

22.2.1 the id116 algorithm

22.3 spectral id91
22.3.1 graph cut
22.3.2 graph laplacian and relaxed graph cuts
22.3.3 unnormalized spectral id91
information bottleneck*

22.4
22.5 a high level view of id91
22.6 summary
22.7 bibliographic remarks
22.8 exercises

23

id84
23.1 principal component analysis (pca)

23.1.1 a more e   cient solution for the case d (cid:29) m
23.1.2 implementation and demonstration

23.2 random projections
23.3 compressed sensing

23.3.1 proofs*

23.4 pca or compressed sensing?
23.5 summary
23.6 bibliographic remarks
23.7 exercises

24

generative models
24.1 maximum likelihood estimator

24.1.1 id113 for continuous ran-

dom variables

24.1.2 maximum likelihood and empirical risk minimization
24.1.3 generalization analysis

24.2 naive bayes
24.3 id156
24.4 latent variables and the em algorithm

290
294
295
300
301
304
305
305

307
310
311
313
315
315
315
317
317
318
320
320
320

323
324
326
326
329
330
333
338
338
339
339

342
343

344
345
345
347
347
348

xvi

contents

24.4.1 em as an alternate maximization algorithm
24.4.2 em for mixture of gaussians (soft id116)

24.5 bayesian reasoning
24.6 summary
24.7 bibliographic remarks
24.8 exercises

25

feature selection and generation
25.1 feature selection

25.1.1 filters
25.1.2 greedy selection approaches
25.1.3 sparsity-inducing norms

25.2 feature manipulation and id172

25.2.1 examples of feature transformations

25.3 id171

25.3.1 dictionary learning using auto-encoders

25.4 summary
25.5 bibliographic remarks
25.6 exercises

part iv advanced theory

26

27

28

rademacher complexities
26.1 the rademacher complexity
26.1.1 rademacher calculus

26.2 rademacher complexity of linear classes
26.3 generalization bounds for id166
26.4 generalization bounds for predictors with low (cid:96)1 norm
26.5 bibliographic remarks

covering numbers
27.1 covering

27.1.1 properties

27.2 from covering to rademacher complexity via chaining
27.3 bibliographic remarks

proof of the fundamental theorem of learning theory
28.1 the upper bound for the agnostic case
28.2 the lower bound for the agnostic case

28.2.1 showing that m( ,   )     0.5 log(1/(4  ))/ 2
28.2.2 showing that m( , 1/8)     8d/ 2
28.3 the upper bound for the realizable case
28.3.1 from  -nets to pac learnability

350
352
353
355
355
356

357
358
359
360
363
365
367
368
368
370
371
371

373

375
375
379
382
383
386
386

388
388
388
389
391

392
392
393
393
395
398
401

29

30

31

multiclass learnability
29.1 the natarajan dimension
29.2 the multiclass fundamental theorem
29.2.1 on the proof of theorem 29.3
29.3 calculating the natarajan dimension
29.3.1 one-versus-all based classes
29.3.2 general multiclass-to-binary reductions
29.3.3 linear multiclass predictors

29.4 on good and bad erms
29.5 bibliographic remarks
29.6 exercises

compression bounds
30.1 compression bounds
30.2 examples

30.2.1 axis aligned rectangles
30.2.2 halfspaces
30.2.3 separating polynomials
30.2.4 separation with margin

30.3 bibliographic remarks

pac-bayes
31.1 pac-bayes bounds
31.2 bibliographic remarks
31.3 exercises

appendix a

technical lemmas

appendix b measure concentration

appendix c

id202

notes
references
index

contents

xvii

402
402
403
403
404
404
405
405
406
408
409

410
410
412
412
412
413
414
414

415
415
417
417

419

422

430

435
437
447

1

introduction

the subject of this book is automated learning, or, as we will more often call
it, machine learning (ml). that is, we wish to program computers so that
they can    learn    from input available to them. roughly speaking, learning is
the process of converting experience into expertise or knowledge. the input to
a learning algorithm is training data, representing experience, and the output
is some expertise, which usually takes the form of another computer program
that can perform some task. seeking a formal-mathematical understanding of
this concept, we   ll have to be more explicit about what we mean by each of the
involved terms: what is the training data our programs will access? how can
the process of learning be automated? how can we evaluate the success of such
a process (namely, the quality of the output of a learning program)?

1.1

what is learning?

let us begin by considering a couple of examples from naturally occurring ani-
mal learning. some of the most fundamental issues in ml arise already in that
context, which we are all familiar with.

bait shyness     rats learning to avoid poisonous baits: when rats encounter
food items with novel look or smell, they will    rst eat very small amounts, and
subsequent feeding will depend on the    avor of the food and its physiological
e   ect. if the food produces an ill e   ect, the novel food will often be associated
with the illness, and subsequently, the rats will not eat it. clearly, there is a
learning mechanism in play here     the animal used past experience with some
food to acquire expertise in detecting the safety of this food. if past experience
with the food was negatively labeled, the animal predicts that it will also have
a negative e   ect when encountered in the future.

inspired by the preceding example of successful learning, let us demonstrate a
typical machine learning task. suppose we would like to program a machine that
learns how to    lter spam e-mails. a naive solution would be seemingly similar
to the way rats learn how to avoid poisonous baits. the machine will simply
memorize all previous e-mails that had been labeled as spam e-mails by the
human user. when a new e-mail arrives, the machine will search for it in the set

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

20

introduction

of previous spam e-mails. if it matches one of them, it will be trashed. otherwise,
it will be moved to the user   s inbox folder.

while the preceding    learning by memorization    approach is sometimes use-
ful, it lacks an important aspect of learning systems     the ability to label unseen
e-mail messages. a successful learner should be able to progress from individual
examples to broader generalization. this is also referred to as inductive reasoning
or inductive id136. in the bait shyness example presented previously, after
the rats encounter an example of a certain type of food, they apply their attitude
toward it on new, unseen examples of food of similar smell and taste. to achieve
generalization in the spam    ltering task, the learner can scan the previously seen
e-mails, and extract a set of words whose appearance in an e-mail message is
indicative of spam. then, when a new e-mail arrives, the machine can check
whether one of the suspicious words appears in it, and predict its label accord-
ingly. such a system would potentially be able correctly to predict the label of
unseen e-mails.

however, inductive reasoning might lead us to false conclusions. to illustrate

this, let us consider again an example from animal learning.

pigeon superstition: in an experiment performed by the psychologist b. f. skinner,

he placed a bunch of hungry pigeons in a cage. an automatic mechanism had
been attached to the cage, delivering food to the pigeons at regular intervals
with no reference whatsoever to the birds    behavior. the hungry pigeons went
around the cage, and when food was    rst delivered, it found each pigeon engaged
in some activity (pecking, turning the head, etc.). the arrival of food reinforced
each bird   s speci   c action, and consequently, each bird tended to spend some
more time doing that very same action. that, in turn, increased the chance that
the next random food delivery would    nd each bird engaged in that activity
again. what results is a chain of events that reinforces the pigeons    association
of the delivery of the food with whatever chance actions they had been perform-
ing when it was    rst delivered. they subsequently continue to perform these
same actions diligently.1

what distinguishes learning mechanisms that result in superstition from useful
learning? this question is crucial to the development of automated learners.
while human learners can rely on common sense to    lter out random meaningless
learning conclusions, once we export the task of learning to a machine, we must
provide well de   ned crisp principles that will protect the program from reaching
senseless or useless conclusions. the development of such principles is a central
goal of the theory of machine learning.

what, then, made the rats    learning more successful than that of the pigeons?
as a    rst step toward answering this question, let us have a closer look at the
bait shyness phenomenon in rats.

bait shyness revisited     rats fail to acquire conditioning between food and
electric shock or between sound and nausea: the bait shyness mechanism in

1 see: http://psychclassics.yorku.ca/skinner/pigeon

1.2 when do we need machine learning?

21

rats turns out to be more complex than what one may expect. in experiments
carried out by garcia (garcia & koelling 1996), it was demonstrated that if the
unpleasant stimulus that follows food consumption is replaced by, say, electrical
shock (rather than nausea), then no conditioning occurs. even after repeated
trials in which the consumption of some food is followed by the administration of
unpleasant electrical shock, the rats do not tend to avoid that food. similar failure
of conditioning occurs when the characteristic of the food that implies nausea
(such as taste or smell) is replaced by a vocal signal. the rats seem to have
some    built in    prior knowledge telling them that, while temporal correlation
between food and nausea can be causal, it is unlikely that there would be a
causal relationship between food consumption and electrical shocks or between
sounds and nausea.

we conclude that one distinguishing feature between the bait shyness learning
and the pigeon superstition is the incorporation of prior knowledge that biases
the learning mechanism. this is also referred to as inductive bias. the pigeons in
the experiment are willing to adopt any explanation for the occurrence of food.
however, the rats    know    that food cannot cause an electric shock and that the
co-occurrence of noise with some food is not likely to a   ect the nutritional value
of that food. the rats    learning process is biased toward detecting some kind of
patterns while ignoring other temporal correlations between events.

it turns out that the incorporation of prior knowledge, biasing the learning
process, is inevitable for the success of learning algorithms (this is formally stated
and proved as the    no-free-lunch theorem    in chapter 5). the development of
tools for expressing domain expertise, translating it into a learning bias, and
quantifying the e   ect of such a bias on the success of learning is a central theme
of the theory of machine learning. roughly speaking, the stronger the prior
knowledge (or prior assumptions) that one starts the learning process with, the
easier it is to learn from further examples. however, the stronger these prior
assumptions are, the less    exible the learning is     it is bound, a priori, by the
commitment to these assumptions. we shall discuss these issues explicitly in
chapter 5.

1.2

when do we need machine learning?

when do we need machine learning rather than directly program our computers
to carry out the task at hand? two aspects of a given problem may call for the
use of programs that learn and improve on the basis of their    experience   : the
problem   s complexity and the need for adaptivity.

tasks that are too complex to program.

    tasks performed by animals/humans: there are numerous tasks that
we human beings perform routinely, yet our introspection concern-
ing how we do them is not su   ciently elaborate to extract a well

22

introduction

de   ned program. examples of such tasks include driving, speech
recognition, and image understanding. in all of these tasks, state
of the art machine learning programs, programs that    learn from
their experience,    achieve quite satisfactory results, once exposed
to su   ciently many training examples.

    tasks beyond human capabilities: another wide family of tasks that
bene   t from machine learning techniques are related to the analy-
sis of very large and complex data sets: astronomical data, turning
medical archives into medical knowledge, weather prediction, anal-
ysis of genomic data, web search engines, and electronic commerce.
with more and more available digitally recorded data, it becomes
obvious that there are treasures of meaningful information buried
in data archives that are way too large and too complex for humans
to make sense of. learning to detect meaningful patterns in large
and complex data sets is a promising domain in which the combi-
nation of programs that learn with the almost unlimited memory
capacity and ever increasing processing speed of computers opens
up new horizons.

adaptivity. one limiting feature of programmed tools is their rigidity     once
the program has been written down and installed, it stays unchanged.
however, many tasks change over time or from one user to another.
machine learning tools     programs whose behavior adapts to their input
data     o   er a solution to such issues; they are, by nature, adaptive
to changes in the environment they interact with. typical successful
applications of machine learning to such problems include programs that
decode handwritten text, where a    xed program can adapt to variations
between the handwriting of di   erent users; spam detection programs,
adapting automatically to changes in the nature of spam e-mails; and
id103 programs.

1.3

types of learning

learning is, of course, a very wide domain. consequently, the    eld of machine
learning has branched into several sub   elds dealing with di   erent types of learn-
ing tasks. we give a rough taxonomy of learning paradigms, aiming to provide
some perspective of where the content of this book sits within the wide    eld of
machine learning.

we describe four parameters along which learning paradigms can be classi   ed.

supervised versus unsupervised since learning involves an interaction be-
tween the learner and the environment, one can divide learning tasks
according to the nature of that interaction. the    rst distinction to note
is the di   erence between supervised and unsupervised learning. as an

1.3 types of learning

23

illustrative example, consider the task of learning to detect spam e-mail
versus the task of anomaly detection. for the spam detection task, we
consider a setting in which the learner receives training e-mails for which
the label spam/not-spam is provided. on the basis of such training the
learner should    gure out a rule for labeling a newly arriving e-mail mes-
sage. in contrast, for the task of anomaly detection, all the learner gets
as training is a large body of e-mail messages (with no labels) and the
learner   s task is to detect    unusual    messages.

more abstractly, viewing learning as a process of    using experience
to gain expertise,    supervised learning describes a scenario in which the
   experience,    a training example, contains signi   cant information (say,
the spam/not-spam labels) that is missing in the unseen    test examples   
to which the learned expertise is to be applied. in this setting, the ac-
quired expertise is aimed to predict that missing information for the test
data. in such cases, we can think of the environment as a teacher that
   supervises    the learner by providing the extra information (labels). in
unsupervised learning, however, there is no distinction between training
and test data. the learner processes input data with the goal of coming
up with some summary, or compressed version of that data. id91
a data set into subsets of similar objets is a typical example of such a
task.

there is also an intermediate learning setting in which, while the
training examples contain more information than the test examples, the
learner is required to predict even more information for the test exam-
ples. for example, one may try to learn a value function that describes for
each setting of a chess board the degree by which white   s position is bet-
ter than the black   s. yet, the only information available to the learner at
training time is positions that occurred throughout actual chess games,
labeled by who eventually won that game. such learning frameworks are
mainly investigated under the title of id23.

active versus passive learners learning paradigms can vary by the role
played by the learner. we distinguish between    active    and    passive   
learners. an active learner interacts with the environment at training
time, say, by posing queries or performing experiments, while a passive
learner only observes the information provided by the environment (or
the teacher) without in   uencing or directing it. note that the learner of a
spam    lter is usually passive     waiting for users to mark the e-mails com-
ing to them. in an active setting, one could imagine asking users to label
speci   c e-mails chosen by the learner, or even composed by the learner, to
enhance
what
spam is.

understanding

its

of

helpfulness of the teacher when one thinks about human learning, of a
baby at home or a student at school, the process often involves a helpful
teacher, who is trying to feed the learner with the information most use-

24

introduction

ful for achieving the learning goal. in contrast, when a scientist learns
about nature, the environment, playing the role of the teacher, can be
best thought of as passive     apples drop, stars shine, and the rain falls
without regard to the needs of the learner. we model such learning sce-
narios by postulating that the training data (or the learner   s experience)
is generated by some random process. this is the basic building block in
the branch of    statistical learning.    finally, learning also occurs when
the learner   s input is generated by an adversarial    teacher.    this may be
the case in the spam    ltering example (if the spammer makes an e   ort
to mislead the spam    ltering designer) or in learning to detect fraud.
one also uses an adversarial teacher model as a worst-case scenario,
when no milder setup can be safely assumed. if you can learn against an
adversarial teacher, you are guaranteed to succeed interacting any odd
teacher.

online versus batch learning protocol the last parameter we mention is
the distinction between situations in which the learner has to respond
online, throughout the learning process, and settings in which the learner
has to engage the acquired expertise only after having a chance to process
large amounts of data. for example, a stockbroker has to make daily
decisions, based on the experience collected so far. he may become an
expert over time, but might have made costly mistakes in the process. in
contrast, in many data mining settings, the learner     the data miner    
has large amounts of training data to play with before having to output
conclusions.

in this book we shall discuss only a subset of the possible learning paradigms.
our main focus is on supervised statistical batch learning with a passive learner
(for example, trying to learn how to generate patients    prognoses, based on large
archives of records of patients that were independently collected and are already
labeled by the fate of the recorded patients). we shall also brie   y discuss online
learning and batch unsupervised learning (in particular, id91).

1.4

relations to other fields

as an interdisciplinary    eld, machine learning shares common threads with the
mathematical    elds of statistics, id205, game theory, and optimiza-
tion. it is naturally a sub   eld of computer science, as our goal is to program
machines so that they will learn. in a sense, machine learning can be viewed as
a branch of ai (arti   cial intelligence), since, after all, the ability to turn expe-
rience into expertise or to detect meaningful patterns in complex sensory data
is a cornerstone of human (and animal) intelligence. however, one should note
that, in contrast with traditional ai, machine learning is not trying to build
automated imitation of intelligent behavior, but rather to use the strengths and

1.5 how to read this book

25

special abilities of computers to complement human intelligence, often perform-
ing tasks that fall way beyond human capabilities. for example, the ability to
scan and process huge databases allows machine learning programs to detect
patterns that are outside the scope of human perception.

the component of experience, or training, in machine learning often refers
to data that is randomly generated. the task of the learner is to process such
randomly generated examples toward drawing conclusions that hold for the en-
vironment from which these examples are picked. this description of machine
learning highlights its close relationship with statistics. indeed there is a lot in
common between the two disciplines, in terms of both the goals and techniques
used. there are, however, a few signi   cant di   erences of emphasis; if a doctor
comes up with the hypothesis that there is a correlation between smoking and
heart disease, it is the statistician   s role to view samples of patients and check
the validity of that hypothesis (this is the common statistical task of hypothe-
sis testing). in contrast, machine learning aims to use the data gathered from
samples of patients to come up with a description of the causes of heart disease.
the hope is that automated techniques may be able to    gure out meaningful
patterns (or hypotheses) that may have been missed by the human observer.

in contrast with traditional statistics, in machine learning in general, and
in this book in particular, algorithmic considerations play a major role. ma-
chine learning is about the execution of learning by computers; hence algorith-
mic issues are pivotal. we develop algorithms to perform the learning tasks and
are concerned with their computational e   ciency. another di   erence is that
while statistics is often interested in asymptotic behavior (like the convergence
of sample-based statistical estimates as the sample sizes grow to in   nity), the
theory of machine learning focuses on    nite sample bounds. namely, given the
size of available samples, machine learning theory aims to    gure out the degree
of accuracy that a learner can expect on the basis of such samples.

there are further di   erences between these two disciplines, of which we shall
mention only one more here. while in statistics it is common to work under the
assumption of certain presubscribed data models (such as assuming the normal-
ity of data-generating distributions, or the linearity of functional dependencies),
in machine learning the emphasis is on working under a    distribution-free    set-
ting, where the learner assumes as little as possible about the nature of the
data distribution and allows the learning algorithm to    gure out which models
best approximate the data-generating process. a precise discussion of this issue
requires some technical preliminaries, and we will come back to it later in the
book, and in particular in chapter 5.

1.5

how to read this book

the    rst part of the book provides the basic theoretical principles that underlie
machine learning (ml). in a sense, this is the foundation upon which the rest

26

introduction

of the book is built. this part could serve as a basis for a minicourse on the
theoretical foundations of ml.

the second part of the book introduces the most commonly used algorithmic
approaches to supervised machine learning. a subset of these chapters may also
be used for introducing machine learning in a general ai course to computer
science, math, or engineering students.

the third part of the book extends the scope of discussion from statistical
classi   cation to other learning models. it covers online learning, unsupervised
learning, id84, generative models, and id171.

the fourth part of the book, advanced theory, is geared toward readers who
have interest in research and provides the more technical mathematical tech-
niques that serve to analyze and drive forward the    eld of theoretical machine
learning.

the appendixes provide some technical tools used in the book. in particular,

we list basic results from measure concentration and id202.

a few sections are marked by an asterisk, which means they are addressed to
more advanced students. each chapter is concluded with a list of exercises. a
solution manual is provided in the course web site.

1.5.1

possible course plans based on this book

a 14 week introduction course for graduate students:

1. chapters 2   4.
2. chapter 9 (without the vc calculation).
3. chapters 5   6 (without proofs).
4. chapter 10.
5. chapters 7, 11 (without proofs).
6. chapters 12, 13 (with some of the easier proofs).
7. chapter 14 (with some of the easier proofs).
8. chapter 15.
9. chapter 16.
10. chapter 18.
11. chapter 22.
12. chapter 23 (without proofs for compressed sensing).
13. chapter 24.
14. chapter 25.

a 14 week advanced course for graduate students:

1. chapters 26, 27.
2. (continued)
3. chapters 6, 28.
4. chapter 7.
5. chapter 31.

1.6 notation

27

6. chapter 30.
7. chapters 12, 13.
8. chapter 14.
9. chapter 8.
10. chapter 17.
11. chapter 29.
12. chapter 19.
13. chapter 20.
14. chapter 21.

1.6

notation

most of the notation we use throughout the book is either standard or de   ned
on the spot. in this section we describe our main conventions and provide a
table summarizing our notation (table 1.1). the reader is encouraged to skip
this section and return to it if during the reading of the book some notation is
unclear.

we denote scalars and abstract objects with lowercase letters (e.g. x and   ).
often, we would like to emphasize that some object is a vector and then we
use boldface letters (e.g. x and   ). the ith element of a vector x is denoted
by xi. we use uppercase letters to denote matrices, sets, and sequences. the
meaning should be clear from the context. as we will see momentarily, the input
of a learning algorithm is a sequence of training examples. we denote by z an
abstract example and by s = z1, . . . , zm a sequence of m examples. historically,
s is often referred to as a training set; however, we will always assume that s is
a sequence rather than a set. a sequence of m vectors is denoted by x1, . . . , xm.
the ith element of xt is denoted by xt,i.
throughout the book, we make use of basic notions from id203. we
denote by d a distribution over some set,2 for example, z. we use the notation
z     d to denote that z is sampled according to d. given a random variable
f : z     r, its expected value is denoted by ez   d[f (z)]. we sometimes use the
shorthand e[f ] when the dependence on z is clear from the context. for f : z    
{true, false} we also use pz   d[f (z)] to denote d({z : f (z) = true}). in the
next chapter we will also introduce the notation dm to denote the id203
over z m induced by sampling (z1, . . . , zm) where each point zi is sampled from
d independently of the other points.
in general, we have made an e   ort to avoid asymptotic notation. however, we
occasionally use it to clarify the main results. in particular, given f : r     r+
and g : r     r+ we write f = o(g) if there exist x0,        r+ such that for all
x > x0 we have f (x)       g(x). we write f = o(g) if for every    > 0 there exists
2 to be mathematically precise, d should be de   ned over some   -algebra of subsets of z.

the user who is not familiar with measure theory can skip the few footnotes and remarks
regarding more formal measurability de   nitions and assumptions.

28

introduction

symbol
r
rd
r+
n
o, o,   ,   ,    ,   o
1[boolean expression]
[a]+
[n]
x, v, w
xi, vi, wi
(cid:104)x, v(cid:105)
(cid:107)x(cid:107)2 or (cid:107)x(cid:107)
(cid:107)x(cid:107)1
(cid:107)x(cid:107)   
(cid:107)x(cid:107)0
a     rd,k
a(cid:62)
ai,j
x x(cid:62)
x1, . . . , xm
xi,j
w(1), . . . , w(t )
w(t)
x
y
z
h
(cid:96) : h    z     r+
d
d(a)
z     d
s = z1, . . . , zm
s     dm
p, e
pz   d[f (z)]
ez   d[f (z)]
n (  , c)
f(cid:48)(x)
f(cid:48)(cid:48)(x)

i

   f (w)

   wi

   f (w)
   f (w)
minx   c f (x)
maxx   c f (x)
argminx   c f (x)
argmaxx   c f (x)
log

table 1.1 summary of notation

meaning

the set of real numbers
the set of d-dimensional vectors over r
the set of non-negative real numbers
the set of natural numbers
asymptotic notation (see text)
indicator function (equals 1 if expression is true and 0 o.w.)
= max{0, a}
the set {1, . . . , n} (for n     n)
(column) vectors
the ith element of a vector

=(cid:80)d
=(cid:112)(cid:104)x, x(cid:105) (the (cid:96)2 norm of x)
=(cid:80)d

i=1 xivi (inner product)
i=1 |xi| (the (cid:96)1 norm of x)
= maxi |xi| (the (cid:96)    norm of x)
the number of nonzero elements of x
a d    k matrix over r
the transpose of a
the (i, j) element of a
the d    d matrix a s.t. ai,j = xixj (where x     rd)
a sequence of m vectors
the jth element of the ith vector in the sequence
the values of a vector w during an iterative algorithm
the ith element of the vector w(t)
instances domain (a set)
labels domain (a set)
examples domain (a set)
hypothesis class (a set)
id168
a distribution over some set (usually over z or over x )
the id203 of a set a     z according to d
sampling z according to d
a sequence of m examples
sampling s = z1, . . . , zm i.i.d. according to d
id203 and expectation of a random variable
= d({z : f (z) = true}) for f : z     {true, false}
expectation of the random variable f : z     r
gaussian distribution with expectation    and covariance c
the derivative of a function f : r     r at x
the second derivative of a function f : r     r at x
the partial derivative of a function f : rd     r at w w.r.t. wi
the gradient of a function f : rd     r at w
the di   erential set of a function f : rd     r at w
= min{f (x) : x     c} (minimal value of f over c)
= max{f (x) : x     c} (maximal value of f over c)
the set {x     c : f (x) = minz   c f (z)}
the set {x     c : f (x) = maxz   c f (z)}
the natural logarithm

1.6 notation

29

x0 such that for all x > x0 we have f (x)       g(x). we write f =    (g) if there
exist x0,        r+ such that for all x > x0 we have f (x)       g(x). the notation
f =   (g) is de   ned analogously. the notation f =   (g) means that f = o(g)
and g = o(f ). finally, the notation f =   o(g) means that there exists k     n
such that f (x) = o(g(x) logk(g(x))).
the inner product between vectors x and w is denoted by (cid:104)x, w(cid:105). whenever we
do not specify the vector space we assume that it is the d-dimensional euclidean
i=1 xiwi. the euclidean (or (cid:96)2) norm of a vector w is
i |wi|p)1/p, and in particular

space and then (cid:104)x, w(cid:105) =(cid:80)d
(cid:107)w(cid:107)2 =(cid:112)(cid:104)w, w(cid:105). we omit the subscript from the (cid:96)2 norm when it is clear from
the context. we also use other (cid:96)p norms, (cid:107)w(cid:107)p = ((cid:80)
(cid:107)w(cid:107)1 =(cid:80)

i |wi| and (cid:107)w(cid:107)    = maxi |wi|.

we use the notation minx   c f (x) to denote the minimum value of the set
{f (x) : x     c}. to be mathematically more precise, we should use inf x   c f (x)
whenever the minimum is not achievable. however, in the context of this book
the distinction between in   mum and minimum is often of little interest. hence,
to simplify the presentation, we sometimes use the min notation even when inf
is more adequate. an analogous remark applies to max versus sup.

part i

foundations

2

a gentle start

let us begin our mathematical analysis by showing how successful learning can be
achieved in a relatively simpli   ed setting. imagine you have just arrived in some
small paci   c island. you soon    nd out that papayas are a signi   cant ingredient
in the local diet. however, you have never before tasted papayas. you have to
learn how to predict whether a papaya you see in the market is tasty or not.
first, you need to decide which features of a papaya your prediction should be
based on. on the basis of your previous experience with other fruits, you decide
to use two features: the papaya   s color, ranging from dark green, through orange
and red to dark brown, and the papaya   s softness, ranging from rock hard to
mushy. your input for    guring out your prediction rule is a sample of papayas
that you have examined for color and softness and then tasted and found out
whether they were tasty or not. let us analyze this task as a demonstration of
the considerations involved in learning problems.

our    rst step is to describe a formal model aimed to capture such learning

tasks.

2.1

a formal model     the statistical learning framework

    the learner   s input: in the basic statistical learning setting, the learner has

access to the following:
    domain set: an arbitrary set, x . this is the set of objects that we
may wish to label. for example, in the papaya learning problem men-
tioned before, the domain set will be the set of all papayas. usually,
these domain points will be represented by a vector of features (like
the papaya   s color and softness). we also refer to domain points as
instances and to x as instance space.

    label set: for our current discussion, we will restrict the label set to
be a two-element set, usually {0, 1} or {   1, +1}. let y denote our
set of possible labels. for our papayas example, let y be {0, 1}, where
1 represents being tasty and 0 stands for being not-tasty.

    training data: s = ((x1, y1) . . . (xm, ym)) is a    nite sequence of pairs in
x   y: that is, a sequence of labeled domain points. this is the input
that the learner has access to (like a set of papayas that have been

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

34

a gentle start

tasted and their color, softness, and tastiness). such labeled examples
are often called training examples. we sometimes also refer to s as a
training set.1

    the learner   s output: the learner is requested to output a prediction rule,
h : x     y. this function is also called a predictor, a hypothesis, or a clas-
si   er. the predictor can be used to predict the label of new domain points.
in our papayas example, it is a rule that our learner will employ to predict
whether future papayas he examines in the farmers    market are going to
be tasty or not. we use the notation a(s) to denote the hypothesis that a
learning algorithm, a, returns upon receiving the training sequence s.

    a simple data-generation model we now explain how the training data is
generated. first, we assume that the instances (the papayas we encounter)
are generated by some id203 distribution (in this case, representing
the environment). let us denote that id203 distribution over x by
d. it is important to note that we do not assume that the learner knows
anything about this distribution. for the type of learning tasks we discuss,
this could be any arbitrary id203 distribution. as to the labels, in the
current discussion we assume that there is some    correct    labeling function,
f : x     y, and that yi = f (xi) for all i. this assumption will be relaxed in
the next chapter. the labeling function is unknown to the learner. in fact,
this is just what the learner is trying to    gure out. in summary, each pair
in the training data s is generated by    rst sampling a point xi according
to d and then labeling it by f .
    measures of success: we de   ne the error of a classi   er to be the id203
that it does not predict the correct label on a random data point generated
by the aforementioned underlying distribution. that is, the error of h is
the id203 to draw a random instance x, according to the distribution
d, such that h(x) does not equal f (x).

formally, given a domain subset,2 a     x , the id203 distribution,
d, assigns a number, d(a), which determines how likely it is to observe a
point x     a. in many cases, we refer to a as an event and express it using
a function    : x     {0, 1}, namely, a = {x     x :   (x) = 1}. in that case,
we also use the notation px   d[  (x)] to express d(a).

we de   ne the error of a prediction rule, h : x     y, to be
x   d[h(x) (cid:54)= f (x)] def= d({x : h(x) (cid:54)= f (x)}).
ld,f (h) def= p

(2.1)

that is, the error of such h is the id203 of randomly choosing an
example x for which h(x) (cid:54)= f (x). the subscript (d, f ) indicates that the
error is measured with respect to the id203 distribution d and the

1 despite the    set    notation, s is a sequence. in particular, the same example may appear

2 strictly speaking, we should be more careful and require that a is a member of some

twice in s and some algorithms can take into account the order of examples in s.
  -algebra of subsets of x , over which d is de   ned. we will formally de   ne our
measurability assumptions in the next chapter.

2.2 empirical risk minimization

35

correct labeling function f . we omit this subscript when it is clear from
the context. l(d,f )(h) has several synonymous names such as the general-
ization error, the risk, or the true error of h, and we will use these names
interchangeably throughout the book. we use the letter l for the error,
since we view this error as the loss of the learner. we will later also discuss
other possible formulations of such loss.

    a note about the information available to the learner the learner is
blind to the underlying distribution d over the world and to the labeling
function f. in our papayas example, we have just arrived in a new island
and we have no clue as to how papayas are distributed and how to predict
their tastiness. the only way the learner can interact with the environment
is through observing the training set.

in the next section we describe a simple learning paradigm for the preceding

setup and analyze its performance.

2.2

empirical risk minimization

as mentioned earlier, a learning algorithm receives as input a training set s,
sampled from an unknown distribution d and labeled by some target function
f , and should output a predictor hs : x     y (the subscript s emphasizes the
fact that the output predictor depends on s). the goal of the algorithm is to
   nd hs that minimizes the error with respect to the unknown d and f .

since the learner does not know what d and f are, the true error is not directly
available to the learner. a useful notion of error that can be calculated by the
learner is the training error     the error the classi   er incurs over the training
sample:

|{i     [m] : h(xi) (cid:54)= yi}|

m

,

(2.2)

ls(h) def=

where [m] = {1, . . . , m}.

the terms empirical error and empirical risk are often used interchangeably

for this error.

since the training sample is the snapshot of the world that is available to the
learner, it makes sense to search for a solution that works well on that data.
this learning paradigm     coming up with a predictor h that minimizes ls(h)    
is called empirical risk minimization or erm for short.

2.2.1

something may go wrong     over   tting

although the erm rule seems very natural, without being careful, this approach
may fail miserably.

to demonstrate such a failure, let us go back to the problem of learning to

36

a gentle start

predict the taste of a papaya on the basis of its softness and color. consider a
sample as depicted in the following:

assume that the id203 distribution d is such that instances are distributed
uniformly within the gray square and the labeling function, f , determines the
label to be 1 if the instance is within the inner blue square, and 0 otherwise. the
area of the gray square in the picture is 2 and the area of the blue square is 1.
consider the following predictor:

hs(x) =

if    i     [m] s.t. xi = x
otherwise.

(2.3)

yi
0

(cid:40)

while this predictor might seem rather arti   cial, in exercise 1 we show a natural
representation of it using polynomials. clearly, no matter what the sample is,
ls(hs) = 0, and therefore this predictor may be chosen by an erm algorithm (it
is one of the empirical-minimum-cost hypotheses; no classi   er can have smaller
error). on the other hand, the true error of any classi   er that predicts the label
1 only on a    nite number of instances is, in this case, 1/2. thus, ld(hs) = 1/2.
we have found a predictor whose performance on the training set is excellent,
yet its performance on the true    world    is very poor. this phenomenon is called
over   tting. intuitively, over   tting occurs when our hypothesis    ts the training
data    too well    (perhaps like the everyday experience that a person who provides
a perfect detailed explanation for each of his single actions may raise suspicion).

2.3

empirical risk minimization with inductive bias

we have just demonstrated that the erm rule might lead to over   tting. rather
than giving up on the erm paradigm, we will look for ways to rectify it. we will
search for conditions under which there is a guarantee that erm does not over   t,
namely, conditions under which when the erm predictor has good performance
with respect to the training data, it is also highly likely to perform well over the
underlying data distribution.

a common solution is to apply the erm learning rule over a restricted search
space. formally, the learner should choose in advance (before seeing the data) a
set of predictors. this set is called a hypothesis class and is denoted by h. each
h     h is a function mapping from x to y. for a given class h, and a training
sample, s, the ermh learner uses the erm rule to choose a predictor h     h,

2.3 empirical risk minimization with inductive bias

37

with the lowest possible error over s. formally,
ermh(s)     argmin
h   h

ls(h),

where argmin stands for the set of hypotheses in h that achieve the minimum
value of ls(h) over h. by restricting the learner to choosing a predictor from
h, we bias it toward a particular set of predictors. such restrictions are often
called an inductive bias. since the choice of such a restriction is determined
before the learner sees the training data, it should ideally be based on some
prior knowledge about the problem to be learned. for example, for the papaya
taste prediction problem we may choose the class h to be the set of predictors
that are determined by axis aligned rectangles (in the space determined by the
color and softness coordinates). we will later show that ermh over this class is
guaranteed not to over   t. on the other hand, the example of over   tting that we
have seen previously, demonstrates that choosing h to be a class of predictors
that includes all functions that assign the value 1 to a    nite set of domain points
does not su   ce to guarantee that ermh will not over   t.

a fundamental question in learning theory is, over which hypothesis classes
ermh learning will not result in over   tting. we will study this question later
in the book.

intuitively, choosing a more restricted hypothesis class better protects us
against over   tting but at the same time might cause us a stronger inductive
bias. we will get back to this fundamental tradeo    later.

2.3.1

finite hypothesis classes

the simplest type of restriction on a class is imposing an upper bound on its size
(that is, the number of predictors h in h). in this section, we show that if h is
a    nite class then ermh will not over   t, provided it is based on a su   ciently
large training sample (this size requirement will depend on the size of h).

limiting the learner to prediction rules within some    nite hypothesis class may
be considered as a reasonably mild restriction. for example, h can be the set of
all predictors that can be implemented by a c++ program written in at most
109 bits of code. in our papayas example, we mentioned previously the class of
axis aligned rectangles. while this is an in   nite class, if we discretize the repre-
sentation of real numbers, say, by using a 64 bits    oating-point representation,
the hypothesis class becomes a    nite class.

let us now analyze the performance of the ermh learning rule assuming that
h is a    nite class. for a training sample, s, labeled according to some f : x     y,
let hs denote a result of applying ermh to s, namely,

hs     argmin
h   h

ls(h).

(2.4)

in this chapter, we make the following simplifying assumption (which will be

relaxed in the next chapter).

38

a gentle start

definition 2.1 (the realizability assumption) there exists h(cid:63)     h s.t.
l(d,f )(h(cid:63)) = 0. note that this assumption implies that with id203 1 over
random samples, s, where the instances of s are sampled according to d and
are labeled by f , we have ls(h(cid:63)) = 0.

the realizability assumption implies that for every erm hypothesis we have
that3 ls(hs) = 0. however, we are interested in the true risk of hs, l(d,f )(hs),
rather than its empirical risk.

clearly, any guarantee on the error with respect to the underlying distribution,
d, for an algorithm that has access only to a sample s should depend on the
relationship between d and s. the common assumption in statistical machine
learning is that the training sample s is generated by sampling points from the
distribution d independently of each other. formally,
    the i.i.d. assumption: the examples in the training set are independently
and identically distributed (i.i.d.) according to the distribution d. that is,
every xi in s is freshly sampled according to d and then labeled according
to the labeling function, f . we denote this assumption by s     dm where
m is the size of s, and dm denotes the id203 over m-tuples induced
by applying d to pick each element of the tuple independently of the other
members of the tuple.

intuitively, the training set s is a window through which the learner
gets partial information about the distribution d over the world and the
labeling function, f . the larger the sample gets, the more likely it is to
re   ect more accurately the distribution and labeling used to generate it.

since l(d,f )(hs) depends on the training set, s, and that training set is picked
by a random process, there is randomness in the choice of the predictor hs
and, consequently, in the risk l(d,f )(hs). formally, we say that it is a random
variable. it is not realistic to expect that with full certainty s will su   ce to
direct the learner toward a good classi   er (from the point of view of d), as
there is always some id203 that the sampled training data happens to
be very nonrepresentative of the underlying d. if we go back to the papaya
tasting example, there is always some (small) chance that all the papayas we
have happened to taste were not tasty, in spite of the fact that, say, 70% of the
papayas in our island are tasty. in such a case, ermh(s) may be the constant
function that labels every papaya as    not tasty    (and has 70% error on the true
distribution of papapyas in the island). we will therefore address the id203
to sample a training set for which l(d,f )(hs) is not too large. usually, we denote
the id203 of getting a nonrepresentative sample by   , and call (1       ) the
con   dence parameter of our prediction.

on top of that, since we cannot guarantee perfect label prediction, we intro-
duce another parameter for the quality of prediction, the accuracy parameter,

3 mathematically speaking, this holds with id203 1. to simplify the presentation, we

sometimes omit the    with id203 1    speci   er.

2.3 empirical risk minimization with inductive bias

39

commonly denoted by  . we interpret the event l(d,f )(hs) >   as a failure of the
learner, while if l(d,f )(hs)       we view the output of the algorithm as an approx-
imately correct predictor. therefore (   xing some labeling function f : x     y),
we are interested in upper bounding the id203 to sample m-tuple of in-
stances that will lead to failure of the learner. formally, let s|x = (x1, . . . , xm)
be the instances of the training set. we would like to upper bound

dm({s|x : l(d,f )(hs) >  }).

let hb be the set of    bad    hypotheses, that is,

hb = {h     h : l(d,f )(h) >  }.

in addition, let

m = {s|x :    h     hb, ls(h) = 0}

be the set of misleading samples: namely, for every s|x     m , there is a    bad   
hypothesis, h     hb, that looks like a    good    hypothesis on s|x. now, recall that
we would like to bound the id203 of the event l(d,f )(hs) >  . but, since
the realizability assumption implies that ls(hs) = 0, it follows that the event
l(d,f )(hs) >   can only happen if for some h     hb we have ls(h) = 0. in
other words, this event will only happen if our sample is in the set of misleading
samples, m . formally, we have shown that

{s|x : l(d,f )(hs) >  }     m .

note that we can rewrite m as

m =

{s|x : ls(h) = 0}.

(2.5)

(cid:91)

h   hb

hence,

dm({s|x : l(d,f )(hs) >  })     dm(m ) = dm(   h   hb{s|x : ls(h) = 0}).

(2.6)
next, we upper bound the right-hand side of the preceding equation using the

union bound     a basic property of probabilities.
lemma 2.2 (union bound) for any two sets a, b and a distribution d we
have

d(a     b)     d(a) + d(b).

applying the union bound to the right-hand side of equation (2.6) yields

dm({s|x : l(d,f )(hs) >  })     (cid:88)

h   hb

dm({s|x : ls(h) = 0}).

(2.7)

next, let us bound each summand of the right-hand side of the preceding in-
equality. fix some    bad    hypothesis h     hb. the event ls(h) = 0 is equivalent

40

a gentle start

to the event    i, h(xi) = f (xi). since the examples in the training set are sampled
i.i.d. we get that

dm({s|x : ls(h) = 0}) = dm({s|x :    i, h(xi) = f (xi)})
d({xi : h(xi) = f (xi)}).

m(cid:89)

=

(2.8)

i=1

for each individual sampling of an element of the training set we have

d({xi : h(xi) = yi}) = 1     l(d,f )(h)     1      ,

where the last inequality follows from the fact that h     hb. combining the
previous equation with equation (2.8) and using the inequality 1           e     we
obtain that for every h     hb,

dm({s|x : ls(h) = 0})     (1      )m     e    m.

(2.9)

combining this equation with equation (2.7) we conclude that

dm({s|x : l(d,f )(hs) >  })     |hb| e    m     |h| e     m.

a graphical illustration which explains how we used the union bound is given in
figure 2.1.

figure 2.1 each point in the large circle represents a possible m-tuple of instances.
each colored oval represents the set of    misleading    m-tuple of instances for some
   bad    predictor h     hb. the erm can potentially over   t whenever it gets a
misleading training set s. that is, for some h     hb we have ls(h) = 0.
equation (2.9) guarantees that for each individual bad hypothesis, h     hb, at most
(1      )m-fraction of the training sets would be misleading. in particular, the larger m
is, the smaller each of these colored ovals becomes. the union bound formalizes the
fact that the area representing the training sets that are misleading with respect to
some h     hb (that is, the training sets in m ) is at most the sum of the areas of the
colored ovals. therefore, it is bounded by |hb| times the maximum size of a colored
oval. any sample s outside the colored ovals cannot cause the erm rule to over   t.

corollary 2.3 let h be a    nite hypothesis class. let        (0, 1) and   > 0

2.4 exercises

41

.

and let m be an integer that satis   es

m     log(|h|/  )

 

then, for any labeling function, f , and for any distribution, d, for which the
realizability assumption holds (that is, for some h     h, l(d,f )(h) = 0), with
id203 of at least 1        over the choice of an i.i.d. sample s of size m, we
have that for every erm hypothesis, hs, it holds that

l(d,f )(hs)      .

the preceeding corollary tells us that for a su   ciently large m, the ermh rule
over a    nite hypothesis class will be probably (with con   dence 1     ) approximately
(up to an error of  ) correct. in the next chapter we formally de   ne the model
of probably approximately correct (pac) learning.

2.4

exercises

1. over   tting of polynomial matching: we have shown that the predictor
de   ned in equation (2.3) leads to over   tting. while this predictor seems to
be very unnatural, the goal of this exercise is to show that it can be described
as a thresholded polynomial. that is, show that given a training set s =
{(xi, f (xi))}m
i=1     (rd    {0, 1})m, there exists a polynomial ps such that
hs(x) = 1 if and only if ps(x)     0, where hs is as de   ned in equation (2.3).
it follows that learning the class of all thresholded polynomials using the erm
rule may lead to over   tting.

2. let h be a class of binary classi   ers over a domain x . let d be an unknown
distribution over x , and let f be the target hypothesis in h. fix some h     h.
show that the expected value of ls(h) over the choice of s|x equals l(d,f )(h),
namely,

e

s|x   dm

[ls(h)] = l(d,f )(h).

3. axis aligned rectangles: an axis aligned rectangle classi   er in the plane
is a classi   er that assigns the value 1 to a point if and only if it is inside a
certain rectangle. formally, given real numbers a1     b1, a2     b2, de   ne the
classi   er h(a1,b1,a2,b2) by

h(a1,b1,a2,b2)(x1, x2) =

if a1     x1     b1 and a2     x2     b2
otherwise

.

1

0

(2.10)

(cid:40)

the class of all axis aligned rectangles in the plane is de   ned as

h2
rec = {h(a1,b1,a2,b2) : a1     b1, and a2     b2}.

note that this is an in   nite size hypothesis class. throughout this exercise we
rely on the realizability assumption.

42

a gentle start

 

then, with proba-

1. let a be the algorithm that returns the smallest rectangle enclosing all
2. show that if a receives a training set of size     4 log(4/  )

positive examples in the training set. show that a is an erm.
bility of at least 1        it returns a hypothesis with error of at most  .
hint: fix some distribution d over x , let r    = r(a   
2) be the rect-
angle that generates the labels, and let f be the corresponding hypothesis.
let a1     a   
1 be a number such that the id203 mass (with respect
to d) of the rectangle r1 = r(a   
2) is exactly  /4. similarly, let
b1, a2, b2 be numbers such that the id203 masses of the rectangles
r2 = r(b1, b   
2), r3 = r(a   
2) are all
exactly  /4. let r(s) be the rectangle returned by a. see illustration in
figure 2.2.

2, b   
2, a2), r4 = r(a   

1, a1, a   
1, b   
1, a   

1, b2, b   

1, a   

1, a   

2, b   

2, b   

1, b   

1, b   

-

+

r   

+
r(s)

+

-

+

r1

+

figure 2.2 axis aligned rectangles.
    show that r(s)     r   .
    show that if s contains (positive) examples in all of the rectangles
r1, r2, r3, r4, then the hypothesis returned by a has error of at
most  .
    for each i     {1, . . . , 4}, upper bound the id203 that s does not
    use the union bound to conclude the argument.

contain an example from ri.

3. repeat the previous question for the class of axis aligned rectangles in rd.
4. show that the runtime of applying the algorithm a mentioned earlier is

polynomial in d, 1/ , and in log(1/  ).

3

a formal learning model

in this chapter we de   ne our main formal learning model     the pac learning
model and its extensions. we will consider other notions of learnability in chap-
ter 7.

3.1

pac learning

in the previous chapter we have shown that for a    nite hypothesis class, if the
erm rule with respect to that class is applied on a su   ciently large training
sample (whose size is independent of the underlying distribution or labeling
function) then the output hypothesis will be probably approximately correct.
more generally, we now de   ne probably approximately correct (pac) learning.
definition 3.1 (pac learnability) a hypothesis class h is pac learnable
if there exist a function mh : (0, 1)2     n and a learning algorithm with the
following property: for every  ,        (0, 1), for every distribution d over x , and
for every labeling function f : x     {0, 1}, if the realizable assumption holds
with respect to h,d, f , then when running the learning algorithm on m    
mh( ,   ) i.i.d. examples generated by d and labeled by f , the algorithm returns
a hypothesis h such that, with id203 of at least 1        (over the choice of
the examples), l(d,f )(h)      .

the de   nition of probably approximately correct learnability contains two
approximation parameters. the accuracy parameter   determines how far the
output classi   er can be from the optimal one (this corresponds to the    approx-
imately correct   ), and a con   dence parameter    indicating how likely the clas-
si   er is to meet that accuracy requirement (corresponds to the    probably    part
of    pac   ). under the data access model that we are investigating, these ap-
proximations are inevitable. since the training set is randomly generated, there
may always be a small chance that it will happen to be noninformative (for ex-
ample, there is always some chance that the training set will contain only one
domain point, sampled over and over again). furthermore, even when we are
lucky enough to get a training sample that does faithfully represent d, because
it is just a    nite sample, there may always be some    ne details of d that it fails

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

44

a formal learning model

to re   ect. our accuracy parameter,  , allows    forgiving    the learner   s classi   er
for making minor errors.

sample complexity
the function mh : (0, 1)2     n determines the sample complexity of learning h:
that is, how many examples are required to guarantee a probably approximately
correct solution. the sample complexity is a function of the accuracy ( ) and
con   dence (  ) parameters. it also depends on properties of the hypothesis class
h     for example, for a    nite class we showed that the sample complexity depends
on log the size of h.
note that if h is pac learnable, there are many functions mh that satisfy the
requirements given in the de   nition of pac learnability. therefore, to be precise,
we will de   ne the sample complexity of learning h to be the    minimal function,   
in the sense that for any  ,   , mh( ,   ) is the minimal integer that satis   es the
requirements of pac learning with accuracy   and con   dence   .

let us now recall the conclusion of the analysis of    nite hypothesis classes

from the previous chapter. it can be rephrased as stating:

corollary 3.2 every    nite hypothesis class is pac learnable with sample
complexity

(cid:24) log(|h|/  )

(cid:25)

.

 

mh( ,   )    

there are in   nite classes that are learnable as well (see, for example, exer-
cise 3). later on we will show that what determines the pac learnability of
a class is not its    niteness but rather a combinatorial measure called the vc
dimension.

3.2

a more general learning model

the model we have just described can be readily generalized, so that it can be
made relevant to a wider scope of learning tasks. we consider generalizations in
two aspects:

removing the realizability assumption
we have required that the learning algorithm succeeds on a pair of data distri-
bution d and labeling function f provided that the realizability assumption is
met. for practical learning tasks, this assumption may be too strong (can we
really guarantee that there is a rectangle in the color-hardness space that fully
determines which papayas are tasty?). in the next subsection, we will describe
the agnostic pac model in which this realizability assumption is waived.

3.2 a more general learning model

45

learning problems beyond binary classi   cation
the learning task that we have been discussing so far has to do with predicting a
binary label to a given example (like being tasty or not). however, many learning
tasks take a di   erent form. for example, one may wish to predict a real valued
number (say, the temperature at 9:00 p.m. tomorrow) or a label picked from
a    nite set of labels (like the topic of the main story in tomorrow   s paper). it
turns out that our analysis of learning can be readily extended to such and many
other scenarios by allowing a variety of id168s. we shall discuss that in
section 3.2.2 later.

3.2.1

releasing the realizability assumption     agnostic pac learning

a more realistic model for the data-generating distribution
recall that the realizability assumption requires that there exists h(cid:63)     h such
that px   d[h(cid:63)(x) = f (x)] = 1. in many practical problems this assumption does
not hold. furthermore, it is maybe more realistic not to assume that the labels
are fully determined by the features we measure on input elements (in the case of
the papayas, it is plausible that two papayas of the same color and softness will
have di   erent taste). in the following, we relax the realizability assumption by
replacing the    target labeling function    with a more    exible notion, a data-labels
generating distribution.

formally, from now on, let d be a id203 distribution over x   y, where,
as before, x is our domain set and y is a set of labels (usually we will consider
y = {0, 1}). that is, d is a joint distribution over domain points and labels. one
can view such a distribution as being composed of two parts: a distribution dx
over unlabeled domain points (sometimes called the marginal distribution) and
a id155 over labels for each domain point, d((x, y)|x). in the
papaya example, dx determines the id203 of encountering a papaya whose
color and hardness fall in some color-hardness values domain, and the conditional
id203 is the id203 that a papaya with color and hardness represented
by x is tasty. indeed, such modeling allows for two papayas that share the same
color and hardness to belong to di   erent taste categories.

the empirical and the true error revised
for a id203 distribution, d, over x    y, one can measure how likely h is
to make an error when labeled points are randomly drawn according to d. we
rede   ne the true error (or risk) of a prediction rule h to be

ld(h) def=

p

(x,y)   d[h(x) (cid:54)= y] def= d({(x, y) : h(x) (cid:54)= y}).

(3.1)

we would like to    nd a predictor, h, for which that error will be minimized.
however, the learner does not know the data generating d. what the learner
does have access to is the training data, s. the de   nition of the empirical risk

46

a formal learning model

remains the same as before, namely,

ls(h) def=

|{i     [m] : h(xi) (cid:54)= yi}|

.

m

given s, a learner can compute ls(h) for any function h : x     {0, 1}. note
that ls(h) = ld(uniform over s)(h).

the goal
we wish to    nd some hypothesis, h : x     y, that (probably approximately)
minimizes the true risk, ld(h).

the bayes optimal predictor.
given any id203 distribution d over x    {0, 1}, the best label predicting
function from x to {0, 1} will be

(cid:40)

1

0

fd(x) =

if p[y = 1|x]     1/2
otherwise

it is easy to verify (see exercise 7) that for every id203 distribution d,
the bayes optimal predictor fd is optimal, in the sense that no other classi   er,
g : x     {0, 1} has a lower error. that is, for every classi   er g, ld(fd)     ld(g).
unfortunately, since we do not know d, we cannot utilize this optimal predictor
fd. what the learner does have access to is the training sample. we can now
present the formal de   nition of agnostic pac learnability, which is a natural
extension of the de   nition of pac learnability to the more realistic, nonrealizable,
learning setup we have just discussed.

clearly, we cannot hope that the learning algorithm will    nd a hypothesis
whose error is smaller than the minimal possible error, that of the bayes predic-
tor.

furthermore, as we shall prove later, once we make no prior assumptions
about the data-generating distribution, no algorithm can be guaranteed to    nd
a predictor that is as good as the bayes optimal one. instead, we require that
the learning algorithm will    nd a predictor whose error is not much larger than
the best possible error of a predictor in some given benchmark hypothesis class.
of course, the strength of such a requirement depends on the choice of that
hypothesis class.
definition 3.3 (agnostic pac learnability) a hypothesis class h is agnostic
pac learnable if there exist a function mh : (0, 1)2     n and a learning algorithm
with the following property: for every  ,        (0, 1) and for every distribution d
over x   y, when running the learning algorithm on m     mh( ,   ) i.i.d. examples
generated by d, the algorithm returns a hypothesis h such that, with id203
of at least 1        (over the choice of the m training examples),

ld(h)     min

h(cid:48)   h ld(h(cid:48)) +  .

3.2 a more general learning model

47

clearly, if the realizability assumption holds, agnostic pac learning provides
the same guarantee as pac learning. in that sense, agnostic pac learning gener-
alizes the de   nition of pac learning. when the realizability assumption does not
hold, no learner can guarantee an arbitrarily small error. nevertheless, under the
de   nition of agnostic pac learning, a learner can still declare success if its error
is not much larger than the best error achievable by a predictor from the class h.
this is in contrast to pac learning, in which the learner is required to achieve
a small error in absolute terms and not relative to the best error achievable by
the hypothesis class.

3.2.2

the scope of learning problems modeled

we next extend our model so that it can be applied to a wide variety of learning
tasks. let us consider some examples of di   erent learning tasks.
    multiclass classi   cation our classi   cation does not have to be binary.
take, for example, the task of document classi   cation: we wish to design a
program that will be able to classify given documents according to topics
(e.g., news, sports, biology, medicine). a learning algorithm for such a task
will have access to examples of correctly classi   ed documents and, on the
basis of these examples, should output a program that can take as input a
new document and output a topic classi   cation for that document. here,
the domain set
is the set of all potential documents. once again, we would
usually represent documents by a set of features that could include counts
of di   erent key words in the document, as well as other possibly relevant
features like the size of the document or its origin. the label set in this task
will be the set of possible document topics (so y will be some large    nite
set). once we determine our domain and label sets, the other components
of our framework look exactly the same as in the papaya tasting example;
our training sample will be a    nite sequence of (feature vector, label) pairs,
the learner   s output will be a function from the domain set to the label set,
and,    nally, for our measure of success, we can use the id203, over
(document, topic) pairs, of the event that our predictor suggests a wrong
label.

    regression in this task, one wishes to    nd some simple pattern in the data    
a functional relationship between the x and y components of the data. for
example, one wishes to    nd a linear function that best predicts a baby   s
birth weight on the basis of ultrasound measures of his head circumference,
abdominal circumference, and femur length. here, our domain set x is some
subset of r3 (the three ultrasound measurements), and the set of    labels,   
y, is the the set of real numbers (the weight in grams). in this context,
it is more adequate to call y the target set. our training data as well as
the learner   s output are as before (a    nite sequence of (x, y) pairs, and
a function from x to y respectively). however, our measure of success is

48

a formal learning model

di   erent. we may evaluate the quality of a hypothesis function, h : x     y,
by the expected square di   erence between the true labels and their predicted
values, namely,

ld(h) def=

(x,y)   d(h(x)     y)2.
e

(3.2)

to accommodate a wide range of learning tasks we generalize our formalism

of the measure of success as follows:

generalized id168s
given any set h (that plays the role of our hypotheses, or models) and some
domain z let (cid:96) be any function from h  z to the set of nonnegative real numbers,
(cid:96) : h    z     r+.

we call such functions id168s.
note that for prediction problems, we have that z = x    y. however, our
notion of the id168 is generalized beyond prediction tasks, and therefore
it allows z to be any domain of examples (for instance, in unsupervised learning
tasks such as the one described in chapter 22, z is not a product of an instance
domain and a label domain).
we now de   ne the risk function to be the expected loss of a classi   er, h     h,

with respect to a id203 distribution d over z, namely,

ld(h) def= e

z   d[(cid:96)(h, z)].

(3.3)

that is, we consider the expectation of the loss of h over objects z picked ran-
domly according to d. similarly, we de   ne the empirical risk to be the expected
loss over a given sample s = (z1, . . . , zm)     z m, namely,

m(cid:88)

i=1

(cid:40)

ls(h) def=

1
m

(cid:96)(h, zi).

(3.4)

the id168s used in the preceding examples of classi   cation and regres-

sion tasks are as follows:
    0   1 loss: here, our random variable z ranges over the set of pairs x   y and

the id168 is

(cid:96)0   1(h, (x, y)) def=

0

1

if h(x) = y
if h(x) (cid:54)= y

this id168 is used in binary or multiclass classi   cation problems.
one should note that, for a random variable,   , taking the values {0, 1},
e     d[  ] = p     d[   = 1]. consequently, for this id168, the de   ni-
tions of ld(h) given in equation (3.3) and equation (3.1) coincide.
    square loss: here, our random variable z ranges over the set of pairs x   y

and the id168 is

(cid:96)sq(h, (x, y)) def= (h(x)     y)2.

3.3 summary

49

this id168 is used in regression problems.

we will later see more examples of useful instantiations of id168s.

to summarize, we formally de   ne agnostic pac learnability for general loss

functions.

definition 3.4 (agnostic pac learnability for general id168s) a
hypothesis class h is agnostic pac learnable with respect to a set z and a
id168 (cid:96) : h    z     r+, if there exist a function mh : (0, 1)2     n
and a learning algorithm with the following property: for every  ,        (0, 1)
and for every distribution d over z, when running the learning algorithm on
m     mh( ,   ) i.i.d. examples generated by d, the algorithm returns h     h
such that, with id203 of at least 1        (over the choice of the m training
examples),

ld(h)     min

h(cid:48)   h ld(h(cid:48)) +  ,

where ld(h) = ez   d[(cid:96)(h, z)].

remark 3.1 (a note about measurability*)
in the aforementioned de   nition,
for every h     h, we view the function (cid:96)(h,  ) : z     r+ as a random variable and
de   ne ld(h) to be the expected value of this random variable. for that, we need
to require that the function (cid:96)(h,  ) is measurable. formally, we assume that there
is a   -algebra of subsets of z, over which the id203 d is de   ned, and that
the preimage of every initial segment in r+ is in this   -algebra. in the speci   c
case of binary classi   cation with the 0   1 loss, the   -algebra is over x    {0, 1}
and our assumption on (cid:96) is equivalent to the assumption that for every h, the
set {(x, h(x)) : x     x} is in the   -algebra.
remark 3.2 (proper versus representation-independent learning*)
in the pre-
ceding de   nition, we required that the algorithm will return a hypothesis from
h. in some situations, h is a subset of a set h(cid:48), and the id168 can be
naturally extended to be a function from h(cid:48)    z to the reals. in this case, we
may allow the algorithm to return a hypothesis h(cid:48)     h(cid:48), as long as it satis   es
the requirement ld(h(cid:48))     minh   h ld(h) +  . allowing the algorithm to output
a hypothesis from h(cid:48) is called representation independent learning, while proper
learning occurs when the algorithm must output a hypothesis from h. represen-
tation independent learning is sometimes called    improper learning,    although
there is nothing improper in representation independent learning.

3.3

summary

in this chapter we de   ned our main formal learning model     pac learning. the
basic model relies on the realizability assumption, while the agnostic variant does

50

a formal learning model

not impose any restrictions on the underlying distribution over the examples. we
also generalized the pac model to arbitrary id168s. we will sometimes
refer to the most general model simply as pac learning, omitting the    agnostic   
pre   x and letting the reader infer what the underlying id168 is from the
context. when we would like to emphasize that we are dealing with the original
pac setting we mention that the realizability assumption holds. in chapter 7
we will discuss other notions of learnability.

3.4

bibliographic remarks

our most general de   nition of agnostic pac learning with general loss func-
tions follows the works of vladimir vapnik and alexey chervonenkis (vapnik &
chervonenkis 1971). in particular, we follow vapnik   s general setting of learning
(vapnik 1982, vapnik 1992, vapnik 1995, vapnik 1998).

pac learning was introduced by valiant (1984). valiant was named the winner
of the 2010 turing award for the introduction of the pac model. valiant   s
de   nition requires that the sample complexity will be polynomial in 1/  and
in 1/  , as well as in the representation size of hypotheses in the class (see also
kearns & vazirani (1994)). as we will see in chapter 6, if a problem is at all pac
learnable then the sample complexity depends polynomially on 1/  and log(1/  ).
valiant   s de   nition also requires that the runtime of the learning algorithm will
be polynomial in these quantities. in contrast, we chose to distinguish between
the statistical aspect of learning and the computational aspect of learning. we
will elaborate on the computational aspect later on in chapter 8, where we
introduce the full pac learning model of valiant. for expository reasons, we
use the term pac learning even when we ignore the runtime aspect of learning.
finally, the formalization of agnostic pac learning is due to haussler (1992).

3.5

exercises
1. monotonicity of sample complexity: let h be a hypothesis class for a
binary classi   cation task. suppose that h is pac learnable and its sample
complexity is given by mh(  ,  ). show that mh is monotonically nonincreasing
in each of its parameters. that is, show that given        (0, 1), and given 0 <
 1      2 < 1, we have that mh( 1,   )     mh( 2,   ). similarly, show that given
      (0, 1), and given 0 <   1       2 < 1, we have that mh( ,   1)     mh( ,   2).

2. let x be a discrete domain, and let hsingleton = {hz : z     x}     {h   }, where
for each z     x , hz is the function de   ned by hz(x) = 1 if x = z and hz(x) = 0
if x (cid:54)= z. h    is simply the all-negative hypothesis, namely,    x     x, h   (x) = 0.
the realizability assumption here implies that the true hypothesis f labels
negatively all examples in the domain, perhaps except one.

3.5 exercises

51

in the realizable setup.

1. describe an algorithm that implements the erm rule for learning hsingleton
2. show that hsingleton is pac learnable. provide an upper bound on the
3. let x = r2, y = {0, 1}, and let h be the class of concentric circles in the
plane, that is, h = {hr : r     r+}, where hr(x) = 1[(cid:107)x(cid:107)   r]. prove that h is
pac learnable (assume realizability), and its sample complexity is bounded
by

sample complexity.

(cid:24) log(1/  )

(cid:25)

.

 

mh( ,   )    

4. in this question, we study the hypothesis class of boolean conjunctions de   ned
as follows. the instance space is x = {0, 1}d and the label set is y = {0, 1}. a
literal over the variables x1, . . . , xd is a simple boolean function that takes the
form f (x) = xi, for some i     [d], or f (x) = 1    xi for some i     [d]. we use the
notation   xi as a shorthand for 1    xi. a conjunction is any product of literals.
in boolean logic, the product is denoted using the     sign. for example, the
function h(x) = x1    (1     x2) is written as x1       x2.

we consider the hypothesis class of all conjunctions of literals over the d
variables. the empty conjunction is interpreted as the all-positive hypothesis
(namely, the function that returns h(x) = 1 for all x). the conjunction x1      x1
(and similarly any conjunction involving a literal and its negation) is allowed
and interpreted as the all-negative hypothesis (namely, the conjunction that
returns h(x) = 0 for all x). we assume realizability: namely, we assume
that there exists a boolean conjunction that generates the labels. thus, each
example (x, y)     x    y consists of an assignment to the d boolean variables
x1, . . . , xd, and its truth value (0 for false and 1 for true).
for instance, let d = 3 and suppose that the true conjunction is x1       x2.

then, the training set s might contain the following instances:

((1, 1, 1), 0), ((1, 0, 1), 1), ((0, 1, 0), 0)((1, 0, 0), 1).

prove that the hypothesis class of all conjunctions over d variables is
pac learnable and bound its sample complexity. propose an algorithm that
implements the erm rule, whose runtime is polynomial in d    m.
5. let x be a domain and let d1,d2, . . . ,dm be a sequence of distributions
over x . let h be a    nite class of binary classi   ers over x and let f     h.
suppose we are getting a sample s of m examples, such that the instances are
independent but are not identically distributed; the ith instance is sampled
from di and then yi is set to be f (xi). let   dm denote the average, that is,
  dm = (d1 +        + dm)/m.

fix an accuracy parameter       (0, 1). show that

p(cid:2)   h     h s.t. l(   dm,f )(h) >   and l(s,f )(h) = 0(cid:3)     |h|e    m.

52

a formal learning model

hint: use the geometric-arithmetic mean inequality.

6. let h be a hypothesis class of binary classi   ers. show that if h is agnostic
pac learnable, then h is pac learnable as well. furthermore, if a is a suc-
cessful agnostic pac learner for h, then a is also a successful pac learner
for h.
7. (*) the bayes optimal predictor: show that for every id203 distri-
bution d, the bayes optimal predictor fd is optimal, in the sense that for
every classi   er g from x to {0, 1}, ld(fd)     ld(g).
id203 distribution, d, if

8. (*) we say that a learning algorithm a is better than b with respect to some

ld(a(s))     ld(b(s))

for all samples s     (x   {0, 1})m. we say that a learning algorithm a is better
than b, if it is better than b with respect to all id203 distributions d
over x    {0, 1}.
1. a probabilistic label predictor is a function that assigns to every domain
point x a id203 value, h(x)     [0, 1], that determines the id203 of
predicting the label 1. that is, given such an h and an input, x, the label for
x is predicted by tossing a coin with bias h(x) toward heads and predicting
1 i    the coin comes up heads. formally, we de   ne a probabilistic label
predictor as a function, h : x     [0, 1]. the loss of such h on an example
(x, y) is de   ned to be |h(x)     y|, which is exactly the id203 that the
prediction of h will not be equal to y. note that if h is deterministic, that
is, returns values in {0, 1}, then |h(x)     y| = 1[h(x)(cid:54)=y].
prove that for every data-generating distribution d over x    {0, 1}, the
bayes optimal predictor has the smallest risk (w.r.t. the id168
(cid:96)(h, (x, y)) = |h(x)   y|, among all possible label predictors, including prob-
abilistic ones).
2. let x be a domain and {0, 1} be a set of labels. prove that for every
distribution d over x   {0, 1}, there exist a learning algorithm ad that is
better than any other learning algorithm with respect to d.
3. prove that for every learning algorithm a there exist a id203 distri-
bution, d, and a learning algorithm b such that a is not better than b
w.r.t. d.

9. consider a variant of the pac model in which there are two example ora-
cles: one that generates positive examples and one that generates negative
examples, both according to the underlying distribution d on x . formally,
given a target function f : x     {0, 1}, let d+ be the distribution over
x + = {x     x : f (x) = 1} de   ned by d+(a) = d(a)/d(x +), for every
a     x +. similarly, d    is the distribution over x     induced by d.

the de   nition of pac learnability in the two-oracle model is the same as the
standard de   nition of pac learnability except that here the learner has access
to m+h( ,   ) i.i.d. examples from d+ and m   ( ,   ) i.i.d. examples from d   . the
learner   s goal is to output h s.t. with id203 at least 1       (over the choice

3.5 exercises

53

of the two training sets, and possibly over the nondeterministic decisions made
by the learning algorithm), both l(d+,f )(h)       and l(d   ,f )(h)      .
1. (*) show that if h is pac learnable (in the standard one-oracle model),

then h is pac learnable in the two-oracle model.
2. (**) de   ne h+ to be the always-plus hypothesis and h    to be the always-
minus hypothesis. assume that h+, h        h. show that if h is pac learn-
able in the two-oracle model, then h is pac learnable in the standard
one-oracle model.

4

learning via uniform convergence

the    rst formal learning model that we have discussed was the pac model.
in chapter 2 we have shown that under the realizability assumption, any    nite
hypothesis class is pac learnable. in this chapter we will develop a general tool,
uniform convergence, and apply it to show that any    nite class is learnable in
the agnostic pac model with general id168s, as long as the range loss
function is bounded.

4.1

uniform convergence is su   cient for learnability

the idea behind the learning condition discussed in this chapter is very simple.
recall that, given a hypothesis class, h, the erm learning paradigm works
as follows: upon receiving a training sample, s, the learner evaluates the risk
(or error) of each h in h on the given sample and outputs a member of h that
minimizes this empirical risk. the hope is that an h that minimizes the empirical
risk with respect to s is a risk minimizer (or has risk close to the minimum) with
respect to the true data id203 distribution as well. for that, it su   ces to
ensure that the empirical risks of all members of h are good approximations of
their true risk. put another way, we need that uniformly over all hypotheses in
the hypothesis class, the empirical risk will be close to the true risk, as formalized
in the following.

definition 4.1 ( -representative sample) a training set s is called  -representative
(w.r.t. domain z, hypothesis class h, id168 (cid:96), and distribution d) if

   h     h,

|ls(h)     ld(h)|      .

the next simple lemma states that whenever the sample is ( /2)-representative,

the erm learning rule is guaranteed to return a good hypothesis.

lemma 4.2 assume that a training set s is  
2 -representative (w.r.t. domain
z, hypothesis class h, id168 (cid:96), and distribution d). then, any output of
ermh(s), namely, any hs     argminh   h ls(h), satis   es

ld(hs)     min

h   h ld(h) +  .

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

4.2 finite classes are agnostic pac learnable

55

proof for every h     h,

ld(hs)     ls(hs) +  

2     ls(h) +  

2     ld(h) +  

2 +  

2 = ld(h) +  ,

where the    rst and third inequalities are due to the assumption that s is  
2 -
representative (de   nition 4.1) and the second inequality holds since hs is an
erm predictor.

the preceding lemma implies that to ensure that the erm rule is an agnostic
pac learner, it su   ces to show that with id203 of at least 1        over the
random choice of a training set, it will be an  -representative training set. the
uniform convergence condition formalizes this requirement.
definition 4.3 (uniform convergence) we say that a hypothesis class h has
the uniform convergence property (w.r.t. a domain z and a id168 (cid:96)) if
there exists a function much : (0, 1)2     n such that for every  ,        (0, 1) and
for every id203 distribution d over z, if s is a sample of m     much ( ,   )
examples drawn i.i.d. according to d, then, with id203 of at least 1       , s
is  -representative.

similar to the de   nition of sample complexity for pac learning, the function
much measures the (minimal) sample complexity of obtaining the uniform con-
vergence property, namely, how many examples we need to ensure that with
id203 of at least 1        the sample would be  -representative.
the term uniform here refers to having a    xed sample size that works for all
members of h and over all possible id203 distributions over the domain.
the following corollary follows directly from lemma 4.2 and the de   nition of

uniform convergence.

if a class h has the uniform convergence property with a
corollary 4.4
function much then the class is agnostically pac learnable with the sample com-
plexity mh( ,   )     much ( /2,   ). furthermore, in that case, the ermh paradigm
is a successful agnostic pac learner for h.

4.2

finite classes are agnostic pac learnable

in view of corollary 4.4, the claim that every    nite hypothesis class is agnostic
pac learnable will follow once we establish that uniform convergence holds for
a    nite hypothesis class.

to show that uniform convergence holds we follow a two step argument, similar
to the derivation in chapter 2. the    rst step applies the union bound while the
second step employs a measure concentration inequality. we now explain these
two steps in detail.
fix some  ,   . we need to    nd a sample size m that guarantees that for any
d, with id203 of at least 1        of the choice of s = (z1, . . . , zm) sampled

56

learning via uniform convergence

i.i.d. from d we have that for all h     h, |ls(h)     ld(h)|      . that is,

dm({s :    h     h,|ls(h)     ld(h)|      })     1       .

equivalently, we need to show that

dm({s :    h     h,|ls(h)     ld(h)| >  }) <   .

writing

{s :    h     h,|ls(h)     ld(h)| >  } =    h   h{s : |ls(h)     ld(h)| >  },

and applying the union bound (lemma 2.2) we obtain

dm({s :    h     h,|ls(h)     ld(h)| >  })     (cid:88)

dm({s : |ls(h)     ld(h)| >  }).

h   h

(4.1)
our second step will be to argue that each summand of the right-hand side
of this inequality is small enough (for a su   ciently large m). that is, we will
show that for any    xed hypothesis, h, (which is chosen in advance prior to the
sampling of the training set), the gap between the true and empirical risks,
|ls(h)     ld(h)|, is likely to be small.
i=1 (cid:96)(h, zi). since
each zi is sampled i.i.d. from d, the expected value of the random variable
(cid:96)(h, zi) is ld(h). by the linearity of expectation, it follows that ld(h) is also
the expected value of ls(h). hence, the quantity |ld(h)   ls(h)| is the deviation
of the random variable ls(h) from its expectation. we therefore need to show
that the measure of ls(h) is concentrated around its expected value.

recall that ld(h) = ez   d[(cid:96)(h, z)] and that ls(h) = 1

(cid:80)m

m

a basic statistical fact, the law of large numbers, states that when m goes to
in   nity, empirical averages converge to their true expectation. this is true for
ls(h), since it is the empirical average of m i.i.d random variables. however, since
the law of large numbers is only an asymptotic result, it provides no information
about the gap between the empirically estimated error and its true value for any
given,    nite, sample size.

instead, we will use a measure concentration inequality due to hoe   ding, which

quanti   es the gap between empirical averages and their expected value.

lemma 4.5 (hoe   ding   s inequality) let   1, . . . ,   m be a sequence of i.i.d. ran-
dom variables and assume that for all i, e[  i] =    and p[a       i     b] = 1. then,
for any   > 0

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

m

m(cid:88)

i=1

p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) >  
(cid:35)

  i       

    2 exp(cid:0)   2 m  2/(b     a)2(cid:1) .

the proof can be found in appendix b.
getting back to our problem, let   i be the random variable (cid:96)(h, zi). since h
is    xed and z1, . . . , zm are sampled i.i.d., it follows that   1, . . . ,   m are also i.i.d.
random variables. furthermore, ls(h) = 1
i=1   i and ld(h) =   . let us
m

(cid:80)m

4.2 finite classes are agnostic pac learnable

57

m(cid:88)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

further assume that the range of (cid:96) is [0, 1] and therefore   i     [0, 1]. we therefore
obtain that
dm({s : |ls(h)     ld(h)| >  }) = p

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) >  
(cid:35)
    2 exp(cid:0)   2 m  2(cid:1) .
dm({s :    h     h,|ls(h)     ld(h)| >  })     (cid:88)
2 exp(cid:0)   2 m  2(cid:1)
= 2|h| exp(cid:0)   2 m  2(cid:1) .

combining this with equation (4.1) yields

  i       

h   h

(4.2)

i=1

m

finally, if we choose

m     log(2|h|/  )

2 2

then

dm({s :    h     h,|ls(h)     ld(h)| >  })       .

corollary 4.6 let h be a    nite hypothesis class, let z be a domain, and let
(cid:96) : h    z     [0, 1] be a id168. then, h enjoys the uniform convergence
property with sample complexity

(cid:24) log(2|h|/  )

(cid:25)

2 2

.

much ( ,   )    

furthermore, the class is agnostically pac learnable using the erm algorithm
with sample complexity

mh( ,   )     much ( /2,   )    

(cid:24) 2 log(2|h|/  )

(cid:25)

 2

.

remark 4.1 (the    discretization trick   ) while the preceding corollary only
applies to    nite hypothesis classes, there is a simple trick that allows us to get
a very good estimate of the practical sample complexity of in   nite hypothesis
classes. consider a hypothesis class that is parameterized by d parameters. for
example, let x = r, y = {  1}, and the hypothesis class, h, be all functions
of the form h  (x) = sign(x       ). that is, each hypothesis is parameterized by
one parameter,        r, and the hypothesis outputs 1 for all instances larger than
   and outputs    1 for instances smaller than   . this is a hypothesis class of an
in   nite size. however, if we are going to learn this hypothesis class in practice,
using a computer, we will probably maintain real numbers using    oating point
representation, say, of 64 bits. it follows that in practice, our hypothesis class
is parameterized by the set of scalars that can be represented using a 64 bits
   oating point number. there are at most 264 such numbers; hence the actual
size of our hypothesis class is at most 264. more generally, if our hypothesis class
is parameterized by d numbers, in practice we learn a hypothesis class of size at
most 264d. applying corollary 4.6 we obtain that the sample complexity of such

58

learning via uniform convergence

 2

classes is bounded by 128d+2 log(2/  )
. this upper bound on the sample complex-
ity has the de   ciency of being dependent on the speci   c representation of real
numbers used by our machine. in chapter 6 we will introduce a rigorous way
to analyze the sample complexity of in   nite size hypothesis classes. neverthe-
less, the discretization trick can be used to get a rough estimate of the sample
complexity in many practical situations.

4.3

summary
if the uniform convergence property holds for a hypothesis class h then in most
cases the empirical risks of hypotheses in h will faithfully represent their true
risks. uniform convergence su   ces for agnostic pac learnability using the erm
rule. we have shown that    nite hypothesis classes enjoy the uniform convergence
property and are hence agnostic pac learnable.

4.4

bibliographic remarks

classes of functions for which the uniform convergence property holds are also
called glivenko-cantelli classes, named after valery ivanovich glivenko and
francesco paolo cantelli, who proved the    rst uniform convergence result in
the 1930s. see (dudley, gine & zinn 1991). the relation between uniform con-
vergence and learnability was thoroughly studied by vapnik     see (vapnik 1992,
vapnik 1995, vapnik 1998). in fact, as we will see later in chapter 6, the funda-
mental theorem of learning theory states that in binary classi   cation problems,
uniform convergence is not only a su   cient condition for learnability but is also
a necessary condition. this is not the case for more general learning problems
(see (shalev-shwartz, shamir, srebro & sridharan 2010)).

4.5

exercises

1. in this exercise, we show that the ( ,   ) requirement on the convergence of
errors in our de   nitions of pac learning, is, in fact, quite close to a sim-
pler looking requirement about averages (or expectations). prove that the
following two statements are equivalent (for any learning algorithm a, any
id203 distribution d, and any id168 whose range is [0, 1]):
1. for every  ,    > 0, there exists m( ,   ) such that    m     m( ,   )

2.

p

s   dm

[ld(a(s)) >  ] <   

lim
m      

e

s   dm

[ld(a(s))] = 0

4.5 exercises

59

(where es   dm denotes the expectation over samples s of size m).

2. bounded id168s: in corollary 4.6 we assumed that the range of the
id168 is [0, 1]. prove that if the range of the id168 is [a, b] then
the sample complexity satis   es

mh( ,   )     much ( /2,   )    

(cid:24) 2 log(2|h|/  )(b     a)2

(cid:25)

 2

.

5

the bias-complexity tradeo   

in chapter 2 we saw that unless one is careful, the training data can mislead the
learner, and result in over   tting. to overcome this problem, we restricted the
search space to some hypothesis class h. such a hypothesis class can be viewed
as re   ecting some prior knowledge that the learner has about the task     a belief
that one of the members of the class h is a low-error model for the task. for
example, in our papayas taste problem, on the basis of our previous experience
with other fruits, we may assume that some rectangle in the color-hardness plane
predicts (at least approximately) the papaya   s tastiness.

is such prior knowledge really necessary for the success of learning? maybe
there exists some kind of universal learner, that is, a learner who has no prior
knowledge about a certain task and is ready to be challenged by any task? let
us elaborate on this point. a speci   c learning task is de   ned by an unknown
distribution d over x    y, where the goal of the learner is to    nd a predictor
h : x     y, whose risk, ld(h), is small enough. the question is therefore whether
there exist a learning algorithm a and a training set size m, such that for every
distribution d, if a receives m i.i.d. examples from d, there is a high chance it
outputs a predictor h that has a low risk.

the    rst part of this chapter addresses this question formally. the no-free-
lunch theorem states that no such universal learner exists. to be more precise,
the theorem states that for binary classi   cation prediction tasks, for every learner
there exists a distribution on which it fails. we say that the learner fails if, upon
receiving i.i.d. examples from that distribution, its output hypothesis is likely
to have a large risk, say,     0.3, whereas for the same distribution, there exists
another learner that will output a hypothesis with a small risk. in other words,
the theorem states that no learner can succeed on all learnable tasks     every
learner has tasks on which it fails while other learners succeed.

therefore, when approaching a particular learning problem, de   ned by some
distribution d, we should have some prior knowledge on d. one type of such prior
knowledge is that d comes from some speci   c parametric family of distributions.
we will study learning under such assumptions later on in chapter 24. another
type of prior knowledge on d, which we assumed when de   ning the pac learning
model, is that there exists h in some prede   ned hypothesis class h, such that
ld(h) = 0. a softer type of prior knowledge on d is assuming that minh   h ld(h)
is small. in a sense, this weaker assumption on d is a prerequisite for using the

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

5.1 the no-free-lunch theorem

61

agnostic pac model, in which we require that the risk of the output hypothesis
will not be much larger than minh   h ld(h).

in the second part of this chapter we study the bene   ts and pitfalls of using
a hypothesis class as a means of formalizing prior knowledge. we decompose
the error of an erm algorithm over a class h into two components. the    rst
component re   ects the quality of our prior knowledge, measured by the minimal
risk of a hypothesis in our hypothesis class, minh   h ld(h). this component is
also called the approximation error, or the bias of the algorithm toward choosing
a hypothesis from h. the second component is the error due to over   tting,
which depends on the size or the complexity of the class h and is called the
estimation error. these two terms imply a tradeo    between choosing a more
complex h (which can decrease the bias but increases the risk of over   tting)
or a less complex h (which might increase the bias but decreases the potential
over   tting).

5.1

the no-free-lunch theorem

in this part we prove that there is no universal learner. we do this by showing
that no learner can succeed on all learning tasks, as formalized in the following
theorem:

theorem 5.1 (no-free-lunch) let a be any learning algorithm for the task
of binary classi   cation with respect to the 0     1 loss over a domain x . let m
be any number smaller than |x|/2, representing a training set size. then, there
exists a distribution d over x    {0, 1} such that:
1. there exists a function f : x     {0, 1} with ld(f ) = 0.
2. with id203 of at least 1/7 over the choice of s     dm we have that

ld(a(s))     1/8.

this theorem states that for every learner, there exists a task on which it fails,
even though that task can be successfully learned by another learner. indeed, a
trivial successful learner in this case would be an erm learner with the hypoth-
esis class h = {f}, or more generally, erm with respect to any    nite hypothesis
class that contains f and whose size satis   es the equation m     8 log(7|h|/6) (see
corollary 2.3).
proof let c be a subset of x of size 2m. the intuition of the proof is that
any learning algorithm that observes only half of the instances in c has no
information on what should be the labels of the rest of the instances in c.
therefore, there exists a    reality,    that is, some target function f , that would
contradict the labels that a(s) predicts on the unobserved instances in c.
note that there are t = 22m possible functions from c to {0, 1}. denote these
functions by f1, . . . , ft . for each such function, let di be a distribution over

62

the bias-complexity tradeo   

c    {0, 1} de   ned by

(cid:40)

di({(x, y)}) =

1/|c|
0

if y = fi(x)
otherwise.

that is, the id203 to choose a pair (x, y) is 1/|c| if the label y is indeed
the true label according to fi, and the id203 is 0 if y (cid:54)= fi(x). clearly,
ldi(fi) = 0.
we will show that for every algorithm, a, that receives a training set of m
examples from c   {0, 1} and returns a function a(s) : c     {0, 1}, it holds that

max
i   [t ]

e

s   dm

i

[ldi (a(s))]     1/4.

(5.1)

clearly, this means that for every algorithm, a(cid:48), that receives a training set of m
examples from x   {0, 1} there exist a function f : x     {0, 1} and a distribution
d over x    {0, 1}, such that ld(f ) = 0 and

e

[ld(a(cid:48)(s))]     1/4.

(5.2)
it is easy to verify that the preceding su   ces for showing that p[ld(a(cid:48)(s))    
1/8]     1/7, which is what we need to prove (see exercise 1).

s   dm

we now turn to proving that equation (5.1) holds. there are k = (2m)m
possible sequences of m examples from c. denote these sequences by s1, . . . , sk.
also, if sj = (x1, . . . , xm) we denote by si
j the sequence containing the instances
in sj labeled by the function fi, namely, si
j = ((x1, fi(x1)), . . . , (xm, fi(xm))). if
the distribution is di then the possible training sets a can receive are si
1, . . . , si
k,
and all these training sets have the same id203 of being sampled. therefore,

e

s   dm

i

[ldi(a(s))] =

1
k

ldi(a(si

j)).

(5.3)

using the facts that    maximum    is larger than    average    and that    average    is
larger than    minimum,    we have

k(cid:88)

j=1

t(cid:88)
k(cid:88)

i=1

j=1

1
k

1
t

j=1

k(cid:88)
t(cid:88)
t(cid:88)

i=1

i=1

    min
j   [k]

1
t

k(cid:88)

j=1

max
i   [t ]

1
k

ldi(a(si

j))     1
t

=

1
k

ldi(a(si

j))

ldi(a(si

j))

ldi(a(si

j)).

(5.4)

next,    x some j     [k]. denote sj = (x1, . . . , xm) and let v1, . . . , vp be the
examples in c that do not appear in sj. clearly, p     m. therefore, for every

5.1 the no-free-lunch theorem

63

function h : c     {0, 1} and every i we have

ldi(h) =

1
2m

x   c

(cid:88)
p(cid:88)
p(cid:88)

r=1

r=1

    1
2m

    1
2p

1[h(x)(cid:54)=fi(x)]

1[h(vr)(cid:54)=fi(vr)]

1[h(vr)(cid:54)=fi(vr)].

(5.5)

hence,

t(cid:88)

i=1

1
t

ldi(a(si

j))     1
t

=

1
2p

t(cid:88)
p(cid:88)

i=1

r=1

p(cid:88)
t(cid:88)

r=1

1
2p

i=1

1
t

1
[a(si

j )(vr)(cid:54)=fi(vr)]

1
[a(si

j )(vr)(cid:54)=fi(vr)]

t(cid:88)

i=1

    1
2

   min
r   [p]

1
t

1
[a(si

j )(vr)(cid:54)=fi(vr)].

(5.6)

next,    x some r     [p]. we can partition all the functions in f1, . . . , ft into t /2
disjoint pairs, where for a pair (fi, fi(cid:48)) we have that for every c     c, fi(c) (cid:54)= fi(cid:48)(c)
if and only if c = vr. since for such a pair we must have si
j , it follows that

j = si(cid:48)

1
[a(si

j )(vr)(cid:54)=fi(vr)] + 1

[a(si(cid:48)

j )(vr)(cid:54)=fi(cid:48) (vr)] = 1,

which yields

t(cid:88)

i=1

1
t

1
[a(si

j )(vr)(cid:54)=fi(vr)] =

1
2

.

combining this with equation (5.6), equation (5.4), and equation (5.3), we
obtain that equation (5.1) holds, which concludes our proof.

5.1.1

no-free-lunch and prior knowledge

how does the no-free-lunch result relate to the need for prior knowledge? let us
consider an erm predictor over the hypothesis class h of all the functions f from
x to {0, 1}. this class represents lack of prior knowledge: every possible function
from the domain to the label set is considered a good candidate. according to the
no-free-lunch theorem, any algorithm that chooses its output from hypotheses
in h, and in particular the erm predictor, will fail on some learning task.
therefore, this class is not pac learnable, as formalized in the following corollary:
corollary 5.2 let x be an in   nite domain set and let h be the set of all
functions from x to {0, 1}. then, h is not pac learnable.

64

the bias-complexity tradeo   

proof assume, by way of contradiction, that the class is learnable. choose
some   < 1/8 and    < 1/7. by the de   nition of pac learnability, there must
be some learning algorithm a and an integer m = m( ,   ), such that for any
data-generating distribution over x   {0, 1}, if for some function f : x     {0, 1},
ld(f ) = 0, then with id203 greater than 1        when a is applied to
samples s of size m, generated i.i.d. by d, ld(a(s))      . however, applying
the no-free-lunch theorem, since |x| > 2m, for every learning algorithm (and
in particular for the algorithm a), there exists a distribution d such that with
id203 greater than 1/7 >   , ld(a(s)) > 1/8 >  , which leads to the
desired contradiction.

how can we prevent such failures? we can escape the hazards foreseen by the
no-free-lunch theorem by using our prior knowledge about a speci   c learning
task, to avoid the distributions that will cause us to fail when learning that task.
such prior knowledge can be expressed by restricting our hypothesis class.

but how should we choose a good hypothesis class? on the one hand, we want
to believe that this class includes the hypothesis that has no error at all (in the
pac setting), or at least that the smallest error achievable by a hypothesis from
this class is indeed rather small (in the agnostic setting). on the other hand,
we have just seen that we cannot simply choose the richest class     the class of
all functions over the given domain. this tradeo    is discussed in the following
section.

5.2

error decomposition

to answer this question we decompose the error of an ermh predictor into two
components as follows. let hs be an ermh hypothesis. then, we can write

ld(hs) =  app +  est where :  app = min

h   h ld(h),

 est = ld(hs)     app. (5.7)

    the approximation error     the minimum risk achievable by a predictor
in the hypothesis class. this term measures how much risk we have because
we restrict ourselves to a speci   c class, namely, how much inductive bias we
have. the approximation error does not depend on the sample size and is
determined by the hypothesis class chosen. enlarging the hypothesis class
can decrease the approximation error.

under the realizability assumption, the approximation error is zero. in

the agnostic case, however, the approximation error can be large.1

1 in fact, it always includes the error of the bayes optimal predictor (see chapter 3), the

minimal yet inevitable error, because of the possible nondeterminism of the world in this
model. sometimes in the literature the term approximation error refers not to
minh   h ld(h), but rather to the excess error over that of the bayes optimal predictor,
namely, minh   h ld(h)      bayes.

5.3 summary

65

    the estimation error     the di   erence between the approximation error
and the error achieved by the erm predictor. the estimation error results
because the empirical risk (i.e., training error) is only an estimate of the
true risk, and so the predictor minimizing the empirical risk is only an
estimate of the predictor minimizing the true risk.

the quality of this estimation depends on the training set size and
on the size, or complexity, of the hypothesis class. as we have shown, for
a    nite hypothesis class,  est increases (logarithmically) with |h| and de-
creases with m. we can think of the size of h as a measure of its complexity.
in future chapters we will de   ne other complexity measures of hypothesis
classes.

since our goal is to minimize the total risk, we face a tradeo   , called the bias-
complexity tradeo   . on one hand, choosing h to be a very rich class decreases the
approximation error but at the same time might increase the estimation error,
as a rich h might lead to over   tting. on the other hand, choosing h to be a
very small set reduces the estimation error but might increase the approximation
error or, in other words, might lead to under   tting. of course, a great choice for
h is the class that contains only one classi   er     the bayes optimal classi   er. but
the bayes optimal classi   er depends on the underlying distribution d, which we
do not know (indeed, learning would have been unnecessary had we known d).
learning theory studies how rich we can make h while still maintaining rea-
sonable estimation error. in many cases, empirical research focuses on designing
good hypothesis classes for a certain domain. here,    good    means classes for
which the approximation error would not be excessively high. the idea is that
although we are not experts and do not know how to construct the optimal clas-
si   er, we still have some prior knowledge of the speci   c problem at hand, which
enables us to design hypothesis classes for which both the approximation error
and the estimation error are not too large. getting back to our papayas example,
we do not know how exactly the color and hardness of a papaya predict its taste,
but we do know that papaya is a fruit and on the basis of previous experience
with other fruit we conjecture that a rectangle in the color-hardness space may
be a good predictor.

5.3

summary

the no-free-lunch theorem states that there is no universal learner. every
learner has to be speci   ed to some task, and use some prior knowledge about
that task, in order to succeed. so far we have modeled our prior knowledge by
restricting our output hypothesis to be a member of a chosen hypothesis class.
when choosing this hypothesis class, we face a tradeo   , between a larger, or
more complex, class that is more likely to have a small approximation error,
and a more restricted class that would guarantee that the estimation error will

66

the bias-complexity tradeo   

be small. in the next chapter we will study in more detail the behavior of the
estimation error. in chapter 7 we will discuss alternative ways to express prior
knowledge.

5.4

bibliographic remarks

(wolpert & macready 1997) proved several no-free-lunch theorems for optimiza-
tion, but these are rather di   erent from the theorem we prove here. the theorem
we prove here is closely related to lower bounds in vc theory, as we will study
in the next chapter.

5.5

exercises
1. prove that equation (5.2) su   ces for showing that p[ld(a(s))     1/8]     1/7.
hint: let    be a random variable that receives values in [0, 1] and whose
expectation satis   es e[  ]     1/4. use lemma b.1 to show that p[       1/8]    
1/7.

2. assume you are asked to design a learning algorithm to predict whether pa-
tients are going to su   er a heart attack. relevant patient features the al-
gorithm may have access to include blood pressure (bp), body-mass index
(bmi), age (a), level of physical activity (p), and income (i).

your choice.

you have to choose between two algorithms; the    rst picks an axis aligned
rectangle in the two dimensional space spanned by the features bp and bmi
and the other picks an axis aligned rectangle in the    ve dimensional space
spanned by all the preceding features.
1. explain the pros and cons of each choice.
2. explain how the number of available labeled training samples will a   ect
3. prove that if |x|     km for a positive integer k     2, then we can replace
2     1
the lower bound of 1/4 in the no-free-lunch theorem with k   1
2k .
namely, let a be a learning algorithm for the task of binary classi   cation. let
m be any number smaller than |x|/k, representing a training set size. then,
there exists a distribution d over x    {0, 1} such that:
    there exists a function f : x     {0, 1} with ld(f ) = 0.
    es   dm[ld(a(s))]     1

2k = 1

2     1
2k .

6

the vc-dimension

in the previous chapter, we decomposed the error of the ermh rule into ap-
proximation error and estimation error. the approximation error depends on
the    t of our prior knowledge (as re   ected by the choice of the hypothesis class
h) to the underlying unknown distribution. in contrast, the de   nition of pac
learnability requires that the estimation error would be bounded uniformly over
all distributions.

our current goal is to    gure out which classes h are pac learnable, and to
characterize exactly the sample complexity of learning a given hypothesis class.
so far we have seen that    nite classes are learnable, but that the class of all
functions (over an in   nite size domain) is not. what makes one class learnable
and the other unlearnable? can in   nite-size classes be learnable, and, if so, what
determines their sample complexity?

we begin the chapter by showing that in   nite classes can indeed be learn-
able, and thus,    niteness of the hypothesis class is not a necessary condition for
learnability. we then present a remarkably crisp characterization of the family
of learnable classes in the setup of binary valued classi   cation with the zero-one
loss. this characterization was    rst discovered by vladimir vapnik and alexey
chervonenkis in 1970 and relies on a combinatorial notion called the vapnik-
chervonenkis dimension (vc-dimension). we formally de   ne the vc-dimension,
provide several examples, and then state the fundamental theorem of statistical
learning theory, which integrates the concepts of learnability, vc-dimension, the
erm rule, and uniform convergence.

6.1

in   nite-size classes can be learnable

in chapter 4 we saw that    nite classes are learnable, and in fact the sample
complexity of a hypothesis class is upper bounded by the log of its size. to show
that the size of the hypothesis class is not the right characterization of its sample
complexity, we    rst present a simple example of an in   nite-size hypothesis class
that is learnable.
example 6.1 let h be the set of threshold functions over the real line, namely,
h = {ha : a     r}, where ha : r     {0, 1} is a function such that ha(x) = 1[x<a].
to remind the reader, 1[x<a] is 1 if x < a and 0 otherwise. clearly, h is of in   nite

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

68

the vc-dimension

size. nevertheless, the following lemma shows that h is learnable in the pac
model using the erm algorithm.
lemma 6.1 let h be the class of thresholds as de   ned earlier. then, h is
pac learnable, using the erm rule, with sample complexity of mh( ,   )    
(cid:100)log(2/  )/ (cid:101).
proof let a(cid:63) be a threshold such that the hypothesis h(cid:63)(x) = 1[x<a(cid:63)] achieves
ld(h(cid:63)) = 0. let dx be the marginal distribution over the domain x and let
a0 < a(cid:63) < a1 be such that

p

x   dx

[x     (a0, a(cid:63))] = p
x   dx

[x     (a(cid:63), a1)] =  .

  mass

  mass

a0

a(cid:63)

a1

(if dx(      , a(cid:63))       we set a0 =        and similarly for a1). given a training set
s, let b0 = max{x : (x, 1)     s} and b1 = min{x : (x, 0)     s} (if no example in s
is positive we set b0 =        and if no example in s is negative we set b1 =    ).
let bs be a threshold corresponding to an erm hypothesis, hs, which implies
that bs     (b0, b1). therefore, a su   cient condition for ld(hs)       is that both
b0     a0 and b1     a1. in other words,

p

s   dm

[ld(hs) >  ]     p
s   dm

[b0 < a0     b1 > a1],

and using the union bound we can bound the preceding by

p

s   dm

[ld(hs) >  ]     p
s   dm

[b0 < a0] + p

s   dm

[b1 > a1].

(6.1)

the event b0 < a0 happens if and only if all examples in s are not in the interval
(a0, a   ), whose id203 mass is de   ned to be  , namely,

p

s   dm

[b0 < a0] = p

s   dm

[   (x, y)     s, x (cid:54)    (a0, a(cid:63))] = (1      )m     e     m.

since we assume m > log(2/  )/  it follows that the equation is at most   /2.
in the same way it is easy to see that ps   dm [b1 > a1]       /2. combining with
equation (6.1) we conclude our proof.

6.2

the vc-dimension
we see, therefore, that while    niteness of h is a su   cient condition for learn-
ability, it is not a necessary condition. as we will show, a property called the
vc-dimension of a hypothesis class gives the correct characterization of its learn-
ability. to motivate the de   nition of the vc-dimension, let us recall the no-free-
lunch theorem (theorem 5.1) and its proof. there, we have shown that without

6.2 the vc-dimension

69

restricting the hypothesis class, for any learning algorithm, an adversary can
construct a distribution for which the learning algorithm will perform poorly,
while there is another learning algorithm that will succeed on the same distri-
bution. to do so, the adversary used a    nite set c     x and considered a family
of distributions that are concentrated on elements of c. each distribution was
derived from a    true    target function from c to {0, 1}. to make any algorithm
fail, the adversary used the power of choosing a target function from the set of
all possible functions from c to {0, 1}.

when considering pac learnability of a hypothesis class h, the adversary
is restricted to constructing distributions for which some hypothesis h     h
achieves a zero risk. since we are considering distributions that are concentrated
on elements of c, we should study how h behaves on c, which leads to the
following de   nition.
definition 6.2 (restriction of h to c) let h be a class of functions from x
to {0, 1} and let c = {c1, . . . , cm}     x . the restriction of h to c is the set of
functions from c to {0, 1} that can be derived from h. that is,

hc = {(h(c1), . . . , h(cm)) : h     h},

where we represent each function from c to {0, 1} as a vector in {0, 1}|c|.

if the restriction of h to c is the set of all functions from c to {0, 1}, then

we say that h shatters the set c. formally:
definition 6.3 (shattering) a hypothesis class h shatters a    nite set c     x
if the restriction of h to c is the set of all functions from c to {0, 1}. that is,
|hc| = 2|c|.
example 6.2 let h be the class of threshold functions over r. take a set
c = {c1}. now, if we take a = c1 + 1, then we have ha(c1) = 1, and if we take
a = c1     1, then we have ha(c1) = 0. therefore, hc is the set of all functions
from c to {0, 1}, and h shatters c. now take a set c = {c1, c2}, where c1     c2.
no h     h can account for the labeling (0, 1), because any threshold that assigns
the label 0 to c1 must assign the label 0 to c2 as well. therefore not all functions
from c to {0, 1} are included in hc; hence c is not shattered by h.

getting back to the construction of an adversarial distribution as in the proof
of the no-free-lunch theorem (theorem 5.1), we see that whenever some set c
is shattered by h, the adversary is not restricted by h, as they can construct
a distribution over c based on any target function from c to {0, 1}, while still
maintaining the realizability assumption. this immediately yields:
corollary 6.4 let h be a hypothesis class of functions from x to {0, 1}. let
m be a training set size. assume that there exists a set c     x of size 2m that is
shattered by h. then, for any learning algorithm, a, there exist a distribution d
over x    {0, 1} and a predictor h     h such that ld(h) = 0 but with id203
of at least 1/7 over the choice of s     dm we have that ld(a(s))     1/8.

70

the vc-dimension

corollary 6.4 tells us that if h shatters some set c of size 2m then we cannot
learn h using m examples. intuitively, if a set c is shattered by h, and we
receive a sample containing half the instances of c, the labels of these instances
give us no information about the labels of the rest of the instances in c     every
possible labeling of the rest of the instances can be explained by some hypothesis
in h. philosophically,

if someone can explain every phenomenon, his explanations are worthless.

this leads us directly to the de   nition of the vc dimension.

definition 6.5 (vc-dimension) the vc-dimension of a hypothesis class h,
denoted vcdim(h), is the maximal size of a set c     x that can be shattered
by h. if h can shatter sets of arbitrarily large size we say that h has in   nite
vc-dimension.

a direct consequence of corollary 6.4 is therefore:

theorem 6.6 let h be a class of in   nite vc-dimension. then, h is not pac
learnable.
proof since h has an in   nite vc-dimension, for any training set size m, there
exists a shattered set of size 2m, and the claim follows by corollary 6.4.

we shall see later in this chapter that the converse is also true: a    nite vc-
dimension guarantees learnability. hence, the vc-dimension characterizes pac
learnability. but before delving into more theory, we    rst show several examples.

6.3

examples

in this section we calculate the vc-dimension of several hypothesis classes. to
show that vcdim(h) = d we need to show that

1. there exists a set c of size d that is shattered by h.
2. every set c of size d + 1 is not shattered by h.

6.3.1

threshold functions
let h be the class of threshold functions over r. recall example 6.2, where
we have shown that for an arbitrary set c = {c1}, h shatters c; therefore
vcdim(h)     1. we have also shown that for an arbitrary set c = {c1, c2} where
c1     c2, h does not shatter c. we therefore conclude that vcdim(h) = 1.

6.3 examples

71

6.3.2

intervals
let h be the class of intervals over r, namely, h = {ha,b : a, b     r, a < b},
where ha,b : r     {0, 1} is a function such that ha,b(x) = 1[x   (a,b)]. take the set
c = {1, 2}. then, h shatters c (make sure you understand why) and therefore
vcdim(h)     2. now take an arbitrary set c = {c1, c2, c3} and assume without
loss of generality that c1     c2     c3. then, the labeling (1, 0, 1) cannot be obtained
by an interval and therefore h does not shatter c. we therefore conclude that
vcdim(h) = 2.

6.3.3

axis aligned rectangles
let h be the class of axis aligned rectangles, formally:

h = {h(a1,a2,b1,b2) : a1     a2 and b1     b2}

where

h(a1,a2,b1,b2)(x1, x2) =

(cid:40)

if a1     x1     a2 and b1     x2     b2
otherwise

1

0

(6.2)

we shall show in the following that vcdim(h) = 4. to prove this we need
to    nd a set of 4 points that are shattered by h, and show that no set of 5
points can be shattered by h. finding a set of 4 points that are shattered is
easy (see figure 6.1). now, consider any set c     r2 of 5 points. in c, take a
leftmost point (whose    rst coordinate is the smallest in c), a rightmost point
(   rst coordinate is the largest), a lowest point (second coordinate is the smallest),
and a highest point (second coordinate is the largest). without loss of generality,
denote c = {c1, . . . , c5} and let c5 be the point that was not selected. now,
de   ne the labeling (1, 1, 1, 1, 0). it is impossible to obtain this labeling by an
axis aligned rectangle. indeed, such a rectangle must contain c1, . . . , c4; but in
this case the rectangle contains c5 as well, because its coordinates are within
the intervals de   ned by the selected points. so, c is not shattered by h, and
therefore vcdim(h) = 4.

c4

c1

c5

c3

c2

figure 6.1 left: 4 points that are shattered by axis aligned rectangles. right: any axis
aligned rectangle cannot label c5 by 0 and the rest of the points by 1.

72

the vc-dimension

6.3.4

finite classes
let h be a    nite class. then, clearly, for any set c we have |hc|     |h| and thus c
cannot be shattered if |h| < 2|c|. this implies that vcdim(h)     log2(|h|). this
shows that the pac learnability of    nite classes follows from the more general
statement of pac learnability of classes with    nite vc-dimension, which we shall
see in the next section. note, however, that the vc-dimension of a    nite class
h can be signi   cantly smaller than log2(|h|). for example, let x = {1, . . . , k},
for some integer k, and consider the class of threshold functions (as de   ned in
example 6.2). then, |h| = k but vcdim(h) = 1. since k can be arbitrarily
large, the gap between log2(|h|) and vcdim(h) can be arbitrarily large.

6.3.5

vc-dimension and the number of parameters

in the previous examples, the vc-dimension happened to equal the number of
parameters de   ning the hypothesis class. while this is often the case, it is not
always true. consider, for example, the domain x = r, and the hypothesis class
h = {h   :        r} where h   : x     {0, 1} is de   ned by h  (x) = (cid:100)0.5 sin(  x)(cid:101). it
is possible to prove that vcdim(h) =    , namely, for every d, one can    nd d
points that are shattered by h (see exercise 8).

6.4

the fundamental theorem of pac learning

we have already shown that a class of in   nite vc-dimension is not learnable. the
converse statement is also true, leading to the fundamental theorem of statistical
learning theory:
theorem 6.7 (the fundamental theorem of statistical learning) let h be a
hypothesis class of functions from a domain x to {0, 1} and let the id168
be the 0     1 loss. then, the following are equivalent:
1. h has the uniform convergence property.
2. any erm rule is a successful agnostic pac learner for h.
3. h is agnostic pac learnable.
4. h is pac learnable.
5. any erm rule is a successful pac learner for h.
6. h has a    nite vc-dimension.

the proof of the theorem is given in the next section.
not only does the vc-dimension characterize pac learnability; it even deter-

mines the sample complexity.

theorem 6.8 (the fundamental theorem of statistical learning     quantita-
tive version) let h be a hypothesis class of functions from a domain x to {0, 1}
and let the id168 be the 0     1 loss. assume that vcdim(h) = d <    .
then, there are absolute constants c1, c2 such that:

6.5 proof of theorem 6.7

73

1. h has the uniform convergence property with sample complexity

c1

c1

 2

 2

d + log(1/  )

    much ( ,   )     c2

d + log(1/  )

 2

2. h is agnostic pac learnable with sample complexity

d + log(1/  )

    mh( ,   )     c2

d + log(1/  )

 2

3. h is pac learnable with sample complexity
    mh( ,   )     c2

d + log(1/  )

c1

 

d log(1/ ) + log(1/  )

 

6.5

the proof of this theorem is given in chapter 28.

remark 6.3 we stated the fundamental theorem for binary classi   cation tasks.
a similar result holds for some other learning problems such as regression with
the absolute loss or the squared loss. however, the theorem does not hold for
all learning tasks. in particular, learnability is sometimes possible even though
the uniform convergence property does not hold (we will see an example in
chapter 13, exercise 2). furthermore, in some situations, the erm rule fails
but learnability is possible with other learning rules.

proof of theorem 6.7
we have already seen that 1     2 in chapter 4. the implications 2     3 and
3     4 are trivial and so is 2     5. the implications 4     6 and 5     6 follow from
the no-free-lunch theorem. the di   cult part is to show that 6     1. the proof
is based on two main claims:
    if vcdim(h) = d, then even though h might be in   nite, when restricting it
to a    nite set c     x , its    e   ective    size, |hc|, is only o(|c|d). that is,
the size of hc grows polynomially rather than exponentially with |c|. this
claim is often referred to as sauer   s lemma, but it has also been stated and
proved independently by shelah and by perles. the formal statement is
given in section 6.5.1 later.
    in section 4 we have shown that    nite hypothesis classes enjoy the uniform
convergence property. in section 6.5.2 later we generalize this result and
show that uniform convergence holds whenever the hypothesis class has a
   small e   ective size.    by    small e   ective size    we mean classes for which
|hc| grows polynomially with |c|.

6.5.1

sauer   s lemma and the growth function
we de   ned the notion of shattering, by considering the restriction of h to a    nite
set of instances. the growth function measures the maximal    e   ective    size of
h on a set of m examples. formally:

74

the vc-dimension

definition 6.9 (growth function) let h be a hypothesis class. then the
growth function of h, denoted   h : n     n, is de   ned as

(cid:12)(cid:12)hc

(cid:12)(cid:12) .

  h(m) =

max

c   x :|c|=m

in words,   h (m) is the number of di   erent functions from a set c of size m to
{0, 1} that can be obtained by restricting h to c.

obviously, if vcdim(h) = d then for any m     d we have   h(m) = 2m. in
such cases, h induces all possible functions from c to {0, 1}. the following beau-
tiful lemma, proposed independently by sauer, shelah, and perles, shows that
when m becomes larger than the vc-dimension, the growth function increases
polynomially rather than exponentially with m.
lemma 6.10 (sauer-shelah-perles) let h be a hypothesis class with vcdim(h)    
  h(m)     (em/d)d.

d <    . then, for all m,   h(m)    (cid:80)d

(cid:1). in particular, if m > d + 1 then

(cid:0)m

i=0

i

proof of sauer   s lemma *
to prove the lemma it su   ces to prove the following stronger claim: for any
c = {c1, . . . , cm} we have

   h,

|hc|     |{b     c : h shatters b}|.

(6.3)

the reason why equation (6.3) is su   cient to prove the lemma is that if vcdim(h)    
d then no set whose size is larger than d is shattered by h and therefore

|{b     c : h shatters b}|     d(cid:88)

(cid:18)m

(cid:19)

.

i

i=0

when m > d + 1 the right-hand side of the preceding is at most (em/d)d (see
lemma a.5 in appendix a).

we are left with proving equation (6.3) and we do it using an inductive argu-
ment. for m = 1, no matter what h is, either both sides of equation (6.3) equal
1 or both sides equal 2 (the empty set is always considered to be shattered by
h). assume equation (6.3) holds for sets of size k < m and let us prove it for
sets of size m. fix h and c = {c1, . . . , cm}. denote c(cid:48) = {c2, . . . , cm} and in
addition, de   ne the following two sets:

y0 = {(y2, . . . , ym) : (0, y2, . . . , ym)     hc     (1, y2, . . . , ym)     hc},

and

y1 = {(y2, . . . , ym) : (0, y2, . . . , ym)     hc     (1, y2, . . . , ym)     hc}.

it is easy to verify that |hc| = |y0| + |y1|. additionally, since y0 = hc(cid:48), using
the induction assumption (applied on h and c(cid:48)) we have that
|y0| = |hc(cid:48)|     |{b     c(cid:48) : h shatters b}| = |{b     c : c1 (cid:54)    b     h shatters b}|.

6.5 proof of theorem 6.7

75

next, de   ne h(cid:48)     h to be

h(cid:48) = {h     h :    h(cid:48)     h s.t. (1     h(cid:48)(c1), h(cid:48)(c2), . . . , h(cid:48)(cm))

= (h(c1), h(c2), . . . , h(cm)},

namely, h(cid:48) contains pairs of hypotheses that agree on c(cid:48) and di   er on c1. using
this de   nition, it is clear that if h(cid:48) shatters a set b     c(cid:48) then it also shatters
the set b    {c1} and vice versa. combining this with the fact that y1 = h(cid:48)
c(cid:48) and
using the inductive assumption (now applied on h(cid:48) and c(cid:48)) we obtain that
|y1| = |h(cid:48)

c(cid:48)|     |{b     c(cid:48) : h(cid:48) shatters b}| = |{b     c(cid:48) : h(cid:48) shatters b     {c1}}|
= |{b     c : c1     b     h(cid:48) shatters b}|     |{b     c : c1     b     h shatters b}|.

overall, we have shown that
|hc| = |y0| + |y1|

    |{b     c : c1 (cid:54)    b     h shatters b}| + |{b     c : c1     b     h shatters b}|
= |{b     c : h shatters b}|,

which concludes our proof.

6.5.2

uniform convergence for classes of small e   ective size
in this section we prove that if h has small e   ective size then it enjoys the
uniform convergence property. formally,
theorem 6.11 let h be a class and let   h be its growth function. then, for
every d and every        (0, 1), with id203 of at least 1        over the choice of
s     dm we have

|ld(h)     ls(h)|     4 +(cid:112)log(  h(2m))

   

.

  

2m

before proving the theorem, let us    rst conclude the proof of theorem 6.7.

proof of theorem 6.7 it su   ces to prove that if the vc-dimension is    nite then
the uniform convergence property holds. we will prove that

much ( ,   )     4

16d
(   )2 log

+

16 d log(2e/d)

(   )2

.

from sauer   s lemma we have that for m > d,   h(2m)     (2em/d)d. combining
this with theorem 6.11 we obtain that with id203 of at least 1       ,

|ls(h)     ld(h)|     4 +(cid:112)d log(2em/d)
for simplicity assume that(cid:112)d log(2em/d)     4; hence,

   

2m

  

(cid:18) 16d

(cid:19)

(   )2

(cid:114)

|ls(h)     ld(h)|     1
  

2d log(2em/d)

m

.

.

76

the vc-dimension

to ensure that the preceding is at most   we need that

m     2d log(m)

(   )2 +

2 d log(2e/d)

(   )2

.

standard algebraic manipulations (see lemma a.2 in appendix a) show that a
su   cient condition for the preceding to hold is that

(cid:18) 2d

(cid:19)

(   )2

m     4

2d
(   )2 log

+

4 d log(2e/d)

(   )2

.

remark 6.4 the upper bound on much we derived in the proof theorem 6.7
is not the tightest possible. a tighter analysis that yields the bounds given in
theorem 6.8 can be found in chapter 28.

proof of theorem 6.11 *
we will start by showing that

(cid:20)

(cid:21)

    4 +(cid:112)log(  h(2m))

   

e

|ld(h)     ls(h)|

.

2m

s   dm

sup
h   h

(6.4)
since the random variable suph   h |ld(h)     ls(h)| is nonnegative, the proof of
the theorem follows directly from the preceding using markov   s inequality (see
section b.1).
h     h, we can rewrite ld(h) = es(cid:48)   dm [ls(cid:48)(h)], where s(cid:48) = z(cid:48)
additional i.i.d. sample. therefore,
|ld(h)     ls(h)|

to bound the left-hand side of equation (6.4) we    rst note that for every
m is an

ls(cid:48)(h)     ls(h)

1, . . . , z(cid:48)

= e

(cid:20)

(cid:21)

e

.

(cid:12)(cid:12)(cid:12) e

s(cid:48)   dm

(cid:12)(cid:12)(cid:12)(cid:21)

s   dm

sup
h   h

sup
h   h

a generalization of the triangle inequality yields

[ls(cid:48)(h)     ls(h)]

|ls(cid:48)(h)     ls(h)|,

(cid:12)(cid:12)(cid:12) e

s(cid:48)   dm

s   dm

(cid:20)
(cid:12)(cid:12)(cid:12)     e

s(cid:48)   dm

and the fact that supermum of expectation is smaller than expectation of supre-
mum yields

e

s(cid:48)   dm

sup
h   h

|ls(cid:48)(h)     ls(h)|     e

s(cid:48)   dm

sup
h   h

|ls(cid:48)(h)     ls(h)|.

formally, the previous two inequalities follow from jensen   s inequality. combin-
ing all we obtain

(cid:20)

(cid:21)

e

s   dm

sup
h   h

|ld(h)     ls(h)|

(cid:21)

|ls(cid:48)(h)     ls(h)|

   

=

e

s,s(cid:48)   dm

e

s,s(cid:48)   dm

sup
h   h

1
m

sup
h   h

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)

i=1

((cid:96)(h, z(cid:48)

i)     (cid:96)(h, zi))

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)

.

(6.5)

(cid:20)
(cid:34)

6.5 proof of theorem 6.7

77

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:35)
(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

,

.

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:34)

(cid:34)
(cid:34)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)

i=1

1
m

1
m

i=1

sup
h   h

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)

i=1

the expectation on the right-hand side is over a choice of two i.i.d. samples
s = z1, . . . , zm and s(cid:48) = z(cid:48)
m. since all of these 2m vectors are chosen
i.i.d., nothing will change if we replace the name of the random vector zi with the
i)    (cid:96)(h, zi))
name of the random vector z(cid:48)
i)     (cid:96)(h, zi)). it follows that for
in equation (6.5) we will have the term    ((cid:96)(h, z(cid:48)
every        {  1}m we have that equation (6.5) equals

1, . . . , z(cid:48)
i. if we do it, instead of the term ((cid:96)(h, z(cid:48)

e

s,s(cid:48)   dm

1
m

sup
h   h

  i((cid:96)(h, z(cid:48)

i)     (cid:96)(h, zi))

since this holds for every        {  1}m, it also holds if we sample each component
of    uniformly at random from the uniform distribution over {  1}, denoted u  .
hence, equation (6.5) also equals

e

     u m  

e

s,s(cid:48)   dm

sup
h   h

  i((cid:96)(h, z(cid:48)

i)     (cid:96)(h, zi))

and by the linearity of expectation it also equals

e

s,s(cid:48)   dm

e

     u m  

  i((cid:96)(h, z(cid:48)

i)     (cid:96)(h, zi))

next,    x s and s(cid:48), and let c be the instances appearing in s and s(cid:48). then, we
can take the supremum only over h     hc. therefore,
i)     (cid:96)(h, zi))

  i((cid:96)(h, z(cid:48)

(cid:34)

(cid:35)

e

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

     u m  

sup
h   h

1
m

(cid:34)

     u m  

max
h   hc

= e

1
m
fix some h     hc and denote   h = 1
i)    (cid:96)(h, zi)). since e[  h] = 0
and   h is an average of independent variables, each of which takes values in
[   1, 1], we have by hoe   ding   s inequality that for every    > 0,

i)     (cid:96)(h, zi))

m

.

p[|  h| >   ]     2 exp(cid:0)   2 m   2(cid:1) .

applying the union bound over h     hc, we obtain that for any    > 0,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) m(cid:88)
(cid:80)m
i=1   i((cid:96)(h, z(cid:48)

  i((cid:96)(h, z(cid:48)

i=1

finally, lemma a.4 in appendix a tells us that the preceding implies

(cid:20)

p

(cid:20)

|  h| >   

max
h   hc

(cid:20)

e

|  h|

max
h   hc

(cid:21)
(cid:21)

    2|hc| exp(cid:0)   2 m   2(cid:1) .
    4 +(cid:112)log(|hc|)
(cid:21)

    4 +(cid:112)log(  h(2m))

   

   

2m

.

.

2m

e

s   dm

sup
h   h

|ld(h)     ls(h)|

combining all with the de   nition of   h, we have shown that

78

the vc-dimension

6.6

summary

the fundamental theorem of learning theory characterizes pac learnability of
classes of binary classi   ers using vc-dimension. the vc-dimension of a class
is a combinatorial property that denotes the maximal sample size that can be
shattered by the class. the fundamental theorem states that a class is pac learn-
able if and only if its vc-dimension is    nite and speci   es the sample complexity
required for pac learning. the theorem also shows that if a problem is at all
learnable, then uniform convergence holds and therefore the problem is learnable
using the erm rule.

6.7

bibliographic remarks

the de   nition of vc-dimension and its relation to learnability and to uniform
convergence is due to the seminal work of vapnik & chervonenkis (1971). the
relation to the de   nition of pac learnability is due to blumer, ehrenfeucht,
haussler & warmuth (1989).

several generalizations of the vc-dimension have been proposed. for exam-
ple, the fat-shattering dimension characterizes learnability of some regression
problems (kearns, schapire & sellie 1994, alon, ben-david, cesa-bianchi &
haussler 1997, bartlett, long & williamson 1994, anthony & bartlet 1999), and
the natarajan dimension characterizes learnability of some multiclass learning
problems (natarajan 1989). however, in general, there is no equivalence between
learnability and uniform convergence. see (shalev-shwartz, shamir, srebro &
sridharan 2010, daniely, sabato, ben-david & shalev-shwartz 2011).

sauer   s lemma has been proved by sauer in response to a problem of erdos
(sauer 1972). shelah (with perles) proved it as a useful lemma for shelah   s theory
of stable models (shelah 1972). gil kalai tells1 us that at some later time, benjy
weiss asked perles about such a result in the context of ergodic theory, and
perles, who forgot that he had proved it once, proved it again. vapnik and
chervonenkis proved the lemma in the context of statistical learning theory.

6.8

exercises

hypothesis classes if h(cid:48)     h then vcdim(h(cid:48))     vcdim(h).

1. show the following monotonicity property of vc-dimension: for every two
2. given some    nite domain set, x , and a number k     |x|,    gure out the vc-
=k = {h     {0, 1}x : |{x : h(x) = 1}| = k}. that is, the set of all functions
that assign the value 1 to exactly k elements of x .

dimension of each of the following classes (and prove your claims):
1. hx

1 http://gilkalai.wordpress.com/2008/09/28/

extremal-combinatorics-iii-some-basic-theorems

6.8 exercises

79

2. hat   most   k = {h     {0, 1}x : |{x : h(x) = 1}|     k or |{x : h(x) = 0}|     k}.
3. let x be the boolean hypercube {0, 1}n. for a set i     {1, 2, . . . , n} we de   ne
a parity function hi as follows. on a binary vector x = (x1, x2, . . . , xn)    
{0, 1}n,

(cid:33)

(cid:32)(cid:88)

i   i

hi (x) =

xi

mod 2 .

(that is, hi computes parity of bits in i.) what is the vc-dimension of the
class of all such parity functions, hn-parity = {hi

: i     {1, 2, . . . , n}}?

4. we proved sauer   s lemma by proving that for every class h of    nite vc-

dimension d, and every subset a of the domain,

|ha|     |{b     a : h shatters b}|     d(cid:88)

i=0

(cid:18)|a|

(cid:19)

.

i

show that there are cases in which the previous two inequalities are strict
(namely, the     can be replaced by <) and cases in which they can be replaced
by equalities. demonstrate all four combinations of = and <.
5. vc-dimension of axis aligned rectangles in rd: let hd

axis aligned rectangles in rd. we have already seen that vcdim(h2
prove that in general, vcdim(hd

rec be the class of
rec) = 4.

rec) = 2d.

6. vc-dimension of boolean conjunctions: let hd

con).

con be the class of boolean
conjunctions over the variables x1, . . . , xd (d     2). we already know that this
class is    nite and thus (agnostic) pac learnable. in this question we calculate
vcdim(hd
1. show that |hd
2. conclude that vcdim(h)     d log 3.
3. show that hd
4. (**) show that vcdim(hd

con shatters the set of unit vectors {ei : i     d}.

con|     3d + 1.

hint: assume by contradiction that there exists a set c = {c1, . . . , cd+1}
that is shattered by hd
con that
satisfy

con. let h1, . . . , hd+1 be hypotheses in hd

con)     d.

(cid:40)

   i, j     [d + 1], hi(cj) =

0

i = j

1

otherwise

for each i     [d + 1], hi (or more accurately, the conjunction that corre-
sponds to hi) contains some literal (cid:96)i which is false on ci and true on cj
for each j (cid:54)= i. use the pigeonhole principle to show that there must be a
pair i < j     d + 1 such that (cid:96)i and (cid:96)j use the same xk and use that fact
to derive a contradiction to the requirements from the conjunctions hi, hj.
mcon of monotone boolean conjunctions over {0, 1}d.
monotonicity here means that the conjunctions do not contain negations.

5. consider the class hd

80

the vc-dimension

as in hd
pothesis. we augment hd
that vcdim(hd
mcon) = d.

con, the empty conjunction is interpreted as the all-positive hy-
mcon with the all-negative hypothesis h   . show
7. we have shown that for a    nite hypothesis class h, vcdim(h)     (cid:98)log(|h|)(cid:99).
however, this is just an upper bound. the vc-dimension of a class can be
much lower than that:
1. find an example of a class h of functions over the real interval x = [0, 1]

2. give an example of a    nite hypothesis class h over the domain x = [0, 1],

such that h is in   nite while vcdim(h) = 1.
where vcdim(h) = (cid:98)log2(|h|)(cid:99).

8. (*) it is often the case that the vc-dimension of a hypothesis class equals (or
can be bounded above by) the number of parameters one needs to set in order
to de   ne each hypothesis in the class. for instance, if h is the class of axis
aligned rectangles in rd, then vcdim(h) = 2d, which is equal to the number
of parameters used to de   ne a rectangle in rd. here is an example that shows
that this is not always the case. we will see that a hypothesis class might
be very complex and even not learnable, although it has a small number of
parameters.

consider the domain x = r, and the hypothesis class

h = {x (cid:55)    (cid:100)sin(  x)(cid:101) :        r}

(here, we take (cid:100)   1(cid:101) = 0). prove that vcdim(h) =    .
hint: there is more than one way to prove the required result. one option
is by applying the following lemma: if 0.x1x2x3 . . ., is the binary expansion of
x     (0, 1), then for any natural number m, (cid:100)sin(2m  x)(cid:101) = (1    xm), provided
that    k     m s.t. xk = 1.
h = {ha,b,s : a     b, s     {   1, 1}} where

9. let h be the class of signed intervals, that is,

(cid:40)

ha,b,s(x) =

s
   s

if x     [a, b]
if x /    [a, b]

calculate vcdim(h).

10. let h be a class of functions from x to {0, 1}.

1. prove that if vcdim(h)     d, for any d, then for some id203 distri-

bution d over x    {0, 1}, for every sample size, m,

e

s   dm

[ld(a(s))]     min

h   h ld(h) +

d     m
2d

hint: use exercise 3 in chapter 5.

2. prove that for every h that is pac learnable, vcdim(h) <    . (note that

this is the implication 3     6 in theorem 6.7.)

11. vc of union: let h1, . . . ,hr be hypothesis classes over some    xed domain

set x . let d = maxi vcdim(hi) and assume for simplicity that d     3.

6.8 exercises

81

1. prove that

vcdim (   r

i=1hi)     4d log(2d) + 2 log(r) .

hint: take a set of k examples and assume that they are shattered by
the union class. therefore, the union class can produce all 2k possible
labelings on these examples. use sauer   s lemma to show that the union
class cannot produce more than rkd labelings. therefore, 2k < rkd. now
use lemma a.2.

2. (*) prove that for r = 2 it holds that

vcdim (h1     h2)     2d + 1.

12. dudley classes: in this question we discuss an algebraic framework for
de   ning concept classes over rn and show a connection between the vc
dimension of such classes and their algebraic properties. given a function
f : rn     r we de   ne the corresponding function, pos (f )(x) = 1[f (x)>0]. for
a class f of real valued functions we de   ne a corresponding class of functions
pos (f) = {pos (f ) : f     f}. we say that a family, f, of real valued func-
tions is linearly closed if for all f, g     f and r     r, (f + rg)     f (where
addition and scalar multiplication of functions are de   ned point wise, namely,
for all x     rn, (f + rg)(x) = f (x) + rg(x)). note that if a family of functions
is linearly closed then we can view it as a vector space over the reals. for a
function g : rn     r and a family of functions f, let f +g def= {f +g : f     f}.
hypothesis classes that have a representation as pos (f + g) for some vector
space of functions f and some function g are called dudley classes.
1. show that for every g : rn     r and every vector space of functions f as

de   ned earlier, vcdim(pos (f + g)) = vcdim(pos (f)).

2. (**) for every linearly closed family of real valued functions f, the vc-
dimension of the corresponding class pos (f) equals the linear dimension
of f (as a vector space). hint: let f1, . . . , fd be a basis for the vector space
f. consider the mapping x (cid:55)    (f1(x), . . . , fd(x)) (from rn to rd). note
that this mapping induces a matching between functions over rn of the
form pos (f ) and homogeneous linear halfspaces in rd (the vc-dimension
of the class of homogeneous linear halfspaces is analyzed in chapter 9).

3. show that each of the following classes can be represented as a dudley

class:
1. the class hsn of halfspaces over rn (see chapter 9).
2. the class hhsn of all homogeneous halfspaces over rn (see chapter 9).
3. the class bd of all functions de   ned by (open) balls in rd. use the

dudley representation to    gure out the vc-dimension of this class.

4. let p d

n denote the class of functions de   ned by polynomial inequalities

of degree     d, namely,
n = {hp : p is a polynomial of degree     d in the variables x1, . . . , xn},
p d

82

the vc-dimension

where, for x = (x1. . . . , xn), hp(x) = 1[p(x)   0] (the degree of a multi-
variable polynomial is the maximal sum of variable exponents over all
of its terms. for example, the degree of p(x) = 3x3
1. use the dudley representation to    gure out the vc-dimension of the

2 + 4x3x2

7 is 5).

1x2

class p d

1     the class of all d-degree polynomials over r.

2. prove that the class of all polynomial classi   ers over r has in   nite

vc-dimension.

3. use the dudley representation to    gure out the vc-dimension of the

class p d

n (as a function of d and n).

7

nonuniform learnability

the notions of pac learnability discussed so far in the book allow the sample
sizes to depend on the accuracy and con   dence parameters, but they are uniform
with respect to the labeling rule and the underlying data distribution. conse-
quently, classes that are learnable in that respect are limited (they must have
a    nite vc-dimension, as stated by theorem 6.7). in this chapter we consider
more relaxed, weaker notions of learnability. we discuss the usefulness of such
notions and provide characterization of the concept classes that are learnable
using these de   nitions.

we begin this discussion by de   ning a notion of    nonuniform learnability    that
allows the sample size to depend on the hypothesis to which the learner is com-
pared. we then provide a characterization of nonuniform learnability and show
that nonuniform learnability is a strict relaxation of agnostic pac learnability.
we also show that a su   cient condition for nonuniform learnability is that h is
a countable union of hypothesis classes, each of which enjoys the uniform con-
vergence property. these results will be proved in section 7.2 by introducing a
new learning paradigm, which is called structural risk minimization (srm). in
section 7.3 we specify the srm paradigm for countable hypothesis classes, which
yields the minimum description length (mdl) paradigm. the mdl paradigm
gives a formal justi   cation to a philosophical principle of induction called oc-
cam   s razor. next, in section 7.4 we introduce consistency as an even weaker
notion of learnability. finally, we discuss the signi   cance and usefulness of the
di   erent notions of learnability.

7.1

nonuniform learnability

   nonuniform learnability    allows the sample size to be nonuniform with respect
to the di   erent hypotheses with which the learner is competing. we say that a
hypothesis h is ( ,   )-competitive with another hypothesis h(cid:48) if, with id203
higher than (1       ),

ld(h)     ld(h(cid:48)) +  .

in pac learnability, this notion of    competitiveness    is not very useful, as we
are looking for a hypothesis with an absolute low risk (in the realizable case) or

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

84

nonuniform learnability

with a low risk compared to the minimal risk achieved by hypotheses in our class
(in the agnostic case). therefore, the sample size depends only on the accuracy
and con   dence parameters. in nonuniform learnability, however, we allow the
sample size to be of the form mh( ,   , h); namely, it depends also on the h with
which we are competing. formally,
definition 7.1 a hypothesis class h is nonuniformly learnable if there exist a
learning algorithm, a, and a function mnulh : (0, 1)2  h     n such that, for every
 ,        (0, 1) and for every h     h, if m     mnulh ( ,   , h) then for every distribution
d, with id203 of at least 1        over the choice of s     dm, it holds that

ld(a(s))     ld(h) +  .

at this point it might be useful to recall the de   nition of agnostic pac learn-
ability (de   nition 3.3):
a hypothesis class h is agnostically pac learnable if there exist a learning algo-
rithm, a, and a function mh : (0, 1)2     n such that, for every  ,        (0, 1) and
for every distribution d, if m     mh( ,   ), then with id203 of at least 1       
over the choice of s     dm it holds that

ld(a(s))     min
note that this implies that for every h     h

h(cid:48)   h ld(h(cid:48)) +  .

ld(a(s))     ld(h) +  .

in both types of learnability, we require that the output hypothesis will be
( ,   )-competitive with every other hypothesis in the class. but the di   erence
between these two notions of learnability is the question of whether the sample
size m may depend on the hypothesis h to which the error of a(s) is compared.
note that that nonuniform learnability is a relaxation of agnostic pac learn-
ability. that is, if a class is agnostic pac learnable then it is also nonuniformly
learnable.

7.1.1

characterizing nonuniform learnability

our goal now is to characterize nonuniform learnability. in the previous chapter
we have found a crisp characterization of pac learnable classes, by showing
that a class of binary classi   ers is agnostic pac learnable if and only if its vc-
dimension is    nite. in the following theorem we    nd a di   erent characterization
for nonuniform learnable classes for the task of binary classi   cation.
theorem 7.2 a hypothesis class h of binary classi   ers is nonuniformly learn-
able if and only if it is a countable union of agnostic pac learnable hypothesis
classes.

the proof of theorem 7.2 relies on the following result of independent interest:

7.2 structural risk minimization

85

union of hypothesis classes, h =(cid:83)

theorem 7.3 let h be a hypothesis class that can be written as a countable
n   n hn, where each hn enjoys the uniform
convergence property. then, h is nonuniformly learnable.

recall that in chapter 4 we have shown that uniform convergence is su   cient
for agnostic pac learnability. theorem 7.3 generalizes this result to nonuni-
form learnability. the proof of this theorem will be given in the next section by
introducing a new learning paradigm. we now turn to proving theorem 7.2.

proof of theorem 7.2 first assume that h = (cid:83)

n   n hn where each hn is ag-
nostic pac learnable. using the fundamental theorem of statistical learning, it
follows that each hn has the uniform convergence property. therefore, using
theorem 7.3 we obtain that h is nonuniform learnable.
for the other direction, assume that h is nonuniform learnable using some
algorithm a. for every n     n, let hn = {h     h : mnulh (1/8, 1/7, h)     n}.
clearly, h =    n   nhn. in addition, using the de   nition of mnulh we know that
for any distribution d that satis   es the realizability assumption with respect to
hn, with id203 of at least 6/7 over s     dn we have that ld(a(s))     1/8.
using the fundamental theorem of statistical learning, this implies that the vc-
dimension of hn must be    nite, and therefore hn is agnostic pac learnable.

the following example shows that nonuniform learnability is a strict relax-
ation of agnostic pac learnability; namely, there are hypothesis classes that are
nonuniform learnable but are not agnostic pac learnable.

where p : r     r is a polynomial of degree n. let h =(cid:83)

example 7.1 consider a binary classi   cation problem with the instance domain
being x = r. for every n     n let hn be the class of polynomial classi   ers of
degree n; namely, hn is the set of all classi   ers of the form h(x) = sign(p(x))
n   n hn. therefore, h is
the class of all polynomial classi   ers over r. it is easy to verify that vcdim(h) =
    while vcdim(hn) = n + 1 (see exercise 12). hence, h is not pac learnable,
while on the basis of theorem 7.3, h is nonuniformly learnable.

7.2

structural risk minimization

we do so by    rst assuming that h can be written as h = (cid:83)

so far, we have encoded our prior knowledge by specifying a hypothesis class
h, which we believe includes a good predictor for the learning task at hand.
yet another way to express our prior knowledge is by specifying preferences over
hypotheses within h. in the structural risk minimization (srm) paradigm,
n   n hn and then
specifying a weight function, w : n     [0, 1], which assigns a weight to each
hypothesis class, hn, such that a higher weight re   ects a stronger preference
for the hypothesis class. in this section we discuss how to learn with such prior
knowledge. in the next section we describe a couple of important weighting
schemes, including minimum description length.

86

nonuniform learnability

concretely, let h be a hypothesis class that can be written as h =(cid:83)

n   n hn.
for example, h may be the class of all polynomial classi   ers where each hn is
the class of polynomial classi   ers of degree n (see example 7.1). assume that for
each n, the class hn enjoys the uniform convergence property (see de   nition 4.3
( ,   ). let us also de   ne
in chapter 4) with a sample complexity function muchn
the function  n : n    (0, 1)     (0, 1) by

 n(m,   ) = min{      (0, 1) : muchn

( ,   )     m}.

(7.1)

in words, we have a    xed sample size m, and we are interested in the lowest
possible upper bound on the gap between empirical and true risks achievable by
using a sample of m examples.

from the de   nitions of uniform convergence and  n, it follows that for every
m and   , with id203 of at least 1        over the choice of s     dm we have
that

   h     hn,

|ld(h)     ls(h)|      n(m,   ).

let w : n     [0, 1] be a function such that(cid:80)   

(7.2)
n=1 w(n)     1. we refer to w as
a weight function over the hypothesis classes h1,h2, . . .. such a weight function
can re   ect the importance that the learner attributes to each hypothesis class,
or some measure of the complexity of di   erent hypothesis classes. if h is a    nite
union of n hypothesis classes, one can simply assign the same weight of 1/n to
all hypothesis classes. this equal weighting corresponds to no a priori preference
to any hypothesis class. of course, if one believes (as prior knowledge) that a
certain hypothesis class is more likely to contain the correct target function,
then it should be assigned a larger weight, re   ecting this prior knowledge. when
h is a (countable) in   nite union of hypothesis classes, a uniform weighting is
not possible but many other weighting schemes may work. for example, one can
  2n2 or w(n) = 2   n. later in this chapter we will provide another
choose w(n) = 6
convenient way to de   ne weighting functions using description languages.

the srm rule follows a    bound minimization    approach. this means that
the goal of the paradigm is to    nd a hypothesis that minimizes a certain upper
bound on the true risk. the bound that the srm rule wishes to minimize is
given in the following theorem.

theorem 7.4 let w : n     [0, 1] be a function such that (cid:80)   
h be a hypothesis class that can be written as h =(cid:83)

n=1 w(n)     1. let
n   n hn, where for each n,
hn satis   es the uniform convergence property with a sample complexity function
. let  n be as de   ned in equation (7.1). then, for every        (0, 1) and
muchn
distribution d, with id203 of at least 1        over the choice of s     dm, the
following bound holds (simultaneously) for every n     n and h     hn.

|ld(h)     ls(h)|      n(m, w(n)      ).

therefore, for every        (0, 1) and distribution d, with id203 of at least

7.2 structural risk minimization

87

1        it holds that

   h     h, ld(h)     ls(h) + min
n:h   hn

 n(m, w(n)      ).

(7.3)

proof for each n de   ne   n = w(n)  . applying the assumption that uniform
convergence holds for all n with the rate given in equation (7.2), we obtain that
if we    x n in advance, then with id203 of at least 1       n over the choice of
s     dm,

|ld(h)     ls(h)|      n(m,   n).

at least 1    (cid:80)

   h     hn,

n   n = 1       (cid:80)

applying the union bound over n = 1, 2, . . ., we obtain that with id203 of
n w(n)     1       , the preceding holds for all n, which

concludes our proof.

denote

n(h) = min{n : h     hn},

(7.4)

and then equation (7.3) implies that

ld(h)     ls(h) +  n(h)(m, w(n(h))      ).

the srm paradigm searches for h that minimizes this bound, as formalized

in the following pseudocode:

prior knowledge:

structural risk minimization (srm)

n hn where hn has uniform convergence with muchn

h =(cid:83)
w : n     [0, 1] where(cid:80)
output: h     argminh   h(cid:2)ls(h) +  n(h)(m, w(n(h))      )(cid:3)

de   ne:  n as in equation (7.1) ; n(h) as in equation (7.4)
input: training set s     dm, con   dence   

n w(n)     1

unlike the erm paradigm discussed in previous chapters, we no longer just care
about the empirical risk, ls(h), but we are willing to trade some of our bias
toward low empirical risk with a bias toward classes for which  n(h)(m, w(n(h))    )
is smaller, for the sake of a smaller estimation error.

next we show that the srm paradigm can be used for nonuniform learning
of every class, which is a countable union of uniformly converging hypothesis
classes.

theorem 7.5 let h be a hypothesis class such that h = (cid:83)

each hn has the uniform convergence property with sample complexity muchn
w : n     [0, 1] be such that w(n) = 6
using the srm rule with rate

n   n hn, where
. let
n2  2 . then, h is nonuniformly learnable

(cid:16)

(cid:17)

mnulh ( ,   , h)     muchn(h)

 /2 ,

6  

(  n(h))2

.

88

nonuniform learnability

proof let a be the srm algorithm with respect to the weighting function w.
( , w(n(h))  ). using the fact that
n w(n) = 1, we can apply theorem 7.4 to get that, with id203 of at least

(cid:80)
for every h     h,  , and   , let m     muchn(h)
1        over the choice of s     dm, we have that for every h(cid:48)     h,

ld(h(cid:48))     ls(h(cid:48)) +  n(h(cid:48))(m, w(n(h(cid:48)))  ).

(cid:2)ls(h(cid:48)) +  n(h(cid:48))(m, w(n(h(cid:48)))  )(cid:3)     ls(h) +  n(h)(m, w(n(h))  ).

the preceding holds in particular for the hypothesis a(s) returned by the srm
rule. by the de   nition of srm we obtain that
ld(a(s))     min
h(cid:48)
finally, if m     muchn(h)
( /2, w(n(h))  ) then clearly  n(h)(m, w(n(h))  )      /2. in
addition, from the uniform convergence property of each hn we have that with
id203 of more than 1       ,

ls(h)     ld(h) +  /2.

combining all the preceding we obtain that ld(a(s))     ld(h) +  , which con-
cludes our proof.

note that the previous theorem also proves theorem 7.3.

remark 7.2 (no-free-lunch for nonuniform learnability) we have shown that
any countable union of classes of    nite vc-dimension is nonuniformly learnable.
it turns out that, for any in   nite domain set, x , the class of all binary valued
functions over x is not a countable union of classes of    nite vc-dimension. we
leave the proof of this claim as a (nontrivial) exercise (see exercise 5). it follows
that, in some sense, the no free lunch theorem holds for nonuniform learning
as well: namely, whenever the domain is not    nite, there exists no nonuniform
learner with respect to the class of all deterministic binary classi   ers (although
for each such classi   er there exists a trivial algorithm that learns it     erm with
respect to the hypothesis class that contains only this classi   er).

it is interesting to compare the nonuniform learnability result given in the-
orem 7.5 to the task of agnostic pac learning any speci   c hn separately. the
prior knowledge, or bias, of a nonuniform learner for h is weaker     it is searching
for a model throughout the entire class h, rather than being focused on one spe-
ci   c hn. the cost of this weakening of prior knowledge is the increase in sample
complexity needed to compete with any speci   c h     hn. for a concrete evalua-
tion of this gap, consider the task of binary classi   cation with the zero-one loss.
assume that for all n, vcdim(hn) = n. since muchn
(where
c is the contant appearing in theorem 6.8), a straightforward calculation shows
that

( ,   ) = c n+log(1/  )

 2

mnulh ( ,   , h)     muchn

( /2,   )     4c

2 log(2n)

.

 2

that is, the cost of relaxing the learner   s prior knowledge from a speci   c hn
that contains the target h to a countable union of classes depends on the log of

7.3 minimum description length and occam   s razor

89

the index of the    rst class in which h resides. that cost increases with the index
of the class, which can be interpreted as re   ecting the value of knowing a good
priority order on the hypotheses in h.

(cid:35)

(cid:35)

.

.

7.3

union of singleton classes, namely, h = (cid:83)

minimum description length and occam   s razor
let h be a countable hypothesis class. then, we can write h as a countable
n   n{hn}. by hoe   ding   s inequality
(lemma 4.5), each singleton class has the uniform convergence property with
rate muc( ,   ) = log(2/  )
. therefore, the function  n given in equation (7.1)

becomes  n(m,   ) =

2 2

(cid:113) log(2/  )
(cid:34)

argmin
hn   h

ls(h) +

2m and the srm rule becomes

(cid:114)    log(w(n)) + log(2/  )

2m

2m

equivalently, we can think of w as a function from h to [0, 1], and then the srm
rule becomes

(cid:114)    log(w(h)) + log(2/  )

(cid:34)

argmin

h   h

ls(h) +

it follows that in this case, the prior knowledge is solely determined by the weight
we assign to each hypothesis. we assign higher weights to hypotheses that we
believe are more likely to be the correct one, and in the learning algorithm we
prefer hypotheses that have higher weights.

in this section we discuss a particular convenient way to de   ne a weight func-
tion over h, which is derived from the length of descriptions given to hypotheses.
having a hypothesis class, one can wonder about how we describe, or represent,
each hypothesis in the class. we naturally    x some description language. this
can be english, or a programming language, or some set of mathematical formu-
las. in any of these languages, a description consists of    nite strings of symbols
(or characters) drawn from some    xed alphabet. we shall now formalize these
notions.

let h be the hypothesis class we wish to describe. fix some    nite set   
of symbols (or    characters   ), which we call the alphabet. for concreteness, we
let    = {0, 1}. a string is a    nite sequence of symbols from   ; for example,
   = (0, 1, 1, 1, 0) is a string of length 5. we denote by |  | the length of a string.
the set of all    nite length strings is denoted      . a description language for h
is a function d : h          , mapping each member h of h to a string d(h). d(h) is
called    the description of h,    and its length is denoted by |h|.

we shall require that description languages be pre   x-free; namely, for every
distinct h, h(cid:48), d(h) is not a pre   x of d(h(cid:48)). that is, we do not allow that any
string d(h) is exactly the    rst |h| symbols of any longer string d(h(cid:48)). pre   x-free
collections of strings enjoy the following combinatorial property:

90

nonuniform learnability

lemma 7.6 (kraft inequality)

if s     {0, 1}    is a pre   x-free set of strings, then

(cid:88)

     s

1

2|  |     1.

(cid:114)|h| + ln(2/  )

,

2m

proof de   ne a id203 distribution over the members of s as follows: re-
peatedly toss an unbiased coin, with faces labeled 0 and 1, until the sequence
of outcomes is a member of s; at that point, stop. for each        s, let p (  )
be the id203 that this process generates the string   . note that since s is
pre   x-free, for every        s, if the coin toss outcomes follow the bits of    then
we will stop only once the sequence of outcomes equals   . we therefore get that,
for every        s, p (  ) = 1
2|  | . since probabilities add up to at most 1, our proof
is concluded.

in light of kraft   s inequality, any pre   x-free description language of a hypoth-
esis class, h, gives rise to a weighting function w over that hypothesis class     we
will simply set w(h) = 1
2|h| . this observation immediately yields the following:
theorem 7.7 let h be a hypothesis class and let d : h     {0, 1}    be a pre   x-
free description language for h. then, for every sample size, m, every con   dence
parameter,    > 0, and every id203 distribution, d, with id203 greater
than 1        over the choice of s     dm we have that,

   h     h, ld(h)     ls(h) +

where |h| is the length of d(h).
proof choose w(h) = 1/2|h|, apply theorem 7.4 with  n(m,   ) =
note that ln(2|h|) = |h| ln(2) < |h|.

(cid:113) ln(2/  )

2m , and

(cid:113)|h|+ln(2/  )

as was the case with theorem 7.4, this result suggests a learning paradigm
for h     given a training set, s, search for a hypothesis h     h that minimizes
the bound, ls(h) +
. in particular, it suggests trading o    empirical
risk for saving description length. this yields the minimum description length
learning paradigm.

2m

minimum description length (mdl)

prior knowledge:
h is a countable hypothesis class
h is described by a pre   x-free language over {0, 1}
for every h     h, |h| is the length of the representation of h
input: a training set s     dm, con   dence   
output: h     argminh   h

(cid:113)|h|+ln(2/  )

ls(h) +

(cid:20)

(cid:21)

2m

example 7.3 let h be the class of all predictors that can be implemented using
some programming language, say, c++. let us represent each program using the

7.3 minimum description length and occam   s razor

91

binary string obtained by running the gzip command on the program (this yields
a pre   x-free description language over the alphabet {0, 1}). then, |h| is simply
the length (in bits) of the output of gzip when running on the c++ program
corresponding to h.

7.3.1

occam   s razor

theorem 7.7 suggests that, having two hypotheses sharing the same empirical
risk, the true risk of the one that has shorter description can be bounded by a
lower value. thus, this result can be viewed as conveying a philosophical message:

a short explanation (that is, a hypothesis that has a short length) tends to be more valid
than a long explanation.

this is a well known principle, called occam   s razor, after william of ockham,
a 14th-century english logician, who is believed to have been the    rst to phrase
it explicitly. here, we provide one possible justi   cation to this principle. the
inequality of theorem 7.7 shows that the more complex a hypothesis h is (in the
sense of having a longer description), the larger the sample size it has to    t to
guarantee that it has a small true risk, ld(h).

at a second glance, our occam razor claim might seem somewhat problematic.
in the context in which the occam razor principle is usually invoked in science,
the language according to which complexity is measured is a natural language,
whereas here we may consider any arbitrary abstract description language. as-
sume that we have two hypotheses such that |h(cid:48)| is much smaller than |h|. by
the preceding result, if both have the same error on a given training set, s, then
the true error of h may be much higher than the true error of h(cid:48), so one should
prefer h(cid:48) over h. however, we could have chosen a di   erent description language,
say, one that assigns a string of length 3 to h and a string of length 100000 to h(cid:48).
suddenly it looks as if one should prefer h over h(cid:48). but these are the same h and
h(cid:48) for which we argued two sentences ago that h(cid:48) should be preferable. where is
the catch here?

(cid:113) ln(2/  )

indeed, there is no inherent generalizability di   erence between hypotheses.
the crucial aspect here is the dependency order between the initial choice of
language (or, preference over hypotheses) and the training set. as we know from
the basic hoe   ding   s bound (equation (4.2)), if we commit to any hypothesis be-
fore seeing the data, then we are guaranteed a rather small estimation error term
ld(h)     ls(h) +
2m . choosing a description language (or, equivalently,
some weighting of hypotheses) is a weak form of committing to a hypothesis.
rather than committing to a single hypothesis, we spread out our commitment
among many. as long as it is done independently of the training sample, our gen-
eralization bound holds. just as the choice of a single hypothesis to be evaluated
by a sample can be arbitrary, so is the choice of description language.

92

nonuniform learnability

7.4

other notions of learnability     consistency

the notion of learnability can be further relaxed by allowing the needed sample
sizes to depend not only on  ,   , and h but also on the underlying data-generating
id203 distribution d (that is used to generate the training sample and to
determine the risk). this type of performance guarantee is captured by the notion
of consistency 1 of a learning rule.
definition 7.8 (consistency) let z be a domain set, let p be a set of
id203 distributions over z, and let h be a hypothesis class. a learn-
ing rule a is consistent with respect to h and p if there exists a function
mconh : (0, 1)2    h    p     n such that, for every  ,        (0, 1), every h     h, and
every d     p, if m     mnulh ( ,   , h,d) then with id203 of at least 1        over
the choice of s     dm it holds that

ld(a(s))     ld(h) +  .

if p is the set of all distributions,2 we say that a is universally consistent with
respect to h.

the notion of consistency is, of course, a relaxation of our previous notion
of nonuniform learnability. clearly if an algorithm nonuniformly learns a class
h it is also universally consistent for that class. the relaxation is strict in the
sense that there are consistent learning rules that are not successful nonuniform
learners. for example, the algorithm memorize de   ned in example 7.4 later is
universally consistent for the class of all binary classi   ers over n. however, as
we have argued before, this class is not nonuniformly learnable.

example 7.4 consider the classi   cation prediction algorithm memorize de   ned
as follows. the algorithm memorizes the training examples, and, given a test
point x, it predicts the majority label among all labeled instances of x that exist
in the training sample (and some    xed default label if no instance of x appears
in the training set). it is possible to show (see exercise 6) that the memorize
algorithm is universally consistent for every countable domain x and a    nite
label set y (w.r.t. the zero-one loss).

intuitively, it is not obvious that the memorize algorithm should be viewed as a
learner, since it lacks the aspect of generalization, namely, of using observed data
to predict the labels of unseen examples. the fact that memorize is a consistent
algorithm for the class of all functions over any countable domain set therefore
raises doubt about the usefulness of consistency guarantees. furthermore, the
sharp-eyed reader may notice that the    bad learner    we introduced in chapter 2,

1 in the literature, consistency is often de   ned using the notion of either convergence in

id203 (corresponding to weak consistency) or almost sure convergence (corresponding
to strong consistency).

2 formally, we assume that z is endowed with some sigma algebra of subsets    , and by    all

distributions    we mean all id203 distributions that have     contained in their
associated family of measurable subsets.

7.5 discussing the di   erent notions of learnability

93

which led to over   tting, is in fact the memorize algorithm. in the next section
we discuss the signi   cance of the di   erent notions of learnability and revisit the
no-free-lunch theorem in light of the di   erent de   nitions of learnability.

7.5

discussing the di   erent notions of learnability

we have given three de   nitions of learnability and we now discuss their useful-
ness. as is usually the case, the usefulness of a mathematical de   nition depends
on what we need it for. we therefore list several possible goals that we aim to
achieve by de   ning learnability and discuss the usefulness of the di   erent de   ni-
tions in light of these goals.

what is the risk of the learned hypothesis?
the    rst possible goal of deriving performance guarantees on a learning algo-
rithm is bounding the risk of the output predictor. here, both pac learning
and nonuniform learning give us an upper bound on the true risk of the learned
hypothesis based on its empirical risk. consistency guarantees do not provide
such a bound. however, it is always possible to estimate the risk of the output
predictor using a validation set (as will be described in chapter 11).

how many examples are required to be as good as the best hypothesis
in h?
when approaching a learning problem, a natural question is how many exam-
ples we need to collect in order to learn it. here, pac learning gives a crisp
answer. however, for both nonuniform learning and consistency, we do not know
in advance how many examples are required to learn h. in nonuniform learning
this number depends on the best hypothesis in h, and in consistency it also
depends on the underlying distribution. in this sense, pac learning is the only
useful de   nition of learnability. on the    ip side, one should keep in mind that
even if the estimation error of the predictor we learn is small, its risk may still
be large if h has a large approximation error. so, for the question    how many
examples are required to be as good as the bayes optimal predictor?    even pac
guarantees do not provide us with a crisp answer. this re   ects the fact that the
usefulness of pac learning relies on the quality of our prior knowledge.

pac guarantees also help us to understand what we should do next if our
learning algorithm returns a hypothesis with a large risk, since we can bound
the part of the error that stems from estimation error and therefore know how
much of the error is attributed to approximation error. if the approximation error
is large, we know that we should use a di   erent hypothesis class. similarly, if a
nonuniform algorithm fails, we can consider a di   erent weighting function over
(subsets of) hypotheses. however, when a consistent algorithm fails, we have
no idea whether this is because of the estimation error or the approximation
error. furthermore, even if we are sure we have a problem with the estimation

94

nonuniform learnability

error term, we do not know how many more examples are needed to make the
estimation error small.

how to learn? how to express prior knowledge?
maybe the most useful aspect of the theory of learning is in providing an answer
to the question of    how to learn.    the de   nition of pac learning yields the
limitation of learning (via the no-free-lunch theorem) and the necessity of prior
knowledge. it gives us a crisp way to encode prior knowledge by choosing a
hypothesis class, and once this choice is made, we have a generic learning rule    
erm. the de   nition of nonuniform learnability also yields a crisp way to encode
prior knowledge by specifying weights over (subsets of) hypotheses of h. once
this choice is made, we again have a generic learning rule     srm. the srm rule
is also advantageous in model selection tasks, where prior knowledge is partial.
we elaborate on model selection in chapter 11 and here we give a brief example.
consider the problem of    tting a one dimensional polynomial to data; namely,
our goal is to learn a function, h : r     r, and as prior knowledge we consider
the hypothesis class of polynomials. however, we might be uncertain regarding
which degree d would give the best results for our data set: a small degree might
not    t the data well (i.e., it will have a large approximation error), whereas a
high degree might lead to over   tting (i.e., it will have a large estimation error).
in the following we depict the result of    tting a polynomial of degrees 2, 3, and
10 to the same training set.

degree 2

degree 3

degree 10

it is easy to see that the empirical risk decreases as we enlarge the degree.
therefore, if we choose h to be the class of all polynomials up to degree 10 then
the erm rule with respect to this class would output a 10 degree polynomial
and would over   t. on the other hand, if we choose too small a hypothesis class,
say, polynomials up to degree 2, then the erm would su   er from under   tting
(i.e., a large approximation error). in contrast, we can use the srm rule on the
set of all polynomials, while ordering subsets of h according to their degree, and
this will yield a 3rd degree polynomial since the combination of its empirical
risk and the bound on its estimation error is the smallest. in other words, the
srm rule enables us to select the right model on the basis of the data itself. the
price we pay for this    exibility (besides a slight increase of the estimation error
relative to pac learning w.r.t. the optimal degree) is that we do not know in

7.5 discussing the di   erent notions of learnability

95

advance how many examples are needed to compete with the best hypothesis in
h.

unlike the notions of pac learnability and nonuniform learnability, the de   ni-
tion of consistency does not yield a natural learning paradigm or a way to encode
prior knowledge. in fact, in many cases there is no need for prior knowledge at
all. for example, we saw that even the memorize algorithm, which intuitively
should not be called a learning algorithm, is a consistent algorithm for any class
de   ned over a countable domain and a    nite label set. this hints that consistency
is a very weak requirement.

which learning algorithm should we prefer?
one may argue that even though consistency is a weak requirement, it is desirable
that a learning algorithm will be consistent with respect to the set of all functions
from x to y, which gives us a guarantee that for enough training examples, we
will always be as good as the bayes optimal predictor. therefore, if we have
two algorithms, where one is consistent and the other one is not consistent, we
should prefer the consistent algorithm. however, this argument is problematic for
two reasons. first, maybe it is the case that for most    natural    distributions we
will observe in practice that the sample complexity of the consistent algorithm
will be so large so that in every practical situation we will not obtain enough
examples to enjoy this guarantee. second, it is not very hard to make any pac
or nonuniform learner consistent with respect to the class of all functions from
x to y. concretely, consider a countable domain, x , a    nite label set y, and
a hypothesis class, h, of functions from x to y. we can make any nonuniform
learner for h be consistent with respect to the class of all classi   ers from x to y
using the following simple trick: upon receiving a training set, we will    rst run
the nonuniform learner over the training set, and then we will obtain a bound
on the true risk of the learned predictor. if this bound is small enough we are
done. otherwise, we revert to the memorize algorithm. this simple modi   cation
makes the algorithm consistent with respect to all functions from x to y. since
it is easy to make any algorithm consistent, it may not be wise to prefer one
algorithm over the other just because of consistency considerations.

7.5.1

the no-free-lunch theorem revisited

recall that the no-free-lunch theorem (theorem 5.1 from chapter 5) implies
that no algorithm can learn the class of all classi   ers over an in   nite domain.
in contrast, in this chapter we saw that the memorize algorithm is consistent
with respect to the class of all classi   ers over a countable in   nite domain. to
understand why these two statements do not contradict each other, let us    rst
recall the formal statement of the no-free-lunch theorem.
let x be a countable in   nite domain and let y = {  1}. the no-free-lunch
theorem implies the following: for any algorithm, a, and a training set size, m,
there exist a distribution over x and a function h(cid:63) : x     y, such that if a

96

nonuniform learnability

will get a sample of m i.i.d. training examples, labeled by h(cid:63), then a is likely to
return a classi   er with a larger error.

the consistency of memorize implies the following: for every distribution over
x and a labeling function h(cid:63) : x     y, there exists a training set size m (that
depends on the distribution and on h(cid:63)) such that if memorize receives at least
m examples it is likely to return a classi   er with a small error.

we see that in the no-free-lunch theorem, we    rst    x the training set size,
and then    nd a distribution and a labeling function that are bad for this training
set size. in contrast, in consistency guarantees, we    rst    x the distribution and
the labeling function, and only then do we    nd a training set size that su   ces
for learning this particular distribution and labeling function.

7.6

summary

we introduced nonuniform learnability as a relaxation of pac learnability and
consistency as a relaxation of nonuniform learnability. this means that even
classes of in   nite vc-dimension can be learnable, in some weaker sense of learn-
ability. we discussed the usefulness of the di   erent de   nitions of learnability.

for hypothesis classes that are countable, we can apply the minimum descrip-
tion length scheme, where hypotheses with shorter descriptions are preferred,
following the principle of occam   s razor. an interesting example is the hypothe-
sis class of all predictors we can implement in c++ (or any other programming
language), which we can learn (nonuniformly) using the mdl scheme.

arguably, the class of all predictors we can implement in c++ is a powerful
class of functions and probably contains all that we can hope to learn in prac-
tice. the ability to learn this class is impressive, and, seemingly, this chapter
should have been the last chapter of this book. this is not the case, because of
the computational aspect of learning: that is, the runtime needed to apply the
learning rule. for example, to implement the mdl paradigm with respect to
all c++ programs, we need to perform an exhaustive search over all c++ pro-
grams, which will take forever. even the implementation of the erm paradigm
with respect to all c++ programs of description length at most 1000 bits re-
quires an exhaustive search over 21000 hypotheses. while the sample complexity
, the runtime is     21000. this is a huge
of learning this class is just 1000+log(2/  )
number     much larger than the number of atoms in the visible universe. in the
next chapter we formally de   ne the computational complexity of learning. in the
second part of this book we will study hypothesis classes for which the erm or
srm schemes can be implemented e   ciently.

 2

7.7 bibliographic remarks

97

7.7

bibliographic remarks

our de   nition of nonuniform learnability is related to the de   nition of an occam-
algorithm in blumer, ehrenfeucht, haussler & warmuth (1987). the concept of
srm is due to (vapnik & chervonenkis 1974, vapnik 1995). the concept of mdl
is due to (rissanen 1978, rissanen 1983). the relation between srm and mdl
is discussed in vapnik (1995). these notions are also closely related to the notion
of id173 (e.g. tikhonov (1943)). we will elaborate on id173 in
the second part of this book.

the notion of consistency of estimators dates back to fisher (1922). our pre-
sentation of consistency follows steinwart & christmann (2008), who also derived
several no-free-lunch theorems.

7.8

exercises

1. prove that for any    nite class h, and any description language d : h    
{0, 1}   , the vc-dimension of h is at most 2 sup{|d(h)| : h     h}     the maxi-
mum description length of a predictor in h. furthermore, if d is a pre   x-free
description then vcdim(h)     sup{|d(h)| : h     h}.

w(hi)     w(hj).

2. let h = {hn : n     n} be an in   nite countable hypothesis class for binary
classi   cation. show that it is impossible to assign weights to the hypotheses
in h such that
    h could be learnt nonuniformly using these weights. that is, the weighting
h   h w(h)     1.
    the weights would be monotonically nondecreasing. that is, if i < j, then

function w : h     [0, 1] should satisfy the condition(cid:80)
   nite. find a weighting function w : h     [0, 1] such that(cid:80)

n=1 hn, where for every n     n, hn is
h   h w(h)    
1 and so that for all h     h, w(h) is determined by n(h) = min{n : h    
hn} and by |hn(h)|.

3.     consider a hypothesis class h =(cid:83)   

    (*) de   ne such a function w when for all n hn is countable (possibly
4. let h be some hypothesis class. for any h     h, let |h| denote the description
length of h, according to some    xed description language. consider the mdl
learning paradigm in which the algorithm returns:

in   nite).

(cid:34)

(cid:114)|h| + ln(2/  )

(cid:35)

2m

,

hs     arg min
h   h

ls(h) +

where s is a sample of size m. for any b > 0, let hb = {h     h : |h|     b},
and de   ne

h   
b = arg min
h   hb

ld(h).

98

nonuniform learnability

prove a bound on ld(hs)   ld(h   
b) in terms of b, the con   dence parameter
  , and the size of the training set m.
    note: such bounds are known as oracle inequalities in the literature: we
wish to estimate how good we are compared to a reference classi   er (or
   oracle   ) h   
b.

5. in this question we wish to show a no-free-lunch result for nonuniform learn-
ability: namely, that, over any in   nite domain, the class of all functions is not
learnable even under the relaxed nonuniform variation of learning.
recall that an algorithm, a, nonuniformly learns a hypothesis class h if
there exists a function mnulh : (0, 1)2  h     n such that, for every  ,        (0, 1)
and for every h     h, if m     mnulh ( ,   , h) then for every distribution d, with
id203 of at least 1        over the choice of s     dm, it holds that

ld(a(s))     ld(h) +  .

n   n hn and, for every n     n, vcdim(hn) is    nite.

so that h =(cid:83)
of classes (hn : n     n) such that h =(cid:83)

if such an algorithm exists then we say that h is nonuniformly learnable.
1. let a be a nonuniform learner for a class h. for each n     n de   ne ha
n =
{h     h : mnul(0.1, 0.1, h)     n}. prove that each such class hn has a    nite
vc-dimension.
2. prove that if a class h is nonuniformly learnable then there are classes hn
3. let h be a class that shatters an in   nite set. then, for every sequence
n   n hn, there exists some n for
which vcdim(hn) =    .
hint: given a class h that shatters some in   nite set k, and a sequence of
classes (hn : n     n), each having a    nite vc-dimension, start by de   ning
subsets kn     k such that, for all n, |kn| > vcdim(hn) and for any
n (cid:54)= m, kn     km =    . now, pick for each such kn a function fn : kn    
{0, 1} so that no h     hn agrees with fn on the domain kn. finally, de   ne
n   n hn
4. construct a class h1 of functions from the unit interval [0, 1] to {0, 1} that
5. construct a class h2 of functions from the unit interval [0, 1] to {0, 1} that

f : x     {0, 1} by combining these fn   s and prove that f    (cid:0)h \(cid:83)

is nonuniformly learnable but not pac learnable.

(cid:1).

is not nonuniformly learnable.

6. in this question we wish to show that the algorithm memorize is a consistent
learner for every class of (binary-valued) functions over any countable domain.
let x be a countable domain and let d be a id203 distribution over x .
1. let {xi : i     n} be an enumeration of the elements of x so that for all

i     j, d({xi})     d({xj}). prove that

(cid:88)

i   n

lim
n      

d({xi}) = 0.

2. given any   > 0 prove that there exists  d > 0 such that

d({x     x : d({x}) <  d}) <  .

7.8 exercises

99

3. prove that for every    > 0, if n is such that d({xi}) <    for all i > n, then

for every m     n,
p

s   dm

[   xi : (d({xi}) >    and xi /    s)]     ne     m.

4. conclude that if x is countable then for every id203 distribution d
over x there exists a function md : (0, 1)    (0, 1)     n such that for every
 ,    > 0 if m > md( ,   ) then

p

s   dm

[d({x : x /    s}) >  ] <   .

5. prove that memorize is a consistent learner for every class of (binary-

valued) functions over any countable domain.

8

the runtime of learning

so far in the book we have studied the statistical perspective of learning, namely,
how many samples are needed for learning. in other words, we focused on the
amount of information learning requires. however, when considering automated
learning, computational resources also play a major role in determining the com-
plexity of a task: that is, how much computation is involved in carrying out a
learning task. once a su   cient training sample is available to the learner, there
is some computation to be done to extract a hypothesis or    gure out the label of
a given test instance. these computational resources are crucial in any practical
application of machine learning. we refer to these two types of resources as the
sample complexity and the computational complexity. in this chapter, we turn
our attention to the computational complexity of learning.

the computational complexity of learning should be viewed in the wider con-
text of the computational complexity of general algorithmic tasks. this area has
been extensively investigated; see, for example, (sipser 2006). the introductory
comments that follow summarize the basic ideas of that general theory that are
most relevant to our discussion.

the actual runtime (in seconds) of an algorithm depends on the speci   c ma-
chine the algorithm is being implemented on (e.g., what the clock rate of the
machine   s cpu is). to avoid dependence on the speci   c machine, it is common
to analyze the runtime of algorithms in an asymptotic sense. for example, we
say that the computational complexity of the merge-sort algorithm, which sorts
a list of n items, is o(n log(n)). this implies that we can implement the algo-
rithm on any machine that satis   es the requirements of some accepted abstract
model of computation, and the actual runtime in seconds will satisfy the follow-
ing: there exist constants c and n0, which can depend on the actual machine,
such that, for any value of n > n0, the runtime in seconds of sorting any n items
will be at most c n log(n). it is common to use the term feasible or e   ciently
computable for tasks that can be performed by an algorithm whose running time
is o(p(n)) for some polynomial function p. one should note that this type of
analysis depends on de   ning what is the input size n of any instance to which
the algorithm is expected to be applied. for    purely algorithmic    tasks, as dis-
cussed in the common computational complexity literature, this input size is
clearly de   ned; the algorithm gets an input instance, say, a list to be sorted, or
an arithmetic operation to be calculated, which has a well de   ned size (say, the

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

8.1 computational complexity of learning

101

number of bits in its representation). for machine learning tasks, the notion of
an input size is not so clear. an algorithm aims to detect some pattern in a data
set and can only access random samples of that data.

we start the chapter by discussing this issue and de   ne the computational
complexity of learning. for advanced students, we also provide a detailed formal
de   nition. we then move on to consider the computational complexity of im-
plementing the erm rule. we    rst give several examples of hypothesis classes
where the erm rule can be e   ciently implemented, and then consider some
cases where, although the class is indeed e   ciently learnable, erm implemen-
tation is computationally hard. it follows that hardness of implementing erm
does not imply hardness of learning. finally, we brie   y discuss how one can show
hardness of a given learning task, namely, that no learning algorithm can solve
it e   ciently.

8.1

computational complexity of learning

recall that a learning algorithm has access to a domain of examples, z, a hy-
pothesis class, h, a id168, (cid:96), and a training set of examples from z that
are sampled i.i.d. according to an unknown distribution d. given parameters
 ,   , the algorithm should output a hypothesis h such that with id203 of
at least 1       ,

ld(h)     min

h(cid:48)   h ld(h(cid:48)) +  .

as mentioned before, the actual runtime of an algorithm in seconds depends on
the speci   c machine. to allow machine independent analysis, we use the standard
approach in computational complexity theory. first, we rely on a notion of an
abstract machine, such as a turing machine (or a turing machine over the reals
(blum, shub & smale 1989)). second, we analyze the runtime in an asymptotic
sense, while ignoring constant factors, thus the speci   c machine is not important
as long as it implements the abstract machine. usually, the asymptote is with
respect to the size of the input to the algorithm. for example, for the merge-sort
algorithm mentioned before, we analyze the runtime as a function of the number
of items that need to be sorted.

in the context of learning algorithms, there is no clear notion of    input size.   
one might de   ne the input size to be the size of the training set the algorithm
receives, but that would be rather pointless. if we give the algorithm a very
large number of examples, much larger than the sample complexity of the learn-
ing problem, the algorithm can simply ignore the extra examples. therefore, a
larger training set does not make the learning problem more di   cult, and, con-
sequently, the runtime available for a learning algorithm should not increase as
we increase the size of the training set. just the same, we can still analyze the
runtime as a function of natural parameters of the problem such as the target
accuracy, the con   dence of achieving that accuracy, the dimensionality of the

102

the runtime of learning

domain set, or some measures of the complexity of the hypothesis class with
which the algorithm   s output is compared.

to illustrate this, consider a learning algorithm for the task of learning axis
aligned rectangles. a speci   c problem of learning axis aligned rectangles is de-
rived by specifying  ,   , and the dimension of the instance space. we can de   ne a
sequence of problems of the type    rectangles learning    by    xing  ,    and varying
the dimension to be d = 2, 3, 4, . . .. we can also de   ne another sequence of    rect-
angles learning    problems by    xing d,    and varying the target accuracy to be
  = 1
3 , . . .. one can of course choose other sequences of such problems. once
a sequence of the problems is    xed, one can analyze the asymptotic runtime as
a function of variables of that sequence.

2 , 1

before we introduce the formal de   nition, there is one more subtlety we need
to tackle. on the basis of the preceding, a learning algorithm can    cheat,    by
transferring the computational burden to the output hypothesis. for example,
the algorithm can simply de   ne the output hypothesis to be the function that
stores the training set in its memory, and whenever it gets a test example x
it calculates the erm hypothesis on the training set and applies it on x. note
that in this case, our algorithm has a    xed output (namely, the function that
we have just described) and can run in constant time. however, learning is still
hard     the hardness is now in implementing the output classi   er to obtain a
label prediction. to prevent this    cheating,    we shall require that the output of
a learning algorithm must be applied to predict the label of a new example in
time that does not exceed the runtime of training (that is, computing the output
classi   er from the input training sample). in the next subsection the advanced
reader may    nd a formal de   nition of the computational complexity of learning.

8.1.1

formal de   nition*

the de   nition that follows relies on a notion of an underlying abstract machine,
which is usually either a turing machine or a turing machine over the reals. we
will measure the computational complexity of an algorithm using the number of
   operations    it needs to perform, where we assume that for any machine that
implements the underlying abstract machine there exists a constant c such that
any such    operation    can be performed on the machine using c seconds.

definition 8.1 (the computational complexity of a learning algorithm)
we de   ne the complexity of learning in two steps. first we consider the compu-
tational complexity of a    xed learning problem (determined by a triplet (z,h, (cid:96))
    a domain set, a benchmark hypothesis class, and a id168). then, in the
second step we consider the rate of change of that complexity along a sequence
of such tasks.
1. given a function f : (0, 1)2     n, a learning task (z,h, (cid:96)), and a learning
algorithm, a, we say that a solves the learning task in time o(f ) if there
exists some constant number c, such that for every id203 distribution d

8.2 implementing the erm rule

103

over z, and input  ,        (0, 1), when a has access to samples generated i.i.d.
by d,
    a terminates after performing at most cf ( ,   ) operations
    the output of a, denoted ha, can be applied to predict the label of a new
    the output of a is probably approximately correct; namely, with proba-
bility of at least 1        (over the random samples a receives), ld(ha)    
minh(cid:48)   h ld(h(cid:48)) +  

example while performing at most cf ( ,   ) operations

2. consider a sequence of learning problems, (zn,hn, (cid:96)n)   

n=1, where problem n
is de   ned by a domain zn, a hypothesis class hn, and a id168 (cid:96)n.
let a be a learning algorithm designed for solving learning problems of
this form. given a function g : n    (0, 1)2     n, we say that the runtime of
a with respect to the preceding sequence is o(g), if for all n, a solves the
problem (zn,hn, (cid:96)n) in time o(fn), where fn : (0, 1)2     n is de   ned by
fn( ,   ) = g(n,  ,   ).
we say that a is an e   cient algorithm with respect to a sequence (zn,hn, (cid:96)n)

if its runtime is o(p(n, 1/ , 1/  )) for some polynomial p.

from this de   nition we see that the question whether a general learning prob-
lem can be solved e   ciently depends on how it can be broken into a sequence
of speci   c learning problems. for example, consider the problem of learning a
   nite hypothesis class. as we showed in previous chapters, the erm rule over
h is guaranteed to ( ,   )-learn h if the number of training examples is order of
mh( ,   ) = log(|h|/  )/ 2. assuming that the evaluation of a hypothesis on an
example takes a constant time, it is possible to implement the erm rule in time
o(|h| mh( ,   )) by performing an exhaustive search over h with a training set
of size mh( ,   ). for any    xed    nite h, the exhaustive search algorithm runs
in polynomial time. furthermore, if we de   ne a sequence of problems in which
|hn| = n, then the exhaustive search is still considered to be e   cient. however, if
we de   ne a sequence of problems for which |hn| = 2n, then the sample complex-
ity is still polynomial in n but the computational complexity of the exhaustive
search algorithm grows exponentially with n (thus, rendered ine   cient).

8.2

implementing the erm rule
given a hypothesis class h, the ermh rule is maybe the most natural learning
paradigm. furthermore, for binary classi   cation problems we saw that if learning
is at all possible, it is possible with the erm rule. in this section we discuss the
computational complexity of implementing the erm rule for several hypothesis
classes.

given a hypothesis class, h, a domain set z, and a id168 (cid:96), the corre-

sponding ermh rule can be de   ned as follows:

104

the runtime of learning

on a    nite input sample s     zm output some h     h that minimizes the empirical loss,
ls(h) = 1|s|

z   s (cid:96)(h, z).

(cid:80)

this section studies the runtime of implementing the erm rule for several

examples of learning tasks.

8.2.1

finite classes

limiting the hypothesis class to be a    nite class may be considered as a reason-
ably mild restriction. for example, h can be the set of all predictors that can be
implemented by a c++ program written in at most 10000 bits of code. other ex-
amples of useful    nite classes are any hypothesis class that can be parameterized
by a    nite number of parameters, where we are satis   ed with a representation
of each of the parameters using a    nite number of bits, for example, the class of
axis aligned rectangles in the euclidean space, rd, when the parameters de   ning
any given rectangle are speci   ed up to some limited precision.
as we have shown in previous chapters, the sample complexity of learning a
   nite class is upper bounded by mh( ,   ) = c log(c|h|/  )/ c, where c = 1 in
the realizable case and c = 2 in the nonrealizable case. therefore, the sample
complexity has a mild dependence on the size of h. in the example of c++
programs mentioned before, the number of hypotheses is 210,000 but the sample
complexity is only c(10, 000 + log(c/  ))/ c.

a straightforward approach for implementing the erm rule over a    nite hy-
pothesis class is to perform an exhaustive search. that is, for each h     h we
calculate the empirical risk, ls(h), and return a hypothesis that minimizes
the empirical risk. assuming that the evaluation of (cid:96)(h, z) on a single exam-
ple takes a constant amount of time, k, the runtime of this exhaustive search
becomes k|h|m, where m is the size of the training set. if we let m to be the
upper bound on the sample complexity mentioned, then the runtime becomes
k|h|c log(c|h|/  )/ c.

the linear dependence of the runtime on the size of h makes this approach
ine   cient (and unrealistic) for large classes. formally, if we de   ne a sequence of
n=1 such that log(|hn|) = n, then the exhaustive search
problems (zn,hn, (cid:96)n)   
approach yields an exponential runtime. in the example of c++ programs, if hn
is the set of functions that can be implemented by a c++ program written in
at most n bits of code, then the runtime grows exponentially with n, implying
that the exhaustive search approach is unrealistic for practical use. in fact, this
problem is one of the reasons we are dealing with other hypothesis classes, like
classes of linear predictors, which we will encounter in the next chapter, and not
just focusing on    nite classes.

it is important to realize that the ine   ciency of one algorithmic approach
(such as the exhaustive search) does not yet imply that no e   cient erm imple-
mentation exists. indeed, we will show examples in which the erm rule can be
implemented e   ciently.

8.2 implementing the erm rule

105

8.2.2

axis aligned rectangles
let hn be the class of axis aligned rectangles in rn, namely,

hn = {h(a1,...,an,b1,...,bn) :    i, ai     bi}

where

h(a1,...,an,b1,...,bn)(x, y) =

if    i, xi     [ai, bi]
otherwise

(8.1)

(cid:40)

1

0

e   ciently learnable in the realizable case
consider implementing the erm rule in the realizable case. that is, we are given
a training set s = (x1, y1), . . . , (xm, ym) of examples, such that there exists an
axis aligned rectangle, h     hn, for which h(xi) = yi for all i. our goal is to    nd
such an axis aligned rectangle with a zero training error, namely, a rectangle
that is consistent with all the labels in s.
we show later that this can be done in time o(nm). indeed, for each i     [n],
set ai = min{xi : (x, 1)     s} and bi = max{xi : (x, 1)     s}. in words, we take
ai to be the minimal value of the i   th coordinate of a positive example in s and
bi to be the maximal value of the i   th coordinate of a positive example in s.
it is easy to verify that the resulting rectangle has zero training error and that
the runtime of    nding each ai and bi is o(m). hence, the total runtime of this
procedure is o(nm).

not e   ciently learnable in the agnostic case
in the agnostic case, we do not assume that some hypothesis h perfectly predicts
the labels of all the examples in the training set. our goal is therefore to    nd
h that minimizes the number of examples for which yi (cid:54)= h(xi). it turns out
that for many common hypothesis classes, including the classes of axis aligned
rectangles we consider here, solving the erm problem in the agnostic setting is
np-hard (and, in most cases, it is even np-hard to    nd some h     h whose error
is no more than some constant c > 1 times that of the empirical risk minimizer
in h). that is, unless p = np, there is no algorithm whose running time is
polynomial in m and n that is guaranteed to    nd an erm hypothesis for these
problems (ben-david, eiron & long 2003).

on the other hand, it is worthwhile noticing that, if we    x one speci   c hypoth-
esis class, say, axis aligned rectangles in some    xed dimension, n, then there exist
e   cient learning algorithms for this class. in other words, there are successful
agnostic pac learners that run in time polynomial in 1/  and 1/   (but their
dependence on the dimension n is not polynomial).

to see this, recall the implementation of the erm rule we presented for the
realizable case, from which it follows that an axis aligned rectangle is determined
by at most 2n examples. therefore, given a training set of size m, we can per-
form an exhaustive search over all subsets of the training set of size at most 2n
examples and construct a rectangle from each such subset. then, we can pick

106

the runtime of learning

the rectangle with the minimal training error. this procedure is guaranteed to
   nd an erm hypothesis, and the runtime of the procedure is mo(n). it follows
that if n is    xed, the runtime is polynomial in the sample size. this does not
contradict the aforementioned hardness result, since there we argued that unless
p=np one cannot have an algorithm whose dependence on the dimension n is
polynomial as well.

8.2.3

boolean conjunctions
a boolean conjunction is a mapping from x = {0, 1}n to y = {0, 1} that can be
expressed as a proposition formula of the form xi1     . . .    xik      xj1     . . .     xjr ,
for some indices i1, . . . , ik, j1, . . . , jr     [n]. the function that such a proposition
formula de   nes is

(cid:40)

1

0

h(x) =

if xi1 =        = xik = 1 and xj1 =        = xjr = 0
otherwise

let hn

c be the class of all boolean conjunctions over {0, 1}n. the size of hn

c is
at most 3n + 1 (since in a conjunction formula, each element of x either appears,
or appears with a negation sign, or does not appear at all, and we also have the
all negative formula). hence, the sample complexity of learning hn
c using the
erm rule is at most n log(3/  )/ .

e   ciently learnable in the realizable case
next, we show that it is possible to solve the erm problem for hn
c in time
polynomial in n and m. the idea is to de   ne an erm conjunction by including
in the hypothesis conjunction all the literals that do not contradict any positively
labeled example. let v1, . . . , vm+ be all the positively labeled instances in the
input sample s. we de   ne, by induction on i     m+, a sequence of hypotheses
(or conjunctions). let h0 be the conjunction of all possible literals. that is,
h0 = x1       x1     x2     . . .     xn       xn. note that h0 assigns the label 0 to all the
elements of x . we obtain hi+1 by deleting from the conjunction hi all the literals
that are not satis   ed by vi+1. the algorithm outputs the hypothesis hm+. note
that hm+ labels positively all the positively labeled examples in s. furthermore,
for every i     m+, hi is the most restrictive conjunction that labels v1, . . . , vi
positively. now, since we consider learning in the realizable setup, there exists
a conjunction hypothesis, f     hn
c, that is consistent with all the examples in
s. since hm+ is the most restrictive conjunction that labels positively all the
positively labeled members of s, any instance labeled 0 by f is also labeled 0 by
hm+ . it follows that hm+ has zero training error (w.r.t. s), and is therefore a
legal erm hypothesis. note that the running time of this algorithm is o(mn).

8.3 e   ciently learnable, but not by a proper erm

107

not e   ciently learnable in the agnostic case
as in the case of axis aligned rectangles, unless p = np, there is no algorithm
whose running time is polynomial in m and n that guaranteed to    nd an erm
hypothesis for the class of boolean conjunctions in the unrealizable case.

8.2.4

learning 3-term dnf

we next show that a slight generalization of the class of boolean conjunctions
leads to intractability of solving the erm problem even in the realizable case.
consider the class of 3-term disjunctive normal form formulae (3-term dnf).
the instance space is x = {0, 1}n and each hypothesis is represented by the
boolean formula of the form h(x) = a1(x)    a2(x)    a3(x), where each ai(x) is
a boolean conjunction (as de   ned in the previous section). the output of h(x) is
1 if either a1(x) or a2(x) or a3(x) outputs the label 1. if all three conjunctions
output the label 0 then h(x) = 0.
of hn
the erm rule is at most 3n log(3/  )/ .

let hn
3dnf is at most 33n. hence, the sample complexity of learning hn

3dnf be the hypothesis class of all such 3-term dnf formulae. the size
3dnf using

however, from the computational perspective, this learning problem is hard.
it has been shown (see (pitt & valiant 1988, kearns et al. 1994)) that unless
rp = np, there is no polynomial time algorithm that properly learns a sequence
of 3-term dnf learning problems in which the dimension of the n   th problem is
n. by    properly    we mean that the algorithm should output a hypothesis that is
a 3-term dnf formula. in particular, since ermhn
outputs a 3-term dnf
formula it is a proper learner and therefore it is hard to implement it. the proof
uses a reduction of the graph 3-coloring problem to the problem of pac learning
3-term dnf. the detailed technique is given in exercise 3. see also (kearns &
vazirani 1994, section 1.4).

3dn f

8.3

e   ciently learnable, but not by a proper erm

in the previous section we saw that it is impossible to implement the erm rule
e   ciently for the class hn
3dnf of 3-dnf formulae. in this section we show that it
is possible to learn this class e   ciently, but using erm with respect to a larger
class.

representation independent learning is not hard
next we show that it is possible to learn 3-term dnf formulae e   ciently. there
is no contradiction to the hardness result mentioned in the previous section as we
now allow    representation independent    learning. that is, we allow the learning
algorithm to output a hypothesis that is not a 3-term dnf formula. the ba-
sic idea is to replace the original hypothesis class of 3-term dnf formula with
a larger hypothesis class so that the new class is easily learnable. the learning

108

the runtime of learning

algorithm might return a hypothesis that does not belong to the original hypoth-
esis class; hence the name    representation independent    learning. we emphasize
that in most situations, returning a hypothesis with good predictive ability is
what we are really interested in doing.
we start by noting that because     distributes over    , each 3-term dnf formula

can be rewritten as

a1     a2     a3 =

(cid:94)

(u     v     w)

u   a1,v   a2,w   a3

next, let us de   ne:    : {0, 1}n     {0, 1}(2n)3
such that for each triplet of literals
u, v, w there is a variable in the range of    indicating if u    v     w is true or false.
so, for each 3-dnf formula over {0, 1}n there is a conjunction over {0, 1}(2n)3
,
with the same truth table. since we assume that the data is realizable, we can
solve the erm problem with respect to the class of conjunctions over {0, 1}(2n)3
.
furthermore, the sample complexity of learning the class of conjunctions in the
higher dimensional space is at most n3 log(1/  )/ . thus, the overall runtime of
this approach is polynomial in n.

intuitively, the idea is as follows. we started with a hypothesis class for which
learning is hard. we switched to another representation where the hypothesis
class is larger than the original class but has more structure, which allows for a
more e   cient erm search. in the new representation, solving the erm problem
is easy.

( 2 n ) 3

0 , 1 }

{

o v e r

i o n s

c o n j u n c t

3-term-dnf formulae over {0, 1}n

8.4

hardness of learning*

we have just demonstrated that the computational hardness of implementing
ermh does not imply that such a class h is not learnable. how can we prove
that a learning problem is computationally hard?

one approach is to rely on cryptographic assumptions. in some sense, cryp-
tography is the opposite of learning. in learning we try to uncover some rule
underlying the examples we see, whereas in cryptography, the goal is to make
sure that nobody will be able to discover some secret, in spite of having access

8.4 hardness of learning*

109

to some partial information about it. on that high level intuitive sense, results
about the cryptographic security of some system translate into results about
the unlearnability of some corresponding task. regrettably, currently one has no
way of proving that a cryptographic protocol is not breakable. even the common
assumption of p (cid:54)= np does not su   ce for that (although it can be shown to
be necessary for most common cryptographic scenarios). the common approach
for proving that cryptographic protocols are secure is to start with some cryp-
tographic assumptions. the more these are used as a basis for cryptography, the
stronger is our belief that they really hold (or, at least, that algorithms that will
refute them are hard to come by).

we now brie   y describe the basic idea of how to deduce hardness of learnabil-
ity from cryptographic assumptions. many cryptographic systems rely on the
assumption that there exists a one way function. roughly speaking, a one way
function is a function f : {0, 1}n     {0, 1}n (more formally, it is a sequence of
functions, one for each dimension n) that is easy to compute but is hard to in-
vert. more formally, f can be computed in time poly(n) but for any randomized
polynomial time algorithm a, and for every polynomial p(  ),

p[f (a(f (x))) = f (x)] < 1

p(n) ,

where the id203 is taken over a random choice of x according to the uniform
distribution over {0, 1}n and the randomness of a.

a one way function, f , is called trapdoor one way function if, for some poly-
nomial function p, for every n there exists a bit-string sn (called a secret key) of
length     p(n), such that there is a polynomial time algorithm that, for every n
and every x     {0, 1}n, on input (f (x), sn) outputs x. in other words, although
f is hard to invert, once one has access to its secret key, inverting f becomes
feasible. such functions are parameterized by their secret key.
now, let fn be a family of trapdoor functions over {0, 1}n that can be calcu-
lated by some polynomial time algorithm. that is, we    x an algorithm that given
a secret key (representing one function in fn) and an input vector, it calculates
the value of the function corresponding to the secret key on the input vector in
polynomial time. consider the task of learning the class of the corresponding
f = {f   1 : f     fn}. since each function in this class can be inverted
inverses, h n
by some secret key sn of size polynomial in n, the class h n
f can be parameter-
ized by these keys and its size is at most 2p(n). its sample complexity is therefore
polynomial in n. we claim that there can be no e   cient learner for this class. if
there were such a learner, l, then by sampling uniformly at random a polynomial
number of strings in {0, 1}n, and computing f over them, we could generate a
labeled training sample of pairs (f (x), x), which should su   ce for our learner to
   gure out an ( ,   ) approximation of f   1 (w.r.t. the uniform distribution over
the range of f ), which would violate the one way property of f .

a more detailed treatment, as well as a concrete example, can be found in
(kearns & vazirani 1994, chapter 6). using reductions, they also show that

110

the runtime of learning

the class of functions that can be calculated by small boolean circuits is not
e   ciently learnable, even in the realizable case.

8.5

summary

the runtime of learning algorithms is asymptotically analyzed as a function of
di   erent parameters of the learning problem, such as the size of the hypothe-
sis class, our measure of accuracy, our measure of con   dence, or the size of the
domain set. we have demonstrated cases in which the erm rule can be imple-
mented e   ciently. for example, we derived e   cient algorithms for solving the
erm problem for the class of boolean conjunctions and the class of axis aligned
rectangles, under the realizability assumption. however, implementing erm for
these classes in the agnostic case is np-hard. recall that from the statistical
perspective, there is no di   erence between the realizable and agnostic cases (i.e.,
a class is learnable in both cases if and only if it has a    nite vc-dimension).
in contrast, as we saw, from the computational perspective the di   erence is im-
mense. we have also shown another example, the class of 3-term dnf, where
implementing erm is hard even in the realizable case, yet the class is e   ciently
learnable by another algorithm.

hardness of implementing the erm rule for several natural hypothesis classes
has motivated the development of alternative learning methods, which we will
discuss in the next part of this book.

8.6

bibliographic remarks

valiant (1984) introduced the e   cient pac learning model in which the runtime
of the algorithm is required to be polynomial in 1/ , 1/  , and the representation
size of hypotheses in the class. a detailed discussion and thorough bibliographic
notes are given in kearns & vazirani (1994).

8.7

exercises
1. let h be the class of intervals on the line (formally equivalent to axis aligned
rectangles in dimension n = 1). propose an implementation of the ermh
learning rule (in the agnostic case) that given a training set of size m, runs
in time o(m2).
hint: use id145.

2. let h1,h2, . . . be a sequence of hypothesis classes for binary classi   cation.
assume that there is a learning algorithm that implements the erm rule in
the realizable case such that the output hypothesis of the algorithm for each
class hn only depends on o(n) examples out of the training set. furthermore,

8.7 exercises

111

assume that such a hypothesis can be calculated given these o(n) examples
in time o(n), and that the empirical risk of each such hypothesis can be
evaluated in time o(mn). for example, if hn is the class of axis aligned
rectangles in rn, we saw that it is possible to    nd an erm hypothesis in the
realizable case that is de   ned by at most 2n examples. prove that in such
cases, it is possible to    nd an erm hypothesis for hn in the unrealizable case
in time o(mn mo(n)).

3. in this exercise, we present several classes for which    nding an erm classi-
   er is computationally hard. first, we introduce the class of n-dimensional
halfspaces, hsn, for a domain x = rn. this is the class of all functions of
the form hw,b(x) = sign((cid:104)w, x(cid:105) + b) where w, x     rn, (cid:104)w, x(cid:105) is their inner
product, and b     r. see a detailed description in chapter 9.
1. show that ermh over the class h = hsn of linear predictors is compu-
tationally hard. more precisely, we consider the sequence of problems in
which the dimension n grows linearly and the number of examples m is set
to be some constant times n.
hint: you can prove the hardness by a reduction from the following prob-
lem:
max fs: given a system of linear inequalities, ax > b with a     rm  n and b    
rm (that is, a system of m linear inequalities in n variables, x = (x1, . . . , xn)),
   nd a subsystem containing as many inequalities as possible that has a solution
(such a subsystem is called feasible).
it has been shown (sankaran 1993) that the problem max fs is np-hard.
show that any algorithm that    nds an ermhsn hypothesis for any training
sample s     (rn   {+1,   1})m can be used to solve the max fs problem of
size m, n. hint: de   ne a mapping that transforms linear inequalities in n
variables into labeled points in rn, and a mapping that transforms vectors
in rn to halfspaces, such that a vector w satis   es an inequality q if and
only if the labeled point that corresponds to q is classi   ed correctly by the
halfspace corresponding to w. conclude that the problem of empirical risk
minimization for halfspaces in also np-hard (that is, if it can be solved in
time polynomial in the sample size, m, and the euclidean dimension, n,
then every problem in the class np can be solved in polynomial time).

2. let x = rn and let hn

k be the class of all intersections of k-many linear
halfspaces in rn. in this exercise, we wish to show that ermhn
is com-
putationally hard for every k     3. precisely, we consider a sequence of
problems where k     3 is a constant and n grows linearly. the training set
size, m, also grows linearly with n.
towards this goal, consider the k-coloring problem for graphs, de   ned as
follows:
given a graph g = (v, e), and a number k, determine whether there exists a
function f : v     {1 . . . k} so that for every (u, v)     e, f (u) (cid:54)= f (v).
the k-coloring problem is known to be np-hard for every k     3 (karp
1972).

k

112

the runtime of learning

k

k

we wish to reduce the k-coloring problem to ermhn
: that is, to prove
that if there is an algorithm that solves the ermhn
problem in time
polynomial in k, n, and the sample size m, then there is a polynomial time
algorithm for the graph k-coloring problem.
given a graph g = (v, e), let {v1 . . . vn} be the vertices in v . construct
a sample s(g)     (rn    {  1})m, where m = |v | + |e|, as follows:
    for every vi     v , construct an instance ei with a negative label.
    for every edge (vi, vj)     e, construct an instance (ei + ej)/2 with a
1. prove that if there exists some h     hn

k that has zero error over s(g)

positive label.

j=1 hj be an erm classi   er in hn

then g is k-colorable.
k over s. de   ne a
coloring of v by setting f (vi) to be the minimal j such that hj(ei) =    1.
use the fact that halfspaces are convex sets to show that it cannot be
true that two vertices that are connected by an edge have the same
color.

2. prove that if g is k-colorable then there exists some h     h n

k that has

hint: let h = (cid:84)k

zero error over s(g).
hint: given a coloring f of the vertices of g, we should come up with k
hyperplanes, h1 . . . hk whose intersection is a perfect classi   er for s(g).
let b = 0.6 for all of these hyperplanes and, for t     k let the i   th weight
of the t   th hyperplane, wt,i, be    1 if f (vi) = t and 0 otherwise.

3. based on the above, prove that for any k     3, the ermhn

problem is

k

np-hard.

4. in this exercise we show that hardness of solving the erm problem is equiv-
alent to hardness of proper pac learning. recall that by    properness    of the
algorithm we mean that it must output a hypothesis from the hypothesis
class. to formalize this statement, we    rst need the following de   nition.

definition 8.2 the complexity class randomized polynomial (rp) time
is the class of all decision problems (that is, problems in which on any instance
one has to    nd out whether the answer is yes or no) for which there exists a
probabilistic algorithm (namely, the algorithm is allowed to    ip random coins
while it is running) with these properties:
    on any input instance the algorithm runs in polynomial time in the input

size.

    if the correct answer is no, the algorithm must return no.
    if the correct answer is yes, the algorithm returns yes with id203

a     1/2 and returns no with id203 1     a.1

clearly the class rp contains the class p. it is also known that rp is
contained in the class np. it is not known whether any equality holds among
these three complexity classes, but it is widely believed that np is strictly

1 the constant 1/2 in the de   nition can be replaced by any constant in (0, 1).

8.7 exercises

113

larger than rp. in particular, it is believed that np-hard problems cannot be
solved by a randomized polynomial time algorithm.
    show that if a class h is properly pac learnable by a polynomial time
algorithm, then the ermh problem is in the class rp. in particular, this
implies that whenever the ermh problem is np-hard (for example, the
class of intersections of halfspaces discussed in the previous exercise),
then, unless np = rp, there exists no polynomial time proper pac
learning algorithm for h.
hint: assume you have an algorithm a that properly pac learns a
class h in time polynomial in some class parameter n as well as in 1/ 
and 1/  . your goal is to use that algorithm as a subroutine to contract
an algorithm b for solving the ermh problem in random polynomial
time. given a training set, s     (x    {  1}m), and some h     h whose
error on s is zero, apply the pac learning algorithm to the uniform
distribution over s and run it so that with id203     0.3 it    nds a
function h     h that has error less than   = 1/|s| (with respect to that
uniform distribution). show that the algorithm just described satis   es
the requirements for being a rp solver for ermh.

part ii

from theory to algorithms

9

linear predictors

in this chapter we will study the family of linear predictors, one of the most
useful families of hypothesis classes. many learning algorithms that are being
widely used in practice rely on linear predictors,    rst and foremost because of
the ability to learn them e   ciently in many cases. in addition, linear predictors
are intuitive, are easy to interpret, and    t the data reasonably well in many
natural learning problems.

we will introduce several hypothesis classes belonging to this family     halfspaces,

id75 predictors, and id28 predictors     and present rele-
vant learning algorithms: id135 and the id88 algorithm for
the class of halfspaces and the least squares algorithm for id75.
this chapter is focused on learning linear predictors using the erm approach;
however, in later chapters we will see alternative paradigms for learning these
hypothesis classes.

first, we de   ne the class of a   ne functions as

ld = {hw,b : w     rd, b     r},

where

hw,b(x) = (cid:104)w, x(cid:105) + b =

(cid:32) d(cid:88)

(cid:33)

wixi

+ b.

it will be convenient also to use the notation

i=1

ld = {x (cid:55)    (cid:104)w, x(cid:105) + b : w     rd, b     r},

which reads as follows: ld is a set of functions, where each function is parame-
terized by w     rd and b     r, and each such function takes as input a vector x
and returns as output the scalar (cid:104)w, x(cid:105) + b.
the di   erent hypothesis classes of linear predictors are compositions of a func-
tion    : r     y on ld. for example, in binary classi   cation, we can choose    to
be the sign function, and for regression problems, where y = r,    is simply the
identity function.

it may be more convenient to incorporate b, called the bias, into w as an
extra coordinate and add an extra coordinate with a value of 1 to all x     x ;
namely, let w(cid:48) = (b, w1, w2, . . . wd)     rd+1 and let x(cid:48) = (1, x1, x2, . . . , xd)    

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

118

linear predictors

rd+1. therefore,

hw,b(x) = (cid:104)w, x(cid:105) + b = (cid:104)w(cid:48), x(cid:48)(cid:105).

it follows that each a   ne function in rd can be rewritten as a homogenous linear
function in rd+1 applied over the transformation that appends the constant 1
to each input vector. therefore, whenever it simpli   es the presentation, we will
omit the bias term and refer to ld as the class of homogenous linear functions
of the form hw(x) = (cid:104)w, x(cid:105).

throughout the book we often use the general term    linear functions    for both

a   ne functions and (homogenous) linear functions.

9.1

halfspaces

the    rst hypothesis class we consider is the class of halfspaces, designed for
binary classi   cation problems, namely, x = rd and y = {   1, +1}. the class of
halfspaces is de   ned as follows:

hsd = sign     ld = {x (cid:55)    sign(hw,b(x)) : hw,b     ld}.

in other words, each halfspace hypothesis in hsd is parameterized by w    
rd and b     r and upon receiving a vector x the hypothesis returns the label
sign((cid:104)w, x(cid:105) + b).

to illustrate this hypothesis class geometrically, it is instructive to consider
the case d = 2. each hypothesis forms a hyperplane that is perpendicular to the
vector w and intersects the vertical axis at the point (0,   b/w2). the instances
that are    above    the hyperplane, that is, share an acute angle with w, are labeled
positively. instances that are    below    the hyperplane, that is, share an obtuse
angle with w, are labeled negatively.

w

+

+

   

   

(cid:16) d+log(1/  )

(cid:17)

in section 9.1.3 we will show that vcdim(hsd) = d + 1. it follows that we
can learn halfspaces using the erm paradigm, as long as the sample size is
   
. therefore, we now discuss how to implement an erm procedure
for halfspaces.

 

we introduce below two solutions to    nding an erm halfspace in the realiz-
able case. in the context of halfspaces, the realizable case is often referred to as
the    separable    case, since it is possible to separate with a hyperplane all the
positive examples from all the negative examples. implementing the erm rule

9.1 halfspaces

119

in the nonseparable case (i.e., the agnostic case) is known to be computationally
hard (ben-david & simon 2001). there are several approaches to learning non-
separable data. the most popular one is to use surrogate id168s, namely,
to learn a halfspace that does not necessarily minimize the empirical risk with
the 0     1 loss, but rather with respect to a di   ferent id168. for example,
in section 9.3 we will describe the id28 approach, which can be
implemented e   ciently even in the nonseparable case. we will study surrogate
id168s in more detail later on in chapter 12.

9.1.1

id135 for the class of halfspaces

linear programs (lp) are problems that can be expressed as maximizing a linear
function subject to linear inequalities. that is,

(cid:104)u, w(cid:105)

max
w   rd
subject to

aw     v

where w     rd is the vector of variables we wish to determine, a is an m   
d matrix, and v     rm, u     rd are vectors. linear programs can be solved
e   ciently,1 and furthermore, there are publicly available implementations of lp
solvers.

we will show that the erm problem for halfspaces in the realizable case can
be expressed as a linear program. for simplicity, we assume the homogenous
case. let s = {(xi, yi)}m
i=1 be a training set of size m. since we assume the
realizable case, an erm predictor should have zero errors on the training set.
that is, we are looking for some vector w     rd for which
   i = 1, . . . , m.

sign((cid:104)w, xi(cid:105)) = yi,

equivalently, we are looking for some vector w for which
   i = 1, . . . , m.

yi(cid:104)w, xi(cid:105) > 0,

let w    be a vector that satis   es this condition (it must exist since we assume
realizability). de   ne    = mini(yi(cid:104)w   , xi(cid:105)) and let   w = w   
   . therefore, for all i
we have

yi(cid:104)   w, xi(cid:105) =

1
  

yi(cid:104)w   , xi(cid:105)     1.

we have thus shown that there exists a vector that satis   es

yi(cid:104)w, xi(cid:105)     1,

   i = 1, . . . , m.

(9.1)

and clearly, such a vector is an erm predictor.

to    nd a vector that satis   es equation (9.1) we can rely on an lp solver as
follows. set a to be the m    d matrix whose rows are the instances multiplied

1 namely, in time polynomial in m, d, and in the representation size of real numbers.

120

linear predictors

by yi. that is, ai,j = yi xi,j, where xi,j is the j   th element of the vector xi. let
v be the vector (1, . . . , 1)     rm. then, equation (9.1) can be rewritten as

aw     v.

the lp form requires a maximization objective, yet all the w that satisfy the
constraints are equal candidates as output hypotheses. thus, we set a    dummy   
objective, u = (0, . . . , 0)     rd.

9.1.2

id88 for halfspaces

a di   erent implementation of the erm rule is the id88 algorithm of
rosenblatt (rosenblatt 1958). the id88 is an iterative algorithm that
constructs a sequence of vectors w(1), w(2), . . .. initially, w(1) is set to be the
all-zeros vector. at iteration t, the id88    nds an example i that is mis-
labeled by w(t), namely, an example for which sign((cid:104)w(t), xi(cid:105)) (cid:54)= yi. then, the
id88 updates w(t) by adding to it the instance xi scaled by the label yi.
that is, w(t+1) = w(t) + yixi. recall that our goal is to have yi(cid:104)w, xi(cid:105) > 0 for
all i and note that

yi(cid:104)w(t+1), xi(cid:105) = yi(cid:104)w(t) + yixi, xi(cid:105) = yi(cid:104)w(t), xi(cid:105) + (cid:107)xi(cid:107)2.

hence, the update of the id88 guides the solution to be    more correct    on
the i   th example.

batch id88

input: a training set (x1, y1), . . . , (xm, ym)
initialize: w(1) = (0, . . . , 0)

for t = 1, 2, . . .
if (    i s.t. yi(cid:104)w(t), xi(cid:105)     0) then
w(t+1) = w(t) + yixi

else

output w(t)

the following theorem guarantees that in the realizable case, the algorithm

stops with all sample points correctly classi   ed.
theorem 9.1 assume that (x1, y1), . . . , (xm, ym) is separable, let b = min{(cid:107)w(cid:107) :
yi(cid:104)w, xi(cid:105)     1}, and let r = maxi (cid:107)xi(cid:107). then, the id88 al-
   i     [m],
gorithm stops after at most (rb)2 iterations, and when it stops it holds that
   i     [m], yi(cid:104)w(t), xi(cid:105) > 0.
proof by the de   nition of the stopping condition, if the id88 stops it
must have separated all the examples. we will show that if the id88 runs
for t iterations, then we must have t     (rb)2, which implies the id88
must stop after at most (rb)2 iterations.

let w(cid:63) be a vector that achieves the minimum in the de   nition of b. that is,

9.1 halfspaces

121

yi(cid:104)w(cid:63), xi(cid:105)     1 for all i, and among all vectors that satisfy these constraints, w(cid:63)
is of minimal norm.

the idea of the proof is to show that after performing t iterations, the cosine
   
of the angle between w(cid:63) and w(t +1) is at least
rb . that is, we will show that
   

t

(cid:104)w(cid:63), w(t +1)(cid:105)
(cid:107)w(cid:63)(cid:107)(cid:107)w(t +1)(cid:107)    

t
rb

.

(9.2)

by the cauchy-schwartz inequality, the left-hand side of equation (9.2) is at
most 1. therefore, equation (9.2) would imply that

   

t
rb

1    

    t     (rb)2,

which will conclude our proof.

to show that equation (9.2) holds, we    rst show that (cid:104)w(cid:63), w(t +1)(cid:105)     t .
indeed, at the    rst iteration, w(1) = (0, . . . , 0) and therefore (cid:104)w(cid:63), w(1)(cid:105) = 0,
while on iteration t, if we update using example (xi, yi) we have that

(cid:104)w(cid:63), w(t+1)(cid:105)     (cid:104)w(cid:63), w(t)(cid:105) = (cid:104)w(cid:63), w(t+1)     w(t)(cid:105)

= (cid:104)w(cid:63), yixi(cid:105) = yi(cid:104)w(cid:63), xi(cid:105)
    1.

therefore, after performing t iterations, we get:

t(cid:88)

(cid:16)(cid:104)w(cid:63), w(t+1)(cid:105)     (cid:104)w(cid:63), w(t)(cid:105)(cid:17)     t,

(cid:104)w(cid:63), w(t +1)(cid:105) =

t=1

as required.

next, we upper bound (cid:107)w(t +1)(cid:107). for each iteration t we have that

(cid:107)w(t+1)(cid:107)2 = (cid:107)w(t) + yixi(cid:107)2

= (cid:107)w(t)(cid:107)2 + 2yi(cid:104)w(t), xi(cid:105) + y2
    (cid:107)w(t)(cid:107)2 + r2

i (cid:107)xi(cid:107)2

(9.3)

(9.4)

where the last inequality is due to the fact that example i is necessarily such
that yi(cid:104)w(t), xi(cid:105)     0, and the norm of xi is at most r. now, since (cid:107)w(1)(cid:107)2 = 0,
if we use equation (9.4) recursively for t iterations, we obtain that

(cid:107)w(t +1)(cid:107)2     tr2     (cid:107)w(t +1)(cid:107)    

(9.5)
combining equation (9.3) with equation (9.5), and using the fact that (cid:107)w(cid:63)(cid:107) =
b, we obtain that

t r.

   

(cid:104)w(t +1), w(cid:63)(cid:105)
(cid:107)w(cid:63)(cid:107)(cid:107)w(t +1)(cid:107)    

   
t
t r

b

=

   

t
b r

.

we have thus shown that equation (9.2) holds, and this concludes our proof.

122

linear predictors

remark 9.1 the id88 is simple to implement and is guaranteed to con-
verge. however, the convergence rate depends on the parameter b, which in
some situations might be exponentially large in d. in such cases, it would be
better to implement the erm problem by solving a linear program, as described
in the previous section. nevertheless, for many natural data sets, the size of b
is not too large, and the id88 converges quite fast.

9.1.3

the vc dimension of halfspaces

to compute the vc dimension of halfspaces, we start with the homogenous case.
theorem 9.2 the vc dimension of the class of homogenous halfspaces in rd
is d.

proof first, consider the set of vectors e1, . . . , ed, where for every i the vector
ei is the all zeros vector except 1 in the i   th coordinate. this set is shattered
by the class of homogenous halfspaces. indeed, for every labeling y1, . . . , yd, set
w = (y1, . . . , yd), and then (cid:104)w, ei(cid:105) = yi for all i.

real numbers a1, . . . , ad+1, not all of them are zero, such that (cid:80)d+1

next, let x1, . . . , xd+1 be a set of d + 1 vectors in rd. then, there must exist
i=1 aixi = 0.
let i = {i : ai > 0} and j = {j : aj < 0}. either i or j is nonempty. let us
   rst assume that both of them are nonempty. then,
|aj|xj.

(cid:88)

(cid:88)

aixi =

i   i

j   j

now, suppose that x1, . . . , xd+1 are shattered by the class of homogenous classes.
then, there must exist a vector w such that (cid:104)w, xi(cid:105) > 0 for all i     i while
(cid:104)w, xj(cid:105) < 0 for every j     j. it follows that

(cid:42)(cid:88)

i   i

(cid:43)

(cid:42)(cid:88)

j   j

(cid:43)

(cid:88)

j   j

0 <

ai(cid:104)xi, w(cid:105) =

aixi, w

=

|aj|xj, w

=

|aj|(cid:104)xj, w(cid:105) < 0,

(cid:88)

i   i

which leads to a contradiction. finally, if j (respectively, i) is empty then the
right-most (respectively, left-most) inequality should be replaced by an equality,
which still leads to a contradiction.

theorem 9.3 the vc dimension of the class of nonhomogenous halfspaces in
rd is d + 1.

proof first, as in the proof of theorem 9.2, it is easy to verify that the set
of vectors 0, e1, . . . , ed is shattered by the class of nonhomogenous halfspaces.
second, suppose that the vectors x1, . . . , xd+2 are shattered by the class of non-
homogenous halfspaces. but, using the reduction we have shown in the beginning
of this chapter, it follows that there are d + 2 vectors in rd+1 that are shattered
by the class of homogenous halfspaces. but this contradicts theorem 9.2.

9.2 id75

123

r

r r

r

r

r r

rr
r r

figure 9.1 id75 for d = 1. for instance, the x-axis may denote the age of
the baby, and the y-axis her weight.

9.2

id75

id75 is a common statistical tool for modeling the relationship be-
tween some    explanatory    variables and some real valued outcome. cast as a
learning problem, the domain set x is a subset of rd, for some d, and the la-
bel set y is the set of real numbers. we would like to learn a linear function
h : rd     r that best approximates the relationship between our variables (say,
for example, predicting the weight of a baby as a function of her age and weight
at birth). figure 9.1 shows an example of a id75 predictor for d = 1.
the hypothesis class of id75 predictors is simply the set of linear

functions,

hreg = ld = {x (cid:55)    (cid:104)w, x(cid:105) + b : w     rd, b     r}.

next we need to de   ne a id168 for regression. while in classi   cation the
de   nition of the loss is straightforward, as (cid:96)(h, (x, y)) simply indicates whether
h(x) correctly predicts y or not, in regression, if the baby   s weight is 3 kg, both
the predictions 3.00001 kg and 4 kg are    wrong,    but we would clearly prefer
the former over the latter. we therefore need to de   ne how much we shall be
   penalized    for the discrepancy between h(x) and y. one common way is to use
the squared-id168, namely,

(cid:96)(h, (x, y)) = (h(x)     y)2.

for this id168, the empirical risk function is called the mean squared
error, namely,

m(cid:88)

i=1

ls(h) =

1
m

(h(xi)     yi)2.

124

linear predictors

in the next subsection, we will see how to implement the erm rule for linear
regression with respect to the squared loss. of course, there are a variety of other
id168s that one can use, for example, the absolute value id168,
(cid:96)(h, (x, y)) = |h(x)     y|. the erm rule for the absolute value id168 can
be implemented using id135 (see exercise 1.)

note that since id75 is not a binary prediction task, we cannot an-
alyze its sample complexity using the vc-dimension. one possible analysis of the
sample complexity of id75 is by relying on the    discretization trick   
(see remark 4.1 in chapter 4); namely, if we are happy with a representation of
each element of the vector w and the bias b using a    nite number of bits (say
a 64 bits    oating point representation), then the hypothesis class becomes    nite
and its size is at most 264(d+1). we can now rely on sample complexity bounds
for    nite hypothesis classes as described in chapter 4. note, however, that to
apply the sample complexity bounds from chapter 4 we also need that the loss
function will be bounded. later in the book we will describe more rigorous means
to analyze the sample complexity of regression problems.

9.2.1

least squares

least squares is the algorithm that solves the erm problem for the hypoth-
esis class of id75 predictors with respect to the squared loss. the
erm problem with respect to this class, given a training set s, and using the
homogenous version of ld, is to    nd

argmin

w

ls(hw) = argmin

w

1
m

((cid:104)w, xi(cid:105)     yi)2.

m(cid:88)

i=1

to solve the problem we calculate the gradient of the objective function and
compare it to zero. that is, we need to solve

m(cid:88)

i=1

2
m

((cid:104)w, xi(cid:105)     yi)xi = 0.

we can rewrite the problem as the problem aw = b where

(cid:33)

(cid:32) m(cid:88)

i=1

a =

xi x(cid:62)

i

m(cid:88)

i=1

and

b =

yixi.

(9.6)

or, in matrix form:

a =

b =

             ...
             ...

x1
...

x1
...

9.2 id75

125

            
            

...
. . . xm
...
...
. . . xm
...

             ...
          y1

x1
...

...
ym

(cid:62)

            

,

...
. . . xm
...

          .

(9.7)

(9.8)

if a is invertible then the solution to the erm problem is

w = a   1 b.

the case in which a is not invertible requires a few standard tools from linear
algebra, which are available in appendix c. it can be easily shown that if the
training instances do not span the entire space of rd then a is not invertible.
nevertheless, we can always    nd a solution to the system aw = b because b
is in the range of a. indeed, since a is symmetric we can write it using its
eigenvalue decomposition as a = v dv (cid:62), where d is a diagonal matrix and v
is an orthonormal matrix (that is, v (cid:62)v is the identity d    d matrix). de   ne
d+ to be the diagonal matrix such that d+
i,i = 0 if di,i = 0 and otherwise
d+

i,i = 1/di,i. now, de   ne

a+ = v d+v (cid:62)

and

  w = a+b.

let vi denote the i   th column of v . then, we have

a   w = aa+b = v dv (cid:62)v d+v (cid:62)b = v dd+v (cid:62)b =

(cid:88)

i:di,i(cid:54)=0

viv(cid:62)

i b.

that is, a   w is the projection of b onto the span of those vectors vi for which
di,i (cid:54)= 0. since the linear span of x1, . . . , xm is the same as the linear span of
those vi, and b is in the linear span of the xi, we obtain that a   w = b, which
concludes our argument.

9.2.2

id75 for polynomial regression tasks

some learning tasks call for nonlinear predictors, such as polynomial predictors.
take, for instance, a one dimensional polynomial function of degree n, that is,

p(x) = a0 + a1x + a2x2 +        + anxn

where (a0, . . . , an) is a vector of coe   cients of size n + 1. in the following we
depict a training set that is better    tted using a 3rd degree polynomial predictor
than using a linear predictor.

126

linear predictors

we will focus here on the class of one dimensional, n-degree, polynomial re-

gression predictors, namely,

hn
poly = {x (cid:55)    p(x)},

where p is a one dimensional polynomial of degree n, parameterized by a vector
of coe   cients (a0, . . . , an). note that x = r, since this is a one dimensional
polynomial, and y = r, as this is a regression problem.

one way to learn this class is by reduction to the problem of id75,
which we have already shown how to solve. to translate a polynomial regression
problem to a id75 problem, we de   ne the mapping    : r     rn+1
such that   (x) = (1, x, x2, . . . , xn). then we have that

p(  (x)) = a0 + a1x + a2x2 +        + anxn = (cid:104)a,   (x)(cid:105)

and we can    nd the optimal vector of coe   cients a by using the least squares
algorithm as shown earlier.

9.3

id28

in id28 we learn a family of functions h from rd to the interval [0, 1].
however, id28 is used for classi   cation tasks: we can interpret h(x)
as the id203 that the label of x is 1. the hypothesis class associated with
id28 is the composition of a sigmoid function   sig : r     [0, 1] over
the class of linear functions ld. in particular, the sigmoid function used in logistic
regression is the logistic function, de   ned as

  sig(z) =

1

1 + exp(   z)

.

(9.9)

the name    sigmoid    means    s-shaped,    referring to the plot of this function,
shown in the    gure:

9.3 id28

127

the hypothesis class is therefore (where for simplicity we are using homogenous

linear functions):

hsig =   sig     ld = {x (cid:55)      sig((cid:104)w, x(cid:105)) : w     rd}.

note that when (cid:104)w, x(cid:105) is very large then   sig((cid:104)w, x(cid:105)) is close to 1, whereas if
(cid:104)w, x(cid:105) is very small then   sig((cid:104)w, x(cid:105)) is close to 0. recall that the prediction of the
halfspace corresponding to a vector w is sign((cid:104)w, x(cid:105)). therefore, the predictions
of the halfspace hypothesis and the logistic hypothesis are very similar whenever
|(cid:104)w, x(cid:105)| is large. however, when |(cid:104)w, x(cid:105)| is close to 0 we have that   sig((cid:104)w, x(cid:105))    
1
2 . intuitively, the logistic hypothesis is not sure about the value of the label so it
guesses that the label is sign((cid:104)w, x(cid:105)) with id203 slightly larger than 50%.
in contrast, the halfspace hypothesis always outputs a deterministic prediction
of either 1 or    1, even if |(cid:104)w, x(cid:105)| is very close to 0.

next, we need to specify a id168. that is, we should de   ne how bad it
is to predict some hw(x)     [0, 1] given that the true label is y     {  1}. clearly,
we would like that hw(x) would be large if y = 1 and that 1     hw(x) (i.e., the
id203 of predicting    1) would be large if y =    1. note that

1     hw(x) = 1    

1

1 + exp(   (cid:104)w, x(cid:105))

exp(   (cid:104)w, x(cid:105))
1 + exp(   (cid:104)w, x(cid:105))

=

=

1

1 + exp((cid:104)w, x(cid:105))

.

therefore, any reasonable id168 would increase monotonically with
or equivalently, would increase monotonically with 1 + exp(   y(cid:104)w, x(cid:105)). the lo-
gistic id168 used in id28 penalizes hw based on the log of
1 + exp(   y(cid:104)w, x(cid:105)) (recall that log is a monotonic function). that is,

1

1+exp(y(cid:104)w,x(cid:105)) ,

(cid:96)(hw, (x, y)) = log (1 + exp(   y(cid:104)w, x(cid:105))) .

therefore, given a training set s = (x1, y1), . . . , (xm, ym), the erm problem
associated with id28 is

m(cid:88)

i=1

argmin
w   rd

1
m

log (1 + exp(   yi(cid:104)w, xi(cid:105))) .

(9.10)

the advantage of the logistic id168 is that it is a convex function with
respect to w; hence the erm problem can be solved e   ciently using standard
methods. we will study how to learn with convex functions, and in particular
specify a simple algorithm for minimizing convex functions, in later chapters.

the erm problem associated with id28 (equation (9.10)) is iden-
tical to the problem of    nding a maximum likelihood estimator, a well-known
statistical approach for    nding the parameters that maximize the joint probabil-
ity of a given data set assuming a speci   c parametric id203 function. we
will study the maximum likelihood approach in chapter 24.

128

linear predictors

9.4

summary

the family of linear predictors is one of the most useful families of hypothesis
classes, and many learning algorithms that are being widely used in practice
rely on linear predictors. we have shown e   cient algorithms for learning linear
predictors with respect to the zero-one loss in the separable case and with respect
to the squared and logistic losses in the unrealizable case. in later chapters we
will present the properties of the id168 that enable e   cient learning.

naturally, linear predictors are e   ective whenever we assume, as prior knowl-
edge, that some linear predictor attains low risk with respect to the underlying
distribution. in the next chapter we show how to construct nonlinear predictors
by composing linear predictors on top of simple classes. this will enable us to
employ linear predictors for a variety of prior knowledge assumptions.

9.5

bibliographic remarks

the id88 algorithm dates back to rosenblatt (1958). the proof of its
convergence rate is due to (agmon 1954, noviko    1962). least squares regression
goes back to gauss (1795), legendre (1805), and adrain (1808).

9.6

exercises

1. show how to cast the erm problem of id75 with respect to the
absolute value id168, (cid:96)(h, (x, y)) = |h(x)     y|, as a linear program;
namely, show how to write the problem

m(cid:88)

i=1

min

w

|(cid:104)w, xi(cid:105)     yi|

as a linear program.
hint: start with proving that for any c     r,

|c| = min
a   0

a s.t. c     a and c        a.

2. show that the matrix a de   ned in equation (9.6) is invertible if and only if

x1, . . . , xm span rd.
3. show that theorem 9.1 is tight in the following sense: for any positive integer
m, there exist a vector w        rd (for some appropriate d) and a sequence of
examples {(x1, y1), . . . , (xm, ym)} such that the following hold:
    r = maxi (cid:107)xi(cid:107)     1.
    (cid:107)w   (cid:107)2 = m, and for all i     m, yi(cid:104)xi, w   (cid:105)     1. note that, using the notation

in theorem 9.1, we therefore get

b = min{(cid:107)w(cid:107) :    i     [m], yi(cid:104)w, xi(cid:105)     1}        

m.

9.6 exercises

129

thus, (br)2     m.

    when running the id88 on this sequence of examples it makes m

updates before converging.

hint: choose d = m and for every i choose xi = ei.
4. (*) given any number m,    nd an example of a sequence of labeled examples
((x1, y1), . . . , (xm, ym))     (r3    {   1, +1})m on which the upper bound of
theorem 9.1 equals m and the id88 algorithm is bound to make m
mistakes.
hint: set each xi to be a third dimensional vector of the form (a, b, yi), where
a2 + b2 = r2     1. let w    be the vector (0, 0, 1). now, go over the proof of
the id88   s upper bound (theorem 9.1), see where we used inequalities
(   ) rather than equalities (=), and    gure out scenarios where the inequality
actually holds with equality.

5. suppose we modify the id88 algorithm as follows: in the update step,
instead of performing w(t+1) = w(t) + yixi whenever we make a mistake, we
perform w(t+1) = w(t) +   yixi for some    > 0. prove that the modi   ed per-
ceptron will perform the same number of iterations as the vanilla id88
and will converge to a vector that points to the same direction as the output
of the vanilla id88.

6. in this problem, we will get bounds on the vc-dimension of the class of

(closed) balls in rd, that is,

where

bv,r(x) =

bd = {bv,r : v     rd, r > 0},

(cid:26) 1

0

if (cid:107)x     v(cid:107)     r
otherwise

.

1. consider the mapping    : rd     rd+1 de   ned by   (x) = (x,(cid:107)x(cid:107)2). show
that if x1, . . . , xm are shattered by bd then   (x1), . . . ,   (xm) are shattered
by the class of halfspaces in rd+1 (in this question we assume that sign(0) =
1). what does this tell us about vcdim(bd)?
2. (*) find a set of d + 1 points in rd that is shattered by bd. conclude that

d + 1     vcdim(bd)     d + 2.

10 boosting

boosting is an algorithmic paradigm that grew out of a theoretical question and
became a very practical machine learning tool. the boosting approach uses a
generalization of linear predictors to address two major issues that have been
raised earlier in the book. the    rst is the bias-complexity tradeo   . we have
seen (in chapter 5) that the error of an erm learner can be decomposed into
a sum of approximation error and estimation error. the more expressive the
hypothesis class the learner is searching over, the smaller the approximation
error is, but the larger the estimation error becomes. a learner is thus faced with
the problem of picking a good tradeo    between these two considerations. the
boosting paradigm allows the learner to have smooth control over this tradeo   .
the learning starts with a basic class (that might have a large approximation
error), and as it progresses the class that the predictor may belong to grows
richer.

the second issue that boosting addresses is the computational complexity of
learning. as seen in chapter 8, for many interesting concept classes the task
of    nding an erm hypothesis may be computationally infeasible. a boosting
algorithm ampli   es the accuracy of weak learners. intuitively, one can think of
a weak learner as an algorithm that uses a simple    rule of thumb    to output a
hypothesis that comes from an easy-to-learn hypothesis class and performs just
slightly better than a random guess. when a weak learner can be implemented
e   ciently, boosting provides a tool for aggregating such weak hypotheses to
approximate gradually good predictors for larger, and harder to learn, classes.

in this chapter we will describe and analyze a practically useful boosting algo-
rithm, adaboost (a shorthand for adaptive boosting). the adaboost algorithm
outputs a hypothesis that is a linear combination of simple hypotheses. in other
words, adaboost relies on the family of hypothesis classes obtained by composing
a linear predictor on top of simple classes. we will show that adaboost enables
us to control the tradeo    between the approximation and estimation errors by
varying a single parameter.

adaboost demonstrates a general theme, that will recur later in the book, of
expanding the expressiveness of linear predictors by composing them on top of
other functions. this will be elaborated in section 10.3.

adaboost stemmed from the theoretical question of whether an e   cient weak
learner can be    boosted    into an e   cient strong learner. this question was raised

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

10.1 weak learnability

131

by kearns and valiant in 1988 and solved in 1990 by robert schapire, then
a graduate student at mit. however, the proposed mechanism was not very
practical. in 1995, robert schapire and yoav freund proposed the adaboost
algorithm, which was the    rst truly practical implementation of boosting. this
simple and elegant algorithm became hugely popular, and freund and schapire   s
work has been recognized by numerous awards.

furthermore, boosting is a great example for the practical impact of learning
theory. while boosting originated as a purely theoretical problem, it has led to
popular and widely used algorithms. indeed, as we shall demonstrate later in
this chapter, adaboost has been successfully used for learning to detect faces in
images.

10.1 weak learnability

recall the de   nition of pac learning given in chapter 3: a hypothesis class,
h, is pac learnable if there exist mh : (0, 1)2     n and a learning algorithm
with the following property: for every  ,        (0, 1), for every distribution d over
x , and for every labeling function f : x     {  1}, if the realizable assumption
holds with respect to h,d, f , then when running the learning algorithm on
m     mh( ,   ) i.i.d. examples generated by d and labeled by f , the algorithm
returns a hypothesis h such that, with id203 of at least 1     , l(d,f )(h)      .
furthermore, the fundamental theorem of learning theory (theorem 6.8 in
chapter 6) characterizes the family of learnable classes and states that every pac
learnable class can be learned using any erm algorithm. however, the de   nition
of pac learning and the fundamental theorem of learning theory ignores the
computational aspect of learning. indeed, as we have shown in chapter 8, there
are cases in which implementing the erm rule is computationally hard (even in
the realizable case).
however, perhaps we can trade computational hardness with the requirement
for accuracy. given a distribution d and a target labeling function f , maybe there
exists an e   ciently computable learning algorithm whose error is just slightly
better than a random guess? this motivates the following de   nition.

definition 10.1 (  -weak-learnability)
    a learning algorithm, a, is a   -weak-learner for a class h if there exists a func-
tion mh : (0, 1)     n such that for every        (0, 1), for every distribution
d over x , and for every labeling function f : x     {  1}, if the realizable
assumption holds with respect to h,d, f , then when running the learning
algorithm on m     mh(  ) i.i.d. examples generated by d and labeled by f ,
the algorithm returns a hypothesis h such that, with id203 of at least
1       , l(d,f )(h)     1/2       .
    a hypothesis class h is   -weak-learnable if there exists a   -weak-learner for

that class.

132

boosting

this de   nition is almost identical to the de   nition of pac learning, which
here we will call strong learning, with one crucial di   erence: strong learnability
implies the ability to    nd an arbitrarily good classi   er (with error rate at most
  for an arbitrarily small   > 0). in weak learnability, however, we only need to
output a hypothesis whose error rate is at most 1/2       , namely, whose error
rate is slightly better than what a random labeling would give us. the hope is
that it may be easier to come up with e   cient weak learners than with e   cient
(full) pac learners.

 

d+log(1/  )

the fundamental theorem of learning (theorem 6.8) states that if a hypothesis
class h has a vc dimension d, then the sample complexity of pac learning h
satis   es mh( ,   )     c1
, where c1 is a constant. applying this with
  = 1/2      we immediately obtain that if d =     then h is not   -weak-learnable.
this implies that from the statistical perspective (i.e., if we ignore computational
complexity), weak learnability is also characterized by the vc dimension of h
and therefore is just as hard as pac (strong) learning. however, when we do
consider computational complexity, the potential advantage of weak learning is
that maybe there is an algorithm that satis   es the requirements of weak learning
and can be implemented e   ciently.

one possible approach is to take a    simple    hypothesis class, denoted b, and
to apply erm with respect to b as the weak learning algorithm. for this to
work, we need that b will satisfy two requirements:
    ermb is e   ciently implementable.
    for every sample that is labeled by some hypothesis from h, any ermb

hypothesis will have an error of at most 1/2       .

then, the immediate question is whether we can boost an e   cient weak learner
into an e   cient strong learner. in the next section we will show that this is
indeed possible, but before that, let us show an example in which e   cient weak
learnability of a class h is possible using a base hypothesis class b.
example 10.1 (weak learning of 3-piece classi   ers using decision stumps)
let x = r and let h be the class of 3-piece classi   ers, namely, h = {h  1,  2,b :
  1,   2     r,   1 <   2, b     {  1}}, where for every x,

(cid:40)

h  1,  2,b(x) =

+b
   b

if x <   1 or x >   2
if   1     x       2

an example hypothesis (for b = 1) is illustrated as follows:

+

  1

   

+

  2

let b be the class of decision stumps, that is, b = {x (cid:55)    sign(x      )   b :       
r, b     {  1}}. in the following we show that ermb is a   -weak learner for h,
for    = 1/12.

10.1 weak learnability

133

to see that, we    rst show that for every distribution that is consistent with
h, there exists a decision stump with ld(h)     1/3. indeed, just note that
every classi   er in h consists of three regions (two unbounded rays and a center
interval) with alternate labels. for any pair of such regions, there exists a decision
stump that agrees with the labeling of these two components. note that for every
distribution d over r and every partitioning of the line into three such regions,
one of these regions must have d-weight of at most 1/3. let h     h be a zero
error hypothesis. a decision stump that disagrees with h only on such a region
has an error of at most 1/3.
finally, since the vc-dimension of decision stumps is 2, if the sample size is
greater than    (log(1/  )/ 2), then with id203 of at least 1       , the ermb
rule returns a hypothesis with an error of at most 1/3 +  . setting   = 1/12 we
obtain that the error of ermb is at most 1/3 + 1/12 = 1/2     1/12.
we see that ermb is a   -weak learner for h. we next show how to implement

the erm rule e   ciently for decision stumps.

10.1.1

e   cient implementation of erm for decision stumps
let x = rd and consider the base hypothesis class of decision stumps over rd,
namely,

hds = {x (cid:55)    sign(       xi)    b :        r, i     [d], b     {  1}}.

for simplicity, assume that b = 1; that is, we focus on all the hypotheses in
hds of the form sign(       xi). let s = ((x1, y1), . . . , (xm, ym)) be a training set.
we will show how to implement an erm rule, namely, how to    nd a decision
stump that minimizes ls(h). furthermore, since in the next section we will
show that adaboost requires    nding a hypothesis with a small risk relative to
some distribution over s, we will show here how to minimize such risk functions.
concretely, let d be a id203 vector in rm (that is, all elements of d are
i di = 1). the weak learner we describe later receives d and
s and outputs a decision stump h : x     y that minimizes the risk w.r.t. d,

nonnegative and(cid:80)

m(cid:88)

i=1

ld(h) =

di1[h(xi)(cid:54)=yi].

note that if d = (1/m, . . . , 1/m) then ld(h) = ls(h).

recall that each decision stump is parameterized by an index j     [d] and a

threshold   . therefore, minimizing ld(h) amounts to solving the problem

       (cid:88)

i:yi=1

(cid:88)

i:yi=   1

       .

min
j   [d]

min
     r

di1[xi,j >  ] +

di1[xi,j     ]

(10.1)

fix j     [d] and let us sort the examples so that x1,j     x2,j     . . .     xm,j. de   ne
: i     [m    1]}   {(x1,j     1), (xm,j + 1)}. note that for any        r
  j = { xi,j +xi+1,j
there exists   (cid:48)       j that yields the same predictions for the sample s as the

2

134

boosting

threshold   . therefore, instead of minimizing over        r we can minimize over
         j.
this already gives us an e   cient procedure: choose j     [d] and          j that
minimize the objective value of equation (10.1). for every j and          j we
have to calculate a sum over m examples; therefore the runtime of this approach
would be o(dm2). we next show a simple trick that enables us to minimize the
objective in time o(dm).
the observation is as follows. suppose we have calculated the objective for
       (xi   1,j, xi,j). let f (  ) be the value of the objective. then, when we consider
  (cid:48)     (xi,j, xi+1,j) we have that

f (  (cid:48)) = f (  )     di1[yi=1] + di1[yi=   1] = f (  )     yidi.

therefore, we can calculate the objective at   (cid:48) in a constant time, given the
objective at the previous threshold,   . it follows that after a preprocessing step
in which we sort the examples with respect to each coordinate, the minimization
problem can be performed in time o(dm). this yields the following pseudocode.

erm for decision stumps

input:

training set s = (x1, y1), . . . , (xm, ym)
distribution vector d

goal: find j(cid:63),   (cid:63) that solve equation (10.1)
initialize: f (cid:63) =    
for j = 1, . . . , d

sort s using the j   th coordinate, and denote
x1,j     x2,j                xm,j     xm+1,j

def= xm,j + 1

f =(cid:80)

i:yi=1 di

if f < f (cid:63)
f (cid:63) = f ,   (cid:63) = x1,j     1, j(cid:63) = j
for i = 1, . . . , m
f = f     yidi
if f < f (cid:63) and xi,j (cid:54)= xi+1,j
f (cid:63) = f ,   (cid:63) = 1

2 (xi,j + xi+1,j), j(cid:63) = j

output j(cid:63),   (cid:63)

10.2

adaboost

adaboost (short for adaptive boosting) is an algorithm that has access to a
weak learner and    nds a hypothesis with a low empirical risk. the adaboost
algorithm receives as input a training set of examples s = (x1, y1), . . . , (xm, ym),
where for each i, yi = f (xi) for some labeling function f . the boosting process
proceeds in a sequence of consecutive rounds. at round t, the booster    rst de   nes

10.2 adaboost

135

m(cid:88)

(cid:80)m

i=1 d(t)

a distribution over the examples in s, denoted d(t). that is, d(t)     rm

+ and
i = 1. then, the booster passes the distribution d(t) and the sample s
to the weak learner. (that way, the weak learner can construct i.i.d. examples
according to d(t) and f .) the weak learner is assumed to return a    weak   
hypothesis, ht, whose error,

 t

def= ld(t) (ht) def=

d(t)

i

1[ht(xi)(cid:54)=yi],

i=1

(cid:17)
2      (of course, there is a id203 of at most    that the weak learner
    1

is at most 1
fails). then, adaboost assigns a weight for ht as follows: wt = 1
.
that is, the weight of ht is inversely proportional to the error of ht. at the end
of the round, adaboost updates the distribution so that examples on which ht
errs will get a higher id203 mass while examples on which ht is correct will
get a lower id203 mass. intuitively, this will force the weak learner to focus
on the problematic examples in the next round. the output of the adaboost
algorithm is a    strong    classi   er that is based on a weighted sum of all the weak
hypotheses. the pseudocode of adaboost is presented in the following.

(cid:16) 1

2 log

 t

adaboost

input:

training set s = (x1, y1), . . . , (xm, ym)
weak learner wl
number of rounds t

invoke weak learner ht = wl(d(t), s)

m , . . . , 1

m ).

initialize d(1) = ( 1
for t = 1, . . . , t :

compute  t =(cid:80)m
(cid:16) 1
i=1 d(t)
    1
(cid:80)m

let wt = 1
2 log
update d(t+1)

(cid:17)

=

 t

i

i

i

1[yi(cid:54)=ht(xi)]

d(t)
j=1 d(t)

j

exp(   wtyiht(xi))

exp(   wtyj ht(xj ))

(cid:16)(cid:80)t

output the hypothesis hs(x) = sign

t=1 wtht(x)

.

for all i = 1, . . . , m

(cid:17)

the following theorem shows that the training error of the output hypothesis

decreases exponentially fast with the number of boosting rounds.

theorem 10.2 let s be a training set and assume that at each iteration of
adaboost, the weak learner returns a hypothesis for which  t     1/2       . then,
the training error of the output hypothesis of adaboost is at most

m(cid:88)
proof for each t, denote ft =(cid:80)

ls(hs) =

1
m

i=1

1[hs(xi)(cid:54)=yi]     exp(   2   2 t ) .

p   t wphp. therefore, the output of adaboost

136

boosting

is ft . in addition, denote

m(cid:88)

i=1

zt =

1
m

e   yift(xi).

note that for any hypothesis we have that 1[h(x)(cid:54)=y]     e   yh(x). therefore, ls(ft )    
zt , so it su   ces to show that zt     e   2  2t . to upper bound zt we rewrite it
as

zt =

(10.2)
where we used the fact that z0 = 1 because f0     0. therefore, it su   ces to show
that for every round t,

=

,

zt
zt   1

   zt   1
zt   2

       z2
z1

   z1
z0

zt
z0

    e   2  2

.

zt+1
zt

(10.3)

to do so, we    rst note that using a simple inductive argument, for all t and i,

hence,

zt+1
zt

d(t+1)

i

=

(cid:80)m

e   yift(xi)
j=1 e   yj ft(xj )

.

i

=

=

=

i=1

j=1

j=1

d(t+1)

e   yj ft(xj )

e   yiwt+1ht+1(xi)

(cid:80)m
m(cid:80)
i=1 e   yift+1(xi)
(cid:80)m
e   yj ft(xj )
m(cid:80)
i=1 e   yift(xi)e   yiwt+1ht+1(xi)
m(cid:88)
= e   wt+1 (cid:88)
1(cid:112)1/ t+1     1
(cid:114)  t+1
= 2(cid:112) t+1(1      t+1).
(cid:115)(cid:18) 1
2(cid:112) t+1(1      t+1)     2

= e   wt+1(1      t+1) + ewt+1 t+1

(1      t+1) +

1      t+1

i:yiht+1(xi)=1

(cid:115)

      

=

=

2

(cid:19)(cid:18) 1

2

d(t+1)

+ ewt+1 (cid:88)
(1      t+1) +(cid:112)1/ t+1     1  t+1

i

d(t+1)

i

i:yiht+1(xi)=   1

1      t+1
 t+1

 t+1

by our assumption,  t+1     1
tonically increasing in [0, 1/2], we obtain that

2       . since the function g(a) = a(1     a) is mono-

(cid:19)

(cid:112)

+   

=

1     4  2.

10.3 linear combinations of base hypotheses

137

finally, using the inequality 1     a     e   a we have that (cid:112)1     4  2     e   4  2/2 =

. this shows that equation (10.3) holds and thus concludes our proof.

e   2  2

each iteration of adaboost involves o(m) operations as well as a single call to
the weak learner. therefore, if the weak learner can be implemented e   ciently
(as happens in the case of erm with respect to decision stumps) then the total
training process will be e   cient.

remark 10.2 theorem 10.2 assumes that at each iteration of adaboost, the
weak learner returns a hypothesis with weighted sample error of at most 1/2      .
according to the de   nition of a weak learner, it can fail with id203   . using
the union bound, the id203 that the weak learner will not fail at all of the
iterations is at least 1       t . as we show in exercise 1, the dependence of the
sample complexity on    can always be logarithmic in 1/  , and therefore invoking
the weak learner with a very small    is not problematic. we can therefore assume
that   t is also small. furthermore, since the weak learner is only applied with
distributions over the training set, in many cases we can implement the weak
learner so that it will have a zero id203 of failure (i.e.,    = 0). this is the
case, for example, in the weak learner that    nds the minimum value of ld(h)
for decision stumps, as described in the previous section.

theorem 10.2 tells us that the empirical risk of the hypothesis constructed by
adaboost goes to zero as t grows. however, what we really care about is the
true risk of the output hypothesis. to argue about the true risk, we note that the
output of adaboost is in fact a composition of a halfspace over the predictions
of the t weak hypotheses constructed by the weak learner. in the next section
we show that if the weak hypotheses come from a base hypothesis class of low
vc-dimension, then the estimation error of adaboost will be small; namely, the
true risk of the output of adaboost would not be very far from its empirical risk.

10.3

linear combinations of base hypotheses

as mentioned previously, a popular approach for constructing a weak learner
is to apply the erm rule with respect to a base hypothesis class (e.g., erm
over decision stumps). we have also seen that boosting outputs a composition
of a halfspace over the predictions of the weak hypotheses. therefore, given a
base hypothesis class b (e.g., decision stumps), the output of adaboost will be
a member of the following class:

(cid:40)

(cid:32) t(cid:88)

(cid:33)

l(b, t ) =

x (cid:55)    sign

wtht(x)

: w     rt ,    t,

ht     b

.

(10.4)

t=1

that is, each h     l(b, t ) is parameterized by t base hypotheses from b and
by a vector w     rt . the prediction of such an h on an instance x is ob-
tained by    rst applying the t base hypotheses to construct the vector   (x) =

(cid:41)

138

boosting

(h1(x), . . . , ht (x))     rt , and then applying the (homogenous) halfspace de   ned
by w on   (x).

in this section we analyze the estimation error of l(b, t ) by bounding the
vc-dimension of l(b, t ) in terms of the vc-dimension of b and t . we will
show that, up to logarithmic factors, the vc-dimension of l(b, t ) is bounded
by t times the vc-dimension of b. it follows that the estimation error of ad-
aboost grows linearly with t . on the other hand, the empirical risk of adaboost
decreases with t . in fact, as we demonstrate later, t can be used to decrease
the approximation error of l(b, t ). therefore, the parameter t of adaboost
enables us to control the bias-complexity tradeo   .
the simple example, in which x = r and the base class is decision stumps,

to demonstrate how the expressive power of l(b, t ) increases with t , consider

hds1 = {x (cid:55)    sign(x       )    b :

       r, b     {  1}}.

r(cid:88)

note that in this one dimensional case, hds1 is in fact equivalent to (nonho-
mogenous) halfspaces on r.
now, let h be the rather complex class (compared to halfspaces on the line)
of piece-wise constant functions. let gr be a piece-wise constant function with at
most r pieces; that is, there exist thresholds        =   0 <   1 <   2 <        <   r =    
such that

gr(x) =

  i1[x   (  i   1,  i]]    i,   i     {  1}.

i=1

denote by gr the class of all such piece-wise constant classi   ers with at most r
pieces.
in the following we show that gt     l(hds1, t ); namely, the class of halfspaces
over t decision stumps yields all the piece-wise constant classi   ers with at most
t pieces.
indeed, without loss of generality consider any g     gt with   t = (   1)t. this
implies that if x is in the interval (  t   1,   t], then g(x) = (   1)t. for example:

now, the function

(cid:32) t(cid:88)

h(x) = sign

(cid:33)

wt sign(x       t   1)

,

(10.5)

where w1 = 0.5 and for t > 1, wt = (   1)t, is in l(hds1, t ) and is equal to g
(see exercise 2).

t=1

10.3 linear combinations of base hypotheses

139

from this example we obtain that l(hds1, t ) can shatter any set of t + 1
instances in r; hence the vc-dimension of l(hds1, t ) is at least t +1. therefore,
t is a parameter that can control the bias-complexity tradeo   : enlarging t
yields a more expressive hypothesis class but on the other hand might increase
the estimation error. in the next subsection we formally upper bound the vc-
dimension of l(b, t ) for any base class b.

10.3.1

the vc-dimension of l(b, t )

the following lemma tells us that the vc-dimension of l(b, t ) is upper bounded
by   o(vcdim(b) t ) (the   o notation ignores constants and logarithmic factors).

lemma 10.3 let b be a base class and let l(b, t ) be as de   ned in equa-
tion (10.4). assume that both t and vcdim(b) are at least 3. then,

vcdim(l(b, t ))     t (vcdim(b) + 1) (3 log(t (vcdim(b) + 1)) + 2).

proof denote d = vcdim(b). let c = {x1, . . . , xm} be a set that is shat-
tered by l(b, t ). each labeling of c by h     l(b, t ) is obtained by    rst choos-
ing h1, . . . , ht     b and then applying a halfspace hypothesis over the vector
(h1(x), . . . , ht (x)). by sauer   s lemma, there are at most (em/d)d di   erent di-
chotomies (i.e., labelings) induced by b over c. therefore, we need to choose
t hypotheses, out of at most (em/d)d di   erent hypotheses. there are at most
(em/d)dt ways to do it. next, for each such choice, we apply a linear predictor,
which yields at most (em/t )t dichotomies. therefore, the overall number of
dichotomies we can construct is upper bounded by

(em/d)dt (em/t )t     m(d+1)t ,

where we used the assumption that both d and t are at least 3. since we assume
that c is shattered, we must have that the preceding is at least 2m, which yields

therefore,

2m     m(d+1)t .

m     log(m)

(d + 1)t
log(2)

.

lemma a.1 in chapter a tells us that a necessary condition for the above to
hold is that

m     2

(d + 1)t
log(2)

log

(d + 1)t
log(2)

    (d + 1)t (3 log((d + 1)t ) + 2),

which concludes our proof.

in exercise 4 we show that for some base classes, b, it also holds that vcdim(l(b, t ))    

   (vcdim(b) t ).

140

boosting

a

c

b

d

figure 10.1 the four types of functions, g, used by the base hypotheses for face
recognition. the value of g for type a or b is the di   erence between the sum of the
pixels within two rectangular regions. these regions have the same size and shape and
are horizontally or vertically adjacent. for type c, the value of g is the sum within
two outside rectangles subtracted from the sum in a center rectangle. for type d, we
compute the di   erence between diagonal pairs of rectangles.

10.4

adaboost for face recognition

we now turn to a base hypothesis that has been proposed by viola and jones for
the task of face recognition. in this task, the instance space is images, represented
as matrices of gray level values of pixels. to be concrete, let us take images of
size 24    24 pixels, and therefore our instance space is the set of real valued
matrices of size 24    24. the goal is to learn a classi   er, h : x     {  1}, that
given an image as input, should output whether the image is of a human face or
not.
each hypothesis in the base class is of the form h(x) = f (g(x)), where f is a
decision stump hypothesis and g : r24,24     r is a function that maps an image
to a scalar. each function g is parameterized by
    an axis aligned rectangle r. since each image is of size 24    24, there are at

most 244 axis aligned rectangles.

    a type, t     {a, b, c, d}. each type corresponds to a mask, as depicted in

figure 10.1.

to calculate g we stretch the mask t to    t the rectangle r and then calculate
the sum of the pixels (that is, sum of their gray level values) that lie within the
red rectangles and subtract it from the sum of pixels in the blue rectangles.
since the number of such functions g is at most 244    4, we can implement a
weak learner for the base hypothesis class by    rst calculating all the possible
outputs of g on each image, and then apply the weak learner of decision stumps
described in the previous subsection. it is possible to perform the    rst step very

10.5 summary

141

figure 10.2 the    rst and second features selected by adaboost, as implemented by
viola and jones. the two features are shown in the top row and then overlaid on a
typical training face in the bottom row. the    rst feature measures the di   erence in
intensity between the region of the eyes and a region across the upper cheeks. the
feature capitalizes on the observation that the eye region is often darker than the
cheeks. the second feature compares the intensities in the eye regions to the intensity
across the bridge of the nose.

e   ciently by a preprocessing step in which we calculate the integral image of
each image in the training set. see exercise 5 for details.

in figure 10.2 we depict the    rst two features selected by adaboost when

running it with the base features proposed by viola and jones.

10.5

summary

boosting is a method for amplifying the accuracy of weak learners. in this chapter
we described the adaboost algorithm. we have shown that after t iterations of
adaboost, it returns a hypothesis from the class l(b, t ), obtained by composing
a linear classi   er on t hypotheses from a base class b. we have demonstrated
how the parameter t controls the tradeo    between approximation and estimation
errors. in the next chapter we will study how to tune parameters such as t , based
on the data.

10.6

bibliographic remarks

as mentioned before, boosting stemmed from the theoretical question of whether
an e   cient weak learner can be    boosted    into an e   cient strong learner (kearns
& valiant 1988) and solved by schapire (1990). the adaboost algorithm has
been proposed in freund & schapire (1995).

boosting can be viewed from many perspectives. in the purely theoretical
context, adaboost can be interpreted as a negative result: if strong learning of
a hypothesis class is computationally hard, so is weak learning of this class. this
negative result can be useful for showing hardness of agnostic pac learning of
a class b based on hardness of pac learning of some other class h, as long as

figure5:the   rstandsecondfeaturesselectedbyadaboost.thetwofeaturesareshowninthetoprowandthenoverlayedonatypicaltrainingfaceinthebottomrow.the   rstfeaturemeasuresthedifferenceinintensitybetweentheregionoftheeyesandaregionacrosstheuppercheeks.thefeaturecapitalizesontheobservationthattheeyeregionisoftendarkerthanthecheeks.thesecondfeaturecomparestheintensitiesintheeyeregionstotheintensityacrossthebridgeofthenose.directlyincreasescomputationtime.4theattentionalcascadethissectiondescribesanalgorithmforconstructingacascadeofclassi   erswhichachievesincreaseddetec-tionperformancewhileradicallyreducingcomputationtime.thekeyinsightisthatsmaller,andthereforemoreef   cient,boostedclassi   erscanbeconstructedwhichrejectmanyofthenegativesub-windowswhiledetectingalmostallpositiveinstances.simplerclassi   ersareusedtorejectthemajorityofsub-windowsbeforemorecomplexclassi   ersarecalledupontoachievelowfalsepositiverates.stagesinthecascadeareconstructedbytrainingclassi   ersusingadaboost.startingwithatwo-featurestrongclassi   er,aneffectiveface   ltercanbeobtainedbyadjustingthestrongclassi   erthresholdtomin-imizefalsenegatives.theinitialadaboostthreshold,,isdesignedtoyieldalowerrorrateonthetrainingdata.alowerthresholdyieldshigherdetectionratesandhigherfalsepositiverates.basedonperformancemeasuredusingavalidationtrainingset,thetwo-featureclassi   ercanbeadjustedtodetect100%ofthefaceswithafalsepositiverateof40%.seefigure5foradescriptionofthetwofeaturesusedinthisclassi   er.thedetectionperformanceofthetwo-featureclassi   erisfarfromacceptableasanobjectdetectionsystem.neverthelesstheclassi   ercansigni   cantlyreducethenumbersub-windowsthatneedfurtherpro-cessingwithveryfewoperations:1.evaluatetherectanglefeatures(requiresbetween6and9arrayreferencesperfeature).2.computetheweakclassi   erforeachfeature(requiresonethresholdoperationperfeature).11142

boosting

h is weakly learnable using b. for example, klivans & sherstov (2006) have
shown that pac learning of the class of intersection of halfspaces is hard (even
in the realizable case). this hardness result can be used to show that agnostic
pac learning of a single halfspace is also computationally hard (shalev-shwartz,
shamir & sridharan 2010). the idea is to show that an agnostic pac learner
for a single halfspace can yield a weak learner for the class of intersection of
halfspaces, and since such a weak learner can be boosted, we will obtain a strong
learner for the class of intersection of halfspaces.

adaboost also shows an equivalence between the existence of a weak learner
and separability of the data using a linear classi   er over the predictions of base
hypotheses. this result is closely related to von neumann   s minimax theorem
(von neumann 1928), a fundamental result in game theory.

adaboost is also related to the concept of margin, which we will study later on
in chapter 15. it can also be viewed as a forward greedy selection algorithm, a
topic that will be presented in chapter 25. a recent book by schapire & freund
(2012) covers boosting from all points of view, and gives easy access to the wealth
of research that this    eld has produced.

10.7

exercises

1. boosting the con   dence: let a be an algorithm that guarantees the fol-
lowing: there exist some constant   0     (0, 1) and a function mh : (0, 1)     n
such that for every       (0, 1), if m     mh( ) then for every distribution d it
holds that with id203 of at least 1      0, ld(a(s))     minh   h ld(h) +  .
suggest a procedure that relies on a and learns h in the usual agnostic

pac learning model and has a sample complexity of

mh( ,   )     k mh( ) +

(cid:24) 2 log(4k/  )

(cid:25)

,

 2

where

k = (cid:100)log(  )/ log(  0)(cid:101).

hint: divide the data into k + 1 chunks, where each of the    rst k chunks
is of size mh( ) examples. train the    rst k chunks using a. argue that the
id203 that for all of these chunks we have ld(a(s)) > minh   h ld(h)+ 
0       /2. finally, use the last chunk to choose from the k hypotheses
is at most   k
that a generated from the k chunks (by relying on corollary 4.6).

2. prove that the function h given in equation (10.5) equals the piece-wise con-

stant function de   ned according to the same thresholds as h.

3. we have informally argued that the adaboost algorithm uses the weighting
mechanism to    force    the weak learner to focus on the problematic examples
in the next iteration. in this question we will    nd some rigorous justi   cation
for this argument.

10.7 exercises

143

show that the error of ht w.r.t. the distribution d(t+1) is exactly 1/2. that

is, show that for every t     [t ]

m(cid:88)

d(t+1)

i

1[yi(cid:54)=ht(xi)] = 1/2.

i=1

4. in this exercise we discuss the vc-dimension of classes of the form l(b, t ).
we proved an upper bound of o(dt log(dt )), where d = vcdim(b). here we
wish to prove an almost matching lower bound. however, that will not be the
case for all classes b.
1. note that for every class b and every number t     1, vcdim(b)    
vcdim(l(b, t )). find a class b for which vcdim(b) = vcdim(l(b, t ))
for every t     1.
hint: take x to be a    nite set.
vcdim(bd)     5 + 2 log(d).
hints:
    for the upper bound, rely on exercise 11.
    for the lower bound, assume d = 2k. let a be a k    d matrix whose
columns are all the d binary vectors in {  1}k. the rows of a form
a set of k vectors in rd. show that this set is shattered by decision
stumps over rd.

2. let bd be the class of decision stumps over rd. prove that log(d)    

3. let t     1 be any integer. prove that vcdim(l(bd, t ))     0.5 t log(d).

hint: construct a set of t
from the previous question, and the rows of the matrices 2a, 3a, 4a, . . . , t
show that the resulting set is shattered by l(bd, t ).

2 k instances by taking the rows of the matrix a
2 a.

image of a, denoted by i(a), is the matrix b such that bi,j =(cid:80)

5. e   ciently calculating the viola and jones features using an inte-
gral image: let a be a 24    24 matrix representing an image. the integral
i(cid:48)   i,j(cid:48)   j ai,j.
    show that i(a) can be calculated from a in time linear in the size of a.
    show how every viola and jones feature can be calculated from i(a) in a
constant amount of time (that is, the runtime does not depend on the
size of the rectangle de   ning the feature).

11 model selection and validation

in the previous chapter we have described the adaboost algorithm and have
shown how the parameter t of adaboost controls the bias-complexity trade-
o   . but, how do we set t in practice? more generally, when approaching some
practical problem, we usually can think of several algorithms that may yield a
good solution, each of which might have several parameters. how can we choose
the best algorithm for the particular problem at hand? and how do we set the
algorithm   s parameters? this task is often called model selection.

to illustrate the model selection task, consider the problem of learning a one
dimensional regression function, h : r     r. suppose that we obtain a training
set as depicted in the    gure.

we can consider    tting a polynomial to the data, as described in chapter 9.
however, we might be uncertain regarding which degree d would give the best
results for our data set: a small degree may not    t the data well (i.e., it will
have a large approximation error), whereas a high degree may lead to over   tting
(i.e., it will have a large estimation error). in the following we depict the result
of    tting a polynomial of degrees 2, 3, and 10. it is easy to see that the empirical
risk decreases as we enlarge the degree. however, looking at the graphs, our
intuition tells us that setting the degree to 3 may be better than setting it to 10.
it follows that the empirical risk alone is not enough for model selection.

degree 2

degree 3

degree 10

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

11.1 model selection using srm

145

in this chapter we will present two approaches for model selection. the    rst
approach is based on the structural risk minimization (srm) paradigm we
have described and analyzed in chapter 7.2. srm is particularly useful when
a learning algorithm depends on a parameter that controls the bias-complexity
tradeo    (such as the degree of the    tted polynomial in the preceding example
or the parameter t in adaboost). the second approach relies on the concept
of validation. the basic idea is to partition the training set into two sets. one
is used for training each of the candidate models, and the second is used for
deciding which of them yields the best results.

in model selection tasks, we try to    nd the right balance between approxi-
mation and estimation errors. more generally, if our learning algorithm fails to
   nd a predictor with a small risk, it is important to understand whether we
su   er from over   tting or under   tting. in section 11.3 we discuss how this can
be achieved.

11.1

model selection using srm

the srm paradigm has been described and analyzed in section 7.2. here we
show how srm can be used for tuning the tradeo    between bias and complexity
without deciding on a speci   c hypothesis class in advance. consider a countable
sequence of hypothesis classes h1,h2,h3, . . .. for example, in the problem of
polynomial regression mentioned, we can take hd to be the set of polynomials
of degree at most d. another example is taking hd to be the class l(b, d) used
by adaboost, as described in the previous chapter.
we assume that for every d, the class hd enjoys the uniform convergence
property (see de   nition 4.3 in chapter 4) with a sample complexity function of
the form

( ,   )     g(d) log(1/  )

muchd

(11.1)
where g : n     r is some monotonically increasing function. for example, in the
case of binary classi   cation problems, we can take g(d) to be the vc-dimension
of the class hd multiplied by a universal constant (the one appearing in the
fundamental theorem of learning; see theorem 6.8). for the classes l(b, d) used
by adaboost, the function g will simply grow with d.

 2

,

recall that the srm rule follows a    bound minimization    approach, where in
our case the bound is as follows: with id203 of at least 1       , for every
d     n and h     hd,

(cid:114)

ld(h)     ls(h) +

g(d)(log(1/  ) + 2 log(d) + log(  2/6))

m

.

(11.2)

this bound, which follows directly from theorem 7.4, shows that for every d and
every h     hd, the true risk is bounded by two terms     the empirical risk, ls(h),

146

model selection and validation

and a complexity term that depends on d. the srm rule will search for d and
h     hd that minimize the right-hand side of equation (11.2).

getting back to the example of polynomial regression described earlier, even
though the empirical risk of the 10th degree polynomial is smaller than that of
the 3rd degree polynomial, we would still prefer the 3rd degree polynomial since
its complexity (as re   ected by the value of the function g(d)) is much smaller.

while the srm approach can be useful in some situations, in many practical
cases the upper bound given in equation (11.2) is pessimistic. in the next section
we present a more practical approach.

11.2

validation

we would often like to get a better estimation of the true risk of the output pre-
dictor of a learning algorithm. so far we have derived bounds on the estimation
error of a hypothesis class, which tell us that for all hypotheses in the class, the
true risk is not very far from the empirical risk. however, these bounds might be
loose and pessimistic, as they hold for all hypotheses and all possible data dis-
tributions. a more accurate estimation of the true risk can be obtained by using
some of the training data as a validation set, over which one can evalutate the
success of the algorithm   s output predictor. this procedure is called validation.
naturally, a better estimation of the true risk is useful for model selection, as

we will describe in section 11.2.2.

11.2.1

hold out set

the simplest way to estimate the true error of a predictor h is by sampling an ad-
ditional set of examples, independent of the training set, and using the empirical
error on this validation set as our estimator. formally, let v = (x1, y1), . . . , (xmv , ymv )
be a set of fresh mv examples that are sampled according to d (independently of
the m examples of the training set s). using hoe   ding   s inequality ( lemma 4.5)
we have the following:

theorem 11.1 let h be some predictor and assume that the id168 is in
[0, 1]. then, for every        (0, 1), with id203 of at least 1        over the choice
of a validation set v of size mv we have

(cid:115)

|lv (h)     ld(h)|    

log(2/  )

2 mv

.

the bound in theorem 11.1 does not depend on the algorithm or the training
set used to construct h and is tighter than the usual bounds that we have seen so
far. the reason for the tightness of this bound is that it is in terms of an estimate
on a fresh validation set that is independent of the way h was generated. to
illustrate this point, suppose that h was obtained by applying an erm predictor

11.2 validation

147

with respect to a hypothesis class of vc-dimension d, over a training set of m
examples. then, from the fundamental theorem of learning (theorem 6.8) we
obtain the bound

ld(h)     ls(h) +

c

d + log(1/  )

m

,

where c is the constant appearing in theorem 6.8. in contrast, from theo-
rem 11.1 we obtain the bound

(cid:114)

(cid:115)

ld(h)     lv (h) +

log(2/  )

2mv

.

therefore, taking mv to be order of m, we obtain an estimate that is more
accurate by a factor that depends on the vc-dimension. on the other hand, the
price we pay for using such an estimate is that it requires an additional sample
on top of the sample used for training the learner.

sampling a training set and then sampling an independent validation set is
equivalent to randomly partitioning our random set of examples into two parts,
using one part for training and the other one for validation. for this reason, the
validation set is often referred to as a hold out set.

11.2.2

validation for model selection

validation can be naturally used for model selection as follows. we    rst train
di   erent algorithms (or the same algorithm with di   erent parameters) on the
given training set. let h = {h1, . . . , hr} be the set of all output predictors of the
di   erent algorithms. for example, in the case of training polynomial regressors,
we would have each hr be the output of polynomial regression of degree r. now,
to choose a single predictor from h we sample a fresh validation set and choose
the predictor that minimizes the error over the validation set. in other words,
we apply ermh over the validation set.
this process is very similar to learning a    nite hypothesis class. the only
di   erence is that h is not    xed ahead of time but rather depends on the train-
ing set. however, since the validation set is independent of the training set we
get that it is also independent of h and therefore the same technique we used
to derive bounds for    nite hypothesis classes holds here as well. in particular,
combining theorem 11.1 with the union bound we obtain:
theorem 11.2 let h = {h1, . . . , hr} be an arbitrary set of predictors and
assume that the id168 is in [0, 1]. assume that a validation set v of size
mv is sampled independent of h. then, with id203 of at least 1       over the
choice of v we have

   h     h,

|ld(h)     lv (h)|    

(cid:115)

log(2|h|/  )

2 mv

.

148

model selection and validation

this theorem tells us that the error on the validation set approximates the
true error as long as h is not too large. however, if we try too many methods
(resulting in |h| that is large relative to the size of the validation set) then we   re
in danger of over   tting.

to illustrate how validation is useful for model selection, consider again the
example of    tting a one dimensional polynomial as described in the beginning
of this chapter. in the following we depict the same training set, with erm
polynomials of degree 2, 3, and 10, but this time we also depict an additional
validation set (marked as red, un   lled circles). the polynomial of degree 10 has
minimal training error, yet the polynomial of degree 3 has the minimal validation
error, and hence it will be chosen as the best model.

11.2.3

the model-selection curve

the model selection curve shows the training error and validation error as a func-
tion of the complexity of the model considered. for example, for the polynomial
   tting problem mentioned previously, the curve will look like:

11.2 validation

149

train
validation

r
o
r
r
e

0.4

0.3

0.2

0.1

0

2

4

6

d

8

10

as can be shown, the training error is monotonically decreasing as we increase
the polynomial degree (which is the complexity of the model in our case). on
the other hand, the validation error    rst decreases but then starts to increase,
which indicates that we are starting to su   er from over   tting.

plotting such curves can help us understand whether we are searching the
correct regime of our parameter space. often, there may be more than a single
parameter to tune, and the possible number of values each parameter can take
might be quite large. for example, in chapter 13 we describe the concept of
id173, in which the parameter of the learning algorithm is a real number.
in such cases, we start with a rough grid of values for the parameter(s) and plot
the corresponding model-selection curve. on the basis of the curve we will zoom
in to the correct regime and employ a    ner grid to search over. it is important to
verify that we are in the relevant regime. for example, in the polynomial    tting
problem described, if we start searching degrees from the set of values {1, 10, 20}
and do not employ a    ner grid based on the resulting curve, we will end up with
a rather poor model.

11.2.4

k-fold cross validation

the validation procedure described so far assumes that data is plentiful and that
we have the ability to sample a fresh validation set. but in some applications,
data is scarce and we do not want to    waste    data on validation. the k-fold
cross validation technique is designed to give an accurate estimate of the true
error without wasting too much data.

in k-fold cross validation the original training set is partitioned into k subsets
(folds) of size m/k (for simplicity, assume that m/k is an integer). for each fold,
the algorithm is trained on the union of the other folds and then the error of its
output is estimated using the fold. finally, the average of all these errors is the

150

model selection and validation

estimate of the true error. the special case k = m, where m is the number of
examples, is called leave-one-out (loo).

k-fold cross validation is often used for model selection (or parameter tuning),
and once the best parameter is chosen, the algorithm is retrained using this
parameter on the entire training set. a pseudocode of k-fold cross validation
for model selection is given in the following. the procedure receives as input a
training set, s, a set of possible parameter values,   , an integer, k, representing
the number of folds, and a learning algorithm, a, which receives as input a
training set as well as a parameter          . it outputs the best parameter as well
as the hypothesis trained by this parameter on the entire training set.

k-fold cross validation for model selection

input:

training set s = (x1, y1), . . . , (xm, ym)
set of parameter values   
learning algorithm a
integer k

partition s into s1, s2, . . . , sk
foreach          

for i = 1 . . . k

hi,   = a(s \ si;   )

(cid:80)k

error(  ) = 1
k

output

i=1 lsi(hi,  )

  (cid:63) = argmin   [error(  )]
h  (cid:63) = a(s;   (cid:63))

the cross validation method often works very well in practice. however, it
might sometime fail, as the arti   cial example given in exercise 1 shows. rig-
orously understanding the exact behavior of cross validation is still an open
problem. rogers and wagner (rogers & wagner 1978) have shown that for k
local rules (e.g., k nearest neighbor; see chapter 19) the cross validation proce-
dure gives a very good estimate of the true error. other papers show that cross
validation works for stable algorithms (we will study stability and its relation to
learnability in chapter 13).

11.2.5

train-validation-test split

in most practical applications, we split the available examples into three sets.
the    rst set is used for training our algorithm and the second is used as a
validation set for model selection. after we select the best model, we test the
performance of the output predictor on the third set, which is often called the
   test set.    the number obtained is used as an estimator of the true error of the
learned predictor.

11.3 what to do if learning fails

151

11.3 what to do if learning fails

consider the following scenario: you were given a learning task and have ap-
proached it with a choice of a hypothesis class, a learning algorithm, and param-
eters. you used a validation set to tune the parameters and tested the learned
predictor on a test set. the test results, unfortunately, turn out to be unsatis-
factory. what went wrong then, and what should you do next?

there are many elements that can be       xed.    the main approaches are listed

in the following:
    get a larger sample
    change the hypothesis class by:

    enlarging it
    reducing it
    completely changing it
    changing the parameters you consider
    change the feature representation of the data
    change the optimization algorithm used to apply your learning rule

in order to    nd the best remedy, it is essential    rst to understand the cause
of the bad performance. recall that in chapter 5 we decomposed the true er-
ror of the learned predictor into approximation error and estimation error. the
approximation error is de   ned to be ld(h(cid:63)) for some h(cid:63)     argminh   h ld(h),
while the estimation error is de   ned to be ld(hs)     ld(h(cid:63)), where hs is the
learned predictor (which is based on the training set s).

the approximation error of the class does not depend on the sample size or
on the algorithm being used. it only depends on the distribution d and on the
hypothesis class h. therefore, if the approximation error is large, it will not help
us to enlarge the training set size, and it also does not make sense to reduce the
hypothesis class. what can be bene   cial in this case is to enlarge the hypothesis
class or completely change it (if we have some alternative prior knowledge in
the form of a di   erent hypothesis class). we can also consider applying the
same hypothesis class but on a di   erent feature representation of the data (see
chapter 25).

the estimation error of the class does depend on the sample size. therefore, if
we have a large estimation error we can make an e   ort to obtain more training
examples. we can also consider reducing the hypothesis class. however, it doesn   t
make sense to enlarge the hypothesis class in that case.

error decomposition using validation
we see that understanding whether our problem is due to approximation error
or estimation error is very useful for    nding the best remedy. in the previous
section we saw how to estimate ld(hs) using the empirical risk on a validation
set. however, it is more di   cult to estimate the approximation error of the class.

152

model selection and validation

instead, we give a di   erent error decomposition, one that can be estimated from
the train and validation sets.

ld(hs) = (ld(hs)     lv (hs)) + (lv (hs)     ls(hs)) + ls(hs).

the    rst term, (ld(hs)     lv (hs)), can be bounded quite tightly using theo-
rem 11.1. intuitively, when the second term, (lv (hs)     ls(hs)), is large we say
that our algorithm su   ers from    over   tting    while when the empirical risk term,
ls(hs), is large we say that our algorithm su   ers from    under   tting.    note that
these two terms are not necessarily good estimates of the estimation and ap-
proximation errors. to illustrate this, consider the case in which h is a class of
vc-dimension d, and d is a distribution such that the approximation error of h
with respect to d is 1/4. as long as the size of our training set is smaller than
d we will have ls(hs) = 0 for every erm hypothesis. therefore, the training
risk, ls(hs), and the approximation error, ld(h(cid:63)), can be signi   cantly di   erent.
nevertheless, as we show later, the values of ls(hs) and (lv (hs)    ls(hs)) still
provide us useful information.

consider    rst the case in which ls(hs) is large. we can write

ls(hs) = (ls(hs)     ls(h(cid:63))) + (ls(h(cid:63))     ld(h(cid:63))) + ld(h(cid:63)).

when hs is an ermh hypothesis we have that ls(hs)   ls(h(cid:63))     0. in addition,
since h(cid:63) does not depend on s, the term (ls(h(cid:63))   ld(h(cid:63))) can be bounded quite
tightly (as in theorem 11.1). the last term is the approximation error. it follows
that if ls(hs) is large then so is the approximation error, and the remedy to the
failure of our algorithm should be tailored accordingly (as discussed previously).

remark 11.1
it is possible that the approximation error of our class is small,
yet the value of ls(hs) is large. for example, maybe we had a bug in our erm
implementation, and the algorithm returns a hypothesis hs that is not an erm.
it may also be the case that    nding an erm hypothesis is computationally hard,
and our algorithm applies some heuristic trying to    nd an approximate erm. in
some cases, it is hard to know how good hs is relative to an erm hypothesis. but,
sometimes it is possible at least to know whether there are better hypotheses.
for example, in the next chapter we will study convex learning problems in
which there are optimality conditions that can be checked to verify whether
our optimization algorithm converged to an erm solution. in other cases, the
solution may depend on randomness in initializing the algorithm, so we can try
di   erent randomly selected initial points to see whether better solutions pop out.

next consider the case in which ls(hs) is small. as we argued before, this
does not necessarily imply that the approximation error is small. indeed, consider
two scenarios, in both of which we are trying to learn a hypothesis class of
vc-dimension d using the erm learning rule. in the    rst scenario, we have a
training set of m < d examples and the approximation error of the class is high.
in the second scenario, we have a training set of m > 2d examples and the

11.3 what to do if learning fails

153

error

error

validation error

train error

m

validationerror

train error

m

figure 11.1 examples of learning curves. left: this learning curve corresponds to the
scenario in which the number of examples is always smaller than the vc dimension of
the class. right: this learning curve corresponds to the scenario in which the
approximation error is zero and the number of examples is larger than the vc
dimension of the class.

approximation error of the class is zero. in both cases ls(hs) = 0. how can we
distinguish between the two cases?

learning curves
one possible way to distinguish between the two cases is by plotting learning
curves. to produce a learning curve we train the algorithm on pre   xes of the
data of increasing sizes. for example, we can    rst train the algorithm on the
   rst 10% of the examples, then on 20% of them, and so on. for each pre   x we
calculate the training error (on the pre   x the algorithm is being trained on)
and the validation error (on a prede   ned validation set). such learning curves
can help us distinguish between the two aforementioned scenarios. in the    rst
scenario we expect the validation error to be approximately 1/2 for all pre   xes,
as we didn   t really learn anything. in the second scenario the validation error
will start as a constant but then should start decreasing (it must start decreasing
once the training set size is larger than the vc-dimension). an illustration of
the two cases is given in figure 11.1.

in general, as long as the approximation error is greater than zero we expect
the training error to grow with the sample size, as a larger amount of data points
makes it harder to provide an explanation for all of them. on the other hand,
the validation error tends to decrease with the increase in sample size. if the
vc-dimension is    nite, when the sample size goes to in   nity, the validation and
train errors converge to the approximation error. therefore, by extrapolating
the training and validation curves we can try to guess the value of the approx-
imation error, or at least to get a rough estimate on an interval in which the
approximation error resides.

getting back to the problem of    nding the best remedy for the failure of
our algorithm, if we observe that ls(hs) is small while the validation error is
large, then in any case we know that the size of our training set is not su   cient
for learning the class h. we can then plot a learning curve. if we see that the

154

model selection and validation

validation error is starting to decrease then the best solution is to increase the
number of examples (if we can a   ord to enlarge the data). another reasonable
solution is to decrease the complexity of the hypothesis class. on the other hand,
if we see that the validation error is kept around 1/2 then we have no evidence
that the approximation error of h is good. it may be the case that increasing
the training set size will not help us at all. obtaining more data can still help
us, as at some point we can see whether the validation error starts to decrease
or whether the training error starts to increase. but, if more data is expensive,
it may be better    rst to try to reduce the complexity of the hypothesis class.

to summarize the discussion, the following steps should be applied:

1. if learning involves parameter tuning, plot the model-selection curve to make

sure that you tuned the parameters appropriately (see section 11.2.3).

2. if the training error is excessively large consider enlarging the hypothesis class,

completely change it, or change the feature representation of the data.

3. if the training error is small, plot learning curves and try to deduce from them

whether the problem is estimation error or approximation error.

4. if the approximation error seems to be small enough, try to obtain more data.
if this is not possible, consider reducing the complexity of the hypothesis class.
5. if the approximation error seems to be large as well, try to change the hy-

pothesis class or the feature representation of the data completely.

11.4

summary

model selection is the task of selecting an appropriate model for the learning
task based on the data itself. we have shown how this can be done using the
srm learning paradigm or using the more practical approach of validation. if
our learning algorithm fails, a decomposition of the algorithm   s error should be
performed using learning curves, so as to    nd the best remedy.

11.5

exercises

1. failure of k-fold cross validation consider a case in that the label is
chosen at random according to p[y = 1] = p[y = 0] = 1/2. consider a
learning algorithm that outputs the constant predictor h(x) = 1 if the parity
of the labels on the training set is 1 and otherwise the algorithm outputs the
constant predictor h(x) = 0. prove that the di   erence between the leave-one-
out estimate and the true error in such a case is always 1/2.
2. let h1, . . . ,hk be k hypothesis classes. suppose you are given m i.i.d. training
i=1hi. consider two

examples and you would like to learn the class h =    k
alternative approaches:
    learn h on the m examples using the erm rule

11.5 exercises

155

    divide the m examples into a training set of size (1      )m and a validation
set of size   m, for some        (0, 1). then, apply the approach of model
selection using validation. that is,    rst train each class hi on the (1    
  )m training examples using the erm rule with respect to hi, and let
  h1, . . . ,   hk be the resulting hypotheses. second, apply the erm rule with
respect to the    nite class {  h1, . . . ,   hk} on the   m validation examples.
describe scenarios in which the    rst method is better than the second and
vice versa.

12 convex learning problems

in this chapter we introduce convex learning problems. convex learning comprises
an important family of learning problems, mainly because most of what we can
learn e   ciently falls into it. we have already encountered id75 with
the squared loss and id28, which are convex problems, and indeed
they can be learned e   ciently. we have also seen nonconvex problems, such as
halfspaces with the 0-1 loss, which is known to be computationally hard to learn
in the unrealizable case.

in general, a convex learning problem is a problem whose hypothesis class is a
convex set, and whose id168 is a convex function for each example. we be-
gin the chapter with some required de   nitions of convexity. besides convexity, we
will de   ne lipschitzness and smoothness, which are additional properties of the
id168 that facilitate successful learning. we next turn to de   ning convex
learning problems and demonstrate the necessity for further constraints such as
boundedness and lipschitzness or smoothness. we de   ne these more restricted
families of learning problems and claim that convex-smooth/lipschitz-bounded
problems are learnable. these claims will be proven in the next two chapters, in
which we will present two learning paradigms that successfully learn all problems
that are either convex-lipschitz-bounded or convex-smooth-bounded.

finally, in section 12.3, we show how one can handle some nonconvex problems
by minimizing    surrogate    id168s that are convex (instead of the original
nonconvex id168). surrogate convex id168s give rise to e   cient
solutions but might increase the risk of the learned predictor.

12.1

convexity, lipschitzness, and smoothness

12.1.1

convexity

definition 12.1 (convex set) a set c in a vector space is convex if for any
two vectors u, v in c, the line segment between u and v is contained in c. that
is, for any        [0, 1] we have that   u + (1       )v     c.

examples of convex and nonconvex sets in r2 are given in the following. for
the nonconvex sets, we depict two points in the set such that the line between
the two points is not contained in the set.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

12.1 convexity, lipschitzness, and smoothness

157

non-convex

convex

given        [0, 1], the combination,   u + (1       )v of the points u, v is called a

convex combination.

definition 12.2 (convex function) let c be a convex set. a function f :
c     r is convex if for every u, v     c and        [0, 1],

f (  u + (1       )v)       f (u) + (1       )f (v) .

in words, f is convex if for any u, v, the graph of f between u and v lies below
the line segment joining f (u) and f (v). an illustration of a convex function,
f : r     r, is depicted in the following.

f (v)

  f (u) + (1       )f (v)

f (  u + (1       )v)

f (u)

u

  u + (1       )v

v

the epigraph of a function f is the set

epigraph(f) = {(x,   ) : f (x)       }.

(12.1)

it is easy to verify that a function f is convex if and only if its epigraph is a
convex set. an illustration of a nonconvex function f : r     r, along with its
epigraph, is given in the following.

158

convex learning problems

f (x)

x

an important property of convex functions is that every local minimum of the
function is also a global minimum. formally, let b(u, r) = {v : (cid:107)v     u(cid:107)     r} be
a ball of radius r centered around u. we say that f (u) is a local minimum of f
at u if there exists some r > 0 such that for all v     b(u, r) we have f (v)     f (u).
it follows that for any v (not necessarily in b), there is a small enough    > 0
such that u +   (v     u)     b(u, r) and therefore

f (u)     f (u +   (v     u)) .

(12.2)

if f is convex, we also have that

f (u +   (v     u)) = f (  v + (1       )u)     (1       )f (u) +   f (v) .

(12.3)

combining these two equations and rearranging terms, we conclude that f (u)    
f (v). since this holds for every v, it follows that f (u) is also a global minimum
of f .

another important property of convex functions is that for every w we can
construct a tangent to f at w that lies below f everywhere. if f is di   erentiable,
this tangent is the linear function l(u) = f (w) + (cid:104)   f (w), u     w(cid:105), where    f (w)
is the gradient of f at w, namely, the vector of partial derivatives of f ,    f (w) =

(cid:16)    f (w)

   w1

, . . . ,    f (w)
   wd

(cid:17)

. that is, for convex di   erentiable functions,

   u, f (u)     f (w) + (cid:104)   f (w), u     w(cid:105).

(12.4)

in chapter 14 we will generalize this inequality to nondi   erentiable functions.
an illustration of equation (12.4) is given in the following.

12.1 convexity, lipschitzness, and smoothness

159

    w ,    f( w )(cid:105)

f (u)

(cid:104)u

f( w ) +

f (w)

w

u

if f is a scalar di   erentiable function, there is an easy way to check if it is

convex.
lemma 12.3 let f : r     r be a scalar twice di   erential function, and let
f(cid:48), f(cid:48)(cid:48) be its    rst and second derivatives, respectively. then, the following are
equivalent:

1. f is convex
2. f(cid:48) is monotonically nondecreasing
3. f(cid:48)(cid:48) is nonnegative

example 12.1
    the scalar function f (x) = x2 is convex. to see this, note that f(cid:48)(x) = 2x

and f(cid:48)(cid:48)(x) = 2 > 0.

    the scalar function f (x) = log(1 + exp(x)) is convex. to see this, observe that
exp(   x)+1 . this is a monotonically increasing function

f(cid:48)(x) = exp(x)
since the exponent function is a monotonically increasing function.

1+exp(x) =

1

the following claim shows that the composition of a convex scalar function

with a linear function yields a convex vector-valued function.
claim 12.4 assume that f : rd     r can be written as f (w) = g((cid:104)w, x(cid:105) + y),
for some x     rd, y     r, and g : r     r. then, convexity of g implies the
convexity of f .
proof let w1, w2     rd and        [0, 1]. we have

f (  w1 + (1       )w2) = g((cid:104)  w1 + (1       )w2, x(cid:105) + y)

= g(  (cid:104)w1, x(cid:105) + (1       )(cid:104)w2, x(cid:105) + y)
= g(  ((cid:104)w1, x(cid:105) + y) + (1       )((cid:104)w2, x(cid:105) + y))
      g((cid:104)w1, x(cid:105) + y) + (1       )g((cid:104)w2, x(cid:105) + y),

where the last inequality follows from the convexity of g.

example 12.2

160

convex learning problems

    given some x     rd and y     r, let f : rd     r be de   ned as f (w) =
((cid:104)w, x(cid:105)     y)2. then, f is a composition of the function g(a) = a2 onto a
linear function, and hence f is a convex function.

    given some x     rd and y     {  1}, let f : rd     r be de   ned as f (w) =
log(1 + exp(   y(cid:104)w, x(cid:105))). then, f is a composition of the function g(a) =
log(1 + exp(a)) onto a linear function, and hence f is a convex function.

finally, the following lemma shows that the maximum of convex functions is
convex and that a weighted sum of convex functions, with nonnegative weights,
is also convex.

claim 12.5 for i = 1, . . . , r, let fi
following functions from rd to r are also convex.
    g(x) = maxi   [r] fi(x)

i=1 wifi(x), where for all i, wi     0.

    g(x) =(cid:80)r

: rd     r be a convex function. the

proof the    rst claim follows by

for the second claim

g(  u + (1       )v) =

i

g(  u + (1       )v) = max
    max
       max
=   g(u) + (1       )g(v).

fi(  u + (1       )v)
[  fi(u) + (1       )fi(v)]
fi(u) + (1       ) max

i

i

i

fi(v)

(cid:88)
   (cid:88)
(cid:88)

i

i

wifi(  u + (1       )v)
wi [  fi(u) + (1       )fi(v)]
wifi(u) + (1       )

(cid:88)

=   
=   g(u) + (1       )g(v).

i

wifi(v)

i

example 12.3 the function g(x) = |x| is convex. to see this, note that g(x) =
max{x,   x} and that both the function f1(x) = x and f2(x) =    x are convex.

12.1.2

lipschitzness

the de   nition of lipschitzness below is with respect to the euclidean norm over
rd. however, it is possible to de   ne lipschitzness with respect to any norm.
definition 12.6 (lipschitzness) let c     rd. a function f : rd     rk is
  -lipschitz over c if for every w1, w2     c we have that (cid:107)f (w1)     f (w2)(cid:107)    
  (cid:107)w1     w2(cid:107).

12.1 convexity, lipschitzness, and smoothness

161

intuitively, a lipschitz function cannot change too fast. note that if f : r     r

is di   erentiable, then by the mean value theorem we have
f (w1)     f (w2) = f(cid:48)(u)(w1     w2) ,

where u is some point between w1 and w2. it follows that if the derivative of f
is everywhere bounded (in absolute value) by   , then the function is   -lipschitz.

example 12.4
    the function f (x) = |x| is 1-lipschitz over r. this follows from the triangle

inequality: for every x1, x2,
|x1|     |x2| = |x1     x2 + x2|     |x2|     |x1     x2| + |x2|     |x2| = |x1     x2|.
since this holds for both x1, x2 and x2, x1, we obtain that ||x1|     |x2||    
|x1     x2|.
    the function f (x) = log(1 + exp(x)) is 1-lipschitz over r. to see this, observe

that

|f(cid:48)(x)| =

exp(x)

1 + exp(x)

1

exp(   x) + 1

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)     1.

    the function f (x) = x2 is not   -lipschitz over r for any   . to see this, take

x1 = 0 and x2 = 1 +   , then

f (x2)     f (x1) = (1 +   )2 >   (1 +   ) =   |x2     x1|.

however, this function is   -lipschitz over the set c = {x : |x|       /2}.
indeed, for any x1, x2     c we have

|x2
1     x2

2| = |x1 + x2| |x1     x2|     2(  /2)|x1     x2| =   |x1     x2|.

    the linear function f : rd     r de   ned by f (w) = (cid:104)v, w(cid:105) + b where v     rd

is (cid:107)v(cid:107)-lipschitz. indeed, using cauchy-schwartz inequality,

|f (w1)     f (w2)| = |(cid:104)v, w1     w2(cid:105)|     (cid:107)v(cid:107)(cid:107)w1     w2(cid:107).

the following claim shows that composition of lipschitz functions preserves

lipschitzness.

claim 12.7 let f (x) = g1(g2(x)), where g1 is   1-lipschitz and g2 is   2-
lipschitz. then, f is (  1  2)-lipschitz. in particular, if g2 is the linear function,
g2(x) = (cid:104)v, x(cid:105) + b, for some v     rd, b     r, then f is (  1 (cid:107)v(cid:107))-lipschitz.

proof

|f (w1)     f (w2)| = |g1(g2(w1))     g1(g2(w2))|

      1(cid:107)g2(w1)     g2(w2)(cid:107)
      1   2 (cid:107)w1     w2(cid:107).

162

convex learning problems

12.1.3

smoothness

the de   nition of a smooth function relies on the notion of gradient. recall that
the gradient of a di   erentiable function f : rd     r at w, denoted    f (w), is the
vector of partial derivatives of f , namely,    f (w) =
definition 12.8 (smoothness) a di   erentiable function f : rd     r is   -
smooth if its gradient is   -lipschitz; namely, for all v, w we have (cid:107)   f (v)    
   f (w)(cid:107)       (cid:107)v     w(cid:107).

, . . . ,    f (w)
   wd

   w1

.

(cid:16)    f (w)

(cid:17)

it is possible to show that smoothness implies that for all v, w we have

f (v)     f (w) + (cid:104)   f (w), v     w(cid:105) +

(12.5)
recall that convexity of f implies that f (v)     f (w)+(cid:104)   f (w), v   w(cid:105). therefore,
when a function is both convex and smooth, we have both upper and lower
bounds on the di   erence between the function and its    rst order approximation.

(cid:107)v     w(cid:107)2 .

  
2

setting v = w     1

     f (w) in the right-hand side of equation (12.5) and rear-

ranging terms, we obtain

(cid:107)   f (w)(cid:107)2     f (w)     f (v).

1
2  

if we further assume that f (v)     0 for all v we conclude that smoothness implies
the following:

(cid:107)   f (w)(cid:107)2     2  f (w) .

(12.6)

a function that satis   es this property is also called a self-bounded function.

example 12.5
    the function f (x) = x2 is 2-smooth. this follows directly from the fact that
f(cid:48)(x) = 2x. note that for this particular function equation (12.5) and
equation (12.6) hold with equality.

    the function f (x) = log(1 + exp(x)) is (1/4)-smooth. indeed, since f(cid:48)(x) =

1

1+exp(   x) we have that

|f(cid:48)(cid:48)(x)| =

exp(   x)

(1 + exp(   x))2 =

1

(1 + exp(   x))(1 + exp(x))

    1/4.

hence, f(cid:48)
tion (12.6) holds as well.

is (1/4)-lipschitz. since this function is nonnegative, equa-

the following claim shows that a composition of a smooth scalar function over

a linear function preserves smoothness.
claim 12.9 let f (w) = g((cid:104)w, x(cid:105) + b), where g : r     r is a   -smooth function,
x     rd, and b     r. then, f is (   (cid:107)x(cid:107)2)-smooth.

12.2 convex learning problems

163

proof by the chain rule we have that    f (w) = g(cid:48)((cid:104)w, x(cid:105) + b)x, where g(cid:48) is the
derivative of g. using the smoothness of g and the cauchy-schwartz inequality
we therefore obtain

f (v) = g((cid:104)v, x(cid:105) + b)

    g((cid:104)w, x(cid:105) + b) + g(cid:48)((cid:104)w, x(cid:105) + b)(cid:104)v     w, x(cid:105) +
  
2
    g((cid:104)w, x(cid:105) + b) + g(cid:48)((cid:104)w, x(cid:105) + b)(cid:104)v     w, x(cid:105) +
  
2
= f (w) + (cid:104)   f (w), v     w(cid:105) +
(cid:107)v     w(cid:107)2.

  (cid:107)x(cid:107)2

2

((cid:104)v     w, x(cid:105))2
((cid:107)v     w(cid:107)(cid:107)x(cid:107))2

example 12.6
    for any x     rd and y     r, let f (w) = ((cid:104)w, x(cid:105)     y)2. then, f is (2(cid:107)x(cid:107)2)-

    for any x     rd and y     {  1}, let f (w) = log(1 + exp(   y(cid:104)w, x(cid:105))). then, f is

smooth.

((cid:107)x(cid:107)2/4)-smooth.

12.2

convex learning problems

recall that in our general de   nition of learning (de   nition 3.4 in chapter 3), we
have a hypothesis class h, a set of examples z, and a id168 (cid:96) : h    z    
r+. so far in the book we have mainly thought of z as being the product of an
instance space and a target space, z = x   y, and h being a set of functions from
x to y. however, h can be an arbitrary set. indeed, throughout this chapter,
we consider hypothesis classes h that are subsets of the euclidean space rd.
that is, every hypothesis is some real-valued vector. we shall, therefore, denote
a hypothesis in h by w. now we can    nally de   ne convex learning problems:
definition 12.10 (convex learning problem) a learning problem, (h, z, (cid:96)),
is called convex if the hypothesis class h is a convex set and for all z     z, the
id168, (cid:96)(  , z), is a convex function (where, for any z, (cid:96)(  , z) denotes the
function f : h     r de   ned by f (w) = (cid:96)(w, z)).

example 12.7 (id75 with the squared loss) recall that linear
regression is a tool for modeling the relationship between some    explanatory   
variables and some real valued outcome (see chapter 9). the domain set x
is a subset of rd, for some d, and the label set y is the set of real numbers.
we would like to learn a linear function h : rd     r that best approximates
the relationship between our variables. in chapter 9 we de   ned the hypothesis
class as the set of homogenous linear functions, h = {x (cid:55)    (cid:104)w, x(cid:105) : w     rd},
and used the squared id168, (cid:96)(h, (x, y)) = (h(x)     y)2. however, we can
equivalently model the learning problem as a convex learning problem as follows.

164

convex learning problems

each linear function is parameterized by a vector w     rd. hence, we can de   ne
h to be the set of all such parameters, namely, h = rd. the set of examples is
z = x   y = rd  r = rd+1, and the id168 is (cid:96)(w, (x, y)) = ((cid:104)w, x(cid:105)   y)2.
clearly, the set h is a convex set. the id168 is also convex with respect
to its    rst argument (see example 12.2).

if (cid:96) is a convex id168 and the class h is convex, then the
lemma 12.11
ermh problem, of minimizing the empirical loss over h, is a convex optimiza-
tion problem (that is, a problem of minimizing a convex function over a convex
set).

proof recall that the ermh problem is de   ned by

ermh(s) = argmin
w   h

ls(w).

(cid:80)m

since, for a sample s = z1, . . . , zm, for every w, ls(w) = 1
i=1 (cid:96)(w, zi),
m
claim 12.5 implies that ls(w) is a convex function. therefore, the erm rule
is a problem of minimizing a convex function subject to the constraint that the
solution should be in a convex set.

under mild conditions, such problems can be solved e   ciently using generic
optimization algorithms. in particular, in chapter 14 we will present a very
simple algorithm for minimizing convex functions.

12.2.1

learnability of convex learning problems

we have argued that for many cases, implementing the erm rule for convex
learning problems can be done e   ciently. but is convexity a su   cient condition
for the learnability of a problem?

to make the quesion more speci   c: in vc theory, we saw that halfspaces in
d-dimension are learnable (perhaps ine   ciently). we also argued in chapter 9
using the    discretization trick    that if the problem is of d parameters, it is
learnable with a sample complexity being a function of d. that is, for a constant
d, the problem should be learnable. so, maybe all convex learning problems over
rd, are learnable?

example 12.8 later shows that the answer is negative, even when d is low. not
all convex learning problems over rd are learnable. there is no contradiction
to vc theory since vc theory only deals with binary classi   cation while here
we consider a wide family of problems. there is also no contradiction to the
   discretization trick    as there we assumed that the id168 is bounded and
also assumed that a representation of each parameter using a    nite number of
bits su   ces. as we will show later, under some additional restricting conditions
that hold in many practical scenarios, convex problems are learnable.
example 12.8 (nonlearnability of id75 even if d = 1) let h = r,
and the loss be the squared loss: (cid:96)(w, (x, y)) = (wx     y)2 (we   re referring to the

12.2 convex learning problems

165

choose   = 1/100,    = 1/2, let m     m( ,   ), and set    = log(100/99)

homogenous case). let a be any deterministic algorithm.1 assume, by way of
contradiction, that a is a successful pac learner for this problem. that is, there
exists a function m(  ,  ), such that for every distribution d and for every  ,    if
a receives a training set of size m     m( ,   ), it should output, with id203
of at least 1       , a hypothesis   w = a(s), such that ld(   w)     minw ld(w)      .
. we will
de   ne two distributions, and will show that a is likely to fail on at least one
of them. the    rst distribution, d1, is supported on two examples, z1 = (1, 0)
and z2 = (  ,   1), where the id203 mass of the    rst example is    while the
id203 mass of the second example is 1       . the second distribution, d2, is
supported entirely on z2.

2m

observe that for both distributions, the id203 that all examples of the
training set will be of the second type is at least 99%. this is trivially true for
d2, whereas for d1, the id203 of this event is
(1       )m     e   2  m = 0.99.

since we assume that a is a deterministic algorithm, upon receiving a training
set of m examples, each of which is (  ,   1), the algorithm will output some   w.
now, if   w <    1/(2  ), we will set the distribution to be d1. hence,

ld1(   w)       (   w)2     1/(4  ).

ld1 (w)     ld1 (0) = (1       ).

min

w

however,

it follows that

ld1(   w)     min

w

ld1(w)     1
4  

    (1       ) >  .

therefore, such algorithm a fails on d1. on the other hand, if   w        1/(2  )
then we   ll set the distribution to be d2. then we have that ld2(   w)     1/4 while
minw ld2(w) = 0, so a fails on d2. in summary, we have shown that for every
a there exists a distribution on which a fails, which implies that the problem is
not pac learnable.

a possible solution to this problem is to add another constraint on the hypoth-
esis class. in addition to the convexity requirement, we require that h will be
bounded ; namely, we assume that for some prede   ned scalar b, every hypothesis
w     h satis   es (cid:107)w(cid:107)     b.

boundedness and convexity alone are still not su   cient for ensuring that the

problem is learnable, as the following example demonstrates.

example 12.9 as in example 12.8, consider a regression problem with the
squared loss. however, this time let h = {w : |w|     1}     r be a bounded

1 namely, given s the output of a is determined. this requirement is for the sake of

simplicity. a slightly more involved argument will show that nondeterministic algorithms
will also fail to learn the problem.

166

convex learning problems

hypothesis class. it is easy to verify that h is convex. the argument will be
the same as in example 12.8, except that now the two distributions, d1,d2 will
be supported on z1 = (1/  , 0) and z2 = (1,   1). if the algorithm a returns
  w <    1/2 upon receiving m examples of the second type, then we will set the
distribution to be d1 and have that

ld1(   w)     min

ld1(w)       (   w/  )2     ld1 (0)     1/(4  )     (1       ) >  .
similarly, if   w        1/2 we will set the distribution to be d2 and have that

w

ld2(   w)     min

w

ld2(w)     (   1/2 + 1)2     0 >  .

this example shows that we need additional assumptions on the learning
problem, and this time the solution is in lipschitzness or smoothness of the
id168. this motivates a de   nition of two families of learning problems,
convex-lipschitz-bounded and convex-smooth-bounded, which are de   ned later.

12.2.2

convex-lipschitz/smooth-bounded learning problems

definition 12.12 (convex-lipschitz-bounded learning problem) a learning
problem, (h, z, (cid:96)), is called convex-lipschitz-bounded, with parameters   , b if
the following holds:
    the hypothesis class h is a convex set and for all w     h we have (cid:107)w(cid:107)     b.
    for all z     z, the id168, (cid:96)(  , z), is a convex and   -lipschitz function.

example 12.10 let x = {x     rd : (cid:107)x(cid:107)       } and y = r. let h = {w     rd :
(cid:107)w(cid:107)     b} and let the id168 be (cid:96)(w, (x, y)) = |(cid:104)w, x(cid:105)     y|. this corre-
sponds to a regression problem with the absolute-value loss, where we assume
that the instances are in a ball of radius    and we restrict the hypotheses to be
homogenous linear functions de   ned by a vector w whose norm is bounded by
b. then, the resulting problem is convex-lipschitz-bounded with parameters
  , b.

definition 12.13 (convex-smooth-bounded learning problem) a learning
problem, (h, z, (cid:96)), is called convex-smooth-bounded, with parameters   , b if
the following holds:
    the hypothesis class h is a convex set and for all w     h we have (cid:107)w(cid:107)     b.
    for all z     z, the id168, (cid:96)(  , z), is a convex, nonnegative, and   -smooth

function.

note that we also required that the id168 is nonnegative. this is needed
to ensure that the id168 is self-bounded, as described in the previous
section.

12.3 surrogate id168s

167

example 12.11 let x = {x     rd : (cid:107)x(cid:107)       /2} and y = r. let h = {w    
rd : (cid:107)w(cid:107)     b} and let the id168 be (cid:96)(w, (x, y)) = ((cid:104)w, x(cid:105)     y)2. this
corresponds to a regression problem with the squared loss, where we assume that
the instances are in a ball of radius   /2 and we restrict the hypotheses to be
homogenous linear functions de   ned by a vector w whose norm is bounded by b.
then, the resulting problem is convex-smooth-bounded with parameters   , b.

we claim that these two families of learning problems are learnable. that is,
the properties of convexity, boundedness, and lipschitzness or smoothness of the
id168 are su   cient for learnability. we will prove this claim in the next
chapters by introducing algorithms that learn these problems successfully.

12.3

surrogate id168s

as mentioned, and as we will see in the next chapters, convex problems can
be learned e      ciently. however, in many cases, the natural id168 is not
convex and, in particular, implementing the erm rule is hard.

as an example, consider the problem of learning the hypothesis class of half-

spaces with respect to the 0     1 loss. that is,

(cid:96)0   1(w, (x, y)) = 1[y(cid:54)=sign((cid:104)w,x(cid:105))] = 1[y(cid:104)w,x(cid:105)   0].

this id168 is not convex with respect to w and indeed, when trying to
minimize the empirical risk with respect to this id168 we might encounter
local minima (see exercise 1). furthermore, as discussed in chapter 8, solving
the erm problem with respect to the 0    1 loss in the unrealizable case is known
to be np-hard.

to circumvent the hardness result, one popular approach is to upper bound
the nonconvex id168 by a convex surrogate id168. as its name
indicates, the requirements from a convex surrogate loss are as follows:

1. it should be convex.

2. it should upper bound the original loss.

for example, in the context of learning halfspaces, we can de   ne the so-called
hinge loss as a convex surrogate for the 0     1 loss, as follows:
(cid:96)hinge(w, (x, y)) def= max{0, 1     y(cid:104)w, x(cid:105)}.

clearly, for all w and all (x, y), (cid:96)0   1(w, (x, y))     (cid:96)hinge(w, (x, y)). in addition,
the convexity of the hinge loss follows directly from claim 12.5. hence, the hinge
loss satis   es the requirements of a convex surrogate id168 for the zero-one
loss. an illustration of the functions (cid:96)0   1 and (cid:96)hinge is given in the following.

168

convex learning problems

(cid:96)hinge

(cid:96)0   1

1

1

y(cid:104)w, x(cid:105)

once we have de   ned the surrogate convex loss, we can learn the problem with
respect to it. the generalization requirement from a hinge loss learner will have
the form

lhinged

(a(s))     min

w   h lhinged

(w) +  ,

where lhinged
can lower bound the left-hand side by l0   1d (a(s)), which yields

(w) = e(x,y)   d[(cid:96)hinge(w, (x, y))]. using the surrogate property, we

w   h lhinged
we can further rewrite the upper bound as follows:

l0   1d (a(s))     min
(cid:18)

l0   1d (a(s))     min

w   h l0   1d (w) +

w   h lhinged
min

(w) +  .

(cid:19)
w   h l0   1d (w)

(w)     min

+  .

that is, the 0   1 error of the learned predictor is upper bounded by three terms:
    approximation error : this is the term minw   h l0   1d (w), which measures how
well the hypothesis class performs on the distribution. we already elabo-
rated on this error term in chapter 5.
    estimation error : this is the error that results from the fact that we only
receive a training set and do not observe the distribution d. we already
elaborated on this error term in chapter 5.

(cid:17)
(w)     minw   h l0   1d (w)
that measures the di   erence between the approximation error with respect
to the surrogate loss and the approximation error with respect to the orig-
inal loss. the optimization error is a result of our inability to minimize the
training loss with respect to the original loss. the size of this error depends
on the speci   c distribution of the data and on the speci   c surrogate loss
we are using.

    optimization error : this is the term

minw   h lhinged

(cid:16)

12.4

summary

we introduced two families of learning problems: convex-lipschitz-bounded and
convex-smooth-bounded. in the next two chapters we will describe two generic

12.5 bibliographic remarks

169

learning algorithms for these families. we also introduced the notion of convex
surrogate id168, which enables us also to utilize the convex machinery for
nonconvex problems.

12.5

bibliographic remarks

12.6

there are several excellent books on convex analysis and optimization (boyd &
vandenberghe 2004, borwein & lewis 2006, bertsekas 1999, hiriart-urruty &
lemar  echal 1996). regarding learning problems, the family of convex-lipschitz-
bounded problems was    rst studied by zinkevich (2003) in the context of online
learning and by shalev-shwartz, shamir, sridharan & srebro (2009) in the con-
text of pac learning.

not a global minimum of ls.

exercises
1. construct an example showing that the 0   1 id168 may su   er from
local minima; namely, construct a training sample s     (x   {  1})m (say, for
x = r2), for which there exist a vector w and some   > 0 such that
1. for any w(cid:48) such that (cid:107)w     w(cid:48)(cid:107)       we have ls(w)     ls(w(cid:48)) (where the
loss here is the 0   1 loss). this means that w is a local minimum of ls.
2. there exists some w    such that ls(w   ) < ls(w). this means that w is
2. consider the learning problem of id28: let h = x = {x    
rd : (cid:107)x(cid:107)     b}, for some scalar b > 0, let y = {  1}, and let the loss
function (cid:96) be de   ned as (cid:96)(w, (x, y)) = log(1 + exp(   y(cid:104)w, x(cid:105))). show that
the resulting learning problem is both convex-lipschitz-bounded and convex-
smooth-bounded. specify the parameters of lipschitzness and smoothness.
3. consider the problem of learning halfspaces with the hinge loss. we limit our
domain to the euclidean ball with radius r. that is, x = {x : (cid:107)x(cid:107)2     r}.
the label set is y = {  1} and the id168 (cid:96) is de   ned by (cid:96)(w, (x, y)) =
max{0, 1     y(cid:104)w, x(cid:105)}. we already know that the id168 is convex. show
that it is r-lipschitz.

4. (*) convex-lipschitz-boundedness is not su   cient for computa-
tional e   ciency:
in the next chapter we show that from the statistical
perspective, all convex-lipschitz-bounded problems are learnable (in the ag-
nostic pac model). however, our main motivation to learn such problems
resulted from the computational perspective     id76 is often
e   ciently solvable. yet the goal of this exercise is to show that convexity
alone is not su   cient for e   ciency. we show that even for the case d = 1,
there is a convex-lipschitz-bounded problem which cannot be learned by any
computable learner.
let the hypothesis class be h = [0, 1] and let the example domain, z, be

170

convex learning problems

the set of all turing machines. de   ne the id168 as follows. for every
turing machine t     z, let (cid:96)(0, t ) = 1 if t halts on the input 0 and (cid:96)(0, t ) = 0
if t doesn   t halt on the input 0. similarly, let (cid:96)(1, t ) = 0 if t halts on the
input 0 and (cid:96)(1, t ) = 1 if t doesn   t halt on the input 0. finally, for h     (0, 1),
let (cid:96)(h, t ) = h(cid:96)(0, t ) + (1     h)(cid:96)(1, t ).
1. show that the resulting learning problem is convex-lipschitz-bounded.
2. show that no computable algorithm can learn the problem.

13 id173 and stability

in the previous chapter we introduced the families of convex-lipschitz-bounded
and convex-smooth-bounded learning problems. in this section we show that all
learning problems in these two families are learnable. for some learning problems
of this type it is possible to show that uniform convergence holds; hence they
are learnable using the erm rule. however, this is not true for all learning
problems of this type. yet, we will introduce another learning rule and will show
that it learns all convex-lipschitz-bounded and convex-smooth-bounded learning
problems.

the new learning paradigm we introduce in this chapter is called regularized
loss minimization, or rlm for short. in rlm we minimize the sum of the em-
pirical risk and a id173 function. intuitively, the id173 function
measures the complexity of hypotheses. indeed, one interpretation of the reg-
ularization function is the structural risk minimization paradigm we discussed
in chapter 7. another view of id173 is as a stabilizer of the learning
algorithm. an algorithm is considered stable if a slight change of its input does
not change its output much. we will formally de   ne the notion of stability (what
we mean by    slight change of input    and by    does not change much the out-
put   ) and prove its close relation to learnability. finally, we will show that using
the squared (cid:96)2 norm as a id173 function stabilizes all convex-lipschitz or
convex-smooth learning problems. hence, rlm can be used as a general learning
rule for these families of learning problems.

13.1

regularized loss minimization

regularized loss minimization (rlm) is a learning rule in which we jointly min-
imize the empirical risk and a id173 function. formally, a id173
function is a mapping r : rd     r, and the regularized loss minimization rule
outputs a hypothesis in

argmin

w

(ls(w) + r(w)) .

(13.1)

regularized loss minimization shares similarities with minimum description length
algorithms and structural risk minimization (see chapter 7). intuitively, the
   complexity    of hypotheses is measured by the value of the id173 func-

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

172

id173 and stability

tion, and the algorithm balances between low empirical risk and    simpler,    or
   less complex,    hypotheses.

there are many possible id173 functions one can use, re   ecting some
prior belief about the problem (similarly to the description language in minimum
description length). throughout this section we will focus on one of the most
simple id173 functions: r(w) =   (cid:107)w(cid:107)2, where    > 0 is a scalar and the
norm is the (cid:96)2 norm, (cid:107)w(cid:107) =

i . this yields the learning rule:

(cid:113)(cid:80)d

i=1 w2

(cid:0)ls(w) +   (cid:107)w(cid:107)2(cid:1) .

a(s) = argmin

w

(13.2)

this type of id173 function is often called tikhonov id173.

as mentioned before, one interpretation of equation (13.2) is using structural
risk minimization, where the norm of w is a measure of its    complexity.    recall
that in the previous chapter we introduced the notion of bounded hypothesis
classes. therefore, we can de   ne a sequence of hypothesis classes, h1     h2    
h3 . . ., where hi = {w : (cid:107)w(cid:107)2     i}. if the sample complexity of each hi depends
on i then the rlm rule is similar to the srm rule for this sequence of nested
classes.

a di   erent interpretation of id173 is as a stabilizer. in the next section
we de   ne the notion of stability and prove that stable learning rules do not
over   t. but    rst, let us demonstrate the rlm rule for id75 with the
squared loss.

13.1.1

ridge regression

applying the rlm rule with tikhonov id173 to id75 with
the squared loss, we obtain the following learning rule:

(cid:32)

m(cid:88)

i=1

(cid:33)

argmin
w   rd

  (cid:107)w(cid:107)2

2 +

1
m

((cid:104)w, xi(cid:105)     yi)2

1
2

.

(13.3)

performing id75 using equation (13.3) is called ridge regression.

to solve equation (13.3) we compare the gradient of the objective to zero and

obtain the set of linear equations

(2  mi + a)w = b,

(cid:32) m(cid:88)

(cid:33)

a =

xi x(cid:62)

i

m(cid:88)

where i is the identity matrix and a, b are as de   ned in equation (9.6), namely,

and

b =

yixi .

(13.4)

i=1

i=1

since a is a positive semide   nite matrix, the matrix 2  mi + a has all its eigen-
values bounded below by 2  m. hence, this matrix is invertible and the solution
to ridge regression becomes

w = (2  mi + a)   1 b.

(13.5)

13.2 stable rules do not over   t

173

in the next section we formally show how id173 stabilizes the algo-
rithm and prevents over   tting. in particular, the analysis presented in the next
sections (particularly, corollary 13.11) will yield:
theorem 13.1 let d be a distribution over x    [   1, 1], where x = {x    
rd : (cid:107)x(cid:107)     1}. let h = {w     rd : (cid:107)w(cid:107)     b}. for any       (0, 1), let m    
150 b2/ 2. then, applying the ridge regression algorithm with parameter    =
 /(3b2) satis   es

e

s   dm

[ld(a(s))]     min

w   h ld(w) +  .

remark 13.1 the preceding theorem tells us how many examples are needed
to guarantee that the expected value of the risk of the learned predictor will be
bounded by the approximation error of the class plus  . in the usual de   nition
of agnostic pac learning we require that the risk of the learned predictor will
be bounded with id203 of at least 1       . in exercise 1 we show how an
algorithm with a bounded expected risk can be used to construct an agnostic
pac learner.

13.2

stable rules do not over   t

intuitively, a learning algorithm is stable if a small change of the input to the
algorithm does not change the output of the algorithm much. of course, there
are many ways to de   ne what we mean by    a small change of the input    and
what we mean by    does not change the output much   . in this section we de   ne
a speci   c notion of stability and prove that under this de   nition, stable rules do
not over   t.

let a be a learning algorithm, let s = (z1, . . . , zm) be a training set of m
examples, and let a(s) denote the output of a. the algorithm a su   ers from
over   tting if the di   erence between the true risk of its output, ld(a(s)), and the
empirical risk of its output, ls(a(s)), is large. as mentioned in remark 13.1,
throughout this chapter we focus on the expectation (with respect to the choice
of s) of this quantity, namely, es[ld(a(s))     ls(a(s))].
we next de   ne the notion of stability. given the training set s and an ad-
ditional example z(cid:48), let s(i) be the training set obtained by replacing the i   th
example of s with z(cid:48); namely, s(i) = (z1, . . . , zi   1, z(cid:48), zi+1, . . . , zm). in our de   -
nition of stability,    a small change of the input    means that we feed a with s(i)
instead of with s. that is, we only replace one training example. we measure
the e   ect of this small change of the input on the output of a, by comparing
the loss of the hypothesis a(s) on zi to the loss of the hypothesis a(s(i)) on zi.
intuitively, a good learning algorithm will have (cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)     0,
since in the    rst term the learning algorithm does not observe the example zi
while in the second term zi is indeed observed. if the preceding di   erence is very
large we suspect that the learning algorithm might over   t. this is because the

174

id173 and stability

learning algorithm drastically changes its prediction on zi if it observes it in the
training set. this is formalized in the following theorem.
theorem 13.2 let d be a distribution. let s = (z1, . . . , zm) be an i.i.d. se-
quence of examples and let z(cid:48) be another i.i.d. example. let u (m) be the uniform
distribution over [m]. then, for any learning algorithm,

e

s   dm

[ld(a(s))     ls(a(s))] =

e

(s,z(cid:48))   dm+1,i   u (m)

[(cid:96)(a(s(i), zi))     (cid:96)(a(s), zi)].
(13.6)

proof since s and z(cid:48) are both drawn i.i.d. from d, we have that for every i,

e
[ld(a(s))] = e
s

s,z(cid:48)[(cid:96)(a(s), z(cid:48))] = e

s,z(cid:48)[(cid:96)(a(s(i)), zi)].

on the other hand, we can write

e
[ls(a(s))] = e
s

s,i

[(cid:96)(a(s), zi)].

combining the two equations we conclude our proof.

when the right-hand side of equation (13.6) is small, we say that a is a stable
algorithm     changing a single example in the training set does not lead to a
signi   cant change. formally,
definition 13.3 (on-average-replace-one-stable) let   : n     r be a mono-
tonically decreasing function. we say that a learning algorithm a is on-average-
replace-one-stable with rate  (m) if for every distribution d

e

(s,z(cid:48))   dm+1,i   u (m)

[(cid:96)(a(s(i), zi))     (cid:96)(a(s), zi)]      (m).

theorem 13.2 tells us that a learning algorithm does not over   t if and only
if it is on-average-replace-one-stable. of course, a learning algorithm that does
not over   t is not necessarily a good learning algorithm     take, for example, an
algorithm a that always outputs the same hypothesis. a useful algorithm should
   nd a hypothesis that on one hand    ts the training set (i.e., has a low empirical
risk) and on the other hand does not over   t. or, in light of theorem 13.2, the
algorithm should both    t the training set and at the same time be stable. as we
shall see, the parameter    of the rlm rule balances between    tting the training
set and being stable.

13.3

tikhonov id173 as a stabilizer

in the previous section we saw that stable rules do not over   t. in this section we
show that applying the rlm rule with tikhonov id173,   (cid:107)w(cid:107)2, leads to
a stable algorithm. we will assume that the id168 is convex and that it
is either lipschitz or smooth.

the main property of the tikhonov id173 that we rely on is that it

makes the objective of rlm strongly convex, as de   ned in the following.

13.3 tikhonov id173 as a stabilizer

175

definition 13.4 (strongly convex functions) a function f is   -strongly con-
vex if for all w, u and        (0, 1) we have

f (  w + (1       )u)       f (w) + (1       )f (u)       
2

  (1       )(cid:107)w     u(cid:107)2.

clearly, every convex function is 0-strongly convex. an illustration of strong

convexity is given in the following    gure.

f (u)

f (w)

      

2   (1       )(cid:107)u     w(cid:107)2

w

  w + (1       )u

u

the following lemma implies that the objective of rlm is (2  )-strongly con-

vex. in addition, it underscores an important property of strong convexity.

lemma 13.5
1. the function f (w) =   (cid:107)w(cid:107)2 is 2  -strongly convex.
2. if f is   -strongly convex and g is convex, then f + g is   -strongly convex.
3. if f is   -strongly convex and u is a minimizer of f , then, for any w,

f (w)     f (u)       
2

(cid:107)w     u(cid:107)2.

proof the    rst two points follow directly from the de   nition. to prove the last
point, we divide the de   nition of strong convexity by    and rearrange terms to
get that

f (u +   (w     u))     f (u)

  

    f (w)     f (u)       
2

(1       )(cid:107)w     u(cid:107)2.

taking the limit        0 we obtain that the right-hand side converges to f (w)    
f (u)      
2(cid:107)w    u(cid:107)2. on the other hand, the left-hand side becomes the derivative
of the function g(  ) = f (u +   (w     u)) at    = 0. since u is a minimizer of f ,
it follows that    = 0 is a minimizer of g, and therefore the left-hand side of the
preceding goes to zero in the limit        0, which concludes our proof.

we now turn to prove that rlm is stable. let s = (z1, . . . , zm) be a training
set, let z(cid:48) be an additional example, and let s(i) = (z1, . . . , zi   1, z(cid:48), zi+1, . . . , zm).
let a be the rlm rule, namely,

(cid:0)ls(w) +   (cid:107)w(cid:107)2(cid:1) .

a(s) = argmin

w

176

id173 and stability

denote fs(w) = ls(w) +   (cid:107)w(cid:107)2, and based on lemma 13.5 we know that fs is
(2  )-strongly convex. relying on part 3 of the lemma, it follows that for any v,

fs(v)     fs(a(s))       (cid:107)v     a(s)(cid:107)2.

on the other hand, for any v and u, and for all i, we have

fs(v)     fs(u) = ls(v) +   (cid:107)v(cid:107)2     (ls(u) +   (cid:107)u(cid:107)2)

= ls(i)(v) +   (cid:107)v(cid:107)2     (ls(i) (u) +   (cid:107)u(cid:107)2)
(cid:96)(u, z(cid:48))     (cid:96)(v, z(cid:48))

(cid:96)(v, zi)     (cid:96)(u, zi)

+

+

m

in particular, choosing v = a(s(i)), u = a(s), and using the fact that v mini-
mizes ls(i) (w) +   (cid:107)w(cid:107)2, we obtain that
fs(a(s(i)))   fs(a(s))     (cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)

(cid:96)(a(s), z(cid:48))     (cid:96)(a(s(i)), z(cid:48))

+

.

m

(13.9)

m

m

(13.7)

(13.8)

.

combining this with equation (13.7) we obtain that

  (cid:107)a(s(i))    a(s)(cid:107)2     (cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)

+

m

(cid:96)(a(s), z(cid:48))     (cid:96)(a(s(i)), z(cid:48))

.

m

(13.10)
the two subsections that follow continue the stability analysis for either lip-
schitz or smooth id168s. for both families of id168s we show that
rlm is stable and therefore it does not over   t.

13.3.1

lipschitz loss
if the id168, (cid:96)(  , zi), is   -lipschitz, then by the de   nition of lipschitzness,

(cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)       (cid:107)a(s(i))     a(s)(cid:107).

(13.11)

similarly,

(cid:96)(a(s), z(cid:48))     (cid:96)(a(s(i)), z(cid:48))       (cid:107)a(s(i))     a(s)(cid:107).

plugging these inequalities into equation (13.10) we obtain

  (cid:107)a(s(i))     a(s)(cid:107)2     2   (cid:107)a(s(i))     a(s)(cid:107)

m

,

which yields

(cid:107)a(s(i))     a(s)(cid:107)     2   
   m

.

plugging the preceding back into equation (13.11) we conclude that

(cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)     2   2
   m
since this holds for any s, z(cid:48), i we immediately obtain:

.

13.3 tikhonov id173 as a stabilizer

177

corollary 13.6 assume that the id168 is convex and   -lipschitz.
then, the rlm rule with the regularizer   (cid:107)w(cid:107)2 is on-average-replace-one-stable
with rate 2   2

   m . it follows (using theorem 13.2) that

e

s   dm

[ld(a(s))     ls(a(s))]     2   2
   m

.

13.3.2

smooth and nonnegative loss

if the loss is   -smooth and nonnegative then it is also self-bounded (see sec-
tion 12.1):

we further assume that        2  
smoothness assumption we have that

(cid:107)   f (w)(cid:107)2     2  f (w).
m , or, in other words, that          m/2. by the

(13.12)

(cid:96)(a(s(i)), zi)   (cid:96)(a(s), zi)     (cid:104)   (cid:96)(a(s), zi), a(s(i))   a(s)(cid:105)+

(cid:107)a(s(i))   a(s)(cid:107)2 .
(13.13)
using the cauchy-schwartz inequality and equation (12.6) we further obtain
that

  
2

(cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)

    (cid:107)   (cid:96)(a(s), zi)(cid:107)(cid:107)a(s(i))     a(s)(cid:107) +

   (cid:112)2  (cid:96)(a(s), zi)(cid:107)a(s(i))     a(s)(cid:107) +

(cid:107)a(s(i))     a(s)(cid:107)2
  
2
(cid:107)a(s(i))     a(s)(cid:107)2 .
  
2

(13.14)

by a symmetric argument it holds that,

(cid:96)(a(s), z(cid:48))     (cid:96)(a(s(i)), z(cid:48))

   (cid:113)

2  (cid:96)(a(s(i)), z(cid:48))(cid:107)a(s(i))     a(s)(cid:107) +

(cid:107)a(s(i))     a(s)(cid:107)2 .

  
2

plugging these inequalities into equation (13.10) and rearranging terms we ob-
tain that

(cid:107)a(s(i))     a(s)(cid:107)    

(cid:96)(a(s(i)), z(cid:48))

.

(cid:113)

(cid:18)(cid:112)(cid:96)(a(s), zi) +
(cid:18)(cid:112)(cid:96)(a(s), zi) +
(cid:113)

   

2  

(   m       )

   

8  
   m

(cid:19)

(cid:19)

combining the preceding with the assumption          m/2 yields

(cid:107)a(s(i))     a(s)(cid:107)    

(cid:96)(a(s(i)), z(cid:48))

.

178

id173 and stability

combining the preceding with equation (13.14) and again using the assumption
         m/2 yield

   

(cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)

   (cid:112)2  (cid:96)(a(s), zi)(cid:107)a(s(i))     a(s)(cid:107) +
(cid:19)(cid:18)(cid:112)(cid:96)(a(s), zi) +
(cid:18) 4  
(cid:113)
(cid:19)2
(cid:18)(cid:112)(cid:96)(a(s), zi) +
(cid:17)
(cid:16)

(cid:96)(a(s(i)), z(cid:48))

8  2
(  m)2

(cid:96)(a(s), zi) + (cid:96)(a(s(i)), z(cid:48))

(cid:113)

    8  
  m
    24  
  m

  m

+

,

(cid:107)a(s(i))     a(s)(cid:107)2

  
2
(cid:96)(a(s(i)), z(cid:48))

(cid:19)2

where in the last step we used the inequality (a+b)2     3(a2+b2). taking expecta-
tion with respect to s, z(cid:48), i and noting that e[(cid:96)(a(s), zi)] = e[(cid:96)(a(s(i)), z(cid:48))] =
e[ls(a(s))], we conclude that:
corollary 13.7 assume that the id168 is   -smooth and nonnegative.
then, the rlm rule with the regularizer   (cid:107)w(cid:107)2, where        2  

m , satis   es

(cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)

e[ls(a(s))].

e(cid:104)

(cid:105)     48  

  m

note that if for all z we have (cid:96)(0, z)     c, for some scalar c > 0, then for

every s,

ls(a(s))     ls(a(s)) +   (cid:107)a(s)(cid:107)2     ls(0) +   (cid:107)0(cid:107)2 = ls(0)     c.

hence, corollary 13.7 also implies that

e(cid:104)

(cid:96)(a(s(i)), zi)     (cid:96)(a(s), zi)

(cid:105)     48    c

  m

.

13.4

controlling the fitting-stability tradeo   

we can rewrite the expected risk of a learning algorithm as

e
[ld(a(s))] = e
s

s

[ls(a(s))] + e

[ld(a(s))     ls(a(s))].

s

(13.15)

the    rst term re   ects how well a(s)    ts the training set while the second term
re   ects the di   erence between the true and empirical risks of a(s). as we have
shown in theorem 13.2, the second term is equivalent to the stability of a. since
our goal is to minimize the risk of the algorithm, we need that the sum of both
terms will be small.

in the previous section we have bounded the stability term. we have shown
that the stability term decreases as the id173 parameter,   , increases.
on the other hand, the empirical risk increases with   . we therefore face a

13.4 controlling the fitting-stability tradeo   

179

tradeo    between    tting and over   tting. this tradeo    is quite similar to the bias-
complexity tradeo    we discussed previously in the book.

we now derive bounds on the empirical risk term for the rlm rule. recall

(cid:0)ls(w) +   (cid:107)w(cid:107)2(cid:1). fix some

that the rlm rule is de   ned as a(s) = argminw
arbitrary vector w   . we have

ls(a(s))     ls(a(s)) +   (cid:107)a(s)(cid:107)2     ls(w   ) +   (cid:107)w   (cid:107)2.

taking expectation of both sides with respect to s and noting that es[ls(w   )] =
ld(w   ), we obtain that
[ls(a(s))]     ld(w   ) +   (cid:107)w   (cid:107)2.
e
s

(13.16)

plugging this into equation (13.15) we obtain
[ld(a(s))]     ld(w   ) +   (cid:107)w   (cid:107)2 + e
e
s

s

[ld(a(s))     ls(a(s))].

combining the preceding with corollary 13.6 we conclude:

corollary 13.8 assume that the id168 is convex and   -lipschitz.
then, the rlm rule with the id173 function   (cid:107)w(cid:107)2 satis   es

   w   , e

[ld(a(s))]     ld(w   ) +   (cid:107)w   (cid:107)2 +

s

2  2
   m

.

this bound is often called an oracle inequality     if we think of w    as a hy-
pothesis with low risk, the bound tells us how many examples are needed so that
a(s) will be almost as good as w   , had we known the norm of w   . in practice,
however, we usually do not know the norm of w   . we therefore usually tune   
on the basis of a validation set, as described in chapter 11.

we can also easily derive a pac-like guarantee1 from corollary 13.8 for convex-

lipschitz-bounded learning problems:
corollary 13.9 let (h, z, (cid:96)) be a convex-lipschitz-bounded learning problem
b2 m . then, the
with parameters   , b. for any training set size m, let    =
rlm rule with the id173 function   (cid:107)w(cid:107)2 satis   es

(cid:113) 2  2

[ld(a(s))]     min
e
s

w   h ld(w) +    b

(cid:114) 8

.

m

in particular, for every   > 0, if m     8  2b2
es[ld(a(s))]     minw   h ld(w) +  .

 2

then for every distribution d,

the preceding corollary holds for lipschitz id168s. if instead the loss
function is smooth and nonnegative, then we can combine equation (13.16) with
corollary 13.7 to get:

1 again, the bound below is on the expected risk, but using exercise 1 it can be used to

derive an agnostic pac learning guarantee.

180

id173 and stability

corollary 13.10 assume that the id168 is convex,   -smooth, and
nonnegative. then, the rlm rule with the id173 function   (cid:107)w(cid:107)2, for
       2  

m , satis   es the following for all w   :
[ld(a(s))]    
e
s

[ls(a(s))]    
e
s

48  
  m

1 +

(cid:18)

(cid:19)

(cid:18)

(cid:19)(cid:0)ld(w   ) +   (cid:107)w   (cid:107)2(cid:1) .

1 +

48  
  m

for example, if we choose    = 48  

m we obtain from the preceding that the
expected true risk of a(s) is at most twice the expected empirical risk of a(s).
furthermore, for this value of   , the expected empirical risk of a(s) is at most
ld(w   ) + 48  

m (cid:107)w   (cid:107)2.

we can also derive a learnability guarantee for convex-smooth-bounded learn-

ing problems based on corollary 13.10.
corollary 13.11 let (h, z, (cid:96)) be a convex-smooth-bounded learning problem
with parameters   , b. assume in addition that (cid:96)(0, z)     1 for all z     z. for any
and set    =  /(3b2). then, for every distribution d,
      (0, 1) let m     150  b2
[ld(a(s))]     min
e
s

w   h ld(w) +   .

 2

13.5

summary

we introduced stability and showed that if an algorithm is stable then it does not
over   t. furthermore, for convex-lipschitz-bounded or convex-smooth-bounded
problems, the rlm rule with tikhonov id173 leads to a stable learning
algorithm. we discussed how the id173 parameter,   , controls the trade-
o    between    tting and over   tting. finally, we have shown that all learning prob-
lems that are from the families of convex-lipschitz-bounded and convex-smooth-
bounded problems are learnable using the rlm rule. the rlm paradigm is the
basis for many popular learning algorithms, including ridge regression (which we
discussed in this chapter) and support vector machines (which will be discussed
in chapter 15).

in the next chapter we will present stochastic id119, which gives us
a very practical alternative way to learn convex-lipschitz-bounded and convex-
smooth-bounded problems and can also be used for e   ciently implementing the
rlm rule.

13.6

bibliographic remarks

stability is widely used in many mathematical contexts. for example, the neces-
sity of stability for so-called inverse problems to be well posed was    rst recognized
by hadamard (1902). the idea of id173 and its relation to stability be-
came widely known through the works of tikhonov (1943) and phillips (1962).

13.7 exercises

181

in the context of modern learning theory, the use of stability can be traced back
at least to the work of rogers & wagner (1978), which noted that the sensitiv-
ity of a learning algorithm with regard to small changes in the sample controls
the variance of the leave-one-out estimate. the authors used this observation to
obtain generalization bounds for the k-nearest neighbor algorithm (see chap-
ter 19). these results were later extended to other    local    learning algorithms
(see devroye, gy  or    & lugosi (1996) and references therein). in addition, practi-
cal methods have been developed to introduce stability into learning algorithms,
in particular the id112 technique introduced by (breiman 1996).

over the last decade, stability was studied as a generic condition for learnabil-
ity. see (kearns & ron 1999, bousquet & elissee    2002, kutin & niyogi 2002,
rakhlin, mukherjee & poggio 2005, mukherjee, niyogi, poggio & rifkin 2006).
our presentation follows the work of shalev-shwartz, shamir, srebro & sridha-
ran (2010), who showed that stability is su   cient and necessary for learning.
they have also shown that all convex-lipschitz-bounded learning problems are
learnable using rlm, even though for some convex-lipschitz-bounded learning
problems uniform convergence does not hold in a strong sense.

13.7

exercises

1. from bounded expected risk to agnostic pac learning: let a be
an algorithm that guarantees the following: if m     mh( ) then for every
distribution d it holds that

e

s   dm

[ld(a(s))]     min

h   h ld(h) +  .

    show that for every        (0, 1), if m     mh(    ) then with id203 of at
least 1        it holds that ld(a(s))     minh   h ld(h) +  .
hint: observe that the random variable ld(a(s))     minh   h ld(h) is
nonnegative and rely on markov   s inequality.

    for every        (0, 1) let

(cid:24) log(4/  ) + log((cid:100)log2(1/  )(cid:101))

(cid:25)

.

 2

mh( ,   ) = mh( /2)(cid:100)log2(1/  )(cid:101) +

suggest a procedure that agnostic pac learns the problem with sample
complexity of mh( ,   ), assuming that the id168 is bounded by
1.
hint: let k = (cid:100)log2(1/  )(cid:101). divide the data into k +1 chunks, where each
of the    rst k chunks is of size mh( /2) examples. train the    rst k chunks
using a. on the basis of the previous question argue that the id203
that for all of these chunks we have ld(a(s)) > minh   h ld(h) +   is
at most 2   k       /2. finally, use the last chunk as a validation set.

2. learnability without uniform convergence: let b be the unit ball of

182

id173 and stability

rd, let h = b, let z = b    {0, 1}d, and let (cid:96) : z    h     r be de   ned as
follows:

d(cid:88)

(cid:96)(w, (x,   )) =

  i(xi     wi)2.

i=1

this problem corresponds to an unsupervised learning task, meaning that we
do not try to predict the label of x. instead, what we try to do is to    nd the
   center of mass    of the distribution over b. however, there is a twist, modeled
by the vectors   . each example is a pair (x,   ), where x is the instance x and
   indicates which features of x are    active    and which are    turned o   .    a
hypothesis is a vector w representing the center of mass of the distribution,
and the id168 is the squared euclidean distance between x and w, but
only with respect to the    active    elements of x.
    show that this problem is learnable using the rlm rule with a sample

complexity that does not depend on d.

    consider a distribution d over z as follows: x is    xed to be some x0, and
each element of    is sampled to be either 1 or 0 with equal id203.
show that the rate of uniform convergence of this problem grows with
d.
hint: let m be a training set size. show that if d (cid:29) 2m, then there is
a high id203 of sampling a set of examples such that there exists
some j     [d] for which   j = 1 for all the examples in the training set.
show that such a sample cannot be  -representative. conclude that the
sample complexity of uniform convergence must grow with log(d).

    conclude that if we take d to in   nity we obtain a problem that is learnable
but for which the uniform convergence property does not hold. compare
to the fundamental theorem of statistical learning.

3. stability and asymptotic erm are su   cient for learnability:

we say that a learning rule a is an aerm (asymptotic empirical risk

minimizer) with rate  (m) if for every distribution d it holds that

e

s   dm

ls(a(s))     min

h   h ls(h)

     (m).

we say that a learning rule a learns a class h with rate  (m) if for every

distribution d it holds that

e

s   dm

prove the following:

ld(a(s))     min

h   h ld(h)

     (m).

(cid:21)

(cid:21)

(cid:20)

(cid:20)

theorem 13.12
if a learning algorithm a is on-average-replace-one-stable
with rate  1(m) and is an aerm with rate  2(m), then it learns h with rate
 1(m) +  2(m).

13.7 exercises

183

4. strong convexity with respect to general norms:

throughout the section we used the (cid:96)2 norm. in this exercise we generalize
some of the results to general norms. let (cid:107)  (cid:107) be some arbitrary norm, and let f
be a strongly convex function with respect to this norm (see de   nition 13.4).
1. show that items 2   3 of lemma 13.5 hold for every norm.
2. (*) give an example of a norm for which item 1 of lemma 13.5 does not

3. let r(w) be a function that is (2  )-strongly convex with respect to some

hold.
norm (cid:107)    (cid:107). let a be an rlm rule with respect to r, namely,

a(s) = argmin

(ls(w) + r(w)) .

w

assume that for every z, the id168 (cid:96)(  , z) is   -lipschitz with respect
to the same norm, namely,
   z,    w, v,

(cid:96)(w, z)     (cid:96)(v, z)       (cid:107)w     v(cid:107) .
prove that a is on-average-replace-one-stable with rate 2  2
  m .

4. (*) let q     (1, 2) and consider the (cid:96)q-norm

(cid:32) d(cid:88)

(cid:33)1/q

(cid:107)w(cid:107)q =

|wi|q

.

it can be shown (see, for example, shalev-shwartz (2007)) that the function

i=1

r(w) =

1

2(q     1)

(cid:107)w(cid:107)2

q

(cid:16)

(cid:17)

is 1-strongly convex with respect to (cid:107)w(cid:107)q. show that if q = log(d)
r(w) is

-strongly convex with respect to the (cid:96)1 norm over rd.

log(d)   1 then

1

3 log(d)

14 stochastic id119

recall that the goal of learning is to minimize the risk function, ld(h) =
ez   d[(cid:96)(h, z)]. we cannot directly minimize the risk function since it depends
on the unknown distribution d. so far in the book, we have discussed learning
methods that depend on the empirical risk. that is, we    rst sample a training
set s and de   ne the empirical risk function ls(h). then, the learner picks a
hypothesis based on the value of ls(h). for example, the erm rule tells us to
pick the hypothesis that minimizes ls(h) over the hypothesis class, h. or, in the
previous chapter, we discussed regularized risk minimization, in which we pick a
hypothesis that jointly minimizes ls(h) and a id173 function over h.

in this chapter we describe and analyze a rather di   erent learning approach,
which is called stochastic id119 (sgd). as in chapter 12 we will
focus on the important family of convex learning problems, and following the
notation in that chapter, we will refer to hypotheses as vectors w that come from
a convex hypothesis class, h. in sgd, we try to minimize the risk function ld(w)
directly using a id119 procedure. id119 is an iterative
optimization procedure in which at each step we improve the solution by taking
a step along the negative of the gradient of the function to be minimized at
the current point. of course, in our case, we are minimizing the risk function,
and since we do not know d we also do not know the gradient of ld(w). sgd
circumvents this problem by allowing the optimization procedure to take a step
along a random direction, as long as the expected value of the direction is the
negative of the gradient. and, as we shall see,    nding a random direction whose
expected value corresponds to the gradient is rather simple even though we do
not know the underlying distribution d.

the advantage of sgd, in the context of convex learning problems, over the
regularized risk minimization learning rule is that sgd is an e   cient algorithm
that can be implemented in a few lines of code, yet still enjoys the same sample
complexity as the regularized risk minimization rule. the simplicity of sgd also
allows us to use it in situations when it is not possible to apply methods that
are based on the empirical risk, but this is beyond the scope of this book.

we start this chapter with the basic id119 algorithm and analyze its
convergence rate for convex-lipschitz functions. next, we introduce the notion of
subgradient and show that id119 can be applied for nondi   erentiable
functions as well. the core of this chapter is section 14.3, in which we describe

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

14.1 id119

185

the stochastic id119 algorithm, along with several useful variants.
we show that sgd enjoys an expected convergence rate similar to the rate
of id119. finally, we turn to the applicability of sgd to learning
problems.

14.1

id119

before we describe the stochastic id119 method, we would like to
describe the standard id119 approach for minimizing a di   erentiable
convex function f (w).

the gradient of a di   erentiable function f : rd     r at w, denoted    f (w),
is the vector of partial derivatives of f , namely,    f (w) =
.
id119 is an iterative algorithm. we start with an initial value of w
(say, w(1) = 0). then, at each iteration, we take a step in the direction of the
negative of the gradient at the current point. that is, the update step is

(cid:16)    f (w)

   w[1] , . . . ,    f (w)

(cid:17)

   w[d]

w(t+1) = w(t)          f (w(t)),

(14.1)

where    > 0 is a parameter to be discussed later. intuitively, since the gradi-
ent points in the direction of the greatest rate of increase of f around w(t),
the algorithm makes a small step in the opposite direction, thus decreasing the
value of the function. eventually, after t iterations, the algorithm outputs the
averaged vector,   w = 1
t=1 w(t). the output could also be the last vector,
t
w(t ), or the best performing vector, argmint   [t ] f (w(t)), but taking the average
turns out to be rather useful, especially when we generalize id119 to
nondi   erentiable functions and to the stochastic case.

(cid:80)t

another way to motivate id119 is by relying on taylor approxima-
tion. the gradient of f at w yields the    rst order taylor approximation of f
around w by f (u)     f (w) + (cid:104)u     w,   f (w)(cid:105). when f is convex, this approxi-
mation lower bounds f , that is,

f (u)     f (w) + (cid:104)u     w,   f (w)(cid:105).

therefore, for w close to w(t) we have that f (w)     f (w(t))+(cid:104)w   w(t),   f (w(t))(cid:105).
hence we can minimize the approximation of f (w). however, the approximation
might become loose for w, which is far away from w(t). therefore, we would like
to minimize jointly the distance between w and w(t) and the approximation of
f around w(t). if the parameter    controls the tradeo    between the two terms,
we obtain the update rule

(cid:16)

f (w(t)) + (cid:104)w     w(t),   f (w(t))(cid:105)(cid:17)

.

w(t+1) = argmin

w

1
2

(cid:107)w     w(t)(cid:107)2 +   

solving the preceding by taking the derivative with respect to w and comparing
it to zero yields the same update rule as in equation (14.1).

186

stochastic id119

figure 14.1 an illustration of the id119 algorithm. the function to be
minimized is 1.25(x1 + 6)2 + (x2     8)2.

14.1.1

analysis of gd for convex-lipschitz functions

to analyze the convergence rate of the gd algorithm, we limit ourselves to
the case of convex-lipschitz functions (as we have seen, many problems lend
themselves easily to this setting). let w(cid:63) be any vector and let b be an upper
bound on (cid:107)w(cid:63)(cid:107). it is convenient to think of w(cid:63) as the minimizer of f (w), but
the analysis that follows holds for every w(cid:63).

we would like to obtain an upper bound on the suboptimality of our solution
t=1 w(t). from the

with respect to w(cid:63), namely, f (   w)     f (w(cid:63)), where   w = 1
de   nition of   w, and using jensen   s inequality, we have that

t

(cid:80)t

f (   w)     f (w(cid:63)) = f

w(t)

(cid:33)

f (w(t))

1
t

(cid:32)
t(cid:88)
(cid:16)
t(cid:88)
(cid:16)
t(cid:88)

t=1

t=1

t=1

    1
t

=

1
t

    f (w(cid:63))

(cid:17)     f (w(cid:63))
(cid:17)

f (w(t))     f (w(cid:63))

.

for every t, because of the convexity of f , we have that

f (w(t))     f (w(cid:63))     (cid:104)w(t)     w(cid:63),   f (w(t))(cid:105).

combining the preceding we obtain

f (   w)     f (w(cid:63))     1
t

t(cid:88)

t=1

(cid:104)w(t)     w(cid:63),   f (w(t))(cid:105).

to bound the right-hand side we rely on the following lemma:

(14.2)

(14.3)

14.1 id119

187

lemma 14.1 let v1, . . . , vt be an arbitrary sequence of vectors. any algorithm
with an initialization w(1) = 0 and an update rule of the form

satis   es

w(t+1) = w(t)       vt

t(cid:88)
(cid:104)w(t)     w(cid:63), vt(cid:105)     (cid:107)w(cid:63)(cid:107)2

+

  
2

2  

(14.4)

(14.5)

(cid:107)vt(cid:107)2.

t(cid:88)

t=1

(cid:113) b2
  2 t , then for every w(cid:63) with (cid:107)w(cid:63)(cid:107)     b we have

in particular, for every b,    > 0, if for all t we have that (cid:107)vt(cid:107)        and if we set
   =

t=1

t(cid:88)

t=1

1
t

(cid:104)w(t)     w(cid:63), vt(cid:105)     b      
t

.

proof using algebraic manipulations (completing the square), we obtain:

(cid:104)w(t)     w(cid:63), vt(cid:105) =

=

=

(cid:104)w(t)     w(cid:63),   vt(cid:105)
1
  
(   (cid:107)w(t)     w(cid:63)       vt(cid:107)2 + (cid:107)w(t)     w(cid:63)(cid:107)2 +   2(cid:107)vt(cid:107)2)
1
2  
(   (cid:107)w(t+1)     w(cid:63)(cid:107)2 + (cid:107)w(t)     w(cid:63)(cid:107)2) +
1
2  

(cid:107)vt(cid:107)2,

  
2

t(cid:88)

t=1

where the last equality follows from the de   nition of the update rule. summing
the equality over t, we have

(cid:16)   (cid:107)w(t+1)     w(cid:63)(cid:107)2 + (cid:107)w(t)     w(cid:63)(cid:107)2(cid:17)

t(cid:88)

t=1

t(cid:88)

t=1

+

  
2

(cid:107)vt(cid:107)2.

(14.6)

(cid:104)w(t)   w(cid:63), vt(cid:105) =

1
2  

the    rst sum on the right-hand side is a telescopic sum that collapses to

(cid:107)w(1)     w(cid:63)(cid:107)2     (cid:107)w(t +1)     w(cid:63)(cid:107)2.

plugging this in equation (14.6), we have

(cid:104)w(t)     w(cid:63), vt(cid:105) =

((cid:107)w(1)     w(cid:63)(cid:107)2     (cid:107)w(t +1)     w(cid:63)(cid:107)2) +

t(cid:88)

t=1

1
2  

    1
2  

=

1
2  

t(cid:88)

t=1

  
2

(cid:107)w(1)     w(cid:63)(cid:107)2 +

(cid:107)vt(cid:107)2

t(cid:88)

t=1

(cid:107)w(cid:63)(cid:107)2 +

  
2

(cid:107)vt(cid:107)2,

t(cid:88)

t=1

  
2

(cid:107)vt(cid:107)2

where the last equality is due to the de   nition w(1) = 0. this proves the    rst
part of the lemma (equation (14.5)). the second part follows by upper bounding
(cid:107)w(cid:63)(cid:107) by b, (cid:107)vt(cid:107) by   , dividing by t , and plugging in the value of   .

188

stochastic id119

lemma 14.1 applies to the gd algorithm with vt =    f (w(t)). as we will
show later in lemma 14.7, if f is   -lipschitz, then (cid:107)   f (w(t))(cid:107)       . we therefore
satisfy the lemma   s conditions and achieve the following corollary:
corollary 14.2 let f be a convex,   -lipschitz function, and let w(cid:63)     argmin{w:(cid:107)w(cid:107)   b} f (w).
if we run the gd algorithm on f for t steps with    =
vector   w satis   es

  2 t , then the output

(cid:113) b2

f (   w)     f (w(cid:63))     b      
t

.

furthermore, for every   > 0, to achieve f (   w)    f (w(cid:63))      , it su   ces to run the
gd algorithm for a number of iterations that satis   es

t     b2  2
 2

.

14.2

subgradients

the gd algorithm requires that the function f be di   erentiable. we now gener-
alize the discussion beyond di   erentiable functions. we will show that the gd
algorithm can be applied to nondi   erentiable functions by using a so-called sub-
gradient of f (w) at w(t), instead of the gradient.

to motivate the de   nition of subgradients, recall that for a convex function f ,

the gradient at w de   nes the slope of a tangent that lies below f , that is,

   u,

f (u)     f (w) + (cid:104)u     w,   f (w)(cid:105).

(14.7)

an illustration is given on the left-hand side of figure 14.2.

the existence of a tangent that lies below f is an important property of convex

functions, which is in fact an alternative characterization of convexity.
lemma 14.3 let s be an open convex set. a function f : s     r is convex i   
for every w     s there exists v such that

   u     s,

f (u)     f (w) + (cid:104)u     w, v(cid:105).

(14.8)

the proof of this lemma can be found in many convex analysis textbooks (e.g.,
(borwein & lewis 2006)). the preceding inequality leads us to the de   nition of
subgradients.

definition 14.4 (subgradients) a vector v that satis   es equation (14.8) is
called a subgradient of f at w. the set of subgradients of f at w is called the
di   erential set and denoted    f (w).

an illustration of subgradients is given on the right-hand side of figure 14.2.
for scalar functions, a subgradient of a convex function f at w is a slope of a
line that touches f at w and is not above f elsewhere.

14.2 subgradients

189

    w ,    f( w )(cid:105)

f (u)

(cid:104)u

f( w ) +

f (w)

w

u

figure 14.2 left: the right-hand side of equation (14.7) is the tangent of f at w. for
a convex function, the tangent lower bounds f . right: illustration of several
subgradients of a nondi   erentiable convex function.

14.2.1

calculating subgradients

how do we construct subgradients of a given convex function? if a function is
di   erentiable at a point w, then the di   erential set is trivial, as the following
claim shows.

claim 14.5
the gradient of f at w,    f (w).

if f is di   erentiable at w then    f (w) contains a single element    

example 14.1 (the di   erential set of the absolute function) consider the
absolute value function f (x) = |x|. using claim 14.5, we can easily construct
the di   erential set for the di   erentiable parts of f , and the only point that
requires special attention is x0 = 0. at that point, it is easy to verify that the
subdi   erential is the set of all numbers between    1 and 1. hence:

                     

   f (x) =

{1}
{   1}
[   1, 1]

if x > 0

if x < 0

if x = 0

for many practical uses, we do not need to calculate the whole set of subgra-
dients at a given point, as one member of this set would su   ce. the following
claim shows how to construct a sub-gradient for pointwise maximum functions.

claim 14.6 let g(w) = maxi   [r] gi(w) for r convex di   erentiable functions
g1, . . . , gr. given some w, let j     argmaxi gi(w). then    gj(w)        g(w).
proof since gj is convex we have that for all u

gj(u)     gj(w) + (cid:104)u     w,   gj(w)(cid:105).

since g(w) = gj(w) and g(u)     gj(u) we obtain that

g(u)     g(w) + (cid:104)u     w,   gj(w)(cid:105),

which concludes our proof.

190

stochastic id119

example 14.2 (a subgradient of the hinge loss) recall the hinge id168
from section 12.3, f (w) = max{0, 1     y(cid:104)w, x(cid:105)} for some vector x and scalar y.
to calculate a subgradient of the hinge loss at some w we rely on the preceding
claim and obtain that the vector v de   ned in the following is a subgradient of
the hinge loss at w:

(cid:40)

v =

if 1     y(cid:104)w, x(cid:105)     0
0
   yx if 1     y(cid:104)w, x(cid:105) > 0

14.2.2

subgradients of lipschitz functions
recall that a function f : a     r is   -lipschitz if for all u, v     a

|f (u)     f (v)|       (cid:107)u     v(cid:107).

the following lemma gives an equivalent de   nition using norms of subgradients.
lemma 14.7 let a be a convex open set and let f : a     r be a convex function.
then, f is   -lipschitz over a i    for all w     a and v        f (w) we have that
(cid:107)v(cid:107)       .
proof assume that for all v        f (w) we have that (cid:107)v(cid:107)       . since v        f (w)
we have

f (w)     f (u)     (cid:104)v, w     u(cid:105).

bounding the right-hand side using cauchy-schwartz inequality we obtain

f (w)     f (u)     (cid:104)v, w     u(cid:105)     (cid:107)v(cid:107)(cid:107)w     u(cid:107)       (cid:107)w     u(cid:107).

an analogous argument can show that f (u)     f (w)       (cid:107)w     u(cid:107). hence f is
  -lipschitz.
now assume that f is   -lipschitz. choose some w     a, v        f (w). since a
is open, there exists   > 0 such that u = w +  v/(cid:107)v(cid:107) belongs to a. therefore,
(cid:104)u     w, v(cid:105) =  (cid:107)v(cid:107) and (cid:107)u     w(cid:107) =  . from the de   nition of the subgradient,

f (u)     f (w)     (cid:104)v, u     w(cid:105) =  (cid:107)v(cid:107).

on the other hand, from the lipschitzness of f we have
     =   (cid:107)u     w(cid:107)     f (u)     f (w).

combining the two inequalities we conclude that (cid:107)v(cid:107)       .

14.2.3

subid119

the id119 algorithm can be generalized to nondi   erentiable functions
by using a subgradient of f (w) at w(t), instead of the gradient. the analysis of
the convergence rate remains unchanged: simply note that equation (14.3) is
true for subgradients as well.

14.3 stochastic id119 (sgd)

191

figure 14.3 an illustration of the id119 algorithm (left) and the stochastic
id119 algorithm (right). the function to be minimized is
1.25(x + 6)2 + (y     8)2. for the stochastic case, the black line depicts the averaged
value of w.

14.3

stochastic id119 (sgd)

in stochastic id119 we do not require the update direction to be based
exactly on the gradient. instead, we allow the direction to be a random vector
and only require that its expected value at each iteration will equal the gradient
direction. or, more generally, we require that the expected value of the random
vector will be a subgradient of the function at the current vector.

stochastic id119 (sgd) for minimizing

f (w)

parameters: scalar    > 0, integer t > 0
initialize: w(1) = 0
for t = 1, 2, . . . , t
choose vt at random from a distribution such that e[vt | w(t)]        f (w(t))
update w(t+1) = w(t)       vt
output   w = 1
t

(cid:80)t

t=1 w(t)

an illustration of stochastic id119 versus id119 is given
in figure 14.3. as we will see in section 14.5, in the context of learning problems,
it is easy to    nd a random vector whose expectation is a subgradient of the risk
function.

14.3.1

analysis of sgd for convex-lipschitz-bounded functions

recall the bound we achieved for the gd algorithm in corollary 14.2. for the
stochastic case, in which only the expectation of vt is in    f (w(t)), we cannot
directly apply equation (14.3). however, since the expected value of vt is a

192

stochastic id119

subgradient of f at w(t), we can still derive a similar bound on the expected
output of stochastic id119. this is formalized in the following theorem.
theorem 14.8 let b,    > 0. let f be a convex function and let w(cid:63)     argminw:(cid:107)w(cid:107)   b f (w).
assume that sgd is run for t iterations with    =
all t, (cid:107)vt(cid:107)        with id203 1. then,

  2 t . assume also that for

(cid:113) b2

e [f (   w)]     f (w(cid:63))     b      
t

.

therefore, for any   > 0, to achieve e[f (   w)]     f (w(cid:63))      , it su   ces to run the
sgd algorithm for a number of iterations that satis   es

t     b2  2
 2

.

proof let us introduce the notation v1:t to denote the sequence v1, . . . , vt.
taking expectation of equation (14.2), we obtain

e
v1:t

[f (   w)     f (w(cid:63))]     e

v1:t

1
t

(f (w(t))     f (w(cid:63)))

.

(cid:34)

t(cid:88)

t=1

(cid:35)

(cid:35)

(14.9)

,

(14.10)

since lemma 14.1 holds for any sequence v1, v2, ...vt , it applies to sgd as well.
by taking expectation of the bound in the lemma we have

(cid:34)

e
v1:t

1
t

t(cid:88)

t=1

(cid:35)

    b      
t

.

(cid:104)w(t)     w(cid:63), vt(cid:105)

(cid:35)

(cid:34)

1
t

    e

v1:t

t(cid:88)

t=1

(cid:35)

t(cid:88)

t=1

it is left to show that

(cid:34)

e
v1:t

1
t

t(cid:88)

t=1

(cid:34)

t(cid:88)

t=1

(f (w(t))     f (w(cid:63)))

(cid:104)w(t)     w(cid:63), vt(cid:105)

which we will hereby prove.

using the linearity of the expectation we have

e
v1:t

1
t

(cid:104)w(t)     w(cid:63), vt(cid:105)

=

1
t

[(cid:104)w(t)     w(cid:63), vt(cid:105)].

e
v1:t

next, we recall the law of total expectation: for every two random variables   ,   ,
and a function g, e  [g(  )] = e   e  [g(  )|  ]. setting    = v1:t and    = v1:t   1 we
get that

e
v1:t

[(cid:104)w(t)     w(cid:63), vt(cid:105)] = e
= e
v1:t   1

v1:t

[(cid:104)w(t)     w(cid:63), vt(cid:105)]

[(cid:104)w(t)     w(cid:63), vt(cid:105)| v1:t   1] .

e
v1:t

once we know v1:t   1, the value of w(t) is not random any more and therefore

e

v1:t   1

e
v1:t

[(cid:104)w(t)     w(cid:63), vt(cid:105)| v1:t   1] = e

v1:t   1

(cid:104)w(t)     w(cid:63), e

vt

[vt | v1:t   1](cid:105) .

14.4 variants

193

since w(t) only depends on v1:t   1 and sgd requires that evt[vt | w(t)]        f (w(t))
we obtain that evt[vt | v1:t   1]        f (w(t)). thus,
e

[f (w(t))     f (w(cid:63))].

(cid:104)w(t)     w(cid:63), e

[vt | v1: t   1](cid:105)    

e

v1: t   1

vt

v1: t   1

overall, we have shown that

e
v1:t

[(cid:104)w(t)     w(cid:63), vt(cid:105)]     e
= e

[f (w(t))     f (w(cid:63))]
[f (w(t))     f (w(cid:63))] .

v1:t   1

v1:t

summing over t, dividing by t , and using the linearity of expectation, we get
that equation (14.10) holds, which concludes our proof.

14.4

variants

in this section we describe several variants of stochastic id119.

14.4.1

adding a projection step

in the previous analyses of the gd and sgd algorithms, we required that the
norm of w(cid:63) will be at most b, which is equivalent to requiring that w(cid:63) is in the
set h = {w : (cid:107)w(cid:107)     b}. in terms of learning, this means restricting ourselves to
a b-bounded hypothesis class. yet any step we take in the opposite direction of
the gradient (or its expected direction) might result in stepping out of this bound,
and there is even no guarantee that   w satis   es it. we show in the following how
to overcome this problem while maintaining the same convergence rate.

the basic idea is to add a projection step; namely, we will now have a two-step
update rule, where we    rst subtract a subgradient from the current value of w
and then project the resulting vector onto h. formally,

2 ) = w(t)       vt

2 )(cid:107)

1.. w(t+ 1
2.. w(t+1) = argminw   h (cid:107)w     w(t+ 1
the projection step replaces the current value of w by the vector in h closest
to it.

clearly, the projection step guarantees that w(t)     h for all t. since h is
convex this also implies that   w     h as required. we next show that the analysis
of sgd with projections remains the same. this is based on the following lemma.

lemma 14.9 (projection lemma) let h be a closed convex set and let v be the
projection of w onto h, namely,

v = argmin

x   h

(cid:107)x     w(cid:107)2.

194

stochastic id119

then, for every u     h,

(cid:107)w     u(cid:107)2     (cid:107)v     u(cid:107)2     0.

proof by the convexity of h, for every        (0, 1) we have that v+  (u   v)     h.
therefore, from the optimality of v we obtain
(cid:107)v     w(cid:107)2     (cid:107)v +   (u     v)     w(cid:107)2

= (cid:107)v     w(cid:107)2 + 2  (cid:104)v     w, u     v(cid:105) +   2(cid:107)u     v(cid:107)2.

rearranging, we obtain

2(cid:104)v     w, u     v(cid:105)          (cid:107)u     v(cid:107)2.

taking the limit        0 we get that

(cid:104)v     w, u     v(cid:105)     0.

therefore,

(cid:107)w     u(cid:107)2 = (cid:107)w     v + v     u(cid:107)2

= (cid:107)w     v(cid:107)2 + (cid:107)v     u(cid:107)2 + 2(cid:104)v     w, u     v(cid:105)
    (cid:107)v     u(cid:107)2.

equipped with the preceding lemma, we can easily adapt the analysis of sgd
to the case in which we add projection steps on a closed and convex set. simply
note that for every t,

(cid:107)w(t+1)     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2
= (cid:107)w(t+1)     w(cid:63)(cid:107)2     (cid:107)w(t+ 1
    (cid:107)w(t+ 1

2 )     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2.

2 )     w(cid:63)(cid:107)2 + (cid:107)w(t+ 1

2 )     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2

therefore, lemma 14.1 holds when we add projection steps and hence the rest
of the analysis follows directly.

14.4.2

variable step size

another variant of sgd is decreasing the step size as a function of t. that is,
rather than updating with a constant   , we use   t. for instance, we can set
   
  t = b
and achieve a bound similar to theorem 14.8. the idea is that when
  
we are closer to the minimum of the function, we take our steps more carefully,
so as not to    overshoot    the minimum.

t

14.4 variants

195

14.4.3

other averaging techniques

(cid:80)t

we have set the output vector to be   w = 1
t=1 w(t). there are alternative
approaches such as outputting w(t) for some random t     [t], or outputting the
t
average of w(t) over the last   t iterations, for some        (0, 1). one can also take
a weighted average of the last few iterates. these more sophisticated averaging
schemes can improve the convergence speed in some situations, such as in the
case of strongly convex functions de   ned in the following.

14.4.4

strongly convex functions*

in this section we show a variant of sgd that enjoys a faster convergence rate for
problems in which the objective function is strongly convex (see de   nition 13.4
of strong convexity in the previous chapter). we rely on the following claim,
which generalizes lemma 13.5.

claim 14.10
have

if f is   -strongly convex then for every w, u and v        f (w) we

(cid:104)w     u, v(cid:105)     f (w)     f (u) +   

2(cid:107)w     u(cid:107)2.

the proof is similar to the proof of lemma 13.5 and is left as an exercise.

sgd for minimizing a   -strongly convex function

goal: solve minw   h f (w)
parameter: t
initialize: w(1) = 0
for t = 1, . . . , t
choose a random vector vt s.t. e[vt|w(t)]        f (w(t))
set   t = 1/(   t)
set w(t+ 1
set w(t+1) = arg minw   h (cid:107)w     w(t+ 1
output:   w = 1
t

2 ) = w(t)       tvt

(cid:80)t

t=1 w(t)

2 )(cid:107)2

theorem 14.11 assume that f is   -strongly convex and that e[(cid:107)vt(cid:107)2]       2.
let w(cid:63)     argminw   h f (w) be an optimal solution. then,

e[f (   w)]     f (w(cid:63))       2
2    t

(1 + log(t )).

proof let    (t) = e[vt|w(t)]. since f is strongly convex and    (t) is in the
subgradient set of f at w(t) we have that

(cid:104)w(t)     w(cid:63),   (t)(cid:105)     f (w(t))     f (w(cid:63)) +   

2(cid:107)w(t)     w(cid:63)(cid:107)2 .

(14.11)

next, we show that

(cid:104)w(t)     w(cid:63),   (t)(cid:105)     e[(cid:107)w(t)     w(cid:63)(cid:107)2     (cid:107)w(t+1)     w(cid:63)(cid:107)2]

2   t

+

  t
2

  2.

(14.12)

196

stochastic id119

since w(t+1) is the projection of w(t+ 1
(cid:107)w(t+ 1

2 )     w(cid:63)(cid:107)2     (cid:107)w(t+1)     w(cid:63)(cid:107)2. therefore,
(cid:107)w(t)     w(cid:63)(cid:107)2     (cid:107)w(t+1)     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2     (cid:107)w(t+ 1
= 2  t(cid:104)w(t)     w(cid:63), vt(cid:105)       2

2 )     w(cid:63)(cid:107)2
t (cid:107)vt(cid:107)2 .

2 ) onto h, and w(cid:63)     h we have that

taking expectation of both sides, rearranging, and using the assumption e[(cid:107)vt(cid:107)2]    
  2 yield equation (14.12). comparing equation (14.11) and equation (14.12) and
summing over t we obtain

t(cid:88)

(e[f (w(t))]     f (w(cid:63)))

(cid:34) t(cid:88)

(cid:18)(cid:107)w(t)     w(cid:63)(cid:107)2     (cid:107)w(t+1)     w(cid:63)(cid:107)2

t=1

    e

t=1

2   t

      

2(cid:107)w(t)     w(cid:63)(cid:107)2

(cid:19)(cid:35)

t(cid:88)

t=1

  t.

+

  2
2

next, we use the de   nition   t = 1/(   t) and note that the    rst sum on the
right-hand side of the equation collapses to      t(cid:107)w(t +1)     w(cid:63)(cid:107)2     0. thus,

t(cid:88)

t=1

(e[f (w(t))]     f (w(cid:63)))       2
2   

1
t

      2
2   

(1 + log(t )).

t(cid:88)

t=1

the theorem follows from the preceding by dividing by t and using jensen   s
inequality.

remark 14.3 rakhlin, shamir & sridharan (2012) derived a convergence rate
in which the log(t ) term is eliminated for a variant of the algorithm in which
we output the average of the last t /2 iterates,   w = 2
t=t /2+1 w(t). shamir &
t
zhang (2013) have shown that theorem 14.11 holds even if we output   w = w(t ).

(cid:80)t

14.5

learning with sgd

we have so far introduced and analyzed the sgd algorithm for general convex
functions. now we shall consider its applicability to learning tasks.

14.5.1

sgd for risk minimization

recall that in learning we face the problem of minimizing the risk function

ld(w) = e

z   d[(cid:96)(w, z)].

we have seen the method of empirical risk minimization, where we minimize the
empirical risk, ls(w), as an estimate to minimizing ld(w). sgd allows us to
take a di   erent approach and minimize ld(w) directly. since we do not know
d, we cannot simply calculate    ld(w(t)) and minimize it with the gd method.
with sgd, however, all we need is to    nd an unbiased estimate of the gradient of

14.5 learning with sgd

197

ld(w), that is, a random vector whose conditional expected value is    ld(w(t)).
we shall now see how such an estimate can be easily constructed.

for simplicity, let us    rst consider the case of di   erentiable id168s.
hence the risk function ld is also di   erentiable. the construction of the random
vector vt will be as follows: first, sample z     d. then, de   ne vt to be the
gradient of the function (cid:96)(w, z) with respect to w, at the point w(t). then, by
the linearity of the gradient we have

e[vt|w(t)] = e

z   d[   (cid:96)(w(t), z)] =     e

z   d[(cid:96)(w(t), z)] =    ld(w(t)).

(14.13)

the gradient of the id168 (cid:96)(w, z) at w(t) is therefore an unbiased estimate
of the gradient of the risk function ld(w(t)) and is easily constructed by sampling
a single fresh example z     d at each iteration t.

the same argument holds for nondi   erentiable id168s. we simply let

vt be a subgradient of (cid:96)(w, z) at w(t). then, for every u we have

(cid:96)(u, z)     (cid:96)(w(t), z)     (cid:104)u     w(t), vt(cid:105).

taking expectation on both sides with respect to z     d and conditioned on the
value of w(t) we obtain

ld(u)     ld(w(t)) = e[(cid:96)(u, z)     (cid:96)(w(t), z)|w(t)]

    e[(cid:104)u     w(t), vt(cid:105)|w(t)]
= (cid:104)u     w(t), e[vt|w(t)](cid:105).
it follows that e[vt|w(t)] is a subgradient of ld(w) at w(t).

to summarize, the stochastic id119 framework for minimizing the

risk is as follows.

stochastic id119 (sgd) for minimizing

ld(w)

parameters: scalar    > 0, integer t > 0
initialize: w(1) = 0
for t = 1, 2, . . . , t
sample z     d
pick vt        (cid:96)(w(t), z)
update w(t+1) = w(t)       vt
output   w = 1
t

(cid:80)t

t=1 w(t)

we shall now use our analysis of sgd to obtain a sample complexity anal-
ysis for learning convex-lipschitz-bounded problems. theorem 14.8 yields the
following:

corollary 14.12 consider a convex-lipschitz-bounded learning problem with
parameters   , b. then, for every   > 0, if we run the sgd method for minimizing

198

stochastic id119

ld(w) with a number of iterations (i.e., number of examples)

(cid:113) b2

t     b2  2
 2

and with    =

  2 t , then the output of sgd satis   es
w   h ld(w) +  .

e [ld(   w)]     min

it is interesting to note that the required sample complexity is of the same order
of magnitude as the sample complexity guarantee we derived for regularized loss
minimization. in fact, the sample complexity of sgd is even better than what
we have derived for regularized loss minimization by a factor of 8.

14.5.2

analyzing sgd for convex-smooth learning problems

in the previous chapter we saw that the regularized loss minimization rule also
learns the class of convex-smooth-bounded learning problems. we now show that
the sgd algorithm can be also used for such problems.
theorem 14.13 assume that for all z, the id168 (cid:96)(  , z) is convex,   -
smooth, and nonnegative. then, if we run the sgd algorithm for minimizing
ld(w) we have that for every w(cid:63),

(cid:18)

(cid:19)

.

(cid:107)w(cid:63)(cid:107)2
2   t

e[ld(   w)]    

1

1         

ld(w(cid:63)) +

proof recall that if a function is   -smooth and nonnegative then it is self-
bounded:

(cid:107)   f (w)(cid:107)2     2  f (w).

to analyze sgd for convex-smooth problems, let us de   ne z1, . . . , zt the random
samples of the sgd algorithm, let ft(  ) = (cid:96)(  , zt), and note that vt =    ft(w(t)).
for all t, ft is a convex function and therefore ft(w(t))   ft(w(cid:63))     (cid:104)vt, w(t)   w(cid:63)(cid:105).
summing over t and using lemma 14.1 we obtain

t(cid:88)

t=1

+

  
2

(cid:107)vt(cid:107)2.

combining the preceding with the self-boundedness of ft yields

t(cid:88)
(ft(w(t))     ft(w(cid:63)))     t(cid:88)

t=1

t=1

(cid:104)vt, w(t)     w(cid:63)(cid:105)     (cid:107)w(cid:63)(cid:107)2
t(cid:88)

2  

t(cid:88)
(ft(w(t))     ft(w(cid:63)))     (cid:107)w(cid:63)(cid:107)2
t(cid:88)
t(cid:88)

(cid:32)

2  

t=1

1

ft(w(t))    

1         

1
t

1
t

t=1

t=1

+     

ft(w(t)).

t=1

ft(w(cid:63)) +

(cid:107)w(cid:63)(cid:107)2
2   t

(cid:33)

.

dividing by t and rearranging, we obtain

next, we take expectation of the two sides of the preceding equation with respect

14.5 learning with sgd

199

to z1, . . . , zt . clearly, e[ft(w(cid:63))] = ld(w(cid:63)). in addition, using the same argument
as in the proof of theorem 14.8 we have that

(cid:34)

e

1
t

t(cid:88)

t=1

(cid:35)

(cid:34)

1
t

t(cid:88)

t=1

(cid:35)

ft(w(t))

= e

ld(w(t))

    e[ld(   w)].

combining all we conclude our proof.

as a direct corollary we obtain:

corollary 14.14 consider a convex-smooth-bounded learning problem with
parameters   , b. assume in addition that (cid:96)(0, z)     1 for all z     z. for every
  > 0, set    =

  (1+3/ ) . then, running sgd with t     12b2  / 2 yields

1

e[ld(   w)]     min

w   h ld(w) +  .

14.5.3

sgd for regularized loss minimization

we have shown that sgd enjoys the same worst-case sample complexity bound
as regularized loss minimization. however, on some distributions, regularized loss
minimization may yield a better solution. therefore, in some cases we may want
to solve the optimization problem associated with regularized loss minimization,
namely,1

(cid:18)   

2

(cid:19)

min

w

(cid:107)w(cid:107)2 + ls(w)

.

(14.14)

de   ne f (w) =   

since we are dealing with convex learning problems in which the id168 is
convex, the preceding problem is also a id76 problem that can
be solved using sgd as well, as we shall see in this section.

2(cid:107)w(cid:107)2 + ls(w). note that f is a   -strongly convex function;
therefore, we can apply the sgd variant given in section 14.4.4 (with h = rd).
to apply this algorithm, we only need to    nd a way to construct an unbiased
estimate of a subgradient of f at w(t). this is easily done by noting that if
we pick z uniformly at random from s, and choose vt        (cid:96)(w(t), z) then the
expected value of   w(t) + vt is a subgradient of f at w(t).

to analyze the resulting algorithm, we    rst rewrite the update rule (assuming

1 we divided    by 2 for convenience.

200

stochastic id119

that h = rd and therefore the projection step does not matter) as follows

w(t+1) = w(t)     1
   t

(cid:16)

(cid:17)

=

vt

  w(t) + vt

(cid:18)

(cid:19)
(cid:18) t     2
t(cid:88)

w(t)     1
   t

1     1
t
t     1
w(t)     1
   t
t
t     1
w(t   1)    
t
=     1
   t

t     1

vi.

vt

=

=

i=1

(cid:19)

    1
   t

vt

1

   (t     1)

vt   1

(14.15)

if we assume that the id168 is   -lipschitz, it follows that for all t we have
(cid:107)vt(cid:107)        and therefore (cid:107)  w(t)(cid:107)       , which yields
(cid:107)  w(t) + vt(cid:107)     2  .

theorem 14.11 therefore tells us that after performing t iterations we have that

e[f (   w)]     f (w(cid:63))     4  2
   t

(1 + log(t )).

14.6

summary

we have introduced the id119 and stochastic id119 algo-
rithms, along with several of their variants. we have analyzed their convergence
rate and calculated the number of iterations that would guarantee an expected
objective of at most   plus the optimal objective. most importantly, we have
shown that by using sgd we can directly minimize the risk function. we do
so by sampling a point i.i.d from d and using a subgradient of the loss of the
current hypothesis w(t) at this point as an unbiased estimate of the gradient (or
a subgradient) of the risk function. this implies that a bound on the number of
iterations also yields a sample complexity bound. finally, we have also shown
how to apply the sgd method to the problem of regularized risk minimization.
in future chapters we show how this yields extremely simple solvers to some
optimization problems associated with regularized risk minimization.

14.7

bibliographic remarks

sgd dates back to robbins & monro (1951). it is especially e   ective in large
scale machine learning problems. see, for example, (murata 1998, le cun 2004,
zhang 2004, bottou & bousquet 2008, shalev-shwartz, singer & srebro 2007,
shalev-shwartz & srebro 2008). in the optimization community it was studied

14.8 exercises

201

in the context of stochastic optimization. see, for example, (nemirovski & yudin
1978, nesterov & nesterov 2004, nesterov 2005, nemirovski, juditsky, lan &
shapiro 2009, shapiro, dentcheva & ruszczy  nski 2009).

the bound we have derived for strongly convex function is due to hazan,
agarwal & kale (2007). as mentioned previously, improved bounds have been
obtained in rakhlin et al. (2012).

14.8

exercises

1. prove claim 14.10. hint: extend the proof of lemma 13.5.
2. prove corollary 14.14.
3. id88 as a subid119 algorithm: let s = ((x1, y1), . . . , (xm, ym))    

(rd    {  1})m. assume that there exists w     rd such that for every i     [m]
we have yi(cid:104)w, xi(cid:105)     1, and let w(cid:63) be a vector that has the minimal norm
among all vectors that satisfy the preceding requirement. let r = maxi (cid:107)xi(cid:107).
de   ne a function

f (w) = max
i   [m]

(1     yi (cid:104)w, xi(cid:105)) .

1 separates the examples in s.

    show that minw:(cid:107)w(cid:107)   (cid:107)w(cid:63)(cid:107) f (w) = 0 and show that any w for which f (w) <
    show how to calculate a subgradient of f .
    describe and analyze the subid119 algorithm for this case. com-
pare the algorithm and the analysis to the batch id88 algorithm
given in section 9.1.2.

4. variable step size (*): prove an analog of theorem 14.8 for sgd with a

   
variable step size,   t = b
  

.

t

15 support vector machines

in this chapter and the next we discuss a very useful machine learning tool: the
support vector machine paradigm (id166) for learning linear predictors in high
dimensional feature spaces. the high dimensionality of the feature space raises
both sample complexity and computational complexity challenges.

the id166 algorithmic paradigm tackles the sample complexity challenge by
searching for    large margin    separators. roughly speaking, a halfspace separates
a training set with a large margin if all the examples are not only on the correct
side of the separating hyperplane but also far away from it. restricting the
algorithm to output a large margin separator can yield a small sample complexity
even if the dimensionality of the feature space is high (and even in   nite). we
introduce the concept of margin and relate it to the regularized loss minimization
paradigm as well as to the convergence rate of the id88 algorithm.

in the next chapter we will tackle the computational complexity challenge

using the idea of kernels.

15.1

margin and hard-id166

let s = (x1, y1), . . . , (xm, ym) be a training set of examples, where each xi     rd
and yi     {  1}. we say that this training set is linearly separable, if there exists
a halfspace, (w, b), such that yi = sign((cid:104)w, xi(cid:105) + b) for all i. alternatively, this
condition can be rewritten as

   i     [m], yi((cid:104)w, xi(cid:105) + b) > 0.

all halfspaces (w, b) that satisfy this condition are erm hypotheses (their 0-1
error is zero, which is the minimum possible error). for any separable training
sample, there are many erm halfspaces. which one of them should the learner
pick?

consider, for example, the training set described in the picture that follows.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

15.1 margin and hard-id166

203

x

x

while both the dashed-black and solid-green hyperplanes separate the four ex-
amples, our intuition would probably lead us to prefer the black hyperplane over
the green one. one way to formalize this intuition is using the concept of margin.
the margin of a hyperplane with respect to a training set is de   ned to be the
minimal distance between a point in the training set and the hyperplane. if a
hyperplane has a large margin, then it will still separate the training set even if
we slightly perturb each instance.

we will see later on that the true error of a halfspace can be bounded in terms
of the margin it has over the training sample (the larger the margin, the smaller
the error), regardless of the euclidean dimension in which this halfspace resides.
hard-id166 is the learning rule in which we return an erm hyperplane that
separates the training set with the largest possible margin. to de   ne hard-id166
formally, we    rst express the distance between a point x to a hyperplane using
the parameters de   ning the halfspace.

claim 15.1 the distance between a point x and the hyperplane de   ned by
(w, b) where (cid:107)w(cid:107) = 1 is |(cid:104)w, x(cid:105) + b|.

proof the distance between a point x and the hyperplane is de   ned as

min{(cid:107)x     v(cid:107) : (cid:104)w, v(cid:105) + b = 0}.

taking v = x     ((cid:104)w, x(cid:105) + b)w we have that

(cid:104)w, v(cid:105) + b = (cid:104)w, x(cid:105)     ((cid:104)w, x(cid:105) + b)(cid:107)w(cid:107)2 + b = 0,

and

(cid:107)x     v(cid:107) = |(cid:104)w, x(cid:105) + b|(cid:107)w(cid:107) = |(cid:104)w, x(cid:105) + b|.

hence, the distance is at most |(cid:104)w, x(cid:105) + b|. next, take any other point u on the
hyperplane, thus (cid:104)w, u(cid:105) + b = 0. we have

(cid:107)x     u(cid:107)2 = (cid:107)x     v + v     u(cid:107)2

= (cid:107)x     v(cid:107)2 + (cid:107)v     u(cid:107)2 + 2(cid:104)x     v, v     u(cid:105)
    (cid:107)x     v(cid:107)2 + 2(cid:104)x     v, v     u(cid:105)
= (cid:107)x     v(cid:107)2 + 2((cid:104)w, x(cid:105) + b)(cid:104)w, v     u(cid:105)
= (cid:107)x     v(cid:107)2,

where the last equality is because (cid:104)w, v(cid:105) = (cid:104)w, u(cid:105) =    b. hence, the distance

204

support vector machines

between x and u is at least the distance between x and v, which concludes our
proof.

on the basis of the preceding claim, the closest point in the training set to the
separating hyperplane is mini   [m] |(cid:104)w, xi(cid:105) + b|. therefore, the hard-id166 rule is

argmax

(w,b):(cid:107)w(cid:107)=1

min
i   [m]

|(cid:104)w, xi(cid:105) + b|

s.t.    i, yi((cid:104)w, xi(cid:105) + b) > 0.

whenever there is a solution to the preceding problem (i.e., we are in the sepa-
rable case), we can write an equivalent problem as follows (see exercise 1):

argmax

(w,b):(cid:107)w(cid:107)=1

min
i   [m]

yi((cid:104)w, xi(cid:105) + b).

(15.1)

next, we give another equivalent formulation of the hard-id166 rule as a quadratic
optimization problem.1

hard-id166

input: (x1, y1), . . . , (xm, ym)
solve:

(w0, b0) = argmin

(w,b)

(cid:107)w(cid:107)2 s.t.    i, yi((cid:104)w, xi(cid:105) + b)     1

(15.2)

output:   w = w0(cid:107)w0(cid:107) ,   b = b0(cid:107)w0(cid:107)

the lemma that follows shows that the output of hard-id166 is indeed the
separating hyperplane with the largest margin. intuitively, hard-id166 searches
for w of minimal norm among all the vectors that separate the data and for
which |(cid:104)w, xi(cid:105) + b|     1 for all i. in other words, we enforce the margin to be 1,
but now the units in which we measure the margin scale with the norm of w.
therefore,    nding the largest margin halfspace boils down to    nding w whose
norm is minimal. formally:

lemma 15.2 the output of hard-id166 is a solution of equation (15.1).

proof let (w(cid:63), b(cid:63)) be a solution of equation (15.1) and de   ne the margin
achieved by (w(cid:63), b(cid:63)) to be   (cid:63) = mini   [m] yi((cid:104)w(cid:63), xi(cid:105) + b(cid:63)). therefore, for all
i we have

yi((cid:104)w(cid:63), xi(cid:105) + b(cid:63))       (cid:63)

or equivalently

yi((cid:104) w(cid:63)

  (cid:63) , xi(cid:105) + b(cid:63)

  (cid:63) )     1.

hence, the pair ( w(cid:63)

  (cid:63) , b(cid:63)

  (cid:63) ) satis   es the conditions of the quadratic optimization

1 a quadratic optimization problem is an optimization problem in which the objective is a

convex quadratic function and the constraints are linear inequalities.

15.1 margin and hard-id166

205

problem given in equation (15.2). therefore, (cid:107)w0(cid:107)     (cid:107) w(cid:63)
for all i,

  (cid:63) (cid:107) = 1

  (cid:63) . it follows that

yi((cid:104)   w, xi(cid:105) +   b) =

1

(cid:107)w0(cid:107) yi((cid:104)w0, xi(cid:105) + b0)     1

(cid:107)w0(cid:107)       (cid:63).

since (cid:107)   w(cid:107) = 1 we obtain that (   w,   b) is an optimal solution of equation (15.1).

15.1.1

the homogenous case

15.1.2

it is often more convenient to consider homogenous halfspaces, namely, halfspaces
that pass through the origin and are thus de   ned by sign((cid:104)w, x(cid:105)), where the bias
term b is set to be zero. hard-id166 for homogenous halfspaces amounts to solving

(cid:107)w(cid:107)2

s.t.    i, yi(cid:104)w, xi(cid:105)     1.

min

w

(15.3)

as we discussed in chapter 9, we can reduce the problem of learning nonhomogenous

halfspaces to the problem of learning homogenous halfspaces by adding one more
feature to each instance of xi, thus increasing the dimension to d + 1.

note, however, that the optimization problem given in equation (15.2) does
not regularize the bias term b, while if we learn a homogenous halfspace in rd+1
using equation (15.3) then we regularize the bias term (i.e., the d + 1 component
of the weight vector) as well. however, regularizing b usually does not make a
signi   cant di   erence to the sample complexity.

the sample complexity of hard-id166
recall that the vc-dimension of halfspaces in rd is d + 1. it follows that the
sample complexity of learning halfspaces grows with the dimensionality of the
problem. furthermore, the fundamental theorem of learning tells us that if the
number of examples is signi   cantly smaller than d/  then no algorithm can learn
an  -accurate halfspace. this is problematic when d is very large.

to overcome this problem, we will make an additional assumption on the
underlying data distribution. in particular, we will de   ne a    separability with
margin       assumption and will show that if the data is separable with margin
   then the sample complexity is bounded from above by a function of 1/  2. it
follows that even if the dimensionality is very large (or even in   nite), as long as
the data adheres to the separability with margin assumption we can still have a
small sample complexity. there is no contradiction to the lower bound given in
the fundamental theorem of learning because we are now making an additional
assumption on the underlying data distribution.

before we formally de   ne the separability with margin assumption, there is a

scaling issue we need to resolve. suppose that a training set s = (x1, y1), . . . , (xm, ym)
is separable with a margin   , namely, the maximal objective value of equa-
tion (15.1) is at least   . then, for any positive scalar    > 0, the training set

206

support vector machines

s(cid:48) = (  x1, y1), . . . , (  xm, ym) is separable with a margin of     . that is, a sim-
ple scaling of the data can make it separable with an arbitrarily large margin. it
follows that in order to give a meaningful de   nition of margin we must take into
account the scale of the examples as well. one way to formalize this is using the
de   nition that follows.
definition 15.3 let d be a distribution over rd    {  1}. we say that d is
separable with a (  ,   )-margin if there exists (w(cid:63), b(cid:63)) such that (cid:107)w(cid:63)(cid:107) = 1 and
such that with id203 1 over the choice of (x, y)     d we have that y((cid:104)w(cid:63), x(cid:105)+
b(cid:63))        and (cid:107)x(cid:107)       . similarly, we say that d is separable with a (  ,   )-margin
using a homogenous halfspace if the preceding holds with a halfspace of the form
(w(cid:63), 0).

in the advanced part of the book (chapter 26), we will prove that the sample
complexity of hard-id166 depends on (  /  )2 and is independent of the dimension
d. in particular, theorem 26.13 in section 26.3 states the following:
theorem 15.4 let d be a distribution over rd  {  1} that satis   es the (  ,   )-
separability with margin assumption using a homogenous halfspace. then, with
id203 of at least 1        over the choice of a training set of size m, the 0-1
error of the output of hard-id166 is at most

(cid:114)

(cid:114)

4 (  /  )2

m

+

2 log(2/  )

m

.

remark 15.1 (margin and the id88)
in section 9.1.2 we have described
and analyzed the id88 algorithm for    nding an erm hypothesis with
respect to the class of halfspaces. in particular, in theorem 9.1 we upper bounded
the number of updates the id88 might make on a given training set. it
can be shown (see exercise 2) that the upper bound is exactly (  /  )2, where   
is the radius of examples and    is the margin.

15.2

soft-id166 and norm id173

the hard-id166 formulation assumes that the training set is linearly separable,
which is a rather strong assumption. soft-id166 can be viewed as a relaxation of
the hard-id166 rule that can be applied even if the training set is not linearly
separable.
the optimization problem in equation (15.2) enforces the hard constraints
yi((cid:104)w, xi(cid:105) + b)     1 for all i. a natural relaxation is to allow the constraint to be
violated for some of the examples in the training set. this can be modeled by
introducing nonnegative slack variables,   1, . . . ,   m, and replacing each constraint
yi((cid:104)w, xi(cid:105) + b)     1 by the constraint yi((cid:104)w, xi(cid:105) + b)     1      i. that is,   i measures
by how much the constraint yi((cid:104)w, xi(cid:105)+b)     1 is being violated. soft-id166 jointly
minimizes the norm of w (corresponding to the margin) and the average of   i
(corresponding to the violations of the constraints). the tradeo    between the two

15.2 soft-id166 and norm id173

207

terms is controlled by a parameter   . this leads to the soft-id166 optimization
problem:

soft-id166

input: (x1, y1), . . . , (xm, ym)
parameter:    > 0
solve:

(cid:32)

(cid:33)

m(cid:88)

  (cid:107)w(cid:107)2 +

1
m

min
w,b,  
s.t.    i, yi((cid:104)w, xi(cid:105) + b)     1       i and   i     0

i=1

  i

(15.4)

output: w, b

we can rewrite equation (15.4) as a regularized loss minimization problem.

recall the de   nition of the hinge loss:

(cid:96)hinge((w, b), (x, y)) = max{0, 1     y((cid:104)w, x(cid:105) + b)}.

given (w, b) and a training set s, the averaged hinge loss on s is denoted by
lhinge

((w, b)). now, consider the regularized loss minimization problem:

s

(cid:16)

(cid:17)

  (cid:107)w(cid:107)2 + lhinge

s

((w, b))

min
w,b

.

(15.5)

claim 15.5 equation (15.4) and equation (15.5) are equivalent.

proof fix some w, b and consider the minimization over    in equation (15.4).
fix some i. since   i must be nonnegative, the best assignment to   i would be 0
if yi((cid:104)w, xi(cid:105) + b)     1 and would be 1     yi((cid:104)w, xi(cid:105) + b) otherwise. in other words,
  i = (cid:96)hinge((w, b), (xi, yi)) for all i, and the claim follows.

we therefore see that soft-id166 falls into the paradigm of regularized loss
minimization that we studied in the previous chapter. a soft-id166 algorithm,
that is, a solution for equation (15.5), has a bias toward low norm separators.
the objective function that we aim to minimize in equation (15.5) penalizes not
only for training errors but also for large norm.

it is often more convenient to consider soft-id166 for learning a homogenous
halfspace, where the bias term b is set to be zero, which yields the following
optimization problem:

(cid:16)

(cid:17)

  (cid:107)w(cid:107)2 + lhinge

s

(w)

min

w

,

(15.6)

where

lhinge

s

(w) =

m(cid:88)

i=1

1
m

max{0, 1     y(cid:104)w, xi(cid:105)}.

208

support vector machines

15.2.1

the sample complexity of soft-id166

we now analyze the sample complexity of soft-id166 for the case of homogenous
halfspaces (namely, the output of equation (15.6)). in corollary 13.8 we derived
a generalization bound for the regularized loss minimization framework assuming
that the id168 is convex and lipschitz. we have already shown that the
hinge loss is convex so it is only left to analyze the lipschitzness of the hinge
loss.
claim 15.6 let f (w) = max{0, 1     y(cid:104)w, x(cid:105)}. then, f is (cid:107)x(cid:107)-lipschitz.

it is easy to verify that any subgradient of f at w is of the form   x where

proof
|  |     1. the claim now follows from lemma 14.7.

corollary 13.8 therefore yields the following:

corollary 15.7 let d be a distribution over x    {0, 1}, where x = {x :
(cid:107)x(cid:107)       }. consider running soft-id166 (equation (15.6)) on a training set s    
dm and let a(s) be the solution of soft-id166. then, for every u,

e

s   dm

[lhinged

(a(s))]     lhinged

(u) +   (cid:107)u(cid:107)2 +

2  2
   m

.

furthermore, since the hinge loss upper bounds the 0   1 loss we also have

e

s   dm

[l0   1d (a(s))]     lhinged

(u) +   (cid:107)u(cid:107)2 +

2  2
   m

.

(cid:113) 2  2

last, for every b > 0, if we set    =

b2m then

e

s   dm

[l0   1d (a(s))]    

e

s   dm

[lhinged

(a(s))]    

min

w:(cid:107)w(cid:107)   b

lhinged

(w) +

(cid:114)

8  2b2

m

.

we therefore see that we can control the sample complexity of learning a half-
space as a function of the norm of that halfspace, independently of the euclidean
dimension of the space over which the halfspace is de   ned. this becomes highly
signi   cant when we learn via embeddings into high dimensional feature spaces,
as we will consider in the next chapter.
remark 15.2 the condition that x will contain vectors with a bounded norm
follows from the requirement that the id168 will be lipschitz. this is
not just a technicality. as we discussed before, separation with large margin
is meaningless without imposing a restriction on the scale of the instances. in-
deed, without a constraint on the scale, we can always enlarge the margin by
multiplying all instances by a large scalar.

15.2.2

margin and norm-based bounds versus dimension

the bounds we have derived for hard-id166 and soft-id166 do not depend on the
dimension of the instance space. instead, the bounds depend on the norm of the

15.2 soft-id166 and norm id173

209

examples,   , the norm of the halfspace b (or equivalently the margin parameter
  ) and, in the nonseparable case, the bounds also depend on the minimum hinge
loss of all halfspaces of norm     b. in contrast, the vc-dimension of the class of
homogenous halfspaces is d, which implies that the error of an erm hypothesis

decreases as (cid:112)d/m does. we now give an example in which   2b2 (cid:28) d; hence

the bound given in corollary 15.7 is much better than the vc bound.

consider the problem of learning to classify a short text document according
to its topic, say, whether the document is about sports or not. we    rst need to
represent documents as vectors. one simple yet e   ective way is to use a bag-
of-words representation. that is, we de   ne a dictionary of words and set the
dimension d to be the number of words in the dictionary. given a document,
we represent it as a vector x     {0, 1}d, where xi = 1 if the i   th word in the
dictionary appears in the document and xi = 0 otherwise. therefore, for this
problem, the value of   2 will be the maximal number of distinct words in a given
document.

a halfspace for this problem assigns weights to words. it is natural to assume
that by assigning positive and negative weights to a few dozen words we will
be able to determine whether a given document is about sports or not with
reasonable accuracy. therefore, for this problem, the value of b2 can be set to
be less than 100. overall, it is reasonable to say that the value of b2  2 is smaller
than 10,000.

on the other hand, a typical size of a dictionary is much larger than 10,000.
for example, there are more than 100,000 distinct words in english. we have
therefore shown a problem in which there can be an order of magnitude di   erence
between learning a halfspace with the id166 rule and learning a halfspace using
the vanilla erm rule.

of course, it is possible to construct problems in which the id166 bound will
be worse than the vc bound. when we use id166, we in fact introduce another
form of inductive bias     we prefer large margin halfspaces. while this induc-
tive bias can signi   cantly decrease our estimation error, it can also enlarge the
approximation error.

15.2.3

the ramp loss*

the margin-based bounds we have derived in corollary 15.7 rely on the fact that
we minimize the hinge loss. as we have shown in the previous subsection, the

term (cid:112)  2b2/m can be much smaller than the corresponding term in the vc
bound,(cid:112)d/m. however, the approximation error in corollary 15.7 is measured

with respect to the hinge loss while the approximation error in vc bounds is
measured with respect to the 0   1 loss. since the hinge loss upper bounds the
0   1 loss, the approximation error with respect to the 0   1 loss will never exceed
that of the hinge loss.

(cid:112)  2b2/m for the 0   1 loss. this follows from the fact that the 0   1 loss is scale

it is not possible to derive bounds that involve the estimation error term

210

support vector machines

insensitive, and therefore there is no meaning to the norm of w or its margin
when we measure error with the 0   1 loss. however, it is possible to de   ne a loss
function that on one hand it is scale sensitive and thus enjoys the estimation

error(cid:112)  2b2/m while on the other hand it is more similar to the 0   1 loss. one

option is the ramp loss, de   ned as

(cid:96)ramp(w, (x, y)) = min{1, (cid:96)hinge(w, (x, y))} = min{1 , max{0, 1     y(cid:104)w, x(cid:105)}}.
the ramp loss penalizes mistakes in the same way as the 0   1 loss and does not
penalize examples that are separated with margin. the di   erence between the
ramp loss and the 0   1 loss is only with respect to examples that are correctly
classi   ed but not with a signi   cant margin. generalization bounds for the ramp
loss are given in the advanced part of this book (see appendix 26.3).

(cid:96)hinge

(cid:96)0   1

(cid:96)ramp

1

1

y(cid:104)w, x(cid:105)

the reason id166 relies on the hinge loss and not on the ramp loss is that
the hinge loss is convex and, therefore, from the computational point of view,
minimizing the hinge loss can be performed e   ciently. in contrast, the problem
of minimizing the ramp loss is computationally intractable.

15.3

optimality conditions and    support vectors   *

the name    support vector machine    stems from the fact that the solution of
hard-id166, w0, is supported by (i.e., is in the linear span of) the examples that
are exactly at distance 1/(cid:107)w0(cid:107) from the separating hyperplane. these vectors are
therefore called support vectors. to see this, we rely on fritz john optimality
conditions.
theorem 15.8 let w0 be as de   ned in equation (15.3) and let i = {i :
|(cid:104)w0, xi(cid:105)| = 1}. then, there exist coe   cients   1, . . . ,   m such that

(cid:88)

i   i

w0 =

  ixi.

the examples {xi : i     i} are called support vectors.
the proof of this theorem follows by applying the following lemma to equa-

tion (15.3).

15.4 duality*

211

lemma 15.9 (fritz john) suppose that

w(cid:63)     argmin

f (w)

w

s.t.    i     [m], gi(w)     0,

   f (w(cid:63)) +(cid:80)

where f, g1, . . . , gm are di   erentiable. then, there exists        rm such that

i   i   i   gi(w(cid:63)) = 0, where i = {i : gi(w(cid:63)) = 0}.

15.4

duality*

historically, many of the properties of id166 have been obtained by considering
the dual of equation (15.3). our presentation of id166 does not rely on duality.
for completeness, we present in the following how to derive the dual of equa-
tion (15.3).

we start by rewriting the problem in an equivalent form as follows. consider

the function

g(w) =

max

     rm:     0

m(cid:88)

i=1

(cid:40)

  i(1     yi(cid:104)w, xi(cid:105)) =

(cid:0)(cid:107)w(cid:107)2 + g(w)(cid:1) .

if    i, yi(cid:104)w, xi(cid:105)     1

0
    otherwise

.

(15.7)

we can therefore rewrite equation (15.3) as

min

w

(cid:32)

(cid:32)

rearranging the preceding we obtain that equation (15.3) can be rewritten as
the problem

min

w

max

     rm:     0

(cid:107)w(cid:107)2 +

1
2

  i(1     yi(cid:104)w, xi(cid:105))

.

(15.8)

now suppose that we    ip the order of min and max in the above equation. this
can only decrease the objective value (see exercise 4), and we have

(cid:32)

m(cid:88)

(cid:107)w(cid:107)2 +

1
2

(cid:32)

i=1

(cid:107)w(cid:107)2 +

1
2

m(cid:88)

i=1

min

w

max

     rm:     0

    max

     rm:     0

min

w

(cid:33)

(cid:33)

  i(1     yi(cid:104)w, xi(cid:105))

  i(1     yi(cid:104)w, xi(cid:105))

.

the preceding inequality is called weak duality. it turns out that in our case,
strong duality also holds; namely, the inequality holds with equality. therefore,
the dual problem is

(cid:33)

(cid:33)

max

     rm:     0

min

w

(cid:107)w(cid:107)2 +

1
2

  i(1     yi(cid:104)w, xi(cid:105))

.

(15.9)

m(cid:88)

i=1

m(cid:88)

i=1

we can simplify the dual problem by noting that once    is    xed, the optimization

212

support vector machines

problem with respect to w is unconstrained and the objective is di   erentiable;
thus, at the optimum, the gradient equals zero:

w     m(cid:88)

m(cid:88)

  iyixi = 0     w =

  iyixi.

i=1

i=1

max

     rm:     0

  iyixi

+

  i

  jyjxj, xi

this shows us that the solution must be in the linear span of the examples, a
fact we will use later to derive id166 with kernels. plugging the preceding into
equation (15.9) we obtain that the dual problem can be rewritten as

       1

2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
       m(cid:88)

i=1

m(cid:88)

i=1

(cid:42)(cid:88)

j

      1     yi
m(cid:88)
m(cid:88)

i=1

j=1

max

     rm:     0

  i     1
2

  i  jyiyj(cid:104)xj, xi(cid:105)

(cid:43)             .
       .

(15.10)

(15.11)

rearranging yields the dual problem

note that the dual problem only involves inner products between instances and
does not require direct access to speci   c elements within an instance. this prop-
erty is important when implementing id166 with kernels, as we will discuss in
the next chapter.

15.5

implementing soft-id166 using sgd

in this section we describe a very simple algorithm for solving the optimization
problem of soft-id166, namely,

(cid:32)

m(cid:88)

i=1

min

w

(cid:107)w(cid:107)2 +

  
2

1
m

max{0, 1     y(cid:104)w, xi(cid:105)}

.

(15.12)

(cid:33)

we rely on the sgd framework for solving regularized loss minimization prob-
lems, as described in section 14.5.3.

recall that, on the basis of equation (14.15), we can rewrite the update rule

of sgd as

w(t+1) =     1
   t

t(cid:88)

j=1

vj,

where vj is a subgradient of the id168 at w(j) on the random example
chosen at iteration j. for the hinge loss, given an example (x, y), we can choose vj
to be 0 if y(cid:104)w(j), x(cid:105)     1 and vj =    y x otherwise (see example 14.2). denoting

  (t) =    (cid:80)

j<t vj we obtain the following procedure.

15.6 summary

213

sgd for solving soft-id166

goal: solve equation (15.12)
parameter: t
initialize:   (1) = 0
for t = 1, . . . , t
   t   (t)
let w(t) = 1
choose i uniformly at random from [m]
if (yi(cid:104)w(t), xi(cid:105) < 1)
set   (t+1) =   (t) + yixi

else

set   (t+1) =   (t)

(cid:80)t

t=1 w(t)

output:   w = 1
t

15.6

summary

id166 is an algorithm for learning halfspaces with a certain type of prior knowl-
edge, namely, preference for large margin. hard-id166 seeks the halfspace that
separates the data perfectly with the largest margin, whereas soft-id166 does
not assume separability of the data and allows the constraints to be violated to
some extent. the sample complexity for both types of id166 is di   erent from the
sample complexity of straightforward halfspace learning, as it does not depend
on the dimension of the domain but rather on parameters such as the maximal
norms of x and w.

the importance of dimension-independent sample complexity will be realized
in the next chapter, where we will discuss the embedding of the given domain
into some high dimensional feature space as means for enriching our hypothesis
class. such a procedure raises computational and sample complexity problems.
the latter is solved by using id166, whereas the former can be solved by using
id166 with kernels, as we will see in the next chapter.

15.7

bibliographic remarks

id166s have been introduced in (cortes & vapnik 1995, boser, guyon & vapnik
1992). there are many good books on the theoretical and practical aspects of
id166s. for example, (vapnik 1995, cristianini & shawe-taylor 2000, sch  olkopf
& smola 2002, hsu, chang & lin 2003, steinwart & christmann 2008). using
sgd for solving soft-id166 has been proposed in shalev-shwartz et al. (2007).

214

support vector machines

15.8

exercises

1. show that the hard-id166 rule, namely,
|(cid:104)w, xi(cid:105) + b|

argmax

(w,b):(cid:107)w(cid:107)=1

min
i   [m]

s.t.    i, yi((cid:104)w, xi(cid:105) + b) > 0,

is equivalent to the following formulation:

argmax

(w,b):(cid:107)w(cid:107)=1

min
i   [m]

yi((cid:104)w, xi(cid:105) + b).

(15.13)

hint: de   ne g = {(w, b) :    i, yi((cid:104)w, xi(cid:105) + b) > 0}.
1. show that

argmax

(w,b):(cid:107)w(cid:107)=1

min
i   [m]

2. show that    (w, b)     g,

yi((cid:104)w, xi(cid:105) + b)     g

min
i   [m]

yi((cid:104)w, xi(cid:105) + b) = min
i   [m]

|(cid:104)w, xi(cid:105) + b|

2. margin and the id88 consider a training set that is linearly sep-
arable with a margin    and such that all the instances are within a ball of
radius   . prove that the maximal number of updates the batch id88
algorithm given in section 9.1.2 will make when running on this training set
is (  /  )2.

3. hard versus soft id166: prove or refute the following claim:

there exists    > 0 such that for every sample s of m > 1 examples, which
is separable by the class of homogenous halfspaces, the hard-id166 and the
soft-id166 (with parameter   ) learning rules return exactly the same weight
vector.
4. weak duality: prove that for any function f of two vector variables x    
x , y     y, it holds that

y   y f (x, y)     max

x   x max
min

y   y min

x   x f (x, y).

16 kernel methods

in the previous chapter we described the id166 paradigm for learning halfspaces
in high dimensional feature spaces. this enables us to enrich the expressive
power of halfspaces by    rst mapping the data into a high dimensional feature
space, and then learning a linear predictor in that space. this is similar to the
adaboost algorithm, which learns a composition of a halfspace over base hy-
potheses. while this approach greatly extends the expressiveness of halfspace
predictors, it raises both sample complexity and computational complexity chal-
lenges. in the previous chapter we tackled the sample complexity issue using
the concept of margin. in this chapter we tackle the computational complexity
challenge using the method of kernels.

we start the chapter by describing the idea of embedding the data into a high
dimensional feature space. we then introduce the idea of kernels. a kernel is a
type of a similarity measure between instances. the special property of kernel
similarities is that they can be viewed as inner products in some hilbert space
(or euclidean space of some high dimension) to which the instance space is vir-
tually embedded. we introduce the    kernel trick    that enables computationally
e   cient implementation of learning, without explicitly handling the high dimen-
sional representation of the domain instances. kernel based learning algorithms,
and in particular kernel-id166, are very useful and popular machine learning
tools. their success may be attributed both to being    exible for accommodating
domain speci   c prior knowledge and to having a well developed set of e   cient
implementation algorithms.

16.1

embeddings into feature spaces

the expressive power of halfspaces is rather restricted     for example, the follow-
ing training set is not separable by a halfspace.

let the domain be the real line; consider the domain points {   10,   9,   8, . . . , 0,
1, . . . , 9, 10} where the labels are +1 for all x such that |x| > 2 and    1 otherwise.
to make the class of halfspaces more expressive, we can    rst map the original
instance space into another space (possibly of a higher dimension) and then
learn a halfspace in that space. for example, consider the example mentioned
previously. instead of learning a halfspace in the original representation let us

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

216

kernel methods

   rst de   ne a mapping    : r     r2 as follows:

  (x) = (x, x2).

we use the term feature space to denote the range of   . after applying    the
data can be easily explained using the halfspace h(x) = sign((cid:104)w,   (x)(cid:105)     b),
where w = (0, 1) and b = 5.

the basic paradigm is as follows:

1. given some domain set x and a learning task, choose a mapping    : x     f,
for some feature space f, that will usually be rn for some n (however, the
range of such a mapping can be any hilbert space, including such spaces of
in   nite dimension, as we will show later).

2. given a sequence of labeled examples, s = (x1, y1), . . . , (xm, ym), create the

image sequence   s = (  (x1), y1), . . . , (  (xm), ym).

3. train a linear predictor h over   s.
4. predict the label of a test point, x, to be h(  (x)).

note that, for every id203 distribution d over x    y, we can readily
de   ne its image id203 distribution d   over f    y by setting, for every
subset a     f    y, d  (a) = d(     1(a)).1 it follows that for every predictor h
over the feature space, ld   (h) = ld(h       ), where h        is the composition of h
onto   .

the success of this learning paradigm depends on choosing a good    for a
given learning task: that is, a    that will make the image of the data distribution
(close to being) linearly separable in the feature space, thus making the resulting
algorithm a good learner for a given task. picking such an embedding requires
prior knowledge about that task. however, often some generic mappings that
enable us to enrich the class of halfspaces and extend its expressiveness are used.
one notable example is polynomial mappings, which are a generalization of the
   we have seen in the previous example.
recall that the prediction of a standard halfspace classi   er on an instance x
is based on the linear mapping x (cid:55)    (cid:104)w, x(cid:105). we can generalize linear mappings
to a polynomial mapping, x (cid:55)    p(x), where p is a multivariate polynomial of
degree k. for simplicity, consider    rst the case in which x is 1 dimensional.
j=0 wjxj, where w     rk+1 is the vector of coe   cients
of the polynomial we need to learn. we can rewrite p(x) = (cid:104)w,   (x)(cid:105) where
   : r     rk+1 is the mapping x (cid:55)    (1, x, x2, x3, . . . , xk). it follows that
learning a k degree polynomial over r can be done by learning a linear mapping
in the (k + 1) dimensional feature space.

in that case, p(x) = (cid:80)k

more generally, a degree k multivariate polynomial from rn to r can be writ-

ten as

p(x) =

(cid:88)

j   [n]r:r   k

r(cid:89)

i=1

wj

xji.

(16.1)

1 this is de   ned for every a such that      1(a) is measurable with respect to d.

16.2 the kernel trick

217

as before, we can rewrite p(x) = (cid:104)w,   (x)(cid:105) where now    : rn     rd is such
that for every j     [n]r, r     k, the coordinate of   (x) associated with j is the

monomial(cid:81)r

i=1 xji.

naturally, polynomial-based classi   ers yield much richer hypothesis classes
than halfspaces. we have seen at the beginning of this chapter an example in
which the training set, in its original domain (x = r), cannot be separable
by a halfspace, but after the embedding x (cid:55)    (x, x2) it is perfectly separable.
so, while the classi   er is always linear in the feature space, it can have highly
nonlinear behavior on the original space from which instances were sampled.

in general, we can choose any feature mapping    that maps the original in-
stances into some hilbert space.2 the euclidean space rd is a hilbert space for
any    nite d. but there are also in   nite dimensional hilbert spaces (as we shall
see later on in this chapter).

the bottom line of this discussion is that we can enrich the class of halfspaces
by    rst applying a nonlinear mapping,   , that maps the instance space into some
feature space, and then learning a halfspace in that feature space. however, if
the range of    is a high dimensional space we face two problems. first, the vc-
dimension of halfspaces in rn is n + 1, and therefore, if the range of    is very
large, we need many more samples in order to learn a halfspace in the range
of   . second, from the computational point of view, performing calculations in
the high dimensional space might be too costly. in fact, even the representation
of the vector w in the feature space can be unrealistic. the    rst issue can be
tackled using the paradigm of large margin (or low norm predictors), as we
already discussed in the previous chapter in the context of the id166 algorithm.
in the following section we address the computational issue.

16.2

the kernel trick

we have seen that embedding the input space into some high dimensional feature
space makes halfspace learning more expressive. however, the computational
complexity of such learning may still pose a serious hurdle     computing linear
separators over very high dimensional data may be computationally expensive.
the common solution to this concern is kernel based learning. the term    kernels   
is used in this context to describe inner products in the feature space. given
an embedding    of some domain space x into some hilbert space, we de   ne
the id81 k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105). one can think of k as specifying
similarity between instances and of the embedding    as mapping the domain set

2 a hilbert space is a vector space with an inner product, which is also complete. a space is

in our case, the norm (cid:107)w(cid:107) is de   ned by the inner product(cid:112)(cid:104)w, w(cid:105). the reason we require

complete if all cauchy sequences in the space converge.

the range of    to be in a hilbert space is that projections in a hilbert space are well
de   ned. in particular, if m is a linear subspace of a hilbert space, then every x in the
hilbert space can be written as a sum x = u + v where u     m and (cid:104)v, w(cid:105) = 0 for all
w     m . we use this fact in the proof of the representer theorem given in the next section.

218

kernel methods

x into a space where these similarities are realized as inner products. it turns
out that many learning algorithms for halfspaces can be carried out just on the
basis of the values of the id81 over pairs of domain points. the main
advantage of such algorithms is that they implement linear separators in high
dimensional feature spaces without having to specify points in that space or
expressing the embedding    explicitly. the remainder of this section is devoted
to constructing such algorithms.

in the previous chapter we saw that regularizing the norm of w yields a small
sample complexity even if the dimensionality of the feature space is high. inter-
estingly, as we show later, regularizing the norm of w is also helpful in overcoming
the computational problem. to do so,    rst note that all versions of the id166 op-
timization problem we have derived in the previous chapter are instances of the
following general problem:

w

min

(f ((cid:104)w,   (x1)(cid:105) , . . . ,(cid:104)w,   (xm)(cid:105)) + r((cid:107)w(cid:107))),

(16.2)
where f : rm     r is an arbitrary function and r : r+     r is a monotoni-
(cid:80)
cally nondecreasing function. for example, soft-id166 for homogenous halfspaces
(equation (15.6)) can be derived from equation (16.2) by letting r(a) =   a2 and
i max{0, 1   yiai}. similarly, hard-id166 for nonhomogenous
f (a1, . . . , am) = 1
m
halfspaces (equation (15.2)) can be derived from equation (16.2) by letting
r(a) = a2 and letting f (a1, . . . , am) be 0 if there exists b such that yi(ai +b)     1
for all i, and f (a1, . . . , am) =     otherwise.
tion (16.2) that lies in the span of {  (x1), . . . ,   (xm)}.
theorem 16.1 (representer theorem) assume that    is a mapping from x to
i=1   i  (xi)

a hilbert space. then, there exists a vector        rm such that w =(cid:80)m

the following theorem shows that there exists an optimal solution of equa-

is an optimal solution of equation (16.2).

proof let w(cid:63) be an optimal solution of equation (16.2). because w(cid:63) is an
element of a hilbert space, we can rewrite w(cid:63) as

m(cid:88)

w(cid:63) =

  i  (xi) + u,

where (cid:104)u,   (xi)(cid:105) = 0 for all i. set w = w(cid:63)     u. clearly, (cid:107)w(cid:63)(cid:107)2 = (cid:107)w(cid:107)2 + (cid:107)u(cid:107)2,
thus (cid:107)w(cid:107)     (cid:107)w(cid:63)(cid:107). since r is nondecreasing we obtain that r((cid:107)w(cid:107))     r((cid:107)w(cid:63)(cid:107)).
additionally, for all i we have that

i=1

(cid:104)w,   (xi)(cid:105) = (cid:104)w(cid:63)     u,   (xi)(cid:105) = (cid:104)w(cid:63),   (xi)(cid:105),

hence

f ((cid:104)w,   (x1)(cid:105) , . . . ,(cid:104)w,   (xm)(cid:105)) = f ((cid:104)w(cid:63),   (x1)(cid:105) , . . . ,(cid:104)w(cid:63),   (xm)(cid:105)) .

w =(cid:80)m

we have shown that the objective of equation (16.2) at w cannot be larger
than the objective at w(cid:63) and therefore w is also an optimal solution. since

i=1   i  (xi) we conclude our proof.

16.2 the kernel trick

219

w =(cid:80)m

on the basis of the representer theorem we can optimize equation (16.2) with
respect to the coe   cients    instead of the coe   cients w as follows. writing

(cid:42)(cid:88)

j=1   j  (xj) we have that for all i

(cid:104)w,   (xi)(cid:105) =

  j  (xj),   (xi)

=

similarly,

(cid:107)w(cid:107)2 =

j

  j  (xj),

(cid:42)(cid:88)

j

(cid:88)

j

(cid:43)

  j  (xj)

=

(cid:43)

m(cid:88)

j=1

m(cid:88)

  j(cid:104)  (xj),   (xi)(cid:105).

  i  j(cid:104)  (xi),   (xj)(cid:105).

i,j=1

let k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105) be a function that implements the id81
with respect to the embedding   . instead of solving equation (16.2) we can solve
the equivalent problem

       m(cid:88)
      (cid:118)(cid:117)(cid:117)(cid:116) m(cid:88)

j=1

min
     rm

f

+ r

  jk(xj, x1), . . . ,

  jk(xj, xm)

      

m(cid:88)
      .

j=1

  i  jk(xj, xi)

(16.3)

i,j=1

to solve the optimization problem given in equation (16.3), we do not need any
direct access to elements in the feature space. the only thing we should know is
how to calculate inner products in the feature space, or equivalently, to calculate
the id81. in fact, to solve equation (16.3) we solely need to know the
value of the m    m matrix g s.t. gi,j = k(xi, xj), which is often called the
gram matrix.

in particular, specifying the preceding to the soft-id166 problem given in equa-

tion (15.6), we can rewrite the problem as

(cid:32)

min
     rm

    t g   +

max(cid:8)0, 1     yi(g  )i

(cid:9)(cid:33)

m(cid:88)

i=1

1
m

,

(16.4)

where (g  )i is the i   th element of the vector obtained by multiplying the gram
matrix g by the vector   . note that equation (16.4) can be written as quadratic
programming and hence can be solved e   ciently. in the next section we describe
an even simpler algorithm for solving soft-id166 with kernels.

once we learn the coe   cients    we can calculate the prediction on a new

instance by

(cid:104)w,   (x)(cid:105) =

m(cid:88)

m(cid:88)

  j(cid:104)  (xj),   (x)(cid:105) =

  jk(xj, x).

j=1

j=1

the advantage of working with kernels rather than directly optimizing w in
the feature space is that in some situations the dimension of the feature space

220

kernel methods

is extremely large while implementing the id81 is very simple. a few
examples are given in the following.

example 16.1 (polynomial kernels) the k degree polynomial kernel is de   ned
to be

k(x, x(cid:48)) = (1 + (cid:104)x, x(cid:48)(cid:105))k.

now we will show that this is indeed a id81. that is, we will show
that there exists a mapping    from the original space to some higher dimensional
space for which k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105). for simplicity, denote x0 = x(cid:48)
0 = 1.
then, we have

k(x, x(cid:48)) = (1 + (cid:104)x, x(cid:48)(cid:105))k = (1 + (cid:104)x, x(cid:48)(cid:105))              (1 + (cid:104)x, x(cid:48)(cid:105))

j

=

j=0

                   
k(cid:89)
k(cid:89)

xjx(cid:48)

       n(cid:88)
(cid:88)
(cid:88)
element of   (x) that equals(cid:81)k

j   {0,1,...,n}k
now, if we de   ne    : rn     r(n+1)k

j   {0,1,...,n}k

i=1

i=1

=

=

xji

      

xjx(cid:48)

j

       n(cid:88)

j=0

xjix(cid:48)

ji

k(cid:89)

i=1

x(cid:48)

ji

.

i=1 xji, we obtain that
k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105).

such that for j     {0, 1, . . . , n}k there is an

since    contains all the monomials up to degree k, a halfspace over the range
of    corresponds to a polynomial predictor of degree k over the original space.
hence, learning a halfspace with a k degree polynomial kernel enables us to learn
polynomial predictors of degree k over the original space.

note that here the complexity of implementing k is o(n) while the dimension

of the feature space is on the order of nk.
example 16.2 (gaussian kernel) let the original instance space be r and
consider the mapping    where for each nonnegative integer n     0 there exists
an element   (x)n that equals

2 xn. then,

    x2

e

(cid:104)  (x),   (x(cid:48))(cid:105) =

(cid:19)

    (x(cid:48))2

2

e

(x(cid:48))n

   (cid:88)

1   
n!

(cid:18) 1   

n=0

n!
    x2+(x(cid:48))2

2

= e

   (cid:107)x   x(cid:48)(cid:107)2

2

= e

(cid:19)(cid:18) 1   
(cid:19)
(cid:18) (xx(cid:48))n

2 xn

    x2

e

   (cid:88)

n!

n!

n=0

.

here the feature space is of in   nite dimension while evaluating the kernel is very

16.2 the kernel trick

221

simple. more generally, given a scalar    > 0, the gaussian kernel is de   ned to
be

k(x, x(cid:48)) = e

   (cid:107)x   x(cid:48)(cid:107)2

2   

.

intuitively, the gaussian kernel sets the inner product in the feature space
between x, x(cid:48) to be close to zero if the instances are far away from each other
(in the original domain) and close to 1 if they are close.    is a parameter that
controls the scale determining what we mean by    close.    it is easy to verify that
k implements an inner product in a space in which for any n and any monomial

of order k there exists an element of   (x) that equals

i=1 xji.
hence, we can learn any polynomial predictor over the original space by using a
gaussian kernel.

e

2 (cid:81)n

   (cid:107)x(cid:107)2

1   
n!

recall that the vc-dimension of the class of all polynomial predictors is in   -
nite (see exercise 12). there is no contradiction, because the sample complexity
required to learn with gaussian kernels depends on the margin in the feature
space, which will be large if we are lucky, but can in general be arbitrarily small.
the gaussian kernel is also called the rbf kernel, for    radial basis func-

tions.   

16.2.1

kernels as a way to express prior knowledge

as we discussed previously, a feature mapping,   , may be viewed as expanding
the class of linear classi   ers to a richer class (corresponding to linear classi   ers
over the feature space). however, as discussed in the book so far, the suitability
of any hypothesis class to a given learning task depends on the nature of that
task. one can therefore think of an embedding    as a way to express and utilize
prior knowledge about the problem at hand. for example, if we believe that
positive examples can be distinguished by some ellipse, we can de   ne    to be all
the monomials up to order 2, or use a degree 2 polynomial kernel.

as a more realistic example, consider the task of learning to    nd a sequence of
characters (   signature   ) in a    le that indicates whether it contains a virus or not.
formally, let xd be the set of all strings of length at most d over some alphabet
set   . the hypothesis class that one wishes to learn is h = {hv : v     xd}, where,
for a string x     xd, hv(x) is 1 i    v is a substring of x (and hv(x) =    1 otherwise).
let us show how using an appropriate embedding this class can be realized by
linear classi   ers over the resulting feature space. consider a mapping    to a space
rs where s = |xd|, so that each coordinate of   (x) corresponds to some string v
and indicates whether v is a substring of x (that is, for every x     xd,   (x) is a
vector in {0, 1}|xd|). note that the dimension of this feature space is exponential
in d. it is not hard to see that every member of the class h can be realized by
composing a linear classi   er over   (x), and, moreover, by such a halfspace whose
norm is 1 and that attains a margin of 1 (see exercise 1). furthermore, for every
x     x , (cid:107)  (x)(cid:107) = o(d). so, overall, it is learnable using id166 with a sample

222

kernel methods

complexity that is polynomial in d. however, the dimension of the feature space
is exponential in d so a direct implementation of id166 over the feature space is
problematic. luckily, it is easy to calculate the inner product in the feature space
(i.e., the id81) without explicitly mapping instances into the feature
space. indeed, k(x, x(cid:48)) is simply the number of common substrings of x and x(cid:48),
which can be easily calculated in time polynomial in d.

this example also demonstrates how feature mapping enables us to use halfspaces

for nonvectorial domains.

16.2.2

characterizing id81s*

as we have discussed in the previous section, we can think of the speci   cation of
the kernel matrix as a way to express prior knowledge. consider a given similarity
function of the form k : x    x     r. is it a valid id81? that is, does
it represent an inner product between   (x) and   (x(cid:48)) for some feature mapping
  ? the following lemma gives a su   cient and necessary condition.
lemma 16.2 a symmetric function k : x    x     r implements an inner
product in some hilbert space if and only if it is positive semide   nite; namely,
for all x1, . . . , xm, the gram matrix, gi,j = k(xi, xj), is a positive semide   nite
matrix.

proof
it is trivial to see that if k implements an inner product in some hilbert
space then the gram matrix is positive semide   nite. for the other direction,
de   ne the space of functions over x as rx = {f : x     r}. for each x     x
let   (x) be the function x (cid:55)    k(  , x). de   ne a vector space by taking all linear
combinations of elements of the form k(  , x). de   ne an inner product on this

vector space to be(cid:42)(cid:88)

(cid:88)

(cid:43)

(cid:88)

  ik(  , xi),

  jk(  , x(cid:48)
j)

=

  i  jk(xi, x(cid:48)
j).

i

j

i,j

this is a valid inner product since it is symmetric (because k is symmetric), it is
linear (immediate), and it is positive de   nite (it is easy to see that k(x, x)     0
with equality only for   (x) being the zero function). clearly,
(cid:104)  (x),   (x(cid:48))(cid:105) = (cid:104)k(  , x), k(  , x(cid:48))(cid:105) = k(x, x(cid:48)),

which concludes our proof.

16.3

implementing soft-id166 with kernels

next, we turn to solving soft-id166 with kernels. while we could have designed
an algorithm for solving equation (16.4), there is an even simpler approach that

16.3 implementing soft-id166 with kernels

223

directly tackles the soft-id166 optimization problem in the feature space,

min

w

(cid:107)w(cid:107)2 +

  
2

1
m

max{0, 1     y(cid:104)w,   (xi)(cid:105)}

,

(16.5)

(cid:32)

m(cid:88)

i=1

(cid:33)

m(cid:88)
m(cid:88)

j=1

while only using kernel evaluations. the basic observation is that the vector w(t)
maintained by the sgd procedure we have described in section 15.5 is always in
the linear span of {  (x1), . . . ,   (xm)}. therefore, rather than maintaining w(t)
we can maintain the corresponding coe   cients   .
formally, let k be the id81, namely, for all x, x(cid:48), k(x, x(cid:48)) =
(cid:104)  (x),   (x(cid:48))(cid:105). we shall maintain two vectors in rm, corresponding to two vectors
  (t) and w(t) de   ned in the sgd procedure of section 15.5. that is,   (t) will be
a vector such that

and   (t) be such that

  (t) =

w(t) =

  (t)
j   (xj)

  (t)

j   (xj).

(16.6)

(16.7)

j=1

the vectors    and    are updated according to the following procedure.

sgd for solving soft-id166 with kernels

goal: solve equation (16.5)
parameter: t
initialize:   (1) = 0
for t = 1, . . . , t
let   (t) = 1
choose i uniformly at random from [m]
for all j (cid:54)= i set   (t+1)
if (yi

(cid:80)m

=   (t)
j k(xj, xi) < 1)
=   (t)

set   (t+1)

j=1   (t)

   t   (t)

j

j

i + yi

i

else

output:   w =(cid:80)m

set   (t+1)

=   (t)

i

i
j=1     j  (xj) where      = 1
t

(cid:80)t

t=1   (t)

the following lemma shows that the preceding implementation is equivalent

to running the sgd procedure described in section 15.5 on the feature space.

tion 15.5, when applied on the feature space, and let   w = (cid:80)m

lemma 16.3 let   w be the output of the sgd procedure described in sec-
j=1     j  (xj) be

the output of applying sgd with kernels. then   w =   w.

proof we will show that for every t equation (16.6) holds, where   (t) is the
result of running the sgd procedure described in section 15.5 in the feature

224

kernel methods

   t   (t), this claim implies
space. by the de   nition of   (t) = 1
that equation (16.7) also holds, and the proof of our lemma will follow. to prove
that equation (16.6) holds we use a simple inductive argument. for t = 1 the
claim trivially holds. assume it holds for t     1. then,

   t   (t) and w(t) = 1

(cid:68)

(cid:69)

(cid:42)(cid:88)

(cid:43)

m(cid:88)

yi

w(t),   (xi)

= yi

  (t)

j   (xj),   (xi)

= yi

  (t)

j k(xj, xi).

j

j=1

hence, the condition in the two algorithms is equivalent and if we update    we
have

m(cid:88)

m(cid:88)

  (t)
j   (xj) + yi  (xi) =

  (t+1)
j

  (xj),

j=1

j=1

  (t+1) =   (t) + yi  (xi) =

which concludes our proof.

16.4

summary

mappings from the given domain to some higher dimensional space, on which a
halfspace predictor is used, can be highly powerful. we bene   t from a rich and
complex hypothesis class, yet need to solve the problems of high sample and
computational complexities. in chapter 10, we discussed the adaboost algo-
rithm, which faces these challenges by using a weak learner: even though we   re
in a very high dimensional space, we have an    oracle    that bestows on us a
single good coordinate to work with on each iteration. in this chapter we intro-
duced a di   erent approach, the kernel trick. the idea is that in order to    nd a
halfspace predictor in the high dimensional space, we do not need to know the
representation of instances in that space, but rather the values of inner products
between the mapped instances. calculating inner products between instances in
the high dimensional space without using their representation in that space is
done using id81s. we have also shown how the sgd algorithm can be
implemented using kernels.

the ideas of feature mapping and the kernel trick allow us to use the framework
of halfspaces and linear predictors for nonvectorial data. we demonstrated how
kernels can be used to learn predictors over the domain of strings.

we presented the applicability of the kernel trick in id166. however, the kernel
trick can be applied in many other algorithms. a few examples are given as
exercises.

this chapter ends the series of chapters on linear predictors and convex prob-
lems. the next two chapters deal with completely di   erent types of hypothesis
classes.

16.5 bibliographic remarks

225

16.5

bibliographic remarks

in the context of id166, the kernel-trick has been introduced in boser et al. (1992).
see also aizerman, braverman & rozonoer (1964). the observation that the
kernel-trick can be applied whenever an algorithm only relies on inner products
was    rst stated by sch  olkopf, smola & m  uller (1998). the proof of the representer
theorem is given in (sch  olkopf, herbrich, smola & williamson 2000, sch  olkopf,
herbrich & smola 2001). the conditions stated in lemma 16.2 are simpli   cation
of conditions due to mercer. many useful id81s have been introduced
in the literature for various applications. we refer the reader to sch  olkopf &
smola (2002).

16.6

exercises

1. consider the task of    nding a sequence of characters in a    le, as described
in section 16.2.1. show that every member of the class h can be realized by
composing a linear classi   er over   (x), whose norm is 1 and that attains a
margin of 1.

2. kernelized id88: show how to run the id88 algorithm while
only accessing the instances via the id81. hint: the derivation is
similar to the derivation of implementing sgd with kernels.

3. kernel ridge regression: the ridge regression problem, with a feature
mapping   , is the problem of    nding a vector w that minimizes the function

m(cid:88)

i=1

f (w) =   (cid:107)w(cid:107)2 +

1
2m

((cid:104)w,   (xi)(cid:105)     yi)2,

(16.8)

and then returning the predictor

h(x) = (cid:104)w, x(cid:105).

such that(cid:80)m

show how to implement the ridge regression algorithm with kernels.

hint: the representer theorem tells us that there exists a vector        rm

i=1   i  (xi) is a minimizer of equation (16.8).

1. let g be the gram matrix with regard to s and k. that is, gij =

k(xi, xj). de   ne g : rm     r by
g(  ) =         t g   +

tion (16.9) then w    =(cid:80)m

i=1      

m(cid:88)

i=1

1
2m

where g  ,i is the i   th column of g. show that if       minimizes equa-

2. find a closed form expression for      .

4. let n be any positive integer. for every x, x(cid:48)     {1, . . . , n} de   ne

i   (xi) is a minimizer of f .

((cid:104)  , g  ,i(cid:105)     yi)2,

(16.9)

k(x, x(cid:48)) = min{x, x(cid:48)}.

226

kernel methods

prove that k is a valid kernel; namely,    nd a mapping    : {1, . . . , n}     h
where h is some hilbert space, such that

   x, x(cid:48)     {1, . . . , n}, k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105).

5. a supermarket manager would like to learn which of his customers have babies
on the basis of their shopping carts. speci   cally, he sampled i.i.d. customers,
where for customer i, let xi     {1, . . . , d} denote the subset of items the
customer bought, and let yi     {  1} be the label indicating whether this
customer has a baby. as prior knowledge, the manager knows that there are
k items such that the label is determined to be 1 i    the customer bought
at least one of these k items. of course, the identity of these k items is not
known (otherwise, there was nothing to learn). in addition, according to the
store regulation, each customer can buy at most s items. help the manager to
design a learning algorithm such that both its time complexity and its sample
complexity are polynomial in s, k, and 1/ .
6. let x be an instance set and let    be a feature mapping of x into some
hilbert feature space v . let k : x    x     r be a id81 that
implements inner products in the feature space v .

consider the binary classi   cation algorithm that predicts the label of
an unseen instance according to the class with the closest average. formally,
given a training sequence s = (x1, y1), . . . , (xm, ym), for every y     {  1} we
de   ne

(cid:88)

i:yi=y

cy =

1
my

  (xi).

where my = |{i : yi = y}|. we assume that m+ and m    are nonzero. then,
the algorithm outputs the following decision rule:

(cid:40)

h(x) =

(cid:107)  (x)     c+(cid:107)     (cid:107)  (x)     c   (cid:107)
otherwise.

1

0

1. let w = c+     c    and let b = 1

2 ((cid:107)c   (cid:107)2     (cid:107)c+(cid:107)2). show that

h(x) = sign((cid:104)w,   (x)(cid:105) + b).

2. show how to express h(x) on the basis of the id81, and without

accessing individual entries of   (x) or w.

17 multiclass, ranking, and complex

prediction problems

multiclass categorization is the problem of classifying instances into one of several
possible target classes. that is, we are aiming at learning a predictor h : x     y,
where y is a    nite set of categories. applications include, for example, catego-
rizing documents according to topic (x is the set of documents and y is the set
of possible topics) or determining which object appears in a given image (x is
the set of images and y is the set of possible objects).

the centrality of the multiclass learning problem has spurred the development
of various approaches for tackling the task. perhaps the most straightforward
approach is a reduction from multiclass classi   cation to binary classi   cation. in
section 17.1 we discuss the most common two reductions as well as the main
drawback of the reduction approach.

we then turn to describe a family of linear predictors for multiclass problems.
relying on the rlm and sgd frameworks from previous chapters, we describe
several practical algorithms for multiclass prediction.

in section 17.3 we show how to use the multiclass machinery for complex pre-
diction problems in which y can be extremely large but has some structure on
it. this task is often called structured output learning. in particular, we demon-
strate this approach for the task of recognizing handwritten words, in which y
is the set of all possible strings of some bounded length (hence, the size of y is
exponential in the maximal length of a word).

finally, in section 17.4 and section 17.5 we discuss ranking problems in which
the learner should order a set of instances according to their    relevance.    a typ-
ical application is ordering results of a search engine according to their relevance
to the query. we describe several performance measures that are adequate for
assessing the performance of ranking predictors and describe how to learn linear
predictors for ranking problems e   ciently.

17.1

one-versus-all and all-pairs

the simplest approach to tackle multiclass prediction problems is by reduction
to binary classi   cation. recall that in multiclass prediction we would like to learn
a function h : x     y. without loss of generality let us denote y = {1, . . . , k}.
in the one-versus-all method (a.k.a. one-versus-rest) we train k binary clas-

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

228

multiclass, ranking, and complex prediction problems

si   ers, each of which discriminates between one class and the rest of the classes.
that is, given a training set s = (x1, y1), . . . , (xm, ym), where every yi is in y, we
construct k binary training sets, s1, . . . , sk, where si = (x1, (   1)1[y1(cid:54)=i] ), . . . , (xm, (   1)1[ym(cid:54)=i] ).
in words, si is the set of instances labeled 1 if their label in s was i, and    1
otherwise. for every i     [k] we train a binary predictor hi : x     {  1} based on
si, hoping that hi(x) should equal 1 if and only if x belongs to class i. then,
given h1, . . . , hk, we construct a multiclass predictor using the rule

h(x)     argmax
i   [k]

hi(x).

(17.1)

when more than one binary hypothesis predicts    1    we should somehow decide
which class to predict (e.g., we can arbitrarily decide to break ties by taking the
minimal index in argmaxi hi(x)). a better approach can be applied whenever
each hi hides additional information, which can be interpreted as the con   dence
in the prediction y = i. for example, this is the case in halfspaces, where the
actual prediction is sign((cid:104)w, x(cid:105)), but we can interpret (cid:104)w, x(cid:105) as the con   dence
in the prediction. in such cases, we can apply the multiclass rule given in equa-
tion (17.1) on the real valued predictions. a pseudocode of the one-versus-all
approach is given in the following.

one-versus-all

input:

training set s = (x1, y1), . . . , (xm, ym)
algorithm for binary classi   cation a
foreach i     y
let si = (x1, (   1)1[y1(cid:54)=i] ), . . . , (xm, (   1)1[ym(cid:54)=i] )
let hi = a(si)
output:
the multiclass hypothesis de   ned by h(x)     argmaxi   y hi(x)

another popular reduction is the all-pairs approach, in which all pairs of
classes are compared to each other. formally, given a training set s = (x1, y1), . . . , (xm, ym),
where every yi is in [k], for every 1     i < j     k we construct a binary training
sequence, si,j, containing all examples from s whose label is either i or j. for
each such an example, we set the binary label in si,j to be +1 if the multiclass
label in s is i and    1 if the multiclass label in s is j. next, we train a binary
classi   cation algorithm based on every si,j to get hi,j. finally, we construct
a multiclass classi   er by predicting the class that had the highest number of
   wins.    a pseudocode of the all-pairs approach is given in the following.

17.1 one-versus-all and all-pairs

229

all-pairs

input:

training set s = (x1, y1), . . . , (xm, ym)
algorithm for binary classi   cation a
foreach i, j     y s.t. i < j
initialize si,j to be the empty sequence
for t = 1, . . . , m
if yt = i add (xt, 1) to si,j
if yt = j add (xt,   1) to si,j
let hi,j = a(si,j)

output:

the multiclass hypothesis de   ned by
h(x)     argmaxi   y

(cid:16)(cid:80)

(cid:17)
j   y sign(j     i) hi,j(x)

although reduction methods such as the one-versus-all and all-pairs are
simple and easy to construct from existing algorithms, their simplicity has a
price. the binary learner is not aware of the fact that we are going to use its
output hypotheses for constructing a multiclass predictor, and this might lead
to suboptimal results, as illustrated in the following example.

example 17.1 consider a multiclass categorization problem in which the in-
stance space is x = r2 and the label set is y = {1, 2, 3}. suppose that instances
of the di   erent classes are located in nonintersecting balls as depicted in the fol-
lowing.

1

2

3

suppose that the id203 masses of classes 1, 2, 3 are 40%, 20%, and 40%,
respectively. consider the application of one-versus-all to this problem, and as-
sume that the binary classi   cation algorithm used by one-versus-all is erm
with respect to the hypothesis class of halfspaces. observe that for the prob-
lem of discriminating between class 2 and the rest of the classes, the optimal
halfspace would be the all negative classi   er. therefore, the multiclass predic-
tor constructed by one-versus-all might err on all the examples from class 2
(this will be the case if the tie in the de   nition of h(x) is broken by the nu-
merical value of the class label). in contrast, if we choose hi(x) = (cid:104)wi, x(cid:105),
where w1 =
, then the classi-
   er de   ned by h(x) = argmaxi hi(x) perfectly predicts all the examples. we see

, w2 = (0, 1), and w3 =

(cid:16)    1   

(cid:16) 1   

, 1   
2

, 1   
2

(cid:17)

(cid:17)

2

2

230

multiclass, ranking, and complex prediction problems

that even though the approximation error of the class of predictors of the form
h(x) = argmaxi(cid:104)wi, x(cid:105) is zero, the one-versus-all approach might fail to    nd a
good predictor from this class.

17.2

linear multiclass predictors

in light of the inadequacy of reduction methods, in this section we study a more
direct approach for learning multiclass predictors. we describe the family of
linear multiclass predictors. to motivate the construction of this family, recall
that a linear predictor for binary classi   cation (i.e., a halfspace) takes the form

h(x) = sign((cid:104)w, x(cid:105)).

an equivalent way to express the prediction is as follows:

h(x) = argmax
y   {  1}

(cid:104)w, yx(cid:105),

where yx is the vector obtained by multiplying each element of x by y.
this representation leads to a natural generalization of halfspaces to multiclass
problems as follows. let    : x    y     rd be a class-sensitive feature mapping.
that is,    takes as input a pair (x, y) and maps it into a d dimensional feature
vector. intuitively, we can think of the elements of   (x, y) as score functions that
assess how well the label y    ts the instance x. we will elaborate on    later on.
given    and a vector w     rd, we can de   ne a multiclass predictor, h : x     y,
as follows:

h(x) = argmax

y   y

(cid:104)w,   (x, y)(cid:105).

that is, the prediction of h for the input x is the label that achieves the highest
weighted score, where weighting is according to the vector w.
let w be some set of vectors in rd, for example, w = {w     rd : (cid:107)w(cid:107)     b},
for some scalar b > 0. each pair (  , w ) de   nes a hypothesis class of multiclass
predictors:

h  ,w = {x (cid:55)    argmax
y   y

(cid:104)w,   (x, y)(cid:105) : w     w}.

of course, the immediate question, which we discuss in the sequel, is how to
construct a good   . note that if y = {  1} and we set   (x, y) = yx and
w = rd, then h  ,w becomes the hypothesis class of homogeneous halfspace
predictors for binary classi   cation.

17.2.1

how to construct   

as mentioned before, we can think of the elements of   (x, y) as score functions
that assess how well the label y    ts the instance x. naturally, designing a good   
is similar to the problem of designing a good feature mapping (as we discussed in

17.2 linear multiclass predictors

231

chapter 16 and as we will discuss in more detail in chapter 25). two examples
of useful constructions are given in the following.

the multivector construction:
let y = {1, . . . , k} and let x = rn. we de   ne    : x    y     rd, where d = nk,
as follows

(cid:124) (cid:123)(cid:122) (cid:125)

(cid:124)

  (x, y) = [ 0, . . . , 0
   r(y   1)n

(cid:123)(cid:122)

   rn

, x1, . . . , xn

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

, 0, . . . , 0
   r(k   y)n

].

(17.2)

that is,   (x, y) is composed of k vectors, each of which is of dimension n, where
we set all the vectors to be the all zeros vector except the y   th vector, which is
set to be x. it follows that we can think of w     rnk as being composed of k
weight vectors in rn, that is, w = [w1;
; wk], hence the name multivec-
tor construction. by the construction we have that (cid:104)w,   (x, y)(cid:105) = (cid:104)wy, x(cid:105), and
therefore the multiclass prediction becomes

. . .

h(x) = argmax

y   y

(cid:104)wy, x(cid:105).

a geometric illustration of the multiclass prediction over x = r2 is given in the
following.

w2

w1

w3

w4

tf-idf:
the previous de   nition of   (x, y) does not incorporate any prior knowledge
about the problem. we next describe an example of a feature function    that
does incorporate prior knowledge. let x be a set of text documents and y be a
set of possible topics. let d be a size of a dictionary of words. for each word in the
dictionary, whose corresponding index is j, let t f (j, x) be the number of times
the word corresponding to j appears in the document x. this quantity is called
term-frequency. additionally, let df (j, y) be the number of times the word
corresponding to j appears in documents in our training set that are not about
topic y. this quantity is called document-frequency and measures whether word
j is frequent in other topics. now, de   ne    : x    y     rd to be such that

(cid:16) m

(cid:17)

,

  j(x, y) = t f (j, x) log

df (j,y)

where m is the total number of documents in our training set. the preced-
ing quantity is called term-frequency-inverse-document-frequency or tf-idf for

232

multiclass, ranking, and complex prediction problems

short. intuitively,   j(x, y) should be large if the word corresponding to j ap-
pears a lot in the document x but does not appear at all in documents that are
not on topic y. if this is the case, we tend to believe that the document x is on
topic y. note that unlike the multivector construction described previously, in
the current construction the dimension of    does not depend on the number of
topics (i.e., the size of y).

17.2.2

cost-sensitive classi   cation

so far we used the zero-one loss as our performance measure of the quality of
h(x). that is, the loss of a hypothesis h on an example (x, y) is 1 if h(x) (cid:54)= y and
0 otherwise. in some situations it makes more sense to penalize di   erent levels
of loss for di   erent mistakes. for example, in object recognition tasks, it is less
severe to predict that an image of a tiger contains a cat than predicting that
the image contains a whale. this can be modeled by specifying a id168,
    : y    y     r+, where for every pair of labels, y(cid:48), y, the loss of predicting
the label y(cid:48) when the correct label is y is de   ned to be    (y(cid:48), y). we assume
that    (y, y) = 0. note that the zero-one loss can be easily modeled by setting
   (y(cid:48), y) = 1[y(cid:48)(cid:54)=y].

17.2.3

erm
we have de   ned the hypothesis class h  ,w and speci   ed a id168    . to
learn the class with respect to the id168, we can apply the erm rule with
respect to this class. that is, we search for a multiclass hypothesis h     h  ,w ,
parameterized by a vector w, that minimizes the empirical risk with respect to
   ,

m(cid:88)

i=1

ls(h) =

1
m

   (h(xi), yi).

we now show that when w = rd and we are in the realizable case, then it is
possible to solve the erm problem e   ciently using id135. indeed,
in the realizable case, we need to    nd a vector w     rd that satis   es

   i     [m],

yi = argmax

y   y

(cid:104)w,   (xi, y)(cid:105).

equivalently, we need that w will satisfy the following set of linear inequalities

   i     [m],    y     y \ {yi},

(cid:104)w,   (xi, yi)(cid:105) > (cid:104)w,   (xi, y)(cid:105).

finding w that satis   es the preceding set of linear equations amounts to solving
a linear program.

as in the case of binary classi   cation, it is also possible to use a generalization

of the id88 algorithm for solving the erm problem. see exercise 2.

in the nonrealizable case, solving the erm problem is in general computa-
tionally hard. we tackle this di   culty using the method of convex surrogate

17.2 linear multiclass predictors

233

id168s (see section 12.3). in particular, we generalize the hinge loss to
multiclass problems.

17.2.4

generalized hinge loss
recall that in binary classi   cation, the hinge loss is de   ned to be max{0, 1    
y(cid:104)w, x(cid:105)}. we now generalize the hinge loss to multiclass predictors of the form

hw(x) = argmax

y(cid:48)   y

(cid:104)w,   (x, y(cid:48))(cid:105).

recall that a surrogate convex loss should upper bound the original nonconvex
loss, which in our case is    (hw(x), y). to derive an upper bound on    (hw(x), y)
we    rst note that the de   nition of hw(x) implies that

(cid:104)w,   (x, y)(cid:105)     (cid:104)w,   (x, hw(x))(cid:105).

therefore,

   (hw(x), y)        (hw(x), y) + (cid:104)w,   (x, hw(x))       (x, y)(cid:105).

since hw(x)     y we can upper bound the right-hand side of the preceding by

y(cid:48)   y (   (y(cid:48), y) + (cid:104)w,   (x, y(cid:48))       (x, y)(cid:105))

max

def= (cid:96)(w, (x, y)).

(17.3)

we use the term    generalized hinge loss    to denote the preceding expression. as
we have shown, (cid:96)(w, (x, y))        (hw(x), y). furthermore, equality holds when-
ever the score of the correct label is larger than the score of any other label, y(cid:48),
by at least    (y(cid:48), y), namely,

   y(cid:48)     y \ {y},

(cid:104)w,   (x, y)(cid:105)     (cid:104)w,   (x, y(cid:48))(cid:105) +    (y(cid:48), y).

it is also immediate to see that (cid:96)(w, (x, y)) is a convex function with respect to w
since it is a maximum over linear functions of w (see claim 12.5 in chapter 12),
and that (cid:96)(w, (x, y)) is   -lipschitz with    = maxy(cid:48)   y (cid:107)  (x, y(cid:48))       (x, y)(cid:107).
remark 17.2 we use the name    generalized hinge loss    since in the binary
case, when y = {  1}, if we set   (x, y) = yx
2 , then the generalized hinge loss
becomes the vanilla hinge loss for binary classi   cation,

(cid:96)(w, (x, y)) = max{0, 1     y(cid:104)w, x(cid:105)}.

geometric intuition:
the feature function    : x    y     rd maps each x into |y| vectors in rd.
the value of (cid:96)(w, (x, y)) will be zero if there exists a direction w such that
when projecting the |y| vectors onto this direction we obtain that each vector is
represented by the scalar (cid:104)w,   (x, y)(cid:105), and we can rank the di   erent points on
the basis of these scalars so that
    the point corresponding to the correct y is top-ranked

234

multiclass, ranking, and complex prediction problems

    for each y(cid:48) (cid:54)= y, the di   erence between (cid:104)w,   (x, y)(cid:105) and (cid:104)w,   (x, y(cid:48))(cid:105) is larger
than the loss of predicting y(cid:48) instead of y. the di   erence (cid:104)w,   (x, y)(cid:105)    
(cid:104)w,   (x, y(cid:48))(cid:105) is also referred to as the    margin    (see section 15.1).

this is illustrated in the following    gure:

w

  (x, y)

(cid:48)(cid:48) )

       (y,y

   

  (x, y(cid:48)(cid:48))

   

(
y,

y(cid:48)
)

  (x, y(cid:48))

17.2.5

multiclass id166 and sgd

once we have de   ned the generalized hinge loss, we obtain a convex-lipschitz
learning problem and we can apply our general techniques for solving such prob-
lems. in particular, the rlm technique we have studied in chapter 13 yields the
multiclass id166 rule:

multiclass id166

input: (x1, y1), . . . , (xm, ym)
parameters:
id173 parameter    > 0
id168     : y    y     r+
class-sensitive feature mapping    : x    y     rd
solve:

(cid:32)

min
w   rd

  (cid:107)w(cid:107)2 +

1
m

(cid:33)
y(cid:48)   y (   (y(cid:48), yi) + (cid:104)w,   (xi, y(cid:48))       (xi, yi)(cid:105))

max

m(cid:88)

i=1

output the predictor hw(x) = argmaxy   y(cid:104)w,   (x, y)(cid:105)

we can solve the optimization problem associated with multiclass id166 us-
ing generic id76 algorithms (or using the method described in
section 15.5). let us analyze the risk of the resulting hypothesis. the analysis
seaid113ssly follows from our general analysis for convex-lipschitz problems given
in chapter 13. in particular, applying corollary 13.8 and using the fact that the
generalized hinge loss upper bounds the     loss, we immediately obtain an analog
of corollary 15.7:
corollary 17.1 let d be a distribution over x    y, let    : x    y     rd,
and assume that for all x     x and y     y we have (cid:107)  (x, y)(cid:107)       /2. let b > 0.

17.2 linear multiclass predictors

235

(cid:113) 2  2
b2m on a training set s     dm

e

consider running multiclass id166 with    =
and let hw be the output of multiclass id166. then,
(w)]     min
s   dm
u:(cid:107)u(cid:107)   b
where l   d(h) = e(x,y)   d[   (h(x), y)] and lg   hinge
with (cid:96) being the generalized hinge-loss as de   ned in equation (17.3).

[l   d(hw)]    

lg   hinge
d

[lg   hinge

s   dm

(u) +

e

d

d

(cid:114)

(w) = e(x,y)   d[(cid:96)(w, (x, y))]

8  2b2

m

,

we can also apply the sgd learning framework for minimizing lg   hinge

(w) as
described in chapter 14. recall claim 14.6, which dealt with subgradients of max
functions. in light of this claim, in order to    nd a subgradient of the generalized
hinge loss all we need to do is to    nd y     y that achieves the maximum in the
de   nition of the generalized hinge loss. this yields the following algorithm:

d

sgd for multiclass learning

parameters:
scalar    > 0, integer t > 0
id168     : y    y     r+
class-sensitive feature mapping    : x    y     rd
initialize: w(1) = 0     rd
for t = 1, 2, . . . , t
sample (x, y)     d
set vt =   (x,   y)       (x, y)
update w(t+1) = w(t)       vt
output   w = 1
t

   nd   y     argmaxy(cid:48)   y (cid:0)   (y(cid:48), y) + (cid:104)w(t),   (x, y(cid:48))       (x, y)(cid:105)(cid:1)

(cid:80)t

t=1 w(t)

our general analysis of sgd given in corollary 14.12 immediately implies:

corollary 17.2 let d be a distribution over x    y, let    : x    y     rd,
and assume that for all x     x and y     y we have (cid:107)  (x, y)(cid:107)       /2. let b > 0.
then, for every   > 0, if we run sgd for multiclass learning with a number of
iterations (i.e., number of examples)

(cid:113) b2

t     b2  2
 2

and with    =

  2 t , then the output of sgd satis   es
(   w)]     min
u:(cid:107)u(cid:107)   b

[lg   hinge

s   dm

[l   d(h   w)]    

e

d

e

s   dm

lg   hinge
d

(u) +  .

remark 17.3
it is interesting to note that the risk bounds given in corol-
lary 17.1 and corollary 17.2 do not depend explicitly on the size of the label
set y, a fact we will rely on in the next section. however, the bounds may de-
pend implicitly on the size of y via the norm of   (x, y) and the fact that the
bounds are meaningful only when there exists some vector u, (cid:107)u(cid:107)     b, for which
lg   hinge
d

(u) is not excessively large.

236

multiclass, ranking, and complex prediction problems

17.3

structured output prediction
structured output prediction problems are multiclass problems in which y is
very large but is endowed with a prede   ned structure. the structure plays a
key role in constructing e   cient algorithms. to motivate structured learning
problems, consider the problem of id42 (ocr). suppose
we receive an image of some handwritten word and would like to predict which
word is written in the image. to simplify the setting, suppose we know how to
segment the image into a sequence of images, each of which contains a patch of
the image corresponding to a single letter. therefore, x is the set of sequences
of images and y is the set of sequences of letters. note that the size of y grows
exponentially with the maximal length of a word. an example of an image x
corresponding to the label y =    workable    is given in the following.

to tackle structure prediction we can rely on the family of linear predictors
described in the previous section. in particular, we need to de   ne a reasonable
id168 for the problem,    , as well as a good class-sensitive feature mapping,
  . by    good    we mean a feature mapping that will lead to a low approximation
error for the class of linear predictors with respect to    and    . once we do this,
we can rely, for example, on the sgd learning algorithm de   ned in the previous
section.

however, the huge size of y poses several challenges:

1. to apply the multiclass prediction we need to solve a maximization problem

over y. how can we predict e   ciently when y is so large?

2. how do we train w e   ciently? in particular, to apply the sgd rule we again

need to solve a maximization problem over y.

3. how can we avoid over   tting?

in the previous section we have already shown that the sample complexity of
learning a linear multiclass predictor does not depend explicitly on the number
of classes. we just need to make sure that the norm of the range of    is not too
large. this will take care of the over   tting problem. to tackle the computational
challenges we rely on the structure of the problem, and de   ne the functions    and
    so that calculating the maximization problems in the de   nition of hw and in
the sgd algorithm can be performed e   ciently. in the following we demonstrate
one way to achieve these goals for the ocr task mentioned previously.

to simplify the presentation, let us assume that all the words in y are of length
r and that the number of di   erent letters in our alphabet is q. let y and y(cid:48) be two

r(cid:88)

t=1

r(cid:88)

t=2

17.3 structured output prediction

237

words (i.e., sequences of letters) in y. we de   ne the function    (y(cid:48), y) to be the
average number of letters that are di   erent in y(cid:48) and y, namely, 1
1[yi(cid:54)=y(cid:48)
i].
next, let us de   ne a class-sensitive feature mapping   (x, y). it will be conve-
nient to think about x as a matrix of size n    r, where n is the number of pixels
in each image, and r is the number of images in the sequence. the j   th column
of x corresponds to the j   th image in the sequence (encoded as a vector of gray
level values of pixels). the dimension of the range of    is set to be d = n q + q2.

i=1

r

the    rst nq feature functions are    type 1    features and take the form:

(cid:80)r

  i,j,1(x, y) =

1
r

xi,t 1[yt=j].

that is, we sum the value of the i   th pixel only over the images for which y
assigns the letter j. the triple index (i, j, 1) indicates that we are dealing with
feature (i, j) of type 1. intuitively, such features can capture pixels in the image
whose gray level values are indicative of a certain letter. the second type of
features take the form

  i,j,2(x, y) =

1
r

1[yt=i] 1[yt   1=j].

that is, we sum the number of times the letter i follows the letter j. intuitively,
these features can capture rules like    it is likely to see the pair    qu    in a word   
or    it is unlikely to see the pair    rz    in a word.    of course, some of these features
will not be very useful, so the goal of the learning process is to assign weights to
features by learning the vector w, so that the weighted score will give us a good
prediction via

hw(x) = argmax

y   y

(cid:104)w,   (x, y)(cid:105).

it is left to show how to solve the optimization problem in the de   nition
of hw(x) e   ciently, as well as how to solve the optimization problem in the
de   nition of   y in the sgd algorithm. we can do this by applying a dynamic
programming procedure. we describe the procedure for solving the maximization
in the de   nition of hw and leave as an exercise the maximization problem in the
de   nition of   y in the sgd algorithm.

to derive the id145 procedure, let us    rst observe that we

can write

  (x, y) =

r(cid:88)

  (x, yt, yt   1),

t=1

for an appropriate    : x    [q]    [q]     {0}     rd, and for simplicity we assume
that y0 is always equal to 0. indeed, each feature function   i,j,1 can be written
in terms of

  i,j,1(x, yt, yt   1) = xi,t 1[yt=j],

238

multiclass, ranking, and complex prediction problems

while the feature function   i,j,2 can be written in terms of

  i,j,2(x, yt, yt   1) = 1[yt=i] 1[yt   1=j].

therefore, the prediction can be written as

r(cid:88)

t=1

  (cid:88)

t=1

hw(x) = argmax

y   y

(cid:104)w,   (x, yt, yt   1)(cid:105).

(17.4)

in the following we derive a id145 procedure that solves every
problem of the form given in equation (17.4). the procedure will maintain a
matrix m     rq,r such that

ms,   =

max

(y1,...,y   ):y   =s

(cid:104)w,   (x, yt, yt   1)(cid:105).

clearly, the maximum of (cid:104)w,   (x, y)(cid:105) equals maxs ms,r. furthermore, we can
calculate m in a recursive manner:

ms,   = max

s(cid:48)

(ms(cid:48),     1 + (cid:104)w,   (x, s, s(cid:48))(cid:105)) .

(17.5)

this yields the following procedure:

id145 for calculating hw(x) as given

in equation (17.4)

input: a matrix x     rn,r and a vector w
initialize:
foreach s     [q]
ms,1 = (cid:104)w,   (x, s,   1)(cid:105)
for    = 2, . . . , r
foreach s     [q]
set ms,   as in equation (17.5)
set is,   to be the s(cid:48) that maximizes equation (17.5)

set yt = argmaxs ms,r
for    = r, r     1, . . . , 2
set y     1 = iy   ,  

output: y = (y1, . . . , yr)

17.4

ranking

ranking is the problem of ordering a set of instances according to their    rele-
vance.    a typical application is ordering results of a search engine according to
their relevance to the query. another example is a system that monitors elec-
tronic transactions and should alert for possible fraudulent transactions. such a
system should order transactions according to how suspicious they are.
n=1 x n be the set of all sequences of instances from

formally, let x     = (cid:83)   

17.4 ranking

239

x of arbitrary length. a ranking hypothesis, h, is a function that receives a
sequence of instances   x = (x1, . . . , xr)     x    , and returns a permutation of [r].
it is more convenient to let the output of h be a vector y     rr, where by
sorting the elements of y we obtain the permutation over [r]. we denote by
  (y) the permutation over [r] induced by y. for example, for r = 5, the vector
y = (2, 1, 6,   1, 0.5) induces the permutation   (y) = (4, 3, 5, 1, 2). that is,
if we sort y in an ascending order, then we obtain the vector (   1, 0.5, 1, 2, 6).
now,   (y)i is the position of yi in the sorted vector (   1, 0.5, 1, 2, 6). this
notation re   ects that the top-ranked instances are those that achieve the highest
(cid:83)   
values in   (y).
in the notation of our pac learning model, the examples domain is z =
r=1(x r    rr), and the hypothesis class, h, is some set of ranking hypotheses.
we de   ne (cid:96)(h, (  x, y)) =    (h(  x), y), for some function     :(cid:83)   
we next turn to describe id168s for ranking. there are many possible ways
to de   ne such id168s, and here we list a few examples. in all the examples
r=1(rr    rr)     r+.
    0   1 ranking loss:    (y(cid:48), y) is zero if y and y(cid:48) induce exactly the same
ranking and    (y(cid:48), y) = 1 otherwise. that is,    (y(cid:48), y) = 1[  (y(cid:48))(cid:54)=  (y)]. such
a id168 is almost never used in practice as it does not distinguish
between the case in which   (y(cid:48)) is almost equal to   (y) and the case in
which   (y(cid:48)) is completely di   erent from   (y).
    kendall-tau loss: we count the number of pairs (i, j) that are in di   erent

order in the two permutations. this can be written as

   (y(cid:48), y) =

2

r(r     1)

1[sign(y(cid:48)

i   y(cid:48)

j )(cid:54)=sign(yi   yj )].

r   1(cid:88)

r(cid:88)

i=1

j=i+1

this id168 is more useful than the 0   1 loss as it re   ects the level of
similarity between the two rankings.
    normalized discounted cumulative gain (ndcg): this measure em-
phasizes the correctness at the top of the list by using a monotonically
nondecreasing discount function d : n     r+. we    rst de   ne a discounted
cumulative gain measure:

g(y(cid:48), y) =

d(  (y(cid:48))i) yi.

r(cid:88)

i=1

in words, if we interpret yi as a score of the    true relevance    of item i, then
we take a weighted sum of the relevance of the elements, while the weight
of yi is determined on the basis of the position of i in   (y(cid:48)). assuming that
all elements of y are nonnegative, it is easy to verify that 0     g(y(cid:48), y)    
g(y, y). we can therefore de   ne a normalized discounted cumulative gain
by the ratio g(y(cid:48), y)/g(y, y), and the corresponding id168 would
be

   (y(cid:48), y) = 1     g(y(cid:48), y)

=

1

g(y, y)

g(y, y)

i=1

(d(  (y)i)     d(  (y(cid:48))i)) yi.

r(cid:88)

240

multiclass, ranking, and complex prediction problems

we can easily see that    (y(cid:48), y)     [0, 1] and that    (y(cid:48), y) = 0 whenever
  (y(cid:48)) =   (y).

a typical way to de   ne the discount function is by

(cid:40)

d(i) =

1

log2(r   i+2)
0

if i     {r     k + 1, . . . , r}
otherwise

where k < r is a parameter. this means that we care more about elements
that are ranked higher, and we completely ignore elements that are not at
the top-k ranked elements. the ndcg measure is often used to evaluate
the performance of search engines since in such applications it makes sense
completely to ignore elements that are not at the top of the ranking.

once we have a hypothesis class and a ranking id168, we can learn a
ranking function using the erm rule. however, from the computational point of
view, the resulting optimization problem might be hard to solve. we next discuss
how to learn linear predictors for ranking.

17.4.1

linear predictors for ranking

a natural way to de   ne a ranking function is by projecting the instances onto
some vector w and then outputting the resulting scalars as our representation
of the ranking function. that is, assuming that x     rd, for every w     rd we
de   ne a ranking function

hw((x1, . . . , xr)) = ((cid:104)w, x1(cid:105), . . . ,(cid:104)w, xr(cid:105)).

(17.6)

as we discussed in chapter 16, we can also apply a feature mapping that maps
instances into some feature space and then takes the inner products with w in the
feature space. for simplicity, we focus on the simpler form as in equation (17.6).
given some w     rd, we can now de   ne the hypothesis class hw = {hw :
w     w}. once we have de   ned this hypothesis class, and have chosen a ranking
id168, we can apply the erm rule as follows: given a training set, s =
(  x1, y1), . . . , (  xm, ym), where each (  xi, yi) is in (x    r)ri, for some ri     n, we
i=1    (hw(  xi), yi).
as in the case of binary classi   cation, for many id168s this problem is
computationally hard, and we therefore turn to describe convex surrogate loss
functions. we describe the surrogates for the kendall tau loss and for the ndcg
loss.

should search w     w that minimizes the empirical loss, (cid:80)m

a hinge loss for the kendall tau id168:
we can think of the kendall tau loss as an average of 0   1 losses for each pair.
in particular, for every (i, j) we can rewrite

1[sign(y(cid:48)

i   y(cid:48)

j )(cid:54)=sign(yi   yj )] = 1[sign(yi   yj )(y(cid:48)

i   y(cid:48)

j )   0].

17.4 ranking

241

i    y(cid:48)
in our case, y(cid:48)
bound as follows:

j = (cid:104)w, xi    xj(cid:105). it follows that we can use the hinge loss upper

1[sign(yi   yj )(y(cid:48)

i   y(cid:48)

j )   0]     max{0, 1     sign (yi     yj)(cid:104)w, xi     xj(cid:105)} .

taking the average over the pairs we obtain the following surrogate convex loss
for the kendall tau id168:

   (hw(  x), y)    

2

r(r     1)

max{0, 1     sign(yi     yj)(cid:104)w, xi     xj(cid:105)} .

r   1(cid:88)

r(cid:88)

i=1

j=i+1

the right-hand side is convex with respect to w and upper bounds the kendall
tau loss. it is also a   -lipschitz function with parameter        maxi,j (cid:107)xi     xj(cid:107).

a hinge loss for the ndcg id168:
the ndcg id168 depends on the predicted ranking vector y(cid:48)     rr via
the permutation it induces. to derive a surrogate id168 we    rst make
the following observation. let v be the set of all permutations of [r] encoded as
vectors; namely, each v     v is a vector in [r]r such that for all i (cid:54)= j we have
vi (cid:54)= vj. then (see exercise 4),

let us denote   (  x, v) =(cid:80)r

  (y(cid:48)) = argmax

v   v

vi y(cid:48)
i.

(17.7)

i=1

r(cid:88)
r(cid:88)
(cid:42)

i=1

i=1 vixi; it follows that

  (hw(  x)) = argmax

v   v

vi(cid:104)w, xi(cid:105)

r(cid:88)

(cid:43)

= argmax

v   v

= argmax

v   v

w,

vixi
(cid:104)w,   (  x, v)(cid:105).

i=1

on the basis of this observation, we can use the generalized hinge loss for cost-
sensitive multiclass classi   cation as a surrogate id168 for the ndcg loss
as follows:

   (hw(  x), y)        (hw(  x), y) + (cid:104)w,   (  x,   (hw(  x)))(cid:105)     (cid:104)w,   (  x,   (y))(cid:105)

[   (v, y) + (cid:104)w,   (  x, v)(cid:105)     (cid:104)w,   (  x,   (y))(cid:105)]

    max
v   v

(cid:34)

r(cid:88)

i=1

= max
v   v

   (v, y) +

(vi       (y)i)(cid:104)w, xi(cid:105)

.

(17.8)

(cid:35)

the right-hand side is a convex function with respect to w.

we can now solve the learning problem using sgd as described in section 17.2.5.

the main computational bottleneck is calculating a subgradient of the loss func-
tion, which is equivalent to    nding v that achieves the maximum in equa-
tion (17.8) (see claim 14.6). using the de   nition of the ndcg loss, this is

242

multiclass, ranking, and complex prediction problems

equivalent to solving the problem

argmin

v   v

(  ivi +   i d(vi)),

where   i =    (cid:104)w, xi(cid:105) and   i = yi/g(y, y). we can think of this problem a little
bit di   erently by de   ning a matrix a     rr,r where

ai,j = j  i + d(j)   i.

now, let us think about each j as a    worker,    each i as a    task,    and ai,j as
the cost of assigning task i to worker j. with this view, the problem of    nding
v becomes the problem of    nding an assignment of the tasks to workers of
minimal cost. this problem is called    the assignment problem    and can be solved
e   ciently. one particular algorithm is the    hungarian method    (kuhn 1955).
another way to solve the assignment problem is using id135. to
do so, let us    rst write the assignment problem as

r(cid:88)

i=1

r(cid:88)

argmin
b   rr,r

+

i,j=1

ai,jbi,j

s.t.    i     [r],

(17.9)

bi,j = 1

r(cid:88)
r(cid:88)

j=1

   j     [r],
   i, j, bi,j     {0, 1}

i=1

bi,j = 1

a matrix b that satis   es the constraints in the preceding optimization problem
is called a permutation matrix. this is because the constraints guarantee that
there is at most a single entry of each row that equals 1 and a single entry of each
column that equals 1. therefore, the matrix b corresponds to the permutation
v     v de   ned by vi = j for the single index j that satis   es bi,j = 1.
the preceding optimization is still not a linear program because of the com-
binatorial constraint bi,j     {0, 1}. however, as it turns out, this constraint is
redundant     if we solve the optimization problem while simply omitting the
combinatorial constraint, then we are still guaranteed that there is an optimal
solution that will satisfy this constraint. this is formalized later.

denote (cid:104)a, b(cid:105) =(cid:80)

mizing (cid:104)a, b(cid:105) such that b is a permutation matrix.

i,j ai,jbi,j. then, equation (17.9) is the problem of mini-
a matrix b     rr,r is called doubly stochastic if all elements of b are non-
negative, the sum of each row of b is 1, and the sum of each column of b is 1.
therefore, solving equation (17.9) without the constraints bi,j     {0, 1} is the
problem

(cid:104)a, b(cid:105) s.t. b is a doubly stochastic matrix.

argmin
b   rr,r

(17.10)

17.5 bipartite ranking and multivariate performance measures

243

the following claim states that every doubly stochastic matrix is a convex

combination of permutation matrices.

claim 17.3 ((birkho    1946, von neumann 1953)) the set of doubly stochastic
matrices in rr,r is the convex hull of the set of permutation matrices in rr,r.

on the basis of the claim, we easily obtain the following:

lemma 17.4 there exists an optimal solution of equation (17.10) that is also
an optimal solution of equation (17.9).

write b = (cid:80)
(cid:80)

proof let b be a solution of equation (17.10). then, by claim 17.3, we can
i   ici, where each ci is a permutation matrix, each   i > 0, and
i   i = 1. since all the ci are also doubly stochastic, we clearly have that
(cid:104)a, b(cid:105)     (cid:104)a, ci(cid:105) for every i. we claim that there is some i for which (cid:104)a, b(cid:105) =
(cid:104)a, ci(cid:105). this must be true since otherwise, if for every i (cid:104)a, b(cid:105) < (cid:104)a, ci(cid:105), we
would have that
(cid:104)a, b(cid:105) =

  i(cid:104)a, b(cid:105) = (cid:104)a, b(cid:105),

  i(cid:104)a, ci(cid:105) >

(cid:88)

(cid:88)

(cid:88)

(cid:42)

(cid:43)

  ici

a,

=

i

i

i

which cannot hold. we have thus shown that some permutation matrix, ci,
satis   es (cid:104)a, b(cid:105) = (cid:104)a, ci(cid:105). but, since for every other permutation matrix c we
have (cid:104)a, b(cid:105)     (cid:104)a, c(cid:105) we conclude that ci is an optimal solution of both equa-
tion (17.9) and equation (17.10).

17.5

bipartite ranking and multivariate performance measures

in the previous section we described the problem of ranking. we used a vector
y     rr for representing an order over the elements x1, . . . , xr. if all elements in y
are di   erent from each other, then y speci   es a full order over [r]. however, if two
elements of y attain the same value, yi = yj for i (cid:54)= j, then y can only specify a
partial order over [r]. in such a case, we say that xi and xj are of equal relevance
according to y. in the extreme case, y     {  1}r, which means that each xi is
either relevant or nonrelevant. this setting is often called    bipartite ranking.    for
example, in the fraud detection application mentioned in the previous section,
each transaction is labeled as either fraudulent (yi = 1) or benign (yi =    1).

seemingly, we can solve the bipartite ranking problem by learning a binary
classi   er, applying it on each instance, and putting the positive ones at the top
of the ranked list. however, this may lead to poor results as the goal of a binary
learner is usually to minimize the zero-one loss (or some surrogate of it), while the
goal of a ranker might be signi   cantly di   erent. to illustrate this, consider again
the problem of fraud detection. usually, most of the transactions are benign (say
99.9%). therefore, a binary classi   er that predicts    benign    on all transactions
will have a zero-one error of 0.1%. while this is a very small number, the resulting
predictor is meaningless for the fraud detection application. the crux of the

244

multiclass, ranking, and complex prediction problems

problem stems from the inadequacy of the zero-one loss for what we are really
interested in. a more adequate performance measure should take into account
the predictions over the entire set of instances. for example, in the previous
section we have de   ned the ndcg loss, which emphasizes the correctness of the
top-ranked items. in this section we describe additional id168s that are
speci   cally adequate for bipartite ranking problems.

as in the previous section, we are given a sequence of instances,   x = (x1, . . . , xr),
and we predict a ranking vector y(cid:48)     rr. the feedback vector is y     {  1}r. we
de   ne a loss that depends on y(cid:48) and y and depends on a threshold        r. this
threshold transforms the vector y(cid:48)     rr into the vector (sign(y(cid:48)
r   
  ))     {  1}r. usually, the value of    is set to be 0. however, as we will see, we
sometimes set    while taking into account additional constraints on the problem.
the id168s we de   ne in the following depend on the following 4 num-

i     ), . . . , sign(y(cid:48)

bers:

true positives: a = |{i : yi = +1     sign(y(cid:48)
false positives: b = |{i : yi =    1     sign(y(cid:48)
false negatives: c = |{i : yi = +1     sign(y(cid:48)
true negatives: d = |{i : yi =    1     sign(y(cid:48)

i       ) = +1}|
i       ) = +1}|
i       ) =    1}|
i       ) =    1}|

(17.11)

the recall (a.k.a. sensitivity) of a prediction vector is the fraction of true
a
a+c . the precision is the fraction of correct
a
a+b . the speci   city

positives y(cid:48)    catches,    namely,
predictions among the positive labels we predict, namely,
is the fraction of true negatives that our predictor    catches,    namely,

d
d+b .

(cid:17)

note that as we decrease    the recall increases (attaining the value 1 when
   =       ). on the other hand, the precision and the speci   city usually decrease
as we decrease   . therefore, there is a tradeo    between precision and recall, and
we can control it by changing   . the id168s de   ned in the following use
various techniques for combining both the precision and recall.
    averaging sensitivity and speci   city: this measure is the average of the
sensitivity and speci   city, namely, 1
. this is also the accuracy
2
on positive examples averaged with the accuracy on negative examples.
here, we set    = 0 and the corresponding id168 is    (y(cid:48), y) =
1     1

(cid:16) a

(cid:16) a

a+c + d

    f1-score: the f1 score is the harmonic mean of the precision and recall:
. its maximal value (of 1) is obtained when both precision
precision + 1
and recall are 1, and its minimal value (of 0) is obtained whenever one of
them is 0 (even if the other one is 1). the f1 score can be written using
the numbers a, b, c as follows; f1 = 2a
2a+b+c . again, we set    = 0, and the
id168 becomes    (y(cid:48), y) = 1     f1.
    f  -score: it is like f1 score, but we attach   2 times more importance to
. it can also be written as

recall than to precision, that is,

a+c + d

(cid:17)

d+b

d+b

recall

2

2

.

1

1+  2
precision +  2

1

1

recall

17.5 bipartite ranking and multivariate performance measures

245

(1+  2)a

f   =
   (y(cid:48), y) = 1     f  .

(1+  2)a+b+  2c . again, we set    = 0, and the id168 becomes
    recall at k: we measure the recall while the prediction must contain at most
k positive labels. that is, we should set    so that a + b     k. this is conve-
nient, for example, in the application of a fraud detection system, where a
bank employee can only handle a small number of suspicious transactions.
    precision at k: we measure the precision while the prediction must contain

at least k positive labels. that is, we should set    so that a + b     k.

the measures de   ned previously are often referred to as multivariate perfor-
mance measures. note that these measures are highly di   erent from the average
a+b+c+d . in the aforemen-
zero-one loss, which in the preceding notation equals
tioned example of fraud detection, when 99.9% of the examples are negatively
labeled, the zero-one loss of predicting that all the examples are negatives is
0.1%. in contrast, the recall of such prediction is 0 and hence the f1 score is also
0, which means that the corresponding loss will be 1.

b+d

17.5.1

linear predictors for bipartite ranking

we next describe how to train linear predictors for bipartite ranking. as in the
previous section, a linear predictor for ranking is de   ned to be

hw(  x) = ((cid:104)w, x1(cid:105), . . . ,(cid:104)w, xr(cid:105)).

the corresponding id168 is one of the multivariate performance measures
described before. the id168 depends on y(cid:48) = hw(  x) via the binary vector
it induces, which we denote by
b(y(cid:48)) = (sign(y(cid:48)

1       ), . . . , sign(y(cid:48)

r       ))     {  1}r.

(17.12)

as in the previous section, to facilitate an e   cient algorithm we derive a convex
surrogate id168 on    . the derivation is similar to the derivation of the
generalized hinge loss for the ndcg ranking loss, as described in the previous
section.

our    rst observation is that for all the values of    de   ned before, there is some

v     {  1}r such that b(y(cid:48)) can be rewritten as

r(cid:88)

i=1

b(y(cid:48)) = argmax

v   v

viy(cid:48)
i.

(17.13)

this is clearly true for the case    = 0 if we choose v = {  1}r. the two measures
for which    is not taken to be 0 are precision at k and recall at k. for precision
at k we can take v to be the set v   k, containing all vectors in {  1}r whose
number of ones is at least k. for recall at k, we can take v to be v   k, which is
de   ned analogously. see exercise 5.

246

multiclass, ranking, and complex prediction problems

once we have de   ned b as in equation (17.13), we can easily derive a convex

surrogate loss as follows. assuming that y     v , we have that

   (hw(  x), y) =    (b(hw(  x)), y)

       (b(hw(  x)), y) +

(bi(hw(  x))     yi)(cid:104)w, xi(cid:105)

(cid:34)

(cid:35)

    max
v   v

   (v, y) +

(vi     yi)(cid:104)w, xi(cid:105)

.

(17.14)

r(cid:88)
r(cid:88)

i=1

i=1

the right-hand side is a convex function with respect to w.

we can now solve the learning problem using sgd as described in section 17.2.5.

the main computational bottleneck is calculating a subgradient of the loss func-
tion, which is equivalent to    nding v that achieves the maximum in equa-
tion (17.14) (see claim 14.6).

in the following we describe how to    nd this maximizer e   ciently for any
performance measure that can be written as a function of the numbers a, b, c, d
given in equation (17.11), and for which the set v contains all elements in {  1}r
for which the values of a, b satisfy some constraints. for example, for    recall at
k    the set v is all vectors for which a + b     k.
the idea is as follows. for any a, b     [r], let

  ya,b = {v :

|{i : vi = 1     yi = 1}| = a     |{i : vi = 1     yi =    1}| = b} .

any vector v     v falls into   ya,b for some a, b     [r]. furthermore, if   ya,b     v
is not empty for some a, b     [r] then   ya,b     v =   ya,b. therefore, we can search
within each   ya,b that has a nonempty intersection with v separately, and then
take the optimal value. the key observation is that once we are searching only
within   ya,b, the value of     is    xed so we only need to maximize the expression

r(cid:88)

i=1

max
v      ya,b

vi(cid:104)w, xi(cid:105).

suppose the examples are sorted so that (cid:104)w, x1(cid:105)                (cid:104)w, xr(cid:105). then, it is
easy to verify that we would like to set vi to be positive for the smallest indices
i. doing this, with the constraint on a, b, amounts to setting vi = 1 for the a
top ranked positive examples and for the b top-ranked negative examples. this
yields the following procedure.

17.6 summary

247

solving equation (17.14)

input:

(x1, . . . , xr), (y1, . . . , yr), w, v,    

assumptions:

    is a function of a, b, c, d
v contains all vectors for which f (a, b) = 1 for some function f
initialize:
p = |{i : yi = 1}|, n = |{i : yi =    1}|
   = ((cid:104)w, x1(cid:105), . . . ,(cid:104)w, xr(cid:105)),   (cid:63) =       
sort examples so that   1       2                  r
let i1, . . . , ip be the (sorted) indices of the positive examples
let j1, . . . , jn be the (sorted) indices of the negative examples
for a = 0, 1, . . . , p
c = p     a
for b = 0, 1, . . . , n such that f (a, b) = 1
d = n     b
calculate     using a, b, c, d
set v1, . . . , vr s.t. vi1 =        = via = vj1 =        = vjb = 1
and the rest of the elements of v equal    1
if          (cid:63)
  (cid:63) =   , v(cid:63) = v

set    =     +(cid:80)r

i=1 vi  i

output v(cid:63)

17.6

summary

many real world supervised learning problems can be cast as learning a multiclass
predictor. we started the chapter by introducing reductions of multiclass learning
to binary learning. we then described and analyzed the family of linear predictors
for multiclass learning. we have shown how this family can be used even if the
number of classes is extremely large, as long as we have an adequate structure
on the problem. finally, we have described ranking problems. in chapter 29 we
study the sample complexity of multiclass learning in more detail.

17.7

bibliographic remarks

the one-versus-all and all-pairs approach reductions have been uni   ed un-
der the framework of error correction output codes (ecoc) (dietterich &
bakiri 1995, allwein, schapire & singer 2000). there are also other types of re-
ductions such as tree-based classi   ers (see, for example, beygelzimer, langford
& ravikumar (2007)). the limitations of reduction techniques have been studied

248

multiclass, ranking, and complex prediction problems

in (daniely et al. 2011, daniely, sabato & shwartz 2012). see also chapter 29,
in which we analyze the sample complexity of multiclass learning.

direct approaches to multiclass learning with linear predictors have been stud-
ied in (vapnik 1998, weston & watkins 1999, crammer & singer 2001). in par-
ticular, the multivector construction is due to crammer & singer (2001).

collins (2000) has shown how to apply the id88 algorithm for structured
output problems. see also collins (2002). a related approach is discriminative
learning of conditional random    elds; see la   erty, mccallum & pereira (2001).
structured output id166 has been studied in (weston, chapelle, vapnik, elissee   
& sch  olkopf 2002, taskar, guestrin & koller 2003, tsochantaridis, hofmann,
joachims & altun 2004).

the dynamic procedure we have presented for calculating the prediction hw(x)
in the structured output section is similar to the forward-backward variables
calculated by the viterbi procedure in id48s (see, for instance, (rabiner &
juang 1986)). more generally, solving the maximization problem in structured
output is closely related to the problem of id136 in id114 (see, for
example, koller & friedman (2009)).

chapelle, le & smola (2007) proposed to learn a ranking function with respect
to the ndcg loss using ideas from structured output learning. they also ob-
served that the maximization problem in the de   nition of the generalized hinge
loss is equivalent to the assignment problem.

agarwal & roth (2005) analyzed the sample complexity of bipartite ranking.
joachims (2005) studied the applicability of structured output id166 to bipartite
ranking with multivariate performance measures.

17.8

exercises

1. consider a set s of examples in rn  [k] for which there exist vectors   1, . . . ,   k
such that every example (x, y)     s falls within a ball centered at   y whose
radius is r     1. assume also that for every i (cid:54)= j, (cid:107)  i       j(cid:107)     4r. con-
sider concatenating each instance by the constant 1 and then applying the
multivector construction, namely,

  (x, y) = [

(cid:124) (cid:123)(cid:122) (cid:125)

0, . . . , 0

   r(y   1)(n+1)

(cid:124)

(cid:123)(cid:122)

   rn+1

, x1, . . . , xn, 1

,

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

0, . . . , 0

   r(k   y)(n+1)

].

show that there exists a vector w     rk(n+1) such that (cid:96)(w, (x, y)) = 0 for
every (x, y)     s.
hint: observe that for every example (x, y)     s we can write x =   y + v for
some (cid:107)v(cid:107)     r. now, take w = [w1, . . . , wk], where wi = [  i ,    (cid:107)  i(cid:107)2/2].

2. multiclass id88: consider the following algorithm:

17.8 exercises

249

multiclass batch id88

input:
a training set (x1, y1), . . . , (xm, ym)
a class-sensitive feature mapping    : x    y     rd
initialize: w(1) = (0, . . . , 0)     rd
for t = 1, 2, . . .
if (    i and y (cid:54)= yi s.t. (cid:104)w(t),   (xi, yi)(cid:105)     (cid:104)w(t),   (xi, y)(cid:105)) then
w(t+1) = w(t) +   (xi, yi)       (xi, y)
else

output w(t)

prove the following:

theorem 17.5 assume that there exists w(cid:63) such that for all i and for all
y (cid:54)= yi it holds that (cid:104)w(cid:63),   (xi, yi)(cid:105)     (cid:104)w(cid:63),   (xi, y)(cid:105)+1. let r = maxi,y (cid:107)  (xi, yi)   
  (xi, y)(cid:107). then, the multiclass id88 algorithm stops after at most (r(cid:107)w(cid:63)(cid:107))2
iterations, and when it stops it holds that    i     [m], yi = argmaxy (cid:104)w(t),   (xi, y)(cid:105).

dure for multiclass prediction. you can assume that    (y(cid:48), y) =(cid:80)r

3. generalize the id145 procedure given in section 17.3 for solv-
ing the maximization problem given in the de   nition of   h in the sgd proce-
t, yt)

t=1   (y(cid:48)

for some arbitrary function   .

4. prove that equation (17.7) holds.
5. show that the two de   nitions of    as de   ned in equation (17.12) and equa-
tion (17.13) are indeed equivalent for all the multivariate performance mea-
sures.

18 id90

a decision tree is a predictor, h : x     y, that predicts the label associated with
an instance x by traveling from a root node of a tree to a leaf. for simplicity
we focus on the binary classi   cation setting, namely, y = {0, 1}, but decision
trees can be applied for other prediction problems as well. at each node on the
root-to-leaf path, the successor child is chosen on the basis of a splitting of the
input space. usually, the splitting is based on one of the features of x or on a
prede   ned set of splitting rules. a leaf contains a speci   c label. an example of
a decision tree for the papayas example (described in chapter 2) is given in the
following:

color?

other

pale green to pale yellow

not-tasty

softness?

other

gives slightly to palm pressure

not-tasty

tasty

to check if a given papaya is tasty or not, the decision tree    rst examines
the color of the papaya. if this color is not in the range pale green to pale
yellow, then the tree immediately predicts that the papaya is not tasty without
additional tests. otherwise, the tree turns to examine the softness of the papaya.
if the softness level of the papaya is such that it gives slightly to palm pressure,
the decision tree predicts that the papaya is tasty. otherwise, the prediction is
   not-tasty.    the preceding example underscores one of the main advantages of
id90     the resulting classi   er is very simple to understand and interpret.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

18.1 sample complexity

251

18.1

sample complexity

a popular splitting rule at internal nodes of the tree is based on thresholding the
value of a single feature. that is, we move to the right or left child of the node on
the basis of 1[xi<  ], where i     [d] is the index of the relevant feature and        r
is the threshold. in such cases, we can think of a decision tree as a splitting of
the instance space, x = rd, into cells, where each leaf of the tree corresponds
to one cell. it follows that a tree with k leaves can shatter a set of k instances.
hence, if we allow id90 of arbitrary size, we obtain a hypothesis class
of in   nite vc dimension. such an approach can easily lead to over   tting.

to avoid over   tting, we can rely on the minimum description length (mdl)
principle described in chapter 7, and aim at learning a decision tree that on one
hand    ts the data well while on the other hand is not too large.

for simplicity, we will assume that x = {0, 1}d. in other words, each instance
is a vector of d bits. in that case, thresholding the value of a single feature
corresponds to a splitting rule of the form 1[xi=1] for some i = [d]. for instance,
we can model the    papaya decision tree    earlier by assuming that a papaya is
parameterized by a two-dimensional bit vector x     {0, 1}2, where the bit x1
represents whether the color is pale green to pale yellow or not, and the bit x2
represents whether the softness is gives slightly to palm pressure or not. with
this representation, the node color? can be replaced with 1[x1=1], and the node
softness? can be replaced with 1[x2=1]. while this is a big simpli   cation, the
algorithms and analysis we provide in the following can be extended to more
general cases.

with the aforementioned simplifying assumption, the hypothesis class becomes
   nite, but is still very large. in particular, any classi   er from {0, 1}d to {0, 1}
can be represented by a decision tree with 2d leaves and depth of d + 1 (see
exercise 1). therefore, the vc dimension of the class is 2d, which means that
the number of examples we need to pac learn the hypothesis class grows with
2d. unless d is very small, this is a huge number of examples.

to overcome this obstacle, we rely on the mdl scheme described in chapter 7.
the underlying prior knowledge is that we should prefer smaller trees over larger
trees. to formalize this intuition, we    rst need to de   ne a description language
for id90, which is pre   x free and requires fewer bits for smaller decision
trees. here is one possible way: a tree with n nodes will be described in n + 1
blocks, each of size log2(d + 3) bits. the    rst n blocks encode the nodes of the
tree, in a depth-   rst order (preorder), and the last block marks the end of the
code. each block indicates whether the current node is:

    an internal node of the form 1[xi=1] for some i     [d]
    a leaf whose value is 1
    a leaf whose value is 0
    end of the code

(cid:114)

252

id90

overall, there are d + 3 options, hence we need log2(d + 3) bits to describe each
block.

assuming each internal node has two children,1 it is not hard to show that
this is a pre   x-free encoding of the tree, and that the description length of a tree
with n nodes is (n + 1) log2(d + 3).
by theorem 7.7 we have that with id203 of at least 1        over a sample
of size m, for every n and every decision tree h     h with n nodes it holds that

ld(h)     ls(h) +

(n + 1) log2(d + 3) + log(2/  )

2m

.

(18.1)

this bound performs a tradeo   : on the one hand, we expect larger, more complex
id90 to have a smaller training risk, ls(h), but the respective value of
n will be larger. on the other hand, smaller id90 will have a smaller
value of n, but ls(h) might be larger. our hope (or prior knowledge) is that we
can    nd a decision tree with both low empirical risk, ls(h), and a number of
nodes n not too high. our bound indicates that such a tree will have low true
risk, ld(h).

18.2

decision tree algorithms

the bound on ld(h) given in equation (18.1) suggests a learning rule for decision
trees     search for a tree that minimizes the right-hand side of equation (18.1).
unfortunately, it turns out that solving this problem is computationally hard.2
consequently, practical decision tree learning algorithms are based on heuristics
such as a greedy approach, where the tree is constructed gradually, and locally
optimal decisions are made at the construction of each node. such algorithms
cannot guarantee to return the globally optimal decision tree but tend to work
reasonably well in practice.

a general framework for growing a decision tree is as follows. we start with
a tree with a single leaf (the root) and assign this leaf a label according to a
majority vote among all labels over the training set. we now perform a series of
iterations. on each iteration, we examine the e   ect of splitting a single leaf. we
de   ne some    gain    measure that quanti   es the improvement due to this split.
then, among all possible splits, we either choose the one that maximizes the
gain and perform it, or choose not to split the leaf at all.

in the following we provide a possible implementation. it is based on a popular
decision tree algorithm known as         (short for    iterative dichotomizer 3   ).
we describe the algorithm for the case of binary features, namely, x = {0, 1}d,

1 we may assume this without loss of generality, because if a decision node has only one

child, we can replace the node by its child without a   ecting the predictions of the decision
tree.
2 more precisely, if np(cid:54)=p then no algorithm can solve equation (18.1) in time polynomial

in n, d, and m.

18.2 decision tree algorithms

253

and therefore all splitting rules are of the form 1[xi=1] for some feature i     [d].
we discuss the case of real valued features in section 18.2.3.

the algorithm works by recursive calls, with the initial call being  (s, [d]),
and returns a decision tree. in the pseudocode that follows, we use a call to a
procedure gain(s, i), which receives a training set s and an index i and evaluates
the gain of a split of the tree according to the ith feature. we describe several
gain measures in section 18.2.1.

 (s, a)

input: training set s, feature subset a     [d]
if all examples in s are labeled by 1, return a leaf 1
if all examples in s are labeled by 0, return a leaf 0
if a =    , return a leaf whose value = majority of labels in s
else :

let j = argmaxi   a gain(s, i)
if all examples in s have the same label

return a leaf whose value = majority of labels in s
else
let t1 be the tree returned by  ({(x, y)     s : xj = 1}, a \ {j}).
let t2 be the tree returned by  ({(x, y)     s : xj = 0}, a \ {j}).
return the tree:

xj = 1?

t2

t1

18.2.1

implementations of the gain measure

di   erent algorithms use di   erent implementations of gain(s, i). here we present
three. we use the notation ps[f ] to denote the id203 that an event holds
with respect to the uniform distribution over s.
train error: the simplest de   nition of gain is the decrease in training error.
formally, let c(a) = min{a, 1   a}. note that the training error before splitting on
feature i is c(ps[y = 1]), since we took a majority vote among labels. similarly,
the error after splitting on feature i is

p
s

[xi = 1] c(p

[y = 1|xi = 1]) + p

[xi = 0]c(p

s

[y = 1|xi = 0]).

s

s

therefore, we can de   ne gain to be the di   erence between the two, namely,
gain(s, i) := c(p

   (cid:16)p

s

s

[y = 1])
[xi = 1] c(p

s

[y = 1|xi = 1]) + p

[xi = 0]c(p

[y = 1|xi = 0])

.

s

s

(cid:17)

254

id90

information gain: another popular gain measure that is used in the  
and c4.5 algorithms of quinlan (1993) is the information gain. the information
gain is the di   erence between the id178 of the label before and after the split,
and is achieved by replacing the function c in the previous expression by the
id178 function,

c(a) =    a log(a)     (1     a) log(1     a).

gini index: yet another de   nition of a gain, which is used by the cart

algorithm of breiman, friedman, olshen & stone (1984), is the gini index,

c(a) = 2a(1     a).

both the information gain and the gini index are smooth and concave upper
bounds of the train error. these properties can be advantageous in some situa-
tions (see, for example, kearns & mansour (1996)).

18.2.2

pruning

the   algorithm described previously still su   ers from a big problem: the
returned tree will usually be very large. such trees may have low empirical risk,
but their true risk will tend to be high     both according to our theoretical
analysis, and in practice. one solution is to limit the number of iterations of  ,
leading to a tree with a bounded number of nodes. another common solution is
to prune the tree after it is built, hoping to reduce it to a much smaller tree,
but still with a similar empirical error. theoretically, according to the bound in
equation (18.1), if we can make n much smaller without increasing ls(h) by
much, we are likely to get a decision tree with a smaller true risk.

usually, the pruning is performed by a bottom-up walk on the tree. each node
might be replaced with one of its subtrees or with a leaf, based on some bound
or estimate of ld(h) (for example, the bound in equation (18.1)). a pseudocode
of a common template is given in the following.

generic tree pruning procedure

input:

function f (t, m) (bound/estimate for the generalization error

of a decision tree t , based on a sample of size m),

tree t .

foreach node j in a bottom-up walk on t (from leaves to root):

   nd t (cid:48) which minimizes f (t (cid:48), m), where t (cid:48) is any of the following:

the current tree after replacing node j with a leaf 1.
the current tree after replacing node j with a leaf 0.
the current tree after replacing node j with its left subtree.
the current tree after replacing node j with its right subtree.
the current tree.

let t := t (cid:48).

18.3 id79s

255

18.2.3

threshold-based splitting rules for real-valued features

in the previous section we have described an algorithm for growing a decision
tree assuming that the features are binary and the splitting rules are of the
form 1[xi=1]. we now extend this result to the case of real-valued features and
threshold-based splitting rules, namely, 1[xi<  ]. such splitting rules yield decision
stumps, and we have studied them in chapter 10.

the basic idea is to reduce the problem to the case of binary features as
follows. let x1, . . . , xm be the instances of the training set. for each real-valued
feature i, sort the instances so that x1,i                xm,i. de   ne a set of thresholds
  0,i, . . . ,   m+1,i such that   j,i     (xj,i, xj+1,i) (where we use the convention x0,i =
       and xm+1,i =    ). finally, for each i and j we de   ne the binary feature
1[xi<  j,i]. once we have constructed these binary features, we can run the  
procedure described in the previous section. it is easy to verify that for any
decision tree with threshold-based splitting rules over the original real-valued
features there exists a decision tree over the constructed binary features with
the same training error and the same number of nodes.

if the original number of real-valued features is d and the number of examples
is m, then the number of constructed binary features becomes dm. calculating
the gain of each feature might therefore take o(dm2) operations. however, using
a more clever implementation, the runtime can be reduced to o(dm log(m)). the
idea is similar to the implementation of erm for decision stumps as described
in section 10.1.1.

18.3

id79s

as mentioned before, the class of id90 of arbitrary size has in   nite vc
dimension. we therefore restricted the size of the decision tree. another way
to reduce the danger of over   tting is by constructing an ensemble of trees. in
particular, in the following we describe the method of id79s, introduced
by breiman (2001).

a id79 is a classi   er consisting of a collection of id90, where
each tree is constructed by applying an algorithm a on the training set s and
an additional random vector,   , where    is sampled i.i.d. from some distribution.
the prediction of the id79 is obtained by a majority vote over the
predictions of the individual trees.

to specify a particular id79, we need to de   ne the algorithm a and
the distribution over   . there are many ways to do this and here we describe one
particular option. we generate    as follows. first, we take a random subsample
from s with replacements; namely, we sample a new training set s(cid:48) of size m(cid:48)
using the uniform distribution over s. second, we construct a sequence i1, i2, . . .,
where each it is a subset of [d] of size k, which is generated by sampling uniformly
at random elements from [d]. all these random variables form the vector   . then,

256

id90

the algorithm a grows a decision tree (e.g., using the   algorithm) based on
the sample s(cid:48), where at each splitting stage of the algorithm, the algorithm is
restricted to choosing a feature that maximizes gain from the set it. intuitively,
if k is small, this restriction may prevent over   tting.

18.4

summary

id90 are very intuitive predictors. typically, if a human programmer
creates a predictor it will look like a decision tree. we have shown that the vc
dimension of id90 with k leaves is k and proposed the mdl paradigm
for learning id90. the main problem with id90 is that they
are computationally hard to learn; therefore we described several heuristic pro-
cedures for training them.

18.5

bibliographic remarks

many algorithms for learning id90 (such as   and c4.5) have been
derived by quinlan (1986). the cart algorithm is due to breiman et al. (1984).
id79s were introduced by breiman (2001). for additional reading we
refer the reader to (hastie, tibshirani & friedman 2001, rokach 2007).

the proof of the hardness of training id90 is given in hya   l & rivest

(1976).

18.6

exercises
1. 1. show that any binary classi   er h : {0, 1}d (cid:55)    {0, 1} can be implemented
as a decision tree of height at most d + 1, with internal nodes of the form
(xi = 0?) for some i     {1, . . . , d}.
domain {0, 1}d is 2d.

2. conclude that the vc dimension of the class of id90 over the

2. (suboptimality of  )

consider the following training set, where x = {0, 1}3 and y = {0, 1}:

((1, 1, 1), 1)

((1, 0, 0), 1)

((1, 1, 0), 0)

((0, 0, 1), 0)

suppose we wish to use this training set in order to build a decision tree of
depth 2 (i.e., for each input we are allowed to ask two questions of the form
(xi = 0?) before deciding on the label).

18.6 exercises

257

1. suppose we run the   algorithm up to depth 2 (namely, we pick the root
node and its children according to the algorithm, but instead of keeping
on with the recursion, we stop and pick leaves according to the majority
label in each subtree). assume that the subroutine used to measure the
quality of each feature is based on the id178 function (so we measure the
information gain), and that if two features get the same score, one of them
is picked arbitrarily. show that the training error of the resulting decision
tree is at least 1/4.

2. find a decision tree of depth 2 that attains zero training error.

19 nearest neighbor

nearest neighbor algorithms are among the simplest of all machine learning
algorithms. the idea is to memorize the training set and then to predict the
label of any new instance on the basis of the labels of its closest neighbors in
the training set. the rationale behind such a method is based on the assumption
that the features that are used to describe the domain points are relevant to
their labelings in a way that makes close-by points likely to have the same label.
furthermore, in some situations, even when the training set is immense,    nding
a nearest neighbor can be done extremely fast (for example, when the training
set is the entire web and distances are based on links).

note that, in contrast with the algorithmic paradigms that we have discussed
so far, like erm, srm, mdl, or rlm, that are determined by some hypothesis
class, h, the nearest neighbor method    gures out a label on any test point
without searching for a predictor within some prede   ned class of functions.

in this chapter we describe nearest neighbor methods for classi   cation and
regression problems. we analyze their performance for the simple case of binary
classi   cation and discuss the e   ciency of implementing these methods.

19.1

k nearest neighbors

throughout the entire chapter we assume that our instance domain, x , is en-
dowed with a metric function   . that is,    : x   x     r is a function that returns
the distance between any two elements of x . for example, if x = rd then    can
be the euclidean distance,   (x, x(cid:48)) = (cid:107)x     x(cid:48)(cid:107) =

(cid:113)(cid:80)d

i=1(xi     x(cid:48)

i)2.

let s = (x1, y1), . . . , (xm, ym) be a sequence of training examples. for each
x     x , let   1(x), . . . ,   m(x) be a reordering of {1, . . . , m} according to their
distance to x,   (x, xi). that is, for all i < m,

  (x, x  i(x))       (x, x  i+1(x)).

for a number k, the id92 rule for binary classi   cation is de   ned as follows:

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

19.2 analysis

259

figure 19.1 an illustration of the decision boundaries of the 1-nn rule. the points
depicted are the sample points, and the predicted label of any new point will be the
label of the sample point in the center of the cell it belongs to. these cells are called a
voronoi tessellation of the space.

id92

input: a training sample s = (x1, y1), . . . , (xm, ym)
output: for every point x     x ,
return the majority label among {y  i(x) : i     k}

when k = 1, we have the 1-nn rule:

hs(x) = y  1(x).

a geometric illustration of the 1-nn rule is given in figure 19.1.

for regression problems, namely, y = r, one can de   ne the prediction to be
the average target of the k nearest neighbors. that is, hs(x) = 1
i=1 y  i(x).
more generally, for some function    : (x   y)k     y, the id92 rule with respect
k
to    is:

hs(x) =   (cid:0)(x  1(x), y  1(x)), . . . , (x  k(x), y  k(x))(cid:1) .

(19.1)

(cid:80)k

it is easy to verify that we can cast the prediction by majority of labels (for
classi   cation) or by the averaged target (for regression) as in equation (19.1) by
an appropriate choice of   . the generality can lead to other rules; for example, if
y = r, we can take a weighted average of the targets according to the distance
from x:

k(cid:88)

(cid:80)k

hs(x) =

  (x, x  i(x))
j=1   (x, x  j (x))

y  i(x).

i=1

19.2

analysis

since the nn rules are such natural learning methods, their generalization prop-
erties have been extensively studied. most previous results are asymptotic con-
sistency results, analyzing the performance of nn rules when the sample size, m,

260

nearest neighbor

goes to in   nity, and the rate of convergence depends on the underlying distribu-
tion. as we have argued in section 7.4, this type of analysis is not satisfactory.
one would like to learn from    nite training samples and to understand the gen-
eralization performance as a function of the size of such    nite training sets and
clear prior assumptions on the data distribution. we therefore provide a    nite-
sample analysis of the 1-nn rule, showing how the error decreases as a function
of m and how it depends on properties of the distribution. we will also explain
how the analysis can be generalized to id92 rules for arbitrary values of k. in
particular, the analysis speci   es the number of examples required to achieve a
true error of 2ld(h(cid:63)) +  , where h(cid:63) is the bayes optimal hypothesis, assuming
that the labeling rule is    well behaved    (in a sense we will de   ne later).

19.2.1

a generalization bound for the 1-nn rule

we now analyze the true error of the 1-nn rule for binary classi   cation with
the 0-1 loss, namely, y = {0, 1} and (cid:96)(h, (x, y)) = 1[h(x)(cid:54)=y]. we also assume
throughout the analysis that x = [0, 1]d and    is the euclidean distance.
we start by introducing some notation. let d be a distribution over x    y.
let dx denote the induced marginal distribution over x and let    : rd     r be
the id1551 over the labels, that is,

  (x) = p[y = 1|x].

recall that the bayes optimal rule (that is, the hypothesis that minimizes ld(h)
over all functions) is

h(cid:63)(x) = 1[  (x)>1/2].

we assume that the id155 function    is c-lipschitz for some
|  (x)      (x(cid:48))|     c(cid:107)x    x(cid:48)(cid:107). in other words, this
c > 0: namely, for all x, x(cid:48)     x ,
assumption means that if two vectors are close to each other then their labels
are likely to be the same.

the following lemma applies the lipschitzness of the id155
function to upper bound the true error of the 1-nn rule as a function of the
expected distance between each test instance and its nearest neighbor in the
training set.
lemma 19.1 let x = [0, 1]d,y = {0, 1}, and d be a distribution over x    y
for which the id155 function,   , is a c-lipschitz function. let
s = (x1, y1), . . . , (xm, ym) be an i.i.d. sample and let hs be its corresponding
1-nn hypothesis. let h(cid:63) be the bayes optimal rule for   . then,

e

s   dm

[ld(hs)]     2 ld(h(cid:63)) + c

s   dm,x   d[(cid:107)x     x  1(x)(cid:107)].

e

1 formally, p[y = 1|x] = lim     0

centered around x.

d({(x(cid:48),1):x(cid:48)   b(x,  )})

d({(x(cid:48),y):x(cid:48)   b(x,  ),y   y}) , where b(x,   ) is a ball of radius   

19.2 analysis

261

proof since ld(hs) = e(x,y)   d[1[hs (x)(cid:54)=y]], we obtain that es[ld(hs)] is the
id203 to sample a training set s and an additional example (x, y), such
that the label of   1(x) is di   erent from y. in other words, we can    rst sample
m unlabeled examples, sx = (x1, . . . , xm), according to dx , and an additional
unlabeled example, x     dx , then    nd   1(x) to be the nearest neighbor of x in
sx, and    nally sample y       (x) and y  1(x)       (  1(x)). it follows that
(cid:21)

sx   dmx ,x   dx ,y     (x),y(cid:48)     (  1(x))

e
[ld(hs)] =
s

[1[y(cid:54)=y(cid:48)]]

(cid:20)

e

=

e

sx   dmx ,x   dx

y     (x),y(cid:48)     (  1(x))

p

[y (cid:54)= y(cid:48)]

.

(19.2)

we next upper bound py     (x),y(cid:48)     (x(cid:48))[y (cid:54)= y(cid:48)] for any two domain points x, x(cid:48):

p

y     (x),y(cid:48)     (x(cid:48))

[y (cid:54)= y(cid:48)] =   (x(cid:48))(1       (x)) + (1       (x(cid:48)))  (x)
= (  (x)       (x) +   (x(cid:48)))(1       (x))
+ (1       (x) +   (x)       (x(cid:48)))  (x)

= 2  (x)(1       (x)) + (  (x)       (x(cid:48)))(2  (x)     1).

using |2  (x)     1|     1 and the assumption that    is c-lipschitz, we obtain that
the id203 is at most:

p

y     (x),y(cid:48)     (x(cid:48))

[y (cid:54)= y(cid:48)]     2  (x)(1       (x)) + c(cid:107)x     x(cid:48)(cid:107).

plugging this into equation (19.2) we conclude that
[2  (x)(1       (x))] + c e

[ld(hs)]     e
e
s

x

s,x

[(cid:107)x     x  1(x)(cid:107)].

finally, the error of the bayes optimal classi   er is
[min{  (x), 1       (x)}]     e

ld(h(cid:63)) = e

x

[  (x)(1       (x))].

x

combining the preceding two inequalities concludes our proof.

the next step is to bound the expected distance between a random x and its
closest element in s. we    rst need the following general id203 lemma. the
lemma bounds the id203 weight of subsets that are not hit by a random
sample, as a function of the size of that sample.
lemma 19.2 let c1, . . . , cr be a collection of subsets of some domain set, x .
let s be a sequence of m points sampled i.i.d. according to some id203
distribution, d over x . then,

       (cid:88)

i:ci   s=   

           r

.

m e

p[ci]

e

s   dm

262

nearest neighbor

proof from the linearity of expectation, we can rewrite:

       =

r(cid:88)

i=1

(cid:2)1[ci   s=   ]

(cid:3) .

p[ci] e

s

next, for each i we have

p[ci]

e
s

i:ci   s=   

       (cid:88)
(cid:3) = p
(cid:2)1[ci   s=   ]
       (cid:88)
           r(cid:88)

p[ci]

e
s

s

i:ci   s=   

i=1

e
s

[ci     s =    ] = (1     p[ci])m     e    p[ci] m.

combining the preceding two equations we get

p[ci] e    p[ci] m     r max

i

p[ci] e    p[ci] m.

finally, by a standard calculus, maxa ae   ma     1

me and this concludes the proof.

equipped with the preceding lemmas we are now ready to state and prove the
main result of this section     an upper bound on the expected error of the 1-nn
learning rule.
theorem 19.3 let x = [0, 1]d,y = {0, 1}, and d be a distribution over x   y
for which the id155 function,   , is a c-lipschitz function. let
hs denote the result of applying the 1-nn rule to a sample s     dm. then,

e

s   dm

[ld(hs)]     2 ld(h(cid:63)) + 4 c

   

    1

d m

d+1 .

proof fix some   = 1/t , for some integer t , let r = t d and let c1, . . . , cr be the
cover of the set x using boxes of length  : namely, for every (  1, . . . ,   d)     [t ]d,
there exists a set ci of the form {x :    j, xj     [(  j     1)/t,   j/t ]}. an illustration
for d = 2, t = 5 and the set corresponding to    = (2, 4) is given in the following.

1

1

therefore,

      p

for each x, x(cid:48) in the same box we have (cid:107)x   x(cid:48)(cid:107)        

       (cid:91)

         
       (cid:91)
and by combining lemma 19.2 with the trivial bound p[(cid:83)
d(cid:0) r
me +  (cid:1) .

[(cid:107)x     x  1(x)(cid:107)]     e

[(cid:107)x     x  1(x)(cid:107)]    

i:ci   s=   

d + p

get that

e
x,s

   

ci

s

e
x,s

i:ci   s(cid:54)=   

d  . otherwise, (cid:107)x   x(cid:48)(cid:107)        

d.

        

   

d

       ,

ci

i:ci   s(cid:54)=    ci]     1 we

19.2 analysis

263

since the number of boxes is r = (1/ )d we get that

[(cid:107)x     x  1(x)(cid:107)]    

e
s,x

   

d

combining the preceding with lemma 19.1 we obtain that

d
finally, setting   = 2 m   1/(d+1) and noting that

[ld(hs)]     2 ld(h(cid:63)) + c
e
s

(cid:17)

.

(cid:16) 2d     d
(cid:16) 2d     d

m e +  

   

m e +  

(cid:17)

.

2d     d
m e

we conclude our proof.

+   =

2d 2   d md/(d+1)

m e

+ 2 m   1/(d+1)

= m   1/(d+1)(1/e + 2)     4m   1/(d+1)

the theorem implies that if we    rst    x the data-generating distribution and
then let m go to in   nity, then the error of the 1-nn rule converges to twice the
bayes error. the analysis can be generalized to larger values of k, showing that

the expected error of the id92 rule converges to (1 +(cid:112)8/k) times the error of

the bayes classi   er. this is formalized in theorem 19.5, whose proof is left as a
guided exercise.

19.2.2

the    curse of dimensionality   

the upper bound given in theorem 19.3 grows with c (the lipschitz coe   cient
of   ) and with d, the euclidean dimension of the domain set x . in fact, it is easy
to see that a necessary condition for the last term in theorem 19.3 to be smaller
than   is that m     (4 c
d/ )d+1. that is, the size of the training set should
increase exponentially with the dimension. the following theorem tells us that
this is not just an artifact of our upper bound, but, for some distributions, this
amount of examples is indeed necessary for learning with the nn rule.

   

theorem 19.4 for any c > 1, and every learning rule, l, there exists a
distribution over [0, 1]d   {0, 1}, such that   (x) is c-lipschitz, the bayes error of
the distribution is 0, but for sample sizes m     (c + 1)d/2, the true error of the
rule l is greater than 1/4.

proof fix any values of c and d. let gd
c be the grid on [0, 1]d with distance of
1/c between points on the grid. that is, each point on the grid is of the form
(a1/c, . . . , ad/c) where ai is in {0, . . . , c    1, c}. note that, since any two distinct
c     [0, 1] is a
points on this grid are at least 1/c apart, any function    : gd
c-lipschitz function. it follows that the set of all c-lipschitz functions over gd
c
contains the set of all binary valued functions over that domain. we can therefore
invoke the no-free-lunch result (theorem 5.1) to obtain a lower bound on the
needed sample sizes for learning that class. the number of points on the grid is
(c + 1)d; hence, if m < (c + 1)d/2, theorem 5.1 implies the lower bound we are
after.

264

nearest neighbor

the exponential dependence on the dimension is known as the curse of di-
mensionality. as we saw, the 1-nn rule might fail if the number of examples is
smaller than    ((c+1)d). therefore, while the 1-nn rule does not restrict itself to
a prede   ned set of hypotheses, it still relies on some prior knowledge     its success
depends on the assumption that the dimension and the lipschitz constant of the
underlying distribution,   , are not too high.

19.3

e   cient implementation*

nearest neighbor is a learning-by-memorization type of rule. it requires the
entire training data set to be stored, and at test time, we need to scan the entire
data set in order to    nd the neighbors. the time of applying the nn rule is
therefore   (d m). this leads to expensive computation at test time.

when d is small, several results from the    eld of computational geometry have
proposed data structures that enable to apply the nn rule in time o(do(1) log(m)).
however, the space required by these data structures is roughly mo(d), which
makes these methods impractical for larger values of d.

to overcome this problem, it was suggested to improve the search method by
allowing an approximate search. formally, an r-approximate search procedure is
guaranteed to retrieve a point within distance of at most r times the distance
to the nearest neighbor. three popular approximate algorithms for nn are the
kd-tree, balltrees, and locality-sensitive hashing (lsh). we refer the reader, for
example, to (shakhnarovich, darrell & indyk 2006).

19.4

summary

the id92 rule is a very simple learning algorithm that relies on the assumption
that    things that look alike must be alike.    we formalized this intuition using
the lipschitzness of the id155. we have shown that with a suf-
   ciently large training set, the risk of the 1-nn is upper bounded by twice the
risk of the bayes optimal rule. we have also derived a lower bound that shows
the    curse of dimensionality        the required sample size might increase expo-
nentially with the dimension. as a result, nn is usually performed in practice
after a id84 preprocessing step. we discuss dimensionality
reduction techniques later on in chapter 23.

19.5

bibliographic remarks

cover & hart (1967) gave the    rst analysis of 1-nn, showing that its risk con-
verges to twice the bayes optimal error under mild conditions. following a lemma
due to stone (1977), devroye & gy  or    (1985) have shown that the id92 rule

19.6 exercises

265

is consistent (with respect to the hypothesis class of all functions from rd to
{0, 1}). a good presentation of the analysis is given in the book of devroye et al.
(1996). here, we give a    nite sample guarantee that explicitly underscores the
prior assumption on the distribution. see section 7.4 for a discussion on con-
sistency results. finally, gottlieb, kontorovich & krauthgamer (2010) derived
another    nite sample bound for nn that is more similar to vc bounds.

19.6

exercises

in this exercise we will prove the following theorem for the id92 rule.
theorem 19.5 let x = [0, 1]d,y = {0, 1}, and d be a distribution over x   y
for which the id155 function,   , is a c-lipschitz function. let hs
denote the result of applying the id92 rule to a sample s     dm, where k     10.
let h(cid:63) be the bayes optimal hypothesis. then,

(cid:32)

(cid:33)

(cid:114) 8

k

[ld(hs)]    
e
s

1 +

ld(h(cid:63)) +

(cid:16)

(cid:17)

   

6 c

d + k

m   1/(d+1).

1. prove the following lemma.

lemma 19.6 let c1, . . . , cr be a collection of subsets of some domain set,
x . let s be a sequence of m points sampled i.i.d. according to some id203
distribution, d over x . then, for every k     2,

e

s   dm

i:|ci   s|<k

       (cid:88)
       =

p[ci]

p[ci]

.

           2rk
r(cid:88)

p[ci] p

m

s

i=1

hints:

    show that

e
s

       (cid:88)

i:|ci   s|<k

[|ci     s| < k] .

    fix some i and suppose that k < p[ci] m/2. use cherno      s bound to show

that

p
s

[|ci     s| < k]     p

[|ci     s| < p[ci]m/2]     e    p[ci] m/8.

s

    use the inequality maxa ae   ma     1

me to show that for such i we have

p[ci] p

s

[|ci     s| < k]     p[ci]e    p[ci] m/8     8
me

.

    conclude the proof by using the fact that for the case k     p[ci] m/2 we

clearly have:

p[ci] p

s

[|ci     s| < k]     p[ci]     2k
m

.

266

nearest neighbor

2. we use the notation y     p as a shorthand for    y is a bernoulli random variable

with expected value p.    prove the following lemma:
lemma 19.7 let k     10 and let z1, . . . , zk be independent bernoulli random
variables with p[zi = 1] = pi. denote p = 1
i=1 zi. show
that

(cid:80)k

(cid:80)
i pi and p(cid:48) = 1
(cid:33)
(cid:114) 8

(cid:32)

k

k

e

z1,...,zk

[y (cid:54)= 1[p(cid:48)>1/2]]    

p
y   p

1 +

k

[y (cid:54)= 1[p>1/2]].

p
y   p

hints:

w.l.o.g. assume that p     1/2. then, py   p[y (cid:54)= 1[p>1/2]] = p. let y(cid:48) = 1[p(cid:48)>1/2].
    show that

e

p
y   p

z1,...,zk

[y (cid:54)= y(cid:48)]     p = p

[p(cid:48) > 1/2](1     2p).

z1,...,zk

    use cherno      s bound (lemma b.3) to show that

where

p[p(cid:48) > 1/2]     e

   k p h( 1

2p   1),

h(a) = (1 + a) log(1 + a)     a.

    to conclude the proof of the lemma, you can rely on the following inequality

(without proving it): for every p     [0, 1/2] and k     10:

(1     2p) e   k p + k

2 (log(2p)+1)    

3. fix some p, p(cid:48)     [0, 1] and y(cid:48)     {0, 1}. show that

p
y   p

[y (cid:54)= y(cid:48)]     p

y   p(cid:48)[y (cid:54)= y(cid:48)] + |p     p(cid:48)|.

(cid:114) 8

p.

k

4. conclude the proof of the theorem according to the following steps:

    as in the proof of theorem 19.3, six some   > 0 and let c1, . . . , cr be the
cover of the set x using boxes of length  . for each x, x(cid:48) in the same
box we have (cid:107)x     x(cid:48)(cid:107)        
d. show that

d  . otherwise, (cid:107)x     x(cid:48)(cid:107)     2

   

      

p[ci]

       (cid:88)
(cid:104)

[ld(hs)]     e
e
s

s

+ max

i

p

s,(x,y)

i:|ci   s|<k
hs(x) (cid:54)= y |    j     [k], (cid:107)x     x  j (x)(cid:107)      

(cid:105)

   

d

.

(19.3)

    bound the    rst summand using lemma 19.6.
    to bound the second summand, let us    x s|x and x such that all the k
neighbors of x in s|x are at distance of at most  
d from x. w.l.o.g
assume that the k nn are x1, . . . , xk. denote pi =   (xi) and let p =
1
k

(cid:80)

   

i pi. use exercise 3 to show that
[hs(x) (cid:54)= y]     e

e

p

y1,...,yj

y     (x)

y1,...,yj

p
y   p

[hs(x) (cid:54)= y] + |p       (x)|.

19.6 exercises

267

w.l.o.g. assume that p     1/2. now use lemma 19.7 to show that

(cid:32)

1 +

(cid:33)

(cid:114) 8

k

[1[p>1/2] (cid:54)= y].

p
y   p

p

y1,...,yj

p
y   p

[hs(x) (cid:54)= y]    

    show that

p
y   p

[1[p>1/2] (cid:54)= y] = p = min{p, 1    p}     min{  (x), 1      (x)} +|p      (x)|.
    combine all the preceding to obtain that the second summand in equa-

tion (19.3) is bounded by(cid:32)

1 +

ld(h(cid:63)) + 3 c  

   

d.

    use r = (2/ )d to obtain that:

(cid:32)

[ld(hs)]    
e
s

1 +

ld(h(cid:63)) + 3 c  

k

(cid:33)
(cid:114) 8
(cid:33)
(cid:114) 8
m   1/(d+1)    (cid:16)

k

   

d +

2(2/ )d k

.

m

(cid:17)

   
6c

d + k

m   1/(d+1)

set   = 2m   1/(d+1) and use
2k
e

6 c m   1/(d+1)

d +

   

to conclude the proof.

20 neural networks

an arti   cial neural network is a model of computation inspired by the structure
of neural networks in the brain. in simpli   ed models of the brain, it consists of
a large number of basic computing devices (neurons) that are connected to each
other in a complex communication network, through which the brain is able to
carry out highly complex computations. arti   cial neural networks are formal
computation constructs that are modeled after this computation paradigm.

learning with neural networks was proposed in the mid-20th century. it yields
an e   ective learning paradigm and has recently been shown to achieve cutting-
edge performance on several learning tasks.

a neural network can be described as a directed graph whose nodes correspond
to neurons and edges correspond to links between them. each neuron receives
as input a weighted sum of the outputs of the neurons connected to its incoming
edges. we focus on feedforward networks in which the underlying graph does not
contain cycles.

in the context of learning, we can de   ne a hypothesis class consisting of neural
network predictors, where all the hypotheses share the underlying graph struc-
ture of the network and di   er in the weights over edges. as we will show in
section 20.3, every predictor over n variables that can be implemented in time
t (n) can also be expressed as a neural network predictor of size o(t (n)2), where
the size of the network is the number of nodes in it. it follows that the family
of hypothesis classes of neural networks of polynomial size can su   ce for all
practical learning tasks, in which our goal is to learn predictors which can be
implemented e   ciently. furthermore, in section 20.4 we will show that the sam-
ple complexity of learning such hypothesis classes is also bounded in terms of the
size of the network. hence, it seems that this is the ultimate learning paradigm
we would want to adapt, in the sense that it both has a polynomial sample com-
plexity and has the minimal approximation error among all hypothesis classes
consisting of e   ciently implementable predictors.

the caveat is that the problem of training such hypothesis classes of neural net-
work predictors is computationally hard. this will be formalized in section 20.5.
a widely used heuristic for training neural networks relies on the sgd frame-
work we studied in chapter 14. there, we have shown that sgd is a successful
learner if the id168 is convex. in neural networks, the id168 is
highly nonconvex. nevertheless, we can still implement the sgd algorithm and

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

20.1 feedforward neural networks

269

hope it will    nd a reasonable solution (as happens to be the case in several
practical tasks). in section 20.6 we describe how to implement sgd for neural
networks. in particular, the most complicated operation is the calculation of the
gradient of the id168 with respect to the parameters of the network. we
present the id26 algorithm that e   ciently calculates the gradient.

20.1

feedforward neural networks

the idea behind neural networks is that many neurons can be joined together
by communication links to carry out complex computations. it is common to
describe the structure of a neural network as a graph whose nodes are the neurons
and each (directed) edge in the graph links the output of some neuron to the
input of another neuron. we will restrict our attention to feedforward network
structures in which the underlying graph does not contain cycles.

a feedforward neural network is described by a directed acyclic graph, g =
(v, e), and a weight function over the edges, w : e     r. nodes of the graph
correspond to neurons. each single neuron is modeled as a simple scalar func-
tion,    : r     r. we will focus on three possible functions for   : the sign
function,   (a) = sign(a), the threshold function,   (a) = 1[a>0], and the sig-
moid function,   (a) = 1/(1 + exp(   a)), which is a smooth approximation to the
threshold function. we call    the    activation    function of the neuron. each edge
in the graph links the output of some neuron to the input of another neuron.
the input of a neuron is obtained by taking a weighted sum of the outputs of
all the neurons connected to it, where the weighting is according to w.

to simplify the description of the calculation performed by the network, we
further assume that the network is organized in layers. that is, the set of nodes
can be decomposed into a union of (nonempty) disjoint subsets, v =      t
t=0vt,
such that every edge in e connects some node in vt   1 to some node in vt, for
some t     [t ]. the bottom layer, v0, is called the input layer. it contains n + 1
neurons, where n is the dimensionality of the input space. for every i     [n], the
output of neuron i in v0 is simply xi. the last neuron in v0 is the    constant   
neuron, which always outputs 1. we denote by vt,i the ith neuron of the tth layer
and by ot,i(x) the output of vt,i when the network is fed with the input vector x.
therefore, for i     [n] we have o0,i(x) = xi and for i = n + 1 we have o0,i(x) = 1.
we now proceed with the calculation in a layer by layer manner. suppose we
have calculated the outputs of the neurons at layer t. then, we can calculate
the outputs of the neurons at layer t + 1 as follows. fix some vt+1,j     vt+1.
let at+1,j(x) denote the input to vt+1,j when the network is fed with the input
vector x. then,

(cid:88)

at+1,j(x) =

w((vt,r, vt+1,j)) ot,r(x),

r: (vt,r,vt+1,j )   e

270

neural networks

and

ot+1,j(x) =    (at+1,j(x)) .

that is, the input to vt+1,j is a weighted sum of the outputs of the neurons in vt
that are connected to vt+1,j, where weighting is according to w, and the output
of vt+1,j is simply the application of the activation function    on its input.

layers v1, . . . , vt   1 are often called hidden layers. the top layer, vt , is called
the output layer. in simple prediction problems the output layer contains a single
neuron whose output is the output of the network.

we refer to t as the number of layers in the network (excluding v0), or the
   depth    of the network. the size of the network is |v |. the    width    of the
network is maxt |vt|. an illustration of a layered feedforward neural network of
depth 2, size 10, and width 5, is given in the following. note that there is a
neuron in the hidden layer that has no incoming edges. this neuron will output
the constant   (0).

input
layer
(v0)

hidden

layer
(v1)

output

layer
(v2)

x1

x2

x3

constant

v0,1

v0,2

v0,3

v0,4

v1,1

v1,2

v1,3

v1,4

v1,5

v2,1

output

20.2

learning neural networks

once we have speci   ed a neural network by (v, e,   , w), we obtain a function
hv,e,  ,w : r|v0|   1     r|vt |. any set of such functions can serve as a hypothesis
class for learning. usually, we de   ne a hypothesis class of neural network predic-
tors by    xing the graph (v, e) as well as the activation function    and letting
the hypothesis class be all functions of the form hv,e,  ,w for some w : e     r.
the triplet (v, e,   ) is often called the architecture of the network. we denote
the hypothesis class by

hv,e,   = {hv,e,  ,w : w is a mapping from e to r}.

(20.1)

20.3 the expressive power of neural networks

271

that is, the parameters specifying a hypothesis in the hypothesis class are the
weights over the edges of the network.

we can now study the approximation error, estimation error, and optimization
error of such hypothesis classes. in section 20.3 we study the approximation
error of hv,e,   by studying what type of functions hypotheses in hv,e,   can
implement, in terms of the size of the underlying graph. in section 20.4 we
study the estimation error of hv,e,  , for the case of binary classi   cation (i.e.,
vt = 1 and    is the sign function), by analyzing its vc dimension. finally, in
section 20.5 we show that it is computationally hard to learn the class hv,e,  ,
even if the underlying graph is small, and in section 20.6 we present the most
commonly used heuristic for training hv,e,  .

20.3

the expressive power of neural networks

in this section we study the expressive power of neural networks, namely, what
type of functions can be implemented using a neural network. more concretely,
we will    x some architecture, v, e,   , and will study what functions hypotheses
in hv,e,   can implement, as a function of the size of v .
we start the discussion with studying which type of boolean functions (i.e.,
functions from {  1}n to {  1}) can be implemented by hv,e,sign. observe that
for every computer in which real numbers are stored using b bits, whenever we
calculate a function f : rn     r on such a computer we in fact calculate a
function g : {  1}nb     {  1}b. therefore, studying which boolean functions can
be implemented by hv,e,sign can tell us which functions can be implemented on
a computer that stores real numbers using b bits.

we begin with a simple claim, showing that without restricting the size of the
network, every boolean function can be implemented using a neural network of
depth 2.

claim 20.1 for every n, there exists a graph (v, e) of depth 2, such that
hv,e,sign contains all functions from {  1}n to {  1}.

proof we construct a graph with |v0| = n + 1,|v1| = 2n + 1, and |v2| = 1. let
e be all possible edges between adjacent layers. now, let f : {  1}n     {  1}
be some boolean function. we need to show that we can adjust the weights so
that the network will implement f . let u1, . . . , uk be all vectors in {  1}n on
which f outputs 1. observe that for every i and every x     {  1}n, if x (cid:54)= ui
then (cid:104)x, ui(cid:105)     n     2 and if x = ui then (cid:104)x, ui(cid:105) = n. it follows that the function
gi(x) = sign((cid:104)x, ui(cid:105)    n + 1) equals 1 if and only if x = ui. it follows that we can
adapt the weights between v0 and v1 so that for every i     [k], the neuron v1,i
implements the function gi(x). next, we observe that f (x) is the disjunction of

272

neural networks

the functions gi(x), and therefore can be written as

(cid:32) k(cid:88)

(cid:33)

,

f (x) = sign

gi(x) + k     1

which concludes our proof.

i=1

the preceding claim shows that neural networks can implement any boolean
function. however, this is a very weak property, as the size of the resulting
network might be exponentially large. in the construction given at the proof of
claim 20.1, the number of nodes in the hidden layer is exponentially large. this
is not an artifact of our proof, as stated in the following theorem.

theorem 20.2 for every n, let s(n) be the minimal integer such that there
exists a graph (v, e) with |v | = s(n) such that the hypothesis class hv,e,sign
contains all the functions from {0, 1}n to {0, 1}. then, s(n) is exponential in n.
similar results hold for hv,e,   where    is the sigmoid function.
proof suppose that for some (v, e) we have that hv,e,sign contains all functions
from {0, 1}n to {0, 1}. it follows that it can shatter the set of m = 2n vectors in
{0, 1}n and hence the vc dimension of hv,e,sign is 2n. on the other hand, the
vc dimension of hv,e,sign is bounded by o(|e| log(|e|))     o(|v |3), as we will
show in the next section. this implies that |v |        (2n/3), which concludes our
proof for the case of networks with the sign activation function. the proof for
the sigmoid case is analogous.

it is possible to derive a similar theorem for hv,e,   for any   , as
remark 20.1
long as we restrict the weights so that it is possible to express every weight using
a number of bits which is bounded by a universal constant. we can even con-
sider hypothesis classes where di   erent neurons can employ di   erent activation
functions, as long as the number of allowed id180 is also    nite.

which functions can we express using a network of polynomial size? the pre-
ceding claim tells us that it is impossible to express all boolean functions using
a network of polynomial size. on the positive side, in the following we show
that all boolean functions that can be calculated in time o(t (n)) can also be
expressed by a network of size o(t (n)2).
theorem 20.3 let t : n     n and for every n, let fn be the set of functions
that can be implemented using a turing machine using runtime of at most t (n).
then, there exist constants b, c     r+ such that for every n, there is a graph
(vn, en) of size at most c t (n)2 + b such that hvn,en,sign contains fn.

the proof of this theorem relies on the relation between the time complexity
of programs and their circuit complexity (see, for example, sipser (2006)). in a
nutshell, a boolean circuit is a type of network in which the individual neurons

20.3 the expressive power of neural networks

273

implement conjunctions, disjunctions, and negation of their inputs. circuit com-
plexity measures the size of boolean circuits required to calculate functions. the
relation between time complexity and circuit complexity can be seen intuitively
as follows. we can model each step of the execution of a computer program as a
simple operation on its memory state. therefore, the neurons at each layer of the
network will re   ect the memory state of the computer at the corresponding time,
and the translation to the next layer of the network involves a simple calculation
that can be carried out by the network. to relate boolean circuits to networks
with the sign activation function, we need to show that we can implement the
operations of conjunction, disjunction, and negation, using the sign activation
function. clearly, we can implement the negation operator using the sign activa-
tion function. the following lemma shows that the sign activation function can
also implement conjunctions and disjunctions of its inputs.

lemma 20.4
suppose that a neuron v, that implements the sign activation
function, has k incoming edges, connecting it to neurons whose outputs are in
{  1}. then, by adding one more edge, linking a    constant    neuron to v, and
by adjusting the weights on the edges to v, the output of v can implement the
conjunction or the disjunction of its inputs.
proof simply observe that if f : {  1}k     {  1} is the conjunction func-
tion, f (x) =    ixi, then it can be written as f (x) = sign
.
similarly, the disjunction function, f (x) =    ixi, can be written as f (x) =
sign

1     k +(cid:80)k

k     1 +(cid:80)k

i=1 xi

(cid:16)

(cid:17)

(cid:16)

(cid:17)

.

i=1 xi

so far we have discussed boolean functions. in exercise 1 we show that neural
networks are universal approximators. that is, for every    xed precision param-
eter,   > 0, and every lipschitz function f : [   1, 1]n     [   1, 1], it is possible to
construct a network such that for every input x     [   1, 1]n, the network outputs
a number between f (x)       and f (x) +  . however, as in the case of boolean
functions, the size of the network here again cannot be polynomial in n. this is
formalized in the following theorem, whose proof is a direct corollary of theo-
rem 20.2 and is left as an exercise.
theorem 20.5 fix some       (0, 1). for every n, let s(n) be the minimal integer
such that there exists a graph (v, e) with |v | = s(n) such that the hypothesis class
hv,e,  , with    being the sigmoid function, can approximate, to within precision
of  , every 1-lipschitz function f : [   1, 1]n     [   1, 1]. then s(n) is exponential
in n.

20.3.1

geometric intuition
we next provide several geometric illustrations of functions f : r2     {  1}
and show how to express them using a neural network with the sign activation
function.

274

neural networks

let us start with a depth 2 network, namely, a network with a single hidden
layer. each neuron in the hidden layer implements a halfspace predictor. then,
the single neuron at the output layer applies a halfspace on top of the binary
outputs of the neurons in the hidden layer. as we have shown before, a halfspace
can implement the conjunction function. therefore, such networks contain all
hypotheses which are an intersection of k     1 halfspaces, where k is the number
of neurons in the hidden layer; namely, they can express all convex polytopes
with k     1 faces. an example of an intersection of 5 halfspaces is given in the
following.

we have shown that a neuron in layer v2 can implement a function that
indicates whether x is in some convex polytope. by adding one more layer, and
letting the neuron in the output layer implement the disjunction of its inputs,
we get a network that computes the union of polytopes. an illustration of such
a function is given in the following.

20.4

the sample complexity of neural networks
next we discuss the sample complexity of learning the class hv,e,  . recall that
the fundamental theorem of learning tells us that the sample complexity of learn-
ing a hypothesis class of binary classi   ers depends on its vc dimension. there-
fore, we focus on calculating the vc dimension of hypothesis classes of the form
hv,e,  , where the output layer of the graph contains a single neuron.
we start with the sign activation function, namely, with hv,e,sign. what is
the vc dimension of this class? intuitively, since we learn |e| parameters, the
vc dimension should be order of |e|. this is indeed the case, as formalized by
the following theorem.
theorem 20.6 the vc dimension of hv,e,sign is o(|e| log(|e|)).

20.4 the sample complexity of neural networks

275

proof to simplify the notation throughout the proof, let us denote the hy-
pothesis class by h. recall the de   nition of the growth function,   h(m), from
section 6.5.1. this function measures maxc   x :|c|=m |hc|, where hc is the re-
striction of h to functions from c to {0, 1}. we can naturally extend the de   -
nition for a set of functions from x to some    nite set y, by letting hc be the
restriction of h to functions from c to y, and keeping the de   nition of   h(m)
intact.
our neural network is de   ned by a layered graph. let v0, . . . , vt be the layers
of the graph. fix some t     [t ]. by assigning di   erent weights on the edges
between vt   1 and vt, we obtain di   erent functions from r|vt   1|     {  1}|vt|. let
h(t) be the class of all possible such mappings from r|vt   1|     {  1}|vt|. then,
h can be written as a composition, h = h(t )     . . .   h(1). in exercise 4 we show
that the growth function of a composition of hypothesis classes is bounded by
the products of the growth functions of the individual classes. therefore,

  h(m)     t(cid:89)

t=1

  h(t) (m).

in addition, each h(t) can be written as a product of function classes, h(t) =
h(t,1)             h(t,|vt|), where each h(t,j) is all functions from layer t     1 to {  1}
that the jth neuron of layer t can implement. in exercise 3 we bound product
classes, and this yields

  h(t) (m)    

  h(t,i)(m).

|vt|(cid:89)

i=1

let dt,i be the number of edges that are headed to the ith neuron of layer t.
since the neuron is a homogenous halfspace hypothesis and the vc dimension
of homogenous halfspaces is the dimension of their input, we have by sauer   s
lemma that

  h(t,i)(m)    (cid:16) em
(cid:80)
  h(m)     (em)

dt,i

(cid:17)dt,i     (em)dt,i.

t,i dt,i = (em)|e|.

overall, we obtained that

now, assume that there are m shattered points. then, we must have   h(m) =
2m, from which we obtain

2m     (em)|e|     m     |e| log(em)/ log(2).

the claim follows by lemma a.2.

next, we consider hv,e,  , where    is the sigmoid function. surprisingly, it
turns out that the vc dimension of hv,e,   is lower bounded by    (|e|2) (see
exercise 5.) that is, the vc dimension is the number of tunable parameters
squared. it is also possible to upper bound the vc dimension by o(|v |2 |e|2),
but the proof is beyond the scope of this book. in any case, since in practice

276

neural networks

we only consider networks in which the weights have a short representation as
   oating point numbers with o(1) bits, by using the discretization trick we easily
obtain that such networks have a vc dimension of o(|e|), even if we use the
sigmoid activation function.

20.5

the runtime of learning neural networks

in the previous sections we have shown that the class of neural networks with an
underlying graph of polynomial size can express all functions that can be imple-
mented e   ciently, and that the sample complexity has a favorable dependence
on the size of the network. in this section we turn to the analysis of the time
complexity of training neural networks.
we    rst show that it is np hard to implement the erm rule with respect to
hv,e,sign even for networks with a single hidden layer that contain just 4 neurons
in the hidden layer.
theorem 20.7 let k     3. for every n, let (v, e) be a layered graph with n
input nodes, k + 1 nodes at the (single) hidden layer, where one of them is the
constant neuron, and a single output node. then, it is np hard to implement the
erm rule with respect to hv,e,sign.

the proof relies on a reduction from the k-coloring problem and is left as

exercise 6.
one way around the preceding hardness result could be that for the purpose
of learning, it may su   ce to    nd a predictor h     h with low empirical error,
not necessarily an exact erm. however, it turns out that even the task of    nd-
ing weights that result in close-to-minimal empirical error is computationally
infeasible (see (bartlett & ben-david 2002)).

one may also wonder whether it may be possible to change the architecture
of the network so as to circumvent the hardness result. that is, maybe erm
with respect to the original network structure is computationally hard but erm
with respect to some other, larger, network may be implemented e   ciently (see
chapter 8 for examples of such cases). another possibility is to use other acti-
vation functions (such as sigmoids, or any other type of e   ciently computable
id180). there is a strong indication that all of such approaches
are doomed to fail. indeed, under some cryptographic assumption, the problem
of learning intersections of halfspaces is known to be hard even in the repre-
sentation independent model of learning (see klivans & sherstov (2006)). this
implies that, under the same cryptographic assumption, any hypothesis class
which contains intersections of halfspaces cannot be learned e   ciently.

a widely used heuristic for training neural networks relies on the sgd frame-
work we studied in chapter 14. there, we have shown that sgd is a successful
learner if the id168 is convex. in neural networks, the id168 is
highly nonconvex. nevertheless, we can still implement the sgd algorithm and

20.6 sgd and id26

277

hope it will    nd a reasonable solution (as happens to be the case in several
practical tasks).

20.6

sgd and id26

the problem of    nding a hypothesis in hv,e,   with a low risk amounts to the
problem of tuning the weights over the edges. in this section we show how to
apply a heuristic search for good weights using the sgd algorithm. throughout
this section we assume that    is the sigmoid function,   (a) = 1/(1 + e   a), but
the derivation holds for any di   erentiable scalar function.

since e is a    nite set, we can think of the weight function as a vector w     r|e|.
suppose the network has n input neurons and k output neurons, and denote by
hw : rn     rk the function calculated by the network if the weight function is
de   ned by w. let us denote by    (hw(x), y) the loss of predicting hw(x) when
the target is y     y. for concreteness, we will take     to be the squared loss,
2(cid:107)hw(x)     y(cid:107)2; however, similar derivation can be obtained for
   (hw(x), y) = 1
every di   erentiable function. finally, given a distribution d over the examples
domain, rn    rk, let ld(w) be the risk of the network, namely,

ld(w) = e

(x,y)   d [   (hw(x), y)] .

recall the sgd algorithm for minimizing the risk function ld(w). we repeat
the pseudocode from chapter 14 with a few modi   cations, which are relevant
to the neural network application because of the nonconvexity of the objective
function. first, while in chapter 14 we initialized w to be the zero vector, here
we initialize w to be a randomly chosen vector with values close to zero. this
is because an initialization with the zero vector will lead all hidden neurons to
have the same weights (if the network is a full layered network). in addition,
the hope is that if we repeat the sgd procedure several times, where each time
we initialize the process with a new random vector, one of the runs will lead
to a good local minimum. second, while a    xed step size,   , is guaranteed to
be good enough for convex problems, here we utilize a variable step size,   t, as
de   ned in section 14.4.2. because of the nonconvexity of the id168, the
choice of the sequence   t is more signi   cant, and it is tuned in practice by a trial
and error manner. third, we output the best performing vector on a validation
set. in addition, it is sometimes helpful to add id173 on the weights,
with parameter   . that is, we try to minimize ld(w) +   
gradient does not have a closed form solution. instead, it is implemented using
the id26 algorithm, which will be described in the sequel.

2(cid:107)w(cid:107)2. finally, the

278

neural networks

sgd for neural networks

parameters:

number of iterations   
step size sequence   1,   2, . . . ,     
id173 parameter    > 0

input:
layered graph (v, e)
di   erentiable activation function    : r     r
initialize:
choose w(1)     r|e| at random
(from a distribution s.t. w(1) is close enough to 0)
for i = 1, 2, . . . ,   
sample (x, y)     d
calculate gradient vi = id26(x, y, w, (v, e),   )
update w(i+1) = w(i)       i(vi +   w(i))
output:

  w is the best performing w(i) on a validation set

id26

input:
example (x, y), weight vector w, layered graph (v, e),
activation function    : r     r
initialize:
denote layers of the graph v0, . . . , vt where vt = {vt,1, . . . , vt,kt}
de   ne wt,i,j as the weight of (vt,j, vt+1,i)
(where we set wt,i,j = 0 if (vt,j, vt+1,i) /    e)

forward:

set o0 = x
for t = 1, . . . , t

for i = 1, . . . , kt

set at,i =(cid:80)kt   1

j=1 wt   1,i,j ot   1,j

set ot,i =   (at,i)

backward:
set   t = ot     y
for t = t     1, t     2, . . . , 1
for i = 1, . . . , kt

  t,i =(cid:80)kt+1

j=1 wt,j,i   t+1,j   (cid:48)(at+1,j)

output:
foreach edge (vt   1,j, vt,i)     e
set the partial derivative to   t,i   (cid:48)(at,i) ot   1,j

20.6 sgd and id26

279

explaining how id26 calculates the gradient:
we next explain how the id26 algorithm calculates the gradient of
the id168 on an example (x, y) with respect to the vector w. let us    rst
recall a few de   nitions from vector calculus. each element of the gradient is
the partial derivative with respect to the variable in w corresponding to one of
the edges of the network. recall the de   nition of a partial derivative. given a
function f : rn     r, the partial derivative with respect to the ith variable at w
is obtained by    xing the values of w1, . . . , wi   1, wi+1, wn, which yields the scalar
function g : r     r de   ned by g(a) = f ((w1, . . . , wi   1, wi + a, wi+1, . . . , wn)),
and then taking the derivative of g at 0. for a function with multiple outputs,
f : rn     rm, the jacobian of f at w     rn, denoted jw(f ), is the m    n matrix
whose i, j element is the partial derivative of fi : rn     r w.r.t. its jth variable
at w. note that if m = 1 then the jacobian matrix is the gradient of the function
(represented as a row vector). two examples of jacobian calculations, which we
will later use, are as follows.
    let f (w) = aw for a     rm,n. then jw(f ) = a.
    for every n, we use the notation    to denote the function from rn to rn
which applies the sigmoid function element-wise. that is,    =   (  ) means
1+exp(     i) . it is easy to verify
that for every i we have   i =   (  i) =
that j  (  ) is a diagonal matrix whose (i, i) entry is   (cid:48)(  i), where   (cid:48) is
the derivative function of the (scalar) sigmoid function, namely,   (cid:48)(  i) =
(1+exp(  i))(1+exp(     i)) . we also use the notation diag(  (cid:48)(  )) to denote this
matrix.

1

1

the chain rule for taking the derivative of a composition of functions can be
written in terms of the jacobian as follows. given two functions f : rn     rm
and g : rk     rn, we have that the jacobian of the composition function,
(f     g) : rk     rm, at w, is

jw(f     g) = jg(w)(f )jw(g).

for example, for g(w) = aw, where a     rn,k, we have that

jw(       g) = diag(  (cid:48)(aw)) a.

to describe the id26 algorithm, let us    rst decompose v into the
layers of the graph, v =      t
t=0vt. for every t, let us write vt = {vt,1, . . . , vt,kt},
where kt = |vt|. in addition, for every t denote wt     rkt+1,kt a matrix which
gives a weight to every potential edge between vt and vt+1. if the edge exists in
e then we set wt,i,j to be the weight, according to w, of the edge (vt,j, vt+1,i).
otherwise, we add a    phantom    edge and set its weight to be zero, wt,i,j = 0.
since when calculating the partial derivative with respect to the weight of some
edge we    x all other weights, these additional    phantom    edges have no e   ect
on the partial derivative with respect to existing edges. it follows that we can
assume, without loss of generality, that all edges exist, that is, e =    t(vt  vt+1).

280

neural networks

next, we discuss how to calculate the partial derivatives with respect to the
edges from vt   1 to vt, namely, with respect to the elements in wt   1. since we
   x all other weights of the network, it follows that the outputs of all the neurons
in vt   1 are    xed numbers which do not depend on the weights in wt   1. denote
the corresponding vector by ot   1. in addition, let us denote by (cid:96)t : rkt     r the
id168 of the subnetwork de   ned by layers vt, . . . , vt as a function of the
outputs of the neurons in vt. the input to the neurons of vt can be written as
at = wt   1ot   1 and the output of the neurons of vt is ot =   (at). that is, for
every j we have ot,j =   (at,j). we obtain that the loss, as a function of wt   1,
can be written as

gt(wt   1) = (cid:96)t(ot) = (cid:96)t(  (at)) = (cid:96)t(  (wt   1ot   1)).

it would be convenient to rewrite this as follows. let wt   1     rkt   1kt be the
column vector obtained by concatenating the rows of wt   1 and then taking the
transpose of the resulting long vector. de   ne by ot   1 the kt    (kt   1kt) matrix

                     

o(cid:62)
t   1
0
...
0

                      .

0
o(cid:62)
t   1
...
0

      
      
. . .
      

0

0
...
o(cid:62)
t   1

ot   1 =

(20.2)

then, wt   1ot   1 = ot   1wt   1, so we can also write

gt(wt   1) = (cid:96)t(  (ot   1 wt   1)).

therefore, applying the chain rule, we obtain that

jwt   1(gt) = j  (ot   1wt   1)((cid:96)t) diag(  (cid:48)(ot   1wt   1)) ot   1.

using our notation we have ot =   (ot   1wt   1) and at = ot   1wt   1, which yields

jwt   1 (gt) = jot((cid:96)t) diag(  (cid:48)(at)) ot   1.

let us also denote   t = jot((cid:96)t). then, we can further rewrite the preceding as

t   1 ,

. . . ,   t,kt   (cid:48)(at,kt) o(cid:62)
t   1

(20.3)

jwt   1(gt) =(cid:0)  t,1   (cid:48)(at,1) o(cid:62)

(cid:1) .

it is left to calculate the vector   t = jot((cid:96)t) for every t. this is the gradient
of (cid:96)t at ot. we calculate this in a recursive manner. first observe that for the
last layer we have that (cid:96)t (u) =    (u, y), where     is the id168. since we
2(cid:107)u   y(cid:107)2 we obtain that ju((cid:96)t ) = (u   y). in particular,
assume that    (u, y) = 1
  t = jot ((cid:96)t ) = (ot     y). next, note that

(cid:96)t(u) = (cid:96)t+1(  (wtu)).

therefore, by the chain rule,

ju((cid:96)t) = j  (wtu)((cid:96)t+1)diag(  (cid:48)(wtu))wt.

20.7 summary

281

in particular,

  t = jot((cid:96)t) = j  (wtot)((cid:96)t+1)diag(  (cid:48)(wtot))wt

= jot+1((cid:96)t+1)diag(  (cid:48)(at+1))wt
=   t+1 diag(  (cid:48)(at+1))wt.

in summary, we can    rst calculate the vectors {at, ot} from the bottom of
the network to its top. then, we calculate the vectors {  t} from the top of
the network back to its bottom. once we have all of these vectors, the partial
derivatives are easily obtained using equation (20.3). we have thus shown that
the pseudocode of id26 indeed calculates the gradient.

20.7

summary

classes of all predictors that can be implemented in runtime of o((cid:112)s(n)). we

neural networks over graphs of size s(n) can be used to describe hypothesis

have also shown that their sample complexity depends polynomially on s(n)
(speci   cally, it depends on the number of edges in the network). therefore, classes
of neural network hypotheses seem to be an excellent choice. regrettably, the
problem of training the network on the basis of training data is computationally
hard. we have presented the sgd framework as a heuristic approach for training
neural networks and described the id26 algorithm which e   ciently
calculates the gradient of the id168 with respect to the weights over the
edges.

20.8

bibliographic remarks

neural networks were extensively studied in the 1980s and early 1990s, but with
mixed empirical success. in recent years, a combination of algorithmic advance-
ments, as well as increasing computational power and data size, has led to a
breakthrough in the e   ectiveness of neural networks. in particular,    deep net-
works    (i.e., networks of more than 2 layers) have shown very impressive practical
performance on a variety of domains. a few examples include convolutional net-
works (lecun & bengio 1995), restricted id82s (hinton, osindero
& teh 2006), auto-encoders (ranzato, huang, boureau & lecun 2007, bengio &
lecun 2007, collobert & weston 2008, lee, grosse, ranganath & ng 2009, le,
ranzato, monga, devin, corrado, chen, dean & ng 2012), and sum-product
networks (livni, shalev-shwartz & shamir 2013, poon & domingos 2011). see
also (bengio 2009) and the references therein.

the expressive power of neural networks and the relation to circuit complexity
have been extensively studied in (parberry 1994). for the analysis of the sample
complexity of neural networks we refer the reader to (anthony & bartlet 1999).
our proof technique of theorem 20.6 is due to kakade and tewari lecture notes.

282

neural networks

klivans & sherstov (2006) have shown that for any c > 0, intersections of nc
halfspaces over {  1}n are not e   ciently pac learnable, even if we allow repre-
sentation independent learning. this hardness result relies on the cryptographic
assumption that there is no polynomial time solution to the unique-shortest-
vector problem. as we have argued, this implies that there cannot be an e   cient
algorithm for training neural networks, even if we allow larger networks or other
id180 that can be implemented e   ciently.

the id26 algorithm has been introduced in rumelhart, hinton &

williams (1986).

20.9

exercises

2. prove theorem 20.5.

1. neural networks are universal approximators: let f :

[   1, 1]n    
[   1, 1] be a   -lipschitz function. fix some   > 0. construct a neural net-
work n : [   1, 1]n     [   1, 1], with the sigmoid activation function, such that
for every x     [   1, 1]n it holds that |f (x)     n (x)|      .
hint: similarly to the proof of theorem 19.3, partition [   1, 1]n into small
boxes. use the lipschitzness of f to show that it is approximately constant
at each box. finally, show that a neural network can    rst decide which box
the input vector belongs to, and then predict the averaged value of f at that
box.
hint: for every f : {   1, 1}n     {   1, 1} construct a 1-lipschitz function
g : [   1, 1]n     [   1, 1] such that if you can approximate g then you can express
f .
3. growth function of product: for i = 1, 2, let fi be a set of functions from
x to yi. de   ne h = f1    f2 to be the cartesian product class. that is, for
every f1     f1 and f2     f2, there exists h     h such that h(x) = (f1(x), f2(x)).
prove that   h(m)       f1(m)   f2(m).
4. growth function of composition: let f1 be a set of functions from x
to z and let f2 be a set of functions from z to y. let h = f2     f1 be the
composition class. that is, for every f1     f1 and f2     f2, there exists h     h
such that h(x) = f2(f1(x)). prove that   h(m)       f2(m)  f1(m).

5. vc of sigmoidal networks: in this exercise we show that there is a graph
(v, e) such that the vc dimension of the class of neural networks over these
graphs with the sigmoid activation function is    (|e|2). note that for every   >
function, 1[(cid:80)
0, the sigmoid activation function can approximate the threshold activation
i xi], up to accuracy  . to simplify the presentation, throughout
1[(cid:80)
the exercise we assume that we can exactly implement the activation function

i xi>0] using a sigmoid activation function.
fix some n.
1. construct a network, n1, with o(n) weights, which implements a function
from r to {0, 1}n and satis   es the following property. for every x     {0, 1}n,

20.9 exercises

283

if we feed the network with the real number 0.x1x2 . . . xn, then the output
of the network will be x.
hint: denote    = 0.x1x2 . . . xn and observe that 10k       0.5 is at least 0.5
if xk = 1 and is at most    0.3 if xk =    1.
2. construct a network, n2, with o(n) weights, which implements a function
from [n] to {0, 1}n such that n2(i) = ei for all i. that is, upon receiving
the input i, the network outputs the vector of all zeros except 1 at the i   th
neuron.

3. let   1, . . . ,   n be n real numbers such that every   i is of the form 0.a(i)

1 a(i)
j     {0, 1}. construct a network, n3, with o(n) weights, which im-
with a(i)
plements a function from [n] to r, and satis   es n2(i) =   i for every i     [n].
4. combine n1, n3 to obtain a network that receives i     [n] and output a(i).
5. construct a network n4 that receives (i, j)     [n]    [n] and outputs a(i)
j .
hint: observe that the and function over {0, 1}2 can be calculated using
o(1) weights.

2 . . . a(i)
n ,

6. conclude that there is a graph with o(n) weights such that the vc di-

mension of the resulting hypothesis class is n2.

6. prove theorem 20.7.

hint: the proof is similar to the hardness of learning intersections of halfs-
paces     see exercise 32 in chapter 8.

part iii

additional learning models

21 online learning

in this chapter we describe a di   erent model of learning, which is called online
learning. previously, we studied the pac learning model, in which the learner
   rst receives a batch of training examples, uses the training set to learn a hy-
pothesis, and only when learning is completed uses the learned hypothesis for
predicting the label of new examples. in our papayas learning problem, this
means that we should    rst buy a bunch of papayas and taste them all. then, we
use all of this information to learn a prediction rule that determines the taste
of new papayas. in contrast, in online learning there is no separation between a
training phase and a prediction phase. instead, each time we buy a papaya, it
is    rst considered a test example since we should predict whether it is going to
taste good. then, after taking a bite from the papaya, we know the true label,
and the same papaya can be used as a training example that can help us improve
our prediction mechanism for future papayas.

concretely, online learning takes place in a sequence of consecutive rounds.
on each online round, the learner    rst receives an instance (the learner buys
a papaya and knows its shape and color, which form the instance). then, the
learner is required to predict a label (is the papaya tasty?). at the end of the
round, the learner obtains the correct label (he tastes the papaya and then knows
whether it is tasty or not). finally, the learner uses this information to improve
his future predictions.

to analyze online learning, we follow a similar route to our study of pac
learning. we start with online binary classi   cation problems. we consider both
the realizable case, in which we assume, as prior knowledge, that all the labels are
generated by some hypothesis from a given hypothesis class, and the unrealizable
case, which corresponds to the agnostic pac learning model. in particular, we
present an important algorithm called weighted-majority. next, we study online
learning problems in which the id168 is convex. finally, we present the
id88 algorithm as an example of the use of surrogate convex id168s
in the online learning model.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

288

online learning

21.1

online classi   cation in the realizable case

online learning is performed in a sequence of consecutive rounds, where at round
t the learner is given an instance, xt, taken from an instance domain x , and is
required to provide its label. we denote the predicted label by pt. after predicting
the label, the correct label, yt     {0, 1}, is revealed to the learner. the learner   s
goal is to make as few prediction mistakes as possible during this process. the
learner tries to deduce information from previous rounds so as to improve its
predictions on future rounds.

clearly, learning is hopeless if there is no correlation between past and present
rounds. previously in the book, we studied the pac model in which we assume
that past and present examples are sampled i.i.d. from the same distribution
source. in the online learning model we make no statistical assumptions regard-
ing the origin of the sequence of examples. the sequence is allowed to be deter-
ministic, stochastic, or even adversarially adaptive to the learner   s own behavior
(as in the case of spam e-mail    ltering). naturally, an adversary can make the
number of prediction mistakes of our online learning algorithm arbitrarily large.
for example, the adversary can present the same instance on each online round,
wait for the learner   s prediction, and provide the opposite label as the correct
label.

to make nontrivial statements we must further restrict the problem. the real-
izability assumption is one possible natural restriction. in the realizable case, we
assume that all the labels are generated by some hypothesis, h(cid:63) : x     y. fur-
thermore, h(cid:63) is taken from a hypothesis class h, which is known to the learner.
this is analogous to the pac learning model we studied in chapter 3. with this
restriction on the sequence, the learner should make as few mistakes as possible,
assuming that both h(cid:63) and the sequence of instances can be chosen by an ad-
versary. for an online learning algorithm, a, we denote by ma(h) the maximal
number of mistakes a might make on a sequence of examples which is labeled by
some h(cid:63)     h. we emphasize again that both h(cid:63) and the sequence of instances
can be chosen by an adversary. a bound on ma(h) is called a mistake-bound and
we will study how to design algorithms for which ma(h) is minimal. formally:
definition 21.1 (mistake bounds, online learnability) let h be a hypoth-
esis class and let a be an online learning algorithm. given any sequence s =
(x1, h(cid:63)(y1)), . . . , (xt , h(cid:63)(yt )), where t is any integer and h(cid:63)     h, let ma(s) be
the number of mistakes a makes on the sequence s. we denote by ma(h) the
supremum of ma(s) over all sequences of the above form. a bound of the form
ma(h)     b <     is called a mistake bound. we say that a hypothesis class h is
online learnable if there exists an algorithm a for which ma(h)     b <    .

our goal is to study which hypothesis classes are learnable in the online model,
and in particular to    nd good learning algorithms for a given hypothesis class.

remark 21.1 throughout this section and the next, we ignore the computa-

21.1 online classi   cation in the realizable case

289

tional aspect of learning, and do not restrict the algorithms to be e   cient. in
section 21.3 and section 21.4 we study e   cient online learning algorithms.

to simplify the presentation, we start with the case of a    nite hypothesis class,

namely, |h| <    .
in pac learning, we identi   ed erm as a good learning algorithm, in the sense
that if h is learnable then it is learnable by the rule ermh. a natural learning
rule for online learning is to use (at any online round) any erm hypothesis,
namely, any hypothesis which is consistent with all past examples.

consistent
input: a    nite hypothesis class h
initialize: v1 = h
for t = 1, 2, . . .
receive xt
choose any h     vt
predict pt = h(xt)
receive true label yt = h(cid:63)(xt)
update vt+1 = {h     vt : h(xt) = yt}

the consistent algorithm maintains a set, vt, of all the hypotheses which
are consistent with (x1, y1), . . . , (xt   1, yt   1). this set is often called the version
space. it then picks any hypothesis from vt and predicts according to this hy-
pothesis.

obviously, whenever consistent makes a prediction mistake, at least one
hypothesis is removed from vt. therefore, after making m mistakes we have
|vt|     |h|     m . since vt is always nonempty (by the realizability assumption it
contains h(cid:63)) we have 1     |vt|     |h|     m . rearranging, we obtain the following:
corollary 21.2 let h be a    nite hypothesis class. the consistent algorithm
enjoys the mistake bound mconsistent(h)     |h|     1.

it is rather easy to construct a hypothesis class and a sequence of examples on
which consistent will indeed make |h|    1 mistakes (see exercise 1). therefore,
we present a better algorithm in which we choose h     vt in a smarter way. we
shall see that this algorithm is guaranteed to make exponentially fewer mistakes.

halving
input: a    nite hypothesis class h
initialize: v1 = h
for t = 1, 2, . . .
receive xt
predict pt = argmaxr   {0,1} |{h     vt : h(xt) = r}|
(in case of a tie predict pt = 1)
receive true label yt = h(cid:63)(xt)
update vt+1 = {h     vt : h(xt) = yt}

290

online learning

theorem 21.3 let h be a    nite hypothesis class. the halving algorithm
enjoys the mistake bound mhalving(h)     log2(|h|).
proof we simply note that whenever the algorithm errs we have |vt+1|     |vt|/2,
(hence the name halving). therefore, if m is the total number of mistakes, we
have

1     |vt +1|     |h| 2   m .
rearranging this inequality we conclude our proof.

of course, halving   s mistake bound is much better than consistent   s mistake
bound. we already see that online learning is di   erent from pac learning   while
in pac, any erm hypothesis is good, in online learning choosing an arbitrary
erm hypothesis is far from being optimal.

21.1.1

online learnability

we next take a more general approach, and aim at characterizing online learn-
ability. in particular, we target the following question: what is the optimal online
learning algorithm for a given hypothesis class h?

we present a dimension of hypothesis classes that characterizes the best achiev-
able mistake bound. this measure was proposed by nick littlestone and we
therefore refer to it as ldim(h).

to motivate the de   nition of ldim it is convenient to view the online learning
process as a game between two players: the learner versus the environment. on
round t of the game, the environment picks an instance xt, the learner predicts a
label pt     {0, 1}, and    nally the environment outputs the true label, yt     {0, 1}.
suppose that the environment wants to make the learner err on the    rst t rounds
of the game. then, it must output yt = 1     pt, and the only question is how it
should choose the instances xt in such a way that ensures that for some h(cid:63)     h
we have yt = h(cid:63)(xt) for all t     [t ].

a strategy for an adversarial environment can be formally described as a
binary tree, as follows. each node of the tree is associated with an instance from
x . initially, the environment presents to the learner the instance associated with
the root of the tree. then, if the learner predicts pt = 1 the environment will
declare that this is a wrong prediction (i.e., yt = 0) and will traverse to the right
child of the current node. if the learner predicts pt = 0 then the environment
will set yt = 1 and will traverse to the left child. this process will continue and
at each round, the environment will present the instance associated with the
current node.

formally, consider a complete binary tree of depth t (we de   ne the depth of
the tree as the number of edges in a path from the root to a leaf). we have
2t +1     1 nodes in such a tree, and we attach an instance to each node. let
v1, . . . , v2t +1   1 be these instances. we start from the root of the tree, and set
x1 = v1. at round t, we set xt = vit where it is the current node. at the end of

21.1 online classi   cation in the realizable case

291

v1

v2

v3

h1

h2

h3

h4

v1
v2
v3

0
0
   

0
1
   

1
   
0

1
   
1

figure 21.1 an illustration of a shattered tree of depth 2. the dashed path
corresponds to the sequence of examples ((v1, 1), (v3, 0)). the tree is shattered by
h = {h1, h2, h3, h4}, where the predictions of each hypothesis in h on the instances
v1, v2, v3 is given in the table (the    *    marid116 that hj(vi) can be either 1 or 0).

is, it+1 = 2it +yt. unraveling the recursion we obtain it = 2t   1 +(cid:80)t   1

round t, we go to the left child of it if yt = 0 or to the right child if yt = 1. that
j=1 yj 2t   1   j.
the preceding strategy for the environment succeeds only if for every (y1, . . . , yt )
there exists h     h such that yt = h(xt) for all t     [t ]. this leads to the following
de   nition.
definition 21.4 (h shattered tree) a shattered tree of depth d is a sequence
of instances v1, . . . , v2d   1 in x such that for every labeling (y1, . . . , yd)     {0, 1}d
there exists h     h such that for all t     [d] we have h(vit) = yt where it =

2t   1 +(cid:80)t   1

j=1 yj 2t   1   j.

an illustration of a shattered tree of depth 2 is given in figure 21.1.

definition 21.5 (littlestone   s dimension (ldim)) ldim(h) is the maximal
integer t such that there exists a shattered tree of depth t , which is shattered
by h.

the de   nition of ldim and the discussion above immediately imply the fol-

lowing:

lemma 21.6 no algorithm can have a mistake bound strictly smaller than
ldim(h); namely, for every algorithm, a, we have ma(h)     ldim(h).
proof let t = ldim(h) and let v1, . . . , v2t    1 be a sequence that satis   es the
requirements in the de   nition of ldim. if the environment sets xt = vit and
yt = 1    pt for all t     [t ], then the learner makes t mistakes while the de   nition
of ldim implies that there exists a hypothesis h     h such that yt = h(xt) for all
t.

let us now give several examples.

example 21.2 let h be a    nite hypothesis class. clearly, any tree that is shat-
tered by h has depth of at most log2(|h|). therefore, ldim(h)     log2(|h|).
another way to conclude this inequality is by combining lemma 21.6 with the-
orem 21.3.
example 21.3 let x = {1, . . . , d} and h = {h1, . . . , hd} where hj(x) = 1 i   

292

online learning

x = j. then, it is easy to show that ldim(h) = 1 while |h| = d can be arbitrarily
large. therefore, this example shows that ldim(h) can be signi   cantly smaller
than log2(|h|).
example 21.4 let x = [0, 1] and h = {x (cid:55)    1[x<a] : a     [0, 1]}; namely, h is
the class of thresholds on the interval [0, 1]. then, ldim(h) =    . to see this,
consider the tree

1/2

1/4

3/4

1/8

3/8

5/8

7/8

this tree is shattered by h. and, because of the density of the reals, this tree
can be made arbitrarily deep.

lemma 21.6 states that ldim(h) lower bounds the mistake bound of any
algorithm. interestingly, there is a standard algorithm whose mistake bound
matches this lower bound. the algorithm is similar to the halving algorithm.
recall that the prediction of halving is made according to a majority vote of
the hypotheses which are consistent with previous examples. we denoted this
t = {h     vt :
set by vt. put another way, halving partitions vt into two sets: v +
h(xt) = 1} and v    
t = {h     vt : h(xt) = 0}. it then predicts according to the
larger of the two groups. the rationale behind this prediction is that whenever
halving makes a mistake it ends up with |vt+1|     0.5|vt|.

the optimal algorithm we present in the following uses the same idea, but
instead of predicting according to the larger class, it predicts according to the
class with larger ldim.

standard optimal algorithm (soa)

input: a hypothesis class h
initialize: v1 = h
for t = 1, 2, . . .

receive xt
for r     {0, 1} let v (r)
predict pt = argmaxr   {0,1} ldim(v (r)

t = {h     vt : h(xt) = r}

)

t

(in case of a tie predict pt = 1)
receive true label yt
update vt+1 = {h     vt : h(xt) = yt}

the following lemma formally establishes the optimality of the preceding al-

gorithm.

21.1 online classi   cation in the realizable case

293

lemma 21.7

soa enjoys the mistake bound msoa(h)     ldim(h).

proof
it su   ces to prove that whenever the algorithm makes a prediction mis-
take we have ldim(vt+1)     ldim(vt)     1. we prove this claim by assuming the
contrary, that is, ldim(vt+1) = ldim(vt). if this holds true, then the de   nition
of pt implies that ldim(v (r)
) = ldim(vt) for both r = 1 and r = 0. but, then
we can construct a shaterred tree of depth ldim(vt) + 1 for the class vt, which
leads to the desired contradiction.

t

combining lemma 21.7 and lemma 21.6 we obtain:

corollary 21.8 let h be any hypothesis class. then, the standard optimal
algorithm enjoys the mistake bound msoa(h) = ldim(h) and no other algorithm
can have ma(h) < ldim(h).

comparison to vc dimension
in the pac learning model, learnability is characterized by the vc dimension of
the class h. recall that the vc dimension of a class h is the maximal number
d such that there are instances x1, . . . , xd that are shattered by h. that is, for
any sequence of labels (y1, . . . , yd)     {0, 1}d there exists a hypothesis h     h
that gives exactly this sequence of labels. the following theorem relates the vc
dimension to the littlestone dimension.
theorem 21.9 for any class h, vcdim(h)     ldim(h), and there are classes
for which strict inequality holds. furthermore, the gap can be arbitrarily larger.
proof we    rst prove that vcdim(h)     ldim(h). suppose vcdim(h) = d and
let x1, . . . , xd be a shattered set. we now construct a complete binary tree of
instances v1, . . . , v2d   1, where all nodes at depth i are set to be xi     see the
following illustration:

x1

x2

x2

x3

x3

x3

x3

now, the de   nition of a shattered set clearly implies that we got a valid shattered
tree of depth d, and we conclude that vcdim(h)     ldim(h). to show that the
gap can be arbitrarily large simply note that the class given in example 21.4 has
vc dimension of 1 whereas its littlestone dimension is in   nite.

294

online learning

21.2

online classi   cation in the unrealizable case

in the previous section we studied online learnability in the realizable case. we
now consider the unrealizable case. similarly to the agnostic pac model, we
no longer assume that all labels are generated by some h(cid:63)     h, but we require
the learner to be competitive with the best    xed predictor from h. this is
captured by the regret of the algorithm, which measures how    sorry    the learner
is, in retrospect, not to have followed the predictions of some hypothesis h     h.
formally, the regret of an algorithm a relative to h when running on a sequence
of t examples is de   ned as

(cid:34) t(cid:88)

t=1

|pt     yt|     t(cid:88)

t=1

(cid:35)

|h(xt)     yt|

,

(21.1)

regreta(h, t ) =

sup

(x1,y1),...,(xt ,yt )

and the regret of the algorithm relative to a hypothesis class h is

regreta(h, t ) = sup
h   h

regreta(h, t ).

(21.2)
we restate the learner   s goal as having the lowest possible regret relative to h.
an interesting question is whether we can derive an algorithm with low regret,
meaning that regreta(h, t ) grows sublinearly with the number of rounds, t ,
which implies that the di   erence between the error rate of the learner and the
best hypothesis in h tends to zero as t goes to in   nity.
we    rst show that this is an impossible mission   no algorithm can obtain a
sublinear regret bound even if |h| = 2. indeed, consider h = {h0, h1}, where h0
is the function that always returns 0 and h1 is the function that always returns
1. an adversary can make the number of mistakes of any online algorithm be
equal to t , by simply waiting for the learner   s prediction and then providing
the opposite label as the true label. in contrast, for any sequence of true labels,
y1, . . . , yt , let b be the majority of labels in y1, . . . , yt , then the number of
mistakes of hb is at most t /2. therefore, the regret of any online algorithm
might be at least t     t /2 = t /2, which is not sublinear in t . this impossibility
result is attributed to cover (cover 1965).

to sidestep cover   s impossibility result, we must further restrict the power
of the adversarial environment. we do so by allowing the learner to randomize
his predictions. of course, this by itself does not circumvent cover   s impossibil-
ity result, since in deriving this result we assumed nothing about the learner   s
strategy. to make the randomization meaningful, we force the adversarial envir-
onment to decide on yt without knowing the random coins    ipped by the learner
on round t. the adversary can still know the learner   s forecasting strategy and
even the random coin    ips of previous rounds, but it does not know the actual
value of the random coin    ips used by the learner on round t. with this (mild)
change of game, we analyze the expected number of mistakes of the algorithm,
where the expectation is with respect to the learner   s own randomization. that
is, if the learner outputs   yt where p[  yt = 1] = pt, then the expected loss he pays

21.2 online classi   cation in the unrealizable case

295

on round t is

p[  yt (cid:54)= yt] = |pt     yt|.

put another way, instead of having the predictions of the learner being in {0, 1}
we allow them to be in [0, 1], and interpret pt     [0, 1] as the id203 to predict
the label 1 on round t.

with this assumption it is possible to derive a low regret algorithm. in partic-

ular, we will prove the following theorem.

theorem 21.10 for every hypothesis class h, there exists an algorithm for
online classi   cation, whose predictions come from [0, 1], that enjoys the regret
bound

|h(xt)   yt|    (cid:112)2 min{log(|h|) , ldim(h) log(et )} t .

   h     h,

t(cid:88)
(cid:16)(cid:112)ldim(h) t

|pt   yt|    t(cid:88)
(cid:17)

t=1

t=1

furthermore, no algorithm can achieve an expected regret bound smaller than
   

.

we will provide a constructive proof of the upper bound part of the preceding
theorem. the proof of the lower bound part can be found in (ben-david, pal, &
shalev-shwartz 2009).

the proof of theorem 21.10 relies on the weighted-majority algorithm for
learning with expert advice. this algorithm is important by itself and we dedicate
the next subsection to it.

21.2.1 weighted-majority

i w(t)

with (cid:80)
(cid:80)

i = 1, and choosing the ith expert with id203 w(t)

weighted-majority is an algorithm for the problem of prediction with expert ad-
vice. in this online learning problem, on round t the learner has to choose the
advice of d given experts. we also allow the learner to randomize his choice by
de   ning a distribution over the d experts, that is, picking a vector w(t)     [0, 1]d,
. after the
learner chooses an expert, it receives a vector of costs, vt     [0, 1]d, where vt,i
is the cost of following the advice of the ith expert. if the learner   s predic-
tions are randomized, then its loss is de   ned to be the averaged cost, namely,
i vt,i = (cid:104)w(t), vt(cid:105). the algorithm assumes that the number of rounds t is
given. in exercise 4 we show how to get rid of this dependence using the doubling
trick.

i w(t)

i

296

online learning

input: number of experts, d ; number of rounds, t

weighted-majority

parameter:    =(cid:112)2 log(d)/t
set w(t) =   w(t)/zt where zt =(cid:80)

initialize:   w(1) = (1, . . . , 1)
for t = 1, 2, . . .

i   w(t)
choose expert i at random according to p[i] = w(t)
receive costs of all experts vt     [0, 1]d
pay cost (cid:104)w(t), vt(cid:105)
update rule    i,   w(t+1)

=   w(t)

i e     vt,i

i

i

i

the following theorem is key for analyzing the regret bound of weighted-

majority.

theorem 21.11 assuming that t > 2 log(d), the weighted-majority algo-
rithm enjoys the bound

t=1

proof we have:

t(cid:88)

(cid:104)w(t), vt(cid:105)     min
i   [d]

vt,i     (cid:112)2 log(d) t .

t(cid:88)

t=1

(cid:88)

i

  w(t)
i
zt

e     vt,i = log

(cid:88)

i

i e     vt,i .
w(t)

log

zt+1
zt

= log

using the inequality e   a     1     a + a2/2, which holds for all a     (0, 1), and the

fact that(cid:80)

i w(t)

i = 1, we obtain
    log

log

zt+1
zt

w(t)

(cid:88)
= log(cid:0)1    (cid:88)
(cid:124)

i

i

i

(cid:0)1       vt,i +   2v2
t,i/2(cid:1)
(cid:1).
t,i/2(cid:1)
(cid:0)  vt,i       2v2
(cid:125)
(cid:123)(cid:122)

w(t)

i

def= b

next, note that b     (0, 1). therefore, taking log of the two sides of the inequality
1     b     e   b we obtain the inequality log(1     b)        b, which holds for all b     1,
and obtain

log

zt+1
zt

       (cid:88)
(cid:0)  vt,i       2v2
t,i/2(cid:1)
=       (cid:104)w(t), vt(cid:105) +   2(cid:88)

w(t)

w(t)

i

i

i v2

t,i/2

          (cid:104)w(t), vt(cid:105) +   2/2.

i

21.2 online classi   cation in the unrealizable case

297

summing this inequality over t we get

log(zt +1)     log(z1) =

log

zt+1
zt

         

t(cid:88)

t=1

t(cid:88)

(cid:104)w(t), vt(cid:105) +

t=1

.

(21.3)

t vt,i and

t   2

2

= e     (cid:80)
(cid:88)

i

t

next, we lower bound zt +1. for each i, we can rewrite   w(t +1)
we get that

i

(cid:33)

(cid:32)(cid:88)

i

e     (cid:80)

(cid:16)

(cid:17)

e     (cid:80)

log zt +1 = log

t vt,i

    log

max

i

t vt,i

=       min

vt,i.

combining the preceding with equation (21.3) and using the fact that log(z1) =
log(d) we get that

(cid:88)

      min

i

t(cid:88)

vt,i     log(d)           

(cid:104)w(t), vt(cid:105) +

t

t=1

t   2

2

,

which can be rearranged as follows:

t(cid:88)

t=1

(cid:104)w(t), vt(cid:105)     min

i

(cid:88)

t

vt,i     log(d)

  

+

   t
2

.

plugging the value of    into the equation concludes our proof.

proof of theorem 21.10

equipped with the weighted-majority algorithm and theorem 21.11, we are
ready to prove theorem 21.10. we start with the simpler case, in which h is
a    nite class, and let us write h = {h1, . . . , hd}. in this case, we can refer to
each hypothesis, hi, as an expert, whose advice is to predict hi(xt), and whose
cost is vt,i = |hi(xt)     yt|. the prediction of the algorithm will therefore be

i hi(xt)     [0, 1], and the loss is

i w(t)

pt =(cid:80)

|pt     yt| =

i hi(xt)     yt
w(t)

i (hi(xt)     yt)
w(t)

(cid:80)
equals(cid:80)

i w(t)

now, if yt = 1, then for all i, hi(xt)     yt     0. therefore, the above equals to
|hi(xt)     yt|. if yt = 0 then for all i, hi(xt)     yt     0, and the above also
i w(t)

|hi(xt)     yt|. all in all, we have shown that

i

i

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) d(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .

d(cid:88)

i=1

|pt     yt| =

furthermore, for each i,(cid:80)

|hi(xt)     yt| = (cid:104)w(t), vt(cid:105).

w(t)

i

makes. applying theorem 21.11 we obtain

t vt,i is exactly the number of mistakes hypothesis hi

298

online learning

corollary 21.12 let h be a    nite hypothesis class. there exists an algorithm
for online classi   cation, whose predictions come from [0, 1], that enjoys the regret
bound

t(cid:88)

t=1

|pt     yt|     min
h   h

|h(xt)     yt|     (cid:112)2 log(|h|) t .

t(cid:88)

t=1

next, we consider the case of a general hypothesis class. previously, we con-
structed an expert for each individual hypothesis. however, if h is in   nite this
leads to a vacuous bound. the main idea is to construct a set of experts in a
more sophisticated way. the challenge is how to de   ne a set of experts that, on
one hand, is not excessively large and, on the other hand, contains experts that
give accurate predictions.

we construct the set of experts so that for each hypothesis h     h and every
sequence of instances, x1, x2, . . . , xt , there exists at least one expert in the set
which behaves exactly as h on these instances. for each l     ldim(h) and each
sequence 1     i1 < i2 <        < il     t we de   ne an expert. the expert simulates
the game between soa (presented in the previous section) and the environment
on the sequence of instances x1, x2, . . . , xt assuming that soa makes a mistake
precisely in rounds i1, i2, . . . , il. the expert is de   ned by the following algorithm.

expert(i1, i2, . . . , il)

input a hypothesis class h ; indices i1 < i2 <        < il
initialize: v1 = h
for t = 1, 2, . . . , t

receive xt
for r     {0, 1} let v (r)
de   ne   yt = argmaxr ldim

(cid:17)

(cid:16)

v (r)
t

t = {h     vt : h(xt) = r}

if

(in case of a tie set   yt = 0)

t     {i1, i2, . . . , il}

predict   yt = 1       yt

else

predict   yt =   yt

update vt+1 = v (  yt)

t

note that each such expert can give us predictions at every round t while only
observing the instances x1, . . . , xt. our generic online learning algorithm is now
an application of the weighted-majority algorithm with these experts.

to analyze the algorithm we    rst note that the number of experts is

ldim(h)(cid:88)

(cid:18)t

(cid:19)

l

l=0

d =

.

(21.4)

it can be shown that when t     ldim(h) + 2, the right-hand side of the equation
is bounded by (et /ldim(h))ldim(h) (the proof can be found in lemma a.5).

21.2 online classi   cation in the unrealizable case

299

is at most the number of mistakes of the best expert plus(cid:112)2 log(d) t . we will

theorem 21.11 tells us that the expected number of mistakes of weighted-majority

next show that the number of mistakes of the best expert is at most the number
of mistakes of the best hypothesis in h. the following key lemma shows that,
on any sequence of instances, for each hypothesis h     h there exists an expert
with the same behavior.

lemma 21.13 let h be any hypothesis class with ldim(h) <    . let x1, x2, . . . , xt
be any sequence of instances. for any h     h, there exists l     ldim(h) and in-
dices 1     i1 < i2 <        < il     t such that when running expert(i1, i2, . . . , il)
on the sequence x1, x2, . . . , xt , the expert predicts h(xt) on each online round
t = 1, 2, . . . , t .

proof fix h     h and the sequence x1, x2, . . . , xt . we must construct l and the
indices i1, i2, . . . , il. consider running soa on the input (x1, h(x1)), (x2, h(x2)),
. . ., (xt , h(xt )). soa makes at most ldim(h) mistakes on such input. we de   ne
l to be the number of mistakes made by soa and we de   ne {i1, i2, . . . , il} to
be the set of rounds in which soa made the mistakes.

now, consider the expert(i1, i2, . . . , il) running on the sequence x1, x2, . . . , xt .
by construction, the set vt maintained by expert(i1, i2, . . . , il) equals the set vt
maintained by soa when running on the sequence (x1, h(x1)), . . . , (xt , h(xt )).
the predictions of soa di   er from the predictions of h if and only if the round is
in {i1, i2, . . . , il}. since expert(i1, i2, . . . , il) predicts exactly like soa if t is not
in {i1, i2, . . . , il} and the opposite of soas    predictions if t is in {i1, i2, . . . , il},
we conclude that the predictions of the expert are always the same as the pre-
dictions of h.

the previous lemma holds in particular for the hypothesis in h that makes the
least number of mistakes on the sequence of examples, and we therefore obtain
the following:

corollary 21.14 let (x1, y1), (x2, y2), . . . , (xt , yt ) be a sequence of examples
and let h be a hypothesis class with ldim(h) <    . there exists l     ldim(h)
and indices 1     i1 < i2 <        < il     t , such that expert(i1, i2, . . . , il) makes
at most as many mistakes as the best h     h does, namely,

t(cid:88)

t=1

min
h   h

|h(xt)     yt|

mistakes on the sequence of examples.

together with theorem 21.11, the upper bound part of theorem 21.10 is

proven.

300

online learning

21.3

online id76

in chapter 12 we studied convex learning problems and showed learnability
results for these problems in the agnostic pac learning framework. in this section
we show that similar learnability results hold for convex problems in the online
learning framework. in particular, we consider the following problem.

online id76

id168 (cid:96) : h    z     r

de   nitions:
hypothesis class h ; domain z ;
assumptions:
h is convex
   z     z, (cid:96)(  , z) is a convex function
for t = 1, 2, . . . , t
learner predicts a vector w(t)     h
environment responds with zt     z
learner su   ers loss (cid:96)(w(t), zt)

t(cid:88)

(cid:96)(w(t), zt)     t(cid:88)

as in the online classi   cation problem, we analyze the regret of the algorithm.
recall that the regret of an online algorithm with respect to a competing hy-
pothesis, which here will be some vector w(cid:63)     h, is de   ned as

regreta(w(cid:63), t ) =

(cid:96)(w(cid:63), zt).

(21.5)

as before, the regret of the algorithm relative to a set of competing vectors, h,
is de   ned as

t=1

t=1

regreta(h, t ) = sup
w(cid:63)   h

regreta(w(cid:63), t ).

in chapter 14 we have shown that stochastic id119 solves convex
learning problems in the agnostic pac model. we now show that a very similar
algorithm, online id119, solves online convex learning problems.

online id119

parameter:    > 0
initialize: w(1) = 0
for t = 1, 2, . . . , t
predict w(t)
receive zt and let ft(  ) = (cid:96)(  , zt)
choose vt        ft(w(t))
update:
1. w(t+ 1
2. w(t+1) = argminw   h (cid:107)w     w(t+ 1

2 ) = w(t)       vt

2 )(cid:107)

21.4 the online id88 algorithm

301

theorem 21.15 the online id119 algorithm enjoys the following
regret bound for every w(cid:63)     h,

if we further assume that ft is   -lipschitz for all t, then setting    = 1/

   

t yields

regreta(w(cid:63), t )     (cid:107)w(cid:63)(cid:107)2

2  

+

  
2

(cid:107)vt(cid:107)2.

t(cid:88)

t=1

regreta(w(cid:63), t )     1
2

((cid:107)w(cid:63)(cid:107)2 +   2)

t .

   

if we further assume that h is b-bounded and we set    = b
   
  

t

then

regreta(h, t )     b   

   

t .

proof the analysis is similar to the analysis of stochastic id119
with projections. using the projection lemma, the de   nition of w(t+ 1
2 ), and the
de   nition of subgradients, we have that for every t,

2 )     w(cid:63)(cid:107)2 + (cid:107)w(t+ 1

2 )     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2

2 )     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2

(cid:107)w(t+1)     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2
= (cid:107)w(t+1)     w(cid:63)(cid:107)2     (cid:107)w(t+ 1
    (cid:107)w(t+ 1
= (cid:107)w(t)       vt     w(cid:63)(cid:107)2     (cid:107)w(t)     w(cid:63)(cid:107)2
=    2  (cid:104)w(t)     w(cid:63), vt(cid:105) +   2(cid:107)vt(cid:107)2
       2  (ft(w(t))     ft(w(cid:63))) +   2(cid:107)vt(cid:107)2.

summing over t and observing that the left-hand side is a telescopic sum we
obtain that
(cid:107)w(t +1)     w(cid:63)(cid:107)2     (cid:107)w(1)     w(cid:63)(cid:107)2        2  

(ft(w(t))     ft(w(cid:63))) +   2

t(cid:88)

t(cid:88)

(cid:107)vt(cid:107)2.

t=1

t=1

rearranging the inequality and using the fact that w(1) = 0, we get that

t(cid:88)
(ft(w(t))     ft(w(cid:63)))     (cid:107)w(1)     w(cid:63)(cid:107)2     (cid:107)w(t +1)     w(cid:63)(cid:107)2

t(cid:88)

(cid:107)vt(cid:107)2

+

  
2

t=1

t=1

    (cid:107)w(cid:63)(cid:107)2

2  

+

  
2

2  

(cid:107)vt(cid:107)2.

t(cid:88)

t=1

this proves the    rst bound in the theorem. the second bound follows from the
assumption that ft is   -lipschitz, which implies that (cid:107)vt(cid:107)       .

21.4

the online id88 algorithm

the id88 is a classic online learning algorithm for binary classi   cation with
the hypothesis class of homogenous halfspaces, namely, h = {x (cid:55)    sign((cid:104)w, x(cid:105)) :

302

online learning

w     rd}. in section 9.1.2 we have presented the batch version of the id88,
which aims to solve the erm problem with respect to h. we now present an
online version of the id88 algorithm.
let x = rd, y = {   1, 1}. on round t, the learner receives a vector xt     rd.
the learner maintains a weight vector w(t)     rd and predicts pt = sign((cid:104)w(t), xt(cid:105)).
then, it receives yt     y and pays 1 if pt (cid:54)= yt and 0 otherwise.

the goal of the learner is to make as few prediction mistakes as possible. in
section 21.1 we characterized the optimal algorithm and showed that the best
achievable mistake bound depends on the littlestone dimension of the class.
we show later that if d     2 then ldim(h) =    , which implies that we have
no hope of making few prediction mistakes. indeed, consider the tree for which
v1 = ( 1
4 , 1, 0, . . . , 0), etc. because of
the density of the reals, this tree is shattered by the subset of h which contains
all hypotheses that are parametrized by w of the form w = (   1, a, 0, . . . , 0), for
a     [0, 1]. we conclude that indeed ldim(h) =    .

4 , 1, 0, . . . , 0), v3 = ( 3

2 , 1, 0, . . . , 0), v2 = ( 1

to sidestep this impossibility result, the id88 algorithm relies on the
technique of surrogate convex losses (see section 12.3). this is also closely related
to the notion of margin we studied in chapter 15.

a weight vector w makes a mistake on an example (x, y) whenever the sign of
(cid:104)w, x(cid:105) does not equal y. therefore, we can write the 0   1 id168 as follows

(cid:96)(w, (x, y)) = 1[y(cid:104)w,x(cid:105)   0].

on rounds on which the algorithm makes a prediction mistake, we shall use the
hinge-loss as a surrogate convex id168

ft(w) = max{0, 1     yt(cid:104)w, xt(cid:105)}.

the hinge-loss satis   es the two conditions:
    ft is a convex function
    for all w, ft(w)     (cid:96)(w, (xt, yt)). in particular, this holds for w(t).

on rounds on which the algorithm is correct, we shall de   ne ft(w) = 0. clearly,
ft is convex in this case as well. furthermore, ft(w(t)) = (cid:96)(w(t), (xt, yt)) = 0.
remark 21.5
in section 12.3 we used the same surrogate id168 for all the
examples. in the online model, we allow the surrogate to depend on the speci   c
round. it can even depend on w(t). our ability to use a round speci   c surrogate
stems from the worst-case type of analysis we employ in online learning.

let us now run the online id119 algorithm on the sequence of
functions, f1, . . . , ft , with the hypothesis class being all vectors in rd (hence,
the projection step is vacuous). recall that the algorithm initializes w(1) = 0
and its update rule is

w(t+1) = w(t)       vt

for some vt        ft(w(t)). in our case, if yt(cid:104)w(t), xt(cid:105) > 0 then ft is the zero

21.4 the online id88 algorithm

303

function and we can take vt = 0. otherwise, it is easy to verify that vt =    ytxt
is in    ft(w(t)). we therefore obtain the update rule

(cid:40)

w(t+1) =

w(t)
w(t) +   ytxt

if yt(cid:104)w(t), xt(cid:105) > 0
otherwise

denote by m the set of rounds in which sign((cid:104)w(t), xt(cid:105)) (cid:54)= yt. note that on
round t, the prediction of the id88 can be rewritten as

pt = sign((cid:104)w(t), xt(cid:105)) = sign

  

yi (cid:104)xi, xt(cid:105)

.

(cid:32)

(cid:88)

i   m:i<t

(cid:33)

this form implies that the predictions of the id88 algorithm and the set
m do not depend on the actual value of    as long as    > 0. we have therefore
obtained the id88 algorithm:

id88

initialize: w1 = 0
for t = 1, 2, . . . , t
receive xt
predict pt = sign((cid:104)w(t), xt(cid:105))
if yt(cid:104)w(t), xt(cid:105)     0
w(t+1) = w(t) + ytxt

else

w(t+1) = w(t)

to analyze the id88, we rely on the analysis of online gradient de-
scent given in the previous section. in our case, the subgradient of ft we use
in the id88 is vt =    1
[yt(cid:104)w(t),xt(cid:105)   0] yt xt. indeed, the id88   s update
is w(t+1) = w(t)     vt, and as discussed before this is equivalent to w(t+1) =
w(t)       vt for every    > 0. therefore, theorem 21.15 tells us that

t(cid:88)

ft(w(t))     t(cid:88)

t(cid:88)
since ft(w(t)) is a surrogate for the 0   1 loss we know that(cid:80)t

ft(w(cid:63))     1
2  

(cid:107)w(cid:63)(cid:107)2

2 +

  
2

t=1

t=1

t=1

t=1 ft(w(t))     |m|.

(cid:107)vt(cid:107)2
2.

denote r = maxt (cid:107)xt(cid:107); then we obtain

|m|     t(cid:88)

t=1

ft(w(cid:63))     1
2  

(cid:107)w(cid:63)(cid:107)2

2 +

  
2

|m| r2

setting    =

   
(cid:107)w(cid:63)(cid:107)
r

|m| and rearranging, we obtain

|m|     r(cid:107)w(cid:63)(cid:107)(cid:112)|m|     t(cid:88)

this inequality implies

t=1

ft(w(cid:63))     0.

(21.6)

304

online learning

theorem 21.16
suppose that the id88 algorithm runs on a sequence
(x1, y1), . . . , (xt , yt ) and let r = maxt (cid:107)xt(cid:107). let m be the rounds on which the
id88 errs and let ft(w) = 1[t   m] [1     yt(cid:104)w, xt(cid:105)]+. then, for every w(cid:63)

|m|     (cid:88)

(cid:115)(cid:88)

ft(w(cid:63)) + r (cid:107)w(cid:63)(cid:107)

ft(w(cid:63)) + r2 (cid:107)w(cid:63)(cid:107)2 .

t

t

in particular, if there exists w(cid:63) such that yt(cid:104)w(cid:63), xt(cid:105)     1 for all t then

|m|     r2 (cid:107)w(cid:63)(cid:107)2.

proof the theorem follows from equation (21.6) and the following claim: given
x, b, c     r+, the inequality x     b
c. the
last claim can be easily derived by analyzing the roots of the convex parabola
q(y) = y2     by     c.

x     c     0 implies that x     c + b2 + b

   

   

the last assumption of theorem 21.16 is called separability with large margin
(see chapter 15). that is, there exists w(cid:63) that not only satis   es that the point
xt lies on the correct side of the halfspace, it also guarantees that xt is not too
close to the decision boundary. more speci   cally, the distance from xt to the
decision boundary is at least    = 1/(cid:107)w(cid:63)(cid:107) and the bound becomes (r/  )2.
when the separability assumption does not hold, the bound involves the term
[1     yt(cid:104)w(cid:63), xt(cid:105)]+ which measures how much the separability with margin require-
ment is violated.

as a last remark we note that there can be cases in which there exists some
w(cid:63) that makes zero errors on the sequence but the id88 will make many
errors. indeed, this is a direct consequence of the fact that ldim(h) =    . the
way we sidestep this impossibility result is by assuming more on the sequence of
examples     the bound in theorem 21.16 will be meaningful only if the cumulative

surrogate loss,(cid:80)

t ft(w(cid:63)) is not excessively large.

21.5

summary

in this chapter we have studied the online learning model. many of the results
we derived for the pac learning model have an analog in the online model. first,
we have shown that a combinatorial dimension, the littlestone dimension, char-
acterizes online learnability. to show this, we introduced the soa algorithm (for
the realizable case) and the weighted-majority algorithm (for the unrealizable
case). we have also studied online id76 and have shown that
online id119 is a successful online learner whenever the id168
is convex and lipschitz. finally, we presented the online id88 algorithm
as a combination of online id119 and the concept of surrogate convex
id168s.

21.6 bibliographic remarks

305

21.6

bibliographic remarks

the standard optimal algorithm was derived by the seminal work of lit-
tlestone (1988). a generalization to the nonrealizable case, as well as other
variants like margin-based littlestone   s dimension, were derived in (ben-david
et al. 2009). characterizations of online learnability beyond classi   cation have
been obtained in (abernethy, bartlett, rakhlin & tewari 2008, rakhlin, srid-
haran & tewari 2010, daniely et al. 2011). the weighted-majority algorithm is
due to (littlestone & warmuth 1994) and (vovk 1990).

the term    online convex programming    was introduced by zinkevich (2003)
but this setting was introduced some years earlier by gordon (1999). the per-
ceptron dates back to rosenblatt (rosenblatt 1958). an analysis for the re-
alizable case (with margin assumptions) appears in (agmon 1954, minsky &
papert 1969). freund and schapire (freund & schapire 1999) presented an anal-
ysis for the unrealizable case with a squared-hinge-loss based on a reduction to
the realizable case. a direct analysis for the unrealizable case with the hinge-loss
was given by gentile (gentile 2003).

for additional information we refer the reader to cesa-bianchi & lugosi (2006)

and shalev-shwartz (2011).

21.7

exercises

1. find a hypothesis class h and a sequence of examples on which consistent

makes |h|     1 mistakes.

2. find a hypothesis class h and a sequence of examples on which the mistake

bound of the halving algorithm is tight.

3. let d     2, x = {1, . . . , d} and let h = {hj : j     [d]}, where hj(x) = 1[x=j].
calculate mhalving(h) (i.e., derive lower and upper bounds on mhalving(h),
and prove that they are equal).

4. the doubling trick:

in theorem 21.15, the parameter    depends on the time horizon t . in this

   
exercise we show how to get rid of this dependence by a simple trick.
consider an algorithm that enjoys a regret bound of the form   

t , but
its parameters require the knowledge of t . the doubling trick, described in
the following, enables us to convert such an algorithm into an algorithm that
does not need to know the time horizon. the idea is to divide the time into
periods of increasing size and run the original algorithm on each period.

the doubling trick

input: algorithm a whose parameters depend on the time horizon
for m = 0, 1, 2, . . .
run a on the 2m rounds t = 2m, . . . , 2m+1     1

306

online learning

show that if the regret of a on each period of 2m rounds is at most   
then the total regret is at most    
2   
2     1

   

t .

  

   

2m,

5. online-to-batch conversions: in this exercise we demonstrate how a suc-
cessful online learning algorithm can be used to derive a successful pac
learner as well.
consider a pac learning problem for binary classi   cation parameterized
by an instance domain, x , and a hypothesis class, h. suppose that there exists
an online learning algorithm, a, which enjoys a mistake bound ma(h) <    .
consider running this algorithm on a sequence of t examples which are sam-
pled i.i.d. from a distribution d over the instance space x , and are labeled by
some h(cid:63)     h. suppose that for every round t, the prediction of the algorithm
is based on a hypothesis ht : x     {0, 1}. show that

e[ld(hr)]     ma(h)

,

t

where the expectation is over the random choice of the instances as well as a
random choice of r according to the uniform distribution over [t ].
hint: use similar arguments to the ones appearing in the proof of theo-
rem 14.8.

22 id91

id91 is one of the most widely used techniques for exploratory data anal-
ysis. across all disciplines, from social sciences to biology to computer science,
people try to get a    rst intuition about their data by identifying meaningful
groups among the data points. for example, computational biologists cluster
genes on the basis of similarities in their expression in di   erent experiments; re-
tailers cluster customers, on the basis of their customer pro   les, for the purpose
of targeted marketing; and astronomers cluster stars on the basis of their spacial
proximity.

the    rst point that one should clarify is, naturally, what is id91? in-
tuitively, id91 is the task of grouping a set of objects such that similar
objects end up in the same group and dissimilar objects are separated into dif-
ferent groups. clearly, this description is quite imprecise and possibly ambiguous.
quite surprisingly, it is not at all clear how to come up with a more rigorous
de   nition.

there are several sources for this di   culty. one basic problem is that the
two objectives mentioned in the earlier statement may in many cases contradict
each other. mathematically speaking, similarity (or proximity) is not a transi-
tive relation, while cluster sharing is an equivalence relation and, in particular,
it is a transitive relation. more concretely, it may be the case that there is a
long sequence of objects, x1, . . . , xm such that each xi is very similar to its two
neighbors, xi   1 and xi+1, but x1 and xm are very dissimilar. if we wish to make
sure that whenever two elements are similar they share the same cluster, then
we must put all of the elements of the sequence in the same cluster. however,
in that case, we end up with dissimilar elements (x1 and xm) sharing a cluster,
thus violating the second requirement.

to illustrate this point further, suppose that we would like to cluster the points

in the following picture into two clusters.

a id91 algorithm that emphasizes not separating close-by points (e.g., the
single linkage algorithm that will be described in section 22.1) will cluster this
input by separating it horizontally according to the two lines:

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

308

id91

in contrast, a id91 method that emphasizes not having far-away points
share the same cluster (e.g., the 2-means algorithm that will be described in
section 22.1) will cluster the same input by dividing it vertically into the right-
hand half and the left-hand half:

another basic problem is the lack of    ground truth    for id91, which is a
common problem in unsupervised learning. so far in the book, we have mainly
dealt with supervised learning (e.g., the problem of learning a classi   er from
labeled training data). the goal of supervised learning is clear     we wish to
learn a classi   er which will predict the labels of future examples as accurately
as possible. furthermore, a supervised learner can estimate the success, or the
risk, of its hypotheses using the labeled training data by computing the empirical
loss. in contrast, id91 is an unsupervised learning problem; namely, there
are no labels that we try to predict. instead, we wish to organize the data in
some meaningful way. as a result, there is no clear success evaluation procedure
for id91. in fact, even on the basis of full knowledge of the underlying data
distribution, it is not clear what is the    correct    id91 for that data or how
to evaluate a proposed id91.

consider, for example, the following set of points in r2:

and suppose we are required to cluster them into two clusters. we have two
highly justi   able solutions:

id91

309

this phenomenon is not just arti   cial but occurs in real applications. a given
set of objects can be clustered in various di   erent meaningful ways. this may
be due to having di   erent implicit notions of distance (or similarity) between
objects, for example, id91 recordings of speech by the accent of the speaker
versus id91 them by content, id91 movie reviews by movie topic versus
id91 them by the review sentiment, id91 paintings by topic versus
id91 them by style, and so on.

to summarize, there may be several very di   erent conceivable id91 so-
lutions for a given data set. as a result, there is a wide variety of id91
algorithms that, on some input data, will output very di   erent id91s.

a id91 model:
id91 tasks can vary in terms of both the type of input they have and the
type of outcome they are expected to compute. for concreteness, we shall focus
on the following common setup:

where(cid:83)k

input     a set of elements, x , and a distance function over it. that is, a function
d : x    x     r+ that is symmetric, satis   es d(x, x) = 0 for all x     x
and often also satis   es the triangle inequality. alternatively, the function
could be a similarity function s : x    x     [0, 1] that is symmetric
and satis   es s(x, x) = 1 for all x     x . additionally, some id91
algorithms also require an input parameter k (determining the number
of required clusters).

output     a partition of the domain set x into subsets. that is, c = (c1, . . . ck)
i=1 ci = x and for all i (cid:54)= j, ci     cj =    . in some situations the
id91 is    soft,    namely, the partition of x into the di   erent clusters
is probabilistic where the output is a function assigning to each domain
point, x     x , a vector (p1(x), . . . , pk(x)), where pi(x) = p[x     ci] is
the id203 that x belongs to cluster ci. another possible output is
a id91 dendrogram (from greek dendron = tree, gramma = draw-
ing), which is a hierarchical tree of domain subsets, having the singleton
sets in its leaves, and the full domain as its root. we shall discuss this
formulation in more detail in the following.

310

id91

in the following we survey some of the most popular id91 methods. in
the last section of this chapter we return to the high level discussion of what is
id91.

22.1

linkage-based id91 algorithms

linkage-based id91 is probably the simplest and most straightforward paradigm
of id91. these algorithms proceed in a sequence of rounds. they start from
the trivial id91 that has each data point as a single-point cluster. then,
repeatedly, these algorithms merge the    closest    clusters of the previous cluster-
ing. consequently, the number of clusters decreases with each such round. if kept
going, such algorithms would eventually result in the trivial id91 in which
all of the domain points share one large cluster. two parameters, then, need to
be determined to de   ne such an algorithm clearly. first, we have to decide how
to measure (or de   ne) the distance between clusters, and, second, we have to
determine when to stop merging. recall that the input to a id91 algorithm
is a between-points distance function, d. there are many ways of extending d to
a measure of distance between domain subsets (or clusters). the most common
ways are

1. single linkage id91, in which the between-clusters distance is de   ned

by the minimum distance between members of the two clusters, namely,

d(a, b) def= min{d(x, y) : x     a, y     b}

2. average linkage id91, in which the distance between two clusters is
de   ned to be the average distance between a point in one of the clusters and
a point in the other, namely,

(cid:88)

d(a, b) def=

1

|a||b|

d(x, y)

x   a, y   b

3. max linkage id91, in which the distance between two clusters is de   ned

as the maximum distance between their elements, namely,
d(a, b) def= max{d(x, y) : x     a, y     b}.

the linkage-based id91 algorithms are agglomerative in the sense that they
start from data that is completely fragmented and keep building larger and
larger clusters as they proceed. without employing a stopping rule, the outcome
of such an algorithm can be described by a id91 dendrogram: that is, a tree
of domain subsets, having the singleton sets in its leaves, and the full domain as
its root. for example, if the input is the elements x = {a, b, c, d, e}     r2 with
the euclidean distance as depicted on the left, then the resulting dendrogram is
the one depicted on the right:

22.2 id116 and other cost minimization id91s

311

a

e

d

c

b

{a, b, c, d, e}

{b, c, d, e}

{b, c}

{d, e}

{a}

{b}

{c}

{d}

{e}

the single linkage algorithm is closely related to kruskal   s algorithm for    nding
a minimal spanning tree on a weighted graph. indeed, consider the full graph
whose vertices are elements of x and the weight of an edge (x, y) is the distance
d(x, y). each merge of two clusters performed by the single linkage algorithm
corresponds to a choice of an edge in the aforementioned graph. it is also possible
to show that the set of edges the single linkage algorithm chooses along its run
forms a minimal spanning tree.

if one wishes to turn a dendrogram into a partition of the space (a id91),

one needs to employ a stopping criterion. common stopping criteria include
    fixed number of clusters        x some parameter, k, and stop merging clusters

as soon as the number of clusters is k.

    distance upper bound        x some r     r+. stop merging as soon as all the
between-clusters distances are larger than r. we can also set r to be
   max{d(x, y) : x, y     x} for some    < 1. in that case the stopping
criterion is called    scaled distance upper bound.   

22.2

id116 and other cost minimization id91s

another popular approach to id91 starts by de   ning a cost function over a
parameterized set of possible id91s and the goal of the id91 algorithm
is to    nd a partitioning (id91) of minimal cost. under this paradigm, the
id91 task is turned into an optimization problem. the objective function
is a function from pairs of an input, (x , d), and a proposed id91 solution
c = (c1, . . . , ck), to positive real numbers. given such an objective function,
which we denote by g, the goal of a id91 algorithm is de   ned as    nding, for
a given input (x , d), a id91 c so that g((x , d), c) is minimized. in order
to reach that goal, one has to apply some appropriate search algorithm.

as it turns out, most of the resulting optimization problems are np-hard, and
some are even np-hard to approximate. consequently, when people talk about,
say, id116 id91, they often refer to some particular common approxima-
tion algorithm rather than the cost function or the corresponding exact solution
of the minimization problem.

many common objective functions require the number of clusters, k, as a

312

id91

parameter. in practice, it is often up to the user of the id91 algorithm to
choose the parameter k that is most suitable for the given id91 problem.
in the following we describe some of the most common objective functions.

k(cid:88)

i=1

x   ci

(cid:88)
k(cid:88)

(cid:88)

i=1

x   ci

    the id116 objective function is one of the most popular id91
objectives. in id116 the data is partitioned into disjoint sets c1, . . . , ck
where each ci is represented by a centroid   i. it is assumed that the input
set x is embedded in some larger metric space (x (cid:48), d) (so that x     x (cid:48))
and centroids are members of x (cid:48). the id116 objective function measures
the squared distance between each point in x to the centroid of its cluster.
the centroid of ci is de   ned to be

(cid:88)

x   ci

  i(ci) = argmin
     x (cid:48)

d(x,   )2.

then, the id116 objective is

gk   means((x , d), (c1, . . . , ck)) =

d(x,   i(ci))2.

this can also be rewritten as

gk   means((x , d), (c1, . . . , ck)) = min

  1,...  k   x (cid:48)

d(x,   i)2.

(22.1)

the id116 objective function is relevant, for example, in digital com-
munication tasks, where the members of x may be viewed as a collection
of signals that have to be transmitted. while x may be a very large set
of real valued vectors, digital transmission allows transmitting of only a
   nite number of bits for each signal. one way to achieve good transmis-
sion under such constraints is to represent each member of x by a    close   
member of some    nite set   1, . . .   k, and replace the transmission of any
x     x by transmitting the index of the closest   i. the id116 objective
can be viewed as a measure of the distortion created by such a transmission
representation scheme.
    the k-medoids objective function is similar to the id116 objective,
except that it requires the cluster centroids to be members of the input
set. the objective function is de   ned by

gk   medoid((x , d), (c1, . . . , ck)) = min

  1,...  k   x

d(x,   i)2.

    the k-median objective function is quite similar to the k-medoids objec-
tive, except that the    distortion    between a data point and the centroid
of its cluster is measured by distance, rather than by the square of the
distance:

gk   median((x , d), (c1, . . . , ck)) = min

  1,...  k   x

d(x,   i).

k(cid:88)

(cid:88)

i=1

x   ci

k(cid:88)

(cid:88)

i=1

x   ci

22.2 id116 and other cost minimization id91s

313

an example where such an objective makes sense is the facility location
problem. consider the task of locating k    re stations in a city. one can
model houses as data points and aim to place the stations so as to minimize
the average distance between a house and its closest    re station.

the previous examples can all be viewed as center-based objectives. the so-
lution to such a id91 problem is determined by a set of cluster centers,
and the id91 assigns each instance to the center closest to it. more gener-
ally, center-based objective is determined by choosing some monotonic function
f : r+     r+ and then de   ning

gf ((x , d), (c1, . . . ck)) = min

  1,...  k   x (cid:48)

where x (cid:48) is either x or some superset of x .

k(cid:88)

(cid:88)

i=1

x   ci

f (d(x,   i)),

some objective functions are not center based. for example, the sum of in-

cluster distances (sod)

gsod((x , d), (c1, . . . ck)) =

k(cid:88)

(cid:88)

i=1

x,y   ci

d(x, y)

and the mincut objective that we shall discuss in section 22.3 are not center-
based objectives.

22.2.1

the id116 algorithm

the id116 objective function is quite popular in practical applications of clus-
tering. however, it turns out that    nding the optimal id116 solution is of-
ten computationally infeasible (the problem is np-hard, and even np-hard to
approximate to within some constant). as an alternative, the following simple
iterative algorithm is often used, so often that, in many cases, the term id116
id91 refers to the outcome of this algorithm rather than to the cluster-
ing that minimizes the id116 objective cost. we describe the algorithm with
respect to the euclidean distance function d(x, y) = (cid:107)x     y(cid:107).

id116

input: x     rn ; number of clusters k
initialize: randomly choose initial centroids   1, . . . ,   k
repeat until convergence
   i     [k] set ci = {x     x : i = argminj (cid:107)x       j(cid:107)}
(break ties in some arbitrary manner)
   i     [k] update   i = 1|ci|

(cid:80)

x   ci

x

lemma 22.1 each iteration of the id116 algorithm does not increase the
id116 objective function (as given in equation (22.1)).

314

id91

proof to simplify the notation, let us use the shorthand g(c1, . . . , ck) for the
id116 objective, namely,

g(c1, . . . , ck) =

min

  1,...,  k   rn

(cid:107)x       i(cid:107)2.

(22.2)

it is convenient to de   ne   (ci) = 1|ci|
  (cid:107)2. therefore, we can rewrite the id116 objective as

x   ci

x and note that   (ci) = argmin     rn

(cid:80)

x   ci

(cid:107)x   

k(cid:88)

(cid:88)

i=1

x   ci

(cid:80)
k(cid:88)

(cid:88)

i=1

x   ci

g(c1, . . . , ck) =

(cid:107)x       (ci)(cid:107)2.

(22.3)

, . . . , c (t   1)
consider the update at iteration t of the id116 algorithm. let c (t   1)
be the previous partition, let   (t   1)
k be the
new partition assigned at iteration t. using the de   nition of the objective as
given in equation (22.2) we clearly have that

=   (c (t   1)

), and let c (t)

1 , . . . , c (t)

k

1

i

i

(cid:88)

x   c(t)

i

k )     k(cid:88)
(cid:80)

i=1

x   ci

g(c (t)

1 , . . . , c (t)

(cid:107)x       (t   1)

i

(cid:107)2.

(22.4)

in addition, the de   nition of the new partition (c (t)

minimizes the expression(cid:80)k

i=1

(cid:107)x       (t   1)

i

1 , . . . , c (t)
k ) implies that it
(cid:107)2 over all possible partitions

(c1, . . . , ck). hence,

k(cid:88)

(cid:88)

i=1

x   c(t)

i

(cid:107)x       (t   1)

i

(cid:107)2     k(cid:88)

(cid:88)

i=1

x   c(t   1)

i

(cid:107)x       (t   1)

i

(cid:107)2.

(22.5)

1

, . . . , c (t   1)

using equation (22.3) we have that the right-hand side of equation (22.5) equals
g(c (t   1)
). combining this with equation (22.4) and equation (22.5),
we obtain that g(c (t)
1 , . . . , c (t)
), which concludes our
proof.

k )     g(c (t   1)

, . . . , c (t   1)

k

k

1

while the preceding lemma tells us that the id116 objective is monotonically
nonincreasing, there is no guarantee on the number of iterations the id116 al-
gorithm needs in order to reach convergence. furthermore, there is no nontrivial
lower bound on the gap between the value of the id116 objective of the al-
gorithm   s output and the minimum possible value of that objective function. in
fact, id116 might converge to a point which is not even a local minimum (see
exercise 2). to improve the results of id116 it is often recommended to repeat
the procedure several times with di   erent randomly chosen initial centroids (e.g.,
we can choose the initial centroids to be random points from the data).

22.3 spectral id91

315

22.3

spectral id91

often, a convenient way to represent the relationships between points in a data
set x = {x1, . . . , xm} is by a similarity graph; each vertex represents a data
point xi, and every two vertices are connected by an edge whose weight is their
similarity, wi,j = s(xi, xj), where w     rm,m. for example, we can set wi,j =
exp(   d(xi, xj)2/  2), where d(  ,  ) is a distance function and    is a parameter.
the id91 problem can now be formulated as follows: we want to    nd a
partition of the graph such that the edges between di   erent groups have low
weights and the edges within a group have high weights.

in the id91 objectives described previously, the focus was on one side
of our intuitive de   nition of id91     making sure that points in the same
cluster are similar. we now present objectives that focus on the other requirement
    points separated into di   erent clusters should be nonsimilar.

22.3.1

graph cut

given a graph represented by a similarity matrix w , the simplest and most
direct way to construct a partition of the graph is to solve the mincut problem,
which chooses a partition c1, . . . , ck that minimizes the objective

cut(c1, . . . , ck) =

wr,s.

k(cid:88)

(cid:88)

i=1

r   ci,s /   ci

for k = 2, the mincut problem can be solved e   ciently. however, in practice it
often does not lead to satisfactory partitions. the problem is that in many cases,
the solution of mincut simply separates one individual vertex from the rest of the
graph. of course, this is not what we want to achieve in id91, as clusters
should be reasonably large groups of points.

several solutions to this problem have been suggested. the simplest solution

is to normalize the cut and de   ne the normalized mincut objective as follows:

ratiocut(c1, . . . , ck) =

(cid:88)

k(cid:88)

i=1

1
|ci|

wr,s.

r   ci,s /   ci

the preceding objective assumes smaller values if the clusters are not too small.
unfortunately, introducing this balancing makes the problem computationally
hard to solve. spectral id91 is a way to relax the problem of minimizing
ratiocut.

22.3.2

graph laplacian and relaxed graph cuts

the main mathematical object for spectral id91 is the graph laplacian
matrix. there are several di   erent de   nitions of graph laplacian in the literature,
and in the following we describe one particular de   nition.

316

id91

di,i =(cid:80)m

definition 22.2 (unnormalized graph laplacian) the unnormalized graph
laplacian is the m    m matrix l = d     w where d is a diagonal matrix with

j=1 wi,j. the matrix d is called the degree matrix.

the following lemma underscores the relation between ratiocut and the lapla-

cian matrix.

lemma 22.3 let c1, . . . , ck be a id91 and let h     rm,k be the matrix
such that

hi,j = 1   

|cj| 1[i   cj ].

then, the columns of h are orthonormal to each other and

ratiocut(c1, . . . , ck) = trace(h(cid:62) l h).

proof let h1, . . . , hk be the columns of h. the fact that these vectors are
orthonormal is immediate from the de   nition. next, by standard algebraic ma-
i lhi and that for

nipulations, it can be shown that trace(h(cid:62) l h) = (cid:80)k
(cid:33)

any vector v we have

(cid:32)(cid:88)

v(cid:62)lv =

1
2

dr,rv2

r     2

vrvswr,s +

ds,sv2
s

r

r,s

s

i=1 h(cid:62)
(cid:88)

=

1
2

r,s

wr,s(vr     vs)2.

(cid:88)

applying this with v = hi and noting that (hi,r     hi,s)2 is nonzero only if
r     ci, s /    ci or the other way around, we obtain that

(cid:88)

(cid:88)

h(cid:62)
i lhi =

1
|ci|

wr,s.

r   ci,s /   ci

are orthonormal and such that each hi,j is either 0 or 1/(cid:112)|cj|. unfortunately,

therefore, to minimize ratiocut we can search for a matrix h whose columns

this is an integer programming problem which we cannot solve e   ciently. instead,
we relax the latter requirement and simply search an orthonormal matrix h    
rm,k that minimizes trace(h(cid:62) l h). as we will see in the next chapter about
pca (particularly, the proof of theorem 23.2), the solution to this problem is
to set u to be the matrix whose columns are the eigenvectors corresponding to
the k minimal eigenvalues of l. the resulting algorithm is called unnormalized
spectral id91.

22.4 information bottleneck*

317

22.3.3

unnormalized spectral id91

unnormalized spectral id91

input: w     rm,m ; number of clusters k
initialize: compute the unnormalized graph laplacian l
let u     rm,k be the matrix whose columns are the eigenvectors of l

corresponding to the k smallest eigenvalues

let v1, . . . , vm be the rows of u
cluster the points v1, . . . , vm using id116
output: clusters c1, . . . , ck of the id116 algorithm

the spectral id91 algorithm starts with    nding the matrix h of the k
eigenvectors corresponding to the smallest eigenvalues of the graph laplacian
matrix. it then represents points according to the rows of h. it is due to the
properties of the graph laplacians that this change of representation is useful.
in many situations, this change of representation enables the simple id116
algorithm to detect the clusters seaid113ssly. intuitively, if h is as de   ned in
lemma 22.3 then each point in the new representation is an indicator vector
whose value is nonzero only on the element corresponding to the cluster it belongs
to.

22.4

information bottleneck*

the information bottleneck method is a id91 technique introduced by
tishby, pereira, and bialek. it relies on notions from id205. to
illustrate the method, consider the problem of id91 text documents where
each document is represented as a bag-of-words; namely, each document is a
vector x = {0, 1}n, where n is the size of the dictionary and xi = 1 i    the word
corresponding to index i appears in the document. given a set of m documents,
we can interpret the bag-of-words representation of the m documents as a joint
id203 over a random variable x, indicating the identity of a document (thus
taking values in [m]), and a random variable y, indicating the identity of a word
in the dictionary (thus taking values in [n]).

with this interpretation, the information bottleneck refers to the identity of
a id91 as another random variable, denoted c, that takes values in [k]
(where k will be set by the method as well). once we have formulated x, y, c
as random variables, we can use tools from id205 to express a
id91 objective. in particular, the information bottleneck objective is

i(x; c)       i(c; y) ,

min
p(c|x)

where i(  ;  ) is the mutual information between two random variables,1    is a

1 that is, given a id203 function, p over the pairs (x, c),

318

id91

parameter, and the minimization is over all possible probabilistic assignments of
points to clusters. intuitively, we would like to achieve two contradictory goals.
on one hand, we would like the mutual information between the identity of
the document and the identity of the cluster to be as small as possible. this
re   ects the fact that we would like a strong compression of the original data. on
the other hand, we would like high mutual information between the id91
variable and the identity of the words, which re   ects the goal that the    relevant   
information about the document (as re   ected by the words that appear in the
document) is retained. this generalizes the classical notion of minimal su   cient
statistics2 used in parametric statistics to arbitrary distributions.

solving the optimization problem associated with the information bottleneck
principle is hard in the general case. some of the proposed methods are similar
to the em principle, which we will discuss in chapter 24.

22.5

a high level view of id91

so far, we have mainly listed various useful id91 tools. however, some fun-
damental questions remain unaddressed. first and foremost, what is id91?
what is it that distinguishes a id91 algorithm from any arbitrary function
that takes an input space and outputs a partition of that space? are there any
basic properties of id91 that are independent of any speci   c algorithm or
task?

one method for addressing such questions is via an axiomatic approach. there
have been several attempts to provide an axiomatic de   nition of id91. let
us demonstrate this approach by presenting the attempt made by kleinberg
(2003).

consider a id91 function, f , that takes as input any    nite domain x

with a dissimilarity function d over its pairs and returns a partition of x .

consider the following three properties of such a function:

scale invariance (si) for any domain set x , dissimilarity function d, and
any    > 0, the following should hold: f (x , d) = f (x ,   d) (where
(  d)(x, y) def=    d(x, y)).

richness (ri) for any    nite x and every partition c = (c1, . . . ck) of x (into
nonempty subsets) there exists some dissimilarity function d over x such
that f (x , d) = c.
i(x; c) =(cid:80)

, where the sum is over all values x can take and all

(cid:16) p(a,b)

(cid:80)

(cid:17)

a

b p(a, b) log

p(a)p(b)

values c can take.

2 a su   cient statistic is a function of the data which has the property of su   ciency with
respect to a statistical model and its associated unknown parameter, meaning that    no
other statistic which can be calculated from the same sample provides any additional
information as to the value of the parameter.    for example, if we assume that a variable is
distributed normally with a unit variance and an unknown expectation, then the average
function is a su   cient statistic.

22.5 a high level view of id91

319

consistency (co) if d and d(cid:48) are dissimilarity functions over x , such that
for every x, y     x , if x, y belong to the same cluster in f (x , d) then
d(cid:48)(x, y)     d(x, y) and if x, y belong to di   erent clusters in f (x , d) then
d(cid:48)(x, y)     d(x, y), then f (x , d) = f (x , d(cid:48)).

a moment of re   ection reveals that the scale invariance is a very natural
requirement     it would be odd to have the result of a id91 function depend
on the units used to measure between-point distances. the richness requirement
basically states that the outcome of the id91 function is fully controlled by
the function d, which is also a very intuitive feature. the third requirement,
consistency, is the only requirement that refers to the basic (informal) de   nition
of id91     we wish that similar points will be clustered together and that
dissimilar points will be separated to di   erent clusters, and therefore, if points
that already share a cluster become more similar, and points that are already
separated become even less similar to each other, the id91 function should
have even stronger    support    of its previous id91 decisions.

however, kleinberg (2003) has shown the following    impossibility    result:

theorem 22.4 there exists no function, f , that satis   es all the three proper-
ties: scale invariance, richness, and consistency.

proof assume, by way of contradiction, that some f does satisfy all three
properties. pick some domain set x with at least three points. by richness,
there must be some d1 such that f (x , d1) = {{x} : x     x} and there also exists
some d2 such that f (x , d2) (cid:54)= f (x , d1).
let        r+ be such that for every x, y     x ,   d2(x, y)     d1(x, y). let d3 =
  d2. consider f (x , d3). by the scale invariance property of f , we should have
f (x , d3) = f (x , d2). on the other hand, since all distinct x, y     x reside in
di   erent clusters w.r.t. f (x , d1), and d3(x, y)     d1(x, y), the consistency of f
implies that f (x , d3) = f (x , d1). this is a contradiction, since we chose d1, d2
so that f (x , d2) (cid:54)= f (x , d1).

it is important to note that there is no single    bad property    among the three
properties. for every pair of the the three axioms, there exist natural id91
functions that satisfy the two properties in that pair (one can even construct such
examples just by varying the stopping criteria for the single linkage id91
function). on the other hand, kleinberg shows that any id91 algorithm
that minimizes any center-based objective function inevitably fails the consis-
tency property (yet, the k-sum-of-in-cluster-distances minimization id91
does satisfy consistency).

the kleinberg impossibility result can be easily circumvented by varying the
properties. for example, if one wishes to discuss id91 functions that have
a    xed number-of-clusters parameter, then it is natural to replace richness by
k-richness (namely, the requirement that every partition of the domain into k
subsets is attainable by the id91 function). k-richness, scale invariance
and consistency all hold for the id116 id91 and are therefore consistent.

320

id91

1, . . . c(cid:48)
j or c(cid:48)

alternatively, one can relax the consistency property. for example, say that two
id91s c = (c1, . . . ck) and c(cid:48) = (c(cid:48)
l) are compatible if for every
clusters ci     c and c(cid:48)
j     c(cid:48), either ci     c(cid:48)
j     ci or ci     c(cid:48)
j =     (it is
worthwhile noting that for every dendrogram, every two id91s that are ob-
tained by trimming that dendrogram are compatible).    re   nement consistency   
is the requirement that, under the assumptions of the consistency property, the
new id91 f (x , d(cid:48)) is compatible with the old id91 f (x , d). many
common id91 functions satisfy this requirement as well as scale invariance
and richness. furthermore, one can come up with many other, di   erent, prop-
erties of id91 functions that sound intuitive and desirable and are satis   ed
by some common id91 functions.

there are many ways to interpret these results. we suggest to view it as indi-
cating that there is no    ideal    id91 function. every id91 function will
inevitably have some    undesirable    properties. the choice of a id91 func-
tion for any given task must therefore take into account the speci   c properties
of that task. there is no generic id91 solution, just as there is no clas-
si   cation algorithm that will learn every learnable task (as the no-free-lunch
theorem shows). id91, just like classi   cation prediction, must take into
account some prior knowledge about the speci   c task at hand.

22.6

summary

id91 is an unsupervised learning problem, in which we wish to partition
a set of points into    meaningful    subsets. we presented several id91 ap-
proaches including linkage-based algorithms, the id116 family, spectral clus-
tering, and the information bottleneck. we discussed the di   culty of formalizing
the intuitive meaning of id91.

22.7

bibliographic remarks

the id116 algorithm is sometimes named lloyd   s algorithm, after stuart
lloyd, who proposed the method in 1957. for a more complete overview of
spectral id91 we refer the reader to the excellent tutorial by von luxburg
(2007). the information bottleneck method was introduced by tishby, pereira
& bialek (1999). for an additional discussion on the axiomatic approach see
ackerman & ben-david (2008).

22.8

exercises

1. suboptimality of id116: for every parameter t > 1, show that there
exists an instance of the id116 problem for which the id116 algorithm

22.8 exercises

321

(might)    nd a solution whose id116 objective is at least t    opt, where
opt is the minimum id116 objective.

2. id116 might not necessarily converge to a local minimum:
show that the id116 algorithm might converge to a point which is not
a local minimum. hint: suppose that k = 2 and the sample points are
{1, 2, 3, 4}     r suppose we initialize the id116 with the centers {2, 4};
and suppose we break ties in the de   nition of ci by assigning i to be the
smallest value in argminj (cid:107)x       j(cid:107).
3. given a metric space (x , d), where |x| <    , and k     n, we would like to    nd
a partition of x into c1, . . . , ck which minimizes the expression

gk   diam((x , d), (c1, . . . , ck)) = max
j   [d]

diam(cj),

where diam(cj) = maxx,x(cid:48)   cj d(x, x(cid:48)) (we use the convention diam(cj) = 0
if |cj| < 2).

similarly to the id116 objective, it is np-hard to minimize the k-
diam objective. fortunately, we have a very simple approximation algorithm:
initially, we pick some x     x and set   1 = x. then, the algorithm iteratively
sets

   j     {2, . . . , k},   j = argmax
x   x

min
i   [j   1]

d(x,   i).

finally, we set

   i     [k], ci = {x     x : i = argmin
j   [k]

d(x,   j)}.

prove that the algorithm described is a 2-approximation algorithm. that
is, if we denote its output by   c1, . . . ,   ck, and denote the optimal solution by
c   
1 , . . . , c   

k , then,

gk   diam((x , d), (   c1, . . . ,   ck))     2    gk   diam((x , d), (c   

1 , . . . , c   

k )).

hint: consider the point   k+1 (in other words, the next center we would have
chosen, if we wanted k + 1 clusters). let r = minj   [k] d(  j,   k+1). prove the
following inequalities

gk   diam((x , d), (   c1, . . . ,   ck))     2r
k ))     r.
gk   diam((x, d), (c   

1 , . . . , c   

4. recall that a id91 function, f , is called center-based id91 if, for
some monotonic function f : r+     r+, on every given input (x , d), f (x , d)
is a id91 that minimizes the objective

gf ((x , d), (c1, . . . ck)) = min

  1,...  k   x (cid:48)
where x (cid:48) is either x or some superset of x .

k(cid:88)

(cid:88)

i=1

x   ci

f (d(x,   i)),

322

id91

prove that for every k > 1 the k-diam id91 function de   ned in the
previous exercise is not a center-based id91 function.
hint: given a id91 input (x , d), with |x| > 2, consider the e   ect of
adding many close-by points to some (but not all) of the members of x , on
either the k-diam id91 or any given center-based id91.

5. recall that we discussed three id91    properties   : scale invariance, rich-

ness, and consistency. consider the single linkage id91 algorithm.
1. find which of the three properties is satis   ed by single linkage with the

fixed number of clusters (any    xed nonzero number) stopping rule.

2. find which of the three properties is satis   ed by single linkage with the

distance upper bound (any    xed nonzero upper bound) stopping rule.

3. show that for any pair of these properties there exists a stopping criterion
for single linkage id91, under which these two axioms are satis   ed.

6. given some number k, let k-richness be the following requirement:
for any    nite x and every partition c = (c1, . . . ck) of x (into nonempty subsets)
there exists some dissimilarity function d over x such that f (x , d) = c.

prove that, for every number k, there exists a id91 function that
satis   es the three properties: scale invariance, k-richness, and consistency.

23 id84

id84 is the process of taking data in a high dimensional
space and mapping it into a new space whose dimensionality is much smaller.
this process is closely related to the concept of (lossy) compression in infor-
mation theory. there are several reasons to reduce the dimensionality of the
data. first, high dimensional data impose computational challenges. moreover,
in some situations high dimensionality might lead to poor generalization abili-
ties of the learning algorithm (for example, in nearest neighbor classi   ers the
sample complexity increases exponentially with the dimension   see chapter 19).
finally, id84 can be used for interpretability of the data, for
   nding meaningful structure of the data, and for illustration purposes.

in this chapter we describe popular methods for id84. in
those methods, the reduction is performed by applying a linear transformation
to the original data. that is, if the original data is in rd and we want to embed
it into rn (n < d) then we would like to    nd a matrix w     rn,d that induces
the mapping x (cid:55)    w x. a natural criterion for choosing w is in a way that will
enable a reasonable recovery of the original x. it is not hard to show that in
general, exact recovery of x from w x is impossible (see exercise 1).

the    rst method we describe is called principal component analysis (pca).
in pca, both the compression and the recovery are performed by linear transfor-
mations and the method    nds the linear transformations for which the di   erences
between the recovered vectors and the original vectors are minimal in the least
squared sense.

next, we describe id84 using random matrices w . we
derive an important lemma, often called the    johnson-lindenstrauss lemma,   
which analyzes the distortion caused by such a random id84
technique.

last, we show how one can reduce the dimension of all sparse vectors using
again a random matrix. this process is known as compressed sensing. in this
case, the recovery process is nonlinear but can still be implemented e   ciently
using id135.

we conclude by underscoring the underlying    prior assumptions    behind pca
and compressed sensing, which can help us understand the merits and pitfalls of
the two methods.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

324

id84

23.1

principal component analysis (pca)

let x1, . . . , xm be m vectors in rd. we would like to reduce the dimensional-
ity of these vectors using a linear transformation. a matrix w     rn,d, where
n < d, induces a mapping x (cid:55)    w x, where w x     rn is the lower dimensionality
representation of x. then, a second matrix u     rd,n can be used to (approxi-
mately) recover each original vector x from its compressed version. that is, for
a compressed vector y = w x, where y is in the low dimensional space rn, we
can construct   x = u y, so that   x is the recovered version of x and resides in the
original high dimensional space rd.

in pca, we    nd the compression matrix w and the recovering matrix u so
that the total squared distance between the original and recovered vectors is
minimal; namely, we aim at solving the problem

m(cid:88)

i=1

argmin

w   rn,d,u   rd,n

(cid:107)xi     u w xi(cid:107)2
2.

(23.1)

to solve this problem we    rst show that the optimal solution takes a speci   c

form.

lemma 23.1 let (u, w ) be a solution to equation (23.1). then the columns of
u are orthonormal (namely, u(cid:62)u is the identity matrix of rn) and w = u(cid:62).
proof fix any u, w and consider the mapping x (cid:55)    u w x. the range of this
mapping, r = {u w x : x     rd}, is an n dimensional linear subspace of rd. let
v     rd,n be a matrix whose columns form an orthonormal basis of this subspace,
namely, the range of v is r and v (cid:62)v = i. therefore, each vector in r can be
written as v y where y     rn. for every x     rd and y     rn we have

(cid:107)x     v y(cid:107)2

2 = (cid:107)x(cid:107)2 + y(cid:62)v (cid:62)v y     2y(cid:62)v (cid:62)x = (cid:107)x(cid:107)2 + (cid:107)y(cid:107)2     2y(cid:62)(v (cid:62)x),

where we used the fact that v (cid:62)v is the identity matrix of rn. minimizing the
preceding expression with respect to y by comparing the gradient with respect
to y to zero gives that y = v (cid:62)x. therefore, for each x we have that

v v (cid:62)x = argmin
  x   r

(cid:107)x       x(cid:107)2
2.

in particular this holds for x1, . . . , xm and therefore we can replace u, w by
v, v (cid:62) and by that do not increase the objective

m(cid:88)

2     m(cid:88)

(cid:107)xi     u w xi(cid:107)2

(cid:107)xi     v v (cid:62)xi(cid:107)2
2.

i=1

i=1

since this holds for every u, w the proof of the lemma follows.

on the basis of the preceding lemma, we can rewrite the optimization problem

given in equation (23.1) as follows:

argmin

u   rd,n:u(cid:62)u =i

m(cid:88)

i=1

(cid:107)xi     u u(cid:62)xi(cid:107)2
2.

(23.2)

23.1 principal component analysis (pca)

325

we further simplify the optimization problem by using the following elementary
algebraic manipulations. for every x     rd and a matrix u     rd,n such that
u(cid:62)u = i we have

(cid:107)x     uu (cid:62)x(cid:107)2 = (cid:107)x(cid:107)2     2x(cid:62)uu (cid:62)x + x(cid:62)u u(cid:62)uu (cid:62)x

= (cid:107)x(cid:107)2     x(cid:62)uu (cid:62)x
= (cid:107)x(cid:107)2     trace(u(cid:62)xx(cid:62)u ),

(23.3)

where the trace of a matrix is the sum of its diagonal entries. since the trace is
a linear operator, this allows us to rewrite equation (23.2) as follows:

(cid:33)

(cid:32)

u(cid:62) m(cid:88)

i=1

xix(cid:62)
i u

.

(23.4)

let a = (cid:80)m

i=1 xix(cid:62)

argmax

u   rd,n:u(cid:62)u =i

trace

i . the matrix a is symmetric and therefore it can be
written using its spectral decomposition as a = vdv (cid:62), where d is diagonal and
v (cid:62)v = vv (cid:62) = i. here, the elements on the diagonal of d are the eigenvalues of
a and the columns of v are the corresponding eigenvectors. we assume without
loss of generality that d1,1     d2,2                dd,d. since a is positive semide   nite
it also holds that dd,d     0. we claim that the solution to equation (23.4) is
the matrix u whose columns are the n eigenvectors of a corresponding to the
largest n eigenvalues.

theorem 23.2 let x1, . . . , xm be arbitrary vectors in rd, let a =(cid:80)m

i=1 xix(cid:62)
i ,
and let u1, . . . , un be n eigenvectors of the matrix a corresponding to the largest
n eigenvalues of a. then, the solution to the pca optimization problem given
in equation (23.1) is to set u to be the matrix whose columns are u1, . . . , un
and to set w = u(cid:62).
proof let vdv (cid:62) be the spectral decomposition of a. fix some matrix u     rd,n
with orthonormal columns and let b = v (cid:62)u . then, vb = vv (cid:62)u = u . it
follows that

u(cid:62)au = b(cid:62)v (cid:62)vdv (cid:62)vb = b(cid:62)db,

n(cid:88)

and therefore

trace(u(cid:62)au ) =

d(cid:88)
(cid:80)n
also orthonormal, which implies that(cid:80)d
addition   b(cid:62)   b = i. then, for every j we have(cid:80)d
(cid:80)n

note that b(cid:62)b = u(cid:62)vv (cid:62)u = u(cid:62)u = i. therefore, the columns of b are
j,i = n. in addition, let   b    
rd,d be a matrix such that its    rst n columns are the columns of b and in
j,i = 1, which implies that

i=1 b2

b2

j,i.

dj,j

  b2

j=1

j=1

i=1

i=1

j,i     1. it follows that:

i=1 b2

trace(u(cid:62)au )    

max

     [0,1]d : (cid:107)  (cid:107)1   n

dj,j  j .

d(cid:88)

j=1

326

id84

(cid:80)n
thonormal columns it holds that trace(u(cid:62)au )    (cid:80)n
it is not hard to verify (see exercise 2) that the right-hand side equals to
j=1 dj,j. we have therefore shown that for every matrix u     rd,n with or-
we obtain that trace(u(cid:62)au ) =(cid:80)n
j=1 dj,j. on the other hand,
if we set u to be the matrix whose columns are the n leading eigenvectors of a
objective of equation (23.4) is(cid:80)n
and noting that(cid:80)m
objective value of equation (23.1) is(cid:80)d

remark 23.1 the proof of theorem 23.2 also tells us that the value of the
i=1 di,i. combining this with equation (23.3)
i=1 di,i we obtain that the optimal

j=1 dj,j, and this concludes our proof.

i=n+1 di,i.

i=1 (cid:107)xi(cid:107)2 = trace(a) =(cid:80)d
(cid:80)m

remark 23.2
it is a common practice to    center    the examples before applying
pca. that is, we    rst calculate    = 1
i=1 xi and then apply pca on the
vectors (x1       ), . . . , (xm       ). this is also related to the interpretation of pca
m
as variance maximization (see exercise 4).

23.1.1

a more e   cient solution for the case d (cid:29) m

in some situations the original dimensionality of the data is much larger than
the number of examples m. the computational complexity of calculating the
pca solution as described previously is o(d3) (for calculating eigenvalues of a)
plus o(md2) (for constructing the matrix a). we now show a simple trick that
enables us to calculate the pca solution more e   ciently when d (cid:29) m.

recall that the matrix a is de   ned to be(cid:80)m

i . it is convenient to rewrite
a = x(cid:62)x where x     rm,d is a matrix whose ith row is x(cid:62)
i . consider the
matrix b = xx(cid:62). that is, b     rm,m is the matrix whose i, j element equals
(cid:104)xi, xj(cid:105). suppose that u is an eigenvector of b: that is, bu =   u for some
       r. multiplying the equality by x(cid:62) and using the de   nition of b we obtain
x(cid:62)xx(cid:62)u =   x(cid:62)u. but, using the de   nition of a, we get that a(x(cid:62)u) =
  (x(cid:62)u). thus, x(cid:62)u

(cid:107)x(cid:62)u(cid:107) is an eigenvector of a with eigenvalue of   .

i=1 xix(cid:62)

we can therefore calculate the pca solution by calculating the eigenvalues of
b instead of a. the complexity is o(m3) (for calculating eigenvalues of b) and
m2d (for constructing the matrix b).

remark 23.3 the previous discussion also implies that to calculate the pca
solution we only need to know how to calculate inner products between vectors.
this enables us to calculate pca implicitly even when d is very large (or even
in   nite) using kernels, which yields the kernel pca algorithm.

23.1.2

implementation and demonstration

a pseudocode of pca is given in the following.

23.1 principal component analysis (pca)

327

figure 23.1 a set of vectors in r2 (blue x   s) and their reconstruction after
id84 to r1 using pca (red circles).

pca

input
a matrix of m examples x     rm,d
number of components n

if (m > d)
a = x(cid:62)x
let u1, . . . , un be the eigenvectors of a with largest eigenvalues

else

b = xx(cid:62)
let v1, . . . , vn be the eigenvectors of b with largest eigenvalues
for i = 1, . . . , n set ui =

(cid:107)x(cid:62)vi(cid:107) x(cid:62)vi

1

output: u1, . . . , un

to illustrate how pca works, let us generate vectors in r2 that approximately
reside on a line, namely, on a one dimensional subspace of r2. for example,
suppose that each example is of the form (x, x + y) where x is chosen uniformly
at random from [   1, 1] and y is sampled from a gaussian distribution with mean
0 and standard deviation of 0.1. suppose we apply pca on this data. then, the
eigenvector corresponding to the largest eigenvalue will be close to the vector
(1/
2). when projecting a point (x, x + y) on this principal component
we will obtain the scalar 2x+y   
. the reconstruction of the original vector will be
2
((x + y/2), (x + y/2)). in figure 23.1 we depict the original versus reconstructed
data.

2, 1/

   

   

next, we demonstrate the e   ectiveness of pca on a data set of faces. we
extracted images of faces from the yale data set (georghiades, belhumeur &
kriegman 2001). each image contains 50  50 = 2500 pixels; therefore the original
dimensionality is very high.

   1.5   1   0.500.511.5   1.5   1   0.500.511.5328

id84

o

o oo
o

o

o

+ + ++
+ +
+

x

x
x

x xx
x
*

*

***

*
*

figure 23.2 images of faces extracted from the yale data set. top-left: the original
images in r50x50. top-right: the images after id84 to r10 and
reconstruction. middle row: an enlarged version of one of the images before and after
pca. bottom: the images after id84 to r2. the di   erent marks
indicate di   erent individuals.

some images of faces are depicted on the top-left side of figure 23.2. using
pca, we reduced the dimensionality to r10 and reconstructed back to the orig-
inal dimension, which is 502. the resulting reconstructed images are depicted
on the top-right side of figure 23.2. finally, on the bottom of figure 23.2 we
depict a 2 dimensional representation of the images. as can be seen, even from a
2 dimensional representation of the images we can still roughly separate di   erent
individuals.

23.2 random projections

329

23.2

random projections

in this section we show that reducing the dimension by using a random linear
transformation leads to a simple compression scheme with a surprisingly low
distortion. the transformation x (cid:55)    w x, when w is a random matrix, is often
referred to as a random projection. in particular, we provide a variant of a famous
lemma due to johnson and lindenstrauss, showing that random projections do
not distort euclidean distances too much.

let x1, x2 be two vectors in rd. a matrix w does not distort too much the

distance between x1 and x2 if the ratio

(cid:107)w x1     w x2(cid:107)

(cid:107)x1     x2(cid:107)

is close to 1. in other words, the distances between x1 and x2 before and after
the transformation are almost the same. to show that (cid:107)w x1     w x2(cid:107) is not too
far away from (cid:107)x1     x2(cid:107) it su   ces to show that w does not distort the norm of
the di   erence vector x = x1     x2. therefore, from now on we focus on the ratio
(cid:107)w x(cid:107)
(cid:107)x(cid:107) .
we start with analyzing the distortion caused by applying a random projection

to a single vector.
lemma 23.3 fix some x     rd. let w     rn,d be a random matrix such that
each wi,j is an independent normal random variable. then, for every       (0, 3)
we have

p

(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) >  

   
n)w x(cid:107)2
(cid:107)x(cid:107)2

(cid:34) (cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)(1/
p(cid:2)(1      )n     (cid:107)w x(cid:107)2     (1 +  )n(cid:3)     1     2e    2n/6.

    2 e    2n/6.

    1

proof without loss of generality we can assume that (cid:107)x(cid:107)2 = 1. therefore, an
equivalent inequality is

with zero mean and variance (cid:80)
able (cid:107)w x(cid:107)2 = (cid:80)n

let wi be the ith row of w . the random variable (cid:104)wi, x(cid:105) is a weighted sum of
d independent normal random variables and therefore it is normally distributed
j = (cid:107)x(cid:107)2 = 1. therefore, the random vari-
n distribution. the claim now follows
directly from a measure concentration property of   2 random variables stated in
lemma b.12 given in section b.7.

i=1((cid:104)wi, x(cid:105))2 has a   2

j x2

the johnson-lindenstrauss lemma follows from this using a simple union

bound argument.

lemma 23.4 (johnson-lindenstrauss lemma) let q be a    nite set of vectors
in rd. let        (0, 1) and n be an integer such that

(cid:114)

  =

6 log(2|q|/  )

n

    3.

330

id84

then, with id203 of at least 1      over a choice of a random matrix w     rn,d
such that each element of w is distributed normally with zero mean and variance
of 1/n we have

proof combining lemma 23.3 and the union bound we have that for every
      (0, 3):

(cid:20)

p

sup
x   q

(cid:12)(cid:12)(cid:12)(cid:12) <  .

(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)w x(cid:107)2
(cid:107)x(cid:107)2     1
(cid:12)(cid:12)(cid:12)(cid:12) >  
(cid:21)

sup
x   q

(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)w x(cid:107)2
(cid:107)x(cid:107)2     1
(cid:114)

  =

6 log(2|q|/  )

.

n

    2|q| e    2n/6.

let    denote the right-hand side of the inequality; thus we obtain that

interestingly, the bound given in lemma 23.4 does not depend on the original
dimension of x. in fact, the bound holds even if x is in an in   nite dimensional
hilbert space.

23.3

compressed sensing

compressed sensing is a id84 technique which utilizes a prior
assumption that the original vector is sparse in some basis. to motivate com-
pressed sensing, consider a vector x     rd that has at most s nonzero elements.
that is,

(cid:107)x(cid:107)0

def= |{i : xi (cid:54)= 0}|     s.

clearly, we can compress x by representing it using s (index,value) pairs. fur-
thermore, this compression is lossless     we can reconstruct x exactly from the s
(index,value) pairs. now, lets take one step forward and assume that x = u   ,
where    is a sparse vector, (cid:107)  (cid:107)0     s, and u is a    xed orthonormal matrix. that
is, x has a sparse representation in another basis. it turns out that many nat-
ural vectors are (at least approximately) sparse in some representation. in fact,
this assumption underlies many modern compression schemes. for example, the
jpeg-2000 format for image compression relies on the fact that natural images
are approximately sparse in a wavelet basis.

can we still compress x into roughly s numbers? well, one simple way to do
this is to multiply x by u(cid:62), which yields the sparse vector   , and then represent
   by its s (index,value) pairs. however, this requires us    rst to    sense    x, to
store it, and then to multiply it by u(cid:62). this raises a very natural question: why
go to so much e   ort to acquire all the data when most of what we get will be
thrown away? cannot we just directly measure the part that will not end up
being thrown away?

23.3 compressed sensing

331

compressed sensing is a technique that simultaneously acquires and com-
presses the data. the key result is that a random linear transformation can
compress x without losing information. the number of measurements needed is
order of s log(d). that is, we roughly acquire only the important information
about the signal. as we will see later, the price we pay is a slower reconstruction
phase. in some situations, it makes sense to save time in compression even at
the price of a slower reconstruction. for example, a security camera should sense
and compress a large amount of images while most of the time we do not need to
decode the compressed data at all. furthermore, in many practical applications,
compression by a linear transformation is advantageous because it can be per-
formed e   ciently in hardware. for example, a team led by baraniuk and kelly
has proposed a camera architecture that employs a digital micromirror array to
perform optical calculations of a linear transformation of an image. in this case,
obtaining each compressed measurement is as easy as obtaining a single raw
measurement. another important application of compressed sensing is medical
imaging, in which requiring fewer measurements translates to less radiation for
the patient.

informally, the main premise of compressed sensing is the following three    sur-

prising    results:

1. it is possible to reconstruct any sparse signal fully if it was compressed by
x (cid:55)    w x, where w is a matrix which satis   es a condition called the re-
stricted isoperimetric property (rip). a matrix that satis   es this property is
guaranteed to have a low distortion of the norm of any sparse representable
vector.

2. the reconstruction can be calculated in polynomial time by solving a linear

program.

3. a random n    d matrix is likely to satisfy the rip condition provided that n

is greater than an order of s log(d).

formally,

definition 23.5 (rip) a matrix w     rn,d is ( , s)-rip if for all x (cid:54)= 0 s.t.
(cid:107)x(cid:107)0     s we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)w x(cid:107)2

(cid:107)x(cid:107)2

2

2

(cid:12)(cid:12)(cid:12)(cid:12)      .

    1

the    rst theorem establishes that rip matrices yield a lossless compression
scheme for sparse vectors. it also provides a (none   cient) reconstruction scheme.

theorem 23.6 let   < 1 and let w be a ( , 2s)-rip matrix. let x be a vector
s.t. (cid:107)x(cid:107)0     s, let y = w x be the compression of x, and let

  x     argmin

v:w v=y

(cid:107)v(cid:107)0

be a reconstructed vector. then,   x = x.

332

id84

proof we assume, by way of contradiction, that   x (cid:54)= x. since x satis   es the
constraints in the optimization problem that de   nes   x we clearly have that
(cid:107)  x(cid:107)0     (cid:107)x(cid:107)0     s. therefore, (cid:107)x       x(cid:107)0     2s and we can apply the rip in-
equality on the vector x       x. but, since w (x       x) = 0 we get that |0     1|      ,
which leads to a contradiction.

the reconstruction scheme given in theorem 23.6 seems to be none   cient
because we need to minimize a combinatorial objective (the sparsity of v). quite
surprisingly, it turns out that we can replace the combinatorial objective, (cid:107)v(cid:107)0,
with a convex objective, (cid:107)v(cid:107)1, which leads to a id135 problem that
can be solved e   ciently. this is stated formally in the following theorem.

theorem 23.7 assume that the conditions of theorem 23.6 holds and that
   
  < 1
1+

. then,

2

x = argmin
v:w v=y

(cid:107)v(cid:107)0 = argmin

(cid:107)v(cid:107)1.

v:w v=y

in fact, we will prove a stronger result, which holds even if x is not a sparse

vector.

   
theorem 23.8 let   < 1
1+
arbitrary vector and denote

2

and let w be a ( , 2s)-rip matrix. let x be an

xs     argmin
v:(cid:107)v(cid:107)0   s

(cid:107)x     v(cid:107)1.

that is, xs is the vector which equals x on the s largest elements of x and equals
0 elsewhere. let y = w x be the compression of x and let

x(cid:63)     argmin

v:w v=y

(cid:107)v(cid:107)1

be the reconstructed vector. then,

(cid:107)x(cid:63)     x(cid:107)2     2

1 +   
1       

s   1/2(cid:107)x     xs(cid:107)1,

2 /(1      ).

   

where    =

note that in the special case that x = xs we get an exact recovery, x(cid:63) = x, so
theorem 23.7 is a special case of theorem 23.8. the proof of theorem 23.8 is
given in section 23.3.1.

finally, the third result tells us that random matrices with n        (s log(d)) are
likely to be rip. in fact, the theorem shows that multiplying a random matrix
by an orthonormal matrix also provides an rip matrix. this is important for
compressing signals of the form x = u    where x is not sparse but    is sparse.
in that case, if w is a random matrix and we compress using y = w x then this
is the same as compressing    by y = (w u )   and since w u is also rip we can
reconstruct    (and thus also x) from y.

23.3 compressed sensing

333

theorem 23.9 let u be an arbitrary    xed d    d orthonormal matrix, let  ,   
be scalars in (0, 1), let s be an integer in [d], and let n be an integer that satis   es

n     100

s log(40d/(    ))

.

 2

let w     rn,d be a matrix s.t. each element of w is distributed normally with
zero mean and variance of 1/n. then, with proabability of at least 1       over the
choice of w , the matrix w u is ( , s)-rip.

23.3.1

proofs*

proof of theorem 23.8
we follow a proof due to cand`es (2008).
let h = x(cid:63)     x. given a vector v and a set of indices i we denote by vi the
vector whose ith element is vi if i     i and 0 otherwise.
the    rst trick we use is to partition the set of indices [d] = {1, . . . , d} into
disjoint sets of size s. that is, we will write [d] = t0       t1       t2 . . . td/s   1 where
for all i, |ti| = s, and we assume for simplicity that d/s is an integer. we de   ne
the partition as follows. in t0 we put the s indices corresponding to the s largest
0 = [d] \ t0.
elements in absolute values of x (ties are broken arbitrarily). let t c
next, t1 will be the s indices corresponding to the s largest elements in absolute
0,1 = [d]\ t0,1. next, t2 will correspond to
value of ht c
the s largest elements in absolute value of ht c
. and, we will construct t3, t4, . . .
in the same way.

. let t0,1 = t0     t1 and t c

0,1

0

to prove the theorem we    rst need the following lemma, which shows that

rip also implies approximate orthogonality.

lemma 23.10 let w be an ( , 2s)-rip matrix. then, for any two disjoint sets
i, j, both of size at most s, and for any vector u we have that (cid:104)w ui , w uj(cid:105)    
 (cid:107)ui(cid:107)2 (cid:107)uj(cid:107)2.
proof w.l.o.g. assume (cid:107)ui(cid:107)2 = (cid:107)uj(cid:107)2 = 1.

(cid:104)w ui , w uj(cid:105) =

(cid:107)w ui + w uj(cid:107)2

2     (cid:107)w ui     w uj(cid:107)2
4

2

.

2    
but, since |j     i|     2s we get from the rip condition that (cid:107)w ui + w uj(cid:107)2
2        (1     )((cid:107)ui(cid:107)2
(1 +  )((cid:107)ui(cid:107)2
2 +
(cid:107)uj(cid:107)2

2) = 2(1 +  ) and that    (cid:107)w ui     w uj(cid:107)2

2) =    2(1      ), which concludes our proof.

2 +(cid:107)uj(cid:107)2

we are now ready to prove the theorem. clearly,

(cid:107)2     (cid:107)ht0,1(cid:107)2 + (cid:107)ht c
to prove the theorem we will show the following two claims:

(cid:107)h(cid:107)2 = (cid:107)ht0,1 + ht c

0,1

0,1

(cid:107)2.

(23.5)

claim 1:. (cid:107)ht c
claim 2:. (cid:107)ht0,1(cid:107)2     2  

0,1

(cid:107)2     (cid:107)ht0(cid:107)2 + 2s   1/2(cid:107)x     xs(cid:107)1.

1      s   1/2(cid:107)x     xs(cid:107)1.

334

id84

combining these two claims with equation (23.5) we get that

(cid:17)
(cid:107)h(cid:107)2     (cid:107)ht0,1(cid:107)2 + (cid:107)ht c

(cid:16) 2  

0,1

    2

1      + 1

s   1/2(cid:107)x     xs(cid:107)1

(cid:107)2     2(cid:107)ht0,1(cid:107)2 + 2s   1/2(cid:107)x     xs(cid:107)1

= 2

1 +   
1       

s   1/2(cid:107)x     xs(cid:107)1,

and this will conclude our proof.

proving claim 1:
to prove this claim we do not use the rip condition at all but only use the fact
that x(cid:63) minimizes the (cid:96)1 norm. take j > 1. for each i     tj and i(cid:48)     tj   1 we
have that |hi|     |hi(cid:48)|. therefore, (cid:107)htj(cid:107)        (cid:107)htj   1(cid:107)1/s. thus,

(cid:107)htj(cid:107)2     s1/2(cid:107)htj(cid:107)        s   1/2(cid:107)htj   1(cid:107)1.

(cid:107)2    (cid:88)

j   2

summing this over j = 2, 3, . . . and using the triangle inequality we obtain that

(cid:107)ht c

0,1

(cid:107)htj(cid:107)2     s   1/2(cid:107)ht c

0

(cid:107)1

(23.6)

next, we show that (cid:107)ht c
(cid:107)1 cannot be large. indeed, from the de   nition of x(cid:63)
we have that (cid:107)x(cid:107)1     (cid:107)x(cid:63)(cid:107)1 = (cid:107)x + h(cid:107)1. thus, using the triangle inequality we
obtain that
(cid:107)x(cid:107)1     (cid:107)x+h(cid:107)1 =

|xi+hi|     (cid:107)xt0(cid:107)1   (cid:107)ht0(cid:107)1+(cid:107)ht c

|xi+hi|+

(cid:107)1   (cid:107)xt c

(cid:88)

(cid:88)

(cid:107)1

0

0

0

i   t0

i   t c

0

(23.7)

(23.8)

(cid:107)1,

(23.9)

and since (cid:107)xt c

(cid:107)1 = (cid:107)x     xs(cid:107)1 = (cid:107)x(cid:107)1     (cid:107)xt0(cid:107)1 we get that

0

(cid:107)ht c

(cid:107)1     (cid:107)ht0(cid:107)1 + 2(cid:107)xt c
combining this with equation (23.6) we get that

(cid:107)2     s   1/2(cid:0)(cid:107)ht0(cid:107)1 + 2(cid:107)xt c

(cid:107)ht c

0,1

(cid:107)1

(cid:1)     (cid:107)ht0(cid:107)2 + 2s   1/2(cid:107)xt c

(cid:107)1.

0

0

0

0

which concludes the proof of claim 1.

proving claim 2:
for the second claim we use the rip condition to get that

since w ht0,1 = w h    (cid:80)
2 =    (cid:88)

(cid:107)w ht0,1(cid:107)2

(1      )(cid:107)ht0,1(cid:107)2

j   2 w htj =    (cid:80)
(cid:104)w ht0,1 , w htj(cid:105) =    (cid:88)

2     (cid:107)w ht0,1(cid:107)2
2.

j   2

j   2

j   2 w htj we have that

(cid:104)w ht0 + w ht1, w htj(cid:105).

from the rip condition on inner products we obtain that for all i     {1, 2} and
j     2 we have

|(cid:104)w hti, w htj(cid:105)|      (cid:107)hti(cid:107)2(cid:107)htj(cid:107)2.

23.3 compressed sensing

335

since (cid:107)ht0(cid:107)2 + (cid:107)ht1(cid:107)2        

2(cid:107)ht0,1(cid:107)2 we therefore get that
(cid:107)htj(cid:107)2.

2 (cid:107)ht0,1(cid:107)2

2    

(cid:88)

   

(cid:107)w ht0,1(cid:107)2

j   2

combining this with equation (23.6) and equation (23.9) we obtain

   

2 (cid:107)ht0,1(cid:107)2s   1/2(cid:107)ht c

0

(cid:107)1.

s   1/2(cid:107)ht c

(cid:107)1.

0

(1      )(cid:107)ht0,1(cid:107)2
rearranging the inequality gives

2    

(cid:107)ht0,1(cid:107)2    

   
2 
1      
finally, using equation (23.8) we get that
(cid:107)ht0,1(cid:107)2       s   1/2 ((cid:107)ht0(cid:107)1 + 2(cid:107)xt c
but since (cid:107)ht0(cid:107)2     (cid:107)ht0,1(cid:107)2 this implies
(cid:107)ht0,1(cid:107)2     2  
1       

0

(cid:107)1)       (cid:107)ht0(cid:107)2 + 2  s   1/2(cid:107)xt c

(cid:107)1,

0

s   1/2(cid:107)xt c

(cid:107)1,

0

which concludes the proof of the second claim.

proof of theorem 23.9
to prove the theorem we follow an approach due to (baraniuk, davenport, de-
vore & wakin 2008). the idea is to combine the johnson-lindenstrauss (jl)
lemma with a simple covering argument.

we start with a covering property of the unit ball.

lemma 23.11 let       (0, 1). there exists a    nite set q     rd of size |q|    (cid:0) 3

(cid:1)d

 

such that

sup

x:(cid:107)x(cid:107)   1

min
v   q

(cid:107)x     v(cid:107)      .

proof let k be an integer and let

q(cid:48) = {x     rd :    j     [d],   i     {   k,   k + 1, . . . , k} s.t. xj = i
k}.

clearly, |q(cid:48)| = (2k + 1)d. we shall set q = q(cid:48)     b2(1), where b2(1) is the unit
(cid:96)2 ball of rd. since the points in q(cid:48) are distributed evenly on the unit (cid:96)    ball,
the size of q is the size of q(cid:48) times the ratio between the volumes of the unit (cid:96)2
and (cid:96)    balls. the volume of the (cid:96)    ball is 2d and the volume of b2(1) is

  d/2

  (1 + d/2)

.

for simplicity, assume that d is even and therefore

  (1 + d/2) = (d/2)!     (cid:16) d/2

(cid:17)d/2

,

e

336

id84

where in the last inequality we used stirling   s approximation. overall we obtained
that

|q|     (2k + 1)d (  /e)d/2 (d/2)   d/2 2   d.

(23.10)
now lets specify k. for each x     b2(1) let v     q be the vector whose ith element
is sign(xi)(cid:98)|xi| k(cid:99)/k. then, for each element we have that |xi     vi|     1/k and
thus

(cid:107)x     v(cid:107)    

   

d
k

.

to ensure that the right-hand side will be at most   we shall set k = (cid:100)   

d/ (cid:101).

plugging this value into equation (23.10) we conclude that

   

|q|     (3

d/(2 ))d (  /e)d/2 (d/2)   d/2 =

(cid:16) 3

(cid:113)   

 

2e

(cid:17)d    (cid:0) 3

 

(cid:1)d

.

let x be a vector that can be written as x = u    with u being some orthonor-
mal matrix and (cid:107)  (cid:107)0     s. combining the earlier covering property and the jl
lemma (lemma 23.4) enables us to show that a random w will not distort any
such x.
lemma 23.12 let u be an orthonormal d    d matrix and let i     [d] be a set
of indices of size |i| = s. let s be the span of {ui : i     i}, where ui is the ith
column of u . let        (0, 1),       (0, 1), and n     n such that

n     24

log(2/  ) + s log(12/ )

 2

.

then, with id203 of at least 1      over a choice of a random matrix w     rn,d
such that each element of w is independently distributed according to n (0, 1/n),
we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)w x(cid:107)

(cid:107)x(cid:107)     1

(cid:12)(cid:12)(cid:12)(cid:12) <  .

sup
x   s

it su   ces to prove the lemma for all x     s with (cid:107)x(cid:107) = 1. we can write
proof
x = ui    where        rs, (cid:107)  (cid:107)2 = 1, and ui is the matrix whose columns are
{ui : i     i}. using lemma 23.11 we know that there exists a set q of size
|q|     (12/ )s such that

sup

  :(cid:107)  (cid:107)=1

min
v   q

(cid:107)       v(cid:107)     ( /4).

but since u is orthogonal we also have that

sup

  :(cid:107)  (cid:107)=1

min
v   q

(cid:107)ui        ui v(cid:107)     ( /4).

applying lemma 23.4 on the set {ui v : v     q} we obtain that for n satisfying

23.3 compressed sensing

337

the condition given in the lemma, the following holds with id203 of at least
1       :

sup
v   q

this also implies that

(cid:107)ui v(cid:107)2     1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)w ui v(cid:107)2
(cid:12)(cid:12)(cid:12)(cid:12)(cid:107)w ui v(cid:107)

(cid:12)(cid:12)(cid:12)(cid:12)      /2,
(cid:12)(cid:12)(cid:12)(cid:12)      /2.

sup
v   q

(cid:107)ui v(cid:107)     1

let a be the smallest number such that

   x     s,

(cid:107)w x(cid:107)
(cid:107)x(cid:107)     1 + a.

clearly a <    . our goal is to show that a      . this follows from the fact that
for any x     s of unit norm there exists v     q such that (cid:107)x     ui v(cid:107)      /4 and
therefore

(cid:107)w x(cid:107)     (cid:107)w ui v(cid:107) + (cid:107)w (x     ui v)(cid:107)     1 +  /2 + (1 + a) /4.

thus,

   x     s,

(cid:107)w x(cid:107)
(cid:107)x(cid:107)     1 + ( /2 + (1 + a) /4) .

but the de   nition of a implies that

a      /2 + (1 + a) /4     a      /2 +  /4
1      /4

     .

this proves that for all x     s we have
this as well since

(cid:107)w x(cid:107)
(cid:107)x(cid:107)     1      . the other side follows from

(cid:107)w x(cid:107)     (cid:107)w ui v(cid:107)     (cid:107)w (x     ui v)(cid:107)     1      /2     (1 +  ) /4     1      .

the preceding lemma tells us that for x     s of unit norm we have

which implies that

(1      )     (cid:107)w x(cid:107)     (1 +  ),

(1     2  )     (cid:107)w x(cid:107)2     (1 + 3  ).

the proof of theorem 23.9 follows from this by a union bound over all choices
of i.

338

id84

23.4

pca or compressed sensing?

suppose we would like to apply a id84 technique to a given
set of examples. which method should we use, pca or compressed sensing? in
this section we tackle this question, by underscoring the underlying assumptions
behind the two methods.

it is helpful    rst to understand when each of the methods can guarantee per-
fect recovery. pca guarantees perfect recovery whenever the set of examples is
contained in an n dimensional subspace of rd. compressed sensing guarantees
perfect recovery whenever the set of examples is sparse (in some basis). on the
basis of these observations, we can describe cases in which pca will be better
than compressed sensing and vice versa.

as a    rst example, suppose that the examples are the vectors of the standard
basis of rd, namely, e1, . . . , ed, where each ei is the all zeros vector except 1 in the
ith coordinate. in this case, the examples are 1-sparse. hence, compressed sensing
will yield a perfect recovery whenever n        (log(d)). on the other hand, pca
will lead to poor performance, since the data is far from being in an n dimensional
subspace, as long as n < d. indeed, it is easy ro verify that in such a case, the
averaged recovery error of pca (i.e., the objective of equation (23.1) divided by
m) will be (d     n)/d, which is larger than 1/2 whenever n     d/2.

we next show a case where pca is better than compressed sensing. consider
m examples that are exactly on an n dimensional subspace. clearly, in such a
case, pca will lead to perfect recovery. as to compressed sensing, note that
the examples are n-sparse in any orthonormal basis whose    rst n vectors span
the subspace. therefore, compressed sensing would also work if we will reduce
the dimension to    (n log(d)). however, with exactly n dimensions, compressed
sensing might fail. pca has also better resilience to certain types of noise. see
(chang, weiss & freeman 2009) for a discussion.

23.5

summary

we introduced two methods for id84 using linear transfor-
mations: pca and random projections. we have shown that pca is optimal in
the sense of averaged squared reconstruction error, if we restrict the reconstruc-
tion procedure to be linear as well. however, if we allow nonlinear reconstruction,
pca is not necessarily the optimal procedure. in particular, for sparse data, ran-
dom projections can signi   cantly outperform pca. this fact is at the heart of
the compressed sensing method.

23.6 bibliographic remarks

339

23.6

bibliographic remarks

pca is equivalent to best subspace approximation using singular value decom-
position (svd). the svd method is described in appendix c. svd dates back
to eugenio beltrami (1873) and camille jordan (1874). it has been rediscovered
many times. in the statistical literature, it was introduced by pearson (1901). be-
sides pca and svd, there are additional names that refer to the same idea and
are being used in di   erent scienti   c communities. a few examples are the eckart-
young theorem (after carl eckart and gale young who analyzed the method in
1936), the schmidt-mirsky theorem, factor analysis, and the hotelling transform.
compressed sensing was introduced in donoho (2006) and in (candes & tao

2005). see also candes (2006).

23.7

exercises

1. in this exercise we show that in the general case, exact recovery of a linear

compression scheme is impossible.
1. let a     rn,d be an arbitrary compression matrix where n     d     1. show

that there exists u, v     rn, u (cid:54)= v such that au = av.

2. conclude that exact recovery of a linear compression scheme is impossible.

2. let        rd such that   1       2                  d     0. show that

d(cid:88)

n(cid:88)

max

     [0,1]d:(cid:107)  (cid:107)1   n

  j  j =

  j.

j=1

j=1

hint: take every vector        [0, 1]d such that (cid:107)  (cid:107)1     n. let i be the minimal
index for which   i < 1. if i = n + 1 we are done. otherwise, show that we can
increase   i, while possibly decreasing   j for some j > i, and obtain a better
solution. this will imply that the optimal solution is to set   i = 1 for i     n
and   i = 0 for i > n.

3. kernel pca: in this exercise we show how pca can be used for construct-
ing nonlinear id84 on the basis of the kernel trick (see
chapter 16).

let x be some instance space and let s = {x1, . . . , xm} be a set of points
in x . consider a feature mapping    : x     v , where v is some hilbert space
(possibly of in   nite dimension). let k : x    x be a id81, that is,
k(x, x(cid:48)) = (cid:104)  (x),   (x(cid:48))(cid:105). kernel pca is the process of mapping the elements
in s into v using   , and then applying pca over {  (x1), . . . ,   (xm)} into
rn. the output of this process is the set of reduced elements.

show how this process can be done in polynomial time in terms of m
and n, assuming that each evaluation of k(  ,  ) can be calculated in a con-
stant time. in particular, if your implementation requires multiplication of
two matrices a and b, verify that their product can be computed. similarly,

340

id84

if an eigenvalue decomposition of some matrix c is required, verify that this
decomposition can be computed.

4. an interpretation of pca as variance maximization:

let x1, . . . , xm be m vectors in rd, and let x be a random vector distributed
according to the uniform distribution over x1, . . . , xm. assume that e[x] = 0.
1. consider the problem of    nding a unit vector, w     rd, such that the
random variable (cid:104)w, x(cid:105) has maximal variance. that is, we would like to
solve the problem

argmax
w:(cid:107)w(cid:107)=1

var[(cid:104)w, x(cid:105)] = argmax
w:(cid:107)w(cid:107)=1

1
m

((cid:104)w, xi(cid:105))2.

m(cid:88)

i=1

show that the solution of the problem is to set w to be the    rst principle
vector of x1, . . . , xm.
2. let w1 be the    rst principal component as in the previous question. now,
suppose we would like to    nd a second unit vector, w2     rd, that maxi-
mizes the variance of (cid:104)w2, x(cid:105), but is also uncorrelated to (cid:104)w1, x(cid:105). that is,
we would like to solve:

argmax

w:(cid:107)w(cid:107)=1, e[((cid:104)w1,x(cid:105))((cid:104)w,x(cid:105))]=0

var[(cid:104)w, x(cid:105)].

show that the solution to this problem is to set w to be the second principal
component of x1, . . . , xm.
hint: note that

e[((cid:104)w1, x(cid:105))((cid:104)w, x(cid:105))] = w(cid:62)

1

e[xx(cid:62)]w = mw(cid:62)

1 aw,

where a = (cid:80)

i xix(cid:62)

constraint e[((cid:104)w1, x(cid:105))((cid:104)w, x(cid:105))] = 0 is equivalent to the constraint

i . since w is an eigenvector of a we have that the

(cid:104)w1, w(cid:105) = 0.

5. the relation between svd and pca: use the svd theorem (corol-

lary c.6) for providing an alternative proof of theorem 23.2.

6. random projections preserve inner products: the johnson-lindenstrauss

lemma tells us that a random projection preserves distances between a    nite
set of vectors. in this exercise you need to prove that if the set of vectors are
within the unit ball, then not only are the distances between any two vectors
preserved, but the inner product is also preserved.

let q be a    nite set of vectors in rd and assume that for every x     q we

have (cid:107)x(cid:107)     1.
1. let        (0, 1) and n be an integer such that
6 log(|q|2/  )

(cid:114)

  =

n

    3.

prove that with id203 of at least 1        over a choice of a random

23.7 exercises

341

matrix w     rn,d, where each element of w is independently distributed
according to n (0, 1/n), we have

|(cid:104)w u, w v(cid:105)     (cid:104)u, v(cid:105)|      

for every u, v     q.
hint: use jl to bound both

(cid:107)w (u+v)(cid:107)

(cid:107)u+v(cid:107)

and

(cid:107)w (u   v)(cid:107)

(cid:107)u   v(cid:107)

.

2. (*) let x1, . . . , xm be a set of vectors in rd of norm at most 1, and assume
that these vectors are linearly separable with margin of   . assume that
d (cid:29) 1/  2. show that there exists a constant c > 0 such that if we randomly
project these vectors into rn, for n = c/  2, then with id203 of at least
99% it holds that the projected vectors are linearly separable with margin
  /2.

24 generative models

we started this book with a distribution free learning framework; namely, we
did not impose any assumptions on the underlying distribution over the data.
furthermore, we followed a discriminative approach in which our goal is not to
learn the underlying distribution but rather to learn an accurate predictor. in
this chapter we describe a generative approach, in which it is assumed that the
underlying distribution over the data has a speci   c parametric form and our goal
is to estimate the parameters of the model. this task is called parametric density
estimation.

the discriminative approach has the advantage of directly optimizing the
quantity of interest (the prediction accuracy) instead of learning the underly-
ing distribution. this was phrased as follows by vladimir vapnik in his principle
for solving problems using a restricted amount of information:

when solving a given problem, try to avoid a more general problem as an intermediate
step.

of course, if we succeed in learning the underlying distribution accurately,
we are considered to be    experts    in the sense that we can predict by using
the bayes optimal classi   er. the problem is that it is usually more di   cult to
learn the underlying distribution than to learn an accurate predictor. however,
in some situations, it is reasonable to adopt the generative learning approach.
for example, sometimes it is easier (computationally) to estimate the parameters
of the model than to learn a discriminative predictor. additionally, in some cases
we do not have a speci   c task at hand but rather would like to model the data
either for making predictions at a later time without having to retrain a predictor
or for the sake of interpretability of the data.

we start with a popular statistical method for estimating the parameters of
the data, which is called the maximum likelihood principle. next, we describe two
generative assumptions which greatly simplify the learning process. we also de-
scribe the em algorithm for calculating the maximum likelihood in the presence
of latent variables. we conclude with a brief description of bayesian reasoning.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

24.1 maximum likelihood estimator

343

24.1

maximum likelihood estimator

let us start with a simple example. a drug company developed a new drug to
treat some deadly disease. we would like to estimate the id203 of survival
when using the drug. to do so, the drug company sampled a training set of m
people and gave them the drug. let s = (x1, . . . , xm) denote the training set,
where for each i, xi = 1 if the ith person survived and xi = 0 otherwise. we can
model the underlying distribution using a single parameter,        [0, 1], indicating
the id203 of survival.

we now would like to estimate the parameter    on the basis of the training
set s. a natural idea is to use the average number of 1   s in s as an estimator.
that is,

m(cid:88)

i=1

(cid:114)

     =

1
m

xi.

(24.1)

clearly, es[    ] =   . that is,      is an unbiased estimator of   . furthermore, since      is
the average of m i.i.d. binary random variables we can use hoe   ding   s inequality
to get that with id203 of at least 1        over the choice of s we have that

|           |    

log(2/  )

2 m

.

(24.2)

another interpretation of      is as the maximum likelihood estimator, as we
formally explain now. we    rst write the id203 of generating the sample s:

p[s = (x1, . . . , xm)] =

  xi (1       )1   xi =   

(cid:80)

(cid:80)
i xi (1       )
i(1   xi).

m(cid:89)

i=1

(cid:88)

(cid:88)

we de   ne the log likelihood of s, given the parameter   , as the log of the preceding
expression:

l(s;   ) = log (p[s = (x1, . . . , xm)]) = log(  )

xi + log(1       )

(1     xi).

i

i

the maximum likelihood estimator is the parameter that maximizes the likeli-
hood

         argmax

l(s;   ).

  

(24.3)

next, we show that in our case, equation (24.1) is a maximum likelihood esti-
mator. to see this, we take the derivative of l(s;   ) with respect to    and equate
it to zero:

(cid:80)

   

i xi
  

(cid:80)
i(1     xi)
1       

= 0.

solving the equation for    we obtain the estimator given in equation (24.1).

344

generative models

24.1.1

id113 for continuous random variables
let x be a continuous random variable. then, for most x     r we have p[x =
x] = 0 and therefore the de   nition of likelihood as given before is trivialized. to
overcome this technical problem we de   ne the likelihood as log of the density of
the id203 of x at x. that is, given an i.i.d. training set s = (x1, . . . , xm)
sampled according to a density distribution p   we de   ne the likelihood of s given
   as

(cid:32) m(cid:89)

m(cid:88)

l(s;   ) = log

p  (xi)

=

log(p  (xi)).

as before, the maximum likelihood estimator is a maximizer of l(s;   ) with
respect to   .

i=1

i=1

as an example, consider a gaussian random variable, for which the density

function of x is parameterized by    = (  ,   ) and is de   ned as follows:

(cid:33)

(cid:18)

p  (x) =

   
1
2  

  

exp

    (x       )2

2  2

we can rewrite the likelihood as
l(s;   ) =     1
2  2

m(cid:88)

i=1

(xi       )2     m log(  

(cid:19)

.

   

2   ).

to    nd a parameter    = (  ,   ) that optimizes this we take the derivative of the
likelihood w.r.t.    and w.r.t.    and compare it to 0. we obtain the following two
equations:

d
d  

d
d  

l(s;   ) =

l(s;   ) =

1
  2

1
  3

(xi       ) = 0

(xi       )2     m
  

= 0

m(cid:88)
m(cid:88)

i=1

i=1

m(cid:88)

i=1

     =

1
m

xi

and

     =

(xi         )2

(cid:118)(cid:117)(cid:117)(cid:116) 1

m

m(cid:88)

i=1

solving the preceding equations we obtain the maximum likelihood estimates:

note that the maximum likelihood estimate is not always an unbiased estimator.
for example, while      is unbiased, it is possible to show that the estimate      of
the variance is biased (exercise 1).

simplifying notation
to simplify our notation, we use p[x = x] in this chapter to describe both the
id203 that x = x (for discrete random variables) and the density of the
distribution at x (for continuous variables).

24.1 maximum likelihood estimator

345

24.1.2

maximum likelihood and empirical risk minimization

the maximum likelihood estimator shares some similarity with the empirical
risk minimization (erm) principle, which we studied extensively in previous
chapters. recall that in the erm principle we have a hypothesis class h and
we use the training set for choosing a hypothesis h     h that minimizes the
empirical risk. we now show that the maximum likelihood estimator is an erm
for a particular id168.

given a parameter    and an observation x, we de   ne the loss of    on x as

(cid:96)(  , x) =     log(p  [x]).

(24.4)

that is, (cid:96)(  , x) is the negation of the log-likelihood of the observation x, assuming
the data is distributed according to p  . this id168 is often referred to as
the log-loss. on the basis of this de   nition it is immediate that the maximum
likelihood principle is equivalent to minimizing the empirical risk with respect
to the id168 given in equation (24.4). that is,

argmin

  

(    log(p  [xi])) = argmax

  

log(p  [xi]).

assuming that the data is distributed according to a distribution p (not neces-
sarily of the parametric form we employ), the true risk of a parameter    becomes

m(cid:88)

i=1

p[x] log(p  [x])

(cid:18) p[x]

p  [x]

(cid:19)
(cid:125)

(cid:88)
(cid:124)

x

+

(cid:18) 1

p[x]

p[x] log

(cid:123)(cid:122)

h(p)

(cid:19)
(cid:125)

,

(24.5)

p[x] log

(cid:123)(cid:122)

dre[p||p  ]

m(cid:88)

i=1

e
x

[(cid:96)(  , x)] =    (cid:88)
(cid:88)
(cid:124)

=

x

x

where dre is called the relative id178, and h is called the id178 func-
tion. the relative id178 is a divergence measure between two probabilities.
for discrete variables, it is always nonnegative and is equal to 0 only if the two
distributions are the same. it follows that the true risk is minimal when p   = p.
the expression given in equation (24.5) underscores how our generative as-
sumption a   ects our density estimation, even in the limit of in   nite data. it
shows that if the underlying distribution is indeed of a parametric form, then by
choosing the correct parameter we can make the risk be the id178 of the distri-
bution. however, if the distribution is not of the assumed parametric form, even
the best parameter leads to an inferior model and the suboptimality is measured
by the relative id178 divergence.

24.1.3

generalization analysis

how good is the maximum likelihood estimator when we learn from a    nite
training set?

346

generative models

to answer this question we need to de   ne how we assess the quality of an approxi-
mated solution of the density estimation problem. unlike discriminative learning,
where there is a clear notion of    loss,    in generative learning there are various
ways to de   ne the loss of a model. on the basis of the previous subsection, one
natural candidate is the expected log-loss as given in equation (24.5).

in some situations, it is easy to prove that the maximum likelihood principle
guarantees low true risk as well. for example, consider the problem of estimating
the mean of a gaussian variable of unit variance. we saw previously that the
maximum likelihood estimator is the average:      = 1
i xi. let   (cid:63) be the optimal
m
parameter. then,

(cid:80)
(cid:18)p  (cid:63) [x]

(cid:19)

e

x   n (  (cid:63),1)

[(cid:96)(    , x)     (cid:96)(  (cid:63), x)] =

e

x   n (  (cid:63),1)

(cid:19)

(x         )2

log

(cid:18)

p    [x]
(x       (cid:63))2 +

    1
2
+ (  (cid:63)         )

1
2
e

x   n (  (cid:63),1)

[x]

+ (  (cid:63)         )   (cid:63)

(24.6)

=

=

=

=

e

x   n (  (cid:63),1)
    2
    (  (cid:63))2
2
2
    2
    (  (cid:63))2
2
2
(           (cid:63))2.
1
2

next, we note that      is the average of m gaussian variables and therefore it is
also distributed normally with mean   (cid:63) and variance   (cid:63)/m. from this fact we
can derive bounds of the form: with id203 of at least 1        we have that
|           (cid:63)|       where   depends on   (cid:63)/m and on   .

in some situations, the maximum likelihood estimator clearly over   ts. for
example, consider a bernoulli random variable x and let p[x = 1] =   (cid:63). as
we saw previously, using hoe   ding   s inequality we can easily derive a guarantee
on |  (cid:63)         | that holds with high id203 (see equation (24.2)). however, if
our goal is to obtain a small value of the expected log-id168 as de   ned in
equation (24.5) we might fail. for example, assume that   (cid:63) is nonzero but very
small. then, the id203 that no element of a sample of size m will be 1 is
(1       (cid:63))m, which is greater than e   2  (cid:63) m. it follows that whenever m     log(2)
2  (cid:63) ,
the id203 that the sample is all zeros is at least 50%, and in that case, the
maximum likelihood rule will set      = 0. but the true risk of the estimate      = 0
is

e
x     (cid:63)

[(cid:96)(    , x)] =   (cid:63)(cid:96)(    , 1) + (1       (cid:63))(cid:96)(    , 0)

=   (cid:63) log(1/    ) + (1       (cid:63)) log(1/(1         ))
=   (cid:63) log(1/0) =    .

this simple example shows that we should be careful in applying the maximum
likelihood principle.

to overcome over   tting, we can use the variety of tools we encountered pre-

24.2 naive bayes

347

viously in the book. a simple id173 technique is outlined in exercise
2.

24.2

naive bayes

the naive bayes classi   er is a classical demonstration of how generative as-
sumptions and parameter estimations simplify the learning process. consider
the problem of predicting a label y     {0, 1} on the basis of a vector of features
x = (x1, . . . , xd), where we assume that each xi is in {0, 1}. recall that the bayes
optimal classi   er is

hbayes(x) = argmax
y   {0,1}

p[y = y|x = x].

to describe the id203 function p[y = y|x = x] we need 2d parameters,
each of which corresponds to p[y = 1|x = x] for a certain value of x     {0, 1}d.
this implies that the number of examples we need grows exponentially with the
number of features.

in the naive bayes approach we make the (rather naive) generative assumption

that given the label, the features are independent of each other. that is,

d(cid:89)

p[x = x|y = y] =

p[xi = xi|y = y].

with this assumption and using bayes    rule, the bayes optimal classi   er can be
further simpli   ed:

i=1

hbayes(x) = argmax
y   {0,1}

= argmax
y   {0,1}

p[y = y|x = x]
p[y = y]p[x = x|y = y]

= argmax
y   {0,1}

p[y = y]

p[xi = xi|y = y].

(24.7)

d(cid:89)

i=1

that is, now the number of parameters we need to estimate is only 2d + 1.
here, the generative assumption we made reduced signi   cantly the number of
parameters we need to learn.

when we also estimate the parameters using the maximum likelihood princi-

ple, the resulting classi   er is called the naive bayes classi   er.

24.3

id156

id156 (lda) is another demonstration of how generative
assumptions simplify the learning process. as in the naive bayes classi   er we
consider again the problem of predicting a label y     {0, 1} on the basis of a

348

generative models

vector of features x = (x1, . . . , xd). but now the generative assumption is as
follows. first, we assume that p[y = 1] = p[y = 0] = 1/2. second, we assume
that the id155 of x given y is a gaussian distribution. finally,
the covariance matrix of the gaussian distribution is the same for both values
of the label. formally, let   0,   1     rd and let    be a covariance matrix. then,
the density distribution is given by

p[x = x|y = y] =

1

(2  )d/2|  |1/2

exp

    1
2

(x       y)t      1(x       y)

(cid:19)

.

(cid:18)

as we have shown in the previous section, using bayes    rule we can write

this means that we will predict hbayes(x) = 1 i   

hbayes(x) = argmax
y   {0,1}

p[y = y]p[x = x|y = y].

(cid:18)p[y = 1]p[x = x|y = 1]

p[y = 0]p[x = x|y = 0]

(cid:19)

> 0.

log

this ratio is often called the log-likelihood ratio.

in our case, the log-likelihood ratio becomes

1

2 (x       0)t      1(x       0)     1

2 (x       1)t      1(x       1)

we can rewrite this as (cid:104)w, x(cid:105) + b where

w = (  1       0)t      1

and b = 1
2

(cid:0)  t

0      1  0       t

1      1  1

(24.8)

(cid:1) .

as a result of the preceding derivation we obtain that under the aforemen-
tioned generative assumptions, the bayes optimal classi   er is a linear classi   er.
additionally, one may train the classi   er by estimating the parameter   0,   1
and    from the data, using, for example, the maximum likelihood estimator.
with those estimators at hand, the values of w and b can be calculated as in
equation (24.8).

24.4

latent variables and the em algorithm

in generative models we assume that the data is generated by sampling from
a speci   c parametric distribution over our instance space x . sometimes, it is
convenient to express this distribution using latent random variables. a natural
example is a mixture of k gaussian distributions. that is, x = rd and we
assume that each x is generated as follows. first, we choose a random number in
{1, . . . , k}. let y be a random variable corresponding to this choice, and denote
p[y = y] = cy. second, we choose x on the basis of the value of y according to
a gaussian distribution

(cid:18)

(cid:19)
y (x       y)

.

(24.9)

p[x = x|y = y] =

1

(2  )d/2|  y|1/2

exp

    1
2

(x       y)t      1

24.4 latent variables and the em algorithm

349

therefore, the density of x can be written as:

p[x = x] =

=

p[y = y]p[x = x|y = y]

(cid:18)

cy

1

(2  )d/2|  y|1/2

exp

    1
2

(x       y)t      1

y (x       y)

(cid:19)

.

k(cid:88)
k(cid:88)

y=1

y=1

note that y is a hidden variable that we do not observe in our data. neverthe-
less, we introduce y since it helps us describe a simple parametric form of the
id203 of x.

more generally, let    be the parameters of the joint distribution of x and y
(e.g., in the preceding example,    consists of cy,   y, and   y, for all y = 1, . . . , k).
then, the log-likelihood of an observation x can be written as

log (p  [x = x]) = log

p  [x = x, y = y]

.

(cid:32) k(cid:88)

y=1

given an i.i.d. sample, s = (x1, . . . , xm), we would like to    nd    that maxi-

mizes the log-likelihood of s,

l(  ) = log

m(cid:88)
m(cid:88)

i=1

=

=

m(cid:89)

i=1

p  [x = xi]

log p  [x = xi]

(cid:32) k(cid:88)

log

p  [x = xi, y = y]

.

i=1

y=1

(cid:33)

(cid:33)

the maximum-likelihood estimator is therefore the solution of the maximization
problem

m(cid:88)

(cid:32) k(cid:88)

(cid:33)

argmax

l(  ) = argmax

log

  

  

i=1

y=1

p  [x = xi, y = y]

.

in many situations, the summation inside the log makes the preceding opti-
mization problem computationally hard. the expectation-maximization (em)
algorithm, due to dempster, laird, and rubin, is an iterative procedure for
searching a (local) maximum of l(  ). while em is not guaranteed to    nd the
global maximum, it often works reasonably well in practice.

em is designed for those cases in which, had we known the values of the latent
variables y , then the maximum likelihood optimization problem would have been
tractable. more precisely, de   ne the following function over m    k matrices and
the set of parameters   :

f (q,   ) =

qi,y log (p  [x = xi, y = y]) .

m(cid:88)

k(cid:88)

i=1

y=1

350

generative models

if each row of q de   nes a id203 over the ith latent variable given x = xi,
then we can interpret f (q,   ) as the expected log-likelihood of a training set
(x1, y1), . . . , (xm, ym), where the expectation is with respect to the choice of
each yi on the basis of the ith row of q. in the de   nition of f , the summation is
outside the log, and we assume that this makes the optimization problem with
respect to    tractable:
assumption 24.1 for any matrix q     [0, 1]m,k, such that each row of q sums
to 1, the optimization problem

is tractable.

argmax

f (q,   )

  

the intuitive idea of em is that we have a    chicken and egg    problem. on one
hand, had we known q, then by our assumption, the optimization problem of
   nding the best    is tractable. on the other hand, had we known the parameters
   we could have set qi,y to be the id203 of y = y given that x = xi.
the em algorithm therefore alternates between    nding    given q and    nding q
given   . formally, em    nds a sequence of solutions (q(1),   (1)), (q(2),   (2)), . . .
where at iteration t, we construct (q(t+1),   (t+1)) by performing two steps.
    expectation step: set

i,y = p  (t)[y = y|x = xi].
q(t+1)

(24.10)

this step is called the expectation step, because it yields a new probabil-
ity over the latent variables, which de   nes a new expected log-likelihood
function over   .

    maximization step: set   (t+1) to be the maximizer of the expected log-

likelihood, where the expectation is according to q(t+1):

  (t+1) = argmax

f (q(t+1),   ).

(24.11)

  

by our assumption, it is possible to solve this optimization problem e   -
ciently.

the initial values of   (1) and q(1) are usually chosen at random and the
procedure terminates after the improvement in the likelihood value stops being
signi   cant.

24.4.1

em as an alternate maximization algorithm

to analyze the em algorithm, we    rst view it as an alternate maximization
algorithm. de   ne the following objective function

g(q,   ) = f (q,   )     m(cid:88)

k(cid:88)

qi,y log(qi,y).

i=1

y=1

24.4 latent variables and the em algorithm

351

the second term is the sum of the entropies of the rows of q. let

(cid:40)

q =

q     [0, 1]m,k :    i,

qi,y = 1

(cid:41)

k(cid:88)

y=1

be the set of matrices whose rows de   ne probabilities over [k]. the following
lemma shows that em performs alternate maximization iterations for maximiz-
ing g.

lemma 24.2 the em procedure can be rewritten as:

q(t+1) = argmax

q   q
  (t+1) = argmax

g(q,   (t))

g(q(t+1),   ) .

  

furthermore, g(q(t+1),   (t)) = l(  (t)).

proof given q(t+1) we clearly have that

argmax

g(q(t+1),   ) = argmax

f (q(t+1),   ).

  

  

therefore, we only need to show that for any   , the solution of argmaxq   q g(q,   )
is to set qi,y = p  [y = y|x = xi]. indeed, by jensen   s inequality, for any q     q
we have that

g(q,   ) =

qi,y log

(cid:32) k(cid:88)
(cid:32)

y=1

log

(cid:32) k(cid:88)
(cid:32) k(cid:88)

y=1

(cid:19)(cid:33)
(cid:33)(cid:33)

(cid:18)p  [x = xi, y = y]

p  [x = xi, y = y]

qi,y

qi,y

qi,y

(cid:33)

log

p  [x = xi, y = y]

y=1

log (p  [x = xi]) = l(  ),

i=1

m(cid:88)
    m(cid:88)
m(cid:88)
m(cid:88)

i=1

i=1

=

=

i=1

352

generative models

while for qi,y = p  [y = y|x = xi] we have

p  [y = y|x = xi] log

(cid:18)p  [x = xi, y = y]

p  [y = y|x = xi]

(cid:19)(cid:33)

(cid:32) k(cid:88)
k(cid:88)

y=1

i=1

m(cid:88)
m(cid:88)
m(cid:88)
m(cid:88)

i=1

i=1

g(q,   ) =

=

=

=

p  [y = y|x = xi] log (p  [x = xi])

y=1

log (p  [x = xi])

k(cid:88)

y=1

log (p  [x = xi]) = l(  ).

p  [y = y|x = xi]

i=1

this shows that setting qi,y = p  [y = y|x = xi] maximizes g(q,   ) over q     q
and shows that g(q(t+1),   (t)) = l(  (t)).

the preceding lemma immediately implies:

theorem 24.3 the em procedure never decreases the log-likelihood; namely,
for all t,

l(  (t+1))     l(  (t)).

proof by the lemma we have

l(  (t+1)) = g(q(t+2),   (t+1))     g(q(t+1),   (t)) = l(  (t)).

24.4.2

em for mixture of gaussians (soft id116)
consider the case of a mixture of k gaussians in which    is a triplet (c,{  1, . . . ,   k},{  1, . . . ,   k})
where p  [y = y] = cy and p  [x = x|y = y] is as given in equation (24.9). for
simplicity, we assume that   1 =   2 =        =   k = i, where i is the identity
matrix. specifying the em algorithm for this case we obtain the following:
    expectation step: for each i     [m] and y     [k] we have that

p  (t) [y = y|x = xi] =

p  (t) [y = y]p  (t)[x = xi|y = y]

(cid:18)

1
zi
1
zi

(cid:19)

=

where zi is a id172 factor which ensures that(cid:80)

(cid:107)xi       (t)
y (cid:107)2
y p  (t) [y = y|x =
    maximization step: we need to set   t+1 to be a maximizer of equation (24.11),

xi] sums to 1.

    1
2

(24.12)

c(t)
y

exp

,

24.5 bayesian reasoning

353

which in our case amounts to maximizing the following expression w.r.t. c
and   :

(cid:18)

(cid:19)

p  (t) [y = y|x = xi]

log(cy)     1
2

(cid:107)xi       y(cid:107)2

.

(24.13)

m(cid:88)

k(cid:88)

i=1

y=1

comparing the derivative of equation (24.13) w.r.t.   y to zero and rear-
ranging terms we obtain:

(cid:80)m
(cid:80)m
i=1 p  (t) [y = y|x = xi] xi
i=1 p  (t) [y = y|x = xi]

.

  y =

that is,   y is a weighted average of the xi where the weights are according
to the probabilities calculated in the e step. to    nd the optimal c we need
to be more careful since we must ensure that c is a id203 vector. in
exercise 3 we show that the solution is:

(cid:80)m
(cid:80)m
i=1 p  (t)[y = y|x = xi]
i=1 p  (t)[y = y(cid:48)|x = xi]

y(cid:48)=1

(cid:80)k

cy =

.

(24.14)

it is interesting to compare the preceding algorithm to the id116 algorithm
described in chapter 22. in the id116 algorithm, we    rst assign each example
to a cluster according to the distance (cid:107)xi       y(cid:107). then, we update each center
  y according to the average of the examples assigned to this cluster. in the em
approach, however, we determine the id203 that each example belongs to
each cluster. then, we update the centers on the basis of a weighted sum over
the entire sample. for this reason, the em approach for id116 is sometimes
called    soft id116.   

24.5

bayesian reasoning

the maximum likelihood estimator follows a frequentist approach. this means
that we refer to the parameter    as a    xed parameter and the only problem is
that we do not know its value. a di   erent approach to parameter estimation
is called bayesian reasoning. in the bayesian approach, our uncertainty about
   is also modeled using id203 theory. that is, we think of    as a random
variable as well and refer to the distribution p[  ] as a prior distribution. as its
name indicates, the prior distribution should be de   ned by the learner prior to
observing the data.

as an example, let us consider again the drug company which developed a
new drug. on the basis of past experience, the statisticians at the drug company
believe that whenever a drug has reached the level of clinic experiments on
people, it is likely to be e   ective. they model this prior belief by de   ning a
density distribution on    such that

(cid:40)

p[  ] =

0.8

0.2

if    > 0.5
if        0.5

(24.15)

354

generative models

as before, given a speci   c value of   , it is assumed that the conditional proba-
bility, p[x = x|  ], is known. in the drug company example, x takes values in
{0, 1} and p[x = x|  ] =   x(1       )1   x.

once the prior distribution over    and the conditional distribution over x
given    are de   ned, we again have complete knowledge of the distribution over
x. this is because we can write the id203 over x as a marginal id203

(cid:88)

(cid:88)

p[x = x] =

p[x = x,   ] =

p[  ]p[x = x|  ],

  

  

where the last equality follows from the de   nition of id155. if
   is continuous we replace p[  ] with the density function and the sum becomes
an integral:

(cid:90)

p[x = x] =

p[  ]p[x = x|  ] d  .

  

seemingly, once we know p[x = x], a training set s = (x1, . . . , xm) tells us
nothing as we are already experts who know the distribution over a new point
x. however, the bayesian view introduces dependency between s and x. this is
because we now refer to    as a random variable. a new point x and the previous
points in s are independent only conditioned on   . this is di   erent from the
frequentist philosophy in which    is a parameter that we might not know, but
since it is just a parameter of the distribution, a new point x and previous points
s are always independent.

in the bayesian framework, since x and s are not independent anymore, what
we would like to calculate is the id203 of x given s, which by the chain
rule can be written as follows:

p[x = x|s] =

p[x = x|  , s]p[  |s] =

p[x = x|  ]p[  |s].

(cid:88)

(cid:88)

  

  

the second inequality follows from the assumption that x and s are independent
when we condition on   . using bayes    rule we have
p[s|  ]p[  ]

p[  |s] =

p[s]

,

and together with the assumption that points are independent conditioned on   ,
we can write

p[  |s] =

p[s|  ]p[  ]

p[s]

=

1
p[s]

p[x = xi|  ]p[  ].

we therefore obtain the following expression for bayesian prediction:

p[x = x|s] =

1
p[s]

p[x = x|  ]

p[x = xi|  ]p[  ].

(24.16)

getting back to our drug company example, we can rewrite p[x = x|s] as

  x+(cid:80)

i xi(1       )1   x+(cid:80)

i(1   xi) p[  ] d  .

p[x = x|s] =

1

p [s]

m(cid:89)
m(cid:89)

i=1

i=1

(cid:88)
(cid:90)

  

24.6 summary

355

it is interesting to note that when p[  ] is uniform we obtain that

p[x = x|s]    

i(1   xi) d  .

solving the preceding integral (using integration by parts) we obtain

(cid:90)

  x+(cid:80)

i xi(1       )1   x+(cid:80)
((cid:80)

i xi) + 1
m + 2

.

p[x = 1|s] =

(cid:80)

recall that the prediction according to the maximum likelihood principle in this
case is p[x = 1|    ] =
i xi
m . the bayesian prediction with uniform prior is rather
similar to the maximum likelihood prediction, except it adds    pseudoexamples   
to the training set, thus biasing the prediction toward the uniform prior.

maximum a posteriori
in many situations, it is di   cult to    nd a closed form solution to the integral
given in equation (24.16). several numerical methods can be used to approxi-
mate this integral. another popular solution is to    nd a single    which maximizes
p[  |s]. the value of    which maximizes p[  |s] is called the maximum a poste-
riori estimator. once this value is found, we can calculate the id203 that
x = x given the maximum a posteriori estimator and independently on s.

24.6

summary

in the generative approach to machine learning we aim at modeling the distri-
bution over the data. in particular, in parametric density estimation we further
assume that the underlying distribution over the data has a speci   c paramet-
ric form and our goal is to estimate the parameters of the model. we have
described several principles for parameter estimation, including maximum like-
lihood, bayesian estimation, and maximum a posteriori. we have also described
several speci   c algorithms for implementing the maximum likelihood under dif-
ferent assumptions on the underlying data distribution, in particular, naive
bayes, lda, and em.

24.7

bibliographic remarks

the maximum likelihood principle was studied by ronald fisher in the beginning
of the 20th century. bayesian statistics follow bayes    rule, which is named after
the 18th century english mathematician thomas bayes.

there are many excellent books on the generative and bayesian approaches
to machine learning. see, for example, (bishop 2006, koller & friedman 2009,
mackay 2003, murphy 2012, barber 2012).

356

generative models

24.8

exercises

1. prove that the maximum likelihood estimator of the variance of a gaussian

variable is biased.

2. id173 for maximum likelihood: consider the following regularized

loss minimization:

m(cid:88)

i=1

1
m

log(1/p  [xi]) +

1
m

(log(1/  ) + log(1/(1       ))) .

    show that the preceding objective is equivalent to the usual empirical error
had we added two pseudoexamples to the training set. conclude that
the regularized maximum likelihood estimator would be

(cid:32)

(cid:33)

m(cid:88)

i=1

     =

1

m + 2

1 +

xi

.

    derive a high id203 bound on |         (cid:63)|. hint: rewrite this as |       e[    ]+
e[    ]      (cid:63)| and then use the triangle inequality and hoe   ding inequality.
    use this to bound the true risk. hint: use the fact that now          1
m+2 to

relate |           (cid:63)| to the relative id178.

3.     consider a general optimization problem of the form:

k(cid:88)

max

c

  y log(cy)

s.t.

cy > 0,

cy = 1 ,

y=1

y

+ is a vector of nonnegative weights. verify that the m step

where        rk
of soft id116 involves solving such an optimization problem.

    let c(cid:63) = 1(cid:80)
    show that the optimization problem is equivalent to the problem:

  . show that c(cid:63) is a id203 vector.

y   y

dre(c(cid:63)||c)

min

c

s.t.

cy > 0,

cy = 1 .

    using properties of the relative id178, conclude that c(cid:63) is the solution to

the optimization problem.

(cid:88)

(cid:88)

y

25 feature selection and generation

in the beginning of the book, we discussed the abstract model of learning, in
which the prior knowledge utilized by the learner is fully encoded by the choice
of the hypothesis class. however, there is another modeling choice, which we
have so far ignored: how do we represent the instance space x ? for example, in
the papayas learning problem, we proposed the hypothesis class of rectangles in
the softness-color two dimensional plane. that is, our    rst modeling choice was
to represent a papaya as a two dimensional point corresponding to its softness
and color. only after that did we choose the hypothesis class of rectangles as a
class of mappings from the plane into the label set. the transformation from the
real world object    papaya    into the scalar representing its softness or its color
is called a feature function or a feature for short; namely, any measurement of
the real world object can be regarded as a feature. if x is a subset of a vector
space, each x     x is sometimes referred to as a feature vector. it is important to
understand that the way we encode real world objects as an instance space x is
by itself prior knowledge about the problem.

furthermore, even when we already have an instance space x which is rep-
resented as a subset of a vector space, we might still want to change it into a
di   erent representation and apply a hypothesis class on top of it. that is, we
may de   ne a hypothesis class on x by composing some class h on top of a
feature function which maps x into some other vector space x (cid:48). we have al-
ready encountered examples of such compositions     in chapter 15 we saw that
kernel-based id166 learns a composition of the class of halfspaces over a feature
mapping    that maps each original instance in x into some hilbert space. and,
indeed, the choice of    is another form of prior knowledge we impose on the
problem.

in this chapter we study several methods for constructing a good feature set.
we start with the problem of feature selection, in which we have a large pool
of features and our goal is to select a small number of features that will be
used by our predictor. next, we discuss feature manipulations and id172.
these include simple transformations that we apply on our original features. such
transformations may decrease the sample complexity of our learning algorithm,
its bias, or its computational complexity. last, we discuss several approaches for
id171. in these methods, we try to automate the process of feature
construction.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

358

feature selection and generation

we emphasize that while there are some common techniques for feature learn-
ing one may want to try, the no-free-lunch theorem implies that there is no ulti-
mate feature learner. any id171 algorithm might fail on some problem.
in other words, the success of each feature learner relies (sometimes implicitly)
on some form of prior assumption on the data distribution. furthermore, the
relative quality of features highly depends on the learning algorithm we are later
going to apply using these features. this is illustrated in the following example.
example 25.1 consider a regression problem in which x = r2, y = r, and
the id168 is the squared loss. suppose that the underlying distribution
is such that an example (x, y) is generated as follows: first, we sample x1 from
the uniform distribution over [   1, 1]. then, we deterministically set y = x1
2.
finally, the second feature is set to be x2 = y + z, where z is sampled from the
uniform distribution over [   0.01, 0.01]. suppose we would like to choose a single
feature. intuitively, the    rst feature should be preferred over the second feature
as the target can be perfectly predicted based on the    rst feature alone, while it
cannot be perfectly predicted based on the second feature. indeed, choosing the
   rst feature would be the right choice if we are later going to apply polynomial
regression of degree at least 2. however, if the learner is going to be a linear
regressor, then we should prefer the second feature over the    rst one, since the
optimal linear predictor based on the    rst feature will have a larger risk than
the optimal linear predictor based on the second feature.

feature selection
throughout this section we assume that x = rd. that is, each instance is repre-
sented as a vector of d features. our goal is to learn a predictor that only relies
on k (cid:28) d features. predictors that use only a small subset of features require a
smaller memory footprint and can be applied faster. furthermore, in applications
such as medical diagnostics, obtaining each possible    feature    (e.g., test result)
can be costly; therefore, a predictor that uses only a small number of features
is desirable even at the cost of a small degradation in performance, relative to
a predictor that uses more features. finally, constraining the hypothesis class to
use a small subset of features can reduce its estimation error and thus prevent
over   tting.

ideally, we could have tried all subsets of k out of d features and choose the
subset which leads to the best performing predictor. however, such an exhaustive
search is usually computationally intractable. in the following we describe three
computationally feasible approaches for feature selection. while these methods
cannot guarantee    nding the optimal subset, they often work reasonably well in
practice. some of the methods come with formal guarantees on the quality of the
selected subsets under certain assumptions. we do not discuss these guarantees
here.

25.1

25.1 feature selection

359

25.1.1

filters

maybe the simplest approach for feature selection is the    lter method, in which
we assess individual features, independently of other features, according to some
quality measure. we can then select the k features that achieve the highest score
(alternatively, decide also on the number of features to select according to the
value of their scores).

many quality measures for features have been proposed in the literature.
maybe the most straightforward approach is to set the score of a feature ac-
cording to the error rate of a predictor that is trained solely by that feature.
to illustrate this, consider a id75 problem with the squared loss.
let v = (x1,j, . . . , xm,j)     rm be a vector designating the values of the jth
feature on a training set of m examples and let y = (y1, . . . , ym)     rm be the
values of the target on the same m examples. the empirical squared loss of an
erm linear predictor that uses only the jth feature would be

min
a,b   r

1
m

(cid:107)av + b     y(cid:107)2,

(cid:80)m

where the meaning of adding a scalar b to a vector v is adding b to all coordinates
of v. to solve this problem, let   v = 1
i=1 vi be the averaged value of the
m
feature and let   y = 1
i=1 yi be the averaged value of the target. clearly (see
m
exercise 1),

(cid:80)m

min
a,b   r

1
m

(cid:107)av + b     y(cid:107)2 = min
a,b   r

1
m

(cid:107)a(v       v) + b     (y       y)(cid:107)2.

(25.1)

taking the derivative of the right-hand side objective with respect to b and
comparing it to zero we obtain that b = 0. similarly, solving for a (once we know
that b = 0) yields a = (cid:104)v       v, y       y(cid:105)/(cid:107)v       v(cid:107)2. plugging this value back into the
objective we obtain the value

(cid:107)y       y(cid:107)2     ((cid:104)v       v, y       y(cid:105))2

(cid:107)v       v(cid:107)2

.

ranking the features according to the minimal loss they achieve is equivalent
to ranking them according to the absolute value of the following score (where
now a higher score yields a better feature):

(cid:104)v       v, y       y(cid:105)
(cid:107)v       v(cid:107)(cid:107)y       y(cid:107) =

1

(cid:113) 1
m(cid:107)v       v(cid:107)2

(cid:113) 1
m (cid:104)v       v, y       y(cid:105)
m(cid:107)y       y(cid:107)2

.

(25.2)

the preceding expression is known as pearson   s correlation coe   cient. the nu-
merator is the empirical estimate of the covariance of the jth feature and the
target value, e[(v     e v)(y     e y)], while the denominator is the squared root of
the empirical estimate for the variance of the jth feature, e[(v     e v)2], times
the variance of the target. pearson   s coe   cient ranges from    1 to 1, where if
the pearson   s coe   cient is either 1 or    1, there is a linear mapping from v to y
with zero empirical risk.

360

feature selection and generation

if pearson   s coe   cient equals zero it means that the optimal linear function
from v to y is the all-zeros function, which means that v alone is useless for
predicting y. however, this does not mean that v is a bad feature, as it might
be the case that together with other features v can perfectly predict y. indeed,
consider a simple example in which the target is generated by the function y =
x1 + 2x2. assume also that x1 is generated from the uniform distribution over
{  1}, and x2 =     1
2 z, where z is also generated i.i.d. from the uniform
distribution over {  1}. then, e[x1] = e[x2] = e[y] = 0, and we also have

2 x1 + 1

e[yx1] = e[x2

1] + 2 e[x2x1] = e[x2

1]     e[x2

1] + e[zx1] = 0.

therefore, for a large enough training set, the    rst feature is likely to have a
pearson   s correlation coe   cient that is close to zero, and hence it will most
probably not be selected. however, no function can predict the target value well
without knowing the    rst feature.

there are many other score functions that can be used by a    lter method.
notable examples are estimators of the mutual information or the area under
the receiver operating characteristic (roc) curve. all of these score functions
su   er from similar problems to the one illustrated previously. we refer the reader
to guyon & elissee    (2003).

25.1.2

greedy selection approaches

greedy selection is another popular approach for feature selection. unlike    lter
methods, greedy selection approaches are coupled with the underlying learning
algorithm. the simplest instance of greedy selection is forward greedy selection.
we start with an empty set of features, and then we gradually add one feature
at a time to the set of selected features. given that our current set of selected
features is i, we go over all i /    i, and apply the learning algorithm on the set
of features i     {i}. each such application yields a di   erent predictor, and we
choose to add the feature that yields the predictor with the smallest risk (on
the training set or on a validation set). this process continues until we either
select k features, where k is a prede   ned budget of allowed features, or achieve
an accurate enough predictor.

example 25.2 (orthogonal matching pursuit) to illustrate the forward
greedy selection approach, we specify it to the problem of id75 with
the squared loss. let x     rm,d be a matrix whose rows are the m training
instances. let y     rm be the vector of the m labels. for every i     [d], let xi
be the ith column of x. given a set i     [d] we denote by xi the matrix whose
columns are {xi : i     i}.
the forward greedy selection method starts with i0 =    . at iteration t, we

look for the feature index jt, which is in

argmin

j

min
w   rt

(cid:107)xit   1   {j}w     y(cid:107)2.

25.1 feature selection

361

then, we update it = it   1     {jt}.

we now describe a more e   cient implementation of the forward greedy selec-
tion approach for id75 which is called orthogonal matching pursuit
(omp). the idea is to keep an orthogonal basis of the features aggregated so
far. let vt be a matrix whose columns form an orthonormal basis of the columns
of xit.

clearly,

(cid:107)xitw     y(cid:107)2 = min
     rt

min

w

(cid:107)vt       y(cid:107)2.

we will maintain a vector   t which minimizes the right-hand side of the equation.
initially, we set i0 =    , v0 =    , and   1 to be the empty vector. at round t, for
every j, we decompose xj = vj + uj where vj = vt   1v (cid:62)
t   1xj is the projection
of xj onto the subspace spanned by vt   1 and uj is the part of xj orthogonal to
vt   1 (see appendix c). then,

min
  ,  

= min
  ,  

= min
  ,  

(cid:107)vt   1   +   uj     y(cid:107)2

(cid:2)(cid:107)vt   1       y(cid:107)2 +   2(cid:107)uj(cid:107)2 + 2  (cid:104)uj, vt   1       y(cid:105)(cid:3)
(cid:2)(cid:107)vt   1       y(cid:107)2 +   2(cid:107)uj(cid:107)2 + 2  (cid:104)uj,   y(cid:105)(cid:3)
(cid:2)  2(cid:107)uj(cid:107)2     2  (cid:104)uj, y(cid:105)(cid:3)
(cid:2)(cid:107)vt   1       y(cid:107)2(cid:3) + min
=(cid:2)(cid:107)vt   1  t   1     y(cid:107)2(cid:3) + min
(cid:2)  2(cid:107)uj(cid:107)2     2  (cid:104)uj, y(cid:105)(cid:3)
= (cid:107)vt   1  t   1     y(cid:107)2     ((cid:104)uj, y(cid:105))2
(cid:107)uj(cid:107)2

= min

  

  

  

.

it follows that we should select the feature

jt = argmax

j

((cid:104)uj, y(cid:105))2
(cid:107)uj(cid:107)2

.

the rest of the update is to set

(cid:20)

vt =

vt   1,

ujt
(cid:107)ujt(cid:107)2

(cid:21)

(cid:20)

  t   1 ;

(cid:21)

.

(cid:104)ujt, y(cid:105)
(cid:107)ujt(cid:107)2

,

  t =

the omp procedure maintains an orthonormal basis of the selected features,
where in the preceding description, the orthoid172 property is obtained
by a procedure similar to gram-schmidt orthoid172. in practice, the
gram-schmidt procedure is often numerically unstable. in the pseudocode that
follows we use svd (see section c.4) at the end of each round to obtain an
orthonormal basis in a numerically stable manner.

362

feature selection and generation

orthogonal matching pursuit (omp)

input:

data matrix x     rm,d, labels vector y     rm,
budget of features t

initialize: i1 =    
for t = 1, . . . , t

use svd to    nd an orthonormal basis v     rm,t   1 of xit
(for t = 1 set v to be the all zeros matrix)
foreach j     [d] \ it let uj = xj     v v (cid:62)xj
let jt = argmaxj /   it:(cid:107)uj(cid:107)>0
update it+1 = it     {jt}

((cid:104)uj ,y(cid:105))2
(cid:107)uj(cid:107)2

output it +1

more e   cient greedy selection criteria
let r(w) be the empirical risk of a vector w. at each round of the forward
greedy selection method, and for every possible j, we should minimize r(w)
over the vectors w whose support is it   1     {j}. this might be time consuming.

a simpler approach is to choose jt that minimizes

argmin

j

     r r(wt   1 +   ej),
min

where ej is the all zeros vector except 1 in the jth element. that is, we keep
the weights of the previously chosen coordinates intact and only optimize over
the new variable. therefore, for each j we need to solve an optimization problem
over a single variable, which is a much easier task than optimizing over t.

an even simpler approach is to upper bound r(w) using a    simple    function
and then choose the feature which leads to the largest decrease in this upper
bound. for example, if r is a   -smooth function (see equation (12.5) in chap-
ter 12), then

r(w +   ej)     r(w) +   

   r(w)

   wj

+     2/2.

minimizing the right-hand side over    yields    =        r(w)
value into the above yields

   wj

   1
   and plugging this

.

(cid:18)    r(w)

(cid:19)2

   wj

r(w +   ej)     r(w)     1
2  

this value is minimized if the partial derivative of r(w) with respect to wj is
maximal. we can therefore choose jt to be the index of the largest coordinate of
the gradient of r(w) at w.

remark 25.3 (adaboost as a forward greedy selection procedure)
it is pos-
sible to interpret the adaboost algorithm from chapter 10 as a forward greedy

25.1 feature selection

363

selection procedure with respect to the function

       m(cid:88)

exp

         yi

d(cid:88)

             .

wjhj(xi)

(25.3)

i=1

j=1

r(w) = log

see exercise 3.

backward elimination
another popular greedy selection approach is backward elimination. here, we
start with the full set of features, and then we gradually remove one feature at a
time from the set of features. given that our current set of selected features is i,
we go over all i     i, and apply the learning algorithm on the set of features i\{i}.
each such application yields a di   erent predictor, and we choose to remove the
feature i for which the predictor obtained from i \ {i} has the smallest risk (on
the training set or on a validation set).

naturally, there are many possible variants of the backward elimination idea.

it is also possible to combine forward and backward greedy steps.

25.1.3

sparsity-inducing norms

the problem of minimizing the empirical risk subject to a budget of k features
can be written as

min

w

ls(w)

s.t.

(cid:107)w(cid:107)0     k,

where1

(cid:107)w(cid:107)0 = |{i : wi (cid:54)= 0}|.

in other words, we want w to be sparse, which implies that we only need to
measure the features corresponding to nonzero elements of w.

convex function (cid:107)w(cid:107)0 with the (cid:96)1 norm, (cid:107)w(cid:107)1 = (cid:80)d

solving this optimization problem is computationally hard (natarajan 1995,
davis, mallat & avellaneda 1997). a possible relaxation is to replace the non-
i=1 |wi|, and to solve the

problem

min

w

ls(w)

s.t. (cid:107)w(cid:107)1     k1,

(25.4)

where k1 is a parameter. since the (cid:96)1 norm is a convex function, this problem
can be solved e   ciently as long as the id168 is convex. a related problem
is minimizing the sum of ls(w) plus an (cid:96)1 norm id173 term,

(ls(w) +   (cid:107)w(cid:107)1) ,

min

w

(25.5)

where    is a id173 parameter. since for any k1 there exists a    such that
1 the function (cid:107)    (cid:107)0 is often referred to as the (cid:96)0 norm. despite the use of the    norm   

notation, (cid:107)    (cid:107)0 is not really a norm; for example, it does not satisfy the positive
homogeneity property of norms, (cid:107)aw(cid:107)0 (cid:54)= |a| (cid:107)w(cid:107)0.

364

feature selection and generation

equation (25.4) and equation (25.5) lead to the same solution, the two problems
are in some sense equivalent.

the (cid:96)1 id173 often induces sparse solutions. to illustrate this, let us

start with the simple optimization problem

(cid:18) 1

2

min
w   r

(cid:19)

w2     xw +   |w|

.

(25.6)

it is easy to verify (see exercise 2) that the solution to this problem is the    soft
thresholding    operator

w = sign(x) [|x|       ]+ ,

(25.7)
def= max{a, 0}. that is, as long as the absolute value of x is smaller

where [a]+
than   , the optimal solution will be zero.

next, consider a one dimensional regression problem with respect to the squared

loss:

argmin
w   rm

we can rewrite the problem as

(cid:32)

(cid:32)

1
2

(cid:88)

i

1
m

x2
i

argmin
w   rm

(cid:32)

1
2m

m(cid:88)
(cid:33)

i=1

for simplicity let us assume that 1
m
then the optimal solution is

(xiw     yi)2 +   |w|

.

(cid:33)

(cid:33)

(cid:32)
m(cid:88)
i = 1, and denote (cid:104)x, y(cid:105) =(cid:80)m

w +   |w|

xiyi

1
m

i=1

.

w2    

(cid:80)

i x2

i=1 xiyi;

w = sign((cid:104)x, y(cid:105)) [|(cid:104)x, y(cid:105)|/m       ]+ .

that is, the solution will be zero unless the correlation between the feature x
and the labels vector y is larger than   .

remark 25.4 unlike the (cid:96)1 norm, the (cid:96)2 norm does not induce sparse solutions.
indeed, consider the problem above with an (cid:96)2 id173, namely,

(cid:32)

m(cid:88)

i=1

1
2m

(xiw     yi)2 +   w2

.

argmin
w   rm

then, the optimal solution is

w =

(cid:104)x, y(cid:105)/m

(cid:107)x(cid:107)2/m + 2  

.

this solution will be nonzero even if the correlation between x and y is very small.
in contrast, as we have shown before, when using (cid:96)1 id173, w will be
nonzero only if the correlation between x and y is larger than the id173
parameter   .

(cid:33)

(cid:33)

25.2 feature manipulation and id172

365

adding (cid:96)1 id173 to a id75 problem with the squared loss

yields the lasso algorithm, de   ned as

(cid:19)

(cid:107)xw     y(cid:107)2 +   (cid:107)w(cid:107)1

.

(25.8)

(cid:18) 1

argmin

w

2m

under some assumptions on the distribution and the id173 parameter
  , the lasso will    nd sparse solutions (see, for example, (zhao & yu 2006)
and the references therein). another advantage of the (cid:96)1 norm is that a vector
with low (cid:96)1 norm can be    sparsi   ed    (see, for example, (shalev-shwartz, zhang
& srebro 2010) and the references therein).

25.2

feature manipulation and id172

feature manipulations or id172 include simple transformations that we
apply on each of our original features. such transformations may decrease the
approximation or estimation errors of our hypothesis class or can yield a faster
algorithm. similarly to the problem of feature selection, here again there are no
absolute    good    and    bad    transformations, but rather each transformation that
we apply should be related to the learning algorithm we are going to apply on
the resulting feature vector as well as to our prior assumptions on the problem.
to motivate id172, consider a id75 problem with the
squared loss. let x     rm,d be a matrix whose rows are the instance vectors
and let y     rm be a vector of target values. recall that ridge regression returns
the vector

(cid:107)xw     y(cid:107)2 +   (cid:107)w(cid:107)2

= (2  mi + x(cid:62)x)   1x(cid:62)y.

(cid:21)

(cid:20) 1

m

argmin

w

suppose that d = 2 and the underlying data distribution is as follows. first we
sample y uniformly at random from {  1}. then, we set x1 to be y + 0.5  , where
   is sampled uniformly at random from {  1}, and we set x2 to be 0.0001y. note
that the optimal weight vector is w(cid:63) = [0; 10000], and ld(w(cid:63)) = 0. however,
the objective of ridge regression at w(cid:63) is   108. in contrast, the objective of ridge
regression at w = [1; 0] is likely to be close to 0.25 +   . it follows that whenever
108   1     0.25    10   8, the objective of ridge regression is smaller at the
   > 0.25
suboptimal solution w = [1; 0]. since    typically should be at least 1/m (see
the analysis in chapter 13), it follows that in the aforementioned example, if the
number of examples is smaller than 108 then we are likely to output a suboptimal
solution.

the crux of the preceding example is that the two features have completely
di   erent scales. feature id172 can overcome this problem. there are
many ways to perform feature id172, and one of the simplest approaches
is simply to make sure that each feature receives values between    1 and 1. in
the preceding example, if we divide each feature by the maximal value it attains

366

feature selection and generation

we will obtain that x1 = y+0.5  
ridge regression is quite close to w(cid:63).

1.5

and x2 = y. then, for        10   3 the solution of

moreover, the generalization bounds we have derived in chapter 13 for reg-
ularized loss minimization depend on the norm of the optimal vector w(cid:63) and
on the maximal norm of the instance vectors.2 therefore, in the aforementioned
example, before we normalize the features we have that (cid:107)w(cid:63)(cid:107)2 = 108, while af-
ter we normalize the features we have that (cid:107)w(cid:63)(cid:107)2 = 1. the maximal norm of
the instance vector remains roughly the same; hence the id172 greatly
improves the estimation error.

feature id172 can also improve the runtime of the learning algorithm.
for example, in section 14.5.3 we have shown how to use the stochastic gradient
descent (sgd) optimization algorithm for solving the regularized loss minimiza-
tion problem. the number of iterations required by sgd to converge also depends
on the norm of w(cid:63) and on the maximal norm of (cid:107)x(cid:107). therefore, as before, using
id172 can greatly decrease the runtime of sgd.

next, we demonstrate in the following how a simple transformation on features,
such as clipping, can sometime decrease the approximation error of our hypoth-
esis class. consider again id75 with the squared loss. let a > 1 be
a large number, suppose that the target y is chosen uniformly at random from
{  1}, and then the single feature x is set to be y with id203 (1     1/a)
and set to be ay with id203 1/a. that is, most of the time our feature is
bounded but with a very small id203 it gets a very high value. then, for
any w, the expected squared loss of w is
(wx     y)2
ld(w) = e 1
2
1     1
a

(wy     y)2 +

(awy     y)2.

(cid:19) 1

(cid:18)

1
a

1
2

=

2

solving for w we obtain that w(cid:63) = 2a   1
a2+a   1 , which goes to zero as a goes to in   n-
ity. therefore, the objective at w(cid:63) goes to 0.5 as a goes to in   nity. for example,
for a = 100 we will obtain ld(w(cid:63))     0.48. next, suppose we apply a    clipping   
transformation; that is, we use the transformation x (cid:55)    sign(x) min{1,|x|}. then,
following this transformation, w(cid:63) becomes 1 and ld(w(cid:63)) = 0. this simple ex-
ample shows that a simple transformation can have a signi   cant in   uence on the
approximation error.

of course, it is not hard to think of examples in which the same feature trans-
formation actually hurts performance and increases the approximation error.
this is not surprising, as we have already argued that feature transformations

2 more precisely, the bounds we derived in chapter 13 for regularized loss minimization

depend on (cid:107)w(cid:63)(cid:107)2 and on either the lipschitzness or the smoothness of the id168.
for linear predictors and id168s of the form (cid:96)(w, (x, y)) =   ((cid:104)w, x(cid:105), y), where    is
convex and either 1-lipschitz or 1-smooth with respect to its    rst argument, we have that
(cid:96) is either (cid:107)x(cid:107)-lipschitz or (cid:107)x(cid:107)2-smooth. for example, for the squared loss,
  (a, y) = 1
   rst argument.

2 ((cid:104)w, x(cid:105)     y)2 is (cid:107)x(cid:107)2-smooth with respect to its

2 (a     y)2, and (cid:96)(w, (x, y)) = 1

25.2 feature manipulation and id172

367

should rely on our prior assumptions on the problem. in the aforementioned ex-
ample, a prior assumption that may lead us to use the    clipping    transformation
is that features that get values larger than a prede   ned threshold value give us no
additional useful information, and therefore we can clip them to the prede   ned
threshold.

25.2.1

examples of feature transformations

we now list several common techniques for feature transformations. usually, it
is helpful to combine some of these transformations (e.g., centering + scaling).
in the following, we denote by f = (f1, . . . , fm)     rm the value of the feature f
over the m training examples. also, we denote by   f = 1
i=1 fi the empirical
m
mean of the feature over all examples.

(cid:80)m

centering:
this transformation makes the feature have zero mean, by setting fi     fi       f .

unit range:
this transformation makes the range of each feature be [0, 1]. formally, let
fmax = maxi fi and fmin = mini fi. then, we set fi     fi   fmin
. similarly,
fmax   fmin
we can make the range of each feature be [   1, 1] by the transformation fi    
    1. of course, it is easy to make the range [0, b] or [   b, b], where b is
2 fi   fmin
fmax   fmin
a user-speci   ed parameter.

(cid:80)m
standardization:
this transformation makes all features have a zero mean and unit variance.
i=1(fi       f )2 be the empirical variance of the feature.
formally, let    = 1
m
then, we set fi     fi      f   
   .

clipping:
this transformation clips high or low values of the feature. for example, fi    
sign(fi) max{b,|fi|}, where b is a user-speci   ed parameter.

sigmoidal transformation:
as its name indicates, this transformation applies a sigmoid function on the
feature. for example, fi    
1+exp(b fi) , where b is a user-speci   ed parameter.
this transformation can be thought of as a    soft    version of clipping: it has a
small e   ect on values close to zero and behaves similarly to clipping on values
far away from zero.

1

368

feature selection and generation

logarithmic transformation:
the transformation is fi     log(b+fi), where b is a user-speci   ed parameter. this
is widely used when the feature is a    counting    feature. for example, suppose
that the feature represents the number of appearances of a certain word in a
text document. then, the di   erence between zero occurrences of the word and
a single occurrence is much more important than the di   erence between 1000
occurrences and 1001 occurrences.

remark 25.5
in the aforementioned transformations, each feature is trans-
formed on the basis of the values it obtains on the training set, independently
of other features    values. in some situations we would like to set the parameter
of the transformation on the basis of other features as well. a notable example
is a transformation in which one applies a scaling to the features so that the
empirical average of some norm of the instances becomes 1.

25.3

id171

so far we have discussed feature selection and manipulations. in these cases, we
start with a prede   ned vector space rd, representing our features. then, we select
a subset of features (feature selection) or transform individual features (feature
transformation). in this section we describe id171, in which we start
with some instance space, x , and would like to learn a function,    : x     rd,
which maps instances in x into a representation as d-dimensional feature vectors.
the idea of id171 is to automate the process of    nding a good rep-
resentation of the input space. as mentioned before, the no-free-lunch theorem
tells us that we must incorporate some prior knowledge on the data distribution
in order to build a good feature representation. in this section we present a few
id171 approaches and demonstrate conditions on the underlying data
distribution in which these methods can be useful.

throughout the book we have already seen several useful feature construc-
tions. for example, in the context of polynomial regression, we have mapped the
original instances into the vector space of all their monomials (see section 9.2.2
in chapter 9). after performing this mapping, we trained a linear predictor on
top of the constructed features. automation of this process would be to learn
a transformation    : x     rd, such that the composition of the class of linear
predictors on top of    yields a good hypothesis class for the task at hand.

in the following we describe a technique of feature construction called dictio-

nary learning.

25.3.1

dictionary learning using auto-encoders

the motivation of dictionary learning stems from a commonly used represen-
tation of documents as a    bag-of-words   : given a dictionary of words d =
{w1, . . . , wk}, where each wi is a string representing a word in the dictionary,

25.3 id171

369

and given a document, (p1, . . . , pd), where each pi is a word in the document,
we represent the document as a vector x     {0, 1}k, where xi is 1 if wi = pj for
some j     [d], and xi = 0 otherwise. it was empirically observed in many text
processing tasks that linear predictors are quite powerful when applied on this
representation. intuitively, we can think of each word as a feature that measures
some aspect of the document. given labeled examples (e.g., topics of the doc-
uments), a learning algorithm searches for a linear predictor that weights these
features so that a right combination of appearances of words is indicative of the
label.

while in text processing there is a natural meaning to words and to the dic-
tionary, in other applications we do not have such an intuitive representation
of an instance. for example, consider the id161 application of object
recognition. here, the instance is an image and the goal is to recognize which
object appears in the image. applying a linear predictor on the pixel-based rep-
resentation of the image does not yield a good classi   er. what we would like
to have is a mapping    that would take the pixel-based representation of the
image and would output a bag of    visual words,    representing the content of the
image. for example, a    visual word    can be    there is an eye in the image.    if
we had such representation, we could have applied a linear predictor on top of
this representation to train a classi   er for, say, face recognition. our question is,
therefore, how can we learn a dictionary of    visual words    such that a bag-of-
words representation of an image would be helpful for predicting which object
appears in the image?

a    rst naive approach for dictionary learning relies on a id91 algorithm
(see chapter 22). suppose that we learn a function c : x     {1, . . . , k}, where
c(x) is the cluster to which x belongs. then, we can think of the clusters as
   words,    and of instances as    documents,    where a document x is mapped to
the vector   (x)     {0, 1}k, where   (x)i is 1 if and only if x belongs to the ith
cluster. now, it is straightforward to see that applying a linear predictor on   (x)
is equivalent to assigning the same target value to all instances that belong to
the same cluster. furthermore, if the id91 is based on distances from a
class center (e.g., id116), then a linear predictor on   (x) yields a piece-wise
constant predictor on x.

   nd a pair of functions such that the reconstruction error,(cid:80)

both the id116 and pca approaches can be regarded as special cases of a
more general approach for dictionary learning which is called auto-encoders. in an
auto-encoder we learn a pair of functions: an    encoder    function,    : rd     rk,
and a    decoder    function,    : rk     rd. the goal of the learning process is to
i (cid:107)xi       (  (xi))(cid:107)2,
is small. of course, we can trivially set k = d and both   ,    to be the identity
mapping, which yields a perfect reconstruction. we therefore must restrict    and
   in some way. in pca, we constrain k < d and further restrict    and    to be
linear functions. in id116, k is not restricted to be smaller than d, but now
   and    rely on k centroids,   1, . . . ,   k, and   (x) returns an indicator vector

370

feature selection and generation

in {0, 1}k that indicates the closest centroid to x, while    takes as input an
indicator vector and returns the centroid representing this vector.

an important property of the id116 construction, which is key in allowing
k to be larger than d, is that    maps instances into sparse vectors. in fact, in
id116 only a single coordinate of   (x) is nonzero. an immediate extension of
the id116 construction is therefore to restrict the range of    to be vectors with
at most s nonzero elements, where s is a small integer. in particular, let    and   
be functions that depend on   1, . . . ,   k. the function    maps an instance vector
x to a vector   (x)     rk, where   (x) should have at most s nonzero elements.
i=1 vi  i. as before, our goal is to have a

the function   (v) is de   ned to be (cid:80)k

small reconstruction error, and therefore we can de   ne

  (x) = argmin

v

(cid:107)x       (v)(cid:107)2 s.t. (cid:107)v(cid:107)0     s,

where (cid:107)v(cid:107)0 = |{j : vj (cid:54)= 0}|. note that when s = 1 and we further restrict (cid:107)v(cid:107)1 =
1 then we obtain the id116 encoding function; that is,   (x) is the indicator
vector of the centroid closest to x. for larger values of s, the optimization problem
in the preceding de   nition of    becomes computationally di   cult. therefore, in
practice, we sometime use (cid:96)1 id173 instead of the sparsity constraint and
de   ne    to be

(cid:2)(cid:107)x       (v)(cid:107)2 +   (cid:107)v(cid:107)1

(cid:3) ,

  (x) = argmin

v

ror, (cid:80)m

where    > 0 is a id173 parameter. anyway, the dictionary learning
problem is now to    nd the vectors   1, . . . ,   k such that the reconstruction er-
i=1 (cid:107)xi       (  (x))(cid:107)2, is as small as possible. even if    is de   ned using
the (cid:96)1 id173, this is still a computationally hard problem (similar to
the id116 problem). however, several heuristic search algorithms may give
reasonably good solutions. these algorithms are beyond the scope of this book.

25.4

summary

many machine learning algorithms take the feature representation of instances
for granted. yet the choice of representation requires careful attention. we dis-
cussed approaches for feature selection, introducing    lters, greedy selection al-
gorithms, and sparsity-inducing norms. next we presented several examples for
feature transformations and demonstrated their usefulness. last, we discussed
id171, and in particular dictionary learning. we have shown that fea-
ture selection, manipulation, and learning all depend on some prior knowledge
on the data.

25.5 bibliographic remarks

371

25.5

bibliographic remarks

guyon & elissee    (2003) surveyed several feature selection procedures, including
many types of    lters.

forward greedy selection procedures for minimizing a convex objective sub-
ject to a polyhedron constraint date back to the frank-wolfe algorithm (frank
& wolfe 1956). the relation to boosting has been studied by several authors,
including, (warmuth, liao & ratsch 2006, warmuth, glocer & vishwanathan
2008, shalev-shwartz & singer 2008). matching pursuit has been studied in the
signal processing community (mallat & zhang 1993). several papers analyzed
greedy selection methods under various conditions. see, for example, shalev-
shwartz, zhang & srebro (2010) and the references therein.

the use of the (cid:96)1-norm as a surrogate for sparsity has a long history (e.g. tib-
shirani (1996) and the references therein), and much work has been done on un-
derstanding the relationship between the (cid:96)1-norm and sparsity. it is also closely
related to compressed sensing (see chapter 23). the ability to sparsify low (cid:96)1
norm predictors dates back to maurey (pisier 1980-1981). in section 26.4 we
also show that low (cid:96)1 norm can be used to bound the estimation error of our
predictor.

id171 and dictionary learning have been extensively studied recently
in the context of deep neural networks. see, for example, (lecun & bengio 1995,
hinton et al. 2006, ranzato et al. 2007, collobert & weston 2008, lee et al.
2009, le et al. 2012, bengio 2009) and the references therein.

25.6

exercises

1. prove the equality given in equation (25.1). hint: let a   , b    be minimizers of
the left-hand side. find a, b such that the objective value of the right-hand
side is smaller than that of the left-hand side. do the same for the other
direction.

2. show that equation (25.7) is the solution of equation (25.6).
3. adaboost as a forward greedy selection algorithm: recall the ad-
aboost algorithm from chapter 10. in this section we give another interpre-
tation of adaboost as a forward greedy selection algorithm.
    given a set of m instances x1, . . . , xm, and a hypothesis class h of    nite
vc dimension, show that there exist d and h1, . . . , hd such that for every
h     h there exists i     [d] with hi(xj) = h(xj) for every j     [m].

    let r(w) be as de   ned in equation (25.3). given some w, de   ne fw to be

the function

fw(  ) =

d(cid:88)

i=1

wihi(  ).

372

feature selection and generation

let d be the distribution over [m] de   ned by
exp(   yifw(xi))

di =

,

z

where z is a id172 factor that ensures that d is a id203
vector. show that

   r(w)

furthermore, denoting  j =(cid:80)m

wj

=     m(cid:88)

i=1

diyihj(xi).

i=1 di1[hj (xi)(cid:54)=yi], show that

   r(w)

= 2 j     1.

conclude that if  j     1/2        then

(cid:12)(cid:12)(cid:12)       /2.
log((cid:112)1     4  2). hint: use the proof of theorem 10.2.

(cid:12)(cid:12)(cid:12)    r(w)

wj

wj

    show that the update of adaboost guarantees r(w(t+1))     r(w(t))    

part iv

advanced theory

26 rademacher complexities

in chapter 4 we have shown that uniform convergence is a su   cient condition
for learnability. in this chapter we study the rademacher complexity, which
measures the rate of uniform convergence. we will provide generalization bounds
based on this measure.

26.1

the rademacher complexity

recall the de   nition of an  -representative sample from chapter 4, repeated here
for convenience.

definition 26.1 ( -representative sample) a training set s is called  -representative
(w.r.t. domain z, hypothesis class h, id168 (cid:96), and distribution d) if

|ld(h)     ls(h)|      .

sup
h   h

we have shown that if s is an  /2 representative sample then the erm rule

is  -consistent, namely, ld(ermh(s))     minh   h ld(h) +  .

to simplify our notation, let us denote

f def= (cid:96)     h def= {z (cid:55)    (cid:96)(h, z) : h     h},

and given f     f, we de   ne

ld(f ) = e

z   d[f (z)]

,

ls(f ) =

1
m

m(cid:88)

f (zi).

i=1

we de   ne the representativeness of s with respect to f as the largest gap be-
tween the true error of a function f and its empirical error, namely,

repd(f, s) def= sup
f   f

(cid:0)ld(f )     ls(f )(cid:1).

now, suppose we would like to estimate the representativeness of s using the
sample s only. one simple idea is to split s into two disjoint sets, s = s1     s2;
refer to s1 as a validation set and to s2 as a training set. we can then estimate
the representativeness of s by

(cid:0)ls1(f )     ls2(f )(cid:1).

sup
f   f

(26.1)

(26.2)

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

376

rademacher complexities

this can be written more compactly by de   ning    = (  1, . . . ,   m)     {  1}m to
be a vector such that s1 = {zi :   i = 1} and s2 = {zi :   i =    1}. then, if we
further assume that |s1| = |s2| then equation (26.2) can be rewritten as

m(cid:88)

i=1

2
m

sup
f   f

  if (zi).

(26.3)

the rademacher complexity measure captures this idea by considering the ex-
pectation of the above with respect to a random choice of   . formally, let f     s
be the set of all possible evaluations a function f     f can achieve on a sample
s, namely,

f     s = {(f (z1), . . . , f (zm)) : f     f}.

let the variables in    be distributed i.i.d. according to p[  i = 1] = p[  i =    1] =
2 . then, the rademacher complexity of f with respect to s is de   ned as follows:

1

r(f     s) def=

1
m

e

     {  1}m

  if (zi)

.

(26.4)

more generally, given a set of vectors, a     rm, we de   ne

(cid:35)

m(cid:88)
(cid:35)

i=1

(cid:34)

(cid:34)

sup
f   f

m(cid:88)

i=1

r(a) def=

1
m

e
  

sup
a   a

  iai

.

(26.5)

the following lemma bounds the expected value of the representativeness of

s by twice the expected rademacher complexity.

lemma 26.2

e
s   dm
proof let s(cid:48) = {z(cid:48)
1, . . . , z(cid:48)
ld(f ) = es(cid:48)[ls(cid:48)(f )]. therefore, for every f     f we have

[ repd(f, s)]     2 e
s   dm

r(f     s).

m} be another i.i.d. sample. clearly, for all f     f,

ld(f )     ls(f ) = e

s(cid:48)[ls(cid:48)(f )]     ls(f ) = e

s(cid:48)[ls(cid:48)(f )     ls(f )].

taking supremum over f     f of both sides, and using the fact that the supremum
of expectation is smaller than expectation of the supremum we obtain

sup
f   f

(cid:0)ld(f )     ls(f )(cid:1) = sup
(cid:34)

f   f
    e
s(cid:48)

taking expectation over s on both sides we obtain

(cid:34)

e
s

sup
f   f

(cid:0)ld(f )     ls(f )(cid:1)(cid:35)

.

sup
f   f

s(cid:48)[ls(cid:48)(f )     ls(f )]
e

(cid:0)ls(cid:48)(f )     ls(f )(cid:1)(cid:35)
(cid:0)ls(cid:48)(f )     ls(f )(cid:1)(cid:35)
m(cid:88)

(f (z(cid:48)

i)     f (zi))

sup
f   f

i=1

sup
f   f

(cid:34)

(cid:34)

    e
s,s(cid:48)

=

1
m

e
s,s(cid:48)

(cid:35)

.

(26.6)

26.1 the rademacher complexity

377

             =
             .
            
             .

next, we note that for each j, zj and z(cid:48)
replace them without a   ecting the expectation:

j are i.i.d. variables. therefore, we can

e
s,s(cid:48)

e
s,s(cid:48)

e

s,s(cid:48),  j

=

1
2

= e
s,s(cid:48)

f   f

f   f

       sup
       sup
       sup
       sup
m(cid:88)
(cid:88)

f   f

f   f

i=1

sup
f   f

sup
f   f

i

(cid:34)

e

s,s(cid:48),  

(cid:88)
(cid:88)

i(cid:54)=j

i(cid:54)=j

(cid:88)

i(cid:54)=j

      (f (z(cid:48)
      (f (zj)     f (z(cid:48)
        j(f (z(cid:48)
      (f (z(cid:48)

j)     f (zj)) +

(f (z(cid:48)

i)     f (zi))

j)) +

(f (z(cid:48)

i)     f (zi))

(26.7)

let   j be a random variable such that p[  j = 1] = p[  j =    1] = 1/2. from
equation (26.7) we obtain that

(l.h.s. of equation (26.7)) +

(r.h.s. of equation (26.7))

j)     f (zj)) +

(f (z(cid:48)

i)     f (zi))

i(cid:54)=j

1
(cid:88)
2
j)     f (zj)) +
(cid:34)
(cid:35)
(cid:88)

= e

s,s(cid:48),  

(f (z(cid:48)

i)     f (zi))
m(cid:88)

  i(f (z(cid:48)

sup
f   f

i=1

i)     f (zi))
(cid:88)

     if (zi)

(26.8)

(cid:35)

.

(26.9)

repeating this for all j we obtain that

(f (z(cid:48)

i)     f (zi))

(cid:34)

e
s,s(cid:48)

finally,

  i(f (z(cid:48)

i)     f (zi))     sup
f   f

  if (z(cid:48)

i) + sup
f   f

i

i

and since the id203 of    is the same as the id203 of      , the right-hand
side of equation (26.9) can be bounded by

(cid:88)
(cid:88)
i) + sup
f   f
s(cid:48)[r(f     s(cid:48))] + m e

  if (z(cid:48)

s

i

i

sup
f   f
= m e

(cid:35)

  if (zi)

[r(f     s)] = 2m e

[r(f     s)].

s

the lemma immediately yields that, in expectation, the erm rule    nds a

hypothesis which is close to the optimal hypothesis in h.
theorem 26.3 we have

e

s   dm

[ld(ermh(s))     ls(ermh(s))]     2 e
s   dm

furthermore, for any h(cid:63)     h

r((cid:96)     h     s).

e

s   dm

[ld(ermh(s))     ld(h(cid:63))]     2 e
s   dm

r((cid:96)     h     s).

378

rademacher complexities

furthermore, if h(cid:63) = argminh ld(h) then for each        (0, 1) with id203 of
at least 1        over the choice of s we have

ld(ermh(s))     ld(h(cid:63))     2 es(cid:48)   dm r((cid:96)     h     s(cid:48))

  

.

proof the    rst inequality follows directly from lemma 26.2. the second in-
equality follows because for any    xed h(cid:63),

ld(h(cid:63)) = e

[ls(h(cid:63))]     e

s

s

[ls(ermh(s))].

the third inequality follows from the previous inequality by relying on markov   s
inequality (note that the random variable ld(ermh(s))     ld(h(cid:63)) is nonnega-
tive).

next, we derive bounds similar to the bounds in theorem 26.3 with a better
dependence on the con   dence parameter   . to do so, we    rst introduce the
following bounded di   erences concentration inequality.
lemma 26.4 (mcdiarmid   s inequality) let v be some set and let f : v m     r
be a function of m variables such that for some c > 0, for all i     [m] and for all
x1, . . . , xm, x(cid:48)

i     v we have
|f (x1, . . . , xm)     f (x1, . . . , xi   1, x(cid:48)

i, xi+1, . . . , xm)|     c.

let x1, . . . , xm be m independent random variables taking values in v . then,
with id203 of at least 1        we have

|f (x1, . . . , xm)     e[f (x1, . . . , xm)]|     c

(cid:113)

ln(cid:0) 2

  

(cid:1) m/2.

on the basis of the mcdiarmid inequality we can derive generalization bounds

with a better dependence on the con   dence parameter.
theorem 26.5 assume that for all z and h     h we have that |(cid:96)(h, z)|     c.
then,
1. with id203 of at least 1       , for all h     h,

(cid:114)

ld(h)     ls(h)     2

e

s(cid:48)   dm

r((cid:96)     h     s(cid:48)) + c

2 ln(2/  )

m

.

in particular, this holds for h = ermh(s).

2. with id203 of at least 1       , for all h     h,

ld(h)     ls(h)     2 r((cid:96)     h     s) + 4 c

(cid:114)

2 ln(4/  )

m

.

in particular, this holds for h = ermh(s).
3. for any h(cid:63), with id203 of at least 1       ,

ld(ermh(s))     ld(h(cid:63))     2 r((cid:96)     h     s) + 5 c

(cid:114)

2 ln (8/  )

m

.

26.1 the rademacher complexity

379

proof first note that the random variable repd(f, s) = suph   h (ld(h)     ls(h))
satis   es the bounded di   erences condition of lemma 26.4 with a constant 2c/m.
combining the bounds in lemma 26.4 with lemma 26.2 we obtain that with
id203 of at least 1       ,

(cid:114)

(cid:114)

m

2 ln(2/  )

    2 e

s(cid:48) r((cid:96)     h     s(cid:48)) + c

repd(f, s)     e repd(f, s) + c
.
the    rst inequality of the theorem follows from the de   nition of repd(f, s).
for the second inequality we note that the random variable r((cid:96)     h     s) also
satis   es the bounded di   erences condition of lemma 26.4 with a constant 2c/m.
therefore, the second inequality follows from the    rst inequality, lemma 26.4,
and the union bound. finally, for the last inequality, denote hs = ermh(s)
and note that

2 ln(2/  )

m

ld(hs)     ld(h(cid:63))

= ld(hs)     ls(hs) + ls(hs)     ls(h(cid:63)) + ls(h(cid:63))     ld(h(cid:63))
    (ld(hs)     ls(hs)) + (ls(h(cid:63))     ld(h(cid:63))) .

(26.10)

the    rst summand on the right-hand side is bounded by the second inequality of
the theorem. for the second summand, we use the fact that h(cid:63) does not depend
on s; hence by using hoe   ding   s inequality we obtain that with probaility of at
least 1       /2,

(cid:114)

ls(h(cid:63))     ld(h(cid:63))     c

ln(4/  )

2m

.

(26.11)

combining this with the union bound we conclude our proof.

the preceding theorem tells us that if the quantity r((cid:96)   h    s) is small then it
is possible to learn the class h using the erm rule. it is important to emphasize
that the last two bounds given in the theorem depend on the speci   c training
set s. that is, we use s both for learning a hypothesis from h as well as for
estimating the quality of it. this type of bound is called a data-dependent bound.

26.1.1

rademacher calculus

let us now discuss some properties of the rademacher complexity measure.
these properties will help us in deriving some simple bounds on r((cid:96)   h     s) for
speci   c cases of interest.

the following lemma is immediate from the de   nition.

lemma 26.6 for any a     rm, scalar c     r, and vector a0     rm, we have

r({c a + a0 : a     a})     |c| r(a).

the following lemma tells us that the convex hull of a has the same complexity

as a.

380

rademacher complexities

lemma 26.7 let a be a subset of rm and let a(cid:48) = {(cid:80)n

n,   j, a(j)     a,   j     0,(cid:107)  (cid:107)1 = 1}. then, r(a(cid:48)) = r(a).
proof the main idea follows from the fact that for any vector v we have

j=1   ja(j) : n    

sup

     0:(cid:107)  (cid:107)1=1

therefore,

  jvj = max

j

vj.

n(cid:88)

j=1

  ja(j)

i

n(cid:88)

j=1

  i

  ia(j)

i

m(cid:88)
m(cid:88)

i=1

i=1

m r(a(cid:48)) = e

  

sup

     0:(cid:107)  (cid:107)1=1

sup

a(1),...,a(n )

n(cid:88)

j=1

= e

  

     0:(cid:107)  (cid:107)1=1

  j sup
a(j)

sup

m(cid:88)

  iai

= e

  

sup
a   a

i=1
= m r(a),

and we conclude our proof.

the next lemma, due to massart, states that the rademacher complexity of

a    nite set grows logarithmically with the size of the set.
lemma 26.8 (massart lemma) let a = {a1, . . . , an} be a    nite set of vectors
in rm. de   ne   a = 1

(cid:80)n

n

i=1 ai. then,
r(a)     max
a   a

(cid:107)a       a(cid:107)

(cid:112)2 log(n )

.

m

proof based on lemma 26.6, we can assume without loss of generality that
  a = 0. let    > 0 and let a(cid:48) = {  a1, . . . ,   an}. we upper bound the rademacher
complexity as follows:

mr(a(cid:48)) = e

  

    e

  

= e

log

  

(cid:20)
(cid:33)(cid:35)
(cid:35)(cid:33)
(cid:33)

max

(cid:21)
(cid:20)
(cid:34)
(cid:32)(cid:88)
a   a(cid:48)(cid:104)  , a(cid:105)
(cid:34)(cid:88)
(cid:32)
(cid:32)(cid:88)
m(cid:89)

a   a(cid:48)

a   a(cid:48)

e
  

log

a   a(cid:48)

i=1

e(cid:104)  ,a(cid:105)

e(cid:104)  ,a(cid:105)

e
  i

[e  iai]

,

    log

= log

(cid:18)

a   a(cid:48) e(cid:104)  ,a(cid:105)(cid:19)(cid:21)

max

// jensen   s inequality

where the last equality occurs because the rademacher variables are indepen-
dent. next, using lemma a.6 we have that for all ai     r,
    exp(a2

exp(ai) + exp(   ai)

e  iai =

i /2),

e
  i

2

26.1 the rademacher complexity

381

and therefore

mr(a(cid:48))     log

(cid:32)(cid:88)
(cid:18)

(cid:18) a2

(cid:19)(cid:33)
m(cid:89)
a   a(cid:48) exp(cid:0)(cid:107)a(cid:107)2/2(cid:1)(cid:19)

exp

i
2

i=1

a   a(cid:48)
|a(cid:48)| max

= log

exp(cid:0)(cid:107)a(cid:107)2/2(cid:1)(cid:33)

(cid:32)(cid:88)

a   a(cid:48)

since r(a) = 1

= log(|a(cid:48)|) + max

a   a(cid:48)((cid:107)a(cid:107)2/2).

    log
   r(a(cid:48)) we obtain from the equation that
r(a)     log(|a|) +   2 maxa   a((cid:107)a(cid:107)2/2)

setting    =(cid:112)2 log(|a|)/ maxa   a (cid:107)a(cid:107)2 and rearranging terms we conclude our

  m

.

proof.

the following lemma shows that composing a with a lipschitz function does
not blow up the rademacher complexity. the proof is due to kakade and tewari.

lemma 26.9 (contraction lemma) for each i     [m], let   i : r     r be a   -
lipschitz function, namely for all   ,        r we have |  i(  )       i(  )|       |         |.
for a     rm let   (a) denote the vector (  1(a1), . . . ,   m(ym)). let       a = {  (a) :
a     a}. then,

r(       a)        r(a).

proof for simplicity, we prove the lemma for the case    = 1. the case    (cid:54)=
1 will follow by de   ning   (cid:48) = 1
      and then using lemma 26.6. let ai =
{(a1, . . . , ai   1,   i(ai), ai+1, . . . , am) : a     a}. clearly, it su   ces to prove that
for any set a and all i we have r(ai)     r(a). without loss of generality we will
prove the latter claim for i = 1 and to simplify notation we omit the subscript
from   1. we have

(cid:34)
(cid:34)

m(cid:88)

i=1

  iai

(cid:35)

(cid:32)

m(cid:88)

i=2

  1  (a1) +

  iai

  (a1) +

(cid:35)
m(cid:88)

i=2

(cid:33)

  iai

mr(a1) = e

  

= e

  

=

=

1
2

1
2

    1
2

sup
a   a1

sup
a   a

e

  2,...,  m

e

  2,...,  m

e

  2,...,  m

(cid:34)
(cid:34)
(cid:34)

(cid:32)
(cid:32)

sup
a   a

sup
a,a(cid:48)   a

sup
a,a(cid:48)   a

  (a1)       (a(cid:48)

|a1     a(cid:48)

1| +

1) +

m(cid:88)

  iai +

m(cid:88)

i=2

  iai +

i=2

i=2

(cid:32)

+ sup
a   a

m(cid:88)

     (a1) +

m(cid:88)
(cid:33)(cid:35)

i=2

  ia(cid:48)

i

  ia(cid:48)

i

(cid:33)(cid:35)

  iai

m(cid:88)
(cid:33)(cid:35)

i=2

,

(26.12)

where in the last inequality we used the assumption that    is lipschitz. next,
we note that the absolute value on |a1     a(cid:48)
1| in the preceding expression can

382

rademacher complexities

be omitted since both a and a(cid:48) are from the same set a and the rest of the
expression in the supremum is not a   ected by replacing a and a(cid:48). therefore,

(cid:34)

(cid:32)

m(cid:88)

(cid:33)(cid:35)

m(cid:88)

  ia(cid:48)

i

mr(a1)     1
2

e

  2,...,  m

sup
a,a(cid:48)   a

a1     a(cid:48)

1 +

  iai +

i=2

i=2

.

(26.13)

but, using the same equalities as in equation (26.12), it is easy to see that the
right-hand side of equation (26.13) exactly equals m r(a), which concludes our
proof.

26.2

rademacher complexity of linear classes

,

in this section we analyze the rademacher complexity of linear classes. to sim-
plify the derivation we    rst de   ne the following two classes:
h1 = {x (cid:55)    (cid:104)w, x(cid:105) : (cid:107)w(cid:107)1     1}
h2 = {x (cid:55)    (cid:104)w, x(cid:105) : (cid:107)w(cid:107)2     1}. (26.14)
the following lemma bounds the rademacher complexity of h2. we allow
the xi to be vectors in any hilbert space (even in   nite dimensional), and the
bound does not depend on the dimensionality of the hilbert space. this property
becomes useful when analyzing kernel methods.
lemma 26.10 let s = (x1, . . . , xm) be vectors in a hilbert space. de   ne: h2    
s = {((cid:104)w, x1(cid:105), . . . ,(cid:104)w, xm(cid:105)) : (cid:107)w(cid:107)2     1}. then,
   

r(h2     s)     maxi (cid:107)xi(cid:107)2

.

m

proof using cauchy-schwartz inequality we know that for any vectors w, v we
have (cid:104)w, v(cid:105)     (cid:107)w(cid:107)(cid:107)v(cid:107). therefore,

(26.15)

mr(h2     s) = e

  

= e

  

= e

  

    e

  

(cid:35)
(cid:35)

  i(cid:104)w, xi(cid:105)

  ixi(cid:105)

(cid:34)
(cid:34)
(cid:34)
(cid:34)

(cid:35)

m(cid:88)
m(cid:88)

i=1

sup

a   h2   s

  iai

sup

i=1

i=1

sup

(cid:104)w,

w:(cid:107)w(cid:107)   1

w:(cid:107)w(cid:107)   1

  ixi(cid:107)2

m(cid:88)
(cid:35)
(cid:107) m(cid:88)
      1/2             
      e
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

i=1

  

.

2

next, using jensen   s inequality we have that

(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

e
  

(cid:35)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

         
      (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

  ixi

= e

  

  ixi

      (cid:13)(cid:13)(cid:13)(cid:13)(cid:13) m(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

2

  ixi

            1/2

.(26.16)

26.3 generalization bounds for id166

383

finally, since the variables   1, . . . ,   m are independent we have

  ixi(cid:107)2

2

= e

  i  j(cid:104)xi, xj(cid:105)

(cid:34)

(cid:107) m(cid:88)

e
  

(cid:35)

i=1

=

=

  

      (cid:88)
(cid:88)
m(cid:88)

i(cid:54)=j

i,j

i=1

      

m(cid:88)

i=1

(cid:107)xi(cid:107)2
2.

(cid:3)

(cid:2)  2

i

(cid:104)xi, xj(cid:105) e

  

[  i  j] +

(cid:104)xi, xi(cid:105) e

  

(cid:107)xi(cid:107)2

2     m max

i

combining this with equation (26.15) and equation (26.16) we conclude our
proof.

next we bound the rademacher complexity of h1     s.

lemma 26.11 let s = (x1, . . . , xm) be vectors in rn. then,

proof using holder   s inequality we know that for any vectors w, v we have
(cid:104)w, v(cid:105)     (cid:107)w(cid:107)1 (cid:107)v(cid:107)   . therefore,

r(h1     s)     max

i

(cid:107)xi(cid:107)   

2 log(2n)

m

.

(cid:114)

m(cid:88)
m(cid:88)

i=1

i=1

(cid:104)w,

(cid:35)

(cid:35)
(cid:35)

  i(cid:104)w, xi(cid:105)

  ixi(cid:105)

m(cid:88)
(cid:35)

i=1

(cid:34)
(cid:34)
(cid:34)
(cid:34)

mr(h1     s) = e

  

= e

  

= e

  

sup

a   h1   s

  iai

sup

w:(cid:107)w(cid:107)1   1

sup

w:(cid:107)w(cid:107)1   1

(cid:107) m(cid:88)

  ixi(cid:107)   

    e

  

.

(26.17)

for each j     [n], let vj = (x1,j, . . . , xm,j)     rm. note that (cid:107)vj(cid:107)2        
let v = {v1, . . . , vn,   v1, . . . ,   vn}. the right-hand side of equation (26.17) is
m r(v ). using massart lemma (lemma 26.8) we have that

m maxi (cid:107)xi(cid:107)   .

i=1

(cid:107)xi(cid:107)   (cid:112)2 log(2n)/m,

r(v )     max

i

which concludes our proof.

26.3

generalization bounds for id166

in this section we use rademacher complexity to derive generalization bounds
for generalized linear predictors with euclidean norm constraint. we will show
how this leads to generalization bounds for hard-id166 and soft-id166.

384

rademacher complexities

we shall consider the following general constraint-based formulation. let h =
{w : (cid:107)w(cid:107)2     b} be our hypothesis class, and let z = x    y be the examples
domain. assume that the id168 (cid:96) : h    z     r is of the form

(cid:96)(w, (x, y)) =   ((cid:104)w, x(cid:105), y),

(26.18)
where    : r    y     r is such that for all y     y, the scalar function a (cid:55)      (a, y)
is   -lipschitz. for example, the hinge-id168, (cid:96)(w, (x, y)) = max{0, 1    
y(cid:104)w, x(cid:105)}, can be written as in equation (26.18) using   (a, y) = max{0, 1    
ya}, and note that    is 1-lipschitz for all y     {  1}. another example is the
absolute id168, (cid:96)(w, (x, y)) = |(cid:104)w, x(cid:105)     y|, which can be written as in
equation (26.18) using   (a, y) = |a     y|, which is also 1-lipschitz for all y     r.
the following theorem bounds the generalization error of all predictors in h

(cid:114)

using their empirical error.
theorem 26.12 suppose that d is a distribution over x    y such that with
id203 1 we have that (cid:107)x(cid:107)2     r. let h = {w : (cid:107)w(cid:107)2     b} and let
(cid:96) : h    z     r be a id168 of the form given in equation (26.18)
such that for all y     y, a (cid:55)      (a, y) is a   -lipschitz function and such that
maxa   [   br,br] |  (a, y)|     c. then, for any        (0, 1), with id203 of at least
1        over the choice of an i.i.d. sample of size m,

   w     h, ld(w)     ls(w) +

2  br   
m

+ c

2 ln(2/  )

m

.

proof let f = {(x, y) (cid:55)      ((cid:104)w, x(cid:105), y) : w     h}. we will show that with
   
id203 1, r(f     s)       br/
m and then the theorem will follow from
theorem 26.5. indeed, the set f     s can be written as

f     s = {(  ((cid:104)w, x1(cid:105), y1), . . . ,   ((cid:104)w, xm(cid:105), ym)) : w     h},

and the bound on r(f   s) follows directly by combining lemma 26.9, lemma 26.10,
and the assumption that (cid:107)x(cid:107)2     r with id203 1.

we next derive a generalization bound for hard-id166 based on the previous
theorem. for simplicity, we do not allow a bias term and consider the hard-id166
problem:

(cid:107)w(cid:107)2

s.t.    i, yi(cid:104)w, xi(cid:105)     1

argmin

w

(26.19)

theorem 26.13 consider a distribution d over x   {  1} such that there exists
some vector w(cid:63) with p(x,y)   d[y(cid:104)w(cid:63), x(cid:105)     1] = 1 and such that (cid:107)x(cid:107)2     r with
id203 1. let ws be the output of equation (26.19). then, with id203
of at least 1        over the choice of s     dm, we have that

(x,y)   d[y (cid:54)= sign((cid:104)ws, x(cid:105))]     2 r (cid:107)w(cid:63)(cid:107)   

p

m

+ (1 + r (cid:107)w(cid:63)(cid:107))

2 ln(2/  )

m

.

(cid:114)

26.3 generalization bounds for id166

385

proof throughout the proof, let the id168 be the ramp loss (see sec-
tion 15.2.3). note that the range of the ramp loss is [0, 1] and that it is a
1-lipschitz function. since the ramp loss upper bounds the zero-one loss, we
have that

p

(x,y)   d[y (cid:54)= sign((cid:104)ws, x(cid:105))]     ld(ws).

let b = (cid:107)w(cid:63)(cid:107)2 and consider the set h = {w : (cid:107)w(cid:107)2     b}. by the de   nition of
hard-id166 and our assumption on the distribution, we have that ws     h with
id203 1 and that ls(ws) = 0. therefore, using theorem 26.12 we have
that

(cid:114)

ld(ws)     ls(ws) +

2br   
m

+

2 ln(2/  )

m

.

 2

remark 26.1 theorem 26.13 implies that the sample complexity of hard-id166
grows like r2 (cid:107)w(cid:63)(cid:107)2
. using a more delicate analysis and the separability assump-
tion, it is possible to improve the bound to an order of r2 (cid:107)w(cid:63)(cid:107)2

the bound in the preceding theorem depends on (cid:107)w(cid:63)(cid:107), which is unknown.
in the following we derive a bound that depends on the norm of the output of
id166; hence it can be calculated from the training set itself. the proof is similar
to the derivation of bounds for structure risk minimization (srm).

.

 

theorem 26.14 assume that the conditions of theorem 26.13 hold. then,
with id203 of at least 1        over the choice of s     dm, we have that

(x,y)   d[y (cid:54)= sign((cid:104)ws, x(cid:105))]     4r(cid:107)ws(cid:107)   

p

m

+

ln( 4 log2((cid:107)ws(cid:107))

  
m

)

.

proof for any integer i, let bi = 2i, hi = {w : (cid:107)w(cid:107)     bi}, and let   i =   
2i2 .
fix i, then using theorem 26.12 we have that with id203 of at least 1       i

+

m

2 ln(2/  i)

2bir   
m

   w     hi, ld(w)     ls(w) +

applying the union bound and using(cid:80)   
i=1   i        we obtain that with id203
of at least 1       this holds for all i. therefore, for all w, if we let i = (cid:100)log2((cid:107)w(cid:107))(cid:101)
(cid:114)
       (4 log2((cid:107)w(cid:107)))2
then w     hi, bi     2(cid:107)w(cid:107), and 2
(cid:114)

ld(w)     ls(w) +

m
4(ln(4 log2((cid:107)w(cid:107))) + ln(1/  ))

. therefore,

2 ln(2/  i)

= (2i)2

+

  i

  

2bir   
m
4(cid:107)w(cid:107)r   

    ls(w) +

+

.

m

m

in particular, it holds for ws, which concludes our proof.

(cid:115)

(cid:114)

386

rademacher complexities

remark 26.2 note that all the bounds we have derived do not depend on the
dimension of w. this property is utilized when learning id166 with kernels, where
the dimension of w can be extremely large.

26.4

generalization bounds for predictors with low (cid:96)1 norm

in the previous section we derived generalization bounds for linear predictors
with an (cid:96)2-norm constraint. in this section we consider the following general (cid:96)1-
norm constraint formulation. let h = {w : (cid:107)w(cid:107)1     b} be our hypothesis class,
and let z = x    y be the examples domain. assume that the id168,
(cid:96) : h    z     r, is of the same form as in equation (26.18), with    : r    y     r
being   -lipschitz w.r.t. its    rst argument. the following theorem bounds the
generalization error of all predictors in h using their empirical error.
theorem 26.15 suppose that d is a distribution over x    y such that with
id203 1 we have that (cid:107)x(cid:107)        r. let h = {w     rd : (cid:107)w(cid:107)1     b} and
let (cid:96) : h    z     r be a id168 of the form given in equation (26.18)
such that for all y     y, a (cid:55)      (a, y) is an   -lipschitz function and such that
maxa   [   br,br] |  (a, y)|     c. then, for any        (0, 1), with id203 of at least
1        over the choice of an i.i.d. sample of size m,

(cid:114)

(cid:114)

   w     h, ld(w)     ls(w) + 2  br

2 log(2d)

m

+ c

2 ln(2/  )

m

.

proof the proof is identical to the proof of theorem 26.12, while relying on
lemma 26.11 instead of relying on lemma 26.10.

it is interesting to compare the two bounds given in theorem 26.12 and the-
orem 26.15. apart from the extra log(d) factor that appears in theorem 26.15,
both bounds look similar. however, the parameters b, r have di   erent meanings
in the two bounds. in theorem 26.12, the parameter b imposes an (cid:96)2 constraint
on w and the parameter r captures a low (cid:96)2-norm assumption on the instances.
in contrast, in theorem 26.15 the parameter b imposes an (cid:96)1 constraint on w
(which is stronger than an (cid:96)2 constraint) while the parameter r captures a low
(cid:96)   -norm assumption on the instance (which is weaker than a low (cid:96)2-norm as-
sumption). therefore, the choice of the constraint should depend on our prior
knowledge of the set of instances and on prior assumptions on good predictors.

26.5

bibliographic remarks

the use of rademacher complexity for bounding the uniform convergence is
due to (koltchinskii & panchenko 2000, bartlett & mendelson 2001, bartlett
& mendelson 2002). for additional reading see, for example, (bousquet 2002,
boucheron, bousquet & lugosi 2005, bartlett, bousquet & mendelson 2005).

26.5 bibliographic remarks

387

our proof of the concentration lemma is due to kakade and tewari lecture
notes. kakade, sridharan & tewari (2008) gave a uni   ed framework for deriving
bounds on the rademacher complexity of linear classes with respect to di   erent
assumptions on the norms.

27 covering numbers

in this chapter we describe another way to measure the complexity of sets, which
is called covering numbers.

27.1

covering
definition 27.1 (covering) let a     rm be a set of vectors. we say that a
is r-covered by a set a(cid:48), with respect to the euclidean metric, if for all a     a
there exists a(cid:48)     a(cid:48) with (cid:107)a     a(cid:48)(cid:107)     r. we de   ne by n (r, a) the cardinality of
the smallest a(cid:48) that r-covers a.
example 27.1 (subspace) suppose that a     rm, let c = maxa   a (cid:107)a(cid:107), and as-
   
sume that a lies in a d-dimensional subspace of rm. then, n (r, a)     (2c
d/r)d.
to see this, let v1, . . . , vd be an orthonormal basis of the subspace. then, any
i=1   ivi with (cid:107)  (cid:107)        (cid:107)  (cid:107)2 = (cid:107)a(cid:107)2     c. let
      r and consider the set

a     a can be written as a = (cid:80)d

a(cid:48) =

(cid:40) d(cid:88)
given a     a s.t. a =(cid:80)d
(cid:107)a     a(cid:48)(cid:107)2 = (cid:107)(cid:88)

i=1

ivi :    i,   (cid:48)
  (cid:48)

(  (cid:48)

i

(cid:41)
i     {   c,   c +  ,   c + 2 , . . . , c}

.

(cid:107)vi(cid:107)2      2 d.

i       i)vi(cid:107)2      2 (cid:88)
(cid:32)
(cid:18) 2c

(cid:19)d

i

=

 

(cid:33)d

.

   
2c
r

d

d; then (cid:107)a     a(cid:48)(cid:107)     r and therefore a(cid:48) is an r-cover of a. hence,

n (r, a)     |a(cid:48)| =

i=1   ivi with (cid:107)  (cid:107)        c, there exists a(cid:48)     a(cid:48) such that

   
choose   = r/

27.1.1

properties

the following lemma is immediate from the de   nition.
lemma 27.2 for any a     rm, scalar c > 0, and vector a0     rm, we have

   r > 0, n (r,{c a + a0 : a     a})     n (cr, a).

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

27.2 from covering to rademacher complexity via chaining

389

next, we derive a contraction principle.
lemma 27.3 for each i     [m], let   i
: r     r be a   -lipschitz function;
namely, for all   ,        r we have |  i(  )       i(  )|       |         |. for a     rm let
  (a) denote the vector (  1(a1), . . . ,   m(am)). let       a = {  (a) : a     a}. then,

n (   r,        a)     n (r, a).

proof de   ne b =        a. let a(cid:48) be an r-cover of a and de   ne b(cid:48) =        a(cid:48).
then, for all a     a there exists a(cid:48)     a(cid:48) with (cid:107)a     a(cid:48)(cid:107)     r. so,
(ai     a(cid:48)

i))2       2(cid:88)

(cid:107)  (a)       (a(cid:48))(cid:107)2 =

(  i(ai)       i(a(cid:48)

i)2     (  r)2.

(cid:88)

hence, b(cid:48) is an (   r)-cover of b.

i

i

27.2

from covering to rademacher complexity via chaining

the following lemma bounds the rademacher complexity of a based on the
covering numbers n (r, a). this technique is called chaining and is attributed
to dudley.
lemma 27.4 let c = min  a maxa   a (cid:107)a       a(cid:107). then, for any integer m > 0,

r(a)     c 2   m   

m

+

6 c
m

log(n (c 2   k, a)).

2   k(cid:113)

m(cid:88)

k=1

proof let   a be a minimizer of the objective function given in the de   nition
of c. on the basis of lemma 26.6, we can analyze the rademacher complexity
assuming that   a = 0.
consider the set b0 = {0} and note that it is a c-cover of a. let b1, . . . , bm
be sets such that each bk corresponds to a minimal (c 2   k)-cover of a. let
a    = argmaxa   a(cid:104)  , a(cid:105) (where if there is more than one maximizer, choose one
in an arbitrary way, and if a maximizer does not exist, choose a    such that
(cid:104)  , a   (cid:105) is close enough to the supremum). note that a    is a function of   . for
each k, let b(k) be the nearest neighbor of a    in bk (hence b(k) is also a function
of   ). using the triangle inequality,
(cid:107)b(k)     b(k   1)(cid:107)     (cid:107)b(k)     a   (cid:107) + (cid:107)a        b(k   1)(cid:107)     c (2   k + 2   (k   1)) = 3 c 2   k.

for each k de   ne the set

  bk = {(a     a(cid:48)) : a     bk, a(cid:48)     bk   1,(cid:107)a     a(cid:48)(cid:107)     3 c 2   k}.

390

covering numbers

we can now write

r(a) =

=

1
m

1
m

e

e(cid:104)  , a   (cid:105)

(cid:34)
e(cid:104)(cid:107)  (cid:107)(cid:107)a        b(m )(cid:107)(cid:105)

(cid:104)  , a        b(m )(cid:105) +

m(cid:88)
m(cid:88)

k=1

+

k=1

(cid:35)

(cid:35)

(cid:104)  , b(k)     b(k   1)(cid:105)

(cid:34)

e

1
m

sup
a      bk

(cid:104)  , a(cid:105)

.

    1
m

   

m and (cid:107)a        b(m )(cid:107)     c 2   m , the    rst summand is at most

since (cid:107)  (cid:107) =
m 2   m . additionally, by massart lemma,
c   
1
m

(cid:104)  , a(cid:105)     3 c 2   k

m

e sup
a      bk

therefore,

(cid:112)2 log(n (c 2   k, a)2)
2   k(cid:113)

m(cid:88)

+

6c
m

k=1

r(a)     c 2   m   

m

(cid:112)log(n (c 2   k, a))

m

.

= 6 c 2   k

log(n (c2   k, a)).

as a corollary we obtain the following:

lemma 27.5 assume that there are   ,    > 0 such that for any k     1 we have

(cid:113)

log(n (c2   k, a))        +   k.

then,

r(a)     6c
m

(   + 2  ) .

k=1 k2   k = 2.

proof the bound follows from lemma 27.4 by taking m         and noting that

(cid:80)   
k=1 2   k = 1 and(cid:80)   
and such that c = maxa   a (cid:107)a(cid:107). we have shown that n (r, a)    (cid:16) 2c
fore, for any k, (cid:113)

example 27.2 consider a set a which lies in a d dimensional subspace of rm
. there-

(cid:17)d

   

   

d

r

(cid:16)

(cid:17)

log(n (c2   k, a))    

(cid:114)
(cid:113)
(cid:113)

   
   

d log

2k+1
   
d log(2
   
d log(2

d) +

k d

d) +

d k.

d
   
   

(cid:19)

(cid:32)

c(cid:112)d log(d)

(cid:33)

.

m

hence lemma 27.5 yields

(cid:18)(cid:113)

r(a)     6c
m

   
d log(2

d) + 2

   

d

= o

27.3 bibliographic remarks

391

27.3

bibliographic remarks

the chaining technique is due to dudley (1987). for an extensive study of cover-
ing numbers as well as other complexity measures that can be used to bound the
rate of uniform convergence we refer the reader to (anthony & bartlet 1999).

28 proof of the fundamental theorem

of learning theory

in this chapter we prove theorem 6.8 from chapter 6. we remind the reader
the conditions of the theorem, which will hold throughout this chapter: h is a
hypothesis class of functions from a domain x to {0, 1}, the id168 is the
0     1 loss, and vcdim(h) = d <    .

we shall prove the upper bound for both the realizable and agnostic cases
and shall prove the lower bound for the agnostic case. the lower bound for the
realizable case is left as an exercise.

28.1

the upper bound for the agnostic case
for the upper bound we need to prove that there exists c such that h is agnostic
pac learnable with sample complexity
mh( ,   )     c

d + ln(1/  )

.

 2

we will prove the slightly looser bound:

mh( ,   )     c

d log(d/ ) + ln(1/  )

 2

.

(28.1)

the tighter bound in the theorem statement requires a more involved proof, in
which a more careful analysis of the rademacher complexity using a technique
called    chaining    should be used. this is beyond the scope of this book.

to prove equation (28.1), it su   ces to show that applying the erm with a

sample size

(cid:18) 64d

(cid:19)

m     4

8

 2    (8d log(e/d) + 2 log(4/  ))

yields an  ,   -learner for h. we prove this result on the basis of theorem 26.5.
let (x1, y1), . . . , (xm, ym) be a classi   cation training set. recall that the sauer-
shelah lemma tells us that if vcdim(h) = d then

+

 2

32d

 2    log
|{(h(x1), . . . , h(xm)) : h     h}|     (cid:16) e m

(cid:17)d

.

denote a = {(1[h(x1)(cid:54)=y1], . . . , 1[h(xm)(cid:54)=ym]) : h     h}. this clearly implies that

d

|a|     (cid:16) e m

(cid:17)d

.

d

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

28.2 the lower bound for the agnostic case

393

combining this with lemma 26.8 we obtain the following bound on the rademacher
complexity:

r(a)    

2d log(em/d)

m

using theorem 26.5 we obtain that with id203 of at least 1       , for every
h     h we have that

ld(h)     ls(h)    

8d log(em/d)

m

+

2 log(2/  )

m

.

repeating the previous argument for minus the zero-one loss and applying the
union bound we obtain that with id203 of at least 1       , for every h     h
it holds that

.

(cid:114)

(cid:114)

|ld(h)     ls(h)|    

8d log(em/d)

m

+

2 log(4/  )

m

    2

8d log(em/d) + 2 log(4/  )

m

.

(cid:114)

(cid:114)

(cid:114)
(cid:114)

to ensure that this is smaller than   we need

m     4

 2    (8d log(m) + 8d log(e/d) + 2 log(4/  )) .

using lemma a.2, a su   cient condition for the inequality to hold is that

m     4

32d

 2    log

+

8

 2    (8d log(e/d) + 2 log(4/  )) .

(cid:18) 64d

(cid:19)

 2

28.2

the lower bound for the agnostic case
here, we prove that there exists c such that h is agnostic pac learnable with
sample complexity

mh( ,   )     c

d + ln(1/  )

.

 2

we will prove the lower bound in two parts. first, we will show that m( ,   )    
0.5 log(1/(4  ))/ 2, and second we will show that for every        1/8 we have that
m( ,   )     8d/ 2. these two bounds will conclude the proof.

28.2.1

showing that m( ,   )     0.5 log(1/(4  ))/ 2
   
2 and any        (0, 1), we have that m( ,   )    
we    rst show that for any   < 1/
0.5 log(1/(4  ))/ 2. to do so, we show that for m     0.5 log(1/(4  ))/ 2, h is not
learnable.

choose one example that is shattered by h. that is, let c be an example such

394

proof of the fundamental theorem of learning theory

that there are h+, h        h for which h+(c) = 1 and h   (c) =    1. de   ne two
distributions, d+ and d   , such that for b     {  1} we have

(cid:40) 1+yb 

2

0

db({(x, y)}) =

if x = c

otherwise.

2

that is, all the distribution mass is concentrated on two examples (c, 1) and
(c,   1), where the id203 of (c, b) is 1+b 
and the id203 of (c,   b) is
1   b 
2 .
let a be an arbitrary algorithm. any training set sampled from db has the
form s = (c, y1), . . . , (c, ym). therefore, it is fully characterized by the vector
y = (y1, . . . , ym)     {  1}m. upon receiving a training set s, the algorithm a
returns a hypothesis h : x     {  1}. since the error of a w.r.t. db only depends
on h(c), we can think of a as a mapping from {  1}m into {  1}. therefore,
we denote by a(y) the value in {  1} corresponding to the prediction of h(c),
where h is the hypothesis that a outputs upon receiving the training set s =
(c, y1), . . . , (c, ym).

note that for any hypothesis h we have

ldb (h) =

1     h(c)b 

2

.

in particular, the bayes optimal hypothesis is hb and

ldb (a(y))     ldb (hb) =

1     a(y)b 

2

    1      

2

=

(cid:40)

 

0

if a(y) (cid:54)= b
otherwise.

fix a. for b     {  1}, let y b = {y     {0, 1}m : a(y) (cid:54)= b}. the distribution db

induces a id203 pb over {  1}m. hence,

p [ldb (a(y))     ldb (hb) =  ] = db(y b) =

(cid:88)

y

pb[y]1[a(y)(cid:54)=b].

denote n + = {y : |{i : yi = 1}|     m/2} and n    = {  1}m \ n +. note that for
any y     n + we have p+[y]     p   [y] and for any y     n    we have p   [y]     p+[y].

28.2 the lower bound for the agnostic case

395

therefore,

(cid:88)
(cid:88)

y   n   

y   n   

1
2

1
2

(p+[y]1[a(y)(cid:54)=+] + p   [y]1[a(y)(cid:54)=   ])

(p+[y]1[a(y)(cid:54)=+] + p+[y]1[a(y)(cid:54)=   ])

p   [y]1[a(y)(cid:54)=   ]

(p+[y]1[a(y)(cid:54)=+] + p   [y]1[a(y)(cid:54)=   ]) +

(p   [y]1[a(y)(cid:54)=+] + p   [y]1[a(y)(cid:54)=   ]) +

    1
2

=

1
2
    1
2

max
b   {  1}

= max
b   {  1}

y

y

y

1
2

y   n +

pb[y]1[a(y)(cid:54)=b]

p+[y]1[a(y)(cid:54)=+] +

(cid:88)
p [ldb (a(y))     ldb (hb) =  ]
(cid:88)
(cid:88)
(cid:88)
(cid:88)
(cid:88)

(cid:88)
y   n + p   [y] = (cid:80)
1    (cid:112)1     exp(   m 2/(1      2))

p   [y] +

p+[y] .

y   n   

y   n +

(cid:16)

1
2

1
2

1
2

=

next note that (cid:80)

y   n +

y   n    p+[y], and both values are the prob-
ability that a binomial (m, (1      )/2) random variable will have value greater
than m/2. using lemma b.11, this id203 is lower bounded by

(cid:17)     1

(cid:16)

(cid:17)
1    (cid:112)1     exp(   2m 2)

,

2

where we used the assumption that  2     1/2. it follows that if m     0.5 log(1/(4  ))/ 2
then there exists b such that

(cid:19)
p [ldb (a(y))     ldb (hb) =  ]
      ,

(cid:113)

(cid:18)

1    

1    

   

4  

    1
2

28.2.2

where the last inequality follows by standard algebraic manipulations. this con-
cludes our proof.

   
let    = 8  and note that        (0, 1/

showing that m( , 1/8)     8d/ 2
we shall now prove that for every   < 1/(8
2). we will construct a family of distri-
butions as follows. first, let c = {c1, . . . , cd} be a set of d instances which are
shattered by h. second, for each vector (b1, . . . , bd)     {  1}d, de   ne a distribu-
tion db such that

2) we have that m( ,   )     8d
 2 .

   

(cid:40) 1
d    1+ybi  

2

0

if    i : x = ci
otherwise.

db({(x, y)}) =

that is, to sample an example according to db, we    rst sample an element ci     c
uniformly at random, and then set the label to be bi with id203 (1 +   )/2
or    bi with id203 (1       )/2.
it is easy to verify that the bayes optimal predictor for db is the hypothesis

396

proof of the fundamental theorem of learning theory

h     h such that h(ci) = bi for all i     [d], and its error is 1     
any other function f : x     {  1}, it is easy to verify that

2 . in addition, for

ldb (f ) =

1 +   

2

   |{i     [d] : f (ci) (cid:54)= bi}|

d

1       

2

+

   |{i     [d] : f (ci) = bi}|

d

.

therefore,

ldb (f )     min

h   h ldb (h) =       |{i     [d] : f (ci) (cid:54)= bi}|

d

.

(28.2)

next,    x some learning algorithm a. as in the proof of the no-free-lunch

theorem, we have that

max

db:b   {  1}d

e

s   dm

b

(cid:20)
ldb (a(s))     min

(cid:21)

(cid:20)
(cid:21)
h   h ldb (h)
(cid:20)
ldb (a(s))     min
      |{i     [d] : a(s)(ci) (cid:54)= bi|

h   h ldb (h)

d

e

db:b   u ({  1}d)

e

s   dm

b

e

db:b   u ({  1}d)

e

s   dm

b

   

=

=

d(cid:88)

i=1

  
d

e

db:b   u ({  1}d)

e

s   dm

b

1[a(s)(ci)(cid:54)=bi],

(cid:21)

(28.3)

(28.4)

(28.5)

(28.6)

d(cid:88)

i=1

  
d

where the    rst equality follows from equation (28.2). in addition, using the
de   nition of db, to sample s     db we can    rst sample (j1, . . . , jm)     u ([d])m, set
xr = cji, and    nally sample yr such that p[yr = bji] = (1 +   )/2. let us simplify
the notation and use y     b to denote sampling according to p[y = b] = (1 +   )/2.
therefore, the right-hand side of equation (28.6) equals

e

j   u ([d])m

e

b   u ({  1}d)

e

   r,yr   bjr

1[a(s)(ci)(cid:54)=bi].

(28.7)

we now proceed in two steps. first, we show that among all learning algorithms,
a, the one which minimizes equation (28.7) (and hence also equation (28.4))
is the maximum-likelihood learning rule, denoted am l. formally, for each i,
am l(s)(ci) is the majority vote among the set {yr : r     [m], xr = ci}. second,
we lower bound equation (28.7) for am l.

lemma 28.1 among all algorithms, equation (28.4) is minimized for a being
the maximum-likelihood algorithm, am l, de   ned as

   i, am l(s)(ci) = sign

(cid:32) (cid:88)

(cid:33)

yr

.

proof fix some j     [d]m. note that given j and y     {  1}m, the training set
s is fully determined. therefore, we can write a(j, y) instead of a(s). let us
also    x i     [d]. denote b  i the sequence (b1, . . . , bi   1, bi+1, . . . , bm). also, for any

r:xr=ci

28.2 the lower bound for the agnostic case

397

y     {  1}m, let yi denote the elements of y corresponding to indices for which
jr = i and let y  i be the rest of the elements of y. we have

b   u ({  1}d)

   r,yr   bjr

1[a(s)(ci)(cid:54)=bi]

e

(cid:88)

e

=

1
2

b  i   u ({  1}d   1)

bi   {  1}

(cid:88)

y

e

(cid:88)

y  i

p [y|b  i, bi]1[a(j,y)(ci)(cid:54)=bi]
(cid:88)

       (cid:88)

yi

bi   {  1}

=

e

b  i   u ({  1}d   1)

p [y  i|b  i]

1
2

p [yi|bi]1[a(j,y)(ci)(cid:54)=bi]

       .

the sum within the parentheses is minimized when a(j, y)(ci) is the maximizer
of p [yi|bi] over bi     {  1}, which is exactly the maximum-likelihood rule. re-
peating the same argument for all i we conclude our proof.

fix i. for every j, let ni(j) = {|t : jt = i|} be the number of instances in which
the instance is ci. for the maximum-likelihood rule, we have that the quantity

e

b   u ({  1}d)

e

   r,yr   bjr

1[am l(s)(ci)(cid:54)=bi]

is exactly the id203 that a binomial (ni(j), (1       )/2) random variable will
be larger than ni(j)/2. using lemma b.11, and the assumption   2     1/2, we
have that

(cid:16)

1    (cid:112)

1     e   2ni(j)  2(cid:17)

.

p [b     ni(j)/2]     1
2

we have thus shown that

d(cid:88)

i=1

  
d

j   u ([d])m

b   u ({  1}d)

e

d(cid:88)
d(cid:88)

i=1

i=1

      
2d

      
2d

e

e

1[a(s)(ci)(cid:54)=bi]

(cid:17)

   r,yr   bjr

(cid:16)
1    (cid:112)
(cid:16)
(cid:17)
1    (cid:112)2  2ni(j)

1     e   2  2ni(j)

,

e

j   u ([d])m

e

j   u ([d])m

where in the last inequality we used the inequality 1     e   a     a.

since the square root function is concave, we can apply jensen   s inequality to

obtain that the above is lower bounded by

(cid:33)

j   u ([d])m

ni(j)

2  2

(cid:32)
1    (cid:114)
d(cid:88)
(cid:16)
d(cid:88)
1    (cid:112)2  2m/d
(cid:17)
(cid:16)
1    (cid:112)2  2m/d

i=1

i=1

.

e

(cid:17)

      
2d

=

=

  
2d
  
2

398

proof of the fundamental theorem of learning theory

as long as m < d

8  2 , this term would be larger than   /4.

(cid:20)

in summary, we have shown that if m < d

8  2 then for any algorithm there

exists a distribution such that

e

ld(a(s))     min

h   h ld(h)

      /4.

s   dm
   (ld(a(s))     minh   h ld(h)) and note that         [0, 1] (see

finally, let     = 1
equation (28.5)). therefore, using lemma b.1, we get that

(cid:21)

(cid:20)

(cid:21)

p[ld(a(s))     min

h   h ld(h) >  ] = p
    1
4

    >

     
  

.

 
  

    e[   ]      
  

choosing    = 8  we conclude that if m < d
1/8 we will have ld(a(s))     minh   h ld(h)      .

512  2 , then with id203 of at least

28.3

the upper bound for the realizable case
here we prove that there exists c such that h is pac learnable with sample
complexity

mh( ,   )     c

d ln(1/ ) + ln(1/  )

 

.

we do so by showing that for m     c d ln(1/ )+ln(1/  )
erm rule. we prove this claim based on the notion of  -nets.
definition 28.2 ( -net) let x be a domain. s     x is an  -net for h     2x
with respect to a distribution d over x if

, h is learnable using the

 

   h     h : d(h)           h     s (cid:54)=    .

theorem 28.3 let h     2x with vcdim(h) = d. fix       (0, 1),        (0, 1/4)
and let

(cid:18)

(cid:18) 16e

(cid:19)

(cid:18) 2

(cid:19)(cid:19)

2d log

+ log

.

  

 

m     8
 

then, with id203 of at least 1        over a choice of s     dm we have that s
is an  -net for h.

proof let

b = {s     x : |s| = m,    h     h,d(h)      , h     s =    }

be the set of sets which are not  -nets. we need to bound p[s     b]. de   ne
2 }.
b(cid:48) = {(s, t )     x : |s| = |t| = m,    h     h,d(h)      , h     s =    , |t     h| >  m

28.3 the upper bound for the realizable case

399

claim 1
p[s     b]     2 p[(s, t )     b(cid:48)].
proof of claim 1 : since s and t are chosen independently we can write

(cid:2)1[(s,t )   b(cid:48)]

(cid:3) = e

(cid:104) e

(cid:2)1[(s,t )   b(cid:48)]

(cid:3)(cid:105)

.

p[(s, t )     b(cid:48)] =

e

(s,t )   d2m

s   dm

t   dm

note that (s, t )     b(cid:48) implies s     b and therefore 1[(s,t )   b(cid:48)] = 1[(s,t )   b(cid:48)] 1[s   b],
which gives

p[(s, t )     b(cid:48)] = e
s   dm
= e

s   dm

e

t   dm
1[s   b] e

t   dm

1[(s,t )   b(cid:48)] 1[s   b]

1[(s,t )   b(cid:48)].

fix some s. then, either 1[s   b] = 0 or s     b and then    hs such that d(hs)      
and |hs     s| = 0. it follows that a su   cient condition for (s, t )     b(cid:48) is that
|t     hs| >  m

2 . therefore, whenever s     b we have

e

t   dm

1[(s,t )   b(cid:48)]    

p

t   dm

[|t     hs| >  m
2 ].

but, since we now assume s     b we know that d(hs) =         . therefore,
|t     hs| is a binomial random variable with parameters    (id203 of success
for a single try) and m (number of tries). cherno      s inequality implies
p[|t   hs|       m
thus,
p[|t     hs| >  m
combining all the preceding we conclude the proof of claim 1.

2 ]     1     p[|t     hs|       m

2 ] = 1     p[|t     hs|      m

    2
m   (m     m  /2)2

2 ]     1/2.

2 ]     e

= e   m  /2     e   m /2     e   d log(1/  )/2 =   d/2     1/2.

claim 2 (symmetrization):
p[(s, t )     b(cid:48)]     e    m/4   h(2m).
proof of claim 2 : to simplify notation, let    = m /2 and for a sequence a =
(x1, . . . , x2m) let a0 = (x1, . . . , xm). using the de   nition of b(cid:48) we get that

p[a     b(cid:48)] = e
    e

a   d2m

a   d2m

max
h   h
max
h   h

1[d(h)    ] 1[|h   a0|=0] 1[|h   a|     ]

1[|h   a0|=0] 1[|h   a|     ].

now, let us de   ne by ha the e   ective number of di   erent hypotheses on a,
namely, ha = {h     a : h     h }. it follows that

p[a     b(cid:48)]     e
    e

a   d2m

a   d2m

max
h   ha

(cid:88)

h   ha

1[|h   a0|=0] 1[|h   a|     ]

1[|h   a0|=0] 1[|h   a|     ].

let j = {j     [2m] : |j| = m}. for any j     j and a = (x1, . . . , x2m) de   ne
aj = (xj1, . . . , xjm). since the elements of a are chosen i.i.d., we have that
for any j     j and any function f (a, a0) it holds that ea   d2m[f (a, a0)] =

400

proof of the fundamental theorem of learning theory

(cid:80)

ea   d2m[f (a, aj)]. since this holds for any j it also holds for the expectation of
j chosen at random from j. in particular, it holds for the function f (a, a0) =

h   ha

1[|h   a0|=0] 1[|h   a|     ]. we therefore obtain that

(cid:88)

p[a     b(cid:48)]     e

a   d2m

= e

a   d2m

e
j   j

(cid:88)

h   ha

1[|h   aj|=0] 1[|h   a|     ]

h   ha
1[|h   a|     ] e
j   j

1[|h   aj|=0].

now,    x some a s.t. |h     a|       . then, ej 1[|h   aj|=0] is the id203 that
when choosing m balls from a bag with at least    red balls, we will never choose
a red ball. this id203 is at most

(1       /(2m))m = (1      /4)m     e    m/4.

we therefore get that

p[a     b(cid:48)]     e

a   d2m

(cid:88)

h   ha

e    m/4     e    m/4 e

a   d2m

|ha|.

using the de   nition of the growth function we conclude the proof of claim 2.

completing the proof: by sauer   s lemma we know that   h(2m)     (2em/d)d.

combining this with the two claims we obtain that

p[s     b]     2(2em/d)d e    m/4.

we would like the right-hand side of the inequality to be at most   ; that is,

2(2em/d)d e    m/4       .

rearranging, we obtain the requirement

m     4
 

(d log(2em/d) + log(2/  )) =

4d
 

log(m) +

4
 

(d log(2e/d) + log(2/  ).

using lemma a.2, a su   cient condition for the preceding to hold is that

a su   cient condition for this is that

 

+

8
 

(cid:18) 8d
(cid:19)
(cid:18) 8d
(cid:19)
(cid:18) 8d 2e
(cid:19)(cid:19)
(cid:19)
(cid:18) 16e

16
 

d 

+

 

m     16d
 

log

log

(cid:18)

log

m     16d
 

=

=

16d

(cid:18)

 

8
 

+

log(2/  )

8
 

(cid:18) 2

(cid:19)(cid:19)

2d log

+ log

.

  

 

(d log(2e/d) + log(2/  ).

(d log(2e/d) + 1

2 log(2/  )

and this concludes our proof.

28.3 the upper bound for the realizable case

401

28.3.1

from  -nets to pac learnability
theorem 28.4 let h be a hypothesis class over x with vcdim(h) = d. let
d be a distribution over x and let c     h be a target hypothesis. fix  ,        (0, 1)
and let m be as de   ned in theorem 28.3. then, with id203 of at least 1       
over a choice of m i.i.d. instances from x with labels according to c we have that
any erm hypothesis has a true error of at most  .

proof de   ne the class hc = {c(cid:97) h : h     h}, where c(cid:97) h = (h\ c)    (c\ h). it is
note that ld(h) = d(h(cid:97) c). therefore, for any h     h with ld(h)       we have
that |(h(cid:97) c)    s| > 0, which implies that h cannot be an erm hypothesis, which

easy to verify that if some a     x is shattered by h then it is also shattered by hc
and vice versa. hence, vcdim(h) = vcdim(hc). therefore, using theorem 28.3
we know that with id203 of at least 1       , the sample s is an  -net for hc.

concludes our proof.

29 multiclass learnability

in chapter 17 we have introduced the problem of multiclass categorization, in
which the goal is to learn a predictor h : x     [k]. in this chapter we address pac
learnability of multiclass predictors with respect to the 0-1 loss. as in chapter 6,
the main goal of this chapter is to:
    characterize which classes of multiclass hypotheses are learnable in the (mul-

ticlass) pac model.

    quantify the sample complexity of such hypothesis classes.

in view of the fundamental theorem of learning theory (theorem 6.8), it is natu-
ral to seek a generalization of the vc dimension to multiclass hypothesis classes.
in section 29.1 we show such a generalization, called the natarajan dimension,
and state a generalization of the fundamental theorem based on the natarajan
dimension. then, we demonstrate how to calculate the natarajan dimension of
several important hypothesis classes.

recall that the main message of the fundamental theorem of learning theory
is that a hypothesis class of binary classi   ers is learnable (with respect to the
0-1 loss) if and only if it has the uniform convergence property, and then it
is learnable by any erm learner. in chapter 13, exercise 2, we have shown
that this equivalence breaks down for a certain convex learning problem. the
last section of this chapter is devoted to showing that the equivalence between
learnability and uniform convergence breaks down even in multiclass problems
with the 0-1 loss, which are very similar to binary classi   cation. indeed, we
construct a hypothesis class which is learnable by a speci   c erm learner, but
for which other erm learners might fail and the uniform convergence property
does not hold.

29.1

the natarajan dimension

in this section we de   ne the natarajan dimension, which is a generalization of
the vc dimension to classes of multiclass predictors. throughout this section,
let h be a hypothesis class of multiclass predictors; namely, each h     h is a
function from x to [k].

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

29.2 the multiclass fundamental theorem

403

to de   ne the natarajan dimension, we    rst generalize the de   nition of shat-

tering.
definition 29.1 (shattering (multiclass version)) we say that a set c     x
is shattered by h if there exist two functions f0, f1 : c     [k] such that
    for every x     c, f0(x) (cid:54)= f1(x).
    for every b     c, there exists a function h     h such that

   x     b, h(x) = f0(x) and    x     c \ b, h(x) = f1(x).

definition 29.2 (natarajan dimension) the natarajan dimension of h, de-
noted ndim(h), is the maximal size of a shattered set c     x .

it is not hard to see that in the case that there are exactly two classes,
ndim(h) = vcdim(h). therefore, the natarajan dimension generalizes the vc
dimension. we next show that the natarajan dimension allows us to general-
ize the fundamental theorem of statistical learning from binary classi   cation to
multiclass classi   cation.

29.2

the multiclass fundamental theorem

theorem 29.3 (the multiclass fundamental theorem) there exist absolute
constants c1, c2 > 0 such that the following holds. for every hypothesis class h
of functions from x to [k], such that the natarajan dimension of h is d, we have
1. h has the uniform convergence property with sample complexity
d log (k) + log(1/  )

d + log(1/  )

    much ( ,   )     c2

2. h is agnostic pac learnable with sample complexity

c1

c1

 2

 2

d + log(1/  )

    mh( ,   )     c2

3. h is pac learnable (assuming realizability) with sample complexity

 2

 

d log (k) + log(1/  )

 2

d log(cid:0) kd

(cid:1) + log(1/  )

 

.

.

.

c1

d + log(1/  )

 

    mh( ,   )     c2

29.2.1

on the proof of theorem 29.3

the lower bounds in theorem 29.3 can be deduced by a reduction from the
binary fundamental theorem (see exercise 5).

the upper bounds in theorem 29.3 can be proved along the same lines of the
proof of the fundamental theorem for binary classi   cation, given in chapter 28
(see exercise 4). the sole ingredient of that proof that should be modi   ed in a
nonstraightforward manner is sauer   s lemma. it applies only to binary classes
and therefore must be replaced. an appropriate substitute is natarajan   s lemma:

404

multiclass learnability

lemma 29.4 (natarajan)

|h|     |x|ndim(h)    k2ndim(h).

the proof of natarajan   s lemma shares the same spirit of the proof of sauer   s

lemma and is left as an exercise (see exercise 3).

29.3

calculating the natarajan dimension

in this section we show how to calculate (or estimate) the natarajan dimen-
sion of several popular classes, some of which were studied in chapter 17. as
these calculations indicate, the natarajan dimension is often proportional to the
number of parameters required to de   ne a hypothesis.

29.3.1

one-versus-all based classes

in chapter 17 we have seen two reductions of multiclass categorization to bi-
nary classi   cation: one-versus-all and all-pairs. in this section we calculate the
natarajan dimension of the one-versus-all method.

recall that in one-versus-all we train, for each label, a binary classi   er that
distinguishes between that label and the rest of the labels. this naturally sug-
gests considering multiclass hypothesis classes of the following form. let hbin    
{0, 1}x be a binary hypothesis class. for every   h = (h1, . . . , hk)     (hbin)k de   ne
t (  h) : x     [k] by

t (  h)(x) = argmax

i   [k]

hi(x).

if there are two labels that maximize hi(x), we choose the smaller one. also, let

hova,k
bin = {t (  h) :   h     (hbin)k}.
what    should    be the natarajan dimension of hova,k
? intuitively, to specify a
hypothesis in hbin we need d = vcdim(hbin) parameters. to specify a hypothe-
sis in hova,k
, we need to specify k hypotheses in hbin. therefore, kd parameters
should su   ce. the following lemma establishes this intuition.

bin

bin

lemma 29.5

if d = vcdim(hbin) then

ndim(hova,k

bin

)     3kd log (kd) .

proof let c     x be a shattered set. by the de   nition of shattering (for mul-
ticlass hypotheses)

(cid:12)(cid:12)(cid:12)(cid:16)hova,k
ses from hbin. therefore,(cid:12)(cid:12)(cid:12)(cid:16)hova,k
(cid:17)

(cid:12)(cid:12)(cid:12)     2|c|.
(cid:17)
(cid:12)(cid:12)(cid:12)     | (hbin)c |k.

on the other hand, each hypothesis in hova,k

bin

bin

bin

c

c

is determined by using k hypothe-

29.3 calculating the natarajan dimension

405

by sauer   s lemma, | (hbin)c |     |c|d. we conclude that

2|c|     (cid:12)(cid:12)(cid:12)(cid:16)hova,k

bin

(cid:17)

(cid:12)(cid:12)(cid:12)     |c|dk.

c

the proof follows by taking the logarithm and applying lemma a.1.

how tight is lemma 29.5? it is not hard to see that for some classes, ndim(hova,k

bin
can be much smaller than dk (see exercise 1). however there are several natural
binary classes, hbin (e.g., halfspaces), for which ndim(hova,k
) =    (dk) (see
exercise 6).

bin

)

29.3.2

general multiclass-to-binary reductions

the same reasoning used to establish lemma 29.5 can be used to upper bound
the natarajan dimension of more general multiclass-to-binary reductions. these
reductions train several binary classi   ers on the data. then, given a new in-
stance, they predict its label by using some rule that takes into account the
labels predicted by the binary classi   ers. these reductions include one-versus-
all and all-pairs.
suppose that such a method trains l binary classi   ers from a binary class hbin,
and r : {0, 1}l     [k] is the rule that determines the (multiclass) label according
to the predictions of the binary classi   ers. the hypothesis class corresponding
to this method can be de   ned as follows. for every   h = (h1, . . . , hl)     (hbin)l
de   ne r(  h) : x     [k] by

finally, let

r(  h)(x) = r(h1(x), . . . , hl(x)).

hr
bin = {r(  h) :   h     (hbin)l}.

similarly to lemma 29.5 it can be proven that:

lemma 29.6

if d = vcdim(hbin) then

ndim(hr

bin)     3 l d log (l d) .

the proof is left as exercise 2.

29.3.3

linear multiclass predictors

(cid:40)

next, we consider the class of linear multiclass predictors (see section 17.2). let
   : x    [k]     rd be some class-sensitive feature mapping and let

h   =

x (cid:55)    argmax
i   [k]

(cid:104)w,   (x, i)(cid:105) : w     rd

.

(29.1)

each hypothesis in h   is determined by d parameters, namely, a vector w    
rd. therefore, we would expect that the natarajan dimension would be upper
bounded by d. indeed:

(cid:41)

406

multiclass learnability

theorem 29.7 ndim(h  )     d .
proof let c     x be a shattered set, and let f0, f1 : c     [k] be the two
functions that witness the shattering. we need to show that |c|     d. for every
x     c let   (x) =   (x, f0(x))       (x, f1(x)). we claim that the set   (c) def=
{  (x) : x     c} consists of |c| elements (i.e.,    is one to one) and is shattered
by the binary hypothesis class of homogeneous linear separators on rd,

h = {x (cid:55)    sign((cid:104)w, x(cid:105)) : w     rd}.

since vcdim(h) = d, it will follow that |c| = |  (c)|     d, as required.
to establish our claim it is enough to show that |h  (c)| = 2|c|. indeed, given
a subset b     c, by the de   nition of shattering, there exists hb     h   for which

   x     b, hb(x) = f0(x)

and    x     c \ b, hb(x) = f1(x).

let wb     rd be a vector that de   nes hb. we have that, for every x     b,

(cid:104)w,   (x, f0(x))(cid:105) > (cid:104)w,   (x, f1(x))(cid:105)     (cid:104)w,   (x)(cid:105) > 0.

similarly, for every x     c \ b,

(cid:104)w,   (x)(cid:105) < 0.

it follows that the hypothesis gb     h de   ned by the same w     rd label the
points in   (b) by 1 and the points in   (c \ b) by 0. since this holds for every
b     c we obtain that |c| = |  (c)| and |h  (c)| = 2|c|, which concludes our
proof.

the theorem is tight in the sense that there are mappings    for which ndim(h  ) =

   (d). for example, this is true for the multivector construction (see section 17.2
and the bibliographic remarks at the end of this chapter). we therefore con-
clude:
corollary 29.8 let x = rn and let    : x    [k]     rnk be the class sensitive
feature mapping for the multi-vector construction:

(cid:124) (cid:123)(cid:122) (cid:125)

  (x, y) = [ 0, . . . , 0
   r(y   1)n

(cid:124)

(cid:123)(cid:122)

   rn

, x1, . . . , xn

(cid:125)

(cid:124) (cid:123)(cid:122) (cid:125)

, 0, . . . , 0
   r(k   y)n

].

let h   be as de   ned in equation (29.1). then, the natarajan dimension of h  
satis   es

(k     1)(n     1)     ndim(h  )     kn.

29.4

on good and bad erms

in this section we present an example of a hypothesis class with the property
that not all erms for the class are equally successful. furthermore, if we allow
an in   nite number of labels, we will also obtain an example of a class that is

29.4 on good and bad erms

407

learnable by some erm, but other erms will fail to learn it. clearly, this also
implies that the class is learnable but it does not have the uniform convergence
property. for simplicity, we consider only the realizable case.
the class we consider is de   ned as follows. the instance space x will be any
   nite or countable set. let pf (x ) be the collection of all    nite and co   nite
subsets of x (that is, for each a     pf (x ), either a or x \ a must be    nite).
instead of [k], the label set is y = pf (x )     {   }, where     is some special label.
for every a     pf (x ) de   ne ha : x     y by

(cid:40)

ha(x) =

a x     a
   
x /    a

finally, the hypothesis class we take is

h = {ha : a     pf (x )}.

let a be some erm algorithm for h. assume that a operates on a sample
labeled by ha     h. since ha is the only hypothesis in h that might return
the label a, if a observes the label a, it    knows    that the learned hypothesis
is ha, and, as an erm, must return it (note that in this case the error of the
returned hypothesis is 0). therefore, to specify an erm, we should only specify
the hypothesis it returns upon receiving a sample of the form

s = {(x1,   ), . . . , (xm,   )}.
we consider two erms: the    rst, agood, is de   ned by

agood(s) = h   ;

that is, it outputs the hypothesis which predicts    *    for every x     x . the second
erm, abad, is de   ned by

abad(s) = h{x1,...xm}c.

the following claim shows that the sample complexity of abad is about |x|-times
larger than the sample complexity of agood. this establishes a gap between
di   erent erms. if x is in   nite, we even obtain a learnable class that is not
learnable by every erm.

  log(cid:0) 1

(cid:1) examples, sampled according to d and labeled by

claim 29.9
1. let  ,    > 0, d a distribution over x and ha     h. let s be an i.i.d. sample
consisting of m     1
ha. then, with id203 of at least 1       , the hypothesis returned by agood
will have an error of at most  .
2. there exists a constant a > 0 such that for every 0 <   < a there exists a
distribution d over x and ha     h such that the following holds. the hypoth-
esis returned by abad upon receiving a sample of size m     |x|   1
, sampled
according to d and labeled by ha, will have error       with id203     e    1
6 .

6 

  

408

multiclass learnability

  log( 1

proof let d be a distribution over x and suppose that the correct labeling
is ha. for any sample, agood returns either h    or ha. if it returns ha then its
true error is zero. thus, it returns a hypothesis with error       only if all the m
examples in the sample are from x \ a while the error of h   , ld(h   ) = pd[a],
is      . assume m     1
   ); then the id203 of the latter event is no more
than (1      )m     e    m       . this establishes item 1.
next we prove item 2. we restrict the proof to the case that |x| = d <    .

the proof for in   nite x is similar. suppose that x = {x0, . . . , xd   1}.
let a > 0 be small enough such that 1     2      e   4  for every   < a and    x
some   < a. de   ne a distribution on x by setting p[x0] = 1     2  and for all
1     i     d    1, p[xi] = 2 
d   1 . suppose that the correct hypothesis is h    and let the
sample size be m. clearly, the hypothesis returned by abad will err on all the
examples from x which are not in the sample. by cherno      s bound, if m     d   1
6  ,
then with id203     e    1
examples
from x . thus the returned hypothesis will have error      .

6 , the sample will include no more than d   1

2

the conclusion of the example presented is that in multiclass classi   cation,
the sample complexity of di   erent erms may di   er. are there    good    erms
for every hypothesis class? the following conjecture asserts that the answer is
yes.

conjecture 29.10 the realizable sample complexity of every hypothesis class
h     [k]

is

x

(cid:18) ndim(h)

(cid:19)

.

 

mh( ,   ) =   o

we emphasize that the   o notation may hide only poly-log factors of  ,   , and
ndim(h), but no factor of k.

29.5

bibliographic remarks

the natarajan dimension is due to natarajan (1989). that paper also established
the natarajan lemma and the generalization of the fundamental theorem. gen-
eralizations and sharper versions of the natarajan lemma are studied in haussler
& long (1995). ben-david, cesa-bianchi, haussler & long (1995) de   ned a large
family of notions of dimensions, all of which generalize the vc dimension and
may be used to estimate the sample complexity of multiclass classi   cation.

the calculation of the natarajan dimension, presented here, together with
calculation of other classes, can be found in daniely et al. (2012). the example
of good and bad erms, as well as conjecture 29.10, are from daniely et al.
(2011).

29.6 exercises

409

29.6

exercises
1. let d, k > 0. show that there exists a binary hypothesis hbin of vc dimension

d such that ndim(hova,k

) = d.

bin

2. prove lemma 29.6.
3. prove natarajan   s lemma.

hint: fix some x0     x . for i, j     [k], denote by hij all the functions f :
x \ {x0}     [k] that can be extended to a function in h both by de   ning
i(cid:54)=j |hij|

f (x0) = i and by de   ning f (x0) = j. show that |h|     |hx\{x0}| +(cid:80)

and use induction.

4. adapt the proof of the binary fundamental theorem and natarajan   s lemma
to prove that, for some universal constant c > 0 and for every hypothesis
class of natarajan dimension d, the agnostic sample complexity of h is

d log(cid:0) kd

(cid:1) + log(1/  )

 

.

 2

mh( ,   )     c

5. prove that, for some universal constant c > 0 and for every hypothesis class

of natarajan dimension d, the agnostic sample complexity of h is

mh( ,   )     c

d + log(1/  )

 2

.

hint: deduce it from the binary fundamental theorem.
6. let h be the binary hypothesis class of (nonhomogenous) halfspaces in rd.
the goal of this exercise is to prove that ndim(hova,k)     (d     1)    (k     1).
1. let hdiscrete be the class of all functions f : [k     1]    [d     1]     {0, 1} for

which there exists some i0 such that, for every j     [d     1]
   i < i0, f (i, j) = 1 while    i > i0, f (i, j) = 0.

2. show that hdiscrete can be realized by h. that is, show that there exists

show that ndim(hova,k
discrete) = (d     1)    (k     1).
a mapping    : [k     1]    [d     1]     rd such that

hdiscrete     {h        : h     h} .

hint: you can take   (i, j) to be the vector whose jth coordinate is 1, whose
last coordinate is i and the rest are zeros.

3. conclude that ndim(hova,k)     (d     1)    (k     1).

30 compression bounds

throughout the book, we have tried to characterize the notion of learnability
using di   erent approaches. at    rst we have shown that the uniform conver-
gence property of a hypothesis class guarantees successful learning. later on we
introduced the notion of stability and have shown that stable algorithms are
guaranteed to be good learners. yet there are other properties which may be
su   cient for learning, and in this chapter and its sequel we will introduce two
approaches to this issue: compression bounds and the pac-bayes approach.

in this chapter we study compression bounds. roughly speaking, we shall see
that if a learning algorithm can express the output hypothesis using a small sub-
set of the training set, then the error of the hypothesis on the rest of the examples
estimates its true error. in other words, an algorithm that can    compress    its
output is a good learner.

30.1

compression bounds

to motivate the results, let us    rst consider the following learning protocol.
first, we sample a sequence of k examples denoted t . on the basis of these
examples, we construct a hypothesis denoted ht . now we would like to estimate
the performance of ht so we sample a fresh sequence of m    k examples, denoted
v , and calculate the error of ht on v . since v and t are independent, we
immediately get the following from bernstein   s inequality (see lemma b.10).

lemma 30.1 assume that the range of the id168 is [0, 1]. then,

(cid:34)

(cid:115)

(cid:35)

p

ld(ht )     lv (ht )    

2lv (ht ) log(1/  )

|v |

+

4 log(1/  )

|v |

      .

to derive this bound, all we needed was independence between t and v .
therefore, we can rede   ne the protocol as follows. first, we agree on a sequence
of k indices i = (i1, . . . , ik)     [m]k. then, we sample a sequence of m examples
s = (z1, . . . , zm). now, de   ne t = si = (zi1, . . . , zik ) and de   ne v to be the
rest of the examples in s. note that this protocol is equivalent to the protocol
we de   ned before     hence lemma 30.1 still holds.

applying a union bound over the choice of the sequence of indices we obtain

the following theorem.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

30.1 compression bounds

411

(cid:114)

theorem 30.2 let k be an integer and let b : z k     h be a mapping from
sequences of k examples to the hypothesis class. let m     2k be a training set
size and let a : z m     h be a learning rule that receives a training sequence s
of size m and returns a hypothesis such that a(s) = b(zi1, . . . , zik ) for some
(i1, . . . , ik)     [m]k. let v = {zj : j /    (i1, . . . , ik)} be the set of examples which
were not selected for de   ning a(s). then, with id203 of at least 1        over
the choice of s we have

ld(a(s))     lv (a(s)) +

lv (a(s))

4k log(m/  )

m

+

8k log(m/  )

m

.

(cid:34)

proof for any i     [m]k let hi = b(zi1, . . . , zik ). let n = m     k. combining
(cid:35)
lemma 30.1 with the union bound we have
(cid:35)

   i     [m]k s.t. ld(hi )     lv (hi )    

(cid:114)
(cid:114)

2lv (hi ) log(1/  )

4 log(1/  )

(cid:34)

p

+

n

n

p

ld(hi )     lv (hi )    

2lv (hi ) log(1/  )

4 log(1/  )

n

+

n

    (cid:88)

i   [m]k
    mk  .

denote   (cid:48) = mk  . using the assumption k     m/2, which implies that n =
m     k     m/2, the above implies that with id203 of at least 1       (cid:48) we have
that

ld(a(s))     lv (a(s)) +

lv (a(s))

4k log(m/  (cid:48))

m

+

8k log(m/  (cid:48))

m

,

(cid:114)

which concludes our proof.

as a direct corollary we obtain:

corollary 30.3 assuming the conditions of theorem 30.2, and further as-
suming that lv (a(s)) = 0, then, with id203 of at least 1       over the choice
of s we have

ld(a(s))     8k log(m/  )

m

.

these results motivate the following de   nition:

(compression scheme) let h be a hypothesis class of
definition 30.4
functions from x to y and let k be an integer. we say that h has a compression
scheme of size k if the following holds:
for all m there exists a : z m     [m]k and b : z k     h such that for all h     h,
if we feed any training set of the form (x1, h(x1)), . . . , (xm, h(xm)) into a and
then feed (xi1 , h(xi1 )), . . . , (xik , h(xik )) into b, where (i1, . . . , ik) is the output
of a, then the output of b, denoted h(cid:48), satis   es ls(h(cid:48)) = 0.

it is possible to generalize the de   nition for unrealizable sequences as follows.

412

compression bounds

definition 30.5
(compression scheme for unrealizable sequences)
let h be a hypothesis class of functions from x to y and let k be an integer.
we say that h has a compression scheme of size k if the following holds:
for all m there exists a : z m     [m]k and b : z k     h such that for all h     h,
if we feed any training set of the form (x1, y1), . . . , (xm, ym) into a and then
feed (xi1, yi1 ), . . . , (xik , yik ) into b, where (i1, . . . , ik) is the output of a, then
the output of b, denoted h(cid:48), satis   es ls(h(cid:48))     ls(h).

the following lemma shows that the existence of a compression scheme for
the realizable case also implies the existence of a compression scheme for the
unrealizable case.
lemma 30.6 let h be a hypothesis class for binary classi   cation, and assume
it has a compression scheme of size k in the realizable case. then, it has a
compression scheme of size k for the unrealizable case as well.

proof consider the following scheme: first,    nd an erm hypothesis and denote
it by h. then, discard all the examples on which h errs. now, apply the realizable
compression scheme on the examples that have not been removed. the output of
the realizable compression scheme, denoted h(cid:48), must be correct on the examples
that have not been removed. since h errs on the removed examples it follows
that the error of h(cid:48) cannot be larger than the error of h; hence h(cid:48) is also an erm
hypothesis.

30.2

examples

in the examples that follows, we present compression schemes for several hy-
pothesis classes for binary classi   cation. in light of lemma 30.6 we focus on the
realizable case. therefore, to show that a certain hypothesis class has a com-
pression scheme, it is necessary to show that there exist a, b, and k for which
ls(h(cid:48)) = 0.

30.2.1

axis aligned rectangles

note that this is an uncountable in   nite class. we show that there is a simple
compression scheme. consider the algorithm a that works as follows: for each
dimension, choose the two positive examples with extremal values at this dimen-
sion. de   ne b to be the function that returns the minimal enclosing rectangle.
then, for k = 2d, we have that in the realizable case, ls(b(a(s))) = 0.

30.2.2

halfspaces
let x = rd and consider the class of homogenous halfspaces, {x (cid:55)    sign((cid:104)w, x(cid:105)) :
w     rd}.

30.2 examples

413

a compression scheme:
w.l.o.g. assume all labels are positive (otherwise, replace xi by yixi). the com-
pression scheme we propose is as follows. first, a    nds the vector w which is
in the convex hull of {x1, . . . , xm} and has minimal norm. then, it represents it
as a convex combination of d points in the sample (it will be shown later that
this is always possible). the output of a are these d points. the algorithm b
receives these d points and set w to be the point in their convex hull of minimal
norm.
next we prove that this indeed is a compression sceme. since the data is
linearly separable, the convex hull of {x1, . . . , xm} does not contain the origin.
consider the point w in this convex hull closest to the origin. (this is a unique
point which is the euclidean projection of the origin onto this convex hull.) we
claim that w separates the data.1 to see this, assume by contradiction that
(cid:107)xi(cid:107)2+(cid:107)w(cid:107)2     (0, 1).
(cid:104)w, xi(cid:105)     0 for some i. take w(cid:48) = (1       )w +   xi for    =
then w(cid:48) is also in the convex hull and

(cid:107)w(cid:107)2

(cid:107)w(cid:48)(cid:107)2 = (1       )2(cid:107)w(cid:107)2 +   2(cid:107)xi(cid:107)2 + 2  (1       )(cid:104)w, xi(cid:105)

    (1       )2(cid:107)w(cid:107)2 +   2(cid:107)xi(cid:107)2
(cid:107)xi(cid:107)4(cid:107)w(cid:107)2 + (cid:107)xi(cid:107)2(cid:107)w(cid:107)4

((cid:107)w(cid:107)2 + (cid:107)xi(cid:107)2)2

=

=

(cid:107)xi(cid:107)2(cid:107)w(cid:107)2
(cid:107)w(cid:107)2 + (cid:107)xi(cid:107)2

= (cid:107)w(cid:107)2   
< (cid:107)w(cid:107)2,

1

(cid:107)w(cid:107)2/(cid:107)xi(cid:107)2 + 1

which leads to a contradiction.

we have thus shown that w is also an erm. finally, since w is in the convex
hull of the examples, we can apply caratheodory   s theorem to obtain that w is
also in the convex hull of a subset of d + 1 points of the polygon. furthermore,
the minimality of w implies that w must be on a face of the polygon and this
implies it can be represented as a convex combination of d points.

it remains to show that w is also the projection onto the polygon de   ned by the
d points. but this must be true: on one hand, the smaller polygon is a subset of
the larger one; hence the projection onto the smaller cannot be smaller in norm.
on the other hand, w itself is a valid solution. the uniqueness of projection
concludes our proof.

30.2.3

separating polynomials
let x = rd and consider the class x (cid:55)    sign(p(x)) where p is a degree r polyno-
mial.

1 it can be shown that w is the direction of the max-margin solution.

414

compression bounds

note that p(x) can be rewritten as (cid:104)w,   (x)(cid:105) where the elements of   (x) are all
the monomials of x up to degree r. therefore, the problem of constructing a com-
pression scheme for p(x) reduces to the problem of constructing a compression
scheme for halfspaces in rd(cid:48)

where d(cid:48) = o(dr).

30.2.4

separation with margin

suppose that a training set is separated with margin   . the id88 algorithm
guarantees to make at most 1/  2 updates before converging to a solution that
makes no mistakes on the entire training set. hence, we have a compression
scheme of size k     1/  2.

30.3

bibliographic remarks

compression schemes and their relation to learning were introduced by little-
stone & warmuth (1986). as we have shown, if a class has a compression scheme
then it is learnable. for binary classi   cation problems, it follows from the funda-
mental theorem of learning that the class has a    nite vc dimension. the other
direction, namely, whether every hypothesis class of    nite vc dimension has a
compression scheme of    nite size, is an open problem posed by manfred war-
muth and is still open (see also (floyd 1989, floyd & warmuth 1995, ben-david
& litman 1998, livni & simon 2013).

31 pac-bayes

31.1

the minimum description length (mdl) and occam   s razor principles allow a
potentially very large hypothesis class but de   ne a hierarchy over hypotheses and
prefer to choose hypotheses that appear higher in the hierarchy. in this chapter
we describe the pac-bayesian approach that further generalizes this idea. in
the pac-bayesian approach, one expresses the prior knowledge by de   ning prior
distribution over the hypothesis class.

pac-bayes bounds
as in the mdl paradigm, we de   ne a hierarchy over hypotheses in our class h.
now, the hierarchy takes the form of a prior distribution over h. that is, we
assign a id203 (or density if h is continuous) p (h)     0 for each h     h
and refer to p (h) as the prior score of h. following the bayesian reasoning
approach, the output of the learning algorithm is not necessarily a single hy-
pothesis. instead, the learning process de   nes a posterior id203 over h,
which we denote by q. in the context of a supervised learning problem, where
h contains functions from x to y, one can think of q as de   ning a randomized
prediction rule as follows. whenever we get a new instance x, we randomly pick
a hypothesis h     h according to q and predict h(x). we de   ne the loss of q on
an example z to be

(cid:96)(q, z) def= e
h   q

[(cid:96)(h, z)].

by the linearity of expectation, the generalization loss and training loss of q can
be written as

ld(q) def= e
h   q

[ld(h)]

and ls(q) def= e
h   q

[ls(h)].

the following theorem tells us that the di   erence between the generalization
loss and the empirical loss of a posterior q is bounded by an expression that
depends on the id181 between q and the prior distribu-
tion p . the kullback-leibler is a natural measure of the distance between two
distributions. the theorem suggests that if we would like to minimize the gen-
eralization loss of q, we should jointly minimize both the empirical loss of q
and the kullback-leibler distance between q and the prior distribution. we will

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

416

pac-bayes

later show how in some cases this idea leads to the regularized risk minimization
principle.
theorem 31.1 let d be an arbitrary distribution over an example domain z.
let h be a hypothesis class and let (cid:96) : h   z     [0, 1] be a id168. let p be
a prior distribution over h and let        (0, 1). then, with id203 of at least
1       over the choice of an i.i.d. training set s = {z1, . . . , zm} sampled according
to d, for all distributions q over h (even such that depend on s), we have

(cid:115)

ld(q)     ls(q) +

d(q||p ) + ln m/  

2(m     1)

,

.

(31.1)

(cid:19)

where

d(q||p ) def= e
h   q

[ln(q(h)/p (h))]

is the id181.

proof for any function f (s), using markov   s inequality:

[f (s)      ] = p

[ef (s)     e ]     es[ef (s)]

s

e 

p
s

(cid:18)

let    (h) = ld(h)     ls(h). we will apply equation (31.1) with the function

f (s) = sup
q

2(m     1) e
h   q

(   (h))2     d(q||p )

.

we now turn to bound es[ef (s)]. the main trick is to upper bound f (s) by
using an expression that does not depend on q but rather depends on the prior
id203 p . to do so,    x some s and note that from the de   nition of d(q||p )
we get that for all q,

2(m     1) e
h   q

(   (h))2     d(q||p ) = e
h   q
    ln e
h   q
= ln e
h   p

[ln(e2(m   1)   (h)2
[e2(m   1)   (h)2
[e2(m   1)   (h)2

],

p (h)/q(h))]

p (h)/q(h)]

(31.2)

where the inequality follows from jensen   s inequality and the concavity of the
log function. therefore,

[ef (s)]     e
e
s

s

e
h   p

[e2(m   1)   (h)2

].

(31.3)

the advantage of the expression on the right-hand side stems from the fact that
we can switch the order of expectations (because p is a prior that does not
depend on s), which yields
[ef (s)]     e
e
h   p
s

[e2(m   1)   (h)2
e
s

(31.4)

].

31.2 bibliographic remarks

417

next, we claim that for all h we have es[e2(m   1)   (h)2
hoe   ding   s inequality tells us that

]     m. to do so, recall that

[   (h)      ]     e   2m 2
p
s

.

this implies that es[e2(m   1)   (h)2
equation (31.4) and plugging into equation (31.1) we get

]     m (see exercise 1). combining this with

p
s

[f (s)      ]     m
e  .

(31.5)

denote the right-hand side of the above   , thus   = ln(m/  ), and we therefore
obtain that with id203 of at least 1        we have that for all q

2(m     1) e
h   q

(   (h))2     d(q||p )       = ln(m/  ).

rearranging the inequality and using jensen   s inequality again (the function x2
is convex) we conclude that

(cid:18)

(cid:19)2     e

h   q

e
h   q

   (h)

(   (h))2     ln(m/  ) + d(q||p )

2(m     1)

.

(31.6)

remark 31.1 (id173) the pac-bayes bound leads to the following
learning rule:

given a prior p , return a posterior q that minimizes the function

(cid:115)

ls(q) +

d(q||p ) + ln m/  

2(m     1)

.

(31.7)

this rule is similar to the regularized risk minimization principle. that is, we
jointly minimize the empirical loss of q on the sample and the kullback-leibler
   distance    between q and p .

31.2

bibliographic remarks

pac-bayes bounds were    rst introduced by mcallester (1998). see also (mcallester
1999, mcallester 2003, seeger 2003, langford & shawe-taylor 2003, langford
2006).

31.3

exercises

1. let x be a random variable that satis   es p[x      ]     e   2m 2

e[e2(m   1)x 2

]     m.

. prove that

418

pac-bayes

2.     suppose that h is a    nite hypothesis class, set the prior to be uniform over
h, and set the posterior to be q(hs) = 1 for some hs and q(h) = 0 for
all other h     h. show that

(cid:115)

ld(hs)     ls(h) +

ln(|h|) + ln(m/  )

2(m     1)

.

compare to the bounds we derived using uniform convergence.

    derive a bound similar to the occam bound given in chapter 7 using the

pac-bayes bound

appendix a technical lemmas

lemma a.1 let a > 0. then: x     2a log(a)     x     a log(x). it follows that a
necessary condition for the inequality x < a log(x) to hold is that x < 2a log(a).
e ] the inequality x     a log(x) holds uncon-
proof first note that for a     (0,
   
e.
ditionally and therefore the claim is trivial. from now on, assume that a >
consider the function f (x) = x     a log(x). the derivative is f(cid:48)(x) = 1     a/x.
thus, for x > a the derivative is positive and the function increases. in addition,

   

f (2a log(a)) = 2a log(a)     a log(2a log(a))

= 2a log(a)     a log(a)     a log(2 log(a))
= a log(a)     a log(2 log(a)).
since a     2 log(a) > 0 for all a > 0, the proof follows.
lemma a.2 let a     1 and b > 0. then: x     4a log(2a)+2b     x     a log(x)+b.
it su   ces to prove that x     4a log(2a) + 2b implies that both x    
proof
2a log(x) and x     2b. since we assume a     1 we clearly have that x     2b.
in addition, since b > 0 we have that x     4a log(2a) which using lemma a.1
implies that x     2a log(x). this concludes our proof.
lemma a.3 let x be a random variable and x(cid:48)     r be a scalar and assume
that there exists a > 0 such that for all t     0 we have p[|x     x(cid:48)| > t]     2e   t2/a2
.
then, e[|x     x(cid:48)|]     4 a.

we have that e[|x     x(cid:48)|] is at most(cid:80)   
with the assumption in the lemma we get that e[|x     x(cid:48)|]     2 a(cid:80)   
proof for all i = 0, 1, 2, . . . denote ti = a i. since ti is monotonically increasing
i=1 ti p[|x     x(cid:48)| > ti   1]. combining this
i=1 ie   (i   1)2
.
(cid:90)    

the proof now follows from the inequalities

   (cid:88)

ie   (i   1)2     5(cid:88)

xe   (x   1)2

dx < 1.8 + 10   7 < 2 .

ie   (i   1)2

+

i=1

i=1

5

lemma a.4 let x be a random variable and x(cid:48)     r be a scalar and assume
that there exists a > 0 and b     e such that for all t     0 we have p[|x     x(cid:48)| >
t]     2b e   t2/a2

. then, e[|x     x(cid:48)|]     a(2 +(cid:112)log(b)).

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

420

technical lemmas

proof for all i = 0, 1, 2, . . . denote ti = a (i+(cid:112)log(b)). since ti is monotonically

increasing we have that

e[|x     x(cid:48)|]     a(cid:112)log(b) +

   (cid:88)

i=1

ti p[|x     x(cid:48)| > ti   1].

   (cid:88)
(i +(cid:112)log(b))e   (i   1+

   

log(b))2

using the assumption in the lemma we have

   (cid:88)

i=1

xe   (x   1)2

dx

(y + 1)e   y2

dy

ti p[|x     x(cid:48)| > ti   1]     2 a b

1+

log(b)

   

(cid:90)    
(cid:90)    
(cid:90)    
(cid:104)   e   y2(cid:105)      

   

   

log(b)

log(b)

i=1

    2 a b

= 2 a b

    4 a b

= 2 a b

ye   y2

dy

log(b)

= 2 a b/b = 2 a.

combining the preceding inequalities we conclude our proof.

lemma a.5 let m, d be two positive integers such that d     m     2. then,

(cid:18)m

(cid:19)

d(cid:88)

k

k=0

   (cid:16) e m

(cid:17)d

.

d

proof we prove the claim by induction. for d = 1 the left-hand side equals
1 + m while the right-hand side equals em; hence the claim is true. assume that
the claim holds for d and let us prove it for d + 1. by the induction assumption
we have

(cid:18)m
(cid:19)

d+1(cid:88)

k

k=0

+

d

   (cid:16) e m
(cid:17)d
(cid:17)d(cid:32)
(cid:16) e m
(cid:17)d(cid:32)
   (cid:16) em

=

d

d

d + 1

(cid:18) m
(cid:19)
(cid:19)d m(m     1)(m     2)       (m     d)
(cid:18) d
(cid:19)d (m     d)
(cid:18) d

(d + 1)d!

(cid:33)

e m

1 +

(cid:33)

1 +

e

(d + 1)d!

.

technical lemmas

421

using stirling   s approximation we further have that

where in the last inequality we used the assumption that d     m     2. on the
other hand,

d

d

d

=

=

   (cid:16) e m
(cid:16) e m
(cid:16) e m
   (cid:16) e m
(cid:16) e m
   (cid:16) e m
(cid:18) e m

=

d

d

d

d + 1

(cid:19)

1 +

1 +

d + 1

d + 1

(d + 1)

(cid:19)d

e
   

(cid:18) d

m     d
   
2  d(d + 1)

(cid:17)d(cid:32)
(cid:17)d(cid:18)
(cid:17)d    d + 1 + (m     d)/
(cid:17)d    d + 1 + (m     d)/2
(cid:17)d    d/2 + 1 + m/2
(cid:17)d    m
(cid:19)d+1
(cid:16) e m
(cid:16) e m
   (cid:16) e m
(cid:16) e m

(cid:17)d    em
(cid:17)d    em
(cid:17)d    em
(cid:17)d    m

d + 1

d + 1

d + 1

d + 1

d + 1

=

=

=

d

d

d

,

d

,

d + 1

  

  

(cid:33)

(m     d)
   

2  d(d/e)d

2  d

(cid:18) d

(cid:19)d

d + 1
1

(1 + 1/d)d
   1
e

which proves our inductive argument.
lemma a.6 for all a     r we have

ea + e   a

proof observe that

therefore,

and

2

ea =

ea + e   a

2

ea2/2 =

    ea2/2.

an
n!

.

n=0

   (cid:88)
   (cid:88)
   (cid:88)

n=0

=

n=0

a2n
2n n!

.

a2n
(2n)!

,

observing that (2n)!     2n n! for every n     0 we conclude our proof.

appendix b measure concentration

(cid:80)m

let z1, . . . , zm be an i.i.d. sequence of random variables and let    be their mean.
the strong law of large numbers states that when m tends to in   nity, the em-
pirical average, 1
i=1 zi, converges to the expected value   , with id203
m
1. measure concentration inequalities quantify the deviation of the empirical
average from the expectation when m is    nite.

b.1

markov   s inequality

(cid:90)    

x=0

(cid:90) a

we start with an inequality which is called markov   s inequality. let z be a
nonnegative random variable. the expectation of z can be written as follows:

e[z] =

p[z     x]dx.

(b.1)

(cid:90) a

since p[z     x] is monotonically nonincreasing we obtain

   a     0, e[z]    

p[z     x]dx    

p[z     a]dx = a p[z     a].

(b.2)

rearranging the inequality yields markov   s inequality:

x=0

x=0

   a     0, p[z     a]     e[z]

a

.

(b.3)

for random variables that take value in [0, 1], we can derive from markov   s

inequality the following.

lemma b.1 let z be a random variable that takes values in [0, 1]. assume that
e[z] =   . then, for any a     (0, 1),

p[z > 1     a]            (1     a)

.

a

this also implies that for every a     (0, 1),
p[z > a]            a
1     a

           a.

proof let y = 1     z. then y is a nonnegative random variable with e[y ] =
1     e[z] = 1       . applying markov   s inequality on y we obtain

p[z     1     a] = p[1     z     a] = p[y     a]     e[y ]

a

=

1       

.

a

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

b.2 chebyshev   s inequality

423

therefore,

p[z > 1     a]     1     1       

a

a +        1

a

.

=

b.2

chebyshev   s inequality
applying markov   s inequality on the random variable (z     e[z])2 we obtain
chebyshev   s inequality:

   a > 0,

p[|z     e[z]|     a] = p[(z     e[z])2     a2]     var[z]

,

(b.4)

a2

where var[z] = e[(z     e[z])2] is the variance of z.

consider the random variable 1
m

i=1 zi. since z1, . . . , zm are i.i.d. it is easy

to verify that

var

(cid:80)m
m(cid:88)

zi

i=1

(cid:35)

(cid:34)

1
m

=

var[z1]

m

.

applying chebyshev   s inequality, we obtain the following:

lemma b.2 let z1, . . . , zm be a sequence of i.i.d. random variables and assume
that e[z1] =    and var[z1]     1. then, for any        (0, 1), with id203 of at
least 1        we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)    

(cid:114) 1

.

   m

m(cid:88)

i=1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
m(cid:88)

m

i=1

zi       

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > a
(cid:35)

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

m

p

zi       

    var[z1]

m a2     1
m a2 .

proof applying chebyshev   s inequality we obtain that for all a > 0

the proof follows by denoting the right-hand side    and solving for a.

the deviation between the empirical average and the mean given previously
decreases polynomially with m. it is possible to obtain a signi   cantly faster
decrease. in the sections that follow we derive bounds that decrease exponentially
fast.

b.3

cherno      s bounds

pi and p[zi = 0] = 1     pi. let p = (cid:80)m

let z1, . . . , zm be independent bernoulli variables where for every i, p[zi = 1] =
i=1 zi. using the

i=1 pi and let z = (cid:80)m

424

measure concentration

monotonicity of the exponent function and markov   s inequality, we have that for
every t > 0

p[z > (1 +   )p] = p[etz > et(1+  )p]     e[etz]

e(1+  )tp

.

(b.5)

next,

etzi]

i

i

=

e[etzi]

i zi] = e[

(cid:89)
(cid:0)piet + (1     pi)e0(cid:1)
(cid:0)1 + pi(et     1)(cid:1)

e[etz] = e[et(cid:80)
(cid:89)
(cid:89)
(cid:89)
   (cid:89)
(cid:80)
i pi(et   1)

epi(et   1)

=

=

i

i

i

= e
= e(et   1)p.

by independence

using 1 + x     ex

combining the above with equation (b.5) and choosing t = log(1 +   ) we obtain

i, p[zi = 1] = pi and p[zi = 0] = 1     pi. let p =(cid:80)m

lemma b.3 let z1, . . . , zm be independent bernoulli variables where for every
i=1 zi.

i=1 pi and let z =(cid:80)m

then, for any    > 0,

p[z > (1 +   )p]     e   h(  ) p,

where

h(  ) = (1 +   ) log(1 +   )       .

using the inequality h(a)     a2/(2 + 2a/3) we obtain

lemma b.4 using the notation of lemma b.3 we also have

p[z > (1 +   )p]     e

   p

  2

2+2  /3 .

for the other direction, we apply similar calculations:

p[z < (1     )p] = p[   z >    (1     )p] = p[e   tz > e   t(1     )p]     e[e   tz]
e   (1     )tp

, (b.6)

b.4 hoe   ding   s inequality

425

by independence

epi(e   t   1)

using 1 + x     ex

and,

e   tzi]

e[e   tz] = e[e   t(cid:80)
e[e   tzi]

i zi] = e[

(cid:89)
(cid:0)1 + pi(e   t     1)(cid:1)

i

=

(cid:89)
(cid:89)
   (cid:89)

=

i

i

i

= e(e   t   1)p.
setting t =     log(1       ) yields

p[z < (1       )p]    

it is easy to verify that h(     )     h(  ) and hence

e     p

e(1     ) log(1     ) p

= e   ph(     ).

lemma b.5 using the notation of lemma b.3 we also have

p[z < (1       )p]     e   ph(     )     e   ph(  )     e

   p

  2

2+2  /3 .

b.4

hoe   ding   s inequality

(cid:80)m
lemma b.6 (hoe   ding   s inequality) let z1, . . . , zm be a sequence of i.i.d.
i=1 zi. assume that e[   z] =    and p[a    
random variables and let   z = 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) >  
(cid:35)
zi     b] = 1 for every i. then, for any   > 0
m
    2 exp(cid:0)   2 m  2/(b     a)2(cid:1) .

(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

m(cid:88)

zi       

p

m

i=1

proof denote xi = zi     e[zi] and   x = 1
i xi. using the monotonicity of
the exponent function and markov   s inequality, we have that for every    > 0
and   > 0,

m

(cid:80)

p[   x      ] = p[e     x     e   ]     e       e[e     x ].

using the independence assumption we also have

(cid:34)(cid:89)

(cid:35)

(cid:89)

e[e     x ] = e

e  xi/m

=

e[e  xi/m].

by hoe   ding   s lemma (lemma b.7 later), for every i we have

i

i

e[e  xi/m]     e

  2(b   a)2

8m2

.

426

measure concentration

therefore,

p[   x      ]     e       (cid:89)

i

setting    = 4m /(b     a)2 we obtain

  2 (b   a)2
8m2 = e      +   2(b   a)2
8m .

e

p[   x      ]     e

    2m 2

(b   a)2 .

applying the same arguments on the variable       x we obtain that p[   x         ]    
    2m 2
e

(b   a)2 . the theorem follows by applying the union bound on the two cases.

lemma b.7 (hoe   ding   s lemma) let x be a random variable that takes values
in the interval [a, b] and such that e[x] = 0. then, for every    > 0,

e[e  x ]     e

  2(b   a)2

8

.

proof since f (x) = e  x is a convex function, we have that for every        (0, 1),
and x     [a, b],

f (x)       f (a) + (1       )f (b).

setting    = b   x

b   a     [0, 1] yields

e  x     b     x
b     a
taking the expectation, we obtain that

e  a +

x     a
b     a

e  b.

e[e  x ]     b     e[x]
b     a

e  a +

e[x]     a
b     a

e  b =

b

b     a

e  a     a
b     a

e  b,

where we used the fact that e[x] = 0. denote h =   (b     a), p =    a
b   a , and
l(h) =    hp + log(1     p + peh). then, the expression on the right-hand side of
the above can be rewritten as el(h). therefore, to conclude our proof it su   ces
to show that l(h)     h2
8 . this follows from taylor   s theorem using the facts:
l(0) = l(cid:48)(0) = 0 and l(cid:48)(cid:48)(h)     1/4 for all h.

b.5

bennet   s and bernstein   s inequalities

bennet   s and bernsein   s inequalities are similar to cherno      s bounds, but they
hold for any sequence of independent random variables. we state the inequalities
without proof, which can be found, for example, in cesa-bianchi & lugosi (2006).

lemma b.8 (bennet   s inequality) let z1, . . . , zm be independent random vari-
ables with zero mean, and assume that zi     1 with id203 1. let

m(cid:88)

i=1

  2     1
m

e[z 2
i ].

b.5 bennet   s and bernstein   s inequalities

427

then for all   > 0,

where

(cid:35)

zi >  

(cid:34) m(cid:88)

i=1

p

    e

   m  2h(  

m  2 ).

h(a) = (1 + a) log(1 + a)     a.

by using the inequality h(a)     a2/(2 + 2a/3) it is possible to derive the

following:

lemma b.9 (bernstein   s inequality) let z1, . . . , zm be i.i.d. random variables
with a zero mean. if for all i, p(|zi| < m ) = 1, then for all t > 0 :

(cid:34) m(cid:88)

p

(cid:35)

(cid:32)

zi > t

    exp

   

(cid:33)

.

(cid:80) e z 2

t2/2
j + m t/3

i=1

b.5.1

application

(cid:34)
(cid:34)

(cid:35)
(cid:35)

(cid:114)
(cid:114)

bernstein   s inequality can be used to interpolate between the rate 1/  we derived
for pac learning in the realizable case (in chapter 2) and the rate 1/ 2 we derived
for the unrealizable case (in chapter 4).
lemma b.10 let (cid:96) : h    z     [0, 1] be a id168. let d be an arbitrary
distribution over z. fix some h. then, for any        (0, 1) we have

1.

2.

p

s   dm

p

s   dm

ls(h)     ld(h) +

ld(h)     ls(h) +

2ld(h) log(1/  )

3 m

2ls(h) log(1/  )

m

+

+

2 log(1/  )

m

4 log(1/  )

m

      

      

proof de   ne random variables   1, . . . ,   m s.t.   i = (cid:96)(h, zi)    ld(h). note that
e[  i] = 0 and that
e[  2

i ] = e[(cid:96)(h, zi)2]     2ld(h) e[(cid:96)(h, zi)] + ld(h)2

= e[(cid:96)(h, zi)2]     ld(h)2
    e[(cid:96)(h, zi)2]
    e[(cid:96)(h, zi)] = ld(h),

where in the last inequality we used the fact that (cid:96)(h, zi)     [0, 1] and thus
(cid:96)(h, zi)2     (cid:96)(h, zi). applying bernsein   s inequality over the   i   s yields

(cid:35)

  i > t

(cid:34) m(cid:88)

i=1

p

(cid:32)
(cid:18)

   

   

(cid:33)
(cid:19)

(cid:80) e   2

t2/2

j + t/3
t2/2

m ld(h) + t/3

    exp

    exp

def=   .

428

measure concentration

solving for t yields

t2/2

m ld(h) + t/3

    t2/2     log(1/  )

3

t     log(1/  ) m ld(h) = 0

= log(1/  )

(cid:115)

    2

(cid:80)
i   i = ls(h)    ld(h), it follows that with id203 of at least 1      ,

3

since 1
m

    t =

log(1/  )

3
log(1/  )

log2(1/  )

+

32

+(cid:112)2 log(1/  ) m ld(h)
(cid:114)

log(1/  )

+ 2 log(1/  ) m ld(h)

ls(h)     ld(h)     2

+

3m

2 log(1/  ) ld(h)

,

m

which proves the    rst inequality. the second part of the lemma follows in a
similar way.

b.6

slud   s inequality

let x be a (m, p) binomial variable. that is, x =(cid:80)m
bility that a normal variable will be greater than or equal to(cid:112)m 2/(1      2). the

i=1 zi, where each zi is 1
with id203 p and 0 with id203 1   p. assume that p = (1    )/2. slud   s
inequality (slud 1977) tells us that p[x     m/2] is lower bounded by the proba-

following lemma follows by standard tail bounds for the normal distribution.
lemma b.11 let x be a (m, p) binomial variable and assume that p = (1    )/2.
then,

(cid:16)

1    (cid:112)1     exp(   m 2/(1      2))

(cid:17)

.

p[x     m/2]     1
2

b.7

concentration of   2 variables

let x1, . . . , xk be k independent normally distributed random variables. that
is, for all i, xi     n (0, 1). the distribution of the random variable x 2
is called
1 +        + x 2
  2 (chi square) and the distribution of the random variable z = x 2
k
k (chi square with k degrees of freedom). clearly, e[x 2
is called   2
i ] = 1 and
e[z] = k. the following lemma states that x 2
k is concentrated around its mean.
lemma b.12 let z       2

i

k. then, for all   > 0 we have
p[z     (1      )k]     e    2k/6,

and for all       (0, 3) we have

p[z     (1 +  )k]     e    2k/6.

b.7 concentration of   2 variables

429

finally, for all       (0, 3),

proof let us write z =(cid:80)k

p [(1      )k     z     (1 +  )k]     1     2e    2k/6.

i where xi     n (0, 1). to prove both bounds
we use cherno      s bounding method. for the    rst inequality, we    rst bound
2 for all a     0
e[e     x 2
we have that

1 ], where    > 0 will be speci   ed later. since e   a     1    a + a2

i=1 x 2

e[e     x 2

1 ]     1        e[x 2

1 ] +

  2
2

e[x 4
1 ].

using the well known equalities, e[x 2
1     a     e   a we obtain that
e[e     x 2

1 ]     1        + 3

p[   z        (1      )k] = p(cid:104)

now, applying cherno      s bounding method we get that

1 ] = 1 and e[x 4

1 ] = 3, and the fact that

2   2     e     + 3
2   2

.

e     z     e   (1    )k  (cid:105)
    e(1    )k   e(cid:2)e     z(cid:3)
= e(1    )k   (cid:16)e(cid:104)
(cid:105)(cid:17)k

1

e     x 2
2   2k

    e(1    )k   e     k+ 3
= e

    k  +

3
2 k  2

.

choose    =  /3 we obtain the    rst inequality stated in the lemma.

for the second inequality, we use a known closed form expression for the

moment generating function of a   2

k distributed random variable:

(b.7)

= (1     2  )   k/2.

      < 1

2 , e(cid:104)

e  z2(cid:105)
e  z     e(1+ )k  (cid:105)
p[z     (1 +  )k)] = p(cid:104)
    e   (1+ )k   e(cid:2)e  z(cid:3)

on the basis of the equation and using cherno      s bounding method we have

   k/2
= e   (1+ )k   (1     2  )
    e   (1+ )k   ek   = e    k  ,

where the last inequality occurs because (1     a)     e   a. setting    =  /6 (which
is in (0, 1/2) by our assumption) we obtain the second inequality stated in the
lemma.

finally, the last inequality follows from the    rst two inequalities and the union

bound.

appendix c id202

c.1

basic de   nitions

in this chapter we only deal with id202 over    nite dimensional euclidean
spaces. we refer to vectors as column vectors.

given two d dimensional vectors u, v     rd, their inner product is

d(cid:88)

uivi.

(cid:104)u, v(cid:105) =

i=1

the euclidean norm (a.k.a. the (cid:96)2 norm) is (cid:107)u(cid:107) =(cid:112)(cid:104)u, u(cid:105). we also use the (cid:96)1
norm, (cid:107)u(cid:107)1 =(cid:80)d

i=1 |ui| and the (cid:96)    norm (cid:107)u(cid:107)    = maxi |ui|.

a subspace of rd is a subset of rd which is closed under addition and scalar
multiplication. the span of a set of vectors u1, . . . , uk is the subspace containing
all vectors of the form

k(cid:88)

  iui

i=1

where for all i,   i     r.

a set of vectors u = {u1, . . . , uk} is independent if for every i, ui is not in the
span of u1, . . . , ui   1, ui+1, . . . , uk. we say that u spans a subspace v if v is the
span of the vectors in u . we say that u is a basis of v if it is both independent
and spans v. the dimension of v is the size of a basis of v (and it can be veri   ed
that all bases of v have the same size). we say that u is an orthogonal set if for
all i (cid:54)= j, (cid:104)ui, uj(cid:105) = 0. we say that u is an orthonormal set if it is orthogonal
and if for every i, (cid:107)ui(cid:107) = 1.
given a matrix a     rn,d, the range of a is the span of its columns and the
null space of a is the subspace of all vectors that satisfy au = 0. the rank of a
is the dimension of its range.

the transpose of a matrix a, denoted a(cid:62), is the matrix whose (i, j) entry

equals the (j, i) entry of a. we say that a is symmetric if a = a(cid:62).

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

c.2 eigenvalues and eigenvectors

431

c.2

eigenvalues and eigenvectors
let a     rd,d be a matrix. a non-zero vector u is an eigenvector of a with a
corresponding eigenvalue    if

au =   u.

ui is an eigenvector of a. furthermore, a can be written as a =(cid:80)d

if a     rd,d is a symmetric matrix of
theorem c.1 (spectral decomposition)
rank k, then there exists an orthonormal basis of rd, u1, . . . , ud, such that each
i=1   iuiu(cid:62)
i ,
where each   i is the eigenvalue corresponding to the eigenvector ui. this can
be written equivalently as a = u du(cid:62), where the columns of u are the vectors
u1, . . . , ud, and d is a diagonal matrix with di,i =   i and for i (cid:54)= j, di,j =
0. finally, the number of   i which are nonzero is the rank of the matrix, the
eigenvectors which correspond to the nonzero eigenvalues span the range of a,
and the eigenvectors which correspond to zero eigenvalues span the null space of
a.

c.3

c.4

positive de   nite matrices
a symmetric matrix a     rd,d is positive de   nite if all its eigenvalues are positive.
a is positive semide   nite if all its eigenvalues are nonnegative.
theorem c.2 let a     rd,d be a symmetric matrix. then, the following are
equivalent de   nitions of positive semide   niteness of a:
    all the eigenvalues of a are nonnegative.
    for every vector u, (cid:104)u, au(cid:105)     0.
    there exists a matrix b such that a = bb(cid:62).

singular value decomposition (svd)
let a     rm,n be a matrix of rank r. when m (cid:54)= n, the eigenvalue decomposition
given in theorem c.1 cannot be applied. we will describe another decomposition
of a, which is called singular value decomposition, or svd for short.

unit vectors v     rn and u     rm are called right and left singular vectors of

a with corresponding singular value    > 0 if

av =   u and a(cid:62)u =   v.

we    rst show that if we can    nd r orthonormal singular vectors with positive
singular values, then we can decompose a = u dv (cid:62), with the columns of u and
v containing the left and right singular vectors, and d being a diagonal r    r
matrix with the singular values on its diagonal.

432

id202

lemma c.3 let a     rm,n be a matrix of rank r. assume that v1, . . . , vr is an
orthonormal set of right singular vectors of a, u1, . . . , ur is an orthonormal set
of corresponding left singular vectors of a, and   1, . . . ,   r are the corresponding
singular values. then,

r(cid:88)

a =

  iuiv(cid:62)
i .

it follows that if u is a matrix whose columns are the ui   s, v is a matrix whose
columns are the vi   s, and d is a diagonal matrix with di,i =   i, then

i=1

a = u dv (cid:62).

adding the vectors vr+1, . . . , vn. de   ne b =(cid:80)r

proof any right singular vector of a must be in the range of a(cid:62) (otherwise,
the singular value will have to be zero). therefore, v1, . . . , vr is an orthonormal
basis of the range of a. let us complete it to an orthonormal basis of rn by
i . it su   ces to prove
that for all i, avi = bvi. clearly, if i > r then avi = 0 and bvi = 0 as well.
for i     r we have

i=1   iuiv(cid:62)

r(cid:88)

bvi =

  jujv(cid:62)

j vi =   iui = avi,

j=1

where the last equality follows from the de   nition.

the next lemma relates the singular values of a to the eigenvalues of a(cid:62)a

and aa(cid:62).
lemma c.4 v, u are right and left singular vectors of a with singular value   
i    v is an eigenvector of a(cid:62)a with corresponding eigenvalue   2 and u =      1av
is an eigenvector of aa(cid:62) with corresponding eigenvalue   2.
proof suppose that    is a singular value of a with v     rn being the corre-
sponding right singular vector. then,

similarly,

a(cid:62)av =   a(cid:62)u =   2v.

aa(cid:62)u =   av =   2u.

for the other direction, if    (cid:54)= 0 is an eigenvalue of a(cid:62)a, with v being the
corresponding eigenvector, then    > 0 because a(cid:62)a is positive semide   nite. let
   =

  , u =      1av. then,

   

  u =

and

   

  

av   
  

= av,

a(cid:62)u =

a(cid:62)av =

1
  

  
  

v =   v.

c.4 singular value decomposition (svd)

433

finally, we show that if a has rank r then it has r orthonormal singular

vectors.
lemma c.5 let a     rm,n with rank r. de   ne the following vectors:

(cid:107)av(cid:107)
(cid:107)av(cid:107)

v1 = argmax

v   rn:(cid:107)v(cid:107)=1

v2 = argmax
(cid:104)v,v1(cid:105)=0

v   rn:(cid:107)v(cid:107)=1

...
vr =

(cid:107)av(cid:107)

argmax

v   rn:(cid:107)v(cid:107)=1
   i<r, (cid:104)v,vi(cid:105)=0

then, v1, . . . , vr is an orthonormal set of right singular vectors of a.

proof first note that since the rank of a is r, the range of a is a subspace of
dimension r, and therefore it is easy to verify that for all i = 1, . . . , r, (cid:107)avi(cid:107) > 0.
let w     rn,n be an orthonormal matrix obtained by the eigenvalue decompo-
sition of a(cid:62)a, namely, a(cid:62)a = w dw (cid:62), with d being a diagonal matrix with
d1,1     d2,2                0. we will show that v1, . . . , vr are eigenvectors of a(cid:62)a
that correspond to nonzero eigenvalues, and, hence, using lemma c.4 it follows
that these are also right singular vectors of a. the proof is by induction. for the
basis of the induction, note that any unit vector v can be written as v = w x,
for x = w (cid:62)v, and note that (cid:107)x(cid:107) = 1. therefore,

(cid:107)av(cid:107)2 = (cid:107)aw x(cid:107)2 = (cid:107)w dw (cid:62)w x(cid:107)2 = (cid:107)w dx(cid:107)2 = (cid:107)dx(cid:107)2 =

d2

i,ixi

2.

therefore,

max
v:(cid:107)v(cid:107)=1

(cid:107)av(cid:107)2 = max
x:(cid:107)x(cid:107)=1

i=1

n(cid:88)

i=1

d2

i,ixi

2.

the solution of the right-hand side is to set x = (1, 0, . . . , 0), which implies that
v1 is the    rst eigenvector of a(cid:62)a. since (cid:107)av1(cid:107) > 0 it follows that d1,1 > 0 as
required. for the induction step, assume that the claim holds for some 1     t    
r     1. then, any v which is orthogonal to v1, . . . , vt can be written as v = w x
with all the    rst t elements of x being zero. it follows that

n(cid:88)

max

v:(cid:107)v(cid:107)=1,   i   t,v(cid:62)vi=0

(cid:107)av(cid:107)2 = max
x:(cid:107)x(cid:107)=1

d2

i,ixi

2.

n(cid:88)

i=t+1

the solution of the right-hand side is the all zeros vector except xt+1 = 1. this
implies that vt+1 is the (t + 1)th column of w . finally, since (cid:107)avt+1(cid:107) > 0 it
follows that dt+1,t+1 > 0 as required. this concludes our proof.

434

id202

corollary c.6 (the svd theorem) let a     rm,n with rank r. then a =
u dv (cid:62) where d is an r    r matrix with nonzero singular values of a and the
columns of u, v are orthonormal left and right singular vectors of a. further-
i,i is an eigenvalue of a(cid:62)a, the ith column of v is the cor-
more, for all i, d2
responding eigenvector of a(cid:62)a and the ith column of u is the corresponding
eigenvector of aa(cid:62).

notes

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

references

abernethy, j., bartlett, p. l., rakhlin, a. & tewari, a. (2008), optimal strategies and
minimax lower bounds for online convex games, in    proceedings of the nineteenth
annual conference on computational learning theory   .

ackerman, m. & ben-david, s. (2008), measures of id91 quality: a working set
of axioms for id91, in    proceedings of neural information processing systems
(nips)   , pp. 121   128.

agarwal, s. & roth, d. (2005), learnability of bipartite ranking functions, in    pro-

ceedings of the 18th annual conference on learning theory   , pp. 16   31.

agmon, s. (1954),    the relaxation method for linear inequalities   , canadian journal

of mathematics 6(3), 382   392.

aizerman, m. a., braverman, e. m. & rozonoer, l. i. (1964),    theoretical foundations
of the potential function method in pattern recognition learning   , automation and
remote control 25, 821   837.

allwein, e. l., schapire, r. & singer, y. (2000),    reducing multiclass to binary: a uni-
fying approach for margin classi   ers   , journal of machine learning research 1, 113   
141.

alon, n., ben-david, s., cesa-bianchi, n. & haussler, d. (1997),    scale-sensitive dimen-

sions, uniform convergence, and learnability   , journal of the acm 44(4), 615   631.

anthony, m. & bartlet, p. (1999), neural network learning: theoretical foundations,

cambridge university press.

baraniuk, r., davenport, m., devore, r. & wakin, m. (2008),    a simple proof of
the restricted isometry property for random matrices   , constructive approximation
28(3), 253   263.

barber, d. (2012), bayesian reasoning and machine learning, cambridge university

press.

bartlett, p., bousquet, o. & mendelson, s. (2005),    local rademacher complexities   ,

annals of statistics 33(4), 1497   1537.

bartlett, p. l. & ben-david, s. (2002),    hardness results for neural network approxi-

mation problems   , theor. comput. sci. 284(1), 53   66.

bartlett, p. l., long, p. m. & williamson, r. c. (1994), fat-shattering and the learn-
ability of real-valued functions, in    proceedings of the seventh annual conference on
computational learning theory   , acm, pp. 299   310.

bartlett, p. l. & mendelson, s. (2001), rademacher and gaussian complexities: risk
bounds and structural results, in    14th annual conference on computational learn-
ing theory, colt 2001   , vol. 2111, springer, berlin, pp. 224   240.

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

438

references

bartlett, p. l. & mendelson, s. (2002),    rademacher and gaussian complexities: risk

bounds and structural results   , journal of machine learning research 3, 463   482.

ben-david, s., cesa-bianchi, n., haussler, d. & long, p. (1995),    characterizations
of learnability for classes of {0, . . . , n}-valued functions   , journal of computer and
system sciences 50, 74   86.

ben-david, s., eiron, n. & long, p. (2003),    on the di   culty of approximately maxi-

mizing agreements   , journal of computer and system sciences 66(3), 496   514.

ben-david, s. & litman, a. (1998),    combinatorial variability of vapnik-chervonenkis
classes with applications to sample compression schemes   , discrete applied mathe-
matics 86(1), 3   25.

ben-david, s., pal, d., & shalev-shwartz, s. (2009), agnostic online learning, in    con-

ference on learning theory (colt)   .

ben-david, s. & simon, h. (2001),    e   cient learning of linear id88s   , advances

in neural information processing systems pp. 189   195.

bengio, y. (2009),    learning deep architectures for ai   , foundations and trends in

machine learning 2(1), 1   127.

bengio, y. & lecun, y. (2007),    scaling learning algorithms towards ai   , large-scale

kernel machines 34.

bertsekas, d. (1999), nonid135, athena scienti   c.
beygelzimer, a., langford, j. & ravikumar, p. (2007),    multiclass classi   cation with

   lter trees   , preprint, june .

birkho   , g. (1946),    three observations on id202   , revi. univ. nac. tucuman,

ser a 5, 147   151.

bishop, c. m. (2006), pattern recognition and machine learning, vol. 1, springer new

york.

blum, l., shub, m. & smale, s. (1989),    on a theory of computation and complexity
over the real numbers: np-completeness, recursive functions and universal machines   ,
am. math. soc 21(1), 1   46.

blumer, a., ehrenfeucht, a., haussler, d. & warmuth, m. k. (1987),    occam   s razor   ,

information processing letters 24(6), 377   380.

blumer, a., ehrenfeucht, a., haussler, d. & warmuth, m. k. (1989),    learnability
and the vapnik-chervonenkis dimension   , journal of the association for computing
machinery 36(4), 929   965.

borwein, j. & lewis, a. (2006), convex analysis and nonlinear optimization, springer.
boser, b. e., guyon, i. m. & vapnik, v. n. (1992), a training algorithm for optimal

margin classi   ers, in    conference on learning theory (colt)   , pp. 144   152.

bottou, l. & bousquet, o. (2008), the tradeo   s of large scale learning, in    nips   ,

pp. 161   168.

boucheron, s., bousquet, o. & lugosi, g. (2005),    theory of classi   cation: a survey of

recent advances   , esaim: id203 and statistics 9, 323   375.

bousquet, o. (2002), concentration inequalities and empirical processes theory ap-

plied to the analysis of learning algorithms, phd thesis, ecole polytechnique.

bousquet, o. & elissee   , a. (2002),    stability and generalization   , journal of machine

learning research 2, 499   526.

boyd, s. & vandenberghe, l. (2004), id76, cambridge university

press.

references

439

breiman, l. (1996), bias, variance, and arcing classi   ers, technical report 460, statis-

tics department, university of california at berkeley.

breiman, l. (2001),    id79s   , machine learning 45(1), 5   32.
breiman, l., friedman, j. h., olshen, r. a. & stone, c. j. (1984), classi   cation and

regression trees, wadsworth & brooks.

cand`es, e. (2008),    the restricted isometry property and its implications for com-

pressed sensing   , comptes rendus mathematique 346(9), 589   592.

candes, e. j. (2006), compressive sampling, in    proc. of the int. congress of math.,

madrid, spain   .

candes, e. & tao, t. (2005),    decoding by id135   , ieee trans. on

id205 51, 4203   4215.

cesa-bianchi, n. & lugosi, g. (2006), prediction, learning, and games, cambridge

university press.

chang, h. s., weiss, y. & freeman, w. t. (2009),    informative sensing   , arxiv preprint

arxiv:0901.4275 .

chapelle, o., le, q. & smola, a. (2007), large margin optimization of ranking mea-

sures, in    nips workshop: machine learning for web search   .

collins, m. (2000), discriminative reranking for natural language parsing, in    machine

learning   .

collins, m. (2002), discriminative training methods for id48: theory
and experiments with id88 algorithms, in    conference on empirical methods
in natural language processing   .

collobert, r. & weston, j. (2008), a uni   ed architecture for natural language process-
ing: deep neural networks with multitask learning, in    international conference on
machine learning (icml)   .

cortes, c. & vapnik, v. (1995),

   support-vector networks   , machine learning

20(3), 273   297.

cover, t. (1965),    behavior of sequential predictors of binary sequences   , trans. 4th
prague conf. id205 statistical decision functions, random processes
pp. 263   272.

cover, t. & hart, p. (1967),    nearest neighbor pattern classi   cation   , information

theory, ieee transactions on 13(1), 21   27.

crammer, k. & singer, y. (2001),    on the algorithmic implementation of multiclass

kernel-based vector machines   , journal of machine learning research 2, 265   292.

cristianini, n. & shawe-taylor, j. (2000), an introduction to support vector machines,

cambridge university press.

daniely, a., sabato, s., ben-david, s. & shalev-shwartz, s. (2011), multiclass learn-

ability and the erm principle, in    conference on learning theory (colt)   .

daniely, a., sabato, s. & shwartz, s. s. (2012), multiclass learning approaches: a

theoretical comparison with implications, in    nips   .

davis, g., mallat, s. & avellaneda, m. (1997),    greedy adaptive approximation   , jour-

nal of constructive approximation 13, 57   98.

devroye, l. & gy  or   , l. (1985), nonparametric density estimation: the l b1 s view,

wiley.

devroye, l., gy  or   , l. & lugosi, g. (1996), a probabilistic theory of pattern recog-

nition, springer.

440

references

dietterich, t. g. & bakiri, g. (1995),    solving multiclass learning problems via error-

correcting output codes   , journal of arti   cial intelligence research 2, 263   286.

donoho, d. l. (2006),    compressed sensing   , id205, ieee transactions

on 52(4), 1289   1306.

dudley, r., gine, e. & zinn, j. (1991),    uniform and universal glivenko-cantelli classes   ,

journal of theoretical id203 4(3), 485   510.

dudley, r. m. (1987),    universal donsker classes and metric id178   , annals of prob-

ability 15(4), 1306   1326.

fisher, r. a. (1922),    on the mathematical foundations of theoretical statistics   , philo-
sophical transactions of the royal society of london. series a, containing papers
of a mathematical or physical character 222, 309   368.

floyd, s. (1989), space-bounded learning and the vapnik-chervonenkis dimension, in

   conference on learning theory (colt)   , pp. 349   364.

floyd, s. & warmuth, m. (1995),    sample compression, learnability, and the vapnik-

chervonenkis dimension   , machine learning 21(3), 269   304.

frank, m. & wolfe, p. (1956),    an algorithm for quadratic programming   , naval res.

logist. quart. 3, 95   110.

freund, y. & schapire, r. (1995), a decision-theoretic generalization of on-line learning
and an application to boosting, in    european conference on computational learning
theory (eurocolt)   , springer-verlag, pp. 23   37.

freund, y. & schapire, r. e. (1999),    large margin classi   cation using the id88

algorithm   , machine learning 37(3), 277   296.

garcia, j. & koelling, r. (1996),    relation of cue to consequence in avoidance learning   ,

foundations of animal behavior: classic papers with commentaries 4, 374.

gentile, c. (2003),    the robustness of the p-norm algorithms   , machine learning

53(3), 265   299.

georghiades, a., belhumeur, p. & kriegman, d. (2001),    from few to many: illumina-
tion cone models for face recognition under variable lighting and pose   , ieee trans.
pattern anal. mach. intelligence 23(6), 643   660.

gordon, g. (1999), regret bounds for prediction problems, in    conference on learning

theory (colt)   .

gottlieb, l.-a., kontorovich, l. & krauthgamer, r. (2010), e   cient classi   cation for

metric data, in    23rd conference on learning theory   , pp. 433   440.

guyon, i. & elissee   , a. (2003),    an introduction to variable and feature selection   ,
journal of machine learning research, special issue on variable and feature selec-
tion 3, 1157   1182.

hadamard, j. (1902),    sur les probl`emes aux d  eriv  ees partielles et leur signi   cation

physique   , princeton university bulletin 13, 49   52.

hastie, t., tibshirani, r. & friedman, j. (2001), the elements of statistical learning,

springer.

haussler, d. (1992),    decision theoretic generalizations of the pac model for neural
net and other learning applications   , information and computation 100(1), 78   150.
haussler, d. & long, p. m. (1995),    a generalization of sauer   s lemma   , journal of

combinatorial theory, series a 71(2), 219   240.

hazan, e., agarwal, a. & kale, s. (2007),    logarithmic regret algorithms for online

id76   , machine learning 69(2   3), 169   192.

references

441

hinton, g. e., osindero, s. & teh, y.-w. (2006),    a fast learning algorithm for deep

belief nets   , neural computation 18(7), 1527   1554.

hiriart-urruty, j.-b. & lemar  echal, c. (1996), convex analysis and minimization al-

gorithms: part 1: fundamentals, vol. 1, springer.

hsu, c.-w., chang, c.-c. & lin, c.-j. (2003),    a practical guide to support vector

classi   cation   .

hya   l, l. & rivest, r. l. (1976),    constructing optimal binary id90 is np-

complete   , information processing letters 5(1), 15   17.

joachims, t. (2005), a support vector method for multivariate performance measures,

in    proceedings of the international conference on machine learning (icml)   .

kakade, s., sridharan, k. & tewari, a. (2008), on the complexity of linear prediction:

risk bounds, margin bounds, and id173, in    nips   .

karp, r. m. (1972), reducibility among combinatorial problems, springer.
kearns, m. j., schapire, r. e. & sellie, l. m. (1994),    toward e   cient agnostic learn-

ing   , machine learning 17, 115   141.

kearns, m. & mansour, y. (1996), on the boosting ability of top-down decision tree

learning algorithms, in    acm symposium on the theory of computing (stoc)   .

kearns, m. & ron, d. (1999),    algorithmic stability and sanity-check bounds for leave-

one-out cross-validation   , neural computation 11(6), 1427   1453.

kearns, m. & valiant, l. g. (1988), learning boolean formulae or    nite automata is
as hard as factoring, technical report tr-14-88, harvard university aiken compu-
tation laboratory.

kearns, m. & vazirani, u. (1994), an introduction to computational learning theory,

mit press.

kleinberg, j. (2003),    an impossibility theorem for id91   , advances in neural

information processing systems pp. 463   470.

klivans, a. r. & sherstov, a. a. (2006), cryptographic hardness for learning intersec-

tions of halfspaces, in    focs   .

koller, d. & friedman, n. (2009), probabilistic id114: principles and tech-

niques, mit press.

koltchinskii, v. & panchenko, d. (2000), rademacher processes and bounding the risk

of function learning, in    high dimensional id203 ii   , springer, pp. 443   457.

kuhn, h. w. (1955),    the hungarian method for the assignment problem   , naval re-

search logistics quarterly 2(1-2), 83   97.

kutin, s. & niyogi, p. (2002), almost-everywhere algorithmic stability and general-
ization error, in    proceedings of the 18th conference in uncertainty in arti   cial
intelligence   , pp. 275   282.

la   erty, j., mccallum, a. & pereira, f. (2001), conditional random    elds: probabilistic
models for segmenting and labeling sequence data, in    international conference on
machine learning   , pp. 282   289.

langford, j. (2006),    tutorial on practical prediction theory for classi   cation   , journal

of machine learning research 6(1), 273.

langford, j. & shawe-taylor, j. (2003), pac-bayes & margins, in    nips   , pp. 423   430.
le cun, l. (2004), large scale online learning., in    advances in neural information
processing systems 16: proceedings of the 2003 conference   , vol. 16, mit press,
p. 217.

442

references

le, q. v., ranzato, m.-a., monga, r., devin, m., corrado, g., chen, k., dean, j. &
ng, a. y. (2012), building high-level features using large scale unsupervised learning,
in    international conference on machine learning (icml)   .

lecun, y. & bengio, y. (1995), convolutional networks for images, speech and time

series, the mit press, pp. 255   258.

lee, h., grosse, r., ranganath, r. & ng, a. (2009), convolutional id50
for scalable unsupervised learning of hierarchical representations, in    international
conference on machine learning (icml)   .

littlestone, n. (1988),    learning quickly when irrelevant attributes abound: a new

linear-threshold algorithm   , machine learning 2, 285   318.

littlestone, n. & warmuth, m. (1986), relating data compression and learnability.

unpublished manuscript.

littlestone, n. & warmuth, m. k. (1994),    the weighted majority algorithm   , infor-

mation and computation 108, 212   261.

livni, r., shalev-shwartz, s. & shamir, o. (2013),    a provably e   cient algorithm for

training deep networks   , arxiv preprint arxiv:1304.7045 .

livni, r. & simon, p. (2013), honest compressions and their application to compression

schemes, in    conference on learning theory (colt)   .

mackay, d. j. (2003),

id205,

id136 and learning algorithms,

cambridge university press.

mallat, s. & zhang, z. (1993),    matching pursuits with time-frequency dictionaries   ,

ieee transactions on signal processing 41, 3397   3415.

mcallester, d. a. (1998), some pac-bayesian theorems, in    conference on learning

theory (colt)   .

mcallester, d. a. (1999), pac-bayesian model averaging, in    conference on learning

theory (colt)   , pp. 164   170.

mcallester, d. a. (2003), simpli   ed pac-bayesian margin bounds., in    conference on

learning theory (colt)   , pp. 203   215.

minsky, m. & papert, s. (1969), id88s: an introduction to computational ge-

ometry, the mit press.

mukherjee, s., niyogi, p., poggio, t. & rifkin, r. (2006),    learning theory: stability is
su   cient for generalization and necessary and su   cient for consistency of empirical
risk minimization   , advances in computational mathematics 25(1-3), 161   193.

murata, n. (1998),    a statistical study of on-line learning   , online learning and neural

networks. cambridge university press, cambridge, uk .

murphy, k. p. (2012), machine learning: a probabilistic perspective, the mit press.
natarajan, b. (1995),    sparse approximate solutions to linear systems   , siam j. com-

puting 25(2), 227   234.

natarajan, b. k. (1989),    on learning sets and functions   , mach. learn. 4, 67   97.
nemirovski, a., juditsky, a., lan, g. & shapiro, a. (2009),    robust stochastic ap-
proximation approach to stochastic programming   , siam journal on optimization
19(4), 1574   1609.

nemirovski, a. & yudin, d. (1978), problem complexity and method e   ciency in opti-

mization, nauka publishers, moscow.

nesterov, y. (2005), primal-dual subgradient methods for convex problems, technical
report, center for operations research and econometrics (core), catholic univer-
sity of louvain (ucl).

references

443

nesterov, y. & nesterov, i. (2004), introductory lectures on id76: a

basic course, vol. 87, springer netherlands.

noviko   , a. b. j. (1962), on convergence proofs on id88s, in    proceedings of the

symposium on the mathematical theory of automata   , vol. xii, pp. 615   622.

parberry, i. (1994), circuit complexity and neural networks, the mit press.
pearson, k. (1901),    on lines and planes of closest    t to systems of points in space   ,
the london, edinburgh, and dublin philosophical magazine and journal of science
2(11), 559   572.

phillips, d. l. (1962),    a technique for the numerical solution of certain integral equa-

tions of the    rst kind   , journal of the acm 9(1), 84   97.

pisier, g. (1980-1981),    remarques sur un r  esultat non publi  e de b. maurey   .
pitt, l. & valiant, l. (1988),    computational limitations on learning from examples   ,

journal of the association for computing machinery 35(4), 965   984.

poon, h. & domingos, p. (2011), sum-product networks: a new deep architecture, in

   conference on uncertainty in arti   cial intelligence (uai)   .

quinlan, j. r. (1986),    induction of id90   , machine learning 1, 81   106.
quinlan, j. r. (1993), c4.5: programs for machine learning, morgan kaufmann.
rabiner, l. & juang, b. (1986),    an introduction to id48   , ieee

assp magazine 3(1), 4   16.

rakhlin, a., shamir, o. & sridharan, k. (2012), making id119 optimal for
strongly convex stochastic optimization, in    international conference on machine
learning (icml)   .

rakhlin, a., sridharan, k. & tewari, a. (2010), online learning: random averages,

combinatorial parameters, and learnability, in    nips   .

rakhlin, s., mukherjee, s. & poggio, t. (2005),    stability results in learning theory   ,

analysis and applications 3(4), 397   419.

ranzato, m., huang, f., boureau, y. & lecun, y. (2007), unsupervised learning of
invariant feature hierarchies with applications to object recognition, in    computer
vision and pattern recognition, 2007. cvpr   07. ieee conference on   , ieee, pp. 1   
8.

rissanen, j. (1978),    modeling by shortest data description   , automatica 14, 465   471.
rissanen, j. (1983),    a universal prior for integers and estimation by minimum descrip-

tion length   , the annals of statistics 11(2), 416   431.

robbins, h. & monro, s. (1951),    a stochastic approximation method   , the annals of

mathematical statistics pp. 400   407.

rogers, w. & wagner, t. (1978),    a    nite sample distribution-free performance bound

for local discrimination rules   , the annals of statistics 6(3), 506   514.

rokach, l. (2007), data mining with id90: theory and applications, vol. 69,

world scienti   c.

rosenblatt, f. (1958),    the id88: a probabilistic model for information storage
(reprinted in

and organization in the brain   , psychological review 65, 386   407.
neurocomputing (mit press, 1988).).

rumelhart, d. e., hinton, g. e. & williams, r. j. (1986), learning internal represen-
tations by error propagation, in d. e. rumelhart & j. l. mcclelland, eds,    paral-
lel distributed processing     explorations in the microstructure of cognition   , mit
press, chapter 8, pp. 318   362.

444

references

sankaran, j. k. (1993),    a note on resolving infeasibility in linear programs by con-

straint relaxation   , operations research letters 13(1), 19   20.

sauer, n. (1972),    on the density of families of sets   , journal of combinatorial theory

series a 13, 145   147.

schapire, r. (1990),    the strength of weak learnability   , machine learning 5(2), 197   

227.

schapire, r. e. & freund, y. (2012), boosting: foundations and algorithms, mit press.
sch  olkopf, b., herbrich, r. & smola, a. (2001), a generalized representer theorem, in

   computational learning theory   , pp. 416   426.

sch  olkopf, b., herbrich, r., smola, a. & williamson, r. (2000), a generalized repre-

senter theorem, in    neurocolt   .

sch  olkopf, b. & smola, a. j. (2002), learning with kernels: support vector machines,

id173, optimization and beyond, mit press.

sch  olkopf, b., smola, a. & m  uller, k.-r. (1998),    nonlinear component analysis as a

kernel eigenvalue problem   , neural computation 10(5), 1299   1319.

seeger, m. (2003),    pac-bayesian generalisation error bounds for gaussian process clas-

si   cation   , the journal of machine learning research 3, 233   269.

shakhnarovich, g., darrell, t. & indyk, p. (2006), nearest-neighbor methods in learning

and vision: theory and practice, mit press.

shalev-shwartz, s. (2007), online learning: theory, algorithms, and applications,

phd thesis, the hebrew university.

shalev-shwartz, s. (2011),    online learning and online id76   , founda-

tions and trends r(cid:13) in machine learning 4(2), 107   194.

shalev-shwartz, s., shamir, o., srebro, n. & sridharan, k. (2010),    learnability,
stability and uniform convergence   , the journal of machine learning research
9999, 2635   2670.

shalev-shwartz, s., shamir, o. & sridharan, k. (2010), learning kernel-based halfs-

paces with the zero-one loss, in    conference on learning theory (colt)   .

shalev-shwartz, s., shamir, o., sridharan, k. & srebro, n. (2009), stochastic convex

optimization, in    conference on learning theory (colt)   .

shalev-shwartz, s. & singer, y. (2008), on the equivalence of weak learnability and
linear separability: new relaxations and e   cient boosting algorithms, in    proceedings
of the nineteenth annual conference on computational learning theory   .

shalev-shwartz, s., singer, y. & srebro, n. (2007), pegasos: primal estimated sub-
gradient solver for id166, in    international conference on machine learning   ,
pp. 807   814.

shalev-shwartz, s. & srebro, n. (2008), id166 optimization: inverse dependence on

training set size, in    international conference on machine learning   , pp. 928   935.

shalev-shwartz, s., zhang, t. & srebro, n. (2010),    trading accuracy for sparsity
in optimization problems with sparsity constraints   , siam journal on optimization
20, 2807   2832.

shamir, o. & zhang, t. (2013), stochastic id119 for non-smooth optimiza-
tion: convergence results and optimal averaging schemes, in    international confer-
ence on machine learning (icml)   .

shapiro, a., dentcheva, d. & ruszczy  nski, a. (2009), lectures on stochastic program-
ming: modeling and theory, vol. 9, society for industrial and applied mathematics.

references

445

shelah, s. (1972),    a combinatorial problem; stability and order for models and theories

in in   nitary languages   , pac. j. math 4, 247   261.

sipser, m. (2006), introduction to the theory of computation, thomson course tech-

nology.

slud, e. v. (1977),    distribution inequalities for the binomial law   , the annals of

id203 5(3), 404   412.

steinwart, i. & christmann, a. (2008), support vector machines, springerverlag new

york.

stone, c. (1977),

   consistent nonparametric regression   , the annals of statistics

5(4), 595   620.

taskar, b., guestrin, c. & koller, d. (2003), max-margin markov networks, in    nips   .
tibshirani, r. (1996),    regression shrinkage and selection via the lasso   , j. royal.

statist. soc b. 58(1), 267   288.

tikhonov, a. n. (1943),    on the stability of inverse problems   , dolk. akad. nauk sssr

39(5), 195   198.

tishby, n., pereira, f. & bialek, w. (1999), the information bottleneck method, in

   the 37   th allerton conference on communication, control, and computing   .

tsochantaridis, i., hofmann, t., joachims, t. & altun, y. (2004), support vector
machine learning for interdependent and structured output spaces, in    proceedings
of the twenty-first international conference on machine learning   .

valiant, l. g. (1984),

   a theory of the learnable   , communications of the acm

27(11), 1134   1142.

vapnik, v. (1992), principles of risk minimization for learning theory, in j. e. moody,
s. j. hanson & r. p. lippmann, eds,    advances in neural information processing
systems 4   , morgan kaufmann, pp. 831   838.

vapnik, v. (1995), the nature of statistical learning theory, springer.
vapnik, v. n. (1982), estimation of dependences based on empirical data, springer-

verlag.

vapnik, v. n. (1998), statistical learning theory, wiley.
vapnik, v. n. & chervonenkis, a. y. (1971),    on the uniform convergence of relative
frequencies of events to their probabilities   , theory of id203 and its applications
xvi(2), 264   280.

vapnik, v. n. & chervonenkis, a. y. (1974), theory of pattern recognition, nauka,

moscow. (in russian).

von luxburg, u. (2007),    a tutorial on spectral id91   , statistics and computing

17(4), 395   416.

von neumann, j. (1928),    zur theorie der gesellschaftsspiele (on the theory of parlor

games)   , math. ann. 100, 295   320.

von neumann, j. (1953),    a certain zero-sum two-person game equivalent to the opti-

mal assignment problem   , contributions to the theory of games 2, 5   12.

vovk, v. g. (1990), aggregating strategies,

in    conference on learning theory

(colt)   , pp. 371   383.

warmuth, m., glocer, k. & vishwanathan, s. (2008), id178 regularized lpboost, in

   algorithmic learning theory (alt)   .

warmuth, m., liao, j. & ratsch, g. (2006), totally corrective boosting algorithms
that maximize the margin, in    proceedings of the 23rd international conference on
machine learning   .

446

references

weston, j., chapelle, o., vapnik, v., elissee   , a. & sch  olkopf, b. (2002), kernel depen-
dency estimation, in    advances in neural information processing systems   , pp. 873   
880.

weston, j. & watkins, c. (1999), support vector machines for multi-class pattern
recognition, in    proceedings of the seventh european symposium on arti   cial neural
networks   .

wolpert, d. h. & macready, w. g. (1997),    no free lunch theorems for optimization   ,

evolutionary computation, ieee transactions on 1(1), 67   82.

zhang, t. (2004), solving large scale linear prediction problems using stochastic gradi-
ent descent algorithms, in    proceedings of the twenty-first international conference
on machine learning   .

zhao, p. & yu, b. (2006),    on model selection consistency of lasso   , journal of machine

learning research 7, 2541   2567.

zinkevich, m. (2003), online convex programming and generalized in   nitesimal gradi-

ent ascent, in    international conference on machine learning   .

index

3-term dnf, 107
f1-score, 244
(cid:96)1 norm, 183, 332, 363, 386

accuracy, 38, 43
activation function, 269
adaboost, 130, 134, 362
all-pairs, 228, 404
approximation error, 61, 64
auto-encoders, 368

id26, 278
backward elimination, 363
bag-of-words, 209
base hypothesis, 137
bayes optimal, 46, 52, 260
bayes rule, 354
bayesian reasoning, 353
bennet   s inequality, 426
bernstein   s inequality, 426
bias, 37, 61, 64
bias-complexity tradeo   , 65
boolean conjunctions, 51, 79, 106
boosting, 130
boosting the con   dence, 142
boundedness, 165

c4.5, 254
cart, 254
chaining, 389
chebyshev   s inequality, 423
cherno    bounds, 423
class-sensitive feature mapping, 230
classi   er, 34
id91, 307
spectral, 315

compressed sensing, 330
compression bounds, 410
compression scheme, 411
computational complexity, 100
con   dence, 38, 43
consistency, 92
consistent, 289
contraction lemma, 381
convex, 156

function, 157

set, 156
strongly convex, 174, 195

convex-lipschitz-bounded learning, 166
convex-smooth-bounded learning, 166
covering numbers, 388
curse of dimensionality, 263

decision stumps, 132, 133
id90, 250
dendrogram, 309, 310
dictionary learning, 368
di   erential set, 188
id84, 323
discretization trick, 57
discriminative, 342
distribution free, 342
domain, 33
domain of examples, 48
doubly stochastic matrix, 242
duality, 211

strong duality, 211
weak duality, 211

dudley classes, 81

e   cient computable, 100
em, 348
empirical error, 35
empirical risk, 35, 48
empirical risk minimization, see erm
id178, 345

relative id178, 345

epigraph, 157
erm, 35
error decomposition, 64, 168
estimation error, 61, 64
expectation-maximization, see em

face recognition, see viola-jones
feasible, 100
feature, 33
id171, 368
feature id172, 365
feature selection, 357, 358
feature space, 215
feature transformations, 367
   lters, 359

understanding machine learning, c(cid:13) 2014 by shai shalev-shwartz and shai ben-david
published 2014 by cambridge university press.
personal use only. not for distribution. do not post.
please link to http://www.cs.huji.ac.il/~shais/understandingmachinelearning

448

index

forward greedy selection, 360
frequentist, 353

gain, 253
gd, see id119
generalization error, 35
generative models, 342
gini index, 254
glivenko-cantelli, 58
gradient, 158
id119, 185
gram matrix, 219
growth function, 73

halfspace, 118

homogenous, 118, 205
non-separable, 119
separable, 118

halving, 289
hidden layers, 270
hilbert space, 217
hoe   ding   s inequality, 56, 425
hold out, 146
hypothesis, 34
hypothesis class, 36

i.i.d., 38
 , 252
improper, see representation independent
inductive bias, see bias
information bottleneck, 317
information gain, 254
instance, 33

instance space, 33

integral image, 143

johnson-lindenstrauss lemma, 329

id116, 311, 313

soft id116, 352

k-median, 312
k-medoids, 312
kendall tau, 239
kernel pca, 326
kernels, 215

gaussian kernel, 220
kernel trick, 217
polynomial kernel, 220
rbf kernel, 220

label, 33
lasso, 365, 386

generalization bounds, 386

latent variables, 348
lda, 347
ldim, 290, 291
learning curves, 153
least squares, 124
likelihood ratio, 348
id156, see lda
linear predictor, 117

homogenous, 118

id135, 119
id75, 122
linkage, 310
lipschitzness, 160, 176, 191

sub-gradient, 190

littlestone dimension, see ldim
local minimum, 158
id28, 126
loss, 35
id168, 48

0-1 loss, 48, 167
absolute value loss, 124, 128, 166
convex loss, 163
generalized hinge-loss, 233
hinge loss, 167
lipschitz loss, 166
log-loss, 345
logistic loss, 127
ramp loss, 209
smooth loss, 166
square loss, 48
surrogate loss, 167, 302

margin, 203
markov   s inequality, 422
massart lemma, 380
max linkage, 310
maximum a-posteriori, 355
maximum likelihood, 343
mcdiarmid   s inequality, 378
mdl, 89, 90, 251
measure concentration, 55, 422
minimum description length, see mdl
mistake bound, 288
mixture of gaussians, 348
model selection, 144, 147
multiclass, 47, 227, 402

cost-sensitive, 232
linear predictors, 230, 405
multi-vector, 231, 406
id88, 248
reductions, 227, 405
sgd, 235
id166, 234

multivariate performance measures, 243

naive bayes, 347
natarajan dimension, 402
ndcg, 239
nearest neighbor, 258

id92, 258

neural networks, 268

feedforward networks, 269
layered networks, 269
sgd, 277

no-free-lunch, 61
non-uniform learning, 84

index

449

sample complexity, 44
sauer   s lemma, 73
self-boundedness, 162
sensitivity, 244
sgd, 190
shattering, 69, 403
single linkage, 310
singular value decomposition, see svd
slud   s inequality, 428
smoothness, 162, 177, 198
soa, 292
sparsity-inducing norms, 363
speci   city, 244
spectral id91, 315
srm, 85, 145
stability, 173
stochastic id119, see sgd
strong learning, 132
structural risk minimization, see srm
structured output prediction, 236
sub-gradient, 188
support vector machines, see id166
svd, 431
id166, 202, 383
duality, 211
generalization bounds, 208, 383
hard-id166, 203, 204
homogenous, 205
kernel trick, 217
soft-id166, 206
support vectors, 210

target set, 47
term-frequency, 231
tf-idf, 231
training error, 35
training set, 33
true error, 35, 45

under   tting, 65, 152
uniform convergence, 54, 55
union bound, 39
unsupervised learning, 308

validation, 144, 146

cross validation, 149
train-validation-test split, 150

vapnik-chervonenkis dimension, see vc

dimension

vc dimension, 67, 70
version space, 289
viola-jones, 139

weak learning, 130, 131
weighted-majority, 295

normalized discounted cumulative gain,

see ndcg

occam   s razor, 91
omp, 360
one-vs-all, 227
one-vs-rest, see one-vs-all
one-vs.-all, 404
online id76, 300
online id119, 300
online learning, 287
optimization error, 168
oracle inequality, 179
orthogonal matching pursuit, see omp
over   tting, 35, 65, 152

pac, 43

agnostic pac, 45, 46
agnostic pac for general loss, 49

pac-bayes, 415
parametric density estimation, 342
pca, 324
pearson   s correlation coe   cient, 359
id88, 120

kernelized id88, 225
multiclass, 248
online, 301

permutation matrix, 242
polynomial regression, 125
precision, 244
predictor, 34
pre   x free language, 89
principal component analysis, see pca
prior knowledge, 63
probably approximately correct, see pac
projection, 193

projection lemma, 193

proper, 49
pruning, 254

rademacher complexity, 375
id79s, 255
random projections, 329
ranking, 238

bipartite, 243
realizability, 37
recall, 244
regression, 47, 122, 172
id173, 171

tikhonov, 172, 174

regularized loss minimization, see rlm
representation independent, 49, 107
representative sample, 54, 375
representer theorem, 218
ridge regression, 172

kernel ridge regression, 225

rip, 331
risk, 35, 45, 48
rlm, 171, 199

