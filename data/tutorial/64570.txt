   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]hacker noon
     * [9]latest
     * [10]editors' choice
     * [11]terms faq
     * [12]sign up for 2.0
     * [13]future of search
     __________________________________________________________________

   [1*xwh6hd8bgi3rkhd8bkrbuw.png]

memorizing is not learning!         6 tricks to prevent overfitting in machine
learning.

   [14]go to the profile of julien despois
   [15]julien despois (button) blockedunblock (button) followfollowing
   mar 22, 2018

introduction

   overfitting may be the most frustrating issue of machine learning. in
   this article, we   re going to see what it is, how to spot it, and most
   importantly how to prevent it from happening.

what is overfitting?

   the word overfitting refers to a model that models the training data
   too well. instead of learning the genral distribution of the data, the
   model learns the expected output for every data point.
   [1*sbuk2qefcp-zvjmkm14wgq.png]

   this is the same a memorizing the answers to a maths quizz instead of
   knowing the formulas. because of this, the model cannot generalize.
   everything is all good as long as you are in familiar territory, but as
   soon as you step outside, you   re lost.
   [1*qkl_6cu0pqam0my-he3qfg.png]
   looks like this little guy doesn   t know how to do a multiplication. he
   only remembers the answers to the questions he has already seen.

   the tricky part is that, at first glance, it may seem that your model
   is performing well because it has a very small error on the training
   data. however, as soon as you ask it to predict new data points, it
   will fail.

how to detect overfitting

   as stated above, overfitting is characterized by the inability of the
   model to generalize. to test this ability, a simple method consists in
   splitting the dataset into two parts: the training set and the test
   set. when selecting models, you might want to split the dataset in
   three, [16]i explain why here.
    1. the training set represents about 80% of the available data, and is
       used to train the model (you don   t say?!).
    2. the test set consists of the remaining 20% of the dataset, and is
       used to test the accuracy of the model on data it has never seen
       before.

   with this split we can check the performance of the model on each set
   to gain insight on how the training process is going, and spot
   overfitting when it happens. this table shows the different cases.
   [1*3xvsvkfde8u89tmwjkz3kg.png]
   overfitting can be seen as the difference between the training and
   testing error.

   note: for this technique to work, you need to make sure both parts are
   representative of your data. a good practice is to shuffle the order of
   the dataset before splitting.
   [1*i5neyjtfxnatwlur6zrw1w.png]

   overfitting can be pretty discouraging because it raises your hopes
   just before brutally crushing them. fortunately, there are a few tricks
   to prevent it from happening.

how to prevent overfitting - model & data

   first, we can try to look at the components of our system to find
   solutions. this means changing data we are using, or which model.
   [1*mnprbp4jviw7rrhth84jqw.png]

gather more data

   you model can only store so much information. this means that the more
   training data you feed it, the less likely it is to overfit. the reason
   is that, as you add more data, the model becomes unable to overfit all
   the samples, and is forced to generalize to make progress.

   collecting more examples should be the first step in every data science
   task, as more data will result in an increased accuracy of the model,
   while reducing the chance of overfitting.
   [1*l392ucqge-ztsojyiern7a.png]
   the more data you get, the less likely the model is to overfit.
   [1*qtcllicf19la5gjrcmahmg.png]

data augmentation & noise

   collecting more data is a tedious and expensive process. if you can   t
   do it, you should try to make your data appear as if it was more
   diverse. to do that, use [17]data augmentation techniques so that each
   time a sample is processed by the model, it   s slightly different from
   the previous time. this will make it harder for the model to learn
   parameters for each sample.
   [1*cwznrhefc9r9xhqn8lnsta.png]
   each iteration sees as different variation of the original sample.

   another good practice is to add noise:
     * to the input: this serves the same purpose as data augmentation,
       but will also work toward making the model robust to natural
       perturbations it could encounter in the wild.
     * to the output: again, this will make the training more diversified.

   note: in both cases, you need to make sure that the magnitude of the
   noise is not too great. otherwise, you could end up respectively
   drowning the information of the input in the noise, or make the output
   incorrect. both will hinder the training process.
   [1*uv2699bedsydp_yfku5itw.png]

simplify the model

   if, even with all the data you now have, your model still manages to
   overfit your training dataset, it may be that the model is too
   powerful. you could then try to reduce the complexity of the model.

   as stated previously, a model can only overfit that much data. by
   progressively reducing its complexity         # of estimators in a random
   forest, # of parameters in a neural network etc.         you can make the
   model simple enough that it doesn   t overfit, but complex enough to
   learn from your data. to do that, it   s convenient to look at the error
   on both datasets depending on the model complexity.

   this also has the advantage of making the model lighter, train faster
   and run faster.
   [1*vuzxfmi5fodz2oecpg-s1g.png]
   on the left, the model is too simple. on the right it overfits.
     __________________________________________________________________

how to prevent overfitting - training process

   a second possibility it to change the way the training is done. this
   includes altering the id168, or the way the model functions
   during training.
   [1*iej-f7g2zk5enycu1odkeg.png]

early termination

   in most cases, the model starts by learning a correct distribution of
   the data, and, at some point, starts to overfit the data. by
   identifying the moment where this shift occurs, you can stop the
   learning process before the overfitting happens. as before, this is
   done by looking at the training error over time.
   [1*xwfbnw3arf39wxk4zki2mw.png]
   when the testing error starts to increase, it   s time to stop!
     __________________________________________________________________

how to prevent overfitting         id173

   id173 is a process of constraining the learning of the model
   to reduce overfitting. it can take many different forms, and we will
   see a couple of them.
   [1*ca6abuqcrhews3e83liehq.png]

l1 and l2 id173

   one of the most powerful and well-known technique of id173 is
   to add a penalty to the id168. the most common are called
   [18]l1 and [19]l2:
    1. the l1 penalty aims to minimize the absolute value of the weights
    2. the l2 penalty aims to minimize the squared magnitude of the
       weights.

   [1*jp9vzwxsrxjocb3wgvotxa.png]

   with the penalty, the model is forced to make compromises on its
   weights, as it can no longer make them arbitrarily large. this makes
   the model more general, which helps combat overfitting.

   the l1 penalty has the added advantage that it enforces [20]feature
   selection, which means that it has a tendency to set to 0 the less
   useful parameters. this helps identify the most relevant features in a
   dataset. the downside is that it is often not as computationally
   efficient as the l2 penalty.

   here is what the weight matrixes would look like. note how the l1
   matrix is sparse with many zeros, and the l2 matrix has slightly
   smaller weights.
   [1*ffyf-xj3lrmdgo68ywjw4q.png]

   another possibility is to [21]add noise to the parameters during the
   training, which helps generalization.
   [1*g8k0qouzmkly1oat2903bq.png]

for deep learning: dropout and dropconnect

   this extremely effective technique is specific to deep learning, as it
   relies on the fact that neural networks process the information from
   one layer to the next. the idea is to randomly deactivate either
   neurons ([22]dropout) or connections (dropconnect) during the training.
   [1*dfrgmzdwprqitkdmotkwtg.jpeg]

   this forces the network to become redundant, as it can no longer rely
   on specific neurons or connections to extract specific features. once
   the training is done, all neuraons and connections are restored. it has
   been shown that this technique is somewhat equivalent to having an
   [23]ensemble approach, which favorises generalization, thus reducing
   overfitting.

conclusion

   as you know by now, overfitting is one of the main issues the data
   scientist has to face. it can be a real pain to deal with if you don   t
   know how to stop it. with the techniques presented in this article, you
   should now be able to prevent your models from cheating the learning
   process, and get the results you deserve!

        you   ve reached the end! i hope you enjoyed this article. if you did,
   feel free to like it, share it, explain it to your cat, follow me on
   medium, or do whatever you feel like doing!     

     if you like data science and artificial intelligence, [24]subscribe
     to the newsletter to receive updates on articles and much more!

     * [25]machine learning
     * [26]data science
     * [27]data
     * [28]tech
     * [29]technology

   (button)
   (button)
   (button) 618 claps
   (button) (button) (button) 1 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of julien despois

[31]julien despois

   deep learning scientist @ l   or  al ai research | creator of
   ai-odyssey.com

     (button) follow
   [32]hacker noon

[33]hacker noon

   how hackers start their afternoons.

     * (button)
       (button) 618
     * (button)
     *
     *

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://hackernoon.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/820b091dc42
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42&source=--------------------------nav_reg&operation=register
   8. https://hackernoon.com/?source=logo-lo_fw4p3zpvhwcc---3a8144eabfe3
   9. https://hackernoon.com/latest-tech-stories/home
  10. https://hackernoon.com/editors-top-tech-stories/home
  11. https://hackernoon.com/your-most-frequently-asked-questions-about-our-terms-of-service-how-to-opt-out-and-more-66abf239a151
  12. https://hackernoon.com/sign-up-for-hacker-noon-2-0-9ff1ea0b60cc
  13. https://community.hackernoon.com/t/what-will-replace-google-search/992/14
  14. https://hackernoon.com/@juliendespois?source=post_header_lockup
  15. https://hackernoon.com/@juliendespois
  16. https://hackernoon.com/stop-feeding-garbage-to-your-model-the-6-biggest-mistakes-with-datasets-and-how-to-avoid-them-3cb7532ad3b7
  17. https://hackernoon.com/stop-feeding-garbage-to-your-model-the-6-biggest-mistakes-with-datasets-and-how-to-avoid-them-3cb7532ad3b7
  18. http://mathworld.wolfram.com/l1-norm.html
  19. http://mathworld.wolfram.com/l2-norm.html
  20. http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-id173/
  21. https://arxiv.org/abs/1505.05424
  22. http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf
  23. https://arxiv.org/abs/1706.06859
  24. http://eepurl.com/catxvt
  25. https://hackernoon.com/tagged/machine-learning?source=post
  26. https://hackernoon.com/tagged/data-science?source=post
  27. https://hackernoon.com/tagged/data?source=post
  28. https://hackernoon.com/tagged/tech?source=post
  29. https://hackernoon.com/tagged/technology?source=post
  30. https://hackernoon.com/@juliendespois?source=footer_card
  31. https://hackernoon.com/@juliendespois
  32. https://hackernoon.com/?source=footer_card
  33. https://hackernoon.com/?source=footer_card

   hidden links:
  35. https://medium.com/p/820b091dc42/share/twitter
  36. https://medium.com/p/820b091dc42/share/facebook
  37. https://medium.com/p/820b091dc42/share/twitter
  38. https://medium.com/p/820b091dc42/share/facebook
