   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]artificial understanding
   [7]artificial understanding
   [8]sign in[9]get started
     __________________________________________________________________

why deep learning works

   [10]go to the profile of monica anderson
   [11]monica anderson (button) blockedunblock (button) followfollowing
   feb 24, 2018

   a math-free computer-science-free description of why deep learning
   works.

   we have now built a base of theory for [12]why ai works, [13]what
   models are and how to create them, [14]what reductionism and holism
   are, and [15]what the process of reduction is. these are the
   fundamentals of ai epistemology. this base allows us to discuss various
   strategies to move towards understanding machines in a well understood
   and controlled manner.

   we are now ready to discuss why deep learning (dl) works. this is the
   fifth and last entry in the ai epistemology primer.

deep learning performs reduction

   this is an unsurprising claim, considering the preceding chapters.
   there are several mutually compatible [16]theories for    how    deep
   learning works. but just as in the first chapter, we will now discuss
   the epistemological aspects,    why    it works, from several viewpoints
   and levels, starting from the bottom. we will use examples from the
   tensorflow system and api (as a library) as a stand-in for all deep
   learning-family algorithms and tf programs, because the available api
   functions heavily shape and constrain solutions that can be implemented
   in this space and the generalization should be straightforward enough.

   consider the following illustration of image understanding using keras
   (an excellent abstraction layer on top of tensorflow):
   [1*_pf5v8u_ucjpwxtn6by4ha.png]
   copied from [17]http://cv-tricks.com/tensorflow-tutorial/keras/

   i like to refer to the input layer as being    on the bottom    rather than
   at the far left as in this image. when viewing it my way, the
   low-to-high dimension we use in my rotated version of the above can be
   mentally mapped to a low-to-high stack of abstraction levels; i   m not
   the only one using this dimension this way. i hope this rotation isn   t
   too confusing.

   we can see that there is an obvious data reduction and an obvious
   complexity reduction. can we determine whether the system is also
   performing what i   d like to call    epistemic reduction   : is it reducing
   away that which is unimportant, and if so, how does it accomplish this?
   how does an operator in a deep learning stack know what makes something
   important (salient)?

   a pure data    reduction    of sorts could be accomplished by compression
   schemes or even random deletion. this is undesirable. we need to
   discard the non-salient parts so that in the end, we are left with what
   is salient. some people have not understood the importance of salience
   based reduction and use [18]lossless compression power of reversible
   algorithms as a measurement of intelligence. which is no more useful
   than believing a simple video camera can understand what it sees.

   so let me conjure up, a bit like in the movie    inside out   , a fairytale
   of what goes on in a deep learning network, except we   ll do it    bottom
   up   . suppose we have built a system for finding faces in an image, with
   the intent of incorporating that as a feature in a camera; many cameras
   have this feature already, so this is not a far-fetched example. we
   implement an image understanding neural network, show the system many
   kinds of images for a few days, perhaps using so-called supervised
   learning in order to improve this story, and then we show it an image
   of a family having a picnic in a park and ask the system to outline
   where the faces are so that the camera can focus sharply on them.

   the input image is converted from rgb color values to an input array
   and the data in this array is then shuffled through many layers of
   operators. and for many of these layers, there are fewer outputs than
   there are inputs, as you can see in the above diagram. which means some
   things have to be discarded by the processing. each layer receives
   initially signals    from below   , i.e. from the input, or from lower
   levels of abstraction, and produces some reduced output to send to the
   next layer operator above.

   to continue the tale, at some early level, some operator is given a few
   adjacent pixels and determines that there is a vertical, slightly
   curved line dividing a darker green area from a lighter green area, so
   it    tells    the operator above this simpler line/color based description
   using some encoding we don   t really care about.

   the operator at the level above might have gotten another matching
   curve and says    these match what i saw a lot of when the label    blade
   of grass    was given as a ground truth label during (supervised)
   learning. if no label is known, then we again assume some other
   uninteresting representation. it is ok to propagate results without
   human-labeled signals because whatever signaling scheme is used will be
   learned by the level above.

   the operator above that says    when i get lots of blades-of-grass
   signals i reduce all of that to a    lawn    signal as i send it upward.

   and eventually we reach the higher operator layers and someone there
   says    we are a face finder application. we are completely uninterested
   in lawns    and discards the lawn as non-salient.

   what remains after you discard all non-faces are the faces.

   you cannot discard anything until you know what it is, or can at least
   estimate whether it   s worth learning. specifically, until you
   understand it at the level of abstraction you are operating at. the low
   level blade-of-grass recognizers could not discard the grass because
   they had no clue about the high level saliencies of lawn-or-not and
   face-or-not that the higher layers specialize in.

   you can only tell what   s salient or not (important or not) at the level
   of understanding and abstraction you are operating at. each layers
   receives    lower level descriptions    from below, discards what it
   recognizes as irrelevant, and sends its own version of    higher level
   descriptions    upward until we reach someone who knows what we are
   really looking for.

   this is of course why deep learning is deep.

   this idea itself is not new. it was discussed by oliver selfridge in
   1959; he described an idea called    pandemonium    which was largely
   ignored by the ai community because of its radical departure from the
   logic based ai promoted by people like john mccarthy and marvin minsky.
   but pandemonium presaged, by almost 60 years, the layer-by layer
   architecture with signals passing up and down that is used today in all
   deep neural networks. this is the reason my online handle is
   @pandemonica

       *    

   so do any tensorflow operators support this reduction?

   let   s start by examining the pooling operators; there are a few in the
   diagram. they are conceptually simple. there are over 50 pooling
   operators in tensorflow. there is an operator named    2x2 max-pool
   operator   . in this diagram, it is used four times:
   [1*ue90wxyeoimjm028c4f1tq.png]
   copied from
   [19]http://bakeaselfdrivingcar.blogspot.com/2017/08/project-2-traffic-s
   ign-recognition-with.html

   it is given four inputs with varying values and propagates the highest
   value of those as its only output. close to the input layer these four
   values may be four adjacent pixels where their values might be a
   brightness in some color channel, but higher up they mean whatever they
   mean. in effect, the max-pool 2x2 discards the    least important    75% of
   its input data, preserving and propagating only one (highest) value.

   in the case of pixels, it might mean the brightest color value. in the
   case of blades of grass, it might mean    there is at least one blade of
   grass here   . the interpretation of what is discarded depends on the
   layer, because in a very real sense, layers represent levels of
   reduction; abstraction levels, if you prefer that term.

   and we should now be clearly seeing one of the most important ideas in
   deep neural networks: reduction has to be done at multiple levels of
   abstraction. each set of decisions about what is reduced away as
   irrelevant and what is kept as possibly relevant can only be made at an
   appropriate abstraction level. we cannot yet abstract away the lawn if
   all we know is there are dark-and-light-green-areas levels.

   this is a simplification; decisions made in this manner will be heeded
   only if they have contributed to positive outcomes in learning.
   unreliable and useless decision makers will be ignored using any of
   several mechanisms that we may apply during learning. more later.

   for now, we continue by examining the most popular subset of all
   tensorflow operators     the convolution family. from the tensorflow
   manual:

     note that although these ops are called    convolution   , they are
     strictly speaking    cross-correlation   

   convolution layers discover cross-correlations and co-occurrences of
   various kinds. co-occurrences to known patterns in the image at various
   locations; spatial relationships within an image itself, like geoff
   hinton   s recent example of the mouth normally being found below the
   nose; and more obviously, in the supervised learning case, correlations
   between discovered patterns and the available meta-information (tags,
   labels) that correlate with the patterns the system may discover. this
   is what allows an an image understander to tag the occurrence of a nose
   in an image with the text string    nose   . beyond this, such systems may
   learn to understand concepts like    behind    and    under   .

   the information that is propagated to the higher levels in the network
   now describes these correlations. uncorrelated information is viewed as
   non-salient and is discarded. in the diagram above, this discarding is
   done by a max pooling layer after the convolution+relu layers. relu is
   a kind of layer operator that discards negative values, introducing a
   non-linearity that is important for dl but not really important for our
   analysis.

   this pattern of three layers         convolution, then relu, then a pooling
   layer         is quite popular because this combination is performing one
   reliable reduction step. these three layer types in this    packaged   
   sequence may appear many times in a dl computational graph. and each of
   these three-layer packages is reducing away things that levels below
   had no chance of evaluating for saliency because they didn   t
      understand    their input at the correct level.

   again,

   this is why deep learning is deep    because you can only do reduction by
   discarding the irrelevant if you understand what is relevant and
   irrelevant at each different level of abstraction.

is deep learning science or not?

   while the deep learning process can be described using mathematical
   notation, mostly using id202, the process itself isn   t
   scientific. we can not explain how this system is capable of forming
   any kind of understanding by just staring at these equations, since
   understanding is an emergent effect of repeated reductions over many
   layers.

   consider the convolution operators. as the tf manual quote clearly
   states, convolution layers discover correlations. many blades of grass
   together typically means a lawn. in tf, a lot of cycles are spent on
   discovering these correlations. once found, the correlation leads to
   some adjustment of some weight to make the correct reduction more
   likely to be re-discovered the next round (because this reduction is
   done multiple times) but in essence, all correlations are forgotten and
   have to be re-discovered in every pass through the deep learning loop
   of upward signaling and downward id119 with minute
   adjustments to erring variables. the system is in effect learning from
   its mistakes, which is a good sign, since that may well be the only way
   to learn anything. at least at these levels. this up-and-down may be
   repeated many times for each image in the learning set.

   this up-and-down makes some sense for image understanding. some are
   using the same algorithms for text. fortunately, in the text case,
   there are very efficient alternatives to this ridiculously expensive
   algorithm. for starters, we can represent the discovered correlations
   explicitly, using regular    pointers    or    object references    in our
   programming languages. or    synapses    in brains.    this (software) neuron
   correlates with that software neuron    says a synapse or reference
   connecting this to that. we shall discuss such systems in the organic
   learning series of blog entries; coming up next.

   neither the deep learning family of algorithms, or organic learning,
   are scientific in any meaningful way. they jump to conclusions on scant
   evidence and trust correlations without insisting on provable
   causality; this is disallowed in scientific theory, where absolutely
   reliable causality is the coin of the realm. f=m*a or go home. most
   deep neural network programming is uncomfortably close to trial and
   error, with only minor clues about how to improve the system when
   reaching mediocre results.    adding more layers doesn   t always help   .
   these kinds of problems are the everyday reality to most practitioners
   of deep neural networks. with no a priori models, there will be no a
   priori guarantees. the best estimate of the reliability and correctness
   of any deep neural network, or even any holistic system we can ever
   devise, is going to be extensive testing. more on this later, in future
   blogs.

   why would we ever use engineered systems that cannot be guaranteed to
   provide the correct answer? because we have no choice. we only use
   holistic methods when the reliable reductionist methods are
   unavailable. as is the case when the task requires the ability to
   perform autonomous reduction of context-rich slices of our rich complex
   reality as a whole. when the task requires understanding.

   don   t we have an alternative to these unreliable machines? sure we do.
   there are billions of humans on the planet that are already masters of
   this complex task. because they live in the rich world and need skills
   that are unavailable with reductionist methods, starting with low level
   things like object permanence. so you can replace a well-performing but
   theoretically unproven contraption     a holistic understanding machine
   built out of deep neural networks     with a well-performing human being
   using a deeply mystical kind of understanding hidden in their opaque
   heads. who earns much more per hour. this doesn   t look like much of an
   improvement. the machine cannot be proven correct because it doesn   t
   function like normal computers. it is performing reduction, the skill
   formerly restricted to animals.

   a holistic skill.

   my favorite soundbite is a mere corollary to the frame problem by
   mccarthy and hayes; you have seen it and you will see it again, since
   it is one of the stronger results of ai epistemology. but we will, in
   but a few years, agree on a definition of intelligence that makes
   autonomous reduction a requirement. this once semi-heretic soundbite
   will then be obvious to all. if it isn   t already.

     all intelligences are fallible

     * [20]artificial intelligence
     * [21]deep learning
     * [22]understanding
     * [23]ai
     * [24]ai epistemology

   (button)
   (button)
   (button) 1.2k claps
   (button) (button) (button) 7 (button) (button)

     (button) blockedunblock (button) followfollowing
   [25]go to the profile of monica anderson

[26]monica anderson

   independent ai researcher, implementer, epistemologist, and educator
   exploring deep neural networks since 2001; ex-googler; founder
   syntience inc. and sens.ai .

     (button) follow
   [27]artificial understanding

[28]artificial understanding

   the epistemology of artificial intelligence

     * (button)
       (button) 1.2k
     * (button)
     *
     *

   [29]artificial understanding
   never miss a story from artificial understanding, when you sign up for
   medium. [30]learn more
   never miss a story from artificial understanding
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://artificial-understanding.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/1b0184686af6
   4. https://medium.com/
   5. https://medium.com/
   6. https://artificial-understanding.com/?source=avatar-lo_bmi6mowe2w0c-4a85ab1ea3d7
   7. https://artificial-understanding.com/?source=logo-lo_bmi6mowe2w0c---4a85ab1ea3d7
   8. https://medium.com/m/signin?redirect=https://artificial-understanding.com/why-deep-learning-works-1b0184686af6&source=--------------------------nav_reg&operation=login
   9. https://medium.com/m/signin?redirect=https://artificial-understanding.com/why-deep-learning-works-1b0184686af6&source=--------------------------nav_reg&operation=register
  10. https://artificial-understanding.com/@monicaanderson?source=post_header_lockup
  11. https://artificial-understanding.com/@monicaanderson
  12. https://artificial-understanding.com/why-ai-works-b682a42b1ba3
  13. https://artificial-understanding.com/our-greatest-invention-1740128b406
  14. https://artificial-understanding.com/two-dirty-words-6703aee8e323
  15. https://artificial-understanding.com/reduction-b73614d79483
  16. http://www.deeplearningbook.org/version-2015-10-03/contents/manifolds.html
  17. http://cv-tricks.com/tensorflow-tutorial/keras/
  18. http://prize.hutter1.net/
  19. http://bakeaselfdrivingcar.blogspot.com/2017/08/project-2-traffic-sign-recognition-with.html
  20. https://artificial-understanding.com/tagged/artificial-intelligence?source=post
  21. https://artificial-understanding.com/tagged/deep-learning?source=post
  22. https://artificial-understanding.com/tagged/understanding?source=post
  23. https://artificial-understanding.com/tagged/ai?source=post
  24. https://artificial-understanding.com/tagged/ai-epistemology?source=post
  25. https://artificial-understanding.com/@monicaanderson?source=footer_card
  26. https://artificial-understanding.com/@monicaanderson
  27. https://artificial-understanding.com/?source=footer_card
  28. https://artificial-understanding.com/?source=footer_card
  29. https://artificial-understanding.com/
  30. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  32. https://medium.com/p/1b0184686af6/share/twitter
  33. https://medium.com/p/1b0184686af6/share/facebook
  34. https://medium.com/p/1b0184686af6/share/twitter
  35. https://medium.com/p/1b0184686af6/share/facebook
