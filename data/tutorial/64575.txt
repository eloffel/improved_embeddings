   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

how to build a three-layer neural network from scratch

   [11]go to the profile of daphne cornelisse
   [12]daphne cornelisse (button) blockedunblock (button) followfollowing
   feb 18, 2018
   [1*qn4w7osekqsoebjiljy7xw.jpeg]
   photo by [13]tha   hamelin on [14]unsplash

   in this post, i will go through the steps required for building a three
   layer neural network. i   ll go through a problem and explain you the
   process along with the most important concepts along the way.

the problem to solve

   a farmer in italy was having a problem with his labelling machine: it
   mixed up the labels of three wine cultivars. now he has 178 bottles
   left, and nobody knows which cultivar made them! to help this poor man,
   we will build a classifier that recognizes the wine based on 13
   attributes of the wine.
   [1*_vwgamxbph0z3sc7fqdp1q.jpeg]

   the fact that our data is labeled (with one of the three cultivar   s
   labels) makes this a [15]supervised learning problem. essentially, what
   we want to do is use our input data (the 178 unclassified wine
   bottles), put it through our [16]neural network, and then get the right
   label for each wine cultivar as the output.

   we will train our algorithm to get better and better at predicting
   (y-hat) which bottle belongs to which label.

   now it is time to start building the neural network!

approach

   building a neural network is almost like building a very complicated
   function, or putting together a very difficult recipe. in the
   beginning, the ingredients or steps you will have to take can seem
   overwhelming. but if you break everything down and do it step by step,
   you will be fine.
   [1*7qyegfnwvharpre5-k3t6a.jpeg]
   overview of the 3 layer neural network, a wine classifier

   in short:
     * the input layer (x) consists of 178 neurons.
     * a1, the first layer, consists of 8 neurons.
     * a2, the second layer, consists of 5 neurons.
     * a3, the third and output layer, consists of 3 neurons.

step 1: the usual prep

   import all necessary libraries (numpy, skicit-learn, pandas) and the
   dataset, and define x and y.
#importing all the libraries and dataset
import pandas as pd
import numpy as np
df = pd.read_csv('../input/w1data.csv')
df.head()
# package imports
# matplotlib
import matplotlib
import matplotlib.pyplot as plt
# scikitlearn is a machine learning utilities library
import sklearn
# the sklearn dataset module helps generating datasets
import sklearn.datasets
import sklearn.linear_model
from sklearn.preprocessing import onehotencoder
from sklearn.metrics import accuracy_score

step 2: initialization

   before we can use our weights, we have to initialize them. because we
   don   t have values to use for the weights yet, we use random values
   between 0 and 1.

   in python, the random.seed function generates    random numbers.   
   however, random numbers are not truly random. the numbers generated are
   pseudorandom, meaning the numbers are generated by a complicated
   formula that makes it look random. in order to generate numbers, the
   formula takes the previous value generated as its input. if there is no
   previous value generated, it often takes the time as a first value.

   that is why we seed the generator         to make sure that we always get the
   same random numbers. we provide a fixed value that the number generator
   can start with, which is zero in this case.
np.random.seed(0)

step 3: forward propagation

   there are roughly two parts of training a neural network. first, you
   are propagating forward through the nn. that is, you are    making steps   
   forward and comparing those results with the real values to get the
   difference between your output and what it should be. you basically see
   how the nn is doing and find the errors.

   after we have initialized the weights with a pseudo-random number, we
   take a linear step forward. we calculate this by taking our input a0
   times the [17]dot product of the random initialized weights plus a
   bias. we started with a bias of 0. this is represented as:
   [1*ctstmneen-orzcvip1dlrw.png]

   now we take our z1 (our linear step) and pass it through our first
   activation function. id180 are very important in neural
   networks. essentially, they convert an input signal to an output
   signal         this is why they are also known as transfer functions. they
   introduce non-linear properties to our functions by converting the
   linear input to a non-linear output, making it possible to represent
   more complex functions.

   there are different kinds of id180 (explained in depth
   in [18]this article). for this model, we chose to use the tanh
   activation function for our two hidden layers         a1 and a2         which gives
   us an output value between -1 and 1.

   since this is a multi-class classification problem (we have 3 output
   labels), we will use the softmax function for the output
   layer         a3         because this will compute the probabilities for the
   classes by spitting out a value between 0 and 1.
   [1*feu0sml-zvq2mghih1xo4g.png]
   tanh function

   by passing z1 through the activation function, we have created our
   first hidden layer         a1         which can be used as input for the
   computation of the next linear step, z2.
   [1*vgutaaaf4pndlj8wtr3xca.png]

   in python, this process looks like this:
# this is the forward propagation function
def forward_prop(model,a0):

    # load parameters from model
    w1, b1, w2, b2, w3, b3 = model['w1'], model['b1'], model['w2'], model['b2'],
 model['w3'],model['b3']

    # do the first linear step
    z1 = a0.dot(w1) + b1

    # put it through the first activation function
    a1 = np.tanh(z1)

    # second linear step
    z2 = a1.dot(w2) + b2

    # put through second activation function
    a2 = np.tanh(z2)

    #third linear step
    z3 = a2.dot(w3) + b3

    #for the third linear activation function we use the softmax function
    a3 = softmax(z3)

    #store all results in these values
    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}
    return cache

   in the end, all our values are stored in the [19]cache.

step 4: backwards propagation

   after we forward propagate through our nn, we backward propagate our
   error gradient to update our weight parameters. we know our error, and
   want to minimize it as much as possible.

   we do this by taking the derivative of the error function, with respect
   to the weights (w) of our nn, using id119.

   lets visualize this process with an analogy.

   imagine you went out for a walk in the mountains during the afternoon.
   but now its an hour later and you are a bit hungry, so it   s time to go
   home. the only problem is that it is dark and there are many trees, so
   you can   t see either your home or where you are. oh, and you forgot
   your phone at home.

   but then you remember your house is in a valley, the lowest point in
   the whole area. so if you just walk down the mountain step by step
   until you don   t feel any slope, in theory you should arrive at your
   home.

   so there you go, step by step carefully going down. now think of the
   mountain as the id168, and you are the algorithm, trying to
   find your home (i.e. the lowest point). every time you take a step
   downwards, we update your location coordinates (the algorithm updates
   the parameters).
   [1*djbjys3plxcvgo8afvasma.png]

   the id168 is represented by the mountain. to get to a low loss,
   the algorithm follows the slope         that is the derivative         of the loss
   function.

   when we walk down the mountain, we are updating our location
   coordinates. the algorithm updates the parameters of the neural
   network. by getting closer to the minimum point, we are approaching our
   goal of minimizing our error.

   in reality, id119 looks more like this:
   [1*t4aysxpcqz2eymj4zkus9q.png]

   we always start with calculating the slope of the id168 with
   respect to z, the slope of the linear step we take.

   notation is as follows: dv is the derivative of the id168, with
   respect to a variable v.
   [1*zajwuagm-onhmc_hlheyta.png]

   next we calculate the slope of the id168 with respect to our
   weights and biases. because this is a 3 layer nn, we will iterate this
   process for z3,2,1 + w3,2,1 and b3,2,1. propagating backwards from the
   output to the input layer.
   [1*d2bcdmslxm23o4aekjpo1a.png]

   this is how this process looks in python:
# this is the backward propagation function
def backward_prop(model,cache,y):
# load parameters from model
    w1, b1, w2, b2, w3, b3 = model['w1'], model['b1'], model['w2'], model['b2'],
model['w3'],model['b3']

    # load forward propagation results
    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']

    # get number of samples
    m = y.shape[0]

    # calculate loss derivative with respect to output
    dz3 = loss_derivative(y=y,y_hat=a3)
# calculate loss derivative with respect to second layer weights
    dw3 = 1/m*(a2.t).dot(dz3) #dw2 = 1/m*(a1.t).dot(dz2)

    # calculate loss derivative with respect to second layer bias
    db3 = 1/m*np.sum(dz3, axis=0)

    # calculate loss derivative with respect to first layer
    dz2 = np.multiply(dz3.dot(w3.t) ,tanh_derivative(a2))

    # calculate loss derivative with respect to first layer weights
    dw2 = 1/m*np.dot(a1.t, dz2)

    # calculate loss derivative with respect to first layer bias
    db2 = 1/m*np.sum(dz2, axis=0)

    dz1 = np.multiply(dz2.dot(w2.t),tanh_derivative(a1))

    dw1 = 1/m*np.dot(a0.t,dz1)

    db1 = 1/m*np.sum(dz1,axis=0)

    # store gradients
    grads = {'dw3':dw3, 'db3':db3, 'dw2':dw2,'db2':db2,'dw1':dw1,'db1':db1}
    return grads

step 5: the training phase

   in order to reach the optimal weights and biases that will give us the
   desired output (the three wine cultivars), we will have to train our
   neural network.

   i think this is very intuitive. for almost anything in life, you have
   to train and practice many times before you are good at it. likewise, a
   neural network will have to undergo many [20]epochs or iterations to
   give us an accurate prediction.

   when you are learning anything, lets say you are reading a book, you
   have a certain pace. this pace should not be too slow, as reading the
   book will take ages. but it should not be too fast, either, since you
   might miss a very valuable lesson in the book.

   in the same way, you have to specify a    learning rate    for the model.
   the learning rate is the multiplier to update the parameters. it
   determines how rapidly they can change. if the learning rate is low,
   training will take longer. however, if the learning rate is too high,
   we might miss a minimum. the learning rate is expressed as:
   [1*xvc3l1gdsvjhmbuz6fte_a.png]
     * := means that this is a definition, not an equation or proven
       statement.
     * a is the learning rate called alpha
     * dl(w) is the derivative of the total loss with respect to our
       weight w
     * da is the derivative of alpha

   we chose a learning rate of 0.07 after some experimenting.
# this is what we return at the end
model = initialise_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)
model = train(model,x,y,learning_rate=0.07,epochs=4500,print_loss=true)
plt.plot(losses)

   [1*kct4fgz4qfqajdvtplzhsa.png]

   finally, there is our graph. you can plot your accuracy and/or loss to
   get a nice graph of your prediction accuracy. after 4,500 epochs, our
   algorithm has an accuracy of 99.4382022472 %.

brief summary

   we start by feeding data into the neural network and perform several
   matrix operations on this input data, layer by layer. for each of our
   three layers, we take the dot product of the input by the weights and
   add a bias. next, we pass this output through an activation function of
   choice.

   the output of this activation function is then used as an input for the
   following layer to follow the same procedure. this process is iterated
   three times since we have three layers. our final output is y-hat,
   which is the prediction on which wine belongs to which cultivar. this
   is the end of the forward propagation process.

   we then calculate the difference between our prediction (y-hat) and the
   expected output (y) and use this error value during id26.

   during id26, we take our error         the difference between our
   prediction y-hat and y         and we mathematically push it back through the
   nn in the other direction. we are learning from our mistakes.

   by taking the derivative of the functions we used during the first
   process, we try to discover what value we should give the weights in
   order to achieve the best possible prediction. essentially we want to
   know what the relationship is between the value of our weight and the
   error that we get out as the result.

   and after many epochs or iterations, the nn has learned to give us more
   accurate predictions by adapting its parameters to our dataset.
   [1*fhjyld96lkst1n1t4v9d7w.jpeg]
   overview of forward and backwards propagation

   this post was inspired by the week 1 challenge from the [21]bletchley
   machine learning bootcamp that started on the 7th of february. in the
   coming nine weeks, i   m one of 50 students who will go through the
   fundamentals of machine learning. every week we discuss a different
   topic and have to submit a challenge, which requires you to really
   understand the materials.

   if you have any questions or suggestions or, let [22]me know!

   or if you want to check out the whole code, you can find it [23]here on
   kaggle.
     __________________________________________________________________

   recommended videos to get a deeper understanding on neural networks:
     * [24]3blue1brown   s series on neural networks
     * [25]siraj raval   s series on deep learning

   thanks to [26]jannes klaas.
     * [27]machine learning
     * [28]deep learning
     * [29]technology
     * [30]tech
     * [31]neural networks

   (button)
   (button)
   (button) 1.92k claps
   (button) (button) (button) 16 (button) (button)

     (button) blockedunblock (button) followfollowing
   [32]go to the profile of daphne cornelisse

[33]daphne cornelisse

   neuroscience student at erasmus university college | instructor
   bletchley ml bootcamp

     (button) follow
   [34]freecodecamp.org

[35]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 1.92k
     * (button)
     *
     *

   [36]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [37]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/99239c4af5d3
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_kosapdmzkikk---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@daphn3cor?source=post_header_lockup
  12. https://medium.freecodecamp.org/@daphn3cor
  13. https://unsplash.com/photos/ok6bbld3b_0?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  14. https://unsplash.com/search/photos/network?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  15. https://towardsdatascience.com/machine-learning-101-supervised-unsupervised-reinforcement-beyond-f18e722069bc
  16. https://en.wikipedia.org/wiki/artificial_neural_network
  17. https://en.wikipedia.org/wiki/dot_product
  18. https://towardsdatascience.com/activation-functions-and-its-types-which-is-better-a9a5310cc8f
  19. https://en.wikipedia.org/wiki/cache_(computing)
  20. https://stackoverflow.com/questions/31155388/meaning-of-an-epoch-in-neural-networks-training
  21. https://ai-bootcamp.org/
  22. https://www.linkedin.com/in/daphnecornelisse/
  23. https://www.kaggle.com/daphnecor/week-1-3-layer-nn?scriptversionid=2495447
  24. https://www.youtube.com/watch?v=aircaruvnkk&list=plzhqobowtqdnu6r1_67000dx_zcjb-3pi
  25. https://www.youtube.com/watch?v=voppzhpvtiq&t=274s
  26. https://medium.com/@jannesklaas?source=post_page
  27. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  28. https://medium.freecodecamp.org/tagged/deep-learning?source=post
  29. https://medium.freecodecamp.org/tagged/technology?source=post
  30. https://medium.freecodecamp.org/tagged/tech?source=post
  31. https://medium.freecodecamp.org/tagged/neural-networks?source=post
  32. https://medium.freecodecamp.org/@daphn3cor?source=footer_card
  33. https://medium.freecodecamp.org/@daphn3cor
  34. https://medium.freecodecamp.org/?source=footer_card
  35. https://medium.freecodecamp.org/?source=footer_card
  36. https://medium.freecodecamp.org/
  37. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  39. https://medium.com/p/99239c4af5d3/share/twitter
  40. https://medium.com/p/99239c4af5d3/share/facebook
  41. https://medium.com/p/99239c4af5d3/share/twitter
  42. https://medium.com/p/99239c4af5d3/share/facebook
