   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]freecodecamp.org
     * [9]dev
     * [10]learn to code for free
     __________________________________________________________________

an intuitive introduction to id3 (gans)

   [11]go to the profile of thalles silva
   [12]thalles silva (button) blockedunblock (button) followfollowing
   jan 7, 2018
   [1*knwzxqjsy0tywiucd8tbgw.png]

warm up

   let   s say there   s a very cool party going on in your neighborhood that
   you really want to go to. but, there is a problem. to get into the
   party you need a special ticket         that was long sold out.

   wait up! isn   t this a id3 article? yes it
   is. but bear with me for now, it is going to be worth it.

   ok, since expectations are very high, the party organizers hired a
   qualified security agency. their primary goal is to not allow anyone to
   crash the party. to do that, they placed a lot of guards at the venue   s
   entrance to check everyone   s tickets for authenticity.

   since you don   t have any martial artistic gifts, the only way to get
   through is by fooling them with a very convincing fake ticket.

   there is a big problem with this plan though         you never actually saw
   how the ticket looks like.

   even if you design a ticket based on your creativity, it   s almost
   impossible to fool the guards at your first trial. besides, you can   t
   show your face until you have a very decent replica of the party   s
   pass.

   to help solve the problem, you decide to call your friend bob to do the
   job for you.

   bob   s mission is very simple. he will try to get into the party with
   your fake pass. if he gets denied, he will come back to you with useful
   tips on how the ticket should look like.

   based on that feedback, you make a new version of the ticket and hand
   it to bob, who goes to try again. this process keeps repeating until
   you become able to design a perfect replica.
   [0*qiugddbndmcrxpn5.jpg]
   that   s a must go party. i actually took that image from a fake ticket
   generator website!

   putting aside the    small holes    in this anecdote, this is pretty much
   how id3 (gans) work.

   nowadays, most of the applications of gans are in the field of computer
   vision. some of the applications include training [13]semi-supervised
   classifiers, and generating high resolution images from low resolution
   counterparts.

   this piece provides an introduction to gans with a hands-on approach to
   the problem of generating images. you can clone the notebook for this
   post [14]here.

id3

   [0*2smzp-1mdx2ttwu6.png]
   generative adversarial network framework.

   gans are generative models devised by [15]goodfellow et al. in 2014. in
   a gan setup, two differentiable functions, represented by neural
   networks, are locked in a game. the two players (the generator and the
   discriminator) have different roles in this framework.

   the generator tries to produce data that come from some id203
   distribution. that would be you trying to reproduce the party   s
   tickets.

   the discriminator acts like a judge. it gets to decide if the input
   comes from the generator or from the true training set. that would be
   the party   s security comparing your fake ticket with the true ticket to
   find flaws in your design.
   [1*nhvz3zlmsuy7qasufm2udw.gif]
   [1*bh7ipm7gggfkfpkvzp2vjq.gif]
   we used a 4 layer convolution network for (both discriminator and
   generator) with batch id172. the model was trained to generate
   svhns and mnist images. above, are the svhn   s (left) and mnist (right)
   generator samples during training.

   in summary, the game follows with:
     * the generator trying to maximize the id203 of making the
       discriminator mistakes its inputs as real.
     * and the discriminator guiding the generator to produce more
       realistic images.

   in the perfect equilibrium, the generator would capture the general
   training data distribution. as a result, the discriminator would be
   always unsure of whether its inputs are real or not.
   [1*mptoquskqb-jsagkqconva.png]
   adapted from the dcgan paper. the generator network implemented here.
   note the non-existence of fully connected and pooling layers.

   in the [16]dcgan paper, the authors describe the combination of some
   deep learning techniques as key for training gans. these techniques
   include: (i) the all convolutional net and (ii) batch id172
   (bn).

   the first emphasizes strided convolutions (instead of pooling layers)
   for both: increasing and decreasing feature   s spatial dimensions. and
   the second normalizes the feature vectors to have zero mean and unit
   variance in all layers. this helps to stabilize learning and to deal
   with poor weight initialization problems.

   without further ado, let   s dive into the implementation details and
   talk more about gans as we go. we present an implementation of a deep
   convolutional generative adversarial network (dcgan). our
   implementation uses tensorflow and follows some practices described in
   the [17]dcgan paper.

generator

   the network has 4 convolutional layers, all followed by bn (except for
   the output layer) and rectified linear unit (relu) activations.

   it takes as an input a random vector z (drawn from a normal
   distribution). after reshaping z to have a 4d shape, we feed it to the
   generator that starts a series of upsampling layers.

   each upsampling layer represents a transpose convolution operation with
   strides 2. transpose convolutions are similar to the regular
   convolutions.

   typically, regular convolutions go from wide and shallow layers to
   narrower and deeper ones. transpose convolutions go the other way. they
   go from deep and narrow layers to wider and shallower.

   the stride of a transpose convolution operation defines the size of the
   output layer. with    same    padding and stride of 2, the output features
   will have double the size of the input layer.

   that happens because, every time we move one pixel in the input layer,
   we move the convolution kernel by two pixels on the output layer. in
   other words, each pixel in the input image is used to draw a square in
   the output image.
   [1*mxlnesh3cg63ayq1nm-m1q.png]
   transpose convolving a 3x3 kernel over a 2x2 input with stride 2 is
   equivalent to convolving a 3x3 kernel over a 5x5 input with stride 2.
   using no padding    valid   , for both.

   in short, the generator begins with this very deep but narrow input
   vector. after each transpose convolution, z becomes wider and
   shallower. all transpose convolutions use a 5x5 kernel   s size with
   depths reducing from 512 all the way down to 3         representing an rgb
   color image.
def transpose_conv2d(x, output_space):
    return tf.layers.conv2d_transpose(x, output_space,
      kernel_size=5, strides=2, padding='same',
      kernel_initializer=tf.random_normal_initializer(mean=0.0,
                                                      stddev=0.02))

   the final layer outputs a 32x32x3 tensor         squashed between values of
   -1 and 1 through the [18]hyperbolic tangent (tanh) function.

   this final output shape is defined by the size of the training images.
   in this case, if training for svhn, the generator produces 32x32x3
   images. however, if training for mnist, it would generate a 28x28
   greyscale image.

   finally, note that before feeding the input vector z to the generator,
   we need to scale it to the interval of -1 to 1. that is to follow the
   choice of using the tanh function.
def generator(z, output_dim, reuse=false, alpha=0.2, training=true):
    """
    defines the generator network
    :param z: input random vector z
    :param output_dim: output dimension of the network
    :param reuse: indicates whether or not the existing model variables should b
e used or recreated
    :param alpha: scalar for lrelu activation function
    :param training: boolean for controlling the batch id172 statistics
    :return: model's output
    """
    with tf.variable_scope('generator', reuse=reuse):
        fc1 = dense(z, 4*4*512)
        # reshape it to start the convolutional stack
        fc1 = tf.reshape(fc1, (-1, 4, 4, 512))
        fc1 = batch_norm(fc1, training=training)
        fc1 = tf.nn.relu(fc1)
        t_conv1 = transpose_conv2d(fc1, 256)
        t_conv1 = batch_norm(t_conv1, training=training)
        t_conv1 = tf.nn.relu(t_conv1)
        t_conv2 = transpose_conv2d(t_conv1, 128)
        t_conv2 = batch_norm(t_conv2, training=training)
        t_conv2 = tf.nn.relu(t_conv2)
        logits = transpose_conv2d(t_conv2, output_dim)
        out = tf.tanh(logits)
        return out

discriminator

   the discriminator is also a 4 layer id98 with bn (except its input
   layer) and leaky relu activations. many id180 will work
   fine with this basic gan architecture. however, leaky relus are very
   popular because they help the gradients flow easier through the
   architecture.

   a regular relu function works by truncating negative values to 0. this
   has the effect of blocking the gradients to flow through the network.
   instead of the function being zero, leaky relus allow a small negative
   value to pass through. that is, the function computes the greatest
   value between the features and a small factor.
def lrelu(x, alpha=0.2):
     # non-linear activation function
    return tf.maximum(alpha * x, x)

   leaky relus represent an attempt to solve the dying relu problem. this
   situation occurs when the neurons get stuck in a state in which relu
   units always output 0s for all inputs. for these cases, the gradients
   are completely shut to flow back through the network.

   this is especially important for gans since the only way the generator
   has to learn is by receiving the gradients from the discriminator.
   [1*har7vvlhcwiqc8vmwslhyw.png]
   [1*o3jyollpmelqakqdtiaqxg.png]
   (left) relu, (right) leaky relu id180. note that leaky
   relus allows a small slope when x is negative.

   the discriminator starts by receives a 32x32x3 image tensor. opposite
   to the generator, the discriminator performs a series of strided 2
   convolutions. each, works by reducing the feature vector   s spatial
   dimensions by half its size, also doubling the number of learned
   filters.

   finally, the discriminator needs to output probabilities. for that, we
   use the logistic sigmoid activation function on the final logits.
def discriminator(x, reuse=false, alpha=0.2, training=true):
    """
    defines the discriminator network
    :param x: input for network
    :param reuse: indicates whether or not the existing model variables should b
e used or recreated
    :param alpha: scalar for lrelu activation function
    :param training: boolean for controlling the batch id172 statistics
    :return: a tuple of (sigmoid probabilities, logits)
    """
    with tf.variable_scope('discriminator', reuse=reuse):
        # input layer is 32x32x?
        conv1 = conv2d(x, 64)
        conv1 = lrelu(conv1, alpha)
        conv2 = conv2d(conv1, 128)
        conv2 = batch_norm(conv2, training=training)
        conv2 = lrelu(conv2, alpha)
        conv3 = conv2d(conv2, 256)
        conv3 = batch_norm(conv3, training=training)
        conv3 = lrelu(conv3, alpha)
        # flatten it
        flat = tf.reshape(conv3, (-1, 4*4*256))
        logits = dense(flat, 1)
        out = tf.sigmoid(logits)
        return out, logits

   note that in this framework, the discriminator acts as a regular binary
   classifier. half of the time it receives images from the training set
   and the other half from the generator.

   back to our adventure, to reproduce the party   s ticket, the only source
   of information you had was the feedback from our friend bob. in other
   words, the quality of the feedback bob provided to you at each trial
   was essential to get the job done.

   in the same way, every time the discriminator notices a difference
   between the real and fake images, it sends a signal to the generator.
   this signal is the gradient that flows backwards from the discriminator
   to the generator. by receiving it, the generator is able to adjust its
   parameters to get closer to the true data distribution.

   this is how important the discriminator is. in fact, the generator will
   be as good as producing data as the discriminator is at telling them
   apart.

losses

   now, let   s describe the trickiest part of this architecture         the
   losses. first, we know the discriminator receives images from both the
   training set and the generator.

   we want the discriminator to be able to distinguish between real and
   fake images. every time we run a mini-batch through the discriminator,
   we get logits. these are the unscaled values from the model.

   however, we can divide the mini-batches that the discriminator receives
   in two types. the first, composed only with real images that come from
   the training set and the second, with only fake images         the ones
   created by the generator.
def model_loss(input_real, input_z, output_dim, alpha=0.2, smooth=0.1):
    """
    get the loss for the discriminator and generator
    :param input_real: images from the real dataset
    :param input_z: random vector z
    :param out_channel_dim: the number of channels in the output image
    :param smooth: label smothing scalar
    :return: a tuple of (discriminator loss, generator loss)
    """
    g_model = generator(input_z, output_dim, alpha=alpha)
    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)
    d_model_fake, d_logits_fake = discriminator(g_model, reuse=true, alpha=alpha
)
    # for the real images, we want them to be classified as positives,
    # so we want their labels to be all ones.
    # notice here we use label smoothing for helping the discriminator to genera
lize better.
    # label smoothing works by avoiding the classifier to make extreme predictio
ns when extrapolating.
    d_loss_real = tf.reduce_mean(
        tf.nn.sigmoid_cross_id178_with_logits(logits=d_logits_real, labels=tf.
ones_like(d_logits_real) * (1 - smooth)))
    # for the fake images produced by the generator, we want the discriminator t
o clissify them as false images,
    # so we set their labels to be all zeros.
    d_loss_fake = tf.reduce_mean(
        tf.nn.sigmoid_cross_id178_with_logits(logits=d_logits_fake, labels=tf.
zeros_like(d_model_fake)))
    # since the generator wants the discriminator to output 1s for its images, i
t uses the discriminator logits for the
    # fake images and assign labels of 1s to them.
    g_loss = tf.reduce_mean(
        tf.nn.sigmoid_cross_id178_with_logits(logits=d_logits_fake, labels=tf.
ones_like(d_model_fake)))
    d_loss = d_loss_real + d_loss_fake
    return d_loss, g_loss

   because both networks train at the same time, gans also need two
   optimizers. each one for minimizing the discriminator and generator   s
   id168s respectively.

   we want the discriminator to output probabilities close to 1 for real
   images and near 0 for fake images. to do that, the discriminator needs
   two losses. therefore, the total loss for the discriminator is the sum
   of these two partial losses. one for maximizing the probabilities for
   the real images and another for minimizing the id203 of fake
   images.
   [1*clrpcazrr51mgy4uswa6sw.png]
   comparing real (left) and generated (right) svhn sample images.
   although some images look blurred and some others are difficult to
   recognize, it   s noticeable that the data distribution was captured by
   the model.

   in the beginning of training two interesting situations occur. first,
   the generator does not know how to create images that resembles the
   ones from the training set. and second, discriminator does not know how
   to categorize the images it receives as real or fake.

   as a result, the discriminator receives two very distinct types of
   batches. one, composed of true images from the training set and another
   containing very noisy signals. as training progresses, the generator
   starts to output images that look closer to the images from the
   training set. that happens, because the generator trains to learn the
   data distribution that composes the training set images.

   at the same time, the discriminator starts to get real good at
   classifying samples as real or fake. as a consequence, the two types of
   mini-batches begin looking similar, in structure, to one another. that,
   as a result makes the discriminator unable to identify images as real
   or fake.

   for the losses, we use vanilla cross-id178 with adam as a good choice
   for the optimizer.
   [1*0klhwo7jnfnnsw7-7yxqnw.png]
   comparing real (left) and generated (right) mnist sample images.
   because mnist images have a simpler data structure, the model was able
   to produce more realistic samples when compared to the svhns.

concluding

   gans are one of the hottest subjects in machine learning right now.
   these models have the potential of unlocking unsupervised learning
   methods that would expand ml to new horizons.

   since its creation, researches have been developing many techniques for
   training gans. in improved techniques for training gans, the authors
   describe state-of-the-art techniques for both image generation and
   semi-supervised learning.

   if you are curious to dig deeper in these subjects, i recommend reading
   [19]generative models.

   also, take a look at:
   [20]dive head first into advanced gans: exploring self-attention and
   spectral norm
   lately, generative models are drawing a lot of attention. much of that
   comes from id3   medium.freecodecamp.org
   [21]semi-supervised learning with id3
   (gans)
   if you ever heard or studied about deep learning, you probably heard
   about mnist, svhn, id163, pascalvoc and
   others   towardsdatascience.com

   and if you need more, that is my [22]deep learning blog.

   enjoy, and thanks for reading!
   [1*rzohqzrjeirb5zy0njacvg.gif]

   credits to [23]sam williams for this awesome    clap    gif! check it out
   in [24]his post.

     * [25]machine learning
     * [26]generative adversarial
     * [27]deep learning
     * [28]tensorflow
     * [29]tech

   (button)
   (button)
   (button) 3.7k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of thalles silva

[31]thalles silva

   id161 & deep learning. personal blog:
   [32]https://sthalles.github.io/

     (button) follow
   [33]freecodecamp.org

[34]freecodecamp.org

   stories worth reading about programming and technology from our open
   source community.

     * (button)
       (button) 3.7k
     * (button)
     *
     *

   [35]freecodecamp.org
   never miss a story from freecodecamp.org, when you sign up for medium.
   [36]learn more
   never miss a story from freecodecamp.org
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.freecodecamp.org/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/7a2264a81394
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394&source=--------------------------nav_reg&operation=register
   8. https://medium.freecodecamp.org/?source=logo-lo_khgmexrz047w---336d898217ee
   9. https://medium.freecodecamp.org/tagged/web-development
  10. https://www.freecodecamp.com/?ref=mn
  11. https://medium.freecodecamp.org/@thalles.silva?source=post_header_lockup
  12. https://medium.freecodecamp.org/@thalles.silva
  13. https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e
  14. https://github.com/sthalles/blog-resources/blob/master/dcgan/dcgan.ipynb
  15. https://arxiv.org/abs/1406.2661
  16. https://arxiv.org/abs/1511.06434
  17. https://arxiv.org/abs/1511.06434
  18. https://reference.wolfram.com/language/ref/tanh.html
  19. https://blog.openai.com/generative-models/#gan
  20. https://medium.freecodecamp.org/dive-head-first-into-advanced-gans-exploring-self-attention-and-spectral-norm-d2f7cdb55ede
  21. https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e
  22. https://sthalles.github.io/
  23. https://medium.com/@samwsoftware
  24. https://medium.freecodecamp.org/want-more-claps-and-followers-how-to-make-a-clap-me-gif-in-5-minutes-db85a24950f6
  25. https://medium.freecodecamp.org/tagged/machine-learning?source=post
  26. https://medium.freecodecamp.org/tagged/generative-adversarial?source=post
  27. https://medium.freecodecamp.org/tagged/deep-learning?source=post
  28. https://medium.freecodecamp.org/tagged/tensorflow?source=post
  29. https://medium.freecodecamp.org/tagged/tech?source=post
  30. https://medium.freecodecamp.org/@thalles.silva?source=footer_card
  31. https://medium.freecodecamp.org/@thalles.silva
  32. https://sthalles.github.io/
  33. https://medium.freecodecamp.org/?source=footer_card
  34. https://medium.freecodecamp.org/?source=footer_card
  35. https://medium.freecodecamp.org/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.freecodecamp.org/dive-head-first-into-advanced-gans-exploring-self-attention-and-spectral-norm-d2f7cdb55ede
  39. https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e
  40. https://medium.com/p/7a2264a81394/share/twitter
  41. https://medium.com/p/7a2264a81394/share/facebook
  42. https://medium.com/p/7a2264a81394/share/twitter
  43. https://medium.com/p/7a2264a81394/share/facebook
