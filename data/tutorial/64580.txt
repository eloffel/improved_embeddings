   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]usf-data science
     * [9]about
     * [10]news
     * [11]student spotlight
     * [12]data science certificate and degree programs
     __________________________________________________________________

deep learning best practices (1)         weight initialization

   [13]go to the profile of neerja doshi
   [14]neerja doshi (button) blockedunblock (button) followfollowing
   mar 26, 2018

   basics, weight initialization pitfalls & best practices
   [1*nfv5fpje_8wsgxsn1gg3gw.jpeg]
   [15]https://pixabay.com/photo-1600668/

motivation

   as a beginner at deep learning, one of the things i realized is that
   there isn   t much online documentation that covers all the deep learning
   tricks in one place. there are lots of small best practices, ranging
   from simple tricks like initializing weights, id173 to
   slightly complex techniques like cyclic learning rates that can make
   training and debugging neural nets easier and efficient. this inspired
   me to write this series of blogs where i will cover as many nuances as
   i can to make implementing deep learning simpler for you.

   while writing this blog, the assumption is that you have a basic idea
   of how neural networks are trained. an understanding of weights,
   biases, hidden layers, activations and [16]id180 will
   make the content clearer. i would recommend [17]this course if you wish
   to build a basic foundation of deep learning.

   note         whenever i refer to layers of a neural network, it implies the
   layers of a simple neural network, i.e. the fully connected layers. of
   course some of the methods i talk about apply to convolutional and
   recurrent neural networks as well. in this blog i am going to talk
   about the issues related to initialization of weight matrices and ways
   to mitigate them. before that, let   s just cover some basics and
   notations that we will be using going forward.

basics and notations

   consider an l layer neural network, which has l-1 hidden layers and 1
   output layer. the parameters (weights and biases) of the layer l are
   represented as
   [1*ubpojod6iatwm2vdma3jbg.png]

   in addition to weights and biases, during the training process,
   following intermediate variables are computed
   [1*xtkal0jovwfoso90zoga1q.png]

   training a neural network consists of 4 steps:
    1. initialize weights and biases.
    2. forward propagation: using the input x, weights w and biases b, for
       every layer we compute z and a. at the final layer, we compute
       f(a^(l-1)) which could be a sigmoid, softmax or linear function of
       a^(l-1) and this gives the prediction y_hat.
    3. compute the id168: this is a function of the actual label y
       and predicted label y_hat. it captures how far off our predictions
       are from the actual target. our objective is to minimize this loss
       function.
    4. backward propagation: in this step, we calculate the gradients of
       the id168 f(y, y_hat) with respect to a, w, and b called
       da, dw and db. using these gradients we update the values of the
       parameters from the last layer to the first.
    5. repeat steps 2   4 for n iterations/epochs till we feel we have
       minimized the id168, without overfitting the train data
       (more on this later!)

   here   s a quick look at steps 2 , 3 and 4 for a network with 2 layers,
   i.e. one hidden layer. (note that i haven   t added the bias terms here
   for simplicity):
   [1*xwlcmkdcosywiacto2ccqw.png]
   forward propagation
   [1*5a9bmihgt_rx8piz_ybtsg.png]
   backward propagation

initializing weights w

   one of the starting points to take care of while building your network
   is to initialize your weight matrix correctly. let us consider 2
   scenarios that can cause issues while training the model:

1. initializing all weights to 0

   let   s just put it out there         this makes your model equivalent to a
   linear model. when you set all weight to 0, the derivative with respect
   to id168 is the same for every w in w^l, thus, all the weights
   have the same values in the subsequent iteration. this makes the hidden
   units symmetric and continues for all the n iterations you run. thus
   setting weights to zero makes your network no better than a linear
   model. it is important to note that setting biases to 0 will not create
   any troubles as non zero weights take care of breaking the symmetry and
   even if bias is 0, the values in every neuron are still different.

2. initializing weights randomly

   initializing weights randomly, following standard normal distribution
   (np.random.randn(size_l, size_l-1) in python) while working with a
   (deep) network can potentially lead to 2 issues         vanishing gradients
   or exploding gradients.

   a) vanishing gradients         in case of deep networks, for any activation
   function, abs(dw) will get smaller and smaller as we go backwards with
   every layer during back propagation. the earlier layers are the slowest
   to train in such a case.

     the weight update is minor and results in slower convergence. this
     makes the optimization of the id168 slow. in the worst case,
     this may completely stop the neural network from training further.

   more specifically, in case of sigmoid(z) and tanh(z), if your weights
   are large, then the gradient will be vanishingly small, effectively
   preventing the weights from changing their value. this is because
   abs(dw) will increase very slightly or possibly get smaller and smaller
   every iteration. with relu(z) vanishing gradients are generally not a
   problem as the gradient is 0 for negative (and zero) inputs and 1 for
   positive inputs.

   b) exploding gradients         this is the exact opposite of vanishing
   gradients. consider you have non-negative and large weights and small
   activations a (as can be the case for sigmoid(z)). when these weights
   are multiplied along the layers, they cause a large change in the cost.
   thus, the gradients are also going to be large. this means that the
   changes in w, by w             * dw, will be in huge steps, the downward moment
   will increase.

     this may result in oscillating around the minima or even
     overshooting the optimum again and again and the model will never
     learn!

   another impact of exploding gradients is that huge values of the
   gradients may cause number overflow resulting in incorrect computations
   or introductions of nan   s. this might also lead to the loss taking the
   value nan.

best practices

   1. using relu/ leaky relu as the activation function, as it is
   relatively robust to the vanishing/exploding gradient issue (especially
   for networks that are not too deep). in the case of leaky relu   s, they
   never have 0 gradient. thus they never die and training continues.

   2. for deep networks, we can use a heuristic to initialize the weights
   depending on the non-linear activation function. here, instead of
   drawing from standard normal distribution, we are drawing w from normal
   distribution with variance k/n, where k depends on the activation
   function. while these heuristics do not completely solve the
   exploding/vanishing gradients issue, they help mitigate it to a great
   extent. the most common are:

   a) for relu(z)         we multiply the randomly generated values of w by:
   [1*-vy3g0w-4njo-dq1jm0p0w.png]

   b) for tanh(z)         the heuristic is called xavier initialization. it is
   similar to the previous one, except that k is 1 instead of 2.
   [1*el7fg2km4zmrcv9w7diftg.png]

   in tensorflow w = tf.get_variable('w', [dims], initializer) where
   initializer = tf.contrib.layers.xavier_initializer()

   c) another commonly used heuristic is:
   [1*r6ufrz9qndzruz5ytrhaaa.png]

   these serve as good starting points for initialization and mitigate the
   chances of exploding or vanishing gradients. they set the weights
   neither too much bigger that 1, nor too much less than 1. so, the
   gradients do not vanish or explode too quickly. they help avoid slow
   convergence, also ensuring that we do not keep oscillating off the
   minima. there exist other variants of the above, where the main
   objective again is to minimize the variance of the parameters.

   3. gradient clipping         this is another way of dealing with the
   exploding gradient problem. we set a threshold value, and if a chosen
   function of a gradient is larger than this threshold, we set it to
   another value. for example, normalize the gradients when the l2 norm
   exceeds a certain threshold    w = w * threshold / l2_norm(w) if
   l2_norm(w) > threshold

   an important point to note is that we have talked about various
   initializations of w, but not the biases b. this is because the
   gradients with respect to bias depend only on the linear activation of
   that layer, and not on the gradients of the deeper layers. thus there
   is no diminishing or explosion of gradients for the bias terms. as
   mentioned earlier, they can be safely initialized to 0.

conclusion

   in this blog, we   ve covered weight initialization pitfalls and some
   mitigation techniques. if i have missed any other useful insights
   related to this topic, i would be happy to learn it from you! in the
   next blog, i will be talking about id173 methods to reduce
   overfitting and gradient checking         a trick to make debugging simpler!

references

    1. [18]https://www.coursera.org/learn/deep-neural-network/lecture/rwqy
       e/weight-initialization-for-deep-networks
    2. [19]neural networks: training with id26 - jeremy jordan
    3. [20]a gentle introduction to exploding gradients in neural networks
       by jason brownlee
    4. [21]vanishing gradient problem
    5. [22]https://www.quora.com/why-is-it-a-problem-to-have-exploding-gra
       dients-in-a-neural-net-especially-in-an-id56

   about me: graduated with ms data science at usf and undergrad in
   computer science, i have 2 years of experience in building predictive
   and recommendation algorithms, and deriving business insights for
   finance and retail clients. i am excited about opportunities for
   applying my machine learning and deep learning knowledge to real-world
   problems.
   do check out my other blogs [23]here!
   linkedin : [24]https://www.linkedin.com/in/neerja-doshi/

     * [25]machine learning
     * [26]deep learning
     * [27]data science
     * [28]neural networks
     * [29]weight initialization

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 3 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of neerja doshi

[31]neerja doshi

   data scientist @ aws, ms data science @ usf,
   [32]https://www.linkedin.com/in/neerja-doshi/

     (button) follow
   [33]usf-data science

[34]usf-data science

   established in 2016, the data institute at usf serves as the umbrella
   organization for data science research and programming at the
   university of san francisco. we offer ms data science, bs data science
   and continuing education certificates.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [35]usf-data science
   never miss a story from usf-data science, when you sign up for medium.
   [36]learn more
   never miss a story from usf-data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/14e5c0295b94
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94&source=--------------------------nav_reg&operation=register
   8. https://medium.com/usf-msds?source=logo-lo_nu4zbycqxliy---e39f862bc3e2
   9. https://medium.com/usf-msds/about
  10. https://medium.com/usf-msds/archive
  11. https://medium.com/usf-msds/tagged/students
  12. https://www.usfca.edu/data-institute
  13. https://medium.com/@neerja.doshi?source=post_header_lockup
  14. https://medium.com/@neerja.doshi
  15. https://pixabay.com/photo-1600668/
  16. https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0
  17. https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome
  18. https://www.coursera.org/learn/deep-neural-network/lecture/rwqye/weight-initialization-for-deep-networks
  19. https://www.jeremyjordan.me/neural-networks-training/
  20. https://machinelearningmastery.com/exploding-gradients-in-neural-networks/
  21. https://www.wikiwand.com/en/vanishing_gradient_problem
  22. https://www.quora.com/why-is-it-a-problem-to-have-exploding-gradients-in-a-neural-net-especially-in-an-id56
  23. https://medium.com/@neerja.doshi
  24. https://www.linkedin.com/in/neerja-doshi/
  25. https://medium.com/tag/machine-learning?source=post
  26. https://medium.com/tag/deep-learning?source=post
  27. https://medium.com/tag/data-science?source=post
  28. https://medium.com/tag/neural-networks?source=post
  29. https://medium.com/tag/weight-initialization?source=post
  30. https://medium.com/@neerja.doshi?source=footer_card
  31. https://medium.com/@neerja.doshi
  32. https://www.linkedin.com/in/neerja-doshi/
  33. https://medium.com/usf-msds?source=footer_card
  34. https://medium.com/usf-msds?source=footer_card
  35. https://medium.com/usf-msds
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/14e5c0295b94/share/twitter
  39. https://medium.com/p/14e5c0295b94/share/facebook
  40. https://medium.com/p/14e5c0295b94/share/twitter
  41. https://medium.com/p/14e5c0295b94/share/facebook
