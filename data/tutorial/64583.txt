   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

topic modelling in python with nltk and gensim

   [16]go to the profile of susan li
   [17]susan li (button) blockedunblock (button) followfollowing
   mar 30, 2018
   [1*4lfelzvxp3uzgc5xa110zw.png]

   in this post, we will learn how to identity which topic is discussed in
   a document, called topic modelling. in particular, we will cover
   [18]id44 (lda): a widely used topic modelling
   technique. and we will apply lda to convert set of research papers to a
   set of topics.

   research paper topic modelling is an unsupervised machine learning
   method that helps us discover hidden semantic structures in a paper,
   that allows us to learn topic representations of papers in a corpus.
   the model can be applied to any kinds of labels on documents, such as
   tags on posts on the website.

the process

     * we pick the number of topics ahead of time even if we   re not sure
       what the topics are.
     * each document is represented as a distribution over topics.
     * each topic is represented as a distribution over words.

   the research paper text data is just a bunch of unlabeled texts and can
   be found [19]here.

text cleaning

   we use the following function to clean our texts and return a list of
   tokens:
import spacy
spacy.load('en')
from spacy.lang.en import english
parser = english()
def tokenize(text):
    lda_tokens = []
    tokens = parser(text)
    for token in tokens:
        if token.orth_.isspace():
            continue
        elif token.like_url:
            lda_tokens.append('url')
        elif token.orth_.startswith('@'):
            lda_tokens.append('screen_name')
        else:
            lda_tokens.append(token.lower_)
    return lda_tokens

   we use [20]nltk   s id138 to find the meanings of words, synonyms,
   antonyms, and more. in addition, we use [21]id138lemmatizer to get
   the root word.
import nltk
nltk.download('id138')
from nltk.corpus import id138 as wn
def get_lemma(word):
    lemma = wn.morphy(word)
    if lemma is none:
        return word
    else:
        return lemma

from nltk.stem.id138 import id138lemmatizer
def get_lemma2(word):
    return id138lemmatizer().lemmatize(word)

   filter out stop words:
nltk.download('stopwords')
en_stop = set(nltk.corpus.stopwords.words('english'))

   now we can define a function to prepare the text for topic modelling:
def prepare_text_for_lda(text):
    tokens = tokenize(text)
    tokens = [token for token in tokens if len(token) > 4]
    tokens = [token for token in tokens if token not in en_stop]
    tokens = [get_lemma(token) for token in tokens]
    return tokens

   open up our data, read line by line, for each line, prepare text for
   lda, then add to a list.

   now we can see how our text data are converted:
import random
text_data = []
with open('dataset.csv') as f:
    for line in f:
        tokens = prepare_text_for_lda(line)
        if random.random() > .99:
            print(tokens)
            text_data.append(tokens)

   [   sociocrowd   ,    social   ,    network   ,    base   ,    framework   ,    crowd   ,
      simulation   ]
   [   detection   ,    technique   ,    clock   ,    recovery   ,    application   ]
   [   voltage   ,    syllabic   ,    companding   ,    domain   ,    filter   ]
   [   perceptual   ,    base   ,    coding   ,    decision   ]
   [   cognitive   ,    mobile   ,    virtual   ,    network   ,    operator   ,    investment   ,
      pricing   ,    supply   ,    uncertainty   ]
   [   id91   ,    query   ,    search   ,    engine   ]
   [   psychological   ,    engagement   ,    enterprise   ,    starting   ,    london   ]
   [   10-bit   ,    200-ms   ,    digitally   ,    calibrate   ,    pipelined   ,    using   ,
      switching   ,    opamps   ]
   [   optimal   ,    allocation   ,    resource   ,    distribute   ,    information   ,
      network   ]
   [   modeling   ,    synaptic   ,    plasticity   ,    within   ,    network   ,    highly   ,
      accelerate   ,    i&amp;f   ,    neuron   ]
   [   tile   ,    interleave   ,    multi   ,    level   ,    discrete   ,    wavelet   ,
      transform   ]
   [   security   ,    cross   ,    layer   ,    protocol   ,    wireless   ,    sensor   ,
      network   ]
   [   objectivity   ,    industrial   ,    exhibit   ]
   [   balance   ,    packet   ,    discard   ,    improve   ,    performance   ,    network   ]
   [   bodyqos   ,    adaptive   ,    radio   ,    agnostic   ,    sensor   ,    network   ]
   [   design   ,    reliability   ,    methodology   ]
   [   context   ,    aware   ,    image   ,    semantic   ,    extraction   ,    social   ]
   [   computation   ,    unstable   ,    limit   ,    cycle   ,    large   ,    scale   ,
      power   ,    system   ,    model   ]
   [   photon   ,    density   ,    estimation   ,    using   ,    multiple   ,    importance   ,
      sampling   ]
   [   approach   ,    joint   ,    blind   ,    space   ,    equalization   ,    estimation   ]
   [   unify   ,    quadratic   ,    programming   ,    approach   ,    mix   ,    placement   ]

lda with gensim

   first, we are creating a dictionary from the data, then convert to
   [22]bag-of-words corpus and save the dictionary and corpus for future
   use.
from gensim import corpora
dictionary = corpora.dictionary(text_data)corpus = [dictionary.doc2bow(text) for
 text in text_data]
import pickle
pickle.dump(corpus, open('corpus.pkl', 'wb'))
dictionary.save('dictionary.gensim')

   we are asking lda to find 5 topics in the data:
import gensim
num_topics = 5
ldamodel = gensim.models.ldamodel.ldamodel(corpus, num_topics = num_topics,  w
ord=dictionary, passes=15)
ldamodel.save('model5.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

   (0,    0.034*   processor    + 0.019*   database    + 0.019*   issue    +
   0.019*   overview      )
   (1,    0.051*   computer    + 0.028*   design    + 0.028*   graphics    +
   0.028*   gallery      )
   (2,    0.050*   management    + 0.027*   object    + 0.027*   circuit    +
   0.027*   efficient      )
   (3,    0.019*   cognitive    + 0.019*   radio    + 0.019*   network    +
   0.019*   distribute      )
   (4,    0.029*   circuit    + 0.029*   system    + 0.029*   rigorous    +
   0.029*   integration      )

   topic 0 includes words like    processor   ,    database   ,    issue    and
      overview   , sounds like a topic related to database. topic 1 includes
   words like    computer   ,    design   ,    graphics    and    gallery   , it is
   definite a graphic design related topic. topic 2 includes words like
      management   ,    object   ,    circuit    and    efficient   , sounds like a
   corporate management related topic. and so on.

   with lda, we can see that different document with different topics, and
   the discriminations are obvious.

   let   s try a new document:
new_doc = 'practical bayesian optimization of machine learning algorithms'
new_doc = prepare_text_for_lda(new_doc)
new_doc_bow = dictionary.doc2bow(new_doc)
print(new_doc_bow)
print(ldamodel.get_document_topics(new_doc_bow))

   [(38, 1), (117, 1)]
   [(0, 0.06669136), (1, 0.40170625), (2, 0.06670282), (3, 0.39819494),
   (4, 0.066704586)]

   my new document is about machine learning algorithms, the lda out put
   shows that topic 1 has the highest id203 assigned, and topic 3
   has the second highest id203 assigned. we agreed!

   remember that the above 5 probabilities add up to 1.

   now we are asking lda to find 3 topics in the data:
ldamodel = gensim.models.ldamodel.ldamodel(corpus, num_topics = 3,  word=dicti
onary, passes=15)
ldamodel.save('model3.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

   (0,    0.029*   processor    + 0.016*   management    + 0.016*   aid    +
   0.016*   algorithm      )
   (1,    0.026*   radio    + 0.026*   network    + 0.026*   cognitive    +
   0.026*   efficient      )
   (2,    0.029*   circuit    + 0.029*   distribute    + 0.016*   database    +
   0.016*   management      )

   we can also find 10 topics:
ldamodel = gensim.models.ldamodel.ldamodel(corpus, num_topics = 10,  word=dict
ionary, passes=15)
ldamodel.save('model10.gensim')
topics = ldamodel.print_topics(num_words=4)
for topic in topics:
    print(topic)

   (0,    0.055*   database    + 0.055*   system    + 0.029*   technical    +
   0.029*   recursive      )
   (1,    0.038*   distribute    + 0.038*   graphics    + 0.038*   regenerate    +
   0.038*   exact      )
   (2,    0.055*   management    + 0.029*   multiversion    + 0.029*   reference    +
   0.029*   document      )
   (3,    0.046*   circuit    + 0.046*   object    + 0.046*   generation    +
   0.046*   transformation      )
   (4,    0.008*   programming    + 0.008*   circuit    + 0.008*   network    +
   0.008*   surface      )
   (5,    0.061*   radio    + 0.061*   cognitive    + 0.061*   network    +
   0.061*   connectivity      )
   (6,    0.085*   programming    + 0.008*   circuit    + 0.008*   subdivision    +
   0.008*   management      )
   (7,    0.041*   circuit    + 0.041*   design    + 0.041*   processor    +
   0.041*   instruction      )
   (8,    0.055*   computer    + 0.029*   efficient    + 0.029*   channel    +
   0.029*   cooperation      )
   (9,    0.061*   stimulation    + 0.061*   sensor    + 0.061*   retinal    +
   0.061*   pixel      )

pyldavis

   [23]pyldavis is designed to help users interpret the topics in a topic
   model that has been fit to a corpus of text data. the package extracts
   information from a fitted lda topic model to inform an interactive
   web-based visualization.

   visualizing 5 topics:
dictionary = gensim.corpora.dictionary.load('dictionary.gensim')
corpus = pickle.load(open('corpus.pkl', 'rb'))
lda = gensim.models.ldamodel.ldamodel.load('model5.gensim')
import pyldavis.gensim
lda_display = pyldavis.gensim.prepare(lda, corpus, dictionary, sort_topics=false
)
pyldavis.display(lda_display)

   [1*5usdzltz8qxa-lriickggw.png]
   figure 1

   saliency: a measure of how much the term tells you about the topic.

   relevance: a weighted average of the id203 of the word given the
   topic and the word given the topic normalized by the id203 of the
   topic.

   the size of the bubble measures the importance of the topics, relative
   to the data.

   first, we got the most salient terms, means terms mostly tell us about
   what   s going on relative to the topics. we can also look at individual
   topic.

   visualizing 3 topics:
lda3 = gensim.models.ldamodel.ldamodel.load('model3.gensim')
lda_display3 = pyldavis.gensim.prepare(lda3, corpus, dictionary, sort_topics=fal
se)
pyldavis.display(lda_display3)

   [1*rmemaawfolkdu7y5ba7uza.png]
   figure 2

   visualizing 10 topics:
lda10 = gensim.models.ldamodel.ldamodel.load('model10.gensim')
lda_display10 = pyldavis.gensim.prepare(lda10, corpus, dictionary, sort_topics=f
alse)
pyldavis.display(lda_display10)

   [1*jpyutxwuzxw7q11hnt5agq.png]
   figure 3

   when we have 5 or 10 topics, we can see certain topics are clustered
   together, this indicates the similarity between topics. what a a nice
   way to visualize what we have done thus far!

   try it out, find a text dataset, remove the label if it is labeled, and
   build a topic model yourself!

   source code can be found on [24]github. i look forward to hearing any
   feedback or questions.

     * [25]machine learning
     * [26]id96
     * [27]nlp
     * [28]python
     * [29]data visualization

   (button)
   (button)
   (button) 1.4k claps
   (button) (button) (button) 20 (button) (button)

     (button) blockedunblock (button) followfollowing
   [30]go to the profile of susan li

[31]susan li

   becoming an expert in ml, nlp, data story telling and encouraging
   others to do the same. sr data scientist, toronto canada.
   [32]https://www.linkedin.com/in/susanli/

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.4k
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/4ef03213cd21
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_20k6jlqecbe0---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@actsusanli?source=post_header_lockup
  17. https://towardsdatascience.com/@actsusanli
  18. https://en.wikipedia.org/wiki/latent_dirichlet_allocation
  19. https://github.com/susanli2016/machine-learning-with-python/blob/master/dataset.csv
  20. http://www.nltk.org/howto/id138.html
  21. http://www.nltk.org/_modules/nltk/stem/id138.html
  22. https://en.wikipedia.org/wiki/bag-of-words_model
  23. https://pypi.python.org/pypi/pyldavis/2.1.1
  24. https://github.com/susanli2016/machine-learning-with-python/blob/master/topic_modeling_gensim.ipynb
  25. https://towardsdatascience.com/tagged/machine-learning?source=post
  26. https://towardsdatascience.com/tagged/topic-modeling?source=post
  27. https://towardsdatascience.com/tagged/nlp?source=post
  28. https://towardsdatascience.com/tagged/python?source=post
  29. https://towardsdatascience.com/tagged/data-visualization?source=post
  30. https://towardsdatascience.com/@actsusanli?source=footer_card
  31. https://towardsdatascience.com/@actsusanli
  32. https://www.linkedin.com/in/susanli/
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/4ef03213cd21/share/twitter
  39. https://medium.com/p/4ef03213cd21/share/facebook
  40. https://medium.com/p/4ef03213cd21/share/twitter
  41. https://medium.com/p/4ef03213cd21/share/facebook
