   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

what are hyperparameters ? and how to tune the hyperparameters in a deep
neural network?

   [16]go to the profile of pranoy radhakrishnan
   [17]pranoy radhakrishnan (button) blockedunblock (button)
   followfollowing
   aug 9, 2017

   what are hyperparameters?

   hyperparameters are the variables which determines the network
   structure(eg: number of hidden units) and the variables which determine
   how the network is trained(eg: learning rate).

   hyperparameters are set before training(before optimizing the weights
   and bias).

hyperparameters related to network structure

number of hidden layers and units

   hidden layers are the layers between input layer and output layer.

      very simple. just keep adding layers until the test error does not
   improve anymore.   

   many hidden units within a layer with id173 techniques can
   increase accuracy. smaller number of units may cause underfitting.

dropout

   [0*doz6esaristenchf.png]
   random neurons are cancelled

   dropout is id173 technique to avoid overfitting (increase the
   validation accuracy) thus increasing the generalizing power.
     * generally, use a small dropout value of 20%-50% of neurons with 20%
       providing a good starting point. a id203 too low has minimal
       effect and a value too high results in under-learning by the
       network.
     * use a larger network. you are likely to get better performance when
       dropout is used on a larger network, giving the model more of an
       opportunity to learn independent representations.

network weight initialization

   ideally, it may be better to use different weight initialization
   schemes according to the activation function used on each layer.

   mostly uniform distribution is used.

activation function

   [0*7yk6yhluy4tlij0m.gif]
   sigmoid activation function

   id180 are used to introduce nonlinearity to models,
   which allows deep learning models to learn nonlinear prediction
   boundaries.

   generally, the rectifier activation function is the most popular.

   sigmoid is used in the output layer while making binary predictions.
   softmax is used in the output layer while making multi-class
   predictions.

hyperparameters related to training algorithm

learning rate

   [0*fa9umdxdzyzuopeo.jpg]
   learning rate

   the learning rate defines how quickly a network updates its parameters.

   low learning rate slows down the learning process but converges
   smoothly. larger learning rate speeds up the learning but may not
   converge.

   usually a decaying learning rate is preferred.

momentum

   momentum helps to know the direction of the next step with the
   knowledge of the previous steps. it helps to prevent oscillations. a
   typical choice of momentum is between 0.5 to 0.9.

number of epochs

   number of epochs is the number of times the whole training data is
   shown to the network while training.

   increase the number of epochs until the validation accuracy starts
   decreasing even when training accuracy is increasing(overfitting).

batch size

   mini batch size is the number of sub samples given to the network after
   which parameter update happens.

   a good default for batch size might be 32. also try 32, 64, 128, 256,
   and so on.

methods used to find out hyperparameters

    1. manual search
    2. grid search
       [18](http://machinelearningmastery.com/grid-search-hyperparameters-
       deep-learning-models-python-keras/)
    3. random search
    4. bayesian optimization

     * [19]machine learning
     * [20]deep lear
     * [21]artificial intelligence
     * [22]hyperparameters

   (button)
   (button)
   (button) 640 claps
   (button) (button) (button) 2 (button) (button)

     (button) blockedunblock (button) followfollowing
   [23]go to the profile of pranoy radhakrishnan

[24]pranoy radhakrishnan

   deep learning engineer, entrepreneur

     (button) follow
   [25]towards data science

[26]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 640
     * (button)
     *
     *

   [27]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [28]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/d0604917584a
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_lnmioomtdn8s---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@pranoyradhakrishnan?source=post_header_lockup
  17. https://towardsdatascience.com/@pranoyradhakrishnan
  18. http://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
  19. https://towardsdatascience.com/tagged/machine-learning?source=post
  20. https://towardsdatascience.com/tagged/deep-lear?source=post
  21. https://towardsdatascience.com/tagged/artificial-intelligence?source=post
  22. https://towardsdatascience.com/tagged/hyperparameter?source=post
  23. https://towardsdatascience.com/@pranoyradhakrishnan?source=footer_card
  24. https://towardsdatascience.com/@pranoyradhakrishnan
  25. https://towardsdatascience.com/?source=footer_card
  26. https://towardsdatascience.com/?source=footer_card
  27. https://towardsdatascience.com/
  28. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  30. https://medium.com/p/d0604917584a/share/twitter
  31. https://medium.com/p/d0604917584a/share/facebook
  32. https://medium.com/p/d0604917584a/share/twitter
  33. https://medium.com/p/d0604917584a/share/facebook
