   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

image classification with convolutional neural networks

   [9]go to the profile of ksenia sorokina
   [10]ksenia sorokina (button) blockedunblock (button) followfollowing
   nov 19, 2017

introduction

   machine learning has been gaining momentum over last decades:
   self-driving cars, efficient web search, speech and image recognition.
   the successful results gradually propagate into our daily live. machine
   learning is a class of artificial intelligence methods, which allows
   the computer to operate in a self-learning mode, without being
   explicitly programmed. it is a very interesting and complex topic,
   which could drive the future of technology.

   two months ago i wanted to change my life and i enrolled in the
   programming course from [11]digital academy         czechitas, prague. in
   addition to studying basic subjects, my task was to invent and develop
   my own project. i decided to focus on machine learning. during my
   course i was lucky to meet a mentor         jan matou  ek from [12]data mind,
   who helped me to discover a new world of id158s.

   goals

   since i   m a newcomer to this issue, i took a ready-made model from
   [13]keras blog. my goals were to understand how the model works,
   describe it; customize the model and teach it to recognize photos of
   cars and elephants.

techniques and tools

   i used python syntax for this project. as a framework i used keras,
   which is a high-level neural network api written in python. but keras
   can   t work by itself, it needs a backend for low-level operations. thus
   i installed a dedicated software library         google   s tensorflow.

   as a development environment i used the pycharm. i used matplotlib for
   visualization.

   for network training and testing i used a dataset of photos of
   elephants and cars downloaded from [14]pixabay.com.

theory

      bit of theory in the beginning does not hurt :)

   neural network

   is a machine learning algorithm, which is built on the principle of the
   organization and functioning of biological neural networks. this
   concept arose in an attempt to simulate the processes occurring in the
   brain by warren mcculloch and walter pitts in 1943.

   neural networks consist of individual units called neurons. neurons are
   located in a series of groups         layers (see figure allow). neurons in
   each layer are connected to neurons of the next layer. data comes from
   the input layer to the output layer along these compounds. each
   individual node performs a simple mathematical calculation.   hen it
   transmits its data to all the nodes it is connected to.
   [1*3fa77_mlnijtsgzfhynu0q@2x.png]

   the last wave of neural networks came in connection with the increase
   in computing power and the accumulation of experience. that brought
   deep learning, where technological structures of neural networks have
   become more complex and able to solve a wide range of tasks that could
   not be effectively solved before. image classification is a prominent
   example.

   convolutional neural networks and image classification

   convolutional neural networks (id98) is a special architecture of
   id158s, proposed by yann lecun in 1988. id98 uses
   some features of the visual cortex. one of the most popular uses of
   this architecture is image classification. for example facebook uses
   id98 for automatic tagging algorithms, amazon         for generating product
   recommendations and google         for search through among users    photos.

   let us consider the use of id98 for image classification in more detail.
   the main task of image classification is acceptance of the input image
   and the following definition of its class. this is a skill that people
   learn from their birth and are able to easily determine that the image
   in the picture is an elephant. but the computer sees the pictures quite
   differently:
   [1*cot55wd6gdojlovlcw0aaq.png]

   instead of the image, the computer sees an array of pixels. for
   example, if image size is 300 x 300. in this case, the size of the
   array will be 300x300x3. where 300 is width, next 300 is height and 3
   is rgb channel values. the computer is assigned a value from 0 to 255
   to each of these numbers.   his value describes the intensity of the
   pixel at each point.

   to solve this problem the computer looks for the characteristics of the
   base level. in human understanding such characteristics are for example
   the trunk or large ears. for the computer, these characteristics are
   boundaries or curvatures. and then through the groups of convolutional
   layers the computer constructs more abstract concepts.

   in more detail: the image is passed through a series of convolutional,
   nonlinear, pooling layers and fully connected layers, and then
   generates the output.

   the convolution layer is always the first.   he image (matrix with pixel
   values) is entered into it. imagine that the reading of the input
   matrix begins at the top left of image. next the software selects a
   smaller matrix there, which is called a filter (or neuron, or core).
   then the filter produces convolution, i.e. moves along the input image.
   the filter   s task is to multiply its values by the original pixel
   values. all these multiplications are summed up. one number is obtained
   in the end. since the filter has read the image only in the upper left
   corner, it moves further and further right by 1 unit performing a
   similar operation. after passing the filter across all positions, a
   matrix is obtained, but smaller then a input matrix.
   [1*1mfep1nnxshubwt-i9_o-q.png]

   this operation, from a human perspective, is analogous to identifying
   boundaries and simple colours on the image. but in order to recognize
   the properties of a higher level such as the trunk or large ears the
   whole network is needed.

   the network will consist of several convolutional networks mixed with
   nonlinear and pooling layers. when the image passes through one
   convolution layer, the output of the first layer becomes the input for
   the second layer. and this happens with every further convolutional
   layer.

   the nonlinear layer is added after each convolution operation. it has
   an activation function, which brings nonlinear property. without this
   property a network would not be sufficiently intense and will not be
   able to model the response variable (as a class label).

   the pooling layer follows the nonlinear layer. it works with width and
   height of the image and performs a downsampling operation on them. as a
   result the image volume is reduced. this means that if some features
   (as for example boundaries) have already been identified in the
   previous convolution operation, than a detailed image is no longer
   needed for further processing, and it is compressed to less detailed
   pictures.

   after completion of series of convolutional, nonlinear and pooling
   layers, it is necessary to attach a fully connected layer. this layer
   takes the output information from convolutional networks. attaching a
   fully connected layer to the end of the network results in an n
   dimensional vector, where n is the amount of classes from which the
   model selects the desired class.

   a fragment of the code of this model written in python will be
   considered further in the practical part.

process

   in the beginning of this part i would like to describe the process of
   supervised machine learning, which was taken as a basis of the model.

   supervised machine learning

   it is one of the ways of machine learning where the model is trained by
   input data and expected output data.

     o create such model, it is necessary to go through the following
   phases:
    1. model construction
    2. model training
    3. model testing
    4. model evaluation

   model construction depends on machine learning algorithms. in this
   projects case, it was neural networks.

   such an algorithm looks like:
    1. begin with its object: model = sequential()
    2. then consist of layers with their types: model.add(type_of_layer())
    3. after adding a sufficient number of layers the model is compiled.
       at this moment keras communicates with tensorflow for construction
       of the model. during model compilation it is important to write a
       id168 and an optimizer algorithm. it looks like:
       model.comile(loss=    name_of_loss_function   , optimizer=
          name_of_opimazer_alg    ) the id168 shows the accuracy of
       each prediction made by the model.

   before model training it is important to scale data for their further
   use.

   after model construction it is time for model training. in this phase,
   the model is trained using training data and expected output for this
   data.

   it   s look this way: model.fit(training_data, expected_output).

   progress is visible on the console when the script runs. at the end it
   will report the final accuracy of the model.

   once the model has been trained it is possible to carry out model
   testing. during this phase a second set of data is loaded. this data
   set has never been seen by the model and therefore it   s true accuracy
   will be verified.

   after the model training is complete, and it is understood that the
   model shows the right result, it can be saved by:
   model.save(   name_of_file.h5   ).

   finally, the saved model can be used in the real world. the name of
   this phase is model evaluation. this means that the model can be used
   to evaluate new data.

   classification model (elephants vs cars)

   here i would like to describe the code that was taken as the basis of
   this project. it is considered that a deep learning model needs a large
   amount of data. but the model given in this script is excellent for
   training with a small amount of data. because of that i took only 200
   photos per class for training and 80 photos per class for expected
   output during training.

   using little data is possible when the image is preprocessing with
   keras imagedatagenerator class.   his class can create a number of
   random transformations, which helps to increase the number of images
   when it is needed.
from keras.preprocessing.image import imagedatagenerator, array_to_img, img_to_a
rray, load_img
datagen = imagedatagenerator(
        rotation_range=40,
        width_shift_range=0.2,
        height_shift_range=0.2,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=true,
        fill_mode='nearest')
img = load_img('train/elephants/adventure-1822636_640.jpg')  # this is a pil ima
ge
x = img_to_array(img)  # this is a numpy array with shape (300, 300, 3)
x = x.reshape((1,) + x.shape)  # this is a numpy array with shape (1, 300, 300,
3)
x.shape
# the .flow() command below generates batches of randomly transformed images
# and saves the results to the `preview/` directory
i = 0
for batch in datagen.flow(x, batch_size=1,
                         save_to_dir='preview', save_prefix='el', save_format='j
peg'):
    i += 1
    if i > 20:
        break  # otherwise the generator would loop indefinitely

   imagedatagenerator has the following arguments:
    1. rotation_range         which is used for random rotations, given in
       degrees in the range from 0 to 180
    2. width_shift_range         which is shown in fraction of total width, used
       for random horizontal shifts
    3. height_shift_range         which is the same as width_shift_range, but
       with height
    4. shear_range         shear intensity, used for linear mapping that
       displaces each point in a fixed direction
    5. zoom_range         use for random zooming
    6. horizontal_flip         unlike other arguments has boolean type, used for
       randomly flipping inputs horizontally
    7. fill_mode         can be    constant   ,    reflect   ,    wrap    or    nearest    as in
       this case; indicates the method of filling the newly formed pixels
    8. these are not all the arguments that could be used, the further
       ones can be found [15]here.

   to specify the input directory load_image is used. also load_image
   means that image will load to pil format.

   image_to_array means that image in pil format returns a 3d numpy array,
   which will be reshaped on further.

   then in the loop with flow(x,y) method, the image transformation takes
   place. random transformations are stored in the    preview    folder and
   look like:
   [1*0vzjrj3gf_kteyofxyqgsq.png]

   the following code fragment will describe construction of the model.
   here the layers begin to be added. this architecture was made on the
   principle of convolutional neural networks. it consists of 3 groups of
   layers, where the convolution layers (conv 2d) alternate with the
   nonlinear layers (relu) and the pooling layers (max pooling 2d). it
   then follows 2 tightly bound layers (dense). consider their structure
   in more detail.
# model
from keras.models import sequential
from keras.layers import conv2d, maxpooling2d
from keras.layers import activation, dropout, flatten, dense
model = sequential()
model.add(conv2d(32, (3, 3), input_shape=(300, 300, 3)))
model.add(activation('relu'))
model.add(maxpooling2d(pool_size=(2, 2)))
model.add(conv2d(32, (3, 3)))
model.add(activation('relu'))
model.add(maxpooling2d(pool_size=(2, 2)))
model.add(conv2d(64, (3, 3)))
model.add(activation('relu'))
model.add(maxpooling2d(pool_size=(2, 2)))
# the model so far outputs 3d feature maps (height, width, features)
model.add(flatten())  # this converts our 3d feature maps to 1d feature vectors
model.add(dense(64))
model.add(activation('relu'))
model.add(dropout(0.5))
model.add(dense(1))
model.add(activation('sigmoid'))
# compile
model.compile(loss='binary_crossid178',
              optimizer='rmsprop',
              metrics=['accuracy'])

   let us look at the first convolution layer conv 2d. the number 32 shows
   the amount of output filter in the convolution. numbers 3, 3 correspond
   to the kernel size, which determinate the width and height of the 2d
   convolution window. an important component of the first convolution
   layer is an input shape, which is the input array of pixels. further
   convolution layers are constructed in the same way, but do not include
   the input shape.

   the activation function of this model is relu. this function setts the
   zero threshold and looks like: f(x) = max(0,x). if x > 0         the volume
   of the array of pixels remains the same, and if x < 0         it cuts off
   unnecessary details in the channel.

   max pooling 2d layer is pooling operation for spatial data. numbers 2,
   2 denote the pool size, which halves the input in both spatial
   dimension.

   after three groups of layers there are two fully connected layers.
   flatten performs the input role. next is dense         densely connected
   layer with the value of the output space (64) and relu activation
   function. it follows dropout, which is preventing overfitting.
   overfitting is the phenomenon when the constructed model recognizes the
   examples from the training sample, but works relatively poorly on the
   examples of the test sample. dropout takes value between 0 and 1.   he
   last fully connected layer has 1 output and sigmoid activation
   function.

   next step is model compiling. it has a binary cross id178 loss
   function, which will show the sum of all individual losses. the
   optimizer algorithm is rmsprop, which is good for recurrent neural
   networks. the accuracy metrics shows the performance of the model.

   the following code fragment prepares the model for training:
batch_size = 16
# this is the augmentation configuration we will use for training
train_datagen = imagedatagenerator(
        rescale=1./255,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=true)
# this is the augmentation configuration we will use for testing:
# only rescaling
test_datagen = imagedatagenerator(rescale=1./255)
# this is a generator that will read pictures found in subfolers of 'data/train'
, and indefinitely generate
# batches of augmented image data
train_generator = train_datagen.flow_from_directory(
        'train',  # this is the target directory
        target_size=(300, 300),  # all images will be resized to 300x300
        batch_size=batch_size,
        class_mode='binary')  # since we use binary_crossid178 loss, we need b
inary labels
# this is a similar generator, for validation data
validation_generator = test_datagen.flow_from_directory(
        'validation',
        target_size=(300, 300),
        batch_size=batch_size,
        class_mode='binary')

   batch size the number of training examples in one forward/backward pass
   (or for 1 epoch, which is expected).

   then the already described image data generator is added for training
   and tasting samples. but it has a new transformation, which is called
   rescale. it multiplies the data by the given value.

   the flow_from_directory(directory) method is added for training and
   testing data. first, the path to the folders is specified. further, the
   target size follows. it shows width and height to which images will be
   resized. next, the batch size is added. finally binary class mode is
   set.

   when the preparation is complete, the code fragment of the training
   follows:
# training
model.fit_generator(
        train_generator,
        steps_per_epoch=400 // batch_size,
        epochs=50,
        validation_data=validation_generator,
        validation_steps=160 // batch_size)
model.save_weights('50_epochs.h5')  # always save your weights after training or
 during training

   training is possible with the help of the fit_generator. here it is
   important to indicate a number of epochs, which defines for how many
   times the training will repeat. 1 epoch is 1 forward pass and 1
   backward pass over all the training examples.

   also, in this section steps_per_epoch and validation_steps are set.
   steps_per_epoch (or number of iterations) shows total number of steps,
   which is used to declare one epoch finished and begin the next.
   typically this number is equal to the number of samples for training
   (in my case it is 400: 200 photos of cars and 200 photos of elephants)
   divided by the batch size (16). it means that the number of iterations:
   200 / 16 = 25. validation_steps is total number of steps (batches of
   samples) to validate before stopping.

   when the model is trained it should be saved with save_weights.

   now, when the model is dissembled it can be run. running takes some
   time. at the end of the program shows this result here:
   [1*q0kyor2i0-oeqjldbmxyoa.png]

   it can be seen that after 50 epochs the validation accuracy is 0.9375,
   it shows the ability of the model to generalize to new data.

   after running the code and saving the model it   s time to check its
   accuracy on the new testing photos. it is possible through scoring
   code. after running this code with the new 400 photos of elephants and
   cars, i got a classification accuracy of 96% (383 photos correct).

results

   as a result of testing the model, i got a very good accuracy: 96% of
   correct classification samples after 50 epochs. the only drawback was
   that i had to wait about 40 minutes until 50 epochs come to the end
   (looking at the fact that i had a very small number of photos for
   training). on this i wondered: what if i can achieve the same result in
   fewer epochs?

   for this, i decided to build two plots. the first shows the dependence
   of the evaluation accuracy on the number of epochs. the evaluation
   accuracy was calculated using additional dataset of 400 pictures. the
   second plot shows the dependence of accuracy and validation accuracy on
   the number of epochs during the testing.
   [1*thyhmk04mfjcono7mbevkw.png]
   [1*vgrbwuw_olduydgz3hrm5w.png]

   on the first plot it can be seen that the high accuracy (96%) is
   achieved after 10 epoch. in subsequent epochs on the plot the accuracy
   does not improve (and even decreases in interval 10   25 epochs).

   the second graph shows the intersection of accuracy and validation
   accuracy. validation accuracy sows the ability of the model to
   generalize to new data. validation dataset contains only the data that
   the model never sees during the training and therefor cannot just
   memorize. if your training data accuracy (   acc   ) keeps improving while
   your validation data accuracy (   val_acc   ) gets worse, you are likely in
   an overfitting situation, i.e. your model starts to basically just
   memorize the data.

   this means that after the 10th epoch the model can show the same
   result, but it will not be better. consequently, this model is be
   sufficient to train on 10 epochs.

conclusion

   in this work, i figured out what is deep learning. i assembled and
   trained the id98 model to classify photographs of cars and elephants. i
   have tested that this model works really well with a small number of
   photos. i measured how the accuracy depends on the number of epochs in
   order to detect potential overfitting problem. i determined that 10
   epochs are enough for a successful training of the model.

   my next step would be to try this model on more data sets and try to
   apply it to practical tasks. i would also like to experiment with the
   neural network design in order to see how a higher efficiency can be
   achieved in various problems.

resources

    1. [16]http://www.datamind.cz/cz/vam-na-miru/umela-inteligence-a-stroj
       ove-uceni-ai-machine-learning
    2. [17]https://en.wikipedia.org/wiki/artificial_neural_network
    3. [18]https://en.wikipedia.org/wiki/deep_learning
    4. [19]https://en.wikipedia.org/wiki/convolutional_neural_network
    5. [20]https://adeshpande3.github.io/adeshpande3.github.io/a-beginner%
       27s-guide-to-understanding-convolutional-neural-networks/
    6. [21]https://www.lynda.com/google-tensorflow-tutorials/building-deep
       -learning-applications-keras-2-0/601801-2.html
    7. [22]https://blog.keras.io/building-powerful-image-classification-mo
       dels-using-very-little-data.html
    8. [23]https://keras.io

     * [24]deep learning
     * [25]convolution networks
     * [26]image classification
     * [27]keras
     * [28]python

   (button)
   (button)
   (button) 1k claps
   (button) (button) (button) 6 (button) (button)

     (button) blockedunblock (button) followfollowing
   [29]go to the profile of ksenia sorokina

[30]ksenia sorokina

     * (button)
       (button) 1k
     * (button)
     *
     *

   [31]go to the profile of ksenia sorokina
   never miss a story from ksenia sorokina, when you sign up for medium.
   [32]learn more
   never miss a story from ksenia sorokina
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/496815db12a8
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@ksusorokina?source=post_header_lockup
  10. https://medium.com/@ksusorokina
  11. https://www.czechitas.cz/en/portfolio/digital-academy
  12. http://www.datamind.cz/en
  13. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  14. http://pixabay.com/
  15. https://keras.io/preprocessing/image/
  16. http://www.datamind.cz/cz/vam-na-miru/umela-inteligence-a-strojove-uceni-ai-machine-learning
  17. https://en.wikipedia.org/wiki/artificial_neural_network
  18. https://en.wikipedia.org/wiki/deep_learning
  19. https://en.wikipedia.org/wiki/convolutional_neural_network
  20. https://adeshpande3.github.io/adeshpande3.github.io/a-beginner's-guide-to-understanding-convolutional-neural-networks/
  21. https://www.lynda.com/google-tensorflow-tutorials/building-deep-learning-applications-keras-2-0/601801-2.html
  22. https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html
  23. https://keras.io/
  24. https://medium.com/tag/deep-learning?source=post
  25. https://medium.com/tag/convolution-networks?source=post
  26. https://medium.com/tag/image-classification?source=post
  27. https://medium.com/tag/keras?source=post
  28. https://medium.com/tag/python?source=post
  29. https://medium.com/@ksusorokina?source=footer_card
  30. https://medium.com/@ksusorokina
  31. https://medium.com/@ksusorokina
  32. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  34. https://medium.com/p/496815db12a8/share/twitter
  35. https://medium.com/p/496815db12a8/share/facebook
  36. https://medium.com/p/496815db12a8/share/twitter
  37. https://medium.com/p/496815db12a8/share/facebook
