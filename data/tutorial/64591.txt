   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]become a member[7]sign in[8]get started
     __________________________________________________________________

visualising high-dimensional datasets using pca and id167 in python

   [9]go to the profile of luuk derksen
   [10]luuk derksen (button) blockedunblock (button) followfollowing
   oct 29, 2016

   the first step around any data related challenge is to start by
   exploring the data itself. this could be by looking at, for example,
   the distributions of certain variables or looking at potential
   correlations between variables.

   the problem nowadays is that most datasets have a large number of
   variables. in other words, they have a high number of dimensions along
   which the data is distributed. visually exploring the data can then
   become challenging and most of the time even practically impossible to
   do manually. however, such visual exploration is incredibly important
   in any data-related problem. therefore it is key to understand how to
   visualise high-dimensional datasets. this can be achieved using
   techniques known as id84. this post will focus on
   two techniques that will allow us to do this: pca and id167.

   more about that later. lets first get some (high-dimensional) data to
   work with.

mnist dataset

   we will use the [11]mnist-dataset in this write-up. there is no need to
   download the dataset manually as we can grab it through using
   [12]scikit learn.
import numpy as np
from sklearn.datasets import fetch_mldata
mnist = fetch_mldata("mnist original")
x = mnist.data / 255.0
y = mnist.target

print x.shape, y.shape
[out] (70000, 784) (70000,)

   we are going to convert the matrix and vector to a pandas dataframe.
   this is very similar to the dataframes used in r and will make it
   easier for us to plot it later on.
import pandas as pd
feat_cols = [ 'pixel'+str(i) for i in range(x.shape[1]) ]
df = pd.dataframe(x,columns=feat_cols)
df['label'] = y
df['label'] = df['label'].apply(lambda i: str(i))
x, y = none, none
print 'size of the dataframe: {}'.format(df.shape)
[out] size of the dataframe: (70000, 785)

   because we dont want to be using 70,000 digits in some calculations
   we   ll take a random subset of the digits. the randomisation is
   important as the dataset is sorted by its label (i.e., the first seven
   thousand or so are zeros, etc.). to ensure randomisation we   ll create a
   random permutation of the number 0 to 69,999 which allows us later to
   select the first five or ten thousand for our calculations and
   visualisations.
rndperm = np.random.permutation(df.shape[0])

   we now have our dataframe and our randomisation vector. lets first
   check what these numbers actually look like. to do this we   ll generate
   30 plots of randomly selected images.
%matplotlib inline
import matplotlib.pyplot as plt
# plot the graph
plt.gray()
fig = plt.figure( figsize=(16,7) )
for i in range(0,30):
    ax = fig.add_subplot(3,10,i+1, title='digit: ' + str(df.loc[rndperm[i],'labe
l']) )
     ax.matshow(df.loc[rndperm[i],feat_cols].values.reshape((28,28)).astype(floa
t))
plt.show()

   [1*aa35y_xiyqnyesyqiaifqq.png]

   now we can start thinking about how we can actually distinguish the
   zeros from the ones and two   s and so on. if you were, for example, a
   post office such an algorithm could help you read and sort the
   handwritten envelopes using a machine instead of having humans do that.
   obviously nowadays we have very advanced methods to do this, but this
   dataset still provides a very good testing ground for seeing how
   specific methods for id84 work and how well they
   work.

   the images are all essentially 28-by-28 pixel images and therefore have
   a total of 784    dimensions   , each holding the value of one specific
   pixel.

   what we can do is reduce the number of dimensions drastically whilst
   trying to retain as much of the    variation    in the information as
   possible. this is where we get to id84. lets first
   take a look at something known as principal component analysis.

id84 using pca

   pca is a technique for reducing the number of dimensions in a dataset
   whilst retaining most information. it is using the correlation between
   some dimensions and tries to provide a minimum number of variables that
   keeps the maximum amount of variation or information about how the
   original data is distributed. it does not do this using guesswork but
   using hard mathematics and it uses something known as the eigenvalues
   and eigenvectors of the data-matrix. these eigenvectors of the
   covariance matrix have the property that they point along the major
   directions of variation in the data. these are the directions of
   maximum variation in a dataset.

   i am not going to get into the actual derivation and calculation of the
   principal components         if you want to get into the mathematics see
   [13]this great page         instead we   ll use the [14]scikit-learn
   implementation of pca.

   since we as humans like our two- and three-dimensional plots lets start
   with that and generate, from the original 784 dimensions, the first
   three principal components. and we   ll also see how much of the
   variation in the total dataset they actually account for.
from sklearn.decomposition import pca
pca = pca(n_components=3)
pca_result = pca.fit_transform(df[feat_cols].values)
df['pca-one'] = pca_result[:,0]
df['pca-two'] = pca_result[:,1]
df['pca-three'] = pca_result[:,2]
print 'explained variation per principal component: {}'.format(pca.explained_var
iance_ratio_)
[out] explained variation per principal component: [ 0.16756229  0.0826886   0.0
5374424]

   now, given that the first two components account for about 25% of the
   variation in the entire dataset lets see if that is enough to visually
   set the different digits apart. what we can do is create a scatterplot
   of the first and second principal component and color each of the
   different types of digits with a different color. if we are lucky the
   same type of digits will be positioned (i.e., clustered) together in
   groups, which would mean that the first two principal components
   actually tell us a great deal about the specific types of digits.
from ggplot import *
chart = ggplot( df.loc[rndperm[:3000],:], aes(x='pca-one', y='pca-two', color='l
abel') ) \
        + geom_point(size=75,alpha=0.8) \
        + ggtitle("first and second principal components colored by digit")
chart

   [1*xjhfk_1gkhe9otdv1ercnq.jpeg]

   from the graph we can see the two components definitely hold some
   information, especially for specific digits, but clearly not enough to
   set all of them apart. luckily there is another technique that we can
   use to reduce the number of dimensions that may prove more helpful. in
   the next few paragraphs we are going to take a look at that technique
   and explore if it gives us a better way of reducing the dimensions for
   visualisation. the method we will be exploring is known as id167
   (t-distributed stochastic neighbouring entities).

t-distributed stochastic neighbouring entities (id167)

   t-distributed stochastic neighbor embedding ([15]id167) is another
   technique for id84 and is particularly well suited
   for the visualization of high-dimensional datasets. contrary to pca it
   is not a mathematical technique but a probablistic one. the
   [16]original paper describes the working of id167 as:

      t-distributed stochastic neighbor embedding (id167) minimizes the
   divergence between two distributions: a distribution that measures
   pairwise similarities of the input objects and a distribution that
   measures pairwise similarities of the corresponding low-dimensional
   points in the embedding   .

   essentially what this means is that it looks at the original data that
   is entered into the algorithm and looks at how to best represent this
   data using less dimensions by matching both distributions. the way it
   does this is computationally quite heavy and therefore there are some
   (serious) limitations to the use of this technique. for example one of
   the recommendations is that, in case of very high dimensional data, you
   may need to apply another id84 technique before
   using id167:
|  it is highly recommended to use another id84
|  method (e.g. pca for dense data or truncatedsvd for sparse data)
|  to reduce the number of dimensions to a reasonable amount (e.g. 50)
|  if the number of features is very high.

   the other key drawback is that it:

      since id167 scales quadratically in the number of objects n, its
   applicability is limited to data sets with only a few thousand input
   objects; beyond that, learning becomes too slow to be practical (and
   the memory requirements become too large)   .

   we will use the [17]scikit-learn implementation of the algorithm in the
   remainder of this writeup.

   contrary to the recommendation above we will first try to run the
   algorithm on the actual dimensions of the data (784) and see how it
   does. to make sure we don   t burden our machine in terms of memory and
   power/time we will only use the first 7,000 samples to run the
   algorithm on.
import time
from sklearn.manifold import tsne
n_sne = 7000
time_start = time.time()
tsne = tsne(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(df.loc[rndperm[:n_sne],feat_cols].values)
print 'id167 done! time elapsed: {} seconds'.format(time.time()-time_start)
[out]
[id167] computing pairwise distances...
[id167] computed conditional probabilities for sample 1000 / 7000
[id167] computed conditional probabilities for sample 2000 / 7000
[id167] computed conditional probabilities for sample 3000 / 7000
[id167] computed conditional probabilities for sample 4000 / 7000
[id167] computed conditional probabilities for sample 5000 / 7000
[id167] computed conditional probabilities for sample 6000 / 7000
[id167] computed conditional probabilities for sample 7000 / 7000
[id167] mean sigma: 2.170716
[id167] error after 97 iterations with early exaggeration: 17.891132
[id167] error after 300 iterations: 2.206017
id167 done! time elapsed: 813.213096142 seconds

   now that we have the two resulting dimensions we can again visualise
   them by creating a scatter plot of the two dimensions and coloring each
   sample by its respective label.
df_tsne = df.loc[rndperm[:n_sne],:].copy()
df_tsne['x-tsne'] = tsne_results[:,0]
df_tsne['y-tsne'] = tsne_results[:,1]
chart = ggplot( df_tsne, aes(x='x-tsne', y='y-tsne', color='label') ) \
        + geom_point(size=70,alpha=0.1) \
        + ggtitle("tsne dimensions colored by digit")
chart

   [1*9e_pvh0wgy99ewvpztydag.png]

   this is already a significant improvement over the pca visualisation we
   used earlier. we can see that the digits are very clearly clustered in
   their own little group. if we would now use a id91 algorithm to
   pick out the seperate clusters we could probably quite accurately
   assign new points to a label.

   we   ll now take the recommandations to heart and actually reduce the
   number of dimensions before feeding the data into the id167 algorithm.
   for this we   ll use pca again. we will first create a new dataset
   containing the fifty dimensions generated by the pca reduction
   algorithm. we can then use this dataset to perform the id167 on
pca_50 = pca(n_components=50)
pca_result_50 = pca_50.fit_transform(df[feat_cols].values)
print 'cumulative explained variation for 50 principal components: {}'.format(np
.sum(pca_50.explained_variance_ratio_))
[out] cumulative explained variation for 50 principal components: 84.6676222833%

   amazingly, the first 50 components roughly hold around 85% of the total
   variation in the data.

   now lets try and feed this data into the id167 algorithm. this time
   we   ll use 10,000 samples out of the 70,000 to make sure the algorithm
   does not take up too much memory and cpu. since the code used for this
   is very similar to the previous id167 code i have moved it to the
   appendix: code section at the bottom of this post. the plot it produced
   is the following one:
   [1*brwllztzxzo89qcb9w5d-w.png]

   from this plot we can clearly see how all the samples are nicely spaced
   apart and grouped together with their respective digits. this could be
   an amazing starting point to then use a id91 algorithm and try to
   identify the clusters or to actually use these two dimensions as input
   to another algorithm (e.g., something like a neural network).

   so we have explored using various id84 techniques
   to visualise high-dimensional data using a two-dimensional scatter
   plot. we have not gone into the actual mathematics involved but instead
   relied on the scikit-learn implementations of all algorithms.

roundup report

   before closing off with the appendix   

   together with some likeminded friends we are sending out weekly
   newsletters with some links and notes that we want to share amongst
   ourselves (why not allow others to read them as well?).

   iframe: [18]/media/66a648193689825c31b72bc793520646?postid=8ef87e7915b

appendix: code

   code: id167 on pca-reduced data
n_sne = 10000
time_start = time.time()
tsne = tsne(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_pca_results = tsne.fit_transform(pca_result_50[rndperm[:n_sne]])
print 'id167 done! time elapsed: {} seconds'.format(time.time()-time_start)
[out]
[id167] computing pairwise distances...
[id167] computed conditional probabilities for sample 1000 / 10000
[...]
[id167] computed conditional probabilities for sample 10000 / 10000
[id167] mean sigma: 1.814452
[id167] error after 100 iterations with early exaggeration: 18.725542
[id167] error after 300 iterations: 2.657761
id167 done! time elapsed: 1620.80310392 seconds

   and for the visualisation
df_tsne = none
df_tsne = df.loc[rndperm[:n_sne],:].copy()
df_tsne['x-tsne-pca'] = tsne_pca_results[:,0]
df_tsne['y-tsne-pca'] = tsne_pca_results[:,1]
chart = ggplot( df_tsne, aes(x='x-tsne-pca', y='y-tsne-pca', color='label') ) \
        + geom_point(size=70,alpha=0.1) \
        + ggtitle("tsne dimensions colored by digit (pca)")
chart

     * [19]data science
     * [20]machine learning
     * [21]data visualization
     * [22]visualization
     * [23]python

   (button)
   (button)
   (button) 2.9k claps
   (button) (button) (button) 10 (button) (button)

     (button) blockedunblock (button) followfollowing
   [24]go to the profile of luuk derksen

[25]luuk derksen

   co-founder of [26]@hoppinhq. data (scientist) junky. all views my own.
   pgp: [27]https://goo.gl/uhsrzw

     * (button)
       (button) 2.9k
     * (button)
     *
     *

   [28]go to the profile of luuk derksen
   never miss a story from luuk derksen, when you sign up for medium.
   [29]learn more
   never miss a story from luuk derksen
   (button) blockedunblock (button) followget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/8ef87e7915b
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/membership?source=upgrade_membership---nav_full
   7. https://medium.com/m/signin?redirect=https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-id167-in-python-8ef87e7915b&source=--------------------------nav_reg&operation=login
   8. https://medium.com/m/signin?redirect=https://medium.com/@luckylwk/visualising-high-dimensional-datasets-using-pca-and-id167-in-python-8ef87e7915b&source=--------------------------nav_reg&operation=register
   9. https://medium.com/@luckylwk?source=post_header_lockup
  10. https://medium.com/@luckylwk
  11. http://yann.lecun.com/exdb/mnist/
  12. http://scikit-learn.org/
  13. https://www.math.hmc.edu/calculus/tutorials/eigenstuff/
  14. http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.pca.html
  15. http://lvdmaaten.github.io/tsne/
  16. http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf
  17. http://scikit-learn.org/stable/modules/generated/sklearn.manifold.tsne.html
  18. https://medium.com/media/66a648193689825c31b72bc793520646?postid=8ef87e7915b
  19. https://medium.com/tag/data-science?source=post
  20. https://medium.com/tag/machine-learning?source=post
  21. https://medium.com/tag/data-visualization?source=post
  22. https://medium.com/tag/visualization?source=post
  23. https://medium.com/tag/python?source=post
  24. https://medium.com/@luckylwk?source=footer_card
  25. https://medium.com/@luckylwk
  26. http://twitter.com/hoppinhq
  27. https://goo.gl/uhsrzw
  28. https://medium.com/@luckylwk
  29. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  31. https://medium.com/p/8ef87e7915b/share/twitter
  32. https://medium.com/p/8ef87e7915b/share/facebook
  33. https://medium.com/p/8ef87e7915b/share/twitter
  34. https://medium.com/p/8ef87e7915b/share/facebook
