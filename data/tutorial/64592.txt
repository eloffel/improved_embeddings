   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

getting started with pytorch part 1: understanding how automatic
differentiation works

   [16]go to the profile of ayoosh kathuria
   [17]ayoosh kathuria (button) blockedunblock (button) followfollowing
   mar 28, 2018

   when i started to code neural networks, i ended up using what everyone
   else around me was using. tensorflow.

   but recently, pytorch has emerged as a major contender in the race to
   be the king of deep learning frameworks. what makes it really luring is
   it   s dynamic computation graph paradigm. don   t worry if the last line
   doesn   t make sense to you now. by the end of this post, it will. but
   take my word that it makes debugging neural networks way easier.

   iframe: [18]/media/ea588a241e3e02fa60ae735d19b6f0e7?postid=5008282073ec

   if you   re wondering why your energy has been low lately, switch to
   pytorch!

prerequisites

   before we begin, i must point out that you should have at least the
   basic idea about:
     * concepts related to training of neural networks, particularly
       id26 and id119.
     * applying the chain rule to compute derivatives.
     * how classes work in python. (or a general idea about object
       oriented programming)

   in case, you   re missing any of the above, i   ve provided links at the
   end of the article to guide you.
     __________________________________________________________________

   so, it   s time to get started with pytorch. this is the first in a
   series of tutorials on pytorch.

   this is the part 1 where i   ll describe the basic building blocks, and
   autograd.

     note: an important thing to notice is that the tutorial is made for
     pytorch 0.3 and lower versions. the latest version on offer is 0.4.
     i   ve decided to stick with 0.3 because as of now, 0.3 is the version
     that is shipped in conda and pip channels. also, most of pytorch
     code that is used in open source hasn   t been updated to incorporate
     some of the changes proposed in 0.4. i, however, will point out at
     certain places where things differ in 0.3 and 0.4.

building block #1 : tensors

   if you   ve ever done machine learning in python, you   ve probably come
   across numpy. the reason why we use numpy is because it   s much faster
   than python lists at doing matrix ops. why? because it does most of the
   heavy lifting in c.

   but, in case of training deep neural networks, numpy arrays simply
   don   t cut it. i   m too lazy to do the actual calculations here (google
   for    flops in one iteration of resnet to get an idea), but code
   utilising numpy arrays alone would take months to train some of the
   state of the art networks.

   this is where tensors come into play. pytorch provides us with a data
   structure called a tensor, which is very similar to numpy   s ndarray.
   but unlike the latter, tensors can tap into the resources of a gpu to
   significantly speed up matrix operations.

   here is how you make a tensor.
in [1]: import torch
in [2]: import numpy as np
in [3]: arr = np.random.randn((3,5))
in [4]: arr
out[4]:
array([[-1.00034281, -0.07042071,  0.81870386],
       [-0.86401346, -1.4290267 , -1.12398822],
       [-1.14619856,  0.39963316, -1.11038695],
       [ 0.00215314,  0.68790149, -0.55967659]])
in [5]: tens = torch.from_numpy(arr)
in [6]: tens
out[6]:

-1.0003 -0.0704  0.8187
-0.8640 -1.4290 -1.1240
-1.1462  0.3996 -1.1104
0.0022  0.6879 -0.5597
[torch.doubletensor of size 4x3]
in [7]: another_tensor = torch.longtensor([[2,4],[5,6]])
in [7]: another_tensor
out[13]:
 2  4
 5  6
[torch.longtensor of size 2x2]
in [8]: random_tensor = torch.randn((4,3))
in [9]: random_tensor
out[9]:
1.0070 -0.6404  1.2707
-0.7767  0.1075  0.4539
-0.1782 -0.0091 -1.0463
 0.4164 -1.1172 -0.2888
[torch.floattensor of size 4x3]

building block #2 : computation graph

   now, we are at the business side of things. when a neural network is
   trained, we need to compute gradients of the id168, with
   respect to every weight and bias, and then update these weights using
   id119.

   with neural networks hitting billions of weights, doing the above step
   efficiently can make or break the feasibility of training.

building block #2.1: computation graphs

   computation graphs lie at the heart of the way modern deep learning
   networks work, and pytorch is no exception. let us first get the hang
   of what they are.

   suppose, your model is described like this:
b = w1 * a
c = w2 * a
d = (w3 * b) + (w4 * c)
l = f(d)

   if i were to actually draw the computation graph, it would probably
   look like this.
   [1*fdl9se9otgzz83f3rofqua.png]
   computation graph for our model

   now, you must note, that the above figure is not entirely an accurate
   representation of how the graph is represented under the hood by
   pytorch. however, for now, it   s enough to drive our point home.

   why should we create such a graph when we can sequentially execute the
   operations required to compute the output?

   imagine, what were to happen, if you didn   t merely have to calculate
   the output but also train the network. you   ll have to compute the
   gradients for all the weights labelled by purple nodes. that would
   require you to figure your way around chain rule, and then update the
   weights.

   the computation graph is simply a data structure that allows you to
   efficiently apply the chain rule to compute gradients for all of your
   parameters.
   [1*ewpog5kayzsqkwmwm_wmfq.png]
   applying the chain rule using computation graphs

   here are a couple of things to notice. first, that the directions of
   the arrows are now reversed in the graph. that   s because we are
   backpropagating, and arrows marks the flow of gradients backwards.

   second, for the sake of these example, you can think of the gradients i
   have written as edge weights. notice, these gradients don   t require
   chain rule to be computed.

   now, in order to compute the gradient of any node, say, l, with respect
   of any other node, say c ( dl / dc) all we have to do is.
    1. trace the path from l to c. this would be l     d     c.
    2. multiply all the edge weights as you traverse along this path. the
       quantity you end up with is: ( dl / dd ) * ( dd / dc ) = ( dl / dc)
    3. if there are multiple paths, add their results. for example in case
       of dl/da, we have two paths. l     d     c     a and l     d     b    a. we add
       their contributions to get the gradient of l w.r.t. a.

   [( dl / dd ) * ( dd / dc ) * ( dc / da )] + [( dl / dd ) * ( dd / db )
   * ( db / da )]

   in principle, one could start at l, and start traversing the graph
   backwards, calculating gradients for every node that comes along the
   way.

building block #3 : variables and autograd

   pytorch accomplishes what we described above using the autograd
   package.

   now, there are basically three important things to understand about how
   autograd works.

building block #3.1 : variable

   the variable, just like a tensor is a class that is used to hold data.
   it differs, however, in the way it   s meant to be used. variables are
   specifically tailored to hold values which change during training of a
   neural network, i.e. the learnable paramaters of our network. tensors
   on the other hand are used to store values that are not to be learned.
   for example, a tensor maybe used to store the values of the loss
   generated by each example.
from torch.autograd import variable
var_ex = variable(torch.randn((4,3))   #creating a variable

   a variable class wraps a tensor. you can access this tensor by
   calling .data attribute of a variable.

   the variable also stores the gradient of a scalar quantity (say, loss)
   with respect to the parameter it holds. this gradient can be accessed
   by calling the .grad attribute. this is basically the gradient computed
   up to this particular node, and the gradient of the every subsequent
   node, can be computed by multiplying the edge weight with the gradient
   computed at the node just before it.

   the third attribute a variable holds is a grad_fn, a function object
   which created the variable.
   [1*we1f2i7l8qrw8iuvx5mopw.png]

     note: pytorch 0.4 merges the variable and tensor class into one, and
     tensor can be made into a    variable    by a switch rather than
     instantiating a new object. but since, we   re doing v 0.3 in this
     tutorial, we   ll go ahead.

building block #3.2 : function

   did i say function above? it is basically an abstraction for, well, a
   function. something that takes an input, and returns an output. for
   example, if we have two variables, a and b, then if,

   c = a + b

   then c is a new variable, and it   s grad_fn is something called
   addbackward (pytorch   s built-in function for adding two variables), the
   function which took a and b as input, and created c.

   then, you may ask, why is a need for an entire new class, when python
   does provide a way to define function?

   while training neural networks, there are two steps: the forward pass,
   and the backward pass. normally, if you were to implement it using
   python functions, you will have to define two functions. one, to
   compute the output during forward pass, and another, to compute the
   gradient to be propagated.

   pytorch abstracts the need to write two separate functions (for
   forward, and for backward pass), into two member of functions of a
   single class called torch.autograd.function.

   pytorch combines variables and functions to create a computation graph.

building block #3.3 : autograd

   let us now dig into how pytorch creates a computation graph. first, we
   define our variables.

   iframe: [19]/media/608ffe15a13dbe03820681e978178ff0?postid=5008282073ec

   the result of the above lines of code is,

   iframe: [20]/media/a92e57021052552eeb8402dcf76c4c12?postid=5008282073ec

   now, let   s dissect what the hell just happened here. if you look at the
   source code, here is how things go.
     * define the leaf variables of the graph (lines 5   9). we start by
       defining a bunch of    variables    (normal, python usage of language,
       not pytorch variables). if you notice, the values we defined are
       the leaf nodes in the our computation graph. it only makes sense
       that we have to define them since these nodes aren   t result of any
       computation. at this point, these guys now occupy memory in our
       python namespace. means, they are hundred percent real. we must set
       the requires_grad attribute to true, otherwise, these variables
       won   t be included in the computation graph, and no gradients would
       be computed for them (and other variables, that depend on these
       particular variables for gradient flow).
     * create the graph (lines 12   15). till now, there is nothing such as
       computation graph in our memory. only the leaf nodes, but as soon
       as you write lines 12   15, a graph is being generated on the fly.
       really important to nail this detail. on the fly. when you write b
       =w1*a, it   s when the graph creation kicks in, and continues until
       line 15. this is precisely the forward pass of our model, when the
       output is being calculated from inputs. the forward function of
       each variable may cache some input values to be used while
       computing the gradient on the backward pass. (for example, if our
       forward function computes w*x, then d(w*x)/d(w) is x, the input
       that needs to be cached)
     * now, the reason i told you the graph i drew earlier wasn   t exactly
       accurate? because when pytorch makes a graph, it   s not the variable
       objects that are the nodes of the graph. it   s a function object,
       precisely, the grad_fn of each variable that forms the nodes of the
       graph. so, the pytorch graph would look like.

   [1*40lf-3ekdszsbtp5jmzvjq.png]
   each function is a node in the pytorch computation graph.
     * i   ve represented the leaf nodes, by their names, but they too have
       their grad_fn   s (which return a none value . it makes sense, as you
       can   t backpropagate beyond leaf nodes). the rest of nodes are now
       replaced by their grad_fn   s. we see that the single node d is
       replaced by three functions, two multiplications, and an addition,
       while loss, is replaced by a minus function.
     * compute the gradients (line 18). we now compute the gradients by
       calling the .backward() function on l. what exactly is going on
       here? first, the gradient at l, is simply 1 (dl / dl). then, we
       invoke it   s backward function, which basically has a job of
       computing the gradients of the output of the function object, w.r.t
       to the inputs of the function object. here, l is the result of
       10         d, which means, backwards function will compute the gradient
       (dl/dd) as -1.
     * now, this computed gradient is multiplied by the accumulated
       gradient (stored in the grad attribute of the variable
       corresponding to the current node, which is dl/dl = 1 in our case),
       and then sent to input node, to be stored in the grad attribute of
       the variable corresponding to input node. technically, what we have
       done is apply the chain rule (dl/dl) * (dl/dd) = dl/dd.
     * now, let us understand how gradient is propagated for the variable
       d. d is calculated from it   s inputs (w3, w4, b, c). in our graph,
       it consists of 3 nodes, 2 multiplications and 1 addition.
     * first, the function addbackward (representing addition operation of
       node d in our graph) computes the gradient of it   s output (w3*b +
       w4*c) w.r.t it   s inputs (w3*b and w4*c ), which is (1 for both).
       now, these local gradients are multiplied by accumulated gradient
       (dl/dd x 1 = -1 for both), and the results are saved in the grad
       attribute of the respective input nodes.
     * then, the function mulbackward (representing multiplication
       operation of w4*c) computes the gradient of it   s input output w.r.t
       to it   s inputs (w4 and c) as (c and w4) respectively. the local
       gradients are multiplied by accumulated gradient (dl/d(w4*c) = -1).
       the resultant value (-1 x c and -1 x w4) is then stored in grad
       attribute of variables w4 and c respectively.
     * gradients for all the nodes are computed in a similar fashion.
     * the gradient of l w.r.t any node can be accessed by calling .grad
       on the variable corresponding to that node, given it   s a leaf node
       (pytorch   s default behavior doesn   t allow you to access gradients
       of non-leaf nodes. more on that in a while). now that we have got
       our gradients, we can update our weights using sgd or whatever
       optimization algorithm you like.

w1 = w1     (learning_rate) * w1.grad    #update the wieghts using gd

   and so forth.

some nifty details of autograd

   so, didn   t i tell you you can   t access the grad attribute of non-leaf
   variables. yeah, that   s the default behavior. you can override it by
   calling .retain_grad() on the variable just after defining it and then
   you   d be able to access it   s grad attribute. but really, what the heck
   is going on under the wraps.

dynamic computation graphs

   pytorch creates something called a dynamic computation graph, which
   means that the graph is generated on the fly. until the forward
   function of a variable is called, there exists no node for the variable
   (it   s grad_fn) in the graph. the graph is created as a result of
   forward function of many variables being invoked. only then, the
   buffers are allocated for the graph and intermediate values (used for
   computing gradients later). when you call backward(), as the gradients
   are computed, these buffers are essentially freed, and the graph is
   destroyed. you can try calling backward() more than once on a graph,
   and you   ll see pytorch will give you an error. this is because the
   graph gets destroyed the first time backward() is called and hence,
   there   s no graph to call backward upon the second time.

   if you call forward again, an entirely new graph is generated. with new
   memory allocated to it.

   by default, only the gradients (grad attribute) for leaf nodes are
   saved, and the gradients for non-leaf nodes are destroyed. but this
   behavior can be changed as described above.

   this is in contrast to the static computation graphs, used by
   tensorflow where the graph is declared before running the program. the
   dynamic graph paradigm allows you to make changes to your network
   architecture during runtime, as a graph is created only when a piece of
   code is run. this means a graph may be redefined during the lifetime
   for a program. this, however, is not possible with static graphs where
   graphs are created before running the program, and merely executed
   later. dynamic graphs also make debugging way easier as the source of
   error is easily traceable.
     __________________________________________________________________

some tricks of trade

requires_grad

   this is an attribute of the variable class. by default, it   s false. it
   comes handy when you have to freeze some layers, and stop them from
   updating parameters while training. you can simply set the
   requires_grad to false, and these variables won   t be included in the
   computation graph. thus, no gradient would be propagated to them, or to
   those layers which depend upon these layers for gradient flow.
   requires_grad, when set to true is contagious, meaning even if one
   operand of an operation has requires_grad set to true, so will the
   result.
   [1*aeo6hqbuhn-_2ymibetb-w.png]
   b is not included in the graph. no gradient is backpropagated through b
   now. a only gets gradients from c now. even if w1 has requires_grad =
   true, there is no way it can receive gradients.

volatile

   this again is a attribute of a variable class, which causes a variable
   to be excluded from the computation graph when it is set to true. it
   might seem quite similar to requires_grad, given it   s also contagious
   when set true. but it has a higher precedence than requires_grad. a
   variable with requires_grad equals to true and volatile equals to true,
   would not be included in the computation graph.

   you might think, what   s the need of having another switch to override
   requires_grad, when we can simply set requires_grad to false? let me
   digress for a while.

   not creating a graph is extremely useful when we are doing id136,
   and don   t need gradients. first, overhead to create a computation graph
   is eliminated, and the speed is boosted. second, if we create a graph,
   and since there is no backward being called afterwords, the buffers
   used to cache values are never freed and may lead to you running out of
   memory.

   generally, we have many layers in the a neural network, for which we
   might have set requires_grad to true while training. to prevent a graph
   from being made at id136, we can do either of two things. set
   requires_grad false on all the layers (maybe, 152 of them?). or, set
   volatile true only on the input, and we   re assured no resultant
   operation will result in a graph being made. your choice.
   [1*gr7elwfsfrnvvao0d1xmgw.png]
   no graph is created for b or any node that depends on b.

     note: pytorch 0.4 has no volatile argument for a combined
     tensor/variable class. instead, the id136 code should be put in
     a torch.no_grad() context manager.

with torch.no_grad():
    -----  your id136 code goes here ----

conclusion

   so, that was autograd for you. understanding how autograd works can
   save you a lot of headache when you   re stuck somewhere, or dealing with
   errors when you   re starting out. thanks for reading so far. i intend to
   write more tutorials on pytorch, dealing with how to use inbuilt
   functions to quickly create complex architectures (or, maybe not so
   quickly, but faster than coding block by block). so, stay tuned!

further reading

    1. [21]understanding id26
    2. [22]understanding the chain rule
    3. classes in python [23]part 1 and [24]part 2
    4. [25]pytorch   s official tutorial

     * [26]machine learning
     * [27]deep learning
     * [28]pytorch
     * [29]neural networks
     * [30]deep neural networks

   (button)
   (button)
   (button) 1.6k claps
   (button) (button) (button) 12 (button) (button)

     (button) blockedunblock (button) followfollowing
   [31]go to the profile of ayoosh kathuria

[32]ayoosh kathuria

   ai researcher in the making.

     (button) follow
   [33]towards data science

[34]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 1.6k
     * (button)
     *
     *

   [35]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [36]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5008282073ec
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_pl8dcf8qcrwf---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@ayoosh?source=post_header_lockup
  17. https://towardsdatascience.com/@ayoosh
  18. https://towardsdatascience.com/media/ea588a241e3e02fa60ae735d19b6f0e7?postid=5008282073ec
  19. https://towardsdatascience.com/media/608ffe15a13dbe03820681e978178ff0?postid=5008282073ec
  20. https://towardsdatascience.com/media/a92e57021052552eeb8402dcf76c4c12?postid=5008282073ec
  21. http://neuralnetworksanddeeplearning.com/chap2.html
  22. https://www.youtube.com/watch?v=mkwbx78l7qg
  23. https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-i/tutorial/
  24. https://www.hackerearth.com/practice/python/object-oriented-programming/classes-and-objects-ii-inheritance-and-composition/tutorial/
  25. http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
  26. https://towardsdatascience.com/tagged/machine-learning?source=post
  27. https://towardsdatascience.com/tagged/deep-learning?source=post
  28. https://towardsdatascience.com/tagged/pytorch?source=post
  29. https://towardsdatascience.com/tagged/neural-networks?source=post
  30. https://towardsdatascience.com/tagged/deep-neural-networks?source=post
  31. https://towardsdatascience.com/@ayoosh?source=footer_card
  32. https://towardsdatascience.com/@ayoosh
  33. https://towardsdatascience.com/?source=footer_card
  34. https://towardsdatascience.com/?source=footer_card
  35. https://towardsdatascience.com/
  36. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  38. https://medium.com/p/5008282073ec/share/twitter
  39. https://medium.com/p/5008282073ec/share/facebook
  40. https://medium.com/p/5008282073ec/share/twitter
  41. https://medium.com/p/5008282073ec/share/facebook
