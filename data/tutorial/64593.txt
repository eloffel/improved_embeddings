   #[1]the morning paper    feed [2]the morning paper    comments feed
   [3]the morning paper    convolutional neural networks, part 1 comments
   feed [4]omid reloaded: scalable and highly-available transaction
   processing [5]convolution neural nets, part 2 [6]alternate [7]alternate
   [8]the morning paper [9]wordpress.com

   [10]skip to content

   [11]the morning paper

an interesting/influential/important paper from the world of cs every weekday
morning, as selected by adrian colyer

     * [12]home
     * [13]about
     * [14]infoq qr editions
     * [15]subscribe
     * [16]privacy

convolutional neural networks, part 1

   march 20, 2017
   tags: [17]deep learning

   having recovered somewhat from the last push on deep learning papers,
   it   s time this week to tackle the next batch of papers from the
      [18]top 100 awesome deep learning papers.    recall that the plan is to
   cover multiple papers per day, in a little less depth than usual per
   paper, to give you a broad sweep of what   s in them (see    [19]an
   experiment with awesome deep learning papers   ). you   ll find the first
   batch of papers in the [20]archives starting from february 27th. for
   the next three days, we   ll be tackling the papers from the
      convolutional neural network models    section, starting with:
     * [21]id163 classification with deep convolutional neural
       networks, krizhevsky et al., 2012
     * [22]maxout networks, goodfellow et al., 2013
     * [23]network in network, lin et al., 2013
     * [24]overfeat: integration recognition, localization and detection
       using convolutional networks, sermanent et al., 2013

   ujjwal karn   s excellent blog post    [25]an intuitive explanation of
   convolutional neural networks    provides a some great background on how
   convolutional networks work if you need a refresher before diving into
   these papers.

id163 classification with deep convolutional neural networks

   this is a highly influential paper that kicked off a whole stream of
   work using deep convolutional neural networks for image processing. two
   factors changed that made this possible: firstly, the availability of
   large enough datasets (specifically, the introduction of id163 with
   millions of images, whereas the previous largest datasets had    only   
   tens of thousands; and secondly the development of powerful enough gpus
   to efficiently train large networks.

   what makes id98s such a good fit for working with image data?

     their capacity can be controlled by varying their depth and breadth,
     and they also make strong and mostly correct assumptions about the
     nature of images (namely, stationarity of statistics and locality of
     pixel dependencies). thus, compared to standard feedforward neural
     networks with similarly-sized layers, id98s have much fewer
     connections and parameters and so they are easier to train, while
     their theoretically-best performance is likely to be only slightly
     worse.

   the network that krizhevsky et al. constructed has eight learned layers
       five of them convolutional and three fully-connected. the output of
   the last layer is a 1000-way softmax which produces a distribution over
   the 1000 class labels (the id163 challenge is to create a classifier
   that can determine which object is in the image). because the network
   is too large to fit in the memory of one gpu, training is split across
   two gpus and the kernels of the 2nd, 4th, and 5th convolutional layers
   are connected only to those kernel maps in the previous layer which
   reside on the same gpu.

   the authors single out four other aspects of the model architecture
   that they feel are particularly important:
    1. the use of relu activation (instead of tanh).    deep convolutional
       neural networks with relus train several times faster than their
       equivalents with tanh units    faster learning has a great influence
       on the performance of large models trained on large datasets   
    2. using multiple gpus (two!), and splitting the kernels between them
       with cross-gpu communication only in certain layers. the scheme
       reduces the top-1 and top-5 error rates by 1.7% and 1.2%
       respectively compared to a net with half as many kernels in each
       layer and trained on just one gpu.
    3. using local response normalisation, which    implements a form of
       lateral inhibition inspired by the type found in real neurons,
       creating competition for big activities amongst neuron outputs
       computed using different kernels   .
    4. using overlapping pooling. let pooling layers be of size z x z, and
       spaced s pixels apart. traditionally pooling was used with s = z,
       so that there was no overlap between pools. krizhevsky et al. used
       s = 2 and z = 3 to give overlapping pooling. this reduced the top-1
       and top-5 error rates by 0.4% and 0.3% respectively.

   to reduce overfitting [26]dropout and data augmentation (translations,
   reflections, and principal component manipulation) is used during
   training.

   the end result:

     on ilsvrc-2010, our network achieves top-1 and top-5 test set error
     rates of 37.5% and 17.0%.

   in ilsvrc-2012, the network achieved a top-5 test error rate of 15.3%,
   compared to 26.2% achieved by the second-best entry. that huge winning
   margin sparked the beginning of a revolution.

     our results show that a large, deep convolutional neural network is
     capable of achieving record-breaking results on a highly challenging
     dataset using purely supervised learning. it is notable that our
     network   s performance degrades if a single convolutional layer is
     removed. for example, removing any of the middle layers results in a
     loss of about 2% for the top-1 performance of the network. so the
     depth really is important for achieving our results.

   for a more in-depth look at this paper, see my [27]earlier write-up on
   the morning paper.


maxout networks

   maxout networks are designed to work hand-in-glove with dropout. as you
   may recall, training with dropout is like training an exponential
   number of models all sharing the same parameters. maxout networks are
   otherwise standard multilayer id88 or deep id98s that use a
   special activation function called the maxout unit. the output of a
   maxout unit is simply the maximum of its inputs.

     in a convolutional network, a maxout feature map can be constructed
     by taking the maximum across k affine feature maps
     (i.e., pool across channels, in addition spatial locations). when
     training with dropout, we perform the elementwise multiplication
     with the dropout mask immediately prior to the multiplication by the
     weights in all cases   we do not drop inputs to the max operator.

   maxout units make a piecewise linear approximation to an arbitrary
   convex function, as illustrated below.

   under evaluation, the combination of maxout and dropout achieved state
   of the art classification performance on mnist, cifar-10, cifar-100,
   and svhn (street view house numbers).

   why does it work so well? dropout does exact model averaging in deeper
   architectures provided that they are locally linear among the space of
   inputs to each layer that are visited by applying different dropout
   masks   

     we argue that dropout training encourages maxout units to have large
     linear regions around inputs that appear in the training data   
     networks of linear operations and maxout may learn to exploit
     dropout   s approximate model averaging technique well.

   in addition, rectifier units that saturate at zero are much more common
   with dropout training. a zero value stops the gradient from flowing
   through the unit making it hard to change under training and become
   active again.

     maxout does not suffer from this problem because gradient always
     flows through every maxout unit   even when a maxout unit is 0, this 0
     is a function of the parameters and may be adjusted. units that take
     on negative activations may be steered to become positive again
     later.

network in network

   a traditional convolutional layer applies convolution by applying
   linear filters to the receptive field, followed by nonlinear
   activation. the outputs are called feature maps. in this paper lin et
   al. point out that such a process cannot learn representations that
   distinguish well between non-linear concepts.

     the convolution filter in id98 is a generalized linear model (glm)
     for the underlying data patch, and we argue that the level of
     abstraction is low with glm. by abstraction we mean that the feature
     is invariant to the variants of the same concept.

   even maxout networks impose the constraint that instances of a latent
   concept lie within a convex set in the input space, to which they make
   a piecewise linear approximation. a key question therefore, is whether
   or not input features do indeed require non-linear functions in order
   to best represent the concepts contained within them. the authors
   assert that they do:

        the data for the same concept often live on a nonlinear manifold,
     therefore the representations that capture these concepts are
     generally highly nonlinear function of the input. in nin, the glm is
     replaced with a    micro network    structure which is a general
     nonlinear function approximator. in this work, we choose multilayer
     id88 as the instantiation of the micro network, which is a
     universal function approximator and a neural network trainable by
     back-propagation.

   and that   s the big idea right there, replace the linear convolutional
   layer with a mini multilayer id88 network (called an mlpconv
   layer). we know that such networks are good at learning functions, so
   let   s just allow the mlpconv network to learn what the best convolution
   function is. since the mlpconv layers sit inside a larger network
   model, the overall approach is called    network in network.   

   the second change that lin et al. make to the traditional architecture
   comes at the last layer:

     instead of adopting the traditional fully connected layers for
     classification in id98, we directly output the spatial average of the
     feature maps from the last mlpconv layer as the confidence of
     categories via a global average pooling layer, and then the
     resulting vector is fed into the softmax layer. in traditional id98,
     it is difficult to interpret how the category level information from
     the objective cost layer is passed back to the previous convolution
     layer due to the fully connected layers which act as a black box in
     between. in contrast, global average pooling is more meaningful and
     interpretable as it enforces correspondance between feature maps and
     categories, which is made possible by a stronger local modeling
     using the micro network.

   on cifar-10 (10 classes of natural images with 50,000 traning images in
   total, and 10,000 testing images), the authors beat the then
   state-of-the-art by more than one percent.

   on cifar-100 they also beat the then best performance (without data
   augmentation, which was also not used to evaluate the n-in-n approach)
   by more than one percent. with the street view house numbers dataset,
   and with mnist the authors get good results, but not quite
   state-of-the-art.

overfeat: integrated recognition, localization and detection using
convolutional networks

   overfeat shows how the features learned by a id98-based classifier can
   also be used for localization and detection. on the ilsvrc 2013 dataset
   overfeat ranked 4th in classification, 1st in localization, and 1st in
   detection. we know what the classification problem is (what object is
   in this image), but what about the localization and detection problems?
   localization is like classification, but the network must also produce
   a bounding box showing the boundary of the detected object:

   the detection problem involves images which may contain many small
   objects, and the network must detect each object and draw its bounding
   box:

     the main point of this paper is to show that training a
     convolutional network to simultaneously classify, locate and detect
     objects in images can boost the classification accuracy and the
     detection and localization accuracy of all tasks. the paper proposes
     a new integrated approach to id164, recognition, and
     localization with a single convnet. we also introduce a novel method
     for localization and detection by accumulating predicted bounding
     boxes.

   there   s a lot of detail in the overfeat paper that we don   t have space
   to cover, so if this work is of interest this paper is one where i
   definitely recommend going on to read the full thing.

   since objects of interest (especially in the later detection task) can
   vary significantly in size and position within the image, overfeat
   applies a convnet at multiple locations within the image in a sliding
   window fashion, and at multiple scales. the system is then trained to
   produce a prediction of the location and size of the bounding box
   containing an object relative to the window. evidence for each object
   category is accumulated at each location and size.

   starting with classification, the model is based on krizhevsky et al.
   (our first paper today) in the first five layers, but no contrast
   normalisation is used, pooling regions are non-overlapping, and a
   smaller stride is used (2 instead of 4). six different scales of input
   are used, resulting in unpooled layer 5 maps of varying resolution.
   these are then pooled and presented to the classifier.

   the following diagram summarises the classifier approach built on top
   of the layer 5 feature maps:

     at an intuitive level, the two halves of the network     i.e. feature
     extraction layers (1-5) and classifier layers (6-output)     are used
     in opposite ways. in the feature extraction portion, the filters are
     convolved across the entire image in one pass. from a computational
     perspective, this is far more efficient than sliding a fixed-size
     feature extractor over the image and then aggregating the results
     from different locations. however, these principles are reversed for
     the classifier portion of the network. here, we want to hunt for a
     fixed-size representation in the layer 5 feature maps across
     different positions and scales. thus the classifier has a fixed-size
     5  5 input and is exhaustively applied to the layer 5 maps. the
     exhaustive pooling scheme (with single pixel shifts (   x,    y))
     ensures that we can obtain fine alignment between the classifier and
     the representation of the object in the feature map.

localization

   for localization the classifier layers are replaced by a regression
   networks trained to predict bounding boxes at each spatial location and
   scale. the regression predictions are then combined, along with the
   classification results at each location. training with multiple scales
   ensure predictions match correctly across scales, and exponentially
   increases the confidence of the merged predictions.

   bounding boxes are combined based on the distance between their centres
   and the intersection of their areas, and the final prediction is made
   by taking the merged bounding boxes with maximum class scores. figure 6
   below gives a visual overview of the process:

detection

   detection training is similar to classification training, but with the
   added necessity of predicting a background task when no object is
   present.

     traditionally, negative examples are initially taken at random for
     training, then the most offending negative errors are added to the
     training set in id64 passes    we perform negative training
     on the fly, by selecting a few interesting negative examples per
     image such as random ones or most offending ones. this approach is
     more computationally expensive, but renders the procedure much
     simpler.

share this:

     * [28]twitter
     * [29]linkedin
     * [30]email
     * [31]print
     *

like this:

   like loading...

related

   from     [32]machine learning
   [33]    omid reloaded: scalable and highly-available
   transaction processing
   [34]convolution neural nets, part 2    
   5 comments [35]leave one    

trackbacks

    1. [36]convolution neural nets, part 2 | the morning paper
    2. [37]collection of morning papers on convolutional neural networks |
       the information age
    3. [38]large-scale evolution of image classifiers | the morning paper
    4. [39]weekly ml drop #5 | micha     usiak
    5. [40]end of term, and thank you to the acm | the morning paper

leave a reply [41]cancel reply

   enter your comment here...

   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________
   ____________________________________________________________

   fill in your details below or click an icon to log in:
     *
     *
     *
     *
     *

   [42]gravatar
   email (required) (address never made public)
   ____________________
   name (required)
   ____________________
   website
   ____________________
   wordpress.com logo

   you are commenting using your wordpress.com account. ( [43]log out /
   [44]change )
   google photo

   you are commenting using your google account. ( [45]log out /
   [46]change )
   twitter picture

   you are commenting using your twitter account. ( [47]log out /
   [48]change )
   facebook photo

   you are commenting using your facebook account. ( [49]log out /
   [50]change )
   [51]cancel

   connecting to %s

   [ ] notify me of new comments via email.

   [ ] notify me of new posts via email.

   post comment

   this site uses akismet to reduce spam. [52]learn how your comment data
   is processed.
     * subscribe

                        [53][mail-new-icon.png?w=600]
      never miss an issue! the morning paper delivered straight to your
                                   inbox.
     * search
       ____________________
     * archives archives [select month________]
     * most read in the last few days
          + [54]ginseng: keeping secrets in registers when you distrust
            the operating system
          + [55]amazon aurora: design considerations for high throughput
            cloud-native id208
          + [56]establishing software root of trust unconditionally
          + [57]amazon aurora: on avoiding distributed consensus for i/os,
            commits, and membership changes
          + [58]the amazing power of word vectors
          + [59]calvin: fast distributed transactions for partitioned
            database systems
          + [60]on the criteria to be used in decomposing systems into
            modules
          + [61]neural ordinary differential equations
          + [62]programming paradigms for dummies: what every programmer
            should know
          + [63]applying the universal scalability law to organisations
     * rss
          + [64]rss - posts
          + [65]rss - comments
     * live on twitter[66]my tweets

   [67]blog at wordpress.com.

   send to email address ____________________ your name
   ____________________ your email address ____________________
   _________________________
   loading send email [68]cancel
   post was not sent - check your email addresses!
   email check failed, please try again
   sorry, your blog cannot share posts by email.

   iframe: [69]likes-master

   %d bloggers like this:

references

   visible links
   1. https://blog.acolyer.org/feed/
   2. https://blog.acolyer.org/comments/feed/
   3. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/feed/
   4. https://blog.acolyer.org/2017/03/17/omid-reloaded-scalable-and-highly-available-transaction-processing/
   5. https://blog.acolyer.org/2017/03/21/convolution-neural-nets-part-2/
   6. https://public-api.wordpress.com/oembed/?format=json&url=https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/&for=wpcom-auto-discovery
   7. https://public-api.wordpress.com/oembed/?format=xml&url=https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/&for=wpcom-auto-discovery
   8. https://blog.acolyer.org/osd.xml
   9. https://s1.wp.com/opensearch.xml
  10. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#content
  11. https://blog.acolyer.org/
  12. https://blog.acolyer.org/
  13. https://blog.acolyer.org/about/
  14. https://blog.acolyer.org/infoq-quarterly-review-editions/
  15. https://blog.acolyer.org/email-subs/
  16. https://blog.acolyer.org/privacy/
  17. https://blog.acolyer.org/tag/deep-learning/
  18. https://github.com/terryum/awesome-deep-learning-papers
  19. https://blog.acolyer.org/2017/02/26/an-experiment-with-awesome-deep-learning-papers/
  20. https://blog.acolyer.org/2017/02/
  21. http://papers.nips.cc/paper/4824-id163-classification-with-deep-convolutional-neural-networks.pdf
  22. http://arxiv.org/pdf/1302.4389v4
  23. http://arxiv.org/pdf/1312.4400
  24. http://arxiv.org/pdf/1312.6229
  25. https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
  26. https://blog.acolyer.org/2017/03/01/optimisation-and-training-techniques-for-deep-learning/
  27. https://blog.acolyer.org/2016/04/20/id163-classification-with-deep-convolutional-neural-networks/
  28. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/?share=twitter
  29. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/?share=linkedin
  30. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/?share=email
  31. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#print
  32. https://blog.acolyer.org/category/machine-learning/
  33. https://blog.acolyer.org/2017/03/17/omid-reloaded-scalable-and-highly-available-transaction-processing/
  34. https://blog.acolyer.org/2017/03/21/convolution-neural-nets-part-2/
  35. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#respond
  36. https://blog.acolyer.org/2017/03/21/convolution-neural-nets-part-2/
  37. https://theinformationageblog.wordpress.com/2017/03/22/collection-of-morning-papers-on-convolutional-neural-networks/
  38. https://blog.acolyer.org/2017/03/28/large-scale-evolution-of-image-classifiers/
  39. http://mlusiak.com/2017/04/01/mlweekly-2017-13/
  40. https://blog.acolyer.org/2017/04/10/end-of-term-and-thank-you-to-the-acm/
  41. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#respond
  42. https://gravatar.com/site/signup/
  43. javascript:highlandercomments.doexternallogout( 'wordpress' );
  44. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/
  45. javascript:highlandercomments.doexternallogout( 'googleplus' );
  46. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/
  47. javascript:highlandercomments.doexternallogout( 'twitter' );
  48. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/
  49. javascript:highlandercomments.doexternallogout( 'facebook' );
  50. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/
  51. javascript:highlandercomments.cancelexternalwindow();
  52. https://akismet.com/privacy/
  53. http://eepurl.com/bbzpm9
  54. https://blog.acolyer.org/2019/04/05/ginseng-keeping-secrets-in-registers-when-you-distrust-the-operating-system/
  55. https://blog.acolyer.org/2019/03/25/amazon-aurora-design-considerations-for-high-throughput-cloud-native-relational-databases/
  56. https://blog.acolyer.org/2019/04/03/establishing-software-root-of-trust-unconditionally/
  57. https://blog.acolyer.org/2019/03/27/amazon-aurora-on-avoiding-distributed-consensus-for-i-os-commits-and-membership-changes/
  58. https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/
  59. https://blog.acolyer.org/2019/03/29/calvin-fast-distributed-transactions-for-partitioned-database-systems/
  60. https://blog.acolyer.org/2016/09/05/on-the-criteria-to-be-used-in-decomposing-systems-into-modules/
  61. https://blog.acolyer.org/2019/01/09/neural-ordinary-differential-equations/
  62. https://blog.acolyer.org/2019/01/25/programming-paradigms-for-dummies-what-every-programmer-should-know/
  63. https://blog.acolyer.org/2015/04/29/applying-the-universal-scalability-law-to-organisations/
  64. https://blog.acolyer.org/feed/
  65. https://blog.acolyer.org/comments/feed/
  66. https://twitter.com/519408925733425153
  67. https://wordpress.com/?ref=footer_blog
  68. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#cancel
  69. https://widgets.wp.com/likes/master.html?ver=20190321#ver=20190321

   hidden links:
  71. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#comment-form-guest
  72. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#comment-form-load-service:wordpress.com
  73. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#comment-form-load-service:twitter
  74. https://blog.acolyer.org/2017/03/20/convolutional-neural-networks-part-1/#comment-form-load-service:facebook
