   #[1]kdnuggets: analytics, big data, data mining and data science feed
   [2]kdnuggets    feed [3]kdnuggets    comments feed [4]kdnuggets   
   simplifying decision tree interpretability with python & scikit-learn
   comments feed [5]webinar: a new era of data science     unlocking big
   data insights with machine learning and spark, may 31 [6]arcelormittal:
   data scientist

kdnuggets

     [7]subscribe to kdnuggets news  | [8]twitter    [9]facebook
   [10]linkedin  |  [11]contact
   ____________________ search
   [menu-30.png]
   [search-icon.png]
     * [12]software
     * [13]news/blog
     * [14]top stories
     * [15]opinions
     * [16]tutorials
     * [17]jobs
     * [18]companies
     * [19]courses
     * [20]datasets
     * [21]education
     * [22]certificates
     * [23]meetings
     * [24]webinars


   [25]kdnuggets home    [26]news    [27]2017    [28]may    [29]tutorials,
   overviews    simplifying decision tree interpretability with python &
   scikit-learn ( [30]17:n20 )

simplifying decision tree interpretability with python & scikit-learn

   [31][prv.gif] previous post
   [32]next post [nxt.gif]

     http likes 158
   tags: [33]id90, [34]interpretability, [35]python,
   [36]scikit-learn

   this post will look at a few different ways of attempting to simplify
   decision tree representation and, ultimately, interpretability. all
   code is in python, with scikit-learn being used for the decision tree
   modeling.
     __________________________________________________________________

   by [37]matthew mayo, kdnuggets.

   when discussing classifiers, id90 are often thought of as
   easily interpretable models when compared to numerous more complex
   classifiers, especially those of the blackbox variety. and this is
   generally true.

   this is especially true of rather comparatively simple models created
   from simple data. this is much-less true of complex id90
   crafted from large amounts of (high-dimensional) data. even otherwise
   straightforward id90 which are of great depth and/or breadth,
   consisting of heavy branching, can be difficult to trace.

   concise, textual representations of id90 can often nicely
   summarize decision tree models. additionally, certain textual
   representations can have further use beyond their summary capabilities.
   for example, automatically generating functions with the ability to
   classify future data by passing instances to such functions may be of
   use in particular scenarios. but let's not get off course --
   interpretability is the goal of what we are discussing here.

   this post will look at a few different ways of attempting to simplify
   decision tree representation and, ultimately, interpretability. all
   code is in python, with scikit-learn being used for the decision tree
   modeling.

building a classifier


   first off, let's use my favorite dataset to build a simple decision
   tree in python using [38]scikit-learn's decision tree classifier,
   specifying information gain as the criterion and otherwise using
   defaults. since we aren't concerned with classifying unseen instances
   in this post, we won't bother with splitting our data, and instead just
   construct a classifier using the dataset in its entirety.

   one of the easiest ways to interpret a decision tree is visually,
   accomplished with scikit-learn using these few lines of code:

   copying the contents of the created file ('dt.dot' in our example) to a
   [39]graphviz rendering agent, we get the following representation of
   our decision tree:

                             decision tree graph

representing the model as a function


   as stated in the outset of this post, we will look at a couple of
   different ways for textually representing id90.

   the first is representing the decision tree model [40]as a function.

   let's call this function and see the results:
  tree_to_code(dt, list(iris.feature_names))

def tree(sepal length (cm), sepal width (cm), petal length (cm), petal width (cm
)):
  if petal length (cm) <= 2.45000004768:
    return [[ 50.   0.   0.]]
  else:  # if petal length (cm) > 2.45000004768
    if petal width (cm) <= 1.75:
      if petal length (cm) <= 4.94999980927:
        if petal width (cm) <= 1.65000009537:
          return [[  0.  47.   0.]]
        else:  # if petal width (cm) > 1.65000009537
          return [[ 0.  0.  1.]]
      else:  # if petal length (cm) > 4.94999980927
        if petal width (cm) <= 1.54999995232:
          return [[ 0.  0.  3.]]
        else:  # if petal width (cm) > 1.54999995232
          if petal length (cm) <= 5.44999980927:
            return [[ 0.  2.  0.]]
          else:  # if petal length (cm) > 5.44999980927
            return [[ 0.  0.  1.]]
    else:  # if petal width (cm) > 1.75
      if petal length (cm) <= 4.85000038147:
        if sepal length (cm) <= 5.94999980927:
          return [[ 0.  1.  0.]]
        else:  # if sepal length (cm) > 5.94999980927
          return [[ 0.  0.  2.]]
      else:  # if petal length (cm) > 4.85000038147
        return [[  0.   0.  43.]]

   interesting. let's see if we can improve interpretability by stripping
   away some of the "functionality," provided it is not required.

representing the model as pseudocode


   next, [41]a slight reworking of the above code results in the promised
   goal of this post's title: a set of decision rules for representing a
   decision tree, in slightly less-pythony pseudocode.

   let's test this function:
  tree_to_pseudo(dt, list(iris.feature_names))

 if ( petal length (cm) <= 2.45000004768 ) {
   return [[ 50.   0.   0.]]
 } else {
   if ( petal width (cm) <= 1.75 ) {
     if ( petal length (cm) <= 4.94999980927 ) {
       if ( petal width (cm) <= 1.65000009537 ) {
         return [[  0.  47.   0.]]
       } else {
         return [[ 0.  0.  1.]]
       }
     } else {
       if ( petal width (cm) <= 1.54999995232 ) {
         return [[ 0.  0.  3.]]
       } else {
         if ( petal length (cm) <= 5.44999980927 ) {
           return [[ 0.  2.  0.]]
         } else {
           return [[ 0.  0.  1.]]
         }
       }
     }
   } else {
     if ( petal length (cm) <= 4.85000038147 ) {
       if ( sepal length (cm) <= 5.94999980927 ) {
         return [[ 0.  1.  0.]]
       } else {
         return [[ 0.  0.  2.]]
       }
     } else {
       return [[  0.   0.  43.]]
     }
   }
 }

   this looks pretty good as well, and -- in my computer science-trained
   mind -- the use of well-placed c-style braces makes this a bit more
   legible then the previous attempt.

   these gems have made me want to modify code to get to true decision
   rules, which i plan on playing with after finishing this post. if i get
   anywhere of note, i will return here and post my findings.

   related:
     * [42]decision tree classifiers: a concise technical overview
     * [43]toward increased id116 id91 efficiency with the naive
       sharding centroid initialization method
     * [44]automatically segmenting data with id91
     __________________________________________________________________

   [45][prv.gif] previous post
   [46]next post [nxt.gif]
     __________________________________________________________________

top stories past 30 days

                                              most popular
    1. [47]another 10 free must-read books for machine learning and data
       science
    2. [48]9 must-have skills you need to become a data scientist, updated
    3. [49]who is a typical data scientist in 2019?
    4. [50]the pareto principle for data scientists
    5. [51]what no one will tell you about data science job applications
    6. [52]19 inspiring women in ai, big data, data science, machine
       learning
    7. [53]my favorite mind-blowing machine learning/ai breakthroughs

                                                most shared
    1. [54]id158s optimization using genetic algorithm
       with python
    2. [55]who is a typical data scientist in 2019?
    3. [56]8 reasons why you should get a microsoft azure certification
    4. [57]the pareto principle for data scientists
    5. [58]r vs python for data visualization
    6. [59]how to work in data science, ai, big data
    7. [60]the deep learning toolset     an overview

[61]latest news

     * [62]download your datax guide to ai in marketing
     * [63]kdnuggets offer: save 20% on strata in london
     * [64]training a champion: building deep neural nets for big ...
     * [65]building a recommender system
     * [66]predict age and gender using convolutional neural netwo...
     * [67]top tweets, mar 27     apr 02: here is a great ex...

   [68]kdnuggets home    [69]news    [70]2017    [71]may    [72]tutorials,
   overviews    simplifying decision tree interpretability with python &
   scikit-learn ( [73]17:n20 )
      2019 kdnuggets. [74]about kdnuggets.  [75]privacy policy. [76]terms
   of service

   [77]subscribe to kdnuggets news
   [78][tw_c48.png] [79]facebook [80]linkedin
   x

   [envelope.png] [81]get kdnuggets, a leading newsletter on ai, data
   science, and machine learning
   email: ______________________________
   sign up
   leave this field empty if you're human: ____________________

references

   visible links
   1. https://www.kdnuggets.com/feed/
   2. https://www.kdnuggets.com/feed
   3. https://www.kdnuggets.com/comments/feed
   4. https://www.kdnuggets.com/2017/05/simplifying-decision-tree-interpretation-decision-rules-python.html/feed
   5. https://www.kdnuggets.com/2017/05/angoss-webinar-new-era-data-science-spark.html
   6. https://www.kdnuggets.com/jobs/17/05-19-arcelormittal-data-scientist.html
   7. https://www.kdnuggets.com/news/subscribe.html
   8. https://twitter.com/kdnuggets
   9. https://www.facebook.com/kdnuggets
  10. https://www.linkedin.com/groups/54257/
  11. https://www.kdnuggets.com/contact.html
  12. https://www.kdnuggets.com/software/index.html
  13. https://www.kdnuggets.com/news/index.html
  14. https://www.kdnuggets.com/news/top-stories.html
  15. https://www.kdnuggets.com/opinions/index.html
  16. https://www.kdnuggets.com/tutorials/index.html
  17. https://www.kdnuggets.com/jobs/index.html
  18. https://www.kdnuggets.com/companies/index.html
  19. https://www.kdnuggets.com/courses/index.html
  20. https://www.kdnuggets.com/datasets/index.html
  21. https://www.kdnuggets.com/education/index.html
  22. https://www.kdnuggets.com/education/analytics-data-mining-certificates.html
  23. https://www.kdnuggets.com/meetings/index.html
  24. https://www.kdnuggets.com/webcasts/index.html
  25. https://www.kdnuggets.com/
  26. https://www.kdnuggets.com/news/index.html
  27. https://www.kdnuggets.com/2017/index.html
  28. https://www.kdnuggets.com/2017/05/index.html
  29. https://www.kdnuggets.com/2017/05/tutorials.html
  30. https://www.kdnuggets.com/2017/n20.html
  31. https://www.kdnuggets.com/2017/05/angoss-webinar-new-era-data-science-spark.html
  32. https://www.kdnuggets.com/jobs/17/05-19-arcelormittal-data-scientist.html
  33. https://www.kdnuggets.com/tag/decision-trees
  34. https://www.kdnuggets.com/tag/interpretability
  35. https://www.kdnuggets.com/tag/python
  36. https://www.kdnuggets.com/tag/scikit-learn
  37. https://www.kdnuggets.com/author/matt-mayo
  38. http://scikit-learn.org/stable/modules/generated/sklearn.tree.decisiontreeclassifier.html
  39. http://webgraphviz.com/
  40. http://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree
  41. http://stackoverflow.com/questions/20224526/how-to-extract-the-decision-rules-from-scikit-learn-decision-tree
  42. https://www.kdnuggets.com/2016/10/decision-trees-concise-technical-overview.html
  43. https://www.kdnuggets.com/2017/03/naive-sharding-centroid-initialization-method.html
  44. https://www.kdnuggets.com/2017/02/automatically-segmenting-data-id91.html
  45. https://www.kdnuggets.com/2017/05/angoss-webinar-new-era-data-science-spark.html
  46. https://www.kdnuggets.com/jobs/17/05-19-arcelormittal-data-scientist.html
  47. https://www.kdnuggets.com/2019/03/another-10-free-must-read-books-for-machine-learning-and-data-science.html
  48. https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html
  49. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  50. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  51. https://www.kdnuggets.com/2019/03/data-science-job-applications.html
  52. https://www.kdnuggets.com/2019/03/women-ai-big-data-science-machine-learning.html
  53. https://www.kdnuggets.com/2019/03/favorite-ml-ai-breakthroughs.html
  54. https://www.kdnuggets.com/2019/03/artificial-neural-networks-optimization-genetic-algorithm-python.html
  55. https://www.kdnuggets.com/2019/03/typical-data-scientist-2019.html
  56. https://www.kdnuggets.com/2019/03/simplilearn-8-reasons-microsoft-azure-certification.html
  57. https://www.kdnuggets.com/2019/03/pareto-principle-data-scientists.html
  58. https://www.kdnuggets.com/2019/03/r-vs-python-data-visualization.html
  59. https://www.kdnuggets.com/2019/03/work-data-science-ai-big-data.html
  60. https://www.kdnuggets.com/2019/03/deep-learning-toolset-overview.html
  61. https://www.kdnuggets.com/news/index.html
  62. https://www.kdnuggets.com/2019/04/datax-download-guide-ai-marketing.html
  63. https://www.kdnuggets.com/2019/04/strata-kdnuggets-offer-save-london.html
  64. https://www.kdnuggets.com/2019/04/sisense-deep-neural-nets-big-data-analytics.html
  65. https://www.kdnuggets.com/2019/04/building-recommender-system.html
  66. https://www.kdnuggets.com/2019/04/predict-age-gender-using-convolutional-neural-network-opencv.html
  67. https://www.kdnuggets.com/2019/04/top-tweets-mar27-apr02.html
  68. https://www.kdnuggets.com/
  69. https://www.kdnuggets.com/news/index.html
  70. https://www.kdnuggets.com/2017/index.html
  71. https://www.kdnuggets.com/2017/05/index.html
  72. https://www.kdnuggets.com/2017/05/tutorials.html
  73. https://www.kdnuggets.com/2017/n20.html
  74. https://www.kdnuggets.com/about/index.html
  75. https://www.kdnuggets.com/news/privacy-policy.html
  76. https://www.kdnuggets.com/terms-of-service.html
  77. https://www.kdnuggets.com/news/subscribe.html
  78. https://twitter.com/kdnuggets
  79. https://facebook.com/kdnuggets
  80. https://www.linkedin.com/groups/54257
  81. https://www.kdnuggets.com/news/subscribe.html

   hidden links:
  83. https://www.kdnuggets.com/
  84. https://www.kdnuggets.com/
