   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]the artificial impostor
     * [9]deep learning
     * [10]python
     * [11]r
     * [12]reading lists
     * [13]data analysis
     __________________________________________________________________

[tensorflow] core concepts and common confusions

from a beginner   s point of view (understanding tensorflow part 1)

   [14]go to the profile of ceshine lee
   [15]ceshine lee (button) blockedunblock (button) followfollowing
   mar 28, 2018
   [1*mcfvxigogafpuliyh4tvdq.jpeg]
   photo by [16]luca bravo on [17]unsplash

a little bit about myself

    1. i   ve been using [18]keras since its pre-1.0 era, and have mostly
       used it to train some standard mlp/id98/id56 models.
    2. last july i started to learn [19]pytorch, and have since been able
       to work with some more [20]advanced or [21]customized models using
       pytorch.
    3. i had worked through the course and exercises when [22]google   s
       udacity deep learning course came out, so i can read a tensorflow
       project and get a gist of what   s going on. but i   ve never done any
       real project directly with tensorflow before. it   s still very
       foreign to me.

   this post will be written from my personal perspective, so the readers
   should already have some basic idea about what deep learning is, and
   preferably are familiar with pytorch.

   20180528 update (gihub repo with links to all posts and notebooks):
   [23]ceshine/tensorflow-crash-course
   tensorflow-crash-course - for those who already have some basic idea
   about deep learning, and preferably are familiar   github.com

why tensorflow

   i   ve been considering picking up tensorflow for a while, and have
   finally decided to do it. the main appeal of learning tensorflow
   includes (compared to using pytorch exclusively. keras as a high-level
   library is not really comparable.):
    1. tensorflow is still arguably the most used deep learning library by
       [24]researchers and engineers. it   s impractical to port all
       interesting research published in tensorflow to pytorch by myself.
       waiting for others to release their port can also be a risky
       business. having a tensorflow fluency that allows you to base your
       work on others    results in tensorflow should be very beneficial.
    2. [25]tensorboard is awesome. granted, there are [26]already
       tensorboard integrations for other deep learning libraries. but
       using it with tensorflow will be more well-documented and
       well-supported.
    3. production readiness. [27]other libraries are catching up, but
       tensorflow has it already all figured out for you([28]tensorflow
       serving and [29]tensorflow lite).
    4. there have [30]some voices asking people to boycott facebook   s
       research ecosystem given the recent turmoil at facebook (the
       company behind pytorch). although i personally still feel
       conflicted about the idea of boycotting a very fine piece of open
       source software, learning the other major alternative can do no
       harm.

   [1*aoyua3hhki9hlynnfaipuq.gif]
   [31]tensorboad graph interface

the tensorflow programming stack

   [1*8e8aq_gljfy8tguzx1f2ia.png]
   [32]source

   tensorflow started with only the kernel and low-level apis, but has
   accumulated a lot of modules and become a behemoth. official
   documentation recommends using [33]estimators and [34]datasets, but i
   personally chose to start from layers apis and low-level apis to have
   the kind of access similar to ones in pytorch, and work my way up to
   estimators and datasets. there will be more posts coming up on these
   topics.

   i find the complexity of its stack one of tensorflow   s main turn-offs.
   [35]the vast of amount of modules to choose from can also overwhelms
   the beginners. it   ll become a lot easier when you finally find a set of
   modules you can work with most comfortably.

graphs and sessions

   this is the one of the most important concept for those who come from
   pytorch. pytorch create a dynamic computational graph on the fly to do
   automatic differentiation. tensorflow, on the other hand, requires you
   to define the graph first. i

   for tensorflow, it   s like building a systems of pipes first(a graph),
   pumping water into it and receiving the processed water in the other
   end (session.run). pytorch allows you to pump water into the system
   while you are building it, so it   s easier to find any sub-units that
   malfunctioned (debugging).
   [1*smfhkwhxhvemg8kqnaj-uw.gif]
   [36]data flow in a graph

   there are some upsides to static graphs, despite the obvious downsides.
   because it is static, tensorflow can infer some parameters like input
   sizes for you when compiling the graph. the trained model will also be
   more portable to other platforms.

   (tensorflow has started to offer their version of dynamic computation
   graph      [37]   eager execution. but it is still in [38]pre-alpha stage.)

   usually you only need one graph. tensorflow implicitly defines a
   default graph for you, but i prefer to explicitly define it and group
   all graph definition in a context:
# graph definition
graph = tf.graph()
with graph.as_default():
  # operations created in this scope will be added to `graph`.
  c = tf.constant("node in graph")
# session
with tf.session(graph=graph) as sess:
    print(sess.run(c).decode("utf8"))

session.run

   this method is basically the whole point of creating a session. it
   starts the data flow in the graph (pumps water into a piping system).
   the two most important parameters are fetches(outputs) and
   feeds(inputs).

   by passing a list of nodes (can be operations, tensors, or variables)
   as fetches, you tell session.run you want the data to flow to the given
   nodes. session.run will close off all the subgraphs that are not
   required to reach those nodes, hence saves execution time.

   by passing a map from values to tensors as a dictionary of feeds, you
   tell session.run to fill the tensors with the given values when running
   the graph. this is how you input information to the graph. (you can
   also use variables instead of tensors in feeds, although it   s not very
   common.)

   in the following example from the [39]official tutorial, y is passed as
   the sole element of fetches, and values to placeholder x is passed in a
   dictionary:
# define a placeholder that expects a vector of three floating-point values,
# and a computation that depends on it.
x = tf.placeholder(tf.float32, shape=[3])
y = tf.square(x)
with tf.session() as sess:
  # feeding a value changes the result that is returned when you evaluate `y`.
  print(sess.run(y, {x: [1.0, 2.0, 3.0]}))  # => "[1.0, 4.0, 9.0]"
  print(sess.run(y, {x: [0.0, 0.0, 5.0]}))  # => "[0.0, 0.0,25.0]"

tensors v.s. variables

   in pytorch, [40]a variable is part of the automatic differentiation
   module and a wrapper around a tensor. it represents a node in the
   computation graph, and stores its parent in the graph and optionally
   its gradients. so basically all tensors in the graph are variables.

   a variable in tensorflow is also a wrapper around a tensor, but has a
   different meaning. a variable contains a tensor that is persistent and
   changeable across different session.runs. so they are usually the ones
   that are updated in back-propagations (e.g. the weights of a model),
   and also any other states we want to keep between different runs. also,
   [41]all variables need to be initialized (usually through an
   [42]tf.global_variables_initialize operation) before they can be used.

   variables does not persist after a session being closed. you have to
   remember saving those variables before closing the session. ([43]the
   official documentation mentioned modifications to variables are visible
   across multiple sessions, but that seems only apply to concurrent
   session running on multiple workers.)

   to save variables/weights in tensorflow usually involves
   [44]serializing all variable into a file. while pytorch relies on the
   [45]state_dict method to extract parameters (a subclass of variable)
   and persistent buffers to be serialized.

   saving models in tensorflow involves defining a saver in the graph
   definition and [46]invoking the save method in a session:
saver = tf.train.saver()
with tf.session() as sess:
  sess.run(init_op)
  # do some work with the model.
  inc_v1.op.run()
  dec_v2.op.run()
  # save the variables to disk.
  save_path = saver.save(sess, "/tmp/model.ckpt")

   there   s also a [47]savedmodel class that not only saves variables, but
   also the graph and the metadata of the graph for you. it is useful when
   you want to export your model.

name scope v.s. variable scope

   pytorch names the parameters in a quite pythonic way:
   [1*wdyii6gymqgisvi1qbfuog.png]
   pytorch parameter naming

   tensorflow, however, uses namespaces to organize tensors/variables and
   operations. tensorboard group operations according namespaces they
   belong to, and generate a nice visual representation of the graph for
   you:
   [1*i7gbfh4kqhadkxpjo7qkdw.png]
   the graph of one of my models

   an example of tensor name is scope_outer/scope_inner/tensor_a:0.
   tensor_a is the actual name of the tensor. the suffix :0 is [48]the
   endpoint used to give the tensors returned from an operation unique
   identifiers, i.e. :0 is the first tensor, :1 is the second and so on.

   some medium or high level apis like [49]tf.layers will handles some of
   the scope naming for you. but i   ve found it not smart enough sometime,
   so i had to do it manually. and it brings me to the question         which
   one to use, tf.name_scope or tf.varialble_scope?

   [50]this stackoverflow answer gave a brilliant explanation to the
   differences between these two, as illustrated in the following graph:
   [1*aft7pf4wyzszjfyqdjumba.png]
   left: tf.variable_scope right: tf.name_scope ([51]source)

   turns out there is only one difference         tf.variable_scope affects
   [52]tf.get_variable, whiletf.name_scope doesn   t.
   [53]tf.variable_scope[54] also has a parameter[55]reuse[56], which
   allow you to reuse the same variable (with the same name in the same
   namespace) in different part of the code without having to pass a
   reference to that variable around.

   here   s an example of mixing tf.variable_scope and tf.name_scope from
   the official documentation:
with tf.variable_scope("foo"):
    with tf.name_scope("bar"):
        v = tf.get_variable("v", [1])
        x = 1.0 + v
assert v.name == "foo/v:0"
assert x.op.name == "foo/bar/add"

   imo, usually you   d want to use variable_scope unless there is a need to
   put operations and variables in different levels of namespaces.

time to get our hands dirty

   that   s pretty much it! in this post we have covered 4 topics that i   ve
   found most confusing to beginners:
    1. the programming stack
    2. graphs and sessions
    3. tensors and variables
    4. name scope and variable scope

   now we can go on and write some actual tensorflow code. in the next two
   or three posts i   ll share some of the exercises i set for myself, using
   low-to-medium level apis. after that maybe i   ll do a piece on how to
   integrate higher level apis like [57]estimators and [58]datasets, so we
   can comfortably move between different levels of apis to suit our
   requirements.

quick links

   [59][notes] understanding tensorflow         part 2
   building id56 models to solve sequential mnistmedium.com
   [60][notes] understanding tensorflow         part 3
   implementing temporal convolutional networksmedium.com
   [61][tensorflow] fashion-mnist with dataset api
   understanding tensorflow part 4medium.com

     * [62]tensorflow
     * [63]deep learning
     * [64]python
     * [65]machine learning
     * [66]data science

   (button)
   (button)
   (button) 267 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [67]go to the profile of ceshine lee

[68]ceshine lee

   humanist. data geek.

     (button) follow
   [69]the artificial impostor

[70]the artificial impostor

   pretending to write about data science, deep learning, and some others
   (a.k.a. the whole ai package).

     * (button)
       (button) 267
     * (button)
     *
     *

   [71]the artificial impostor
   never miss a story from the artificial impostor, when you sign up for
   medium. [72]learn more
   never miss a story from the artificial impostor
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://medium.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/5f0ebb253ad4
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-1-5f0ebb253ad4&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-1-5f0ebb253ad4&source=--------------------------nav_reg&operation=register
   8. https://medium.com/the-artificial-impostor?source=logo-lo_wob1xrhl0egh---99edadf89480
   9. https://medium.com/the-artificial-impostor/tagged/deep-learning
  10. https://medium.com/the-artificial-impostor/tagged/python
  11. https://medium.com/the-artificial-impostor/tagged/r-language
  12. https://medium.com/the-artificial-impostor/tagged/readings
  13. https://medium.com/the-artificial-impostor/tagged/data-analysis
  14. https://medium.com/@ceshine?source=post_header_lockup
  15. https://medium.com/@ceshine
  16. https://unsplash.com/photos/1rqxsxgfb0m?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  17. https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditcopytext
  18. https://github.com/keras-team/keras
  19. https://github.com/pytorch/pytorch
  20. https://github.com/ceshine/pix2pix-deblur
  21. https://github.com/ceshine/favorita_sales_forecasting
  22. https://eu.udacity.com/course/deep-learning--ud730
  23. https://github.com/ceshine/tensorflow-crash-course
  24. https://twitter.com/karpathy/status/972299022374289410
  25. https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard
  26. https://github.com/lanpa/tensorboard-pytorch
  27. https://github.com/onnx/onnx
  28. https://www.tensorflow.org/serving/
  29. https://www.tensorflow.org/mobile/tflite/
  30. http://nautil.us/blog/is-facebook-really-scarier-than-google
  31. https://www.tensorflow.org/programmers_guide/graph_viz
  32. https://www.tensorflow.org/get_started/premade_estimators
  33. https://www.tensorflow.org/programmers_guide/estimators
  34. https://www.tensorflow.org/get_started/datasets_quickstart
  35. https://www.tensorflow.org/api_docs/python/
  36. https://www.tensorflow.org/programmers_guide/graphs
  37. https://research.googleblog.com/2017/10/eager-execution-imperative-define-by.html
  38. https://github.com/tensorflow/tensorflow/blob/v1.6.0/tensorflow/contrib/eager/readme.md
  39. https://www.tensorflow.org/programmers_guide/graphs#using_wzxhzdk166wzxhzdk167tfsessionrunwzxhzdk168wzxhzdk169_to_execute_operations
  40. http://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_autograd.html
  41. https://www.tensorflow.org/versions/r1.0/programmers_guide/variables
  42. https://www.tensorflow.org/api_docs/python/tf/global_variables_initializer
  43. https://www.tensorflow.org/programmers_guide/variables
  44. https://www.tensorflow.org/programmers_guide/saved_model
  45. http://pytorch.org/docs/master/nn.html#torch.nn.module.state_dict
  46. https://www.tensorflow.org/programmers_guide/saved_model#saving_variables
  47. https://www.tensorflow.org/programmers_guide/saved_model#overview_of_saving_and_restoring_models
  48. https://stackoverflow.com/questions/37849322/how-to-understand-the-term-tensor-in-tensorflow/37870634#37870634
  49. https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/layers
  50. https://stackoverflow.com/a/43581502/4575168
  51. https://stackoverflow.com/a/43581502/4575168
  52. https://www.tensorflow.org/api_docs/python/tf/get_variable
  53. https://www.tensorflow.org/versions/r1.0/programmers_guide/variable_scope
  54. https://www.tensorflow.org/versions/r1.0/programmers_guide/variable_scope
  55. https://www.tensorflow.org/versions/r1.0/programmers_guide/variable_scope
  56. https://www.tensorflow.org/versions/r1.0/programmers_guide/variable_scope
  57. https://www.tensorflow.org/programmers_guide/estimators
  58. https://www.tensorflow.org/get_started/datasets_quickstart
  59. https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5
  60. https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7
  61. https://medium.com/the-artificial-impostor/tensorflow-fashion-mnist-with-dataset-api-cce1e3cc8cd4
  62. https://medium.com/tag/tensorflow?source=post
  63. https://medium.com/tag/deep-learning?source=post
  64. https://medium.com/tag/python?source=post
  65. https://medium.com/tag/machine-learning?source=post
  66. https://medium.com/tag/data-science?source=post
  67. https://medium.com/@ceshine?source=footer_card
  68. https://medium.com/@ceshine
  69. https://medium.com/the-artificial-impostor?source=footer_card
  70. https://medium.com/the-artificial-impostor?source=footer_card
  71. https://medium.com/the-artificial-impostor
  72. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  74. https://github.com/ceshine/tensorflow-crash-course
  75. https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5
  76. https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-3-7f6633fcc7c7
  77. https://medium.com/the-artificial-impostor/tensorflow-fashion-mnist-with-dataset-api-cce1e3cc8cd4
  78. https://medium.com/p/5f0ebb253ad4/share/twitter
  79. https://medium.com/p/5f0ebb253ad4/share/facebook
  80. https://medium.com/p/5f0ebb253ad4/share/twitter
  81. https://medium.com/p/5f0ebb253ad4/share/facebook
