   #[1]publisher [2]medium [3]alternate

   [4]homepage
   [5]homepage
   [6]sign in[7]get started

   [8]towards data science
     * [9]data science
     * [10]machine learning
     * [11]programming
     * [12]visualization
     * [13]ai
     * [14]data journalism
     * [15]contribute
     __________________________________________________________________

convolutional neural network         ii

   [16]go to the profile of mandar deshpande
   [17]mandar deshpande (button) blockedunblock (button) followfollowing
   mar 26, 2018
   [0*0moppcp99wievusk.png]

   continuing our learning from the [18]last post we will be covering the
   following topics in this post:
     * convolution over volume
     * multiple filters at one time
     * one layer of convolution network
     * understanding the dimensional change

   i have tried to explain most topics through illustrations as much as
   possible. if something isn   t easy to understand please ping me.

     let   s get started!

convolution over volume

   don   t be scared to read    volume   , its just a way of saying images with
   more than one channel i.e. rgb or any other channels.

   up until now we just had just a single channel so we were just
   concerned about the height and width of the image. but with the
   addition of more than one channel we need to take care of the filters
   involved, as they should also encompass convolution across all channels
   (3 here).
    so if the image dimension is n x n x #channels, so the filter which
   was earlier f x f would also now be required to be of dimension f x f x
   #channels.

   intuitively our input data is no more 2 dimensional but in fact 3
   dimension if you consider the channels, and hence the name volume.

   below is a simple example of convolution over volume, of an image
   having dimension 6 x 6 x 3 with 3 denoting the 3 channels r, g and b.
   similarly the filter is of dimension 3 x 3 x 3.
   [0*1fiybbmdstrkigus.png]
   fig 1. convolution over volume (colour image         3 channels) with filter
   of dimension 3 x 3

   in the given example the purpose of the filter is to detect vertical
   edges. if the edge needs to be detected only in the r channel,then only
   the weights in r channel need to set for the requirement. if you need
   to detect vertical edges across all channels, then all filter channels
   will have the same weight as demonstrated above.

multiple filters at one time

   there is a high chance that you may need to extract alot of different
   features from an image, for which you will use multiple filters. if
   individual filters are convolved separately,it will increase the
   computation time and so its more convenient to use all required filters
   at a time directly.

   convolution is carried out individually as is the case with a single
   filter, and then results of both convolutions are merged together in a
   stack to form an output volume with a 3rd dimenion representative of
   the numbers of filters.

   below example considers a 6 x 6 x 3 image as above, but we are using 2
   filters ( for vertical edge and horizontal edge) of dimension 3 x 3 x
   3. the resultant images of 4 x 4 each are stacked together to get a
   output volume of 4 x 4 x 2.
   [0*gjrce3rd3ylabd7h.png]
   fig 2. convolution over volume with multiple filter of dimensions 3 x 3

   the output dimension can be calculated for any general case using the
   following equation :
   [0*r04xglxo-7dn29fr.png]
   fig 3. equation governing the output image/signal dimension wrt input
   and filter dimension

   here, nc is the number of channels in the input image and nf are the
   number of filters used.

one layer of convolution network

   if we consider one layer of a convolution network without the pooling
   layer and flattening, then it will appear close to this:
   [0*tx-3bjreqdjrnytj.png]
   fig 4. single layer of convolution network only with relu activation

   it is important to understand that like in any other neural network, a
   convolutional neural network also has the input data x which is an
   image here and model weights given by the filters f1 and f2 i.e. w.
   once the weights and the input image is convolved we get the weighted
   output w * x and then we add the bias b.

   one of the key functions here is the relu activation function, which is
   rectified linear unit. relu helps us add non-linearity to the learning
   model and helps us better train/ learn the model weights for the
   generalized case.
   [0*9vx8ehwa2-gf3mab.png]
   fig 5. relu activation function

   for values which are below a certain threshold ( here 0), the relu
   function doesn   t update the parameters at all. it simply dies. for a
   particular training example to be considered for training, it needs to
   have a set minimum value for the neuron to be activated. also relu
   helps us reduce the vanishing and exploding gradient problem faced in
   most deep neural network, as relu provides efficient gradient
   propagation.

understanding the dimensional change

   now that we have got a conceptual understanding of what is happening in
   a single layer, lets formulize a outline for a any given layer l of a
   convolution network.
   [0*-zjhfgvymdmb9xaz.png]
   fig 6. equations governing change in input dimensions based on filters,
   padding and stride

   after computing individual components dimension, the final dimension n
   of layer l can be calculated using:
   [0*b9tiy4v7retcktpn.png]
   fig 7. dimensions of last layer l with filter size f, padding p and a
   stride of s

conclusion

   we started this post by exploring convolution over volume and then
   continued the discussion with multiple filters at once. it easy to
   understand that the volume here is simply refers to the 3 color
   channels of the image data. the discussion about a single layer of a
   convolution network is very vital for grasping the entire network
   architecture, so make sure these concepts (relu, id180)
   are clear. the final section spoke about how we can assess our model   s
   layer dimension according to the amount of padding and stride used in
   the convolution with a filter.

     in the next post, i will be explaining various id98 architectures in
     details and assessing its unique features
     __________________________________________________________________

   originally published at [19]mandro .github.io on november 18, 2017.

checkout my other posts on machine learning and deep learning :
[20]https://medium.com/@razzormandar

     * [21]machine learning
     * [22]deep learning
     * [23]id161
     * [24]neural networks
     * [25]basics

   (button)
   (button)
   (button) 73 claps
   (button) (button) (button) (button)

     (button) blockedunblock (button) followfollowing
   [26]go to the profile of mandar deshpande

[27]mandar deshpande

   deep learning and machine learning learner throughout the year.
   technology analyst (machine learning developer) 5 days a week.

     (button) follow
   [28]towards data science

[29]towards data science

   sharing concepts, ideas, and codes.

     * (button)
       (button) 73
     * (button)
     *
     *

   [30]towards data science
   never miss a story from towards data science, when you sign up for
   medium. [31]learn more
   never miss a story from towards data science
   (button) get updatesget updates

references

   visible links
   1. https://plus.google.com/103654360130207659246
   2. https://towardsdatascience.com/osd.xml
   3. android-app://com.medium.reader/https/medium.com/p/a11303f807dc
   4. https://medium.com/
   5. https://medium.com/
   6. https://medium.com/m/signin?redirect=https://towardsdatascience.com/convolutional-neural-network-ii-a11303f807dc&source=--------------------------nav_reg&operation=login
   7. https://medium.com/m/signin?redirect=https://towardsdatascience.com/convolutional-neural-network-ii-a11303f807dc&source=--------------------------nav_reg&operation=register
   8. https://towardsdatascience.com/?source=logo-lo_5j4zzkhore0h---7f60cf5620c9
   9. https://towardsdatascience.com/data-science/home
  10. https://towardsdatascience.com/machine-learning/home
  11. https://towardsdatascience.com/programming/home
  12. https://towardsdatascience.com/data-visualization/home
  13. https://towardsdatascience.com/artificial-intelligence/home
  14. https://towardsdatascience.com/data-journalism/home
  15. https://towardsdatascience.com/contribute/home
  16. https://towardsdatascience.com/@razzormandar?source=post_header_lockup
  17. https://towardsdatascience.com/@razzormandar
  18. https://towardsdatascience.com/id98-part-i-9ec412a14cb1
  19. https://mandro .github.io/2017/11/18/convolutional-neural-networks-ii/
  20. https://medium.com/@razzormandar
  21. https://towardsdatascience.com/tagged/machine-learning?source=post
  22. https://towardsdatascience.com/tagged/deep-learning?source=post
  23. https://towardsdatascience.com/tagged/computer-vision?source=post
  24. https://towardsdatascience.com/tagged/neural-networks?source=post
  25. https://towardsdatascience.com/tagged/basics?source=post
  26. https://towardsdatascience.com/@razzormandar?source=footer_card
  27. https://towardsdatascience.com/@razzormandar
  28. https://towardsdatascience.com/?source=footer_card
  29. https://towardsdatascience.com/?source=footer_card
  30. https://towardsdatascience.com/
  31. https://medium.com/@medium/personalize-your-medium-experience-with-users-publications-tags-26a41ab1ee0c#.hx4zuv3mg

   hidden links:
  33. https://medium.com/p/a11303f807dc/share/twitter
  34. https://medium.com/p/a11303f807dc/share/facebook
  35. https://medium.com/p/a11303f807dc/share/twitter
  36. https://medium.com/p/a11303f807dc/share/facebook
