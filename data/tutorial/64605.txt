                            [1]machine learning blog
                     5 algorithms to train a neural network

   by alberto quesada, [2]artelnics.

   natural selection image

   the procedure used to carry out the learning process in a neural
   network is called the training algorithm. there are many different
   training algorithms, with different characteristics and performance.

problem formulation

   the learning problem in neural networks is formulated in terms of the
   minimization of a id168, f. this function is in general,
   composed of an error and a id173 terms. the error term
   evaluates how a neural network fits the data set. on the other hand,
   the id173 term is used to prevent overfitting, by controlling
   the effective complexity of the neural network.

   the id168 depends on the adaptative parameters (biases and
   synaptic weights) in the neural network. we can conveniently group them
   together into a single n-dimensional weight vector w. the picture below
   represents the id168 f(w).

                    neural network id168 picture

   as we can see in the previous picture, the point w* is minima of the
   id168. at any point a, we can calculate the first and second
   derivatives of the id168. the first derivatives are grouped in
   the gradient vector, whose elements can be written as

                        [i]f(w) = df/dw[i] (i = 1,...,n)

   similarly, the second derivatives of the id168 can be grouped
   in the hessian matrix,

               h[i,j]f(w) = d^2f/dw[i]  dw[j] (i,j = 1,...,n)

   the problem of minimizing continuous and differentiable functions of
   many variables has been widely studied. many of the conventional
   approaches to this problem are directly applicable to that of training
   neural networks.

one-dimensional optimization

   although the id168 depends on many parameters, one-dimensional
   optimization methods are of great importance here. indeed, they are
   very often used in the training process of a neural network.

   certainly, many training algorithms first compute a training direction
   d and then a training rate    that minimizes the loss in that direction,
   f(  ). the next picture illustrates this one-dimensional function.

                      one-dimensional function picture

   the points   [1] and   [2] define an interval that contains the minimum
   of f,   *.

   in this regard, one-dimensional optimization methods search for the
   minimum of a given one-dimensional function. some of the algorithms
   which are widely used are the golden section method and brent's method.
   both reduce the bracket of a minimum until the distance between the two
   outer points in the bracket is less than a defined tolerance.

multidimensional optimization

   the learning problem for neural networks is formulated as searching of
   a parameter vector w^* at which the id168 f takes a minimum
   value. the necessary condition states that if the neural network is at
   a minimum of the id168, then the gradient is the zero vector.

   the id168 is, in general, a non-linear function of the
   parameters. as a consequence, it is not possible to find closed
   training algorithms for the minima. instead, we consider a search
   through the parameter space consisting of a succession of steps. at
   each step, the loss will decrease by adjusting the neural network
   parameters.

   in this way, to train a neural network we start with some parameter
   vector (often chosen at random). then, we generate a sequence of
   parameters, so that the id168 is reduced at each iteration of
   the algorithm. the change of loss between two steps is called the loss
   decrement. the training algorithm stops when a specified condition, or
   stopping criterion, is satisfied.

   now, we are going to describe the most important training algorithms
   for neural networks.

                       neural networks algorithm types

1. id119

   id119, also known as steepest descent, is the simplest
   training algorithm. it requires information from the gradient vector,
   and hence it is a first order method.

   let denote f(w[i]) = f[i] and    f(w[i]) = g[i]. the method begins at a
   point w[0] and, until a stopping criterion is satisfied, moves from
   w[i]to w[i+1] in the training direction d[i] = -g[i]. therefore, the
   id119 method iterates in the following way:

                   w[i+1] = w[i] - g[i]    [i], i=0,1,...

   the parameter    is the training rate. this value can either set to a
   fixed value or found by one-dimensional optimization along the training
   direction at each step. an optimal value for the training rate obtained
   by line minimization at each successive step is generally preferable.
   however, there are still many software tools that only use a fixed
   value for the training rate.

   the next picture is an activity diagram of the training process with
   id119. as we can see, the parameter vector is improved in
   two steps: first, the id119 training direction is computed.
   second, a suitable training rate is found.

                          id119 diagram

   the id119 training algorithm has the severe drawback of
   requiring many iterations for functions which have long, narrow valley
   structures. indeed, the downhill gradient is the direction in which the
   id168 decreases most rapidly, but this does not necessarily
   produce the fastest convergence. the following picture illustrates this
   issue.

                          id119 picture

   id119 is the recommended algorithm when we have very big
   neural networks, with many thousand parameters. the reason is that this
   method only stores the gradient vector (size n), and it does not store
   the hessian matrix (size n^2).

2. newton's method

   the newton's method is a second order algorithm because it makes use of
   the hessian matrix. the objective of this method is to find better
   training directions by using the second derivatives of the loss
   function.

   let denote f(w[i]) = f[i],    f(w[i]) = g[i] and hf(w[i]) = h[i].
   consider the quadratic approximation of f at w[0] using the taylor's
   series expansion

         f = f[0] + g[0]    (w - w[0]) + 0.5    (w - w[0])^2    h[0]

   h[0] is the hessian matrix of f evaluated at the point w[0]. by setting
   g equal to 0 for the minimum of f(w), we obtain the next equation

                      g = g[0] + h[0]    (w - w[0]) = 0

   therefore, starting from a parameter vector w[0], newton's method
   iterates as follows

                  w[i+1] = w[i] - h[i]^-1  g[i], i=0,1,...

   the vector h[i]^-1  g[i] is known as the newton's step. note that this
   change for the parameters may move towards a maximum rather than a
   minimum. this occurs if the hessian matrix is not positive definite.
   thus, the function evaluation is not guaranteed to be reduced at each
   iteration. in order to prevent such troubles, the newton's method
   equation is usually modified as:

              w[i+1] = w[i] - (h[i]^-1  g[i])    [i], i=0,1,...

   the training rate,   , can either be set to a fixed value or found by
   line minimization. the vector d = h[i]^-1  g[i] is now called the
   newton's training direction.

   the state diagram for the training process with the newton's method is
   depicted in the next figure. here improvement of the parameters is
   performed by obtaining first the newton's training direction and then a
   suitable training rate.

                           newton's method diagram

   the picture below illustrates the performance of this method. as we can
   see, the newton's method requires less steps than id119 to
   find the minimum value of the id168.

                            newton's method graph

   however, the newton's method has the difficulty that the exact
   evaluation of the hessian and its inverse are quite expensive in
   computational terms.

3. conjugate gradient

   the conjugate gradient method can be regarded as something intermediate
   between id119 and newton's method. it is motivated by the
   desire to accelerate the typically slow convergence associated with
   id119. this method also avoids the information requirements
   associated with the evaluation, storage, and inversion of the hessian
   matrix, as required by the newton's method.

   in the conjugate gradient training algorithm, the search is performed
   along conjugate directions which produces generally faster convergence
   than id119 directions. these training directions are
   conjugated with respect to the hessian matrix.

   let denote d the training direction vector. then, starting with an
   initial parameter vector w[0] and an initial training direction vector
   d[0] = -g[0], the conjugate gradient method constructs a sequence of
   training directions as:

                  d[i+1] = g[i+1] + d[i]    [i], i=0,1,...

   here    is called the conjugate parameter, and there are different ways
   to calculate it. two of the most used are due to fletcher and reeves
   and to polak and ribiere. for all conjugate gradient algorithms, the
   training direction is periodically reset to the negative of the
   gradient.

   the parameters are then improved according to the next expression. the
   training rate,   , is usually found by line minimization.

                   w[i+1] = w[i] + d[i]    [i], i=0,1,...

   the picture below depicts an activity diagram for the training process
   with the conjugate gradient. here improvement of the parameters is done
   by first computing the conjugate gradient training direction and then
   suitable training rate in that direction.

                         conjugate gradient diagram

   this method has proved to be more effective than id119 in
   training neural networks. since it does not require the hessian matrix,
   conjugate gradient is also recommended when we have very big neural
   networks.

4. quasi-id77

   application of the newton's method is computationally expensive, since
   it requires many operations to evaluate the hessian matrix and compute
   its inverse. alternative approaches, known as quasi-newton or variable
   metrix methods, are developed to solve that drawback. these methods,
   instead of calculating the hessian directly and then evaluating its
   inverse, build up an approximation to the inverse hessian at each
   iteration of the algorithm. this approximation is computed using only
   information on the first derivatives of the id168.

   the hessian matrix is composed of the second partial derivatives of the
   id168. the main idea behind the quasi-id77 is to
   approximate the inverse hessian by another matrix g, using only the
   first partial derivatives of the id168. then, the quasi-newton
   formula can be expressed as:

               w[i+1] = w[i] - (g[i]  g[i])    [i], i=0,1,...

   the training rate    can either be set to a fixed value or found by line
   minimization. the inverse hessian approximation g has different
   flavours. two of the most used are the davidon   fletcher   powell formula
   (dfp) and the broyden   fletcher   goldfarb   shanno formula (bfgs).

   the activity diagram of the quasi-newton training process is
   illustrated bellow. improvement of the parameters is performed by first
   obtaining the quasi-newton training direction and then finding a
   satisfactory training rate.

                       quasi newton algorithm diagram

   this is the default method to use in most cases: it is faster than
   id119 and conjugate gradient, and the exact hessian does not
   need to be computed and inverted.

5. levenberg-marquardt algorithm

   the levenberg-marquardt algorithm, also known as the damped
   least-squares method, has been designed to work specifically with loss
   functions which take the form of a sum of squared errors. it works
   without computing the exact hessian matrix. instead, it works with the
   gradient vector and the jacobian matrix.

   consider a id168 which can be expressed as a sum of squared
   errors of the form

                          f =     e[i]^2, i=0,...,m

   here m is the number of instances in the data set.

   we can define the jacobian matrix of the id168 as that
   containing the derivatives of the errors with respect to the
   parameters,

            j[i,j]f(w) = de[i]/dw[j] (i = 1,...,m & j = 1,...,n)

   where m is the number of instances in the data set and n is the number
   of parameters in the neural network. note that the size of the jacobian
   matrix is m  n.

   the gradient vector of the id168 can be computed as:

                                  f = 2 j^t  e

   here e is the vector of all error terms.

   finally, we can approximate the hessian matrix with the following
   expression.

                            hf     2 j^t  j +   i

   where    is a damping factor that ensures the positiveness of the
   hessian and i is the identity matrix.

   the next expression defines the parameters improvement process with the
   levenberg-marquardt algorithm

    w[i+1] = w[i] - (j[i]^t  j[i]+  [i]i)^-1  (2 j[i]^t  e[i]), i=0,1,...

   when the damping parameter    is zero, this is just newton's method,
   using the approximate hessian matrix. on the other hand, when    is
   large, this becomes id119 with a small training rate.

   the parameter    is initialized to be large so that first updates are
   small steps in the id119 direction. if any iteration happens
   to result in a failure, then    is increased by some factor. otherwise,
   as the loss decreases,    is decreased, so that the levenberg-marquardt
   algorithm approaches the id77. this process typically
   accelerates the convergence to the minimum.

   the picture below represents a state diagram for the training process
   of a neural network with the levenberg-marquardt algorithm. the first
   step is to calculate the loss, the gradient and the hessian
   approximation. then the damping parameter is adjusted so as to reduce
   the loss at each iteration.

                    levenberg-marquardt algorithm diagram

   as we have seen the levenberg-marquardt algorithm is a method tailored
   for functions of the type sum-of-squared-error. that makes it to be
   very fast when training neural networks measured on that kind of
   errors. however, this algorithm has some drawbacks. the first one is
   that it cannnot be applied to functions such as the root mean squared
   error or the cross id178 error. also, it is not compatible with
   id173 terms. finally, for very big data sets and neural
   networks, the jacobian matrix becomes huge, and therefore it requires a
   lot of memory. therefore, the levenberg-marquardt algorithm is not
   recommended when we have big data sets and/or neural networks.

memory and speed comparison

   the next graph depicts the computational speed and the memory
   requirements of the training algorithms discussed in this post. as we
   can see, the slowest training algorithm is usually id119,
   but it is the one requiring less memory. on the contrary, the fastest
   one might be the levenberg-marquardt algorithm, but usually requires a
   lot of memory. a good compromise might be the quasi-id77.

                  performance comparison between algorithms

   to conclude, if our neural networks has many thousands of parameters we
   can use id119 or conjugate gradient, in order to save
   memory. if we have many neural networks to train with just a few
   thousands of instances and a few hundreds of parameters, the best
   choice might be the levenberg-marquardt algorithm. in the rest of
   situations, the quasi-id77 will work well.

  related posts:

     * [3]retail store sales forecasting.
     * [4]methods binary classification.
     * [5]customer segmentation using advanced analytics.

references

   1. https://www.neuraldesigner.com/blog
   2. https://www.artelnics.com/
   3. https://www.neuraldesigner.com/blog/retail-store-sales-forecasting
   4. https://www.neuraldesigner.com/blog/methods-binary-classification
   5. https://www.neuraldesigner.com/blog/customer_segmentation_using_advanced_analytics
