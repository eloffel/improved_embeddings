   #[1]machine learning plus    feed [2]machine learning plus    comments
   feed [3]machine learning plus    id96 with gensim (python)
   comments feed

machine learning plus

   [4]machine learning plusmenu

     * [5]home
     * [6]all posts
     * data manipulation
          + [7]numpy tutorial part 1
          + [8]numpy tutorial part 2
          + [9]101 numpy exercises
          + [10]101 pandas exercises
          + close
     * predictive modeling
          + [11]id75 in r
          + [12]id28 in r
          + [13]caret package tutorial
          + [14]naive bayes algorithm from scratch
          + [15]feature selection in r
          + [16]15 id74 for classification
          + close
     * statistics
          + [17]statistical significance tests tutorial
          + close
     * nlp
          + [18]gensim tutorial
          + [19]grid search lda model (scikit learn)
          + [20]id96     lda (gensim)
          + [21]lemmatization approaches
          + [22]visualizing topic models
          + [23]cosine similarity
          + close
     * python
          + [24]list comprehensions
          + [25]parallel processing
          + [26]python @property
          + [27]debugging with pdb
          + [28]id157 tutorial
          + [29]logging guide
          + close
     * [30]plots
          + [31]matplotlib tutorial
          + [32]top 50 matplotlib visualizations
          + [33]matplotlib histogram
          + close
     * [34]time series
          + [35]time series analysis in python
          + [36]arima time series forecasting in python (guide)
          + close
     * [37]contact us
     * [38]search

the blog

   [39]blog id96 with gensim (python)

id96 with gensim (python)

   id96 is a technique to extract the hidden topics from large
   volumes of text. id44(lda) is a popular
   algorithm for id96 with excellent implementations in the
   python   s gensim package. the challenge, however, is how to extract good
   quality of topics that are clear, segregated and meaningful. this
   depends heavily on the quality of text preprocessing and the strategy
   of finding the optimal number of topics. this tutorial attempts to
   tackle both of these problems.

contents

   [40]1. introduction
   [41]2. prerequisites     download nltk stopwords and spacy model
   [42]3. import packages
   [43]4. what does lda do?
   [44]5. prepare stopwords
   [45]6. import newsgroups data
   [46]7. remove emails and newline characters
   [47]8. tokenize words and clean-up text
   [48]9. creating bigram and trigram models
   [49]10. remove stopwords, make bigrams and lemmatize
   [50]11. create the dictionary and corpus needed for id96
   [51]12. building the topic model
   [52]13. view the topics in lda model
   [53]14. compute model perplexity and coherence score
   [54]15. visualize the topics-keywords
   [55]16. building lda mallet model
   [56]17. how to find the optimal number of topics for lda?
   [57]18. finding the dominant topic in each sentence
   [58]19. find the most representative document for each topic
   [59]20. topic distribution across documents

1. introduction

   one of the primary applications of natural language processing is to
   automatically extract what topics people are discussing from large
   volumes of text. some examples of large text could be feeds from social
   media, customer reviews of hotels, movies, etc, user feedbacks, news
   stories, e-mails of customer complaints etc.

   knowing what people are talking about and understanding their problems
   and opinions is highly valuable to businesses, administrators,
   political campaigns. and it   s really hard to manually read through such
   large volumes and compile the topics.

   thus is required an automated algorithm that can read through the text
   documents and automatically output the topics discussed.

   in this tutorial, we will take a real example of the    20 newsgroups   
   dataset and use lda to extract the naturally discussed topics.

   i will be using the id44 (lda) from gensim
   package along with the mallet   s implementation (via gensim). mallet has
   an efficient implementation of the lda. it is known to run faster and
   gives better topics segregation.

   we will also extract the volume and percentage contribution of each
   topic to get an idea of how important a topic is.

   let   s begin!
   [topic-modeling-in-lda-with-gensim-1024x682.jpg]
   id96 with gensim in python. photo by jeremy bishop.

2. prerequisites     download nltk stopwords and spacy model

   we will need the stopwords from nltk and spacy   s en model for text
   pre-processing. later, we will be using the spacy model for
   lemmatization.

   lemmatization is nothing but converting a word to its root word. for
   example: the lemma of the word    machines    is    machine   . likewise,
      walking       >    walk   ,    mice       >    mouse    and so on.
# run in python console
import nltk; nltk.download('stopwords')

# run in terminal or command prompt
python3 -m spacy download en

3. import packages

   the core packages used in this tutorial are re, gensim, spacy and
   pyldavis. besides this we will also using matplotlib, numpy and pandas
   for data handling and visualization. let   s import them.
import re
import numpy as np
import pandas as pd
from pprint import pprint

# gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import coherencemodel

# spacy for lemmatization
import spacy

# plotting tools
import pyldavis
import pyldavis.gensim  # don't skip this
import matplotlib.pyplot as plt
%matplotlib inline

# enable logging for gensim - optional
import logging
logging.basicconfig(format='%(asctime)s : %(levelname)s : %(message)s', level=lo
gging.error)

import warnings
warnings.filterwarnings("ignore",category=deprecationwarning)

4. what does lda do?

   lda   s approach to id96 is it considers each document as a
   collection of topics in a certain proportion. and each topic as a
   collection of keywords, again, in a certain proportion.

   once you provide the algorithm with the number of topics, all it does
   it to rearrange the topics distribution within the documents and
   keywords distribution within the topics to obtain a good composition of
   topic-keywords distribution.

   when i say topic, what is it actually and how it is represented?

   a topic is nothing but a collection of dominant keywords that are
   typical representatives. just by looking at the keywords, you can
   identify what the topic is all about.

   the following are key factors to obtaining good segregation topics:
    1. the quality of text processing.
    2. the variety of topics the text talks about.
    3. the choice of id96 algorithm.
    4. the number of topics fed to the algorithm.
    5. the algorithms tuning parameters.

5. prepare stopwords

   we have already downloaded the stopwords. let   s import them and make it
   available in stop_words.
# nltk stop words
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
stop_words.extend(['from', 'subject', 're', 'edu', 'use'])

6. import newsgroups data

   we will be using the 20-newsgroups dataset for this exercise. this
   version of the dataset contains about 11k newsgroups posts from 20
   different topics. this is available as [60]newsgroups.json.

   this is imported using pandas.read_json and the resulting dataset has 3
   columns as shown.
# import dataset
df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/new
sgroups.json')
print(df.target_names.unique())
df.head()

['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'
 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'
 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'
 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'
 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'
 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']

   [61]20 newsgroups dataset

   20 newsgroups dataset

7. remove emails and newline characters

   as you can see there are many emails, newline and extra spaces that is
   quite distracting. let   s get rid of them using [62]id157.
# convert to list
data = df.content.values.tolist()

# remove emails
data = [re.sub('\s*@\s*\s?', '', sent) for sent in data]

# remove new line characters
data = [re.sub('\s+', ' ', sent) for sent in data]

# remove distracting single quotes
data = [re.sub("\'", "", sent) for sent in data]

pprint(data[:1])

['from: (wheres my thing) subject: what car is this!? nntp-posting-host: '
 'rac3.wam.umd.edu organization: university of maryland, college park lines: '
 '15 i was wondering if anyone out there could enlighten me on this car i saw '
 'the other day. it was a 2-door sports car, looked to be from the late 60s/ '
 'early 70s. it was called a bricklin. the doors were really small. in '
 'addition, the front bumper was separate from the rest of the body. this is '
 'all i know.  (..truncated..)]

   after removing the emails and extra spaces, the text still looks messy.
   it is not ready for the lda to consume. you need to break down each
   sentence into a list of words through id121, while clearing up
   all the messy text in the process.

   gensim   s simple_preprocess is great for this.

8. tokenize words and clean-up text

   let   s tokenize each sentence into a list of words, removing
   punctuations and unnecessary characters altogether.

   gensim   s simple_preprocess() is great for this. additionally i have set
   deacc=true to remove the punctuations.
def sent_to_words(sentences):
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=true))  # deac
c=true removes punctuations

data_words = list(sent_to_words(data))

print(data_words[:1])

[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp
', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university',
'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone'
, 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the',
 'other', 'day', (..truncated..))]]

9. creating bigram and trigram models

   bigrams are two words frequently occurring together in the document.
   trigrams are 3 words frequently occurring.

   some examples in our example are:    front_bumper   ,    oil_leak   ,
      maryland_college_park    etc.

   gensim   s phrases model can build and implement the bigrams, trigrams,
   quadgrams and more. the two important arguments to phrases are
   min_count and threshold. the higher the values of these param, the
   harder it is for words to be combined to bigrams.
# build the bigram and trigram models
bigram = gensim.models.phrases(data_words, min_count=5, threshold=100) # higher
threshold fewer phrases.
trigram = gensim.models.phrases(bigram[data_words], threshold=100)

# faster way to get a sentence clubbed as a trigram/bigram
bigram_mod = gensim.models.phrases.phraser(bigram)
trigram_mod = gensim.models.phrases.phraser(trigram)

# see trigram example
print(trigram_mod[bigram_mod[data_words[0]]])

['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_
posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_
college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'cou
ld', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it',
 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'e
arly', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'sma
ll', 'in', 'addition', 'the', 'front_bumper' (..truncated..)]

10. remove stopwords, make bigrams and lemmatize

   the bigrams model is ready. let   s define the functions to remove the
   stopwords, make bigrams and lemmatization and call them sequentially.
# define functions for stopwords, bigrams, trigrams and lemmatization
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_wo
rds] for doc in texts]

def make_bigrams(texts):
    return [bigram_mod[doc] for doc in texts]

def make_trigrams(texts):
    return [trigram_mod[bigram_mod[doc]] for doc in texts]

def lemmatization(texts, allowed_postags=['noun', 'adj', 'verb', 'adv']):
    """https://spacy.io/api/annotation"""
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed
_postags])
    return texts_out

   let   s call the functions in order.
# remove stop words
data_words_nostops = remove_stopwords(data_words)

# form bigrams
data_words_bigrams = make_bigrams(data_words_nostops)

# initialize spacy 'en' model, keeping only tagger component (for efficiency)
# python3 -m spacy download en
nlp = spacy.load('en', disable=['parser', 'ner'])

# do lemmatization keeping only noun, adj, vb, adv
data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['noun', 'ad
j', 'verb', 'adv'])

print(data_lemmatized[:1])

[['where', 's', 'thing', 'car', 'nntp_post', 'host', 'rac_wam', 'umd', 'organiza
tion', 'university', 'maryland_college', 'park', 'line', 'wonder', 'anyone', 'co
uld', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late',
'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumpe
r', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'en
gine', 'spec', 'year', 'production', 'car', 'make', 'history', 'whatev', 'info',
 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]

11. create the dictionary and corpus needed for id96

   the two main inputs to the lda topic model are the dictionary( word)
   and the corpus. let   s create them.
# create dictionary
 word = corpora.dictionary(data_lemmatized)

# create corpus
texts = data_lemmatized

# term document frequency
corpus = [ word.doc2bow(text) for text in texts]

# view
print(corpus[:1])

[[(0, 1), (1, 2), (2, 1), (3, 1), (4, 1), (5, 1), (6, 5), (7, 1), (8, 1), (9, 2)
, (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1
), (19, 1), (20, 1), (21, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27,
1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36,
 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45
, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1)]]

   gensim creates a unique id for each word in the document. the produced
   corpus shown above is a mapping of (word_id, word_frequency).

   for example, (0, 1) above implies, word id 0 occurs once in the first
   document. likewise, word id 1 occurs twice and so on.

   this is used as the input by the lda model.

   if you want to see what word a given id corresponds to, pass the id as
   a key to the dictionary.
 word[0]

'addition'

   or, you can see a human-readable form of the corpus itself.
# human readable format of corpus (term-frequency)
[[( word[id], freq) for id, freq in cp] for cp in corpus[:1]]

[[('addition', 1),
  ('anyone', 2),
  ('body', 1),
  ('bricklin', 1),
  ('bring', 1),
  ('call', 1),
  ('car', 5),
  ('could', 1),
  ('day', 1),
  ('door', 2),
  ('early', 1),
  ('engine', 1),
  ('enlighten', 1),
  ('front_bumper', 1),
  ('maryland_college', 1),
  (..truncated..)]]

   alright, without digressing further let   s jump back on track with the
   next step: building the topic model.

12. building the topic model

   we have everything required to train the lda model. in addition to the
   corpus and dictionary, you need to provide the number of topics as
   well.

   apart from that, alpha and eta are hyperparameters that affect sparsity
   of the topics. according to the gensim docs, both defaults to
   1.0/num_topics prior.

   chunksize is the number of documents to be used in each training chunk.
   update_every determines how often the model parameters should be
   updated and passes is the total number of training passes.
# build lda model
lda_model = gensim.models.ldamodel.ldamodel(corpus=corpus,
                                            word= word,
                                           num_topics=20,
                                           random_state=100,
                                           update_every=1,
                                           chunksize=100,
                                           passes=10,
                                           alpha='auto',
                                           per_word_topics=true)

13. view the topics in lda model

   the above lda model is built with 20 different topics where each topic
   is a combination of keywords and each keyword contributes a certain
   weightage to the topic.

   you can see the keywords for each topic and the weightage(importance)
   of each keyword using lda_model.print_topics() as shown next.
# print the keyword in the 10 topics
pprint(lda_model.print_topics())
doc_lda = lda_model[corpus]

[(0,
  '0.016*"car" + 0.014*"power" + 0.010*"light" + 0.009*"drive" + 0.007*"mount" '
  '+ 0.007*"controller" + 0.007*"cool" + 0.007*"engine" + 0.007*"back" + '
  '0.006*"turn"'),
 (1,
  '0.072*"line" + 0.066*"organization" + 0.037*"write" + 0.032*"article" + '
  '0.028*"university" + 0.027*"nntp_post" + 0.026*"host" + 0.016*"reply" + '
  '0.014*"get" + 0.013*"thank"'),
 (2,
  '0.017*"patient" + 0.011*"study" + 0.010*"slave" + 0.009*"wing" + '
  '0.009*"disease" + 0.008*"food" + 0.008*"eat" + 0.008*"pain" + '
  '0.007*"treatment" + 0.007*"syndrome"'),
 (3,
  '0.013*"key" + 0.009*"use" + 0.009*"may" + 0.007*"public" + 0.007*"system" + '
  '0.007*"order" + 0.007*"government" + 0.006*"state" + 0.006*"provide" + '
  '0.006*"law"'),
 (4,
  '0.568*"ax" + 0.007*"rlk" + 0.005*"tufts_university" + 0.004*"ei" + '
  '0.004*"m" + 0.004*"vesa" + 0.004*"differential" + 0.004*"chz" + 0.004*"lk" '
  '+ 0.003*"weekly"'),
 (5,
  '0.029*"player" + 0.015*"master" + 0.015*"steven" + 0.009*"tor" + '
  '0.009*"van" + 0.008*"king" + 0.008*"scripture" + 0.007*"cal" + '
  '0.007*"helmet" + 0.007*"det"'),
 (6,
  '0.028*"system" + 0.020*"problem" + 0.019*"run" + 0.018*"use" + 0.016*"work" '
  '+ 0.015*"do" + 0.013*"window" + 0.013*"driver" + 0.013*"bit" + 0.012*"set"'),
 (7,
  '0.017*"israel" + 0.011*"israeli" + 0.010*"war" + 0.010*"armenian" + '
  '0.008*"kill" + 0.008*"soldier" + 0.008*"attack" + 0.008*"government" + '
  '0.007*"lebanese" + 0.007*"greek"'),
 (8,
  '0.018*"money" + 0.018*"year" + 0.016*"pay" + 0.012*"car" + 0.010*"drug" + '
  '0.010*"president" + 0.009*"rate" + 0.008*"face" + 0.007*"license" + '
  '0.007*"american"'),
 (9,
  '0.028*"god" + 0.020*"evidence" + 0.018*"christian" + 0.012*"believe" + '
  '0.012*"reason" + 0.011*"faith" + 0.009*"exist" + 0.008*"bible" + '
  '0.008*"religion" + 0.007*"claim"'),
 (10,
  '0.030*"physical" + 0.028*"science" + 0.012*"direct" + 0.012*"st" + '
  '0.012*"scientific" + 0.009*"waste" + 0.009*"jeff" + 0.008*"cub" + '
  '0.008*"brown" + 0.008*"msg"'),
 (11,
  '0.016*"wire" + 0.011*"keyboard" + 0.011*"md" + 0.009*"pm" + 0.008*"air" + '
  '0.008*"input" + 0.008*"fbi" + 0.007*"listen" + 0.007*"tube" + '
  '0.007*"koresh"'),
 (12,
  '0.016*"motif" + 0.014*"serial_number" + 0.013*"son" + 0.013*"father" + '
  '0.011*"choose" + 0.009*"server" + 0.009*"event" + 0.009*"value" + '
  '0.007*"collin" + 0.007*"prediction"'),
 (13,
  '0.098*"_" + 0.043*"max" + 0.015*"dn" + 0.011*"cx" + 0.009*"eeg" + '
  '0.008*"gateway" + 0.008*"c" + 0.005*"mu" + 0.005*"mr" + 0.005*"eg"'),
 (14,
  '0.024*"book" + 0.009*"april" + 0.007*"group" + 0.007*"page" + '
  '0.007*"new_york" + 0.007*"iran" + 0.006*"united_state" + 0.006*"author" + '
  '0.006*"include" + 0.006*"club"'),
 (15,
  '0.020*"would" + 0.017*"say" + 0.016*"people" + 0.016*"think" + 0.014*"make" '
  '+ 0.014*"go" + 0.013*"know" + 0.012*"see" + 0.011*"time" + 0.011*"get"'),
 (16,
  '0.026*"file" + 0.017*"program" + 0.012*"window" + 0.012*"version" + '
  '0.011*"entry" + 0.011*"software" + 0.011*"image" + 0.011*"color" + '
  '0.010*"source" + 0.010*"available"'),
 (17,
  '0.027*"game" + 0.027*"team" + 0.020*"year" + 0.017*"play" + 0.016*"win" + '
  '0.010*"good" + 0.009*"season" + 0.008*"fan" + 0.007*"run" + 0.007*"score"'),
 (18,
  '0.036*"drive" + 0.024*"card" + 0.020*"mac" + 0.017*"sale" + 0.014*"cpu" + '
  '0.010*"price" + 0.010*"disk" + 0.010*"board" + 0.010*"pin" + 0.010*"chip"'),
 (19,
  '0.030*"space" + 0.010*"sphere" + 0.010*"earth" + 0.009*"item" + '
  '0.008*"launch" + 0.007*"moon" + 0.007*"mission" + 0.007*"nasa" + '
  '0.007*"orbit" + 0.006*"research"')]

   how to interpret this?

   topic 0 is a represented as _0.016   car    + 0.014   power    + 0.010   light    +
   0.009   drive    + 0.007   mount    + 0.007   controller    + 0.007   cool    +
   0.007   engine    + 0.007   back    +    0.006   turn   .

   it means the top 10 keywords that contribute to this topic are:    car   ,
      power   ,    light   .. and so on and the weight of    car    on topic 0 is
   0.016.

   the weights reflect how important a keyword is to that topic.

   looking at these keywords, can you guess what this topic could be? you
   may summarise it either are    cars    or    automobiles   .

   likewise, can you go through the remaining topic keywords and judge
   what the topic is?
   [63]inferring topic from keywords

   inferring topic from keywords

14. compute model perplexity and coherence score

   model perplexity and [64]topic coherence provide a convenient measure
   to judge how good a given topic model is. in my experience, topic
   coherence score, in particular, has been more helpful.
# compute perplexity
print('\nperplexity: ', lda_model.log_perplexity(corpus))  # a measure of how go
od the model is. lower the better.

# compute coherence score
coherence_model_lda = coherencemodel(model=lda_model, texts=data_lemmatized, dic
tionary= word, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\ncoherence score: ', coherence_lda)

perplexity:  -8.86067503009

coherence score:  0.532947587081

   there you have a coherence score of 0.53.

15. visualize the topics-keywords

   now that the lda model is built, the next step is to examine the
   produced topics and the associated keywords. there is no better tool
   than pyldavis package   s interactive chart and is designed to work well
   with jupyter notebooks.
# visualize the topics
pyldavis.enable_notebook()
vis = pyldavis.gensim.prepare(lda_model, corpus,  word)
vis

   [65]pyldavis output

   pyldavis output

   so how to infer pyldavis   s output?

   each bubble on the left-hand side plot represents a topic. the larger
   the bubble, the more prevalent is that topic.

   a good topic model will have fairly big, non-overlapping bubbles
   scattered throughout the chart instead of being clustered in one
   quadrant.

   a model with too many topics, will typically have many overlaps, small
   sized bubbles clustered in one region of the chart.

   alright, if you move the cursor over one of the bubbles, the words and
   bars on the right-hand side will update. these words are the salient
   keywords that form the selected topic.

   we have successfully built a good looking topic model.

   given our prior knowledge of the number of natural topics in the
   document, finding the best model was fairly straightforward.

   upnext, we will improve upon this model by using mallet   s version of
   lda algorithm and then we will focus on how to arrive at the optimal
   number of topics given any large corpus of text.

16. building lda mallet model

   so far you have seen gensim   s inbuilt version of the lda algorithm.
   mallet   s version, however, often gives a better quality of topics.

   gensim provides a wrapper to implement mallet   s lda from within gensim
   itself. you only need to [66]download the zipfile, unzip it and provide
   the path to mallet in the unzipped directory to
   gensim.models.wrappers.ldamallet. see how i have done this below.
# download file: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip
mallet_path = 'path/to/mallet-2.0.8/bin/mallet' # update this path
ldamallet = gensim.models.wrappers.ldamallet(mallet_path, corpus=corpus, num_top
ics=20,  word= word)

# show topics
pprint(ldamallet.show_topics(formatted=false))

# compute coherence score
coherence_model_ldamallet = coherencemodel(model=ldamallet, texts=data_lemmatize
d, dictionary= word, coherence='c_v')
coherence_ldamallet = coherence_model_ldamallet.get_coherence()
print('\ncoherence score: ', coherence_ldamallet)

[(13,
  [('god', 0.022175351915726671),
   ('christian', 0.017560827817656381),
   ('people', 0.0088794630371958616),
   ('bible', 0.008215251235200895),
   ('word', 0.0077491376899412696),
   ('church', 0.0074112053696280414),
   ('religion', 0.0071198844038407759),
   ('man', 0.0067936049221590383),
   ('faith', 0.0067469935676330757),
   ('love', 0.0064556726018458093)]),
 (1,
  [('organization', 0.10977647987951586),
   ('line', 0.10182379194445974),
   ('write', 0.097397469098389255),
   ('article', 0.082483883409554246),
   ('nntp_post', 0.079894209047330425),
   ('host', 0.069737542931658306),
   ('university', 0.066303010266865026),
   ('reply', 0.02255404338163719),
   ('distribution_world', 0.014362591143681011),
   ('usa', 0.010928058478887726)]),
 (8,
  [('file', 0.02816690014008405),
   ('line', 0.021396171035954908),
   ('problem', 0.013508104862917751),
   ('program', 0.013157894736842105),
   ('read', 0.012607564538723234),
   ('follow', 0.01110666399839904),
   ('number', 0.011056633980388232),
   ('set', 0.010522980454939631),
   ('error', 0.010172770328863986),
   ('write', 0.010039356947501835)]),
 (7,
  [('include', 0.0091670556506405262),
   ('information', 0.0088169700741662776),
   ('national', 0.0085576474249260924),
   ('year', 0.0077667133447435295),
   ('report', 0.0070406099268710129),
   ('university', 0.0070406099268710129),
   ('book', 0.0068979824697889113),
   ('program', 0.0065219646283906432),
   ('group', 0.0058866241377521916),
   ('service', 0.0057180644157460714)]),
 (..truncated..)]

coherence score:  0.632431683088

   just by changing the lda algorithm, we increased the coherence score
   from .53 to .63. not bad!

17. how to find the optimal number of topics for lda?

   my approach to finding the optimal number of topics is to build many
   lda models with different values of number of topics (k) and pick the
   one that gives the highest coherence value.

   choosing a    k    that marks the end of a rapid growth of topic coherence
   usually offers meaningful and interpretable topics. picking an even
   higher value can sometimes provide more granular sub-topics.

   if you see the same keywords being repeated in multiple topics, it   s
   probably a sign that the    k    is too large.

   the compute_coherence_values() (see below) trains multiple lda models
   and provides the models and their corresponding coherence scores.
def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    compute c_v coherence for various number of topics

    parameters:
    ----------
    dictionary : gensim dictionary
    corpus : gensim corpus
    texts : list of input texts
    limit : max num of topics

    returns:
    -------
    model_list : list of lda topic models
    coherence_values : coherence values corresponding to the lda model with resp
ective number of topics
    """
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.wrappers.ldamallet(mallet_path, corpus=corpus, num
_topics=num_topics,  word= word)
        model_list.append(model)
        coherencemodel = coherencemodel(model=model, texts=texts, dictionary=dic
tionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

# can take a long time to run.
model_list, coherence_values = compute_coherence_values(dictionary= word, corp
us=corpus, texts=data_lemmatized, start=2, limit=40, step=6)

# show graph
limit=40; start=2; step=6;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("num topics")
plt.ylabel("coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()

   [67]choosing the optimal number of lda topics

   choosing the optimal number of lda topics
# print the coherence scores
for m, cv in zip(x, coherence_values):
    print("num topics =", m, " has coherence value of", round(cv, 4))

num topics = 2  has coherence value of 0.4451
num topics = 8  has coherence value of 0.5943
num topics = 14  has coherence value of 0.6208
num topics = 20  has coherence value of 0.6438
num topics = 26  has coherence value of 0.643
num topics = 32  has coherence value of 0.6478
num topics = 38  has coherence value of 0.6525

   if the coherence score seems to keep increasing, it may make better
   sense to pick the model that gave the highest cv before flattening out.
   this is exactly the case here.

   so for further steps i will choose the model with 20 topics itself.
# select the model and print the topics
optimal_model = model_list[3]
model_topics = optimal_model.show_topics(formatted=false)
pprint(optimal_model.print_topics(num_words=10))

[(0,
  '0.025*"game" + 0.018*"team" + 0.016*"year" + 0.014*"play" + 0.013*"good" + '
  '0.012*"player" + 0.011*"win" + 0.007*"season" + 0.007*"hockey" + '
  '0.007*"fan"'),
 (1,
  '0.021*"window" + 0.015*"file" + 0.012*"image" + 0.010*"program" + '
  '0.010*"version" + 0.009*"display" + 0.009*"server" + 0.009*"software" + '
  '0.008*"graphic" + 0.008*"application"'),
 (2,
  '0.021*"gun" + 0.019*"state" + 0.016*"law" + 0.010*"people" + 0.008*"case" + '
  '0.008*"crime" + 0.007*"government" + 0.007*"weapon" + 0.007*"police" + '
  '0.006*"firearm"'),
 (3,
  '0.855*"ax" + 0.062*"max" + 0.002*"tm" + 0.002*"qax" + 0.001*"mf" + '
  '0.001*"giz" + 0.001*"_" + 0.001*"ml" + 0.001*"fp" + 0.001*"mr"'),
 (4,
  '0.020*"file" + 0.020*"line" + 0.013*"read" + 0.013*"set" + 0.012*"program" '
  '+ 0.012*"number" + 0.010*"follow" + 0.010*"error" + 0.010*"change" + '
  '0.009*"entry"'),
 (5,
  '0.021*"god" + 0.016*"christian" + 0.008*"religion" + 0.008*"bible" + '
  '0.007*"life" + 0.007*"people" + 0.007*"church" + 0.007*"word" + 0.007*"man" '
  '+ 0.006*"faith"'),
 (..truncated..)]

   those were the topics for the chosen lda model.

18. finding the dominant topic in each sentence

   one of the practical application of id96 is to determine what
   topic a given document is about.

   to find that, we find the topic number that has the highest percentage
   contribution in that document.

   the format_topics_sentences() function below nicely aggregates this
   information in a presentable table.
def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):
    # init output
    sent_topics_df = pd.dataframe()

    # get main topic in each document
    for i, row in enumerate(ldamodel[corpus]):
        row = sorted(row, key=lambda x: (x[1]), reverse=true)
        # get the dominant topic, perc contribution and keywords for each docume
nt
        for j, (topic_num, prop_topic) in enumerate(row):
            if j == 0:  # => dominant topic
                wp = ldamodel.show_topic(topic_num)
                topic_keywords = ", ".join([word for word, prop in wp])
                sent_topics_df = sent_topics_df.append(pd.series([int(topic_num)
, round(prop_topic,4), topic_keywords]), ignore_index=true)
            else:
                break
    sent_topics_df.columns = ['dominant_topic', 'perc_contribution', 'topic_keyw
ords']

    # add original text to the end of the output
    contents = pd.series(texts)
    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)
    return(sent_topics_df)


df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus
=corpus, texts=data)

# format
df_dominant_topic = df_topic_sents_keywords.reset_index()
df_dominant_topic.columns = ['document_no', 'dominant_topic', 'topic_perc_contri
b', 'keywords', 'text']

# show
df_dominant_topic.head(10)

   [68]dominant topic for each document

   dominant topic for each document

19. find the most representative document for each topic

   sometimes just the topic keywords may not be enough to make sense of
   what a topic is about. so, to help with understanding the topic, you
   can find the documents a given topic has contributed to the most and
   infer the topic by reading that document. whew!!
# group top 5 sentences under each topic
sent_topics_sorteddf_mallet = pd.dataframe()

sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('dominant_topic')

for i, grp in sent_topics_outdf_grpd:
    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,
                                             grp.sort_values(['perc_contribution
'], ascending=[0]).head(1)],
                                            axis=0)

# reset index
sent_topics_sorteddf_mallet.reset_index(drop=true, inplace=true)

# format
sent_topics_sorteddf_mallet.columns = ['topic_num', "topic_perc_contrib", "keywo
rds", "text"]

# show
sent_topics_sorteddf_mallet.head()

   [69]most representative topic for each document

   most representative topic for each document

   the tabular output above actually has 20 rows, one each for a topic. it
   has the topic number, the keywords, and the most representative
   document. the perc_contribution column is nothing but the percentage
   contribution of the topic in the given document.

20. topic distribution across documents

   finally, we want to understand the volume and distribution of topics in
   order to judge how widely it was discussed. the below table exposes
   that information.
# number of documents for each topic
topic_counts = df_topic_sents_keywords['dominant_topic'].value_counts()

# percentage of documents for each topic
topic_contribution = round(topic_counts/topic_counts.sum(), 4)

# topic number and keywords
topic_num_keywords = df_topic_sents_keywords[['dominant_topic', 'topic_keywords'
]]

# concatenate column wise
df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribu
tion], axis=1)

# change column names
df_dominant_topics.columns = ['dominant_topic', 'topic_keywords', 'num_documents
', 'perc_documents']

# show
df_dominant_topics

   [70]topic volume distribution

   topic volume distribution

21. conclusion

   we started with understanding what id96 can do. we built a
   basic topic model using gensim   s lda and visualize the topics using
   pyldavis. then we built mallet   s lda implementation. you saw how to
   find the optimal number of topics using coherence scores and how you
   can come to a logical understanding of how to choose the optimal model.

   finally we saw how to aggregate and present the results to generate
   insights that may be in a more actionable.

   hope you enjoyed reading this. i would appreciate if you leave your
   thoughts in the comments section below.

   edit: i see some of you are experiencing errors while using the lda
   mallet and i don   t have a solution for some of the issues. so, i   ve
   implemented a workaround and more useful [71]topic model
   visualizations. hope you will find it helpful.

related

   tags:[72]gensim, [73]nlp, [74]python, [75]id96
   [76][ezoic.png] report this ad

   search ____________________

recent posts

     * [77]python logging     simplest guide with full code and examples
     * [78]matplotlib histogram     how to visualize distributions in python
     * [79]arima model     complete guide to time series forecasting in
       python
     * [80]time series analysis in python     a comprehensive guide with
       examples
     * [81]matplotlib tutorial     a complete guide to python plot w/
       examples
     * [82]id96 visualization     how to present the results of
       lda models?
     * [83]top 50 matplotlib visualizations     the master plots (with full
       python code)
     * [84]list comprehensions in python     my simplified guide
     * [85]python @property explained     how to use and when? (full
       examples)
     * [86]how naive bayes algorithm works? (with example and full code)
     * [87]parallel processing in python     a practical guide with examples
     * [88]cosine similarity     understanding the math and how it works
       (with python codes)
     * [89]gensim tutorial     a complete beginners guide
     * [90]lemmatization approaches with examples in python
     * [91]feature selection     ten effective techniques with examples
     * [92]101 pandas exercises for data analysis
     * [93]lda in python     how to grid search best topic models?
     * [94]id96 with gensim (python)
     * [95]python debugging with pdb
     * [96]caret package     a practical guide to machine learning in r
     * [97]101 numpy exercises for data analysis (python)
     * [98]numpy tutorial part 2     vital functions for data analysis
     * [99]numpy tutorial part 1     introduction to arrays
     * [100]python id157 tutorial and examples: a simplified
       guide
     * [101]top 15 id74 for classification models

tags

   [102]@property [103]bigrams [104]classification [105]corpus [106]cosine
   similarity [107]data manipulation [108]debugging [109]doc2vec
   [110]id74 [111]fasttext [112]feature selection
   [113]gensim [114]klar [115]lda [116]lemmatization [117]linear
   regression [118]logistic [119]lsi [120]matplotlib [121]multiprocessing
   [122]naive bayes [123]nlp [124]nltk [125]numpy [126]pandas
   [127]parallel processing [128]phraser [129]practice exercise
   [130]python [131]r [132]regex [133]regression [134]residual analysis
   [135]scikit learn [136]significance tests [137]soft cosine similarity
   [138]spacy [139]summarization [140]taggeddocument [141]textblob
   [142]tfidf [143]time series [144]id96 [145]visualization
   [146]id97
   [147][ezoic.png] report this ad[148] [ezoic.png] report this ad

     * [149]home
     * [150]all posts
     * [151]data manipulation
     * [152]predictive modeling
     * [153]statistics
     * [154]nlp
     * [155]python
     * [156]plots
     * [157]time series
     * [158]contact us

      copyright by machine learning plus | all rights reserved |
   [159]privacy policy | [160]terms of use | [161]about
   type and press    enter    to search ____________________

   [p?c1=2&c2=20015427&cv=2.0&cj=1]

   quantcast

references

   visible links
   1. https://www.machinelearningplus.com/feed/
   2. https://www.machinelearningplus.com/comments/feed/
   3. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/feed/
   4. https://www.machinelearningplus.com/
   5. https://www.machinelearningplus.com/
   6. https://www.machinelearningplus.com/blog/
   7. https://www.machinelearningplus.com/python/numpy-tutorial-part1-array-python-examples/
   8. https://www.machinelearningplus.com/python/numpy-tutorial-python-part2/
   9. https://www.machinelearningplus.com/python/101-numpy-exercises-python/
  10. https://www.machinelearningplus.com/python/101-pandas-exercises-python/
  11. https://www.machinelearningplus.com/machine-learning/complete-introduction-linear-regression-r/
  12. https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/
  13. https://www.machinelearningplus.com/machine-learning/caret-package/
  14. https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/
  15. https://www.machinelearningplus.com/machine-learning/feature-selection/
  16. https://www.machinelearningplus.com/machine-learning/evaluation-metrics-classification-models-r/
  17. https://www.machinelearningplus.com/statistics/statistical-significance-tests-r/
  18. https://www.machinelearningplus.com/nlp/gensim-tutorial/
  19. https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/
  20. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/
  21. https://www.machinelearningplus.com/nlp/lemmatization-examples-python/
  22. https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/
  23. https://www.machinelearningplus.com/nlp/cosine-similarity/
  24. https://www.machinelearningplus.com/python/list-comprehensions-in-python/
  25. https://www.machinelearningplus.com/python/parallel-processing-python/
  26. https://www.machinelearningplus.com/python/python-property/
  27. https://www.machinelearningplus.com/python/python-debugging/
  28. https://www.machinelearningplus.com/python/python-regex-tutorial-examples/
  29. https://www.machinelearningplus.com/python/python-logging-guide/
  30. https://www.machinelearningplus.com/category/plots/
  31. https://www.machinelearningplus.com/plots/matplotlib-tutorial-complete-guide-python-plot-examples/
  32. https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/
  33. https://www.machinelearningplus.com/plots/matplotlib-histogram-python-examples/
  34. https://www.machinelearningplus.com/category/time-series/
  35. https://www.machinelearningplus.com/time-series/time-series-analysis-python/
  36. https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/
  37. https://www.machinelearningplus.com/contact-us/
  38. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/
  39. https://www.machinelearningplus.com/blog/
  40. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#1introduction
  41. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#2prerequisitesdownloadnltkstopwordsandspacymodelforlemmatization
  42. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#3importpackages
  43. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#4whatdoesldado
  44. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#5preparestopwords
  45. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#6importnewsgroupsdata
  46. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#7removeemailsandnewlinecharacters
  47. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#8tokenizewordsandcleanuptextusingsimple_preprocess
  48. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#9createbigramandtrigrammodels
  49. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#10removestopwordsmakebigramsandlemmatize
  50. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#11createthedictionaryandcorpusneededfortopicmodeling
  51. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#12buildingthetopicmodel
  52. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#13viewthetopicsinldamodel
  53. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#14computemodelperplexityandcoherencescore
  54. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#15visualizethetopicskeywords
  55. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#16buildingldamalletmodel
  56. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda
  57. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#18dominanttopicineachsentence
  58. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#19findthemostrepresentativedocumentforeachtopic
  59. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#20topicdistributionacrossdocuments
  60. https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json
  61. https://www.machinelearningplus.com/wp-content/uploads/2018/03/input_dataset.png
  62. https://www.machinelearningplus.com/python/python-regex-tutorial-examples/
  63. https://www.machinelearningplus.com/wp-content/uploads/2018/03/inferring-topic-from-keywords.png
  64. https://rare-technologies.com/what-is-topic-coherence/
  65. https://www.machinelearningplus.com/wp-content/uploads/2018/03/pyldavis.png
  66. https://www.machinelearningplus.com/wp-content/uploads/2018/03/mallet-2.0.8.zip
  67. https://www.machinelearningplus.com/wp-content/uploads/2018/03/choosing-the-optimal-number-of-lda-topics-2.png
  68. https://www.machinelearningplus.com/wp-content/uploads/2018/03/dominant_topic_for_each_document.png
  69. https://www.machinelearningplus.com/wp-content/uploads/2018/03/most_representative_topic_for_each_document.png
  70. https://www.machinelearningplus.com/wp-content/uploads/2018/03/topic-volume-distribution-1.png
  71. https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/
  72. https://www.machinelearningplus.com/tag/gensim/
  73. https://www.machinelearningplus.com/tag/nlp/
  74. https://www.machinelearningplus.com/tag/python/
  75. https://www.machinelearningplus.com/tag/topic-modeling/
  76. https://www.ezoic.com/what-is-ezoic/
  77. https://www.machinelearningplus.com/python/python-logging-guide/
  78. https://www.machinelearningplus.com/plots/matplotlib-histogram-python-examples/
  79. https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/
  80. https://www.machinelearningplus.com/time-series/time-series-analysis-python/
  81. https://www.machinelearningplus.com/plots/matplotlib-tutorial-complete-guide-python-plot-examples/
  82. https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/
  83. https://www.machinelearningplus.com/plots/top-50-matplotlib-visualizations-the-master-plots-python/
  84. https://www.machinelearningplus.com/python/list-comprehensions-in-python/
  85. https://www.machinelearningplus.com/python/python-property/
  86. https://www.machinelearningplus.com/predictive-modeling/how-naive-bayes-algorithm-works-with-example-and-full-code/
  87. https://www.machinelearningplus.com/python/parallel-processing-python/
  88. https://www.machinelearningplus.com/nlp/cosine-similarity/
  89. https://www.machinelearningplus.com/nlp/gensim-tutorial/
  90. https://www.machinelearningplus.com/nlp/lemmatization-examples-python/
  91. https://www.machinelearningplus.com/machine-learning/feature-selection/
  92. https://www.machinelearningplus.com/python/101-pandas-exercises-python/
  93. https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/
  94. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/
  95. https://www.machinelearningplus.com/python/python-debugging/
  96. https://www.machinelearningplus.com/machine-learning/caret-package/
  97. https://www.machinelearningplus.com/python/101-numpy-exercises-python/
  98. https://www.machinelearningplus.com/python/numpy-tutorial-python-part2/
  99. https://www.machinelearningplus.com/python/numpy-tutorial-part1-array-python-examples/
 100. https://www.machinelearningplus.com/python/python-regex-tutorial-examples/
 101. https://www.machinelearningplus.com/machine-learning/evaluation-metrics-classification-models-r/
 102. https://www.machinelearningplus.com/tag/property/
 103. https://www.machinelearningplus.com/tag/bigrams/
 104. https://www.machinelearningplus.com/tag/classification/
 105. https://www.machinelearningplus.com/tag/corpus/
 106. https://www.machinelearningplus.com/tag/cosine-similarity/
 107. https://www.machinelearningplus.com/tag/data-manipulation/
 108. https://www.machinelearningplus.com/tag/debugging/
 109. https://www.machinelearningplus.com/tag/doc2vec/
 110. https://www.machinelearningplus.com/tag/evaluation-metrics/
 111. https://www.machinelearningplus.com/tag/fasttext/
 112. https://www.machinelearningplus.com/tag/feature-selection/
 113. https://www.machinelearningplus.com/tag/gensim/
 114. https://www.machinelearningplus.com/tag/klar/
 115. https://www.machinelearningplus.com/tag/lda/
 116. https://www.machinelearningplus.com/tag/lemmatization/
 117. https://www.machinelearningplus.com/tag/linear-regression/
 118. https://www.machinelearningplus.com/tag/logistic/
 119. https://www.machinelearningplus.com/tag/lsi/
 120. https://www.machinelearningplus.com/tag/matplotlib/
 121. https://www.machinelearningplus.com/tag/multiprocessing/
 122. https://www.machinelearningplus.com/tag/naive-bayes/
 123. https://www.machinelearningplus.com/tag/nlp/
 124. https://www.machinelearningplus.com/tag/nltk/
 125. https://www.machinelearningplus.com/tag/numpy/
 126. https://www.machinelearningplus.com/tag/pandas/
 127. https://www.machinelearningplus.com/tag/parallel-processing/
 128. https://www.machinelearningplus.com/tag/phraser/
 129. https://www.machinelearningplus.com/tag/practice-exercise/
 130. https://www.machinelearningplus.com/tag/python/
 131. https://www.machinelearningplus.com/tag/r/
 132. https://www.machinelearningplus.com/tag/regex/
 133. https://www.machinelearningplus.com/tag/regression/
 134. https://www.machinelearningplus.com/tag/residual-analysis/
 135. https://www.machinelearningplus.com/tag/scikit-learn/
 136. https://www.machinelearningplus.com/tag/significance-tests/
 137. https://www.machinelearningplus.com/tag/soft-cosine-similarity/
 138. https://www.machinelearningplus.com/tag/spacy/
 139. https://www.machinelearningplus.com/tag/summarization/
 140. https://www.machinelearningplus.com/tag/taggeddocument/
 141. https://www.machinelearningplus.com/tag/textblob/
 142. https://www.machinelearningplus.com/tag/tfidf/
 143. https://www.machinelearningplus.com/tag/time-series/
 144. https://www.machinelearningplus.com/tag/topic-modeling/
 145. https://www.machinelearningplus.com/tag/visualization/
 146. https://www.machinelearningplus.com/tag/id97/
 147. https://www.ezoic.com/what-is-ezoic/
 148. https://www.ezoic.com/what-is-ezoic/
 149. https://www.machinelearningplus.com/
 150. https://www.machinelearningplus.com/blog/
 151. https://www.machinelearningplus.com/category/data-manipulation/
 152. https://www.machinelearningplus.com/category/predictive-modeling/
 153. https://www.machinelearningplus.com/category/statistics/
 154. https://www.machinelearningplus.com/category/nlp/
 155. https://www.machinelearningplus.com/category/python/
 156. https://www.machinelearningplus.com/category/plots/
 157. https://www.machinelearningplus.com/category/time-series/
 158. https://www.machinelearningplus.com/contact-us/
 159. https://www.machinelearningplus.com/privacy-policy/
 160. https://www.machinelearningplus.com/terms-of-use/
 161. https://www.machinelearningplus.com/about

   hidden links:
 163. https://www.machinelearningplus.com/
 164. https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#top
